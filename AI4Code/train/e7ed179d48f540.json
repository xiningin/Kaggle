{"cell_type":{"dda6da4a":"code","673b50e6":"code","ad3e82e5":"code","18863a9c":"code","88119016":"code","8df089ce":"code","cfc46613":"code","026fd121":"code","625fb30f":"code","659a2410":"code","aabe8f5a":"code","13d17c64":"code","517449d6":"code","81e85e10":"code","404e0609":"code","207af0cb":"code","048393be":"code","12a560bf":"code","593e3804":"code","3a59ed11":"code","3c13c056":"code","000fb87e":"code","2ae92a45":"code","e9d2f46e":"code","b1dbd98d":"code","1a4d47f3":"code","c39496a7":"code","322b81d7":"code","31eb89e9":"code","b87e5bcd":"code","e015f95e":"code","b4fc511b":"code","202cae0a":"code","f6b85472":"code","e91bb023":"code","49516700":"code","3ef227cb":"code","58b836ac":"code","084b0d0c":"code","229e4015":"code","97c473e6":"code","3df60bf0":"code","28ae249f":"code","55f3bb99":"code","22cbf146":"code","ebd0b384":"code","e259a1de":"code","e9cdc997":"code","554c030d":"code","5749959c":"code","80d8bda6":"code","d1d23dbb":"code","b38c675a":"code","9a65f483":"code","201954ce":"code","85bd95d7":"code","a4b1eeed":"code","25fd8b47":"code","bcf68c51":"code","3d0a3582":"code","16f7ab03":"code","152d05f5":"code","cbae23a6":"code","597e1c0a":"code","43304f70":"code","357ab17c":"code","ecfa8324":"code","fe43fcf6":"code","a377a2e1":"markdown","5e58d788":"markdown","7c684803":"markdown","c8c5c953":"markdown","40eb245b":"markdown","be775c71":"markdown","4c621fd2":"markdown","ed67db1e":"markdown","a6e9687b":"markdown","377cd2c3":"markdown","9394a28a":"markdown","f3be33a9":"markdown","8dcfcc68":"markdown","8f3398e7":"markdown","457a1056":"markdown","435eb4d8":"markdown","16994ad5":"markdown","bc1eb1f1":"markdown","094d7a1e":"markdown","d2810c02":"markdown","fdb8d6b9":"markdown","5f3f5a03":"markdown","d9fae621":"markdown","bb25abf9":"markdown","83c91a9f":"markdown","e5ffd3d6":"markdown","6a227763":"markdown","fb9be509":"markdown","272143fb":"markdown","69fe7635":"markdown","35d4978a":"markdown","f6923efc":"markdown","6b62834e":"markdown","0bd59900":"markdown","71e151e0":"markdown","dca5b880":"markdown","cd95af45":"markdown","0ad93ffe":"markdown","eeaeb3e8":"markdown","e89eeade":"markdown","5c0698d8":"markdown","38536dea":"markdown","1b8714a3":"markdown","5eec17d9":"markdown","46f4275c":"markdown","65eb35ab":"markdown","1fdcbeea":"markdown","e9c74b75":"markdown","f7fe0c8d":"markdown"},"source":{"dda6da4a":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (12, 10)\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV\nfrom sklearn import metrics\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier, plot_importance","673b50e6":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsub = pd.read_csv(\"..\/input\/gender_submission.csv\")","ad3e82e5":"train.head()","18863a9c":"train.describe(include=\"all\")","88119016":"test.head()","8df089ce":"test.describe(include=\"all\")","cfc46613":"sub.head()","026fd121":"sub.describe()","625fb30f":"corr = train.drop('PassengerId',axis=1).corr()\nsns.heatmap(corr, annot=True, cmap='YlOrBr')","659a2410":"sns.countplot(train['Pclass'], hue=train['Survived'])","aabe8f5a":"plt.hist(train[train[\"Survived\"]==1]['Fare'], label=\"Yes\", alpha=0.7)\nplt.hist(train[train[\"Survived\"]==0]['Fare'], label=\"No\", alpha=0.7)\nplt.legend(title='Survived')\nplt.xlabel(\"Fare\")","13d17c64":"Survival = train.Survived # Label for training set\nfull = pd.concat([train.drop('Survived', axis=1), test])","517449d6":"full.describe(include=\"all\")","81e85e10":"full1 = full.drop(['PassengerId', 'Cabin', 'Ticket'], axis=1)\nfull1.describe(include=\"all\")","404e0609":"pclass = pd.get_dummies(full1['Pclass'], prefix=\"Pclass_\")\npclass.head()","207af0cb":"full1['Sex_'] = np.where(full1.Sex == 'male', 1, 0)\nfull1.head()","048393be":"Embarked = pd.get_dummies(full1['Embarked'], prefix=\"Embarked_\")\nEmbarked.head()","12a560bf":"full2 = pd.concat([full1, pclass, Embarked], axis=1)\nfull2.head()","593e3804":"full3 = full2.drop([\"Pclass\", \"Sex\", \"Embarked\"], axis=1)\nfull3.describe(include=\"all\")","3a59ed11":"Title = pd.get_dummies(full.Name.map(lambda x: x.split(',')[1].split('.')[0].split()[-1]))\nTitle.head()","3c13c056":"full3['FamilySize'] = full3.SibSp + full3.Parch + 1\nfull3['Single'] = np.where((full3.SibSp + full3.Parch) == 0, 1, 0)","000fb87e":"full4 = pd.concat([full3, Title], axis=1)\nfull4.drop('Name', axis=1, inplace=True)\nfull4.head()","2ae92a45":"plt.figure(figsize=(26, 24))\nsns.heatmap(full4.corr(), annot=True, cmap=\"YlOrBr\")","e9d2f46e":"full5 = full4.loc[:,'Age':'Single'] # or: full4.iloc[:,0:13]\nfull5.head()","b1dbd98d":"plt.figure(figsize=(16, 14))\nsns.heatmap(full5.corr(), annot=True, cmap=\"YlOrBr\")","1a4d47f3":"full6 = full5.drop(['SibSp','Parch'], axis=1)\nfull6.head()","c39496a7":"plt.figure(figsize=(16, 14))\nsns.heatmap(full6.corr(), annot=True, cmap=\"YlOrBr\")","322b81d7":"train_full = full6.iloc[:891]\ntest_full = full6.iloc[891:]","31eb89e9":"train_full.describe(include=\"all\") # missing in Age","b87e5bcd":"test_full.describe(include=\"all\") # missing in Age and Fare","e015f95e":"# Impute Age in the training set\ntrain_age_imputer = SimpleImputer()\ntrain_imputed = train_full.copy()\ntrain_imputed['Age_'] = train_age_imputer.fit_transform(train_full.iloc[:,0:1])\ntrain_imputed['Fare_'] = train_imputed['Fare'] # not imputing Fare here, just kepp the training set consistent with the test set\ntrain_imputed.drop(['Age', 'Fare'], axis=1, inplace=True)\ntrain_imputed.head()","b4fc511b":"# Impute Age, Fare in the test set\ntest_age_imputer = SimpleImputer()\ntest_fare_imputer = SimpleImputer()\n\ntest_imputed = test_full.copy()\ntest_imputed['Age_'] = test_age_imputer.fit_transform(test_full.iloc[:,0:1])\ntest_imputed['Fare_'] = test_age_imputer.fit_transform(test_full.iloc[:,1:2])\n\ntest_imputed.drop([\"Age\",\"Fare\"], axis=1, inplace=True)\ntest_imputed.head()","202cae0a":"train_imputed.describe(include=\"all\")","f6b85472":"test_imputed.describe(include=\"all\")","e91bb023":"sns.countplot(Survival, palette=\"coolwarm\")","49516700":"sns.countplot(Survival, hue=train_imputed[\"Sex_\"], palette=\"coolwarm\")","3ef227cb":"sns.countplot(Survival, hue=train[\"Pclass\"], palette=\"coolwarm\")","58b836ac":"sns.countplot('Pclass', data=train, palette=\"coolwarm\")","084b0d0c":"plt.hist(train_imputed[Survival==1]['Age_'], label=\"Survived\", alpha=0.7)\nplt.hist(train_imputed[Survival==0]['Age_'], label=\"Not Survived\", alpha=0.7)\nplt.legend()\nplt.xlabel(\"Age_Imputed\")","229e4015":"# Original data set\nplt.hist(train[train[\"Survived\"]==1]['Fare'], label=\"Survived\", alpha=0.5)\nplt.hist(train[train[\"Survived\"]==0]['Fare'], label=\"Not Survived\", alpha=0.5)\nplt.legend()\nplt.xlabel(\"Fare\")\nplt.title(\"Original Fare\")","97c473e6":"# Imputed data set\nplt.hist(train_imputed[Survival==1]['Fare_'], label=\"Survived\", alpha=0.5)\nplt.hist(train_imputed[Survival==0]['Fare_'], label=\"Not Survived\", alpha=0.5)\nplt.legend()\nplt.xlabel(\"Fare\")\nplt.title(\"Imputed Fare\")","3df60bf0":"# Using original data set\nsns.countplot('Embarked', data=train, palette=\"coolwarm\")","28ae249f":"# Using original data set\nsns.countplot(Survival, data=train, hue=\"Embarked\", palette=\"coolwarm\")","55f3bb99":"sns.countplot('FamilySize', data=train_imputed, palette=\"coolwarm\", hue=Survival)\nplt.legend(loc=1)","22cbf146":"sns.countplot('Single', data=train_imputed, palette=\"coolwarm\", hue=Survival)\nplt.legend(loc=1)","ebd0b384":"kfold = KFold(n_splits=5, random_state=1, shuffle=True)\nkfold","e259a1de":"accuracy = {}","e9cdc997":"m1_nb = GaussianNB()","554c030d":"accuracy['Gaussian Naive Bayes'] = np.mean(cross_val_score(m1_nb, train_imputed, Survival, scoring=\"accuracy\", cv=kfold))","5749959c":"m2_log = LogisticRegression(solver='newton-cg') # 'lbfgs', 'sag' failed to converge","80d8bda6":"accuracy['Logistic Regression'] = np.mean(cross_val_score(m2_log, train_imputed, Survival, scoring=\"accuracy\", cv=kfold))","d1d23dbb":"m3_knn = KNeighborsClassifier(n_neighbors = 5)","b38c675a":"accuracy['K Nearest Neighbors'] = np.mean(cross_val_score(m3_knn, train_imputed, Survival, scoring=\"accuracy\", cv=kfold))","9a65f483":"m4_rf = RandomForestClassifier(n_estimators=10)","201954ce":"accuracy['Random Forest'] = np.mean(cross_val_score(m4_rf, train_imputed, Survival, scoring=\"accuracy\", cv=kfold))","85bd95d7":"m5_svc = SVC(gamma='scale')","a4b1eeed":"accuracy['SVM'] = np.mean(cross_val_score(m5_svc, train_imputed, Survival, scoring=\"accuracy\", cv=kfold))","25fd8b47":"m6_gb = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)","bcf68c51":"accuracy['Gradient Boosting'] = np.mean(cross_val_score(m6_gb, train_imputed, Survival, scoring=\"accuracy\", cv=kfold))","3d0a3582":"accuracy","16f7ab03":"max_accuracy = max(accuracy, key=accuracy.get)\nprint(max_accuracy, '\\taccuracy:', accuracy[max_accuracy])","152d05f5":"train_imputed.columns","cbae23a6":"m6_gb.fit(train_imputed, Survival)\nm6_gb.feature_importances_","597e1c0a":"plot_importance(m6_gb)","43304f70":"param_grid = {'max_depth': [1,3,5,10,15], 'n_estimators': [50,100,200,500,1000], 'learning_rate': [1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(XGBClassifier(), param_grid, cv=kfold)\ngrid.fit(train_imputed[[\"Fare_\", \"Age_\", \"FamilySize\", \"Sex_\"]], Survival)  # with 4 selected features\ngrid.best_params_","357ab17c":"gb = XGBClassifier(max_depth=3, n_estimators=1000, learning_rate=0.01)\nnp.mean(cross_val_score(gb, train_imputed[[\"Fare_\", \"Age_\", \"FamilySize\", \"Sex_\"]], Survival, scoring=\"accuracy\", cv=kfold))","ecfa8324":"gb.fit(train_imputed[[\"Fare_\", \"Age_\", \"FamilySize\", \"Sex_\"]], Survival)\npredictions = gb.predict(test_imputed[[\"Fare_\", \"Age_\", \"FamilySize\", \"Sex_\"]])","fe43fcf6":"submission = pd.DataFrame({ 'PassengerId': test.PassengerId,\n                            'Survived': predictions })\nsubmission.to_csv(\"TitanicSubmission.csv\", index=False)","a377a2e1":"Drop `SibSp` and `Parch` because they are highly correlated `FamilySize`.","5e58d788":"### Grid Search for Hyperparameters","7c684803":"According to the table above:\n- Approximately 38.4% passengers survived;\n- There are missing values in the 'Age' column, the 'Cabin' column, and the 'Embarked' column;\n- Someone went onboard for free, it knid of makes sense if they were invited.","c8c5c953":"### 3. K-nearest Neighbors","40eb245b":"According to the plot above:\n - `Survival` is correlated with `Pclass` (-0.34) and `Fare` (0.26), while `PassengerId` (-0.005), `Age` (-0.077), `SibSp` (-0.035), and `Parch` (0.082) don't  seem to be correlated with `Survival`;\n - `Age` and `Fare` seem to be correlated with `Pclass`;\n - `SibSp` seems to be correlated with `Age`;\n - `Parch` seems to be correlated with `SibSp`.","be775c71":"### 4. Random Forests","4c621fd2":"For those who survived, the numbers in the three classes are similar. Is it because of the different number of passengeres in each class? Let's check!","ed67db1e":"### Is there any outliers or abnormal values?\nAccording to the results of `describe` above, all the values of all the variables seem reasonable.","a6e9687b":"### Load the datasets","377cd2c3":"### Check correlations on new features","9394a28a":"### Survival by Age\nThe age distributions of survived and not survived seem similar, while more kids survived and more people in their 20s didn't survive. There must be other variables influencing Survival.","f3be33a9":"### Extract information from `Name`","8dcfcc68":"### Take a look at the datasets.","8f3398e7":"The prediction above was made based on gender, where approximately 36.4% passengers survived.","457a1056":"The number of passengers in class 3 is twice as the numbers in class 1 and class 2, which makes sense.","435eb4d8":"### Survival by FamilySize\nIt seems like those who had a family size of 2 or 3 or 4 are more likely to survive.","16994ad5":"# More EDA","bc1eb1f1":"### Check correlations without those titles\nThose titles are correlated with `Sex`, especially `Mr`, `Miss` and `Mrs`. So I decide not to keep these titles.","094d7a1e":"### Divide back to training and test sets","d2810c02":"According to the table above:\n- There are missing values in the 'Age' column, the 'Fare' column, and the 'Cabin' column;\n- Someone went onboard for free, it knid of makes sense if they were invited.","fdb8d6b9":"### Survival vs Pclass\nMore people in class 1 survived, while more people in class 2 didn't and much more people in class** 3 didn't. And there are much more people in class 3 than in class 1 and 2.","5f3f5a03":"### Survival vs Fare\nPassengers paid more tended to be more likely to **survive.","d9fae621":"### Survival by Pclass\nFor those who didn't survive, most of them are in the lowest class, class 3.","bb25abf9":"### Survival by Sex\nMore men didn't survive.","83c91a9f":"### Check again to make sure there is no missing values","e5ffd3d6":"### Load some libraries","6a227763":"# Important features","fb9be509":"### Survival by Embarked\nMost people embarked Titanic form port \"S\". The survival condition seems similar for all three ports.","272143fb":"## Let's start modeling!\nSince this is a classification problem, we will use the following six methods.   \nDivide training set into training and validation sets (5-fold cross validation).","69fe7635":"### Predictions","35d4978a":"### 6. Gradient Boosting","f6923efc":"### Remove `PassengerId`, `Cabin`, and `Ticket`","6b62834e":"# Initial EDA","0bd59900":"### 2. Logistic Regression","71e151e0":"### 1. Gaussian Naive Bayes","dca5b880":"### Survival by Single\nIt seems like those who are single are less likely to survive; those who have family are almost equally likely to survive as not survive.","cd95af45":"## Seems like the best model here is **Gradient Boosting**","0ad93ffe":"## WELL DONE!","eeaeb3e8":"\n### One-hot encode `Pclass`, `Sex` and `Embarked`","e89eeade":"### Combine train and test for preprocessing","5c0698d8":"# Submit","38536dea":"# Feature engineering\n- Remove `PassengerId` because it's not informative;\n- Remove `Cabin` because it has too many missing values;\n- Remove `Ticket`;\n- Represent categorical variables with dummy variables;\n- Extract information from `Name`;\n- Create a new variable indicating family size;\n- Create a new variable indicating whether the passenger is single.","1b8714a3":"### 5. Support Vector Machines","5eec17d9":"### Survival Condition of the Titanic Tragedy\nMore people didn't survive.","46f4275c":"### Create a new variable indicating family size and another new variable indicating whether the passenger is single","65eb35ab":"### Correlations among the continuous variables","1fdcbeea":"# Impute the missing values\nAssume that the missing mechnaism is MAR (missing at random).  \nImpute missing data in training and test set separately, because we don't want any information of the test set being leaked into the training set.","e9c74b75":"### Survival by Fare\nIt seems like those paid more are more likely to survive.   \n*Distrubution  of the imputed data set seems very similar as the original one.*","f7fe0c8d":"# **Titanic Survival Prediction**\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we are goint to complete the analysis of **what sorts of people were likely to survive**. In particular, **machine learning tools** will be applied to predict which passengers survived the tragedy."}}