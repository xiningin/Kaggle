{"cell_type":{"2c55e491":"code","ce2ee0d0":"code","f883a5a6":"code","8673d46c":"code","2807a939":"code","628a66db":"code","4072ca6d":"code","4b5ecff4":"code","0090ce7d":"code","a343fa7b":"code","ee63d43c":"code","9d8d7d22":"code","8b1f1536":"code","a8625c3c":"markdown","32efefb9":"markdown","2050c1d8":"markdown","999ce76a":"markdown","16fa8ffd":"markdown","f314e99d":"markdown","0f8b5e5c":"markdown","15a12d9c":"markdown","3fb8dbc9":"markdown","a2ffa94e":"markdown","a010ad0e":"markdown","ec28a0de":"markdown","7f9e816c":"markdown"},"source":{"2c55e491":"import tensorflow as tf\nimport keras\nimport tensorflow_probability as tfp\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport copy\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter, MultipleLocator\nimport matplotlib.ticker as ticker","ce2ee0d0":"def negFun(x):#\u4e8c\u7ef4\u51fd\u6570,\u5373\u7b49\u5f0f\u53f3\u8fb9\u7684-f(x,y)\n    return tf.cast(-1.0*tf.ones(x[:,0].shape),tf.float64)#\u9f50\u6b21;\u53ef\u4ee5\u8bbe\u4e3atf.math.exp((x[:,0]-0.5)**2+(x[:,1]-0.5)**2)","f883a5a6":"def modelPlot(model): #\u7ed8\u5236\u9884\u6d4b\u51fa\u7684U(x,y)\n    L=1.0\n    gridNum = 100\n    h = L\/gridNum\n    X1 = tf.Variable(tf.random.normal(((gridNum+1)**2,2)),name='w');\n    for i in range(gridNum+1):\n      for j in range(gridNum+1):\n        v = i*(gridNum+1) + j\n        X1[v,0].assign(h*float(i)-L)\n        X1[v,1].assign(h*float(j)-L)\n    X2 = tf.Variable(tf.random.normal(((gridNum+1)*(gridNum),2)),name='w');\n    for i in range(gridNum+1):\n      for j in range(gridNum):\n        v = i*(gridNum) + j\n        X2[v,0].assign(h*float(j+1))\n        X2[v,1].assign(h*float(i)-L)\n    X3 = tf.Variable(tf.random.normal(((gridNum+1)*(gridNum),2)),name='w');\n    for i in range(gridNum+1):\n      for j in range(gridNum):\n        v = i*(gridNum) + j\n        X3[v,0].assign(h*float(i)-L)\n        X3[v,1].assign(h*float(j+1))\n    # plt.scatter(X1[:,0],X1[:,1],c = 'g')\n    # plt.scatter(X2[:,0],X2[:,1],c = 'r')\n    # plt.scatter(X3[:,0],X3[:,1],c = 'b')\n    # plt.show()\n    # print(X2)\n    U1 = model(X1)     #\u7eff\u8272\u90e8\u5206\n    U2 = model(X2)     #\u7ea2\u8272\u90e8\u5206\n    U3 = model(X3)     #\u84dd\u8272\u90e8\u5206\n    Z = np.zeros((gridNum+1,gridNum+1)) #\u7eff\u8272\n    for i in range(gridNum+1):\n      for j in range(gridNum+1):\n        v = i*(gridNum+1) + j\n        Z[i][j] = float(U1[v])\n    X = np.arange(-L-L\/gridNum,0,L\/gridNum)\n    Y = np.arange(-L-L\/gridNum,0,L\/gridNum)\n    X, Y = np.meshgrid(X, Y)\n    plt.rcParams['savefig.dpi'] = 300 #\u56fe\u7247\u50cf\u7d20\n    plt.rcParams['figure.dpi'] = 300 #\u5206\u8fa8\u7387\n    fig = plt.figure()\n    ax = fig.gca(projection='3d')\n    ax.set_zlim(-0.1,0.2)\n    zmajorLocator   = MultipleLocator(0.05)\n    ax.zaxis.set_major_locator(zmajorLocator)\n    ax.tick_params(which='major',length=5,labelsize=7)\n    #plt.subplot(2,2,1)\n    surf = ax.plot_surface(X,Y,Z, cmap=\"rainbow\",linewidth=0, antialiased=False)\n    ax.view_init(90,0)\n   \n    Z = np.zeros((gridNum+1,gridNum)) \n    for i in range(gridNum+1):\n      for j in range(gridNum):\n        v = i*(gridNum) + j\n        Z[i][j] = float(U3[v])\n    X = np.arange(L\/gridNum,L+L\/gridNum,L\/gridNum)\n    Y = np.arange(-L-L\/gridNum,0,L\/gridNum)\n    X, Y = np.meshgrid(X, Y)\n    #fig = plt.figure()\n    #ax = fig.gca(projection='3d')\n    #plt.subplot(2,2,1)\n    surf = ax.plot_surface(X,Y,Z, cmap=\"rainbow\",linewidth=0, antialiased=False)\n    ax.view_init(90,0)\n   \n    Z = np.zeros((gridNum,gridNum+1))\n    for i in range(gridNum+1):\n      for j in range(gridNum):\n         v = i*(gridNum) + j\n         Z[j][i] = float(U2[v])\n    X = np.arange(-L-L\/gridNum,0,L\/gridNum)\n    Y = np.arange(L\/gridNum,L\/gridNum+L,L\/gridNum)\n    X, Y = np.meshgrid(X, Y)\n    #fig = plt.figure()\n    #ax = fig.gca(projection='3d')\n    #plt.subplot(2,2,1)\n    surf = ax.plot_surface(X,Y,Z,cmap=\"rainbow\",linewidth=0, antialiased=False)\n    ax.view_init(90,0)\n    # Add a color bar which maps values to colors.\n    #fig.colorbar(surf, shrink=0.5, aspect=5)\n    cb1 = fig.colorbar(surf, shrink=0.5, aspect=5)\n    tick_locator = ticker.MaxNLocator(nbins=5)  # colorbar\u4e0a\u7684\u523b\u5ea6\u503c\u4e2a\u6570\n    cb1.locator = tick_locator\n    cb1.set_ticks([np.min(Z), 0,0.05, 0.10, np.max(Z)])\n    cb1.update_ticks()\n    plt.xlabel('x') \n    plt.ylabel('y')\n    plt.title('predicted u')\n    plt.savefig(\"L_predicted.png\")\n    plt.show()","8673d46c":"def generate_L_Trainset(gridNum):\n        XX = np.linspace(-1.0,1.0,gridNum*2+1)\n        pL = []\n        for i in range(XX.shape[0]):\n            for j in range(XX.shape[0]):\n                pL.append([XX[i],XX[j]])\n        pL = np.array(pL)\n        s1 = np.where(pL[:,0]>0.0)[0]\n        s2 = np.where(pL[:,1]>0.0)[0]\n        s = [i for i in s1 if i in s2]\n        ex = np.arange(0,pL.shape[0],1)\n        ex = [i for i in ex if i not in s]\n        ed1 = np.where(pL[:,0] == -1.0)[0]\n        ed2 = np.where(pL[:,1] == -1.0)[0]\n        ed3 = np.where(pL[:,0] == 1.0)[0]\n        ed3 = [i for i in ed3 if i  in ex]\n        ed4 = np.where(pL[:,1] == 1.0)[0]\n        ed4 = [i for i in ed4 if i  in ex]\n        ed5 = np.where(pL[:,0] == 0.0)[0]\n        ex5 = np.where(pL[:,1]>=0)[0]\n        ex5 = [i for i in ex5 if i in ed5]\n        ed6 = np.where(pL[:,1] == 0.0)[0]\n        ex6 = np.where(pL[:,0]>=0)[0]\n        ex6 = [i for i in ex6 if i in ed6]\n        edge = np.unique(np.concatenate([ed1,ed2,ed3,ed4,ex5,ex6],0))\n        edge = edge.astype(int)\n        ex = [i for i in ex if i not in edge]\n        interior = pL[ex]\n        boundary = pL[edge]\n        return tf.cast(tf.convert_to_tensor(interior),tf.float64),tf.cast(tf.convert_to_tensor(boundary),tf.float64)\ndef regionPlot(x_inner,x_boundarys):\n    plt.scatter(x_inner[:,0],x_inner[:,1],c= 'b')\n    plt.scatter(x_boundarys[:,0],x_boundarys[:,1],c= 'r')\n    plt.show()","2807a939":"#\u6d4b\u8bd5\u6b63\u786e\u7387\u7684\u4e00\u73af\nx_1,x_2 = generate_L_Trainset(10)\nx_validate =  tf.Variable(tf.concat([x_1,x_2],0))\nregionPlot(x_1,x_2)","628a66db":"def modelTrainAdam(model,x_inner,x_boundarys,loss):\n     loss_fun = tf.keras.losses.MeanSquaredError()\n     boundary_num = x_boundarys.shape[0]\n     #x_inner,x_boundarys = generateTrainset(12000,3000)\n     x =  tf.concat([x_boundarys,x_inner],0)\n     with tf.GradientTape() as tape:\n                with tf.GradientTape() as g0:\n                      g0.watch(x)\n                      with tf.GradientTape() as g:\n                          g.watch(x)\n                          U = model(x)#outMagnify(model(x),-1,1)\n                      dy_dx = g.gradient(U,x)\n                batch_jacobian = g0.batch_jacobian(dy_dx, x)\n                Uxx = batch_jacobian[:,0,0]\n                Uyy = batch_jacobian[:,1,1]\n                #print(laplaceOperator.shape)\n                lap = Uxx + Uyy\n                #print(lap)\n                neg = negFun(x)\n                interior = loss(lap-neg,0.0)\n                boundary = loss(U[0:boundary_num,:],tf.zeros(U[0:boundary_num,:].shape))\n                lv = tf.cast(pdeCoef*interior+boundaryCoef*boundary,tf.float64)\n#                 pdeCoef = boundary\/interior\n#                 boundaryCoef = 1.0 - pdeCoef\n     grads = tape.gradient(lv, model.trainable_variables)\n     #assign_new_model_parameters(params_1d)\n     for i in range(len(grads)):\n            if grads[i] is not None:\n                 grads[i] = tf.clip_by_norm(grads[i], 5)  # \u9608\u503c\u8fd9\u91cc\u8bbe\u4e3a5\n     adamOptimizer.apply_gradients(zip(grads, model.trainable_variables))\n     return lv\n","4072ca6d":"# model = tf.keras.Sequential(\n# [tf.keras.Input(shape=[2,]),\n# tf.keras.layers.Dense(125, \"tanh\"),\n# tf.keras.layers.Dense(100, \"tanh\"),\n# tf.keras.layers.Dense(75, \"tanh\"),\n# tf.keras.layers.Dense(50, \"tanh\"),\n# tf.keras.layers.Dense(25, \"tanh\"),\n# tf.keras.layers.Dense(1, \"tanh\")])\nmodel = keras.models.load_model('..\/input\/model-saved\/20211203_1.h5')\nmodel.summary() #\u8f93\u51fa\u7f51\u7edc\u7ed3\u6784\n\n","4b5ecff4":"adamOptimizer = tf.keras.optimizers.Adam(lr=0.0003)\nloss = tf.keras.losses.MeanSquaredError()              #\u635f\u5931\u51fd\u6570\uff0cMSE\nmaxepoch = 10000                                         #3000\u6b21\nerr_list = np.zeros((1,maxepoch))                      #\u7528\u6765\u7ed8\u5236\u8bad\u7ec3\u8bef\u5dee","0090ce7d":"x_1,x_2 = generate_L_Trainset(50)\nregionPlot(x_1,x_2)","a343fa7b":"loss = tf.keras.losses.MeanSquaredError()\n#boundary_num = x_boundary.shape[0]\n\nshapes = tf.shape_n(model.trainable_variables)\nn_tensors = len(shapes)\ncount = 0\nidx = [] # stitch indices\npart = [] # partition indices\n\nfor i, shape in enumerate(shapes):\n    n = np.product(shape)\n    idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n    part.extend([i]*n)\n    count += n\n\npart = tf.constant(part)\nprint(count)","ee63d43c":"m = 50 #\u9650\u5236\u8bb0\u5f55\u6b21\u6570 \ns_list = np.zeros((count,m))#delta x\u7684\u5217\u8868\ny_list = np.zeros((count,m))#delta gradient\u5217\u8868\nrho = np.zeros((1,m))#sk'yk\u7684\u5217\u8868\nLBFGS_error_decline = np.zeros((1,15000))\n\n\n\n\ndef assignWeights(model,paraments):#\u5c06\u67d0\u4e2a\u66f4\u65b0\u8fc7\u7684\u6743\u503c\u5411\u91cf\u8fd4\u8fd8\u5230\u6a21\u578b\u53c2\u6570\n    #print(model.trainable_variables[0])\n    for i in range(len(model.trainable_variables)):\n        tmp = tf.reshape(idx[i],[tf.reduce_prod(shapes[i].numpy()),1])\n        model.trainable_variables[i].assign(tf.reshape(paraments[tmp[0,0]:tmp[-1,0]+1,0],shapes[i].numpy()))\n    #print(model.trainable_variables[0])\ndef reSet(model,m_t):\n    for i in range(len(m_t)):\n        model.trainable_variables[i].assign(m_t[i])\ndef calculateLoss_Grad(model,x_inner,x_boundary):\n    #------------------------------------------------------\n    x = tf.Variable(tf.concat([x_boundary,x_inner],0))\n    dy_dx = tf.Variable([])\n    with tf.GradientTape() as tape:\n                with tf.GradientTape() as g:\n                    g.watch(x)\n                    with tf.GradientTape() as g0:            \n                          #g0.watch(dy_dx)\n                          U = model(x)#outMagnify(model(x),-1,1)\n                          #print(U)\n                          dy_dx = g0.gradient(U,x)\n                          #print(dy_dx)\n                batch_jacobian = g.batch_jacobian(dy_dx, x)\n                #print(batch_jacobian)\n                Uxx = batch_jacobian[:,0,0]\n                Uyy = batch_jacobian[:,1,1]\n                #print(laplaceOperator.shape)\n                lap = Uxx + Uyy\n                neg = negFun(x)\n                #print(b_num)\n                interior = loss(lap-neg,0.0)\n                boundary = loss(U[0:x_boundary.shape[0],:],0.0)\n                dev_b = loss(lap[0:x_boundary.shape[0]]-neg[0:x_boundary.shape[0]],0.0)\n                loss_value = tf.cast(pdeCoef*interior+boundaryCoef*boundary,tf.float64)\n#                 pdeCoef = b_num*dev_b\/interior\/lap.shape[0]                       #this is how lambda method works\n#                 boundaryCoef = 1.0 - pdeCoef                   #seeing Neural Network for Solving PDEs on youTube\n    grad = tape.gradient(loss_value,model.trainable_variables)  \n    #------------------------------------------------------\n    return loss_value,grad\n\ndef L_BFGS(model,x_inner,x_boundary,maxepoch = 10000):\n    current = 0\n    loss_value,grad = calculateLoss_Grad(model,x_inner,x_boundary)\n    #print('original loss = {:.8f}'.format(float(loss_value)))\n    lr = 0.001 #\u6839\u636e\u51fd\u6570\u662f\u5426\u4e0b\u964d\u8c03\u6574\n    m_t = copy.deepcopy(model.trainable_variables)\n    w_line = tf.reshape(model.trainable_variables[0],[tf.reduce_prod(shapes[0].numpy()),1])\n    old_gd = tf.reshape(grad[0],[tf.reduce_prod(shapes[0].numpy()),1])\n    for i in range(1,len(model.trainable_variables)):\n             old_gd = tf.concat([old_gd,tf.reshape(grad[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n             w_line = tf.concat([w_line,tf.reshape(model.trainable_variables[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n    #print(old_gd)\n    #print(Gk,old_gd)\n    #print('inner product of search gradient and -gradient:{:.8f}'.format(float(np.sum(-old_gd*old_gd))))\n    old_gd = -lr*old_gd\n    s_list[:,0] = tf.cast(old_gd,tf.float64).numpy().flatten()#\u521d\u59cb\u65b9\u5411\u53d6\u4e3a\u68af\u5ea6\u4e0b\u964d\u65b9\u5411\n    w_line = w_line + tf.cast(old_gd,tf.float32)\n    assignWeights(model,w_line)\n    lv,gd = calculateLoss_Grad(model,x_inner,x_boundary)\n    #print(lv)\n    \n    yk = tf.reshape(gd[0],[tf.reduce_prod(shapes[0].numpy()),1])\n    for i in range(1,len(model.trainable_variables)):\n         yk = tf.concat([yk,tf.reshape(gd[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n    yk = tf.cast(yk - old_gd,tf.float64)\n    y_list[:,0] = yk.numpy().flatten()\n    rho[0,0] = np.sum(y_list[:,0]*s_list[:,0])\n    \n    old_gd = tf.reshape(gd[0],[tf.reduce_prod(shapes[0].numpy()),1])\n    for i in range(1,len(model.trainable_variables)):\n             old_gd = tf.concat([old_gd,tf.reshape(gd[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n    odd = old_gd.numpy().flatten()\n    p = odd - np.sum(odd*s_list[:,0])*y_list[:,0]\/rho[0,0]\n    p = p - np.sum(p*y_list[:,0])*s_list[:,0]\/rho[0,0]\n    p = p + np.sum(s_list[:,0]*odd)*s_list[:,0]\/rho[0,0]\n    #print('inner product of search direction and gradient:{:.8f}'.format(float(np.sum(-p*odd))))\n    p = -p*lr\n    s_list[:,1] = p\n    w_line = w_line + tf.cast(tf.convert_to_tensor(p),tf.float32)\n    assignWeights(model,w_line)\n    lv,gd = calculateLoss_Grad(model,x_inner,x_boundary)\n    LBFGS_error_decline[0,current] = float(lv)\n    current = current + 1\n    #print(lv)\n    \n    def linear_transform_1(index,p):#si'si*p\n        return np.sum(s_list[:,index]*p)*s_list[:,index]\n    def linear_transform_2(index,p):#Vi*p\n        return p - np.sum(s_list[:,index]*p)*y_list[:,index]\/rho[0,index]\n    def linear_transform_3(index,p):\n        return p - np.sum(y_list[:,index]*p)*s_list[:,index]\/rho[0,index]\n    for j in range(1,m):   #\u524dm\u6b21\u8fed\u4ee3\n        yk = tf.reshape(gd[0],[tf.reduce_prod(shapes[0].numpy()),1])\n        for i in range(1,len(model.trainable_variables)):\n             yk = tf.concat([yk,tf.reshape(gd[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n        yk = tf.cast(yk - old_gd,tf.float64)\n        y_list[:,j] = yk.numpy().flatten()\n        rho[0,j] = np.sum(y_list[:,j]*s_list[:,j])\n        old_gd = tf.reshape(gd[0],[tf.reduce_prod(shapes[0].numpy()),1])#\u68af\u5ea6\n        for i in range(1,len(model.trainable_variables)):\n             old_gd = tf.concat([old_gd,tf.reshape(gd[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n        odd = old_gd.numpy().flatten()\n        q1 = copy.deepcopy(odd)\n        for i in range(j+1):\n            q1 = linear_transform_2(j-i,q1)\n        for i in range(j+1):\n            q1 = linear_transform_3(i,q1)\n        #print(q1)\n        for i in range(0,j):\n             q2 = copy.deepcopy(odd)      \n             for k in range(j-i):\n                 #print('V[{:d}]'.format(j-k),end='')\n                 q2 = linear_transform_2(j-k,q2)\n             q2 = linear_transform_1(i,q2)\n             for k in range(j-i):\n                 q2 = linear_transform_2(k+i+1,q2)\n             q1 = q1 + q2\/rho[0,i]\n        q1 = q1 + linear_transform_1(j,odd)\/rho[0,j]\n        #if float(np.sum(-q1*odd))>0\n        #print('inner product of search direction and gradient:{:.8f}'.format())\n        #q1 = odd\n        #print('inner product of search direction and gradient:{:.8f}'.format(float(np.sum(-q1*odd))))\n        q1 = -q1*lr #\u65b0\u641c\u7d22\u65b9\u5411\n        w_line = w_line + tf.cast(tf.convert_to_tensor(q1),tf.float32)\n        assignWeights(model,w_line)\n        lv,gd = calculateLoss_Grad(model,x_inner,x_boundary)\n        LBFGS_error_decline[0,current] = float(lv)\n        current = current + 1\n        print('epoch = {:d}'.format(j),end = '|')\n        print(' = {:.8f}'.format(float(lv)))\n        if j < m-1:\n           s_list[:,j+1] = q1\n    #print(s_list[:,-1])\n    #print(y_list[:,-1])\n    for j in range(maxepoch-m):\n        s_list[:,0:-2] = s_list[:,1:-1]\n        y_list[:,0:-2] = y_list[:,1:-1]\n        rho[0,0:-2] = rho[0,1:-1]\n        yk = tf.reshape(gd[0],[tf.reduce_prod(shapes[0].numpy()),1])\n        for i in range(1,len(model.trainable_variables)):\n             yk = tf.concat([yk,tf.reshape(gd[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n        yk = tf.cast(yk - old_gd,tf.float64)\n        y_list[:,m-1] = yk.numpy().flatten()\n        rho[0,m-1] = np.sum(y_list[:,m-1]*s_list[:,m-1])\n        old_gd = tf.reshape(gd[0],[tf.reduce_prod(shapes[0].numpy()),1])#\u68af\u5ea6\n        for i in range(1,len(model.trainable_variables)):\n             old_gd = tf.concat([old_gd,tf.reshape(gd[i],[tf.reduce_prod(shapes[i].numpy()),1])],0)\n        odd = old_gd.numpy().flatten()\n        q1 = copy.deepcopy(odd)\n        for i in range(m):\n            q1 = linear_transform_2(m-1-i,q1)\n        for i in range(m):\n            q1 = linear_transform_3(i,q1)\n        #print(q1)\n        for i in range(0,m-1):\n             q2 = copy.deepcopy(odd)      \n             for k in range(m-1-i):\n                 #print('V[{:d}]'.format(j-k),end='')\n                 q2 = linear_transform_2(m-1-k,q2)\n             q2 = linear_transform_1(i,q2)\n             for k in range(m-1-i):\n                 q2 = linear_transform_2(k+i+1,q2)\n             q1 = q1 + q2\/rho[0,i]\n        q1 = q1 + linear_transform_1(m-1,odd)\/rho[0,m-1]\n        #q1 = odd\n        #print('inner product of search direction and gradient:{:.8f}'.format(float(np.sum(-q1*odd))))\n        q1 = -q1*lr #\u65b0\u641c\u7d22\u65b9\u5411\n        w_line = w_line + tf.cast(tf.convert_to_tensor(q1),tf.float32)\n        assignWeights(model,w_line)\n        lv,gd = calculateLoss_Grad(model,x_inner,x_boundary)\n        LBFGS_error_decline[0,current] = float(lv)\n        current = current + 1\n        print('epoch = {:d}'.format(j+m),end = '|')\n        print(' = {:.8f}'.format(float(lv)))\n        plt.rcParams['savefig.dpi'] = 300 #\u56fe\u7247\u50cf\u7d20\n        plt.rcParams['figure.dpi'] = 300 #\u5206\u8fa8\u7387       \n        plt.plot(LBFGS_error_decline[np.where(LBFGS_error_decline>0)],'r')\n        plt.title('LBFGS training error loss')\n        plt.savefig(\"LBFGS error decline.png\")","9d8d7d22":"#x_train =  tf.Variable(tf.concat([x_boundary,x_inner],0))\ntest_count = 0     #\u8bb0\u5f55\u6bcf\u4e00\u6b21\u6d4b\u8bd5\u96c6\u6570\u636e\u7cbe\u5ea6\ntest_period = 10   #\u6bcf\u969410\u6b21\u68c0\u67e5\u6d4b\u8bd5\u96c6\npdeCoef = 0.5\nboundaryCoef = 0.5\n#boundary_num = x_boundary.shape[0]\nfor i in range(maxepoch):\n    rs = modelTrainAdam(model,x_1,x_2,loss)\n    #print(rs)\n    if i%test_period == 0:\n        #modelPlot(model)\n        #boundary_num = x_2.shape[0]\n        loss_value,grad = calculateLoss_Grad(model,x_1,x_2)\n        err_list[0,test_count]=float(loss_value)\n        test_count = test_count + 1\n        print('epoch = '+str(i+1),end='|')\n        print('validation_loss = {:.8f}' .format(loss_value),end='|')\n        print('train_loss = {:.8f}' .format(rs),end='|')\n        print('pdeCoef = {:.8f}'.format(pdeCoef))\n#         x_inner,x_boundary = generate_L_Trainset(50)\n#         x_train =  tf.Variable(tf.concat([x_boundary,x_inner],0))\n        #boundary_num = x_boundary.shape[0]\n        if(rs<1.0e-5):\n            break\nplt.rcParams['savefig.dpi'] = 300 #\u56fe\u7247\u50cf\u7d20\nplt.rcParams['figure.dpi'] = 300 #\u5206\u8fa8\u7387       \nplt.plot(err_list[np.where(err_list>0)],'r')\nplt.title('SGD training error loss')\nplt.savefig(\"SGD error decline.png\")\nplt.show()\n# L_BFGS(model,x_1,x_2)\n#print(lv)\n# BFGS_maxepoch = 3000\n# err_list = np.zeros((1,BFGS_maxepoch))                      #\u7528\u6765\u7ed8\u5236\u8bad\u7ec3\u8bef\u5dee\n# test_count = 0     #\u8bb0\u5f55\u6bcf\u4e00\u6b21\u6d4b\u8bd5\u96c6\u6570\u636e\u7cbe\u5ea6\n# test_period = 10   #\u6bcf\u969410\u6b21\u68c0\u67e5\u6d4b\u8bd5\u96c6\n# #x_inner,x_boundary = generate_L_Trainset(1200,150)\n# #Gk = generate_first_hessian(model,x_train,boundary_num,pdeCoef,boundaryCoef)\n# Gk = tf.cast(tf.eye(count),tf.float64)\n# #boundary_num = x_2.shape[0]\n# loss_value,grad = calculateLoss_Grad(model,x_1,x_2)\n# for i in range(BFGS_maxepoch):\n#          #print('epoch = {:d}' .format(i+1),end='')\n#          loss_value,grad,Gk = BFGS(loss_value,grad,Gk,model,x_1,x_2)\n#          if i%test_period == 0:\n#             #modelPlot(model)\n#             #boundary_num = x_2.shape[0]\n#             rs,gd = calculateLoss_Grad(model,x_1,x_2)\n#             err_list[0,test_count]=float(rs)\n#             test_count = test_count + 1\n#             print('epoch = '+str(i+1),end='|')\n#             print('loss = {:.8f}' .format(rs))\n#             if(rs<1.0e-4):\n#                 break\n# plt.plot(-np.log10(err_list[np.where(err_list>0)]),'r')\n# plt.title('BFGS training error loss(-lg)')\n# plt.savefig(\"BFGS error decline.png\")","8b1f1536":"modelPlot(model)\nloss_value,grad = calculateLoss_Grad(model,x_1,x_2)\nprint(loss_value)\nmodel.save('20211203_2.h5') #results before using L-BFGS\n# print(adamOptimizer)","a8625c3c":"Adam\u4f18\u5316\u51fd\u6570\uff08\u6ce8\u610f\u4f7f\u7528\u4e86\u68af\u5ea6\u88c1\u526a\uff09","32efefb9":"\u8bbe\u8ba1\u7f51\u7edc\u7ed3\u6784:2+4*[30]+1","2050c1d8":"\u5f15\u5165BFGS\u7b97\u6cd5(\u624b\u5199),\u5168\u5c40\u53d8\u91cf","999ce76a":"\u4e3b\u8981\u51fd\u6570\u90e8\u5206:\u5148\u8fd0\u884cAdam\uff0c\u518d\u8fd0\u884cBFGS","16fa8ffd":"\u5bfc\u5165\u7b2c\u4e09\u65b9\u5e93","f314e99d":"\u751f\u6210\u90e8\u5206\u6563\u70b9\u5e76\u7ed8\u56fe","0f8b5e5c":"\u91cd\u65b0\u8bbe\u7f6e\u8bad\u7ec3\u96c6\uff08\u4e4b\u524d\u7684\u592a\u5c11\u4e86\uff09\n","15a12d9c":"\u4fdd\u5b58\u7ed3\u679c","3fb8dbc9":"\u7ed8\u56fe\u51fd\u6570","a2ffa94e":"\u5f15\u5165right hand \u51fd\u6570","a010ad0e":"L-BFGS\u51fd\u6570\u90e8\u5206","ec28a0de":"\u4f18\u5316\u5668\u91c7\u7528Adam","7f9e816c":"\u751f\u6210L\u578b\u5e73\u9762\u533a\u57df\u7684\u51fd\u6570;\n\u7ed8\u5236\u533a\u57df\u6563\u70b9\u7684\u51fd\u6570"}}