{"cell_type":{"70729db3":"code","3471d144":"code","4df50e8c":"code","41fececc":"code","508709bb":"code","19dfed7e":"code","2cf958fe":"code","d98a8835":"code","1dd43bae":"code","707ecbc7":"code","9cf4da81":"code","dbbbb82d":"code","36e21436":"code","f433c115":"code","64440a5b":"code","ecb29cf7":"code","4d6261bc":"code","4955f606":"code","ef6a984d":"code","0829b811":"code","1b3a41ba":"code","df0bf1b1":"code","78b0d456":"code","7066c612":"code","e498377b":"code","506d6d52":"code","2eee898b":"code","e683ecf0":"code","ba0c7c25":"code","fcc48b4c":"code","ac7e611d":"code","d8ece761":"code","67d6ed6a":"code","830f130f":"markdown","25767533":"markdown","0d755f8a":"markdown","4f069d66":"markdown","faf12676":"markdown","7b7fdb74":"markdown"},"source":{"70729db3":"import nltk\nfrom nltk.corpus import stopwords\nimport csv\nfrom nltk.tag import pos_tag # for proper noun\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport pandas as pd\nimport math\nfrom nltk.stem import PorterStemmer\nimport re\nimport pytrends\nfrom pytrends.request import TrendReq\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tag import DefaultTagger\nimport os\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\nimport sklearn\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom bert_serving.client import BertClient\nimport gensim\nfrom gensim.parsing.preprocessing import remove_stopwords","3471d144":"def readfile(path):\n    filename=path\n    f = open((filename), \"r\")\n    text=f.read() #append each line in the file to a list\n    f.close()\n    return text","4df50e8c":"def preprocess(text):\n    # spaces after fullstop.\n    text = re.sub(r'(?<=[.])(?=[^\\s])(?=[^0-9])', r' ', text)\n    \n    sent_tokens = nltk.sent_tokenize(text)\n   \n    word_tokens = nltk.word_tokenize(text)\n    word_tokens_lower=[word.lower() for word in word_tokens]\n    \n    stopWords = list(set(stopwords.words(\"english\")))\n    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n    #print(len(word_tokens_refined))\n    \n    stem = []\n    ps = PorterStemmer()\n    for w in word_tokens_refined:\n        stem.append(ps.stem(w))\n    word_tokens_refined=stem\n    \n    if word_tokens_refined[-1]=='?':\n        word_tokens_refined=word_tokens_refined[:-1]\n    \n    return text,sent_tokens,word_tokens,word_tokens_lower,word_tokens_refined","41fececc":"def qphrases(sent_tokens):\n\n    priority1=[\"``\"]\n    priority2=[\"moderna\", \"pfizer\", \"astrazeneca\", \"novavax\",\"covid\", \"covid-19\", \"coronavirus\", \"vaccine\",\"vaccines\",\n               \"success\", \"failure\",\"world\", \"health\", \"assembly\",\"science\", \"solutions\",\"global\",\"crisis\",\"controlled\",\n               \"transmission\",\"pandemic\",\"cervical cancer\", \"tuberculosis\",\"influenza preparedness\", \"meningitis\",\"epilepsy\",\n               \"disorders\", \"emergency\",\"preparedness\",\"quarantine\",\"immunization\",\"infection\",'self-quarantine' , \n               'transmitted' , 'ministry of health' , 'nhs' , 'world beater' , 'masks' , 'towel','johns hopkins',\n               'ant\u00f3nio guterres','pandemic','world health organization', 'belgian health institute sciensano',\n               'infections','alexander de croo','guardian','boris johnson','priti patel','virus', 'duke university',\n                'india','serum institute','vaccine', 'rachel silverman','pfizer','covax']\n    \n    priority3=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\",\n                \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \n               \"lastly\", \"finally\", \"summary\",\n              \"determination\",\"collaboration\",\"innovation\",\"resolution\"]\n    \n    negphrase=[\"anti vaxx\", \"sham\", \"unscientific\", \"pseudoscience\",\"failure\",\"but\",\n            \"anti masks\", \"sadly\", \"weak\", \"delay\", \"cancel\", \"unemploy\", \"blame\", \"suffer\", \n               \"anti\", \"protest\", \"threaten\",\"die\"]\n\n    cue_phrases={}\n    for sentence in sent_tokens:\n        cue_phrases[sentence] = 0\n        word_tokens = nltk.word_tokenize(sentence)\n        for word in word_tokens:\n            if word.lower() in priority1:\n                cue_phrases[sentence] += 3\n            elif word.lower() in priority2:\n                cue_phrases[sentence] += 2\n            elif word.lower() in priority3:\n                cue_phrases[sentence] += 1\n            elif word.lower() in negphrase:\n                cue_phrases[sentence] -=1\n    maximum_frequency = max(cue_phrases.values())\n    for k in cue_phrases.keys():\n        try:\n            cue_phrases[k] = cue_phrases[k] \/ maximum_frequency\n            cue_phrases[k]=round(cue_phrases[k],3)\n        except ZeroDivisionError:\n            x=0\n    return cue_phrases","508709bb":"def numerical_data(sent_tokens):\n    numeric_data={}\n    for sentence in sent_tokens:\n        numeric_data[sentence] = 0\n        word_tokens = nltk.word_tokenize(sentence)\n        for k in word_tokens:\n            if k.isdigit():\n                numeric_data[sentence] += 1\n    maximum_frequency = max(numeric_data.values())\n    for k in numeric_data.keys():\n        try:\n            numeric_data[k] = (numeric_data[k]\/maximum_frequency)\n            numeric_data[k] = round(numeric_data[k], 3)\n        except ZeroDivisionError:\n            x=0\n    return numeric_data","19dfed7e":"def sentence_length(sent_tokens):\n    sent_len_score={}\n    for sentence in sent_tokens:\n        sent_len_score[sentence] = 0\n        word_tokens = nltk.word_tokenize(sentence)\n        if len(word_tokens) in range(0,10):\n            sent_len_score[sentence]=1-0.05*(10-len(word_tokens))\n        elif len(word_tokens) in range(7,20):                    # calculate ideal sentence length using some other criteria? \n            sent_len_score[sentence]=1\n        else:\n            sent_len_score[sentence]=1-(0.05)*(len(word_tokens)-20)\n    for k in sent_len_score.keys():\n        sent_len_score[k]=round(sent_len_score[k],4)\n    #print(sent_len_score.values())\n    return sent_len_score","2cf958fe":"def sentence_position(sent_tokens):\n    sentence_position={}\n    d=1\n    no_of_sent=len(sent_tokens)\n    for i in range(no_of_sent):\n        a=1\/d\n        b=1\/(no_of_sent-d+1)\n        sentence_position[sent_tokens[d-1]]=max(a,b)\n        d=d+1\n    for k in sentence_position.keys():\n        sentence_position[k]=round(sentence_position[k],3)\n#    print(sentence_position.values())\n    return sentence_position","d98a8835":"def frequency(word_tokens_refined, sent_tokens):\n    ps = PorterStemmer()\n    freqTable = {}\n    for word in word_tokens_refined:    \n        if word in freqTable:         \n            freqTable[word] += 1    \n        else:         \n            freqTable[word] = 1\n    for k in freqTable.keys():\n        freqTable[k]= math.log10(1+freqTable[k])\n    #Compute word frequnecy score of each sentence\n    word_frequency={}\n    for sentence in sent_tokens:\n        word_frequency[sentence]=0\n        e=nltk.word_tokenize(sentence)\n        f=[]\n        for word in e:\n            f.append(ps.stem(word))\n        for word,freq in freqTable.items():\n            if word in f:\n                word_frequency[sentence]+=freq\n    maximum=max(word_frequency.values())\n    for key in word_frequency.keys():\n        try:\n            word_frequency[key]=word_frequency[key]\/maximum\n            word_frequency[key]=round(word_frequency[key],3)\n        except ZeroDivisionError:\n            x=0\n#    print(word_frequency.values())\n    return freqTable, word_frequency\n","1dd43bae":" def uppercase(sent_tokens):\n    upper_case={}\n    for sentence in sent_tokens:\n        upper_case[sentence] = 0\n        word_tokens = nltk.word_tokenize(sentence)\n        for k in word_tokens:\n            if k.isupper():\n                upper_case[sentence] += 1\n    maximum_frequency = max(upper_case.values())\n    for k in upper_case.keys():\n        try:\n            upper_case[k] = (upper_case[k]\/maximum_frequency)\n            upper_case[k] = round(upper_case[k], 3)\n        except ZeroDivisionError:\n            x=0\n#    print(upper_case.values())\n    return upper_case","707ecbc7":"def propernouns(sent_tokens):\n    proper_noun={}\n    for sentence in sent_tokens:\n        tagged_sent = pos_tag(sentence.split())\n        propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n        proper_noun[sentence]=len(propernouns)\n    maximum_frequency = max(proper_noun.values())\n    for k in proper_noun.keys():\n        try:\n            proper_noun[k] = (proper_noun[k]\/maximum_frequency)\n            proper_noun[k] = round(proper_noun[k], 3)\n        except ZeroDivisionError:\n            x=0\n#    print(proper_noun.values())\n    return proper_noun","9cf4da81":"def headmatch(sent_tokens):\n    stopWords = list(set(stopwords.words(\"english\")))\n    ps = PorterStemmer()\n    head_match={}\n    heading=sent_tokens[0]\n    for sentence in sent_tokens:\n        head_match[sentence]=0\n        word_tokens = nltk.word_tokenize(sentence)\n        for k in word_tokens:\n            if k not in stopWords:\n                k = ps.stem(k)\n                if k in ps.stem(heading):\n                    head_match[sentence] += 1\n    maximum_frequency = max(head_match.values())\n    for k in head_match.keys():\n        try:\n            head_match[k] = (head_match[k]\/maximum_frequency)\n            head_match[k] = round(head_match[k], 3)\n        except ZeroDivisionError:\n            x=0\n#    print(head_match.values())\n    return head_match","dbbbb82d":"def sentence_similarity(sent_tokens):\n    a=len(sent_tokens)\n    N=a\n    res = [] \n    for i in range(0,N):\n        X=sent_tokens[i]\n        for j in range(0,N):\n            Y=sent_tokens[j]\n            X_list = word_tokenize(X)  \n            Y_list = word_tokenize(Y) \n            sw = stopwords.words('english')  \n            l1 =[];l2 =[] \n\n            X_set = {w for w in X_list if not w in sw}  \n            Y_set = {w for w in Y_list if not w in sw} \n            rvector = X_set.union(Y_set)  \n            for w in rvector: \n                if w in X_set: l1.append(1) # create a vector \n                else: l1.append(0) \n                if w in Y_set: l2.append(1) \n                else: l2.append(0) \n            c = 0\n            for i in range(len(rvector)): \n                c+= l1[i]*l2[i] \n            cosine = c \/ float((sum(l1)*sum(l2))**0.5)\n            res.append(cosine)\n\n\n\n    suml=[]\n    i=0\n    for k in range(0,N):\n        tot=0\n        for j in range(i,(i+N-1)):              \n            tot+=res[j]\n        suml.append(tot)\n        i+=N\n    max_sum=max(suml)\n    for i in range(0,len(suml)):\n        try:\n            suml[i] = (suml[i]\/max_sum)\n            suml[i] = round(suml[i], 3)\n        except ZeroDivisionError:\n            x=0\n    \n    temp={}\n    for i,sentence in enumerate(sent_tokens):\n        temp[sentence]=suml[i]\n    return temp","36e21436":"def create_tfidf_table():\n    parent=[]\n    path='C:\\\\Users\\\\umang\\\\Documents\\\\nlp\\\\COVID_19_dataset\\\\COVID_19_dataset\\\\documents\\\\'\n    filelist = os.listdir(path)\n    for file in filelist:\n        f = open(path+file, \"r\")\n        text=f.read()\n        word_tokens = nltk.word_tokenize(text)\n        word_tokens_lower=[word.lower() for word in word_tokens]\n        stopWords = list(set(stopwords.words(\"english\")))\n        word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n        stem = []\n        ps = PorterStemmer()\n        for w in word_tokens_refined:\n            stem.append(ps.stem(w))\n        word_tokens_refined=stem\n    \n        text=' '.join([i for i in word_tokens_refined])\n        parent.append(text)\n    \n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform(parent)\n    feature_names = vectorizer.get_feature_names()\n    dense = vectors.todense()\n    denselist = dense.tolist()\n    tfidf_df = pd.DataFrame(denselist, columns=feature_names)\n    tfidf_dict=tfidf_df.to_dict()\n    \n    \n    return tfidf_dict","f433c115":"def tfidf(sent_tokens, document_index):\n    tfidf_dict=create_tfidf_table()\n    tfidf_score=[]\n    for sentence in sent_tokens:\n        total=0\n        word_tokens = nltk.word_tokenize(sentence)\n        word_tokens_lower=[word.lower() for word in word_tokens]\n        stopWords = list(set(stopwords.words(\"english\")))\n        word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n        stem = []\n        ps = PorterStemmer()\n        for w in word_tokens_refined:\n            stem.append(ps.stem(w))\n        word_tokens_refined=stem\n        for word in word_tokens_refined:\n            if word in tfidf_dict.keys():\n                total+=tfidf_dict[word][document_index-1]\n        tfidf_score.append(total)\n    tfidf_list=[i\/max(tfidf_score) for i in tfidf_score]\n    temp={}\n    for i,sentence in enumerate(sent_tokens):\n        temp[sentence]=tfidf_list[i]\n    return temp","64440a5b":"def pytrends_func(article_text):\n    text,sent_tokens,word_tokens,word_tokens_lower,word_tokens_refined=preprocess(article_text)\n    pytrend_freqtable={}\n    for i in word_tokens_refined:\n        if i in pytrend_freqtable:\n            pytrend_freqtable[i]+=1\n        else:\n            pytrend_freqtable[i]=1\n    #print(pytrend_freqtable)\n    punctuation=['.',',','``','\\'\\'','-','_', '?', '!', ';', '\\'s']\n    for i in punctuation:\n        try:\n            del pytrend_freqtable[i]\n        except:\n            pass\n    freqdf=pd.DataFrame()\n    freqdf['Keys']=list(pytrend_freqtable.keys())\n    freqdf['values']=list(pytrend_freqtable.values())\n    freqdf=freqdf.sort_values('values', ascending=False)\n    kw_list =freqdf.iloc[0,0]\n    print(kw_list)\n\n    pytrends = TrendReq(hl='en-US', tz=360)\n\n    pytrends.build_payload(kw_list, cat=0, timeframe='today 5-y', geo='', gprop='')\n    print(pytrends.related_queries())\n    temp=(list(pytrends.related_queries().keys())[:10])\n    #print(pytrends.related_queries())\n    score_list=[]\n    for sentence in sent_tokens:\n        score=0\n        word_tokens = nltk.word_tokenize(sentence)\n        word_tokens_lower=[word.lower() for word in word_tokens]\n        stopWords = list(set(stopwords.words(\"english\")))\n        word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n        stem = []\n        ps = PorterStemmer()\n        for w in word_tokens_refined:\n            stem.append(ps.stem(w))\n        word_tokens_refined=stem\n        for word in word_tokens:\n            if word in temp:\n                score+=1\n        score_list.append(score)\n    trends_dict={}\n    for i,sentence in enumerate(sent_tokens):\n        trends_dict[sentence]=score_list[i]\n    return trends_dict","ecb29cf7":"def calc_scores():\n    text=readfile()\n    text,sent_tokens,word_tokens,word_tokens_lower,word_tokens_refined=preprocess(text)\n    \n    cue_phrases_dict=qphrases(sent_tokens)\n    numerical_data_dict=numerical_data(sent_tokens)\n    sentence_length_dict=sentence_length(sent_tokens)\n    sentence_position_dict=sentence_position(sent_tokens)\n    freqTable, word_frequency_dict=frequency(word_tokens_refined, sent_tokens)\n    uppercase_dict=uppercase(sent_tokens)\n    propernouns_dict=propernouns(sent_tokens)\n    headmatch_dict=headmatch(sent_tokens)\n    sent_similarity_dict=sentence_similarity(sent_tokens)\n    tfidf_score_dict=tfidf(sent_tokens,1)\n    \n    #trends_dict=pytrends_func(text)\n    \n    df=pd.DataFrame()\n    df=df.append(pd.DataFrame({\n        'Cue_phrases':list(cue_phrases_dict.values()),\n        'Numerical_Data':list(numerical_data_dict.values()),\n        'Sentence_length': list(sentence_length_dict.values()),\n        'Sentence_position':list(sentence_position_dict.values()),\n        'Word_frequency':list(word_frequency_dict.values()),\n        'Uppercase':list(uppercase_dict.values()),\n        'Proper_nouns':list(propernouns_dict.values()),\n        'Headmatch':list(headmatch_dict.values()),\n        'Sentence_similarity':list(sent_similarity_dict.values()),\n        'TFIDF':list(tfidf_score_dict.values()),\n        'Key':list(cue_phrases_dict.keys())})\n                              ,ignore_index=True)\n    score=(df['Cue_phrases']+df['Numerical_Data']+df['Sentence_length']+df['Sentence_position']+df['Word_frequency']+df['Uppercase']+df['Proper_nouns']+df['Headmatch']+df['Sentence_similarity']+df['TFIDF'])\/(int(len(df.columns))-1)\n    df['Score']=score\n    return df","4d6261bc":"\ndf_model=pd.DataFrame(columns=['Cue_phrases',\n                               'Numerical_Data',\n                               'Sentence_length',\n                               'Sentence_position',\n                               'Word_frequency','Uppercase',\n                               'Proper_nouns',\n                               'Headmatch',\n                               'Sentence_similarity',\n                               'TFIDF',\n                               'Label',\n                               'Key'])\nprint(df_model)","4955f606":"path1='C:\\\\Users\\\\umang\\\\Documents\\\\nlp\\\\COVID_19_dataset\\\\COVID_19_dataset\\\\documents\\\\'\npath2='C:\\\\Users\\\\umang\\\\Documents\\\\nlp\\\\COVID_19_dataset\\\\COVID_19_dataset\\\\summary\\\\'\n\nfilelist = os.listdir(path1)\nfor index,file in enumerate(filelist):\n    f = open(path1+file, \"r\")\n    f1=open(path2+file,\"r\")\n    text=f.read()\n    text1=f1.read()\n    sent_tokens = nltk.sent_tokenize(text)\n    word_tokens = nltk.word_tokenize(text)\n    sent_tokens1 = nltk.sent_tokenize(text1)\n    word_tokens1 = nltk.word_tokenize(text1)\n    word_tokens_lower=[word.lower() for word in word_tokens]\n    stopWords = list(set(stopwords.words(\"english\")))\n    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n    \n    cue_phrases_dict=qphrases(sent_tokens)\n\n    numerical_data_dict=numerical_data(sent_tokens)\n\n    sentence_length_dict=sentence_length(sent_tokens)\n \n    sentence_position_dict=sentence_position(sent_tokens)\n\n    freqTable, word_frequency_dict=frequency(word_tokens_refined, sent_tokens)\n\n    uppercase_dict=uppercase(sent_tokens)\n\n    propernouns_dict=propernouns(sent_tokens)\n\n    headmatch_dict=headmatch(sent_tokens)\n\n    sent_similarity_dict=sentence_similarity(sent_tokens)\n\n    tfidf_score=tfidf(sent_tokens,index+1)\n    \n    #trends_dict=pytrends_func(text)\n    \n    Label={}\n    for sentence in sent_tokens:\n        Label[sentence]=0\n        for sentence1 in sent_tokens1:\n            if(sentence==sentence1):\n                Label[sentence]+=1\n                \n    df_model=df_model.append(pd.DataFrame({\n        'Cue_phrases':list(cue_phrases_dict.values()),\n        'Numerical_Data':list(numerical_data_dict.values()),\n        'Sentence_length': list(sentence_length_dict.values()),\n        'Sentence_position':list(sentence_position_dict.values()),\n        'Word_frequency':list(word_frequency_dict.values()),\n        'Uppercase':list(uppercase_dict.values()),\n        'Proper_nouns':list(propernouns_dict.values()),\n        'Headmatch':list(headmatch_dict.values()),\n        'Sentence_similarity':list(sent_similarity_dict.values()),\n        'TFIDF':list(tfidf_score.values()),\n        'Label':list(Label.values()),\n        'Key':list(cue_phrases_dict.keys())})\n                              ,ignore_index=True)\n    \n    f.close()\n    f1.close()","ef6a984d":"X = list(df_model.iloc[:,:9].values)\nY = list(df_model.iloc[:,10].values)","0829b811":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=0)\n\nclassifier = RandomForestClassifier(n_estimators = 10,criterion='entropy',random_state=0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred )*100)","1b3a41ba":"model_df=calc_scores()\nmodel_df=model_df.drop('Score', axis=1)\nx=list(model_df.iloc[:,:9].values)\npredicted_summary_labels=classifier.predict(x)\nmodel_df['Label']=predicted_summary_labels\nmodel_summary=''\nfor i,label in enumerate(model_df['Label']): \n        if label==1: \n            model_summary += \" \" + str(model_df['Key'].iloc[i])\nmodel_summary","df0bf1b1":"query1=\"What did Bill Gates say on Tuesday?\"\n\nquery2='What recently showed strong protection against COVID-19?'\n\nquery3='What likely means other frontrunners will also show strong protection against COVID-19?'","78b0d456":"def answer_query(query):\n    q_text,q_sent_tokens,q_word_tokens,q_word_tokens_lower,q_word_tokens_refined=preprocess(query)\n    refined_words_match=[]\n    summary_df=calc_scores()\n    for sentence in summary_df['Key']:\n        sum_text,sum_sent_tokens,sum_word_tokens,sum_word_tokens_lower,sum_word_tokens_refined=preprocess(str(sentence))\n        score=0\n        for word in q_word_tokens_refined:\n            if word in sum_word_tokens_refined:\n                score+=1\n        refined_words_match.append(score)\n\n    \n    tagged = nltk.pos_tag(q_word_tokens)\n    query_sentence=summary_df['Key'].iloc[refined_words_match.index(max(refined_words_match))]\n    t_text,t_sent_tokens,t_word_tokens,t_word_tokens_lower,t_word_tokens_refined=preprocess(query_sentence)\n    tagged = nltk.pos_tag(t_word_tokens)\n    index=0\n    temp=[]\n    ps = PorterStemmer()\n    for t in t_word_tokens_lower:\n        if ps.stem(t)==q_word_tokens_refined[index]:\n            break\n        temp.append(t)\n    temp=list(nltk.pos_tag(temp))\n    print(temp)\n    #print(q_word_tokens_refined)\n    answer=[]\n    for i in range(len(temp)-1,-1,-1):\n        if temp[i][1] not in ['NN','NNS','CD','JJ','JJR','JJS']:\n            break\n        answer.append(temp[i][0])\n    \n    if answer == []:\n        answer=summary_df['Key'].iloc[refined_words_match.index(max(refined_words_match))]\n        print('Query: ', query)\n        print('Answer: ',answer)\n    else:\n        answer=' '.join(answer[-1::-1])\n        print('Query: ', query)\n        print('Answer: ',answer)","7066c612":"answer_query(query3)","e498377b":"answer_query(query2)","506d6d52":"answer_query(query3)","2eee898b":"bc = BertClient()","e683ecf0":"similarity_dict={}\n\n\ndef retrieveAndPrintFAQAnswer(question_embedding,sentence_embeddings,sentences):\n    max_sim=-1\n    index_sim=-1\n    for index ,faq_embedding in enumerate(sentence_embeddings):\n        sim=cosine_similarity(faq_embedding,question_embedding)[0][0]\n        print(sim,sentences[index])\n        similarity_dict[sentences[index]] = sim\n        if sim>max_sim:\n            max_sim=sim\n            index_sim=index\n\n\ndef clean_sentence(sentence,stopwords=False):\n    \n    sentence=sentence.lower().strip()\n    sentence=re.sub(r'[^a-z0-9\\s]','',sentence)\n    \n    if stopwords:\n        sentence=remove_stopwords(sentence)\n        \n    return sentence\n\ndef get_cleaned_sentences(text,stopwords=False):\n    cleaned_sentences=[]\n    sent_tokens = nltk.sent_tokenize(text)\n    for sentence in sent_tokens:\n        cleaned=clean_sentence(sentence,stopwords)\n        cleaned_sentences.append(cleaned)\n    return cleaned_sentences\n","ba0c7c25":"article_text = \"\"\n\nimport numpy as np\nimport os\npath1='C:\\\\Users\\\\umang\\\\Documents\\\\nlp\\\\COVID_19_dataset\\\\COVID_19_dataset\\\\summary\\\\'\nfilelist = os.listdir(path1)\nfor file in filelist:\n    f = open(path1+file, \"r\")\n    text=f.read() #append each line in the file to a list\n    article_text+=text\n\n    f.close()\ncleaned_sentences=get_cleaned_sentences(article_text,stopwords=False)","fcc48b4c":"query4=\"in this worldwide catastrophe how have countries controlled transferral?\"\n\nquery5=\"which organizations have a vital impact in relevance to trial and tracking ?\"\n\nquery6=\"what should we get ready for?\"\n\nquery7=\"what do achievements from vaccine programs mean?\"","ac7e611d":"question_orig = query5","d8ece761":"question=clean_sentence(question_orig,stopwords=False)\ncleaned_sentences=get_cleaned_sentences(article_text,stopwords=False)\n\nsent_bertphrase_embeddings=[]\nfor sent in cleaned_sentences:\n    sent_bertphrase_embeddings.append(bc.encode([sent]))\n    \nquestion_embedding=bc.encode([question])\n\nretrieveAndPrintFAQAnswer(question_embedding,sent_bertphrase_embeddings,cleaned_sentences)\nanswer_list = sorted(similarity_dict, key=similarity_dict.get,reverse=True)","67d6ed6a":"no = 3\nprint(\"QUESTION : \",question_orig)\nprint(\"\")\nprint(\"POSSIBLE ANSWERS\")\nfor i in range(no):\n    print(similarity_dict[answer_list[i]],\" : \",answer_list[i])\n    print(\"\")","830f130f":"# FUNCTIONS","25767533":"# MODEL  ","0d755f8a":"bert-serving-start -model_dir C:\\Users\\umang\\Documents\\nlp\\uncased_L-12_H-768_A-12\\uncased_L-12_H-768_A-12 -num_worker=1 -max_seq_len 50","4f069d66":"# QUERY ","faf12676":"### Summary from Model","7b7fdb74":"# BERT "}}