{"cell_type":{"7729201b":"code","59a2b0ec":"code","992d844e":"code","2b383bd0":"code","fdd7c461":"code","3200856d":"code","dcc80175":"code","e6b0da87":"code","1e32eb29":"code","75cbdd1a":"code","0e0304c2":"code","4d5850f9":"code","0a98acff":"code","f84115b0":"code","581084df":"code","59c0a5fb":"code","d8cd23b8":"code","04d15643":"code","2386188a":"code","0867115e":"code","491bc0fd":"code","092a6a5c":"code","ec0cea7a":"code","4946da17":"code","7f5b7b84":"code","a305b6a2":"markdown","4f0b78d1":"markdown","61a0e653":"markdown","9975cdf6":"markdown","18c7d11c":"markdown","b3ab9841":"markdown","4be98fb3":"markdown","c90b81f5":"markdown","8d3ce029":"markdown","a5af7ff2":"markdown","8bfda2f0":"markdown"},"source":{"7729201b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59a2b0ec":"## Libraries","992d844e":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline","2b383bd0":"cancer_features=pd.read_csv('\/kaggle\/input\/breast-cancer-pca\/breast_cancer_features.csv')","fdd7c461":"cancer_result=pd.read_csv('\/kaggle\/input\/breast-cancer-pca\/breast_cancer_result.csv')","3200856d":"cancer_features.head()","dcc80175":"cancer_features.info()","e6b0da87":"cancer_result.head()","1e32eb29":"cancer_result.info()","75cbdd1a":"from sklearn.preprocessing import StandardScaler","0e0304c2":"scaler = StandardScaler()\nscaler.fit(cancer_features)","4d5850f9":"scaled_data = scaler.transform(cancer_features)","0a98acff":"from sklearn.decomposition import PCA","f84115b0":"pca = PCA(n_components=2)","581084df":"pca.fit(scaled_data)","59c0a5fb":"x_pca = pca.transform(scaled_data)","d8cd23b8":"scaled_data.shape","04d15643":"x_pca.shape","2386188a":"type(cancer_result)","0867115e":"cancer_result=cancer_result.to_numpy()","491bc0fd":"type(cancer_result)","092a6a5c":"plt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=cancer_result,cmap='plasma')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","ec0cea7a":"pca.components_","4946da17":"df_comp = pd.DataFrame(pca.components_,columns=cancer_features.columns)","7f5b7b84":"plt.figure(figsize=(12,6))\nsns.heatmap(df_comp,cmap='plasma',)","a305b6a2":"This heatmap and the color bar basically represent the correlation between the various feature and the principal component itself.\n\nConclusion\nHopefully this information is useful to you when dealing with high dimensional data!\n\nFor more information see: https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp\/\n\nPlease feel free to leave me a comment or your question.\n\nYour upvote is appreciated :).","4f0b78d1":"In this numpy matrix array, each row represents a principal component, and each column relates back to the original features. we can visualize this relationship with a heatmap:","61a0e653":"## PCA Visualization\n\nThis dataset has 569 entries and 30 features.As we know it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance.","9975cdf6":"## The Data","18c7d11c":"Now we can transform this data to its first 2 principal components.","b3ab9841":"# Principal Component Analysis\n\n# Hello my kaggle friens,\n\nLet's discuss PCA! PCA isn't exactly a full machine learning algorithm, but instead an unsupervised learning algorithm.\n\n\nRemember that PCA is just a transformation of your data and attempts to find out what features explain the most variance in your data.","4be98fb3":"PCA with Scikit Learn uses a very similar process to other preprocessing functions that come with SciKit Learn. We instantiate a PCA object, find the principal components using the fit method, then apply the rotation and dimensionality reduction by calling transform().\n\nWe can also specify how many components we want to keep when creating the PCA object.","c90b81f5":"Great! We've reduced 30 dimensions to just 2! Let's plot these two dimensions out!","8d3ce029":"Clearly by using these two components we can easily separate these two classes.\n\n## Interpreting the components \n\nUnfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n\nThe components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:","a5af7ff2":"cancer_result has two classes:\n                Benign:0\n                Malignant:1","8bfda2f0":"First we need to convert cancer_result dataframe to numpy array in case we want to use it as a parameter in scatter_plot"}}