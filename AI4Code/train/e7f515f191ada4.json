{"cell_type":{"29170eea":"code","c22a6171":"code","8bed7348":"code","3e06d4c4":"code","10ca9a63":"code","4e1985f5":"code","dc75a933":"code","b3f76e8e":"code","fbc52440":"code","5e0ac8ea":"code","f1401231":"code","5794b93f":"code","6fba0273":"markdown","d8932b33":"markdown","b64c94bc":"markdown"},"source":{"29170eea":"import numpy as np \nimport pandas as pd \nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.datasets import load_digits\nfrom sklearn import metrics\n%matplotlib inline\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n","c22a6171":"cancer = load_breast_cancer()\ndigits = load_digits()\n\ndata = cancer","8bed7348":"df = pd.DataFrame(data= np.c_[data['data'], data['target']],\n                     columns= list(data['feature_names']) + ['target'])\ndf['target'] = df['target'].astype('uint16')","3e06d4c4":"df","10ca9a63":"df.head()","4e1985f5":"# adaboost experiments\n# create x and y train\nX = df.drop('target', axis=1)\ny = df[['target']]\n\n# split data into train and test\/validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","dc75a933":"# check the average cancer occurence rates in train and test data, should be comparable\nprint(y_train.mean())\nprint(y_test.mean())","b3f76e8e":"# base estimator: a weak learner with max_depth=2\nshallow_tree = DecisionTreeClassifier(max_depth=2, random_state = 100)","fbc52440":"# fit the shallow decision tree \nshallow_tree.fit(X_train, y_train)\n\n# test error\ny_pred = shallow_tree.predict(X_test)\nscore = metrics.accuracy_score(y_test, y_pred)\nscore","5e0ac8ea":"# adaboost with the tree as base estimator\n\nestimators = list(range(1, 50, 3))\n\nabc_scores = []\nfor n_est in estimators:\n    ABC = AdaBoostClassifier(\n    base_estimator=shallow_tree, \n    n_estimators = n_est)\n    \n    ABC.fit(X_train, y_train)\n    y_pred = ABC.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    abc_scores.append(score)\n    ","f1401231":"abc_scores","5794b93f":"# plot test scores and n_estimators\n# plot\nplt.plot(estimators, abc_scores)\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.ylim([0.85, 1])\nplt.show()","6fba0273":"Now, we will see the accuracy using the AdaBoost algorithm. In this following code, we will write code to calculate the accuracy of the AdaBoost models as we increase the number of trees from 1 to 50 with a step of 3 in the lines:\n\n'estimators = list(range(1, 50, 3))'\n\n'for n_est in estimators:'\n\nWe finally end up with the accuracy of all the models in a single list abc_scores.","d8932b33":"We will use the breast cancer dataset in which the target variable has 1 if the person has cancer and 0 otherwise. Let's load the data.","b64c94bc":"Here we will explore the breast cancer dataset and try to train the model to predict if the person is having breast cancer or not. We will start off with a weak learner, a decision tree with maximum depth = 2.\n\nWe will then build an adaboost ensemble with 50 trees with a step of 3 and compare the performance with the weak learner.\n"}}