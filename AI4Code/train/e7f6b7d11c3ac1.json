{"cell_type":{"f30f02da":"code","a2d71c5a":"code","83f7181b":"code","66a07ca0":"code","55f2281d":"code","cb64320e":"code","0c767acc":"code","af74750d":"code","7b7684f0":"code","4c3e77f9":"code","4989b01b":"code","987eafaf":"markdown","77bf3a59":"markdown","6f1ea406":"markdown","8decdf98":"markdown","96706462":"markdown","b13f2176":"markdown","c42b2dd1":"markdown","906b7ae1":"markdown","cc9380b9":"markdown","6d9e226b":"markdown"},"source":{"f30f02da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2d71c5a":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nimport wandb\nwandb.init(project='ML&DL Ex3 - Titanic - Kaggle Comp.', save_code=True)","83f7181b":"titanic_training_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_training_data.head()","66a07ca0":"\"\"\"Returns a dataframe\ndataframe as first input, spec. of train or test data as second (train by default).\n\"\"\"\ndef clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool) # set NaNs as booleans for Cabin, embarked and Age.\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool)\n    \n    # Sex and Pclass are split by their possible values and converted to booleans \n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1)\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1)\n    if train:\n        df = df.drop(['Survived'], axis=1) # Remove \"Survived\" column if its a train set. \n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index # get numeric columns, which are the ones that are not object or boolean columns\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) \/ (x.std())) # Normalize numeric values\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean()) #For age and Fare, replace NaNs for the mean. \n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    return df\n\nlabels = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\ntitanic_training_data = clean_titanic(titanic_training_data)\ntitanic_training_data.head()","55f2281d":"titanic_data_tensor = torch.tensor(titanic_training_data.astype('float').values, dtype=torch.float32)\ntitanic_data_tensor.shape","cb64320e":"dataset = torch.utils.data.TensorDataset(titanic_data_tensor, labels)","0c767acc":"\"\"\"70% for training and rest for validation, randomly split\n\"\"\"\ntraining_size = int(0.7 * len(dataset)) \nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True)","af74750d":"#TODO Xavier Uniform to the weight and set the bias to 0\n\n\"\"\" \n\"\"\"\ndef init_my_layer(m):\n    torch.nn.init.xavier_uniform_(m.weight)\n    torch.nn.init.constant_(m.bias, 0)\n    return m","7b7684f0":"class LinearModel(nn.Module):\n    def __init__(self):\n        super(LinearModel, self).__init__()\n        self.activation = nn.Sigmoid()\n        self.ln1 = init_my_layer(nn.Linear(12, 1))\n\n    def forward(self, x):\n        x = self.ln1(x)\n        return self.activation(x)","4c3e77f9":"num_epochs = 400 # should be more than enought, but can be changed\nlr_train = 3e-3 # e.q to 0.003, you can change it if needed\nlambda_l2 = 1e-5\n\nnet = LinearModel()\n\ncriterion = torch.nn.BCELoss() # for Binary Classification\n#loss_function = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(net.parameters(), lr=lr_train, weight_decay=lambda_l2)\n\nfor epoch in range(num_epochs):\n    training_loss = 0\n    #TODO TRAINING LOOP\n    for local_batch, local_labels in data_loader_train:\n        y_pred = net(local_batch)\n        training_loss = criterion(y_pred, local_labels.unsqueeze(1))             \n        \n        # zero the gradients before running\n        # the backward pass.\n        optimizer.zero_grad()\n\n        # Backward pass to compute the gradient\n        # of loss w.r.t our learnable params. \n        training_loss.backward()\n\n        # Update params\n        optimizer.step()\n\n     \n    validation_loss = 0\n    #TODO VALIDATION LOOP\n    with torch.set_grad_enabled(False):\n        for local_batch, local_labels in data_loader_val:\n            y_pred = net(local_batch)\n            validation_loss = criterion(y_pred, local_labels.unsqueeze(1))\n\n            # zero the gradients before running\n            # the backward pass.\n            #optimizer.zero_grad()\n\n            # Backward pass to compute the gradient\n            # of loss w.r.t our learnable params. \n            #validation_loss.backward()\n\n            # Update params\n            #optimizer.step()\n    wandb.log({'training_loss': training_loss, 'validation_loss': validation_loss})\n\n    # SAVE THE MODEL\n    checkpoint = {\n          'state_dict': net.state_dict(), \n          'optimizer' : optimizer.state_dict(),\n          'epoch' : epoch,     \n    }\n\n    torch.save(checkpoint, \"checkpoint.pth\")\n\n","4989b01b":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)\n    #print(out_df.head())","987eafaf":"Create a `TensorDataset` to get tuple of data and label","77bf3a59":"We then transform the data from numpy (pandas representation) into torch's `Tensor`","6f1ea406":"Layer initialization using Xavier Uniform on the weights and a constant 0 value on the bias","8decdf98":"Initialize the network (call it `net`, it would make things easier later), the loss, the optimizer, and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","96706462":"We then split between the training and validation set","b13f2176":"We first need to read the datasets","c42b2dd1":"This loop computes the prediction on the test dataset and creates a submission file\n\nYou then just have to click the submit button to get your score. Lucky you!","906b7ae1":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","cc9380b9":"Dataframe needs to be cleaned. Knowing if some information are unknown can be very important to determine if someone survived","6d9e226b":"Import all the needed library and init Weights and Biases"}}