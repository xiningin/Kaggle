{"cell_type":{"4379de69":"code","12d79065":"code","1893eb17":"code","a707840b":"code","9c675f62":"code","03086cc8":"code","e41611a1":"code","7afa4a2f":"code","5873b1db":"code","49af8e30":"code","0a10451e":"code","73224573":"code","ed474b7b":"code","db9075ae":"code","dc3adb97":"code","9b682514":"code","2f1ecd8f":"code","32fd9245":"code","2a330b0f":"code","0c686d96":"code","1251e4c1":"markdown","42656b70":"markdown","1eb0c91d":"markdown","c7411721":"markdown","25daa44b":"markdown"},"source":{"4379de69":"import numpy as np\nimport pandas as pd\nimport random\nimport gc\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso, BayesianRidge, ARDRegression\nfrom sklearn.svm import LinearSVR, NuSVR, SVR\nfrom sklearn.ensemble import BaggingRegressor","12d79065":"import warnings\nwarnings.filterwarnings(\"ignore\")","1893eb17":"def metric(y_true, y_pred):\n    m1 = np.sum(np.abs(y_true - y_pred), axis=0)\n    mae = m1\/len(y_true)\n    return mae, np.mean(m1\/np.sum(y_true, axis=0))","a707840b":"## Load test and train fnc and loading features\n\nfnc_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\nloading_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\n\nlabels_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","9c675f62":"## Shift features of site two samples by the difference of mean of site 1 and known site 2 samples\n\ndf = df.reset_index(drop = True)\ntest_df = test_df.reset_index(drop = True)\n\nfeatures = loading_features + fnc_features\nfeatures = [i for i in features if i!='IC_20']\n\nsite_2_samples = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv')['Id'].to_list()\n\ntest_df_1 = test_df[test_df.columns]\ntest_df_2 = test_df[test_df.columns]\ntest_df_2[features] = test_df_2[features] + df[features].mean() - test_df.set_index('Id').loc[site_2_samples][features].mean() \n\nA = [0]*(len(df)\/\/7)+[1]*(len(df)\/\/7) + [2]*(len(df)\/\/7)+[3]*(len(df)\/\/7) + [4]*(len(df)\/\/7)+[5]*(len(df)\/\/7) + [6]*(len(df) - 6*len(df)\/\/7 + 3)\nrandom.seed(242)\nrandom.shuffle(A)\n\ndf['fold_column'] = A\n\ndf[['Id','fold_column']].to_csv('Fold.csv')","03086cc8":"## Parameter dictionary of all the models\n## Bayesian Optimization is performed to get the parameter values\n\nparameters = {\"svr\":{\"age\": {\"scale\": [492.5262413687941, 69.60339043492256], \"standarize\": [26.68867995074479] ,\"minmax\": [24.77771582775962]} ,\n                     \"domain1_var1\": {\"scale\": [669.9958281136152, 10.129864960288367], \"standarize\": [6.444806955515817],\"minmax\": [4.80302732891827] },\n                     \"domain1_var2\": {\"scale\": [10.0, 0.5900323195931239], \"standarize\": [0.419552161343763] ,\"minmax\": [0.35000260353760754] },\n                     \"domain2_var1\": {\"scale\": [187.26035099407156, 6.209128731900369], \"standarize\": [3.3428562888199176] ,\"minmax\": [2.714687711798464]},\n                     \"domain2_var2\": {\"scale\": [217.2048608764603, 3.495537661134675] , \"standarize\": [1.8514027605800645] ,\"minmax\": [1.715123371502329] }},\n             \"enet\":{\"age\": {\"scale\": [1043.096643138724, 1e-05, 1.0], \"standarize\": [0.07623488779397786, 0.5521844228202069],\"minmax\": [0.006883473190049635, 1.0]} ,\n                     \"domain1_var1\": {\"scale\": [122.24848596303966, 1e-05, 0.01083596277157703] , \"standarize\": [0.2989438656376394, 0.5049192027013706] ,\"minmax\":[0.021381372803152998, 1.0]  },\n                     \"domain1_var2\": {\"scale\": [203.9103318225802, 2.5638626472832602e-05, 2.1139739815894028e-05], \"standarize\": [1.0, 1.0],\"minmax\": [0.24025111111794356, 1.4665060755095122e-05]},\n                     \"domain2_var1\": {\"scale\": [97.3794261549355, 1e-05, 0.03158534564456641], \"standarize\": [0.616837113004086, 0.11581009464872395],\"minmax\": [0.018701442894351585, 0.5800401393240704]},\n                     \"domain2_var2\": {\"scale\": [135.56649737157719, 1.0625964729570441e-05, 0.007895224389801971], \"standarize\": [0.5318561320573426, 1.0] ,\"minmax\": [0.05113816834656483, 2.2729917560510718e-05]}},\n             \"lasso\":{\"age\": {\"scale\": [87.27496554104196, 0.00011811992287947767], \"standarize\": [0.053591469701310505],\"minmax\": [0.007091211302720107]} ,\n                     \"domain1_var1\": {\"scale\": [4493.160497947272, 1e-05], \"standarize\": [0.16738307730624005],\"minmax\": [0.021483066649991587]},\n                     \"domain1_var2\": {\"scale\": [7403.58647502064, 0.0003124334926440588], \"standarize\": [0.3415539161049886],\"minmax\": [0.05052060866913413]},\n                     \"domain2_var1\": {\"scale\": [3014.1643937179506, 1e-05], \"standarize\": [0.15570482251801265],\"minmax\": [0.018670197689250108]},\n                     \"domain2_var2\": {\"scale\": [572.2741674526715, 7.235349978379449e-05], \"standarize\": [0.19463950211830205],\"minmax\": [0.026324889974712697]}},\n             \"ridge\":{\"age\": {\"scale\": [2981.19407531989, 1e-05], \"standarize\": [835.5424030320825],\"minmax\": [13.241076418640132]} ,\n                     \"domain1_var1\": {\"scale\": [2974.9135224877314, 9.207208910407203e-05], \"standarize\": [6097.898352449914],\"minmax\": [109.1407510448333]},\n                     \"domain1_var2\": {\"scale\": [1165.1948996221117, 0.0633286354054813], \"standarize\": [10000.0],\"minmax\": [1130.1525442567247]},\n                     \"domain2_var1\": {\"scale\": [221.72697425311426, 0.010485943145327647], \"standarize\": [7509.783693100307],\"minmax\": [140.27822544908207]},\n                     \"domain2_var2\": {\"scale\": [397.91938242690884, 0.0039534304599317775], \"standarize\": [10000.0],\"minmax\": [256.18335952365607]}},\n             \"bagging_regressor\":{\"age\": {\"scale\": [836.8450355986731, 5.842699153641068e-05, 99, 0.7980778723979312, 0.8553728294699361], \"standarize\": [1e-06, 100, 0.6, 1.0] ,\"minmax\": [0.7772232840469472, 100, 0.6, 0.8339762285063499]} ,\n                     \"domain1_var1\": {\"scale\": [6226.685140116834, 8.062292860122505e-06, 20, 0.5985935749843405, 0.7023122872651539], \"standarize\": [0.08296550200208345, 52, 1.0, 0.24081559768307517],\"minmax\": [ 0.00013331958105521457, 58, 0.577752531444629, 0.17834040671908657] },\n                     \"domain1_var2\": {\"scale\": [8508.354139514926, 1.8589590180720532e-05, 90, 0.4640510986867237, 0.6414842839258486], \"standarize\": [1e-06, 100, 0.5977205133375352, 0.1] ,\"minmax\": [ 1.0, 84, 1.0, 0.1] },\n                     \"domain2_var1\": {\"scale\": [1558.1747599874511, 8.471138739339728e-05, 57, 0.7486934751239087, 0.6064330217872465], \"standarize\": [8.809009860834274e-05, 21, 0.8107088693735142, 0.16684953796049679] ,\"minmax\": [0.0031809362709809643, 63, 0.8448463504161474, 0.18151653061841017]},\n                     \"domain2_var2\": {\"scale\": [249.06441572172292, 0.0026501818830299132, 71, 0.39085489144145025, 0.8101067709206109] , \"standarize\": [1.0, 56, 1.0, 0.12137532990594137] ,\"minmax\": [1e-06, 83, 1.0, 0.12854547267982455] }},\n             \"bayesian_ridge\":{\"age\": {\"scale\": [169.93122783930556], \"standarize\": [26.68867995074479] ,\"minmax\": [24.77771582775962]} ,\n                     \"domain1_var1\": {\"scale\": [200], \"standarize\": [6.444806955515817],\"minmax\": [4.80302732891827] },\n                     \"domain1_var2\": {\"scale\": [146.56814055758105], \"standarize\": [0.419552161343763] ,\"minmax\": [0.35000260353760754] },\n                     \"domain2_var1\": {\"scale\": [200], \"standarize\": [3.3428562888199176] ,\"minmax\": [2.714687711798464]},\n                     \"domain2_var2\": {\"scale\": [147.03591195143287] , \"standarize\": [1.8514027605800645] ,\"minmax\": [1.715123371502329] }},\n             \"ardregression\":{\"age\": {\"scale\": [169.93122783930556], \"standarize\": [26.68867995074479] ,\"minmax\": [24.77771582775962]} ,\n                     \"domain1_var1\": {\"scale\": [200], \"standarize\": [6.444806955515817],\"minmax\": [4.80302732891827] },\n                     \"domain1_var2\": {\"scale\": [146.56814055758105], \"standarize\": [0.419552161343763] ,\"minmax\": [0.35000260353760754] },\n                     \"domain2_var1\": {\"scale\": [200], \"standarize\": [3.3428562888199176] ,\"minmax\": [2.714687711798464]},\n                     \"domain2_var2\": {\"scale\": [147.03591195143287] , \"standarize\": [1.8514027605800645] ,\"minmax\": [1.715123371502329] }},\n             \"kernel_ridge\":{\"age\": {\"scale\":  [218.25312369655543, 1.4956781741767941e-05], \"standarize\": [26.68867995074479] ,\"minmax\": [0.016034788688041528]} ,\n                     \"domain1_var1\": {\"scale\": [621.3861221113601, 2.8212635925665707e-06] , \"standarize\": [6.444806955515817],\"minmax\": [0.14177151020560527] },\n                     \"domain1_var2\": {\"scale\":  [610.6037663955775, 2.26327745611855e-05], \"standarize\": [0.419552161343763] ,\"minmax\": [1.0] },\n                     \"domain2_var1\": {\"scale\": [1177.188601274852, 1e-06], \"standarize\": [3.3428562888199176] ,\"minmax\": [0.19666007449883396]},\n                     \"domain2_var2\": {\"scale\": [2065.8127095964114, 8.714379924658953e-06] , \"standarize\": [1.8514027605800645] ,\"minmax\": [0.36572817396633966]  }},\n             \"nusvr\":{\"age\": {\"scale\":  [413.66352024507285, 0.30586824827165876, 192.7364622961058], \"standarize\": [0.7661214333172722, 31.23665506792682] ,\"minmax\": [0.8255583612455453, 22.779047246626615]} ,\n                     \"domain1_var1\": {\"scale\": [208.522847268132, 0.7538169413317486, 2.3961220451735383] , \"standarize\": [0.7135776229598471, 6.4016655709129475],\"minmax\": [0.6912174356470082, 6.536167703644929] },\n                     \"domain1_var2\": {\"scale\":  [216.1974537027112, 0.8173591733778397, 0.5482189695163461], \"standarize\": [0.961358505480476, 0.3282382578756948] ,\"minmax\": [0.9991027770074039, 0.17301045865711506] },\n                     \"domain2_var1\": {\"scale\": [261.73100829747614, 0.6225071836999567, 2.4341415966735354] , \"standarize\": [0.6315442586778667, 4.296746217326582] ,\"minmax\": [0.5835277539151535, 4.605191484904114] },\n                     \"domain2_var2\": {\"scale\": [223.1976334943342, 0.7807010597122734, 3.1841909940192195] , \"standarize\": [0.8691858535045746, 1.5804030121351031] ,\"minmax\": [0.7194666704568422, 1.9191151694259774]}},\n             \"linear_svr\":{\"age\": {\"scale\":   [131.90871225961428, 800.5475555641115] , \"standarize\": [0.7661214333172722, 31.23665506792682] ,\"minmax\": [0.43568842406380115]} ,\n                     \"domain1_var1\": {\"scale\": [10000.0, 456.18462937926853] , \"standarize\": [0.7135776229598471, 6.4016655709129475],\"minmax\": [0.057999064969975776]  },\n                     \"domain1_var2\": {\"scale\":  [375.9609066943138, 139.43946061565356] , \"standarize\": [0.961358505480476, 0.3282382578756948] ,\"minmax\": [0.007176080716423259]},\n                     \"domain2_var1\": {\"scale\": [10000.0, 420.34207749811713] , \"standarize\": [0.6315442586778667, 4.296746217326582] ,\"minmax\": [0.07061991957605784]},\n                     \"domain2_var2\": {\"scale\": [10.0, 2.1357134106514724] , \"standarize\": [0.8691858535045746, 1.5804030121351031] ,\"minmax\": [0.023951883407266204]}},\n             \"power0\":{\"age\": {\"scale\":   [157.33583611404953, 1e-06] , \"standarize\": [0.16597684517161287] ,\"minmax\": [0.002645919092791747]} ,\n                     \"domain1_var1\": {\"scale\": [288.59271602571766, 2.625473985218113e-06] , \"standarize\": [1.0],\"minmax\": [0.023180405930276215]},\n                     \"domain1_var2\": {\"scale\": [159.97687787439807, 1.9524622248687853e-05] , \"standarize\": [1.0] ,\"minmax\": [0.23941016046328426]},\n                     \"domain2_var1\": {\"scale\": [135.6366302805808, 4.899060757019108e-06] , \"standarize\": [1.0] ,\"minmax\": [0.03016669508052957]},\n                     \"domain2_var2\": {\"scale\": [181.663465549777, 4.495467291813097e-06] , \"standarize\": [1.0] ,\"minmax\": [0.05240332035181045]}},\n             \"power1\":{\"age\": {\"scale\":   [109.4692528739947, 0.00011800535892912307] , \"standarize\": [0.7661214333172722, 31.23665506792682] ,\"minmax\": [0.19027501418938883]} ,\n                     \"domain1_var1\": {\"scale\": [193.24898317054877, 0.00021547271211618736] , \"standarize\": [0.7135776229598471, 6.4016655709129475],\"minmax\": [1.0]  },\n                     \"domain1_var2\": {\"scale\": [31.728634639636926, 0.020224460668951207] , \"standarize\": [0.961358505480476, 0.3282382578756948] ,\"minmax\": [1.0]},\n                     \"domain2_var1\": {\"scale\": [124.57018781462524, 0.00027272661869402835] , \"standarize\": [0.6315442586778667, 4.296746217326582] ,\"minmax\": [1.0]},\n                     \"domain2_var2\": {\"scale\": [169.1220187616997, 0.0002639424761593445] , \"standarize\": [0.8691858535045746, 1.5804030121351031] ,\"minmax\": [1.0]}},\n             \"power2\":{\"age\": {\"scale\":   [184.70387463740076, 1e-06] , \"standarize\": [0.26352752038983795] ,\"minmax\": [0.004841131997346627]} ,\n                     \"domain1_var1\": {\"scale\": [55.88885309194922, 5.791934559291647e-06] , \"standarize\": [1.0],\"minmax\": [0.027400581972783424]},\n                     \"domain1_var2\": {\"scale\":  [433.90185279217513, 0.1205169267601079] , \"standarize\": [1.0] ,\"minmax\": [0.2975556654409895]},\n                     \"domain2_var1\": {\"scale\": [31.997927215173238, 0.00012695504821855731] , \"standarize\": [1.0] ,\"minmax\": [0.03098204335491305]},\n                     \"domain2_var2\": {\"scale\": [24.90099861684024, 0.0001322209114073255] , \"standarize\": [1.0] ,\"minmax\": [0.054167605149726517]}},\n             'multi_lasso':{'scaling': [3791.751469250612, 1e-05], 'standarize': [0.21959169710006501], 'minmax': [0.029412486001607965]},\n             'multi_enet':{'scaling': [3791.751469250612, 1e-05, 0.9999], 'standarize': [0.2196602883675783, 0.9999], 'minmax': [0.028758339435775493, 0.9999]}}","e41611a1":"targets = [\"age\",\"domain1_var1\",\"domain1_var2\",\"domain2_var1\",\"domain2_var2\"]\nmodels = ['enet','lasso','ridge', 'svr', 'bagging_regressor', 'bayesian_ridge', 'ardregression', 'kernel_ridge', 'nusvr', 'linear_svr', 'power0', 'power1', 'power2']\ndata_preprocessings = ['scale','standarize','minmax']\n\nno_folds = 7\n\nresults_test_1 = pd.DataFrame()\nresults_test_2 = pd.DataFrame()\nresults_train = df[['Id','fold_column']+targets]\nresults_test_1['Id'] = test_df_1['Id']\nresults_test_2['Id'] = test_df_2['Id']\n\nw = {\"age\":0.3, \"domain1_var1\":0.175, \"domain1_var2\":0.175, \"domain2_var1\":0.175,\"domain2_var2\":0.175}\n\nfor target in targets:\n    for model in models:\n        for pp in data_preprocessings:\n            print(\"Model: \",model,\" | \",\"Target: \",target,\" | \",\"Preprocessing: \",pp)\n            y_oof = np.zeros(df.shape[0])\n            y_test_1 = np.zeros((test_df.shape[0], 7))\n            y_test_2 = np.zeros((test_df.shape[0], 7))\n            for i in range(no_folds):\n                gc.collect()\n                train_df = df[df['fold_column']!=i]\n                val_df = df[df['fold_column']==i]\n                train_df = train_df[train_df[target].notnull()]\n                parameter = parameters[model][target][pp]\n                if pp == 'scale':\n                    train_df[fnc_features] *= 1\/parameter[0]\n                    val_df[fnc_features] *= 1\/parameter[0]\n                    t1 = test_df_1[features]\n                    t1[fnc_features] *= 1\/parameter[0]\n                    t2 = test_df_2[features]\n                    t2[fnc_features] *= 1\/parameter[0]\n                    t1 = t1.values\n                    t2 = t2.values\n                    parameter = parameter[1:]\n                    \n                if pp == 'standarize':\n                    scaler = StandardScaler()\n                    scaler.fit(train_df[features])\n                    train_df[features] = scaler.transform(train_df[features])\n                    val_df[features] = scaler.transform(val_df[features])\n                    t1 = scaler.transform(test_df_1[features])\n                    t2 = scaler.transform(test_df_2[features])\n                    \n                if pp == 'minmax':\n                    scaler = MinMaxScaler()\n                    scaler.fit(train_df[features])\n                    train_df[features] = scaler.transform(train_df[features])\n                    val_df[features] = scaler.transform(val_df[features])\n                    t1 = scaler.transform(test_df_1[features])\n                    t2 = scaler.transform(test_df_2[features])\n                    \n                if model == 'svr':\n                    M = SVR(C=parameter[0], cache_size=3000.0)\n                \n                if model == 'enet':\n                    M = ElasticNet(alpha = parameter[0], l1_ratio = parameter[1], normalize = False, max_iter = 5000, tol = 1e-5)\n                    \n                if model == 'lasso':\n                    M = Lasso(alpha = parameter[0], normalize = False, max_iter = 5000, tol = 1e-5)\n                \n                if model == 'ridge':\n                    M = Ridge(alpha = parameter[0], normalize = False, max_iter = 5000, tol = 1e-5)\n                    \n                if model == 'bagging_regressor':\n                    M = BaggingRegressor(Ridge(alpha = parameter[0]), n_estimators=parameter[1], random_state=42, \n                                     max_samples=parameter[2], max_features=parameter[3])\n                \n                if model == 'bayesian_ridge':\n                    M = BayesianRidge( n_iter=1000, tol=10e-05)\n                    \n                if model == 'ardregression':\n                    M = ARDRegression()\n                    \n                if model == 'kernel_ridge':\n                    M = KernelRidge(alpha= parameter[0],kernel ='rbf')\n                    \n                if model == 'nusvr':\n                    M = NuSVR(nu=parameter[0], C=parameter[1])\n                    \n                if model == 'linear_svr':\n                    M = LinearSVR(C=parameter[0])\n                    \n                if model in ['power0', 'power1', 'power2']:\n                    M = TweedieRegressor(power = int(model[-1]), alpha = parameter[0])\n                    \n                M.fit(train_df[features].values, train_df[target].values)\n                \n                y_oof[val_df.index] = M.predict(val_df[features])\n                y_test_1[:, i] = M.predict(t1)\n                y_test_2[:, i] = M.predict(t2)\n                \n            results_train[target+'.'+model+'.'+pp] = y_oof\n            results_test_1[target+'.'+model+'.'+pp] = y_test_1.mean(axis=1)\n            results_test_2[target+'.'+model+'.'+pp] = y_test_2.mean(axis=1)\n            \n            score = metric(df[df[target].notnull()][target].values, results_train[df[target].notnull()][target+'.'+model+'.'+pp].values)\n            print('Score: ',score)\n            print()","7afa4a2f":"## For multi-tasking models\n\nfor model in ['multi_enet', 'multi_lasso']:\n    for pp in data_preprocessings:\n        print(\"Model: \",model,\" | \",\"Preprocessing: \",pp)\n        y_oof = np.zeros((df.shape[0], 5))\n        y_test_1 = np.zeros((test_df.shape[0], 5))\n        y_test_2 = np.zeros((test_df.shape[0], 5))\n        for fold_no in range(7):\n            gc.collect()\n            train_df = df[df['fold_column']!=fold_no]\n            val_df = df[df['fold_column']==fold_no]\n            for t in target:\n                train_df = train_df[train_df[t].notnull()]\n            parameter = parameters[model][pp]\n            if pp == 'scaling':\n                train_df[fnc_features] *= 1\/parameter[0]\n                val_df[fnc_features] *= 1\/parameter[0]\n                t1 = test_df_1[features]\n                t1[fnc_features] *= 1\/parameter[0]\n                t2 = test_df_2[features]\n                t2[fnc_features] *= 1\/parameter[0]\n                t1 = t1.values\n                t2 = t2.values\n                parameter = parameter[1:]\n\n            if pp == 'standarize':\n                scaler = StandardScaler()\n                scaler.fit(train_df[features])\n                train_df[features] = scaler.transform(train_df[features])\n                val_df[features] = scaler.transform(val_df[features])\n                t1 = scaler.transform(test_df_1[features])\n                t2 = scaler.transform(test_df_2[features])\n\n            if pp == 'minmax':\n                scaler = MinMaxScaler()\n                scaler.fit(train_df[features])\n                train_df[features] = scaler.transform(train_df[features])\n                val_df[features] = scaler.transform(val_df[features])\n                t1 = scaler.transform(test_df_1[features])\n                t2 = scaler.transform(test_df_2[features])\n                \n            if model == 'multi_enet':\n                M = MultiTaskElasticNet(alpha = parameter[0], l1_ratio = parameter[1],\n                                         normalize = False,\n                                         max_iter = 5000, tol = 1e-5)\n            \n            if model == 'multi_lasso':\n                M = MultiTaskLasso(alpha = parameter[0],\n                                         normalize = False,\n                                         max_iter = 5000, tol = 1e-5)\n            \n            M.fit(train_df[features].values, train_df[target].values)\n            y_oof[val_df.index] = M.predict(val_df[features])\n            y_test_1 += M.predict(t1)\n            y_test_2 += M.predict(t2)\n            \n        for x, t in enumerate(target):\n            results_train[t+'.'+model+'.'+pp] = y_oof[:,x]\n            results_test_1[t+'.'+model+'.'+pp] = y_test_1[:,x]\/7\n            results_test_2[t+'.'+model+'.'+pp] = y_test_2[:,x]\/7\n            mae, score = metric(results_train[df[t].notnull()][t].values, results_train[df[t].notnull()][t+'.'+model+'.'+pp].values)\n            print('Feature: ', t, \" | \", 'Score: ', score, \" | \", 'MAE: ', mae)\n        print()","5873b1db":"results_train.to_csv('OOF_preds.csv')\nresults_test_1.to_csv('Test_preds_1.csv')\nresults_test_2.to_csv('Test_preds_2.csv')","49af8e30":"oof = pd.read_csv('OOF_preds.csv')\n\nfor i in sorted(oof.columns):\n    t = i.split('.')[0]\n    if (t in targets) and i!=t:\n        mae, score = metric(oof[oof[t].notnull()][t].values, oof[oof[t].notnull()][i].values)\n        print(\"Target\", t ,' | ',i ,' | ','MAE', mae, '|', 'Score', score)","0a10451e":"## Correlation matrices\n\nfor i in targets:\n    c = []\n    print(\"#######################\",i,\"############################\")\n    for j in oof.columns:\n        if (i in j) and (j!=i):\n            c.append(j)\n    corrMatrix = oof[c].corr()\n    sns.heatmap(corrMatrix)\n    plt.show()","73224573":"test_1 = pd.read_csv('Test_preds_1.csv')\ntest_2 = pd.read_csv('Test_preds_1.csv')","ed474b7b":"model_names = ['bayesian_ridge', 'bagging_regressor', 'ElasticNet', 'Ridge', 'SVR',  'NuSVR']","db9075ae":"results_test_1 = pd.DataFrame()\nresults_test_2 = pd.DataFrame()\nresults_train = oof[['Id','fold']+targets]\nresults_test_1['Id'] = test_1['Id']\nresults_test_2['Id'] = test_2['Id']\n\nw = {'age': 0.3,'domain1_var1': 0.175,'domain1_var2': 0.175,'domain2_var1': 0.175,'domain2_var2': 0.175}\n\noveral_score = 0\nfor target in targets:\n    print(\"Target\",target)\n    all_scores = []\n    y_oof = np.zeros(oof.shape[0])\n    y_test_1 = np.zeros((test_1.shape[0], len(model_names)))\n    y_test_2 = np.zeros((test_2.shape[0], len(model_names)))\n    features = [i for i in oof.columns if (target in i) and (i!=target)]\n    y_oof = np.zeros((oof.shape[0],len(model_names)))\n    for k,model in enumerate([BayesianRidge(), BaggingRegressor(Ridge()),ElasticNet(max_iter=5000, tol=10e-05), Ridge(tol=10e-05), SVR(max_iter=5000, tol=10e-05), NuSVR(tol=10e-05)]):\n        for fold_no in range(7):\n            gc.collect()\n            train_df = oof[oof['fold']!=fold_no]\n            val_df = oof[oof['fold']==fold_no]\n            train_df = train_df[train_df[target].notnull()]\n\n            model.fit(train_df[features].values, train_df[target].values)\n\n            y_oof[val_df.index,k] = model.predict(val_df[features])\n            y_test_1[:, k] += model.predict(test_1[features])\/7\n            y_test_2[:, k] += model.predict(test_2[features])\/7\n        \n        mae, score = metric(oof[oof[target].notnull()][target].values, y_oof[oof[target].notnull(),k])\n        all_scores.append(mae)\n        print(model_names[k],mae,score)\n    weights = 0 - np.array(all_scores)\n    weights = 0.001+weights - weights.min()\n    weights = weights\/weights.sum()\n    TTTT = pd.DataFrame(np.corrcoef(y_oof.T))\n    TTTT.index = model_names\n    TTTT.columns = model_names\n    display(TTTT)\n    print(weights)\n    results_train[target] = (y_oof*weights).sum(axis=1)  \n    results_test_1[target] = (y_test_1*weights).sum(axis=1)  \n    results_test_2[target] = (y_test_2*weights).sum(axis=1)  \n \n    mae, score = metric(oof[oof[target].notnull()][target].values, results_train[oof[target].notnull()][target].values)\n    print('Target: ', target, \" | \",'Score: ',score,\" | \",\"MAE: \", mae)\n    overal_score += w[target]*score\n    print()\n\nprint('Overall Score: ', overal_score)","dc3adb97":"results_train.to_csv('OOF_preds.csv')\nresults_test_1.to_csv('Test_preds_1.csv')\nresults_test_2.to_csv('Test_preds_2.csv')","9b682514":"sub_df = pd.melt(results_test_1[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nsub_df","2f1ecd8f":"sub_df.to_csv(\"submission_test_1.csv\", index=False)","32fd9245":"sub_df = pd.melt(results_test_2[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nsub_df","2a330b0f":"sub_df.to_csv(\"submission_test_2.csv\", index=False)","0c686d96":"r1 = pd.read_csv('submission_test_1.csv').set_index('Id')\nr2 = pd.read_csv('submission_test_2.csv').set_index('Id')\np = pd.read_csv('..\/input\/classifier9siteprobs\/prob_8.csv').drop(columns = ['Unnamed: 0']).set_index('Id')\n\nresults = []\nfor i in tqdm(r1.index):\n    \n    prob = p.loc[int(i.split('_')[0])]['prob']\n    \n    if prob>=0.7:\n        prob = 1\n    if prob<=0.3:\n        prob = 0\n    \n    value = r1.loc[i]['Predicted']*(1-prob) + r2.loc[i]['Predicted']*(prob)\n    \n    results.append({'Id':i, 'Predicted':value})\n    \nr = pd.DataFrame(results)\nr.to_csv(\"submission.csv\", index=False)","1251e4c1":"# **Level-1**\nAt level-1,\n+ We have 46 different models\n+ 7 fold cross-validation is used\n+ Hyper-parameters of each model are tuned using bayesian hyperparameter optimization\n+ Models are ElasticNet, Lasso, Ridge, SVR, Bagging Regressor, Bayesian Ridge, Ard Regression, Kernel Ridge, NuSVR, Linear SVR, GLM (power=0, power=1, and power=2), MultiTasking ElasticNet, MultiTasking Lasso and MultiTasking Neural Network\n+ Three different variants of each model are prepared using three different data pre-processing techniques - scaling fnc features, feature standarization and minmax scaling of features. ","42656b70":"# Level - 2 & 3\nAt level 2,\n+ We have 6 different models\n+ Models are - Bayesian Ridge, Bagging Regressor, ElasticNet, Ridge, SVR, and NuSVR\n\nAt level 3, we took the weighted average of predictions by 6 different models of level two, where weights are given on the basis of their out-of-fold prediction scores.","1eb0c91d":"A lot of features and a small training dataset makes this comptetion a challenge to generalize for the samples with different behaviour. My teammate [@kumar_shubham](https:\/\/www.kaggle.com\/ks2019) found that site 2 samples can be effectively predicted by adding the difference of mean of site-1 and mean of known site-2 samples to site-2 samples, other details can be found in [this](https:\/\/www.kaggle.com\/ks2019\/35th-place-blend-best-3) kernel. This kernel has the stacked ensemble which we used for predictions. <br>\n(Note: We used [this](https:\/\/www.kaggle.com\/ttahara\/trends-simple-nn-baseline) public kernel for NN predictions.)","c7411721":"# Level - 1","25daa44b":"# Level-2"}}