{"cell_type":{"19776895":"code","2912d4d0":"code","670c0fec":"code","f368c997":"code","6f4c3a86":"code","63fb60bc":"code","aadf7ba3":"code","ef24637f":"code","ef5de735":"code","f2df7629":"code","49055818":"code","4206e548":"code","15cd488e":"code","45102dc2":"code","23c7ccb7":"code","b3aa4861":"code","34cfdcc4":"code","6878fa0d":"code","589fd925":"code","6a50128c":"code","026d22d2":"code","cfbe16b0":"code","181059fc":"code","14bf564f":"code","84b3365c":"code","7dfdce83":"code","29d001d0":"code","d611eeaa":"code","1365f213":"code","525eadd7":"code","aee51747":"code","17ef406d":"code","f1b35b0f":"code","45cee50e":"code","9a7fec32":"code","975680f0":"code","31e3ae92":"code","5bcf5f31":"code","30ab8614":"code","56b5aa40":"code","dcd94dbe":"code","3b907338":"code","b036e12a":"code","861a1a1e":"code","b8b0bb04":"code","a933994d":"code","c393ae94":"code","93420a5b":"code","653ea0ef":"code","e6f8f89a":"code","29702be6":"code","54fa0876":"code","4e110daa":"code","198fe46b":"code","e0e52642":"code","d21e866e":"code","7efe6342":"code","e8430e29":"code","2cbec956":"code","caa86be7":"code","c8e582d8":"code","90a31bd9":"code","f6cf6edb":"code","4c048823":"code","c637f93e":"code","a304b9f0":"code","0115f009":"code","d18c6d68":"code","503726ec":"code","137a083f":"code","f8e056c2":"code","3d5d303a":"code","13ea7c38":"code","d12ee31f":"code","3e915420":"code","f0726563":"code","16dac3f4":"code","0b512da7":"code","12cc7582":"code","b5120c81":"code","43afb0d0":"code","c0833b8b":"code","e84a03f8":"code","bed106a6":"code","69628286":"markdown","f9be8b83":"markdown","a4ab490f":"markdown","bdd5d3c0":"markdown","16970968":"markdown","04464beb":"markdown","186f39ce":"markdown","cc8df2f1":"markdown","7d9e8c1d":"markdown","504916ff":"markdown","cf5c4487":"markdown","b3f525ea":"markdown","452f3c30":"markdown","b6176134":"markdown","cc7e716d":"markdown","c68b59e8":"markdown","2bc209c3":"markdown","38ccd749":"markdown","886e014e":"markdown","381800ee":"markdown","d8981f3d":"markdown","0b90d776":"markdown","22888333":"markdown","9d03f3cd":"markdown","eb1c985e":"markdown","45874e39":"markdown","18675c03":"markdown","81704098":"markdown","d3a890b4":"markdown","46ff1819":"markdown","90331554":"markdown","2f424584":"markdown","124c592e":"markdown","b0b86661":"markdown","055ee6fe":"markdown","70d6fb93":"markdown","dfd7b85a":"markdown","fc72c55f":"markdown","16cd7c77":"markdown","2daa3c77":"markdown","8c76fbba":"markdown","f60bc767":"markdown","d44cc9f0":"markdown","bef53a03":"markdown","c2d41b26":"markdown","e23c7867":"markdown","bc65b1ab":"markdown","fbabcf27":"markdown","d8e5e9f6":"markdown","f494f411":"markdown"},"source":{"19776895":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport seaborn as sns; sns.set(style='white')\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom wordcloud import WordCloud\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport math\nimport pickle\n\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nfrom sklearn.model_selection import StratifiedKFold\n\npd.set_option('max_colwidth', 40)","2912d4d0":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 3 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","670c0fec":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","f368c997":"Dropout_new = 0.15     # originally 0.1\nn_split = 5            # originally 5\nlr = 3e-5              # originally 3e-5","6f4c3a86":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","63fb60bc":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","aadf7ba3":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","ef24637f":"%%time\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting all Train for Outlier analysis...')\n    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n    preds_start_train += preds_train[0]\/skf.n_splits\n    preds_end_train += preds_train[1]\/skf.n_splits\n\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","ef5de735":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\nprint(jac) # Jaccard CVs","f2df7629":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)","49055818":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\ntest.sample(10)","4206e548":"# Visualization training prediction results\nall = []\nstart = []\nend = []\nstart_pred = []\nend_pred = []\nfor k in range(input_ids.shape[0]):\n    a = np.argmax(preds_start_train[k,])\n    b = np.argmax(preds_end_train[k,])\n    start.append(np.argmax(start_tokens[k]))\n    end.append(np.argmax(end_tokens[k]))        \n    if a>b:\n        st = train.loc[k,'text']\n        start_pred.append(0)\n        end_pred.append(len(st))\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n        start_pred.append(a)\n        end_pred.append(b)\n    all.append(st)\ntrain['start'] = start\ntrain['end'] = end\ntrain['start_pred'] = start_pred\ntrain['end_pred'] = end_pred\ntrain['selected_text_pred'] = all\ntrain.sample(10)","15cd488e":"def metric_tse(df,col1,col2):\n    # Calc metric of tse-competition - according to https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/overview\/evaluation\n    return df.apply(lambda x: jaccard(x[col1],x[col2]),axis=1)","45102dc2":"# Analytics\ntrain = train.replace({'sentiment': {'negative': -1, 'neutral': 0, 'positive': 1}})\ntrain['len_text'] = train['text'].str.len()\ntrain['len_selected_text'] = train['selected_text'].str.len()\ntrain['diff_num'] = train['end']-train['start']\ntrain['share'] = train['len_selected_text']\/train['len_text']","23c7ccb7":"# Prediction analytics\ntrain['selected_text_pred'] = train['selected_text_pred'].map(lambda x: x.lstrip(' '))\ntrain['len_selected_text_pred'] = train['selected_text_pred'].str.len()\ntrain['diff_num_pred'] = train['end_pred']-train['start_pred']\ntrain['share_pred'] = train['len_selected_text_pred']\/train['len_text']\n# len_equal\ntrain['len_equal'] = 0\ntrain.loc[(train['start'] == train['start_pred']) & (train['end'] == train['end_pred']), 'len_equal'] = 1\n# metric\ntrain['metric'] = metric_tse(train,'selected_text','selected_text_pred')\n# res\ntrain['res'] = 0\ntrain.loc[train['metric'] == 1, 'res'] = 1","b3aa4861":"def rep_3chr(text):\n    # Checks if there are 3 or more repetitions of characters in words\n    chr3 = 0\n    for word in text.split():\n        for c in set(word):\n            if word.rfind(c+c+c) > -1:\n                chr3 = 1                \n    return chr3","34cfdcc4":"# Analysis of 3 or more repetitions of characters in words\ntrain['text_chr3'] = train['text'].apply(rep_3chr)\ntrain['selected_text_chr3'] = train['selected_text'].apply(rep_3chr)\ntrain['selected_text_pred_chr3'] = train['selected_text_pred'].apply(rep_3chr)","6878fa0d":"# result\ncol_interesting = ['sentiment', 'len_text', 'text_chr3', 'selected_text', 'len_selected_text', 'diff_num', 'share', \n                   'selected_text_chr3', 'selected_text_pred', 'len_selected_text_pred', 'diff_num_pred', 'share_pred',\n                   'selected_text_pred_chr3', 'len_equal', 'metric', 'res']\ntrain[col_interesting].head(10)","589fd925":"print('Total metric =',train['metric'].mean())","6a50128c":"train.describe()","026d22d2":"# Outlier\ntrain_outlier = train[train['res'] == 0].reset_index(drop=True)\ntrain_outlier","cfbe16b0":"train_outlier.describe()","181059fc":"sh_out = str(round(len(train_outlier)*100\/len(train),1))\nprint('Number of outliers is ' + sh_out + '% from training data')","14bf564f":"# Good prediction\ntrain_good = train[train['res'] == 1].reset_index(drop=True)\ntrain_good","84b3365c":"train_good.describe()","7dfdce83":"print('Share of all data')\ntrain[['share', 'share_pred']].hist(bins=10)","29d001d0":"print('Share of outlier data')\ntrain_outlier[['share', 'share_pred']].hist(bins=10)","d611eeaa":"# Only one word in 'selected_text'\ntrain_outlier[train_outlier['diff_num']==0]","1365f213":"train_good[train_good['diff_num']==0]","525eadd7":"# 'selected_text' = 'text'\ntrain_outlier[train_outlier['share']==1]","aee51747":"train_good[train_good['share']==1]","17ef406d":"# Only one word in 'text'\ntrain_outlier[train_outlier[\"text\"].str.find(' ') == -1].head(5)","f1b35b0f":"len(train_outlier[train_outlier[\"text\"].str.find(' ') == -1])","45cee50e":"train_good[train_good[\"text\"].str.find(' ') == -1].head(5)","9a7fec32":"len(train_good[train_good[\"text\"].str.find(' ') == -1])","975680f0":"def plot_word_cloud(x, col):\n    corpus=[]\n    for k in x[col].str.split():\n        for i in k:\n            corpus.append(i)\n    plt.figure(figsize=(12,8))\n    word_cloud = WordCloud(\n                              background_color='black',\n                              max_font_size = 80\n                             ).generate(\" \".join(corpus[:50]))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()\n    return corpus[:50]","31e3ae92":"# All training text\nprint('Word Cloud for all text in training data')\ntrain_all = plot_word_cloud(train, 'text')","5bcf5f31":"train_all","30ab8614":"# All test\nprint('Word Cloud for all text in test')\ntest_all = plot_word_cloud(test, 'text')","56b5aa40":"test_all","dcd94dbe":"# All training selected_text\nprint('Word Cloud for selected_text in training data')\ntrain_selected_text = plot_word_cloud(train, 'selected_text')","3b907338":"train_selected_text","b036e12a":"# Oitlier WordCloud\nprint('Word Cloud for Outliers')\noutlier_max = plot_word_cloud(train_outlier, 'selected_text')","861a1a1e":"outlier_max","b8b0bb04":"# Worst oitlier WordCloud\nprint('Word Cloud for the 100 worst outliers')\noutlier_max100 = plot_word_cloud(train_outlier.nsmallest(100, 'metric', keep='all'), 'selected_text')","a933994d":"outlier_max100","c393ae94":"# Worst oitlier WordCloud\nprint('Word Cloud for the 1000 worst outliers')\noutlier_max1000 = plot_word_cloud(train_outlier.nsmallest(1000, 'metric', keep='all'), 'selected_text')","93420a5b":"outlier_max1000","653ea0ef":"# Good prediction WordCloud\nprint('Word Cloud for good prediction')\ngood_max = plot_word_cloud(train_good, 'selected_text')","e6f8f89a":"good_max","29702be6":"def subtext_analysis(col, subtext, df1, str1, df2, str2):\n    # Calc statistics as table for subtext in the df1[col] (smaller) in compare to df2[col] (bigger) \n    \n    result = pd.DataFrame(columns = ['subtext', str1, str2, 'share,%'])\n    if (len(df1) > 0) and (len(df2) > 0):\n        for i in range(len(subtext)):\n            result.loc[i,'subtext'] = subtext[i]\n            num1 = len(df1[df1[col].str.find(subtext[i]) > -1])\n            result.loc[i, str1] = num1\n            num2 = len(df2[df2[col].str.find(subtext[i]) > -1])\n            result.loc[i, str2] = num2\n            result.loc[i,'share,%'] = round(num1*100\/num2,1) if num2 != 0 else 0\n    print('Number of all data is', len(df2))\n    display(result.sort_values(by=['share,%', str1], ascending=False))","54fa0876":"def subtext_analysis_one_df(col, subtext, df, str):\n    # Calc statistics as table for subtext in the df[col]\n    \n    result = pd.DataFrame(columns = ['subtext', str, 'share of all,%'])\n    num_all = len(df)\n    if (num_all > 0):\n        for i in range(len(subtext)):\n            result.loc[i,'subtext'] = subtext[i]\n            num = len(df[df[col].str.find(subtext[i]) > -1])\n            result.loc[i, str] = num\n            result.loc[i,'share of all,%'] = round(num*100\/num_all,1)\n    print('Number of all data is', len(df))\n    display(result.sort_values(by='share of all,%', ascending=False))    ","4e110daa":"subtext_test = ['SAD', 'bullying', 'Uh', 'oh', 'onna', 'fun', 'addicted', 'Power', 'well', 'unhappy', 'funny', 'Tears', 'Fears', 'sleeeeepy', ' ', ',', '?', '!' ,'!!', '!!!', ':\/', '...', 'http', '****']","198fe46b":"subtext_analysis(\"selected_text\", subtext_test, train_outlier, 'train_outliers', train, 'train_all')","e0e52642":"subtext_analysis(\"selected_text\", subtext_test, train_good, 'train_good', train, 'train_all')","d21e866e":"subtext_analysis_one_df(\"selected_text\", subtext_test, train, 'test_all')","7efe6342":"subtext_analysis_one_df(\"selected_text\", subtext_test, test, 'test_all')","e8430e29":"test['text_chr3'] = test['text'].apply(rep_3chr)\ntest.head(10)","2cbec956":"test.describe()","caa86be7":"print('Metric of prediction for training data')\ntrain[['metric']].hist(bins=10)","c8e582d8":"print('Metric of prediction for outliers of training data')\ntrain_outlier[['metric']].hist(bins=10)","90a31bd9":"train_outlier1 = train_outlier.nsmallest(1000, 'metric', keep='all')\ntrain_outlier2 = train_outlier.nsmallest(2000, 'metric', keep='all')\ntrain_outlier3 = train_outlier.nsmallest(3000, 'metric', keep='all')\ntrain_outlier5 = train_outlier.nsmallest(5000, 'metric', keep='all')\ntrain_outlier8 = train_outlier.nsmallest(8000, 'metric', keep='all')","f6cf6edb":"subtext_analysis(\"selected_text\", subtext_test, train_outlier1, 'in worst 1000 outliers', train_outlier, 'in all outliers')","4c048823":"subtext_analysis(\"selected_text\", subtext_test, train_outlier2, 'in worst 2000 outliers', train_outlier, 'in all outliers')","c637f93e":"subtext_analysis(\"selected_text\", subtext_test, train_outlier3, 'in worst 3000 outliers', train_outlier, 'in all outliers')","a304b9f0":"subtext_analysis(\"selected_text\", subtext_test, train_outlier5, 'in worst 5000 outliers', train_outlier, 'in all outliers')","0115f009":"subtext_analysis(\"selected_text\", subtext_test, train_outlier8, 'in worst 8000 outliers', train_outlier, 'in all outliers')","d18c6d68":"train_outlier1.describe()","503726ec":"train_outlier2.describe()","137a083f":"train_outlier3.describe()","f8e056c2":"train_outlier5.describe()","3d5d303a":"train_outlier8.describe()","13ea7c38":"# Histograms of interesting features in training data\ncol_hist = ['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', \n            'text_chr3', 'selected_text_chr3', 'selected_text_pred_chr3', 'metric']","d12ee31f":"print('Statistics for 1000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier1[col_hist].hist(ax=ax)\nplt.show()","3e915420":"print('Statistics for 2000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier2[col_hist].hist(ax=ax)\nplt.show()","f0726563":"print('Statistics for 3000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier3[col_hist].hist(ax=ax)\nplt.show()","16dac3f4":"print('Statistics for 5000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier5[col_hist].hist(ax=ax)\nplt.show()","0b512da7":"print('Statistics for 8000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier8[col_hist].hist(ax=ax)\nplt.show()","12cc7582":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True, title=None):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.title(title)\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Good')\n            blue_patch = mpatches.Patch(color='blue', label='Outlier')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})","b5120c81":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(preds_start_train, train['res'], title='Predicted start places of selected text in training data')\nplt.show()","43afb0d0":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(preds_end_train, train['res'], title='Predicted end places of selected text in training data')\nplt.show()","c0833b8b":"data = train[['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', 'diff_num', 'share', 'metric', 'res']].dropna()\ndata","e84a03f8":"# Thanks to https:\/\/www.kaggle.com\/kashnitsky\/topic-7-unsupervised-learning-pca-and-clustering\ninertia = []\npca = PCA(n_components=2)\n# fit X and apply the reduction to X \nx_3d = pca.fit_transform(data)\nfor k in range(1, 8):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(x_3d)\n    inertia.append(np.sqrt(kmeans.inertia_))\nplt.plot(range(1, 8), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');","bed106a6":"# Thanks to https:\/\/www.kaggle.com\/arthurtok\/a-cluster-of-colors-principal-component-analysis\n# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=5, random_state=0)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_3d)\nLABEL_COLOR_MAP = {0 : 'r',\n                   1 : 'g',\n                   2 : 'b',\n                   3 : 'y',\n                   4 : 'c'}\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (7,7))\nplt.scatter(x_3d[:,0],x_3d[:,1], c= label_color, alpha=0.9)\nplt.show()","69628286":"Code from notebook https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972","f9be8b83":"## 5.4. Metric analysis <a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","a4ab490f":"## 2. Download data & FE <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","bdd5d3c0":"Using my notebook https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert","16970968":"## Previous successful commits","04464beb":"### Commit 18\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* BATCH_SIZE = 24      # originally 32\n\nLB = 0.704","186f39ce":"Code from notebook https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972\n\n**Upgrade:** add prediction for training data for Outlier analysis and parameters tuning","cc8df2f1":"Long 'selected text' are not predicted correctly (get too long)","7d9e8c1d":"## 3.2. Model training <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","504916ff":"There are a number of clear clusters that allow us to hope that we can improve the solution.","cf5c4487":"### Commit 3 (with original parameters)\n\n* Dropout_new = 0.1\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","b3f525ea":"### Commit 9\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* LeakyReLU_alpha=0.05\n\nLB = 0.711","452f3c30":"### Commit 17\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-5\n\nLB = 0.709","b6176134":"I hope you find this kernel useful and enjoyable.","cc7e716d":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data & FE](#2)\n1. [Model tuning](#3)\n   - [My upgrade of parameters](#3.1)\n   - [Model training](#3.2)\n1. [Submission](#4)\n1. [Outlier analysis](#5)\n    - [Training prediction result visualization](#5.1)\n    - [WordCloud](#5.2)\n    - [Subtext analysis](#5.3)\n    - [Metric analysis](#5.4)\n    - [PCA visualization](#5.5)\n    - [Clustering](#5.6)","c68b59e8":"Code from notebook https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972","2bc209c3":"### Commit 14\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n\n**LB = 0.715 (the best)**","38ccd749":"## Acknowledgements\n* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert)\n* [COVID-19 (Week5) Global Forecasting - EDA&ExtraTR](https:\/\/www.kaggle.com\/vbmokin\/covid-19-week5-global-forecasting-eda-extratr)\n* [TSE2020] RoBERTa (CNN) & Random Seed Distribution (https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution)\n* Chris Deotte's post: https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/142404#809872\n* [Faster (2x) TF roBERTa](https:\/\/www.kaggle.com\/seesee\/faster-2x-tf-roberta)\n* Many thanks to Chris Deotte for his TF roBERTa dataset at https:\/\/www.kaggle.com\/cdeotte\/tf-roberta\n* https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds","886e014e":"There are a number of clear patterns that allow us to hope that we can improve the solution.","381800ee":"## 3. Model tuning <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","d8981f3d":"## 4. Submission <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","0b90d776":"## 5.6. Clustering <a class=\"anchor\" id=\"5.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","22888333":"Your comments and feedback are most welcome.","9d03f3cd":"### Commit 10\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* LeakyReLU_alpha=0.3\n\nLB = 0.711","eb1c985e":"### Commit 6\n\n* Dropout_new = 0.15\n* n_split = 7\n* lr = 3e-5\n\nLB = 0.709","45874e39":"### Commit 21\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-4\n* num_cnn2 = 96          # originally 64\n\nLB = 0.712","18675c03":"Using my notebook https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert","81704098":"# Results of analysis:\n1. Outlier analysis of the best solutions on basic roBERTa - pls. see https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/155419\n2. Analysis of the predictions with the worst score=0 from roBERTa - pls. see https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/155616\n3. New (commit 22): **analysis of 3 or more repetitions of characters in words**","d3a890b4":"### Commit 5\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.713","46ff1819":"## 3.1. My upgrade of parameters <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","90331554":"There are problems in processing: \"!\", \"!!\", \"!!!\", \":\/\", \"...\", \"http\" etc.","2f424584":"## 5.2. WordCloud <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","124c592e":"### Commit 20\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 1e-4\n\nLB = 0.709","b0b86661":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Tweet Sentiment Extraction](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction)","055ee6fe":"### Commit 12\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 3e-5\n* SEED = 42\n\nLB = 0.711","70d6fb93":"Using my notebook https:\/\/www.kaggle.com\/vbmokin\/covid-19-week5-global-forecasting-eda-extratr","dfd7b85a":"## 5.3. Subtext analysis <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","fc72c55f":"[Go to Top](#0)","16cd7c77":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","2daa3c77":"## 5. Outlier analysis <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","8c76fbba":"### Commit 7\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 4e-5\n\nLB = 0.709","f60bc767":"## 5.5. PCA visualization <a class=\"anchor\" id=\"5.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","d44cc9f0":"### Commit 19\n\n* Dropout_new = 0.125\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","bef53a03":"### Commit 8\n\n* Dropout_new = 0.15\n* n_split = 5\n* lr = 2e-5\n\nLB = 0.712","c2d41b26":"# Overview\n\nIn this notebook, I analyze and visualize the outliers of the NLP solution from very good notebook \"[TSE2020] RoBERTa (CNN) & Random Seed Distribution\"(https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution) using the functions from my notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert) including PCA processing, Kmeans clustering, WordCloud and others. More over I try to improve the original solution.\n\nAdd chapters \"**Subtext analysis**\" and \"**Metric analysis**\" from the commit 10.","e23c7867":"### Commit 15\n\n* Dropout_new = 0.16\n* n_split = 5\n* lr = 3e-5\n* SEED = 777\n\nLB = 0.710","bc65b1ab":"The main problem is in the predicting of the longest and shortest selected_text which are most or least different from the given text","fbabcf27":"### Commit 13\n\n* Dropout_new = 0.16\n* n_split = 5\n* lr = 3e-5\n\nLB = 0.711","d8e5e9f6":"## 5.1. Training prediction result visualization <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","f494f411":"Text from a single word almost always processes correctly"}}