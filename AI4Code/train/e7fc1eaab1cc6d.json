{"cell_type":{"7b51de2b":"code","60eaff2a":"code","5faabdb3":"code","19b59ddc":"code","a150f5ff":"code","1e15f8b9":"code","752cf0b0":"code","3d579b49":"code","13c41a73":"code","e8787d49":"code","44114584":"code","891cb1ab":"code","38bff14a":"code","99979f8e":"code","fc1fcddc":"code","5afd7bbf":"code","502cb3c2":"code","cb301e53":"code","809e10f0":"code","4a74f590":"code","d77c5894":"code","27217f26":"code","9a3460c0":"code","f4f8f7fd":"code","8b5f0cd7":"code","c42c2b15":"code","d88d0211":"code","77af3b3b":"code","b76cef44":"code","71cc377d":"code","c3779fa3":"code","775493f7":"code","8ccad61c":"code","6a1769b6":"code","a16d9b50":"code","d2c67c61":"code","2311af47":"code","0fe51eac":"code","1c228867":"code","27619d66":"code","3e93f046":"code","552f130e":"code","871ebeb7":"code","15fae650":"code","de0f0915":"code","f22b49e7":"markdown","2550aacb":"markdown","84d2b152":"markdown","97706df0":"markdown","94e23806":"markdown","b96affca":"markdown","2a95ef88":"markdown","08cc0d7f":"markdown","d75fc0cb":"markdown","e51779f5":"markdown","2ef6c327":"markdown","0e4dc591":"markdown","bc4a2698":"markdown","c7704214":"markdown","14cba312":"markdown","10257f7d":"markdown","638e8a17":"markdown"},"source":{"7b51de2b":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","60eaff2a":"df_train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ndf_sub = pd.read_csv(\"..\/input\/ensemble-of-public-submissions\/submission.csv\")","5faabdb3":"unique_pressures = df_train[\"pressure\"].unique()\nsorted_pressures = np.sort(unique_pressures)","19b59ddc":"total_pressures_len = len(sorted_pressures)\n\ndef find_nearest(prediction):\n    insert_idx = np.searchsorted(sorted_pressures, prediction)\n    if insert_idx == total_pressures_len:\n        # If the predicted value is bigger than the highest pressure in the train dataset,\n        # return the max value.\n        return sorted_pressures[-1]\n    elif insert_idx == 0:\n        # Same control but for the lower bound.\n        return sorted_pressures[0]\n    lower_val = sorted_pressures[insert_idx - 1]\n    upper_val = sorted_pressures[insert_idx]\n    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val","a150f5ff":"%%time\ndf_sub[\"pressure\"] = df_sub[\"pressure\"].apply(find_nearest)","1e15f8b9":"df_sub","752cf0b0":"df_sub.to_csv(\"submission_round_LB154.csv\", index=False)\nsubmission_round_LB154 = df_sub.copy()","3d579b49":"pressure_freq = df_train['pressure'].value_counts().to_frame()\npressure_freq['freq'] = df_train['pressure'].value_counts(normalize = True).values\npressure_freq = pressure_freq.sort_index().reset_index()\npressure_freq.columns = ['pressure', 'count', 'freq']","13c41a73":"pressure_freq # already sorted by 'pressure' due to sort_index()","e8787d49":"pressure_freq['count_pre'] = pressure_freq['count'].shift(1)\npressure_freq[pressure_freq['count_pre'] == pressure_freq['count']]","44114584":"pressure_freq['pressure_pre'] = pressure_freq['pressure'].shift(1)\npressure_freq['pressure_step'] = pressure_freq['pressure'] - pressure_freq['pressure_pre']","891cb1ab":"PRESSURE_MIN = pressure_freq['pressure'].min()\nPRESSURE_MAX = pressure_freq['pressure'].max()\nPRESSURE_STEP = pressure_freq['pressure_step'].mean()\nPRESSURE_STEP","38bff14a":"pressure_freq['pressure_step'].describe()","99979f8e":"pressure_freq['freq_pre'] = pressure_freq['freq'].shift(1)\npressure_freq['freq_relative_pct'] = pressure_freq['freq'] \/ (pressure_freq['freq'] + pressure_freq['freq_pre'])\n# 'pressure_prob' will be the neighboring probability weighted cut point to be used\npressure_freq['pressure_prob'] = pressure_freq['pressure_pre'] + \\\n                                 pressure_freq['pressure_step'] * pressure_freq['freq_relative_pct']","fc1fcddc":"# 'pressure_half' will be the middle cut point for the usual equal weighted rounding\npressure_freq['pressure_half'] = (pressure_freq['pressure_pre'] + pressure_freq['pressure']) \/ 2\nplt.figure(figsize=(10,6))\nplt.plot(pressure_freq['pressure_prob'] - pressure_freq['pressure_half'], \n             label = 'Differences in two cut points within a pressure step')\nplt.plot(pressure_freq['freq'], label = 'Relative freqs of 950 distinct pressures')\nplt.title('Differences in two cut points of two rounding methods')\nplt.legend()\nplt.show()","5afd7bbf":"pressure_freq","502cb3c2":"#sorted_pressures = np.sorted(pressure_freq['pressure'])\ntotal_pressures_len = len(sorted_pressures)\n\ndef find_nearest_prob(prediction):\n    '''\n    Probability weighted rounding.\n    Just modify the lines after 'upper_val' of function 'find_nearest'\n    '''\n    insert_idx = np.searchsorted(sorted_pressures, prediction)\n    if insert_idx == total_pressures_len:\n        # If the predicted value is bigger than the highest pressure in the train dataset,\n        # return the max value.\n        return sorted_pressures[-1]\n    elif insert_idx == 0:\n        # Same control but for the lower bound.\n        return sorted_pressures[0]\n    lower_val = sorted_pressures[insert_idx - 1]\n    upper_val = sorted_pressures[insert_idx]\n    cut_val = pressure_freq['pressure_prob'][insert_idx]\n        # Existing usual rounding without freqency adjustment\n#     return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n        # New probability weighted rounding adjusted for the different in relative frequencies\n    return lower_val if prediction < cut_val else upper_val  \n","cb301e53":"df_sub = pd.read_csv(\"..\/input\/ensemble-of-public-submissions\/submission.csv\")","809e10f0":"sum(df_sub['pressure'] < PRESSURE_MIN)","4a74f590":"np.searchsorted(sorted_pressures, df_sub['pressure'].min())","d77c5894":"sum(df_sub['pressure'] > PRESSURE_MAX)","27217f26":"sum(df_sub['pressure'] > PRESSURE_MAX) \/ len(df_sub)","9a3460c0":"PRESSURE_MAX","f4f8f7fd":"df_sub['pressure'].max() ","8b5f0cd7":"df_sub['pressure'].max() - PRESSURE_MAX","c42c2b15":" (df_sub['pressure'].max() - PRESSURE_MAX) \/ PRESSURE_STEP","d88d0211":"df_sub","77af3b3b":"%%time\ndf_sub[\"pressure\"] = df_sub[\"pressure\"].apply(find_nearest_prob)","b76cef44":"df_sub.to_csv(\"submission.csv\", index=False)\nsubmission_prob = df_sub.copy()","71cc377d":"df_sub","c3779fa3":"y_old = submission_round_LB154['pressure']\ny_new = submission_prob['pressure']","775493f7":"(y_new - y_old).describe().round(3)","8ccad61c":"sum((y_new > y_old))","6a1769b6":"sum((y_new > y_old))\/len(df_sub)","a16d9b50":"sum((y_new < y_old))\n","d2c67c61":"sum((y_new < y_old))\/len(df_sub)","2311af47":"(0.0112 + 0.0108) * 0.0703 ","0fe51eac":"(sum(y_new > y_old) - sum(y_new < y_old))\/len(df_sub)","1c228867":"0.0004033300198807157 * PRESSURE_STEP","27619d66":"sub_1 = pd.read_csv('..\/input\/finetune-of-tensorflow-bidirectional-lstm\/submission.csv')\nsub_2 = pd.read_csv('..\/input\/finetune-of-tensorflow-bidirectional-lstm\/submission.csv')","3e93f046":"%%time\nsub_1[\"pressure\"] = sub_1[\"pressure\"].apply(find_nearest)\nsub_1.to_csv(\"submission_LB157_round_LB155.csv\", index=False)","552f130e":"%%time\nsub_2[\"pressure\"] = sub_2[\"pressure\"].apply(find_nearest_prob)\nsub_2.to_csv(\"submission_LB157_prob_LB155.csv\", index=False)","871ebeb7":"y_old = sub_1['pressure']\ny_new = sub_2['pressure']","15fae650":"sum((y_new > y_old))\/len(sub_1)","de0f0915":"sum((y_new < y_old))\/len(sub_1)","f22b49e7":"# Motivation\nOut of 950 discrete pressure values, only 12 values have their neighbors with the same counts! \n\n*Relative freq are not equal!*\n\nInspired by recent wonderful dummy rounding strategy, I explored a probability weighted rounding idea, which is illustrated by a simple scenario:\n\n*  Assume the step width is 1 and a prediction is 0.3 and we need to round to either 0 or 1. A dummy will round to *0*, assuming equal probability weights.\n*  But if the actual probablities were not equal, say, 0.01 at 0 and 0.04 at 1. Then the relative proportion within the step is 0.01\/(0.01 + 0.04) = 20%. Thus the prob adjusted cut point becomes relative proportion x step = 20% x 1 = 0.2. This new cut point will round the prediction 0.3 to *1*, instead of 0.","2550aacb":"Signs of differences between two methods were very similar (1.12% vs 1.08%). Just flip the sign by chance?\nSo the new prob rounding prediction has changed 2.2% of pressures in the submission data, compared to non-prob rounding.\n\nBut it is hard to believe that prob rounding reduce mae for all these 2.2%. \nIf that happens, **at the best**, the new rounding may further improve mae by 2.2% * PRESSURE_STEP = **0.0015**.\n\nThis is not observed. \nThe simple non-prob rounding methond has improved the score by 0.002. There might be very little room for prob rounding to further improve: may cut score only by 0.0000284?","84d2b152":"## Reference\n1.  https:\/\/www.kaggle.com\/snnclsr\/a-dummy-approach-to-improve-your-score-postprocess\n1.  https:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm\n1.  https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153\/\n","97706df0":"## Results\nAfter applying to two public submissions, LB 1.56 and 1.57, both reduced the LB scores by **0.002**, which seemed very similar to the dummy method, or invisible from LB with only the 3 decimals.\n\nTwo methods actually produced 2.2% different predictions of the test data. So the probability weighted rounding idea may have the potential to *further* improve up to 2.2% $*$ 0.0703 = **0.00155** at the best scenario or just change on the 5th decimal point, say **0.00003** at the worst scenarario.","94e23806":"# What we know","b96affca":"# Another public LB 0.157, cut 0.002","2a95ef88":"## Prob rounding function","08cc0d7f":"## Construct look up table: pressure_freq","d75fc0cb":"Max pressure in the ensemble submission can be larger than PRESSURE_MAX, but within the measurement error range: PRESSURE_STEP, ~ 0.0703.\n\nOver-estimation or overfitting seem not a problem?","e51779f5":"There are not jump 2 or more steps among 950 distinct pressures.","2ef6c327":"## Out of 950 discrete pressure values, only 12 values have their neighbor with the same counts!\n\n*  So relative frequencies are dominately not equal!","0e4dc591":"# What is proposed","bc4a2698":"# Compare","c7704214":"Why does prob rounding not improve further here?\n*  The presure relative frequency distribution is clearly single mode.\n* The relative frequencies from two neighors are just too local and have not borrowed any strength from the whole spectrum. Some smoothed distribution may improve.\n\nWill prob rounding work better for other competitions? \nMaybe, if the relative freqency distribution is not sinlge mode, but has multiple modes or complicated patterns.\n\nPlease leave your comments if any or you also thought about the similar idea but see little improvement. Thanks!","14cba312":"## Data re-loading and overview","10257f7d":"## Discussion\n","638e8a17":"## Apply probability rounding"}}