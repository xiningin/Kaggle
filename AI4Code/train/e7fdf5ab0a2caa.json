{"cell_type":{"0b13f8fe":"code","fda7f716":"code","ec53b3db":"code","c60f4f30":"code","1ebc9283":"code","7fc7846d":"code","df00ffb9":"code","fcccc650":"code","999560cd":"code","e6b16ba8":"code","b0a2f169":"code","95896154":"code","9fefaa6e":"code","232a5a09":"code","1000397d":"code","f3bcf7de":"code","8f85f57e":"code","92323692":"code","dd525412":"code","a2495f68":"code","6a0367f4":"code","7f2c1923":"code","f08b9cbb":"code","56a25025":"code","64a42086":"code","89679ef0":"code","ad759ccd":"code","16c65caa":"code","49c549c9":"code","43d087d6":"code","811f4292":"code","38e7b2d8":"markdown","26dddf18":"markdown","669925fb":"markdown","8e0e239f":"markdown","758dd3ca":"markdown","5a4594dd":"markdown","d91b8282":"markdown","11d78ece":"markdown","2667068c":"markdown","08e7745c":"markdown","a2639b42":"markdown","8c2bb9bd":"markdown"},"source":{"0b13f8fe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns","fda7f716":"import tensorflow as tf\ntf.__version__","ec53b3db":"data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\")","c60f4f30":"data.info()","1ebc9283":"data.head()","7fc7846d":"data.drop('id', axis = 1, inplace=True)","df00ffb9":"data.isna().sum()","fcccc650":"sns.countplot(data['target'], palette=\"bwr\")\nplt.show()\ndata['target'].value_counts(normalize=True);","999560cd":"X = data.drop('target', axis=1)\ny = data['target'].to_numpy()\n\nX.shape, y.shape","e6b16ba8":"X[:5]","b0a2f169":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","95896154":"X[:1]","9fefaa6e":"from sklearn.model_selection import train_test_split\n\ntf.random.set_seed(42)\n\nX_train_vaild, X_test, y_train_vaild, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n\nX_train_vaild.shape, X_test.shape, y_train_vaild.shape, y_test.shape","232a5a09":"tf.random.set_seed(42)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_vaild, y_train_vaild, test_size=0.25, random_state=42)\n\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","1000397d":"# let's build a model to find patterns in it\n\n# Set random seed\ntf.random.set_seed(42)\n\n# 1. Create a model\nmodel_1 = tf.keras.Sequential([\n           tf.keras.layers.Dense(50, input_dim=100, activation='relu'),\n           tf.keras.layers.Dense(15, activation='relu'),\n           tf.keras.layers.Dense(8, activation='relu'),         \n           tf.keras.layers.Dense(2, activation='softmax')\n])\n\n# 2. Comile the model\nmodel_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                 metrics=['accuracy'])\n\n# 3. Fit the model\nhistory = model_1.fit(X_train, \n                      tf.one_hot(y_train, depth=2), \n                      epochs=20,\n                      verbose = 1,\n                      validation_data=(X_valid, tf.one_hot(y_valid, depth=2)))","f3bcf7de":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.0, 1.0])\nplt.legend(loc='lower right');","8f85f57e":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0.0, 1])\nplt.legend(loc='upper right');","92323692":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_matrix(y_true=y_test, \n                 y_pred=model_1.predict(X_test).argmax(axis=1)), annot=True,\n                 fmt=\"d\");","dd525412":"model_1.evaluate(X_test, tf.one_hot(y_test, depth=2))[1] * 100","a2495f68":"model_1.summary()","6a0367f4":"# Let's check out a way of viewing our deep learning models\nfrom tensorflow.keras.utils import plot_model\n\n# See the inputs and outputs of each layer\nplot_model(model_1, show_shapes=True)","7f2c1923":"df_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\")","f08b9cbb":"test_passengerIds = df_test['id'].values\ndf_test.drop('id', axis = 1, inplace=True)\ndf_test.head()","56a25025":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_new = sc.fit_transform(df_test)","64a42086":"X_new[:1]","89679ef0":"y_pred = model_1.predict(X_new).argmax(axis = 1)","ad759ccd":"y_pred[:5]","16c65caa":"y_pred.shape","49c549c9":"test_passengerIds.shape","43d087d6":"output = pd.DataFrame({'id':test_passengerIds, 'target': y_pred})\noutput.to_csv('submission.csv', index=False)","811f4292":"output","38e7b2d8":"# Building and Training our model","26dddf18":"# Splitting traning set","669925fb":"as we can see data here is not balanced!","8e0e239f":"# Check if there is null values","758dd3ca":"# checking wheather if the target data is balanced or not.","5a4594dd":"# Test Data","d91b8282":"# Quick Look at the Data\nLet\u2019s take a look at the top five rows:","11d78ece":"# The Dataset\nFor this notebook we will use Tabular Playground Series - Mar 2021.\n\nLet's define the path to the dataset:","2667068c":"# Feature scaling","08e7745c":"# Split Data","a2639b42":"# Import Packages\nLets load all the needed packages for this notebook:","8c2bb9bd":"# Testing the model"}}