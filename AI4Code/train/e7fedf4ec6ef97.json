{"cell_type":{"1ac31028":"code","8fd0e299":"code","569ba15c":"code","fde2f7ab":"code","512349be":"code","845220f8":"code","54582465":"code","0cec3de7":"code","fa26609f":"code","d8103fcd":"code","4a117658":"code","65ec30f2":"code","73b63d13":"code","f60d5c15":"code","286f135e":"code","97ab8909":"code","3ac74771":"code","04d54e1d":"code","7395e816":"code","629b289e":"code","03ff0039":"code","427e3d35":"code","b563259c":"code","c90804d5":"code","8b67f104":"code","73c08bb3":"code","b232b54c":"code","fd4bb827":"markdown","e1aea1db":"markdown","5b3b4e09":"markdown","c2af2201":"markdown","54150847":"markdown","b4c62ad8":"markdown","9e9fd0cb":"markdown"},"source":{"1ac31028":"# imports \nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# visualization\nimport matplotlib.pyplot as plt\n\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# model selection\nfrom sklearn.model_selection import train_test_split\n\n# accuracy score\nfrom sklearn.metrics import accuracy_score\n\n# NLP\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Model\nfrom sklearn.naive_bayes import MultinomialNB\n","8fd0e299":"# load the data\ndata = pd.read_csv('..\/input\/nlp-starter-test\/social_media_clean_text.csv')","569ba15c":"# print head of the dataframe\ndata.head()","fde2f7ab":"# print data info\ndata.info()","512349be":"stopwords = set(STOPWORDS)","845220f8":"def wordcloud_plot(name_of_feature):\n    plt.figure(figsize=(10, 10))\n    wordcloud = WordCloud(\n                              background_color='black',\n                              stopwords=stopwords,\n                              max_words=200,\n                              max_font_size=40, \n                              random_state=42\n                             ).generate(str(name_of_feature))\n    fig = plt.figure(1)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","54582465":"relevant_text = data[data['choose_one']=='Relevant']['text']\nirrelevant_text = data[data['choose_one']=='Not Relevant']['text']","0cec3de7":"wordcloud_plot(relevant_text)","fa26609f":"wordcloud_plot(irrelevant_text)","d8103fcd":"# Sample dataset\nX = data['text']\ny = data['choose_one']","4a117658":"# Intialize CountVectorizer\ncv = CountVectorizer()","65ec30f2":"# fit and transform CountVectorizer \nX1 = cv.fit_transform(X)","73b63d13":"# Create train and test split with test size 33 %\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.33, random_state=53)","f60d5c15":"# Intiate Naive_bayes\nclf = MultinomialNB()","286f135e":"clf.fit(X_train, y_train)","97ab8909":"y_pred_clf = clf.predict(X_test)","3ac74771":"clf_score = accuracy_score(y_pred_clf, y_test)\nprint('Accuracy with CountVectorizer : ', clf_score*100)","04d54e1d":"# initiate TfidfVEctorizer\ntv = TfidfVectorizer()","7395e816":"# fit and tranform the tfidfVectorizer\nX2 = tv.fit_transform(X)","629b289e":"# create train and test split \nX_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.33, random_state=53)","03ff0039":"# initialize Naive Bayes\nclf = MultinomialNB()","427e3d35":"clf.fit(X_train, y_train)","b563259c":"y_pred_clf = clf.predict(X_test)","c90804d5":"score = accuracy_score(y_pred_clf, y_test)\nprint('Accuracy with TfidfVectorizer : ', score*100)","8b67f104":"text = data['text'].iloc[0]","73c08bb3":"# create Tfidf transform\ntemp = tv.transform([text])","b232b54c":"clf.predict(temp)[0]","fd4bb827":"## Load Dataset","e1aea1db":"2. TfidfVectorizer","5b3b4e09":"## Imports","c2af2201":"## Predict the input","54150847":"## Plot WordCloud for each category","b4c62ad8":"## Modeling\n   1. CountVEctorizer","9e9fd0cb":"Wow, We got the currect prediction."}}