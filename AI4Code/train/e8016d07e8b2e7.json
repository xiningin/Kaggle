{"cell_type":{"24125575":"code","0f673c33":"code","9f74dcef":"code","1f2fdbec":"code","2e4c573f":"code","fe4c48e2":"code","f0ff3828":"code","44991fb5":"code","ba74eff7":"code","2dab5e5d":"code","3907c4cf":"code","f1aa1773":"code","8611b37f":"markdown","b61f4d95":"markdown","f21577ec":"markdown","4117bc4d":"markdown"},"source":{"24125575":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f673c33":"!pip install scorecardpy\n\nimport scorecardpy as sc","9f74dcef":"path = '\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json'\ndf = pd.read_json(path, lines=True)\ndf.head()","1f2fdbec":"df.shape","2e4c573f":"df.columns","fe4c48e2":"df.shape, \\\ndf.groupby(by=['authors', 'date']).size().shape, \\\ndf.groupby(by=['authors', 'date', 'category']).size().shape, \\\ndf.groupby(by=['authors', 'date', 'category', 'headline', 'link','headline']).size().shape ","f0ff3828":"df['init'] = df['headline'].apply(lambda _: str(_)[0].lower() if len(_) > 0 else '_')\ndf['init'].value_counts().tail(20)                                  ","44991fb5":"import random\nrandom.randint(0,1)\naleatorio_0_1 = [0 if random.random() + 0.1 < 0.5 else round(random.uniform(0, 1), 0) for _ in range(df.shape[0])]\nrandom.shuffle(aleatorio_0_1)\n\ndf['target_buena_aceptaci\u00f3n'] = aleatorio_0_1\ndf['target_buena_aceptaci\u00f3n'] = df[['headline', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 0 if _[0].lower().startswith('p') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['headline', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 0 if _[0].lower().startswith('b') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['headline', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 1 if _[0].lower().startswith('t') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['headline', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 1 if _[0].lower().startswith('w') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['category', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 1 if _[0].lower().startswith('s') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['category', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 0 if _[0].lower().startswith('q') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['category', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 0 if _[0].lower().startswith('g') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['category', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 1 if _[0].lower().startswith('b') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'] = df[['init', 'target_buena_aceptaci\u00f3n']].apply(lambda _: 0 if _[0].lower().startswith('z') else _[1], axis=1)\ndf['target_buena_aceptaci\u00f3n'].value_counts(dropna=False, normalize=True)\ndf['target_buena_aceptaci\u00f3n'].value_counts(dropna=False, normalize=True)","ba74eff7":"from slugify import slugify\n\ndf['authors'] = df['authors'].apply(slugify)\ndf['category'] = df['category'].apply(slugify)\ndf.head()","2dab5e5d":"df.tail()","3907c4cf":"def _encoder_cat_prob(data, col_eval, col_target='target', val_target=1, sufijo='_encoder_prob', drop=False,\n                     letal_group=True, window_letal=0.1, sufijo_letal='_letal_group'):\n    \"\"\"\n    APLICADO A CATEGORIAS MUY POBLADAS, como departamento, subsector economico del cliente, cargos de trabajo \n    \n    return: \n           Obtendremos un valor num\u00e9rico, donde a mas alto mas porbabilidad de caer en el valor del target indicado,\n           indirectamnete este resultado compirme categorias que tengan igual probabilidad hacia el target, lo cual \n           podremos contrastar dejando el valor drop=False \n    \"\"\"\n    \n    dicc_map_e = {} \n    new_col = col_eval + sufijo\n    letal_col = col_eval + sufijo_letal\n    col_woe = col_eval + '_encoder_woe'\n    letal_woe = '_letal_woe'\n    \n    for val in data[col_eval].unique():\n        dicc_map_e[val] = round(\n            data[data[col_eval] == val][col_target].value_counts(normalize=True).to_dict().get(val_target, 0), \n            1\n        )\n        \n    data[new_col] = data[col_eval].map(dicc_map_e)\n    \n    #print(data[col_eval].value_counts())\n    if drop:\n        del data[col_eval]\n        \n    print(\"\/\"*100)\n    print(data[new_col].value_counts())\n    \n    #### COMPARANDO CON WOE \/ IV\n    cortes = sc.woebin(\n        data[[col_eval, col_target]],\n        y=col_target\n    )\n    dic_maxprob_corte = {}\n    for k, v in cortes.items():\n        #print(k, type(v))\n        #print(\">\"*20, ' ', k.upper())\n        v = v.sort_values(by=['badprob'], ascending=False).reset_index(drop=True).reset_index().rename(columns={'index': 'top'})\n        v['top'] = v['top'] + 1\n        #display(v[['top', 'badprob']])\n        lista_top_bins = [_.lower().replace('%', '').replace(',',' ') for _ in list(v['bin'])]\n        #print(lista_top_bins)\n        dic_maxprob_corte[k] = v[v['bin'] != 'missing']['badprob'].max()\n    \n    def search_bin_woe(val):\n        for el in lista_top_bins:\n            if val.lower() in el:\n                return lista_top_bins.index(el) + 1\n        return 0\n    \n    data[col_woe] = data[col_eval].apply(search_bin_woe)\n    print(\"\/\"*100)\n    print(data[col_woe].value_counts())\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n    # CONTRASTE DEL COMPACTADO DE CATEGORIAS\n    if not drop:\n        print(\"\/\"*100)\n        print(\n            sns.heatmap(\n                pd.crosstab(data[col_eval], data[new_col], values=data[col_target].apply(lambda _: 1 if _ == val_target else 0), aggfunc=np.mean), \n                annot=True, center=0.4, cmap=\"viridis\", ax=axes[0]) \n        )\n        \n    print(\"\/\"*100)\n    # CREACION DE  GRUPO LETAL\n    print(\"Creacion de grupo letal\")\n    if letal_group:\n        maximo_prob = data[new_col].max()\n        print(\".... maxima probabilidad encontrada hacia el target\", maximo_prob)\n        data[letal_col] = data[new_col].apply(lambda _: maximo_prob if _ >= maximo_prob - window_letal else 0)\n        \n        maximo_prob_woe = data[col_woe].max()\n        data[letal_woe] = data[col_woe].apply(lambda _: maximo_prob_woe if _ >= maximo_prob_woe else 0)\n        \n    \n    # MAP DE CALOR DE CORRELACIONAES LIENALES CON LAS TRES COLUMNAS OBTENDAS INCLUIDO EL TARGET\n    \n        print(\n            sns.heatmap(\n                data[[letal_col, new_col, letal_woe, col_woe, col_target]].corr(method='pearson'), \n                annot=True, center=0,cmap='Spectral', ax=axes[1], fmt='g') \n        )\n    \n    return data","f1aa1773":"df = _encoder_cat_prob(df, 'category', col_target='target_buena_aceptaci\u00f3n', window_letal=0.1)","8611b37f":"### Crear\u00e9 un target fictici\u00f3 (tuvo buenas aceptaci\u00f3n)","b61f4d95":"### Limpiar\u00e9 las variables categoricas a usar (authors y category)","f21577ec":"#### Creando cortes acorde a la mayor probabilidad del target","4117bc4d":"### EL METODO CREADO MUESTRA MAYOR CORRELACION DIRECTA CON EL TARGET FICTICIO, TANTO EN EL GRUPO CATEGORICO ORDINAL COMO EN EL LETAL =)"}}