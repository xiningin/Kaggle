{"cell_type":{"dece5233":"code","970738e7":"code","0fbdd7ff":"code","88956cca":"code","cfba86c1":"code","236a8a86":"code","c8d588ca":"code","cc41bed7":"code","aee3671a":"code","c9da81a0":"code","7424d65c":"code","0004a488":"code","af3b00d9":"code","20f7f3eb":"code","5f586774":"code","9eabd312":"code","16cbc7bc":"code","5d59280d":"code","e086e88c":"code","13fb8766":"code","bfc6e94f":"markdown","9ddf0eb6":"markdown","e88b0c0a":"markdown","be69d15d":"markdown","b3033e53":"markdown","b005e70b":"markdown","3e9e3947":"markdown"},"source":{"dece5233":"#import libraries\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport json\nfrom pprint import pprint\nimport random\nimport string\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim.corpora import Dictionary\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_colwidth', 100)","970738e7":"documents_dir='..\/input\/CORD-19-research-challenge\/document_parses\/pdf_json\/'\nfilenames = os.listdir(documents_dir)\nprint(\"Number of documents :\", len(filenames))","0fbdd7ff":"random.shuffle(filenames)","88956cca":"file = json.load(open('..\/input\/CORD-19-research-challenge\/document_parses\/pdf_json\/0000028b5cc154f68b8a269f6578f21e31f62977.json', 'rb'))","cfba86c1":"pprint(file[\"metadata\"][\"title\"])","236a8a86":"def clean(text):\n    text = str(text).lower()\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(r'\\(.*?\\)', '', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    text = re.sub(r\"\\w+\u2026|\u2026\", \"\", text)  # Remove ellipsis (and last word)\n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n    return text","c8d588ca":"def remove_stopwords_and_tokenize(text):\n    my_stopwords = set(stopwords.words(\"english\"))\n    tokens = word_tokenize(text)  # tokenize \n    tokens = [t for t in tokens if not t in my_stopwords]  # Remove stopwords\n    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n    return tokens","cc41bed7":"def parse_body_text(body_text):\n    body =\"\"\n    for item in body_text:\n        body += item[\"section\"]\n        body += \"\\n\\n\"\n        body += item[\"text\"]\n        body += \"\\n\\n\"\n    body=clean(body)\n    tokens=remove_stopwords_and_tokenize(body)\n    return body,tokens","aee3671a":"all_text = []\nall_tokens=[]\nall_titles=[]\nfor i,filename in enumerate(filenames[:1000]):\n    filepath = documents_dir + filename\n    file = json.load(open(filepath, 'rb'))\n    text,tokens=parse_body_text(file[\"body_text\"])\n    all_text.append(text)\n    all_tokens.append(tokens)\n    all_titles.append(file[\"metadata\"][\"title\"])\n","c9da81a0":"data=pd.DataFrame()\ndata['text']=all_text\ndata['tokens']=all_tokens\ndata['doc_id']=filenames[:1000]\ndata['title']=all_titles\ndel all_text,all_tokens,all_titles","7424d65c":"data.head(2)","0004a488":"# Create a dictionary representation of the documents.\ndictionary = Dictionary(data[\"tokens\"])\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.5)","af3b00d9":"# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in data[\"tokens\"]]","20f7f3eb":"from gensim.models import LdaModel\n\n# Build LDA model\nlda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, random_state=100,\n                chunksize=200, passes=100)","5f586774":"lda_model.print_topics()[:5]","9eabd312":"lda_model[corpus][0]","16cbc7bc":"def get_document_topic_table(lda_model, corpus, texts=data):\n    # Init output\n    document_topic_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(lda_model[corpus]):           \n        row = sorted(row_list, key=lambda x: (x[1]), reverse=True)\n        topic_num=row[0][0]\n        prop_topic=row[0][1]\n        wp = lda_model.show_topic(topic_num)\n        topic_keywords = \", \".join([word for word, prop in wp])\n        document_topic_df.at[i,'best_topic']=topic_num\n        document_topic_df.at[i,'prop_topic']=prop_topic\n        document_topic_df.at[i,'topic_keywords']=topic_keywords\n        document_topic_df.at[i,'document_num']=i\n    return document_topic_df\n    \ndocument_topic_df = get_document_topic_table(lda_model=lda_model, corpus=corpus, texts=data[\"tokens\"])","5d59280d":"document_topic_df.head(2)","e086e88c":"def get_topic_id(doc_id):\n    for i,row in data.iterrows():\n        if(row[\"doc_id\"]==doc_id):\n            #print(document_topic_df[\"best_topic\"][i])\n            return document_topic_df[\"best_topic\"][i]\n    return -1\n\ndef get_matching_topics_docs(topic_id):\n    matched_topics=[]\n    for i,row in document_topic_df.iterrows():\n        \n        if(row[\"best_topic\"]==topic_id):\n            topic_prop_doc=(topic_id,row[\"prop_topic\"],i)\n            matched_topics.append(topic_prop_doc)\n        \n    return matched_topics\n    \ndef get_top_k_topics(matched_topics,k):\n    top_k=sorted(matched_topics, key=lambda x: [x[1]], reverse=True)\n    print(top_k[:k])\n    k_topics_df=pd.DataFrame(columns=[\"doc_id\",\"topic_id\",\"topic_prop\",\"title\"])\n    i=0\n    for topic_id,topic_prop,doc_num in top_k[:k]:\n        k_topics_df.at[i,'doc_id']=data[\"doc_id\"][doc_num]\n        k_topics_df.at[i,'topic_id']=topic_id\n        k_topics_df.at[i,'topic_prop']=topic_prop\n        k_topics_df.at[i,'title']=data[\"title\"][doc_num]\n        i+=1\n    return k_topics_df\n\ndef recommend_k_topics(doc_id,k):\n    topic_id=get_topic_id(doc_id)\n    if(topic_id!=-1):\n        matched_topics=get_matching_topics_docs(topic_id) \n        return get_top_k_topics(matched_topics,k)\n    \n    \nk_topics_df=recommend_k_topics('328401206bf2e3657e352ad5c5a2e566cc09736d.json',5)","13fb8766":"k_topics_df","bfc6e94f":"## Recommend k topics","9ddf0eb6":"# NOTEBOOK GOAL \ud83c\udfaf\n\nIn this notebook , we will find related articles by using Topic modelling. Here I am using Latent Dirichlet Allocation(LDA).\nLDA is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n","e88b0c0a":"# ABOUT DATASET \ud83d\udcc1\n\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19).CORD-19 is a resource of over 400,000 scholarly articles, including over 150,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. ","be69d15d":"# Step 4: Results","b3033e53":"# Step 1 : Data Cleaning","b005e70b":"# Step 2 : Apply LDA model","3e9e3947":"## Document - Topic Table"}}