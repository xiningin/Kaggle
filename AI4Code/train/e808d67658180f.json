{"cell_type":{"10fb7cd3":"code","86cfde11":"code","54e89ea6":"code","ec391150":"code","dd67c136":"code","86173bbc":"code","449c4051":"code","e1df2111":"code","6ef0530e":"code","3a162069":"code","0f3bcde7":"code","5394d6fd":"code","4c9e2f07":"code","6e7e349d":"code","de55a420":"code","5400d611":"code","2ab2372e":"code","4635a4ff":"code","abd33d60":"code","7206fdd3":"markdown","f1e6297d":"markdown","f3fed728":"markdown","d48351f5":"markdown","3f06e6b9":"markdown","e8fe009d":"markdown","745b08f4":"markdown","5b07531b":"markdown","5c2652ea":"markdown","8ee2ff4a":"markdown","f6e86ea3":"markdown","c9e691c7":"markdown"},"source":{"10fb7cd3":"!pip install -U sentence-transformers\n\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Pre-Processing\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\n\n# Modeling\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding,Dropout, Bidirectional, GlobalMaxPool1D,Conv1D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom transformers import DistilBertTokenizerFast\nfrom sentence_transformers import SentenceTransformer\n\n# Conclusion\nfrom tabulate import tabulate\nfrom IPython.display import HTML, display","86cfde11":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","54e89ea6":"df = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding=\"latin-1\")\ndf.head(3)","ec391150":"sms = pd.DataFrame()\nsms['message'] = df['v2']\nsms['class'] = df['v1']\nsms.head(3)","dd67c136":"sms.shape","86173bbc":"stopwords_en = stopwords.words(\"english\")\nnlp = spacy.load('en_core_web_sm') #en_core_web_lg\ndef en_preprocess(text):\n    lowered = text.lower()\n    tokenized = []\n    for i in nlp(lowered):\n        word = i.lemma_\n        pos = i.pos_\n        if i.is_alpha and pos not in ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE'] and i.text not in stopwords_en:\n            tokenized.append(word)\n    preprocessed = \" \".join(tokenized)\n    return preprocessed","449c4051":"sms['message_corpus'] = sms['message'].apply(lambda x: en_preprocess(x))\nsms.head(3)","e1df2111":"cv = TfidfVectorizer(max_features=3000, use_idf=True)\nx = cv.fit_transform(sms['message_corpus']).toarray()\ny = pd.get_dummies(sms[\"class\"],drop_first=True)\ny = np.array(y)","6ef0530e":"%%time\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_test = lgb.Dataset(x_test, y_test, reference=lgb_train)\n\nparams = {'task': 'train',             \n          'boosting_type': 'gbdt',    \n          'objective': 'binary',   \n          'metric': {'binary_logloss'},  \n          'num_class': 1,              \n          'learning_rate': 0.1,        \n          'num_leaves': 30,            \n          'min_data_in_leaf': 10,     \n          'verbose' : -1,\n          'num_iteration': 100}      \nlgb_results = {}                                   \nmodel = lgb.train(params=params,                   \n                  train_set=lgb_train,             \n                  valid_sets=[lgb_train, lgb_test], \n                  valid_names=['Train', 'Test'],    \n                  num_boost_round=100,             \n                  early_stopping_rounds=10,        \n                  verbose_eval=False,\n                  evals_result=lgb_results) \n\ny_pred = model.predict(x_test, num_iteration=model.best_iteration).round(0)\nscore_model1 = accuracy_score(y_pred,y_test)\nprint(score_model1)","3a162069":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntrain_encodings = tokenizer(sms.message.tolist(),add_special_tokens = True, truncation = True,  padding = True, return_tensors=\"tf\")","0f3bcde7":"x = np.array(train_encodings['input_ids'])\ny = pd.get_dummies(sms[\"class\"])\ny = np.array(y)\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=0)\nx_train,x_valid,y_train,y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=0)","5394d6fd":"model = Sequential()\nmodel.add(Embedding(tokenizer.vocab_size, 100, input_length=300))\nmodel.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\nmodel.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(50, activation=\"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2,activation = 'softmax'))\nmodel.compile(loss = 'binary_crossentropy',optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","4c9e2f07":"%%time\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train, y_train, batch_size=16, epochs=3, validation_data=(x_valid, y_valid))","6e7e349d":"score_model2 = model.evaluate(x_test,y_test)[1]*100\nprint(score_model2)","de55a420":"model_sentenc = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cpu')","5400d611":"%%time\nsent_embeddings = model_sentenc.encode(sms.message.tolist())","2ab2372e":"x = np.array(sent_embeddings)\ny = pd.get_dummies(sms[\"class\"],drop_first=True)\ny = np.array(y)\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=0)","4635a4ff":"%%time\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_test = lgb.Dataset(x_test, y_test, reference=lgb_train)\n\nparams = {'task': 'train',             \n          'boosting_type': 'gbdt',    \n          'objective': 'binary',   \n          'metric': {'binary_logloss'},  \n          'num_class': 1,              \n          'learning_rate': 0.1,        \n          'num_leaves': 10,            \n          'min_data_in_leaf': 30,     \n          'verbose' : -1,\n          'num_iteration': 100}      \nlgb_results = {}                                   \nmodel = lgb.train(params=params,                   \n                  train_set=lgb_train,             \n                  valid_sets=[lgb_train, lgb_test], \n                  valid_names=['Train', 'Test'],    \n                  num_boost_round=100,             \n                  early_stopping_rounds=20,        \n                  verbose_eval=False,\n                  evals_result=lgb_results)\n\ny_pred = model.predict(x_test, num_iteration=model.best_iteration).round(0)\nscore_model3 = accuracy_score(y_pred,y_test)\nprint(score_model3)","abd33d60":"display(\n    HTML(\n        '<h3>Evaluation - Accuracy<\/h3>' + \n        tabulate(\n            [['Score', score_model1, score_model2, score_model3]],\n            [\"\", \"Model1 : TF-IDF with LightGBM\", \"Model2 : BertTokenizer with Bi-LSTM\", 'Model3 : Sentence Transformer with LightGBM'], tablefmt=\"html\"\n        )\n    )\n)","7206fdd3":"# 6. Conclusion\nModel1 is simpler method, but accuracy may be acceptable in practise.  \nThe others are better in accuracy, but are time-taking and not easy to understand the logic.","f1e6297d":"# 4. Pre-processing","f3fed728":"<h2 style=\"text-align:center;font-size:200%;;\">Spam Detection with 3 Models<\/h2>","d48351f5":"# 7. References\n* **spaCy POS mapping**  \nhttps:\/\/qiita.com\/kei_0324\/items\/400f639b2f185b39a0cf  \n* **Sentence Transformer Models**  \nhttps:\/\/huggingface.co\/sentence-transformershttps:\/\/huggingface.co\/sentence-transformers  \n* **Introduction to Transformer**  \nhttps:\/\/note.com\/npaka\/n\/n5bb043191cc9https:\/\/note.com\/npaka\/n\/n5bb043191cc9","3f06e6b9":"## Table of Contents\n>1. [Overview](#1.-Overview)  \n>1. [Import libraries](#2.-Import-libraries)\n>1. [Load the dataset](#3.-Load-the-dataset)\n>1. [Pre-processing](#4.-Pre-processing)\n>1. [Modeling](#5.-Modeling)\n>    * [Model1 : TF-IDF with LightGBM](#Model1-:-TF-IDF-with-LightGBM)\n>    * [Model2 : BertTokenizer with Bi-LSTM](#Model2-:-BertTokenizer-with-Bi-LSTM)\n>    * [Model3 : Sentence Transformer with LightGBM](#Model3-:-Sentence-Transformer-with-LightGBM)\n>1. [Conclusion](#6.-Conclusion)\n>1. [References](#7.-References)","e8fe009d":"## Model3 : Sentence Transformer with LightGBM","745b08f4":"## Model2 : BertTokenizer with Bi-LSTM","5b07531b":"# 2. Import libraries","5c2652ea":"# 3. Load the dataset","8ee2ff4a":"# 1. Overview\nOn this dataset, I built 3 models to detect spam e-mails.  \nOne of them is simpler model which has TF-IDF features. The others are Transformer-related models.\n* Model1 : TF-IDF with LightGBM\n* Model2 : BertTokenizer with Bi-LSTM\n* Model3 : Sentence Transformer with LightGBM","f6e86ea3":"## Model1 : TF-IDF with LightGBM","c9e691c7":"# 5. Modeling"}}