{"cell_type":{"f0a3759c":"code","d3fec38c":"code","33ad25cb":"code","27afd79d":"code","05b6a190":"code","4f4af0c2":"code","1be91113":"code","40dff9cb":"code","8c4765d1":"code","04510634":"code","7c394208":"code","a4067d82":"code","a1975129":"code","fbdb22d6":"code","23100fa1":"code","4c8089ac":"code","6fc10b66":"code","97fb42a3":"code","43b698ae":"code","06d53803":"code","954e8d3a":"code","0e18f797":"code","3714dabb":"code","1f8f3ff3":"code","d052eda8":"code","d1788961":"code","3be1418b":"code","562eef44":"code","5ac54fb9":"code","6c80c3e5":"code","40b4241c":"code","9a3ce26b":"code","c4658ee6":"code","6973acaa":"code","d42b36e5":"code","792677d2":"code","665cc1b6":"code","83c6f121":"code","77c679e1":"code","5e48f086":"code","9cbbfbc9":"code","84effad1":"code","8d4416a8":"code","18fb3069":"code","f7d23f2a":"code","25bcae46":"code","76dd17b1":"code","7f4254a0":"code","c0ca5ab9":"code","8de95e4c":"code","4cb92b8d":"code","b8b66b05":"code","56a62fa6":"code","3fa68c45":"code","713d0fac":"code","f1487ae9":"code","21152a40":"code","38daab4c":"code","c406b303":"code","8640b09d":"code","606029bd":"code","0946ad19":"code","1e997800":"code","e02537b0":"code","0afcbf78":"code","fa1cc680":"code","d6ac8df7":"code","0beb00ec":"code","d9d58cfe":"code","ebf8fd77":"code","a2682b5c":"code","cbcf83c8":"code","e5032c68":"code","48d88f01":"code","68abb794":"code","4f264a91":"code","7d49211c":"code","e244be4b":"code","89273de9":"code","cbc340df":"code","97acb675":"markdown","c08d7698":"markdown","92b54d8c":"markdown","0f26a7d4":"markdown","8010292d":"markdown","d3b0d5e5":"markdown","3755addc":"markdown","3745db5a":"markdown","6d7b1120":"markdown","d55e1f4b":"markdown","dd634c5b":"markdown","9e0b810e":"markdown","3f28b5bf":"markdown","a82a830d":"markdown","ff3ec183":"markdown","c728df90":"markdown","fd64f863":"markdown","cc78d20c":"markdown","518a006f":"markdown","3561bfa0":"markdown","1678fdef":"markdown","00c8c2ab":"markdown","9261cbf2":"markdown","edf05182":"markdown","924dacc6":"markdown","18294cea":"markdown","cdd44358":"markdown","359eac2a":"markdown","f6a5581b":"markdown","75acfde9":"markdown","844f48e3":"markdown","8b2d3cb9":"markdown","9f915973":"markdown","5afc0e7d":"markdown","6ece60e5":"markdown","02671355":"markdown","f447e108":"markdown","f7f14ac9":"markdown","62fad542":"markdown","01157b9d":"markdown","ade7d6af":"markdown","988e71c1":"markdown","53971f8f":"markdown","13aa0f38":"markdown","3f720b88":"markdown","8d04467d":"markdown","d3f7005f":"markdown","5697024b":"markdown","f446db5f":"markdown","f4652cdd":"markdown"},"source":{"f0a3759c":"import csv\nimport os\n\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', None)\n\n# Plotting\n\nfrom matplotlib import pyplot\nimport seaborn as sns \nimport plotly\nimport plotly.offline as py\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport cufflinks as cf\n\npy.init_notebook_mode(connected=True)\ncf.go_offline()\n\n# Data Preprocessing, Models, Feature and Model Selection\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport lightgbm as lgb","d3fec38c":"# Load Necessary Datasets\n\napplication_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\napplication_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprevious_applications = pd.read_csv(\"..\/input\/home-credit-default-risk\/previous_application.csv\")\nbureau = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau.csv\")\n\n# Data Cleaning\n\n# Convert DAYS_BIRTH to age in years of each client (it's expressed in negative days)\n\napplication_train[\"DAYS_BIRTH\"] = application_train[\"DAYS_BIRTH\"]\/(-365)","33ad25cb":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    missing = pd.concat([total, percent], axis=1)\n    missing.rename(columns= {0:'Total', 1:'Percent'}, inplace = True)\n    return missing\n\ndef plot_iploty_stats(application_train, feature):\n    temp = application_train[feature].value_counts()\n    df1 = pd.DataFrame({feature: temp.index,'Number of contracts': temp.values})\n    \n    # Calculate the percentage of target=1 per category value\n    \n    cat_perc = application_train[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n    \n    trace = go.Bar(\n        x = temp.index,\n        y = temp \/ temp.sum()*100)\n    data = [trace]\n    layout = go.Layout(\n        title = 'Percentage of contracts according to '+feature,\n        xaxis=dict(\n            title='Values',\n            tickfont=dict(\n                size=14,\n                color='rgb(107, 107, 107)'\n            )\n        ),\n        yaxis=dict(\n            title='Percentage of contracts',\n            titlefont=dict(\n                size=16,\n                color='rgb(107, 107, 107)'\n            ),\n            tickfont=dict(\n                size=14,\n                color='rgb(107, 107, 107)'\n            )\n            \n        )\n    )\n    fig = go.Figure(data=data, layout=layout)\n    fig.update_yaxes(range=[0, 100])\n    \n    py.iplot(fig, filename='statistics')\n    \n    trace = go.Bar(\n        x = cat_perc[feature],\n        y = cat_perc.TARGET\n    )\n    data = [trace]\n    layout = go.Layout(\n        title = 'Percent of contracts with TARGET==1 according to '+feature,\n        xaxis=dict(\n            title='Values',\n            tickfont=dict(\n                size=14,\n                color='rgb(107, 107, 107)'\n            )\n        ),\n        yaxis=dict(\n            title='Percent of target with value 1',\n            titlefont=dict(\n                size=16,\n                color='rgb(107, 107, 107)'\n            ),\n            tickfont=dict(\n                size=14,\n                color='rgb(107, 107, 107)'\n            )\n        )\n    )\n    fig = go.Figure(data=data, layout=layout)\n    \n    py.iplot(fig, filename='schoolStateNames')\n    \ndef plot_repayed_perc(application_train, feature, round_feat=-1):\n    #percentage of the loans repayed or not according to the feature chosen in input \n    if round_feat > -1:\n        application_train[feature] = np.round(application_train[feature], round_feat)\n    temp = application_train[feature].value_counts()\n    \n    temp_y0 = []\n    temp_y1 = []\n    for val in temp.index:\n        temp_y1.append(np.sum(application_train[\"TARGET\"][application_train[feature]==val] == 1))\n        temp_y0.append(np.sum(application_train[\"TARGET\"][application_train[feature]==val] == 0))    \n    trace1 = go.Bar(\n        x = temp.index,\n        y = (temp_y1 \/ temp.sum()) * 100,\n        name='YES'\n    )\n    trace2 = go.Bar(\n        x = temp.index,\n        y = (temp_y0 \/ temp.sum()) * 100, \n        name='NO'\n    )\n\n    data = [trace1, trace2]\n    fig = go.Figure(data=data)\n    fig.update_layout(showlegend=True, title = 'Loan Defaults Decomposed by ' + feature + ' (Percentage)')\n\n    iplot(fig)\n    \ndef heatmap_coor_matrix(application_train, corr_pearson):\n    data = [go.Heatmap(\n        z= corr_pearson,\n        x=application_train.columns.values,\n        y=application_train.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        opacity = 1.0 )\n       ]\n    fig = go.Figure(data=data)\n    fig.update_layout(title = 'Pearson Correlation between features', \n                      xaxis = dict(ticks='', nticks=36),\n                      yaxis = dict(ticks='' ),\n                      width = 900, height = 900,\n                      margin=dict(l=240)) \n    py.iplot(fig, filename='coorelation_heatmap')","27afd79d":"application_train[\"DAYS_BIRTH\"].head()","05b6a190":"application_train['NAME_TYPE_SUITE'].unique()","4f4af0c2":"application_train['NAME_TYPE_SUITE'].isna().sum()","1be91113":"application_train['CODE_GENDER'].value_counts()","40dff9cb":"temp = application_train[\"TARGET\"].value_counts()\ntemp = (temp \/ temp.sum())*100\ntemp.iplot(kind='bar', labels='labels', values='values', colors ='green', title='Loan Repayed or Not')","8c4765d1":"temp = application_train[\"TARGET\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values})\n\ndf.iplot(kind='pie', labels='labels', values='values', title='Loan Repayed or not')","04510634":"plot_iploty_stats(application_train, 'OCCUPATION_TYPE')","7c394208":"plot_iploty_stats(application_train, 'CODE_GENDER')","a4067d82":"# Tabulated Default Rates Decomposed by Gender\n\nnp.round(pd.crosstab(application_train.CODE_GENDER, application_train.TARGET, margins=True, normalize=0), 3)","a1975129":"plot_iploty_stats(application_train, 'NAME_FAMILY_STATUS')","fbdb22d6":"plot_iploty_stats(application_train, \"ORGANIZATION_TYPE\")","23100fa1":"plot_repayed_perc(application_train, \"NAME_INCOME_TYPE\")","4c8089ac":"plot_iploty_stats(application_train, \"NAME_CONTRACT_TYPE\")","6fc10b66":"plot_repayed_perc(application_train, \"DAYS_BIRTH\", round_feat=0)","97fb42a3":"previous_applications['NAME_CASH_LOAN_PURPOSE'].value_counts()","43b698ae":"temp = previous_applications['NAME_CASH_LOAN_PURPOSE'].value_counts()\n\ntemp.iplot(kind='bar', color=\"blue\", \n           xTitle = 'Organization Name', yTitle = \"Count\", \n           title = 'Types of NAME_CASH_LOAN_PURPOSE in previous applications ')","06d53803":"missing_data(application_train)","954e8d3a":"missing_data(bureau)","0e18f797":"missing_data(previous_applications)","3714dabb":"# Spin out categorical variables into dummied 0\/1 indicator variables\n\ntrain_one_hot = pd.get_dummies(application_train)","1f8f3ff3":"# Check what our data looks like now\n\ntrain_one_hot.head()","d052eda8":"# Check the dimensions after dummying out factors\n\ntrain_one_hot.shape","d1788961":"corr_pearson = train_one_hot.corr().values","3be1418b":"heatmap_coor_matrix(application_train, corr_pearson)    ","562eef44":"# Absolute value correlation matrix\n\ncorr_matrix = train_one_hot.corr().abs()","5ac54fb9":"corr_matrix","6c80c3e5":"# where: Replace values where the condition is False. \n# triu: Upper triangle of an array. Return a copy of a matrix with the elements below the k-th diagonal zeroed.\n\ncorr_upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(bool))","40b4241c":"corr_upper","9a3ce26b":"# Select columns with correlations above threshold\n\nthreshold = 0.9\ndrop = []\nfor column in corr_upper.columns:\n    if any(corr_upper[column]>threshold):\n        drop.append(column)\n\n\nprint(\"columns to drop \" + str(len(drop)))","c4658ee6":"# Some of the features we re going to drop\n\ndrop[:10]","6973acaa":"train = train_one_hot.drop(columns = drop)","d42b36e5":"train","792677d2":"threshold = 0.55\ntrain_missing = (train.isnull().sum() \/ len(train)).sort_values(ascending = False)","665cc1b6":"train_missing = train_missing.index[train_missing > threshold]\n\nprint(\"we are going to drop \" + str(len(train_missing))+\" columns\")","83c6f121":"train.drop(columns = train_missing, inplace = True)","77c679e1":"train.head()","5e48f086":"#Save and Drop ['SK_ID_CURR'] column because the id is just number and it shouldn't have a predictive power\n\nID = train[\"SK_ID_CURR\"] # the ids\n\ntrain_clean = train.drop(columns = ['SK_ID_CURR'] )\n\n\n# Save the \"TARGET\" column and drop it\n\ntrain_target = train['TARGET'] # the target list\ntrain_clean.drop(columns = ['TARGET'], inplace = True)","9cbbfbc9":"train_clean.head()","84effad1":"print('we started from ', application_train.shape)\nprint('With just one-hot encoding we had ', train_one_hot.shape)\nprint('after feature selection we now have ', train_clean.shape)","8d4416a8":"from sklearn.preprocessing import StandardScaler","18fb3069":"#One-hot encoding on the test set \ntest_one_hot = pd.get_dummies(application_test)\n\n#and remove irrelevant features from the test set\n\nrelevant_features = list(train_clean.columns)\n\nto_drop = [col for col in test_one_hot.columns if col not in relevant_features]\ntest = test_one_hot.drop(columns = to_drop)\n\nprint('we started from ', application_test.shape)\nprint('With just one-hot encoding we had ', test_one_hot.shape)\nprint('after feature selection we now have ', test.shape)\n","f7d23f2a":"#there are 3 columns in train that are not present in test, remove them from train\n\nfor col in train_clean.columns:\n    if col not in test.columns.tolist():\n        train_clean.drop(columns = [col], inplace = True)\n\nprint('now we have ', train_clean.shape)","25bcae46":"sc = StandardScaler()\ntrain = sc.fit_transform(train_clean)\n\ntest  = sc.fit_transform(test)","76dd17b1":"# the set of parameters for Light GBM\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'auc'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 10\nparams['min_data'] = 50\nparams['max_depth'] = 10","7f4254a0":"# Split the train dataset in test and train\n\nx_train, x_test, y_train, y_test = train_test_split(train ,train_target , test_size=0.4, random_state=18)","c0ca5ab9":"# Create the LightGBM data containers\n\ntrain_data = lgb.Dataset(x_train, label=y_train)\n\ntest_data = lgb.Dataset(x_test, label=y_test)","8de95e4c":"# Train the model\n\nmodel = lgb.train(params,\n                  train_data,\n                  valid_sets=test_data,\n                  num_boost_round=5000,\n                  early_stopping_rounds=100, \n                  verbose_eval=False)","4cb92b8d":"# Predict on train \n\npred_train = model.predict(train)","b8b66b05":"# Accuracy \n\ny_target = np.array(train_target)\n\ny_predictions = np.array(pred_train)\nauc = roc_auc_score(y_target, y_predictions)\nauc","56a62fa6":"# Calculate ROC curves\n\nlgbm_fpr, lgbm_tpr, _ = roc_curve(y_target, pred_train)\n\n# Plot the roc curve for the model\n\npyplot.plot(lgbm_fpr, lgbm_tpr, marker='.', label='LightGBM')\n\n# Axis labels\n\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n\n# Show the legend\n\npyplot.legend()\n\n# Show the plot\n\npyplot.show()","3fa68c45":"# Precict on the test set to make a submission\n\npreds = model.predict(test)\n\n#save the prediction into a csv file\nsubmissions = pd.DataFrame()\nsubmissions['SK_ID_CURR'] = application_test['SK_ID_CURR']\nsubmissions['TARGET'] = preds\nsubmissions.to_csv(\"predictions.csv\", index=False)","713d0fac":"def numeric_aggregation(table, key, name ):\n    for col in table:\n        if col != key and \"SK_ID\" in col:\n            table = table.drop(columns=col)\n    numeric_table = table.select_dtypes(\"number\")\n    numeric_table[key] = table[key]\n    agg = numeric_table.groupby(key).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    columns = [key]\n\n    for var in agg.columns.levels[0]:\n        if var != key:\n            for stat in agg.columns.levels[1][:-1]:\n                columns.append('%s_%s_%s' % (name, var, stat))\n    agg.columns = columns\n    return agg\n\ndef correlation_func(table):\n    corrs = []\n\n    for col in table.columns:\n        if col != \"TARGET\":\n            corr = table['TARGET'].corr(table[col])\n            corrs.append((col, corr))\n\n    corrs = sorted(corrs, key=lambda x: abs(x[1]), reverse=True)\n    return corrs\n\ndef categorical_aggregation(table, key, name ):\n    try:\n        categoricals = pd.get_dummies(table.select_dtypes(\"object\"))\n        categoricals[key] = table[key]\n    except ValueError:\n        return None\n\n    agg = categoricals.groupby(key).agg([\"sum\", \"mean\"])\n\n    columns = []\n\n    for var in agg.columns.levels[0]:\n        for stat in [\"count\", \"count_norm\"]:\n            columns.append('%s_%s_%s' % (name, var, stat))\n    agg.columns = columns\n    return agg","f1487ae9":"bureau = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau.csv\")\nbureau_balance = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau_balance.csv\")\ntrain_data = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_train.csv\")\ntest_data = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_test.csv\")\nprevious_application = pd.read_csv(\"..\/input\/home-credit-default-risk\/previous_application.csv\")\nPOS_CASH_balance = pd.read_csv(\"..\/input\/home-credit-default-risk\/POS_CASH_balance.csv\")\ninstallments_payments = pd.read_csv(\"..\/input\/home-credit-default-risk\/installments_payments.csv\")\ncredit_card_balance = pd.read_csv(\"..\/input\/home-credit-default-risk\/credit_card_balance.csv\")","21152a40":"bureau_num_agg = numeric_aggregation(bureau, key=\"SK_ID_CURR\", name=\"bureau\")\nbureau_categorical_agg = categorical_aggregation(bureau, key=\"SK_ID_CURR\", name=\"bureau\")","38daab4c":"previous_application_num_agg = numeric_aggregation(previous_application, key=\"SK_ID_CURR\", name=\"previous_application\")\nprevious_application_categorical_agg = categorical_aggregation(previous_application, key=\"SK_ID_CURR\", name=\"previous_application\")\ndel previous_application","c406b303":"POS_CASH_balance_num_agg = numeric_aggregation(POS_CASH_balance, key=\"SK_ID_CURR\", name=\"POS_CASH_balance\")\nPOS_CASH_balance_categorical_agg = categorical_aggregation(POS_CASH_balance, key=\"SK_ID_CURR\", name=\"POS_CASH_balance\")\ndel POS_CASH_balance","8640b09d":"installments_payments_num_agg = numeric_aggregation(installments_payments, key=\"SK_ID_CURR\", name=\"installments_payments\")\ndel installments_payments","606029bd":"credit_card_balance_num_agg = numeric_aggregation(credit_card_balance, key=\"SK_ID_CURR\", name=\"credit_card_balance\")\ncredit_card_balance_categorical_agg = categorical_aggregation(credit_card_balance, key=\"SK_ID_CURR\", name=\"credit_card_balance\")\ndel credit_card_balance","0946ad19":"bureau_balance_num_agg = numeric_aggregation(bureau_balance, key=\"SK_ID_BUREAU\", name=\"bureau_balance\")\nbureau_balance_categorical_agg = categorical_aggregation(bureau_balance, key=\"SK_ID_BUREAU\", name=\"bureau_balance\")\ndel bureau_balance","1e997800":"bureau_by_loan = bureau_balance_num_agg.merge(bureau_balance_categorical_agg, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\nbureau_balance_by_client = numeric_aggregation(bureau_by_loan.drop(columns=['SK_ID_BUREAU']), key='SK_ID_CURR', name='client')\ndel bureau\ndel bureau_by_loan","e02537b0":"train_data = train_data.merge(bureau_num_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(bureau_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(previous_application_num_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(previous_application_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(POS_CASH_balance_num_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(POS_CASH_balance_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(installments_payments_num_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(credit_card_balance_num_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(credit_card_balance_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ntrain_data = train_data.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","0afcbf78":"test_data = test_data.merge(bureau_num_agg, on = 'SK_ID_CURR', how = 'left')\ndel bureau_balance_num_agg\ntest_data = test_data.merge(bureau_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ndel bureau_categorical_agg\ntest_data = test_data.merge(previous_application_num_agg, on = 'SK_ID_CURR', how = 'left')\ndel previous_application_num_agg\ntest_data = test_data.merge(previous_application_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ndel previous_application_categorical_agg\ntest_data = test_data.merge(POS_CASH_balance_num_agg, on = 'SK_ID_CURR', how = 'left')\ndel POS_CASH_balance_num_agg\ntest_data = test_data.merge(POS_CASH_balance_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ndel POS_CASH_balance_categorical_agg\ntest_data = test_data.merge(installments_payments_num_agg, on = 'SK_ID_CURR', how = 'left')\ndel installments_payments_num_agg\ntest_data = test_data.merge(credit_card_balance_num_agg, on = 'SK_ID_CURR', how = 'left')\ndel credit_card_balance_num_agg\ntest_data = test_data.merge(credit_card_balance_categorical_agg, on = 'SK_ID_CURR', how = 'left')\ndel credit_card_balance_categorical_agg\ntest_data = test_data.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\ndel bureau_balance_by_client","fa1cc680":"train_labels = train_data[\"TARGET\"]\ntrain_data, test_data = train_data.align(test_data, join=\"inner\", axis=1)\ntrain_data[\"TARGET\"] = train_labels\nprint(train_data.shape)\nprint(test_data.shape)\n\n# handling the missing values\nmis_val_count = train_data.isnull().sum()\npercentage = mis_val_count\/len(train_data)\n\n# drop the columns with missing values higher than a threshold\ncolumns = percentage[percentage < 0.4].index\ntrain_data = train_data[columns]\ntest_data = test_data[columns[:-1]] #drop the target column","d6ac8df7":"corrs = train_data.corr()\n\n# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])\n\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Remove one of the columns\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n\ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))\n\ntrain_corrs_removed = train_data.drop(columns = cols_to_remove)\ntest_corrs_removed = test_data.drop(columns = cols_to_remove)","0beb00ec":"train_corrs_removed.to_csv('extended_train.csv', index = False)\ntest_corrs_removed.to_csv('extended_test.csv', index = False)","d9d58cfe":"del train_corrs_removed, test_corrs_removed","ebf8fd77":"extended_train = pd.read_csv(\"extended_train.csv\")\nextended_test = pd.read_csv(\"extended_test.csv\")","a2682b5c":"print(extended_test.shape)\nprint(extended_train.shape)","cbcf83c8":"x_train = pd.get_dummies(extended_train.iloc[:,:-1])\ny_train = extended_train.iloc[:,-1]\nx_test = pd.get_dummies(extended_test)\ndel extended_train\ndel extended_test","e5032c68":"drop_list = []\nfor column in x_train.columns:\n    if column in x_test.columns:\n        continue\n    else:\n        drop_list.append(column)","48d88f01":"x_train = x_train.drop(columns = drop_list)\nprint(x_train.shape)\nprint(x_test.shape)","68abb794":"imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\nimp_median.fit(x_train)\nx_train = imp_median.transform(x_train)\nimp_median.fit(x_test)\nx_test = imp_median.transform(x_test)","4f264a91":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","7d49211c":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 50)\n\ninputs=x_train[:,1:]\noutputs = y_train\nx_test = x_test[:,1:]\n\n# Empty array for test predictions\ny_pred = np.zeros(x_test.shape[0])\n    \nfor train, test in kfold.split(inputs, outputs):\n    \n    model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                           class_weight = 'balanced', learning_rate = 0.05, \n                           reg_alpha = 0.1, reg_lambda = 0.1, \n                           subsample = 0.8, n_jobs = -1, random_state = 50)\n    # Train the model\n    model.fit(inputs[train, :], outputs[train], eval_metric = 'auc',\n              eval_set = [(inputs[test, :], outputs[test]), (inputs[train, :], outputs[train])],\n                  eval_names = ['valid', 'train'],\n              early_stopping_rounds = 100, verbose = 200)\n    # Record the best iteration\n    best_iteration = model.best_iteration_\n    print(1)\n    # Make predictions\n    y_pred += model.predict_proba(x_test, num_iteration = best_iteration)[:, 1] \/ kfold.n_splits","e244be4b":"submission = pd.read_csv(\"..\/input\/home-credit-default-risk\/sample_submission.csv\")","89273de9":"submission[\"TARGET\"] = y_pred","cbc340df":"submission.to_csv('submission.csv', index = False)","97acb675":"### Functions for Feature Extraction","c08d7698":"### 3. Some statistics about the Family status of each client\n\nWe can see that married individuals constitute a large proportion of the loans issued by Home Credit. There appears to be little impact of family status on default risk.","92b54d8c":"### Matrix of Pearson Correlations","0f26a7d4":"### Question: Are the data balanced or imbalanced?","8010292d":"The structure of the data is explained in the following image\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png)","d3b0d5e5":"### Scale the data","3755addc":"### Aggregate: bureau ballance to bureau","3745db5a":"## Feature Extraction","6d7b1120":"### 6. Some statistics about the Clients Contract type","d55e1f4b":"### Import tables containing ancillary data","dd634c5b":"### Align train and test datasets\n\nIn this step, we align the formats of the train and test datasets to create predictions for the test cases on the basis of the models we have built using the training data.","9e0b810e":"### Answer: The data are highly unbalanced\n\n- 91.92% of clients repaid the loan on time \n- 8.07% of clients didn't repaid the loan on time\n\nThis means that we cannot use accuracy as a error metric but we use ROC curves and the _area under the curve_ (AUC) metric to evaluate the power of our predictions. ","3f28b5bf":"Which fields constitute `NAME_TYPE_SUITE` (flags _Who accompanied client when applying for the previous application_, see data dictionary.","a82a830d":"# Modelling with Light GBM\n\n### Model building and training\n\nWe need to convert our training data into LightGBM dataset format(this is mandatory for LightGBM training).\n\nAfter creating a converting dataset, I created a python dictionary with parameters and their values. Accuracy of your model totally depends on the values you provide to parameters.\n\nIn this section we will train a model with just the data from application_train, later we will try to extract some features from the other dataset and re_train a model on a dataset with extended data.","ff3ec183":"### Some stat about the age of each client","c728df90":"### 5- fold Cross validation of LGBM","fd64f863":"### Outcome Decompositions by Features","cc78d20c":"### Create dummy variables for categorical variables, i.e. factors (data cleaning)","518a006f":"### Remove columns by cross correlation","3561bfa0":"# Previous Applications\n\nSome elementary exploration of clients' previous loan applications. We present tabular and graphical breakdowns of the purpose of cash loans in previous loan applications.","1678fdef":"We can **drop columns with too many missing values**: We can choose a threshold and drop every column that has a percentage of missing values over the chosen threshold. \n\nMany learners (for example from the scikit-learn library) do not handle missing values in the feature matrix. \n\nWe use this approach and also **median imputation**.","00c8c2ab":"Here our aim is to aggregate the information in tables bureau, previous application, posh cash balance, installments payments and credit card balance to merge with application table. The numeric aggregation function fetches the columns with numeric values in the following tables and group by \"SK_ID_CURR\" column to aggregate the information. For each numeric column, the aggregated information represented with count, mean, maximum, minimum and sum for each unique \"SK_ID_CURR\" value.  On the other hand, the categorical aggregation function fetches the categorical columns in the following tables, applies one hot encoding and then group by \"SK_ID_CURR\" column to aggregate the information. For each categorical column, the aggregated information represented with mean and sum for each unique \"SK_ID_CURR\" value. After that, to extract the information from bureau balance table, we group by \"SK_ID_BUREAU\" column and then merge with Bureau balance on \u201cSK_ID_BUREAU\u201d. Finally we merge all the aggregated tables with application train and application test tables to have a single table containing all the information from other sources. ","9261cbf2":"# Home Credit Kaggle Competition\n\n- Flaminia Spasiano (1889394)\n- Onur \u00c7opur (1891194)\n- Anil Keshwani (1919705)\n\n### Project Description\n\nOur predictive analysis consisted of data cleaning, exploratory data analysis and modelling, the latter including feature selection and model evaluation. \n\nThe initial **data cleaning** required:  \n\n- basic recoding of specific columns (e.g. age of client)\n- creation of dummy variables to replace categorical variables such as client gender or housing situation\n- imputation of missing values (we imputed missing values according to column medians)\n\nWe performed this using a combination of readily available functions (Numpy and Pandas) and convenience functions we wrote ourselves. \n\nDuring **exploratory data analysis**, we created a number of cross-tabulations of the target variable (default or late loan repayment) with features we might expect to be predictive such as income type, loan type and age. We produced a range of interactive visualisations using Plotly (via custom wrapper functions for convenience) to help build some intuition about the dataset. \n\nTo conduct **modelling**, we merged data from the ancillary tables (e.g. _bureau_, _bureau\\_balance_, _previous\\_appliation_ etc.) with the main _application_ table after aggregation on the `SK_IDD_CURR` key. We then performed feature selection on this extended dataset by eliminating highly collinear variables (i.e. those with high absolute correlations). \n\nWe attempted several modelling approaches including:  \n\n- penalised logistic regression\n- support vector machines\n- light gradient boosting\n\nWe used k-fold ($k=5$) cross-validation to optimise hyperparameters in our models (e.g. weight of penalisation term in regularised logistic regression) and to provide estimates of test error.\n\nDue to highly imbalanced classes in the target variable and in line with the assessment criterion, we of course used ROC curves and the _area under the curve_ (AUC) metric to assess our models.\n\nAs appears in our notebook, our best performance was achieved with light gradient boosting performed on the extended version of the dataset where data from ancillary tables was aggregated and joined onto the core _application_ table. \n\n### Note on the Outcome Variable: `TARGET`\n\n- TARGET == 1: _late payment more than X days on at least one of the first Y installments of the loan in our sample_\n- TARGET == 0: client repaid loan on time\n\nNB We will often refer to loans being paid or not paid for brevity, in the understanding this refers to the repayment status at the due date.\n\n### Resources We Used\n\nA list of key notebooks from which we adapted code. \n\n- A Gentle Introduction: https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction\n- Feature Selection: https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-feature-selection\n- Home Credit Default Risk Extensive EDA: https:\/\/www.kaggle.com\/gpreda\/home-credit-default-risk-extensive-eda\n- Home Credit : Complete EDA + Feature Importance: https:\/\/www.kaggle.com\/codename007\/home-credit-complete-eda-feature-importance\/notebook\n- Light-GBM : https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc","edf05182":"# LightGBM on Extended Data\n\nWe now perform the same modelling on the extended dataset we have created.\n\nNB Writing to an external file and re-reading in of the data is obviously not necessary when working with a standalone file, but we keep it for consistency with our [Kaggle Notebook](https:\/\/www.kaggle.com\/onurcopur\/defaultrisk-dreamteam), and as an indication of the workflow we used when conducting analysis to allow us to work with the dataset after merging and feature selection without having to re-run these steps each time.","924dacc6":"### Aggregation: installment Payments","18294cea":"In this section we will focus on extracting information from other data sources besides the main _application_ table and create new features with this information to increase the prediction accuracy.","cdd44358":"### Aggregation: Bureau\n","359eac2a":"### Merge with Test Data","f6a5581b":"## Create the submission csv","75acfde9":"### 2. Breakdown of clients by gender\n\nThe number of female clients is almost double the number of male clients. Looking at the percent of defaulted credits, males have a higher chance of not returning their loans.","844f48e3":"### Merge with Train Data","8b2d3cb9":"### Save the extended Data ","9f915973":"Count number of missing values in `NAME_TYPE_SUITE`","5afc0e7d":"### Plotting Convenience Functions","6ece60e5":"### Aggregation: Credit card ballance","02671355":"# Exploratory Data Analysis\n\nWe have included some components of our initial exploration of the datasets to indicate the process we went through to familiarise ourselves with the problem domain, visualise the data and locate missing values by field. \n\nWe use a number of convenience and wrapper functions for repetitive tasks.","f447e108":"Check that `DAYS_BIRTH` has been recoded into an age in (positive) years","f7f14ac9":"### One hot encode categorical features","62fad542":"### Aggregation: Bureau Ballence","01157b9d":"### 1. Breakdowns of the occupation type of each client\n\nWe can see that low-skill laborers have the highest likelihood of failing to repay the loan on time when grouping by occupational status. This group comprises less than 1% of loans at Home Credit, however with laborers, sales staff and \"core staff\" all making up larger factions at 26%, 15% and 13% respectively. ","ade7d6af":"### Aggregation: Pos_Cash Balance","988e71c1":"Tabulate `CODE_GENDER`, which flags the gender of clients taking out loans","53971f8f":"# Missing Values\n\nWe provide breakdowns of the missingness by field for the core tables we use for modelling. Later, we will impute this using medians.","13aa0f38":"### 4. Some statistics about the organization type of each client\n\nWe can see that clients' organisation types make a sizeable difference to default risk - at least on visual inspection - with clients whose organisation is in \"transport type 3\" being at higher risk of default than \"industry type 12\" for example.","3f720b88":"### 5. Some statistics about the Clients income type (businessman, working, maternity leave....)","8d04467d":"## Import the Extended Data","d3f7005f":"### Aggregation: previous application","5697024b":"### Put the column median instead of missing values","f446db5f":"### Feature Selection\n\nWe can drop **Collinear Variables**\n\nCollinear variables are those which are highly correlated with one another. These can:\n\n- decrease the model's availablility to learn\n- decrease model interpretability; and \n- decrease generalization performance on the test set\n\nThese are three things we want to increase, so removing collinear variables is a useful step. We can pick an arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold.","f4652cdd":"# Test preprocessing"}}