{"cell_type":{"fb22a216":"code","bc59a53b":"code","88437fd4":"code","746da50f":"code","3818ebc2":"code","f887721a":"code","9e366084":"code","6c54af86":"markdown","131c09be":"markdown","34dbb9ca":"markdown","a8eb997e":"markdown","49ed9e83":"markdown","8020874d":"markdown","8f491bf1":"markdown"},"source":{"fb22a216":"import pandas as pd\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport nltk,collections\nfrom nltk.util import ngrams\nimport string\nimport re\nimport spacy\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud\n\ndata = pd.read_csv(\"..\/input\/question2.csv\")\nprint(data['transcript'].head())\n\nstop_words = set(stopwords.words('english')) \nconcatinated_data = data.to_string(header=False,index=False,index_names=False)\nword_tokens = word_tokenize(concatinated_data) ","bc59a53b":"def cleanDataForNLP(TextData):\n    TextData.lower()\n    TextData = re.sub('[^A-Za-z]+', ' ', TextData)    \n    word_tokens = word_tokenize(TextData)\n    filteredText = \"\"\n    for w in word_tokens:\n        if w not in stop_words and len(w) > 2 and not w.isnumeric() and w not in string.punctuation:\n            filteredText = filteredText + \" \" + w\n    \n    return filteredText.strip()","88437fd4":"textData=cleanDataForNLP(concatinated_data)\ntokenized = textData.split()\nngram = ngrams(tokenized, 3)\nngramFreq = collections.Counter(ngram)\nmostcommonngrams=ngramFreq.most_common(10)\nprint(mostcommonngrams)","746da50f":"nlp = spacy.load('en')\nnlpdata=nlp(textData)\nposTags=[(x.text, x.pos_) for x in nlpdata if x.pos_ != u'SPACE']\nnounPhrases=[np.text for np in nlpdata.noun_chunks]\nnounPhrasesFiltered=[]\nfor nounPhrase in nounPhrases:\n    if(len(nounPhrase.split(\" \"))>1):\n        nounPhrasesFiltered.append(nounPhrase)\n\ndf = pd.DataFrame({'nounPhrases':nounPhrasesFiltered})\nnounPhrasesCount=df['nounPhrases'].value_counts()\nprint(\"Extracted Phrases\")\nprint(nounPhrasesCount.head(10))","3818ebc2":"wordcloud = WordCloud(\n                          background_color='white',\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(df['nounPhrases']))\n\nprint(wordcloud)\nfig = plt.figure(figsize=(10,6))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","f887721a":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nvectorizer = TfidfVectorizer()\nvectorizedText = vectorizer.fit_transform(df['nounPhrases'])\nwords = vectorizer.get_feature_names()\n\nkmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1) \nkmeans.fit(vectorizedText)\ncommon_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\nfor num, centroid in enumerate(common_words):\n    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))","9e366084":"for index, row in df.iterrows():\n    y = vectorizer.transform([row['nounPhrases']])\n    print (row['nounPhrases'] + \" - \" + str(kmeans.predict(y)))","6c54af86":"**Client Chat Phrase extraction & clustering **\n\n**Getting my hands dirty with Client Chat Phrase extraction & clustering I used different techniques to clean text data to improve accuracy. I tried stopwards removal,stemming, tfidfvectorization, kmeans algorithm for clustering. Please note that most of the data in dataset consists of words which are in hinglish - \"hindi language words written in english\" and hence prebuilt pos taggers and word2vec will not give good accuracy on such data.\nAnd due to short on time for this kernal right now I will not be able to build custom pos tagger or word2vec.**\n","131c09be":"**Phrase Clustering -**\n**To do clustering on phrases which are extracted from dataset above now we will convert words to vector format using tfidftransformer.\nThen Kmeans clustering is used cluster import words in different 5 clusters.\nThen Kmeans model is used to predict previously extracted phrases.**","34dbb9ca":"1. **Importing Important functions\/Libraries\nNLTK\nspacy\nngrams\nstopwards**\n\n2. **Load dataset from csv to pandas datadrame**\n","a8eb997e":"**Phrase Selection\nNow to get important phrases present in our data, I will first try to tag words using POS tagger and then use chunking on noun words to finally form noun phrases.**\n","49ed9e83":"I have created this function \"**cleanDataForNLP**\"  .  I am doing below mentioned things in this function\n1.  **Convert data to lowercase**\n2.  **Remove stopwords**\n3.  **Remove words having length less then 3**\n4.  **Remove punctuation marks as these are of no use to us**\n5.  **Also removing numeric characters**","8020874d":"Let us do basic word cloud formation to check most occuring words for Dataset Analysis.","8f491bf1":"**I have used n-grams to get context present in dataset.\nThe basic point of n-grams is that they capture the language structure from the statistical point of view, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with.**"}}