{"cell_type":{"c49536a2":"code","c1efb7bb":"code","6dbffd9d":"code","6f39cdb9":"code","cbe17c98":"code","65e1db0f":"code","8b0b89ca":"code","3b32e87b":"code","2623337f":"code","9a69bdc4":"code","192a35dd":"code","ddd7f14b":"code","a0359c02":"code","05ebb942":"code","cc5a7f4a":"code","7231d915":"code","ae06bbf1":"code","5d5751a6":"code","1182d484":"code","eb1196e1":"code","941de925":"code","17aebfee":"code","e7c4cdd2":"code","831c01de":"code","9bf7e940":"code","2402fbe8":"code","6592c233":"code","78c9e12d":"code","8ddff760":"code","a370c309":"code","6e76e747":"code","4e1ddfd8":"code","130aa119":"code","fd226560":"code","04733456":"code","4552d139":"code","f185054f":"code","962bfa86":"code","0aee32a9":"code","a3853d13":"code","8c7eab23":"code","40bc2d5a":"code","9fe08a38":"code","fc3dba1f":"code","ab87c441":"code","ce067057":"code","e7695c85":"code","f6bf0e9c":"code","117521ae":"code","dc10cb1c":"code","264c05e0":"code","9c5af9ab":"code","361c738a":"code","7b84a1b6":"code","c33320a8":"code","ab2ff501":"markdown","7570c3bd":"markdown","f71a011c":"markdown","e76abadb":"markdown","6df3ba0b":"markdown","b7ee07d0":"markdown","073b1ea4":"markdown","1168e380":"markdown","17e85811":"markdown","41248715":"markdown","cabe790d":"markdown","c52dc13e":"markdown","63c448e3":"markdown","507065ca":"markdown","41c85368":"markdown","05edccd9":"markdown","eee21f74":"markdown","e36c5cba":"markdown","10493156":"markdown","1be497df":"markdown","8e175ead":"markdown","4bcc65fd":"markdown","b2eb6740":"markdown","48af5448":"markdown","3a7af0b9":"markdown","665b9cf0":"markdown","16097fe8":"markdown","e39338f7":"markdown","661cb35e":"markdown","378dfcd6":"markdown","f920545c":"markdown","dc3d7c59":"markdown","4b10512a":"markdown","3ba48c7b":"markdown","2bed68cf":"markdown","1f183ba7":"markdown","977813bc":"markdown","64f7d673":"markdown","1cbdf5d8":"markdown","1ddc19b2":"markdown","d890b9bc":"markdown","2c449645":"markdown","8b796646":"markdown","2d751f96":"markdown","778fa419":"markdown","f14be532":"markdown","db3e457c":"markdown","62d0f0c3":"markdown","2796f112":"markdown","d10be018":"markdown","c26c8991":"markdown","a1dc68e2":"markdown","215510e9":"markdown","02a79b66":"markdown","974aec6b":"markdown","8ee8d0b2":"markdown","2f8ae786":"markdown","1aae6558":"markdown","8c49bc43":"markdown","6879ec72":"markdown","cba79a03":"markdown","e9099799":"markdown","b73b4722":"markdown","3d2862aa":"markdown","9c5bc819":"markdown","40ecfa1f":"markdown","c9c8e8bd":"markdown","c82a65fd":"markdown","d4744d24":"markdown","b565d921":"markdown","1e107fe6":"markdown","aba21c73":"markdown","fdcfd537":"markdown","c0f45b08":"markdown","ae457a94":"markdown","d13188e2":"markdown","0aae3d62":"markdown","f8a5d30f":"markdown","0e14219e":"markdown","719d3464":"markdown","893fe2f2":"markdown","3fe1b612":"markdown","fefe8d15":"markdown","b4b3f90a":"markdown","547375bc":"markdown","7370978e":"markdown","b9c762f5":"markdown","e36ea701":"markdown","71371130":"markdown","45650e3b":"markdown","1711955f":"markdown","f5fe69a0":"markdown","3c359a85":"markdown","8cd25ea7":"markdown","0613552a":"markdown","deb1ea05":"markdown","99260e55":"markdown","95043fff":"markdown","9bbb6122":"markdown","1152e69a":"markdown","17773c1d":"markdown","5f2150ed":"markdown"},"source":{"c49536a2":"import numpy as np \nimport pandas as pd\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\nimport itertools\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, roc_curve, precision_recall_curve, average_precision_score, confusion_matrix\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, ShuffleSplit, learning_curve, cross_val_predict, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom imblearn.over_sampling import SMOTE\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy","c1efb7bb":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head(5)","6dbffd9d":"print('Linhas: ', df.shape[0])\nprint('Colunas: ', df.shape[1])","6f39cdb9":"df.describe().T","cbe17c98":"df.isnull().sum().max()","65e1db0f":"df.columns","8b0b89ca":"print('Transa\u00e7\u00f5es n\u00e3o fraudulentas: ', round(df['Class'].value_counts()[0] \/ len(df) * 100, 2), '% dos dados')\nprint('Transa\u00e7\u00f5es fraudulentas: ', round(df['Class'].value_counts()[1] \/ len(df) * 100, 2), '% dos dados')","3b32e87b":"plt.figure(figsize=(8, 4))\n\nsns.countplot('Class', data=df, palette=['b', 'r'])\nplt.title('Distribui\u00e7\u00e3o da classe \\n0: N\u00e3o fraudulenta | 1: Fraudulenta')\n\nplt.show()","2623337f":"# Reescala a distribui\u00e7\u00e3o para ficar com m\u00e9dia 0 e desvio-padr\u00e3o 1\nstd_scaler = StandardScaler()\n# Utiliza o IQR (Inter Quartile Range) no escalonamento\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n\ndf.drop(['Time', 'Amount'], axis=1, inplace=True)","9a69bdc4":"# Armazena os dados das colunas criadas anteriormente\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\n# Deleta as colunas criadas\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n\n# Insere os valores das colunas criadas na 1\u00aa e 2\u00aa colunas do dataframe, respectivamente\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\ndf.head()","192a35dd":"# Features\nX = df.drop('Class', axis=1)\n# Target\ny = df['Class']\n\n# Separa os dados de maneira estratificada (mantendo as propor\u00e7\u00f5es originais)\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    \n    # Armazena os dados originais\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \n    # Transforma em array\n    original_Xtrain = original_Xtrain.values\n    original_Xtest = original_Xtest.values\n    original_ytrain = original_ytrain.values\n    original_ytest = original_ytest.values\n    \n    # Verifica se as distribui\u00e7\u00f5es de treino e teste s\u00e3o similares\n    train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n    test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n    \n    print('Distribui\u00e7\u00f5es:')\n    print(train_counts_label\/ len(original_ytrain))\n    print(test_counts_label\/ len(original_ytest))\n    print('-' * 100)","ddd7f14b":"# Pegar uma amostra aleat\u00f3ria. frac = 1 significa que a fra\u00e7\u00e3o conter\u00e1 todos os dados\ndf = df.sample(frac=1)\n\n# Transa\u00e7\u00f5es fraudulentas\nfraud_df = df.loc[df['Class'] == 1]\n# Transa\u00e7\u00f5es n\u00e3o fraudulentas (a amostra ter\u00e1 tamanho de 492)\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\n# concatenando os dataframes anteriores\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# novo dataframe com a amostra aleat\u00f3ria dos dataframes anteriores. Novamente, a fra\u00e7\u00e3o conter\u00e1 todos os dados.\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","a0359c02":"print('Distrui\u00e7\u00e3o das classes no novo dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\n# Distrui\u00e7\u00e3o das classes no dataframe balanceado\nsns.countplot('Class', data=new_df, palette=['b', 'r'])\nplt.title('Distrui\u00e7\u00e3o das classes no novo dataset', fontsize=14)\n\nplt.show()","05ebb942":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 20))\n\n# Correla\u00e7\u00e3o do dataframe original\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', vmin=-1, vmax=1,annot=True,  annot_kws={'size': 8}, ax=ax1)\nax1.set_title('Correla\u00e7\u00e3o dos dados desbalanceados', fontsize=14)\n\n# Correla\u00e7\u00e3o do dataframe balanceado\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot=True, vmin=-1, vmax=1, annot_kws={'size': 8}, ax=ax2)\nax2.set_title('Correla\u00e7\u00e3o dos dados balanceados', fontsize=14)\n\nplt.show()","cc5a7f4a":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\ncolors = ['b', 'r']\n\nsns.boxplot(x=\"Class\", y=\"V16\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Classe Negativa')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Classe Negativa')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Classe Negativa')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Classe Negativa')\n\nplt.show()","7231d915":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist, ax=ax1, fit=norm, color='r')\nax1.set_title('Distribui\u00e7\u00e3o V14 \\n(transa\u00e7\u00f5es fraudulentas)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist, ax=ax2, fit=norm, color='b')\nax2.set_title('Distribui\u00e7\u00e3o V12 \\n(transa\u00e7\u00f5es fraudulentas)', fontsize=14)\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist, ax=ax3, fit=norm, color='g')\nax3.set_title('Distribui\u00e7\u00e3o V10 \\n(transa\u00e7\u00f5es fraudulentas)', fontsize=14)\n\nplt.show()","ae06bbf1":"# Remover os outliers de V14 (correla\u00e7\u00e3o negativa alta com a classe)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n# Valores do quartil 25 e quartil 75\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('QUARTIL 25: {} | QUARTIL 75: {}'.format(q25, q75))\n# Interquartile range\nv14_iqr = q75 - q25\nprint('IQR: ', v14_iqr)\n\n# Limiar\nv14_cut_off = v14_iqr * 1.5\n# Limite superior e inferior\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('LIMIAR: ', v14_cut_off)\nprint('V14 LIMITE INFERIOR', v14_lower)\nprint('V14 LIMITE SUPERIOR', v14_upper)\n\n# Ouliers (fora os limites estabelecidos anteriormente)\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('V14 QUANTIDADE DE OUTLIERS EM FRAUDES:', len(outliers))\n\n# Novo dataframe sem os outliers\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n\n# Remover os outliers de V12\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 LIMITE INFERIOR: {}'.format(v12_lower))\nprint('V12 LIMITE SUPERIOR: {}'.format(v12_upper))\n\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n\nprint('V12 OUTLIERS: {}'.format(outliers))\nprint('V12 QUANTIDADE DE OUTLIERS EM FRAUDES: {}'.format(len(outliers)))\n\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('N\u00daMERO DE INST\u00c2NCIAS AP\u00d3S A REMO\u00c7\u00c3O DOS OUTLIERS: {}'.format(len(new_df)))\nprint('----' * 44)\n\n\n# Remover os outliers de V10\n\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 LIMITE INFERIOR: {}'.format(v10_lower))\nprint('V10 SUPERIOR: {}'.format(v10_upper))\n\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n\nprint('V10 OUTLIERS: {}'.format(outliers))\nprint('V10 QUANTIDAADE DE OUTLIERS EM FRAUDES: {}'.format(len(outliers)))\n\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n\n\nprint('---' * 42)\nprint('N\u00daMERO DE INST\u00c2NCIAS AP\u00d3S A REMO\u00c7\u00c3O DOS OUTLIERS: {}'.format(len(new_df)))","5d5751a6":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n\ncolors = ['b', 'r']\n\n# V14\nsns.boxplot(x='Class', y='V14', data = new_df, ax=ax1, palette=colors)\nax1.set_title('V14 \\nRedu\u00e7\u00e3o dos outliers', fontsize=14)\n\n# V12\nsns.boxplot(x='Class', y='V12', data = new_df, ax=ax2, palette=colors)\nax2.set_title('V12 \\nRedu\u00e7\u00e3o dos outliers', fontsize=14)\n\n# V10\nsns.boxplot(x='Class', y='V10', data = new_df, ax=ax3, palette=colors)\nax3.set_title('V10 \\nRedu\u00e7\u00e3o dos outliers', fontsize=14)\n\n\nplt.show()","1182d484":"X = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# T-SNE\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n\n# PCA\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n\n# TruncatedSVD\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)","eb1196e1":"# Cria a figura\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\nf.suptitle('Clusters usando Redu\u00e7\u00e3o de Dimensionalidade', fontsize=14)\n\n# Cores\nblue = mpatches.Patch(color='b', label='Sem fraude')\nred = mpatches.Patch(color='r', label='Fraude')\n\n\n# t-SNE\nax1.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=(y==0), cmap='coolwarm', label='Sem fraude', linewidths=2)\nax1.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=(y==1), cmap='coolwarm', label='Fraude', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\n# Desenha uma grade nas figuras\nax1.grid(True)\nax1.legend(handles=[blue, red])\n\n# PCA\nax2.scatter(X_reduced_pca[:, 0], X_reduced_pca[:, 1], c=(y==0), cmap='coolwarm', label='Sem fraude', linewidths=2)\nax2.scatter(X_reduced_pca[:, 0], X_reduced_pca[:, 1], c=(y==1), cmap='coolwarm', label='Fraude', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\nax2.legend(handles=[blue, red])\n\n\n# TruncatedSVD\nax3.scatter(X_reduced_svd[:, 0], X_reduced_svd[:, 1], c=(y==0), cmap='coolwarm', label='Sem Fraude', linewidths=2)\nax3.scatter(X_reduced_svd[:, 0], X_reduced_svd[:, 1], c=(y==1), cmap='coolwarm', label='Fraude', linewidths=2)\nax3.set_title('SVD', fontsize=14)\n\nax3.grid(True)\nax3.legend(handles=[blue, red])\n\n\nplt.show()","941de925":"X = new_df.drop('Class', axis=1)\ny = new_df['Class']","17aebfee":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e7c4cdd2":"# Transformar em array\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","831c01de":"classifiers = {\n    'LogisticRegression': LogisticRegression(),\n    'KNearest': KNeighborsClassifier(),\n    'SVC': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier()\n}","9bf7e940":"for key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    \n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    \n    print('Classificador: ', classifier.__class__.__name__, 'possui um score de', round(training_score.mean(), 2) * 100, '%')","2402fbe8":"# Logistic Regression\n# Par\u00e2metros\nlog_reg_params = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n}\n\n# GridSearch\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n# Treinamento\ngrid_log_reg.fit(X_train, y_train)\n# Melhores par\u00e2metros\nlog_reg = grid_log_reg.best_estimator_\n\n\n\n# Kneighbors\nknears_params = {\n    'n_neighbors': list(range(2, 5, 1)),\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\nknears_neighbors = grid_knears.best_estimator_\n\n\n\n# SVC\nsvc_params = {\n    'C': [0.5, 0.7, 0.9, 1],\n    'kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n}\n\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\nsvc = grid_svc.best_estimator_\n\n\n\n# DecisionTree\ntree_params = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': list(range(2, 4, 1)),\n    'min_samples_leaf': list(range(5, 7, 1))\n}\n\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\ntree_clf = grid_tree.best_estimator_","6592c233":"# Scores das valida\u00e7\u00f5es cruzadas dos 4 modelos\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score: ', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score: ', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score: ', round(tree_score.mean() * 100, 2).astype(str) + '%')","78c9e12d":"undersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print('Train: ', train_index, 'Test:', test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n\n# Transformar em array\nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values\n\n# Listas para armazenar os scores\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementa\u00e7\u00e3o da t\u00e9cnica \"NearMiss\", apenas para ver a distribui\u00e7\u00e3o\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('Distribui\u00e7\u00e3o NearMiss: ', Counter(y_nearmiss))\n\n# Valida\u00e7\u00e3o cruzada da maneira correta\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg)\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","8ddff760":"def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    # Cria a figura com 4 subplots\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 14), sharey=True)\n    \n    # Define os limites do eixo y\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n        \n        # Primeiro \"estimator\"\n        \n        # Calcula a curva de aprendizado\n        train_sizes, train_scores, test_scores = learning_curve(estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n        # M\u00e9dia do treinamento\n        train_scores_mean = np.mean(train_scores, axis=1)\n        # Desvio-padr\u00e3o do treinamento\n        train_scores_std = np.std(train_scores, axis=1)\n        # M\u00e9dia do teste\n        test_scores_mean = np.mean(test_scores, axis=1)\n        # Desvio-padr\u00e3o do teste\n        test_scores_std = np.std(test_scores, axis=1)\n        # Preenche os eixos\n        ax1.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1,color=\"#ff9124\")\n        ax1.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n        # Insere nos eixos os scores de treino\n        ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",label=\"Score de treino\")\n        # Insere nos eixos os scores da valida\u00e7\u00e3o cruzada\n        ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Score da valida\u00e7\u00e3o cruzada\")\n        # T\u00edtulo\n        ax1.set_title(\"Logistic Regression\", fontsize=14)\n        # Legenda do eixo X\n        ax1.set_xlabel('Training size (m)')\n        # Legenda do eixo y\n        ax1.set_ylabel('Score')\n        # Desenha uma grande nos eixos\n        ax1.grid(True)\n        # Insere a legenda na melhor localiza\u00e7\u00e3o (onde n\u00e3o sobreponha as linhas desenhadas nos eixos)\n        ax1.legend(loc=\"best\")\n        \n        # Segundo \"estimator\" \n        train_sizes, train_scores, test_scores = learning_curve(estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax2.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n        ax2.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n        ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Score de treino\")\n        ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax2.set_title(\"Knears Neighbors\", fontsize=14)\n        ax2.set_xlabel('Training size (m)')\n        ax2.set_ylabel('Score')\n        ax2.grid(True)\n        ax2.legend(loc=\"best\")\n\n        \n        # Terceiro \"estimator\"\n        train_sizes, train_scores, test_scores = learning_curve(estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax3.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1,color=\"#ff9124\")\n        ax3.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n        ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Score de treino\")\n        ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax3.set_title(\"Support Vector Classifier\", fontsize=14)\n        ax3.set_xlabel('Training size (m)')\n        ax3.set_ylabel('Score')\n        ax3.grid(True)\n        ax3.legend(loc=\"best\")\n\n        \n        # Quarto \"estimator\"\n        train_sizes, train_scores, test_scores = learning_curve(estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax4.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n        ax4.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n        ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Score de treino\")\n        ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n        ax4.set_title(\"Decision Tree Classifier\", fontsize=14)\n        ax4.set_xlabel('Training size (m)')\n        ax4.set_ylabel('Score')\n        ax4.grid(True)\n        ax4.legend(loc=\"best\")\n        \n        # Retorna a figura\n        return plt","a370c309":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=-1);","6e76e747":"# Logistic Regression\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method='decision_function')\n\n# Knears Neighbors\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\n# SVC\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5, method='decision_function')\n\n# Decision Tree Classifier\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","4e1ddfd8":"print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","130aa119":"# False Positive Rate e True Positive Rate de todos os 4 classificadores\nlog_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    # Cria a figura\n    plt.figure(figsize=(12,6))\n    # T\u00edtulo\n    plt.title('ROC Curve \\n4 Classificadores', fontsize=18)\n    # ROC AUC scores\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    # Linha central\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    # Legenda do eixo X\n    plt.xlabel('False Positive Rate', fontsize=16)\n    # Legenda do eixo y\n    plt.ylabel('True Positive Rate', fontsize=16)\n    \n    plt.legend()\n   \n\ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","fd226560":"def log_roc_curve(log_fpr, log_tpr):\n    # Cria a figura\n    plt.figure(figsize=(10, 5))\n    # T\u00edtulo\n    plt.title('LogisticRegression ROC Curve', fontsize=16)\n    \n    # FPR e TPR na cor azul\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    # Linha central na cor vermelha\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01, 1, 0, 1])\n    \n    \nlog_roc_curve(log_fpr, log_tpr)\nplt.show()","04733456":"precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)\n\n# Predi\u00e7\u00f5es\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n\nprint('---' * 45)\n\n# Como deveria ser\nprint('Como deveria ser:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))","4552d139":"# M\u00e9dia de precision-recall\nundersample_y_score = log_reg.decision_function(original_Xtest)\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(undersample_average_precision))","f185054f":"fig = plt.figure(figsize=(10, 5))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='b', alpha=0.3, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='r')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Undersampling Precision-Recall curve: \\nAverage Precision-Recall score = {0:0.2f}'.format(undersample_average_precision), fontsize=16)","962bfa86":"print('Tamanho do X (treino): {} | Tamanho do y (treino): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Tamanho do X (teste): {} | Tamanho do y (teste): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# Lista para armazenar os scores\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Par\u00e2metros da Logistic Regression\nlog_reg_params = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n}\n\n# Randomized SearchCV\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementa\u00e7\u00e3o do SMOTE\n# Cross-validation da maneira correta\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    # Pipeline\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE durante a valida\u00e7\u00e3o cruzada\n    # Treinamento do modelo\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    # Melhores par\u00e2metros\n    best_est = rand_log_reg.best_estimator_\n    # Predi\u00e7\u00f5es\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    # Armazena os \"scores\" nas listas criadas anteriormente\n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \n# Exibe os \"scores\"\nprint('---' * 40)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","0aee32a9":"labels = ['N\u00e3o fraudulenta', 'fraudulenta']\n\n# Predi\u00e7\u00e3o com SMOTE\nsmote_prediction = best_est.predict(original_Xtest)\n# Printa a \"classification report\"\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","a3853d13":"# Logistic Regression treinado com SMOTE\ny_pred_log_reg = best_est.predict(X_test)\n\n# Outros modelos com undersampling\ny_pred_knear = knears_neighbors.predict(X_test)\ny_pred_svc = svc.predict(X_test)\ny_pred_tree = tree_clf.predict(X_test)\n\n# Matriz de confus\u00e3o de todos os modelos\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\n# Cria a figura e os axes\nfig, ax = plt.subplots(2, 2,figsize=(12,6))\n\n# Exibe a matriz de confus\u00e3o do modelo Logistic Regression\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.Blues)\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n# Exibe a matriz de confus\u00e3o do modelo Knears Neighbors\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.Blues)\nax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n# Exibe a matriz de confus\u00e3o do modelo SVC\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.Blues)\nax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n# Exibe a matriz de confus\u00e3o do modelo Decision Tree\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.Blues)\nax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nplt.tight_layout()\nplt.show()","8c7eab23":"# classification_reports de todos os modelos\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('---' * 40)\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('---' * 40)\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('---' * 40)\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_tree))","40bc2d5a":"# Logistic Regression com \"undersampling\"\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)\n\n# Logistic Regression com SMOTE\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n# Dicion\u00e1rio com os scores das duas t\u00e9cnicas (undersampling e oversampling)\nd = {\n    'T\u00e9cnica': ['Random undersampling', 'Oversampling (SMOTE)'],\n    'Score': [undersample_score, oversample_score]\n}\n\n# Cria um dataframe com o dicion\u00e1rio\nfinal_df = pd.DataFrame(data=d)\n\n# Armazena o \"Score\" em outra vari\u00e1vel\nscore = final_df['Score']\n# Remove a coluna \"Score\"\nfinal_df.drop('Score', axis=1, inplace=True)\n# Insere os dados armazenados anteriormente na segunda coluna\nfinal_df.insert(1, 'Score', score)\n\nfinal_df","9fe08a38":"# Tamanho da camada de entrada\nn_inputs = X_train.shape[1]\n\n# Cria\u00e7\u00e3o da rede\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","fc3dba1f":"# Visualiza\u00e7\u00e3o da arquitetura da rede\nundersample_model.summary()","ab87c441":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","ce067057":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True)","e7695c85":"undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200)","f6bf0e9c":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","117521ae":"# Dados originais\nundersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(10,5))\n\nfig.add_subplot(111)\nplot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n","dc10cb1c":"# SMOTE\nsm = SMOTE('minority', random_state=42)\n\n# Treina os dados originais utilizando SMOTE\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)\n\n# Modelo com os melhores par\u00e2metros\nlog_reg_sm = grid_log_reg.best_estimator_\n# Treina o modelo utilizando os dados\nlog_reg_sm.fit(Xsm_train, ysm_train)","264c05e0":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","9c5af9ab":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","361c738a":"oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)","7b84a1b6":"oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","c33320a8":"# DAdos originais\noversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(10,5))\n\nfig.add_subplot(111)\nplot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)","ab2ff501":"### Detalhes importantes:","7570c3bd":"# Classificadores","f71a011c":"- Nunca testar em conjuntos de dados \"oversampled\" ou \"undersampled\"\n- Caso for implementada uma \"cross-validation\", lembrar de aplicar o \"oversample\" ou \"undersample\" nos dados de treino **durante** a valida\u00e7\u00e3o, n\u00e3o antes\n- N\u00e3o usar a **accuracy** como m\u00e9trica para ocnjunto de dados desbalanceados. Ao inv\u00e9s disso, usar **f1-score**, **precision\/recall score** ou **confusion matrix**.","e76abadb":"### Objetivos","6df3ba0b":"- Resolve o desbalanceamento de classes: SMOTE cria pontos sint\u00e9ticos da classe minorit\u00e1ria para alcan\u00e7ar o balanceamento entre a classe minorit\u00e1ria e majorit\u00e1ria\n- Localiza\u00e7\u00e3o dos pontos sint\u00e9ticos: SMOTE calcula a dist\u00e2ncia dos vizinhos mais pr\u00f3ximos da classe minorit\u00e1ria e cria pontos sint\u00e9ticos nessas dist\u00e2ncias\n- Efeito final: mais informa\u00e7\u00f5es s\u00e3o mantidas, j\u00e1 que n\u00e3o precisamos excluir nenhuma linha ,diferente da undersampling aleat\u00f3ria.\n- Accuracy\/Time tradeoff: embora o SMOTE provavelmente seja mais preciso que a \"undersampling\" aleat\u00f3ria, levar\u00e1 mais tempo para ser executado.","b7ee07d0":"# Redu\u00e7\u00e3o de dimensionalidade e clusteriza\u00e7\u00e3o","073b1ea4":"Iremos fazer um \"split\" **estratificado** (com propor\u00e7\u00f5es iguais) utilizando os **dados originais (df)**.","1168e380":"- **True Positive:** classifica\u00e7\u00e3o correta de transa\u00e7\u00e3o fraudulenta\n- **False Positive:** classifica\u00e7\u00e3o incorreta de transa\u00e7\u00e3o fraudulenta\n- **True Negative:** classifica\u00e7\u00e3o correta de transa\u00e7\u00e3o n\u00e3o fraudulenta\n- **False Negative:** classifica\u00e7\u00e3o incorreta de transa\u00e7\u00e3o n\u00e3o fraudulenta\n- **Precision:** True Positives \/ (True Positives + False Positives)\n- **Recall:** True Positivies \/ (True Positives + False Negatives)\n- A **Precision** mede o qu\u00e3o preciso o modelo \u00e9 em detectar transa\u00e7\u00f5es fraudulentas, enquanto o **Recall** \u00e9 a quantidade de transa\u00e7\u00f5es fraudulentas o modelo \u00e9 capaz de detectar.","17e85811":"Essa rede neural possui a seguinte arquitetura:\n- 1 camada de entrada\n- 1 camada oculta\n- 1 camada de sa\u00edda","41248715":"### Valida\u00e7\u00e3o cruzada","cabe790d":"O algoritmo **t-SNE** pode agrupar de maneira bem acurada os casos fraudulentos ou n\u00e3o, principalmente pelo fato da \"subsample\" ser bem pequena. Isso indica que os modelos que usaremos ter\u00e3o um bom desempenho na separa\u00e7\u00e3o das classes.","c52dc13e":"V14 \u00e9 a \u00fanica \"feature\" com uma distribui\u00e7\u00e3o normal comparada com as outras.","63c448e3":"A **confusion matrix (matriz de confus\u00e3o)** consiste em:\n- True Negatives (quadrado superior esquerdo): n\u00famero de classifica\u00e7\u00f5es corretas da classe 0 (transa\u00e7\u00e3o n\u00e3o fraudulenta)\n- False Negatives (quadrado superior direito): n\u00famero de classifica\u00e7\u00f5es incorretas da classe 0 (transa\u00e7\u00e3o n\u00e3o fraudulenta)\n- False Positives (quadrado inferior esquerdo): n\u00famero de classifica\u00e7\u00f5es incorretas da classe 1 (transa\u00e7\u00e3o fraudulenta)\n- True Positives (quadrado inferior direito): n\u00famero de classifica\u00e7\u00f5es corretas da classe 1 (transa\u00e7\u00e3o fraudulenta).","507065ca":"# Bibliotecas necess\u00e1rias","41c85368":"# Distribui\u00e7\u00e3o igualit\u00e1ria e correla\u00e7\u00f5es","05edccd9":"Nessa fase do notebook, iremos primeiro escalonar as colunas **Time** e **Amount** e depois criar uma \"subsample\" do conjunto de dados com quantidade iguais de casos fraudulentos e n\u00e3o fraudulentos.","eee21f74":"- **Interquartile range (IQR):** O c\u00e1lculo \u00e9 feito pela diferen\u00e7a entre 75\u00ba percentil e o 25\u00ba percentil. Com isso podemos criar uma **limiar** entre o 75\u00ba e 25\u00ba e qualquer inst\u00e2ncia fora dessa limiar ser\u00e1 excluida.\n","e36c5cba":"### Dividir os dados (dataframe original)","10493156":"### Cria\u00e7\u00e3o da rede neural com a t\u00e9cnica \"undersampling\"","1be497df":"# Remo\u00e7\u00e3o de outliers","8e175ead":"#### Predi\u00e7\u00f5es","4bcc65fd":"### Entendendo t-SNE:","b2eb6740":"#### Compila\u00e7\u00e3o e treinamento da rede","48af5448":"- Primeiro iremos **visualizar as distribui\u00e7\u00f5es** de algumas \"features\" que iremos usar para elimiar os \"outliers\"\n- Depois, **determinar a limiar** que usaremos, ou seja, decidir qual n\u00famero iremos multiplicar com o IQR\n    > q25 - limiar = limite inferior | q75 + limiar = limite superior\n- Por \u00faltimo, criaremos a condi\u00e7\u00e3o para exclus\u00e3o, onde as int\u00e2ncias que excederem os limites ser\u00e3o removidas.","3a7af0b9":"### Correla\u00e7\u00f5es","665b9cf0":"#### Fun\u00e7\u00e3o para exibir a matriz de confus\u00e3o","16097fe8":"Temos que ser cuidadosos com rela\u00e7\u00e3o \u00e0 limiar para a remo\u00e7\u00e3o de \"outliers\". Determinamos essa limiar multiplicando um n\u00famero (1.5, por exemplo) pelo IQR. Quanto maior a limiar, menos \"outliers\" ser\u00e3o detectados (Se usarmos o n\u00famero 3 na multiplica\u00e7\u00e3o, por exemplo) e quanto menor a limiar, mais \"outliers\" ser\u00e3o detectados. O melhor \u00e9 focarmos apenas nos \"outliers\" extremos, assim diminuindo o risco de ocorrer a perda de informa\u00e7\u00f5es, fazendo com que o modelo tenha acur\u00e1cia menor.","e39338f7":"# Carregamento dos dados","661cb35e":"### \"Undersample\" durante a valida\u00e7\u00e3o cruzada","378dfcd6":"### Classificadores","f920545c":"- Caso a diferen\u00e7a entre o \"score\" de treino e teste seja grande, isso pode significar que o modelo esteja sofrendo de **overfitting (alta vari\u00e2ncia)**\n- Caso o \"score\" seja baixo no treino e no teste, o modelo pode estar sofrendo de **underfitting (alta bias)**.","dc3d7c59":"O modelo **LogisticRegression**, al\u00e9m de apresentar \"scores\" altos no treino e na valida\u00e7\u00e3o cruzada (isso que dizer que n\u00e3o houve **underfitting**), tamb\u00e9m mostrou que n\u00e3o h\u00e1 tanta diferen\u00e7a (gap) entre os \"scores\" de treino e teste, sinal de que houve **overfitting**.\n","4b10512a":"#### Matriz de confus\u00e3o","3ba48c7b":"A t\u00e9cnica de **oversamplig (SMOTE)** obteve um resultado bem superior \u00e0 t\u00e9cnica de **random undersampling**. Isso pode ter ocorrido pelo fato de que durante o balanceamente dos dados com **undersampling** removeu diversos exemplos de dados **n\u00e3o fraudulentos**, afim de que as duas classes ficassem com uma quantidade igual de exemplos, e isso ocasionou uma **perda de informa\u00e7\u00f5es** que poderiam ter sido \u00fateis ao modelo. J\u00e1 o **SMOTE** n\u00e3o removeu essas informa\u00e7\u00f5es \u00fateis e sim criou pontos sint\u00e9ticos de dados **fraudulentos**, ou seja, as informa\u00e7\u00f5es \u00fateis continuaram presentes no conjunto de dados.","2bed68cf":"### Informa\u00e7\u00f5es sobre as \"features\"","1f183ba7":"Observando as matrizes de confus\u00e3o dos **modelos de classifica\u00e7\u00e3o**, vimos resultados semelhantes, por\u00e9m como j\u00e1 esperado, o modelo Logistic Regression apresentou um resultado superior aos outros. J\u00e1 na parte das **redes neurais**, o teste com os dados subamostrados (undersampling) n\u00e3o foi muito satisfat\u00f3rio, e houve uma grande quantidade de falsos positivos. Em uma perspectiva de neg\u00f3cio, esse modelo classificaria diversas transa\u00e7\u00f5es **n\u00e3o fraudulentas** como sendo **fraudulentas**, o que poderia acabar irritando alguns clientes e prejudicar a reputa\u00e7\u00e3o da empresa. A rede neural treinada com os dados superasmostrados (oversampling com a t\u00e9cnica SMOTE) obteve resultados satisfat\u00f3rios, com uma quantidade baixa de Falso Positivos e Falso Negativos. Ent\u00e3o, \u00e9 melhor usar um modelo de classifica\u00e7\u00e3o \"tradicional\", como Logistic Regression ou Redes Neurais? Depende muito dos recursos dispon\u00edveis. Redes Neurais costumam ter resultados melhores, por\u00e9m o custo computacional (e consequentemente de capital) para compilar redes neurais mais complexas pode ser bem elevado. Aqui usamos redes simples, com apenas uma camada oculta, mas existem arquiteturas de rede bem mais complexas. Al\u00e9m do mais, o modelo Logistic Regression apresentou bons resultados, e sua implementa\u00e7\u00e3o requer pouco poder computacional, sendo capaz de ser executado em um PC modesto como o meu.","977813bc":"#### Classe negativa","64f7d673":"# Um olhar mais atento \u00e0 LogisticRegression","1cbdf5d8":"### Considera\u00e7\u00f5es sobre a curva de aprendizado:","1ddc19b2":"Antes de aplicar o **undersampling**, \u00e9 importante separar o \"dataframe\" original. O objetivo disso \u00e9 que possamos treinar o modelo nos dados com **undersampling** e testar nos dados originais.","d890b9bc":"- A montante de transa\u00e7\u00f5es e relativamente **pequena**. A m\u00e9dia \u00e9 aproximadamente **USD 88**\n- N\u00e3o h\u00e1 valores nulos\n- A grande maioria das transa\u00e7\u00f5es s\u00e3o **n\u00e3o fraudulentas** (99.83%), enquanto transa\u00e7\u00f5es **fraudulentas** ocorrem apenas em 0.17% do conjunto de dados.","2c449645":"### Boxplots sem os \"outliers\"","8b796646":"Abaixo faremos a **predi\u00e7\u00e3o cruzada** usando os 4 classificadores:","2d751f96":"Nessa se\u00e7\u00e3o implementaremos uma rede neural simples (com apenas uma camada oculta) com intuito de verificar se a rede performa bem na predi\u00e7\u00e3o de transa\u00e7\u00f5es fraudulentas e n\u00e3o fraudulentas, al\u00e9m de utilizar os dois tipos de reamostragem (undersample e oversample).","778fa419":"O modelo de **LogisticRegression** obteve um resultado melhor sobre os outros modelos.","f14be532":"# Escalonamento e ditribui\u00e7\u00e3o dos dados","db3e457c":"# Redes neurais com undersampling e oversampling (SMOTE)","62d0f0c3":"A valida\u00e7\u00e3o final do modelos modelos de classifica\u00e7\u00e3o ser\u00e1 feita nos dados que passaram pela **random undersampling** e n\u00e3o nos dados originais.","2796f112":"### Corrigindo erros comuns em conjuntos de dados desbalanceados","d10be018":"Passos:\n- Determinar o quanto nossas classes s\u00e3o desbalanceadas\n- Uma vez determinadas quantas instancias de **transa\u00e7\u00f5es fraudulentas** (classe 1), vamos selecionar o mesmo n\u00famero de transa\u00e7\u00f5es **n\u00e3o fraudulentas** (classe 0).\n- Ap\u00f3s isso, teremos um conjunto de dados com propor\u00e7\u00e3o 50\/50. Depois, vamos embaralhar os dados.","c26c8991":"### Cria\u00e7\u00e3o da rede neural com a t\u00e9cnica \"oversampling (SMOTE)\"","a1dc68e2":"- Enteder a distribui\u00e7\u00e3o dos dados\n- Criar um \"sub-conjuntos de dados\" de raz\u00e3o 50\/50 com as transa\u00e7\u00f5es **Fraudulentas** e **N\u00e3o fraudulentas**.\n- Determinar os classificadores a serem usados e decidir qual tem maior acur\u00e1cia\n- Criar uma rede neural e comparar a acur\u00e1cia com o clasificador\n- Entender erros comuns presentes em conjuntos de dados desbalanceados.","215510e9":"### Descri\u00e7\u00e3o","02a79b66":"### Colunas","974aec6b":"# Refer\u00eancias","8ee8d0b2":"### Split","2f8ae786":"### Distribui\u00e7\u00e3o da classe","1aae6558":"### Scores","8c49bc43":"### Boxplots","6879ec72":"Com a valida\u00e7\u00e3o cruzada, **LogisticRegression** se saiu melhor dentre os classificadores.","cba79a03":"#### Predi\u00e7\u00f5es","e9099799":"Para entender esse algoritmo, \u00e9 importante entender os seguintes termos:\n- Dist\u00e2ncia Euclidiana\n- Probabilidade Condicional\n- Distribui\u00e7\u00e3o normal e T.","b73b4722":"Assim como no teste anterior, **LogisticRegression** se saiu melhor.","3d2862aa":"\u00c9 importante observar que na correla\u00e7\u00e3o dos **dados desbalanceados (df)** a grande maioria das features n\u00e3o possuiam correla\u00e7\u00f5es entre si (correla\u00e7\u00e3o = 0), j\u00e1 nos dados **balanceados (new_df)**, as correla\u00e7\u00f5es entre \"features\" eram bem mais n\u00edtidas.","9c5bc819":"O principal objetivo dessa se\u00e7\u00e3o \u00e9 remover \"outliers\" extremos das \"features\" que possuem correla\u00e7\u00e3o alta com a classe.","40ecfa1f":"# Introdu\u00e7\u00e3o","c9c8e8bd":"Nessa fase ser\u00e1 implementada a t\u00e9cnica \"random undersampling\", que basicamente consiste em remover dados para tornar o conjuntos de dados **mais balanceado**.","c82a65fd":"### Grid Search","d4744d24":"Nessa se\u00e7\u00e3o iremos treinar 4 tipos de classificadores para decidir qual deles \u00e9 mais efetivo na detec\u00e7\u00e3o de **transa\u00e7\u00f5es fraudulentas**","b565d921":"### Passos:","1e107fe6":"**SMOTE** significa \"Synthetic Minority Over-sampling Technique\". Ao contr\u00e1rio do \"undersampling\" aleat\u00f3rio, SMOTE cria novos pontons sint\u00e9ticos com o intuito de tornar as classes balanceadas e \u00e9 uma das t\u00e9cnicas alternativas para lidar com datasets desbalanceados.","aba21c73":"### Valores nulos","fdcfd537":"### Entendendo os dados ","c0f45b08":"### Remo\u00e7\u00e3o de \"outliers\" \"tradeoff\"","ae457a94":"### \"Undersampling\" aleat\u00f3ria","d13188e2":"### M\u00e9todo \"Interquartile range\" (dist\u00e2ncia interquartil)","0aae3d62":"# An\u00e1lise dos dados","f8a5d30f":"### Porcentagem das transa\u00e7\u00f5es fraudulentas VERSUS n\u00e3o fraudulentas","0e14219e":"- **Oversample**: ou \"superamostra\", consiste em  aumentar as amostras da classe minorit\u00e1ria, com intuito de balance\u00e1-la com a classe majorit\u00e1ria. Aplicar essa t\u00e9cnica geralmente \u00e9 conhecido como **oversampling (superamostragem)** e o conjunto de dados fica conhecido como **oversampled (superamostrado)**\n- **Undersample**: ou \"subamostra\", consiste em excluir amostras da classe majorit\u00e1ria, com intuito de balance\u00e1-la com a classe minorit\u00e1ria. Aplicar essa t\u00e9cnica geralmente \u00e9 conhecido como **undersampling (subamostragem)** e o conjunto de dados fica conhecido como **undersampled (subamostrado)**.\n- **Cross-validation**: valida\u00e7\u00e3o cruzada\n- **accuracy**: acur\u00e1cia\n- **confusion matrix**: matriz de confus\u00e3o","719d3464":"#### Compila\u00e7\u00e3o e treinamento da rede","893fe2f2":"# Conclus\u00e3o","3fe1b612":"- **scaled amount** e **scaled time** s\u00e3o as colunas com dados escalonados\n- H\u00e1 **492** caso de fraude no conjunto de dados, ent\u00e3o pegaremos aleatoriamente 492 casos n\u00e3o fraudulentos para criar o novo \"dataframe\"\n- Depois, uniremos os 492 casos de fraude e n\u00e3o fraude para **criar o subsample**.","fefe8d15":"Um dos problemas dessas t\u00e9cnica \u00e9 que temos a **perda de informa\u00e7\u00f5es**. Nesse caso, usaremos apenas **492** transa\u00e7\u00f5es n\u00e3o fraudulentas, de um total de **284,315**.","b4b3f90a":"### Gloss\u00e1rio","547375bc":"### Curva ROC","7370978e":"- **Correla\u00e7\u00e3o negativa:** V16, V14, V12 e V10. Isso que dizer que, quanto menor seus valores, mais prov\u00e1vel a transa\u00e7\u00e3o ser fraudulenta\n- **Correla\u00e7\u00e3o positiva:** V2, V4, e V11. Isso que dizer que, quanto maior seus valores, mais prov\u00e1vel a transa\u00e7\u00e3o ser fraudulenta.","b9c762f5":"Para evitar esses dois problemas: \n- **Overfitting**: o modelo de classifica\u00e7\u00e3o ir\u00e1 assumir que a maioria das transa\u00e7\u00f5es \u00e9 do tipo **n\u00e3o fraudulenta**\n- **Correla\u00e7\u00f5es erradas**: como n\u00e3o conhecemos a maioria das \"features\", elas s\u00e3o \u00fateis para entender como cada \"feature\"  influencia o resultado (fraudulento ou n\u00e3o fraudulento). Em conjunto de dados desbalanceados, o modelo n\u00e3o consegue perceber a verdadeira correla\u00e7\u00e3o entre as \"features\"  e a \"classe\". ","e36ea701":"### Shape","71371130":"Nessa fase final de teste, \u00e9 importante lembrar que o modelo treinar\u00e1 com os dados utilizando **ambas** as t\u00e9cnicas de reamostragem (**random undersampling** e **oversampling (SMOTE)**) e depois far\u00e1 as predi\u00e7\u00f5es utilizando os **dados originais de teste**.","45650e3b":"Nesse caso, a \"subsample\" ser\u00e1 um dataframe com propor\u00e7\u00e3o 50\/50 de transa\u00e7\u00f5es fraudulentas e n\u00e3o fraudulentas, ou seja, teremos 50% de dados para cada tipo de transa\u00e7\u00e3o.","1711955f":"# LogisticRegression com os dados de teste","f5fe69a0":"### Mas para qu\u00ea criar uma \"subsample\"?","3c359a85":"Apesar dos modelos **LogisticRegression** e **SVC** possuirem scores pr\u00f3ximos, **LogisticRregression** continua se saindo melhor em rela\u00e7\u00e3o aos outros.","8cd25ea7":"**Todo esse projeto foi baseado no incr\u00edvel notebook** [Credit Fraud || Dealing with Imbalanced Datasets](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\/notebook) criado por [Janio Martinez](https:\/\/www.kaggle.com\/janiobachmann). Meu trabalho consistiu em analisar, estudar as t\u00e9cnicas e codificar esse notebook, al\u00e9m de traduzir e inserir novos campos de texto e coment\u00e1rios, para melhor compreens\u00e3o dos c\u00f3digos. S\u00f3 tenho a agradecer ao criador do notebook original por disponibilizar um material t\u00e3o rico e que me permitiu adquirir tanto conhecimento.","0613552a":"### ROC AUC","deb1ea05":"- **PCA**: a descri\u00e7\u00e3o dos dados nos diz que as \"features\" passaram pela transforma\u00e7\u00e3o PCA(Principal Component Analysis), com intuito de reduzir as dimens\u00f5es dos dados (exceto **time** e **amount**)\n- **Escalonamento**: antes da aplica\u00e7\u00e3o do PCA, os dados foram escalonados.   ","99260e55":"O conjunto de dados \u00e9 bastante desbalanceado. Grande parte das transa\u00e7\u00f5es \u00e9 do tipo **n\u00e3o fradulenta**. Se esse conjunto de dados for usado como base para um modelo de predi\u00e7\u00e3o, o algoritmo ir\u00e1 assumir que a maioria das transa\u00e7\u00f5es \u00e9 do tipo **n\u00e3o fradulenta**, e n\u00e3o queremos que nosso modelo assuma e sim que detecte padr\u00f5es que deem sinais de fraude.","95043fff":"Com exce\u00e7\u00e3o das colunas **transaction** e **amount**, n\u00e3o sabemos o que s\u00e3o as outras colunas (por raz\u00e3o de privacidade, como dito anteriormente). A \u00fanica coisa que sabemos das colunas desconhecidas \u00e9 que elas est\u00e3o escalonadas.","9bbb6122":"# T\u00e9cnica SMOTE (Oversampling)","1152e69a":"Apesar do notebook estar escrito em portugu\u00eas, alguns termos ser\u00e3o escritos em ingl\u00eas, pois a a tradu\u00e7\u00e3o poderia gerar uma confus\u00e3o em quem j\u00e1 est\u00e1 acostumado aos termos em ingl\u00eas. Aqui um pequeno gloss\u00e1rio destes termos:","17773c1d":"### Fun\u00e7\u00e3o para plotar as curvas de aprendizado dos modelos","5f2150ed":"O objetivo desse notebook \u00e9 utilizar v\u00e1rios models preditivos para detectar se uma transa\u00e7\u00e3o \u00e9 **fraudulenta** ou **n\u00e3o fraudulenta**. Como descrito no conjunto de dados, as \"features\" s\u00e3o escalonadas e os nomes das \"features\" n\u00e3o s\u00e3o mostrados por raz\u00e3o de privacidade."}}