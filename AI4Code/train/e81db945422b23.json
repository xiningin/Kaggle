{"cell_type":{"1a28caa8":"code","02121cf8":"code","4a416552":"code","69f7d680":"code","925afe1f":"code","c52e436f":"code","043fea24":"code","ddcd418f":"markdown","003ea67f":"markdown","aa7de390":"markdown","e31ca020":"markdown","cf546eda":"markdown","30289c4e":"markdown"},"source":{"1a28caa8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","02121cf8":"ratings=pd.read_csv('..\/input\/ratings.csv')\nbooks=pd.read_csv('..\/input\/books.csv')\nprint(ratings.head())\nbooks['elo']=1200.0\nprint(books.head())\n","4a416552":"from sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\n\nfrom scipy.sparse import coo_matrix, csr_matrix\ndef read_data(filename):\n    \"\"\" Reads in the last.fm dataset, and returns a tuple of a pandas dataframe\n    and a sparse matrix of song\/user\/playcount \"\"\"\n    # read in triples of user\/song\/playcount from the input dataset\n    data = pd.read_csv(filename,\n                             usecols=[0,1,2],        #[36, 11, 10] vrk_pat_primkey,prd_atc_primkey,vdp_aantal\n                             names=['song', 'user','plays'],skiprows=1) #[:1000000]   # user = patient, or prescriptionnr song=atc\n\n    data=data.dropna(axis=0, how='any')  #drop nan\n    data['plays']=data['plays']+1\n    print(data.head())\n    # map each song and user to a unique numeric value\n    data['user'] = data['user'].astype(\"category\")\n    data['song'] = data['song'].astype(\"category\")\n\n    # create a sparse matrix of all the users\/plays\n    plays = coo_matrix((data['plays'].astype(float),\n                       (data['song'].cat.codes.copy(),\n                        data['user'].cat.codes.copy())))\n    data['song_nr']=data['song'].cat.codes.copy()\n    return data, plays,data.groupby(['song_nr','song']).plays.sum(),data['user'].cat.codes.copy()\n\ndata,matrix,songsd,user=read_data('..\/input\/ratings.csv')\ndata.head()","69f7d680":"from sklearn.preprocessing import normalize\n\n\ndef cosine(plays):\n    normalized = normalize(plays)\n    return normalized.dot(normalized.T)\n\n\ndef bhattacharya(plays):\n    plays.data = np.sqrt(plays.data)\n    return cosine(plays)\n\n\ndef ochiai(plays):\n    plays = csr_matrix(plays)\n    plays.data = np.ones(len(plays.data))\n    return cosine(plays)\n\n\ndef bm25_weight(data, K1=1.2, B=0.8):\n    \"\"\" Weighs each row of the matrix data by BM25 weighting \"\"\"\n    # calculate idf per term (user)\n    N = float(data.shape[0])\n    idf = np.log(N \/ (1 + np.bincount(data.col)))\n\n    # calculate length_norm per document (artist)\n    row_sums = np.squeeze(np.asarray(data.sum(1)))\n    average_length = row_sums.sum() \/ N\n    length_norm = (1.0 - B) + B * row_sums \/ average_length\n\n    # weight matrix rows by bm25\n    ret = coo_matrix(data)\n    ret.data = ret.data * (K1 + 1.0) \/ (K1 * length_norm[ret.row] + ret.data) * idf[ret.col]\n    return ret\n\n\ndef bm25(plays):\n    plays = bm25_weight(plays)\n    return plays.dot(plays.T)\n\ndef get_largest(row, N=10):\n    if N >= row.nnz:\n        best = zip(row.data, row.indices)\n    else:\n        ind = np.argpartition(row.data, -N)[-N:]\n        best = zip(row.data[ind], row.indices[ind])\n    return sorted(best, reverse=True)\n\n\ndef calculate_similar_artists(similarity, artists, artistid):\n    neighbours = similarity[artistid]\n    top = get_largest(neighbours)\n    return [(artists[other], score, i) for i, (score, other) in enumerate(top)]\n\n\nsongsd = dict(enumerate(data['song'].cat.categories))\nuser_count = data.groupby('song').size()\nto_generate = sorted(list(songsd), key=lambda x: -user_count[x])\n\nsimilarity = bm25(matrix)","925afe1f":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=50, n_iter=7, random_state=42)\nXr=svd.fit_transform(bm25(matrix))  \nprint(svd.explained_variance_ratio_)  \nprint(svd.explained_variance_ratio_.sum())","c52e436f":"from sklearn.metrics.pairwise import cosine_similarity\nUdf=pd.DataFrame(cosine_similarity(Xr))","043fea24":"booknr=4536\nprint(Udf[booknr].sort_values(ascending=False)[:10])\nbooks[books['id'].isin(Udf[booknr].sort_values(ascending=False)[:10].index )]","ddcd418f":"Normalize\n--","003ea67f":"SVD\n--\nthrow away noise","aa7de390":"Top recommended book\n--","e31ca020":"Cosine Similarity\n--","cf546eda":"import in sparse matrix\n--","30289c4e":"Improving this selection recommender;.\n--\nhttps:\/\/www.kaggle.com\/philippsp\/book-recommender-collaborative-filtering-shiny\n\nhttps:\/\/philippsp.shinyapps.io\/BookRecommendation\/\nimpressed by the interface, i have to say **i am not impressed by the selection result**s...\ni think this recommender does the better job. its faster and better\n"}}