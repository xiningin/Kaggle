{"cell_type":{"b8f55694":"code","80a436ba":"code","837f059b":"code","93c8e5d0":"code","b43452a8":"code","48f2943b":"code","6d335b63":"code","21935952":"code","5aea60c3":"code","ffa70ba9":"code","20d999a6":"code","d405aa08":"markdown","2d35014c":"markdown","4c72912c":"markdown","897ba793":"markdown","c5a58949":"markdown","c3641929":"markdown","9c5aa7fd":"markdown"},"source":{"b8f55694":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport sys\nimport pickle\nfrom sklearn import preprocessing\nfrom time import time\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n# from sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport sys\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsys.path.append('\/kaggle\/input\/my-input\/')\nfrom feature_format import featureFormat\nfrom feature_format import targetFeatureSplit\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80a436ba":"original = \"\/kaggle\/input\/my-input\/final_project_dataset.pkl\"\ndestination = \"final_project_dataset_unix.pkl\"\n\ncontent = ''\noutsize = 0\nwith open(original, 'rb') as infile:\n    content = infile.read()\nwith open(destination, 'wb') as output:\n    for line in content.splitlines():\n        outsize += len(line) + 1\n        output.write(line + str.encode('\\n'))","837f059b":"data_dict = pickle.load(open(\"\/kaggle\/input\/unixfile\/final_project_dataset_unix.pkl\", 'rb') )\n\ndata_dict.pop('TOTAL')\nfbase = featureFormat(data_dict, ['poi','salary','total_payments'])\n# featureFormat()\nprint(\"Strength:\",len(data_dict))\nprint(list(data_dict.keys())[0],\"\\n\",data_dict[list(data_dict.keys())[0]])\n\nfor key in range(len(fbase)):\n    if fbase[key][1]<1000000:\n        if fbase[key][0]==True:\n            plt.scatter(fbase[key][1],fbase[key][2],color = 'b')\n        else:\n            plt.scatter(fbase[key][1],fbase[key][2],color = 'r')\n            \nplt.ylabel('tot_payments')\nplt.xlabel('salary')   \nplt.show()\n            ","93c8e5d0":"f1 = featureFormat(data_dict,['poi','salary','bonus'])\nf2 = featureFormat(data_dict,['poi','salary','total_payments','deferral_payments'])\nf3 = featureFormat(data_dict,['poi','salary','total_payments','deferred_income'])\nf4 =  featureFormat(data_dict,['poi','from_this_person_to_poi','from_poi_to_this_person'])\nfig,a =  plt.subplots(2,2,squeeze=False,figsize=(17,10))\na[0][1].set(ylim=(0, 20000000))\na[1][0].set(ylim=(0, 20000000))\na[1][1].set(xlim=(0,210), ylim=(0,320))\nfor key in range(len(f1)):\n    if f1[key][0] == True:\n         a[0][0].scatter(f1[key][1],f1[key][2],color = 'b')\n    else:\n        a[0][0].scatter(f1[key][1],f2[key][2],color = 'r')\nx = np.arange(0,1000000,0.1)\n# a[0][0].ylabel('bonus')\n# a[0][0].xlabel('salary')  \na[0][0].plot(x,x**1.07,'b')\n\ncount = 0\nsubcount = 0\nfor key in range(len(f2)):\n    if f2[key][0] == True:\n        count+=1\n        if f2[key][3]==0:\n             a[0][1].plot(f2[key][1],f2[key][2],'b^')\n             subcount+=1\n        else:\n            a[0][1].plot(f2[key][1],f2[key][2],'bo')\n    else:\n        if f2[key][3]==0:\n            a[0][1].plot(f2[key][1],f2[key][2],'r^')\n        else:\n            a[0][1].plot(f2[key][1],f2[key][2],'ro')\n\ncount1  = 0\nsubcount1 = 0\nfor key in range(len(f3)):\n    if f3[key][0] == True:\n        count1+=1\n        if f3[key][3]==0:\n             a[1][0].plot(f3[key][1],f3[key][2],'b^')\n             subcount1+=1\n        else:\n            a[1][0].plot(f3[key][1],f3[key][2],'bo')\n    else:\n        if f3[key][3]==0:\n            a[1][0].plot(f3[key][1],f3[key][2],'r^')\n        else:\n            a[1][0].plot(f3[key][1],f3[key][2],'ro')\n\nfor key in range(len(f4)):\n    if f4[key][0] == True:\n         a[1][1].scatter(f4[key][1],f4[key][2],color = 'b')\n    else:\n        a[1][1].scatter(f4[key][1],f4[key][2],color = 'r')\n\nprint(subcount,\" out of \",count,\" poi have deffereal payments 0\",\" & \\n\",subcount1,\" out of \",count1,\" poi have deffered income 0\")\nplt.show()","b43452a8":"def listmaker(key,normalizer):\n    new_list=[]\n\n    for i in data_dict:\n        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n            new_list.append(0.)\n        elif data_dict[i][key]>=0:\n            new_list.append(float(data_dict[i][key])\/float(data_dict[i][normalizer]))\n    return new_list\n\nfraction_from_poi_email=listmaker(\"from_poi_to_this_person\",\"to_messages\")\nfraction_to_poi_email=listmaker(\"from_this_person_to_poi\",\"from_messages\")\nj = 0\nfor i in data_dict:\n    data_dict[i][\"fraction_from_poi_email\"]=fraction_from_poi_email[j]\n    data_dict[i][\"fraction_to_poi_email\"]=fraction_to_poi_email[j]\n    j+=1\n    \nfeatures_list = ['poi','shared_receipt_with_poi','fraction_from_poi_email','fraction_to_poi_email']\nfeatures_list2 = ['poi','shared_receipt_with_poi','fraction_from_poi_email','fraction_to_poi_email','deferral_payments']\ndata = featureFormat(data_dict, features_list2)\nvalue , features = targetFeatureSplit(data)\n\nfor key in range(len(data)):\n    if fbase[key][0]==True:\n        plt.scatter(data[key][2],data[key][3],color = 'b')\n    else:\n        plt.scatter(data[key][2],data[key][3],color = 'r')\n# plt.scatter(abc[:][0],abc[:][1])\n\nplt.ylabel('fraction_to_poi')\nplt.xlabel('fraction_from_poi')   \nplt.show()\nX_train, X_test, Y_train, Y_test = train_test_split(features, value, test_size = 0.1)    ","48f2943b":"from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV\n\nfrom sklearn.metrics import f1_score\n\nclf = xgboost.XGBClassifier()\nreg = LogisticRegression()\nreg2 = svm.SVC()\nreg3 = RandomForestClassifier(max_depth=10, criterion = 'entropy')\nmodel = AdaBoostClassifier(n_estimators=100,base_estimator = reg3)\n# reg3.fit(X_train,Y_train)\n# print(accuracy_score(Y_test, model.predict(X_test)))\n# model.predict(X_test)","6d335b63":"import warnings\nwarnings.filterwarnings('ignore')\n\nmodel.fit(X_train,Y_train)\nskf = StratifiedKFold(n_splits = 8)\nf1sc = np.array(features)\nv1 = np.array(value)\nacc = []\nfor train_index, test_index in skf.split(f1sc, v1):\n    X1_train, X1_test = f1sc[train_index], f1sc[test_index]\n    Y1_train, Y1_test = v1[train_index], v1[test_index]\n    model.fit(X1_train,Y1_train)\n    acc.append(accuracy_score(Y1_test,model.predict(X1_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y1_test),np.array(model.predict(X1_test)))[311:315]))\nprint(acc)","21935952":"parameters = {\n    'learning_rate' : [0.2 , 0.3 , 0.5 , 0.6],\n    'gamma' : [0, 0.2, 0.5, 0.8],\n    'max_depth' : [3, 4, 6, 8, 10],\n    'min_child_weight' : [0.5, 1, 2, 4, 6],\n}\n\nrnd_search = RandomizedSearchCV(clf,param_distributions=parameters,cv = 8,n_jobs = -1,scoring = 'accuracy')\nrnd_search.fit(f1sc, v1)\nclf2 = rnd_search.best_estimator_\naccuracy2 = []\ni = 0\nfor train_index, test_index in skf.split(f1sc, v1):\n    X1_train, X1_test = f1sc[train_index], f1sc[test_index]\n    Y1_train, Y1_test = v1[train_index], v1[test_index]\n    clf2.fit(X1_train,Y1_train)\n    accuracy2.append(accuracy_score(Y1_test,clf2.predict(X1_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y1_test),np.array(clf2.predict(X1_test)))[311:315]))\n    \nprint(accuracy2)","5aea60c3":"param = {\n    'n_estimators' : [80 , 100 , 120 , 150],\n    'max_depth' : [3, 4, 6, 8, 10],\n    'min_samples_split' : [2, 3]\n}\nrnd = RandomizedSearchCV(reg3,param_distributions=param,cv = 8,n_jobs = -1,scoring = 'f1')\nrnd.fit(f1sc,v1)\nreg3_ = rnd.best_estimator_\n\naccuracy3 = []\nfor train_index, test_index in skf.split(f1sc, v1):\n    X2_train, X2_test = f1sc[train_index], f1sc[test_index]\n    Y2_train, Y2_test = v1[train_index], v1[test_index]\n    reg3_.fit(X2_train,Y2_train)\n    accuracy3.append(accuracy_score(Y2_test,reg3_.predict(X2_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y2_test),np.array(reg3_.predict(X2_test)))[311:315]))\n    \nprint(accuracy3)","ffa70ba9":"accuracy4 = []\nfor train_index, test_index in skf.split(f1sc, v1):\n    X3_train, X3_test = f1sc[train_index], f1sc[test_index]\n    Y3_train, Y3_test = v1[train_index], v1[test_index]\n    reg2.fit(X3_train,Y3_train)\n    accuracy4.append(accuracy_score(Y3_test,reg2.predict(X3_test)))\n    print('weighted f1-score:',float(classification_report(np.array(Y3_test),np.array(reg2.predict(X3_test)))[311:315]))\n    \nprint(accuracy4)","20d999a6":"pickle.dump(clf2, open(\"best_classifier.pkl\", \"wb\") )\npickle.dump(data_dict, open(\"dataset_out.pkl\", \"wb\") )\npickle.dump(features_list2, open(\"final_features.pkl\", \"wb\") )","d405aa08":"We conclude with the best classifier being XGboost giving accuracy ~ 0.92","2d35014c":"Some plots depicting various relations","4c72912c":"Introducing various classifiers and regressions ","897ba793":"Adding modified features","c5a58949":"Eliminating *Total* Column","c3641929":"Importing the self uploaded pkl file","9c5aa7fd":"Since I could not find some of the files in the project directory,I loaded them separately."}}