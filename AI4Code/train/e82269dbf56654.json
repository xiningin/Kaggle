{"cell_type":{"31716f1c":"code","fb090cd1":"code","24ad799e":"code","1830bbd6":"code","8dd412ff":"code","d59d51bb":"code","c15f68ee":"code","4f7dfe67":"code","dd33ad54":"code","59dd0665":"code","fc396e39":"code","baf742ad":"code","3c89358b":"code","3ce369db":"code","192629de":"code","575bf3fb":"code","3bda0069":"code","641848d2":"code","b0e5cf5a":"code","1ab0a99f":"code","e7320aef":"markdown","afade87c":"markdown","7604bdce":"markdown","6d5323dd":"markdown","0e6ce450":"markdown","1a7808f8":"markdown","b261d0ed":"markdown","6bb4daed":"markdown","6fe7d381":"markdown","3602479f":"markdown","70b19a6a":"markdown","02dc75f7":"markdown"},"source":{"31716f1c":"# Packages for data analysis \nimport pandas as pd\nimport numpy as np\n\n# Packages for visualisation \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Packages for machine learning\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Others\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fb090cd1":"heart = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\nheart.head()","24ad799e":"heart.info()","1830bbd6":"# Extract numerical columns\nnum_cols=heart.select_dtypes(include=np.number).columns.tolist()\nprint('There are', len(num_cols), 'numerical features, including:')\nprint(num_cols, \"\\n\")\n\n# Extract categorical features\ncat_cols=heart.select_dtypes(object).columns.tolist()\nprint('There are', len(cat_cols), 'categorical features, including:')\nprint(cat_cols)","8dd412ff":"heart.isnull().sum()","d59d51bb":"print('There are', len(cat_cols), 'categorical features, including:', \"\\n\", cat_cols, '\\n')\n# Extract details on categorical features\nfor i in cat_cols:\n    unique_no = heart[i].nunique()\n    unique_name = heart[i].unique().tolist()\n    print(i, 'has', unique_no, 'unique variables, including:')\n    print(unique_name, \"\\n\")","c15f68ee":"# Summary of categorical data\npalette = ['#8abbd0', '#FB9851', '#36E2BD','#D0E1E1']\n\nfor feature in cat_cols:\n    fig, ax = plt.subplots(1,3, figsize=(15,3))\n    fig.patch.set_facecolor('#F2F2F2')\n\n    sns.countplot(x=heart[feature], data=heart, ax=ax[0], palette=palette, alpha=0.8)\n    for p, label in zip(ax[0].patches, heart[feature].value_counts().index):\n        ax[0].annotate(p.get_height(), (p.get_x()+p.get_width()\/3, p.get_height()*1.03))\n#    ax[0].spines['top'].set_visible(False)\n#    ax[0].spines['right'].set_visible(False)\n                    \n    heart[feature].value_counts().plot.pie(autopct='%1.1f%%', startangle = 90, ax=ax[1], colors=palette, frame=True)\n    ax[1].set_ylabel('')\n    ax[1].set_title(feature)\n\n    sns.histplot(x=feature,data=heart, hue='HeartDisease',ax=ax[2], alpha=0.3, shrink=.8)  \n    \nplt.tight_layout","4f7dfe67":"# Summary of categorical data\npalette = ['#8abbd0', '#FB9851', '#36E2BD','#D0E1E1']","dd33ad54":"heart.describe()","59dd0665":"fig = plt.figure(figsize=(6,4)) \ncorr = heart.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(heart.corr(), cmap='Blues', annot=True, mask=mask, linewidth=0.5)","fc396e39":"# Plots on numerical features to check data quality and data distribution\ncolor ='#8abbd0'\n\nfor i in num_cols:\n    fig, ax = plt.subplots(1,4, figsize=(15,3))\n    fig.patch.set_facecolor( '#F2F2F2')\n    sns.histplot(heart[i], bins=10, ax=ax[0],  color=color, kde=True)\n    ax[0].lines[0].set_color('#F97A1F')\n    sns.kdeplot(x=i,data=heart, hue='HeartDisease',ax=ax[1],shade=True, alpha=0.3)\n    sns.boxplot(x=i, data=heart,ax=ax[2], color=color)\n    sns.boxplot(x=i, data=heart, hue='HeartDisease',y=[\"\"]*len(heart),ax=ax[3],palette=['#8abbd0','#F97A1F'],boxprops=dict(alpha=.3))\n    plt.tight_layout","baf742ad":"# Remove outlier\nrow = heart[heart['RestingBP']==0].index\nheart = heart.drop(heart.index[row])\n\n# Impute zero values with median\nmedian_values = heart['Cholesterol'].median()\nrow = heart[heart['Cholesterol']==0].index\nheart.loc[row, 'Cholesterol'] = median_values\n\n# Adjust feature sizes\nnum_features = num_cols[:-1]\nscaler = StandardScaler()\nscaler.fit(heart[num_features])\n\n# Convert categorical values to numerical values\nle = LabelEncoder()\n#heart[cat_cols] = heart[cat_cols].astype('str').apply(le.fit_transform)\nheart[cat_cols] = heart[cat_cols].apply(le.fit_transform)","3c89358b":"X = heart.iloc[:,:-1]\ny=heart.iloc[:,-1]\n\n# Split train test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","3ce369db":"# Function to determine model performance based on various metrics\ndef model_performance(clf, X_test, y_test, y_pred):\n    \n    # Calculate the accuracy of the model\n    score = clf.score(X_test, y_test).round(3)*100 \n    \n    # Calculate the precision score of the model\n    precision = precision_score(y_test, y_pred).round(3)*100\n    \n     # Calculate the recall score of the model\n    recall = recall_score(y_test, y_pred).round(3)*100\n    \n    # Calculate the F1 score of the model\n    f1 = f1_score(y_test, y_pred).round(3)*100\n    \n    # Calculaate the ROC_AUC score\n    try:\n        y_pred_prob = clf.predict_proba(X_test) [:,1]\n    except:\n        roc = 'N.A'        \n    else: \n        roc = roc_auc_score(y_test, y_pred_prob).round(3)*100 \n    \n    results = [name, score, precision, recall, f1, roc]\n        \n    return results","192629de":"# Function to determine feature importance for tree-based models\ndef feature_importance(name, clf):\n    importances =clf.feature_importances_\n    importance_table = np.array([[name], importances])\n    importance_table=[element for sublist in importance_table for element in sublist]\n    return importance_table","575bf3fb":"# Function to determine feature importance for linear machine learning algorithms\ndef feature_coefficient(name, clf):\n    importances = clf.coef_[-1]\n    importance_table = np.array([[name], importances])\n    importance_table=[element for sublist in importance_table for element in sublist]\n    return importance_table  ","3bda0069":"# Select Classifiers and inistaniate models\n\nclfs = [  \n            ('KNN', KNeighborsClassifier()),\n            ('LG', LogisticRegression(solver='liblinear')),\n            ('Linear SVC', LinearSVC()),\n            ('DT', DecisionTreeClassifier()),\n            ('RF', RandomForestClassifier()),\n            ('GNB', GaussianNB())\n            ]  \n       \nresults = []\nimportance_tables = []\nconfusion_tables = []\n\nfor name, clf in  clfs:\n\n    # Train models\n    clf.fit(X_train, y_train)  \n    # Predict target value     \n    y_pred = clf.predict(X_test)      \n    \n    # Result table to show how different model performs\n    result = model_performance(clf, X_test, y_test, y_pred)        \n    results.append(result)\n    \n    # Result to show importance of different features\n    if name == 'RF' or name =='DT':\n        importance = feature_importance(name, clf)\n        importance_tables.append(importance)\n    if name == 'LG' or name =='Linear SVC':\n        importance = feature_coefficient(name, clf)\n        importance_tables.append(importance)\n        \n    # Determine confusion matrix for different models\n    cm = confusion_matrix(y_test, y_pred)\n    confusion_tables.append([name,cm]) ","641848d2":"# Print result table to show how different model performs\nresults = pd.DataFrame(results, columns=['Model', 'Accuracy','Precision', 'Recall', 'F1', 'ROC_AUC'])\nresults = results.sort_values(by='Accuracy', ascending = False)\n#results.style.set_caption('Performance comparison for different classifiers')\npd.options.display.float_format = '{:,.1f}'.format\nresults.set_index('Model')","b0e5cf5a":"# Plot confusion matrix for different models\nfig = plt.figure(figsize=(15,8))\nfig.patch.set_facecolor('#F2F2F2')\n\nfor i in range(len(confusion_tables)):\n    ax = plt.subplot(2,3, i+1)\n    model_name = confusion_tables[i][0]\n    ax = sns.heatmap(confusion_tables[i][1], cmap='Blues', annot=True, fmt='d', square=True)\n    ax.set_ylabel('True label')\n    ax.set_xlabel('Predicted label')\n    ax.set_title(model_name)\n    for _, spine in ax.spines.items():\n        spine.set_visible(True)\nplt.suptitle('Confusion matrix for different models', \n                     fontfamily='serif', fontsize=20, color='#173b56', fontweight='bold')\nplt.tight_layout(pad=2)","1ab0a99f":"# Plot feature importance for different models\ncols_name = [['Model'], X.columns]\ncols_name=[element for sublist in cols_name for element in sublist]\nimportance_tables = pd.DataFrame(importance_tables, columns= cols_name)\nimportance_tables1 = importance_tables.set_index('Model')\n    \nfig = plt.figure(figsize=(15,6))\nfig.patch.set_facecolor('#F2F2F2')\n\nfor i, (col_name, row) in enumerate(importance_tables1.iterrows()):\n    indices = np.argsort(row)\n    ax = plt.subplot(1,4, i+1)\n    ax.barh(range(len(row)), row[indices], color='#8abbd0')\n    ax.set_yticks(range(len(row)))\n    ax.set_yticklabels(row.index[indices])    \n    ax.set_title(col_name)\n   \nplt.suptitle('Feature Importance for different models', \n                     fontfamily='serif', fontsize=20, color='#173b56', fontweight='bold')\nplt.tight_layout()","e7320aef":"#### Comment\nHigher chance of getting heart disease if:\n- `Age` > 55\n- `FastingBS` == Yes\n- `MaxHR` is low\n- `Oldpeak` is high\n\nNote that:\n- `RestingBS` has extreme outlier\n- `Cholesterol` has many zero values, we will need to fix them in ML section","afade87c":"## <font color='#2E45B8'>3.2 Model prediction and evaluation<\/font>","7604bdce":"#### Comment\n- There are 7 numerical columns, including:\n    - ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak', 'HeartDisease'] \n\n- There are 5 categorical features, including:\n    - ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']","6d5323dd":"#### Comment\n- Low correlation between each feature","0e6ce450":"#### Comment\nHigher chance of getting heart disease if:\n- `gender` ==  male\n- `chest pain type` == ASY\n- `ExcerciseAngina` == Y\n- `ST_slope` == Flat","1a7808f8":"#### Comment\n- The dataset contains 12 columns, including 11 features and 1 target variable\n- There are 5 objects, 1 float64 and 6 int64 in the dataset","b261d0ed":"# <font color='\t#2E45B8'>1. Data Preparation<\/font>\n## <font color='#2E45B8'>1.1 Import libraries and data<\/font>","6bb4daed":"## <font color='#2E45B8'>2.2 Numerical Features<\/font>","6fe7d381":"### <font color='#2E45B8'>1.2.2 Data quality<\/font>","3602479f":"# <font color='\t#2E45B8'>3. Machine Learning<\/font>\n## <font color='#2E45B8'>3.1 Data preprocessing<\/font>\n\nIn this section, we will preprocess the data before feeding data into training models.\n- Remove outlier from dataset\n    - Looking at the boxplot above, we notice that `RestingBP`  has extreme outlier at 0. This is probably wrong value, we will remove it from dataset. \n- Impute missing values \n    - `Cholesterol` has a few zero values, we will replace them with median\n- Scaling features: \n    - In the EDA section, we see that there is significant difference in order of magnitude between numerical features. Therefore, I will use StandardScaler to adjust feature sizes\n- Convert categorical values to numerical values: \n    - I will use LabelEncoder to transform the data","70b19a6a":"# <font color='\t#2E45B8'>2. EDA<\/font>\n## <font color='#2E45B8'>2.1 Categorical Features<\/font>","02dc75f7":"## <font color='#2E45B8'>1.2 Data inspection<\/font>\n### <font color='#2E45B8'>1.2.1 Data types<\/font>"}}