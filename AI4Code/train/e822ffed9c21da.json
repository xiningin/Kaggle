{"cell_type":{"9806a017":"code","ce37b2ea":"code","1e2d3e9e":"code","641abc01":"code","f74927de":"code","97a1fa96":"code","0e227474":"code","949e9d5b":"code","225675f1":"code","4ff2e2ac":"code","f9d2a9de":"code","d42aa692":"code","315cd1f2":"code","d6565bf9":"code","3259cad4":"code","91aaab46":"code","34f6489b":"code","ea055c2b":"code","4347950c":"markdown","11d671ef":"markdown","b8e52957":"markdown","e0a6c352":"markdown","52bcd0f4":"markdown","72b0ff8d":"markdown","f68d4b0e":"markdown","cfe8b2d5":"markdown","e2b2c671":"markdown","a59e41d2":"markdown","6008fd7b":"markdown","d0cd68e3":"markdown","f45b4fdf":"markdown","c34b0b58":"markdown","d8192b53":"markdown","6ef5d752":"markdown","494367f6":"markdown","ff9ec323":"markdown","509a2c06":"markdown","c1b36502":"markdown","f233552f":"markdown","dffea21c":"markdown","846b5bef":"markdown"},"source":{"9806a017":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,sys\n\nimport pydicom\nimport tensorflow as tf\n\nfrom sklearn.utils import shuffle\nfrom scipy.ndimage import imread\nfrom scipy.misc import imresize\n\nfrom glob import glob\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","ce37b2ea":"cxr_paths = glob(os.path.join('..', 'input', 'pulmonary-chest-xray-abnormalities','Montgomery', 'MontgomerySet', '*', '*.png'))\nprint(\"There are {} images in the dataset\".format(len(cxr_paths)))","1e2d3e9e":"cxr_images = [(c_path, \n               [os.path.join('\/'.join(c_path.split('\/')[:-2]),'ManualMask','leftMask', os.path.basename(c_path)),\n               os.path.join('\/'.join(c_path.split('\/')[:-2]),'ManualMask','rightMask', os.path.basename(c_path))]\n              ) for c_path in cxr_paths]","641abc01":"cxr_images[0]","f74927de":"from skimage.io import imread as imread_raw\nfrom skimage.transform import resize\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage') # skimage is really annoying","97a1fa96":"def imread(in_path):\n    OUT_DIM = (512, 512)\n    \n    # use the skimge function to read the file specified in the path\n    img_data = imread_raw(in_path)\n    \n    # make sure the image data is between the range 0-255 and convert the variable into uint8\n    n_img = (255*resize(img_data, OUT_DIM, mode = 'constant')).clip(0,255).astype(np.uint8)\n        \n    return np.expand_dims(n_img, -1)","0e227474":"# init empty array for images and masks or in this case segmentations\nimg_vol, seg_vol = [], []\n\nfor img_path, s_paths in tqdm(cxr_images):\n    # first read the image paths\n    img_vol += [imread(img_path)]    \n    \n    # read both images, stack them up, then store them    \n    seg_vol += [np.max(np.stack([imread(s_path) for s_path in s_paths],0),0)]\n    \nimg_vol = np.stack(img_vol,0)\nseg_vol = np.stack(seg_vol,0)\n\nprint('Images', img_vol.shape, 'Segmentations', seg_vol.shape)","949e9d5b":"# Get a random patient\nnp.random.seed(64)\n\nrandomPatient = int(np.random.rand()*138)\nt_img, m_img = img_vol[randomPatient], seg_vol[randomPatient]\n\n#plot it\nscan = t_img[:,:,0]\nmask = m_img[:,:,0]\nsegmented = scan*mask\n\ndef drawImage(ax_img,img,label):\n    ax_img.imshow(img,interpolation='none',cmap='bone')\n    ax_img.set_title(label)\n    ax_img.set_axis_off()\n    \n\nfig, (ax_img, ax_mask,ax_segmentedImage) = plt.subplots(1,3, figsize = (12, 6))\ndrawImage(ax_img,scan,label='CXR')\ndrawImage(ax_mask,mask,label='Labeled Mask')\ndrawImage(ax_segmentedImage,segmented,label='Segmented Image')","225675f1":"tf.set_random_seed(6464)","4ff2e2ac":"def tf_relu(x): \n    return tf.nn.relu(x)\n\ndef d_tf_relu(s): \n    return tf.cast(tf.greater(s,0),dtype=tf.float32)\n\ndef tf_softmax(x): \n    return tf.nn.softmax(x)\n\ndef np_sigmoid(x): \n    1\/(1 + np.exp(-1 *x))","f9d2a9de":"class conlayer_left():\n    \n    def __init__(self,ker,in_c,out_c):\n        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n\n    def feedforward(self,input,stride=1,dilate=1):\n        self.input  = input\n        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n        self.layerA = tf_relu(self.layer)\n        return self.layerA\n\nclass conlayer_right():\n    \n    def __init__(self,ker,in_c,out_c):\n        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n\n    def feedforward(self,input,stride=1,dilate=1,output=1):\n        self.input  = input\n\n        current_shape_size = input.shape\n\n        self.layer = tf.nn.conv2d_transpose(input,self.w,\n        output_shape=[batch_size] + [int(current_shape_size[1].value*2),int(current_shape_size[2].value*2),int(current_shape_size[3].value\/2)],strides=[1,2,2,1],padding='SAME')\n        self.layerA = tf_relu(self.layer)\n        return self.layerA","d42aa692":"train_images = (img_vol - img_vol.min()) \/ (img_vol.max() - img_vol.min())\ntrain_labels = (seg_vol - seg_vol.min()) \/ (seg_vol.max() - seg_vol.min())\n\nprint('Images', img_vol.shape, 'Segmentations', seg_vol.shape)","315cd1f2":"num_epoch = 100\ninit_lr = 0.0001\nbatch_size = 2","d6565bf9":"l1_1 = conlayer_left(3,1,3)\nl1_2 = conlayer_left(3,3,3)\nl1_3 = conlayer_left(3,3,3)\n\nl2_1 = conlayer_left(3,3,6)\nl2_2 = conlayer_left(3,6,6)\nl2_3 = conlayer_left(3,6,6)\n\nl3_1 = conlayer_left(3,6,12)\nl3_2 = conlayer_left(3,12,12)\nl3_3 = conlayer_left(3,12,12)\n\nl4_1 = conlayer_left(3,12,24)\nl4_2 = conlayer_left(3,24,24)\nl4_3 = conlayer_left(3,24,24)\n\nl5_1 = conlayer_left(3,24,48)\nl5_2 = conlayer_left(3,48,48)\nl5_3 = conlayer_left(3,48,24)","3259cad4":"# right\nl6_1 = conlayer_right(3,24,48)\nl6_2 = conlayer_left(3,24,24)\nl6_3 = conlayer_left(3,24,12)\n\nl7_1 = conlayer_right(3,12,24)\nl7_2 = conlayer_left(3,12,12)\nl7_3 = conlayer_left(3,12,6)\n\nl8_1 = conlayer_right(3,6,12)\nl8_2 = conlayer_left(3,6,6)\nl8_3 = conlayer_left(3,6,3)\n\nl9_1 = conlayer_right(3,3,6)\nl9_2 = conlayer_left(3,3,3)\nl9_3 = conlayer_left(3,3,3)","91aaab46":"l10_final = conlayer_left(3,3,1)","34f6489b":"x = tf.placeholder(shape=[None,512,512,1],dtype=tf.float32)\ny = tf.placeholder(shape=[None,512,512,1],dtype=tf.float32)\n\nlayer1_1 = l1_1.feedforward(x)\nlayer1_2 = l1_2.feedforward(layer1_1)\nlayer1_3 = l1_3.feedforward(layer1_2)\n\nlayer2_Input = tf.nn.max_pool(layer1_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\nlayer2_1 = l2_1.feedforward(layer2_Input)\nlayer2_2 = l2_2.feedforward(layer2_1)\nlayer2_3 = l2_3.feedforward(layer2_2)\n\nlayer3_Input = tf.nn.max_pool(layer2_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\nlayer3_1 = l3_1.feedforward(layer3_Input)\nlayer3_2 = l3_2.feedforward(layer3_1)\nlayer3_3 = l3_3.feedforward(layer3_2)\n\nlayer4_Input = tf.nn.max_pool(layer3_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\nlayer4_1 = l4_1.feedforward(layer4_Input)\nlayer4_2 = l4_2.feedforward(layer4_1)\nlayer4_3 = l4_3.feedforward(layer4_2)\n\nlayer5_Input = tf.nn.max_pool(layer4_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\nlayer5_1 = l5_1.feedforward(layer5_Input)\nlayer5_2 = l5_2.feedforward(layer5_1)\nlayer5_3 = l5_3.feedforward(layer5_2)\n\nlayer6_Input = tf.concat([layer5_3,layer5_Input],axis=3)\nlayer6_1 = l6_1.feedforward(layer6_Input)\nlayer6_2 = l6_2.feedforward(layer6_1)\nlayer6_3 = l6_3.feedforward(layer6_2)\n\nlayer7_Input = tf.concat([layer6_3,layer4_Input],axis=3)\nlayer7_1 = l7_1.feedforward(layer7_Input)\nlayer7_2 = l7_2.feedforward(layer7_1)\nlayer7_3 = l7_3.feedforward(layer7_2)\n\nlayer8_Input = tf.concat([layer7_3,layer3_Input],axis=3)\nlayer8_1 = l8_1.feedforward(layer8_Input)\nlayer8_2 = l8_2.feedforward(layer8_1)\nlayer8_3 = l8_3.feedforward(layer8_2)\n\nlayer9_Input = tf.concat([layer8_3,layer2_Input],axis=3)\nlayer9_1 = l9_1.feedforward(layer9_Input)\nlayer9_2 = l9_2.feedforward(layer9_1)\nlayer9_3 = l9_3.feedforward(layer9_2)\n\nlayer10 = l10_final.feedforward(layer9_3)\n\ncost = tf.reduce_mean(tf.square(layer10-y))\nauto_train = tf.train.AdamOptimizer(learning_rate=init_lr).minimize(cost)","ea055c2b":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for iter in range(num_epoch):\n        \n        # train\n        for current_batch_index in range(0,len(train_images),batch_size):\n            current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n            current_label = train_labels[current_batch_index:current_batch_index+batch_size,:,:,:]\n            sess_results = sess.run([cost,auto_train],feed_dict={x:current_batch,y:current_label})\n            print(' Iter: ', iter, \" Cost:  %.32f\"% sess_results[0],end='\\r')\n        print('\\n-----------------------')\n        train_images,train_labels = shuffle(train_images,train_labels)\n\n        if iter % 10 == 0:\n            test_example =   train_images[:2,:,:,:]\n            test_example_gt = train_labels[:2,:,:,:]\n            sess_results = sess.run([layer10],feed_dict={x:test_example})\n\n            sess_results = sess_results[0][0,:,:,:]\n            test_example = test_example[0,:,:,:]\n            test_example_gt = test_example_gt[0,:,:,:]\n\n            plt.figure()\n            plt.imshow(np.squeeze(test_example),cmap='gray')\n            plt.axis('off')\n            plt.title('Original Image')  \n            plt.savefig(str(iter)+\"a_Original_Image.png\")\n\n            plt.figure()\n            plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n            plt.axis('off')\n            plt.title('Ground Truth Mask')  \n            plt.savefig(str(iter)+\"b_Original_Mask.png\")            \n\n            plt.figure()\n            plt.imshow(np.squeeze(sess_results),cmap='gray')\n            plt.axis('off')\n            plt.title('Generated Mask') \n            plt.savefig(str(iter)+\"c_Generated_Mask.png\")\n\n            plt.figure()\n            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(test_example_gt)),cmap='gray')\n            plt.axis('off')\n            plt.title(\"Ground Truth Overlay\")   \n            plt.savefig(str(iter)+\"d_Original_Image_Overlay.png\")\n\n            plt.figure()\n            plt.axis('off')\n            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(sess_results)),cmap='gray')\n            plt.title(\"Generated Overlay\")                 \n\n            plt.close('all')","4347950c":"## Generate architecture","11d671ef":"In this section we will use the [U-Net algorithm](https:\/\/arxiv.org\/abs\/1505.04597) for segmenting the lungs.\n\nFor some guidance we will use the following diagram from the article and also use the working sample by Jae Duk Seo in his [article](https:\/\/towardsdatascience.com\/medical-image-segmentation-part-1-unet-convolutional-networks-with-interactive-code-70f0f17f46c6).\n\nNote the colouration as they will be useful when generating the layers.\n\n![img2](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*aRMefObpm7AMVOZYYiQAMQ.png)  \n\n","b8e52957":"## Normalize the data","e0a6c352":"## Reading the Data\n\nLet us use the data uploaded by Kevin Mader [kernel](https:\/\/www.kaggle.com\/kmader\/gaussian-mixture-lung-segmentation) to train our segmentation U-Net algorithm.  It is noted that he has also done this but uses code that I cannot follow at the moment due to my lack of inexperiance but please refer to his guide.","52bcd0f4":"### Combining layer or bottle neck layer or Green in the above image","72b0ff8d":"## Function to read the images","f68d4b0e":"Obtain the masks for each 138 patients. ","cfe8b2d5":"## Generate Layers","e2b2c671":"An example patient in the list has the structures below.  \n\ncxr_images[ **#ofPatients**][**0 = Location of the Patient Original Scan**][** 0\/1 = Mask for the left and right lung respectively**]","a59e41d2":"### Right Side of the U-Net Architecture or Blue in the above image\n","6008fd7b":"## Hyper-Parameters","d0cd68e3":"As you can see from the above line there are 138 patients consisting of images with a 512x512 image and segmentation corresponding to the labeled lung segmentation.  Let us look at them side by side to get a better intuitive understading of the above images.","f45b4fdf":"These images are 512 x 512 images so let us create a global variable so that we can follow it.","c34b0b58":"# Imports ","d8192b53":"## Set up environment and utility functions","6ef5d752":"# Pre-Processing Step","494367f6":"### Generate Classes for Convolution Layers","ff9ec323":"Look at the ouputs to view the results of the U-Net alogirthm","509a2c06":"### Activation Functions","c1b36502":"## Mask Generation","f233552f":"## Reading Imports\n\nWe need more imports for reading the actual data","dffea21c":"### Left Side of th U-Net architecutre or Red in the above image","846b5bef":"# Introduction\n\nTo maximize our search I think it might be advantageous to perform some segmentation on the dataset before applying and algorithm to identify bounding boxes.  I know this might be tedious but coming from the classical ML approach where by features are extracted from the data domain and classification is done on the feature space.  I think this approach is far more intuitve for me at the moment.  Therefore, I propose the following steps:\n\n1. Pre-Processing\n    - Masking the image based on the distribution of bounding boxes\n2. Feature Extraction\n    - Extract histogram as a feature for each of the test cases\n3. Classification\n    - Use classification techniques to classify normal vs. ( opaque, non-opaque) cases\n \n **This notebook will only focus on the pre-processing aspects of this workflow using another dataset to generate the lung segmentation portion.  We can further retrain the model on our model and see if it has improved. **"}}