{"cell_type":{"ce2d4e10":"code","a4f54b89":"code","5584c55e":"code","4afab642":"code","39c26b8e":"code","54c93654":"code","ad9263dd":"code","2e983625":"code","bcc3f1d9":"code","f5738fc2":"code","4f5eba80":"code","f91a5f04":"code","ab461d6c":"code","92470481":"code","a1d9846f":"code","e3aad8e2":"code","b9137417":"code","bf2cc8d2":"code","ddfa3066":"code","f42ef799":"code","2c163f5b":"code","ca3b6bed":"code","71086cb3":"code","a122e694":"code","81ee1dd0":"code","b7782625":"code","71266c2f":"code","cdba7885":"code","9105b63f":"code","53618ecb":"code","9eae3c85":"code","f1993343":"markdown","73bd732d":"markdown","2580a59c":"markdown","94b614f9":"markdown","d0376542":"markdown","2477d820":"markdown","ebd92901":"markdown","9c536a6c":"markdown","27bc04fa":"markdown","7f18700a":"markdown","7e849ea9":"markdown","93426ed3":"markdown","a5b0a494":"markdown","e17a9a9e":"markdown","9eb99925":"markdown","1b6f76ad":"markdown","9f8a01f9":"markdown"},"source":{"ce2d4e10":"DEBUG = False","a4f54b89":"import sys\nsys.path.append('..\/input\/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport scipy.stats as stats\nfrom scipy.stats import kurtosis\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nfrom tqdm import tqdm, trange\nfrom pprint import pprint\nimport warnings\n#warnings.filterwarnings('ignore')\n\n!cp -r ..\/input\/pytorchtabnet\/tabnet\/* .\/\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nos.listdir('..\/input\/lish-moa')\n\n#pd.set_option('max_columns', 2000)","5584c55e":"from sklearn.metrics import log_loss as _log_loss\n\ndef log_loss(y_true, y_pred):\n    losses = []\n    for col in range(y.shape[1]):\n        losses.append(_log_loss(y_true[:,col], y_pred[:,col], eps=1e-15))\n    \n    return sum(losses) \/ y_true.shape[1]","4afab642":"n_comp = 80\nDropout_Model = 0.25\nQT_n_quantile_min=10, \nQT_n_quantile_max=200\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')","39c26b8e":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","54c93654":"assert (train_features['sig_id'] == train_targets_scored['sig_id']).all()\n\ntrain_features.drop('sig_id', axis=1, inplace=True)\ntrain_targets_scored.drop('sig_id', axis=1, inplace=True)\ntrain_targets_nonscored.drop('sig_id', axis=1, inplace=True)\ntest_features.drop('sig_id', axis=1, inplace=True)","ad9263dd":"train_cp = train_features['cp_type'] == 'trt_cp'\ntest_ctl = test_features['cp_type'] == 'ctl_vehicle'\n\nctl_df = sample_submission[test_ctl].iloc[:,1:]\nctl_df.values[:,:] = 0\n\n#train_features = train_features[train_cp].drop('cp_type', axis=1)\n#train_targets_scored = train_targets_scored[train_cp]\n#test_features = test_features[~test_ctl].drop('cp_type', axis=1)","2e983625":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nCATEGORICAL = ['cp_type', 'cp_time', 'cp_dose']","bcc3f1d9":"def encode_categorical(data):\n    data = pd.get_dummies(data, columns=CATEGORICAL)\n    return data\n\ntrain_features.encoded = encode_categorical(train_features)\ntest_features.encoded  = encode_categorical(test_features)","f5738fc2":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","4f5eba80":"# PCA\n\nprint(f\"Feature count: {len(train_features.columns)}\")\nall_features = pd.concat([train_features.encoded, test_features.encoded])\n\ndef reduce_dimensions(df, n_components, whiten=False): # randomized\n    '''Handles numerical columns. Thin wrapper for sklearn.decomposition.PCA().'''\n    pca = PCA(n_components=n_components)\n    \n    reduced = pca.fit_transform(df)\n    print(\"\\nPrincipal variances\\n-------------------\\n\" + str(pca.explained_variance_))\n    return reduced, pca.explained_variance_\n\ntemp_, variance = reduce_dimensions(all_features, n_components=130)\n\ntrain_features.reduced = temp_[:train_features.shape[0]]\ntest_features.reduced = temp_[-test_features.shape[0]:]","f91a5f04":"vt = VarianceThreshold(.9)\nall_features.threshold = vt.fit_transform(all_features.values)\ntrain_features.threshold = all_features.threshold[:train_features.shape[0]]\ntest_features.threshold = all_features.threshold[-test_features.shape[0]:]","ab461d6c":"train_features.enhanced = np.concatenate((train_features.threshold, train_features.reduced), axis=1)\ntest_features.enhanced = np.concatenate((test_features.threshold, test_features.reduced), axis=1)","92470481":"def get_folds(X, y, n_splits=7):\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits)\n    return mskf.split(X, y)","a1d9846f":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = torch.tensor(features, dtype=torch.float, device=DEVICE)\n        self.targets = torch.tensor(targets, dtype=torch.float, device=DEVICE)\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : self.features[idx],\n            'y' : self.targets[idx]         \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = torch.tensor(features, dtype=torch.float, device=DEVICE)\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : self.features[idx]\n        }\n        return dct    ","e3aad8e2":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","b9137417":"# HyperParameters\n\nEPOCHS = 200\nBATCH_SIZE = 1024\nLEARNING_RATE = .1e-2\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 100\nEARLY_STOP = True\nDropout_model = 0.7\n\nnum_features = train_features.reduced.shape[1]\nnum_targets = len(train_targets_scored.columns)\nhidden_size=200","bf2cc8d2":"class Chunk(nn.Module):\n    def __init__(self, in_size, out_size, bn=True, dropout=True):\n        super().__init__()\n        self.in_size = in_size\n        self.out_size = out_size\n        if bn:\n            self.bn = nn.BatchNorm1d(in_size)\n        else:\n            self.bn = nn.Identity()\n        if dropout:\n            self.dropout = nn.Dropout(Dropout_Model)\n        else:\n            self.dropout = nn.Identity()\n        self.dense = nn.Linear(in_size, out_size) #nn.utils.weight_norm\n    \n    def forward(self, x):\n        x = self.bn(x)\n        x = self.dropout(x)\n        x = self.dense(x)\n        \n        return x\n\nclass Bottleneck(nn.Module):\n    def __init__(self, size, squeeze_factor=4, **kwargs):\n        super().__init__()\n        self.size = size\n        self.bottleneck_size = size\/\/squeeze_factor\n        self.chunk1 = Chunk(size, self.bottleneck_size)\n        self.chunk2 = Chunk(self.bottleneck_size, size, dropout=False)\n    \n    def forward(self, x):\n        x_skip = x\n        x = F.leaky_relu(self.chunk1(x))\n        x = self.chunk2(x)\n        \n        return torch.cat([x, x_skip], dim=1) # out_size = size*2\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super().__init__()\n        self.chunk1 = Chunk(num_features, hidden_size)\n        self.chunk2 = Chunk(hidden_size, hidden_size)\n        self.chunk3 = Chunk(hidden_size, num_targets)\n    \n    def forward(self, x):\n        x = F.leaky_relu(self.chunk1(x))\n        x = F.leaky_relu(self.chunk2(x))\n        x = self.chunk3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","ddfa3066":"from pytorch_tabnet.tab_network import TabNet as _TabNet\n\nclass TabNet(_TabNet):\n    def forward(self, x):\n        return super().forward(x)[0]","f42ef799":"input_dim = train_features.encoded.shape[1]\noutput_dim = train_targets_scored.values.shape[1]\n\ndef get_tabnet(input_dim=input_dim, output_dim=output_dim):\n    model = TabNet(input_dim, output_dim,\n                   n_d=24, n_a=24,\n                   n_steps=1, n_independent=2, n_shared=0)\n    \n    model.to(DEVICE)\n    \n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss = loss_fn\n    \n    return model, optimizer, loss","2c163f5b":"def get_scheduler(optimizer, loader, epochs=EPOCHS):\n    return optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n                                         pct_start=0.1,\n                                         div_factor=40,\n                                         max_lr=optimizer.defaults['lr'],\n                                         epochs=epochs,\n                                         steps_per_epoch=len(loader))","ca3b6bed":"def get_model(num_features, num_targets, hidden_size):\n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    ).to(DEVICE)\n    \n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    loss = loss_fn\n    \n    return model, optimizer, loss\n\ndef get_dataloader(*args, batch_size=BATCH_SIZE, shuffle=False):\n    if len(args) == 1:\n        dataset = TestDataset(*args)\n    elif len(args) == 2:\n        dataset = MoADataset(*args)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    return loader","71086cb3":"def train_epoch(model, optimizer, scheduler, loss_fn, dataloader, device=DEVICE, output=None):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'], data['y']\n        if output is None:\n            outputs = model(inputs)\n        else:\n            outputs = model(inputs)[output]\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\n@torch.no_grad()\ndef valid_epoch(model, loss_fn, dataloader, device=DEVICE, output=None):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        if output is None:\n            outputs = model(inputs)\n        else:\n            outputs = model(inputs)[output]\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds","a122e694":"def train_session(model, train_data, valid_data, epochs=EPOCHS, scheduler=None, optimizer=None,\n                  loss=None, session_name=None, patience=EARLY_STOPPING_STEPS, output=None):\n    \n    early_step = 0\n    best_loss = np.inf\n    session_stats = {'Session': session_name}\n    \n    if not hasattr(model, 'saved_states'):\n        model.saved_states = []\n    \n    for epoch in range(epochs):\n        \n        train_loss = train_epoch(model, optimizer, scheduler, loss, train_data, output=output)\n        valid_loss, valid_preds = valid_epoch(model, loss, valid_data, output=output)\n        \n        session_stats.update(\n                                Epoch=epoch+1, \n                                train_loss=f\"{train_loss:.5f}\",\n                                valid_loss=f\"{valid_loss:.5f}\",\n                                lr=f\"{scheduler.get_last_lr()[0]:.2}\"\n                            )\n        \n        print(\", \".join(f\"{key}: {value}\" for key, value in session_stats.items()))\n        \n        if valid_loss < best_loss:\n            early_step -=1\n            best_loss = valid_loss\n            torch.save(model.state_dict(), f\"{session_name}.pt\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= patience):\n                break\n                \n    model.saved_states.append(f\"{session_name}.pt\")\n    \n# Averaging on folds\n\ndef train_kfold(model, optimizer, loss, \n                x, y, \n                initialization, \n                epochs=EPOCHS, \n                prefix=\"\", \n                batch_size=BATCH_SIZE,\n                patience=EARLY_STOPPING_STEPS):\n    \n    y_pred = np.array(y, dtype=np.float)\n    \n    for n, fold in enumerate(get_folds(x, y)):\n        \n        session_name = prefix + f\" Fold {n+1}\" if prefix else f\"Fold {n+1}\"\n        train_idx, valid_idx = fold\n        \n        trainloader = get_dataloader(x[train_idx], y[train_idx], shuffle=True, batch_size=batch_size)\n        validloader = get_dataloader(x[valid_idx], y[valid_idx], shuffle=False, batch_size=batch_size)\n        \n        \n        model.load_state_dict(initialization)\n#        model, optimizer, loss = get_model()\n#        loss_fn, loss_tr = loss\n        \n        scheduler = get_scheduler(optimizer, trainloader, epochs)\n        \n        train_session(model, trainloader, validloader, \n                      optimizer=optimizer, loss=loss,\n                      session_name=session_name,\n                      epochs=epochs,\n                      scheduler=scheduler,\n                      patience=patience)\n        \n        model.load_state_dict(torch.load(model.saved_states[-1]))\n        y_pred[valid_idx] = model(validloader.dataset.features).sigmoid().detach().cpu().numpy()\n        \n    return y_pred","81ee1dd0":"RUN_PRETRAINING=False\n\nif RUN_PRETRAINING:\n    x, y = train_features.encoded.values, train_targets_nonscored.values\n    \n    trainloader = get_dataloader(x, y, shuffle=True, batch_size=2048)\n    \n    pretrain_tabnet, optimizer, loss = get_tabnet(input_dim=x.shape[1], output_dim=y.shape[1])\n    optimizer.defaults['lr'] = .01\n    scheduler = get_scheduler(optimizer=optimizer, loader=trainloader, epochs=40)\n    \n    train_session(pretrain_tabnet, trainloader, trainloader,\n                  optimizer=optimizer, \n                  loss=loss, \n                  scheduler=scheduler,\n                  session_name=\"pretrain\",\n                  epochs=40,\n                  patience=1)\n    \n    pretrain = torch.load(\"pretrain.pt\")\n    pretrain.pop(\"tabnet.final_mapping.weight\")\n    torch.save(pretrain, \"pretrain.pt\")","b7782625":"LOAD_PRETRAIN = False\nFREEZE_LAYERS = False\n\ntabnet, optimizer, loss = get_tabnet(input_dim=train_features.encoded.shape[1])\n\ntorch.save(tabnet.state_dict(), \"init.pt\")\ninitialization = torch.load(\"init.pt\")\n\nif LOAD_PRETRAIN:\n    pretrain = torch.load(\"pretrain.pt\")\n    #pretrain = torch.load(\"..\/input\/tabnet-pretrain\/pretrain.pt\")\n    \n    for key in pretrain.keys():\n        initialization[key] = pretrain[key]\n\nif FREEZE_LAYERS:\n    modules = dict(tabnet.tabnet.named_modules())\n    unfreeze_modules = ['final_mapping', 'feat_transformers.1', 'att_transformers.1']\n    unfreeze_params = [param for name in unfreeze_modules for param in modules[name].parameters()]\n    \n    optimizer = optim.Adam(unfreeze_params, lr=2e-02, weight_decay=WEIGHT_DECAY)\n\noptimizer.defaults['lr'] = 2e-02\n\nx, y = train_features.encoded.values, train_targets_scored.values\n\ny_cv = train_kfold(tabnet, optimizer, loss, x, y, initialization=initialization, \n                   prefix=\"TabNet\", epochs=200, patience=1, batch_size=2048)","71266c2f":"y_true = train_targets_scored.values\n\nprint(f\"CV Score: {log_loss(y_true,y_cv)}\")","cdba7885":"TRAIN_MLP = True\n\nif TRAIN_MLP:\n    mlp, optimizer, loss = get_model(num_features=train_features.enhanced.shape[1], \n                                     num_targets=train_targets_scored.shape[1], hidden_size=200)\n    mlp.saved_states = []\n    torch.save(mlp.state_dict(), \"init.pt\")\n    optimizer.defaults['lr'] = 0.04\n    \n    initialization = torch.load(\"init.pt\")\n    \n    x, y = train_features.enhanced, train_targets_scored.values\n    y_cv = train_kfold(mlp, optimizer, loss, x, y, initialization=initialization, \n                       prefix=\"MLP\", epochs=100, patience=1)","9105b63f":"y_true = train_targets_scored.values\n\nprint(log_loss(y_true,y_cv))","53618ecb":"def predict(x, model):\n    y_pred_ = []\n    state_dicts = [torch.load(saved_state) for saved_state in model.saved_states]\n    for sd in state_dicts:\n        model.load_state_dict(sd)\n        y_pred__ = model(x).sigmoid().detach().cpu().numpy()\n        y_pred_.append(y_pred__)\n        \n    return sum(y_pred_) \/ len(state_dicts)","9eae3c85":"ENSEMBLE = True\n\ntarget_cols = list(sample_submission.columns[1:])\ntest_idx = sample_submission.index\n\nx = {'TabNet': torch.tensor(test_features.encoded.values, dtype=torch.float, device=DEVICE),\n     'MLP': torch.tensor(test_features.enhanced, dtype=torch.float, device=DEVICE)}\n\nif ENSEMBLE:\n    models = {'MLP': mlp, 'TabNet': tabnet}\n    y_preds = []\n    for name in models:\n        y_preds.append(predict(x[name], models[name]))\n    y_pred = sum(y_preds) \/ len(models)\nelse:\n    y_pred = predict(x['TabNet'], tabnet)\n\npredictions_df = pd.DataFrame(y_pred, columns=target_cols)\n\nsample_submission.update(predictions_df)\nsample_submission.update(ctl_df)\nsample_submission.to_csv('submission.csv', index=False)","f1993343":"## TABNET\n\n    class TabNet(torch.nn.Module):\n        def __init__(self, input_dim, output_dim, n_d=8, n_a=8,\n                     n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                     n_independent=2, n_shared=2, epsilon=1e-15,\n                     virtual_batch_size=128, momentum=0.02, device_name='auto',\n                     mask_type=\"sparsemax\"):","73bd732d":"## 3. Download data<a class=\"anchor\" id=\"3\"><\/a>","2580a59c":"### Tabnet pretraining","94b614f9":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>","d0376542":"### 4.6 Dataset Classes<a class=\"anchor\" id=\"4.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","2477d820":"## 6. Prediction & Submission <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","ebd92901":"### 4.5 CV folds<a class=\"anchor\" id=\"4.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","9c536a6c":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Mechanisms of Action (MoA) Prediction](https:\/\/www.kaggle.com\/c\/lish-moa)","27bc04fa":"# Training\n\n    def train_session(model, train_data, valid_data, epochs=EPOCHS, scheduler=None, optimizer=None,\n                      loss=None, session_name=None, early_stopping_steps=EARLY_STOPPING_STEPS, output=None):\n                  \n    def train_kfold(model, optimizer, loss, x, y, initialization, epochs=EPOCHS, prefix=\"\", output=None,\n                    batch_size=BATCH_SIZE):\n","7f18700a":"[Go to Top](#0)","7e849ea9":"## 2. Global parameters <a class=\"anchor\" id=\"2\"><\/a>","93426ed3":"### 4.3 PCA features<a class=\"anchor\" id=\"4.3\"><\/a>","a5b0a494":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"><\/a>","e17a9a9e":"### 4.2 Seed<a class=\"anchor\" id=\"4.2\"><\/a>","9eb99925":"Forked from [MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual](https:\/\/www.kaggle.com\/vbmokin\/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual) (and butchered beyond all recognition, probably).","1b6f76ad":"### 4.7 Smoothing<a class=\"anchor\" id=\"4.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","9f8a01f9":"## 5. Modelling<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}