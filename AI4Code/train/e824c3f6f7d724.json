{"cell_type":{"05032d37":"code","6d82f380":"code","7b63e0f1":"code","58e6408b":"code","8caaf3a1":"code","a6b170cc":"code","ed625d0b":"code","62768431":"code","345bcad9":"code","6521cfe7":"code","37c64d33":"code","56fdbf10":"code","b977e2b5":"code","a3ff07b7":"code","374e593e":"code","ff7fce05":"code","f4ec6b54":"code","3a9fad39":"code","0e16cd0b":"code","df1bc07e":"code","4c924b54":"code","8015a0bd":"code","034081ff":"code","47e12410":"code","a54c2cdb":"code","614b9387":"code","1b5f24cf":"code","a0e1c0d6":"code","b61c588f":"code","27b46269":"code","5419ea49":"code","e2cf7a78":"code","da99f2bb":"code","ec3969b5":"code","dc850e4c":"code","14fa53a8":"code","f26fd5b2":"code","a07db252":"code","58251b62":"code","9f082b51":"code","beb75fe8":"code","a9b8e6ac":"code","019f449a":"code","3589256f":"code","daaba339":"code","b7e3d352":"code","f830a3fc":"code","1306b806":"code","269d1388":"code","1611f3dd":"code","13ef6f0e":"code","188be5a7":"code","e7a5440e":"code","b3896ca6":"code","c0d53077":"code","f64917aa":"code","360a9205":"code","0bd47ffe":"code","036c4da5":"code","81eeb246":"code","4f269c9b":"code","c44d1c4b":"code","b8655704":"code","ff5bc7c1":"markdown","9c08bd03":"markdown","04ca8c52":"markdown"},"source":{"05032d37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d82f380":"# Imports statements.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport multiprocessing\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom collections import defaultdict\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom IPython.display import display\n%matplotlib inline","7b63e0f1":"def display_all_details(dataframe):\n    print(('='*50)+'DATA'+('='*50))\n    print(('-'*50)+'SHAPE'+('-'*50))\n    print(dataframe.shape)\n    print(('-'*50)+'COLUMNS'+('-'*50))\n    print(dataframe.columns)\n    print(('-'*50)+'DESCRIBE'+('-'*50))\n    print(dataframe.describe())\n    print(('-'*50)+'INFO'+('-'*50))\n    print(dataframe.info())\n    print(('='*50)+'===='+('='*50))","58e6408b":"covid19_tweets_data = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","8caaf3a1":"covid19_tweets_data.head()","a6b170cc":"covid19_tweets_data.tail()","ed625d0b":"display_all_details(covid19_tweets_data)","62768431":"covid19_tweets_data.isnull().sum()","345bcad9":"def display_missing_values_info(df):\n    missing_values_count_df = df.isnull().sum()\n    print(('='*50)+'DATA WITH MISSING VALUES'+('='*50))\n    print(missing_values_count_df[missing_values_count_df>0])\n    print(('='*50)+'DATA WITHOUT MISSING VALUES'+('='*50))\n    print(missing_values_count_df[missing_values_count_df==0])\n    \n    ","6521cfe7":"display_missing_values_info(covid19_tweets_data)","37c64d33":"for tweets in covid19_tweets_data.text.head(20):\n    print(tweets)","56fdbf10":"for tweet in covid19_tweets_data.text:\n    link = re.search(\"(?P<url>https?:\/\/[^\\s]+)\", myString).group(\"url\")\n    if link!=None:\n        covid19_tweets_data['links'] = link\n    else:\n        covid19_tweets_data['links'] = pd.NA\n        ","b977e2b5":"covid19_tweets_data.head()","a3ff07b7":"covid19_tweets_data.links.isna().sum()","374e593e":"def clean_text_column(row):\n    text = row['text'].lower()\n    text = re.sub(\"(?P<url>https?:\/\/[^\\s]+)\",'',text)\n    text = re.sub(r'[^(a-zA-Z\\s)]','',text)\n    text = re.sub(r'\\(','',text)\n    text = re.sub(r'\\)','',text)\n    text = text.replace('\\n',' ')\n    text = text.strip()\n    return text","ff7fce05":"covid19_tweets_data['cleaned_text'] = covid19_tweets_data.apply(clean_text_column,axis = 1)","f4ec6b54":"for tweets in covid19_tweets_data.cleaned_text.head(20):\n    print(tweets)","3a9fad39":"covid19_tweets_data.cleaned_text.str.isspace().sum()","0e16cd0b":"covid19_tweets_data.shape","df1bc07e":"covid19_tweets_data.drop(covid19_tweets_data[covid19_tweets_data['cleaned_text'].str.isspace()==True].index,inplace = True)","4c924b54":"covid19_tweets_data.shape","8015a0bd":"sent = [row for row in covid19_tweets_data.cleaned_text]\nphrases = Phrases(sent, min_count=1, progress_per=50000)\nbigram = Phraser(phrases)\nsentences = bigram[sent]","034081ff":"sentences[:10]","47e12410":"filtered_sentences = []\nfor tweet in sentences:\n    filtered_sentences.append(remove_stopwords(tweet))","a54c2cdb":"filtered_sentences","614b9387":"filtered_sentences_2 = []\nfor tweet in filtered_sentences:\n    filtered_sentences_2.append(re.sub(r'\\b\\w{1,2}\\b', '',tweet))","1b5f24cf":"filtered_sentences_2","a0e1c0d6":"w2v_model = Word2Vec(min_count=3,\n                     window=4,\n                     size=200,\n                     sample=1e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=multiprocessing.cpu_count()-1)","b61c588f":"w2v_model.build_vocab(filtered_sentences_2, progress_per=50000)","27b46269":"w2v_model.train(filtered_sentences_2, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)","5419ea49":"w2v_model.init_sims(replace=True)","e2cf7a78":"w2v_model.save(\"word2vec.model\")","da99f2bb":"word_vectors = Word2Vec.load(\".\/word2vec.model\").wv","ec3969b5":"model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)","dc850e4c":"len(model.cluster_centers_[0])","14fa53a8":"word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)","f26fd5b2":"word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)","a07db252":"positive_cluster_center = model.cluster_centers_[0]\nnegative_cluster_center = model.cluster_centers_[1]","58251b62":"words = pd.DataFrame(word_vectors.vocab.keys())","9f082b51":"words.columns = ['words']","beb75fe8":"\nwords['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])","a9b8e6ac":"words.vectors[0].dtype","019f449a":"\nwords['cluster'] = words.vectors.apply(lambda x: model.predict(np.array([x])))","3589256f":"words.cluster","daaba339":"\nwords.cluster = words.cluster.apply(lambda x: x[0])","b7e3d352":"words.cluster.unique()","f830a3fc":"words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]","1306b806":"\nwords['closeness_score'] = words.apply(lambda x: 1\/(model.transform([x.vectors]).min()), axis=1)\n","269d1388":"words['sentiment_coeff'] = words.closeness_score * words.cluster_value","1611f3dd":"words.head()","13ef6f0e":"words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)","188be5a7":"sentiment_map = pd.read_csv('.\/sentiment_dictionary.csv')\nsentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))","e7a5440e":"sentiment_dict","b3896ca6":"tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\ntfidf.fit(covid19_tweets_data.cleaned_text)\nfeatures = pd.Series(tfidf.get_feature_names())\ntransformed = tfidf.transform(covid19_tweets_data.cleaned_text)","c0d53077":"'covid' in features.unique()","f64917aa":"def create_tfidf_dictionary(x, transformed_file, features):\n    '''\n    create dictionary for each input sentence x, where each word has assigned its tfidf score\n    \n    inspired  by function from this wonderful article: \n    https:\/\/medium.com\/analytics-vidhya\/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n    \n    x - row of dataframe, containing sentences, and their indexes,\n    transformed_file - all sentences transformed with TfidfVectorizer\n    features - names of all words in corpus used in TfidfVectorizer\n\n    '''\n    vector_coo = transformed_file[x.name].tocoo()\n    vector_coo.col = features.iloc[vector_coo.col].values\n    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n    return dict_from_coo\n\ndef replace_tfidf_words(x, transformed_file, features):\n    '''\n    replacing each word with it's calculated tfidf dictionary with scores of each word\n    x - row of dataframe, containing sentences, and their indexes,\n    transformed_file - all sentences transformed with TfidfVectorizer\n    features - names of all words in corpus used in TfidfVectorizer\n    '''\n    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n    return list(map(lambda y:dictionary[f'{y}'], x.cleaned_text.split()))","360a9205":"replaced_tfidf_scores = covid19_tweets_data.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)","0bd47ffe":"def replace_sentiment_words(word, sentiment_dict):\n    '''\n    replacing each word with its associated sentiment score from sentiment dict\n    '''\n    try:\n        out = sentiment_dict[word]\n    except KeyError:\n        out = 0\n    return out","036c4da5":"replaced_closeness_scores = covid19_tweets_data.cleaned_text.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))","81eeb246":"covid19_tweets_data.columns","4f269c9b":"replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, covid19_tweets_data.cleaned_text]).T\nreplacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence']\nreplacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\nreplacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n#replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]","c44d1c4b":"replacement_df.head()","b8655704":"replacement_df.prediction.value_counts()","ff5bc7c1":"Now Word2Vec model is trained and saved","9c08bd03":"Let's create Word2Vec using cleaned text vocabulary.","04ca8c52":"Since there is no target class, it means we have to go with unsupervised techniques to classify the text into sentiments."}}