{"cell_type":{"6dd3c4c4":"code","7df424ee":"code","bad028bf":"code","7621d793":"code","af01e02b":"code","9b212b35":"code","f07030ce":"code","a9766af7":"code","f64f7734":"code","d8837bbe":"code","d89b7bed":"code","dd3734d3":"code","d989db83":"code","41692dbc":"code","3e2b8d64":"code","e21a8e58":"code","300102ad":"markdown","b31703df":"markdown"},"source":{"6dd3c4c4":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","7df424ee":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data =  pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","bad028bf":"config = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","7621d793":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","af01e02b":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","9b212b35":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('..\/input\/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x","f07030ce":"def get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-base')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","a9766af7":"from nltk import tokenize\nimport random","f64f7734":"def synthesize_excerpt(train_data,r=0.5):\n    N=train_data.shape[0]\n    Ns=int(N*r)\n    idxs=np.repeat(np.arange(N),3)#np.random.randint(N,size=Ns)\n    ret_data=train_data.copy()\n    for i in idxs:\n        rand_target=train_data.iloc[i,:]\n        new=tokenize.sent_tokenize(rand_target['excerpt'])\n        random.shuffle(new)\n        new_excerpt=''.join(new)\n        rand_target['excerpt']=new_excerpt\n        ret_data=ret_data.append(rand_target, ignore_index=True)\n    ret_data = ret_data.sample(frac=1).reset_index(drop=True)\n    return ret_data\n","d8837bbe":"train_data=synthesize_excerpt(train_data,1.5)","d89b7bed":"train_embeddings1 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model0\/model0.bin')\ntest_embeddings1 = get_embeddings(test_data,'..\/input\/clr-roberta\/model0\/model0.bin')\n'''\ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model1\/model1.bin')\ntest_embeddings2 = get_embeddings(test_data,'..\/input\/clr-roberta\/model1\/model1.bin')\n\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model2\/model2.bin')\ntest_embeddings3 = get_embeddings(test_data,'..\/input\/clr-roberta\/model2\/model2.bin')\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model3\/model3.bin')\ntest_embeddings4 = get_embeddings(test_data,'..\/input\/clr-roberta\/model3\/model3.bin')\n\ntrain_embeddings5 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model4\/model4.bin')\ntest_embeddings5 = get_embeddings(test_data,'..\/input\/clr-roberta\/model4\/model4.bin')\n'''","dd3734d3":"train_X=train_embeddings1\ntrain_Y=train_data.target.values\ntest_X=test_embeddings1\n\nprint('train_X: ',train_embeddings1.shape)\nprint('train_Y: ',train_data.target.shape)\nprint('test_X: ',test_embeddings1.shape)\n","d989db83":"from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA","41692dbc":"selected=Lasso(alpha=0.0).fit(train_embeddings1,train_Y)\nY_pred=selected.predict(test_embeddings1)","3e2b8d64":"ret=pd.DataFrame(test_data['id'])\nret['target']=Y_pred","e21a8e58":"ret.to_csv('submission.csv',index=False)","300102ad":"## From Embedding to Target","b31703df":"This notebook uses below given notebooks to make predictions.\n\n1. LB 0.468 https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3\n2. LB 0.474 https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm"}}