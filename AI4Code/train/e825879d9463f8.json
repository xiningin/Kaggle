{"cell_type":{"fc5736af":"code","b2a24fe4":"code","aa0f4737":"code","1f926fa1":"code","41455fc0":"code","bd601bf6":"code","83a8cbd2":"code","f692511f":"code","8a197985":"code","7903b615":"code","15d67a7f":"code","2334b025":"code","189a9f74":"code","db2c2b8c":"code","fb6a900c":"code","580977d3":"code","ade897b8":"code","93c04679":"code","86f0b1c4":"code","1034d344":"markdown","e6a1cc18":"markdown","df65d120":"markdown","59483602":"markdown","d669eb4d":"markdown","14018a59":"markdown","f4ec30ec":"markdown","70d68ab8":"markdown","1f3e2a02":"markdown","55ddb66d":"markdown","3a8d3337":"markdown","22d22796":"markdown"},"source":{"fc5736af":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\ndevice = 'GPU' if 'GPU' in tf.test.gpu_device_name() else 'CPU\/TPU'\nprint('Device:', device)\n\nimport os, gc, random, datetime\nif device == 'GPU':\n    import cudf\n    import cupy as cp\nimport pandas as pd\nimport numpy as np\nimport janestreet\nimport xgboost as xgb\nimport datatable as dtable\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\nfrom time import time\nfrom numba import njit\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","b2a24fe4":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","aa0f4737":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","1f926fa1":"%%time\n\nprint('Loading...')\ntrain = dtable.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\nfeatures = [c for c in train.columns if 'feature' in c]\n\nprint('Filling...')\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain[features] = train[features].fillna(method = 'ffill').fillna(0)\ntrain['action'] = (train['resp'] > 0).astype('int')\n\nprint('Finish.')","41455fc0":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead) \n    but it must be broadcastable for addition.\n\n    Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n    output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        \n        scaled_attention_logits += (mask * -1e9)  \n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    \n    def __init__(self, d_model, num_heads):\n        \n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model \/\/ self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm = [0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        \n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, \n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n        return output, attention_weights\n\ndef point_wise_feed_forward_network(d_model, dff):\n    \n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation = 'swish'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    \n    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n        \n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training = training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training = training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    \n    def __init__(self, num_layers, d_model, num_heads, dff, \n                 maximum_position_encoding, rate = 0.1):\n        \n        super(TransformerEncoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dff = dff\n        self.maximum_position_encoding = maximum_position_encoding\n        self.rate = rate\n\n#         self.pos_encoding = positional_encoding(self.maximum_position_encoding, \n#                                                 self.d_model)\n#         self.embedding = tf.keras.layers.Dense(self.d_model)\n        self.pos_emb = tf.keras.layers.Embedding(input_dim = self.maximum_position_encoding, \n                                                 output_dim = self.d_model)\n\n        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate) \n                           for _ in range(self.num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(self.rate)\n        \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'num_layers': self.num_layers,\n            'd_model': self.d_model,\n            'num_heads': self.num_heads,\n            'dff': self.dff,\n            'maximum_position_encoding': self.maximum_position_encoding,\n            'dropout': self.dropout,\n        })\n        return config\n\n    def call(self, x, training, mask = None):\n\n        seq_len = tf.shape(x)[1]\n\n        # adding embedding and position encoding.\n#         x += self.pos_encoding[:, :seq_len, :]\n#         x = self.embedding(x)\n        positions = tf.range(start = 0, limit = seq_len, delta = 1)\n        x += self.pos_emb(positions)\n\n        x = self.dropout(x, training = training)\n\n        for i in range(self.num_layers):\n\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)","bd601bf6":"def create_transformer_model(num_columns, num_labels, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (window_size, num_columns))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dense(d_model)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('swish')(x)\n    x = tf.keras.layers.SpatialDropout1D(dropout_rate)(x)\n    x = TransformerEncoder(num_layers, d_model, num_heads, dff, window_size, dropout_rate)(x)\n    out = tf.keras.layers.Dense(num_labels, activation = 'sigmoid')(x[:, -1, :])\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tfa.optimizers.AdamW(weight_decay = weight_decay, learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","83a8cbd2":"batch_size = 4096 * strategy.num_replicas_in_sync\nnum_layers = 1\nd_model = 96\nnum_heads = 1\ndff = 64\nwindow_size = 3\ndropout_rate = 0.15\nweight_decay = 0\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3 * strategy.num_replicas_in_sync\nverbose = 1","f692511f":"with strategy.scope():\n    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\nmodel.summary()\n\nK.clear_session()\ndel model\nrubbish = gc.collect()","8a197985":"# Use Tensorflow Dataset\ndef prepare_dataset(X, y, window_size, batch_size, mode = 'training'):\n    x_ds = tf.data.Dataset.from_tensor_slices(X) \n    y_ds = tf.data.Dataset.from_tensor_slices(y[window_size - 1:])\n    x_ds = x_ds.window(window_size, shift = 1, drop_remainder = True)\n    x_ds = x_ds.flat_map(lambda window: window.batch(window_size))\n    dataset = tf.data.Dataset.zip((x_ds, y_ds))\n    if mode == 'training':\n        buffer_size = batch_size * 8\n        dataset = dataset.repeat()\n        dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration = True)\n        dataset = dataset.batch(batch_size)#, drop_remainder = True\n    elif mode == 'validation':\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.cache() \n    elif mode == 'testing':\n        dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Use Numpy [may cause Out-of-Memory (OOM) error]\ndef rolling_window(a, shape):  # rolling window for 2D array\n    s = (a.shape[0] - shape[0] + 1,) + (a.shape[1] - shape[1] + 1,) + shape\n    strides = a.strides + a.strides\n    return np.squeeze(np.lib.stride_tricks.as_strided(a, shape = s, strides = strides), axis = 1)","7903b615":"X_tr = train.loc[train['date'] < 303, features].values\ny_tr = train.loc[train['date'] < 303, 'action'].values\n\nX_tr2 = train.loc[(train['date'] >= 303) & (train['date'] <= 367), features].values\ny_tr2 = train.loc[(train['date'] >= 303) & (train['date'] <= 367), 'action'].values\n\nX_val = train.loc[train['date'] > 367, features].values\ny_val = train.loc[train['date'] > 367, 'action'].values\n\nrubbish = gc.collect()","15d67a7f":"X_tr = rolling_window(X_tr, (window_size, len(features)))\nX_val = rolling_window(X_val, (window_size, len(features)))\ny_tr = y_tr[window_size - 1:]\ny_val = y_val[window_size - 1:]\nX_tr2 = rolling_window(X_tr2, (window_size, len(features)))\ny_tr2 = y_tr2[window_size - 1:]","2334b025":"start_time_fold = time()\n\nckp_path = 'JSTransformer.hdf5'\nwith strategy.scope():\n    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\nrlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = verbose, \n                        min_delta = 1e-4, mode = 'max')\nckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                      save_best_only = True, save_weights_only = True, mode = 'max')\nes = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                   baseline = None, restore_best_weights = True, verbose = 0)\nhistory = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), batch_size = batch_size,\n                    epochs = 1000, callbacks = [rlr, ckp, es], verbose = verbose)\nhist = pd.DataFrame(history.history)\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] ROC AUC:\\t', hist['val_AUC'].max())\n\nK.clear_session()\ndel model, X_tr, y_tr\nrubbish = gc.collect()","189a9f74":"start_time_fold = time()\n\nwith strategy.scope():\n    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, \n                                     window_size, dropout_rate, weight_decay, label_smoothing, \n                                     learning_rate \/ 100)\nmodel.load_weights(ckp_path)\nes = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                   baseline = None, restore_best_weights = True, verbose = 0)\nhistory2 = model.fit(X_tr2, y_tr2, validation_data = (X_val, y_val), batch_size = batch_size,\n                    epochs = 1000, callbacks = [es], verbose = verbose)\nhist2 = pd.DataFrame(history2.history)\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] ROC AUC:\\t', hist2['val_AUC'].max())\nfinetune_epochs = hist2['val_AUC'].argmax() + 1\n\nK.clear_session()\ndel model\nrubbish = gc.collect()","db2c2b8c":"with strategy.scope():\n    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, \n                                     window_size, dropout_rate, weight_decay, label_smoothing, \n                                     learning_rate \/ 100)\nmodel.load_weights(ckp_path)\nmodel.fit(np.concatenate((X_tr2, X_val)), np.concatenate((y_tr2, y_val)), \n          batch_size = batch_size,epochs = finetune_epochs, verbose = verbose)\nmodel.save_weights(ckp_path)\n\nK.clear_session()\ndel model, X_tr2, y_tr2, X_val, y_val\nrubbish = gc.collect()","fb6a900c":"# gkf = GroupKFold(n_splits = 5)\n# for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n    \n#     start_time_fold = time()\n#     X_tr, X_val = train.loc[tr, features].values, train.loc[te, features].values\n#     y_tr, y_val = train.loc[tr, 'action'].values, train.loc[te, 'action'].values\n    \n#     train_steps = int((X_tr.shape[0] \/\/ batch_size) + 1)\n#     val_steps = int((X_val.shape[0] \/\/ batch_size) + 1)\n    \n#     dataset_tr = prepare_dataset(X_tr, y_tr, window_size, batch_size, 'training')\n#     dataset_val = prepare_dataset(X_val, y_val, window_size, batch_size, 'validation')\n    \n#     ckp_path = f'JSModel_{fold}.hdf5'\n#     with strategy.scope():\n#         model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\n#     rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = verbose, \n#                             min_delta = 1e-4, mode = 'max')\n#     ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n#                           save_best_only = True, save_weights_only = True, mode = 'max')\n#     es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n#                        baseline = None, restore_best_weights = True, verbose = 0)\n#     history = model.fit(dataset_tr, steps_per_epoch = train_steps, \n#                         validation_data = dataset_val, validation_steps = val_steps, \n#                         epochs = 1000, callbacks = [rlr, ckp, es], verbose = verbose)\n#     hist = pd.DataFrame(history.history)\n    \n#     K.clear_session()\n#     del model\n#     rubbish = gc.collect()\n    \n#     # Finetune 3 epochs on validation set with small learning rate\n#     with strategy.scope():\n#         model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate \/ 100)\n#     model.load_weights(ckp_path)\n#     dataset_val = prepare_dataset(X_val, y_val, window_size, batch_size, 'training')\n#     model.fit(dataset_val, steps_per_epoch = val_steps, epochs = 3, verbose = 0)\n#     model.save_weights(ckp_path)\n    \n#     print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] Fold {fold} ROC AUC:\\t', hist['val_AUC'].max())\n    \n#     K.clear_session()\n#     del model, X_tr, X_val, y_tr, y_val, dataset_tr, dataset_val\n#     rubbish = gc.collect()\n#     break","580977d3":"with strategy.scope():\n    model = create_transformer_model(len(features), 1, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\nmodel.load_weights('.\/JSTransformer.hdf5')","ade897b8":"# https:\/\/www.kaggle.com\/gogo827jz\/optimise-speed-of-filling-nan-function\n@njit\ndef fast_fillna(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array\n\n# The first run of numba decorated function requires compiling, which takes longer time than the later runs. So, we compile it before submission.\ntrain.loc[0, features[1:]] = fast_fillna(train.loc[0, features[1:]].values, 0)","93c04679":"env = janestreet.make_env()\nenv_iter = env.iter_test()","86f0b1c4":"opt_th = 0.505\ntmp = np.zeros((1, window_size, len(features)))\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        x_tt[0] = fast_fillna(x_tt[0], tmp[0, -1])\n        tmp[0] = np.concatenate((tmp[0, 1:], x_tt))\n        pred = model(tmp, training = False).numpy().item()\n        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","1034d344":"# Training","e6a1cc18":"Train on both training-2 and validation sets.","df65d120":"# Train-Test-Split Training\n\nSplit the train set into three folds, i.e., training-1, training-2 and validation sets. First, train the more on training-1 set and validate it on the validation set. Then use the training-2 set to find the best number of finetuning epochs. Finally, finetune on both training-2 and validation sets and submit.","59483602":"Check the best number of epochs for finetuning.","d669eb4d":"# Preprocessing","14018a59":"# Submitting","f4ec30ec":"These are some configurations. I use a small model for inference test.","70d68ab8":"# GroupCV Training","1f3e2a02":"Base Transformer structure from https:\/\/www.tensorflow.org\/tutorials\/text\/transformer, modified with Swish activation function.","55ddb66d":"Train on the training-1 set and validate on the validation set.","3a8d3337":"# Load Model","22d22796":"# Jane Street: \ud83d\udd25Transformer\ud83d\udd25 Baseline\n\nTransformer is very popular among time-series competitions. This notebook provides the training and inference pipelines for transformer encoder implementation. Due to the long training time, I only test a small model with small window size for inference (e.g., window size equals to 5). You may get better results by tuning the structure and hyperparameters as well as the window size.\n\nIn the latest version, I use numpy sliding-window function to create training samples instead of TensorFlow Dataset. It makes training faster but more memory consuming.\n\n![Transformer.png](attachment:Transformer.png)"}}