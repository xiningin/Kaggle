{"cell_type":{"0c87a7be":"code","18710b09":"code","48b72a4b":"code","7666ad26":"code","081b6e69":"code","517743de":"code","a6aa9311":"code","cf46dccf":"code","0484151d":"code","c968e3f7":"code","2ec7b6cc":"code","8d609f46":"code","ab78e017":"code","575ffb61":"code","91d238ce":"code","da1da059":"code","ff62d5d3":"code","71df173a":"markdown","8df5d797":"markdown"},"source":{"0c87a7be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18710b09":"import matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport seaborn as sns\n\nfrom IPython.display import clear_output, display\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","48b72a4b":"getChar = lambda x: \"ABCDEFGHIKLMNOPQRSTUVWXY\"[x]","7666ad26":"def prepData(df):\n    df = df.copy()\n    \n    y = df.label\n    y = pd.get_dummies(y, drop_first=False)\n\n    X = df.drop('label', axis=1).to_numpy() \/ 255\n    X = X.reshape(-1, 28, 28, 1)\n    return X, y\n\ndf_train = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')\ndf_test = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')\n\nX_train, y_train = prepData(df_train)\nX_test, y_test = prepData(df_test)","081b6e69":"index = 123\nplt.imshow(X_train[index,:,:], cmap='gray')\nprint('Letter:', getChar(y_train.iloc[index].argmax()))","517743de":"# 80% ish\ndef Model2DConvolution():\n    model = Sequential()\n    model.add(Dense(32, activation='relu', input_shape=(28,28,1)))\n    model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size = (3, 3), strides = 2, padding='valid'))\n    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(Flatten())\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(24, activation='softmax'))\n    model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n    return model\n\n# For PyTorch-comparison:\ndef miniModel():\n    model = Sequential()\n    model.add(Flatten(input_shape=(28,28,1)))\n    model.add(Dense(512, activation = 'relu'))\n    model.add(Dense(128, activation = 'relu'))\n    model.add(Dense(24, activation ='sigmoid'))\n    model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n    return model","a6aa9311":"model = miniModel()\nmodel.summary()","cf46dccf":"model.fit(X_train, y_train, epochs = 10, batch_size = 50)","0484151d":"testIndex = 120\n\npred = model.predict(X_test[testIndex].reshape(-1, 28,28, 1))\npredChar = getChar(pred.argmax())\ntruth = getChar(y_test.to_numpy().argmax(axis=1)[testIndex])\n\nprint('Ground:', truth)\nprint('Predic:', predChar)\n    \nplt.imshow(X_test[testIndex].reshape(28, 28), cmap='gray');","c968e3f7":"y_pred = model.predict(X_test).argmax(axis=1)\ny_true = y_test.to_numpy().argmax(axis=1)\n\ncm = confusion_matrix(y_true, y_pred)\n\nf, ax=plt.subplots(figsize=(15,8))\nsns.heatmap(cm,\n            annot=True,\n            linewidths=0.005,\n            linecolor=\"red\",\n            fmt=\".0f\",\n            ax=ax,\n            xticklabels=[getChar(x) for x in range(24)],\n            yticklabels=[getChar(x) for x in range(24)]\n           )\n\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Ground truth\")\nplt.show()","2ec7b6cc":"print(accuracy_score(y_true, y_pred))","8d609f46":"def convertToTensor(X, y):\n    X = torch.tensor(X.reshape(-1, 28*28)).float()\n    y = torch.tensor(y.to_numpy()).float() # multidim categorical\n    #y = torch.tensor(y.to_numpy().argmax(axis=1)).long()\n    return X, y\n\nX_train_, y_train_ = convertToTensor(X_train, y_train)\nX_test_, y_test_ = convertToTensor(X_test, y_test)","ab78e017":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()        \n        self.l1 = nn.Linear(28*28, 512)\n        self.act1 = nn.ReLU()\n        self.l2 = nn.Linear(512, 128)\n        self.act2 = nn.ReLU()\n        self.l3 = nn.Linear(128, 24)\n\n    def forward(self, x):\n        x = self.act1(self.l1(x))\n        x = self.act2(self.l2(x))\n        x = torch.sigmoid(self.l3(x))\n        return x\n    \nmodel_torch = NeuralNetwork()\nprint(model_torch)","575ffb61":"lossFn = nn.BCELoss()\n#lossFn = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(model_torch.parameters())\nBS = 32\nlosses, accs = list(), list()\ni = 0\nwhile True:\n    i += 1\n    \n    # Batching with random choices: (a bit hacky ...)\n    sample = np.random.randint(0, X_train.shape[0], size=BS)\n    X = X_train_[sample]\n    y = y_train_[sample]\n    \n    # Prediction + error:\n    out = model_torch(X)\n    loss = lossFn(out, y)\n    \n    # Backprop:\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Infos + plotting:\n    y = y.argmax(axis=1)\n    cats = torch.argmax(out, axis=1)\n    acc = (cats == y).float().mean()\n\n    losses.append(loss.item())\n    accs.append(acc.item()*100)\n    \n    if i % 20 == 0:\n        test = (model_torch(X_test_).argmax(axis=1) == y_test_.argmax(axis=1)).float().mean().numpy()*100\n        print(f\"ITERATION: {i}\\tACC-TRAIN: {accs[-1]:.1f}\\tLOSS: {losses[-1]:.2f}\\t ACC-TEST:{test:.2f}\")\n        clear_output(wait=True)\n\n    # Stop if takes too long:\n    if i > 5000:\n        print(\"Stop by max iter!\")\n        break\n        \n    # Stop if train-acc is greater than X:\n    if np.mean(accs[-100:]) > 98:\n        print(\"> 98%\")\n        break","91d238ce":"fig, ax = plt.subplots(figsize=(12,8))\nax2 = ax.twinx()\n\nax.plot(accs, label='acc', color='orange')\nax.legend()\n\nax2.plot(losses, label='loss', color='red')\nax2.legend()\nplt.grid()","da1da059":"y_pred_ = model_torch(X_test_).detach().numpy().argmax(axis=1)\ny_true_ = y_test_.detach().numpy().argmax(axis=1)\n\ncm = confusion_matrix(y_true_, y_pred_)\n\nf, ax=plt.subplots(figsize=(15,8))\nsns.heatmap(cm,\n            annot=True,\n            linewidths=0.005,\n            linecolor=\"red\",\n            fmt=\".0f\",\n            ax=ax,\n            xticklabels=[getChar(x) for x in range(24)],\n            yticklabels=[getChar(x) for x in range(24)]\n           )\n\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Ground truth\")\nplt.show()","ff62d5d3":"(model_torch(X_test_).argmax(axis=1) == y_test_.argmax(axis=1)).float().mean().numpy()*100","71df173a":"# The \"same\" with PyTorch:","8df5d797":"# With Keras"}}