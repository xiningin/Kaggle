{"cell_type":{"fa0b80e2":"code","c1215d01":"code","9c9ded41":"code","0413c3fe":"code","e66f9232":"code","d13d25a2":"code","46fdb936":"code","98b15345":"code","018a4ad4":"code","28a092ca":"code","a213760a":"code","68b70bc0":"code","90eef41c":"code","d445ec58":"code","e3591f22":"code","d4efacd2":"code","8c8cc597":"markdown"},"source":{"fa0b80e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport librosa\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import skew\nSAMPLE_RATE = 45100\n\n#from sklearn.model_selection import KFold, RepeatedKFold\nfrom tqdm import tqdm, tqdm_pandas\n\ntqdm.pandas()\nimport scipy\ndata_path = '..\/input\/'\nss = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))","c1215d01":"#loading data\naudio_train_files = os.listdir('..\/input\/train_curated\/')\naudio_test_files = os.listdir('..\/input\/test\/')\n\ntrain = pd.read_csv('..\/input\/train_curated.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","9c9ded41":"#function from EDA kernel: https:\/\/www.kaggle.com\/codename007\/a-very-extensive-freesound-exploratory-analysis\ndef clean_filename(fname, string):   \n    file_name = fname.split('\/')[1]\n    if file_name[:2] == '__':        \n        file_name = string + file_name\n    return file_name\n\n#returns mfcc features with mean and standard deviation along time\ndef get_mfcc(name, path):\n    b, _ = librosa.core.load(path + name, sr = SAMPLE_RATE)\n    assert _ == SAMPLE_RATE\n    try:\n        ft1 = librosa.feature.mfcc(b, sr = SAMPLE_RATE, n_mfcc=20)\n        ft2 = librosa.feature.zero_crossing_rate(b)[0]\n        ft3 = librosa.feature.spectral_rolloff(b)[0]\n        ft4 = librosa.feature.spectral_centroid(b)[0]\n        ft1_trunc = np.hstack((np.mean(ft1, axis=1), np.std(ft1, axis=1), skew(ft1, axis = 1), np.max(ft1, axis = 1), np.min(ft1, axis = 1)))\n        ft2_trunc = np.hstack((np.mean(ft2), np.std(ft2), skew(ft2), np.max(ft2), np.min(ft2)))\n        ft3_trunc = np.hstack((np.mean(ft3), np.std(ft3), skew(ft3), np.max(ft3), np.min(ft3)))\n        ft4_trunc = np.hstack((np.mean(ft4), np.std(ft4), skew(ft4), np.max(ft4), np.min(ft4)))\n        return pd.Series(np.hstack((ft1_trunc, ft2_trunc, ft3_trunc, ft4_trunc)))\n    except:\n        print('bad file')\n        return pd.Series([0]*115)","0413c3fe":"#preparing data\ntrain_data = pd.DataFrame()\ntrain_data['fname'] = train['fname']\ntest_data = pd.DataFrame()\ntest_data['fname'] = audio_test_files\n\ntrain_data = train_data['fname'].apply(get_mfcc, path='..\/input\/train_curated\/')\nprint('done loading train mfcc')\ntest_data = test_data['fname'].apply(get_mfcc, path='..\/input\/test\/')\nprint('done loading test mfcc')\n\ntrain_data['fname'] = train['fname']\ntest_data['fname'] = audio_test_files","e66f9232":"train_data['label'] = train['labels'].apply(lambda x:x.split(',')[0])\ntest_data['label'] = np.zeros((len(audio_test_files)))","d13d25a2":"train_data.head()","46fdb936":"def extract_features(files, path):\n    features = {}\n    cnt = 0\n    for f in tqdm(files):\n        features[f] = {}\n        fs, data = scipy.io.wavfile.read(os.path.join(path, f))\n        abs_data = np.abs(data)\n        diff_data = np.diff(data)\n        def calc_part_features(data, n=2, prefix=''):\n            f_i = 1\n            for i in range(0, len(data), len(data)\/\/n):\n                features[f]['{}mean_{}_{}'.format(prefix, f_i, n)] = np.mean(data[i:i + len(data)\/\/n])\n                features[f]['{}std_{}_{}'.format(prefix, f_i, n)] = np.std(data[i:i + len(data)\/\/n])\n                features[f]['{}min_{}_{}'.format(prefix, f_i, n)] = np.min(data[i:i + len(data)\/\/n])\n                features[f]['{}max_{}_{}'.format(prefix, f_i, n)] = np.max(data[i:i + len(data)\/\/n])\n        features[f]['len'] = len(data)\n        if features[f]['len'] > 0:\n            n = 1\n            calc_part_features(data, n=n)\n            calc_part_features(abs_data, n=n, prefix='abs_')\n            calc_part_features(diff_data, n=n, prefix='diff_')\n\n            n = 2\n            calc_part_features(data, n=n)\n            calc_part_features(abs_data, n=n, prefix='abs_')\n            calc_part_features(diff_data, n=n, prefix='diff_')\n\n            n = 3\n            calc_part_features(data, n=n)\n            calc_part_features(abs_data, n=n, prefix='abs_')\n            calc_part_features(diff_data, n=n, prefix='diff_')\n        cnt += 1\n    features = pd.DataFrame(features).T.reset_index()\n    features.rename(columns={'index': 'fname'}, inplace=True)\n    return features\n\npath = os.path.join(data_path, 'train_curated')\ntrain_files = train.fname.values\ntrain_features = extract_features(train_files, path)\n\npath = os.path.join(data_path, 'test')\ntest_files = ss.fname.values\ntest_features = extract_features(test_files, path)","98b15345":"train_data = train_data.merge(train_features, on='fname', how='left')\ntest_data = test_data.merge(test_features, on='fname', how='left')\ntrain_data.head()","018a4ad4":"#Functions from LightGBM baseline: https:\/\/www.kaggle.com\/opanichev\/lightgbm-baseline\n# Construct features set\nX = train_data.drop(['label', 'fname'], axis=1)\nfeature_names = list(X.columns)\nX = X.values\nlabels = np.sort(np.unique(train_data.label.values))\nnum_class = len(labels)\nc2i = {}\ni2c = {}\nfor i, c in enumerate(labels):\n    c2i[c] = i\n    i2c[i] = c\ny = np.array([c2i[x.split(',')[0]] for x in train_data.label.values])","28a092ca":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=10, shuffle = True)\nclf = LGBMClassifier(max_depth=5, learning_rate=0.05, n_estimators=1000,\n                    n_jobs=-1, random_state=0, reg_alpha=0.2, \n                    colsample_bylevel=0.5, colsample_bytree=0.5)\nclf.fit(X_train, y_train)\nprint(accuracy_score(clf.predict(X_val), y_val))\n#more functions from LightGBM baseline: https:\/\/www.kaggle.com\/opanichev\/lightgbm-baseline","a213760a":"p = clf.predict_proba(test_data.drop(['label', 'fname'], axis =1).values)","68b70bc0":"p.shape","90eef41c":"for i in range(len(labels)):\n    submission[i2c[i]] = p[:, i]","d445ec58":"submission.head()","e3591f22":"submission.to_csv('submission.csv', index = False)","d4efacd2":"!ls","8c8cc597":"This is an example that shows how to use xgboost to handle audio classification. But it need to be modifid from multi-class to multi-label"}}