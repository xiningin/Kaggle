{"cell_type":{"55cc486e":"code","da0c0980":"code","cff3d965":"code","432d3ba5":"code","d38d1f5f":"code","a5e25bbf":"code","d37ae7d2":"code","8eb0e5c6":"code","8fe7040e":"code","4c6292a6":"code","3872705d":"code","be3ca174":"code","0378ca8f":"code","8b206903":"code","94902d94":"code","d27b779e":"code","139ee2b6":"code","f5ed095e":"code","ea8641fe":"code","6b9e91c4":"code","9e98e0c4":"code","f66a42b6":"code","0258222a":"code","9166ec40":"code","d9059919":"code","77fdd58f":"code","69292675":"code","c0f55f25":"code","16ed76f0":"code","b854f8f1":"code","79d3d9b5":"code","d2e130be":"code","3e04117f":"code","f7cac0b6":"code","d3eb480f":"code","de575584":"code","1d945c4f":"code","a1b13178":"code","57c1afae":"code","eb6ac952":"code","efeea439":"markdown","75410f6a":"markdown","a1728340":"markdown","2efd0337":"markdown","3b70dc17":"markdown","409db593":"markdown","4cbf8f22":"markdown","562bf1d0":"markdown","a259bf39":"markdown","06d39eac":"markdown","a0a2b21b":"markdown","ab46fba7":"markdown","933b72d3":"markdown","b13c1c1e":"markdown","ed00ccb5":"markdown","8267aae3":"markdown","1537e1ce":"markdown","523bdd63":"markdown","0c25aad6":"markdown","6a6cdbe2":"markdown","8b046c52":"markdown","0db1a5e1":"markdown","c6196789":"markdown"},"source":{"55cc486e":"# Load libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\n# Load datasets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ndataset = [train, test]","da0c0980":"train.shape, test.shape # One missing column in test would be 'Survived' feature.","cff3d965":"train.head()","432d3ba5":"train.isnull().sum()\n# ['Age'] is missing 177 values\n# ['Cabin'] is missing 687 values\n# ['Embarked'] is missing 2 values","d38d1f5f":"test.isnull().sum()\n# ['Age'] is missing 86 values\n# ['Fare'] is missing 1 value\n# ['Cabin'] is missing 327 values","a5e25bbf":"def visualization(feature):\n    O = train[train['Survived'] == 1][feature].value_counts()\n    X = train[train['Survived'] == 0][feature].value_counts()\n    visual_df = pd.DataFrame([O, X])\n    visual_df.index = ['Survived','Dead']\n    visual_df.plot(kind = 'bar',stacked = True, figsize = (12, 5), title = feature)","d37ae7d2":"visualization('Pclass')\n# Survived : Likely to have lived when Pclass == 1\n# Dead : Likely to have died when Pclass == 3","8eb0e5c6":"visualization('Sex')\n# Survived : Likely to have lived when female\n# Dead : Likely to have died when male","8fe7040e":"visualization('Embarked')\n# Embarked == S : More likely to have died\n# Embarked == C : More likely to have survived\n# Embarked == Q : More likely to have died","4c6292a6":"for vector in dataset :\n    vector['FamilySize'] = vector['SibSp'] + vector['Parch'] + 1\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index = False).mean()","3872705d":"for vector in dataset :\n    vector['Alone'] = 0 # Alone on board\n    vector.loc[vector['FamilySize'] == 1, 'Alone'] = 1 # With Family\ntrain[['Alone', 'Survived']].groupby(['Alone'], as_index = False).mean()","be3ca174":"train = train.drop(['SibSp', 'Parch', 'FamilySize'], axis = 1)\ntest = test.drop(['SibSp', 'Parch', 'FamilySize'], axis = 1)","0378ca8f":"train['Embarked'].value_counts() + test['Embarked'].value_counts()","8b206903":"train['Embarked'] = train['Embarked'].fillna('S')\ntest['Embarked'] = test['Embarked'].fillna('S')","94902d94":"test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)","d27b779e":"facet = sns.FacetGrid(train, aspect = 4)\nfacet.map(sns.kdeplot,'Fare', shade = True)\nfacet.set(xlim = (0, train['Fare'].max()))\nfacet.add_legend()\n \nplt.show() ","139ee2b6":"train['Fare_Division'] = pd.qcut(train['Fare'], 4)\ntest['Fare_Division'] = pd.qcut(test['Fare'], 4)\n\ntrain[['Fare_Division', 'Survived']].groupby(['Fare_Division'], as_index = False).mean()","f5ed095e":"train['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest['Title'] = test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","ea8641fe":"train['Title'].value_counts() #  Mr,  Miss, Mrs are the majority","6b9e91c4":"test['Title'].value_counts() # Mr, Miss, Mrs are the majority.","9e98e0c4":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","f66a42b6":"train['Age'].isnull().sum(), test['Age'].isnull().sum()","0258222a":"test[test['Age'].isnull()] # There's  no  other Ms. than her","9166ec40":"train[train['Title'] == \"Ms\"]['Age'].value_counts() # Average age  of Ms in train data is 28","d9059919":"test.loc[test['Age'].isnull(), 'Age'] = 28","77fdd58f":"facet = sns.FacetGrid(train, aspect = 4)\nfacet.map(sns.kdeplot,'Age', shade = True)\nfacet.set(xlim = (0, train['Age'].max()))\nfacet.add_legend()\n \nplt.show() ","69292675":"train['Age_Division'] = pd.qcut(train['Age'], 5)\ntest['Age_Division'] = pd.qcut(test['Age'], 5)\n\ntrain[['Age_Division', 'Survived']].groupby(['Age_Division'], as_index = False).mean()","c0f55f25":"train.head()","16ed76f0":"sexMap = {\"male\" : 0, \"female\" : 1}\ntrain['Sex'] = train['Sex'].map(sexMap)\ntest['Sex'] = test['Sex'].map(sexMap)","b854f8f1":"embarkedMap = {\"S\" : 0, \"C\" : 1, \"Q\" : 2}\ntrain['Embarked'] = train['Embarked'].map(embarkedMap)\ntest['Embarked'] = test['Embarked'].map(embarkedMap)","79d3d9b5":"# Since  Mr, Miss, and Mrs were majority\ntitleMap = {\"Mr\" : 0, \"Miss\" : 1, \"Mrs\" : 2, \"Master\" : 3, \"Dr\" : 3, \"Rev\" : 3, \"Mlle\" : 3, \"Col\" : 3, \"Major\" : 3, \n                \"Ms\" : 3, \"Lady\" : 3, \"Jonkheer\" : 3, \"Mme\" : 3, \"Capt\" : 3, \"Sir\" : 3, \"Don\" : 3, \"Countess\" : 3}\ntrain['Title'] = train['Title'].map(titleMap)\ntest['Title'] = test['Title'].map(titleMap)","d2e130be":"# Fare\ntrain.loc[train['Fare'] <= 7.91 , 'Fare'] = 0\ntrain.loc[(7.91 < train['Fare']) & (train['Fare'] <= 14.454) , 'Fare'] = 1\ntrain.loc[(14.454 < train['Fare']) & (train['Fare'] <= 31) , 'Fare'] = 2\ntrain.loc[31 < train['Fare'] , 'Fare'] = 3\n    \ntest.loc[test['Fare'] <= 7.91 , 'Fare'] = 0\ntest.loc[(7.91 < test['Fare']) & (test['Fare'] <= 14.454) , 'Fare'] = 1\ntest.loc[(14.454 < test['Fare']) & (test['Fare'] <= 31) , 'Fare'] = 2\ntest.loc[31 < test['Fare'] , 'Fare'] = 3","3e04117f":"train.loc[train['Age'] <= 20 , 'Age'] = 0\ntrain.loc[(20 < train['Age']) & (train['Age'] <= 26) , 'Age'] = 1\ntrain.loc[(26 < train['Age']) & (train['Age'] <= 30) , 'Age'] = 2\ntrain.loc[(30 < train['Age']) & (train['Age'] <= 38) , 'Age'] = 3\ntrain.loc[38 < train['Age'] , 'Age'] = 4\n\ntest.loc[test['Age'] <= 20 , 'Age'] = 0\ntest.loc[(20 < test['Age']) & (test['Age'] <= 26) , 'Age'] = 1\ntest.loc[(26 < test['Age']) & (test['Age'] <= 30) , 'Age'] = 2\ntest.loc[(30 < test['Age']) & (test['Age'] <= 38) , 'Age'] = 3\ntest.loc[38 < test['Age'] , 'Age'] = 4","f7cac0b6":"train = train.drop(['PassengerId', 'Ticket', 'Cabin', 'Fare_Division', 'Age_Division'], axis = 1)\ntest = test.drop(['Ticket', 'Cabin', 'Fare_Division', 'Age_Division'], axis = 1)","d3eb480f":"train.head()","de575584":"train['tmp'] = 1\ntest['tmp'] = 1 \n# 'tmp' are required to ease the computation later on.\n# When computing weights for lost function, while other parameters are multiplied with each values of predictors,\n# the first parameter is not. So, that's why we give a ['tmp'] = 1 so we  can still multiply from train\/test_list but\n# doesn't affect value.\n\ntrain_df = pd.DataFrame(train, columns= ['tmp', 'Pclass', 'Sex', 'Age', 'Fare', 'Alone', 'Title'])\ntest_df = pd.DataFrame(test, columns= ['tmp', 'Pclass', 'Sex', 'Age', 'Fare', 'Alone', 'Title'])\ntarget = train['Survived']\n\ntrain_list = train_df.values.tolist()\ntrain_list = np.array(train_list)\n\ntest_list = test_df.values.tolist()\ntest_list = np.array(test_list)\n\ntarget_list = target.values.tolist()\ntarget_list = np.array(target_list)","1d945c4f":"#Weight parameters are needed : w0, w1, w2... : w_list = []\n\nimport random\nw_list = np.zeros(7)\nfor i in range(0, 7) :\n    w_list[i] = random.random() * 2 - 1\n\n#check randomly generated parameters\nfor i in range(0, 7) :\n    print(w_list[i])\n\n# set learning rate alpha : a\na = 0.0001","a1b13178":"cnt = 0\n# total iteration of 300,000\nwhile(cnt < 300000) :\n    Zi = train_list[:].dot(w_list) # Zi = w0 + w1 * x1 + w2 * x2 + ... + wn * xn   \n    Hi = 1 \/ (1 + np.exp(-Zi)) # Hi = 1 \/ (1 + e^(-Zi)) \n    Hi_y = Hi - target_list\n\n    for i in range(0, 7) :\n        w_list[i] = w_list[i] - a * np.sum(train_list[:, i] * Hi_y) \/ 891\n    cnt = cnt + 1","57c1afae":"answer_list = []\nfor i in range (0, 418) :\n    Zx = test_list[i].dot(w_list)\n    if(Zx >= 0) :\n        answer_list.append(1)\n    else :\n        answer_list.append(0)","eb6ac952":"submission_df = pd.DataFrame({\n    \"PassengerId\" : test[\"PassengerId\"],\n    \"Survived\" : answer_list\n})\nsubmission_df.to_csv('submission.csv', index = False)","efeea439":"It is pretty obvious that we should retrieve <u>information from categorical values<\/u>, and later <u>map them to numerical values<\/u> to make use of them in making our model(logistic regression).\n\n#### Are there missing values?\nYes, some features do have missing values, and we have to decide on how to manage those values.","75410f6a":"Understanding of what is given is very important since well explored datasets give you a broad perspective of what you should do later on such as feature engineering and data cleansing.","a1728340":"## Data Exploration & Visualization","2efd0337":"Age data  seems to be more well sparsed compared to Fare data. So let's split age data into <b>five groups<\/b>, and this is what we've got.","3b70dc17":"#### SibSp & Parch \u2192 FamilySize \u2192 Alone\nMake a new feature of ourselves named 'FamilySize' by merging the value of SibSp, Parch, and add 1(ticket holder :self).","409db593":"## Gradient Descent Algorithm","4cbf8f22":"## Feature Engineering","562bf1d0":"Passenger 980's age data couldn't be filled because she was the only Ms. from test  data. So let's fill her age data with the mean value of Ms in train data.","a259bf39":"#### Fare\nFill in 'Fare' by median value grouped by 'Pclass'.","06d39eac":"#### Embarked\nOnly 2 datas from train were missing so let's fill them with 'S' which accounts for most of the data.","a0a2b21b":"#### Visualization","ab46fba7":"In order to change these numerical values into category, we must group them into a few categories. Let's go with <b>four groups<\/b>, and here's what we've got.","933b72d3":"It turns out that people who have embarked <b>alone<\/b> has <u>50% survival rate<\/u> whereas people who were on board <b>with family<\/b> only had <u>30% survival rate<\/u>.<br>\nPrevious datas(SibSp, Parch, FamilySize) are now redundant so let's drop them from our DataFrame","b13c1c1e":"#### Age \u2192 Name(Title) \u2192 Age\nLet's extract title information from ['Name'] feature in order to fill out missing ['Age'] data.<br>\nWe will fill in 'Age' with mean value grouped by ['Title']","ed00ccb5":"It seems there was an unfilled data in test\/","8267aae3":"It will mostly be about mapping out the categorical datas into numerical ones.","1537e1ce":"Here you go with the submission file!","523bdd63":"## Data Cleansing","0c25aad6":"Now, I must say this part affects in accuracy of our result. The parameters I submitted were <br>\n<b>w_list = np.array([-0.6317466601918094, -0.05653350853635963, 0.8556811494570249, -0.03946554989032114, -0.8449024756038757, 0.2661752869088194, 0.002126179261684902])<\/b> <br>\nso if you want to follow my kernel, fix w_list with given array.","6a6cdbe2":"Greetings to all starters of Kaggle! Upon scrolling down to read this kernel, you may want to be pre-noticed that this kernel focuses more on coding simple gradient descent algorithms and logistic regression w\/ python without using any packages that help out modeling. Thus, accuracy of my submission varied a lot depending on the random initial parameters(to be explained down there), and the fixed parameters are the result of more than 10 submissions.\n\nI am also a starter of Kaggle so I know a lot of people start out here, kernel  section :),  wandering around looking for good examples they can learn from. I also learned a lot from other Kagglers and thanks to them I was able to score 0.78468, so even if you\u2019re not interested in my kernel, I recommend you to visit here and there including Youtube.\n\nFor those of you who have decided to stay put with my kernel, I welcome all sorts of comments and it would be delightful if your comment can improve my result. So let\u2019s dive in shall we?","8b046c52":"## Introduction","0db1a5e1":"#### Cabin\n77% of Cabin data is missing, so though it may be a helpful predictor we're going to <b>drop<\/b>  this feature completely later on.","c6196789":"![](http:\/\/)There were more categories for FamilySize, since I expected three to four. So, let's take a deep dive and merge FamilySize once again into <b>['Alone']<\/b> or <b>not['Alone']<\/b> and see if it also affects in survival."}}