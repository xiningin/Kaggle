{"cell_type":{"7e986010":"code","b08e4c28":"code","b9157ab2":"code","5164da3c":"code","949d4b9d":"code","25340cb5":"code","3c11c286":"code","02a5fc51":"code","53a7b079":"code","86fa9ed8":"code","22f0c4a1":"markdown","17bac3ca":"markdown","c4f7f600":"markdown","54d35ff5":"markdown","92955354":"markdown","b931aa7d":"markdown"},"source":{"7e986010":"import numpy as np  # for handling multi-dimensional array operation\nimport pandas as pd  # for reading data from csv \nimport statsmodels.api as sm  # for finding the p-value \nfrom sklearn.preprocessing import MinMaxScaler  # for normalization\n\nfrom sklearn.model_selection import train_test_split as tts # to split our data into train and test samples.\n#If you want to see how to implement  this split from scratch you can check out my other project Glass Classification using KNN from Scratch in my profile.\n\nfrom sklearn.metrics import accuracy_score # for calculating our accuracy in the end \nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score","b08e4c28":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","b9157ab2":"data.head() ","5164da3c":"diagnosis_map = {'M':1, 'B':-1}  #We use -1 instead of 0 because of how SVM works. \ndata['diagnosis'] = data['diagnosis'].map(diagnosis_map)\n\ndata.drop(data.columns[[-1, 0]], axis=1, inplace=True) # axis 1 -> columns, 'inplace = True' means we do the drop operation inplace and return None.","949d4b9d":"data.head()","25340cb5":"y = data.loc[:, 'diagnosis']  # Select diagnosis column.\n\nX = data.iloc[:, 1:]  # Select columns other than diagnosis.\n\nX_normalized = MinMaxScaler().fit_transform(X.values) # Scaling the values in X between (0,1).\nX = pd.DataFrame(X_normalized)\n\nX_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)\n","3c11c286":"class SVM:\n    \n    def init(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate # \ud835\udefc in formula\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters \n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                self.update(x_i, y[idx])\n\n    def update(self,x,y):\n        distance = 1 - (y * (np.dot(x, self.w) + self.b))\n        hinge_loss = max(0,distance)\n        if(hinge_loss == 0):\n            self.w = self.w - self.lr * (2 * self.lambda_param * self.w)\n        else: \n            self.w = self.w - self.lr * (2 * self.lambda_param * self.w - np.dot(x,y))\n            self.b = self.b + self.lr * y\n        \n        \n    def predict(self, X):\n        eq = np.dot(X, self.w) + self.b\n        return np.sign(eq)\n","02a5fc51":"clf = SVM()\nclf.init()\nclf.fit(X_train.to_numpy(), y_train.to_numpy())","53a7b079":"y_test_predicted = clf.predict(X_test.to_numpy())","86fa9ed8":"print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\nprint(\"recall on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\nprint(\"precision on test dataset: {}\".format(precision_score(y_test.to_numpy(), y_test_predicted)))\n","22f0c4a1":"### Train-test Split and Normalization","17bac3ca":"\u201cSupport Vector Machine\u201d (SVM) is one of the supervised machine learning algorithms. It is generally used for binary classification. Suppose we have a dataset with labeled examples($x_i$, $y_i$); using this dataset algorithms defines a hyperplane that has the following properties:\n- Goal of this hyperplane is to create a seperation between two classes with a maximum margin. \n- Our equation for algorithm is $wx + b = 0$. Our equation for margin is $\\frac{2}{||w||}$\n- $wx_i + b >= 1$ for $y_i = 1$ and $wx_i + b <= 1$ for $y_i = -1$; so our condition for correct classification is equation and class has to have same sign and equation for our condition is $y_i(wx_i + b) >= 1$.\n\nSo to define this hyperplane we need come up with optimal values for w and b. For this we will used the cost function and then apply gradient descent. The lost function we will be using is hinge loss. Equation of hinge loss is $l = max(0, 1 - y_i(wx_i + b))$. What this means is,\n- If l is 0, that means '1 - $y_i(wx_i + b)$' is a smaller or equal to 0. For this to happen $y_i(wx_i + b)$ has to be bigger or equal to 1. We already know that '$y_i(wx_i + b) >= 1$' is our condition for correct classification. So to sum it up, if l is equal to 0 that means classification we did is correct.\n- If l is something other than 0,  that means our classification is wrong.\n\nOn top of the hinge loss to we also add a regularization parameter to balance the margin maximization and loss. Our final function when we add this parameter will be \nJ = \u03bb\\$||w||^2$ + $\\frac{1}{n}$  $\\sum_{i=1}^{n} max(0, 1 - y_i(wx_i + b)) $.\n\nNow that we have the loss function, we take the partial derivatives with respect to the weights and bias to find gradients.<br\/>\nIf $y_i(w*x_i + b)) >= 1$:\n- $\\frac{dJ}{w}$ = 2\u03bbw\n- $\\frac{dJ}{b}$ = 0 <br\/>\n \nElse:\n- $\\frac{dJ}{w}$ = 2\u03bbw - $y_ix_i$\n- $\\frac{dJ}{b}$ = $y_i$\n\nOur update rule based on those derivatives are:\n- w = w - $\\alpha * dw$\n- b = b + $\\alpha * db$\n\n\n","c4f7f600":"# Data Preparation","54d35ff5":"Since SVM accepts numerical values we need to transform the values in diagnosis feature. \n- M --> Malignant(cancer), \n- B --> Benign(harmless)\n\nOther than that first and last colums are useless for us. So lets drop those columns.","92955354":"#### Train-test Split: \n_We split our data into two seperate subsets which are train and test subsets. Like the names suggest we use train subset to train our model and test subset to test how is our model performing. Our objective doing this is to estimate the performance of the machine learning model on data not used to train the model so that we can get an objective result how is our model performing._\n\n#### Normalization:\n_Normalization is basically scaling the values to use some common scale without losing information. If data we are using has some features that has drastically different scale of numbers then normalization is crucial for algorithms that uses numeric values. If we don't use normalization in these kind of algorithms that huge difference could cause problems when we combine those features during modeling._","b931aa7d":"# SVM IMPLEMENTATION\n"}}