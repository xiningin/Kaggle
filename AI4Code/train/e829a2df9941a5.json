{"cell_type":{"3ac93c96":"code","5fe671e4":"code","03a7efb9":"code","1c0c74fb":"code","d258f762":"code","b241426a":"code","a5c0278a":"code","05f4fa4c":"code","fcdc5816":"code","a6633572":"code","8dbcffb0":"code","8392bea0":"code","2c142922":"code","742646bd":"code","182a571c":"code","3c67a571":"code","d375bde7":"code","7970f79d":"code","e3febb8d":"code","772e39ff":"code","64c25276":"code","f257047f":"code","32be897d":"code","ad84c755":"code","4eb1b505":"code","759b03bc":"code","357d39fa":"code","c9abecdd":"code","c5942fc6":"code","5bfc6f46":"code","59644f40":"code","359e29bf":"code","075433e7":"code","4cb934e8":"code","f2fb8c0e":"code","4930e6f3":"code","9c9590f5":"code","57879602":"code","804bd8b2":"code","1448308b":"code","3e8150a9":"markdown","4d9f7b81":"markdown","a5c98d43":"markdown","3802d154":"markdown","b881ebf6":"markdown","59bdb3c3":"markdown","082aa73b":"markdown","6488b123":"markdown","f5e93039":"markdown","8c5c1a51":"markdown","46e6f1fb":"markdown","4e7980c5":"markdown","d3fdec86":"markdown","e372609a":"markdown","ff5137c4":"markdown","ce49e55b":"markdown","7f2a7117":"markdown","cf20c192":"markdown","45439c48":"markdown"},"source":{"3ac93c96":"!pip install -q lucifer-ml","5fe671e4":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statistics\nimport numpy as np\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom luciferml.preprocessing import Preprocess as prep\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report\n\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","03a7efb9":"df = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")\ndf.head()","1c0c74fb":"msno.matrix(df)","d258f762":"msno.heatmap(df)","b241426a":"df.isna().sum()","a5c0278a":"df.corr()","05f4fa4c":"sns.pairplot(df, kind=\"reg\")\nplt.show()","fcdc5816":"fig, axes = plt.subplots(len(df.columns)\/\/3, 3, figsize=(10, 10))\n\ni = 0\nfor triaxis in axes:\n    for axis in triaxis:\n        df.hist(column = df.columns[i], bins = 100, ax=axis)\n        i = i+1","a6633572":"fig, axes = plt.subplots(len(df.columns)\/\/3, 3, figsize=(15, 10))\n\ni = 0\nfor triaxis in axes:\n    for axis in triaxis:\n        sns.histplot(data=df, x=df.columns[i], bins = 100, ax=axis, hue='Potability',element=\"step\")\n        i = i+1","8dbcffb0":"sns.histplot(df[['ph']], kde=True)\nplt.show()","8392bea0":"sns.boxplot(x=df[['ph']].values)\nplt.show()","2c142922":"sns.boxplot(x=df[['ph']].fillna(df[['ph']].median()).values)\nplt.show()","742646bd":"try:\n    statistics.mode(df[['ph']].values.squeeze())\nexcept statistics.StatisticsError as e:\n    print(e)","182a571c":"df[['ph']] = df[['ph']].fillna(df[['ph']].median())","3c67a571":"sns.histplot(df[['ph']], kde=True)\nplt.show()","d375bde7":"sns.boxplot(df[['Sulfate']].values)\nplt.show()","7970f79d":"sns.boxplot(x=df[['Sulfate']].fillna(df[['Sulfate']].median()).values)\nplt.show()","e3febb8d":"df[['Sulfate']]=df[['Sulfate']].fillna(df[['Sulfate']].median())","772e39ff":"sns.histplot(df[['Sulfate']], kde=True)\nplt.show()","64c25276":"sns.boxplot(x=df[['Trihalomethanes']].values)\nplt.show()","f257047f":"df[['Trihalomethanes']] = df[['Trihalomethanes']].fillna(df[['Trihalomethanes']].median())","32be897d":"sns.boxplot(x=df[['Trihalomethanes']].values)\nplt.show()","ad84c755":"sns.histplot(df[['Trihalomethanes']], kde=True)\nplt.show()","4eb1b505":"df.isna().sum()","759b03bc":"def correct_skewness(data):\n    \n    '''\n        Function to correct the data skewness;\n            data: takes data as input.\n                (Funtion takes data for the operation and uses except column to exclude that particular colum\n                from the datset.)\n    '''\n    data_set = prep.skewcorrect(data, except_columns=['Potability'])\n    return data_set\n\ndata_set = correct_skewness(df)","357d39fa":"scaler = StandardScaler()\nX = scaler.fit_transform(df.iloc[:,:-1])\ny = df.iloc[:,-1]","c9abecdd":"# After balancing the dataset was performing bit better\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","c5942fc6":"plt.figure(figsize=(18, 18))\nplt.subplot(1, 2, 1)\nplt.title(\"BEFORE\")\ndf['Potability'].value_counts().plot(kind='pie',autopct='%1.2f%%',colors=['lightcoral','lime'], startangle=90,textprops={'fontsize': 14})\nplt.subplot(1, 2, 2)\nplt.title(\"AFTER\")\ny.value_counts().plot(kind='pie',autopct='%1.2f%%',colors=['lightcoral','lime'], startangle=90,textprops={'fontsize': 14})","5bfc6f46":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","59644f40":"def make_model(name, model, parameters=None, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test):\n    model_og = model\n    model_og.fit(X_train, y_train)\n    print(name)\n    print(\"-\"*10)\n    print(\"Scores without GRIDSEARCH\")\n    y_pred = model_og.predict(X_test)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)\n    plt.show()\n    print(\"-\"*10)\n    \n    if parameters:\n        model_with = GridSearchCV(model, parameters)\n        model_with.fit(X_train, y_train)\n        print(\"Best Params : \", model_with.best_params_)\n        print(\"-\"*10)\n        print(\"Scores with GRIDSEARCH\")\n        y_pred = model_with.predict(X_test)\n        report = classification_report(y_test, y_pred, output_dict=True)\n        sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)\n        plt.show()\n        print(\"-\"*10)\n        return model_og, model_with.best_estimator_\n    else: \n        return model_og","359e29bf":"LR, LR_grid = make_model(\"Logistic Regression\", LogisticRegression(), {'penalty':['l1', 'l2', 'elasticnet', 'none'], 'C':[1, 10]})","075433e7":"svc, svc_grid = make_model('SVC', svm.SVC(), {'kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'C':[1, 10], 'degree':[3, 5, 7]})","4cb934e8":"tree, tree_grid = make_model(\"Desicion Tree\", DecisionTreeClassifier(), {'criterion': ['gini', 'entropy'], 'splitter': ['best','random']})","f2fb8c0e":"knn, knn_grid = make_model('K-Nearest', KNeighborsClassifier(), {'n_neighbors':[3, 5, 9, 7], 'weights' : ['uniform', 'distance']})","4930e6f3":"RF, RF_grid = make_model(\"Random Forest\", RandomForestClassifier(), {'n_estimators':[100, 150, 300, 250], 'criterion' : [\"gini\", \"entropy\"]})","9c9590f5":"NB = make_model(\"Navie Bayes\", GaussianNB())","57879602":"GRB = make_model(\"GRB\", GradientBoostingClassifier())","804bd8b2":"Ada = make_model(\"ADA BOOST\", AdaBoostClassifier())","1448308b":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nestimator_list = [\n    ('knn',knn_grid),\n    ('svm',svc_grid),\n    ('DT',tree_grid),\n    ('rf',RF),\n    ('GRB', GRB)\n]\n\n# Build stack model\nstack_model = StackingClassifier(\n    estimators=estimator_list, final_estimator=LogisticRegression()\n)\n\nstack_model.fit(X_train, y_train)\ny_pred = stack_model.predict(X_test)\n\nreport = classification_report(y_test, y_pred, output_dict=True)\nsns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)\nplt.show()","3e8150a9":"--------------","4d9f7b81":"<textarea disabled name=\"comments\" id=\"comments\" style=\"width:97%;font:36px\/44px cursive;border:10px double yellowgreen;text-align:center\">\nImports\n<\/textarea><br>","a5c98d43":"----","3802d154":"# Filling Missing Values","b881ebf6":"Calculating total missing values","59bdb3c3":"---","082aa73b":"## Missing Values","6488b123":"<textarea disabled name=\"comments\" id=\"comments\" style=\"width:97%;font:36px\/44px cursive;border:10px double yellowgreen;text-align:center\">\nMODELS\n<\/textarea><br>","f5e93039":"**Infrence**: None of them following any relation","8c5c1a51":"As in the above graphs, the count of outliers are there so we are using **median** and not **mean**. And we can not use **mode** since all values are unique.\n<br>\n![image.png](attachment:c57e0d0c-b22e-45a3-8b9a-7552ce1863fd.png)","46e6f1fb":"<textarea disabled name=\"comments\" id=\"comments\" style=\"width:97%;font:36px\/44px cursive;border:10px double yellowgreen;text-align:center\">\nEDA\n<\/textarea><br>","4e7980c5":"---","d3fdec86":"------------------------------------------------------------","e372609a":"Please Up-vote \ud83d\udd4a\ufe0f","ff5137c4":"----","ce49e55b":"## Correlation","7f2a7117":"<b> NO Multicollinearity <\/b>\n><b>What Is Multicollinearity?<\/b>\n<br>\nMulticollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. Multicollinearity can lead to skewed or misleading results when a researcher or analyst attempts to determine how well each independent variable can be used most effectively to predict or understand the dependent variable in a statistical model.","cf20c192":"Checking if there is any pattern in missing values","45439c48":"**Infrence**: Solids is skewed"}}