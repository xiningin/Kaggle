{"cell_type":{"399336a5":"code","8fce0d4c":"code","34fed8be":"code","805dd954":"code","ef1ed3ae":"code","a5e33457":"code","fc045997":"code","3a384d37":"code","fb195357":"code","7869dce2":"code","17ea8f53":"code","ff109136":"code","f11fff6a":"code","69bd1a4e":"code","9057578c":"code","aa1e4c0c":"code","81910260":"code","f72ca1f5":"code","b7f7e6f8":"code","c5e36160":"code","e8a67930":"code","87393fb2":"code","b2fc34d8":"markdown","01c3aebd":"markdown","e119969b":"markdown","8956b8c0":"markdown","536585e0":"markdown","078f0618":"markdown","6ff18296":"markdown","24f6e2e7":"markdown","f5a71dd9":"markdown","9c2060fd":"markdown","6d3c4714":"markdown","44393007":"markdown","93f66a95":"markdown","9140ec23":"markdown","310bdb08":"markdown","bf4814f2":"markdown","a203b07b":"markdown","9cdd0280":"markdown","3777efa3":"markdown","c89aa6bc":"markdown"},"source":{"399336a5":"!pip install tf-nightly-gpu-2.0-preview tfp-nightly\n!pip install -q pydot\n!apt-get install graphviz\n!pip install keras-tqdm","8fce0d4c":"# plotting inline\n%matplotlib inline\n\n# importing necessary modules\nimport keras\nimport random\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as sp\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Activation, concatenate, Input, Embedding\nfrom tensorflow.keras.layers import Reshape, Concatenate, BatchNormalization, Dropout, Add, Lambda\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom copy import deepcopy\nfrom keras_tqdm import TQDMNotebookCallback\n\n# turning off automatic plot showing, and setting style\nplt.ioff()\nplt.style.use('bmh')","34fed8be":"# setting seed \nnp.random.seed(10)\n\n# generating big and small datasets\nX = np.clip(np.random.normal(0.0, 1.0, 1000).reshape(-1,1), -3, 3)\n\n# let us generate a grid to check how models fit the data\nx_grid = np.linspace(-5, 5, 1000).reshape(-1,1)\n\n# defining the function - noisy\nnoise = lambda x: sp.norm(0.00, 0.01 + (x**2)\/10)\ntarget_toy = lambda x: (x + 0.3*np.sin(2*np.pi*(x + noise(x).rvs(1)[0])) + \n                        0.3*np.sin(4*np.pi*(x + noise(x).rvs(1)[0])) + \n                        noise(x).rvs(1)[0] - 0.5)\n\n# defining the function - no noise\ntarget_toy_noiseless = lambda x: (x + 0.3*np.sin(2*np.pi*(x)) + 0.3*np.sin(4*np.pi*(x)) - 0.5)\n\n# runnning the target\ny = np.array([target_toy(e) for e in X])\ny_noiseless = np.array([target_toy_noiseless(e) for e in x_grid])","805dd954":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.5, markersize=5)\n#plt.plot(x_grid, y_noiseless, 'r--')\nplt.title('Data for estimating uncertainty and risk')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.legend();\nplt.show()","ef1ed3ae":"# function to get a randomized prior functions model\ndef get_regular_nn():\n\n    # shared input of the network\n    net_input = Input(shape=(1,), name='input')\n\n    # trainable network body\n    trainable_net = Sequential([Dense(16, 'elu'),\n                                Dense(16, 'elu'),\n                                Dense(16, 'elu')], \n                               name='layers')(net_input)\n    \n    # trainable network output\n    trainable_output = Dense(1, activation='linear', name='output')(trainable_net)\n\n    # defining the model and compiling it\n    model = Model(inputs=net_input, outputs=trainable_output)\n    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n    \n    # returning the model \n    return model","a5e33457":"# generating the model\nregular_nn = get_regular_nn();\n\n# fitting the model\nregular_nn.fit(X, y, batch_size=16, epochs=500, verbose=0)","fc045997":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.5, markersize=5)\nplt.plot(x_grid, regular_nn.predict(x_grid), label='neural net fit', color='tomato', alpha=0.8)\nplt.title('Neural network fit for median expected value')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-3.5,3.5); plt.ylim(-5, 3)\nplt.legend();\nplt.show()","3a384d37":"# implementing the tilted (quantile) loss\nimport tensorflow.keras.backend as K\ndef tilted_loss(q,y,f):\n    e = (y-f)\n    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\n\n# losses for 10th, 50th and 90th percentile\nloss_10th_p = lambda y,f: tilted_loss(0.10,y,f)\nloss_50th_p = lambda y,f: tilted_loss(0.50,y,f)\nloss_90th_p = lambda y,f: tilted_loss(0.90,y,f)","fb195357":"# function to get a randomized prior functions model\ndef get_quantile_reg_nn():\n\n    # shared input of the network\n    net_input = Input(shape=(1,), name='input')\n\n    # trainable network body\n    trainable_net = Sequential([Dense(16, 'elu'),\n                                Dense(16, 'elu'),\n                                Dense(16, 'elu')], \n                               name='shared')(net_input)\n    \n    # trainable network output\n    output_10th_p = Sequential([Dense(8, activation='elu'), \n                                Dense(1, activation='linear')],\n                               name='output_10th_p')(trainable_net)\n    output_50th_p = Sequential([Dense(8, activation='elu'), \n                                Dense(1, activation='linear')],\n                               name='output_50th_p')(trainable_net)\n    output_90th_p = Sequential([Dense(8, activation='elu'), \n                                Dense(1, activation='linear')],\n                               name='output_90th_p')(trainable_net)\n    \n    # defining the model and compiling it\n    model = Model(inputs=net_input, outputs=[output_10th_p, output_50th_p, output_90th_p])\n    model.compile(loss=[loss_10th_p, loss_50th_p, loss_90th_p], optimizer='adam')\n    \n    # returning the model \n    return model","7869dce2":"# checking final architecture\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(get_quantile_reg_nn()).create(prog='dot', format='svg'))","17ea8f53":"# generating the model\nquantile_nn = get_quantile_reg_nn();\n\n# fitting the model\nquantile_nn.fit(X, [y]*3, batch_size=16, epochs=500, verbose=0)","ff109136":"# output of the neural net\nquantile_output = np.array(quantile_nn.predict(x_grid)).reshape(3, 1000)\n\n# getting quantiles\noutput_10th_p = quantile_output[0,:]\noutput_50th_p = quantile_output[1,:]\noutput_90th_p = quantile_output[2,:]","f11fff6a":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.5, markersize=5)\nplt.plot(x_grid, output_10th_p, label='10th and 90th percentile', color='dodgerblue', linewidth=1.8, alpha=0.8)\nplt.plot(x_grid, output_50th_p, label='50th percentile', color='tomato', linewidth=1.8, alpha=0.8)\nplt.plot(x_grid, output_90th_p, color='dodgerblue', linewidth=1.8, alpha=0.8)\nplt.title('Estimating risk: Neural network fit for median, 10th and 90th percentile')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-3.5,3.5); plt.ylim(-5, 3)\nplt.legend();\nplt.show()","69bd1a4e":"# function to get a randomized prior functions model\ndef get_quantile_reg_rpf_nn():\n\n    # shared input of the network\n    net_input = Input(shape=(1,), name='input')\n\n    # trainable network body\n    trainable_net = Sequential([Dense(16, 'elu'),\n                                Dense(16, 'elu'),\n                                Dense(16, 'elu')], \n                               name='trainable_shared')(net_input)\n    \n    # trainable network outputs\n    train_out_1 = Sequential([Dense(8, activation='elu'), \n                              Dense(1, activation='linear')],\n                              name='train_out_1')(trainable_net)\n    train_out_2 = Sequential([Dense(8, activation='elu'), \n                              Dense(1, activation='linear')],\n                              name='train_out_2')(trainable_net)\n    train_out_3 = Sequential([Dense(8, activation='elu'), \n                              Dense(1, activation='linear')],\n                              name='train_out_3')(trainable_net)\n    \n    # prior network body\n    prior_net = Sequential([Dense(16, 'elu', kernel_initializer='glorot_normal', \n                                  trainable=False),\n                            Dense(16, 'elu', kernel_initializer='glorot_normal', \n                                  trainable=False),\n                            Dense(16, 'elu', kernel_initializer='glorot_normal', \n                                  trainable=False)], \n                           name='prior_shared')(net_input)\n\n    # prior network outputs\n    prior_out_1 = Dense(1, 'elu', kernel_initializer='glorot_normal', \n                        trainable=False, name='prior_out_1')(prior_net)\n    prior_out_2 = Dense(1, 'elu', kernel_initializer='glorot_normal', \n                        trainable=False, name='prior_out_2')(prior_net)\n    prior_out_3 = Dense(1, 'elu', kernel_initializer='glorot_normal', \n                        trainable=False, name='prior_out_3')(prior_net)\n\n    # using a lambda layer so we can control the weight (beta) of the prior network\n    prior_out_1 = Lambda(lambda x: x * 3.0, name='prior_scale_1')(prior_out_1)\n    prior_out_2 = Lambda(lambda x: x * 3.0, name='prior_scale_2')(prior_out_2)\n    prior_out_3 = Lambda(lambda x: x * 3.0, name='prior_scale_3')(prior_out_3)\n    \n    # adding all the outputs together\n    add_out_1 = add([train_out_1, prior_out_1], name='add_out_1')\n    add_out_2 = add([train_out_2, prior_out_2], name='add_out_2')\n    add_out_3 = add([train_out_3, prior_out_3], name='add_out_3')\n    \n    # defining the model and compiling it\n    model = Model(inputs=net_input, outputs=[add_out_1, add_out_2, add_out_3])\n    model.compile(loss=[loss_10th_p, loss_50th_p, loss_90th_p], optimizer='adam')\n    \n    # returning the model \n    return model","9057578c":"# checking final architecture\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(get_quantile_reg_rpf_nn()).create(prog='dot', format='svg'))","aa1e4c0c":"class MyMultiOutputKerasRegressor(KerasRegressor):\n    \n    # initializing\n    def __init__(self, **kwargs):\n        KerasRegressor.__init__(self, **kwargs)\n        \n    # simpler fit method\n    def fit(self, X, y, **kwargs):\n        KerasRegressor.fit(self, X, [y]*3, **kwargs)","81910260":"# wrapping our base model around a sklearn estimator\nbase_model = MyMultiOutputKerasRegressor(build_fn=get_quantile_reg_rpf_nn, \n                                         epochs=500, batch_size=16, verbose=0)\n\n# create a bagged ensemble of 10 base models\nbag = BaggingRegressor(base_estimator=base_model, n_estimators=10, verbose=2)","f72ca1f5":"# fitting the ensemble\nbag.fit(X, y)","b7f7e6f8":"# output of the neural net\nquantile_output = np.array([np.array(e.predict(x_grid)).reshape(3, 1000) for e in bag.estimators_])","c5e36160":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.5, markersize=5)\nplt.plot(x_grid, quantile_output[0,0,:], label='10th and 90th percentile', color='dodgerblue', linewidth=1, alpha=0.5)\nfor i in range(1,10):\n    plt.plot(x_grid, quantile_output[i,0,:], color='dodgerblue', linewidth=1, alpha=0.5)\nplt.plot(x_grid, quantile_output[0,1,:], label='50th percentile', color='tomato', linewidth=1, alpha=0.8)\nfor i in range(1,10):\n    plt.plot(x_grid, quantile_output[i,1,:], color='tomato', linewidth=1, alpha=0.8)\nfor i in range(10):\n    plt.plot(x_grid, quantile_output[i,2,:], color='dodgerblue', linewidth=1, alpha=0.5)\nplt.title('Estimating risk and uncertainty: Randomized Prior Functions fit for median, 10th and 90th percentile\\nShowing individual ensemble members (posterior samples)')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-3.5,3.5); plt.ylim(-5, 3)\nplt.legend();\nplt.show()","e8a67930":"# let us take median and quantiles for each parameter #\n\n# median\np50_median = np.quantile(quantile_output[:,1,:], 0.5, axis=0)\np50_90 = np.quantile(quantile_output[:,1,:], 0.9, axis=0)\np50_10 = np.quantile(quantile_output[:,1,:], 0.1, axis=0)\n\n# 10th percentile\np10_median = np.quantile(quantile_output[:,0,:], 0.5, axis=0)\np10_90 = np.quantile(quantile_output[:,0,:], 0.9, axis=0)\np10_10 = np.quantile(quantile_output[:,0,:], 0.1, axis=0)\n\n# 90th percentile\np90_median = np.quantile(quantile_output[:,2,:], 0.5, axis=0)\np90_90 = np.quantile(quantile_output[:,2,:], 0.9, axis=0)\np90_10 = np.quantile(quantile_output[:,2,:], 0.1, axis=0)","87393fb2":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.5, markersize=5)\nplt.plot(x_grid.reshape(1,-1)[0], p10_median, label='10th and 90th percentile', color='dodgerblue', linewidth=1.5, alpha=0.8)\nplt.fill_between(x_grid.reshape(1,-1)[0], p10_10, p10_90, color='dodgerblue', alpha=0.3, label='uncertainty over 10th and 90th percentiles')\nplt.plot(x_grid.reshape(1,-1)[0], p90_median, color='dodgerblue', linewidth=1.5, alpha=0.8)\nplt.fill_between(x_grid.reshape(1,-1)[0], p90_10, p90_90, color='dodgerblue', alpha=0.3)\nplt.plot(x_grid.reshape(1,-1)[0], p50_median, label='50th percentile', color='tomato', linewidth=1.5, alpha=0.8)\nplt.fill_between(x_grid.reshape(1,-1)[0], p50_10, p50_90, color='tomato', alpha=0.3, label='uncertainty over median')\nplt.title('Estimating risk and uncertainty: Randomized Prior Functions fit for median, 10th and 90th percentile\\nShowing median, 10th and 90th percentile across ensemble members')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-3.5,3.5); plt.ylim(-5, 3)\nplt.legend();\nplt.show()","b2fc34d8":"The fit is reasonable, but there's a lot to improve yet. First, let us add the capacity of estimating **risk** to the network!","01c3aebd":"![](http:\/\/)We then proceed to fit the model. Note that we need (at least I don't know any workarounds) to duplicate our target and pass one copy of $y$ to each of the heads of the network, hence `[y]*3` in the `.fit()` method.","e119969b":"# 5. Conclusion\n\nIn this post, we worked out the difference between two essential measures for decision-making: *risk* and *uncertainty*. We saw that risk can be seen as the intrinsic variance of our data, and can be modelled by a quantile regression. Uncertainty, in the other hand, is the variance of our estimate and can be modelled by a bayesian deep learning algorithm such as Randomized Prior Functions. Joining both worlds, we could create a model that models risk and uncertainty at the same time, being very useful for decision-making.\n\nI hope you liked the post! See you soon!","8956b8c0":"Cool! The last adaptation we have to do is improve the `KerasRegressor` class to work with multi-output models. That's because `sklearn` won't accept the `[y]*3` syntax I used before. ","536585e0":"We show the fit below. It's close to what we would imagine a regular neural network fit would be for this data:","078f0618":"# 4. Uncertainty and risk with Randomized Prior Functions\n\nNow we add uncertainty estimation, by wrapping our quantile regression model around the [Randomized Prior Functions](https:\/\/papers.nips.cc\/paper\/8080-randomized-prior-functions-for-deep-reinforcement-learning.pdf) framework. Randomized Prior Functions provide a simple and principled way to estimate uncertainty in neural networks. In short, we're going to build an bootstrapped ensemble of networks composed by an untrainable *prior* network **$p$**, and a trainable network **$f$**, which are combined to form the final output **$Q$**, through a scaling factor **$\\beta$**:\n\n**$$\\large Q = f + \\beta\\cdot p$$**\n\nThe uncertainty is given by the variance of predictions across ensemble members. Both bootstrapping and ensembling and the use of priors contribute to building uncertainty. If you want a deeper analysis, please do check my [blog post](https:\/\/gdmarmerola.github.io\/intro-randomized-prior-functions\/) about this model.\n\nCool. So let us implement this model in `get_quantile_reg_rpf_nn`, in the code below:","6ff18296":"Cool, the model is implemented thorugh `get_regular_nn`. We then build a model by calling it and fitting it to our data:","24f6e2e7":"Now we're ready to fit the model! We just take our `get_quantile_reg_rpf_nn`, wrap it around the `MyMultiOutputKerasRegressor` and build a bootstrapped ensemble using `BaggingRegressor`!","f5a71dd9":"# Risk and Uncertainty in Deep Learning\n\nNeural networks have been pushing what is possible in a lot of domains and are becoming a standard tool in industry. As they start being a vital part of business decision making, methods that try to open the neural network \"black box\" are becoming increasingly popular. [LIME](https:\/\/github.com\/marcotcr\/lime), [SHAP](https:\/\/github.com\/slundberg\/shap) and [Embeddings](https:\/\/distill.pub\/2019\/activation-atlas\/) are nice ways to explain what the model learned and why it makes the decisions it makes. On the other hand, instead of trying to explain what the model learned, we can also try to get insights about what the model **does not know**, which implies estimating two different quantities: **risk** and **uncertainty**. [Variational Inference](https:\/\/arxiv.org\/abs\/1505.05424), [Monte Carlo Dropout](https:\/\/arxiv.org\/abs\/1506.02142) and [Bootstrapped Ensembles](https:\/\/arxiv.org\/abs\/1602.04621) are some examples of research in this area. \n\nAt first glance, risk and uncertainty may seem to be the same thing, but in reality they are, in some cases, orthogonal concepts. **Risk** stands for the intrinsic volatility over the outcome of a decision: when we roll a dice, for instance, we always **risk** getting a bad outcome, even if we precisely know the possible outcomes. **Uncertainty**, on the other hand, stands for the confusion about what the possible outcomes are: if someone gives us a strange dice we have never used before, we'll have to roll it for a while before we can even **know** what to expect about its outcomes. Risk is a fixed property of our problem, which can't be cleared by collecting more data, while uncertainty is a property of our beliefs, and can be cleared with more data. Actually we can have **uncertainty** over our belief of what the **risk** actually is! \n\nIf this seems strange at first, don't worry: this topic has been the object of [heated discussions](https:\/\/twitter.com\/ianosband\/status\/1014466510885216256) among experts in the area recently. The main question is if a given model estimates the **risk** (*aleatoric uncertainty*) or **uncertainty** (*epistemic uncertainty*). Some references make this discussion very interesting. I put some of them at the end of the post, for your reference. \n\nIn our case, we'll focus on a simple example to illustrate the how the concepts are different and how to use a neural network to estimate them at the same time.\n\n# Why is this relevant?\n\nRisk measures the volatility in making a decision, while uncertainty measures the condifence in our belief about this volatility. Both measures are essential for decision making, particularly in decisions that put a lot of resources at stake. For instance, suppose you use a model to sell your house. A model with a good risk estimate will tell you the volatility in closing price given a specified tolerance of days on-market. Then, a model with a good uncertainty estimate will tell you how your closing price volatility estimate is reliable, given the amount of data you have. In this post, we'll simulate a synthetic case for you to understand and run a model that can do both estimates at the same time. \n","9c2060fd":"This problem is good for measuring both risk and uncertainty. Risk gets bigger where the intrinsic noise from the data generating process is larger, which in this case is away from the origin, due to our choice of $\\epsilon \\sim \\mathcal{N}(0, 0.01 + 0.1 \\cdot x^2)$. Uncertainty gets bigger where there's less data, which is also away from the origin, due to the distribution of $x$ being a normal  $x \\sim N(0.0,1.0)$. \n\nSo, let us start to build a risk and uncertainty estimating model for this data! The first step is to use a vanilla neural network to estimate expected values.","6d3c4714":"# 2. Expected values with regular neural network\n\nLet us start with the simplest model: a vanilla neural network. Below, we build the `get_regular_nn` function to tidy up the compilation of the model. Warming up for the next challenge of estimating risk, we use `mean_absolute_error` as the loss function, which will try estimating the median expected value at each $x$.","44393007":"Then, we build the function `get_quantile_reg_nn` to generate our model. The model is a multi-head MLP with one output (and corresponding loss function) for each percentile. We have a shared network `trainable_net`, which connects to three heads `output_xxth_p`, which will output the corresponding quantiles.","93f66a95":"Seems like there's a lot going on here, but you don't need to worry. We have a shared `net_input` for both `prior_net` and `trainable_net`, which in turn are just 3-layer MLPs. In the `trainable_net`, we let the weights be trained by backpropagation, while in the `prior_net` we lock them by setting the `trainable` parameter to `False`. Both nets have three output heads, `train_out_x` and `prior_out_x`, each for each quantile, still keeping the prior locked to training. Furthermore, we apply a `Lambda` layer to `prior_out_x`, which multiplies its output by 3. This is our implementation of **$\\beta$** from the original formula! Finally, the outputs of prior and trainable are added together via an `add` layer to complete the model.\n\nTake a look at the architecture below to see if it makes sense to you. In short, our model is composed by two parallel multi-output networks, where one of them is allowed to train and the other is not. ","9140ec23":"# 1. Data\n\nWe'll use the same data generating process of my [last post](https:\/\/gdmarmerola.github.io\/intro-randomized-prior-functions\/), borrowed from [Blundell et. al (2015)](https:\/\/arxiv.org\/pdf\/1505.05424.pdf). I add some heteroskedastic noise and use a gaussian distribution to generate $X$, so that risk and uncertainty are bigger when we get far from the origin. The process will look like this:\n\n$$ y = x + 0.3 \\cdot{} sin(2\u03c0(x + \\epsilon)) + 0.3 \\cdot{} sin(4 \\cdot{} \\pi \\cdot{}(x + \\epsilon)) + \\epsilon$$\n\nwhere $\\epsilon \\sim \\mathcal{N}(0, 0.01 + 0.1 \\cdot x^2)$ and $x \\sim N(0.0,1.0)$:","310bdb08":"The result makes a lot of sense. The network learned the shape of our data's distribution, effectively estimating risk. This is very beneficial for decision making: we  can actually quantify how much we are putting at stake when we choose to perform some action given the network's prediction.\n\nBut that leads to the next question: how *reliable* are these estimates of risk? That's where uncertainty comes into play, as we'll see next.","bf4814f2":"# 3. Risk with quantile regression\n\nWe add risk to our model by making the network perform quantile regression. Specifically, we will implement in `get_quantile_reg_nn` a network to estimate the median (50th percentile), the 10th and 90th percentile. The quantiles will give us the sense of volatility we want, and will be our proxy of risk. It is not hard to do that: we just have to change the objective function from L2 loss (mean squared error) to L1 loss (mean absolute error) for the median, and use the [quantile loss](https:\/\/heartbeat.fritz.ai\/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0) for the 10th and 90th percentiles. I heavily used [Deep Quantile Regression](https:\/\/towardsdatascience.com\/deep-quantile-regression-c85481548b5a) by Sachin Abeywardana as inpiration, and I really recommend the read!\n\nFirst, we implement the quantile (tilted) loss in Keras language and build loss functions for the 10th, 50th and 90th percentile:","a203b07b":"The results are really cool. Below, I'm plotting the median, 10th and 90th percentile of each ensemble member. If you look at the curves, you'll see that ensemble members agree a lot around the origin, but start to disagree more further away. This \"disagreement\" is our uncertainty! This effectively measures the variance of our estimates, giving us a distribution over *functions* for the median, 10th and 90th percentiles.","9cdd0280":"We can see the multi-output architecture in the following diagram:","3777efa3":"# Risk vs. Uncertainty Discussion\n\nIf you want to read more about risk and uncertainty, look at the references below:\n\n* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https:\/\/arxiv.org\/pdf\/1506.02142.pdf): arguments that dropout (at test time) in NNs has a connection to gaussian processes and motivates its usage as a bayesian method\n* [Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout](http:\/\/bayesiandeeplearning.org\/2016\/papers\/BDL_4.pdf): motivates that dropout with fixed $p$ estimates risk and not uncertainty\n* [Randomized Prior Funcions in Deep Reinforcement Learning](https:\/\/papers.nips.cc\/paper\/8080-randomized-prior-functions-for-deep-reinforcement-learning.pdf): shows the shortcomings with other techniques and motivates use of bootstrap and prior functions","c89aa6bc":"For a more familiar view, we can also plot the 80% confidence interval for our distributions of *functions*. Here we see the uncertainty more clearly, and how it gets bigger as we move away from the data."}}