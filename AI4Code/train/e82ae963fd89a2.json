{"cell_type":{"28887ac7":"code","a9b7aa9d":"code","50aafe00":"code","3e5e334d":"code","c7071207":"code","7cefd91e":"code","7b603be9":"code","34975fdf":"code","d3e26d8e":"code","29ef8fcf":"code","93d7d88c":"code","ea7feeb0":"code","c272d2f3":"code","d462ec5e":"code","bf77ad05":"code","eb2bfaf6":"code","01153c4b":"code","3a077b60":"code","ef3efb7f":"code","76bbad55":"code","f6787b59":"code","6d4272d8":"code","e9bb0161":"code","bcdddc4e":"code","2270b40c":"code","105c93c2":"code","97b50892":"code","fef565b4":"code","0df53419":"code","eb101db2":"code","06e55f7e":"markdown","efd5c882":"markdown","2f55872f":"markdown","365ed65b":"markdown","122315d9":"markdown","81a06305":"markdown"},"source":{"28887ac7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9b7aa9d":"# Make DataFrame of the given data \ndata = pd.DataFrame({\"A\":[1,2,4,1,2,4], \n                    \"B\":[4,5,6,7,8,9], \n                    \"C\":[0,0,0,0,0,0],\n                    \"D\":[1,1,1,1,1,1]})\ndata.head()","50aafe00":"from sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(data)","3e5e334d":"# check which feature is selected\nvar_thres.get_support()","c7071207":"constant_feature=[column for column in data.columns if column not in data.columns[var_thres.get_support()]]\nconstant_feature","7cefd91e":"#let's drop this feature\ndata.drop(constant_feature,axis=1,inplace=True)\ndata.head()","7b603be9":"df=pd.read_csv('\/kaggle\/input\/santaner\/train.csv',nrows=10000)\ndf.head()","34975fdf":"df.shape","d3e26d8e":"X=df.drop(labels=[\"TARGET\"],axis=1)\ny=df[\"TARGET\"]","29ef8fcf":"from sklearn.model_selection import train_test_split\n# seperate dataset into train and test\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=0)\nX_train.shape,X_test.shape","93d7d88c":"# let's apply variance threshold\n\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_train)","ea7feeb0":"# finding non constant feature\nprint(\"Original Feature size: \",len(X.columns))\nprint(\"After Applying Variance Thresold\",var_thres.get_support().sum())","c272d2f3":"constant_feature=[column for column in X_train.columns if column not in X_train.columns[var_thres.get_support()]]\nconstant_feature","d462ec5e":"# let's drop constant feature\nX_train.drop(constant_feature,axis=1,inplace=True)\nX_train.shape","bf77ad05":"from sklearn.datasets import load_boston\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","eb2bfaf6":"# loading the dataset\ndata=load_boston()\ndf=pd.DataFrame(data.data,columns=data.feature_names)\ndf['MEDV']=data.target\ndf.head()","01153c4b":"data.feature_names","3a077b60":"X=df.drop('MEDV',axis=1)\ny=df['MEDV']","ef3efb7f":"X.head()","76bbad55":"# train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\nX_train.shape,X_test.shape","f6787b59":"import seaborn as sns\n# using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X_train.corr()\nsns.heatmap(cor,annot=True,cmap=plt.cm.CMRmap_r)\nplt.show()","6d4272d8":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\ndef correlation(dataset,threshold):\n    col_corr=set()\n    corr_matrix=dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])>threshold:\n                colname=corr_matrix.columns[i]\n                col_corr.add(colname)\n    return col_corr","e9bb0161":"corr_features=correlation(X_train,0.7)\ncorr_features","bcdddc4e":"X_train.drop(corr_features,axis=1)\nX_test.drop(corr_features,axis=1)","2270b40c":"df=pd.read_csv('\/kaggle\/input\/santaner\/train.csv',nrows=10000)\ndf.head()","105c93c2":"X=df.drop(labels=[\"TARGET\"],axis=1)\ny=df[\"TARGET\"]","97b50892":"from sklearn.model_selection import train_test_split\n# seperate dataset into train and test\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=0)\nX_train.shape,X_test.shape","fef565b4":"corrmat=X_train.corr()\nfig,ax=plt.subplots()\nfig.set_size_inches(11,11)\nsns.heatmap(corrmat)","0df53419":"corr_features=correlation(X_train,0.8)\nlen(corr_features)","eb101db2":"X_train.drop(corr_features,axis=1)\nX_train.shape","06e55f7e":"### Variance Threshold\nDocs: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html\n\nFeature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.","efd5c882":"### let's practice on bigger dataset\n\nDataset link :https:\/\/www.kaggle.com\/c\/santander-customer-satisfaction\/data?select=train.csv","2f55872f":"## 2. Feature Selection - With Correlation","365ed65b":"## let's practice on bigger dataset","122315d9":"In this step we will be removing the feature which have constant feature which are actually not important for solving the problem statement","81a06305":"## Feature Selection- Dropping constant features"}}