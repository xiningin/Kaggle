{"cell_type":{"fcfc39a9":"code","536a085e":"code","cb90c1ef":"code","dec1b177":"code","4c12b278":"code","6339b02b":"code","3c5ed453":"code","3e445eef":"code","4cf74433":"code","b31a281d":"code","02eec1b4":"code","aa909287":"code","77fad217":"code","d52ecd5e":"code","473bfdd9":"code","c011c847":"code","470af7e2":"code","962b71b9":"code","aded430b":"code","0eb0caf7":"code","e3be457a":"code","138156d5":"code","df11f91b":"code","3be8a515":"markdown","e9a5fdfc":"markdown","18a9455c":"markdown","7571c801":"markdown","7163b61e":"markdown","b0f658a3":"markdown","0f196a2a":"markdown","cf2d073c":"markdown","c9c38abc":"markdown","f3227320":"markdown","c7afdd85":"markdown","83d9efe3":"markdown","1607080b":"markdown","1116894d":"markdown"},"source":{"fcfc39a9":"import pandas as pd\nimport numpy as np\nimport nltk\nimport spacy","536a085e":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_df","cb90c1ef":"print(\"Total Number of Samples \", len(train_df))\nprint(\"\\nTotal Number of Features \", len(train_df.columns))","dec1b177":"train_df['text'].isna().any()","4c12b278":"df = train_df[['id','text','target']]","6339b02b":"df.target.value_counts()","3c5ed453":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nsns.barplot(x=['Normal','Disaster'], y= df.target.value_counts().values)\nplt.show()","3e445eef":"import warnings\nwarnings.filterwarnings(action='ignore')\n\nl = len(df)\ndf.drop_duplicates(subset='text', inplace=True)\nprint(\"Total Duplicates \", l - len(df))","4cf74433":"len(df)","b31a281d":"null_rows = df['text'][df['text'].isna()]\nnull_rows","02eec1b4":"import re","aa909287":"# helper function\ndef clean_text(text):\n    te = str(text).encode('ascii','ignore').decode('UTF-8')\n    te = re.sub(r'@[\\w]+', '', te)\n    te = re.sub(r'https?:\/\/t.co\/[\\w]+', '', te)\n    te = re.sub(r'#', '', te)\n    te = re.sub(r\"RT @[\\w]+:\",'',te)\n    te = re.sub(r\"RT @[\\w]+:\",'',te)\n    te = re.sub(r\" RT \",'',te)\n    te = re.sub(r\"https:\/\/[\\w]+.[\\w]+\/[\\w]+\",'',te)\n    te = re.sub(r\"[][]\",'',te)\n    te = re.sub(r\"&amp\",\"and\", te)\n    # remove the characters [\\], ['] and [\"]\n    text = re.sub(r\"\\\\\", \"\", te)    \n    text = re.sub(r\"\\'\", \"\", text)    \n    text = re.sub(r\"\\\"\", \"\", text)    \n    \n    # convert text to lowercase\n    text = text.strip().lower()\n    \n    # replace punctuation characters with spaces\n    filters='!\"\\'#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n    translate_dict = dict((c, \" \") for c in filters)\n    translate_map = str.maketrans(translate_dict)\n    text = text.translate(translate_map)\n\n    return text","77fad217":"from sklearn.feature_extraction.text import TfidfVectorizer","d52ecd5e":"# Transform each text into a vector of word counts\nvectorizer = TfidfVectorizer(stop_words=\"english\",\n                             preprocessor=clean_text,\n                             ngram_range=(1, 2))\n\ntraining_features = vectorizer.fit_transform(df.text)    ","473bfdd9":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import precision_score, recall_score, make_scorer, f1_score, accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score","c011c847":"est = []\nest.append(('LogisticRegression', Pipeline([('LR', LogisticRegression())])))\nest.append(('BernoulliNB', Pipeline([('BNB', BernoulliNB())])))\nest.append(('MultinomialNB', Pipeline([('MNB', MultinomialNB())])))\nest.append(('LinearSVC', Pipeline([('LNB', LinearSVC())])))","470af7e2":"%%time\n\n# Training\nmodel_scores = {}\n\np_scorer = make_scorer(precision_score)\nr_scorer = make_scorer(recall_score)\nf1_scorer = make_scorer(f1_score)\na_scorer = make_scorer(accuracy_score)\n\nfor i in est:\n    kfold = KFold(n_splits=7, shuffle=True, random_state=4)\n    p_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=p_scorer)\n    r_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=r_scorer)\n    f1_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=f1_scorer)\n    a_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=a_scorer)\n    \n    model_scores.update({ i[0]:{'accuracy': a_scores.mean(), 'f1_score':f1_scores.mean(), 'precision': p_scores.mean(), 'recall':r_scores.mean()} })","962b71b9":"for i in model_scores:\n    print('\\n', i)\n    print('\\n', model_scores[i])","aded430b":"# model with top f1 score\n\ntop_models_score = sorted(model_scores.items(), key=lambda k:k[1]['f1_score'], reverse=True)\ntop_models_score[0]","0eb0caf7":"top_model = dict(est)[top_models_score[0][0]]\ntop_model.fit(training_features, df.target)","e3be457a":"# load test csv\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","138156d5":"test_features = vectorizer.transform(test_df.text)\npredictions = top_model.predict(test_features)","df11f91b":"submission = pd.DataFrame(columns=['id', 'target'])\nsubmission['id'] = test_df['id']\nsubmission['target'] = predictions\n\nsubmission.to_csv('submission.csv', index=False)","3be8a515":"# VECTORIZATION\n*****************\n* Convert the text into numerical features using Tf-idf","e9a5fdfc":"### Check whether the target class is balanced or imbalanced","18a9455c":"# MODEL TRAINING & EVALUATION","7571c801":"# CONCLUSION\n********************\n\n* We will choose the model that gives the best F1 score which is a combination of precision and recall.","7163b61e":"# SIMPLE TEXT CLASSIFICATION STEPS\n*****************\n\n* **SUMMARY OF DATA**\n\n    - Total Samples\n    - Total Features\n    - Check Null Values\n    - Check the balance of the target classes\n**********\n* **CLEANING** \n\n    - Drop Duplicates\n    - Drop Null Values\n    - Resampling for imbalanced classes\n***************\n* **TEXT PREPROCESSING**\n\n    * Removing irrelevant words such as @mentions or http links etc.\n    * Remove Punctuations\n    * Lowercase \n*****************\n* **VECTORIZATION**\n\n    * Convert text into numerical features using Tf-idf\n*************\n* **MODEL TRAINING & EVALUATION**\n\n    * Creating pipeline of simple models\n****************\n* **CONCLUSION**\n\n    * Choosing model with best F1 score\n*******************\n* **PREDICTION**\n\n    * Predicting on test dataset\n******************\n****************","b0f658a3":"### Dropping Duplicates","0f196a2a":"# PREDICTION","cf2d073c":"# SUMMARY OF DATA","c9c38abc":"### Check for null data in the \"text\" column","f3227320":"# LOADING DATA","c7afdd85":"# CLEANING\n******************\n\n* Drop Duplicates\n* Drop Null Values","83d9efe3":"There is a slight difference between the two classes, so we can say it is balanced. Hence would not require any kind of resampling techniques.","1607080b":"### Dropping NaN Values\n\n***************\n\nThere are no NaN values in the df","1116894d":"# TEXT PREPROCESSING\n*************\n\n* Removing irrelevant words such as @mentions or http links etc.\n* Remove Punctuations\n* Lowercase "}}