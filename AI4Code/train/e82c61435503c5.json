{"cell_type":{"f25c6cef":"code","e8139f9a":"code","94e17932":"code","7254a8a2":"code","b3aebcbf":"code","ca5e5d06":"code","746c40ed":"code","bc42bae9":"code","a410f586":"code","9e0238b7":"code","32181b21":"code","0b6cf52c":"code","459cb719":"code","53081a74":"code","66299736":"code","164df01e":"code","2c5c989b":"code","2e1c554a":"code","b7c0a669":"code","55c47c57":"code","b88367ae":"code","ccfbcf9f":"code","a021e514":"code","3707cf23":"code","9b53e079":"code","eeefc098":"code","9c7c24e3":"code","b490f541":"code","bd49bc12":"code","fc3bb466":"code","554f9924":"code","53cf9b9d":"code","3250a769":"code","e91bae92":"code","dd3359fb":"code","61d3a26a":"code","a047f2b3":"code","216efcc2":"code","d9c63261":"code","6a993390":"code","af176de5":"code","bbea49d3":"code","1a86c01e":"code","84499f17":"code","286af176":"code","855f5e05":"code","ee8ff203":"code","e95a90bd":"code","7e7066b5":"code","89a453dd":"code","02f176ba":"code","cd12877d":"code","5d1c6320":"code","2f221149":"code","b69f67e6":"code","eb19c801":"code","bff4397c":"code","dd894c48":"code","65d4adc9":"code","0b0cc83d":"code","8ae52359":"markdown","0076eed8":"markdown","0e003916":"markdown","d9f2777c":"markdown","263d3bcf":"markdown","9a8b60f5":"markdown","3745a297":"markdown","4f42f891":"markdown","c2568622":"markdown","2e6c3044":"markdown","fbf08849":"markdown","2219c7e1":"markdown"},"source":{"f25c6cef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(\"Shape of train data:\",train.shape)\nprint(\"Shape of test data:\",test.shape)","e8139f9a":"total = train.append(test)\nprint(total.shape)","94e17932":"# Missing values heatmap\nsns.heatmap(total.isnull())","7254a8a2":"# It is clear that there are missing values, let's see which fields have missing values and how much\ns = total.isnull().sum()\ns[s>0]","b3aebcbf":"# Let's see some columns\ntotal.head()","ca5e5d06":"# Let's first drop some non necessary fields like name and Ticket\ntrain.drop(['Ticket','Name','PassengerId'],inplace=True, axis=1)\ntest.drop(['Ticket','Name','PassengerId'],inplace=True, axis=1)\nprint(train.columns)\nprint(test.columns)","746c40ed":"train.head()","bc42bae9":"# Let's fill Cabin with 1 if there is a cabin or 0 if there is null\ntrain.loc[~train['Cabin'].isnull(),'Cabin']=1\ntrain.loc[train['Cabin'].isnull(),'Cabin']=0\ntest.loc[~test['Cabin'].isnull(),'Cabin']=1\ntest.loc[test['Cabin'].isnull(),'Cabin']=0","a410f586":"train.isnull().sum()","9e0238b7":"test.isnull().sum()","32181b21":"train['Embarked'].value_counts()","0b6cf52c":"train.loc[train['Sex']=='male', 'Sex']=1\ntrain.loc[train['Sex']=='female', 'Sex']=0\n\ntest.loc[test['Sex']=='male', 'Sex']=1\ntest.loc[test['Sex']=='female', 'Sex']=0\n\ntrain.loc[train['Embarked']=='S', 'Embarked']=0\ntrain.loc[train['Embarked']=='C', 'Embarked']=1\ntrain.loc[train['Embarked']=='Q', 'Embarked']=2\n\ntest.loc[test['Embarked']=='S', 'Embarked']=0\ntest.loc[test['Embarked']=='C', 'Embarked']=1\ntest.loc[test['Embarked']=='Q', 'Embarked']=2","459cb719":"train.head()","53081a74":"test.head()","66299736":"y = train['Survived']\ntrain.drop('Survived',axis=1,inplace=True)","164df01e":"# Now all features are numeric, we can apply KNNImputer\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=1)\nimputer.fit(train)","2c5c989b":"train = imputer.transform(train)\ntest = imputer.transform(test)","2e1c554a":"total.drop(['Ticket','Name','PassengerId','Survived'],axis=1,inplace=True)\ntotal.head()","b7c0a669":"train = pd.DataFrame(train, columns = total.columns)\ntest = pd.DataFrame(test,columns = total.columns)","55c47c57":"# Family = SibSp+Parch\ntrain['Family']=train['SibSp']+train['Parch']\ntest['Family']=test['SibSp']+test['Parch']","b88367ae":"train = pd.concat([train,y],axis=1)\ntrain.head()","ccfbcf9f":"sns.heatmap(train.corr())","a021e514":"# male=1, female=0\nsns.countplot(x=train['Survived'],hue=train['Sex'])","3707cf23":"sns.countplot(x=train['Survived'],hue=train['Pclass'])","9b53e079":"plt.figure(figsize=(20,6))\nax1=plt.subplot(1,2,1)\nax2=plt.subplot(1,2,2)\nsns.violinplot(x=train['Survived'], y=train['Age'],ax=ax1)\nsns.violinplot(x=train['Survived'], y=train['Age'],hue=train['Sex'],ax=ax2)\nplt.show()","eeefc098":"# Let's get dummies for categorical data\n# Let's first combine train and test\ntotal = train.append(test)\ntotal.head()","9c7c24e3":"sns.distplot(total['Age'].where(total['Survived']==0))","b490f541":"def convertAge(total):\n    total.loc[total['Age']<5,'Age']=1\n    total.loc[(total['Age']>=5) & (total['Age']<10),'Age']=2\n    total.loc[(total['Age']>=10) & (total['Age']<20),'Age']=3\n    total.loc[(total['Age']>=20) & (total['Age']<40),'Age']=4\n    total.loc[(total['Age']>=40) & (total['Age']<60),'Age']=5\n    total.loc[(total['Age']>=60),'Age']=6\n    \n    return total\n    \ntotal=convertAge(total)","bd49bc12":"total['Age'].value_counts()","fc3bb466":"# Let's make Pclass, Sex, Cabin, Embarked to strings type because Pandas get_dummies method recognizes string attributes as categorical\ncat = {'Pclass':str, 'Sex':str, 'Cabin':str, 'Embarked':str,'Age':str}\ntotal= total.astype(cat)","554f9924":"total.dtypes","53cf9b9d":"total = pd.get_dummies(total, drop_first=True)\ntotal.head()","3250a769":"train = total.iloc[:891]\ntest = total.iloc[891:]\ny = train['Survived']\ntrain.drop('Survived',inplace=True,axis=1)\ntest.drop('Survived',inplace=True,axis=1)\nprint(train.shape)\nprint(test.shape)\nprint(y.shape)","e91bae92":"def normalizer(sr1, sr2):\n    mini = sr1.min()\n    maxi = sr1.max()\n    sr1 = ((sr1-mini)\/(maxi-mini))\n    sr2 = ((sr2-mini)\/(maxi-mini))\n    return sr1, sr2\ntrain['Fare'], test['Fare'] = normalizer(train['Fare'],test['Fare'])","dd3359fb":"train.head()","61d3a26a":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train,y,random_state=0,test_size=0.2,stratify = y)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a047f2b3":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nparameters = {'n_neighbors':[i for i in range(1,10)]}\n\nknn = KNeighborsClassifier()\ngrd = GridSearchCV(estimator=knn, param_grid=parameters, n_jobs=-1, cv=10, return_train_score=True)\ngrd.fit(X_train,y_train)","216efcc2":"grd.best_params_","d9c63261":"# This is actually rediculous, model was highly overfitting with neighbors=5\nknn = KNeighborsClassifier(n_neighbors=26)\nknn.fit(X_train,y_train)\nprint(\"Train Score:\",knn.score(X_train,y_train))\nprint(\"Test Score:\",knn.score(X_test,y_test))","6a993390":"from sklearn.linear_model import RidgeClassifier\nparameters = {'alpha':[0.01,0.1,1.0,5.0,10,100,1000],'max_iter':[500,1000,1500,2000]}\n\nridge = RidgeClassifier()\ngrd = GridSearchCV(estimator=ridge, param_grid=parameters, n_jobs=-1, cv=10, return_train_score=True)\ngrd.fit(X_train,y_train)","af176de5":"grd.best_params_","bbea49d3":"# Here model performs significantly okay with CV parameters\nridge = RidgeClassifier(alpha=0.01, max_iter=500)\nridge.fit(X_train,y_train)\nprint(\"Train Score:\", ridge.score(X_train,y_train))\nprint(\"Test Score:\", ridge.score(X_test,y_test))","1a86c01e":"import xgboost as xgb\n# xgtrain = xgb.DMatrix(data=X_train, label=y_train)\n# xg_clf = xgb.XGBClassifier(max_depth=3)\n\n# parameters = {'learning_rate':[0.01,0.1,1],'alpha':[0.001,0.01,0.1,1.0,10],'lambda':[0.001,0.01,0.1,1.0,10],'n_estimators':[500,1000,1500]}\n\n# grd = GridSearchCV(estimator=xg_clf, param_grid=parameters, n_jobs=-1)\n# grd.fit(X_train,y_train)","84499f17":"grd.best_params_","286af176":"xg_clf = xgb.XGBClassifier(max_depth=3, alpha=5,reg_lambda=10,learning_rate=0.2,n_estimator=1000)\nxg_clf.fit(X_train,y_train)\nprint('Train score:',xg_clf.score(X_train,y_train))\nprint('Test score:',xg_clf.score(X_test,y_test))","855f5e05":"y_ridge_t = ridge.predict(X_test)\ny_xg_t = xg_clf.predict(X_test)","ee8ff203":"from sklearn.metrics import confusion_matrix,f1_score\nprint(\"F1-Score Ridge:\", f1_score(y_test,y_ridge_t))\nprint(\"Ridge Confusion Matrix:\")\nconfusion_matrix(y_test,y_ridge_t)","e95a90bd":"print(\"F1-Score Ridge:\", f1_score(y_test,y_xg_t))\nprint(\"Ridge Confusion Matrix:\")\nconfusion_matrix(y_test,y_xg_t)","7e7066b5":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nprint(\"NB Train score:\",nb.score(X_train,y_train))\nprint(\"NB Test score:\",nb.score(X_test,y_test))\ny_nb_t = nb.predict(X_test)\nprint(\"F1-Score Ridge:\", f1_score(y_test,y_nb_t))\nprint(\"Confusion Matrix:\")\nconfusion_matrix(y_test, y_nb_t)","89a453dd":"from sklearn.ensemble import VotingClassifier\nvc = VotingClassifier(estimators=[('ridge',ridge),('xgb',xg_clf),('nb',nb)],voting='hard')\nvc.fit(X_train,y_train)\nprint(\"Voting Train score:\",vc.score(X_train,y_train))\nprint(\"Voting Test score:\",vc.score(X_test,y_test))\ny_vc_t = vc.predict(X_test)\nprint(\"F1-Score Ridge:\", f1_score(y_test,y_vc_t))\nprint(\"Confusion Matrix:\")\nconfusion_matrix(y_test, y_vc_t)","02f176ba":"from sklearn.ensemble import StackingClassifier\nstk = StackingClassifier(estimators=[('ridge',ridge),('xgb',xg_clf)],final_estimator=xg_clf)\nstk.fit(X_train,y_train)\nprint(\"Stacking Train score:\",stk.score(X_train,y_train))\nprint(\"Stacking Test score:\",stk.score(X_test,y_test))\ny_stk_t = stk.predict(X_test)\nprint(\"F1-Score Ridge:\", f1_score(y_test,y_stk_t))\nprint(\"Confusion Matrix:\")\nconfusion_matrix(y_test, y_stk_t)","cd12877d":"y_final_t = (0.25*y_ridge_t+0.3*y_xg_t+0.2*y_vc_t+0.3*y_stk_t)\nmask1 = y_final_t<0.8\nmask2 = y_final_t>=0.8\ny_final_t[mask1]=0\ny_final_t[mask2]=1\ny_final_t","5d1c6320":"confusion_matrix(y_test,y_final_t)","2f221149":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_final_t)","b69f67e6":"# Let's prepare final models\nridge.fit(train,y)\nxg_clf.fit(train,y)\nvc.fit(train,y)\nstk.fit(train,y)","eb19c801":"pred_ridge = ridge.predict(test)\npred_xg = xg_clf.predict(test)\npred_vc = vc.predict(test)\npred_stk = stk.predict(test)\npred_final = (0.2*pred_ridge+0.3*pred_xg+0.25*pred_vc+0.25*pred_stk)\nmask1 = pred_final<0.8\nmask2 = pred_final>=0.8\npred_final[mask1]=0\npred_final[mask2]=1\npred_final = pred_final.astype(int)","bff4397c":"test = pd.read_csv('..\/input\/titanic\/test.csv')","dd894c48":"submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':pred_final})","65d4adc9":"submission.head()","0b0cc83d":"submission.to_csv('submission.csv',index=False)","8ae52359":"Lets seperate the train and test","0076eed8":"# KNN Imputer - scikit-learn, Voting and Stacking\nMissing Value imputations using k-Nearest Neighbours.","0e003916":"### Models - KNN, Ridge, Naive Bayes, XGBoost, Voting and Stacking","d9f2777c":"Missing values are imputed but the returned 'train' and 'test' are numpy arrays. We will convert them to dataframe","263d3bcf":"Lets do some trick! \nPS- Its non conventional","9a8b60f5":"Lets also convert age to categories as it seems 20-40 age group has maximum number of deceased.","3745a297":"Lets now apply voting classifiers and stacking","4f42f891":"We have seen models and their accuracies but let's see more metrics","c2568622":"Everything is done, we will now do some feature engineering.","2e6c3044":"#### we will proceed with the titanic data\nKNN stands for k-Nearest Neighbors. It is sometimes useful to consult neighbors to fill in missing values, specially when there is an inherent relation.\n\n### PS- Although, Notebook score is significant, I did not create it to perform better on leaderboard, it is to illutrate KNN Imputer, Voting and Stacking","fbf08849":"Imputation of missing values is one of the most important task while preprocessing the data. Althoigh dropping the missing valued objects is an option, it causes data loss.\nThere are several ways to impute the missing values which include filling the missing values with mean, median and mode.\nHowever, filling the missing values with univariate analysis and central tendency measures are not always appropriate.","2219c7e1":"KNN needs all fields to have numeric values. So we need to convert text to numbers first"}}