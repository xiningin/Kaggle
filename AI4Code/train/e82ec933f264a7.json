{"cell_type":{"493cb4f8":"code","0197392e":"code","8b4b49ac":"code","0228e263":"code","c4c42f63":"code","2e37c70a":"code","da148256":"code","3e541106":"code","49515a69":"code","32f57d4f":"code","1861af72":"code","cf9c7303":"code","93060c73":"code","2bdfe379":"code","a7b92556":"code","a8f10961":"code","a82d617e":"code","a978aa5f":"code","ea2e5fbf":"code","ff93c39f":"code","1925c568":"code","399f861a":"code","fdb43a9f":"code","d46d22b9":"code","ab8dfc2f":"code","cafdb1c7":"code","19b1c0a0":"code","581c43e5":"code","709b7242":"code","45bddd66":"code","4ef7a9bb":"code","ad879c25":"code","736a95a6":"code","35609790":"code","c37055d7":"code","5020f95d":"code","1fae42b9":"code","393a70f6":"code","e42e226a":"code","f37f1124":"code","05195873":"code","ccf1e955":"code","19616824":"markdown","80b8f38f":"markdown","f186c2b9":"markdown","7be5b33d":"markdown","678f3be3":"markdown","ff365ff5":"markdown","ca876f57":"markdown","81a0db19":"markdown","ca641f2a":"markdown","583297f4":"markdown","53e63847":"markdown","dbb95803":"markdown","bb9bb892":"markdown","2fa90b6e":"markdown","ba19581d":"markdown","9706d56f":"markdown","fc9a8fc5":"markdown","6a4fef9d":"markdown","f2149250":"markdown","f581fad4":"markdown","7201f78b":"markdown","7280ca29":"markdown","e953c69d":"markdown","66be33e1":"markdown","709fd96f":"markdown","949e6cc9":"markdown","63265f34":"markdown","ec893bdb":"markdown","9e9f2dd1":"markdown","c0372cbb":"markdown","c380903e":"markdown"},"source":{"493cb4f8":"# The code was generated by FastBenchmark telegram bot\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport xgboost as xgb\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_squared_log_error\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\n\nwarnings.simplefilter(action='ignore')\npd.options.mode.chained_assignment = None\n\n# which correlation is good enough?\ncorrelation_level = 0.7\n\n# how much points to draw to visualize correlations\nsequence_length = 100\n\n# list of errors\nerrors = []\n# list of columns to delete\nto_drop = []\n\n# coefficient to make plots for categories (if more categories - will not plot it)\ncat_coef = 10\n\n# reading the data\ndata = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv', sep = ',')\n\n\n# cheking if target column is in columns\ntarget_col_name = 'target'\n\n\n\nif target_col_name not in data.columns:\n    print('No ' + str(target_col_name) + ' in test set')\n    errors.append('No ' + str(target_col_name) + ' in test set')\n    to_drop.append(target_col_name)\n    quit()\n\n# coefficient to check if categories are rare enough\ncat_check = 0.01*len(data[target_col_name])\n\n# make train and test\ntrain, test = train_test_split(data, test_size=0.2)\n\n# remove rows were target is Na\ntrain = train[train[target_col_name].notna()]\nif target_col_name in test.columns:\n    test = test[test[target_col_name].notna()]\n\n","0197392e":"train.head()\n","8b4b49ac":"sns.pairplot(train, hue=target_col_name)","0228e263":"def nan_numeric(data):\n    result = np.nanmean(data)\n    return result\n\n\ndef nan_categorical(data):\n    result = -1\n    return result\n","c4c42f63":"# TARGET COLUMN (categorical) - target\n\ncol_name = train.columns[13]\ntrain[col_name] = train[col_name].astype(str)\nif col_name in test.columns:\n    test[col_name] = test[col_name].astype(str)\ncategoryType = 'category_bool'\ntry:\n    sns.displot(x=train[col_name])\nexcept:\n    sns.distplot(train[col_name])\n","2e37c70a":"if categoryType == 'category' or categoryType == 'category_bool':\n    # replacing category names with numbers\n    encoder = LabelEncoder()\n    encoder.fit(train[col_name])\n    target = encoder.transform(train[col_name])\n    target_real_values = {'Category name': encoder.classes_, 'Category value': range(len(encoder.classes_))}\n    if col_name in test.columns:\n        test_target = encoder.transform(test[col_name])\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    target = pd.to_numeric(target, errors='coerce')\n    target[pd.isna(target)] = np.nanmean(target)\n    if col_name in test.columns:\n        test_target = pd.to_numeric(test_target, errors='coerce')\n        test_target[pd.isna(test_target)] = np.nanmean(test_target)\n\nelse:\n    print('This version can predict only numeric and categorical values values')\n    quit()\n\n\n\nif col_name in train.columns:\n    to_drop.append(col_name)\nelse:\n    print('No ' + str(col_name) + ' in data set')\n    errors.append('No ' + str(col_name) + ' in data set')\n    to_drop.append(col_name)\n\n","da148256":"# NUMERIC  COLUMN - enrollee_id\n\ncol_name = train.columns[0]\nto_drop.append(col_name)","3e541106":"# CATEGORY COLUMN - city\n\ncol_name = train.columns[1]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","49515a69":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","32f57d4f":"# NUMERIC  COLUMN - city_development_index\n\ncol_name = train.columns[2]\ntry:\n    sns.displot(x=train[col_name].dropna())\nexcept:\n    sns.distplot(train[col_name].dropna())\n","1861af72":"pd.DataFrame(train[col_name].describe())\n","cf9c7303":"if col_name in test.columns:\n\n    # dealing with NaNs in column\n    train[col_name] = pd.to_numeric(train[col_name], errors='coerce')\n    test[col_name] = pd.to_numeric(test[col_name], errors='coerce')\n    train[col_name][pd.isna(train[col_name])] = nan_numeric(train[col_name])\n    test[col_name][pd.isna(test[col_name])] = nan_numeric(train[col_name])\n\n# if there is no such column in test set - add information to errors and delete column in training set\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","93060c73":"if col_name in test.columns:\n    train[col_name + '_log'] = np.log(train[col_name] + 1 - min(0, min(train[col_name])))\n    test[col_name + '_log'] = np.log(test[col_name] + 1 - min(0, min(test[col_name])))\n    try:\n        sns.displot(x=train[col_name + '_log'])\n    except:\n        sns.distplot(train[col_name + '_log'])\n","2bdfe379":"# CATEGORY COLUMN - gender\n\ncol_name = train.columns[3]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","a7b92556":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","a8f10961":"# CATEGORY COLUMN - relevent_experience\n\ncol_name = train.columns[4]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","a82d617e":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","a978aa5f":"# CATEGORY COLUMN - enrolled_university\n\ncol_name = train.columns[5]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","ea2e5fbf":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","ff93c39f":"# CATEGORY COLUMN - education_level\n\ncol_name = train.columns[6]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","1925c568":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","399f861a":"# CATEGORY COLUMN - major_discipline\n\ncol_name = train.columns[7]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","fdb43a9f":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","d46d22b9":"# CATEGORY COLUMN - experience\n\ncol_name = train.columns[8]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","ab8dfc2f":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","cafdb1c7":"# CATEGORY COLUMN - company_size\n\ncol_name = train.columns[9]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","19b1c0a0":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","581c43e5":"# CATEGORY COLUMN - company_type\n\ncol_name = train.columns[10]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","709b7242":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","45bddd66":"# CATEGORY COLUMN - last_new_job\n\ncol_name = train.columns[11]\ntry:\n    sns.displot(x=train.dropna()[col_name], hue=train.dropna()[target_col_name])\nexcept: pass\n","4ef7a9bb":"if col_name in test.columns:\n    try:\n        train[col_name] = train[col_name].astype(float)\n        test[col_name] = test[col_name].astype(float)\n    except: pass\n    if train.dtypes[col_name] in ['int64', 'float64'] and test.dtypes[col_name] in ['int64', 'float64']:\n        test[col_name + '_encoded'] = test[col_name]\n        train[col_name + '_encoded'] = train[col_name]\n    else:\n        # replacing category names with numbers\n        encoder = LabelEncoder()\n        encoder.fit(train[col_name].astype(str))\n        train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n        try:\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n        except:\n            encoder = LabelEncoder()\n            encoder.fit(train[col_name].append(test[col_name]).astype(str))\n            train[col_name + '_encoded'] = encoder.transform(train[col_name].astype(str))\n            test[col_name + '_encoded'] = encoder.transform(test[col_name].astype(str))\n            if max(test[col_name + '_encoded']) > max(train[col_name + '_encoded']):\n                other_cat = max(train[col_name + '_encoded']) + 1\n                test[col_name + '_encoded'][test[col_name + '_encoded'] > max(train[col_name + '_encoded'])] = other_cat\n    to_drop.append(col_name)\n\n    # dealing with NaN values\n    train[col_name + '_encoded'] = pd.to_numeric(train[col_name + '_encoded'], errors='coerce')\n    train[col_name + '_encoded'][pd.isna(train[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n    test[col_name + '_encoded'] = pd.to_numeric(test[col_name + '_encoded'], errors='coerce')\n    test[col_name + '_encoded'][pd.isna(test[col_name + '_encoded'])] = nan_categorical(train[col_name + '_encoded'])\n\n\n# if there is no such column in test set - add information to errors and delete column in training set\n\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","ad879c25":"# NUMERIC  COLUMN - training_hours\n\ncol_name = train.columns[12]\ntry:\n    sns.displot(x=train[col_name].dropna())\nexcept:\n    sns.distplot(train[col_name].dropna())\n","736a95a6":"pd.DataFrame(train[col_name].describe())\n","35609790":"if col_name in test.columns:\n\n    # dealing with NaNs in column\n    train[col_name] = pd.to_numeric(train[col_name], errors='coerce')\n    test[col_name] = pd.to_numeric(test[col_name], errors='coerce')\n    train[col_name][pd.isna(train[col_name])] = nan_numeric(train[col_name])\n    test[col_name][pd.isna(test[col_name])] = nan_numeric(train[col_name])\n\n# if there is no such column in test set - add information to errors and delete column in training set\nelse:\n    print('No ' + str(col_name) + ' in test set')\n    errors.append('No ' + str(col_name) + ' in test set')\n    to_drop.append(col_name)\n\n","c37055d7":"if col_name in test.columns:\n    train[col_name + '_log'] = np.log(train[col_name] + 1 - min(0, min(train[col_name])))\n    test[col_name + '_log'] = np.log(test[col_name] + 1 - min(0, min(test[col_name])))\n    try:\n        sns.displot(x=train[col_name + '_log'])\n    except:\n        sns.distplot(train[col_name + '_log'])\n","5020f95d":"# clean data\nfor name in to_drop:\n    if name in train.columns:\n        del train[name]\n    if name in test.columns:\n        del test[name]\n\n","1fae42b9":"# MAKE MODEL\n\n# subsample for feature engineering\nreal_train = train\nreal_target = target\nreal_test = test\ntrain, test, target, test_target = train_test_split(train, target, test_size=0.2)\n\n# more rounds -> better prediction, but longer training\nnum_boost_rounds = 1000\neta = 10\/num_boost_rounds\n\n# parameters for xgboost\nxgb_params = {\n    'eta': eta,\n    'subsample': 0.80,\n    'objective': 'binary:logistic',\n    'eval_metric': 'error'\n}\n\n# transforming the data for xgboost\ndtrain = xgb.DMatrix(train, target)\ndtest = xgb.DMatrix(test)\ndreal_test = xgb.DMatrix(real_test)\n# training the model\nmodel = xgb.train(dict(xgb_params), dtrain, num_boost_round=num_boost_rounds)\n\n# predict\npreds = model.predict(dtest)\npreds = pd.to_numeric(preds)\npreds[pd.isna(preds)] = np.nanmean(preds)\n\nreal_preds = model.predict(dreal_test)\nreal_preds = pd.to_numeric(real_preds)\nreal_preds[pd.isna(real_preds)] = np.nanmean(real_preds)\n\npreds_categorical = []\ntest_target_categorical = []\n\nfor i in range(len(preds)):\n    if round(preds[i]) in target_real_values['Category value']:\n        pred_name_index = target_real_values['Category value'].index(round(preds[i]))\n    else:\n        pred_name_index = target_real_values['Category value'].index(min(target_real_values['Category value']))\n    test_target_name_index = target_real_values['Category value'].index(test_target[i])\n    preds_categorical.append(target_real_values['Category name'][pred_name_index])\n    test_target_categorical.append(target_real_values['Category name'][test_target_name_index])\n","393a70f6":"# preparing the data and printing the statistics\nxgb_fea_imp = pd.DataFrame(list(model.get_fscore().items()),\ncolumns=['feature', 'importance']).sort_values('importance', ascending=False)\nxgb_fea_imp.style.hide_index()\n","e42e226a":"for_plot = pd.DataFrame({'Id': range(len(preds_categorical)), 'Predictions': preds_categorical, 'Real values': test_target_categorical})\nsns.scatterplot(data=for_plot, x='Id', y='Predictions', hue='Real values')\npd.DataFrame(classification_report(test_target_categorical, preds_categorical, output_dict=True)).T\n","f37f1124":"for_plot = pd.DataFrame({'Id': range(len(preds)), 'Prediction': preds, 'Real values': np.round(test_target)})\nsns.scatterplot(data=for_plot, x='Id', y='Prediction', hue='Real values')\nprint('roc auc score is ' + str(roc_auc_score(np.round(test_target), preds)))\n\n","05195873":"# how much top errors to work with\ntop_errors = 5\n\n# how much best variables to use\ntop_features = 4\n\n# how much rows from training set to look at\ntop_rows = 5\n\n# how much could be the distance btw values in % to take them as similar (to assume that two values are in same claster)\ndist = 5\n\npreds_df = pd.DataFrame({'prediction': preds})\npreds_df.index = test.index\ntest_target_df = pd.DataFrame({target_col_name: test_target})\ntest_target_df.index = test.index\ntarget_df = pd.DataFrame({target_col_name: target})\ntarget_df.index = train.index\ntest_with_target = test.join(test_target_df).join(preds_df)\ntrain_with_target = train.join(target_df)\ntest_with_target = test.join(test_target_df).join(preds_df)\ntrain_with_target = train.join(target_df)\ntest_with_target['abs_err'] = abs(test_with_target[target_col_name] - test_with_target['prediction'])\ntest_top_errors = test_with_target.sort_values(by='abs_err', ascending = False).head(top_errors)\nmean_top_err = test_top_errors.describe().iloc[1:2, :]\nshow_order = [target_col_name, 'prediction', 'abs_err'] + xgb_fea_imp['feature'].tolist()\ntest_top_errors[show_order]\n","ccf1e955":"features = xgb_fea_imp['feature'][0:top_features].tolist()\nto_compare = train_with_target\nfor feature in features:\n    condition1 = train_with_target[feature] >= (mean_top_err[feature]*(1 - (dist\/100))).values[0]\n    condition2 = train_with_target[feature] < (mean_top_err[feature]*(1 + (dist\/100))).values[0]\n    condition = condition1 & condition2\n    if to_compare[condition].shape[0] >= top_rows:\n        to_compare = to_compare[condition]\n    else:\n        break\nshow_order = [target_col_name] + xgb_fea_imp['feature'].tolist()\nto_compare[show_order].head(top_rows)\n","19616824":"## company_type variable (categorical)\nHere we will deal with **company_type** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","80b8f38f":"## Errors statistics\nHere we will look at the largest mistakes and compare it to our training set.  \nThat could help us with feature generation.\nFirst we will look at largest absolute errors.\n","f186c2b9":"Now we will try to find most similar rows to this largest error rows in our training set","7be5b33d":"## enrolled_university variable (categorical)\nHere we will deal with **enrolled_university** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","678f3be3":"## last_new_job variable (categorical)\nHere we will deal with **last_new_job** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","ff365ff5":"## gender variable (categorical)\nHere we will deal with **gender** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","ca876f57":"## city variable (categorical)\nHere we will deal with **city** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**","81a0db19":"**city_development_index** variable statistics:","ca641f2a":"## enrollee_id variable (numeric)\nId column. Will be dropped\n","583297f4":"## Target variable target (categorical)\nHere we will deal with **target** variable which is our target variable \n(the variable which values, we want to predict).  \nFirst we will look at its values distribution.\nAfter we will transform the data so we could be able to use it in xgboost ML model.\n\n","53e63847":"## Prediction for target\nHere we will train xgboost model to predict our target variable **target**.  \nActually, all what we do above was just a sort of preparation. \nAnd now we will use all we done above to make better prediction for target variable.  \nHere you can use different models, make ensembles, and try to improve the solution.  \n\n","dbb95803":"## training_hours variable (numeric)\nHere we will deal with **training_hours** variable which is a numeric variable.  \nWe will look at at it's statistics, distribution and connection with target variable **target**\n","bb9bb892":"Now we will prepare **city_development_index** variable to use it in xgboost ML model.","2fa90b6e":"So data is loaded and now we have training and test sets.  \nWe will train our model on training set and will try to predict **target** values in test\n data set using other variables from test set.  \nBut first we need to look at the data and make some data preparation so we could use it to\ntrain xgboost model.   \n\nHere is the pair plot for given data (by diagonal we can see variable values distribution).  \nIt is very useful for better understanding dependencies in data:   \n","ba19581d":"## Remove all useless columns\nHere we will remove all useless columns\n","9706d56f":"This report was generated using [FastBenchmark](http:\/\/fastbenchmark.me\/) project.  \n# Prediction for target using aug_train.csv data set.  \nHere we will try to predict **target** using **aug_train.csv** data set.  \n## Reading the data and splitting it into train and test sets\nFirst we need to import all necessary libraries, read the data from csv and make \ntraining-test split. Standard split rate is 80 to 20, but you can change it in code \n(by changing 0.2 to any other value in _make train and test_ string)  \n","fc9a8fc5":"## education_level variable (categorical)\nHere we will deal with **education_level** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","6a4fef9d":"## company_size variable (categorical)\nHere we will deal with **company_size** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","f2149250":"## city_development_index variable (numeric)\nHere we will deal with **city_development_index** variable which is a numeric variable.  \nWe will look at at it's statistics, distribution and connection with target variable **target**\n","f581fad4":"Let's look at the data","7201f78b":"**training_hours** variable statistics:","7280ca29":"Classification report and plot for prediction:","e953c69d":"Now we will prepare **training_hours** variable to use it in xgboost ML model.","66be33e1":"It could also be useful to add log transformed numeric variable to data set,so let's do so and check its distribution:","709fd96f":"## relevent_experience variable (categorical)\nHere we will deal with **relevent_experience** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","949e6cc9":"## NaN functions\nHere are the functions for NaN values.  \nBy default we will replace NaNs in numeric columns by np.nanmean and for categories with -1.  \n","63265f34":"It could also be useful to add log transformed numeric variable to data set,so let's do so and check its distribution:","ec893bdb":"## major_discipline variable (categorical)\nHere we will deal with **major_discipline** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n","9e9f2dd1":"We will also look at roc curve score for non-rounded dataand plot the non-rounded prediction","c0372cbb":"Feature importance for **target** prediction:","c380903e":"## experience variable (categorical)\nHere we will deal with **experience** variable which is a categorical variable.  \nWe will look at at it's distribution and connection with target variable **target**\n"}}