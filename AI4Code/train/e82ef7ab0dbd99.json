{"cell_type":{"d17bcc85":"code","4fea4f12":"code","6f955183":"code","dda4e89f":"code","70fe3bc2":"code","03125773":"code","39698ca7":"code","bae6d6f8":"code","2abebfdd":"code","a441889c":"code","3ff14c89":"code","e4739852":"code","cb89c2b1":"code","f75c031d":"code","83d37985":"code","75837d43":"code","7a43dc46":"code","38690bc9":"code","b5b906bd":"markdown","0877a292":"markdown","0628c2c1":"markdown","b91b6d5b":"markdown","0d5dcd79":"markdown","619bc42e":"markdown","e5f3a54f":"markdown"},"source":{"d17bcc85":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport sklearn.metrics as m\nimport sklearn.tree as tree\nimport sklearn.ensemble as ensemble\nimport sklearn.model_selection as ms\nimport sklearn.svm as svm\nimport sklearn.neural_network as nn\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4fea4f12":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","6f955183":"data.info()","dda4e89f":"data.sample(5)","70fe3bc2":"data.shape[0] - data.count() # no blank values","03125773":"print(data['DEATH_EVENT'].value_counts(normalize=True)) # About 32% of the data is positive class\nprint()\nprint(data['DEATH_EVENT'].value_counts())","39698ca7":"data_corr = data.corr()['DEATH_EVENT'] * 100\ndata_corr.sort_values()","bae6d6f8":"X = data.copy()\nX.drop('DEATH_EVENT', axis=1, inplace=True)\ny = data['DEATH_EVENT'].copy()\n\nX_train, X_test, y_train, y_test = ms.train_test_split(X, y, train_size=200, shuffle=True, stratify=y, random_state=42)","2abebfdd":"print(y_train.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","a441889c":"rf_clf = ensemble.RandomForestClassifier(random_state=42)\ndt_clf = tree.DecisionTreeClassifier(random_state=42)\next_clf = ensemble.ExtraTreesClassifier(random_state=42)\nmlp_clf = nn.MLPClassifier(random_state=42)\n\nvoting_classifier = ensemble.VotingClassifier([\n                                            ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                                            ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                                            ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                                            ('mlp_clf', nn.MLPClassifier(random_state=42))\n                                            ], voting='hard')","3ff14c89":"rf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\next_clf.fit(X_train, y_train)\nmlp_clf.fit(X_train, y_train)\nvoting_classifier.fit(X_train, y_train)","e4739852":"estimators = [rf_clf, dt_clf, ext_clf, mlp_clf, voting_classifier]\n\ncv = ms.RepeatedKFold(n_splits=4, n_repeats=10, random_state=42)\n\nfor estimator in estimators:\n    cv_accuracy = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\n    f1_score = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n    print(estimator.__class__.__name__)\n    print(f'Avg Accuracy: {np.mean(cv_accuracy) * 100}')\n    print(f'Std Accuracy: {np.std(cv_accuracy) * 100}')\n    print(f'Avg F1: {np.mean(f1_score) * 100}')\n    print(f'Std F1: {np.std(f1_score) * 100}')\n    print()","cb89c2b1":"# MLPClassifier (with no tuning) perform the worse\n# Drop MLPClassifier from the ensemble model and retrain it\n\nnew_voting_classifier = ensemble.VotingClassifier([\n                                            ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                                            ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                                            ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                                            ], voting='hard')\n\n\nnew_voting_classifier.fit(X_train, y_train)\nnvc_accuracy_score = ms.cross_val_score(new_voting_classifier, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\nnvc_f1_score = ms.cross_val_score(new_voting_classifier, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n\n\nprint(np.mean(nvc_accuracy_score) * 100)\nprint(np.std(nvc_accuracy_score) * 100)\nprint(np.mean(nvc_f1_score) * 100)\nprint(np.std(nvc_f1_score) * 100)","f75c031d":"# Let's change the voting method to soft to see if there is any gain in performance\n\nnew_soft_voting_classifier = ensemble.VotingClassifier([\n                                            ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                                            ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                                            ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                                            ], voting='soft')\n\nnew_soft_voting_classifier.fit(X_train, y_train)\nnsvc_accuracy_score = ms.cross_val_score(new_soft_voting_classifier, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\nnsvc_f1_score = ms.cross_val_score(new_soft_voting_classifier, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n\nprint(np.mean(nsvc_accuracy_score) * 100)\nprint(np.std(nsvc_accuracy_score) * 100)\nprint(np.mean(nsvc_f1_score) * 100)\nprint(np.std(nsvc_f1_score) * 100)\n\n# Seems that hard voting is the way to go","83d37985":"estimators = [rf_clf, dt_clf, ext_clf, mlp_clf, new_voting_classifier]\n\nfor estimator in estimators:\n    print(estimator.__class__.__name__)\n    print(estimator.score(X_test, y_test) * 100)\n    print()","75837d43":"data_corr = data.corr()['DEATH_EVENT'] * 100\ndata_corr.sort_values()","7a43dc46":"X = data[['time', 'ejection_fraction', 'age', 'serum_creatinine']].copy()\ny = data['DEATH_EVENT'].copy()\n\nX_train, X_test, y_train, y_test = ms.train_test_split(X, y, train_size=200, shuffle=True, stratify=y, random_state=42)\n\n\nrf_clf = ensemble.RandomForestClassifier(random_state=42)\ndt_clf = tree.DecisionTreeClassifier(random_state=42)\next_clf = ensemble.ExtraTreesClassifier(random_state=42)\n\nvoting_classifier = ensemble.VotingClassifier([\n                                            ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                                            ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                                            ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                                            ], voting='hard')\n\n\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\next_clf.fit(X_train, y_train)\nvoting_classifier.fit(X_train, y_train)\n\n\nestimators = [rf_clf, dt_clf, ext_clf, voting_classifier]\n\ncv = ms.RepeatedKFold(n_splits=4, n_repeats=10, random_state=42)\n\nfor estimator in estimators:\n    cv_accuracy = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\n    f1_score = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n    print(estimator.__class__.__name__)\n    print(f'Avg Accuracy: {np.mean(cv_accuracy) * 100}')\n    print(f'Std Accuracy: {np.std(cv_accuracy) * 100}')\n    print(f'Avg F1: {np.mean(f1_score) * 100}')\n    print(f'Std F1: {np.std(f1_score) * 100}')\n    print()","38690bc9":"for estimator in estimators:\n    print(estimator.__class__.__name__)\n    print(estimator.score(X_test, y_test) * 100)\n    print()","b5b906bd":"# Candidates Models & Ensemble (BASELINE)\n\n* My approach will be to train individual classifier models as well as an ensemble model (voting classifier)\n* I will assess the individual classifier models based on the default hyperparameter values\n* The voting classifier will be based on all the individual classifier models with their default hyperparameter values\n* ALL features will be used for this baseline models","0877a292":"# Candidates Models & Ensemble (Feature Selection)\n\n* Let's select features that have at least 20% correlation with the target","0628c2c1":"# Conclusion\n\n* Interestingly, the ExtraTreesClassifier perform the best on the training data (with feature selection) with the highest F1 score of 76.4% though the standard deviation on its F1 score is also the highest\n* On the testing data, the Voting Classifier and Random Forest model perform the same\n* All the models perform slightly better with feature selection\n* The reduced feature dataset also result in a slight increaser in the scores' standard deviation","b91b6d5b":"# Commentary\n\n* As it stands with no hyperparameter tuning, RandomForestClassifier perform the best\n    * F1 Score stands at 73.6% with Accuracy at 84.5%\n  \n \n* The new voting classifier perform 'much better' with lower bias and variance when it drop MLPClassifier from its ensemble\n    * Accuracy improves from 78.6% to 83.2%\n    * F1 Score improves from 56.2% to 71.0%\n    * Notably, new voting classifier has a slightly lower standard deviation as compare to the best model: RandomForestClassifier\n\n\n|                  \t| RandomForestClassifier (%) \t| New Voting Classifier (%) \t| Old Voting Classifier (%) \t|\n|------------------\t|----------------------------\t|---------------------------\t|---------------------------\t|\n| Average Accuracy \t| 84.5                       \t| 83.2                      \t| 78.6                      \t|\n| Std Accuracy     \t| 3.8                        \t| 3.7                       \t| 5.5                       \t|\n| Average F1       \t| 73.6                       \t| 71.0                      \t| 56.2                      \t|\n| Std F1           \t| 6.2                        \t| 6.0                       \t| 11.4                      \t|\n","0d5dcd79":"# Splitting Data\n* Data is splited such that BOTH the training and testing dataset contain the exact same proportion of death events","619bc42e":"# Basic EDA","e5f3a54f":"# Performance on Testing Data (BASELINE MODELS)"}}