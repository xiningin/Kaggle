{"cell_type":{"9013c005":"code","55b36764":"code","bad977c4":"code","8cb76392":"code","b84ea0df":"code","31ef78c2":"code","86abd82a":"code","4d737892":"code","799263d8":"code","aa7ff823":"code","0210f27e":"code","a24ab93d":"code","05bb16f8":"code","4b70e770":"code","b5d542ca":"code","59f7869e":"code","0752d430":"code","812c0237":"markdown","7bec0c5f":"markdown","e5fc1540":"markdown","71884fa1":"markdown","80bcd3e1":"markdown","195056d4":"markdown","5af2a1e7":"markdown"},"source":{"9013c005":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","55b36764":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.layers import Input , Dense , Flatten , Dropout , BatchNormalization\nfrom tensorflow.keras.layers import Conv2D , SeparableConv1D , ReLU , Activation , MaxPool2D\nfrom tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator \nfrom PIL import Image\n\nprint(\"tensorflow version :\" ,tf.__version__)","bad977c4":"train_norm = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\"\ntrain_pnuem = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\"\ntest_norm = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test\/NORMAL\"\ntest_pnuem = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test\/PNEUMONIA\"\nval_norm = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\"\nval_pnuem = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\"\ntrain = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train\"\ntest = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test\"\nval = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\"","8cb76392":"def plot_img(item_dir , num_img = 3):\n    all_dir = os.listdir(item_dir)\n    files = [os.path.join(item_dir , file) for file in all_dir][:num_img]\n    \n    plt.figure(figsize=(20 , 12))\n    for index , img_p in enumerate(files):\n        plt.subplot(2 ,3, index +1)\n        img = plt.imread(img_p)\n        plt.imshow(img , cmap='gray')\n    plt.tight_layout()","b84ea0df":"print(\"CHEST X-RAY OF NORMAL PERSON\")\nplot_img(train_norm)","31ef78c2":"print(\"CHEST X-RAY OF A PNEUMONIA PATIENT\")\nplot_img(train_pnuem)","86abd82a":"img_dims = 256\nBATCH_SIZE = 32\nEPOCHS = 20","4d737892":"def load_data(img_dims , batch_size):\n    train_loader = ImageDataGenerator(rescale= 1.\/255 , zoom_range=0.3 , vertical_flip=True)\n    test_loader = ImageDataGenerator(rescale= 1.\/ 255 , zoom_range = 0.3)\n    val_loader = ImageDataGenerator(rescale= 1.\/255)\n    \n    train_data_gen = train_loader.flow_from_directory(directory=train , \n                                                  batch_size=batch_size ,\n                                                  shuffle = True ,\n                                                 target_size= (img_dims , img_dims),\n                                                  class_mode='binary')\n    test_data_gen = test_loader.flow_from_directory(directory=test ,\n                                                batch_size= batch_size ,\n                                                shuffle = True ,\n                                                target_size= (img_dims , img_dims) ,\n                                                class_mode= 'binary')\n    val_data_gen = val_loader.flow_from_directory(directory=val ,\n                                              batch_size=batch_size ,\n                                              shuffle= True ,\n                                              target_size = (img_dims , img_dims) ,\n                                              class_mode= 'binary')\n    return train_data_gen , test_data_gen , val_data_gen","799263d8":"train_data , test_data , val_data = load_data(img_dims=img_dims , batch_size=BATCH_SIZE)","aa7ff823":"model = Sequential()\n# layer 1\nmodel.add(Conv2D(filters= 16 , kernel_size = (3,3) , activation = 'relu' ,input_shape = (img_dims , img_dims , 3) , padding = 'same' ,))\nmodel.add(MaxPool2D(pool_size = (2, 2)))\n# layer 2\nmodel.add(Conv2D(filters = 32 , kernel_size =(3,3) , activation = 'relu' , padding = 'same'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n# layer 3\nmodel.add(Conv2D(filters = 64 , kernel_size =(3,3) , activation = 'relu' , padding = 'same'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n# layer 4 FLATTEN\nmodel.add(Flatten())\n# layer 5 FC\nmodel.add(Dense(512 , activation = 'relu'))\n# layer 6 FC\nmodel.add(Dense(256 , activation = 'relu'))\n# output\nmodel.add(Dense(1 , activation = 'sigmoid'))","0210f27e":"model.summary()","a24ab93d":"model.compile(optimizer= 'adam' ,\n              loss = 'binary_crossentropy' ,\n              metrics= ['accuracy'])","05bb16f8":"# checkpoint\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"best_weights.hdf5\", save_best_only = True ,save_weights_only=True , verbose=1)\n","4b70e770":"model = Sequential()\n# layer 1\nmodel.add(Conv2D(filters= 16 , kernel_size = (3,3) , activation = 'relu' ,input_shape = (img_dims , img_dims , 3) , padding = 'same' ,))\nmodel.add(MaxPool2D(pool_size = (2, 2)))\n# layer 2\nmodel.add(Conv2D(filters = 32 , kernel_size =(3,3) , activation = 'relu' , padding = 'same'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n# layer 3\nmodel.add(Conv2D(filters = 64 , kernel_size =(3,3) , activation = 'relu' , padding = 'same'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\n# layer 4 FLATTEN\nmodel.add(Flatten())\n# layer 5 FC\nmodel.add(Dense(512 , activation = 'relu'))\n# DROPOUT 1 \nmodel.add(Dropout(rate= 0.7))\n# layer 6 FC\nmodel.add(Dense(256 , activation = 'relu'))\n# DROPOUT 2\nmodel.add(Dropout(rate = 0.5))\n#layer 7 FC\nmodel.add(Dense(128 , activation = 'relu'))\n# DROPOUT 3\nmodel.add(Dropout(rate = 0.3))\n# output\nmodel.add(Dense(1 , activation = 'sigmoid'))","b5d542ca":"model.summary()","59f7869e":"model.compile(optimizer= 'adam' ,\n              loss = 'binary_crossentropy' ,\n              metrics= ['accuracy'])","0752d430":"history  = model.fit(train_data ,\n                     epochs=EPOCHS , \n                     validation_data=test_data , \n                     callbacks=[cp_callback])\n\n","812c0237":"lets plot and see some of the images","7bec0c5f":"now lets specify some Hyper paraneters for our model","e5fc1540":"now we have created our directory to save checkpoints for our model and created the checkpoint and we have to restart training <br>\nbut this time we will modify our model and add dropout to it\nas we saw  our first model has a variance <br>\ntrain_accuracy ~ 96% and test accuracy ~ 84% <br>\nwhich can be reduced by regularization","71884fa1":"Alright now our data is prepared\nnow we need to build our model\nfirst we will build our model without dropout regularization\n\n#### Basic layout of the model\n\n1. Input Layer\n2. Conv2D - same padding , relu , 16 filters , ksize = 3x3\n3. MaxPool2D - f = 2\n4. Conv2D - same padding , relu , 32 filters , ksize = 3x3\n5. MaxPool2D - f = 2\n6. Conv2D - same padding , relu , 64 filters , ksize = 3x3\n7. MaxPool2D f = 2\n8. Flatten\n9. Dense 512 , relu\n10. Dense 256 , relu\n11. Dense 1 , sigmoid -- output\n","80bcd3e1":"now we have created our model its time to compile we will be using adam optimizer and cross entropy loss","195056d4":"Now we have imported all our needed modules \n## The Task \nGiven an image of a chest X-ray the model has identify whther the patient has pnuemonia or not\n## The approach\nwe are going to use Convolution Nueral Networks to create the model with ConV and MaxPool layers the framework which we are going to use is tensorflow.keras.\n\n\nso first lets load our Dataset","5af2a1e7":"now lets create a function to process our data"}}