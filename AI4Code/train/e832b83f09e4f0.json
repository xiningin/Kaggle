{"cell_type":{"344cc9d8":"code","5c5f0a3a":"code","82a477e8":"code","94876a97":"code","c709359a":"code","52dbe357":"code","15a4332e":"code","50d28d29":"code","9dcdbafb":"code","dd6ff1f0":"code","92293a4b":"code","e0ff63c1":"code","28aa7dbe":"code","f59b0cc3":"code","cc0d025b":"markdown","4201cbdc":"markdown","cf66cc00":"markdown","9ab0bb0e":"markdown","48f27492":"markdown","002b14cd":"markdown","632fafd6":"markdown","f643baee":"markdown","c3e46923":"markdown","3cd92674":"markdown","a5e3146a":"markdown","c6e3fe17":"markdown","3efe8986":"markdown","fbb61b94":"markdown","6bba383a":"markdown","74a2ff6c":"markdown","ef5cb859":"markdown","9dcfb69d":"markdown","812c6025":"markdown"},"source":{"344cc9d8":"import matplotlib.pyplot as plt\nx = range(-10,10)\ny = [x*x for x in x]\nplt.scatter(x, y)\nplt.show()","5c5f0a3a":"import matplotlib.pyplot as plt\nx = range(-10,10)\ny = [max(x,0) for x in x]\nplt.scatter(x, y)\nplt.show()","82a477e8":"from sympy import *\nx = Symbol('x')\nf = x**2\nfdash = f.diff(x)\nfdash","94876a97":"from sympy import *\nx = Symbol('x')\nf = 1\/x\nfdash = f.diff(x)\nfdash","c709359a":"import matplotlib.pyplot as plt\nx = range(-10,10)\ny = [1\/x for x in x]\nplt.scatter(x, y)\nplt.show()","52dbe357":"import tensorflow as tf\nimport numpy\nx = tf.Variable([10.0])\n\nwith tf.GradientTape() as tape:\n  y = x**2\nfdash=tape.gradient(y, x)\nfdash.numpy()","15a4332e":"import tensorflow as tf\n#Function with inputs x\ndef fu(x):\n    return x ** 3.0 \n#Reset the values of x\ndef reset():\n    x = tf.Variable(10.0) \n    return x\nx = reset()\nfor i in range(50):\n    #Find derivative of x with respect to y using auto differentiation\n    with tf.GradientTape() as tape:\n        y = fu(x)\ngrads = tape.gradient(y, [x])\n\nprint (grads[0].numpy)\n#Update x\nnewx=x.assign(x - 0.1*grads[0].numpy())\nnewx.numpy()","50d28d29":"##chain rule###\nfrom sympy import * \nf = symbols('f', cls=Function)\nt= symbols('t')\nx =  t**2\ny = sin(t)\ng = f(x,y)\nDerivative(g,t).doit()","9dcdbafb":"##sum rule###\nfrom sympy import * \nf = symbols('f', cls=Function)\nt= symbols('t')\nx =  t**2\ny = sin(t)\ng = f(x)+f(y)\nDerivative(g,t).doit()","dd6ff1f0":"#We are taking example of classical MNIST dataset which has 784 images of 0-9 digits. We have first starting input matrix of 784 \n#columns and output of 10 categories y\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nmnist = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\na_0 = tf.compat.v1.placeholder(tf.float32, shape=(None, 784))\ny = tf.compat.v1.placeholder(tf.float32, shape=(None, 10))\n\n# here is am constructing one hidden layers with w1 as weight 1 and b1 as bais 1 which passes the network to next layer\n# w2 is output layer which has b2 as bais of 1 and output the 10 prediction \nmiddle = 50\nw_1 = tf.Variable(tf.random.truncated_normal([784, middle]))\nb_1 = tf.Variable(tf.random.truncated_normal([1, middle]))\nw_2 = tf.Variable(tf.random.truncated_normal([middle, 10]))\nb_2 = tf.Variable(tf.random.truncated_normal([1, 10]))\n\n# this is the activation function sigmoid which is 1\/1+exp^(-x)\ndef sigmoid(x):\n    return tf.compat.v1.div(tf.constant(1.0),\n                  tf.compat.v1.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n\n","92293a4b":"z_1 = tf.compat.v1.add(tf.matmul(a_0, w_1), b_1)\na_1 = sigmoid(z_1)\nz_2 = tf.compat.v1.add(tf.matmul(a_1, w_2), b_2)\na_2 = sigmoid(z_2)\ndiff = tf.subtract(a_2, y)##This becomes our cost function","e0ff63c1":"## Derivative of the cost function###\ndef sigmaprime(x):\n    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))\n## we multiply diff with derivative of cost function , this becomes the input backward to the hidden layer\nd_z_2 = tf.multiply(diff, sigmaprime(z_2))\nd_b_2 = d_z_2\n## Now the updated weight will be mutiplication of this new quantity into transpose of a1 because we are propogating backward \n#hence the dimension of matrix are reversed.\nd_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)\nd_a_1 = tf.matmul(d_z_2, tf.transpose(w_2))\nd_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\nd_b_1 = d_z_1\nd_w_1 = tf.matmul(tf.transpose(a_0), d_z_1)\nprint(d_w_1,d_z_1)","28aa7dbe":"import sympy as sym\nvars = sym.symbols('x1 x2') # Define x1 and x2 variables\nf = sym.sympify(['x1**2', 'ln(x2)']) # Define functions of x1 and x2\nJ = sym.zeros(len(f),len(vars)) # Initialise Jacobian matrix\n# Fill Jacobian matrix with entries\nfor i, fi in enumerate(f):\n    for j, s in enumerate(vars):\n        J[i,j] = sym.diff(fi, s)\nprint(J)\nprint(sym.Matrix.det(J))","f59b0cc3":"from sympy import Function, hessian, pprint\nfrom sympy.abc import x, y,z\ng2 = x**3 + y**2+z\npprint(hessian(g2, (x, y,z)))\n","cc0d025b":"Here we have created a simple cost function which changes value of x by 10% of its gradient. The gradient at value 10 will be 300 for function X***3 , the new value of x as per our formula is adjusted as 10% of gradient value so 10-30=-20","4201cbdc":"## The Backward Propogation\nWe will use difference (error) to update weights of network and propogate results backwards into neural networks. We will use \u03c3'(z)=\u03c3(z)(1\u2212\u03c3(z)) which is derivative\/gradient of our sigmoid function ,since sigmoid is exponenetial the result is just sigmoid * 1\/1-sigmoid. Notice the derivative of cost function is telling us the way where we need to update weights for our variables.","cf66cc00":"The first row of matrix is second order partial derivative w.r.t x, the second w.r.t y and third row w.r.t to z.Note this matrix is symmetrical matrix, if a Hessian matrix is symmetrical than the function is continous a function ","9ab0bb0e":"## Not all functions are differentiable at every point, non continous functions","48f27492":"## Using Gradient to update variable in Tensorflow for a basic cost function\n\nCost function is an error function that consists of difference between actual value and predicted value of model, the first order derivative of the cost function is called gradient, which help us to know whether the loss is increasing or decreasing with each cycle of learning in Neural Network.The weights of nueral network are updated wherever we see a decreasing cost function in case of error minimzing algorithms.","002b14cd":"# Partial Derivative if we have more than one feature being used in prediction- Jacobian","632fafd6":"In earlier example we have one variables being used in prediction . Now when we add more features say x2 in model, we have two variables x1 and x2. Lets say our cost function becomes L=y-x1^2-log(x2). Having two variables now we have differentiate the cost function with respect to both x1 and x2. here partial derivatives comes into picture.\n\nHere how it works:\nWhile differentiating w.r.t x1 we will keep x2 as constant and vice versa\n\nSo dL\/dx1=-2x * -log(x2) and dL\/dx2= -x1^2 * -1\/x2 ..now instead of one value the gradient becomes a matrix  \n [dL\/dx1,dL\/dx2] . This is called the Jacobian in calculus terminology. We have to take the determinant of the matrix to arrive at the gradient and put values of variable at that point. We will use sympy to get the Jacobian for these functions","f643baee":"\n# Chain Rule for function inside function\n\nIf we have a function of x embeded inside onther function of x then we apply chain rule of differentiation\n\nf(x)=g(h(x)) then df\/dx=dg\/dh*dh\/dx let us take g(x)=cosx and h(x)=x^2 then\ndf\/dx=d\/dx(cosx^^2)*2x=-sinx^^2*2x","c3e46923":"## What if we have complex cost function of two functions\n\nLets say if have little more complex cost function L=y-sin(x) * x^3, now we know taking its derivative will be simple by chain rule. I will be -3x^2* sinx + cosx* x^3, this can be used to update weights now in backpropogation\n","3cd92674":"## Derivatives- Gradient\nA derivative for a function at one particular point is a slope of function, i.e. Rate of change.\n\nMathematically:\n\ndf\/dx=lim(delta(x) ->0) (f(x+delta(x))-f(x-delta(x))\/2*delta(x)\n\ndelta(x) is a very very small change in x value, a derivative or a slope function is thus value of x+delta(x) - value \nof x-delta(x) divided by 2 * delta(x).\n\nNormally a derivative of alegebric function is just : d(y=x^n)\/dx=n*x^n-1.\n\nNote: trignometric function derivative are different, since when you try to differentiate a periodic curve you will get another periodic curve.Hence when you differntiate sinx you arrive at cos x becuase the slope of sinx and cos x are just shifted by 90\n\nExponential function is a peculiar case which derivatives have same value as function, this because exponential series is just algebric series with infintely increasing value of x \n\ne(x) = 1 + x\/1 + x2\/2! + x3\/3! +......\n\nSo no matter how many times we differentiate we arrive at same value.\n \n \nWe will calculate derivatives using sympy function derivatives.","a5e3146a":"# Two or more functions, the sum rule and product rule\nLet us say we function f(x) =g(x)+h(x) which is sum of two function of x. Lets say g(x)=x^2 and h(x) = x^3\nThe df\/dx=dg\/dx+dh\/dx=2x+3x^2\n\nLet us now come to where we have two functions multiplied then we apply product rule\nlets have g(x)=sinx and h(x)=x^2 and f=g(x)h(x)\n\ndf\/dx=h(x)*dg\/dx+g(x)*dh\/dx=X^2*cosx+2x*sinx","c6e3fe17":"# Tensorflow 1.0 example of chain rule- The Backpropogation Algorithm.\n\nTensorflow work on computational graphs for which i will utilize older version of TF to demonstrate how backpropogation works. We will first examine how backpropogation works and then use a complex cost function to explain chain rule in Neural Network\n\nWe will use just three layer network to show this","3efe8986":"# Slope of slope- The Hessian\n Here is the article for reference:https:\/\/mlexplained.com\/2018\/02\/02\/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1\/\n \nThe Hessian is second order derivative of a function. If we differentiate a cost function 2 times we will get a Hessian matrix which tells us curvature of function, which in neural networks is roughly equivalent to momentum. If momentum is large the convergence happens faster. The eigenvalues of the Hessian represents the curvature of the loss function is in the direction of the corresponding eigenvector. Stronger curves mean a faster change in the gradient\n \nLet me take example of cost of function=x^3+y^2+z now we will use sympy go get the Hessian matrix","fbb61b94":"## What is a function?\n\nA function is basically a mapping between two variables. By one variable you can derive another variable, this is core of all supervised Machine learning that we do, and estimating the right function that represent data is the crux of ML.\n\nMathematically a function is written like this :\n\n    y=f(x)=x^2(x square)\n    y=f(x)=max(x,0)\n\nHere y is a function of x. The first one is squares of the value of x while the second one takes any positive value of x and 0 if x is negative.\n\nDiagramatically these are resprested by following diagram using matplotlib","6bba383a":"We see the function is infinite a x =0 hence you will find that we are unable to calculate the value of function at x=0. Thus this function will not be have a derivative at x=0","74a2ff6c":"****The second function is also called, RELU - rectified linear unit is one of activation function used in neural networks.**","ef5cb859":"## Calculating gradient using Tensorflow\n\nTF comes with with handy tf.GradientTape function to calculate this, note in high level libraries Keras, this function is automatically called while training a Neural Network.","9dcfb69d":"## The Forward Propogation\nZ1 is output of from first hidden layer which is simply multiplication of your input from input layer and the weights w1 which are randomly choosen weights in step w1 plus bias term : w1*a0+b1. The matmul function means matrix multiplication \nthen output of z1 is passed in activation function sigmoid \n\nZ2 takes output from activation function and does weight and values multiplication and adding a bias term , then this passed to final sigmoid function which gives us the prediction.\n\nFinally we have an error term diff which is difference of a2 and y actual minus predicted","812c6025":"Calculus forms basics of Nueral Network Working and many other machine learning algorthm.We will try to understand Basic calculus of gradients that is applied in ML using Python."}}