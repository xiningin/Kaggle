{"cell_type":{"53f43534":"code","8c60f136":"code","8ad47117":"code","93235733":"code","ba68b189":"code","efb3be14":"code","9c138434":"code","42e83618":"code","183da618":"markdown","1f1e4a3e":"markdown","0d1947af":"markdown","7b02907e":"markdown","90e52ec5":"markdown","4c73bca2":"markdown"},"source":{"53f43534":"\"\"\"\n    Imports\n\"\"\"\n\nimport numpy as np\nfrom random import random, randint, sample\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt","8c60f136":"\"\"\"\n    Torch device\n\"\"\"\n\nif torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"\ndevice = torch.device(dev)\ndtype = torch.float32","8ad47117":"\"\"\"\n    Utils\n\"\"\"\n\ndef initialize_weight(x):\n    nn.init.xavier_uniform_(x.weight)\n    if x.bias is not None:\n        nn.init.constant_(x.bias, 0)","93235733":"class PositionalEncoding(nn.Module):\n    def __init__(self, dimensionality=512):\n        super(PositionalEncoding, self).__init__()\n        # Attributes\n        self.dimensionality = dimensionality\n        # \n        num_timescales = dimensionality \/\/ 2\n        max_timescale = 10000.0\n        min_timescale = 1.0\n        log_timescale_increment = (math.log(float(max_timescale) \/ float(min_timescale)) \/ max(num_timescales - 1, 1))\n        inv_timescales = min_timescale * torch.exp(torch.arange(num_timescales, dtype=torch.float32) * -log_timescale_increment)\n        self.register_buffer('inv_timescales', inv_timescales)\n    \n    def forward(self, x):\n        max_length = x.size()[1]\n        position = torch.arange(max_length, dtype=torch.float, device=device)\n        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)\n        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n        signal = F.pad(signal, (0, 0, 0, self.dimensionality % 2))\n        signal = signal.view(1, max_length, self.dimensionality)\n        return signal","ba68b189":"class MultiHeadAttention(nn.Module):\n    def __init__(self, dimensionality, dropout, heads=8):\n        super(MultiHeadAttention, self).__init__()\n        # Number of headds\n        self.heads = heads\n        # Size of the features for one head\n        self.att_size = att_size = dimensionality \/\/ heads\n        # Scaling factor\n        self.scale = att_size ** -0.5\n        \n        # Queries projection\n        self.linear_q = nn.Linear(dimensionality, heads * att_size, bias=False)\n        # Keys projection\n        self.linear_k = nn.Linear(dimensionality, heads * att_size, bias=False)\n        # Values projection\n        self.linear_v = nn.Linear(dimensionality, heads * att_size, bias=False)\n        \n        # Attention dropout\n        self.att_dropout = nn.Dropout(dropout)\n        # Layer that merges result of heads computation\n        self.output_layer = nn.Linear(heads * att_size, dimensionality, bias=False)\n        \n        # Initialization\n        initialize_weight(self.linear_q)\n        initialize_weight(self.linear_k)\n        initialize_weight(self.linear_v)\n        initialize_weight(self.output_layer)\n\n    def forward(self, q, k, v, mask, cache=None):\n        \"\"\"\n            params:\n            - q = float tensor of shape [batch_size, input_seq_length, dimensionality]\n            - k = float tensor of shape [batch_size, input_seq_length, dimensionality]\n            - v = float tensor of shape [batch_size, input_seq_length, dimensionality]\n            - mask = boolean tensor of shape [batch_size, 1, input_seq_length]\n            - cache = Dictionnary containing already computed projections of keys and values ot lower computational cost\n            \n            return: features = float tensor of shape [batch_size, input_seq_length, dimensionality]\n        \"\"\"        \n        orig_q_size = q.size()\n\n        d_k = self.att_size\n        d_v = self.att_size\n        batch_size = q.size(0)\n\n        \n        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n        q = self.linear_q(q).view(batch_size, -1, self.heads, d_k) # q.shape = [batch_size, query_seq_length, h, d_k]\n        # Pulling cached data if possible\n        if cache is not None and 'encdec_k' in cache:\n            k, v = cache['encdec_k'], cache['encdec_v']\n        else:\n            k = self.linear_k(k).view(batch_size, -1, self.heads, d_k)# k.shape = [batch_size, query_seq_length, h, d_k]\n            v = self.linear_v(v).view(batch_size, -1, self.heads, d_v)# v.shape = [batch_size, query_seq_length, h, d_k]\n            \n            if cache is not None:\n                cache['encdec_k'], cache['encdec_v'] = k, v\n        # Transpositions for formulas\n        q = q.transpose(1, 2)                  # [batch_size, h, query_seq_length, d_k]\n        v = v.transpose(1, 2)                  # [batch_size, h, values_seq_length, d_v]\n        k = k.transpose(1, 2).transpose(2, 3)  # [batch_size, h, d_k, keys_seq_length]\n\n        # Scaled Dot-Product Attention.\n        # Attention(Q, K, V) = softmax((QK^T)\/sqrt(d_k))V\n        q.mul_(self.scale)\n        x = torch.matmul(q, k)  # [batch_size, h, query_seq_length, keys_seq_length]\n        # As we are in log space, masked values are -inf\n        x.masked_fill_(mask.unsqueeze(1), -1e9)\n        x = torch.softmax(x, dim=3)\n        # x is now our attention matrix\n        # Applying our dropout\n        x = self.att_dropout(x)\n        x = x.matmul(v)  # [batch_size, h, query_seq_length, d_k]\n        \n        # Contiguous rearange the order of the data in the buffer\n        x = x.transpose(1, 2).contiguous()  # [batch_size, query_seq_length, h, d_v]\n        x = x.view(batch_size, -1, self.heads * d_v) # [batch_size, query_seq_length, dimensionality]\n        \n        x = self.output_layer(x)\n        # Now everything is back together\n        assert x.size() == orig_q_size\n        return x","efb3be14":"class DecoderLayer(nn.Module):\n    def __init__(self, dimensionality, units, dropout):\n        super(DecoderLayer, self).__init__()\n        # Layer norm\n        self.self_attention_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        # Multi-head attention \n        self.self_attention = MultiHeadAttention(dimensionality, dropout)\n        # Dropout \n        self.self_attention_dropout = nn.Dropout(dropout)\n\n        # Layer norm\n        self.enc_dec_attention_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        # Multi-head attention \n        self.enc_dec_attention = MultiHeadAttention(dimensionality, dropout)\n        # Dropout \n        self.enc_dec_attention_dropout = nn.Dropout(dropout)\n\n        # Layer norm\n        self.ffn_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        # Dense Layer\n        self.layer1 = nn.Linear(dimensionality, units)\n        self.relu = nn.ReLU()\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        # Dense Layer\n        self.layer2 = nn.Linear(units, dimensionality)\n        # Dropout\n        self.ffn_dropout = nn.Dropout(dropout)\n        \n        # Initialization\n        initialize_weight(self.layer1)\n        initialize_weight(self.layer2)\n\n    def forward(self, x, f, inputs_pad_mask, cache=None):\n        \"\"\"\n            params:\n            - x = float tensor of shape [batch_size, target_seq_length, dimensionality]\n            - f = float tensor of shape [batch_size, input_seq_length, dimensionality]\n            - inputs_pad_mask = boolean tensor of shape [batch_size, 1, input_seq_length]\n            - cache = Dictionnary of cache\n            \n            return: features = float tensor of shape [batch_size, target_seq_length, dimensionality]\n        \"\"\"\n        target_len = x.size()[1]\n        # Creates self mask for autoregressive property\n        self_mask = torch.triu(torch.ones(target_len, target_len, dtype=torch.uint8, device=device), diagonal=1).unsqueeze(0)\n\n        # Self-attention\n        y = self.self_attention_norm(x)\n        y = self.self_attention(y, y, y, self_mask)\n        y = self.self_attention_dropout(y)\n        x = x + y\n\n        y = self.enc_dec_attention_norm(x)\n        y = self.enc_dec_attention(y, f, f, inputs_pad_mask, cache)\n        y = self.enc_dec_attention_dropout(y)\n        x = x + y\n\n        y = self.ffn_norm(x)\n        y = self.layer1(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n        y = self.layer2(y)\n        y = self.ffn_dropout(y)\n        x = x + y\n        \n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, \n            vocab_size=1000,\n            pad_token=0,\n            embeddings=None,\n            use_embeddings=True,\n            n_layers=3,\n            dimensionality=512,\n            units=2048,\n            dropout=0.1\n        ):\n        super(Decoder, self).__init__()\n        # Checks\n        assert pad_token < vocab_size, \"pad_token must be within the vocabulary\"\n        # Attributes\n        self.dimensionality = dimensionality\n        self.pad_token = pad_token\n        self.vocab_size = vocab_size\n        self.use_embeddings = use_embeddings\n        if use_embeddings:\n            # Input embedding\n            if embeddings is None:\n                self.embedding = nn.Embedding(vocab_size, dimensionality)\n                # Initialization\n                nn.init.normal_(self.embedding.weight, mean=0, std=dimensionality**-0.5)\n            else:\n                self.embedding = embeddings\n        # Embedding dropout\n        self.embedding_dropout = nn.Dropout(dropout)\n        # Scaling factor is the square root of dimensionality\n        self.scale_factor = dimensionality ** 0.5\n        \n        # Layers\n        decoders = [DecoderLayer(dimensionality, units, dropout) for _ in range(n_layers)]\n        self.layers = nn.ModuleList(decoders)\n        self.last_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        \n        # Positional encoding\n        self.positional_encoding = PositionalEncoding(dimensionality)\n        \n        self.cache = None\n    \n    def forward(self, features, targets):\n        \"\"\"\n            params:\n            - features = tuple:\n                - float tensor of shape [batch_size, input_seq_length, dimensionality]\n                - boolean tensor of shape [batch_size, 1, input_seq_length]\n            - targets = integer tensor of shape [batch_size, target_seq_length)]\n            \n            return: features = float tensor of shape [batch_size, target_seq_length, dimensionality]\n        \"\"\"\n        f, inputs_pad_mask = features\n        x = targets\n        if self.use_embeddings:\n            # Padding mask\n            targets_pad_mask = (x == self.pad_token).unsqueeze(-2)\n            # Turn word into vectors\n            x = self.embedding(x)\n            # Zeros the pad\n            x.masked_fill_(targets_pad_mask.squeeze(1).unsqueeze(-1), 0)\n        # Shifting\n        x = x[:, :-1]\n        x = F.pad(x, (0, 0, 1, 0))\n        # Scale the embedding\n        x *= self.scale_factor\n        # Add positional encoding\n        x += self.positional_encoding(x)\n        # Dropout\n        x = self.embedding_dropout(x)\n        # Go through the layers\n        for i, dec_layer in enumerate(self.layers):\n            layer_cache = None\n            if self.cache is not None:\n                if i not in self.cache:\n                    self.cache[i] = {}\n                layer_cache = self.cache[i]\n            x = dec_layer(x, f, inputs_pad_mask, layer_cache)\n        x = self.last_norm(x)\n        if self.use_embeddings:\n            x = torch.matmul(x, self.embedding.weight.transpose(0, 1))\n        return x\n        ","9c138434":"class EncoderLayer(nn.Module):\n    def __init__(self, dimensionality, units, dropout):\n        super(EncoderLayer, self).__init__()\n        # Layer norm\n        self.self_attention_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        # Multi-head Attention\n        self.self_attention = MultiHeadAttention(dimensionality, dropout)\n        # Dropout\n        self.self_attention_dropout = nn.Dropout(dropout)\n        # Layer norm\n        self.ffn_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        # Dense Layer\n        self.layer1 = nn.Linear(dimensionality, units)\n        self.relu = nn.ReLU()\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        # Dense Layer\n        self.layer2 = nn.Linear(units, dimensionality)\n        # Dropout\n        self.ffn_dropout = nn.Dropout(dropout)\n        # Initialization\n        initialize_weight(self.layer1)\n        initialize_weight(self.layer2)\n\n    def forward(self, x, mask):\n        \"\"\"\n            params:\n            - x = float tensor of shape [batch_size, input_seq_length, dimensionality]\n            - mask = boolean tensor of shape [batch_size, 1, input_seq_length]\n            \n            return: features = float tensor of shape [batch_size, input_seq_length, dimensionality]\n        \"\"\"\n        # Muti-head attention part\n        y = self.self_attention_norm(x)\n        y = self.self_attention(y, y, y, mask)\n        y = self.self_attention_dropout(y)\n        x = x + y\n        # Feed forward part\n        y = self.ffn_norm(x)\n        y = self.layer1(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n        y = self.layer2(y)\n        y = self.ffn_dropout(y)\n        x = x + y\n        \n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self,\n            vocab_size=1000,\n            pad_token=0,\n            use_embeddings=True,\n            n_layers=3,\n            dimensionality=512,\n            units=2048,\n            dropout=0.1\n        ):\n        super(Encoder, self).__init__()\n        # Checks\n        assert pad_token < vocab_size, \"pad_token must be within the vocabulary\"\n        # Attributes\n        self.dimensionality = dimensionality\n        self.pad_token = pad_token\n        self.vocab_size = vocab_size\n        self.use_embeddings = use_embeddings\n        if use_embeddings:\n            # Input embedding\n            self.embedding = nn.Embedding(vocab_size, dimensionality)\n            # Initialization\n            nn.init.normal_(self.embedding.weight, mean=0, std=dimensionality**-0.5)\n        # Embedding dropout\n        self.embedding_dropout = nn.Dropout(dropout)\n        # Scaling factor is the square root of dimensionality\n        self.scale_factor = dimensionality ** 0.5\n        \n        # Layers\n        encoders = [EncoderLayer(dimensionality, units, dropout) for _ in range(n_layers)]\n        self.layers = nn.ModuleList(encoders)\n        self.last_norm = nn.LayerNorm(dimensionality, eps=1e-6)\n        \n        # Positional encoding\n        self.positional_encoding = PositionalEncoding(dimensionality)\n    \n    def get_embeddings(self):\n        if self.use_embeddings:\n            return self.embedding\n        else:\n            return None\n    \n    def forward(self, x):\n        \"\"\"\n            params:\n            - x = integer tensor of shape [batch_size, input_seq_length)]\n            \n            return: features = float tensor of shape [batch_size, input_seq_length, dimensionality]\n        \"\"\"\n        if self.use_embeddings:\n            # Padding mask\n            pad_mask = (x == self.pad_token).unsqueeze(-2)\n            # Turn word into vectors\n            x = self.embedding(x)\n            # Zeros the pad\n            x.masked_fill_(pad_mask.squeeze(1).unsqueeze(-1), 0)\n        else:\n            pad_mask = torch.zeros(x.shape[:-1]).bool().unsqueeze(-2)\n        # Scale the embedding\n        x *= self.scale_factor\n        # Add positional encoding\n        x += self.positional_encoding(x)\n        # Dropout\n        x = self.embedding_dropout(x)\n        # Go through layers\n        for enc_layer in self.layers:\n            x = enc_layer(x, pad_mask)\n        # Layer norm\n        x = self.last_norm(x)\n        # There we go it's encoded\n        return x, pad_mask\n","42e83618":"class Transformer(nn.Module):\n    def __init__(self, \n            encoder_cfg={},\n            decoder_cfg={},\n            dimensionality=512,\n            units=2048,\n            dropout=0.1,\n            share_embeddings=True\n        ):\n        super(Transformer, self).__init__()\n        # Checks\n        # Encoder configuration\n        encoder_cfg[\"dimensionality\"] = dimensionality\n        encoder_cfg[\"units\"] = units\n        encoder_cfg[\"dropout\"] = dropout\n        # Encoder module\n        self.encoder = Encoder(**encoder_cfg)\n        # Decoder configuration\n        decoder_cfg[\"dimensionality\"] = dimensionality\n        decoder_cfg[\"units\"] = units\n        decoder_cfg[\"dropout\"] = dropout\n        \n        if share_embeddings:\n            decoder_cfg[\"embeddings\"] = self.encoder.get_embeddings()\n        # Decoder module\n        self.decoder = Decoder(**decoder_cfg)\n        \n        assert not share_embeddings or self.encoder.vocab_size == self.decoder.vocab_size, \"If embeddings are shared, vocab_size must be the same between inputs and targets\"\n        assert self.encoder\n        \n        \n    def forward(self, inputs, targets=None):\n        \"\"\"\n            params:\n            - inputs = integer tensor of shape [batch_size, input_seq_length)]\n            - targets = integer tensor of shape [batch_size, target_seq_length]\n            \n            return: features = float tensor of shape []\n        \"\"\"\n        # Encode the inputs into features\n        features = self.encoder(inputs)\n        # Decode targets and features\n        features = self.decoder(features, targets)\n        return features","183da618":"# Decoder\nThe decoder compute queries and extract knowledge from input features\n![Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2019.20.00.png](attachment:Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2019.20.00.png)","1f1e4a3e":"Explanations quoted from the paper: [*Attention is all you need, 2017*](https:\/\/arxiv.org\/pdf\/1706.03762.pdf)\n\nCode heavily inspired by [this github repo](https:\/\/github.com\/tunz\/transformer-pytorch\/blob\/master\/model\/transformer.py)","0d1947af":"# Positional Encoding\nQuote from the paper, section 3.5:\n> Since our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed.\nIn this work, we use sine and cosine functions of different frequencies:\n$$\n\\begin{equation}\nPE_{(pos,2i)} = sin \\left(\\frac{pos}{10000^{\\frac{2i}{dmodel}}} \\right) \\\\\nPE_{(pos,2i+1)} = cos \\left(\\frac{pos}{10000^{\\frac{2i}{dmodel}}}\\right)\n\\end{equation}\n$$\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \u00b7 2\u03c0$. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\nWe also experimented with using learned positional embeddings instead, and found that the two\nversions produced nearly identical results [...]. We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n\n","7b02907e":"# Multi-head attention\nThe principal layer of the model, where the magic happens.\nIt combines multiple \"heads\" of attention like so:\n![Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2022.08.11.png](attachment:Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2022.08.11.png)\n\nEach head is framed as follow:\n![Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2022.34.55.png](attachment:Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2022.34.55.png)","90e52ec5":"# Encoder\n\nThe encoder build features from raw data.\n\n![Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2019.04.29.png](attachment:Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2019.04.29.png)","4c73bca2":"# Transformer\nNow it's time to put everything together.\nHere is the complete architecture of the transformer:\n\n![Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2019.01.27.png](attachment:Capture%20d%E2%80%99e%CC%81cran%202021-03-06%20a%CC%80%2019.01.27.png)"}}