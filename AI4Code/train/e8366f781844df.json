{"cell_type":{"ef2539fa":"code","1d2ebf90":"code","26d95a22":"code","9b66efc3":"code","0be6551a":"code","4e406ce4":"code","154cf78a":"code","2913da9f":"code","5f2ac24b":"code","928836c2":"code","dfe30c59":"code","1b5fb589":"code","2a02ffd6":"code","c5a3c5a3":"code","e9d17a90":"code","99f898c1":"code","4cba5b19":"code","18807cec":"code","98cdbe2d":"code","2c021ecf":"code","a8be30c6":"code","9ad40bee":"markdown","2dec2267":"markdown"},"source":{"ef2539fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # notes_data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input notes_data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d2ebf90":"!pip install xlrd\n!pip install openpyxl","26d95a22":"df = pd.read_excel('..\/input\/declarative-vs-nondeclarative\/Declarative.xlsx')\ndf.columns","9b66efc3":"notes_data = df[['IDS Number','Article Title','Abstract']]\nnotes_data.shape","0be6551a":"#Lower case (1)\n#The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, \u2018Analytics\u2019 and \u2018analytics\u2019 will be taken as different words.\nnotes_data['Abstract'] = notes_data['Abstract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#Removing Punctuation (2)\n#The next step is to remove punctuation, as it doesn\u2019t add any extra information while treating text notes_data. Therefore removing all instances of it will help us reduce the size of the training notes_data.\nnotes_data['Abstract'] = notes_data['Abstract'].str.replace('[^\\w\\s]','')\n\n#Removal of Stop Words (3)\n#As we discussed earlier, stop words (or commonly occurring words) should be removed from the text notes_data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries.\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\nnotes_data['Abstract'] = notes_data['Abstract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","4e406ce4":"\nimport os\nimport pandas as pd\nimport re\nfrom IPython.display import display, HTML\nimport spacy\nimport en_core_web_lg\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n\n##for clustering\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk import word_tokenize\n\n##To download packages directly from jupyter notebook\nimport sys ##execute the pip installs if you are running this for the first time\n# !{sys.executable} -m pip install gensim\n# !{sys.executable} -m pip install pyLDAvis\n\n# display(HTML(\"<style>div.output_scroll { width: 80em; }<\/style>\"))\n\n%matplotlib inline  \nimport matplotlib.pyplot as plt\n","154cf78a":"\"\"\"notes_data = notes_data.set_index('IDS Number')\"\"\"","2913da9f":"def clean_text(x):\n    x = \" \".join(x.split())\n    x= \" \".join((\" \".join(x.split(\"[**\"))).split(\"**]\"))\n    x = re.sub(r\"\\([^()]*\\)\", \"\", x)\n    key_value_strip =(x.split(\":\"))\n    ##remove all sub strings which have a length lesser than 50 characters\n    string = \" \".join([sub_unit for sub_unit in key_value_strip if len(sub_unit)>50])\n    x = re.sub(r\"(\\d+)+(\\.|\\))\", \"\", string)## remove all serialization eg 1. 1)\n    x = re.sub(r\"(\\*|\\?|=)+\", \"\", x) ##removing all *, ? and =\n    x = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", x) ## removing consecutive dupicate words\n    x = x.replace(\"FOLLOW UP\", \"FOLLOWUP\")\n    x = x.replace(\"FOLLOW-UP\", \"FOLLOWUP\")\n    x = re.sub(r\"(\\b)(f|F)(irst)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\",\"\",x)##remove firstname\n    x = re.sub(r\"(\\b)(l|L)(ast)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\", \"\", x)\n    x = re.sub(r\"(\\b)(d|D)\\.?(r|R)\\.?(\\b)\", \"\", x) #remove Dr abreviation\n    x = re.sub(r\"([^A-Za-z0-9\\s](\\s)){2,}\", \"\", x)##remove consecutive punctuations\n    \n\n    return(x.replace(\"  \", \" \"))\n\n\nnotes_data[\"Abstract\"] = notes_data[\"Abstract\"].apply(lambda x: clean_text(x))","5f2ac24b":"\nfrom gensim.models import TfidfModel, LsiModel\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import matutils\nfrom sklearn.cluster import KMeans","928836c2":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")\n\ndef tokenize(text):\n    text_wordlist = []\n    for x in re.split(r\"([.,!?\\s]+)\", text):\n        if x and x not in [\".\", \" \"] and x.lower() not in stop_words:\n            text_wordlist.append(x)\n    return(text_wordlist)\n\n\ntexts = list(notes_data[\"Abstract\"].apply( lambda text: tokenize(text)))\n                    \n# print(texts)\n                     \n# frequency_dictionary = defaultdict(int)\n# for text in texts:\n#     for token in text:\n#         frequency[token] += 1","dfe30c59":"from gensim import corpora\nfrom gensim.models.ldamodel import LdaModel\n\ndictionary = corpora.Dictionary(texts)\n# print(dictionary.id2token) ## to see the actual dictionary generated\ncorpus = [dictionary.doc2bow(text) for text in texts] ## document to bag of words\n\n# fit LDA model\ntranscripts_topics = LdaModel(corpus=corpus,\n                           id2word=dictionary,\n                           num_topics=30,\n                           alpha='auto', ##Learns an asymmetric prior from the corpus;\n##1D array of length=number of expected topics that expresses our a-priori belief for the each topics\u2019 probability.\n                           passes=100 ##Number of passes through the corpus during training.\n                             ) \nprint(transcripts_topics)\n# print out first 5 topics\nfor i, topic in enumerate(transcripts_topics.print_topics(5)):\n    print ('%d: %s\\n'%(i+1, topic))","1b5fb589":"# extract all document-topic distritbutions to dictionnary\ndocument_key = list(notes_data.index) ##get index of transcripts for topic in each\ndocument_topic = {}\nfor doc_id in range(len(corpus)):\n    docbok = corpus[doc_id]\n    doc_topics = transcripts_topics.get_document_topics(docbok, 0)\n    tmp = []\n    for topic_id, topic_prob in doc_topics:\n        tmp.append(topic_prob)\n    document_topic[document_key[doc_id]] = tmp","2a02ffd6":"# convert dictionnary of document-topic distritbutions to dataframe\ndf = pd.DataFrame.from_dict(document_topic, orient='index')\ndf","c5a3c5a3":"topic_column_names = ['topic_' + str(i) for i in range(0, 30)]\ndf.columns = topic_column_names\ndf","e9d17a90":"\ndf['Abstract'] = (notes_data['Abstract'])","99f898c1":"def find_topic(row):\n    if (row.loc[row>0.9]).any():\n        return row.loc[row>0.9].index[0]\n    else:\n        return None\n    \ndef find_propensity(row):\n    if (row.loc[row>0.9]).any():\n        return row.loc[row>0.9].values[0]\n    else:\n        return None\ndf['Abstract'] = df['Abstract'].astype(str)   \ndf['topic'] = df.loc[:, df.columns !='Abstract'].apply(find_topic, axis = 1)\ndf['propensity'] = df.iloc[:, 0:29].apply(find_propensity, axis = 1)","4cba5b19":"df.drop(columns=df.columns[:30], inplace=True)","18807cec":"display(df.head())","98cdbe2d":"# fig = plt.figure(figsize=(10,4))\nplt.rcParams[\"figure.figsize\"] = (20,4)\n\ntopic_frequency = df.iloc[:, :2].groupby('topic').count()\nax = topic_frequency.plot.bar( legend=False)\nplt.title(\"Frequency of Topics\", size=12)\n# plt.xticks(rotation=45)\nax.tick_params(axis='x', which='minor', labelsize='small', labelcolor='m', rotation=30)\n\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n\nplt.show()","2c021ecf":"topics_all = pd.DataFrame.from_dict(document_topic, orient='index')\ntopic_column_names = ['topic_' + str(i) for i in range(0, 30)]\ntopics_all.columns = topic_column_names\ntopics_all.to_csv(os.path.join(\".\/topic_propensities.csv\"))\n\n# print(topics_all.describe())\ndisplay(topics_all.head())","a8be30c6":"from scipy.cluster import hierarchy\nplt.figure(figsize=(25, 7))  \nplt.title(\"Dendrograms\")  \ndend = hierarchy.dendrogram(hierarchy.linkage(topics_all, method='ward'))\nplt.axhline(y=9, color='r', linestyle='--')\n","9ad40bee":"***Clustering by Assigning Final Topics to Transcripts***","2dec2267":"1. words o remove from abstract : https:\/\/www.editage.com\/all-about-publication\/research\/impressive-Verbs-to-use-in-your-Research-Paper.html \n\n* \u201cOur aim with this paper was \u2026\u201d for the Introduction. \n* \u201cWe analysed\/measured \u2026\u201d for the Methods. \n* \u201cWe found that \u2026\u201d for the Results. \n* \u201cWe discussed \u2026\u201d for the Discussion. \n* \u201cIn conclusion \u2026\u201d for the Conclusion. \n\nUse signals\nOne efficient way of writing a compressed abstract that quickly and effectively conveys\nthe message is to use signals, words and phrases such as:\nYour problem\/question: This paper asks if \u2026\n(Instead of: The question that this paper wants to answer\u2026)\nYour method: We used\u2026\n(Instead of: The methods employed in this research\u2026)\nYour results: I found that\u2026\n(Instead of: The following three aspects were discovered\u2026)\nFurthermore, using transitional words and phrases is a good way to connect sentences\nand explain logic relations within a paragraph:\nCausality: Thus, Therefore, in other words, in conclusion\nContrast: However, nevertheless, unlike\nAddition: In addition, furthermore, moreover\nComparison: Similarly, by comparison, equally\n\n\nargue, assert, claim, state\nassume, hypothesize, suggest\nfind, discover\ndemonstrate, prove, tes\n\n\nGiven the fact that x = y \u2026\nDespite the fact that x = y \u2026\nNotwithstanding the fact that x = y \u2026\n\n\n This study challenges\n This study focuses\n"}}