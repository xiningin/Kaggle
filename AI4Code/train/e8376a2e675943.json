{"cell_type":{"4950b9cd":"code","56401bc2":"code","f1308cf7":"code","a9321849":"code","26d956e5":"code","ea301c68":"code","68ad2df2":"code","4605d36e":"code","ad60e519":"code","b2edd86c":"code","67a2a459":"code","739ffea6":"code","ae5ac688":"code","719d94df":"code","bdbfe706":"code","b0d6e634":"code","70135cc7":"code","728dbca8":"code","36d98700":"code","7108b25a":"code","ead773ae":"code","132eb05b":"markdown","7b06e8c0":"markdown","417ac705":"markdown","0793b279":"markdown","bc488e20":"markdown","8b97be0f":"markdown","cce9c0dc":"markdown","49fc962d":"markdown","c5b32111":"markdown","f81c971c":"markdown","af7364bc":"markdown"},"source":{"4950b9cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport string\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n\n# Any results you write to the current directory are saved as output.","56401bc2":"basepath=\"\/kaggle\/input\/cat-in-the-dat\"\nprint(\"List of files in the directory %s\" %(os.listdir(basepath)))\ntraindata=os.path.join(basepath, 'train.csv')\ntestdata=os.path.join(basepath, 'test.csv')","f1308cf7":"train=pd.read_csv(traindata)\ntest=pd.read_csv(testdata)\n\ntrain_X=train.drop(['id','target'],axis=1)\ntrain_y=train['target']\ntest_X=test.drop('id',axis=1)\n\ntrain_X.head()","a9321849":"print(\"Shape of train and test data is %s %s\" %(train_X.shape, test_X.shape))","26d956e5":"traintestencode=pd.concat([train_X,test_X])\n\nprint(\"Various ordinal types %s\" %(traintestencode['ord_1'].unique()))\nprint(\"Various ordinal types %s\" %(traintestencode['ord_2'].unique()))\nprint(\"Various ordinal types %s\" %(traintestencode['ord_3'].unique()))\nprint(\"Various ordinal types %s\" %(traintestencode['ord_4'].unique()))\nprint(\"Various ordinal types %s\" %(traintestencode['ord_5'].unique()))\n\nord_1_map = {'Grandmaster': 5, 'Master': 4, 'Expert': 3,'Contributor': 2, 'Novice': 1}\n\nord_2map = {'Lava Hot':6,'Boiling Hot': 5, 'Hot': 4, 'Warm': 3, \n               'Freezing': 2, 'Cold': 1}\n\ntraintestencode['ord_1']=traintestencode['ord_1'].replace(ord_1_map)\ntraintestencode['ord_2']=traintestencode['ord_2'].replace(ord_2map)","ea301c68":"print(\"No of unique categories in the ord_5 ->\" ,traintestencode['ord_5'].nunique())\nprint(\"No of unique categories in the ord_4 ->\" ,traintestencode['ord_4'].nunique())\nprint(\"No of unique categories in the ord_3 ->\" ,traintestencode['ord_3'].nunique())","68ad2df2":"ord345=traintestencode[['ord_3','ord_4','ord_5']]\n\nn_orig_features = ord345.shape[1]\nhash_vector_size = 10\nct = ColumnTransformer([(f't_{i}', FeatureHasher(n_features=hash_vector_size, \n                        input_type='string'), i) for i in range(n_orig_features)])\n\nordencoded = ct.fit_transform(ord345)  # n_orig_features * hash_vector_size\n","4605d36e":"hashedfeatures=pd.DataFrame(data=ordencoded.toarray())\nhashedfeatures.columns=['hash_ord_'+str(f) for f in hashedfeatures.columns]\n\ntraintestencode.reset_index(inplace=True)\n\ntraintestencode=pd.concat([traintestencode,hashedfeatures],axis=1)\ntraintestencode.drop(['ord_3','ord_4','ord_5','index'],axis=1,inplace=True)","ad60e519":"traintestencode.shape","b2edd86c":"nominalfeatures=[ feat for feat in traintestencode.columns if feat.split('_')[0]=='nom' ]","67a2a459":"for nominal in nominalfeatures:\n        print(\"No of unique values for the column %s is %s\" %(nominal,traintestencode[nominal].nunique()))","739ffea6":"def nominalohencoding(features):\n    featurelist=[]\n    hashnominallist=[]\n    for feature in features:\n            if traintestencode[feature].nunique()<=10:\n                featurelist.append(feature)\n            else:\n                hashnominallist.append(feature)\n    dummies = pd.get_dummies(traintestencode[featurelist], drop_first=True, sparse=True)\n    return dummies,featurelist,hashnominallist","ae5ac688":"nominals,nominallist,hashnominallist=nominalohencoding(nominalfeatures)\ntraintestencode=pd.concat([traintestencode,nominals],axis=1)\ntraintestencode.drop(nominallist,axis=1,inplace=True)","719d94df":"traintestencode.columns","bdbfe706":"hashnominal=traintestencode[hashnominallist]\nhash_vector_size=20\n\nct = ColumnTransformer([(f't_{i}', FeatureHasher(n_features=hash_vector_size, \n                        input_type='string'), i) for i in range(len(hashnominallist))])\n\nhashnominalencoded = ct.fit_transform(hashnominal)  # n_orig_features * hash_vector_size\n\nhashedfeatures=pd.DataFrame(data=hashnominalencoded)\nhashedfeatures.columns=['hash_nom_'+str(f) for f in hashedfeatures.columns]\n\n#traintestencode.reset_index(inplace=True)\n\ntraintestencode=pd.concat([traintestencode,hashedfeatures],axis=1)\ntraintestencode.drop(hashnominallist,axis=1,inplace=True)","b0d6e634":"traintestencode.head()","70135cc7":"binlist=['bin_3','bin_4']\nbindummies = pd.get_dummies(traintestencode[binlist], drop_first=True, sparse=True)\ntraintestencode=pd.concat([traintestencode,bindummies],axis=1)\ntraintestencode.drop(binlist,axis=1,inplace=True)","728dbca8":"def encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n\ntraintestencode = encode(traintestencode, 'day', 365)\ntraintestencode = encode(traintestencode, 'month', 12)\n\ntraintestencode.drop(['day','month'],axis=1,inplace=True)","36d98700":"X_train=traintestencode.loc[:train_X.shape[0]-1,:]\nX_test=traintestencode.loc[train_X.shape[0]:,:]","7108b25a":"kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\npred_test_full =0\ncv_score =[]\ni=1\n\nfor train_index,test_index in kf.split(X_train,train_y):\n    print('{} of KFold {}'.format(i,kf.n_splits))\n    #print(\"dfdf\",X_train[train_index])\n    xtr,xvl= X_train.loc[train_index],X_train.loc[test_index]\n    ytr,yvl = train_y.loc[train_index],train_y.loc[test_index]\n    \n    #model\n    lr = LogisticRegression()\n    lr.fit(xtr,ytr)\n    score = roc_auc_score(yvl,lr.predict(xvl))\n    print('ROC AUC score:',score)\n    cv_score.append(score)    \n    pred_test = lr.predict_proba(X_test)[:,1]\n    pred_test_full +=pred_test\n    i+=1","ead773ae":"# Make submission\ny_pred = pred_test_full\/5\nsubmission = pd.DataFrame({'id': test['id'].values.tolist(), 'target': y_pred})\nsubmission.to_csv('submission.csv', index=False)","132eb05b":"**Work in progress!!!!!!!**","7b06e8c0":"> For columns having unique value less than 10 we will do one hot encoding for the rest we will do hashing","417ac705":"*One hot encode the bin3 and bin4 features*","0793b279":"****Dealing with ordinal features****","bc488e20":"We will do the hash encoding for the remaining ordinal variables as the categories are more","8b97be0f":"**Cross validation**","cce9c0dc":"**Let's check for nominal columns**","49fc962d":"**First things first!!**\n> Building logistic regression","c5b32111":"**Dealing with cyclical features**","f81c971c":"**Let's get into cleaning the features for modelling**\n1. Dealing with ordinal features\n2. Dealing with nominal features\n3. Dealing with binary features\n4. Dealing with cyclical features","af7364bc":"**Moving to binary features**"}}