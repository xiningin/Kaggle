{"cell_type":{"787e42ce":"code","4d1aaf70":"code","98af2dae":"code","bb3cc6d5":"code","7f28a9da":"markdown","3f69f372":"markdown","8af605c1":"markdown","83f0c3fe":"markdown","68c964e3":"markdown","ef88c18c":"markdown","cdd8d525":"markdown","5986a5e3":"markdown","b78f000c":"markdown","a66c8dc3":"markdown","180193d8":"markdown"},"source":{"787e42ce":"# Import data analysis and wrangling libraries\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# Import visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Acquire Data\nfeatures = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","4d1aaf70":"\n\n\n\ndef get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'\n    \ndef title_map(title):\n    if title in ['Mr']:            \n        return 1\n    elif title in ['Master']:\n        return 2\n    elif title in ['Ms', 'Mlle', 'Miss']:\n        return 3\n    elif title in ['Mme', 'Mrs']:\n        return 4\n    else:\n        return 5\n    \n# Get Title from names    \nfeatures['Title'] = features['Name'].apply(get_title).apply(title_map)\n    \n    \n#One Hot Encoding - Convert categorical data to binary vectors - Train data\none_hot = pd.get_dummies(features['Sex'])\none_hot2 = pd.get_dummies(features['Embarked'])\none_hot3 = pd.get_dummies(features['Title'])\nfeatures = features.join(one_hot).join(one_hot2).join(one_hot3)\n\n#Clean Data - Drop unwanted Train Data\nfeatures = features.drop('Sex',  axis = 1)\nfeatures = features.drop('Embarked', axis = 1)\nfeatures = features.drop('Cabin', axis = 1)\nfeatures = features.drop('Name', axis =1)\nfeatures = features.drop('Ticket', axis = 1)\n\n#fill blank values with mean value\nfeatures = features.fillna(features.mean())\nfeatures.iloc[:,5:].head(5)\n\nlabels = np.array(features['Survived'])\nfeatures = features.drop('Survived', axis = 1)\n\n\n#Converting features data frame into np array to be used in regressor model \nfeatures = np.array(features)\n\n#random forest Regression\nrf = RandomForestRegressor (n_estimators = 1000, random_state = 42)\nrf.fit(features, labels)\n\ntest_data['Title'] = test_data['Name'].apply(get_title).apply(title_map)\n\n#One Hot Encoding - Convert categorical data to binary vectors - Test Data\none_hot = pd.get_dummies(test_data['Sex'])\none_hot2 = pd.get_dummies(test_data['Embarked'])\none_hot3 = pd.get_dummies(test_data['Title'])\n\ntest_data = test_data.join(one_hot)\ntest_data = test_data.join(one_hot2)\ntest_data = test_data.join(one_hot3)\n\n#Clean Data - Drop unwanted Test Data\ntest_data = test_data.drop('Sex',  axis = 1)\ntest_data = test_data.drop('Embarked', axis = 1)\ntest_data = test_data.drop('Cabin', axis = 1)\ntest_data = test_data.drop('Name', axis =1)\ntest_data = test_data.drop('Ticket', axis = 1)\n\ntest_data = test_data.fillna(test_data.mean())\n\ntest_data = np.array(test_data)\npredictions = rf.predict(test_data)\n\n\nprint(predictions)\n\n#Export Result to CSV \ntest_dataset_copy = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_dataset_copy[\"PassengerId\"],\n        \"Survived\": predictions\n})\n\nsubmission.to_csv('submission_rf.csv', index=False)","98af2dae":"\n# Import data analysis and wrangling libraries\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# Import visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Acquire Data\nfeatures = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\n\ndef get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'\n    \ndef title_map(title):\n    if title in ['Mr']:            \n        return 1\n    elif title in ['Master']:\n        return 2\n    elif title in ['Ms', 'Mlle', 'Miss']:\n        return 3\n    elif title in ['Mme', 'Mrs']:\n        return 4\n    else:\n        return 5\n        \n# Get Title from names    \nfeatures['Title'] = features['Name'].apply(get_title).apply(title_map)\n    \n#One Hot Encoding - Convert categorical data to binary vectors - Train data\none_hot = pd.get_dummies(features['Sex'])\none_hot2 = pd.get_dummies(features['Embarked'])\none_hot3 = pd.get_dummies(features['Title'])\nfeatures = features.join(one_hot).join(one_hot2).join(one_hot3)\n\n\n#Clean Data - Drop unwanted Train Data\nfeatures = features.drop('Sex',  axis = 1)\nfeatures = features.drop('Embarked', axis = 1)\nfeatures = features.drop('Cabin', axis = 1)\nfeatures = features.drop('Name', axis =1)\nfeatures = features.drop('Ticket', axis = 1)\n\n#fill blank values with mean value\nfeatures = features.fillna(features.mean())\n\n\n#labels = np.array(features['Survived'])\nlabels = features['Survived']\nfeatures = features.drop('Survived', axis = 1)\n\nfeature_list = list(features.columns)\n#features = np.array(features)\n\nsvclassifier = SVC(kernel = 'linear')\nsvclassifier.fit(features, labels)\n\ntest_data['Title'] = test_data['Name'].apply(get_title).apply(title_map)\n\n\n#One Hot Encoding - Convert categorical data to binary vectors - Test Data\none_hot = pd.get_dummies(test_data['Sex'])\none_hot2 = pd.get_dummies(test_data['Embarked'])\none_hot3 = pd.get_dummies(test_data['Title'])\n\ntest_data = test_data.join(one_hot)\ntest_data = test_data.join(one_hot2)\ntest_data = test_data.join(one_hot3)\n\n#Clean Data - Drop unwanted Test Data\ntest_data = test_data.drop('Sex',  axis = 1)\ntest_data = test_data.drop('Embarked', axis = 1)\ntest_data = test_data.drop('Cabin', axis = 1)\ntest_data = test_data.drop('Name', axis =1)\ntest_data = test_data.drop('Ticket', axis = 1)\n\ntest_data = test_data.fillna(test_data.mean())\n\npredictions = svclassifier.predict(test_data)\n\nprint(predictions)\n\n\n#Export Result to CSV \ntest_dataset_copy = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_dataset_copy[\"PassengerId\"],\n        \"Survived\": predictions\n})\n\nsubmission.to_csv('submission_svm.csv', index=False)","bb3cc6d5":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn import cross_validation\nfrom sklearn.model_selection import cross_val_score\n\n#Acquire Data\nfeatures = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\n\ndef get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'\n    \ndef title_map(title):\n    if title in ['Mr']:            \n        return 1\n    elif title in ['Master']:\n        return 2\n    elif title in ['Ms', 'Mlle', 'Miss']:\n        return 3\n    elif title in ['Mme', 'Mrs']:\n        return 4\n    else:\n        return 5\n        \n# Get Title from names    \nfeatures['Title'] = features['Name'].apply(get_title).apply(title_map)\n    \n#One Hot Encoding - Convert categorical data to binary vectors - Train data\none_hot = pd.get_dummies(features['Sex'])\none_hot2 = pd.get_dummies(features['Embarked'])\none_hot3 = pd.get_dummies(features['Title'])\nfeatures = features.join(one_hot).join(one_hot2).join(one_hot3)\n\n\n#Clean Data - Drop unwanted Train Data\nfeatures = features.drop('Sex',  axis = 1)\nfeatures = features.drop('Embarked', axis = 1)\nfeatures = features.drop('Cabin', axis = 1)\nfeatures = features.drop('Name', axis =1)\nfeatures = features.drop('Ticket', axis = 1)\n\n#fill blank values with mean value\nfeatures = features.fillna(features.mean())\n\n\n#labels = np.array(features['Survived'])\nlabels = features['Survived']\nfeatures = features.drop('Survived', axis = 1)\n\nfeature_list = list(features.columns)\n#features = np.array(features)\n\nx = np.array(features)\ny = np.array(labels)\n\nclf = xgb.XGBClassifier()\ncv = cross_validation.KFold(len(x), n_folds=20, shuffle=True, random_state=1)\nscores = cross_validation.cross_val_score(clf, x, y, cv=cv, n_jobs=1, scoring='accuracy')\nclf.fit(x,y)\n\n\ntest_data['Title'] = test_data['Name'].apply(get_title).apply(title_map)\n\n\n#One Hot Encoding - Convert categorical data to binary vectors - Test Data\none_hot = pd.get_dummies(test_data['Sex'])\none_hot2 = pd.get_dummies(test_data['Embarked'])\none_hot3 = pd.get_dummies(test_data['Title'])\n\ntest_data = test_data.join(one_hot)\ntest_data = test_data.join(one_hot2)\ntest_data = test_data.join(one_hot3)\n\n#Clean Data - Drop unwanted Test Data\ntest_data = test_data.drop('Sex',  axis = 1)\ntest_data = test_data.drop('Embarked', axis = 1)\ntest_data = test_data.drop('Cabin', axis = 1)\ntest_data = test_data.drop('Name', axis =1)\ntest_data = test_data.drop('Ticket', axis = 1)\n\ntest_data = test_data.fillna(test_data.mean())\n\npredictions = clf.predict(test_data)\n\nprint(predictions)\n\n#Export Result to CSV \ntest_dataset_copy = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_dataset_copy[\"PassengerId\"],\n        \"Survived\": predictions\n})\n\nsubmission.to_csv('submission_xgb.csv', index=False)","7f28a9da":"**Model 2 : SVM - Linear Accuracy 79 %**","3f69f372":"<h1>Learning Machine Learning from Scrtach\n<h3>Nishank Magoo -  Dec 2020\n\n\n\n*! This is an ongoing project that I will it update regularly, so stay tuned !*\n\n\n\n","8af605c1":"**Question or Problem Definition **","83f0c3fe":"**Model 1 :  SVM - Linear Accuracy 79 %**","68c964e3":"This notebook is going to be focused on learning and understanding all machine learning algorithms .This will be divided into 3 parts for better comprehension -\n\n* Supervised Learning\n* Unsupervised Learning\n* Deep Learning","ef88c18c":"**Workflow stages**","cdd8d525":"Competition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. ","5986a5e3":"**Introduction**","b78f000c":"**Here are some of the most important Supervised learning algorithms **\n\n1. K Nearest neighbour - Distance based Classification Model\n2. Naive Bayes - Probability based Classification Model\n3.  Logistic Regression - Classification Model for linearly separable data\n4.  Linear regression - linear Algebra Based Regression Model\n5.  Support vector Machines - Similar to logistic Regression Classification Model. The goal is to find the best line that has maximum probability of classifying unseen points correctly. How you define the notion of \"best\" gives you different models like SVM and LR\n6.  Decison tree - nested If-Else Classfictaion \n7.  Random Forest - Decision tree + Row Sampling + Column Sampling . Low bias + high Variance ---Apply RF---> low bias + Low Variance\n8.  Gradient boosting - Decision tree + Optimization by Differentiable Loss Function . Examples of loss function - Square Loss, Hinge Loss, Logistic Loss, log loss, Exponential Loss\n9.  XGBoost - Gradient bosting + row sampling + column sampling . high bias + low Variance ---Apply XGB---> low bias + Low Variance\n\n**Here are some of the most important UnSupervised learning algorithms**\n\n1. K Means - centroid based Model \n2. Hierarchichal - hierarchy based Model\n3. DBSCAN - Density-based spatial clustering of applications with noise - Densiy Based Model . (Dense- Sparse)\n\n**And Lastly, Deep Learning**\n\n1. Artificial Neural network (Input + One Hidden layer + Output)\n2. Deep Multi Layer Perceptrons (Input + Multiple hidden Layers + Output)\n3. Convolutional Neural Network - Majorly for Image . Receptive Field. Horizontal Edge Detection. Vertical Edge detection. Padding. Strides\n4. Recurrent neural network - Majorly for \"Sequence of Words\"\n\n\n\n","a66c8dc3":"**Model 3 : XGBoost Model**","180193d8":"Every Problem goes through these seven stages - \n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results."}}