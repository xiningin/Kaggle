{"cell_type":{"34171103":"code","bc8abd01":"code","4fda6d9a":"code","0c392725":"code","4f12a7f8":"code","47705a0b":"code","8a84ca8f":"code","e4701cf8":"code","aa771fe0":"code","2166ba2e":"code","4d4ec9b5":"code","5658bb3e":"code","64564b1b":"code","bdd72a46":"code","be6767b7":"code","dfe7e918":"code","2adaba24":"code","668e3456":"code","114ae9be":"code","952f4618":"code","ffa0eec6":"code","9c0b127c":"code","046c2840":"code","e8c11132":"code","c141cd2e":"code","538c678b":"code","970c2832":"code","2b47991e":"code","a53e9538":"code","410d61c7":"code","c636c156":"code","7e30f856":"code","db02a88f":"code","6d638be3":"code","9dff35f3":"code","c0f4a9ee":"code","65db9ecb":"code","a4293714":"code","f31cdee6":"code","1eb3b041":"code","cc078695":"code","4a910f6d":"markdown","f2a5e34e":"markdown","f98c0029":"markdown","0f3ee885":"markdown","d08e6e57":"markdown","7c4e83ce":"markdown","de36b3b5":"markdown","bc3f3a32":"markdown","fef32cd7":"markdown","b411067f":"markdown","0d481c1e":"markdown","da04aa51":"markdown","b832ebf3":"markdown","021721ec":"markdown","11c2cb37":"markdown","9364e4ac":"markdown","449d4995":"markdown","b7ddf491":"markdown","4d778204":"markdown","761e947d":"markdown","35bf9d9f":"markdown","407c9ac5":"markdown","d4f25047":"markdown","fff92161":"markdown","2e19cfe6":"markdown","545d59b9":"markdown"},"source":{"34171103":"import pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/cleaned\/train_cleaned.csv')\ntest = pd.read_csv('\/kaggle\/input\/cleaned\/test_cleaned.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)","bc8abd01":"train.head()","4fda6d9a":"test.head()","0c392725":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train['clean_text'], train['target'], shuffle=True, test_size=0.2, random_state=0)","4f12a7f8":"from sklearn.feature_extraction.text import CountVectorizer\n\nbv = CountVectorizer(max_features=50000, ngram_range=(1, 2))\ntrain_data_features = bv.fit_transform(X_train)\nval_data_features = bv.transform(X_val)","47705a0b":"vocab = bv.get_feature_names()\nprint(vocab[:10])","8a84ca8f":"# Calculate the ratio of feature f\ndef pr(y_i):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","e4701cf8":"import numpy as np\n\nx = train_data_features # these are simply counts of unigrams and bigrams as a sparse matrix\ny = y_train.values # targets\n\nr = np.log(pr(1)\/pr(0)) # probability matrix for each feature based on the training set\nb = np.log((y==1).mean() \/ (y==0).mean()) # bias","aa771fe0":"r.shape, r","2166ba2e":"y_val_pre_preds = val_data_features @ r.T + b # multiply the probability matrix with the features in the validation set\ny_val_preds = y_val_pre_preds.T > 0 # get disaster tweet predictions\n(y_val_preds == y_val.values).mean() # estimate accuracy","4d4ec9b5":"x = train_data_features.sign() # binarize\nr = np.log(pr(1)\/pr(0))\n\ny_val_pre_preds = val_data_features.sign() @ r.T + b \ny_val_preds = y_val_pre_preds.T>0\n(y_val_preds == y_val.values).mean()","5658bb3e":"from sklearn.linear_model import LogisticRegression\n\nlogistic = LogisticRegression(C=0.5)\nlogistic.fit(train_data_features, y_train)\ny_val_preds = logistic.predict(val_data_features)\n(y_val_preds==y_val).mean()","64564b1b":"logistic = LogisticRegression(C=0.5)\nlogistic.fit(train_data_features.sign(), y_train)\ny_val_preds = logistic.predict(val_data_features.sign())\n(y_val_preds==y_val).mean()","bdd72a46":"import eli5\n\neli5.show_weights(logistic, vec=bv, top=25)","be6767b7":"x = train_data_features\ny = y_train.values\n\nr = np.log(pr(1)\/pr(0))\nx_nb = x.multiply(r)\n\nlogistic = LogisticRegression(C=0.5)\nlogistic.fit(x_nb, y);\n\nval_x_nb = val_data_features.multiply(r)\ny_val_preds = logistic.predict(val_x_nb)\n(y_val_preds.T==y_val.values).mean()","dfe7e918":"x = train_data_features.sign()\ny = y_train.values\n\nr = np.log(pr(1)\/pr(0))\nx_nb = x.multiply(r)\n\nlogistic = LogisticRegression(C=0.5)\nlogistic.fit(x_nb, y);\n\nval_x_nb = val_data_features.sign().multiply(r)\ny_val_preds = logistic.predict(val_x_nb)\n(y_val_preds.T==y_val.values).mean()","2adaba24":"eli5.show_weights(logistic, vec=bv, top=25)","668e3456":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators = 200)\nforest = forest.fit(train_data_features, y_train)","114ae9be":"# Predict on training set\ny_train_pred = forest.predict(train_data_features)\n\n# Predict on validation set\ny_val_pred = forest.predict(val_data_features)","952f4618":"(y_val_pred == y_val).mean()","ffa0eec6":"logistic = LogisticRegression(C=0.5)\nlogistic.fit(train_data_features.sign(), y_train)\ny_val_pred = logistic.predict(val_data_features.sign())","9c0b127c":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_confusion_matrix(y_true, y_pred, ax, vmax=None,\n                          normed=True, title='Confusion matrix'):\n    cm = confusion_matrix(y_true, y_pred)\n    if normed:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    sns.heatmap(cm, vmax=vmax, annot=True, square=True, ax=ax, \n                cmap='Blues', cbar=False, linecolor='k',\n               linewidths=1)\n    ax.set_title(title, fontsize=16)\n    ax.set_ylabel('True labels', fontsize=12)\n    ax.set_xlabel('Predicted labels', y=1.10, fontsize=12)","046c2840":"fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\nplot_confusion_matrix(y_train, y_train_pred, ax=axis1, \n                      title='Confusion matrix (train data)')\nplot_confusion_matrix(y_val, y_val_pred, ax=axis2, \n                      title='Confusion matrix (validation data)')","e8c11132":"from sklearn.metrics import classification_report, f1_score\n\nprint('Classification report on Test set: \\n', classification_report(y_val, y_val_pred))","c141cd2e":"y_val_probs = logistic.predict_proba(val_data_features.sign())[:, 1]\n\nfor thresh in np.arange(0.3, 0.5, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, f1_score(y_val, (y_val_probs>thresh).astype(int))))","538c678b":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\ntrain_data_features = tfidf.fit_transform(X_train)\nval_data_features = tfidf.transform(X_val)","970c2832":"vocab = tfidf.get_feature_names()\nprint(vocab[:10])","2b47991e":"from sklearn.linear_model import LogisticRegression\n\nlogistic = LogisticRegression(C=0.5)\nlogistic.fit(train_data_features, y_train)\ny_val_preds = logistic.predict(val_data_features)\n(y_val_preds==y_val).mean()","a53e9538":"import eli5\n\neli5.show_weights(logistic, vec=tfidf, top=25)","410d61c7":"x = train_data_features\ny = y_train.values\n\nr = np.log(pr(1)\/pr(0))\nx_nb = x.multiply(r)\n\nlogistic = LogisticRegression(C=0.5)\nlogistic.fit(x_nb, y);\n\nval_x_nb = val_data_features.multiply(r)\ny_val_preds = logistic.predict(val_x_nb)\n(y_val_preds.T==y_val.values).mean()","c636c156":"eli5.show_weights(logistic, vec=tfidf, top=25)","7e30f856":"forest = RandomForestClassifier(n_estimators = 200)\nforest = forest.fit(train_data_features, y_train)","db02a88f":"# Predict on training set\ny_train_pred = forest.predict(train_data_features)\n\n# Predict on validation set\ny_val_pred = forest.predict(val_data_features)","6d638be3":"(y_val_pred == y_val).mean()","9dff35f3":"test = pd.read_csv('\/kaggle\/input\/cleaned\/test_cleaned.csv')\n\ntest.head()","c0f4a9ee":"test['clean_text']","65db9ecb":"from sklearn.feature_extraction.text import CountVectorizer\n\ntrain_data_features = bv.fit_transform(X_train)\ntest['clean_text'] = test['clean_text'].apply(lambda x: str(x))\ntest_data_features = bv.transform(test['clean_text'])","a4293714":"logistic = LogisticRegression(C=0.5)\nlogistic.fit(train_data_features.sign(), y_train)\ny_test_pred = logistic.predict(test_data_features.sign())","f31cdee6":"y_test_probs = logistic.predict_proba(test_data_features.sign())[:, 1]","1eb3b041":"(y_test_probs>0.45).sum()\/3262","cc078695":"test['target'] = y_test_pred\ntest.head()","4a910f6d":"There we go. We got the best performance out of binarized logistic regression with an accuracy of ~82.2% on the validation set. This is definitely not a bad start, but we should be able to improve on this. In the next few notebooks I'll use word embeddings using GloVe and FastText and eventually will move on to deep learning models using LSTM!","f2a5e34e":"Binarized version is slightly better. Let's take a look at the words used by the model for classifying disaster tweets using the amazing `eli5` package","f98c0029":"Not bad. We get significant improvement over the theoretical model. Let's see if the binarized version works better","0f3ee885":"<h2>Final thoughts on the baseline model<\/h2>","d08e6e57":"According to the Logistic Regression model, 36% of tweets in the test set are classified as disaster tweets","7c4e83ce":"<b>Train the TFIDF model<\/b>\n\nOnce again we'll use unigrams and bigrams","de36b3b5":"<h2>Load and split the data<\/h2>\n\nLet's load our cleaned data first","bc3f3a32":"<h2>NBSVM<\/h2>","fef32cd7":"Apply the ratio <i>r<\/i> to the <b>Naive Bayes formula<\/b> and get predictions on the validation set","b411067f":"About the same as non-binarized model. Naive Bayes is simply a theoretical model, which doesn't really apply well to the real word.  We can surely improve on this model by actually using the data that we are given. ","0d481c1e":"The highest f1 score was achieved was at the threshold of approx. 0.45","da04aa51":"<h2>Random Forest Classifier<\/h2>","b832ebf3":"<h2>Naive Bayes<\/h2>\n\nRefer to Jeremy Howard's lecture: https:\/\/www.youtube.com\/watch?v=37sFIak42Sc&feature=youtu.be&t=3745\n\nNaive Bayes theorem is an extension of <b>Bayes' Theorem<\/b>, which can be defined as <i>the probability of an event occurring given the probability of another event that has already occurred<\/i>. It can be mathematically stated as: \n\n$P(A|B) = \\frac{\\text{P(B|A)P(A)}}{\\text{P(B)}}$\n\n- The idea is to find P(A) given the probability of event B\n- P(A) is <i>a priori<\/i> probability of event A, i.e., prior to seeing new evidence B. \n- P(A|B) is the <i>a posteriori<\/i> probability of A conditional upon event B occurring. \n\nIn our case, it can be generalized to: what is the probability of a certain tweet being a disaster tweet, i.e., classification of 1, given a collection of tokens in that tweet?  \n\n<h4>Naive Assumption<\/h4>\n\nThis assumption states that all the features must be <i>independent<\/i> of each other.  This is definitely not realistic in most real-life scenarios, but it is a good starting point for a theoretical model.  For example, we know and can assert definitively, that the words in a sentence are not independent of each other.  That's the whole idea behind CBOW and SkipGram models that words aren't independent of each other and we can predict them based on context. \n\nWe define the <b>log-count ratio<\/b>, <i>r<\/i>, for each word <i>f<\/i>:\n\n$r = \\log \\frac{\\text{ratio of feature $f$ in disaster tweets}}{\\text{ratio of feature $f$ in non-disaster tweets}}$\n\nwhere ratio of feature, <i>f<\/i>, in disaster tweets is the number of times a disaster tweet has a feature divided by the number of disaster tweets","021721ec":"Now let's try <b>Binarized Naive Bayes<\/b>, which disregards the frequency of a token and simply assigns 1, if it is present and 0 if it is not. ","11c2cb37":"<b>Train the Bag of N-grams model<\/b>","9364e4ac":"<h2>Random Forest Classifier<\/h2>\n\nNow let's try how logistic regression compares with a random forest classifier","449d4995":"<h2>Bag on N-grams<\/h2>\n\nI'll implement BOW using `CountVectorizer` from `sklearn`. This is pretty straight-forward -- just count the occurrence of each ngram in the corpus. I'll use unigrams and bigrams here.  We don't have that much data and trigrams are likely to just add noise","b7ddf491":"<h2>NBSVM<\/h2>","4d778204":"<h2>Logistic Regression<\/h2>","761e947d":"Does not perform as well as the simple logistic regression model, which is a bit surprising given its track record historically, but we have only 50,000 features as opposed to million, which is a huuuuge limiting factor. Given this, it's probably going to suffer in the binarized version as well, but let's run it anyway","35bf9d9f":"<b>Back to the Best model -- Binarized Logistic Regression<\/b>","407c9ac5":"<h2>TFIDF<\/h2>\n\nTerm-frequency inverse document frequency.  I have described this in detail in many places before. Logic will tell you that the binariezed version will give exactly the same results as BOW so let's just quickly run through regular logistic regression and random forest even though we don't expect either to perform as well as the binarized logistic regression model above","d4f25047":"A combined version of Naive Bayes and SVM where rather than training the Logistic Regression model on simply the counts of unigrams and bigrams, we train it on the log-count ratios of features.  See the video from Jeremy Howard above for the intuition and the brilliant explanation for why this works even though seemingly both approaches should give similar results. ","fff92161":"<h2>Logistic Regression<\/h2>","2e19cfe6":"I'll start with creating some simple frequency-based baseline models using BOW, Naive Bayes classifier and logistic regression.  In later notebooks, I'll improve on these models","545d59b9":"<h2>Submission set<\/h2>\n\nMake predictions on the submission set using binarized logistic regression"}}