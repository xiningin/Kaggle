{"cell_type":{"386e64d6":"code","dca65554":"code","0f5736d2":"code","b57a6304":"code","5b32705a":"code","3d69b6aa":"code","db8c3704":"code","9aff29ba":"code","3fc4acf6":"code","25277533":"code","8209435a":"code","e956e05a":"code","450fbb6e":"code","8e7963b2":"code","f407f34c":"code","99785b97":"code","e49507d4":"code","927c0abb":"code","c3e6d990":"code","e0804c11":"code","95c19abd":"code","516f3ea0":"code","31ef1204":"code","bc740261":"code","fa46f282":"code","086b1a48":"code","e833e18e":"code","6c1a3681":"code","60f27ba6":"code","aa7f73f2":"code","d5d17fa7":"code","a4f14970":"code","9ea35d32":"code","ea7a9614":"code","95e44796":"code","14c6d188":"code","70e2c587":"code","e26c979d":"code","5ba3b407":"code","dabed1bb":"code","4ab2c080":"code","e63af81f":"code","74bf5e4a":"code","2d7cf6b0":"code","29b03990":"code","bccc63a1":"code","de109dbd":"code","d9da0231":"code","0e345a8b":"code","1621c20f":"code","65888d15":"code","cdc43be1":"code","72903730":"code","6337bac5":"code","53362ffe":"code","060b77a8":"code","b99f2968":"code","30ea367c":"code","d7a8c679":"code","11c60d47":"markdown","854aa607":"markdown","521d3179":"markdown","58eda20b":"markdown","14c069e6":"markdown","8bea87a3":"markdown","ccb82941":"markdown","c5543f5a":"markdown","8c71a53b":"markdown","a0e6d4e0":"markdown","1ea1b17f":"markdown","92311223":"markdown","9b9cc83e":"markdown","e146e6f2":"markdown","05cb4506":"markdown","3708175a":"markdown","ddff85f6":"markdown","6da2c422":"markdown","964428c5":"markdown","d91caf23":"markdown","7e22d86e":"markdown","4129776a":"markdown","6ab98d35":"markdown","b7a02bdf":"markdown","fbdb6054":"markdown","0833cdab":"markdown","023f999c":"markdown","df7525d8":"markdown","fbf080b9":"markdown","70aeb260":"markdown","2864f38b":"markdown","21a9596e":"markdown","b8c25523":"markdown","069516d5":"markdown","993890e1":"markdown","3d79a11c":"markdown","5b1f4566":"markdown","12092abb":"markdown"},"source":{"386e64d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\nimport warnings\nwarnings.filterwarnings('ignore')","dca65554":"#Importing Data\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","0f5736d2":"train.shape, test.shape","b57a6304":"#Firstly, I'm just checking if there is repeated variables (same ID)\n#len(set(x)) tell the size of the set of unique elements of x\nunique_value = len(set(train.Id))\nprint(unique_value)","5b32705a":"# ckeck the columns\ntrain.columns","3d69b6aa":"# Let's check for missing data from train dataset\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending=False)\nmissing_values = pd.concat([total,percent],axis=1,keys=['TOTAL','PERCENT(%)'])\nmissing_values.head(30)","db8c3704":"# Dropping missing values\n# Let's delete variables (columns) with missing values.\ntrain = train.drop((missing_values[missing_values['TOTAL']>=8]).index, 1)\n\n# Let's also fill the only observation missing an 'Electrical' field.\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n\n# Double check that there no data is missing anymore\ntrain.isnull().sum().max()","9aff29ba":"# After deleting the columns\ntrain.columns","3fc4acf6":"train.head()","25277533":"# Let's check for missing data from test dataset.\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()*100).sort_values(ascending=False)\n\n#Let's build a table\nmissing_values1 = pd.concat([total,percent],axis=1,keys=['TOTAL','PERCENT(%)'])\nmissing_values1.head(40)","8209435a":"# Deleting features with missing values in the test data as in the train data.\ntest = test.drop((missing_values1[missing_values1['TOTAL']>4]).index,1)","e956e05a":"# missing data from test dataset\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\n\n#Let's build a table\nmissing_values1 = pd.concat([total,percent],axis=1,keys=['TOTAL','PERCENT'])\nmissing_values1.head(40)","450fbb6e":"test.isnull().sum().max()","8e7963b2":"null_features = (missing_values1[missing_values1['TOTAL']>0]).index\nnull_features","f407f34c":"for feature in null_features:\n        test[feature] = test[feature].fillna(test[feature].mode()[0])","99785b97":"# check again if there is any null values\ntest.isnull().sum().max()","e49507d4":"#Descriptive statistics summary\ntrain['SalePrice'].describe()","927c0abb":"train.boxplot('SalePrice')","c3e6d990":"sns.set()\nsns.pairplot(train[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']], size = 2.5)\nplt.show();","e0804c11":"# Getting the histogram and normal probability plot\nfrom scipy import stats\nfrom scipy.stats import norm\n\nplt.figure(figsize = (12,6))\nsns.distplot(train['SalePrice'], kde = True, hist=True, fit = norm);\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\n#plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt, fit=True, rvalue=True)","95c19abd":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","516f3ea0":"#applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])","31ef1204":"#transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt, rvalue=True)","bc740261":"#skewness and kurtosis after log transformation\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","fa46f282":"#correlation matrix\ncorrmat = train.corr()\nplt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);","086b1a48":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","e833e18e":"# most correlated features\n\ntop_corr = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\nsns.heatmap(train[top_corr].corr(), annot=True)\ntop_corr","6c1a3681":"Overall_SalePrice = train[['SalePrice', 'OverallQual']]\nplt.subplots(figsize = (10, 6))\nsns.boxplot(data = train, x = 'OverallQual', y = 'SalePrice')","60f27ba6":"# Differentiate numerical features (minus the target) and categorical features\n\ncategorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\n\n#Separating the dataset in categorical and numerical features\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","aa7f73f2":"train_cat.shape,train_num.shape","d5d17fa7":"# checkin skewness of all features\nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","a4f14970":"#Let's fix the features with skewness > 0.5\nskewness = skewness[abs(skewness)>0.5]\nskewness.index","9ea35d32":"#we can treat skewness of a feature with the help fof log transformation.so we'll apply the same here.\nskewness = np.log1p(skewness)\nskewness.sort_values(ascending=False)","ea7a9614":"train_cat.head()","95e44796":"# using get_dummies here it is used for data manipulation. It converts categorical data into dummy or indicator variables\ntrain_cat = pd.get_dummies(train_cat)\ntrain_cat.shape","14c6d188":"# concatenating train_num (numerical variable) and train_cat (categorical variable)\n\ntrain1 = pd.concat([train_cat,train_num],axis=1)\ntrain1.shape","70e2c587":"# differentiate between numerical and categorical varibles\ncategorical_features = test.select_dtypes(include = [\"object\"]).columns\nnumerical_features = test.select_dtypes(exclude = [\"object\"]).columns\n\ntest_num = test[numerical_features]\ntest_cat = test[categorical_features]","e26c979d":"test_num.shape,test_cat.shape","5ba3b407":"# finding skewness of all features\nskewness_test = test_num.apply(lambda x: skew(x))\nskewness_test.sort_values(ascending=False)","dabed1bb":"#we can treat skewness of a feature with the help fof log transformation.so we'll apply the same here.\nskewness = np.log1p(skewness)\nskewness.sort_values(ascending=False)","4ab2c080":"test_cat.head()","e63af81f":"# we are selecting features where skewness is greater than 0.5 to fix their skewness\n#skewness_test = skewness[abs(skewness)>0.5]\n#len(skewness_test.index)","74bf5e4a":"test_cat = pd.get_dummies(test_cat)\ntest_cat.head()","2d7cf6b0":"test1 = pd.concat([test_cat,test_num],axis=1)\ntest1.shape","29b03990":"#Let's see the outliers in the below scatterplot\nplt.scatter(train1.SalePrice,train1.GrLivArea,c = 'blue')\nplt.xlabel('GrLivArea', fontsize=14)\nplt.ylabel('SalePrice', fontsize=14)\n#plt.title('View Outliers')\nplt.show()","bccc63a1":"# set minimum and maximum threshold values to detect ouliers using standard deviation\nlower = train1['SalePrice'].mean() - 3*train1['SalePrice'].std()\nupper = train1['SalePrice'].mean() + 3*train1['SalePrice'].std()\nprint(\"Upper: \", upper)\nprint(\"Lower: \", lower)","de109dbd":"#To remove these outliers from datasets:\ntrain1 = train1[(train1['SalePrice'] > lower) & (train1['SalePrice'] < upper)]\ntrain1.shape","d9da0231":"# importing all the required library for modeling here we are going to use statsmodels \nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split","0e345a8b":"cols = [col for col in train1.columns if col not in test1.columns]\ncols.remove('SalePrice')\ntrain1 = train1.drop(cols,axis=1)","1621c20f":"# assining the required data to the respective variables  \nX = train1.drop(['SalePrice'],axis=1)\ny = train1['SalePrice']\n","65888d15":"# checking shapes\ntest1.shape,train1.shape","cdc43be1":"test1.columns","72903730":"train1.columns","6337bac5":"from xgboost import XGBRegressor\n\n# Split between train and test\ntrain_X, test_X, train_y, test_y = train_test_split(X,y,random_state=0, test_size=0.2)\n\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n\n#call the fit to the model\nmodel.fit(train_X, train_y, \n             early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], \n             verbose=False)\n\n#Make predictions from test dataset\npredictions = model.predict(test_X)","53362ffe":"from xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\nmodel_clas = XGBClassifier()\nmodel_clas.fit(train_X, train_y)\nfrom xgboost import plot_importance\nplot_importance(model, max_num_features=10) # top 10 most important features;","060b77a8":"from sklearn.metrics import mean_absolute_error\nprint(\"Root Mean Squared Error : \",math.sqrt(sum((test_y-predictions)**2)\/len(test_y)))\n\nprint(\"Mean Absolute Error (MAE): {:.2f}\".format(mean_absolute_error(predictions, test_y)))\n\ndef mean_absolute_percentage_error(test_y, predictions1): \n   test_y, predictions1 = np.array(test_y), np.array(predictions1)\n   return np.mean(np.abs((test_y - predictions) \/ test_y)) * 100\n\nprint(\"Mean Absolute Percentage Error (MAPE) (%): {:.2f}\".format(mean_absolute_percentage_error(test_y, predictions)))","b99f2968":"# parity plot \nplt.scatter(predictions1,test_y,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([11.0,13.2],[11.0,13.2],c='red')\nplt.show()\n\nfrom sklearn.metrics import r2_score\n\ncoefficient_of_dermination = r2_score(predictions, test_y)\ncoefficient_of_dermination","30ea367c":"predictions = model.predict(test1)\n","d7a8c679":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nfinal_submission = pd.DataFrame({'Id':submission['Id'],'SalePrice':predictions})\nfinal_submission.to_csv('submission1.csv', index=False)","11c60d47":"****Missing values****","854aa607":"Convert categorical variable into dummy\/indicator variables.","521d3179":"****Hello everyone, this is the first thing that I'm posting on the kaggle. I hope you like it and interact with suggestions.****","58eda20b":"****Skewness of test data****","14c069e6":"The above scatterplot comparing the actual values against predicted values in an easy understandable way.","8bea87a3":"**Skewness**: distribution asymmetry. \n\nCloser to 0, the more \u201cperfect\u201d is the asymmetry;\n\nValues \u200b\u200b> 0 there is a positive asymmetry;\n\nValues \u200b\u200b< 0 there is a negative asymmetry;\n\nIt is also possible to calculate the skewness with a pandas method called skew().","ccb82941":"Skewness can be a positive or negative number (or zero). Distributions that are symmetrical with respect to the mean, such as the normal distribution, have zero skewness. A distribution that \u201cleans\u201d to the right has negative skewness, and a distribution that \u201cleans\u201d to the left has positive skewness.\n\nAs a general guideline, skewness values that are within \u00b11 of the normal distribution\u2019s skewness indicate sufficient normality for the use of parametric tests.","c5543f5a":"Let's evaluate the importance of each feature for the model","8c71a53b":"Let's see if there are many outliers and how to handle this data","a0e6d4e0":"****First import libraries****","1ea1b17f":"****Outliers****","92311223":"I plotted a correlation matrix to identify a possible significant correlation between input variables, thus mapping the possibility of multicollinearity, as these variables need to be linearly independent:","9b9cc83e":"****Let's analyze the skewness of all features****","e146e6f2":"OverallQual is the variable with the highest correlation with the variable response (SalesPrice). In the chart above, we can see this relationship.","05cb4506":"\n****Let's analyze the number of \u200b\u200bcategorical and numerical variables****","3708175a":"**Dummy variables\u00b6**","ddff85f6":"For a really good fit, we must know that a **linear regression model** must obey some premises:\n\n\n**Normal distribution of residuals**: Since some goodness-of-fit tests assume that errors are normally distributed \u2014 such as the F test \u2014 if this assumption is not met, confidence interval calculations may not be reliable for the model's predictions.\n\n**Expectation of residuals is 0**: Intuitively, we expect the average of the errors to be 0, as we want the smallest possible error.\n\n**Residual independence**: There must be no self-correlation between the residuals, otherwise the distribution of the residual may be affected, in addition to being able to indicate underestimation\/overestimation.\n\n**Homoscedasticity**: For any value of X, the variance of the residual is the same, that is, the residuals must show constant variation. When this premise is not obeyed, we have the presence of heteroscedasticity, which makes it more difficult to determine the true standard deviation of the errors, casting doubt on the result of the confidence intervals","6da2c422":"Now, after transformation(preprocessing), let's go join them (test_cat + test_num) to get the whole test set back.","964428c5":"## Data exploration ","d91caf23":"The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.","7e22d86e":"The variable with more then 15% missing values will be deleted:\n\n* 'PoolQC'\n* 'MiscFeature'\n* 'Alley'\n* 'Fence'\n* 'FireplaceQu'\n* 'LotFrontage'\n\nSome features have the same number of missing values, so this refers to the same observation. Then, it will be considered only one variable for representing the feature. For example:\n\n'GarageCars' will represent the 'GarageType','GarageYrBlt', 'GarageFinish','GarageQual' and 'GarageCond'.\n\nFinally 'Electrical' have only one null value so we replace it with its mode.","4129776a":"![image.png](attachment:611de2ff-f374-4543-b4ac-d2912e887175.png)","6ab98d35":"****Scatter plots between 'SalePrice' and correlated variables****\n\nLook the Scatter plot between 'SalePrice' and its correlated Variables","b7a02bdf":"What do these graphics tell us?\n\n* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness.","fbdb6054":"One way to normalize the data is through logarithmic transformation.","0833cdab":"Firstly,an exploration of the data will be carried out following the understandings:\n\n\n**Understand the problem.** Look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n\n**Univariable study.** Focus on the dependent variable (SalePrice) and try to know a little bit more about it.\n\n**Multivariate study**. Try to understand how the dependent variable and independent variables relate.\n\n**Basic cleaning.** Clean the dataset and handle the missing data, outliers and categorical variables.\n\n**Test assumptions.** Check if our data meets the assumptions required by most multivariate techniques.\n","023f999c":"## Correlation Matrix (heatmap style)","df7525d8":"![image.png](attachment:85ad72d2-db94-49f8-ad0e-a0a8747c29bb.png)","fbf080b9":"In the figure above we can see the 10 most important features for the model.","70aeb260":"### Relationship with categorical features","2864f38b":"# Modeling","21a9596e":"We use kurtosis to quantify a phenomenon\u2019s tendency to produce values that are far from the mean. There are various ways to describe the information that kurtosis conveys about a data set: \u201ctailedness\u201d (note that the far-from-the-mean values are in the distribution\u2019s tails), \u201ctail magnitude\u201d or \u201ctail weight,\u201d and \u201cpeakedness\u201d (this last one is somewhat problematic, though, because kurtosis doesn\u2019t directly measure peakedness or flatness).\n\nThe normal distribution has a kurtosis value of 3. The following diagram gives a general idea of how kurtosis greater than or less than 3 corresponds to non-normal distribution shapes.\n\nMore informations about it, click [here](https:\/\/www.allaboutcircuits.com\/technical-articles\/understanding-the-normal-distribution-parametric-tests-skewness-and-kurtosis\/)","b8c25523":"## ****Analysing SalePrice****","069516d5":"![image.png](attachment:09912f17-7822-4643-ae60-91f4b8bb03ff.png)","993890e1":"For categorical variables and numerical variable we are just filling the Null values with most frequent(mode) from specified columns","3d79a11c":"## Clearing the data","5b1f4566":"**An Introduction to XGBoost**\n\n\nThe name XGBoost comes from eXtreme Gradient Boosting, and represents a category of algorithm based on Decision Trees  with Gradient Boosting.\n\nGradient increase means that the algorithm uses the Descent Gradient algorithm to minimize loss while new models are added.\n\nExtremely flexible - since it has a large number of hyperparameters that can be improved -, you can adjust XGBoost determinedly to the scenario of your problem, whatever it may be.\n\nDecision Trees and Gradient Boosting\nDecision trees are methods where there is a function that takes a vector of values \u200b\u200b(of attributes) as input and returns a decision (output).\n\nFor a decision tree to arrive at the output value, it performs a series of steps, or tests, creating various branches throughout the process.\n\nEach node in this tree represents a single decision. The more times an attribute is used for decision retrieval, the greater its relative importance in the model.\n\nGradient Boosting\nGradient Boosting, relatively recent technique, has proven to be very powerful.\n\nIts potential is such that algorithms based on this technique have been gaining more prominence in Data Science projects and Kaggle competitions.\n\nThe principle of Gradient Boosting is the ability to combine results from many \"weak\" classifiers, present decision trees, which combine to form something like a \"strong decision committee\".","12092abb":"The relationship seems to be stronger between 'OverallQual' and SalePrice. The box plot shows how sales prices increase with the overall quality."}}