{"cell_type":{"a95bc960":"code","e5e8a117":"code","471ccbd4":"code","58d6a69e":"code","17b93641":"code","224f6d71":"code","a66a41fe":"code","6ab971bc":"code","05937638":"code","9359426c":"code","99dcbc72":"code","3c954856":"code","cc0f8ea0":"code","7e35fd8d":"code","79962184":"code","6cf20bf1":"code","8d3be63b":"code","1e4687d0":"code","f117be8a":"code","129019a9":"code","20d83ad3":"code","974e9313":"code","cc635b59":"code","7b19e72c":"code","4c8bbbcd":"code","6c7d9cb3":"code","8ac32114":"code","eedc40a2":"code","9840c6ce":"code","ad848d90":"code","a754af4e":"code","a339a4bb":"code","87fa809e":"code","83ab2d4c":"code","509bbddb":"code","0f206dfa":"code","0a437b66":"code","82c8ecae":"code","a7d97d8e":"code","c8eb5c82":"code","f9d1285b":"markdown","14ebebc8":"markdown","1043af51":"markdown","d7354433":"markdown","73c0542a":"markdown","7ad86888":"markdown","eb1944d9":"markdown","2dd824b6":"markdown","175913b8":"markdown","114ac5d9":"markdown","714f2af0":"markdown","2e4a8d98":"markdown","8e458fa9":"markdown","76beee42":"markdown","7d6d94a9":"markdown","60d7ae48":"markdown","75ec3c70":"markdown","29d3dcb3":"markdown","8757ef88":"markdown","84144bce":"markdown","71844a53":"markdown","fb388f4a":"markdown","781eafb4":"markdown","24fe5011":"markdown","fe8c8896":"markdown","7f5f2f3a":"markdown","842c6fa2":"markdown","6332d442":"markdown"},"source":{"a95bc960":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e5e8a117":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","471ccbd4":"df = pd.read_csv(\"\/kaggle\/input\/vishwatatva-datacon-2020\/train.csv\")\ndf.head()","58d6a69e":"df.describe()","17b93641":"df.isnull().sum()","224f6d71":"def replace_nan():\n    for i in range(1, 22):\n        try:\n            mean = df.iloc[:, i].mean(skipna=True)\n            df.iloc[:, i].fillna(mean, inplace = True)\n        except TypeError:\n            pass\n        \nreplace_nan()\ndf.dropna(inplace=True)","a66a41fe":"df.isnull().sum()","6ab971bc":"df.dtypes","05937638":"df = df[df['total_processing_time'] != '#NAME?']\ndf = df[(df['coding_standard_output'] != '0')]","9359426c":"names = ['video_duration', 'coding_standard', 'width', 'height', 'bitrate',\n       'framerate', 'i_frames', 'p_frames', 'b_frames', 'frames', 'i_size',\n       'p_size', 'b_size', 'size', 'coding_standard_output', 'bitrate_output',\n       'framerate_output', 'output_width', 'output_height',\n       'allocated _memory', 'total_processing_time']","99dcbc72":"for i in names:\n    df = df[df[i] != '#NAME?']\n    df = df[df[i] != '-']","3c954856":"for i in names:\n    try:\n        df[i] = pd.to_numeric(df[i], downcast = \"float\")\n    except:\n        continue","cc0f8ea0":"df.dtypes","7e35fd8d":"fig = plt.figure(figsize = (10, 6))\nimport random\ncolors = ['red', 'green', 'blue', 'yellow']\nfor i in list(df.columns)[1:-1]:\n    try:\n        c = random.randint(0,3)\n        plt.scatter(x = list(df['total_processing_time']),y = list(df[i]), color = colors[c])\n        plt.ylabel(i)\n        plt.xlabel(\"Total Processing Time\")\n        plt.show()\n    except:\n        print(\"Plot not Possible\")","79962184":"fig = plt.figure(figsize = (20, 10))\nsns.heatmap(df.corr(), annot = True)","6cf20bf1":"col_name = list(df.columns)\nfig = plt.figure(figsize = (20, 5))\nfor i in range(1, 22):\n    try:\n        print(col_name[i])\n        sns.boxplot(data=df.iloc[:, i], orient=\"h\")\n        plt.show()\n    except:\n        print(\"Categorical Data\")\n       ","8d3be63b":"df = df[(df['video_duration']<3000)&(df['width']<100000)\n       &(df['b_size']>-10)&(df['frames']<25000)&(df['p_frames']<60000)&(df['i_frames']<10**8)&(df['b_frames']<50)\n       &(df['framerate']<100000)&(df['bitrate']<3.5*10**6)&(df['b_frames']<2000)&(df['b_frames']>-10)\n       &(df['framerate_output']<40000)&(df['bitrate_output']<0.2*10**8)\n       &(df['output_height']< 50000)&(df['p_size']<10**8)&(df['i_size']<5*10**9)]","1e4687d0":"col_name = list(df.columns)\nfig = plt.figure(figsize = (20, 5))\nfor i in range(1, 22):\n    try:\n        print(col_name[i])\n        sns.boxplot(data=df.iloc[:, i], orient=\"h\")\n        plt.show()\n    except:\n        print(\"Categorical Data\")\n       ","f117be8a":"df","129019a9":"df_1 = pd.get_dummies(df[\"coding_standard\"])\ndf_1.head()","20d83ad3":"df_2 =  pd.get_dummies(df[\"coding_standard_output\"])\ndf_2.rename({\"flv\":\"flv_o\", \"h264\":\"h264_o\", \"mpeg4\":\"mpeg4_o\", \"vp8\":\"vp8_o\"}, axis=\"columns\",inplace=True)\ndf_2.head()","974e9313":"df.drop(columns=['id', 'coding_standard', 'coding_standard_output'], inplace=True)\nfinal_df = pd.concat([df, df_1],axis = 1)\nfinal_df = pd.concat([final_df, df_2],axis = 1)\nfinal_df.head()","cc635b59":"x = list(final_df[\"total_processing_time\"])\nfinal_df.drop(columns=\"total_processing_time\", inplace = True)\nfinal_df['total_processing_time'] = x\nfinal_df.head()","7b19e72c":"list(final_df.columns)[0:26]","4c8bbbcd":"final_df.drop(columns=['allocated _memory', 'size'], inplace = True)","6c7d9cb3":"from sklearn.model_selection import train_test_split\n\nX = final_df.iloc[:, 0:24].values\nY = final_df.iloc[:, -1].values\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1)","8ac32114":"from sklearn.ensemble import RandomForestRegressor\n\nreg_1 = RandomForestRegressor(n_estimators=100)\nreg_1.fit(x_train, y_train)\ns_1 = reg_1.score(x_test, y_test)","eedc40a2":"from sklearn.ensemble import GradientBoostingRegressor\n\nreg_2 = GradientBoostingRegressor(loss='ls', learning_rate=0.50, n_estimators=128, max_depth = 11)\nreg_2.fit(x_train, y_train)\ns_2 = reg_2.score(x_test, y_test)","9840c6ce":"from sklearn.ensemble import BaggingRegressor\n\nreg_3 = BaggingRegressor(n_estimators=128)\nreg_3.fit(x_train, y_train)\ns_3 = reg_3.score(x_test, y_test)","ad848d90":"x_train = final_df.iloc[:, 0:24].values\ny_train = final_df.iloc[:, -1].values\n\n\nreg_1.fit(x_train, y_train)\nreg_2.fit(x_train, y_train)\nreg_3.fit(x_train, y_train)","a754af4e":"test_df = pd.read_csv('\/kaggle\/input\/vishwatatva-datacon-2020\/test.csv')\ntest_df.head()","a339a4bb":"df_test_1 = pd.get_dummies(test_df[\"coding_standard\"])\ndf_test_1.head()","87fa809e":"df_test_2 = pd.get_dummies(test_df[\"coding_standard_output\"])\ndf_test_2.rename({\"flv\":\"flv_o\", \"h264\":\"h264_o\", \"mpeg4\":\"mpeg4_o\", \"vp8\":\"vp8_o\"}, axis=\"columns\",inplace=True)\ndf_test_2.head()","83ab2d4c":"ids = list(test_df['id'])\ntest_df.drop(columns=['id', 'coding_standard', 'coding_standard_output'], inplace=True)\ntest_df = pd.concat([test_df, df_test_1],axis = 1)\ntest_df = pd.concat([test_df, df_test_2],axis = 1)\ntest_df.head()","509bbddb":"test_df.isnull().sum()","0f206dfa":"def replace_nan():\n    for i in range(0, 18):\n        try:\n            mean = test_df.iloc[:, i].mean(skipna=True)\n            test_df.iloc[:, i].fillna(mean, inplace = True)\n        except TypeError:\n            pass\nreplace_nan()","0a437b66":"test_df.isnull().sum()","82c8ecae":"test_df.drop(columns=['allocated _memory', 'size'], inplace=True)","a7d97d8e":"x = test_df.iloc[:, 0:24].values\nprediction_1 = np.array(reg_1.predict(x))\nprediction_2 = np.array(reg_2.predict(x))\nprediction_3 = np.array(reg_3.predict(x))\n# Formula\nprediction = (prediction_1*s_1 + prediction_2*s_2 + prediction_3*s_3)\/(s_1+s_2+s_3)","c8eb5c82":"sub = pd.DataFrame({'id':ids, 'total_processing_time':prediction})\nsub.to_csv(\"Submission.csv\", index=False)","f9d1285b":"## Heat Map:\n**Shows us the correlation bettween the different data categories and the target variable**","14ebebc8":"### Scatter Plot:\n\n**We can get to know about the relation of data with our target variable, lets plot some graphs**","1043af51":"## The Brown Coders\n\n### Yashowardhan Shinde || Aryan Kenchappagol\n\n### Problem Statement: Given a train dataset predict the total_processing_time of the test dataset.\n\n## Importing Necessary Libraries","d7354433":"**Now we have successfully removed all the abnormalities in the data set now we can do our further analysis before that we will convert all the columns to float data type.**","73c0542a":"**It is clearly visible that our data set has many extreme values which are not good for our prdiction we will try and remove all such values and refine our data set we will initially start with removing the top 10 Percentile(%) of the data in columns with maximum outliers and then modify the data according to the performance of our model. This will make sure that the model makes the best prediction possible.**","7ad86888":"### Converting Categorical Data into Numerical Data:\n**We will now do One Hot Encoding to the categorical data columns. To accomplish this task we have used the get_dummies method in the Pandas Library this will create a new data frame with encoded values which we will concat to the main data frame** ","eb1944d9":"**We will be dropping the allocated memory and size columns as those columsn have high irregularities and have poor co relation with the target variable**","2dd824b6":"## Data Preparation:\n**For this task we will use the train test split method from sklearn wich will segregate our data into training data and evaluation data we will keep the evaluation size 0.1 as we just want to check our model accuracy so that we can make any changes if needed**","175913b8":"**Now after removing all the nan values we will check the data types of all the columns as we need numerical values for machine learning algorithmsn we will convert the categorical data into numerical data.**","114ac5d9":"## Data Cleaning and Analysis:\n**Analysing the training Dataset checking the important vales for every column like mean, standard deviation of data and interquartile range also checking if there are any nan values.**","714f2af0":"**we can clearly see that there are some Nan Values Present in the data set so as the number of Nan values is less we can fill those values with the mean of their particular column.**\n\n**REPLACING NAN VALUES WITH MEAN VALUES**","2e4a8d98":"## Visual Analysis :\n \n1. Scatter Plots\n2. Heat Map\n3. Box Plot","8e458fa9":"**As you can see above all the columns now have Numerical data type except for coding_standard_output and coding_standard as these columns consist of categorical data.**","76beee42":"**It is visilble that we are getting an accuracy of 98% or 99% across all the models which is a great accuracy and now very little can be done to improve it anymore so we will settle at this score and make predictions for our model using this regression model.**\n\n**Now we will train our model using the complete train data set.**","7d6d94a9":"## Weighted Average of all three Models:\n\n1. Random Forest\n2. Gradient Boost\n3. Bagging Regressor","60d7ae48":"#### It can be seen that many of the columns are still pandas objets so this means that there are som abnormal values in these columns so we will fing those values and drop them or replace them with the mean of that particular column.\n\n### It is observed that the different str values in the columns are :\n1. \"0\"\n2. \"#NAME?\"\n3. \"-\"\n\n**Now we will drop the columns with these values**","75ec3c70":"## Loading Train Dataset","29d3dcb3":"**Removing the column with categorical data and appending the Encoded data to the Final Data Frame**","8757ef88":"### Result Evaluation:\n**As we can clearly see most of the data is now in our desired range so we will move forward and do some more analsis before training our model.[](http:\/\/)**","84144bce":"## Model Selection:\n\n**As our Target Variable has continuous values we will make use of a Regression Model for this task we will be using the RandomForestRegression Model, Gradient Boost Regressor and Bagging Regressor from Sklearn.ensemble. We tried using other models like a LinearRegression model but that was giving a lower accuracry. We will be using the concept of weighted average for the final prediction.**","71844a53":"##### **Task Has been Successfully completed. with score ranging in category of 95-100%**","fb388f4a":"**Finally Predicting the values and creating a file called submission.csv**","781eafb4":"**After a lot of trial and error we finally settled at this particular filteration as thsi filteration was giving usthe best possible result for every column considered the top 10 percentile(%) data was removed. The Target variable is left untouched.**","24fe5011":"**Checking for all the null values and confirming if there are any more null values.**","fe8c8896":"**For the second encoding we will have to rename the columns as we dont want two cols with same name in our data set.**","7f5f2f3a":"**Dropping the unecessary columns like the allocated memory and size.**","842c6fa2":"## Loading Test Data Set for making Prediction:\n\n**We will check all the null values and changethem to mean values this will be followed by one hot encoding the categorical data and then predicting the values for our task**","6332d442":"## Box Plot:\n**Very Important as box plot will help us deal with the outliers in the data set and will help improve accuracyof our selected model.**"}}