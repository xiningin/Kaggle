{"cell_type":{"107f74e9":"code","d27390da":"code","29f90b9f":"code","051e1818":"code","a048fd7d":"code","ec4edd8b":"code","e419131e":"code","ec7edc8f":"code","68515248":"code","4ae8e2d3":"code","f7a7f0a0":"markdown","677524bf":"markdown","92c8dae7":"markdown","42376049":"markdown","d0a7fe45":"markdown","d4972615":"markdown","41ee493c":"markdown","06f56d4e":"markdown","25c9fcd7":"markdown","2de469ed":"markdown"},"source":{"107f74e9":"!pip install torch\n!pip install tokenizers\n!pip install transformers","d27390da":"# Train a tokenizer\nimport tokenizers\n \nbwpt = tokenizers.BertWordPieceTokenizer(vocab_file=None)\n \nfilepath = \"input file directory\"\n\nbwpt.train(\n    files=[filepath],\n    vocab_size=50000,\n    min_frequency=3,\n    limit_alphabet=1000\n)\n\nbwpt.save('\/kaggle\/working\/', 'name')","29f90b9f":"# Load the tokenizer\nfrom transformers import BertTokenizer, LineByLineTextDataset\n\nvocab_file_dir = '\/kaggle\/input\/bert-bangla\/bangla-vocab.txt' \n\ntokenizer = BertTokenizer.from_pretrained(vocab_file_dir)\n\nsentence = '\u09b6\u09c7\u09b7 \u09a6\u09bf\u0995\u09c7 \u09b8\u09c7\u09a8\u09be\u09ac\u09be\u09b9\u09bf\u09a8\u09c0\u09b0 \u09b8\u09a6\u09b8\u09cd\u09af\u09b0\u09be \u098f\u09b8\u09ac \u0998\u09b0 \u09a4\u09be\u0981\u09b0 \u09aa\u09cd\u09b0\u09b6\u09be\u09b8\u09a8\u09c7\u09b0 \u0995\u09be\u099b\u09c7 \u09b9\u09b8\u09cd\u09a4\u09be\u09a8\u09cd\u09a4\u09b0 \u0995\u09b0\u09c7\u09a8'\n\nencoded_input = tokenizer.tokenize(sentence)\nprint(encoded_input)\n# print(encoded_input['input_ids'])","051e1818":"%%time\n\n'''\ntransformers has a predefined class LineByLineTextDataset()\nwhich reads your text line by line and converts them to tokens\n'''\n\ndataset= LineByLineTextDataset(\n    tokenizer = tokenizer,\n    file_path = '\/kaggle\/input\/bert-bangla\/raw_bangla_for_BERT.txt',\n    block_size = 128  # maximum sequence length\n)\n\nprint('No. of lines: ', len(dataset)) # No of lines in your datset","a048fd7d":"from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n\nconfig = BertConfig(\n    vocab_size=50000,\n    hidden_size=768, \n    num_hidden_layers=6, \n    num_attention_heads=12,\n    max_position_embeddings=512\n)\n \nmodel = BertForMaskedLM(config)\nprint('No of parameters: ', model.num_parameters())\n\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)","ec4edd8b":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='\/kaggle\/working\/',\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n    prediction_loss_only=True,\n)","e419131e":"%%time\ntrainer.train()\ntrainer.save_model('\/kaggle\/working\/')","ec7edc8f":"from transformers import pipeline\n\nmodel = BertForMaskedLM.from_pretrained('\/kaggle\/working\/')\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=model,\n    tokenizer=tokenizer\n)","68515248":"fill_mask('\u09b2\u09be\u09b6 \u0989\u09a6\u09cd\u09a7\u09be\u09b0 \u0995\u09b0\u09c7 \u09ae\u09af\u09bc\u09a8\u09be\u09a4\u09a6\u09a8\u09cd\u09a4\u09c7\u09b0 \u099c\u09a8\u09cd\u09af \u0995\u0995\u09cd\u09b8\u09ac\u09be\u099c\u09be\u09b0 [MASK] \u09ae\u09b0\u09cd\u0997\u09c7 \u09aa\u09be\u09a0\u09bf\u09af\u09bc\u09c7\u099b\u09c7 \u09aa\u09c1\u09b2\u09bf\u09b6')","4ae8e2d3":"fill_mask('\u09e7\u09ef\u09ed\u09e7 \u09b8\u09be\u09b2\u09c7 \u09ac\u09be\u0982\u09b2\u09be\u09a6\u09c7\u09b6 \u09ef \u09ae\u09be\u09b8 \u09ae\u09c1\u0995\u09cd\u09a4\u09bf\u09af\u09c1\u09a6\u09cd\u09a7 \u0995\u09b0\u09c7 [MASK] \u0985\u09b0\u09cd\u099c\u09a8 \u0995\u09b0\u09c7')","f7a7f0a0":"# **Train the model**\nWe are at the last step of our language model pretraining.\nCall the trainer's `train()` method and sit back and watch a movie cause this is going to take a lot of time depending on your corpus size.\n\nRemember **Google's BERT-base** was trained on **4 cloud TPUs for 4 uninterrupted days**. That is equivalent to **16 GPU days**!\n\nI trained a model on a random newspaper article corpus of only 500MB containing around 2.2M sentences and 30M words and that took almost 4 hrs!\n\nDon't forget to save the model! Cause you know, if you fall asleep (I am certain you will) and wake up and see runtime disconnected! RIP!","677524bf":"# **Dataset**\nYou can use your own text corpus or you can download one from [OSCAR](https:\/\/oscar-corpus.com\/), these are huge multilingual corpora obtained by language classification and filtering of Common Crawl dumps of the Web.\n\nOne thing to keep in mind, you will get better results by pretraining your data on more and more data.\n\nIf you are using your own corpus, make sure that your text corpus is **one sentence-per-line** like this:\n\n```\nMr. Cassius crossed the highway, and stopped suddenly.\nSomething glittered in the nearest red pool before him.\nGold, surely!\nBut, wonderful to relate, not an irregular, shapeless fragment of crude ore, fresh from Nature's crucible.\nLooking at it more attentively, he saw that it bore the inscription, \"May to Cass.\"\nLike most of his fellow gold-seekers, Cass was superstitious.\n```\n\nI am using a text corpus (almost 500MB containing around 2.2M sentences and 30M words) of random newspaper articles in Bangla (A less resourceful language), cause we all know there are several pretrained English BERT language models released by Google ([BERT github repo](https:\/\/https:\/\/github.com\/google-research\/bert)) and these models provide SOA performance for various nlp tasks.\n\nBut you can follow this notebook for pretraining a model in any language of your choice.","92c8dae7":"# **Check your model's prediction**\nLoad your pretained model from the saved model directory and a make a pipeline for masked word prediction task. \n","42376049":"# **Defining model**\nNow that have the training data ready to be fed into the model, let's define the model.\nFirst we have to define the configuration of the BERT model.\n`vocab_size` should be the size of your trained vocabulary. Keep the rest of the arguments as they are. I am expecting that you have a thorough knowledge on the transformers model to understand the parameters \n\nWe will be using `BertForMaskedLM` from transformers library which is built on top of masked language modelling(MLM) excluding the next sentence prediction(NSP) task. \n\nYou also need to define a `DataCollator`. What is `DataCollator` you ask?\n\nA `DataCollator` is a function that takes a list of samples from a Dataset\nand collate them into a batch, as a dictionary of Tensors.\n\n* collates batches of tensors, honoring their tokenizer's pad_token\n* preprocesses batches for masked language modeling","d0a7fe45":"# **Defining training arguments**\n`per_device_train_batch_size` is theoretically not the same as the batch size for BERT model. This is true when you have more than 1 GPU\/TPU.\n\nBut as of now in practicality, assuming that you are training the model on 1 GPU(In colab\/your pc) `per_device_train_batch_size` is the bach size for your BERT model, which is I have set 32 (**recommended batch size for BERT in the paper =16 or 32**). \n\nThen instantiate a trainer with the predefined model, tokenizer, datacollator and dataset.\n","d4972615":"# **Conclusion**\nMy model did a fairly decent job! As I said earlier, BERT needs a ton of text to understand a language better. Google's BERT-base was trained on TeraBytes of raw text data containing around 3.3B words (around 110x of what we trained on).\n\nI trained my model on random newspaper articles. Its better to train your BERT model on a domain specific text for your task. You will definitely get a better result in that domain.\n\n\nSo, Congratulations! \nYou can now train your own BERT model in any language.\n\nNow there might be a question arising in your mind.\n\n***Can I train a model by using the weights from a pretrained model?***\n\nYes you can. Notice in the model defining section I defined the model in this manner, \n\n`model = BertForMaskedLM(config)`\n\nHere the `BertConfig` is passed as the argument, instead of this what you have to do is `model = BertForMaskedLM.from_pretained('bert-base-cased')` \n\nor if you want to load the model from local directory\n`model = BertForMaskedLM.from_pretained('your_model_directory')`\n\nThank you!\n\n","41ee493c":"# **How to pretrain a language model (like BERT) with your own corpus from scratch**\n\nIn 2017 google published a paper titled as '[Attention is all you need](https:\/\/arxiv.org\/abs\/1706.03762)', where they proposed a new type of encoder-decoder architecture that outdid all the previous sequence-2-sequence models in various natural language processing(NLP) tasks such as machine translation, question answering, named entity recognition(NER) etc. \n\nIn 2019 google introduced BERT- **Bidirectional Encoder Representations from Transformers** ([paper](https:\/\/arxiv.org\/abs\/1810.04805)),  which is designed to pre-train a language model from a vast corpus of rew text. What distinguishes it from existing word-embedding models like **Word2vec**, **ELMo** etc. is that it is a **truly bidirectional model**, meaning it is trained on unlabeled text by jointly conditioning **both left and right context simultaneously**. \n\n(Transformers? BERT? What is up with the architecture names google? what's next? Avengers?)\n\nI will put down some links for you to get a clear concept on transformers if you haven't already :\n* The paper [Attention is all you need](https:\/\/arxiv.org\/abs\/1706.03762)\n* Jay Alammar's post [The Illustrated Transformer](http:\/\/jalammar.github.io\/illustrated-transformer\/)\n* [Transformer explained](https:\/\/youtu.be\/z1xs9jdZnuY?list=WL) by Minsuk heo\n* [BERT research series](https:\/\/www.youtube.com\/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6) by ChrisMcCormick\n\n\nIn this notebook, we will be using [tokenizers](https:\/\/github.com\/huggingface\/tokenizers) and [transformers](https:\/\/huggingface.co\/transformers\/) library.\n\nPre-requisites : `torch`, `tensorflow`","06f56d4e":"I will translate the sentence and the predictions for you to understand the results.\n\n`fill_mask('In 1971, Bangladesh gained [MASK] by fighting for 9 months')`\nThe answer should be ***independece***.\n\nThe model's prediction were (sorted by probability score):\n`prize, independence, that, country, war `","25c9fcd7":"# **Tokenization**\n\nWe will have to train our own tokenizer and build a vocabulary for our corpus. \nWe will be choosing `BertWordPieceTokenizer` from `tokenizers` library. Arbitrarily choose a `vocab_size=50,000`. The model will be saved to the output directory as `'name-vocab.txt'` file. \n\nI had a pretrained tokenizer for Bangla, so I am using that.","2de469ed":"I will translate the sentence and the predictions for you to understand the results.\n\n`fill_mask('The body was recovered and sent to Cox's Bazar [MASK] morgue for autopsy')`\nThe answer should be ***hospital***.\n\nThe model's prediction were (sorted by probability score):\n`city, hospital, hospital's, medical, college `"}}