{"cell_type":{"9ff393ad":"code","860cad79":"code","a030ef7b":"code","02002304":"code","04427558":"code","e2e94456":"code","eb076316":"code","017233a9":"code","b67ee080":"code","508256ca":"code","a7e9e73e":"code","0c0a09e7":"code","ffa37b10":"code","f1870e8c":"code","ed13559a":"code","53c064ea":"code","bf9880b9":"code","54700f59":"code","6e87bda9":"code","70c2e8ab":"code","1c87fa7a":"code","8199ca9e":"code","66b916ec":"code","b4db761f":"code","022f92a1":"code","1bf3a56c":"code","2e486995":"code","3dffc6e1":"code","135d5771":"code","8405f040":"code","2df635f7":"code","06c68bc6":"code","edebb26d":"code","3acf7274":"code","333d29aa":"code","c30a8d6d":"code","fd3d4ca7":"code","0158966a":"code","eca23c9e":"code","132ab2a5":"code","cc067ac0":"code","e5998f07":"code","ebd3c2bf":"code","573ad35e":"code","bd360e9f":"code","d07f67f8":"markdown","fbaf2fc3":"markdown","766d3d08":"markdown","f533a13f":"markdown","1479ccaf":"markdown","f9576963":"markdown","4d263879":"markdown","93d07541":"markdown","ea2531ee":"markdown","2650c0a3":"markdown","faea2a5e":"markdown","3ad6a35c":"markdown","faeff917":"markdown","4980ef64":"markdown","7d04e5ef":"markdown","a2c0d4ba":"markdown"},"source":{"9ff393ad":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\n\nos.listdir(\"..\/input\/chest-xray-pneumonia\/chest_xray\")","860cad79":"len(os.listdir(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\"))","a030ef7b":"train_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\"\ntest_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\"\nval_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/val\"\n\nprint(\"Train set:\\n========================================\")\nnum_pneumonia = len(os.listdir(os.path.join(train_dir, 'PNEUMONIA')))\nnum_normal = len(os.listdir(os.path.join(train_dir, 'NORMAL')))\nprint(f\"PNEUMONIA={num_pneumonia}\")\nprint(f\"NORMAL={num_normal}\")\n\nprint(\"Test set:\\n========================================\")\nprint(f\"PNEUMONIA={len(os.listdir(os.path.join(test_dir, 'PNEUMONIA')))}\")\nprint(f\"NORMAL={len(os.listdir(os.path.join(test_dir, 'NORMAL')))}\")\n\nprint(\"Validation set:\\n========================================\")\nprint(f\"PNEUMONIA={len(os.listdir(os.path.join(val_dir, 'PNEUMONIA')))}\")\nprint(f\"NORMAL={len(os.listdir(os.path.join(val_dir, 'NORMAL')))}\")\n\npneumonia = os.listdir(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\")\npneumonia_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\"\n\nplt.figure(figsize=(20, 10))\n\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(pneumonia_dir, pneumonia[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \nplt.tight_layout()","02002304":"normal = os.listdir(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\")\nnormal_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\"\n\nplt.figure(figsize=(20, 10))\n\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(normal_dir, normal[i]))\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    \nplt.tight_layout()","04427558":"normal_img = os.listdir(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\")[0]\nnormal_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\"\nsample_img = plt.imread(os.path.join(normal_dir, normal_img))\nplt.imshow(sample_img, cmap='gray')\nplt.colorbar()\nplt.title('Raw Chest X Ray Image')\n\nprint(f\"The dimensions of the image are {sample_img.shape[0]} pixels width and {sample_img.shape[1]} pixels height, one single color channel.\")\nprint(f\"The maximum pixel value is {sample_img.max():.4f} and the minimum is {sample_img.min():.4f}\")\nprint(f\"The mean value of the pixels is {sample_img.mean():.4f} and the standard deviation is {sample_img.std():.4f}\")","e2e94456":"sns.distplot(sample_img.ravel(),\n            label=f\"Pixel Mean {np.mean(sample_img):.4f} & Standard Deviation {np.std(sample_img):.4f}\", kde=False)\nplt.legend(loc='upper center')\nplt.title('Distribution of Pixel Intensities in the Image')\nplt.xlabel('Pixel Intensity')\nplt.ylabel('# Pixels in Image')","eb076316":"from keras.preprocessing.image import ImageDataGenerator\n\nimage_generator = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    samplewise_center=True,\n    samplewise_std_normalization=True\n)","017233a9":"train = image_generator.flow_from_directory(train_dir, \n                                            batch_size=8, \n                                            shuffle=True, \n                                            class_mode='binary',\n                                            target_size=(180, 180))\n\nvalidation = image_generator.flow_from_directory(val_dir, \n                                                batch_size=1, \n                                                shuffle=False, \n                                                class_mode='binary',\n                                                target_size=(180, 180))\n\ntest = image_generator.flow_from_directory(test_dir, \n                                            batch_size=1, \n                                            shuffle=False, \n                                            class_mode='binary',\n                                            target_size=(180, 180))","b67ee080":"sns.set_style('white')\ngenerated_image, label = train.__getitem__(0)\nplt.imshow(generated_image[0], cmap='gray')\nplt.colorbar()\nplt.title('Raw Chest X Ray Image')\n\nprint(f\"The dimensions of the image are {generated_image.shape[1]} pixels width and {generated_image.shape[2]} pixels height, one single color channel.\")\nprint(f\"The maximum pixel value is {generated_image.max():.4f} and the minimum is {generated_image.min():.4f}\")\nprint(f\"The mean value of the pixels is {generated_image.mean():.4f} and the standard deviation is {generated_image.std():.4f}\")","508256ca":"sns.distplot(generated_image.ravel(),\n            label=f\"Pixel Mean {np.mean(generated_image):.4f} & Standard Deviation {np.std(generated_image):.4f}\", kde=False)\nplt.legend(loc='upper center')\nplt.title('Distribution of Pixel Intensities in the Image')\nplt.xlabel('Pixel Intensity')\nplt.ylabel('# Pixels in Image')","a7e9e73e":"# Class weights\n\nweight_for_0 = num_pneumonia \/ (num_normal + num_pneumonia)\nweight_for_1 = num_normal \/ (num_normal + num_pneumonia)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint(f\"Weight for class 0: {weight_for_0:.2f}\")\nprint(f\"Weight for class 1: {weight_for_1:.2f}\")","0c0a09e7":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(180, 180, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(180, 180, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])","ffa37b10":"model.summary()","f1870e8c":"r = model.fit(\n    train, \n    epochs=10,\n    validation_data=validation, \n    class_weight=class_weight,\n    steps_per_epoch=100,\n    validation_steps=25,\n)","ed13559a":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='Val_Loss')\nplt.legend()\nplt.title('Loss Evolution')\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['accuracy'], label='Accuracy')\nplt.plot(r.history['val_accuracy'], label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution')","53c064ea":"evaluation = model.evaluate(test)\nprint(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n\nevaluation = model.evaluate(train)\nprint(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")","bf9880b9":"from sklearn.metrics import confusion_matrix, classification_report\n\npred = model.predict(test)\n\nprint(confusion_matrix(test.classes, pred > 0.5))\npd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))","54700f59":"print(confusion_matrix(test.classes, pred > 0.7))\npd.DataFrame(classification_report(test.classes, pred > 0.7, output_dict=True))","6e87bda9":"from keras.applications.densenet import DenseNet121\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\n\nbase_model = DenseNet121(input_shape=(180, 180, 3), include_top=False, weights='imagenet', pooling='avg')\n\nbase_model.summary()","70c2e8ab":"layers = base_model.layers\nprint(f\"The model has {len(layers)} layers\")","1c87fa7a":"print(f\"The input shape {base_model.input}\")\nprint(f\"The output shape {base_model.output}\")","8199ca9e":"\n#model = Sequential()\nbase_model = DenseNet121(include_top=False, weights='imagenet')\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x)\n\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n#model.add(base_model)\n#model.add(GlobalAveragePooling2D())\n#model.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])","66b916ec":"r = model.fit(\n    train, \n    epochs=10,\n    validation_data=validation,\n    class_weight=class_weight,\n    steps_per_epoch=100,\n    validation_steps=25,\n)","b4db761f":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='Val_Loss')\nplt.legend()\nplt.title('Loss Evolution')\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['accuracy'], label='Accuracy')\nplt.plot(r.history['val_accuracy'], label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution')","022f92a1":"evaluation = model.evaluate(test)\nprint(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n\nevaluation = model.evaluate(train)\nprint(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")","1bf3a56c":"predicted_vals = model.predict(test, steps=len(test))","2e486995":"print(confusion_matrix(test.classes, predicted_vals > 0.5))\npd.DataFrame(classification_report(test.classes, predicted_vals > 0.5, output_dict=True))","3dffc6e1":"from keras.models import Sequential\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.applications import VGG16\n\n\nvgg16_base_model = VGG16(input_shape=(180,180,3),include_top=False,weights='imagenet')\n\n    \n","135d5771":"vgg16_base_model.summary()","8405f040":"\n\n    vgg16_model = tf.keras.Sequential([\n        vgg16_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])","2df635f7":"    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    vgg16_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)","06c68bc6":"r = vgg16_model.fit(train,\n          epochs=10,\n          validation_data=validation,\n          class_weight=class_weight,\n          steps_per_epoch=100,\n          validation_steps=25)\n\n","edebb26d":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='Val_Loss')\nplt.legend()\nplt.title('Loss Evolution')\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['accuracy'], label='Accuracy')\nplt.plot(r.history['val_accuracy'], label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution')","3acf7274":"evaluation =vgg16_model.evaluate(test)\nprint(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n\nevaluation = vgg16_model.evaluate(train)\nprint(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")","333d29aa":"from keras.applications import ResNet50\n\nresnet_base_model = ResNet50(input_shape=(180,180,3), include_top=False, weights='imagenet')\n\n","c30a8d6d":"resnet_base_model.summary()","fd3d4ca7":"    resnet_model = tf.keras.Sequential([\n        resnet_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    resnet_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)","0158966a":"r = resnet_model.fit(train,\n          epochs=10,\n          validation_data=validation,\n          class_weight=class_weight,\n          steps_per_epoch=100,\n          validation_steps=25)\n","eca23c9e":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='Val_Loss')\nplt.legend()\nplt.title('Loss Evolution')\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['accuracy'], label='Accuracy')\nplt.plot(r.history['val_accuracy'], label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution')","132ab2a5":"evaluation =resnet_model.evaluate(test)\nprint(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n\nevaluation = resnet_model.evaluate(train)\nprint(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")","cc067ac0":"from keras.applications import InceptionV3\n\ninception_base_model = InceptionV3(input_shape=(180,180,3),include_top=False,weights='imagenet')\n\n","e5998f07":"    inception_model = tf.keras.Sequential([\n        inception_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    inception_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)","ebd3c2bf":"r = inception_model.fit(train,\n          epochs=10,\n          validation_data=validation,\n          class_weight=class_weight,\n          steps_per_epoch=100,\n          validation_steps=25)\n","573ad35e":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='Val_Loss')\nplt.legend()\nplt.title('Loss Evolution')\n\nplt.subplot(2, 2, 2)\nplt.plot(r.history['accuracy'], label='Accuracy')\nplt.plot(r.history['val_accuracy'], label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution')","bd360e9f":"evaluation =inception_model.evaluate(test)\nprint(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n\nevaluation = inception_model.evaluate(train)\nprint(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")","d07f67f8":"# Computer Vision\n\n\nComputer vision is an interdisciplinary scientific field that deals with how computers can gain a high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.\nWe can use Computer Vision to determine whether a person is affected by pneumonia or not.","fbaf2fc3":"# Ivestigate pixel value distribution","766d3d08":"# AI for Medical Diagnosis\nComputer Vision (CV) has a lot of applications in medical diagnosis:\n\n* Dermatology\n* Ophthakmology\n* Histopathology.\n\nX-rays images are critical for the detection of lung cancer, pneumenia ... In this notebook you will learn:\n\n* Data pre-processing\n* Preprocess images properly for the train, validation and test sets.\n* Set-up a pre-trained neural network to make disease predictions on chest X-rays.\n\nIn this notebook you will work with chest X-ray images taken from the public ChestX-ray8 dataset.","f533a13f":"#  Import Packages and Functions\nWe'll make use of the following packages:\n\n* numpy and pandas is what we'll use to manipulate our data\n* matplotlib.pyplot and seaborn will be used to produce plots for visualization\n* util will provide the locally defined utility functions that have been provided for this assignment\nWe will also use several modules from the keras framework for building deep learning models.\n\nRun the next cell to import all the necessary packages.\n\n","1479ccaf":"# ResNet\n\nSee the full explanation and schemes in the Research Paper on Deep Residual Learning (https:\/\/arxiv.org\/pdf\/1512.03385.pdf)","f9576963":"# VGG16\nPresented in 2014, VGG16 has a very simple and classical architecture, with blocks of 2 or 3 convolutional layers followed by a pooling layer, plus a final dense network composed of 2 hidden layers (of 4096 nodes each) and one output layer (of 1000 nodes). Only 3x3 filters are used.\n\n![download7.png](attachment:download7.png)","4d263879":"# Pneumonia Detection with Convolutional Neural Networks\nComputer Vision can be realized using Convolutional neural networks (CNN) They are neural networks making features extraction over an image before classifying it. The feature extraction performed consists of three basic operations:\n\n* Filter an image for a particular feature (convolution)\n* Detect that feature within the filtered image (using the ReLU activation)\n* Condense the image to enhance the features (maximum pooling)\n\n# The convolution process is illustrated below\n\n![download.png](attachment:download.png)\n\nUsing convolution filters with different dimensions or values results in differents features extracted\n\nFeatures are then detected using the reLu activation on each destination pixel.\n\n![download1.png](attachment:download1.png)\n\nFeatures are the enhanced with MaxPool layers\n![download2.png](attachment:download2.png)\n\nThe stride parameters determines the distance between each filters. The padding one determines if we ignore the borderline pixels or not (adding zeros helps the neural network to get information on the border)\n\n![download3.png](attachment:download3.png)\n\nThe outputs are then concatened in Dense layers\n\n![download4.png](attachment:download4.png)\n\nBy using a sigmoid activation, the neural network determines which class the image belongs to\n![download5.png](attachment:download5.png)","93d07541":"# What is Pneumonia ?\nFrom Mayo Clinic's Article on pneumonia\n\nPneumonia is an infection that inflames the air sacs in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. A variety of organisms, including bacteria, viruses and fungi, can cause pneumonia.\n\nPneumonia can range in seriousness from mild to life-threatening. It is most serious for infants and young children, people older than age 65, and people with health problems or weakened immune systems.\n\n![download.png](attachment:download.png)","ea2531ee":"# Evaluation","2650c0a3":"# Building a CNN model\n\n## Impact of imbalance data on loss function\n\nLoss Function:\n$$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$\n\nWe can rewrite the the overall average cross-entropy loss over the entire training set `D` of size `N` as follows:\n$$\\mathcal{L}_{cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum_{\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$\n\n\n\nWhen we have an imbalance data, using a normal loss function will result a model that bias toward the dominating class. One solution is to use a weighted loss function. Using weighted loss function will balance the contribution in the loss function.\n\n$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$","faea2a5e":"# 2. Image Preprocessing\nBefore training, we'll first modify your images to be better suited for training a convolutional neural network. For this task we'll use the Keras ImageDataGenerator function to perform data preprocessing and data augmentation.\n\nThis class also provides support for basic data augmentation such as random horizontal flipping of images.\nWe also use the generator to transform the values in each batch so that their mean is 0 and their standard deviation is 1 (this will faciliate model training by standardizing the input distribution).\nThe generator also converts our single channel X-ray images (gray-scale) to a three-channel format by repeating the values in the image across all channels (we will want this because the pre-trained model that we'll use requires three-channel inputs).","3ad6a35c":"# Transfer Learning\n# DenseNet\nDensenet is a convolutional network where each layer is connected to all other layers that are deeper in the network:\n\n* The first layer is connected to the 2nd, 3rd, 4th etc.\n* The second layer is conected to the 3rd, 4th, 5th etc.\n\n![download.png](attachment:download.png)\n\nfor more information about the DenseNet Architecture visit this website : https:\/\/keras.io\/api\/applications\/densenet\/\n","faeff917":"The dataset is divided into three sets: 1) Train set 2) Validation set and 3) Test set.","4980ef64":"# Data Visualization\n","7d04e5ef":"# InceptionNet\nAlso known as GoogleNet, this architecture presents sub-networks called inception modules, which allows fast training computing, complex patterns detection, and optimal use of parameters\n\nfor more information visit https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43022.pdf\n\n","a2c0d4ba":"# Build a separate generator fo valid and test sets\n\nNow we need to build a new generator for validation and t esting data.\n\nWhy can't use the same generator as for the training data?\n\nLook back at the generator we wrote for the training data.\n\nIt normalizes each image per batch, meaning thatit uses batch statistics.\nWe should not do this with the test and validation data, since in a real life scenario we don't process incoming images a batch at a time (we process one image at a time).\nKnowing the average per batch of test data would effectively give our model an advantage (The model should not have any information about the test data).\nWhat we need to do is to normalize incomming test data using the statistics computed from the training set."}}