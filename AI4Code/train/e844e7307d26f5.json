{"cell_type":{"31ac1037":"code","291fab2d":"code","b3b39f3c":"code","a6ee7549":"code","ed959c4d":"code","5623e746":"code","9d3142e3":"code","7b5773e0":"code","c04bc9a5":"code","2f6f90a3":"code","8b2a1ef3":"code","cc23d4a9":"code","b9c1e847":"code","d94aced1":"code","cb15e073":"code","34e8f7e2":"code","491f44f3":"code","9ee107b4":"code","3c213c57":"code","cfd06a27":"code","2360ed2b":"code","fd6a21d9":"code","2f76fa44":"code","3bb46f0a":"code","daf2db4e":"code","4dae59a9":"code","26a1f636":"code","055c0cab":"code","e16c6629":"code","160f6eb9":"code","8cb99172":"code","f8384cac":"code","43348752":"code","d7f93a8d":"code","96cd6434":"code","2010b1b6":"code","4fb38cc4":"code","2c27b135":"code","3031215a":"code","0fba8e4a":"code","2ebce412":"code","01ccf941":"code","793765a1":"code","d9d8fe53":"code","f9baa5d1":"code","6481f410":"code","2c8abae4":"code","5ee56394":"code","524c859d":"code","99abcfca":"code","ce4899ce":"code","67d21887":"code","90361dec":"code","f331b66e":"code","635c7229":"code","83ec5aec":"code","c65493a6":"code","43d8aaee":"code","4d975064":"code","70104512":"code","f53da493":"code","956272b2":"code","37fe4c0b":"code","cae55c1b":"code","31c77b3f":"code","2993ae1c":"code","e339234d":"code","8731359c":"code","32b778e2":"code","7631e2df":"code","822d734d":"code","773ebb4b":"code","cb2b78cd":"code","1e423188":"code","7d92d23d":"code","ca8a7e95":"code","df68b6d7":"code","0b37bc80":"code","7478306f":"code","ade0e183":"code","97de1de7":"code","0bd91c78":"code","01e0e9db":"code","90a004dd":"code","4b201697":"code","1a4bfa41":"code","3a088792":"code","7933ec4d":"code","597d440a":"code","20cd73ff":"code","61bb48f9":"code","4a08b54d":"code","ef4c44e1":"code","d751c783":"code","007fa107":"code","0bb2a4e5":"code","85e91b73":"code","d133e61d":"code","ef76532c":"code","45f7680a":"code","74ae6bdb":"code","d249f266":"code","6b9bc80d":"code","c636cb24":"code","c160ae09":"code","cfdc5973":"code","5154a8de":"code","c50decd2":"code","2d8034d4":"code","7779dc1d":"code","7e5f775c":"code","ec34b8e4":"code","983fa5ea":"code","745df5ce":"code","a6fa6fa7":"code","435a468b":"code","a9e58615":"code","37b40ad6":"code","e8d823a6":"code","10846dae":"code","94ad220b":"code","1e129c50":"code","27294dc3":"code","c87162ee":"code","50f788d5":"code","bc9a666c":"code","ac73c1fb":"code","91360089":"code","69cdea63":"code","f11219cf":"code","b52eeccf":"code","e1a2f068":"code","6cdaa6b0":"code","b47e8901":"code","ed79a826":"code","47df3933":"code","8b0df008":"code","8150d4ba":"code","77fb4217":"markdown","66d7c0d8":"markdown","adb1e3ed":"markdown","2237ff05":"markdown","41189d5c":"markdown","2867e6ff":"markdown","c4cb57ee":"markdown","3358e9e6":"markdown","6204155e":"markdown","c053d7ed":"markdown","00cf1b97":"markdown","f61da9ce":"markdown","c81b7da1":"markdown","dbb26619":"markdown","82b182c3":"markdown","66a353a1":"markdown","df16f3de":"markdown","387d0cc7":"markdown","ba8c9d69":"markdown","74aa4e0a":"markdown","26f11495":"markdown","b29fd53b":"markdown","3cd819a8":"markdown","6576ccc7":"markdown","996acb70":"markdown","cc08cc51":"markdown","d3761293":"markdown","e8c70fd7":"markdown","1ac9c409":"markdown","318f5c76":"markdown","82955338":"markdown","772893c9":"markdown","2a5dbaf5":"markdown","7777f0a8":"markdown","0e5d920b":"markdown","fc104887":"markdown","20ceec93":"markdown","ded6b56b":"markdown","3e2d1201":"markdown","bd84676b":"markdown","aa9d1dac":"markdown","0aa5628c":"markdown","c21d5ebb":"markdown","017ec7c3":"markdown","9fde85f8":"markdown","32d8da7d":"markdown"},"source":{"31ac1037":"# Import libraries | Standard\nimport pandas as pd\nimport numpy as np\nimport os\nimport datetime\nimport warnings\nfrom time import time\n\n# Import libraries | Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Import libraries | Sk-learn\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\n\n# Pretty display for notebooks\n%matplotlib inline","291fab2d":"pd.set_option('display.max_columns', None)  ","b3b39f3c":"#pd.set_option('display.max_rows', None)","a6ee7549":"warnings.filterwarnings('ignore')","ed959c4d":"def distribution(data, features, transformed = False):\n    \"\"\"\n    Visualization code for displaying distributions of features\n    \"\"\"\n    \n    # Create figure\n    fig = plt.figure(figsize = (11,5));\n\n    # Skewed feature plotting\n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(1, 2, i+1)\n        ax.hist(data[feature], bins = 25, color = '#00A0A0')\n        ax.set_title(\"'%s' Feature Distribution\"%(feature), fontsize = 14)\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Number of Records\")\n        ax.set_ylim((0, 2000))\n        ax.set_yticks([0, 500, 1000, 1500, 2000])\n        ax.set_yticklabels([0, 500, 1000, 1500, \">2000\"])\n\n    # Plot aesthetics\n    if transformed:\n        fig.suptitle(\"Log-transformed Distributions of Continuous Data Features\", \\\n            fontsize = 16, y = 1.03)\n    else:\n        fig.suptitle(\"Distributions of Continuous Data Features\", \\\n            fontsize = 16, y = 1.03)\n\n    fig.tight_layout()\n    fig.show()","5623e746":"def eval_train_predict(learner, sample_size, train_X, train_y, test_X, test_y, transform_y, log_constant): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set       \n       - train_X: features training set\n       - train_y: sales training set\n       - test_X: features testing set\n       - test_y: sales testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data\n    start = time() # Get start time\n    learner = learner.fit(train_X[:sample_size], train_y[:sample_size])\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['time_train'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    start = time() # Get start time\n    predictions = learner.predict(test_X)\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['time_pred'] = end - start\n            \n    # Compute Weighted Mean Absolute Error on Test Set\n    if transform_y == 'log':\n        results['WMAE'] = weighted_mean_absolute_error(np.exp(test_y) - 1 - log_constant, \n                                                       np.exp(predictions) - 1 - log_constant, \n                                                       compute_weights(test_X['IsHoliday']))\n    else:\n        results['WMAE'] = weighted_mean_absolute_error(test_y, predictions, compute_weights(test_X['IsHoliday']))\n                   \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results","9d3142e3":"def eval_visualize(results):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n    \"\"\"\n  \n    # Create figure\n    fig, ax = plt.subplots(1, 3, figsize = (18,8))\n\n    # Constants\n    bar_width = 0.1\n    colors = ['#A00000','#00A0A0','#00A000','#E3DAC9','#555555', '#87CEEB']\n    metrics = ['time_train', 'time_pred', 'WMAE']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(metrics):\n            # Creative plot code\n            ax[j%3].bar(0+k*bar_width, results[learner][0][metric], width = bar_width, color = colors[k])\n            ax[j%3].set_xlabel(\"Models\")\n            ax[j%3].set_xticklabels([''])\n                \n    # Add unique y-labels\n    ax[0].set_ylabel(\"Time (in seconds)\")\n    ax[1].set_ylabel(\"Time (in seconds)\")\n    ax[2].set_ylabel(\"WMAE\")\n    \n    # Add titles\n    ax[0].set_title(\"Model Training\")\n    ax[1].set_title(\"Model Predicting\")\n    ax[2].set_title(\"WMAE on Testing Set\")\n \n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    plt.legend(handles = patches, bbox_to_anchor = (-.80, 2.43), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n    \n    # Aesthetics\n    plt.suptitle(\"Performance Metrics for Supervised Learning Models\", fontsize = 16, y = 1.10)\n    plt.tight_layout()\n    plt.show()","7b5773e0":"def train_predict(learner, train_X, train_y, test_X, test_y, transform_y, log_constant, verbose=0): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - train_X: features training set\n       - train_y: sales training set\n       - test_X: features testing set\n       - test_y: sales testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data\n    start = time() # Get start time\n    learner = learner.fit(train_X, train_y)\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['time_train'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    start = time() # Get start time\n    predictions = learner.predict(test_X)\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['time_pred'] = end - start\n            \n    # Compute Weighted Mean Absolute Error on Test Set\n    if transform_y == 'log':\n        results['WMAE'] = weighted_mean_absolute_error(np.exp(test_y) - 1 - log_constant, \n                                                       np.exp(predictions) - 1 - log_constant, \n                                                       compute_weights(test_X['IsHoliday']))\n    else:\n        results['WMAE'] = weighted_mean_absolute_error(test_y, predictions, compute_weights(test_X['IsHoliday']))\n    \n\n    #Extract the feature importances\n    importances = learner.feature_importances_\n\n    # Success\n    print(\"Learner Name :\", learner.__class__.__name__)\n    print(\"Training     :\", round(results['time_train'],2), \"secs \/\", len(train_y), \"records\")\n    print(\"Predicting   :\", round(results['time_pred'],2), \"secs \/\", len(test_y), \"records\")\n    print(\"Weighted MAE :\", round(results['WMAE'],2))\n\n    if verbose == 1:\n        # Plot\n        print(\"\\n<Feature Importance>\\n\")\n        feature_plot(importances, train_X, train_y, 10)\n\n        print(\"\\n<Feature Weightage>\\n\")\n        topk = len(train_X.columns)\n        indices = np.argsort(importances)[::-1]\n        columns = train_X.columns.values[indices[:topk]]\n        values = importances[indices][:topk]\n\n        for i in range(topk):\n            print('\\t' + columns[i] + (' ' * (15 - len(columns[i])) + ': ' + str(values[i])))\n            \n        print(\"\\n<Learner Params>\\n\", model.get_params())\n    \n    # Return the model & predictions\n    return (learner, predictions)","c04bc9a5":"def feature_plot(importances, train_X, train_y, topk=5):\n    \n    # Display the most important features\n    indices = np.argsort(importances)[::-1]\n    columns = train_X.columns.values[indices[:topk]]\n    values = importances[indices][:topk]\n\n    # Creat the plot\n    fig = plt.figure(figsize = (18,5))\n    plt.title(\"Normalized Weights for First \" + str(topk) + \" Most Predictive Features\", fontsize = 16)\n    plt.bar(np.arange(topk), values, width = 0.6, align=\"center\", color = '#00A000', \\\n          label = \"Feature Weight\")\n    plt.bar(np.arange(topk) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n          label = \"Cumulative Feature Weight\")\n    plt.xticks(np.arange(topk), columns)\n    plt.xlim((-0.5, 9.5))\n    plt.ylabel(\"Weight\", fontsize = 12)\n    plt.xlabel(\"Feature\", fontsize = 12)\n    \n    plt.legend(loc = 'upper left')\n    plt.tight_layout()\n    plt.show()  ","2f6f90a3":"def reduce_mem_usage(df, verbose=True):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))    \n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","8b2a1ef3":"def compute_weights(holidays):\n    return holidays.apply(lambda x: 1 if x==0 else 5)","cc23d4a9":"def weighted_mean_absolute_error(pred_y, test_y, weights):\n    return 1\/sum(weights) * sum(weights * abs(test_y - pred_y))","b9c1e847":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ncnt = 0\nenv = 'Outside Kaggle'\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        cnt += 1\n        print(os.path.join(dirname, filename))\n        \nif cnt > 0:\n    env = 'Kaggle Kernel'","d94aced1":"print('Environment:', env)","cb15e073":"# Read input files\nif env == 'Kaggle Kernel':\n    features = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv\")\n    stores = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\n    train = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv\")\n    test = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv\")\nelse:    \n    features = pd.read_csv(\"data\/features.csv\")\n    stores = pd.read_csv(\"data\/stores.csv\")\n    train = pd.read_csv(\"data\/train.csv\")\n    test = pd.read_csv(\"data\/test.csv\")","34e8f7e2":"stores.head()","491f44f3":"stores.info()","9ee107b4":"stores.describe()","3c213c57":"#missing data\ntotal = stores.isnull().sum().sort_values(ascending=False)\npercent = (stores.isnull().sum()\/stores.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","cfd06a27":"stores['Size'].groupby(stores['Type']).mean()","2360ed2b":"# Create figure\n#plt.figure()\n#plt.scatter(stores['Type'], stores['Store'])\n#plt.ylabel('Store ID')\n#plt.xlabel('Store Type')\n\nfig, ax = plt.subplots(1, 2, figsize = (15,6))\nax[0].bar(stores['Type'].unique(), stores['Size'].groupby(stores['Type']).count())\nax[0].set_ylabel('# of Stores')\nax[0].set_xlabel('Store Type')\nax[0].yaxis.grid(True, linewidth=0.3)\n\nax[1].scatter(stores['Type'], stores['Size'])\nax[1].scatter(stores['Type'].unique(), stores['Size'].groupby(stores['Type']).mean()) #Store Type Average Store Size Vs \nax[1].set_ylabel('Store Size (Total \/ Average)')\nax[1].set_xlabel('Store Type')\nax[1].yaxis.grid(True, linewidth=0.3)\n\n#plt.figure(figsize=(6,6))\n#plt.yticks(np.arange(len(features_missing)),features_missing.index,rotation='horizontal')\n#plt.xlabel('fraction of rows with missing data')\n#plt.barh(np.arange(len(features_missing)), features_missing)","fd6a21d9":"stores[(stores['Size'] < 40000) & (~stores['Type'].isin(['C']))]","2f76fa44":"#Explore Weekly Sales - histogram\nsns.distplot(stores['Size'])","3bb46f0a":"features.head()","daf2db4e":"features.info()","4dae59a9":"features.describe()","26a1f636":"#missing data\ntotal = features.isnull().sum().sort_values(ascending=False)\npercent = (features.isnull().sum()\/features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","055c0cab":"# Distribution of NaNs for all columns\nfeatures_missing = features.isna().sum()\/len(features) * 100","e16c6629":"plt.figure(figsize=(6,6))\nplt.yticks(np.arange(len(features_missing)),features_missing.index,rotation='horizontal')\nplt.xlabel('fraction of rows with missing data')\nplt.barh(np.arange(len(features_missing)), features_missing)","160f6eb9":"fig, ax = plt.subplots(2, 2, figsize = (15,12))\n\n# Plot 1: Year Vs # of Records\nax[0,0].barh(features['Date'].str.slice(start=0, stop=4).unique(), \n          features['Date'].str.slice(start=0, stop=4).value_counts())\nax[0,0].set_xlabel('# of Records')\nax[0,0].set_ylabel('Year')\nax[0,0].yaxis.grid(True, linewidth=0.3)\n\n# Plot 2: Month Vs # of Records with Missing Values - Unemployment\nax[1,0].barh(features['Date'].str.slice(start=0, stop=7)[features['Unemployment'].isna()].unique(), \n          features['Date'].str.slice(start=0, stop=7)[features['Unemployment'].isna()].value_counts())\nax[1,0].set_xlabel('# of Records with Missing Values - Unemployment')\nax[1,0].set_ylabel('Month')\nax[1,0].yaxis.grid(True, linewidth=0.3)\n\n# Plot 3: Month Vs # of Records with Missing Values - CPI\nax[1,1].barh(features['Date'].str.slice(start=0, stop=7)[features['CPI'].isna()].unique(), \n          features['Date'].str.slice(start=0, stop=7)[features['CPI'].isna()].value_counts())\nax[1,1].set_xlabel('# of Records with Missing Values - CPI')\nax[1,1].set_ylabel('Month')\nax[1,1].yaxis.grid(True, linewidth=0.3)\n\n#plt.figure(figsize=(6,6))\n#plt.yticks(np.arange(len(features_missing)),features_missing.index,rotation='horizontal')\n#plt.xlabel('fraction of rows with missing data')\n#plt.barh(np.arange(len(features_missing)), features_missing)","8cb99172":"holidays = ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08', #Super Bowl\n           '2010-09-10', '2011-09-09', '2012-09-07', '2013-02-06',  #Labor Day\n           '2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29',  #Thanksgiving\n           '2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']  #Christmas","f8384cac":"# Validate Holidays\nfeatures['IsHoliday'][features['Date'].isin(holidays)].value_counts()","43348752":"features['Date'][features['IsHoliday'].isin([1])][~features['Date'].isin(holidays)].value_counts()","d7f93a8d":"features[['CPI','Unemployment']].groupby([features['Store'], features['Date'].str.slice(start=0, stop=7)]).mean().head(84)","96cd6434":"features.groupby(features['Date'].str.slice(start=0, stop=7))['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'].count()","2010b1b6":"#Explore Distribution\ndistribution(features, ['CPI','Unemployment'])","4fb38cc4":"#Explore Distribution\ndistribution(features, ['Temperature','Fuel_Price'])","2c27b135":"#Explore Distribution\ndistribution(features, ['MarkDown1','MarkDown2'])","3031215a":"#Explore Distribution\ndistribution(features, ['MarkDown3','MarkDown4'])","0fba8e4a":"#Explore Distribution\ndistribution(features, ['MarkDown5'])","2ebce412":"train.head()","01ccf941":"train.info()","793765a1":"train.describe()","d9d8fe53":"# Explore Date Range\ntrain['Date'].str.slice(start=0, stop=4).value_counts()","f9baa5d1":"# Validate Holidays\ntrain['IsHoliday'][train['Date'].isin(holidays)].value_counts()","6481f410":"train['Date'][train['IsHoliday'].isin([1])][~train['Date'].isin(holidays)].value_counts()","2c8abae4":"#Explore Distribution\ndistribution(train, ['Weekly_Sales'])","5ee56394":"train['Store'][train['Weekly_Sales'] < 0].count()","524c859d":"train_outliers = pd.merge(train, stores, how='left', on=['Store'])","99abcfca":"# Average Weekly Sales by Store Type\ntrain_outliers.groupby(['Type'])['Weekly_Sales'].mean()","ce4899ce":"# Average Weekly Sales for possibly misclassified Stores\ntrain_outliers = train_outliers[train_outliers['Store'].isin([3,5,33,36])]\ntrain_outliers.groupby(['Store','Type'])['Weekly_Sales'].mean()","67d21887":"# Average Weekly Sales by Store Type\nfig, ax = plt.subplots(1, 2, figsize = (15,6))\nax[0].bar(train_outliers['Type'].unique(), train_outliers.groupby(['Type'])['Weekly_Sales'].mean())\nax[0].set_ylabel('Average Weekly Sales')\nax[0].set_xlabel('Store Type')\nax[0].yaxis.grid(True, linewidth=0.3)\n\nax[1].bar([3,5,33,36], train_outliers.groupby(['Store','Type'])['Weekly_Sales'].mean())\nax[1].set_ylabel('Average Weekly Sales')\nax[1].set_xlabel('Store ID')\nax[1].yaxis.grid(True, linewidth=0.3)","90361dec":"train_outliers = None","f331b66e":"test.head()","635c7229":"test.info()","83ec5aec":"test.describe()","c65493a6":"test['Date'].str.slice(start=0, stop=4).value_counts()","43d8aaee":"# Validate Holidays\ntest['IsHoliday'][test['Date'].isin(holidays)].value_counts()","4d975064":"test['Date'][test['IsHoliday'].isin([1])][~test['Date'].isin(holidays)].value_counts()","70104512":"stores[stores['Store'].isin([3,5,33,36])].index","f53da493":"stores.iat[2, 1] = stores.iat[4, 1] = stores.iat[32, 1] = stores.iat[35, 1] = 'C'","956272b2":"features['MarkDown1'] = features['MarkDown1'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown2'] = features['MarkDown2'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown3'] = features['MarkDown3'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown4'] = features['MarkDown4'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown5'] = features['MarkDown5'].apply(lambda x: 0 if x < 0 else x)","37fe4c0b":"%%time\n# For each Store, propogate values of CPI & Unemployment to the rows with NaN values\nfor i in range(len(features)):\n\n    if features.iloc[i]['Date'] == '2013-04-26':\n        CPI_new = features.iloc[i]['CPI']\n        Unemployment_new = features.iloc[i]['Unemployment']\n    \n    if np.isnan(features.iloc[i]['CPI']):\n        features.iat[i, 9] = CPI_new\n        features.iat[i, 10] = Unemployment_new","cae55c1b":"%%time\n# For each date, retrive the corresponding week number\nfeatures['Week'] = 0\n\nfor i in range(len(features)):\n    features.iat[i, 12] = datetime.date(int(features.iloc[i]['Date'][0:4]), \n                                        int(features.iloc[i]['Date'][5:7]), \n                                        int(features.iloc[i]['Date'][8:10])).isocalendar()[1]","31c77b3f":"features['Year'] = features['Date'].str.slice(start=0, stop=4)","2993ae1c":"#missing data for 2012 & 2013\ntotal = features[features['Year'].isin(['2012','2013'])].isnull().sum().sort_values(ascending=False)\npercent = (features[features['Year'].isin(['2012','2013'])].isnull().sum()\/\n           features[features['Year'].isin(['2012','2013'])].isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(4)","e339234d":"%%time\n# For 2010 & 2011 records, for each store, copy over MarkDown values from 2012\n\n# Iterate through stores\nfor i in range(1, len(features['Store'].unique())):\n    \n    # For 2010, iterate through weeks 5 thru 52\n    for j in range(5, 52):\n        idx = features.loc[(features.Year == '2010') & (features.Store == i) & (features.Week == j),['Date']].index[0]\n        \n        features.iat[idx, 4] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown1']].values[0]\n        features.iat[idx, 5] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown2']].values[0]\n        features.iat[idx, 6] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown3']].values[0]\n        features.iat[idx, 7] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown4']].values[0]\n        features.iat[idx, 8] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown5']].values[0]\n        \n    # For 2011, iterate through weeks 1 thru 44\n    for j in range(1, 44):\n        idx = features.loc[(features.Year == '2011') & (features.Store == i) & (features.Week == j),['Date']].index[0]\n        \n        features.iat[idx, 4] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown1']].values[0]\n        features.iat[idx, 5] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown2']].values[0]\n        features.iat[idx, 6] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown3']].values[0]\n        features.iat[idx, 7] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown4']].values[0]\n        features.iat[idx, 8] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown5']].values[0]        ","8731359c":"features.drop(columns=['Year'], axis=1, inplace=True)","32b778e2":"# Now fill all the missing MarkDown values with 0\nfeatures.fillna(0, inplace=True)","7631e2df":"train['Weekly_Sales'] = train['Weekly_Sales'].apply(lambda x: 0 if x < 0 else x)","822d734d":"train = pd.merge(train, stores, how='left', on=['Store'])\ntrain = pd.merge(train, features, how='left', on=['Store','Date'])\n\ntest = pd.merge(test, stores, how='left', on=['Store'])\ntest = pd.merge(test, features, how='left', on=['Store','Date'])","773ebb4b":"train['Store'][train['IsHoliday_x'] != train['IsHoliday_y']].count()","cb2b78cd":"test['Store'][test['IsHoliday_x'] != test['IsHoliday_y']].count()","1e423188":"train.drop(columns=['IsHoliday_y'], axis=1, inplace=True)\ntest.drop(columns=['IsHoliday_y'], axis=1, inplace=True)","7d92d23d":"train.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)\ntest.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)","ca8a7e95":"train['IsHoliday'] = train['IsHoliday'].apply(lambda x: 1 if x==True else 0)\ntest['IsHoliday'] = test['IsHoliday'].apply(lambda x: 1 if x==True else 0)","df68b6d7":"train = pd.get_dummies(train, columns=['Type'])\ntest = pd.get_dummies(test, columns=['Type'])","0b37bc80":"train['Week'] = test['Week'] = 0","7478306f":"%%time\n# For each date, retrive the corresponding week number\nfor i in range(len(train)):\n    train.iat[i, 15] = datetime.date(int(train.iloc[i]['Date'][0:4]), \n                                     int(train.iloc[i]['Date'][5:7]), \n                                     int(train.iloc[i]['Date'][8:10])).isocalendar()[1]","ade0e183":"%%time\n# For each date, retrive the corresponding week number\nfor i in range(len(test)):\n    test.iat[i, 14] = datetime.date(int(test.iloc[i]['Date'][0:4]), \n                                    int(test.iloc[i]['Date'][5:7]), \n                                    int(test.iloc[i]['Date'][8:10])).isocalendar()[1]","97de1de7":"# Create checkpoint\ntrain.to_csv('train_prescaled.csv', index=False)\ntest.to_csv('test_prescaled.csv', index=False)","0bd91c78":"# Restore checkpoint\ntrain = pd.read_csv(\"train_prescaled.csv\")\ntest = pd.read_csv(\"test_prescaled.csv\")","01e0e9db":"# Create Submission dataframe\nsubmission = test[['Store', 'Dept', 'Date']].copy()\nsubmission['Id'] = submission['Store'].map(str) + '_' + submission['Dept'].map(str) + '_' + submission['Date'].map(str)\nsubmission.drop(['Store', 'Dept', 'Date'], axis=1, inplace=True)","90a004dd":"train['Year'] = train['Date'].str.slice(start=0, stop=4)\ntest['Year'] = test['Date'].str.slice(start=0, stop=4)","4b201697":"# Drop non-numeric columns\ntrain.drop(columns=['Date'], axis=1, inplace=True)\ntest.drop(columns=['Date'], axis=1, inplace=True)","1a4bfa41":"skewed = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\ntrain[skewed] = train[skewed].apply(lambda x: np.log(x + 1))\ntest[skewed] = test[skewed].apply(lambda x: np.log(x + 1))","3a088792":"log_constant = 0","7933ec4d":"train['Weekly_Sales'] = train['Weekly_Sales'].apply(lambda x: np.log(x + 1 + log_constant))","597d440a":"distribution(train, ['Weekly_Sales'])","20cd73ff":"colormap = plt.cm.RdBu\ncorr = train.astype(float).corr()\n\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.set(font_scale=0.9)\nsns.heatmap(round(corr,2),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)","61bb48f9":"corr_cutoff = 0.8\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= corr_cutoff:\n            if columns[j]:\n                columns[j] = False\n                \nselected_columns = train.columns[columns]\nhighcorr_columns = train.columns.difference(selected_columns)","4a08b54d":"highcorr_columns","ef4c44e1":"train.drop(columns=highcorr_columns, axis=1, inplace=True)\ntest.drop(columns=highcorr_columns, axis=1, inplace=True)","d751c783":"train_X, val_X, train_y, val_y = train_test_split(train.drop('Weekly_Sales', axis = 1), \n                                                  train['Weekly_Sales'], \n                                                  test_size = 0.2, \n                                                  random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(train_X.shape[0]))\nprint(\"Validation set has {} samples.\".format(val_X.shape[0]))","007fa107":"# Validate shape\ntrain_X.shape, train_y.shape, val_X.shape, val_y.shape, test.shape","0bb2a4e5":"# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\n\nnumerical = ['Store', 'Dept', 'IsHoliday', 'Size', 'Temperature', 'Fuel_Price', \n             'CPI', 'Unemployment', 'Week', 'Type_B', 'Type_C',\n             'MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n\ntrain_scaled = pd.DataFrame(data = train_X)\ntrain_scaled[numerical] = scaler.fit_transform(train_X[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(train_scaled.head(n = 5))","85e91b73":"val_scaled = pd.DataFrame(data = val_X)\nval_scaled[numerical] = scaler.transform(val_X[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(val_scaled.head(n = 5))","d133e61d":"test_scaled = pd.DataFrame(data = test)\ntest_scaled[numerical] = scaler.transform(test[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(test_scaled.head(n = 5))","ef76532c":"# Free up memory\ntrain = test = features = stores = None","45f7680a":"# Create checkpoint\ntrain_scaled.to_csv('train_X_scaled.csv', index=False)\nval_scaled.to_csv('val_X_scaled.csv', index=False)\ntrain_y.to_csv('train_y.csv', index=False, header=['Weekly_Sales'])\nval_y.to_csv('val_y.csv', index=False, header=['Weekly_Sales'])\ntest_scaled.to_csv('test_X_scaled.csv', index=False)","74ae6bdb":"# Restore checkpoint\ntrain_scaled = pd.read_csv(\"train_X_scaled.csv\")\nval_scaled = pd.read_csv(\"val_X_scaled.csv\")\ntrain_y = pd.read_csv(\"train_y.csv\")\nval_y = pd.read_csv(\"val_y.csv\")\ntest_scaled = pd.read_csv(\"test_X_scaled.csv\")","d249f266":"# Reduce memory usage\n#train_scaled=reduce_mem_usage(train_scaled)\n#test_scaled=reduce_mem_usage(test_scaled)","6b9bc80d":"train_X = train_scaled\nval_X = val_scaled","c636cb24":"# Free up memory\ntrain_scaled = val_scaled = None","c160ae09":"# Convert Dataframe to Series\ntrain_y = train_y.iloc[:,0]\nval_y = val_y.iloc[:,0]","cfdc5973":"%%time\n# Initialize base models\nmodel_A = LinearRegression()\nmodel_B = ElasticNet(random_state=1)\nmodel_C = RandomForestRegressor(random_state=1)\nmodel_D = GradientBoostingRegressor(random_state=1)\nmodel_E = xgb.XGBRegressor()\nmodel_F = LGBMRegressor(random_state=1)\n\nsamples = len(train_y) # 100% of training set\n\n# Collect results on the learners\nresults = {}\nfor model in [model_A, model_B, model_C, model_D, model_E, model_F]:\n    model_name = model.__class__.__name__\n    results[model_name] = {}\n    for i, samples in enumerate([samples]):\n        results[model_name][i] = eval_train_predict(model, samples, train_X, train_y, val_X, val_y, 'log', log_constant)","5154a8de":"# Evaluate Metrics\neval_visualize(results)","c50decd2":"results","2d8034d4":"model_rf_base = RandomForestRegressor(random_state=42, verbose=1)","7779dc1d":"model_rf_base, pred_y_rf_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","7e5f775c":"pred_y_rf_test = model_rf_base.predict(test_scaled)","ec34b8e4":"param_grid = { \n    'n_estimators': [10, 50, 100, 150],\n    'max_features': [None, 'auto'],\n    'bootstrap': [True, False],\n    'max_depth':[None],\n    'random_state': [42], \n    'verbose': [1]\n}","983fa5ea":"#%%time\n#CV = GridSearchCV(estimator=model_rf_base, param_grid=param_grid, cv=2, verbose=1)\n#CV.fit(train_X, train_y)","745df5ce":"#CV.best_params_ # latest","a6fa6fa7":"# Using best params from GridSearch\n#model.set_params(**CV.best_params_)","435a468b":"model = RandomForestRegressor(random_state=42, \n                              n_estimators=150, \n                              bootstrap=True, \n                              max_features=None, \n                              max_depth=None, \n                              min_samples_leaf=1,\n                              min_samples_split=3,\n                              verbose=1)","a9e58615":"model, pred_y_rf_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","37b40ad6":"pred_y_rf_test = model.predict(test_scaled)","e8d823a6":"# Default model\nmodel = LGBMRegressor()","10846dae":"model, pred_y_lgbm_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","94ad220b":"param_grid = {\n    'boosting_type': ['gbdt'], \n    'objective': ['regression'],\n    'random_state': [42],\n    'min_data_in_leaf':[3],\n    'min_depth':[2],\n    'learning_rate': [0.3],\n    #'n_estimators': [1000, 3000],\n    'n_estimators': [3000],\n    #'num_leaves': [60, 70, 80],\n    'max_bin': [150,200,255,300]\n}","1e129c50":"%%time\n#CV_lgbm = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, scoring='neg_mean_absolute_error')\n#CV = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1)\n#CV.fit(train_X, train_y)","27294dc3":"#print(\"Best parameter (CV score=%0.3f):\" % CV.best_score_)\n#print(CV.best_params_)","c87162ee":"# Using best params from GridSearch\n#model.set_params(**CV.best_params_)","50f788d5":"model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n       importance_type='split', learning_rate=0.3, max_bin=150,\n       max_depth=-1, min_child_samples=5, min_child_weight=0.001,\n       min_data_in_leaf=3, min_depth=2, min_split_gain=0.0,\n       n_estimators=3000, n_jobs=-1, num_leaves=80, objective='regression',\n       random_state=42, reg_alpha=0.1, reg_lambda=2, silent=True,\n       subsample=1.0, subsample_for_bin=200000, subsample_freq=0,\n       verbose=1)\n#Weighted MAE : 1238.72","bc9a666c":"model, pred_y_lgbm_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","ac73c1fb":"pred_y_lgbm_test = model.predict(test_scaled)","91360089":"# Blend the results of the two regressors and save the prediction to a CSV file.\npred_y_val = ((np.exp(pred_y_rf_val) - 1 - log_constant) * 0.7) + ((np.exp(pred_y_lgbm_val) - 1 - log_constant) * 0.3)\npred_y = ((np.exp(pred_y_rf_test) - 1 - log_constant) * 0.7) + ((np.exp(pred_y_lgbm_test) - 1 - log_constant) * 0.3)","69cdea63":"val_y = np.exp(val_y) - 1 - log_constant","f11219cf":"# make predictions\nprint(\"Weighted Mean Absolute Error: \", weighted_mean_absolute_error(pred_y_val, val_y, compute_weights(val_X['IsHoliday'])))","b52eeccf":"submission['Weekly_Sales'] = pred_y","e1a2f068":"submission[['Id','Weekly_Sales']].to_csv('submission.csv', index=False)","6cdaa6b0":"val_X.columns","b47e8901":"tmp = pd.DataFrame(scaler.inverse_transform(val_X), columns = val_X.columns)","ed79a826":"tmp = tmp.assign(weekly_sales=val_y.values)","47df3933":"tmp = pd.concat([tmp, pd.DataFrame(pred_y_val)], axis=1)","8b0df008":"tmp.head(5000).to_csv('tmp5000.csv', index=False)","8150d4ba":"tmp.to_csv('tmp5000.csv', index=False)","77fb4217":"## 4. Modelling","66d7c0d8":"## 1. Stores Data","adb1e3ed":"### Merge the following datasets:\n1. Stores + Features + Train\n2. Stores + Features + Test\n3. Remove duplicate columns from each dataset","2237ff05":"## 2. Merge Datasets","41189d5c":"### Stores Data | Correct Type for 4 stores","2867e6ff":"###### Takeaways: \n1. Column TYPE is a candidate for one-hot encoding. \n2. Most stores are of TYPE='A'. Only a few stores are of TYPE='C'.\n3. TYPE columns seem to be linked to Store Size. Average store size of TYPE 'A' is ~ 175k, TYPE 'B' is ~ 100k and TYPE 'C' is ~40k\n4. Four stores [3, 5, 33 & 36] whose size is < 40k, seem to have been incorrectly tagged as Types A & B","c4cb57ee":"## Model Stacking","3358e9e6":"#### Tuned Model","6204155e":"#### Columns: MarkDown1, MarkDown2, MarkDown3, MarkDown4 & MarkDown5\nAs noted above, columns MARKDOWN* are missing values for the whole of 2010 and 2011 (upto Nov). For each store, 2012 values would be copied over to records with missing values. Also, to facilitate the copy, new columns WEEK and YEAR would be derived from DATE.","c053d7ed":"train_y = train_scaled['Weekly_Sales']\ntrain_X = train_scaled.drop('Weekly_Sales', axis = 1)\n\nval_y = val_scaled['Weekly_Sales']\nval_X = val_scaled.drop('Weekly_Sales', axis = 1)","00cf1b97":"### Features Data | Negative values for MarkDowns:","f61da9ce":"## 4. Test Data","c81b7da1":"## 3. Train Data","dbb26619":"### Features Data | NaN values for multiple columns:","82b182c3":"### Log Transform Skewed Features","66a353a1":"#### Default Model","df16f3de":"## 2. Features Data","387d0cc7":"### Scale Datasets","ba8c9d69":"model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                      importance_type='split', learning_rate=0.3, max_depth=-1,\n                      min_child_samples=5, min_child_weight=0.001, min_data_in_leaf=2,\n                      min_depth=3, min_split_gain=0.0, n_estimators=3000, n_jobs=-1,\n                      num_leaves=80, objective='regression', random_state=42,\n                      reg_alpha=0.1, reg_lambda=2, silent=True, subsample=1.0,\n                      subsample_for_bin=200000, subsample_freq=0,\n                      verbose=1)\n#Weighted MAE : 1275.72","74aa4e0a":"###### Takeaways: \n1. Data requires pre-processing\n2. Column(s) ISHOLIDAY has been validated\n3. Column(s) UNEMPLOYMENT & CPI have missing values for May, Jun & Jul 2013. For these columns as the values dont change significantly month on month, value from Apr 2013 would be propogated over for each store. \n4. Column(s) MARKDOWN* have missing values for 2010 (entire year) and 2011 (until Nov). Additionally, there are missing values for other other dates as well. \n5. CPI and UNEMPLOYMENT value are a bit skewed. MARKDOWN* columns are skewed. ","26f11495":"###### Takeaway: \n1. MarkDown4 and Type_A are highly correlated to other existing features and have been dropped. ","b29fd53b":"###### Takeaway: With respect to WMAE, Random Forest and Light GBM have turned out to be the top performing base models and would be further evaluated.","3cd819a8":"train_scaled.drop(columns=['Temperature', 'Fuel_Price'], axis=1, inplace=True)\ntest_scaled.drop(columns=['Temperature', 'Fuel_Price'], axis=1, inplace=True)","6576ccc7":"### Evaluate Random Forest (Ensemble)","996acb70":"## 3. Feature Engineering","cc08cc51":"### Evaluate Light GBM (Boosting)","d3761293":"### Select and evaluate candidate models","e8c70fd7":"### Column #3: Week\nNew numeric column being created to replace YEAR. ","1ac9c409":"### Column #1: IsHoliday\nColumn has boolean values and would ned converted to numeric. ","318f5c76":"train=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","82955338":"train['MarkDown1'] = train['MarkDown1'].apply(lambda x: np.log(x + 1 + MarkDown1_min))\ntrain['MarkDown2'] = train['MarkDown2'].apply(lambda x: np.log(x + 1 + MarkDown2_min))\ntrain['MarkDown3'] = train['MarkDown3'].apply(lambda x: np.log(x + 1 + MarkDown3_min))\ntrain['MarkDown4'] = train['MarkDown4'].apply(lambda x: np.log(x + 1 + MarkDown4_min))\ntrain['MarkDown5'] = train['MarkDown5'].apply(lambda x: np.log(x + 1 + MarkDown5_min))\n\ntest['MarkDown1'] = test['MarkDown1'].apply(lambda x: np.log(x + 1 + MarkDown1_min))\ntest['MarkDown2'] = test['MarkDown2'].apply(lambda x: np.log(x + 1 + MarkDown2_min))\ntest['MarkDown3'] = test['MarkDown3'].apply(lambda x: np.log(x + 1 + MarkDown3_min))\ntest['MarkDown4'] = test['MarkDown4'].apply(lambda x: np.log(x + 1 + MarkDown4_min))\ntest['MarkDown5'] = test['MarkDown5'].apply(lambda x: np.log(x + 1 + MarkDown5_min))","772893c9":"###### Takeaways: \n1. Column DATE is non-numeric and is a candidate for pre-processing.\n2. 1285 records with Weekly Sales < 0\n3. Data spans years 2010, 2011 and 2012\n4. As suspected above, four stores [3, 5, 33 & 36] seem to have incorrectly classified as Type A & B. Average Weekly Sales for these stores is in line with the average for Type C. Hence, these would need to be reclassified as Type C.","2a5dbaf5":"### Column #2: Type\nColumn is categorical and would be converted to numeric via one-hot encoding. ","7777f0a8":"### Analyze Feature Correlation","0e5d920b":"### Split Training dataset into Train & Validation","fc104887":"MarkDown1_min = abs(min(train['MarkDown1'].min(),test['MarkDown1'].min()))\nMarkDown2_min = abs(min(train['MarkDown2'].min(),test['MarkDown2'].min()))\nMarkDown3_min = abs(min(train['MarkDown3'].min(),test['MarkDown3'].min()))\nMarkDown4_min = abs(min(train['MarkDown4'].min(),test['MarkDown4'].min()))\nMarkDown5_min = abs(min(train['MarkDown5'].min(),test['MarkDown5'].min()))","20ceec93":"## 1. Missing\/Incorrect Values","ded6b56b":"### Train Data | Negative Values for Weekly Sales","3e2d1201":"#### Tuned Model","bd84676b":"# Import Libraries, Datasets & Declare Functions","aa9d1dac":"model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                      importance_type='split', learning_rate=0.3, max_depth=-1,\n                      min_child_samples=5, min_child_weight=0.001, min_data_in_leaf=2,\n                      min_depth=3, min_split_gain=0.0, n_estimators=3000, n_jobs=-1,\n                      num_leaves=80, objective='regression', random_state=42,\n                      reg_alpha=0.1, reg_lambda=2, silent=True, subsample=1.0,\n                      subsample_for_bin=200000, subsample_freq=0,\n                      verbose=1)\n#Weighted MAE : 1324.72","0aa5628c":"###### Takeaways: \n1. Column DATE is non-numeric and is a candidate for pre-processing.\n2. Data spans years 2012 and 2013","c21d5ebb":"# Data Pre-Processing","017ec7c3":"#### Default Model","9fde85f8":"#### Columns: CPI and Unemployment\nAs noted above, columns are missing values for 3 months May, Jun & Jul 2013. Values from Apr 2019 would be propogated to records with missing values. ","32d8da7d":"# Data Exploration"}}