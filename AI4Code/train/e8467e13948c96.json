{"cell_type":{"b69ae0a7":"code","2e8dc6e4":"code","b2443edb":"code","5f3d1887":"code","e9a2dcda":"code","2d1e6e0d":"code","9a1aba63":"code","0760ca86":"code","e55e2100":"code","21892c8f":"code","a31966fd":"code","9b338256":"code","79437215":"code","ecbc05f9":"code","a4766cac":"code","74b76b0c":"code","5423fbc4":"code","aaa82dba":"code","54db97fc":"code","24fb62e5":"code","e95a2d4f":"code","8857bfe0":"code","3407724c":"code","e9303ad1":"code","7ec4c6fe":"code","0951954a":"code","ad6c7abd":"code","ebf526d7":"code","4e2bf363":"code","d4f4958a":"markdown"},"source":{"b69ae0a7":"import pandas as pd\nimport matplotlib.pyplot as plt","2e8dc6e4":"data = pd.read_csv('..\/input\/ISIS Religious Texts v1.csv', delimiter=',', encoding='latin-1')\ndata.head(8)","b2443edb":"pd.unique(data.Magazine)","5f3d1887":"data['Magazine'].value_counts()","e9a2dcda":"data.Magazine.isna().sum()","2d1e6e0d":"data[data.Magazine.isna()]","9a1aba63":"data.isna().sum()","0760ca86":"data.shape","e55e2100":"data = data.drop(['Purpose'], axis=1)\ndata.shape","21892c8f":"data.isna().sum()","a31966fd":"data = data.dropna()\ndata.sample(8)","9b338256":"pd.unique(data.Source)","79437215":"len(pd.unique(data.Source))","ecbc05f9":"len(pd.unique(data.Type))","a4766cac":"data['Type'].value_counts()","74b76b0c":"used_col = ['Quote','Magazine']\n\ndata = data[used_col]\ndata.sample(8)","5423fbc4":"data['Magazine_id'] = data['Magazine'].factorize()[0]\ndata.sample(8)","aaa82dba":"encoding_data, mapping_index = data['Magazine'].factorize()\nprint(encoding_data)\nprint(mapping_index)\n\nfor i in range(len(mapping_index)):\n    print(i,mapping_index[i])","54db97fc":"data.groupby(['Magazine'])['Quote'].count().plot.bar()","24fb62e5":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(\n    stop_words='english',\n    ngram_range=(1,2),\n    norm='l2',\n    encoding='latin-1',\n    min_df=5,\n    sublinear_tf=True\n)\n\nfeatures = tfidf.fit_transform(data['Quote']).toarray()\nlabels = data['Magazine_id']\n\nfeatures.shape","e95a2d4f":"# find the best model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.model_selection import cross_val_score\n\nmodels = [\n    LogisticRegression(random_state=0),\n    RandomForestClassifier(n_estimators=200,max_depth=3,random_state=0),\n    LinearSVC(),\n    MultinomialNB()\n]\n\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n    \n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\n\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\nimport seaborn as sns\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","8857bfe0":"cv_df.groupby(['model_name'])['accuracy'].mean().sort_values(ascending=False)","3407724c":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data['Quote'], \n    data['Magazine_id'], \n    test_size=0.3, \n    random_state=0\n)\n\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\n\ntfidf_transfomer = TfidfTransformer()\nX_train_tfidf = tfidf_transfomer.fit_transform(X_train_counts)\n\nclf = LogisticRegression().fit(X_train_tfidf, y_train)","e9303ad1":"sample1 = data.sample(1)\nprint(sample1.Magazine)\nprint(data.Quote[sample1.index[0]])","7ec4c6fe":"pred = clf.predict(count_vect.transform([data.Quote[sample1.index[0]]]))\nprint(mapping_index[pred][0])","0951954a":"sample2 = data.sample(1)\nprint(sample2.Magazine)\nprint(data.Quote[sample2.index[0]])","ad6c7abd":"pred = clf.predict(count_vect.transform([data.Quote[sample2.index[0]]]))\nprint(mapping_index[pred][0])","ebf526d7":"magazine_id_df = data[['Magazine', 'Magazine_id']].drop_duplicates().sort_values('Magazine_id')\nmagazine_to_id = dict(magazine_id_df.values)\nid_to_magazine = dict(magazine_id_df[['Magazine_id', 'Magazine']].values)\ndata.head()","4e2bf363":"model = LogisticRegression()\n\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n    features, labels, data.index, test_size=0.33, random_state=0\n)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(5,5))\n\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=magazine_id_df['Magazine'].values, \n            yticklabels=magazine_id_df['Magazine'].values)\n\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\nfrom sklearn.metrics import accuracy_score\n\nprint('accuracy: %s' % (accuracy_score(y_test, y_pred)))","d4f4958a":"============================================================"}}