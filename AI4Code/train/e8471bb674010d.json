{"cell_type":{"c1b0a322":"code","7be2ecb5":"code","7690007a":"code","126a2a7a":"code","882162a5":"code","11561e2e":"code","f31c72b7":"code","2916e0b1":"code","388f3ce4":"markdown","3b2ac792":"markdown","c9c4e76d":"markdown","e53b9f64":"markdown","3907f62a":"markdown","da2c19b3":"markdown","48cf0536":"markdown"},"source":{"c1b0a322":"# Used most of coding from this kernel \nimport optuna\nimport lightgbm as lgb\nimport pickle\n\nimport riiideducation\nimport dask.dataframe as dd\nimport  pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score","7be2ecb5":"# START_IDX = 80000000","7690007a":"train = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n                usecols=[1, 2, 3,4,7,8,9], dtype={'timestamp': 'int64', 'user_id': 'int32' ,'content_id': 'int16','content_type_id': 'int8','answered_correctly':'int8','prior_question_elapsed_time': 'float32','prior_question_had_explanation': 'boolean'}\n              )\ntrain = train[train.content_type_id == False]\n#arrange by timestamp\n\n\ntrain = train.sort_values(['timestamp'], ascending=True)\n\ntrain.drop(['timestamp','content_type_id'], axis=1,   inplace=True)\n\nresults_c = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\nresults_c.columns = [\"answered_correctly_content\"]\n\nresults_u = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum'])\nresults_u.columns = [\"answered_correctly_user\", 'sum']","126a2a7a":"#reading in question df\nquestions_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv',\n                            usecols=[0,1, 3,4],\n                            dtype={'question_id': 'int16',\n                              'part': 'int8','bundle_id': 'int8','tags': 'str'}\n                          )\ntag = questions_df[\"tags\"].str.split(\" \", n = 10, expand = True) \ntag.columns = ['tags1','tags2','tags3','tags4','tags5','tags6']\n\nquestions_df =  pd.concat([questions_df,tag],axis=1)\nquestions_df['tags1'] = pd.to_numeric(questions_df['tags1'], errors='coerce')\nquestions_df['tags2'] = pd.to_numeric(questions_df['tags2'], errors='coerce')\nquestions_df['tags3'] = pd.to_numeric(questions_df['tags3'], errors='coerce')\nquestions_df['tags4'] = pd.to_numeric(questions_df['tags4'], errors='coerce')\nquestions_df['tags5'] = pd.to_numeric(questions_df['tags5'], errors='coerce')\nquestions_df['tags6'] = pd.to_numeric(questions_df['tags6'], errors='coerce')","882162a5":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\ntrain['prior_question_had_explanation'].fillna(False, inplace=True)\ntrain[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(train[\"prior_question_had_explanation\"])","11561e2e":"file = '..\/input\/riiid-lgb-training-and-save-model\/trained_model.pkl'\nmodel = pickle.load(open(file, 'rb'))\nprint('Trained LGB model was loaded!')","f31c72b7":"env = riiideducation.make_env()","2916e0b1":"iter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = test_df.sort_values(['user_id','timestamp'], ascending=False)\n    test_df['answer_time'] = test_df.groupby(['user_id'])['prior_question_elapsed_time'].shift(1)\n    \n    test_df = pd.merge(test_df, results_u, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_c, on=['content_id'],  how=\"left\")    \n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')    \n    test_df['answered_correctly_user'].fillna(0.5, inplace=True)\n    test_df['answered_correctly_content'].fillna(0.5, inplace=True)\n    test_df['sum'].fillna(0, inplace=True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.transform(test_df[\"prior_question_had_explanation\"])\n    test_df['answered_correctly'] =  model.predict(test_df[['answered_correctly_user', 'answered_correctly_content', 'sum','bundle_id','part','prior_question_elapsed_time','prior_question_had_explanation_enc',\n                                                           'tags1','tags2','tags3']])\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","388f3ce4":"# Predictions","3b2ac792":"# Config","c9c4e76d":"# Model fitting\nI use a set of tuned hyperparameters used in my another kernel: [LGB hyperparameter tuning](https:\/\/www.kaggle.com\/code1110\/riiid-lgb-hyperparameter-tuning).","e53b9f64":"# Load data","3907f62a":"# Preprocess","da2c19b3":"The data size is huge, and thus it takes very long to get a submission score if we do training and inference in a single notebook. One way to avoid this long-waiting time is to split training and inference in separate notebooks. Here I demonstrate how to do it.\n\nIn this notebook only inference is done using the pretrained model shown in my another kernel: [LGB training and save model](https:\/\/www.kaggle.com\/code1110\/riiid-lgb-training-and-save-model?scriptVersionId=44905542).\n\nThis notebook is heavily based on \n\n- https:\/\/www.kaggle.com\/lgreig\/simple-lgbm-baseline\n- https:\/\/www.kaggle.com\/jsylas\/riiid-lgbm-starter\n\nPlease upvote these notebooks too.","48cf0536":"# Load Model\nWe can simply use pickle:)"}}