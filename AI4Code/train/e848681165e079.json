{"cell_type":{"c83ddc42":"code","c4bd111a":"code","c6a7dd8f":"code","544347cf":"code","c140a668":"code","734a35c4":"code","0fd87ec3":"code","25c11a31":"code","9c06e071":"code","64b81a16":"code","89399bf5":"code","bb024c7c":"code","b85b2a8f":"code","dc67b88e":"code","cbed7ca9":"code","a295ccc7":"code","feccc315":"code","25c18a4f":"code","e2d6f59c":"code","6cec3aad":"code","91fc25f1":"code","4a2d6ca2":"code","5128db9c":"code","0528c9fd":"code","576a522f":"code","405067e9":"code","e1d98f76":"code","f557b010":"code","537e163e":"code","399ef141":"markdown","db6dd250":"markdown","0449279a":"markdown","4d51564e":"markdown","6bf4edad":"markdown","b29e4d79":"markdown","4e1cd3a2":"markdown","7e36a3c6":"markdown","259387e1":"markdown","8d96546f":"markdown","a597199c":"markdown","a073a6d4":"markdown","c7c26457":"markdown","8cd810fb":"markdown","4fb34527":"markdown","64ec8ad1":"markdown","68668dbd":"markdown","714001f3":"markdown","dfed7c01":"markdown","9a553af9":"markdown","8de7a09d":"markdown","d857b1e9":"markdown","6c819920":"markdown","3d6a47b5":"markdown","0564d95b":"markdown","0b07e815":"markdown","767c3d1e":"markdown","6496103c":"markdown","fed43ac5":"markdown","bce4e367":"markdown","5b804170":"markdown"},"source":{"c83ddc42":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4bd111a":"df=pd.read_csv('\/kaggle\/input\/spam-mails-dataset\/spam_ham_dataset.csv')\ndf.head()","c6a7dd8f":"df.info()","544347cf":"df.isna().sum()","c140a668":"import seaborn as sns\nsns.countplot(x=\"label\",data=df)","734a35c4":"print(df['text'][0])","0fd87ec3":"!pip install contractions","25c11a31":"import contractions\n#contractions is the package in python used to expand the contractions in english language to their original form. Example: I'll to \"I will\"\nfrom tqdm import tqdm\n#tqdm package is used to track the progress of work. It displays the percentage of loop done.\nimport nltk\n#nltk is a suite of libraries that are mainly used for dealing with problems related to Natural language processing.\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n#donwloadin the stopwords of english language\nstopwords=stopwords.words('english')\n#Removing stopwords 'no','nor' and 'not'\nstopwords.remove('no')\nstopwords.remove('nor')\nstopwords.remove('not')","9c06e071":"processed_mails=[]\nfor i in tqdm(df['text']):\n    #Regular expression that removes all the html tags pressent in the reviews\n    i=re.sub('(<[\\w\\s]*\/?>)',\"\",i)\n    #Expanding all the contractions present in the review to is respective actual form\n    i=contractions.fix(i)\n    #Removing all the special charactesrs from the review text\n    i=re.sub('[^a-zA-Z0-9\\s]+',\"\",i)\n    #Removing all the digits present in the review text\n    i=re.sub('\\d+',\"\",i)\n    #Making all the review text to be of lower case as well as removing the stopwords and words of length less than 3\n    processed_mails.append(\" \".join([j.lower() for j in i.split() if j not in stopwords and len(j)>=3]))","64b81a16":"#Creating a new datafram using the Processed Reviews\nprocessed_df=pd.DataFrame({'text':processed_mails,'Spam\/Ham':list(df['label_num'])})\nprocessed_df.head()","89399bf5":"#Splitting the data into dependent and independent variables i.e, features and the target columns\nX=processed_df['text']\nY=processed_df['Spam\/Ham']\n#Splitting the data such that 33% will be used for testing and the remaining 67% will be used for training. \nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,stratify=Y,test_size=0.33)\n#when stratify is provided the splitting of data into train and test datasets agree with the composition of actual possitive and negative reviews present in the dataset","bb024c7c":"from gensim.models import Word2Vec","b85b2a8f":"words_in_sentences=[]\nfor i in tqdm(x_train):\n    words_in_sentences.append(i.split())","dc67b88e":"model = Word2Vec(sentences=words_in_sentences, vector_size=200,workers=-1)","cbed7ca9":"model.wv.most_similar('lottery', topn=10)","a295ccc7":"model.wv.get_vector('job')","feccc315":"vocab=list(model.wv.key_to_index.keys())\nprint(len(vocab))","25c18a4f":"def avg_w2vec(sentences):\n    transformed=[]\n    for sentence in tqdm(sentences):\n        count=0\n        vector=np.zeros(200)\n        for word in sentence.split():\n            if word in vocab:\n                vector+=model.wv.get_vector(word)\n                count+=1\n        if count!=0:\n            vector\/=count\n            transformed.append(vector)\n        else:\n            print(sentence)\n    return np.array(transformed)","e2d6f59c":"x_train_transformed=avg_w2vec(x_train)\nx_test_transformed=avg_w2vec(x_test)","6cec3aad":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\ngrid_params = { 'n_neighbors' : [10,20,30,40,50,60],\n               'metric' : ['manhattan']}\nknn=KNeighborsClassifier()\nclf = RandomizedSearchCV(knn, grid_params, random_state=0,n_jobs=-1,verbose=1)\nclf.fit(x_train_transformed,y_train)","91fc25f1":"clf.best_params_","4a2d6ca2":"clf.best_score_","5128db9c":"clf.cv_results_","0528c9fd":"from sklearn.metrics import roc_curve, auc,classification_report,confusion_matrix\ntrain_fpr,train_tpr,thresholds=roc_curve(y_train,clf.predict_proba(x_train_transformed)[:,1])\ntest_fpr,test_tpr,thresholds=roc_curve(y_test,clf.predict_proba(x_test_transformed)[:,1])","576a522f":"import matplotlib.pyplot as plt\nplt.plot(train_fpr,train_tpr,label=\"Training Accuracy=\"+str(round(auc(train_fpr, train_tpr),2)))\nplt.plot(test_fpr,test_tpr,label=\"Testing Accuracy =\"+str(round(auc(test_fpr, test_tpr),2)))\nplt.scatter(train_fpr,train_tpr,label=\"Training Accuracy=\"+str(round(auc(train_fpr, train_tpr),2)))\nplt.scatter(test_fpr,test_tpr,label=\"Testing Accuracy =\"+str(round(auc(test_fpr, test_tpr),2)))\nplt.legend()\nplt.xlabel(\"Thresholds\")\nplt.ylabel(\"ACCURACY\")\nplt.title(\"Training and Testing ROC Curves\")\nplt.show()","405067e9":"sns.heatmap(confusion_matrix(y_train,clf.predict(x_train_transformed)),annot=True)\nplt.show()","e1d98f76":"print(classification_report(y_train,clf.predict(x_train_transformed)))","f557b010":"sns.heatmap(confusion_matrix(y_test,clf.predict(x_test_transformed)),annot=True)\nplt.show()","537e163e":"print(classification_report(y_test,clf.predict(x_test_transformed)))","399ef141":"# **Conclusion**\n### In case of Spam mail Detection, we will more focus of False Possitives. This is so because, In case of False-Possitive the model will predict the unspam mail as a spam which results in loss of some useful data. If the ham mail that was recognised as spam contain any important information, it will not be reached to the user.\n### The model that was obtained in this notebook has very less False possitive when compare to False Negatives.","db6dd250":"## Plotting the ROC curve for the prediction on both Training and Testing Data","0449279a":"# <center style=\"color:red\"> <b> &#127881;&#127882;<u> Upvote the notebook if it is useful and Informative. <\/u>&#127881; &#127882;<\/b><\/center>","4d51564e":"<center>\n    <img src=\"https:\/\/lh3.googleusercontent.com\/proxy\/5nM_9EbJxrsKFCdJrhx76zjWyK8L9XNT5ROMU3FS-YutloIdQU0dLNAg8AJEjoCKWc-OBerjmgYvTgZBSds_ADOy\"\/>\n<\/center>","6bf4edad":"## <center> Observing Data<\/center>","b29e4d79":"<center>\n    <img src=\"https:\/\/miro.medium.com\/max\/1400\/1*f-k9vMJGKqctXjvO8dM-UQ.gif\"\/>\n<\/center>","4e1cd3a2":"## Confusion Matrix and Classification Report for Prediction made on Training and Testing data","7e36a3c6":"## Using K-Nearest Neighbors Algorithm to predict whether a given mail is Spam or not","259387e1":"#### Word2Vec is a neural network model, that provides a numerical vector representation for a given word. This numerical vector is often called as \"Word Embedding\". Despite other methods of Bag of Words and TF-IDF, Word2Vec takes the context of the word into consideration while converting a word to a numerical vector. ","8d96546f":"Word2Vec model takes a list of lists as input for training. In our case, every mail should be divided into a list of words and all such lists must be appended to an empty list so that the data is ready for training.","a597199c":"## Creating the data for training the Word2Vec model","a073a6d4":"### Training a Word2Vec Model with our Custom data","c7c26457":"## Total number of words in the Vocabulary on which the Word2Vec model is trained.","8cd810fb":"# <center><u> Spam Email Detection Using Average Word2Vec <\/u><\/center>","4fb34527":"### Looking at Top 5 records in the Data","64ec8ad1":"<center>\n    <img src=\"http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/Knn_k1_z96jba.png\"\/>\n    <\/center>","68668dbd":"## Printing all similar words in the vocabulary for a given random word present in our vocabulary","714001f3":"## Average Word2Vec for converting a given Sentence into numerical vector. \n\nNote: Word2Vec is capable of providing an embedding for a given word but not for a sentence","dfed7c01":"## <center> Data Preparation <\/center>","9a553af9":"## Word Embedding for a given word","8de7a09d":"### Composition of Spam and Ham mails in the Data using simple CountPlot of Seaborn","d857b1e9":"# <center><b> Model Evaluation <\/b><\/center>","6c819920":"### Creating a Dataframe using all the cleaned Mails","3d6a47b5":"## <center> Data Cleaning <\/center>","0564d95b":"<center>\n<img src=\"https:\/\/exceldashboardschool.com\/wp-content\/uploads\/2020\/01\/how-to-clean-data-in-excel-the-ultimate-guide.png\">\n<\/center>","0b07e815":"### Checking Whether the Data contains any Null Values","767c3d1e":"<center>\n    <img src=\"https:\/\/i0.wp.com\/yaronvazana.com\/wp-content\/uploads\/2018\/09\/average-vectors.png?resize=698%2C354\">\n<\/center>","6496103c":"<center>\n    <img src=\"https:\/\/devopedia.org\/images\/article\/221\/4080.1570464995.png\"\/>\n<\/center>","fed43ac5":"<center>\n    <img src=\"https:\/\/miro.medium.com\/max\/1105\/1*nBgCTU_hAVG00eYkcRf6Mw.png\">\n<\/center>","bce4e367":"### Understanding the datatypes of the columns in the dataframe","5b804170":"### **Average Word2Vec is a technique in which the average of word embeddings of all the words given in a sentence is used as the numerical vector for a given sentence.**"}}