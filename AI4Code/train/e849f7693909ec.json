{"cell_type":{"2f1cf6a1":"code","9c451002":"code","73584557":"code","fd52ab66":"code","bc4257a3":"code","4864a470":"code","b45b05df":"code","34f48eb7":"code","b5d81209":"code","e5c6bf74":"code","8e0f92aa":"code","236685c5":"code","bdc0d961":"code","b5da8dcc":"code","52b2256c":"code","ccb29f4e":"code","019af356":"code","a24aa4c6":"code","2dfac8ca":"code","a984733a":"code","01654faa":"code","eb19b92e":"code","b2fbd8a0":"code","811125a8":"code","29936692":"code","4536cb30":"code","bc1f44e9":"code","1a6878a1":"code","41fae3f4":"code","285714b4":"code","887bff47":"code","c43c1e29":"code","99beed2f":"code","682ceda7":"code","5f50b00f":"code","4928a21a":"code","3da7e78b":"code","53aec735":"code","8a265ad1":"code","07477ad4":"code","e7bd3932":"code","85a4b0da":"code","5c6cb697":"code","8416693e":"code","36a84a50":"code","33f913f7":"code","d17fbdb3":"code","44d1092c":"code","10013625":"code","273d5c8c":"code","b2d3dc0a":"code","05a6a94d":"code","28ddedcc":"code","8ff997b8":"code","cd69aba2":"code","ae9764a6":"code","666a5a70":"code","c5e0e91d":"code","eab2a047":"code","28e1fd78":"code","af18cc8b":"code","1ca51224":"code","1f6ed23d":"code","b07ae99d":"code","772e2b49":"code","f867f1a8":"code","e6f18622":"code","443f81b6":"code","b24cea23":"code","52e01efd":"code","946ff820":"code","799cbd40":"code","b8248f2a":"code","30892990":"code","5ef3370d":"code","dad4a8b7":"code","5babf426":"code","a82cba81":"code","7ddf0d6b":"code","e3fec140":"code","66eaca66":"code","bd38d00e":"code","f1cef0d8":"code","720e7ca9":"code","a1610050":"code","1677871c":"code","9b96adc5":"code","dd290738":"code","a639b400":"code","52e23d3f":"code","b6965f72":"code","49d0a5fd":"code","0d173722":"code","beb9c0bf":"markdown","e01d1d2d":"markdown","6c97ab81":"markdown","4422d615":"markdown","75d38e5b":"markdown","bbdc1a18":"markdown","332b3f68":"markdown","ae1687ab":"markdown","28fe99ec":"markdown","c7148626":"markdown","231fb2d3":"markdown","ded6ee08":"markdown","8565544c":"markdown","070241e4":"markdown","bddb08d8":"markdown","a309962c":"markdown","e0441780":"markdown","acc540e4":"markdown","637d53a1":"markdown","5ce2069b":"markdown","7320c788":"markdown","fdfc692c":"markdown","582b56e9":"markdown","0e58d0cf":"markdown","e1c016b8":"markdown","a7d1651e":"markdown","cea4adb3":"markdown","713e528e":"markdown","1d272ce0":"markdown","39c53124":"markdown","65d5c95d":"markdown","d6a9d382":"markdown","38c3122e":"markdown","164899c7":"markdown","75275ecb":"markdown","d2ae271f":"markdown","3fb3a13e":"markdown","e89afdfe":"markdown","482cd64d":"markdown","fd0088f1":"markdown","5f0c8e6f":"markdown","4b73ba20":"markdown","30aa8f3f":"markdown","01bfaf4b":"markdown","65324616":"markdown","a9ba8999":"markdown","561d7250":"markdown","a003b906":"markdown","6f262b08":"markdown","dc6e7573":"markdown","1f3a1512":"markdown","1e3bd365":"markdown","a557ca5f":"markdown","582eb9c3":"markdown","b84a3f89":"markdown","a0daa1ee":"markdown","73ccd575":"markdown","fbb0d8a0":"markdown","439e3c44":"markdown","5044f3b6":"markdown","ea4b201d":"markdown","edc4a5d5":"markdown","53a6f4ac":"markdown","b4764c19":"markdown","866d8eea":"markdown","57996ad3":"markdown","ea0afd46":"markdown","40ff3e25":"markdown","0c4c4aff":"markdown","97cec823":"markdown","c6744d1e":"markdown","b07c15b4":"markdown","b95d98fd":"markdown","e717ce95":"markdown","f150ccd7":"markdown","bda09e31":"markdown","040eb847":"markdown","5ab6340b":"markdown","d3cb6460":"markdown","1c462c55":"markdown"},"source":{"2f1cf6a1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import preprocessing\nfrom IPython.core.interactiveshell import InteractiveShell\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n# from collections import defaultdicta\nfrom sklearn.preprocessing import LabelEncoder","9c451002":"#Reading Data\nbike = pd.read_csv(\"..\/input\/bikes-dataset\/day.csv\", parse_dates=['dteday'])","73584557":"#Check the data shape\nbike.shape","fd52ab66":"bike.corr().min()","bc4257a3":"bike_dup_check = bike.drop_duplicates()\nbike_dup_check.shape","4864a470":"bike.head()","b45b05df":"# checking the info of dataframe\nbike.info()","34f48eb7":"#Checking statistics info\nbike.describe()","b5d81209":"#Checking the percentage of missing data in all columns\nbike.isnull().sum()\/len(bike.index)","e5c6bf74":"bike.columns","8e0f92aa":"bike_new_data=bike[['season', 'yr', 'mnth', 'holiday', 'weekday',\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n       'cnt']]\n\nbike_new_data.info()","236685c5":"# Convert to 'category' data type\n\nbike_new_data['season']=bike_new_data['season'].astype('category')\nbike_new_data['weathersit']=bike_new_data['weathersit'].astype('category')\nbike_new_data['mnth']=bike_new_data['mnth'].astype('category')\nbike_new_data['weekday']=bike_new_data['weekday'].astype('category')\nbike_new_data.info()","bdc0d961":"# This code does 3 things:\n# 1) Create Dummy variable\n# 2) Drop original variable for which the dummy was created\n# 3) Drop first dummy variable for each set of dummies created.\n\nbike_new_data = pd.get_dummies(bike_new_data, drop_first=True)\n\n\n# We will visualise the changes after the dummy creation.\n\nbike_new_data.info()","b5da8dcc":"bike_new_data.shape","52b2256c":"# Creating a new df for just showcasing the feature engineering capability\nbike_feat_eng=bike","ccb29f4e":"bike_feat_eng['weekno']=bike_feat_eng['dteday'].dt.week","019af356":"bike_feat_eng['rc_ratio']=round((bike['registered']\/bike_feat_eng['casual']),1)","a24aa4c6":"bike_feat_eng['temp_ratio']=round(bike_feat_eng['atemp']\/bike_feat_eng['temp'],1)","2dfac8ca":"bike_feat_eng['wci']=(bike_feat_eng['atemp']+bike_feat_eng['hum'])\/4","a984733a":"bike_feat_eng.head()","01654faa":"# Check the shape before spliting\n\nbike_new_data.shape","eb19b92e":"# Check the info before spliting\n\nbike_new_data.info()","b2fbd8a0":"from sklearn.model_selection import train_test_split\n\n# We specify 'random_state' so that the train and test data set always have the same rows, respectively\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(bike_new_data, train_size = 0.70, test_size = 0.30, random_state = 333)","811125a8":"df_train.shape","29936692":"df_train.info()","4536cb30":"df_test.shape","bc1f44e9":"df_test.info()","1a6878a1":"# Create a new dataframe of only numeric variables:\n\nbike_num=df_train[[ 'temp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bike_num , diag_kind='kde')\nplt.show()","41fae3f4":"df_train.info()","285714b4":"# Build boxplot of all categorical variables (before creating dummies) againt the target variable 'cnt' \n# to see how each of the predictor variable stackup against the target variable.\n\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bike)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bike)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bike)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bike)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bike)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bike)\nplt.show()","887bff47":"# Let's check the correlation coefficients to see which variables are highly correlated. Note:\n# here im considering only those variables (dataframe: bike_new_data) that were chosen for analysis\n\nplt.figure(figsize = (25,20))\nsns.heatmap(bike_new_data.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","c43c1e29":"from sklearn.preprocessing import MinMaxScaler","99beed2f":"scaler = MinMaxScaler()","682ceda7":"# Checking the values before scaling\ndf_train.head()","5f50b00f":"df_train.columns","4928a21a":"# Apply scaler() to all the numeric variables\n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","3da7e78b":"# Checking values after scaling\ndf_train.head()","53aec735":"df_train.describe()","8a265ad1":"y_train = df_train.pop('cnt')\nX_train = df_train","07477ad4":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","e7bd3932":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","85a4b0da":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","5c6cb697":"col = X_train.columns[rfe.support_]\ncol","8416693e":"X_train.columns[~rfe.support_]","36a84a50":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","33f913f7":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d17fbdb3":"import statsmodels.api as sm\n\n# Add a constant\nX_train_lm1 = sm.add_constant(X_train_rfe)\n\n# Create a first fitted model\nlr1 = sm.OLS(y_train, X_train_lm1).fit()","44d1092c":"# Check the parameters obtained\n\nlr1.params","10013625":"# Print a summary of the linear regression model obtained\nlr1.summary()","273d5c8c":"X_train_new = X_train_rfe.drop([\"atemp\"], axis = 1)","b2d3dc0a":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","05a6a94d":"# Add a constant\nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr2 = sm.OLS(y_train, X_train_lm2).fit()","28ddedcc":"# Check the parameters obtained\n\nlr2.params","8ff997b8":"# Print a summary of the linear regression model obtained\nlr2.summary()","cd69aba2":"X_train_new = X_train_new.drop([\"hum\"], axis = 1)","ae9764a6":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","666a5a70":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c5e0e91d":"# Add a constant\nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr3 = sm.OLS(y_train, X_train_lm3).fit()","eab2a047":"# Check the parameters obtained\n\nlr3.params","28e1fd78":"# Print a summary of the linear regression model obtained\nlr3.summary()","af18cc8b":"X_train_new = X_train_new.drop([\"season_3\"], axis = 1)","1ca51224":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1f6ed23d":"# Add a constant\nX_train_lm4 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr4 = sm.OLS(y_train, X_train_lm4).fit()","b07ae99d":"# Check the parameters obtained\n\nlr4.params","772e2b49":"# Print a summary of the linear regression model obtained\nlr4.summary()","f867f1a8":"X_train_new = X_train_new.drop([\"mnth_10\"], axis = 1)","e6f18622":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","443f81b6":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b24cea23":"# Add a constant\nX_train_lm5 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr5 = sm.OLS(y_train, X_train_lm5).fit()","52e01efd":"# Check the parameters obtained\n\nlr5.params","946ff820":"# Print a summary of the linear regression model obtained\nlr5.summary()","799cbd40":"X_train_new = X_train_new.drop([\"mnth_3\"], axis = 1)","b8248f2a":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","30892990":"# Add a constant\nX_train_lm6 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr6 = sm.OLS(y_train, X_train_lm6).fit()","5ef3370d":"# Check the parameters obtained\n\nlr6.params","dad4a8b7":"# Print a summary of the linear regression model obtained\nlr6.summary()","5babf426":"y_train_pred = lr6.predict(X_train_lm6)","a82cba81":"res = y_train-y_train_pred","7ddf0d6b":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((res), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","e3fec140":"Text(0.5, 0, 'Errors')","66eaca66":"bike_new_data=bike_new_data[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bike_num, diag_kind='kde')\nplt.show()","bd38d00e":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f1cef0d8":"# Apply scaler() to all numeric variables in test dataset. Note: we will only use scaler.transform, \n# as we want to use the metrics that the model learned from the training data to be applied on the test data. \n# In other words, we want to prevent the information leak from train to test dataset.\n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","720e7ca9":"df_test.head()","a1610050":"df_test.describe()\n","1677871c":"y_test = df_test.pop('cnt')\nX_test = df_test\nX_test.info()","9b96adc5":"#Selecting the variables that were part of final model.\ncol1=X_train_new.columns\n\nX_test=X_test[col1]\n\n# Adding constant variable to test dataframe\nX_test_lm6 = sm.add_constant(X_test)\n\nX_test_lm6.info()","dd290738":"# Making predictions using the final model (lr6)\n\ny_pred = lr6.predict(X_test_lm6)","a639b400":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","52e23d3f":"# Plotting y_test and y_pred to understand the spread\n# import matplotlib.pyplot as plt\n# import numpy as np\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) ","b6965f72":"# We already have the value of R^2 (calculated in above step)\n\nr2=0.8203092200749708","49d0a5fd":"# Get the shape of X_test\nX_test.shape","0d173722":"# n is number of rows in X\nn = X_test.shape[0]\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nadjusted_r2","beb9c0bf":"## Building Linear Model using 'STATS MODEL'","e01d1d2d":"####  Residual Analysis Of Training Data","6c97ab81":"#### VIF Check","4422d615":"# Final Model Interpretation ","75d38e5b":"# MODEL EVALUATION","bbdc1a18":"Formula for Adjusted R^2\n\nR2adj.=1\u2212(1\u2212R2)\u2217n\u22121n\u2212p\u22121","332b3f68":"## Hypothesis Testing:","ae1687ab":"F-Statistics is used for testing the overall significance of the Model: Higher the F-Statistics, more significant the Model is.","28fe99ec":"# Bike Sharing Assignment(Linear Regression model)","c7148626":"INSIGHT\/CONCLUSION: - From the lr6 model summary, it is evident that all our coefficients are not equal to zero. which means We REJECT the NULL HYPOTHESIS ---","231fb2d3":"### The equation of best fitted surface based on model lr6:","ded6ee08":"## Weather Comfort Index:\nAn arbitrary index of the suitability of environmental conditions to physical activity.\n\nIt is calculated using the formula --> Comfort Index = (temperature + relative humidity)\/4.","8565544c":"#- Verify the info and shape of the dataframes after split: ---","070241e4":"## Visualising Catagorical Variables","bddb08d8":"NOTE: - Removing the variable 'atemp' based on its High p-value & High VIF ---","a309962c":"#### VIF Check","e0441780":"# ## VIF Check","acc540e4":"CONCLUSION: - The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable. - We will refer this map back-and-forth while building the linear model so as to validate different correlated values along with VIF & p-value, for identifying the correct variable to select\/eliminate from the model. ---","637d53a1":"INSIGHT\/CONCLUSION: - From the above histogram, we could see that the Residuals are normally distributed. Hence our assumption for Linear Regression is valid. ---","5ce2069b":"Now that we have fitted the model and checked the assumptions, it's time to go ahead and make predictions using the final model (lr6)","7320c788":"### Applying the scaling on the test sets\n","fdfc692c":"## Adjusted R^2 Value for TEST\n","582b56e9":"## Feeling to Actual temperature ratio:\nWe will calculate the Feeling (atemp) to actual (temp) temperature ratio from the columns provided","0e58d0cf":"## BUILDING A LINEAR MODEL\nDividing into X and Y sets for the model building","e1c016b8":"### Error terms are normally distributed with mean zero (not X, Y)\n","a7d1651e":"### There is No Multicollinearity between the predictor variables\n","cea4adb3":"As per our final Model, the top 3 predictor variables that influences the bike booking are: --- temp,weathersit_3,Year (yr)\n- Temperature (temp,) - A coefficient value of \u20180.5636\u2019 indicated that a unit increase in temp variable increases the bike hire numbers by 0.5636 units. \n- Weather Situation 3 (weathersit_3) - A coefficient value of \u2018-0.3070\u2019 indicated that, w.r.t Weathersit1, a unit increase in Weathersit3 variable decreases the bike hire numbers by 0.3070 units. \n- Year (yr) - A coefficient value of \u20180.2308\u2019 indicated that a unit increase in yr variable increases the bike hire numbers by 0.2308 units. --- SO IT IS RECOMMENDED TO GIVE THESE VARIABLES UTMOST IMPORTANCE WHILE PLANNING, TO ACHIEVE MAXIMUM BOOKING.  \n\nThe next best features that can also be considered are - - \n- season_4: - A coefficient value of \u20180.128744\u2019 indicated that w.r.t season_1, a unit increase in season_4 variable increases the bike hire numbers by 0.128744 units. - \n- windspeed: - A coefficient value of \u2018-0.155191\u2019 indicated that, a unit increase in windspeed variable decreases the bike hire numbers by 0.155191 units. --- \n\nNOTE: - The details of weathersit_1 & weathersit_3 - weathersit_1: Clear, Few clouds, Partly cloudy, Partly cloudy - weathersit_3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds - The details of season1 & season4 - season1: spring - season4: winter","713e528e":"### There is a linear relationship between X and Y\n","1d272ce0":"RESCALING THE FEATURES","39c53124":"### F Statistics","65d5c95d":"F-statistic: 233.8\nProb (F-statistic): 3.77e-181","d6a9d382":"\n# Problem Statement\n- A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\n- A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\n- In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\n- They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands\n- Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","38c3122e":"NOTE: - Removing the variable 'season3' based on its Very High 'VIF' value. - Even though the VIF of season3 is second highest, we decided to drop 'season3' and not 'temp' based on general knowledge that temperature can be an important factor for a business like bike rentals, and wanted to retain 'temp'. ---","164899c7":"Based on the high level look at the data and the data dictionary, the following variables can be removed from further analysis: - instant : Its only an index value - dteday : This has the date, Since we already have seperate columns for 'year' & 'month' we could live without this column. - casual & registered : Both these columns contains the count of bike booked by different categories of customers. Since our objective is to find the total count of bikes and not by specific categoy, we will ignore these two columns. More over, we have created a new variable to have the ratio of these customer types. - We will save the new dataframe as bike_new_data, so that the original dataset is preserved for any future analysis\/validation ---","75275ecb":"# Correlation Matrix","d2ae271f":"## lr6 model coefficient values\n\n    const 0.084143\n    yr 0.230846\n    workingday 0.043203\n    temp 0.563615\n    windspeed -0.155191\n    season_2 0.082706\n    season_4 0.128744\n    mnth_9 0.094743\n    weekday_6 0.056909\n    weathersit_2 -0.074807\n    weathersit_3 -0.306992","3fb3a13e":"We will create DUMMY variables for 4 categorical variables 'mnth', 'weekday', 'season' & 'weathersit'. - Before creating dummy variables, we will have to convert them into 'category' data types. ---","e89afdfe":"Conclusion: There is no missing values in the data","482cd64d":"    temp: A coefficient value of \u20180.5636\u2019 indicated that a unit increase in temp variable, increases the bike hire numbers by 0.5636 units.\n    \n    weathersit_3: A coefficient value of \u2018-0.3070\u2019 indicated that, w.r.t Weathersit1, a unit increase in Weathersit3 variable, decreases the bike hire numbers by 0.3070 units.\n    \n    yr: A coefficient value of \u20180.2308\u2019 indicated that a unit increase in yr variable, increases the bike hire numbers by 0.2308 units.\n    season_4: A coefficient value of \u20180.128744\u2019 indicated that w.r.t season_1, a unit increase in season_4 variable increases the bike hire numbers by 0.128744 units.\n    windspeed: A coefficient value of \u2018-0.155191\u2019 indicated that, a unit increase in windspeed variable decreases the bike hire numbers by 0.155191 units.\n    workingday: A coefficient value of \u20180.043203\u2019 indicated that, a unit increase in workingday variable increases the bike hire numbers by 0.043203 units.\n    season_2: A coefficient value of \u20180.082706\u2019 indicated that w.r.t season_1, a unit increase in season_2 variable decreases the bike hire numbers by 0.082706 units.\n    mnth_9: A coefficient value of \u20180.094743\u2019 indicated that w.r.t mnth_1, a unit increase in mnth_9 variable increases the bike hire numbers by 0.094743 units.\n    weekday_6: A coefficient value of \u20180.056909\u2019 indicated that w.r.t weekday_1, a unit increase in weekday_6 variable increases the bike hire numbers by 0.056909 units.\n        weathersit_2: A coefficient value of \u2018-0.074807\u2019 indicated that, w.r.t Weathersit1, a unit increase in Weathersit2 variable, decreases the bike hire numbers by 0.074807 units.\n    const: The Constant value of \u20180.084143\u2019 indicated that, in the absence of all other predictor variables (i.e. when x1,x2...xn =0), The bike rental can still increase by 0.084143 units.","fd0088f1":"NOTE: - Removing the variable 'mnth_3' based on its High 'p-value'. ---","5f0c8e6f":"NOTE: - Removing the variable 'mnth_10' based on its Very High p-value. ---","4b73ba20":"## Dividing into X_test and y_test\n","30aa8f3f":"conclusions: - Using the pair plot, we could see there is a linear relation between temp and atemp variable with the predictor \u2018cnt\u2019. ---","01bfaf4b":"## Final Result Comparison","65324616":"## Model 2","a9ba8999":"## VALIDATE ASSUMPTIONS\n","561d7250":"COnclusion: - This model looks good, as there seems to be VERY LOW Multicollinearity between the predictors and the p-values for all the predictors seems to be significant. For now, we will consider this as our final model (unless the Test data metrics are not significantly close to this number). ---","a003b906":"#### INSIGHT\/CONCLUSION: There were 6 categorical variables in the dataset. \nI used Box plot (refer the fig above) to study their effect on the dependent variable (\u2018cnt\u2019) . The inference that I could derive were: - season: Almost 32% of the bike booking were happening in season3 with a median of over 5000 booking (for the period of 2 years). This was followed by season2 & season4 with 27% & 25% of total booking. \n\nThis indicates, season can be a good predictor for the dependent variable. - mnth: Almost 10% of the bike booking were happening in the months 5,6,7,8 & 9 with a median of over 4000 booking per month. This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable. - weathersit: Almost 67% of the bike booking were happening during \u2018weathersit1 with a median of close to 5000 booking (for the period of 2 years). \n\nThis was followed by weathersit2 with 30% of total booking. This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable. - holiday: Almost 97.6% of the bike booking were happening when it is not a holiday which means this data is clearly biased. \n\nThis indicates, holiday CANNOT be a good predictor for the dependent variable. - weekday: weekday variable shows very close trend (between 13.5%-14.8% of total booking on all days of the week) having their independent medians between 4000 to 5000 bookings. This variable can have some or no influence towards the predictor. \n\nI will let the model decide if this needs to be added or not. - workingday: Almost 69% of the bike booking were happening in \u2018workingday\u2019 with a median of close to 5000 booking (for the period of 2 years). This indicates, workingday can be a good predictor for the dependent variable. ---","6f262b08":"Model 1","dc6e7573":"### Model 4","1f3a1512":"### VIF Check","1e3bd365":"Splitting the data to Train and Test: - We will now split the data into TRAIN and TEST (70:30 ratio) - We will use train_test_split method from sklearn package for this ---","a557ca5f":"Hypothesis testing states that:\nH0:B1=B2=...=Bn=0 \nH1:  at least one  Bi!=0","582eb9c3":"### VIF Check","b84a3f89":"# Registered to Casual user ratio:\nWe will calculate the registered to casual user ratio from the columns (registered & casual) provided","a0daa1ee":"## Creating Dummy VAriables","73ccd575":"## Removing the unwanted columns","fbb0d8a0":"INSIGHT\/CONCLUSION: - From the VIF calculation we could find that there is no multicollinearity existing between the predictor variables, as all the values are within permissible range of below 5. ---","439e3c44":"# IMPORTING  PACKAGES","5044f3b6":"## Week number:\nWe will extract the week number (1-52) from the date (dteday) column provided","ea4b201d":"## Quarter:\nWe will calculate the Quarter from the month (mnth) column provided","edc4a5d5":"# SPLITTING THE DATA","53a6f4ac":"### Model 6","b4764c19":"# Business Problem\nRequired to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.","866d8eea":"## MAKING PREDICTION USING FINAL MODEL\nNow that we have fitted the model and checked the assumptions, it's time to go ahead and make predictions using the final model (lr6)","57996ad3":"### Model 5","ea0afd46":"## Feature Engineering\nWe could create various new features that could be useful for this analysis as below. But, I am only mentioning the possible methods for the same.","40ff3e25":"## DATA UNDERSTANDING","0c4c4aff":"##  R^2 Value for TEST\n","97cec823":"# Interpretation of Coefficients:\n","c6744d1e":"## FINAL REPORT","b07c15b4":"## Model 3","b95d98fd":"# RFE\n    Recursive feature elimination: We will be using the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)","e717ce95":"FINAL RESULT COMPARISON: --- \n- Train R^2 :0.824 - Train Adjusted R^2 :0.821\n- Test R^2 :0.820 - Test Adjusted R^2 :0.812\n- This seems to be a really good model that can very well 'Generalize' various datasets. ---","f150ccd7":"Conclusion : data shape of bike_dup_check and bike is same.So no duplicates in the data","bda09e31":"NOTE: - Removing the variable 'hum' based on its Very High 'VIF' value. - Even though the VIF of hum is second highest, we decided to drop 'hum' and not 'temp' based on general knowledge that temperature can be an important factor for a business like bike rentals, and wanted to retain 'temp'. ---","040eb847":"cnt=0.084143+(yr\u00d70.230846)+(workingday\u00d70.043203)+(temp\u00d70.563615)\u2212(windspeed\u00d70.155191)+(season2\u00d70.082706)+(season4\u00d70.128744)+(mnth9\u00d70.094743)+(weekday6\u00d70.056909)\u2212(weathersit2\u00d70.074807)\u2212(weathersit3\u00d70.306992)","5ab6340b":"# Data Check for data cleaning requirement","d3cb6460":"The F-Statistics value of 233 (which is greater than 1) and the p-value of '~0.0000' states that the overall model is significant","1c462c55":"### VIF Check"}}