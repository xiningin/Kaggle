{"cell_type":{"491ecba3":"code","7f50ef1e":"code","4cfc8292":"code","48c3f9cb":"code","d848dea7":"code","96f6dc43":"code","3dd041f4":"code","5697706e":"code","5826553b":"code","353c50da":"code","a2035a9f":"code","790553b3":"code","4ab0a5d5":"code","ee6ea03a":"code","16e8dba9":"code","0104419d":"code","9bc315a8":"code","b42ebdc8":"code","349c4ba1":"code","d2d18f19":"code","87f17f1a":"code","a5c5b14a":"code","5f8e542e":"code","57a838f0":"code","f5532899":"code","a6475aa9":"code","1975157c":"code","e7fe2aee":"code","9bb589fe":"code","7cbe529c":"code","dc2e6749":"code","5d098700":"code","80369efe":"code","993c5cc0":"code","d674f932":"code","ceb3d14c":"code","96eb198c":"code","c509e9f4":"code","6e26611d":"code","c9d0b6bb":"code","3657773d":"code","cd384e76":"code","443f0a50":"code","a45e50e3":"code","3e71fc12":"code","e1722d0a":"code","34406620":"code","9982f7ff":"code","85d3169a":"code","a38bee9c":"code","e9bf420f":"code","4882e031":"code","2c7afc19":"code","e12807b3":"code","cff68e86":"code","bfd50507":"code","0096ee22":"code","b10ec143":"code","5c0e0839":"code","5181f628":"code","5a3017fd":"code","e455ddd7":"code","d10d73c2":"code","a994571d":"code","d2169a77":"code","b26f3a13":"code","9f59d885":"code","fb338868":"code","47b3b5fb":"code","fd399b51":"code","7a23d679":"code","8aa687e9":"code","91754cd9":"code","6754c7de":"code","3249f590":"code","4a920317":"code","58c6c10e":"code","50f92312":"code","9e91d1a4":"code","64b4b7ee":"code","45782dea":"code","8e4b820b":"code","0681636b":"code","84f3dc9f":"code","e44a26c9":"code","0bc5f1a5":"code","dc9f734c":"code","45e88eaf":"code","5ecadf35":"code","da71f4de":"code","8fad3822":"code","2f629b5d":"code","13603072":"code","17176dea":"code","ba3afbfb":"code","ac021d89":"code","f425f102":"code","ccd476a8":"code","360af434":"markdown","12279ce7":"markdown","23e4bc88":"markdown","beecbede":"markdown","e4be090f":"markdown","3044b508":"markdown","a07bfd45":"markdown","ce26e66b":"markdown","be0b224a":"markdown","f490c0b9":"markdown","8050eba2":"markdown","cdd23c3e":"markdown","c65e06a4":"markdown","6b1044c5":"markdown","2156088b":"markdown","509f52f6":"markdown","157390c8":"markdown","f2eb77e0":"markdown","86bc5593":"markdown","c35feb98":"markdown","58d4e7e6":"markdown","199ba39f":"markdown","a5bfc1fd":"markdown","50b3e46c":"markdown","764bb1e7":"markdown","0c831279":"markdown","197f3bd3":"markdown","2f26261b":"markdown","de2bf2ac":"markdown","cb7d7aaf":"markdown"},"source":{"491ecba3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f50ef1e":"# load event dataset.\ne_df=pd.read_csv('\/kaggle\/input\/ecommerce-dataset\/events.csv')\ne_df.head()","4cfc8292":"# view the number of data\nprint(\"This data contains \",e_df.shape[0], \"cases of user activities in e-commerce website\")","48c3f9cb":"# view missing values in each columns\ne_df.isnull().value_counts()","d848dea7":"# view counts by event type\ne_df.event.value_counts()","96f6dc43":"# see if there is any missing transaction id when event is 'transaction'\ne_df.transactionid[e_df.event=='transaction'].isnull().value_counts()","3dd041f4":"# remove duplicates if any. (all columns' values are idential)\nprint('Number of rows before removing duplicates: ', e_df.shape[0])\nmsk=e_df.duplicated()\ne_df=e_df[~msk]\ne_df.reset_index(drop=True, inplace=True)\nprint('Number of rows after removing duplicates: ', e_df.shape[0])","5697706e":"# convert unix timestamp to readable dates (GMT)te\n# separate the 'ordinary' timestamp and the milliseconds\nlist=[]\nfor i, unix in enumerate(e_df['timestamp']):\n    timestamp, ms = divmod(unix, 1000)\n\n    # create the datetime from the timestamp \n    # add the milliseconds separately\n    dt = datetime.datetime.fromtimestamp(timestamp) + datetime.timedelta(milliseconds=ms)\n\n    formatted_time = dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n    list.append(formatted_time)\n\ne_df['date_time']=pd.DataFrame({'date_time':list})\ne_df.head()\n","5826553b":"print('Start Date of Dataset: ', e_df['date_time'].min())\nprint('End Date of Dataset: ', e_df['date_time'].max())","353c50da":"# distribution by event\n# plot\ntotalcases=e_df.shape[0]\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nax1.hist(e_df['event'], bins=3)\nax1.set_xlabel('Type of Events')\nax1.set_ylabel('Number of Events (Unit: million)')\nax1.set_title('Distribution by Event Type')\nfor x, counts in zip(e_df['event'].unique().tolist(), e_df['event'].value_counts().tolist()):\n    text=str(round((counts\/totalcases)*100,2)) +'%'\n    ax1.text(x, counts, text, fontsize=12)\n    \n# unique number of visitors by event\nfor event in e_df['event'].unique().tolist():\n    height=len(e_df['visitorid'][e_df['event']==event].unique())\n    ax2.bar(x=event, height=height)\n    \n    text=len(e_df['visitorid'][e_df['event']==event].unique())\/len(e_df['visitorid'][e_df['event']==event])\n    text=str(round(text*100,2)) + '%'\n    ax2.text(event, height, text, fontsize=12)\nax2.set_title('Unique Number of Visitors')\nplt.show()","a2035a9f":"# obtain vistor id,item id, and date time of 'tranaction'\nitem_tra=e_df[['visitorid','itemid','date_time']][e_df['event']=='transaction']\n# obtain vistor id,item id, and date time of 'add to cart'\nitem_atc=e_df[['visitorid','itemid','date_time']][e_df['event']=='addtocart']\n# obtain vistor id,item id, and date time of 'view'\nitem_viw=e_df[['visitorid','itemid','date_time']][e_df['event']=='view']","790553b3":"# create a dataframe of visitor, itemid found in all three events\nm=item_tra.merge(item_atc, how='inner', on=['visitorid','itemid'], suffixes=[' (transaction)', ' (add_to_cart)'])\nm=m.merge(item_viw, how='inner', on=['visitorid','itemid'])\nm=m.rename(columns={'date_time':'date_time (view)'})\nm.head()","4ab0a5d5":"# convert datatype of the date columns\nm['date_time (transaction)']=pd.to_datetime(m['date_time (transaction)'])\nm['date_time (add_to_cart)']=pd.to_datetime(m['date_time (add_to_cart)'])\nm['date_time (view)']=pd.to_datetime(m['date_time (view)'])","ee6ea03a":"# the following dataframe is about the cases that a visitor viewed an item before making purchase decision\n\n# find rows with time difference is larger than 0 minute\nmsk=(m['date_time (transaction)']-m['date_time (view)'])>np.timedelta64(0,'m')\nm1=m[msk]","16e8dba9":"# seperate multiple-view transaction and single-view transaction\n\n# using duplicated function\nmul_viw=m1.duplicated(subset=['visitorid','itemid'], keep=False)\nsig_viw=m1[~mul_viw]\nmul_viw=m1[mul_viw]\n\n# last occurrence in duplicates = view when a visitor made purchase\n# therefore, filter dataframe to leave only views before last occurence\nnotlast=mul_viw.duplicated(subset=['visitorid','itemid'], keep='last')  # except for last occurance\n\n# count the number of view\n# obtain the average number of it\navg_viw=mul_viw[notlast].groupby(['visitorid','itemid']).count()['date_time (view)'].mean()\nprint('Average Number of Views Before Purchase: {0:.0f}'.format(avg_viw))","0104419d":"# view basic statistics to see outlier\nmul_viw[notlast].groupby(['visitorid','itemid']).count()['date_time (view)'].describe()","9bc315a8":"# count the number of view using groupby function\nn_viw=mul_viw[notlast].groupby(['visitorid','itemid']).count()['date_time (view)']\nn_viw=pd.DataFrame(n_viw)\nn_viw.head()","b42ebdc8":"# create a dataframe with the count of view from the previous cell\nn_viw2=pd.DataFrame(n_viw.value_counts(), columns=['count'])\nn_viw2=n_viw2.reset_index()\n\n# add a row with the value of instant purchase\n# item view at the time of transaction counted as '0' here\nn_viw2.loc[len(n_viw2)]=['0',sig_viw.shape[0]]\n\n# add the column of share in percentage\nn_viw2=n_viw2.sort_values('count', ascending=False)\nn_viw2['share (%)']=((n_viw2['count']\/(n_viw2['count'].sum()))*100).round(2)\nn_viw2=n_viw2.reset_index(drop=True)\nn_viw2","349c4ba1":"# aggregate values lower than 5th highest values into 'others'\nn_viw3=n_viw2[0:5].copy()\nothers=n_viw2[5::].sum().tolist()\nothers[0]='others'\nn_viw3.loc[len(n_viw3)]=others\n\n# set new index\nn_viw3['index']=['instant purchase', 'view 1', 'view 2','view 3', 'view 5', 'others']\nn_viw3=n_viw3.set_index(['index'])","d2d18f19":"# generate a pie plot of share by number of view before transaciton\nn_viw3.plot(y='share (%)', kind='pie', autopct='%1.1f%%',  shadow=True, startangle=-90, legend=False, figsize=(8,8), fontsize=20)\nplt.title('The Number of Item Views Before Purchase Decision', fontsize=20, pad=20)\nplt.ylabel('')\nplt.show()","87f17f1a":"# single view\n# calcuate the average time period for a visitor to take for purchase\ndiff_s=sig_viw['date_time (transaction)']-sig_viw['date_time (view)']\navg_time=(diff_s).mean()\ntotalsec=avg_time.seconds\nhrs, remainder = divmod(totalsec,3600)\nmins, sec = divmod(remainder,60)\nprint('Time Period From Single View to Transaction: %s hour %s mininutes %s seconds \\n' % (hrs,mins,sec))\n\n# multiple view\n# calcuate the average time period for a visitor to take for purchase\n\n# sort values\nmul_viw=mul_viw.sort_values(['visitorid','itemid','date_time (view)'])\nmul_viw=mul_viw.reset_index(drop=True)\n\n# initial view to transaction\nnotinitial=mul_viw.duplicated(subset=['visitorid','itemid'], keep='first') # except for first occurance\nmul_viw1=mul_viw[~notinitial] # only first occurence\n\n# get the time difference\ndiff_m1=mul_viw1['date_time (transaction)']-mul_viw1['date_time (view)']\navg_time1=(diff_m1).mean()\ntotaldays1=avg_time1.days\ntotalsec1=avg_time1.seconds\nhrs1, remainder1 = divmod(totalsec1,3600)\nmins1, sec1 = divmod(remainder1,60)\nprint('Time Period From Multiple View (initial) to Transaction: %s days %s hours %s mininutes %s seconds' % (totaldays1,hrs1,mins1,sec1))\n\n# last view to transaction\nnotlast=mul_viw.duplicated(subset=['visitorid','itemid'], keep='last')  # except for last occurance\nmul_viw2=mul_viw[~notlast] # only last occurence\n\n# get the time difference\ndiff_m2=mul_viw2['date_time (transaction)']-mul_viw2['date_time (view)']\navg_time2=(diff_m2).mean()\ntotaldays2=avg_time2.days\ntotalsec2=avg_time2.seconds\nhrs2, remainder2 = divmod(totalsec2,3600)\nmins2, sec2 = divmod(remainder2,60)\nprint('Time Period From Multiple View (last) to Transaction: %s days %s hours %s mininutes %s seconds' % (totaldays2,hrs2,mins2,sec2))","a5c5b14a":"def quan_list(percentile):\n    list=[diff_s.quantile(percentile).seconds,diff_m1.quantile(percentile).seconds,diff_m2.quantile(percentile).seconds]\n    return list","5f8e542e":"def time_cal(column):\n    hr, remainder=divmod(column, 3600)\n    mins,sec=divmod(remainder, 60)\n    list=[]\n    for i in range(len(column)):\n        t='%sh%sm%ss'%(hr[i],mins[i],sec[i])\n        t=str(t)\n        list.append(str(t))\n    return list","57a838f0":"q1=quan_list(.25)\nq2=quan_list(.50)\nq3=quan_list(.75)\n\ndata={'Transaction Type':['Single View','Multiple Views(initial)','Multiple Views(last)'],\\\n     '25th percentile': q1,\\\n     '50th percentile': q2,\\\n     '75th percentile': q3}\ndata=pd.DataFrame(data)\ndata=data.set_index('Transaction Type')\nt_data=data.transpose()\n\nq1time=time_cal(t_data['Single View'])\nq2time=time_cal(t_data['Multiple Views(initial)'])\nq3time=time_cal(t_data['Multiple Views(last)'])\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,8))\n\n# plot the same data on both axes\ndata.plot(ax=ax1, kind='bar')\ndata.plot(ax=ax2, kind='bar')\n\n# zoom-in \/ limit the view to different portions of the data\nax1.set_ylim(30000, 35000)  # outliers only\nax2.set_ylim(0, 2000)  # most of the data\n\n# hide the spines between ax and ax2\nax1.spines['top'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\nax1.xaxis.tick_top()\nax1.tick_params(labeltop='off')  # don't put tick labels at the top\nax2.xaxis.tick_bottom()\nax2.set_xticklabels(ax2.get_xticklabels(), rotation = 45, ha=\"right\", fontsize=15)\nax2.legend('')\n\n# This looks pretty good, and was fairly painless, but you can get that\n# cut-out diagonal lines look with just a bit more work. The important\n# thing to know here is that in axes coordinates, which are always\n# between 0-1, spine endpoints are at these locations (0,0), (0,1),\n# (1,0), and (1,1).  Thus, we just need to put the diagonals in the\n# appropriate corners of each of our axes, and so long as we use the\n# right transform and disable clipping.\n\nd = .015  # how big to make the diagonal lines in axes coordinates\n# arguments to pass to plot, just so we don't keep repeating them\nkwargs = dict(transform=ax1.transAxes, color='k', clip_on=False)\nax1.plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\nax1.plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n\nkwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\nax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\nax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n\nx=[-0.4,-0.1,0]\nfor x,y,qt in zip(x,t_data['Single View'],q1time):\n    plt.text(x, y+500, qt, fontsize=12)\nx=[0.6,0.9,1]\nfor x,y,qt in zip(x,t_data['Multiple Views(initial)'],q2time):\n    if x<1:\n        plt.text(x, y+200, qt, fontsize=12)\n    else:\n        plt.text(x, 4000, qt, fontsize=12)\nx=[1.6,1.9,2]\nfor x,y,qt in zip(x,t_data['Multiple Views(last)'],q3time):\n    plt.text(x, y+500, qt, fontsize=12 )\nax1.set_ylabel('time period (second)')\nax2.set_ylabel('time period (second)')\nax1.set_title('Time taken to make purchase decision', pad=50, fontsize=20)\nplt.show()","f5532899":"# load item-related datasets\ncate=pd.read_csv('\/kaggle\/input\/ecommerce-dataset\/category_tree.csv')\nitem1=pd.read_csv('\/kaggle\/input\/ecommerce-dataset\/item_properties_part1.csv')\nitem2=pd.read_csv('\/kaggle\/input\/ecommerce-dataset\/item_properties_part2.csv')","a6475aa9":"# view category tree dataset\ncate.head()","1975157c":"len(cate.categoryid.unique())","e7fe2aee":"len(cate.parentid.unique())","9bb589fe":"cate.info()","7cbe529c":"# view item dataset 1\nitem1.head()","dc2e6749":"item1.property.value_counts()","5d098700":"item1=item1[item1.property=='categoryid']\nitem1.reset_index(drop=True, inplace=True)\nitem1.shape","80369efe":"# do the same with item2\nitem2=item2[item2.property=='categoryid']\nitem2.reset_index(drop=True, inplace=True)\nitem2.shape","993c5cc0":"# drop column, timestamp and property\nitem1=item1.drop(columns=['timestamp','property'])\nitem2=item2.drop(columns=['timestamp','property'])\nitem1.head()","d674f932":"# stack two item dataframes\nitem=pd.concat([item1, item2], ignore_index=True)\nitem.shape","ceb3d14c":"# reset index\nitem.reset_index(drop=True, inplace=True)\n\n# rename column name 'value' to 'categoryid'\nitem.rename(columns={'value':'categoryid'}, inplace=True)\nitem.head()","96eb198c":"# check duplitcates\nitem.duplicated().value_counts()","c509e9f4":"# drop duplicates\nmsk=item.duplicated()\nitem=item[~msk]","6e26611d":"# count the number of category id by item id\n(item.groupby('itemid').count().categoryid>1).value_counts()","c9d0b6bb":"# number of category id\n# create a dataframe\nnum_cate=item.groupby('itemid').count()\nnum_cate.reset_index(inplace=True)\nnum_cate.rename(columns={'categoryid':'num_categoryid'}, inplace=True)\nnum_cate.head()","3657773d":"cate.info()","cd384e76":"item.info()","443f0a50":"item.categoryid=item.categoryid.astype(int)","a45e50e3":"# merge item dataframe with category tree to link item id with parent id\nitem=item.merge(cate, how='inner', on='categoryid')\nitem.head()","3e71fc12":"# create a dataframe with 'itemid' and 'parent id'\nip=item[['itemid','parentid']]\n\n# check duplicates\nip.duplicated().value_counts()","e1722d0a":"# remove duplicates\nmsk=ip.duplicated()\nip=ip[~msk]\n\n# view the number of parent ids by item id\nip.groupby('itemid').count()","34406620":"# number of parent id\n# create a dataframe\nnum_pare=ip.groupby('itemid').count()\nnum_pare.reset_index(inplace=True)\nnum_pare.rename(columns={'parentid':'num_parentid'}, inplace=True)","9982f7ff":"num_pare","85d3169a":"# call dataframe that includes 'transaction' information. \nitem_tra.shape","a38bee9c":"item_viw.reset_index(drop=True, inplace=True)","e9bf420f":"item_viw","4882e031":"msk=item_viw.duplicated(['visitorid','itemid'])\nitem_viw=item_viw[~msk]","2c7afc19":"# visitor id and item id that didn't lead to transaction\nnta=pd.concat([item_viw,item_tra], ignore_index=True)\nmsk=nta.duplicated(['visitorid','itemid'], keep=False)\nnta=nta[~msk]\nnta.reset_index(drop=True, inplace=True)\nnta","e12807b3":"# randomly select 25000 observation from nta(no-transaction) dataframe above\nindex=np.random.choice(nta.index, 25000)\nnta_r=nta.loc[index].reset_index(drop=True)\nnta_r.head()","cff68e86":"# add a new column\nnta_r['purchase']=[0]*nta_r.shape[0]\nnta_r.head()","bfd50507":"nta_r.shape","0096ee22":"# add a new column\nitem_tra['purchase']=[1]*item_tra.shape[0]\nitem_tra.head()","b10ec143":"item_tra.reset_index(drop=True, inplace=True)","5c0e0839":"item_tra.shape","5181f628":"# concatenate transaction data and non-transaction data (the cases that a visitor after all didn't buy the item viewed during this data collection period)\ndata=pd.concat([nta_r,item_tra], ignore_index=True).sort_values('date_time').reset_index(drop=True)\ndata.head()","5a3017fd":"# merge with category id, parent id dataframe\ndata=data.merge(num_cate, how='inner', on='itemid')\ndata=data.merge(num_pare, how='inner', on='itemid')","e455ddd7":"data.groupby('num_categoryid').sum().purchase","d10d73c2":"data.groupby('num_parentid').sum().purchase","a994571d":"# pick items under one category and view each sales number.\ncate1=data[data.num_categoryid==1]\ncate1=cate1.merge(item[['itemid','categoryid']], how='inner', on=['itemid'])\ncate1_sales=cate1.groupby('categoryid').sum().purchase\n\n# plot the sales number along the category id. \npd.DataFrame(cate1_sales).plot()\nplt.ylabel('purchase')\nplt.show()","d2169a77":"# pick items under one parent id and view each sales number.\npare1=data[data.num_parentid==1]\npare1=pare1.merge(item[['itemid','parentid']], how='inner', on=['itemid'])\npare1_sales=pare1.groupby('parentid').sum().purchase\n\n# plot the sales number along the parent id.\npd.DataFrame(pare1_sales).plot()\nplt.ylabel('purchase')\nplt.show()","b26f3a13":"# the item distribution by cateogory id\nitem.categoryid.hist()\nplt.show()","9f59d885":"# the item distribution by parent id\nitem.parentid.hist()\nplt.show()","fb338868":"# sort data by date time an in ascending order. \ndata=data.sort_values('date_time').reset_index(drop=True)\ndata.head()","47b3b5fb":"# convert data type of the column, date_time to use date_time functions.\ndata.date_time=pd.to_datetime(data.date_time)\ndata.info()","fd399b51":"# extract days of week of each date and add them into a new column.\ndata['dayofweek']=data.date_time.dt.dayofweek","7a23d679":"# count the number of date by day of week.\ndata.groupby('dayofweek').count()","8aa687e9":"# view how many purchase there are by day of week.\ndata[data.purchase==1].groupby('dayofweek').count()","91754cd9":"# compare the number of purchase with that of non-purchase by day of week.\nplt.plot(data[data.purchase==0].groupby('dayofweek').count(), color='red')\nplt.plot(data[data.purchase==1].groupby('dayofweek').count(), color='blue')\nplt.legend(['No purchase','Purchase'], labelcolor=['red','blue'])\nplt.show()","6754c7de":"# extract hour and week number and add them to a new column, respectively\ndata['hour']=data.date_time.dt.hour\ndata['week']=data.date_time.dt.isocalendar().week ","3249f590":"# convert data type\nitem_viw['date_time']=pd.to_datetime(item_viw['date_time'])\nitem_viw.info()","4a920317":"# convert data type\nitem_tra['date_time']=pd.to_datetime(item_tra['date_time'])\nitem_tra.info()","58c6c10e":"# count each visitor's previous view of each item\nlist=[]\nfor i in range(len(data)):\n    row=item_viw[(item_viw['visitorid']==data.loc[i,'visitorid'])&(item_viw['itemid']==data.loc[i,'itemid'])&(item_viw['date_time']<data.loc[i,'date_time'])]\n\n    if len(row)==0:\n        list.append(0)\n\n    else:\n        list.append(len(row))\nlist[0:10]","50f92312":"# add a new column, 'previous view'\ndata['previous_view']=pd.DataFrame({'view_count':list})\ndata.head()","9e91d1a4":"# count each visitor's total number of transaction in the past\nlist2=[]\nfor i in range(len(data)):\n    row=item_tra[(item_tra['visitorid']==data.loc[i,'visitorid'])&(item_tra['date_time']<data.loc[i,'date_time'])]\n\n    if len(row)==0:\n        list2.append(0)\n\n    else:\n        list2.append(len(row))\nlist2[0:10]","64b4b7ee":"# add a new column, 'previous transaction'\ndata['previous_transaction']=pd.DataFrame({'previous_transaction':list2})","45782dea":"# export the csv file\ndata.to_csv('data_for_modeling.csv', index=False)","8e4b820b":"plt.plot(data[['dayofweek','purchase']].groupby('dayofweek').sum())\nplt.xticks(np.arange(7),['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], rotation=45)\nplt.ylabel('Purchase')\nplt.show()","0681636b":"plt.plot(data[['hour','purchase']].groupby('hour').sum())\nplt.ylabel('Hour')\nplt.ylabel('Purchase')\nplt.show()","84f3dc9f":"plt.plot(data[['week','purchase']].groupby('week').sum())\nplt.xlabel('Week')\nplt.ylabel('Purchase')\nplt.show()","e44a26c9":"plt.plot(data[['previous_view','purchase']].groupby('previous_view').sum())\nplt.xlabel('Number of previous item view (current)')\nplt.ylabel('Purchase')\nplt.show()","0bc5f1a5":"plt.plot(data[['previous_transaction','purchase']].groupby('previous_transaction').sum())\nplt.xlim(0,5)\nplt.xlabel(\"Number of a visitor's previous transaction (total)\")\nplt.ylabel('Purchase')\nplt.show()","dc9f734c":"from sklearn import preprocessing ","45e88eaf":"# convert features to numpy array\nX=data[['dayofweek','hour','previous_view','previous_transaction']].values\nX[0:5]","5ecadf35":"# convert a target values to numpy array\ny=data['purchase'].values\ny[0:5]","da71f4de":"X=preprocessing.StandardScaler().fit(X).transform(X.astype(float))\nX[0:5]","8fad3822":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\nprint('Train Set: ', X_train.shape, y_train.shape)\nprint('Test Set: ', X_test.shape, y_test.shape)","2f629b5d":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nKs=10\nmean_acc=np.zeros((Ks-1)) # create numpy array of which elements are nine 0s.\nstd_acc=np.zeros((Ks-1))\n\nfor n in range(1, Ks):\n    \n    # train model\n    clf=KNeighborsClassifier(n_neighbors=n).fit(X_train, y_train)\n    yhat=clf.predict(X_test)\n    mean_acc[n-1]=metrics.accuracy_score(y_test, yhat) # make a list of accuracy score by model\n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])","13603072":"plt.plot(range(1,Ks), mean_acc, 'g') # x axis is 1 to 10, y axis is accuracy record, green line.\nplt.fill_between(range(1, Ks), (mean_acc)-(1 * std_acc), (mean_acc)+(1*std_acc), alpha=0.10)\nplt.legend(('Accuracy', '+\/- 3xstd')) # double parentheses\nplt.ylabel('Accuracy')\nplt.xlabel('Number of Neighbors(K)')\nplt.tight_layout()\nplt.show()","17176dea":"n=7\nKNN_7=KNeighborsClassifier(n_neighbors=n).fit(X_train, y_train)","ba3afbfb":"from sklearn.tree import DecisionTreeClassifier\nPurchaseTree=DecisionTreeClassifier(criterion='entropy', max_depth=4)\nPurchaseTree.fit(X_train, y_train)\npredTree=PurchaseTree.predict(X_test)\nprint(predTree[0:5])\nprint(y_test[0:5])","ac021d89":"# evaluation\n\nprint(\"Decision Tree's Accuracy: {0:.2f}%\".format(metrics.accuracy_score(y_test, predTree)*100))","f425f102":"from sklearn.linear_model import LogisticRegression\n\n# 'c' parameter = inverse of regularization (the smaller, the stronger regularization)\nLR=LogisticRegression(C=0.01, solver='liblinear').fit(X_train, y_train)\nyhat=LR.predict(X_test)\nyhat_proba=LR.predict_proba(X_test)\nyhat_proba[0:10]","ccd476a8":"# evaluation\n\n# visualize 'Confusion Matrix'\nimport itertools\n\ndef plot_confusion_matrix (cm, classes, \n                         normalize=False, \n                         title='Confusion matrix',\n                         cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be appliced by setting 'normalize=True'.\n    \"\"\"\n    \n    if normalize:\n        cm=cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n        print('Normalized confusion matrix')\n    else:\n        print('Confusion matrix, without normalization')\n        \n    print(cm)\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks=np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    fmt='.2f' if normalize else 'd'\n    thresh=cm.max()\/2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j,i, format(cm[i,j], fmt),\n                 horizontalalignment='center',\n                 color='white' if cm[i,j]> thresh else 'black')\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# compute confusion matrix\ncnf_matrix=metrics.confusion_matrix(y_test, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n# plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Purchase=1','No Purchase=0'], normalize=False, title='Confusion matrix')\n\n\n# classification report\nprint('\\nClassification Report\\n',metrics.classification_report(y_test, yhat))\n\n\n# jaccard score\nprint('Jaccard Score: ', metrics.jaccard_score(y_test,yhat))\n\n# logloss\nprint('\\nLogloss: ',metrics.log_loss(y_test, yhat_proba))\n# more ideal classifiers have progressively smaller values of log loss.\n","360af434":"### The first row in the dataframe above indicates that Visitor(id=172) had viewed the item(id=10034) four times before resolving to purchase. The second row is about another item the same visitor purchased and this time, the visitor viewed that item five times.  ","12279ce7":"### There was no clear relationship between category and transaction (sales). ","23e4bc88":"## Step 03-1. K-Nearest Neighbors Algorithm","beecbede":"### The best model is when k value is 7 with 92% of model accuracy.","e4be090f":"## Q3. Is there any relationship between Item Category and Transaction?","3044b508":"### There are some outliers such as 844 views for buying a single item, and hence I will check the distribution of view counts to have a better idea about general purchase cases. ","a07bfd45":"### Only'categoryid' and 'available' were given in the column 'property' while the rest are hashed for confidentiality purpose. I will use only categoryid and hence leave only the rows with that.","ce26e66b":"### In the dataframe above, multiple views are mixed in for one transaction, which means there are cases that a visitor checked the item multiple times. I'll check both timelines: one is from the first item view to transaction, and other is from the last item view to transaction.","be0b224a":"## Step 03-3. Logistic Regression Algorithm","f490c0b9":"### In case of single-view transactions, they generally take around 3 to 11 minutes to reach the end of buying journey (view to transaction). On the other hand, multiple-view transactions showed more dispersed time range, about 9 minutes to 9.5 hours with 30 minutes as a median. However, when multiple-view buyers checked the product for the last time before purchase, they took similar short amount of time to single-view buyers.","8050eba2":"### As dates in current dataset is in unix timestamp (the number of seconds since 1970-01-01), I'll convert them into readable dates.","cdd23c3e":"## 2. Machine Learning Classification Modeling","c65e06a4":"### 'Category ID' is a subset or child of 'Parent Id'.","6b1044c5":"### This dataset is recorded from 3rd May, 2015 to 18th September, 2015 (GMT).","2156088b":"### When analysing the distribution of events, 'View' occupies 96.67%, 'Add to cart' 2.52%, 'Transaction' 0.81%.\n### The number of unique visitors was around the half of total number in all three types of event, for example, total view is 2.6 million, but unique visitor for the views is the half, which means that on average one visitor caused 2 actions.","509f52f6":"## Q2. How many times does it take for one single transaction to be made?","157390c8":"### 'date_time (transaction)'","f2eb77e0":"### When analysing the item view numbers, I found around 50% of transactions were made without more-than-once view: a visitor checked an item, added to cart and checked out. About 30% of transactions were made after a buyer view an item once or twice. In summary, 80% of total transactions were made after less-than-three-times item view. ","86bc5593":"## Step 03-2. Decision Tree Algorithm","c35feb98":"### choosen feature: dayofweek, hour, previous view, previous transaction ","58d4e7e6":"## Q4. Is there any relationship between View Time and Transaction?","199ba39f":"## Step 01. Normalize data","a5bfc1fd":"### All transaction events have transaction ID.","50b3e46c":"## Step 02. Train\/Test Split","764bb1e7":"### There is no general trend in the number of sales along category id.","0c831279":"### There is no general trend in the number of sales along parent id.","197f3bd3":"# 1. EDA (Exploratory Data Analysis)","2f26261b":"## Q1. How many times did a customer view an item before making purchase decision?","de2bf2ac":"### I had assumed that the more Category ID or Parent ID an item has, the more transactions it might have as the item could have exposed in multiple category pages. However, when I checked the distribution of data, the number of items with multiple categories was not many.","cb7d7aaf":"### Except in 'transactionid', there are no missing values."}}