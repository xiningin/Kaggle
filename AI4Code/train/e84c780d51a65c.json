{"cell_type":{"b33f1fb6":"code","43242417":"code","bbb00a5e":"code","14008fa8":"code","53d2cfd9":"code","ecd846d8":"code","f97ebca8":"code","59cbbe7a":"code","647c41eb":"code","354b3b14":"code","df308d9a":"code","944c9df3":"code","b39e2542":"code","2c5f7e94":"code","8d94292e":"code","1328f156":"code","7f3a1cfd":"markdown","61b6ec8b":"markdown","33e8af25":"markdown"},"source":{"b33f1fb6":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\n\nimport timm\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import NearestNeighbors\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nfrom torchvision import datasets, transforms","43242417":"IM_FOLDER = '..\/input\/shopee-product-matching\/test_images'\nMODEL_PATH = '..\/input\/shopeekfoldevaluation\/kfolds_strategies_evaluation\/kfolds_strategies_evaluation\/train_n_lbgr_88_3fold\/Fold02_Valid0.725_Train0.724_Ep003.pth'\n\nFOIS = [0, 1, 2]\nSAMPLE = None\n\nDIM = (512,512)\nEMB_SIZE = 1536\n\nNUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nSEED = 2020\nLR = 3e-4\n\n################################################# MODEL ####################################################################\nmodel_name = 'efficientnet_b3' #efficientnet_b0-b7\n\n################################################ Metric Loss and its params #######################################################\nloss_module = 'arcface' #'cosface' #'adacos'\ns = 30.0\nm = 0.5 \nls_eps = 0.0\neasy_margin = False\n\n############################################## Model Params ###############################################################\nmodel_params = {\n    'model_name':'efficientnet_b3',\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.0,\n    'loss_module':loss_module,\n    's':30.0,\n    'margin':28.6, # degree (0.5 radian)\n    'ls_eps':0.0,\n    'theta_zero':0.785,\n    'pretrained':None\n}\n\n########### Device ###########\nDEVICE = torch.device(\"cuda\")","bbb00a5e":"def get_valid_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","14008fa8":"class ShopeeDataset(Dataset):\n    def __init__(self, csv, transforms=None):\n\n        self.csv = csv.reset_index()\n        self.augmentations = transforms\n        \n        if('label_group' in self.csv.columns):\n            self.is_test = False\n        else:\n            self.is_test = True\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        image = cv2.imread(row.filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n        \n        if(self.is_test):\n            return image\n        else:\n            return image, torch.tensor(row.label_group)","53d2cfd9":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 model_name='efficientnet_b0',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 pretrained=None):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=False)\n        if(pretrained):\n            print('Loading pretrained model from:', pretrained)\n            self.backbone.load_state_dict(torch.load(pretrained, map_location='cpu'))\n            \n        final_in_features = self.backbone.classifier.in_features\n        \n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n            \n        self.use_fc = use_fc\n        if use_fc:\n            print('use_fc')\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        \n        return x","ecd846d8":"# Test loading model properly\nmodel = ShopeeNet(**model_params)\nmodel.to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))","f97ebca8":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target,row[col]))\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score\n\ndef compute_f1(df, pred_col='preds'):\n    res_df = df.copy()\n    target_dict = res_df.groupby('label_group').posting_id.agg('unique').to_dict()\n    res_df['target'] = res_df.label_group.map(target_dict)\n    res_df['f1'] = res_df.apply(getMetric(pred_col),axis=1)\n    return res_df.f1.mean()","59cbbe7a":"def predict_fn(dataloader, df, model, k=50, metric='cosine', threshold=0.4):\n    emb_list = []\n    for images in tqdm(dataloader):\n        if(type(images) == tuple or type(images) == list):\n            images = images[0]\n        images = images.to(DEVICE)\n        embeddings = model(images).detach().cpu().numpy()\n        \n        # l2 norm\n        embeddings \/= np.linalg.norm(embeddings, 2, axis=1, keepdims=True)\n        \n        emb_list.append(embeddings)\n        \n    emb_vectors = np.vstack(emb_list)\n    print(emb_vectors.shape)\n    \n    model = NearestNeighbors(n_neighbors=k, metric=metric)\n    model.fit(emb_vectors)\n    distances, indices = model.kneighbors(emb_vectors)\n\n    res_df = df.copy()\n    preds = []\n    for dist, inds in tqdm(zip(distances, indices)):\n        IDX = np.where(dist<threshold)[0]\n        IDS = inds[IDX]\n        o = df.iloc[IDS].posting_id.values\n        preds.append(o)\n            \n    res_df['preds'] = preds\n    \n    return res_df","647c41eb":"test_df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\ntest_df['filepath'] = test_df['image'].apply(lambda x: os.path.join(IM_FOLDER, x))\n\ntest_dataset = ShopeeDataset(csv=test_df, transforms=get_valid_transforms(),)\ntest_loader = torch.utils.data.DataLoader(test_dataset,batch_size=VALID_BATCH_SIZE,num_workers=NUM_WORKERS,\n                                               shuffle=False,pin_memory=True,drop_last=False)","354b3b14":"K = 50 if(50 < len(test_df)) else len(test_df)\n    \ntest_res_df =  predict_fn(test_loader, test_df, model, k=K)","df308d9a":"test_res_df['matches'] = test_res_df['preds'].map(lambda x: ' '.join(x.tolist()))","944c9df3":"sub = test_res_df[['posting_id', 'matches']]","b39e2542":"sub.to_csv('submission.csv', index=False)","2c5f7e94":"# valid_df = pd.read_csv('..\/input\/shopeekfoldevaluation\/train_vanila_88_3fold.csv').sort_values('label_group')\n# valid_df = valid_df[valid_df.fold==0]\n# valid_df['filepath'] = valid_df['image'].apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/train_images', x))\n\n# le = LabelEncoder()\n# valid_df['label_group'] = le.fit_transform(valid_df.label_group)\n\n# valid_dataset = ShopeeDataset(csv=valid_df, transforms=get_valid_transforms(),)\n# valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,num_workers=NUM_WORKERS,\n#                                                shuffle=False,pin_memory=True,drop_last=False)\n\n# valid_res_df = predict_fn(valid_loader, valid_df, model)\n\n# compute_f1(valid_res_df)","8d94292e":"# all_df = pd.read_csv('..\/input\/shopeekfoldevaluation\/train_vanila_88_3fold.csv').sort_values('label_group')\n# all_df['filepath'] = all_df['image'].apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/train_images', x))\n\n# le = LabelEncoder()\n# all_df['label_group'] = le.fit_transform(all_df.label_group)\n\n# all_dataset = ShopeeDataset(csv=all_df, transforms=get_valid_transforms(),)\n# all_loader = torch.utils.data.DataLoader(all_dataset,batch_size=16,num_workers=NUM_WORKERS,\n#                                                shuffle=False,pin_memory=True,drop_last=False)\n\n# all_res_df = predict_fn(all_loader, all_df, model)\n\n# compute_f1(all_res_df)","1328f156":"# a = next(iter(all_loader))","7f3a1cfd":"# 2. Load model","61b6ec8b":"# 1. Config","33e8af25":"# 3. Predict"}}