{"cell_type":{"53b33660":"code","0537d15f":"code","f65eb555":"code","e886ea2a":"code","cd1c06e9":"code","42397622":"code","5f2ea900":"code","5cda6890":"code","2809c8d6":"code","15cfc940":"code","b2c2c9cf":"code","851c7bb6":"code","95e917f5":"code","f1c17548":"code","1c419735":"code","e7010719":"code","5063a713":"code","056ffb7f":"markdown","93c3b92e":"markdown","2da9c962":"markdown","d70563a5":"markdown","b463d7a6":"markdown","351a8fdf":"markdown","91bf35be":"markdown","36a7b050":"markdown","b53c4ce5":"markdown","2d9a8224":"markdown","549bcc47":"markdown","ed774acb":"markdown","7e1c881e":"markdown","33a33240":"markdown","0ed5bf40":"markdown","b83635bd":"markdown","592f56c7":"markdown","19f2309d":"markdown","ffc58a75":"markdown"},"source":{"53b33660":"import numpy as np\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport sys\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport matplotlib","0537d15f":"features = pd.read_csv('..\/input\/deepsat-sat4\/X_test_sat4.csv', skiprows=lambda i: i % 50 != 0).values\nfeatures = features \/ 255.\nlabels = pd.read_csv('..\/input\/deepsat-sat4\/y_test_sat4.csv', skiprows=lambda i: i % 50 != 0).values\nlabels = np.max(labels, axis=1)\n\nfeatures, labels = shuffle(features, labels)\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                        features, labels, test_size=0.33, random_state=42)","f65eb555":"def log(x):\n    return 1 \/ (1 + np.exp(-1 * x))\n\ndef d_log(x):\n    return log(x) * ( 1 - log(x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef d_tanh(x):\n    return 1 - np.tanh(x) ** 2 \n\ndef ReLu(x):\n    mask = (x > 0.0) * 1.0\n    return x * mask\n\ndef d_ReLu(x):\n    mask = (x > 0.0) * 1.0\n    return mask    \n\ndef elu(matrix):\n    mask = (matrix<=0) * 1.0\n    less_zero = matrix * mask\n    safe =  (matrix>0) * 1.0\n    greater_zero = matrix * safe\n    final = 3.0 * (np.exp(less_zero) - 1) * less_zero\n    return greater_zero + final\n\ndef d_elu(matrix):\n    safe = (matrix>0) * 1.0\n    mask2 = (matrix<=0) * 1.0\n    temp = matrix * mask2\n    final = (3.0 * np.exp(temp))*mask2\n    return (matrix * safe) + final","e886ea2a":"# 1. Declare Weights\nnp.random.seed(1234)\n\nw1 = np.random.randn(3136,256) * 0.2\nw2 =np.random.randn(256,128) * 0.2\nw3 =np.random.randn(128,1) * 0.2\n\nw1_sgd,w2_sgd ,w3_sgd = w1,w2,w3\nw1_m,w2_m ,w3_m = w1,w2,w3\nw1_ng,w2_ng,w3_ng =  w1,w2,w3\nw1_adagrad,w2_adagrad,w3_adagrad =  w1,w2,w3\nw1_adadelta,w2_adadelta,w3_adadelta =  w1,w2,w3\nw1_RSMprop,w2_RSMprop,w3_RSMprop =  w1,w2,w3\nw1_adam,w2_adam,w3_adam =  w1,w2,w3\nw1_nadam,w2_nadam,w3_nadam =  w1,w2,w3\n\nw1_sgd_noise,w2_sgd_noise ,w3_sgd_noise = w1,w2,w3\nw1_noise,w2_noise,w3_noise  = w1,w2,w3\nw1_noise_noise,w2_noise_noise,w3_noise_noise  = w1,w2,w3\nw1_noise_adam,w2_noise_adam,w3_noise_adam  = w1,w2,w3","cd1c06e9":"num_epoch = 10\ntotal_cost = 0\nlearn_rate = 0.0003\ncost_array =[]\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_sgd)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_sgd)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_sgd)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =    grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_sgd.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_sgd.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =    grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        w3_sgd = w3_sgd - learn_rate * grad_3\n        w2_sgd = w2_sgd - learn_rate * grad_2\n        w1_sgd = w1_sgd - learn_rate * grad_1\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. SGD - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","42397622":"v1,v2,v3 = 0,0,0\nalpha = 0.001\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_m)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_m)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_m)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =    grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_m.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_m.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        v3 = v3 * alpha + learn_rate * grad_3\n        v2 = v2 * alpha + learn_rate * grad_2\n        v1 = v1 * alpha + learn_rate * grad_1\n\n        w3_m = w3_m - v3\n        w2_m = w2_m - v2\n        w1_m = w1_m - v1\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Momentum - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","5f2ea900":"v1,v2,v3 = 0,0,0\nalpha = 0.001\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_ng)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_ng)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_ng)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =    grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_ng.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_ng.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        # ------- FAKE GRADIENT --------\n        fake_w3_ng = w3_ng - alpha * v3\n        fake_w2_ng = w2_ng - alpha * v2\n        fake_w1_ng = w1_ng - alpha * v1\n        \n        l1 = X.dot(fake_w1_ng)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(fake_w2_ng)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(fake_w3_ng)\n        l3A = log(l3)   \n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3_fake =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(fake_w3_ng.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2_fake =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(fake_w2_ng.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1_fake =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n        # ------- FAKE GRADIENT --------\n\n        v3 = v3 * alpha + learn_rate * grad_3_fake\n        v2 = v2 * alpha + learn_rate * grad_2_fake\n        v1 = v1 * alpha + learn_rate * grad_1_fake\n\n        w3_ng = w3_ng - v3\n        w2_ng = w2_ng - v2\n        w1_ng = w1_ng - v1\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Nesterov - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","5cda6890":"Adagrad_lr_1,Adagrad_lr_2,Adagrad_lr_3 = 0,0,0\nAdagrad_e = 0.00000001\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_adagrad)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_adagrad)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_adagrad)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_adagrad.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_adagrad.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        Adagrad_lr_3 = Adagrad_lr_3 + grad_3 ** 2\n        Adagrad_lr_2 = Adagrad_lr_2 + grad_2 ** 2\n        Adagrad_lr_1 = Adagrad_lr_1 + grad_1 ** 2\n\n        w3_adagrad = w3_adagrad - (learn_rate\/np.sqrt(Adagrad_lr_3 + Adagrad_e)) *grad_3\n        w2_adagrad = w2_adagrad - (learn_rate\/np.sqrt(Adagrad_lr_2 + Adagrad_e)) *grad_2\n        w1_adagrad = w1_adagrad - (learn_rate\/np.sqrt(Adagrad_lr_1 + Adagrad_e)) *grad_1\n    \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Adagrad - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","2809c8d6":"AdaDelta_e,AdaDelta_v = 0.000001,0.001\nAdaDelta_1,AdaDelta_2,AdaDelta_3 = 0,0,0\nAdaDelta_1_v,AdaDelta_2_v,AdaDelta_3_v = 0,0,0\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n        \n        l1 = X.dot(w1_adadelta)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_adadelta)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_adadelta)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_adadelta.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_adadelta.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        AdaDelta_3 = AdaDelta_v * AdaDelta_3 + (1-AdaDelta_v) * grad_3 ** 2\n        AdaDelta_2 = AdaDelta_v * AdaDelta_2 + (1-AdaDelta_v) * grad_2 ** 2\n        AdaDelta_1 = AdaDelta_v * AdaDelta_1 + (1-AdaDelta_v) * grad_1 ** 2\n\n        mid_grad_3 = - ( np.sqrt(AdaDelta_3_v + AdaDelta_e) \/ np.sqrt(AdaDelta_3 + AdaDelta_e) ) * grad_3\n        mid_grad_2 = - ( np.sqrt(AdaDelta_2_v + AdaDelta_e) \/ np.sqrt(AdaDelta_2 + AdaDelta_e) ) * grad_2\n        mid_grad_1 = - ( np.sqrt(AdaDelta_1_v + AdaDelta_e) \/ np.sqrt(AdaDelta_1 + AdaDelta_e) ) * grad_1\n\n        AdaDelta_3_v = AdaDelta_v * AdaDelta_3_v + (1-AdaDelta_v) * mid_grad_3 ** 2\n        AdaDelta_2_v = AdaDelta_v * AdaDelta_2_v + (1-AdaDelta_v) * mid_grad_2 ** 2\n        AdaDelta_1_v = AdaDelta_v * AdaDelta_1_v + (1-AdaDelta_v) * mid_grad_1 ** 2\n\n        w3_adadelta = w3_adadelta - mid_grad_3\n        w2_adadelta = w2_adadelta - mid_grad_2\n        w1_adadelta = w1_adadelta - mid_grad_1\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Adadelta - Cost:{:1.3}\".format(e + 1, total_cost))\n\n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \n# exclude from cost_array due to high cost\n# cost_array.append(cost_temp_array)","15cfc940":"RMSprop_1,RMSprop_2,RMSprop_3 = 0,0,0\nRMSprop_v,RMSprop_e= 0.9,0.00000001\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_RSMprop)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_RSMprop)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_RSMprop)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_RSMprop.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_RSMprop.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        RMSprop_3 = RMSprop_v*RMSprop_3 + (1- RMSprop_v)*grad_3**2\n        RMSprop_2 = RMSprop_v*RMSprop_2 + (1- RMSprop_v)*grad_2**2\n        RMSprop_1 = RMSprop_v*RMSprop_1 + (1- RMSprop_v)*grad_1**2\n\n        w3_RSMprop = w3_RSMprop - (learn_rate\/np.sqrt(RMSprop_3 + RMSprop_e)) * grad_3\n        w2_RSMprop = w2_RSMprop - (learn_rate\/np.sqrt(RMSprop_2 + RMSprop_e)) * grad_2\n        w1_RSMprop = w1_RSMprop - (learn_rate\/np.sqrt(RMSprop_1 + RMSprop_e)) * grad_1\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. RMSprop - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","b2c2c9cf":"Adam_m_1,Adam_m_2,Adam_m_3 = 0,0,0\nAdam_v_1,Adam_v_2,Adam_v_3 = 0,0,0\nAdam_Beta_1,Adam_Beta_2 = 0.9,0.999\nAdam_e = 0.00000001\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_adam)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_adam)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_adam)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_adam.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_adam.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        Adam_m_3 = Adam_Beta_1 * Adam_m_3 + ( 1-Adam_Beta_1 ) *grad_3\n        Adam_m_2 = Adam_Beta_1 * Adam_m_2 + ( 1-Adam_Beta_1 ) *grad_2\n        Adam_m_1 = Adam_Beta_1 * Adam_m_1 + ( 1-Adam_Beta_1 ) *grad_1\n\n        Adam_v_3 = Adam_Beta_2 * Adam_v_3 + ( 1-Adam_Beta_2 ) *grad_3 **2 \n        Adam_v_2 = Adam_Beta_2 * Adam_v_2 + ( 1-Adam_Beta_2 ) *grad_2 **2 \n        Adam_v_1 = Adam_Beta_2 * Adam_v_1 + ( 1-Adam_Beta_2 ) *grad_1 **2 \n        \n        Adam_m_3_hat = Adam_m_3\/(1-Adam_Beta_1)\n        Adam_m_2_hat = Adam_m_2\/(1-Adam_Beta_1)\n        Adam_m_1_hat = Adam_m_1\/(1-Adam_Beta_1)\n        \n        Adam_v_3_hat = Adam_v_3\/(1-Adam_Beta_2)\n        Adam_v_2_hat = Adam_v_2\/(1-Adam_Beta_2)\n        Adam_v_1_hat = Adam_v_1\/(1-Adam_Beta_2)\n        \n        w3_adam = w3_adam - (learn_rate\/(np.sqrt(Adam_v_3_hat) + Adam_e)) * Adam_m_3_hat\n        w2_adam = w2_adam - (learn_rate\/(np.sqrt(Adam_v_2_hat) + Adam_e)) * Adam_m_2_hat\n        w1_adam = w1_adam - (learn_rate\/(np.sqrt(Adam_v_1_hat) + Adam_e)) * Adam_m_1_hat\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Adam - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","851c7bb6":"Nadam_m_1,Nadam_m_2,Nadam_m_3 = 0,0,0\nNadam_v_1,Nadam_v_2,Nadam_v_3 = 0,0,0\nNadam_Beta_1,Nadam_Beta_2 = 0.9,0.999\nNadam_e = 0.00000001\ntotal_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_nadam)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_nadam)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_nadam)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_nadam.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_nadam.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        Nadam_m_3 = Nadam_Beta_1 * Nadam_m_3 + (1 - Nadam_Beta_1) * grad_3\n        Nadam_m_2 = Nadam_Beta_1 * Nadam_m_2 + (1 - Nadam_Beta_1) * grad_2\n        Nadam_m_1 = Nadam_Beta_1 * Nadam_m_1 + (1 - Nadam_Beta_1) * grad_1\n        \n        Nadam_v_3 = Nadam_Beta_2 * Nadam_v_3 + (1- Nadam_Beta_2) * grad_3 ** 2\n        Nadam_v_2 = Nadam_Beta_2 * Nadam_v_2 + (1- Nadam_Beta_2) * grad_2 ** 2\n        Nadam_v_1 = Nadam_Beta_2 * Nadam_v_1 + (1- Nadam_Beta_2) * grad_1 ** 2\n\n        Nadam_m_3_hat = Nadam_m_3\/ (1 - Nadam_Beta_1)\n        Nadam_m_2_hat = Nadam_m_2\/ (1 - Nadam_Beta_1)\n        Nadam_m_1_hat = Nadam_m_1\/ (1 - Nadam_Beta_1)\n\n        Nadam_v_3_hat = Nadam_v_3\/ (1 - Nadam_Beta_2)\n        Nadam_v_2_hat = Nadam_v_2\/ (1 - Nadam_Beta_2)\n        Nadam_v_1_hat = Nadam_v_1\/ (1 - Nadam_Beta_2)\n         \n        w3_nadam = w3_nadam - (learn_rate\/( np.sqrt(Nadam_v_3_hat) + Nadam_e )) * ( Nadam_Beta_1  * Nadam_m_3_hat + ( ( (1-Nadam_Beta_1) * grad_3 ) \/ (1 - Nadam_Beta_1)  ) )\n        w2_nadam = w2_nadam - (learn_rate\/( np.sqrt(Nadam_v_2_hat) + Nadam_e )) * ( Nadam_Beta_1  * Nadam_m_2_hat + ( ( (1-Nadam_Beta_1) * grad_2 ) \/ (1 - Nadam_Beta_1)  ) )\n        w1_nadam = w1_nadam - (learn_rate\/( np.sqrt(Nadam_v_1_hat) + Nadam_e )) * ( Nadam_Beta_1  * Nadam_m_1_hat + ( ( (1-Nadam_Beta_1) * grad_1 ) \/ (1 - Nadam_Beta_1)  ) )\n    \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Nadam - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","95e917f5":"total_cost = 0\nn_value = 0.001\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_sgd_noise)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_sgd_noise)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_sgd_noise)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        grad_3_part_1 = l3A - y\n        grad_3_part_2 = d_log(l3)\n        grad_3_part_3 = l2A\n        grad_3 =     grad_3_part_3.T.dot(grad_3_part_1 * grad_3_part_2)    \n\n        grad_2_part_1 = (grad_3_part_1 * grad_3_part_2).dot(w3_sgd_noise.T)\n        grad_2_part_2 = d_tanh(l2)\n        grad_2_part_3 = l1A\n        grad_2 =    grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2)\n\n        grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2_sgd_noise.T)\n        grad_1_part_2 = d_elu(l1)\n        grad_1_part_3 = X\n        grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 *grad_1_part_2)\n\n        # ------ Calculate The Additive Noise -------\n        ADDITIVE_NOISE_STD = n_value \/ (np.power((1 + e), 0.55))\n        ADDITIVE_GAUSSIAN_NOISE = np.random.normal(loc=0,scale=ADDITIVE_NOISE_STD)\n        # ------ Calculate The Additive Noise -------\n\n        w3_sgd_noise = w3_sgd_noise - learn_rate* (grad_3 + ADDITIVE_GAUSSIAN_NOISE)\n        w2_sgd_noise = w2_sgd_noise - learn_rate* (grad_2 + ADDITIVE_GAUSSIAN_NOISE)\n        w1_sgd_noise = w1_sgd_noise - learn_rate* (grad_1 + ADDITIVE_GAUSSIAN_NOISE)\n        \n    if e % 1 == 0 :\n        print(\"e:{:2d}. SGD with Gaussian Noise - Cost:{:1.3}\".format(e + 1, total_cost))    \n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","f1c17548":"total_cost = 0\nn, p = 1, .5 \ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_noise)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_noise)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_noise)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        gradient_weight_3 = np.random.gumbel(size=w3.shape)\n        gradient_weight_2 = np.random.gumbel(size=w2.shape)\n        gradient_weight_1 = np.random.gumbel(size=w1.shape)\n\n        w3_noise = w3_noise - learn_rate* gradient_weight_3\n        w2_noise = w2_noise - learn_rate* gradient_weight_2\n        w1_noise = w1_noise - learn_rate* gradient_weight_1\n    \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Noise - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n        \ncost_array.append(cost_temp_array)","1c419735":"total_cost = 0\ncost_temp_array = []\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_noise_noise)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_noise_noise)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_noise_noise)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        gradient_weight_3 = np.random.gumbel(size=w3.shape)\n        gradient_weight_2 = np.random.gumbel(size=w2.shape)\n        gradient_weight_1 = np.random.gumbel(size=w1.shape)\n\n        # ------ Calculate The Additive Noise -------\n        ADDITIVE_NOISE_STD = n_value \/ (np.power((1 + e), 0.55))\n        ADDITIVE_GAUSSIAN_NOISE = np.random.normal(loc=0,scale=ADDITIVE_NOISE_STD)\n        # ------ Calculate The Additive Noise -------\n\n        w3_noise_noise = w3_noise_noise - learn_rate* (gradient_weight_3 + ADDITIVE_GAUSSIAN_NOISE)\n        w2_noise_noise = w2_noise_noise - learn_rate* (gradient_weight_2 + ADDITIVE_GAUSSIAN_NOISE)\n        w1_noise_noise = w1_noise_noise - learn_rate* (gradient_weight_1 + ADDITIVE_GAUSSIAN_NOISE)\n    \n    if e % 1 == 0 :\n        print(\"e:{:2d}. Noise + Gaussian Noise - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n\ncost_array.append(cost_temp_array)","e7010719":"total_cost = 0\ncost_temp_array = []\nnoise_adam_m1,noise_adam_m2,noise_adam_m3 = 0,0,0\nnoise_adam_v1,noise_adam_v2,noise_adam_v3 = 0,0,0\nnoise_Adam_Beta_1,noise_Adam_Beta_2 = 0.9,0.999\nnoise_Adam_e = 0.00000001\n\nfor e in range(num_epoch):\n    for i in range(len(X_train)):\n        \n        X = np.expand_dims(X_train[i],axis=0)\n        y = np.expand_dims(np.array([y_train[i]]), axis=1)\n\n        l1 = X.dot(w1_noise_adam)\n        l1A = elu(l1)\n\n        l2 = l1A.dot(w2_noise_adam)\n        l2A = tanh(l2)       \n\n        l3 = l2A.dot(w3_noise_adam)\n        l3A = log(l3)   \n\n        cost = np.square(l3A - y).sum() * 0.5\n        total_cost = total_cost + cost\n\n        gradient_weight_3 = np.random.gumbel(size=w3.shape)\n        gradient_weight_2 = np.random.gumbel(size=w2.shape)\n        gradient_weight_1 = np.random.gumbel(size=w1.shape)\n\n        noise_adam_m3 = noise_Adam_Beta_1 * noise_adam_m3 + (1 - noise_Adam_Beta_1) * gradient_weight_3\n        noise_adam_m2 = noise_Adam_Beta_1 * noise_adam_m2 + (1 - noise_Adam_Beta_1) * gradient_weight_2\n        noise_adam_m1 = noise_Adam_Beta_1 * noise_adam_m1 + (1 - noise_Adam_Beta_1) * gradient_weight_1\n        \n        noise_adam_v3 = noise_Adam_Beta_2 * noise_adam_v3 + (1 - noise_Adam_Beta_2) * gradient_weight_3 ** 2\n        noise_adam_v2 = noise_Adam_Beta_2 * noise_adam_v2 + (1 - noise_Adam_Beta_2) * gradient_weight_2 ** 2\n        noise_adam_v1 = noise_Adam_Beta_2 * noise_adam_v1 + (1 - noise_Adam_Beta_2) * gradient_weight_1 ** 2\n\n        noise_adam_m3_hat = noise_adam_m3\/(1 -noise_Adam_Beta_1 )\n        noise_adam_m2_hat = noise_adam_m2\/(1 -noise_Adam_Beta_1 )\n        noise_adam_m1_hat = noise_adam_m1\/(1 -noise_Adam_Beta_1 )\n\n        noise_adam_v3_hat = noise_adam_v3\/(1 -noise_Adam_Beta_2 )\n        noise_adam_v2_hat = noise_adam_v2\/(1 -noise_Adam_Beta_2 )\n        noise_adam_v1_hat = noise_adam_v1\/(1 -noise_Adam_Beta_2 )\n\n        w3_noise_adam = w3_noise_adam - (learn_rate \/ ( np.sqrt(noise_adam_v3_hat)  +noise_Adam_e )) * noise_adam_m3_hat\n        w2_noise_adam = w2_noise_adam - (learn_rate \/ ( np.sqrt(noise_adam_v2_hat)  +noise_Adam_e )) * noise_adam_m2_hat\n        w1_noise_adam = w1_noise_adam - (learn_rate \/ ( np.sqrt(noise_adam_v1_hat)  +noise_Adam_e )) * noise_adam_m1_hat\n\n    if e % 1 == 0 :\n        print(\"e:{:2d}. Noise Adam - Cost:{:1.3}\".format(e + 1, total_cost))\n        \n    cost_temp_array.append(total_cost)\n    total_cost = 0\n    \ncost_array.append(cost_temp_array)","5063a713":"bar_color = ['b', 'g', 'saddlebrown', 'steelblue', \n            'orangered', 'y', 'paleturquoise', 'royalblue',\n            'salmon','silver','skyblue','slateblue','peru','plum']\nlabels_z = ['SGD', 'Momentum', 'Nesterov', 'Adagrad', 'RMSprop',\n            'Adam', 'Nadam', 'SGD with Gaussian Noise', 'Noise',\n            'Noise + Gaussian Additive Noise', 'Noise Adam']\n\nplt.figure(figsize=(12, 8))\nfor i in range(len(cost_array)):\n    plt.plot(np.arange(num_epoch), cost_array[i],color=bar_color[i],linewidth=3,label=str(labels_z[i]) )\nplt.title(\"Total Cost per Training\")\nplt.legend()\nplt.show()","056ffb7f":"<h1 id=\"adagrad\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Adagrad\n        <a class=\"anchor-link\" href=\"#adagrad\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","93c3b92e":"<h1 id=\"noise\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Noise\n        <a class=\"anchor-link\" href=\"#noise\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","2da9c962":"<div style=\"height:200px; width:100%;\">\n    <img style=\"height:200px; width:100%;\" src=\"http:\/\/csc.lsu.edu\/~saikat\/deepsat\/images\/sat_img.png\"\/>\n<\/div>","d70563a5":"<h1 id=\"noisenoise\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Noise + Gaussian Additive Noise\n        <a class=\"anchor-link\" href=\"#noisenoise\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","b463d7a6":"<h1 id=\"nadam\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Nadam\n        <a class=\"anchor-link\" href=\"#nadam\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","351a8fdf":"<h1 id=\"weights\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Weights\n        <a class=\"anchor-link\" href=\"#weights\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","91bf35be":"<h1 id=\"momentum\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Momentum\n        <a class=\"anchor-link\" href=\"#momentum\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","36a7b050":"<h1 id=\"noiseadam\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Noise Adam\n        <a class=\"anchor-link\" href=\"#noiseadam\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","b53c4ce5":"<h1 id=\"activation\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Activation Functions\n        <a class=\"anchor-link\" href=\"#activation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","2d9a8224":"<h1 id=\"nesterov\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Nesterov\n        <a class=\"anchor-link\" href=\"#nesterov\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","549bcc47":"<h1 id=\"adam\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Adam\n        <a class=\"anchor-link\" href=\"#adam\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ed774acb":"<h1 id=\"adadelta\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Adadelta\n        <a class=\"anchor-link\" href=\"#adadelta\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","7e1c881e":"<h1 id=\"rmsprop\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>RMSprop\n        <a class=\"anchor-link\" href=\"#rmsprop\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","33a33240":"<h1 id=\"reference\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Reference\n        <a class=\"anchor-link\" href=\"#reference\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","0ed5bf40":"<h1 id=\"sgdnoise\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>SGD with Gaussian Noise\n        <a class=\"anchor-link\" href=\"#sgdnoise\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","b83635bd":"Full credits goes to Jae Duk Seo and his [Medium](https:\/\/towardsdatascience.com\/only-numpy-implementing-and-comparing-gradient-descent-optimization-algorithms-google-brains-8870b133102b)","592f56c7":"<h1 id=\"analyze\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","19f2309d":"<h1 id=\"dataset\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ffc58a75":"<h1 id=\"sgd\" style=\"color:blue; border: 1px dotted green;\"> \n    <center>SGD\n        <a class=\"anchor-link\" href=\"#sgd\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}