{"cell_type":{"6d01994d":"code","779aba36":"code","543bd970":"code","0cce4432":"code","c71a83b2":"code","a4f32112":"code","966b4274":"code","171b6f5a":"code","d44ac911":"code","84357ff0":"code","3a2c8dc4":"code","1022cb9c":"code","c95a23e5":"code","cd0d74b5":"code","a8157aa4":"code","82c16e56":"code","1f2f2320":"code","91890be8":"code","508c6e37":"code","c444b721":"code","b70a0055":"code","5d541e3e":"code","6f89ff4d":"code","72fbb4e0":"code","0e1d77c7":"code","b3e2bb87":"code","fcab6e77":"code","52f66702":"code","d8b135c2":"code","e3e103de":"markdown","95f230b4":"markdown","a836d1e7":"markdown","0f00f3ec":"markdown","2b1efc1b":"markdown","168e8966":"markdown","8214d820":"markdown","d85ed2bf":"markdown","7a167f2e":"markdown","bdaf1f7a":"markdown","010ebc99":"markdown","749fd26a":"markdown"},"source":{"6d01994d":"%matplotlib inline\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sys\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization,SpatialDropout1D,Bidirectional, Embedding, LSTM\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nimport re\n\nnp.set_printoptions(threshold=sys.maxsize)\n\npd.set_option('display.max_rows',1000)\npd.set_option('display.max_columns',1000)\n\n","779aba36":"# Inladen van de training set\n\ndataset = pd.read_csv('\/kaggle\/input\/rnnstoringen\/storingen.csv')\ndataset.head()","543bd970":"# Toevoegen van 'failure' kolom\ndataset['failure'] = dataset['ttf'].apply(lambda row: 0 if row > 50 else 1)\ndataset.head()","0cce4432":"# Gebalanceerdheid\nsns.countplot(data=dataset, x='failure')","c71a83b2":"# Opsplitsen in training set en test set\nX = dataset[dataset['engine_id'] <= 75]\ny = dataset[dataset['engine_id'] > 75]\n\ntargets = X['failure']\nfeatures = X.drop(['engine_id','failure', 'ttf'], axis=1)\n\ntest_targets = y['failure']\ntest_features = y.drop(['engine_id', 'failure', 'ttf'], axis=1)\n","a4f32112":"features.head()","966b4274":"# Normalisatie van de training set en test set\nscaler = StandardScaler()\nscaler.fit(features)\nfeatures = scaler.transform(features)\ntest_features = scaler.transform(test_features)","171b6f5a":"# Logistic regression\nX_train = features\nX_test = test_features\n\ny_train = targets\ny_test = test_targets\n\nlr = LogisticRegression()\nlr = lr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nprint('Results')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))","d44ac911":"# Random Forest Trees\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('Results')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))","84357ff0":"# Vanilla neural network\nmodel = Sequential()\nmodel.add(Dense(16, activation='relu', input_shape=(25,)))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(0.0004), metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X_train, y_train, epochs=100)","3a2c8dc4":"y_pred = (model.predict(X_test)).reshape(-1,)\ny_pred = [0 if pred < 0.5 else 1 for pred in y_pred]\n\nprint('Results')\nprint(classification_report(y_test, y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))","1022cb9c":"# Data preparation\nseq_lenght = 50 # Zero index list\n\nX_train_seq = []\ny_train_seq = []\n\nX_test_seq = []\ny_test_seq = []\n\nmmscaler = MinMaxScaler()\nmmscaler.fit(dataset.drop(['engine_id', 'ttf', 'failure', 'cycle', 'ttf', 'failure'], axis=1))\n\nengine_ids = dataset['engine_id'].unique()\nfor engine_id in engine_ids:\n    engine_set = dataset[dataset['engine_id'] == engine_id]\n    subset_lenght = len(engine_set)\n\n    for i in range(subset_lenght):\n        if i - seq_lenght < 0:\n            # 0 padding \n            subset = pd.DataFrame(0, columns=list(dataset.columns), index=np.arange(abs(i-seq_lenght)))\n            real_data = engine_set.iloc[:i]\n            subset = subset.append(real_data, ignore_index=True)\n        else: \n            subset = engine_set.iloc[i - (seq_lenght):i]\n            \n        if engine_id <= 75:\n            y_train_seq.append(subset.failure.iloc[-1])\n            X_train_seq.append(mmscaler.transform(subset.drop(['engine_id', 'ttf', 'failure', 'cycle'], axis=1)))\n        else:\n            y_test_seq.append(subset.failure.iloc[-1])\n            X_test_seq.append(mmscaler.transform(subset.drop(['engine_id', 'ttf', 'failure', 'cycle'], axis=1)))","c95a23e5":"X_train_seq = np.array(X_train_seq)\nX_test_seq = np.array(X_test_seq)\ny_train_seq = np.array(y_train_seq)\ny_test_seq = np.array(y_test_seq)","cd0d74b5":"print('X_train:', X_train_seq.shape)\nprint('y_train:', y_train_seq.shape)\nprint('\\n')\nprint('X_test:', X_test_seq.shape)\nprint('X_test:', y_test_seq.shape)","a8157aa4":"# Uitwerking LSTM\n\nlstm = Sequential()\nlstm.add(LSTM(52, input_shape=X_train_seq[0].shape, activation='tanh', dropout=0.4))\nlstm.add(Dense(1, activation='sigmoid'))\nlstm.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(0.0004), metrics=['accuracy'])\nlstm.summary()","82c16e56":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearlystopper = EarlyStopping(patience=7)\ncheckpointer = ModelCheckpoint('lstm-classification.h5', verbose=1, save_best_only=True)\n\nhist = lstm.fit(X_train_seq, y_train_seq, epochs=25, validation_split=0.2, batch_size=32, callbacks=[earlystopper, checkpointer])","1f2f2320":"from tensorflow.keras.models import load_model\n\nlstm_model = load_model('\/kaggle\/working\/lstm-classification.h5')\ny_pred = (lstm_model.predict(X_test_seq)).reshape(-1,)\ny_pred = [0 if pred < 0.5 else 1 for pred in y_pred]\n\nprint('Results')\nprint(classification_report(y_test_seq, y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test_seq, y_pred))","91890be8":"# Uitwerking regressie van de ttf\nregression_targets = X['ttf']\ntest_regression_targets = y['ttf']\n\n# Normalisatie van de training set en test set\n# Gebeurd tijdens classificatie al\n","508c6e37":"# Lineaire regressie\nlreg = LinearRegression()\nlreg.fit(X_train, regression_targets)\n\ny_pred = lreg.predict(X_test)\n\nprint('Results')\nprint('r2:', r2_score(test_regression_targets, y_pred))\nprint('mse:', mean_absolute_error(test_regression_targets, y_pred))","c444b721":"from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=0.1, random_state=42)\nridge.fit(X_train, regression_targets)\n\ny_pred = ridge.predict(X_test)\n\nprint('Results')\nprint('r2:', r2_score(test_regression_targets, y_pred))\nprint('mse:', mean_absolute_error(test_regression_targets, y_pred))","b70a0055":"# Random forest regressor\nfr = RandomForestRegressor(n_estimators=60, criterion='mae', random_state = 42, verbose=3, n_jobs=-1)\nfr.fit(X_train, regression_targets)\n\ny_pred = fr.predict(X_test)\n\nprint('Results')\nprint('r2:', r2_score(test_regression_targets, y_pred))\nprint('mse:', mean_absolute_error(test_regression_targets, y_pred))","5d541e3e":"# Data preparation\nseq_lenght = 50 # Zero index list\n\nX_train_seq_reg = []\ny_train_seq_reg = []\n\nX_test_seq_reg = []\ny_test_seq_reg = []\n\nmmscaler = MinMaxScaler()\nmmscaler.fit(dataset.drop(['engine_id', 'cycle', 'ttf', 'failure'], axis=1))\n\nengine_ids = dataset['engine_id'].unique()\nfor engine_id in engine_ids:\n    engine_set = dataset[dataset['engine_id'] == engine_id]\n    subset_lenght = len(engine_set)\n\n    for i in range(subset_lenght):\n        if i - seq_lenght < 0:\n            # 0 padding \n            subset = pd.DataFrame(0, columns=list(dataset.columns), index=np.arange(abs(i-seq_lenght)))\n            real_data = engine_set.iloc[:i]\n            subset = subset.append(real_data, ignore_index=True)\n        else: \n            subset = engine_set.iloc[i - (seq_lenght):i]\n            \n        if engine_id <= 75:\n            y_train_seq_reg.append(subset.ttf.iloc[-1])\n            X_train_seq_reg.append(mmscaler.transform(subset.drop(['engine_id', 'ttf', 'failure', 'cycle'], axis=1)))\n        else:\n            y_test_seq_reg.append(subset.ttf.iloc[-1])\n            X_test_seq_reg.append(mmscaler.transform(subset.drop(['engine_id', 'ttf', 'failure', 'cycle'], axis=1)))","6f89ff4d":"X_train_seq_reg = np.array(X_train_seq_reg)\nX_test_seq_reg = np.array(X_test_seq_reg)\ny_train_seq_reg = np.array(y_train_seq_reg)\ny_test_seq_reg = np.array(y_test_seq_reg)","72fbb4e0":"# Predictie van de resterende cycli via LSTM\n\nfrom tensorflow.keras.layers import Bidirectional\n\nlstm = Sequential()\nlstm.add(Bidirectional(LSTM(49, input_shape=X_train_seq_reg[0].shape, return_sequences=True, activation='tanh', dropout=0.2)))\nlstm.add(Dropout(0.3))\nlstm.add(Bidirectional(LSTM(90, return_sequences=True, activation='tanh', dropout=0.2)))\nlstm.add(Dropout(0.3))\nlstm.add(Bidirectional(LSTM(49, return_sequences=True, activation='tanh', dropout=0.2)))\nlstm.add(Dropout(0.2))\nlstm.add(Dense(1, activation='linear'))\nlstm.compile(loss='mse', optimizer=tensorflow.keras.optimizers.Adam(0.0004), metrics=['mse'])","0e1d77c7":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearlystopper = EarlyStopping(patience=7)\ncheckpointer = ModelCheckpoint('lstm-regression.h5', verbose=1, save_best_only=True)\n\nhist = lstm.fit(X_train_seq_reg, y_train_seq_reg, epochs=250, validation_split=0.2, batch_size=64, callbacks=[earlystopper, checkpointer], shuffle=False)","b3e2bb87":"from tensorflow.keras.models import load_model\n\nlstm_model = load_model('\/kaggle\/working\/lstm-regression.h5')\ny_pred = lstm_model.predict(X_test_seq)\ny_pred = [np.average(pred) for pred in y_pred]\n\nprint('Results')\nprint('r2:', r2_score(test_regression_targets, y_pred))\nprint('mae:', mean_absolute_error(test_regression_targets, y_pred))","fcab6e77":"# histogram van de error (y_pred - y_test)\nfig, ax = plt.subplots(figsize=(16, 10))\nax.hist(y_pred - test_regression_targets)\nax.set_title('Mean absolute error LSMT')\nax.set_xlabel('mae')\nax.set_ylabel('amount')\nplt.show()","52f66702":"# Classificatie via een CNN\n\n","d8b135c2":"# Regressie via CNN\n\n","e3e103de":"#### Voorspelling op basis van time series data","95f230b4":"Je kan vermoeden dat het al dan niet falen van de motoren binnen een zeker tijdinterval niet alleen bepaald wordt door de huidige meetwaarden maar voor een deel ook afgeleid kan worden uit het tijdsverloop van de meetwaarden.(=tijdreeks). Dit betekent dat de modellen rekening moeten houden met de huidige sample maar ook met een aantal voorgaande samples. Daarom zullen we onze toevlucht nemen tot recurrent neurals networks (LSTM of GRU) die via feedback verbindingen uitermate geschikt zijn om tijdsafhankelijkheden in time series te capteren.\n\n\nDoorloop de volgende stappen:\n\n- Opbouwen van de training set en test set:\n\nDe training set en test set zullen hier bestaan uit sequenties met zekere lengte (seq_length). Neem om te starten een seq_length=30. Om deze sequenties op te bouwen ga je als volgt te werk:\nOverloop per engine_id de tijdreeks met een sliding window. Het window start bij de rij met de hoogste ttf en schuift rij per rij verder tot de motor faalt (laatste meting van die motor). De features die telkens binnen het window vallen vormen een sequentie (van in dit geval 30 waarden). Dit zijn de meest recente meetwaarde samen met 29 voorgaande meetwaarden. Bij de start zijn er nog geen voorgaande waarden, vandaar dat zero padding toegepast kan worden om toch tot een sequentie van 30 te komen. In plaats van zero padding kan je ook padding doen met de randwaarden (de meting aan het begin van de sequentie meerdere keren naar het verleden copi\u00ebren). De bij een sequentie horende target waarde is de 'failure' waarde die hoort bij de meest recente meetwaarden in de sequentie.\n\n- Compileer een training set en een test set. \n\nDe samples (=sequenties) afkomstig van de eerste 75 motoren komen in de training set, de resterende in de test set. \n\n\n- Normaliseer de training set en test set (bijvoorbeeld via een MinMAX scaler)\n\n\n- Trainen van een LSTM netwerk\n\nTrain een LSTM op de training set en varieer de hyperparameters om een zo hoog mogelijke accuraatheid te bekomen. Mogelijke hyperparameters van het netwerk zijn de batch size, aantal layers, het aantal units per layer, dropout, recurrent dropout, optimizer, ...\nProbeer eveneens een bidirectional LSTM. Bijvoorbeeld: model.add(Bidirectional(LSTM(64)))\n\n- Evalueer het LSTM netwerk en vergelijk de accuracy met deze van de niet-temporele modellen.\n\n- Optimaliseer het neuraal netwerk om het aantal false negatives te reduceren (een falen binnen de 50 cylussen dat niet ontdekt wordt)\n\n- Onderzoek of je met langere of kortere sequenties betere resultaten behaalt. \n\n\n\n\n","a836d1e7":"Maak op basis van de test set een histogram van de verschillen tussen de voorspelde ttf en de werkelijke ttf.\nWat kan je daaruit besluiten? Lijkt het model eerder een onderschatting of een overschatting te maken van de ttf?\n","0f00f3ec":"#### Preprocessing\n\nVoeg een extra 'failure' kolom toe aan de dataset die weergeeft of de motor al dan niet binnen de 50 cycli zal falen. Met andere woorden: is de ttf groter dan 50 is de waarde 0, is de ttf kleiner of gelijk aan 50, dan is de waarde gelijk aan 1.\n\nBekijk ook in welke mate de data gebalanceerd is.\n","2b1efc1b":"# Sessie 05 - Recurrent Neural Networks - Assignment ","168e8966":"#### Predictie op basis van enkelvoudige data samples (geheugenloos)","8214d820":"Vooraleer de data als tijdreeks te beschouwen en LSTM of GRU modellen te trainen is het interessant om eerst te bekijken hoe goed je kan voorspellen met modellen die de temporele component niet gebruiken en elke meting onafhankelijk van de vorige beschouwen.\nConcreet betekent dit dat je voor een bepaald tijdstip op basis van de features de target probeert te voorspellen, zonder te kijken naar de voorgaande metingen. Elke rij in de dataset wordt onafhankelijk van de andere bekeken.\n\n- Splits de dataset op in een training set en een test set. De eerste 75 motoren worden gebruikt om te trainen, de andere motoren om te testen.\n- Train een logistic regression classifier en een random forest tree op de training set en evalueer met de test set. Bespreek de bekomen resultaten.\n- Train een vanilla neuraal netwerk, waarmee een klassiek feedforward neuraal netwerk wordt bedoeld. Evalueer en bespreek.","d85ed2bf":"De opdracht bestaat erin een LSTM te trainen voor het voorspellen van storingen in motoren. Het bestand 'storingen.csv' bevat de data.\nDe dataset is tot stand gekomen door 100 motoren van hetzelfde type te bemeten en te tellen hoeveel cycli ze nog operationeel zijn vooraleer een storing zich manifesteert. \n\n\nDe dataset is als volgt opgebouwd:\n- engine_ID = Het ID een bepaalde motor.\n- cycle = nummer van de cyclus van de motor.\n- setting1, setting2, setting3 = de verschillende operationele modi.\n- sensor1 -> sensor 21: metingen van de 21 verschillende sensoren.\n- ttf = time to failure: hoeveel cycli de motor nog kan draaien vooraleer er een storing optreedt.\n\n\nEen eerste benadering is het probleem te vereenvoudigen door het te vertalen naar een **classificatieprobleem**. Hierbij probeer je te voorspellen of de motor bijvoorbeeld binnen de komende 50 cycli al dan niet zal falen. \n\n\nDe tweede benadering is het probleem beschouwen als een **regressieprobleem** waarbij je de resterende cycli (ttf) probeert te voorspellen.","7a167f2e":"## 1.2 Regressie van de ttf\n\n- In plaats van te voorspellen of de motor zal falen binnen 50 cycli, voorspel je nu het aantal resterende cycli. De target is nu de time to failure (ttf).\n- Splits ook hier de dataset op in een training set en een test set. De data van de eerste 75 motoren wordt gebruikt om mee te trainen, de andere motoren om mee te testen.\n- Train een lineair regressiemodel en een random forest regressor op de training set en evalueer met de test set. Bespreek de bekomen resultaten in termen van Mean Absolute Error (MAE) en de R\u00b2-score.\n- Train vervolgens een LSTM. Dit is vergelijkbaar met wat je hebt gedaan bij classificatie. Let wel op dat je de juiste activatie- en loss functies gebruikt. Vergelijk de bekomen MAE en de R\u00b2-score met deze van de niet-temporele modellen.","bdaf1f7a":"## 1. Opsporen van storingen","010ebc99":"### 1.1 Classificatie van de ttf","749fd26a":"### Uitbreiding: time series prediction met convolutional neural networks\n\nAls uitbreiding kan je proberen om een convolutional neural network te trainen op de sequenties. Elke sequenties kan je zien als een 2D array, gelijkaardig aan een afbeelding. \nHet trainen zal dus gelijkaardig zijn aan wat je gedaan hebt bij classificatie van images, alleen is de afbeelding hier een sequentie van sensorwaarden en de klasse het al dan niet falen binnen 50 cycli.\n\nProbeer ook het regressieprobleem uit via een convolutional neural network waarbij je de ttf voorspelt.\n"}}