{"cell_type":{"ec2f9c98":"code","66756565":"code","ce177cb9":"code","5c674354":"code","e0c70f47":"code","03b55378":"code","5c97c191":"code","21064069":"code","db954056":"code","496487eb":"code","db57215a":"code","ea3e1636":"code","07bec08f":"code","d269a684":"code","ab436db6":"code","71ae5aaf":"code","8eacb499":"code","079d2edf":"code","2f00f09c":"code","861d9dea":"code","869752c7":"code","6d5c6fb3":"code","a7f56562":"code","bb98e6e4":"code","57cbbbb4":"code","a8aa820f":"code","85364cbc":"code","bd0369f2":"code","36d22f82":"code","d3a93bfa":"code","7b8f103b":"code","4c9f775d":"code","c0eed347":"code","72afd400":"code","80b58555":"code","56502c67":"code","782cfebb":"code","30845ef5":"code","b7a73501":"code","bbf19a91":"code","25d594f4":"code","4f95bdeb":"code","30fba93c":"code","9a9bec8f":"code","1d78011f":"code","8647ae36":"code","2ce901fa":"code","4ec4af89":"code","1c41f5e9":"code","6b38dd5d":"code","0d02940c":"code","40d35c63":"code","0d118ab7":"code","1936f7d4":"code","41900b42":"code","53f496de":"code","f414e752":"code","933994df":"code","03a31744":"code","6083f84a":"code","677d842e":"code","39bc33d4":"code","39dfe9c5":"code","ff04b19a":"code","6faf35fe":"code","21bcbe62":"code","65952708":"code","412326c8":"code","ce406fd7":"code","aa9b1bf9":"code","71c90fed":"code","ee5fa2b9":"code","7277de1c":"code","8b447c9f":"code","607f945e":"code","92b23da2":"code","67a53496":"code","0132e494":"code","82377d92":"code","4a805958":"code","ec833955":"code","4c3c0575":"code","d321ee87":"code","4f40e31e":"code","eb81e424":"code","3a5a1b96":"code","7bfde49a":"code","5e795b16":"code","0484573e":"code","e247f83b":"code","64eb2162":"code","66627443":"code","195c2c65":"code","660e7a94":"code","608d23ca":"code","4e07c78a":"code","76948c26":"code","e7a8afc1":"code","bc2baeaf":"code","7df73cd3":"code","4a6409a5":"markdown","58e4917b":"markdown","d83facc0":"markdown","189ae20b":"markdown","f5c7581a":"markdown","ce9fe8ca":"markdown","afba135b":"markdown","021e4275":"markdown","4ac52cca":"markdown","c10a4bab":"markdown","eb90ba88":"markdown","4f8a291d":"markdown","3e5bf5cf":"markdown"},"source":{"ec2f9c98":"import numpy as np\nimport pandas as pd\n\nimport scipy\nfrom scipy import stats\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\n\nOUTPUT_PATH = '.\/'\n\nUPLOAD_PREVIOUS = True\n\nFEATURE_TYPE = 'scaled'\n\nprint(\"Setup Complete\")","66756565":"dataset_paths = {\n    'categories': '..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv',\n    'items': '..\/input\/competitive-data-science-predict-future-sales\/items.csv',\n    'sales': '..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv',\n    'shops': '..\/input\/competitive-data-science-predict-future-sales\/shops.csv',\n    'test': '..\/input\/competitive-data-science-predict-future-sales\/test.csv'\n}\npreproceed_paths = {\n    'df_rolled': '\/kaggle\/input\/fstsfresh\/df_rolled.csv',\n    'features': '\/kaggle\/input\/fstsfresh\/features.csv',\n    'features_filtered': '\/kaggle\/input\/fstsfresh\/features_filtered.csv'\n}\nprint('Paths are ready')","ce177cb9":"dataset = { name: pd.read_csv(path) for name, path in dataset_paths.items()}\ndataset.keys()","5c674354":"proceed = { name: pd.read_csv(path) for name, path in preproceed_paths.items()}\ndataset.keys()","e0c70f47":"sales = dataset['sales']\nitems = dataset['items']\ncategories = dataset['categories']\nshops = dataset['shops']","03b55378":"sales.date = sales.date.astype('datetime64[ns]')\n\nprint(\"Before:\", sales.shape)\n\nfrom datetime import date\n\nsales = sales.loc[sales.date < np.datetime64(date(2015, 11, 1))]\nsales.tail()\n\nsales_train = sales[\n    (sales[\"item_cnt_day\"] < 1000)\n    & (sales[\"item_price\"] > 0)\n    & (sales[\"item_price\"] < 60000)\n].copy()\nprint(\"After:\", sales.shape)\n\nsales.head()","5c97c191":"force_category = {\n    'category': {\n        \"PC - \u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438\": \"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\",\n        \"\u0418\u0433\u0440\u044b MAC - \u0426\u0438\u0444\u0440\u0430\": \"\u0418\u0433\u0440\u044b\",\n        \"\u0418\u0433\u0440\u044b Android - \u0426\u0438\u0444\u0440\u0430\": \"\u0418\u0433\u0440\u044b\",\n        \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)\": \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438\",\n        \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)\": \"\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438\",\n    },\n    'shop': {\n        '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d \u0427\u0421': '\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d',\n        '\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d': '\u0421\u043a\u043b\u0430\u0434',\n        '\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f': '\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f',\n        '!\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56 \u0444\u0440\u0430\u043d': '\u042f\u043a\u0443\u0442\u0441\u043a',\n        '!\u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\" \u0444\u0440\u0430\u043d': '\u042f\u043a\u0443\u0442\u0441\u043a',\n    },\n}\n\npattern = {\n    'category': ' - ',\n    'shop': ' ',\n}\n\ndef create_transformer(force_category, pattern):\n    def _wrapped(value):\n        if value in force_category:\n            return force_category[value]\n\n        split = value.split(pattern)\n        if len(split) > 1:\n            return split[0]\n\n        return value\n    return _wrapped\n    \nmake_cat_name = create_transformer(force_category['category'], pattern['category'])\nmake_city_name = create_transformer(force_category['shop'], pattern['shop'])","21064069":"class PreprocessignPipeline:\n    \n    class PipelineIterator:\n        def __init__(self, dataset, tasks, task_queue):\n            self.tasks = tasks\n            self.task_queue = task_queue\n            self.dataset = dataset\n            self.current_task = None\n            self.result_storage = {}\n            self.proceed = False\n            \n        def __iter__(self):\n            if not self.proceed:\n                dataset = self.dataset\n                for task in self.task_queue:\n                    self.current_task = self.tasks[task]\n                    try:\n                        proceed_task = self.current_task(dataset)\n                        if not proceed_task is None:\n                            dataset = proceed_task\n                        self.result_storage[task] = dataset\n                        print(f'Stage - {task} complete')\n                    except:\n                        print(f'Exception occured in stage {task}')\n                        raise\n                    yield self.result_storage[task]\n                self.proceed = True\n            else:\n                for task in self.task_queue:\n                    yield self.result_storage[task]\n            \n        def proceed_all(self):\n            if not self.proceed:\n                dataset = self.dataset\n                for task in self.task_queue:\n                    self.current_task = self.tasks[task]\n                    try:\n                        proceed_task = self.current_task(dataset)\n                        if not proceed_task is None:\n                            dataset = proceed_task\n                        self.result_storage[task] = dataset\n                        print(f'Stage - {task} complete')\n                    except:\n                        print(f'Exception occured in stage {task}')\n                        raise\n                    self.proceed = True\n            return self.result_storage\n        \n    def __init__(self, tasks, task_queue):\n        self.tasks = tasks\n        self.task_queue = task_queue\n        \n    def __call__(self, dataset):\n        return self.PipelineIterator(dataset, self.tasks, self.task_queue)","db954056":"data_preprocessing = {}\n\n# Add column created by transformer\ndef append_columns(dataset, columns, transformers):\n    for column, transformer in zip(columns, transformers):\n        dataset[column] = transformer(dataset)\n\n# Add corresponding category and shop id's to each sale\ndata_preprocessing['id_merging_stage'] = lambda dataset: dataset.merge(\n    items, \n    on='item_id'\n).merge(\n    shops,\n    on='shop_id'\n).merge(\n    categories,\n    on='item_category_id'\n)\n\n# Add summary among shop_id and category_id above similar time periods (daily intervals)\ndata_preprocessing['summarizing_and_name_merging_stage'] = lambda dataset: dataset.groupby(\n    ['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_category_name', 'shop_name']\n).item_cnt_day.sum().reset_index().sort_values('date')\n\ndata_preprocessing['add_generalized_names_and_encode_stage'] = lambda dataset: append_columns(\n    dataset=dataset, \n    columns=[\n        'global_item_category_name',\n        'city_name',\n        'global_item_category_name_id',\n        'city_id',\n    ], \n    transformers=[\n        lambda _dataset: _dataset[\"item_category_name\"].apply(\n            make_cat_name\n        ),\n        lambda _dataset: _dataset['shop_name'].apply(\n            make_city_name\n        ),\n        lambda _dataset: LabelEncoder().fit_transform(_dataset['global_item_category_name']),\n        lambda _dataset: LabelEncoder().fit_transform(_dataset['city_name']),\n    ]\n)\n\ndata_preprocessing['create_full_matrix_stage'] = lambda _dataset: _dataset.set_index('date') \\\n    .groupby([\n        'shop_id',\n        'item_category_id',\n        'date_block_num',\n        'city_id',\n        'global_item_category_name_id',\n        'id'\n    ]).item_cnt_day.sum() \\\n    .reset_index().rename(columns={'item_cnt_day': 'item_cnt_month'}) \\\n    .groupby(['shop_id', 'item_category_id', 'date_block_num', 'city_id', 'global_item_category_name_id', 'id']).item_cnt_month.sum().reset_index() \\\n    .groupby(['date_block_num', 'id']).item_cnt_month.sum().unstack().fillna(0) \\\n    .stack().reset_index().rename(columns={0:'item_cnt_month'}) \\","496487eb":"pipeline = PreprocessignPipeline(\n    tasks=data_preprocessing, \n    task_queue = [\n        'id_merging_stage',\n        'summarizing_and_name_merging_stage',\n        'add_generalized_names_and_encode_stage',\n    ]\n)","db57215a":"pipeline_test = pipeline(sales)\npipeline_train = pipeline(sales_train)","ea3e1636":"_ = pipeline_test.proceed_all()\n_ = pipeline_train.proceed_all()","07bec08f":"task_df = {}\n\ntask_df['test'] = pipeline_test.result_storage['add_generalized_names_and_encode_stage']\ntask_df['train'] = pipeline_train.result_storage['add_generalized_names_and_encode_stage']","d269a684":"task_df['test']","ab436db6":"idx = task_df['test'].loc[:,['city_id', 'global_item_category_name_id', 'city_name', 'global_item_category_name']].value_counts().sort_index()\nidx = pd.DataFrame({'id': [i for i in range(idx.size)]}, idx.index)\nidx.reset_index(inplace=True)","71ae5aaf":"task_df['test'] = task_df['test'].merge(idx, on=['city_id', 'global_item_category_name_id', 'city_name', 'global_item_category_name'])\ntask_df['train'] = task_df['train'].merge(idx, on=['city_id', 'global_item_category_name_id', 'city_name', 'global_item_category_name'])","8eacb499":"idx['pair_name'] = idx['city_name'] + ' - ' + idx['global_item_category_name']","079d2edf":"task_df_copy['test']","2f00f09c":"task_df_copy = {}\n\ntask_df_copy['test'] = task_df['test'].copy()\ntask_df_copy['test']['pair_name'] = task_df_copy['test']['city_name'] + ' - ' + task_df_copy['test']['global_item_category_name']\n\ntask_df['test'] = data_preprocessing['create_full_matrix_stage'](task_df['test'])\ntask_df['train'] = data_preprocessing['create_full_matrix_stage'](task_df['train'])","861d9dea":"raw_dataset = task_df['train']","869752c7":"from tsfresh import extract_features, select_features","6d5c6fb3":"def get_target_values(dataset):\n    return dataset.groupby(['date_block_num', 'id']).item_cnt_month.sum().unstack(0)\n\ndef upload_df(dataset, path, name):\n    with open(os.path.join(path, name + '.csv'), 'w+') as writer:\n        dataset.to_csv(writer)","a7f56562":"from tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frame\n\nif not UPLOAD_PREVIOUS:\n    df_rolled = roll_time_series(raw_dataset, column_id='id', column_sort='date_block_num', min_timeshift=11, max_timeshift=33, rolling_direction=1)\n    upload_df(df_rolled.reset_index(), OUTPUT_PATH, 'df_rolled')\nelse:\n    df_rolled = proceed['df_rolled']\n    df_rolled = df_rolled.set_index('Unnamed: 0').sort_index()","bb98e6e4":"df_rolled","57cbbbb4":"df_rolled.id.value_counts().sort_index()","a8aa820f":"y_rolled = get_target_values(task_df['train'])\ny_rolled_test = get_target_values(task_df['test'])\ny_rolled_test = y_rolled_test.loc[:, 11:].unstack()\ny_rolled = y_rolled.loc[:, 11:].unstack()","85364cbc":"y_rolled = y_rolled.reset_index().set_index(['id', 'date_block_num']).loc[:,0].fillna(0)\ny_rolled_test = y_rolled_test.reset_index().set_index(['id', 'date_block_num']).loc[:,0].fillna(0)","bd0369f2":"y_rolled.index","36d22f82":"if not UPLOAD_PREVIOUS:\n    features = extract_features(df_rolled, column_id='id', column_sort='date_block_num')\n    upload_df(features.reset_index(), OUTPUT_PATH, 'features')\nelse:\n    features = proceed['features'].drop('Unnamed: 0', axis=1)\n    features = features.rename(columns={'level_0':'id', 'level_1':'date_block_num'}).set_index(['id', 'date_block_num'])","d3a93bfa":"features","7b8f103b":"y_rolled = y_rolled[set(y_rolled.index) & set(features.index)]","4c9f775d":"y_rolled.reset_index()","c0eed347":"from tsfresh.utilities.dataframe_functions import impute\n\nimpute(features)","72afd400":"if not UPLOAD_PREVIOUS:\n    features_filtered = select_features(features, y_rolled)\n    features_filtered = features_filtered.reset_index().rename(columns={'level_0': 'id', 'level_1': 'date_block_num'})\n    upload_df(features_filtered.reset_index(), OUTPUT_PATH, 'features_filtered')\nelse:\n    features_filtered = proceed['features_filtered'].drop('index', axis=1)\n    features_filtered.drop('Unnamed: 0', axis=1, inplace=True)","80b58555":"features_filtered","56502c67":"# assert False","782cfebb":"features_filtered[features_filtered.date_block_num == 11].set_index(['id', 'date_block_num'])","30845ef5":"from sklearn.preprocessing import StandardScaler","b7a73501":"features = {}","bbf19a91":"def standartize(dataset):\n    scaler = StandardScaler()\n    try:\n        scaler.fit(dataset.to_numpy())\n        return scaler.transform(dataset.to_numpy())\n    except:\n        scaler.fit(dataset)\n        return scaler.transform(dataset)","25d594f4":"df = pd.DataFrame(\n    data=standartize(features_filtered), \n    index=features_filtered.index, \n    columns=features_filtered.columns\n)\ndf['id'] = features_filtered['id']\ndf['date_block_num'] = features_filtered['date_block_num']\nscaled_features = df","4f95bdeb":"features['filtered'] = features_filtered\nfeatures['scaled'] = scaled_features","30fba93c":"y_rolled.reset_index()[y_rolled.reset_index().date_block_num == 11].set_index(['id', 'date_block_num'])","9a9bec8f":"from sklearn.metrics import mean_squared_error as mse\n\ndef get_features(date, train_df):\n    return train_df[train_df.date_block_num == date].set_index(['id', 'date_block_num']).sort_index()\n        \ndef get_target(date, target_vector):\n    return target_vector.reset_index()[target_vector.reset_index().date_block_num == date].set_index(['id', 'date_block_num']).sort_index()\n\nclass RegressionValidator:\n    def __init__(self, model, *args, **kwargs):\n        self.model = model(**kwargs)\n        \n    def validate(self, X_features, target_vector, test_target):\n        train_df = X_features\n        \n        def get_features(date):\n            return train_df[train_df.date_block_num == date].set_index(['id', 'date_block_num']).sort_index()\n        \n        current_date = 23\n        max_date = 32\n        \n        errors = [\n            [], [], []\n        ]\n        \n        while current_date < max_date:\n            current_features = get_features(current_date)\n            validation_window = get_features(current_date + 1)\n#             print(validation_window )\n            \n            current_target = get_target(current_date, target_vector)\n            validation_target = get_target(current_date + 1, test_target)\n\n            fitted = self.model.fit(current_features, current_target)\n\n            predictions = self.model.predict(validation_window)\n            \n#             print(validation_target.loc[:,0].to_list())\n            if predictions.transpose().shape[0] == 1:\n                predictions = predictions.transpose()[0]\n            else:\n                predictions = predictions.transpose()\n        \n            errors[0].append(current_date)\n            assert validation_target.loc[:,0].shape == predictions.shape, f'Shapes are pred:{validation_target.loc[:,0].shape} and truth:{predictions.shape}\\nCurrent validation set: {current_date}'\n            errors[1].append(mse(validation_target.loc[:,0].to_list(), predictions))\n            \n            report = pd.DataFrame({'true_values': validation_target.loc[:,0].to_list(), 'predicted': predictions})\n            errors[2].append(report)\n            \n            current_date += 1\n        \n        return errors\n        \n        ","1d78011f":"def get_statistics(report):\n    stat = report.copy()\n    stat['residuals'] = stat['predicted'] - stat['true_values'] \n    stat['id'] = stat.index\n    stat['abs_residuals'] = stat['residuals'].abs()\n    stat['percentage'] = 2*(stat['residuals'])\/(stat['true_values'] + stat['predicted'])\n    return stat","8647ae36":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV","2ce901fa":"errors = {}","4ec4af89":"validator_lin = RegressionValidator(LinearRegression)\nerrors['original'] = validator_lin.validate(features[FEATURE_TYPE], y_rolled, y_rolled_test)\nvalidator_ridge = RegressionValidator(Ridge, alpha=1e-10)\nerrors['ridge'] = validator_ridge.validate(features[FEATURE_TYPE], y_rolled, y_rolled_test)\nvalidator_ridge = RegressionValidator(Lasso, alpha=0.275)\nerrors['lasso'] = validator_ridge.validate(features[FEATURE_TYPE], y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(3, 2, figsize=(20, 17))\n\naxs[0][0].set_title(\"MSE histogram in validation scheme [original]\")\naxs[0][1].set_title(\"MSE scatterplot [original]\")\naxs[0][1].set_xlabel('date_block_num')\naxs[0][1].set_ylabel('MSE')\naxs[1][0].set_title(\"MSE histogram in validation scheme [ridge]\")\naxs[1][1].set_title(\"MSE scatterplot [ridge]\")\naxs[1][1].set_xlabel('date_block_num')\naxs[1][1].set_ylabel('MSE')\naxs[2][0].set_title(\"MSE histogram in validation scheme [lasso]\")\naxs[2][1].set_title(\"MSE scatterplot [lasso]\")\naxs[2][1].set_xlabel('date_block_num')\naxs[2][1].set_ylabel('MSE')\n\nsns.scatterplot(errors['original'][0], errors['original'][1], ax=axs[0][1])\nsns.histplot(errors['original'][1], ax=axs[0][0])\nsns.scatterplot(errors['ridge'][0], errors['ridge'][1], ax=axs[1][1])\nsns.histplot(errors['ridge'][1], ax=axs[1][0])\nsns.scatterplot(errors['lasso'][0], errors['lasso'][1], ax=axs[2][1])\nsns.histplot(errors['lasso'][1], ax=axs[2][0])","1c41f5e9":"def stat_info(errors):\n    mean_residuals = pd.DataFrame({'abs_resid': [0 for _ in range(438)], 'resid': [0 for _ in range(438)]}, [i for i in range(438)])\n    residual_series = None\n    for _idx, report in enumerate(errors[2]):\n        residuals = get_statistics(report)\n        mean_residuals['abs_resid'] += residuals['abs_residuals']\n        mean_residuals['resid'] += residuals['residuals']\n        resids = residuals['residuals'].reset_index()\n        resids['window'] = _idx\n        resids.set_index(['index', 'window'], inplace=True)\n        if residual_series is None:\n            residual_series = resids\n        else:\n            residual_series = pd.concat([residual_series, resids])\n    mean_residuals \/= len(errors[2])\n    return mean_residuals, residual_series","6b38dd5d":"maximal_feature = get_features(31, features[FEATURE_TYPE])\nmaximal_target = get_target(31, y_rolled)\n\nvalidation_feature = get_features(32, features[FEATURE_TYPE])\nvalidation_target = get_target(32, y_rolled)\n\nalpha = [1e-11, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]","0d02940c":"ridge_res = RidgeCV(alpha, store_cv_values=True).fit(maximal_feature, maximal_target)\nsns.scatterplot(np.log(alpha), np.log([abs(ridge_res.cv_values_.transpose()[i][0]).sum() for i in range(15)]))","40d35c63":"# from time import sleep\n\n# while True:\n#     sleep(3)","0d118ab7":"proceed_residuals = {}\n\nproceed_residuals['original'] = stat_info(errors['original'])\nproceed_residuals['ridge'] = stat_info(errors['ridge'])\nproceed_residuals['lasso'] = stat_info(errors['lasso'])","1936f7d4":"stable_residual_interval = proceed_residuals['original'][1].reset_index().rename(columns={'index': 'id'}).merge(idx)\nstable_residual_interval['abs_residuals'] = abs(stable_residual_interval['residuals'])\nsns.boxplot(data=stable_residual_interval, x='residuals')","41900b42":"total_error = stable_residual_interval.residuals @ stable_residual_interval.residuals\noutliers = stable_residual_interval[stable_residual_interval.abs_residuals > 93]\noutliers_error = outliers.residuals @ outliers.residuals\nprint(f'{len(outliers)\/len(stable_residual_interval)} residuals cause {outliers_error\/total_error} mistake')","53f496de":"giants = []\ntotals = []\n\nfor i in range(9):\n    slice_ = stable_residual_interval[stable_residual_interval.window == i]    \n    giants.append(slice_.sort_values('abs_residuals').tail(22))\n    totals.append(slice_.residuals @ slice_.residuals)\n    giants[i]['part'] = (slice_.residuals ** 2) \/ totals[i]\n    print(f'Window {i}: 5% cause {(giants[i].residuals @ giants[i].residuals)\/(totals[i])}')","f414e752":"giant_outliers = pd.concat(giants)\n\npair_name_bag = tuple(giant_outliers.loc[:,['pair_name', 'window', 'residuals']].set_index(['pair_name', 'window']).unstack().fillna(0).index)\n\ngiant_outliers_matrix = {}\n\ntarget = stable_residual_interval[stable_residual_interval.pair_name.isin(pair_name_bag)]\n\ngiant_outliers_matrix['pair_name'] = target.loc[:,['pair_name', 'window', 'residuals']].set_index(['pair_name', 'window']).unstack().fillna(0)","933994df":"giant_outliers_matrix['pair_name'].transpose().plot(figsize=(30, 24))","03a31744":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print (giant_outliers.pair_name.value_counts())\nsns.lineplot(x=giant_outliers.pair_name.value_counts().value_counts().index.to_list(), y=giant_outliers.pair_name.value_counts().value_counts().to_list())","6083f84a":"stable_residual_interval = proceed_residuals['ridge'][1].reset_index().rename(columns={'index': 'id'}).merge(idx)\nstable_residual_interval['abs_residuals'] = abs(stable_residual_interval['residuals'])\nsns.boxplot(data=stable_residual_interval, x='residuals')","677d842e":"total_error = stable_residual_interval.residuals @ stable_residual_interval.residuals\noutliers = stable_residual_interval[stable_residual_interval.abs_residuals > 47]\noutliers_error = outliers.residuals @ outliers.residuals\nprint(f'{len(outliers)\/len(stable_residual_interval)} residuals cause {outliers_error\/total_error} mistake')","39bc33d4":"giants = []\ntotals = []\n\nfor i in range(9):\n    slice_ = stable_residual_interval[stable_residual_interval.window == i]    \n    giants.append(slice_.sort_values('abs_residuals').tail(22))\n    totals.append(slice_.residuals @ slice_.residuals)\n    giants[i]['part'] = (slice_.residuals ** 2) \/ totals[i]\n    print(f'Window {i}: 5% cause {(giants[i].residuals @ giants[i].residuals)\/(totals[i])}')","39dfe9c5":"giant_outliers = pd.concat(giants)\n\npair_name_bag = tuple(giant_outliers.loc[:,['pair_name', 'window', 'residuals']].set_index(['pair_name', 'window']).unstack().fillna(0).index)\n\ngiant_outliers_matrix = {}\n\ntarget = stable_residual_interval[stable_residual_interval.pair_name.isin(pair_name_bag)]\n\ngiant_outliers_matrix['pair_name'] = target.loc[:,['pair_name', 'window', 'residuals']].set_index(['pair_name', 'window']).unstack().fillna(0)","ff04b19a":"giant_outliers_matrix['pair_name'].transpose().plot(figsize=(30, 24))","6faf35fe":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print (giant_outliers.pair_name.value_counts())\nsns.lineplot(x=giant_outliers.pair_name.value_counts().value_counts().index.to_list(), y=giant_outliers.pair_name.value_counts().value_counts().to_list())","21bcbe62":"stable_residual_interval = proceed_residuals['lasso'][1].reset_index().rename(columns={'index': 'id'}).merge(idx)\nstable_residual_interval['abs_residuals'] = abs(stable_residual_interval['residuals'])\nsns.boxplot(data=stable_residual_interval, x='residuals')","65952708":"total_error = stable_residual_interval.residuals @ stable_residual_interval.residuals\noutliers = stable_residual_interval[stable_residual_interval.abs_residuals > 47]\noutliers_error = outliers.residuals @ outliers.residuals\nprint(f'{len(outliers)\/len(stable_residual_interval)} residuals cause {outliers_error\/total_error} mistake')","412326c8":"giants = []\ntotals = []\n\nfor i in range(9):\n    slice_ = stable_residual_interval[stable_residual_interval.window == i]    \n    giants.append(slice_.sort_values('abs_residuals').tail(22))\n    totals.append(slice_.residuals @ slice_.residuals)\n    giants[i]['part'] = (slice_.residuals ** 2) \/ totals[i]\n    print(f'Window {i}: 5% cause {(giants[i].residuals @ giants[i].residuals)\/(totals[i])}')","ce406fd7":"giant_outliers","aa9b1bf9":"giant_outliers = pd.concat(giants)\n\npair_name_bag = tuple(giant_outliers.loc[:,['pair_name', 'window', 'residuals']].set_index(['pair_name', 'window']).unstack().fillna(0).index)\n\ngiant_outliers_matrix = {}\n\ntarget = stable_residual_interval[stable_residual_interval.pair_name.isin(pair_name_bag)]\n\ngiant_outliers_matrix['pair_name'] = target.loc[:,['pair_name', 'window', 'residuals']].set_index(['pair_name', 'window']).unstack().fillna(0)","71c90fed":"giant_outliers_matrix['pair_name'].transpose().plot(figsize=(30, 24))","ee5fa2b9":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print (giant_outliers.pair_name.value_counts())\nsns.lineplot(x=giant_outliers.pair_name.value_counts().value_counts().index.to_list(), y=giant_outliers.pair_name.value_counts().value_counts().to_list())","7277de1c":"if not UPLOAD_PREVIOUS:\n    raw_features = extract_features(df_rolled, column_id='id', column_sort='date_block_num')\n    upload_df(raw_features.reset_index(), OUTPUT_PATH, 'features')\nelse:\n    raw_features = proceed['features'].drop('Unnamed: 0', axis=1)\n    raw_features = raw_features.rename(columns={'level_0':'id', 'level_1':'date_block_num'}).set_index(['id', 'date_block_num'])","8b447c9f":"raw_features.replace([np.inf, -np.inf], np.nan, inplace=True)\nraw_features_na = raw_features.dropna(axis=1, how='all', thresh=1000)","607f945e":"raw_features_pruned = raw_features.dropna(axis=1, how='any')","92b23da2":"features_pruned = {'raw': raw_features_pruned}","67a53496":"validator_lin = RegressionValidator(LinearRegression)\nerrors['pruned_raw'] = validator_lin.validate(raw_features_pruned.reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['pruned_raw'][0][2:], errors['pruned_raw'][1][2:], ax=axs[1])\nsns.histplot(errors['pruned_raw'][1], ax=axs[0])","0132e494":"df = pd.DataFrame(\n    data=standartize(raw_features_pruned), \n    index=raw_features_pruned.index, \n    columns=raw_features_pruned.columns\n)\nraw_features_pruned_scaled = df\n\nraw_features_pruned_scaled","82377d92":"validator_lin = RegressionValidator(LinearRegression)\nerrors['pruned_scaled'] = validator_lin.validate(raw_features_pruned_scaled.reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['pruned_scaled'][0], errors['pruned_scaled'][1], ax=axs[1])\nsns.histplot(errors['pruned_scaled'][1], ax=axs[0])","4a805958":"from gc import collect\nfrom sklearn.decomposition import PCA","ec833955":"def produce_pca(dataset, components='mle'):\n    decomposer = PCA(n_components=components)\n    return decomposer.fit_transform(dataset)","4c3c0575":"def produce_df(dataset):\n    df = pd.DataFrame(data=dataset)\n    df['id'] = raw_features_pruned_scaled.reset_index()['id']\n    df['date_block_num'] = raw_features_pruned_scaled.reset_index()['date_block_num']\n    return df","d321ee87":"raw_features_pruned_pca = {n_comp: produce_df(produce_pca(raw_features_pruned_scaled, n_comp)) for n_comp in range(50, 311, 5)}","4f40e31e":"validator_lin = RegressionValidator(LinearRegression)\nerrors['pruned_pca'] = {}\nfor i in raw_features_pruned_pca:\n    errors['pruned_pca'][i] = validator_lin.validate(raw_features_pruned_pca[i].reset_index(), y_rolled, y_rolled_test)","eb81e424":"sns.scatterplot(x=[i for i in raw_features_pruned_pca  if i > 270], y=[np.mean(errors['pruned_pca'][i][1]) for i in raw_features_pruned_pca if i > 270])","3a5a1b96":"validator_lin = RegressionValidator(LinearRegression)\nerrors['pruned_pca_validated'] = validator_lin.validate(raw_features_pruned_pca[300].reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['pruned_pca_validated'][0], errors['pruned_pca_validated'][1], ax=axs[1])\nsns.histplot(errors['pruned_pca_validated'][1], ax=axs[0])","7bfde49a":"def apply_standart(dataset, droped_idx=False):\n    df = pd.DataFrame(\n        data=standartize(dataset), \n        index=dataset.index, \n        columns=dataset.columns\n    )\n    return df","5e795b16":"raw_features_na_zero = raw_features_na.fillna(0)","0484573e":"validator_lin = RegressionValidator(LinearRegression)\nerrors['na_zero'] = validator_lin.validate(apply_standart(raw_features_na_zero).reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['na_zero'][0], errors['na_zero'][1], ax=axs[1])\nsns.histplot(errors['na_zero'][1], ax=axs[0])","e247f83b":"raw_features_na_high = raw_features_na.fillna(-9999999)","64eb2162":"validator_lin = RegressionValidator(LinearRegression)\nerrors['na_high'] = validator_lin.validate(apply_standart(raw_features_na_high).reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['na_high'][0], errors['na_high'][1], ax=axs[1])\nsns.histplot(errors['na_high'][1], ax=axs[0])","66627443":"def apply_to(dataset, func):\n    for column in dataset.columns:\n        dataset[column] = dataset[column].replace(np.NaN, func(dataset[column]))\n    return dataset","195c2c65":"raw_features_na_mean = apply_to(raw_features_na, np.mean)","660e7a94":"validator_lin = RegressionValidator(LinearRegression)\nerrors['na_mean'] = validator_lin.validate(apply_standart(raw_features_na_mean).reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['na_mean'][0], errors['na_mean'][1], ax=axs[1])\nsns.histplot(errors['na_mean'][1], ax=axs[0])","608d23ca":"raw_features_na_median = apply_to(raw_features_na, np.median)","4e07c78a":"validator_lin = RegressionValidator(LinearRegression)\nerrors['na_median'] = validator_lin.validate(apply_standart(raw_features_na_median).reset_index(), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['na_median'][0], errors['na_median'][1], ax=axs[1])\nsns.histplot(errors['na_median'][1], ax=axs[0])","76948c26":"from sklearn.impute import KNNImputer\n\ndef knn_impute(dataset):\n    imputer = KNNImputer(n_neighbors=2)\n    try:\n        return imputer.fit_transform(dataset.to_numpy())\n    except:\n        return imputer.fit_transform(dataset)","e7a8afc1":"raw_features_na_knn = produce_df(knn_impute(raw_features_na))","bc2baeaf":"produce_df(apply_standart(raw_features_na_knn).reset_index().drop('index', axis=1))","7df73cd3":"validator_lin = RegressionValidator(LinearRegression)\nerrors['na_knn'] = validator_lin.validate(produce_df(apply_standart(raw_features_na_knn).reset_index().drop('index', axis=1)), y_rolled, y_rolled_test)\n\n_, axs = plt.subplots(1, 2, figsize=(14, 4))\n\naxs[0].set_title(\"MSE histogram in validation scheme\")\naxs[1].set_title(\"MSE scatterplot\")\naxs[1].set_xlabel('date_block_num')\naxs[1].set_ylabel('MSE')\n\nsns.scatterplot(errors['na_knn'][0], errors['na_knn'][1], ax=axs[1])\nsns.histplot(errors['na_knn'][1], ax=axs[0])","4a6409a5":"# Magic with non-selected features","58e4917b":"### a) Pruned features","d83facc0":"### Ridge","189ae20b":"### Original linear regression","f5c7581a":"## Error exploration","ce9fe8ca":"4) Filling median","afba135b":"### b) Missing values filling","021e4275":"2) Filling as outlier","4ac52cca":"## Basic Regressions(Naive, Ridge, Lasso, PLS)","c10a4bab":"1) Filling zeros","eb90ba88":"### Lasso","4f8a291d":"3) Filling mean","3e5bf5cf":"5) KNN-Imputer"}}