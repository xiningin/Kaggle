{"cell_type":{"e17e9c37":"code","57f994f1":"code","4b859865":"code","b32d6f11":"code","11be4926":"code","a3133763":"code","c6eae834":"code","b3b64d56":"code","3b9c0796":"code","9d44d9e7":"code","c75ccc61":"code","15d0e50c":"code","eac6c57b":"code","ff4c76ee":"code","61dd9b58":"code","a2d70787":"code","654434df":"code","54dced37":"code","083e54c3":"code","d77594b3":"code","aea6d823":"code","cd2d1272":"code","11e706f9":"code","c7c31a56":"code","5baf83ba":"code","396528b8":"code","2b9d3160":"code","fa49aaf9":"code","0e6f4c94":"markdown","4de3cef9":"markdown","c4b3ebad":"markdown","8d3d9283":"markdown","f4817f3a":"markdown","94c143dd":"markdown","c1697a5e":"markdown","f04a17b4":"markdown"},"source":{"e17e9c37":"!pip install gensim==3.8.3","57f994f1":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\ntqdm.pandas(desc=\"progress-bar\")\n\nimport gensim\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\n\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding\n\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt","4b859865":"df = pd.read_csv('..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv',delimiter=',',encoding='latin-1')\ndf = df[['Category','Message']]\ndf = df[pd.notnull(df['Message'])]\n\ndf.head(10)","b32d6f11":"df.shape","11be4926":"data = df['Category'].value_counts()\nplt.figure(figsize=(10,9))\nsns.barplot(data.index, data.values, alpha=0.8)\nplt.ylabel('Total each Category', fontsize=12)\nplt.xlabel('Category', fontsize=12)\n\nplt.show();","a3133763":"df['Category'].value_counts()","c6eae834":"temp = df.copy()","b3b64d56":"temp['word_length'] = temp['Message'].apply(lambda x:len(x.split()))","3b9c0796":"plt.figure(figsize=(12, 8))\ntemp[temp.Category=='ham'].word_length.plot(bins=35, kind='hist', color='blue', \n                                       label='Ham messages', alpha=0.6)\ntemp[temp.Category == 'spam'].word_length.plot(kind='hist', color='red', \n                                       label='Spam messages', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Message Length\", fontsize = 15)\nplt.show()","9d44d9e7":"example = df[df.index == 10][['Message', 'Category']].values[0]\nprint(example[0])\nprint('Message:', example[1])","c75ccc61":"def cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['Message'] = df['Message'].apply(cleanText)","15d0e50c":"def tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            #if len(word) < 0:\n            if len(word) <= 0:\n                continue\n            tokens.append(word.lower())\n    return tokens\n\ntrain, test = train_test_split(df, test_size=0.000001 , random_state=42)\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.Category]), axis=1)","eac6c57b":"train_tagged.values","ff4c76ee":"# The maximum number of words to be used. (most frequent)\nmax_fatures = 500000\n\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n\ntokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Message'].values)\nX = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X)\nprint('Found %s unique tokens.' % len(X))","61dd9b58":"X = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","a2d70787":"d2v_model = Doc2Vec(dm=1, dm_mean=1, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\nd2v_model.build_vocab([x for x in tqdm(train_tagged.values)])","654434df":"for epoch in range(30):\n    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    d2v_model.alpha -= 0.002\n    d2v_model.min_alpha = d2v_model.alpha","54dced37":"len(d2v_model.wv.vocab)","083e54c3":"# save the vectors in a new matrix\nembedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n\nfor i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n    while i in vec <= 1000:\n        embedding_matrix[i]=vec","d77594b3":"# init layer\nmodel = Sequential()\n\n# emmbed word vectors\nmodel.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\nmodel.add(LSTM(50,return_sequences=False))\nmodel.add(Dense(2,activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])\n\n# output model skeleton\nmodel.summary()","aea6d823":"plot_model(model, to_file='model.png')","cd2d1272":"Y = pd.get_dummies(df['Category']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","11e706f9":"history = model.fit(X_train, Y_train, epochs = 20, batch_size = 64, verbose = 2)","c7c31a56":"plt.plot(history.history['acc'])\nplt.title('Accuracy')\nplt.ylabel('acc')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.title('Loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5baf83ba":"_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint('Train: %.3f, Test: %.4f' % (train_acc, test_acc))","396528b8":"model.save('Mymodel.h5')","2b9d3160":"message = ['Congratulations! you have won a Rp 1.000,000 Indomaret gift card. Go to http:\/\/bit.ly\/123456 to claim now.']\n\nseq = tokenizer.texts_to_sequences(message)\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\npred = model.predict(padded)\n\nlabels = ['ham','spam']\nprint(pred, labels[np.argmax(pred)])","fa49aaf9":"message = ['thanks for accepting my request to connect']\n\nseq = tokenizer.texts_to_sequences(message)\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\npred = model.predict(padded)\n\nlabels = ['ham','spam']\nprint(pred, labels[np.argmax(pred)])","0e6f4c94":"## Create LSTM Model","4de3cef9":"## Evaluate Model","c4b3ebad":"## Model Doc2Vec for LSTM","8d3d9283":"# Text Classification Using LSTM","f4817f3a":"## Text Processing","94c143dd":"## Library","c1697a5e":"## Read Data","f04a17b4":"## Visualizing the Data"}}