{"cell_type":{"f75ec96b":"code","ba9e8df0":"code","e647e9f6":"code","946a69a7":"code","42dbc2e2":"code","d4bd2795":"code","d87f499b":"code","563e5e3e":"code","e7811124":"markdown","02ccdf31":"markdown","27291d43":"markdown","fec9df9c":"markdown","88edf24a":"markdown","7f619348":"markdown","a9e30868":"markdown","1572c74d":"markdown","8fc85e0e":"markdown"},"source":{"f75ec96b":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","ba9e8df0":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\n# Preview the data\ntrain.head()","e647e9f6":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target', 'id'], axis=1)\ntest.drop(['id'], axis=1, inplace=True)\n\n# Preview features\nfeatures.head()","946a69a7":"test.head()","42dbc2e2":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\nnumerical_cols = [col for col in features.columns if col.startswith('cont')]\nX = features.copy()\nX_test = test.copy()\n\n# ordinal-encode categorical columns\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(X[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(X_test[object_cols])\n\n# scaler numerical columns\nscaler = StandardScaler()\nX[numerical_cols] = scaler.fit_transform(X[numerical_cols])\nX_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n# Preview the ordinal-encoded and scaler features\nX.head()","d4bd2795":"X_test.head()","d87f499b":"# Define the model  lightGBM\nimport lightgbm as lgb\nparams = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": 6,\n    \"early_stopping_round\": 200,\n    \"reg_alpha\": 9.03513073170552,\n    \"reg_lambda\": 0.024555737897445917,\n    \"colsample_bytree\": 0.2185112060137363,\n    \"learning_rate\": 0.003049106861273527,\n    \"max_depth\": 65,\n    \"num_leaves\": 51,\n    \"min_child_samples\": 177,\n    \"n_estimators\": 160000,\n    \"cat_smooth\": 93.60968300634175,\n    \"max_bin\": 537,\n    \"min_data_per_group\": 117,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.6709049555262285,\n    \"cat_l2\": 7.5586732660804445,\n    \"verbose\": -1\n}\n\nfinal_predictions = []\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid)\n    model = lgb.train(params=params,\n                      train_set=lgb_train,\n                      valid_sets=[lgb_valid],\n                      verbose_eval=1000)\n\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_predictions.append(preds_test)\n    print(fold, mean_squared_error(y_valid, preds_valid, squared=False))","563e5e3e":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': sub.id,\n                       'target': preds})\noutput.to_csv('submission.csv', index=False)","e7811124":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","02ccdf31":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","27291d43":"# Step 4: Train a model: \nUse LightGBM and KFold to split data\n","fec9df9c":"Next, we break off a validation set from the training data.","88edf24a":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","7f619348":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","a9e30868":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","1572c74d":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","8fc85e0e":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset."}}