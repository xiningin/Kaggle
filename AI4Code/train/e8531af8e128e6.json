{"cell_type":{"ff68db52":"code","ac42852f":"code","f9112696":"code","2aafb773":"code","a7142e6c":"code","55265f6b":"code","00e1429a":"code","bd34df33":"code","ab953536":"code","5bba0fc6":"code","29e206fa":"code","0d7d7863":"code","a152be72":"code","331d7bac":"code","1fac0830":"code","ecea7d2b":"code","4bf89ab4":"code","c500fe04":"code","be9073ec":"code","14ae5e8e":"code","35e9fcd5":"markdown","d37bac6d":"markdown","936d6f18":"markdown","ecb72691":"markdown","ca6f2a4d":"markdown","ae549bc7":"markdown","85c8d69d":"markdown","7286e1ed":"markdown","62063aed":"markdown","23137ff9":"markdown","b448f3c4":"markdown","48ebc909":"markdown","f64c6c28":"markdown","49adfa56":"markdown","055a9700":"markdown","ec154f96":"markdown"},"source":{"ff68db52":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","ac42852f":"rawData = pd.read_csv(\"..\/input\/weatherAUS.csv\")","f9112696":"rawData.head()","2aafb773":"rawData.shape","a7142e6c":"rawData.isnull().sum()","55265f6b":"workingData = rawData.drop(['Evaporation', 'Location', 'Date', 'Sunshine', 'Cloud9am', 'Cloud3pm','RISK_MM', 'RainTomorrow'], axis=1).copy()\nY = rawData['RainTomorrow'].copy()\n#We will take this opportunity to also map our catagorical values.\nY = Y.map({'Yes':1, 'No':0})\nY = pd.DataFrame(Y, columns = ['RainTomorrow'])\nworkingData['RainToday'] = workingData['RainToday'].map({'Yes':1, 'No':0})","00e1429a":"workingData.isnull().sum()","bd34df33":"workingData = workingData.drop(['Pressure9am', 'Pressure3pm','WindDir9am'], axis = 1)\n\n#Fill the missing data with numerical averages.\nmeanMatrix = workingData.drop(['WindDir3pm', 'RainToday', 'WindGustDir'], axis=1).copy()\nmeanWorkingData = meanMatrix.fillna(meanMatrix.mean())","ab953536":"#We will replace the observations that don't have a reading for the 'RainToday' feature with the most popular observation for that feature.\nrawData['RainToday'].value_counts()","5bba0fc6":"#So we will replace all missing RainToday valuse with 'No':0\nmeanWorkingData['RainToday'] = workingData['RainToday'].fillna(0)\n#Brief investigation.\nmeanWorkingData.head(10)","29e206fa":"#Ensure all Nulls are gone\nmeanWorkingData.isnull().sum()","0d7d7863":"from sklearn import preprocessing\nxScaled = preprocessing.scale(meanWorkingData.drop(['RainToday'], axis = 1))\n\n\n\n#xScaled['RainToday'] = meanWorkingData['RainToday'].values\nxScaled = pd.DataFrame(xScaled, columns=meanWorkingData.drop(['RainToday'], axis =1).columns)\nxScaled['RainToday'] = meanWorkingData['RainToday']\n#We will briefly visualise the correlation matrix to investigate if there are (m)any relationships. We can expect there to be temperature relationships, since those values will remain more or less close to one another, similar with wind speeds, since they should not change drastically in a daily observation.\nsns.heatmap(data = xScaled.corr(), cmap = 'Blues')","a152be72":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nnFeatures = [1, 3, 5, 7, 9, 11]\nxSelectors = []\nxFeatures = []\n\nfor feature in nFeatures:\n    xSelectionModel = SelectKBest(f_classif, k=feature).fit(xScaled, Y['RainTomorrow'])\n    xSelectors.append(xSelectionModel)\n    \nfor selector in xSelectors:\n    featureList = xScaled.columns[selector.get_support()]\n    xFeatures.append(featureList)","331d7bac":"from sklearn.base import clone as skClone\n\ndef produceModels ( xElements, yElements, featureList, modelObject ):\n    output = []\n    for featureset in featureList:\n        print('Generating model.')\n        #Filter the xElements to the appropriate values in the xSelector\n        xFilteredElements = xElements[featureset]\n        print(xFilteredElements.shape)\n        classifierModel = skClone(modelObject)\n        classifierModel = classifierModel.fit(xFilteredElements, yElements['RainTomorrow'])\n        output.append(classifierModel)\n        print('Done.')\n        #print('Coefficients: {0}'.format(classifierModel.coef_))\n    return output\n\ndef compareModels ( xElements, yElements, featureList, modelList, showCoeffs = False):\n    print('Comparing models.')\n    iterator = 0\n    for model in modelList:\n        #Filter the xElements to the appropriate values in the xSelector\n        xFilteredElements = xElements[featureList[iterator]]\n        if showCoeffs:\n            print(model.coef_)\n        print('Comparing model with {0} features:'.format(nFeatures[iterator]))\n        print('Accuracy : {0}'.format(model.score(xFilteredElements, yElements)))\n        iterator += 1\n    return\n\n######## Is there a good way of comparing the predicting features similar to an adjusted r-squared or p-value for each of the predictors? Please let me know if you can think of a good guide on how to do this for future reference. Thanks.","1fac0830":"from sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(xScaled, Y, test_size = 0.2, random_state = 28122018)","ecea7d2b":"from sklearn.linear_model import LogisticRegression\nlogisticRegressor = LogisticRegression(random_state = 28122018, solver='lbfgs')\nlogisticModels = produceModels(xTrain, yTrain, xFeatures, logisticRegressor)","4bf89ab4":"compareModels(xTest, yTest, xFeatures, logisticModels)","c500fe04":"from sklearn import tree\ndecisionTreeClassifier = tree.DecisionTreeClassifier(random_state = 28122018)\ntreeModels = produceModels(xTrain, yTrain, xFeatures, decisionTreeClassifier)","be9073ec":"compareModels(xTest, yTest, xFeatures, treeModels)","14ae5e8e":"from sklearn.neighbors import KNeighborsClassifier as KNC\n#Using 5 neighbors\nprint('KNN (5)')\nKNNClassifier5 = KNC(n_neighbors=5)\nKNNModels5 = produceModels(xTrain, yTrain, xFeatures, KNNClassifier5)\ncompareModels(xTest, yTest, xFeatures, KNNModels5)\n\n#Using 7 neighbors\nprint('KNN (7)')\nKNNClassifier7 = KNC(n_neighbors=7)\nKNNModels7 = produceModels(xTrain, yTrain, xFeatures, KNNClassifier7)\ncompareModels(xTest, yTest, xFeatures, KNNModels7)\n\n#Using 10 neighbors\nprint('KNN (10)')\nKNNClassifier10 = KNC(n_neighbors=10)\nKNNModels10 = produceModels(xTrain, yTrain, xFeatures, KNNClassifier10)\ncompareModels(xTest, yTest, xFeatures, KNNModels10)","35e9fcd5":"### KNN Classification","d37bac6d":"We will briefly re-examine the dataset.","936d6f18":"With a  cursory examination, we're missing over a third of values for Evaporation, Sunshine, and cloud measures. We will drop these features and proceed with analysing the data.\nWe will also drop the RISK_MM feature as it's already a predictor of the outcome for tomorrow as describe in the dataset. We will also, for the time being, drop the Date and Location.","ecb72691":"For the missing categorical values, determining if it's a randomly missing value or if it's a non-randomly missing is required to correctly replace the missing values. Otherwise, we can discard them entirely in the form of either: Removing the feature entirely or removing the obersvations entirely. In this instance we will remove the wind direction variable, since we are also dropping the location for this analysis. ","ca6f2a4d":"# Australian rain data.\n\nHello Kaggle. \n\nThis will be my first attempt at reading in and working with data. \nI'm a graduate of mathematics and chemistry at university but I never considered computer science,  I wasn't even slightly aware of data science as an academic entity. After a bit of digging I found the world of data science. To date I've started a few Udemy tutorials and read a few kernels but I haven't shared any of my attempts. This will be my first shared kernel so please feel free to give critique, insight, or point out errors of understanding and better ways to do things.","ae549bc7":"## We will now attempt several types of Classification\n### Feature Selection\nWe will attempt to try the x best features of our remaining feature set. We will attempt models with one feature, three features, five features, and all the features and compare their predictive power.\nSince we standardised our data by scaling it between -1, 1, we cannot use the chi2 test since it assumes that variance and frequency can't be negative, but we can compare ANOVA f scores.","85c8d69d":"## Prepare the data by splitting into training and test sets","7286e1ed":"### Decision tree","62063aed":"For values with less than 10, 000  missing numerical elements, we will replace the numerical values with the averages. We will drop elements that have more missing elements than 10, 000.","23137ff9":"The data is now ready to be scaled. We will use the sklearn preprocessing library to scale the data.","b448f3c4":"## Model Preparation\nWe will attempt several types of clasification model, but we want to try iterating over all of the predictor variables types that we are showing above. We will use our list of xSelectors to produce a list of fit classifiers of each type.","48ebc909":"### Logistic Regression\nWe will begin by attempting to fit a logisitc regression. ","f64c6c28":"## Importing libraries","49adfa56":"## Investigate, understand, and clean the data\n\n### Investigating\nWe will begin investigating the data by viewing it, gathering information about its shape and the values in it.\nWe will first have a quick look at the data, then see it's shape.","055a9700":"## Read in the data","ec154f96":"### Investigate: Brief analysis\nThe dataset contains 24 columns, the last of which is whether or not the next day rained. \nFor any given entry, which is a given day at a given location, there are:\n\n1. Date: The date of the observation.\n2. Location: The location of the weather station.\n#### Temperature\n3. MinTemp: The minimum temperature of that day in degrees celsius.\n4. MaxTemp: The maximum temperature of that day in degrees celcius. \n#### Volume difference\n5. Rainfall: The volume of rainfall recorded for that day in mm.\n6. Evaporation: The Class A pan evaporation in mm.\n#### Sunshine\n7. Sunshine: The number of hours of sunshine in the given day.\n#### Wind\n8. WindGustDir: The direction of the STRONGEST wind gust in that given day.\n9. WindGustSpeed: The greatest speed of the STRONGEST wind gust in that given day.\n10. WindDir9am: Categorical Wind diration\n11. WindDir3pm: Categorical Wind diration\n12. WindSpeed9am: Wind speed \n13. WindSpeed3pm: Wind Speed\n#### Humidity and pressure\n14. Humidity9am: Humidity \n15. Humidity3pm: Humidity\n16. Pressure9am: Pressure\n17. Pressure3pm: Pressure\n#### Overhead cloud recording, sky obstruction\/cloud volume\n18. Cloud9am: The fraction of the sky obstructed by clouds at 9 am on a given day, measured in eighths. \n19. Cloud9pm: The fraction of the sky obstructed by clouds at 3 pm on a given day. \n#### Temperature difference\n20. Temp9am: Temperature\n21. Temp3pm: Temperature\n####  Misc.\n22. RainToday:  Whether or not it rained on the day of the observation\n23. Risk_MM: The risk of rain on a given day as determined by the station.\n#### Target\n24. RainTomorrow: Did the next day have rain recorded?\n\n### Investigation data count\nWe have 142, 193 entries and  >15 features.\nWe will start by looking at the null entries and determine how to handle them on a case basis."}}