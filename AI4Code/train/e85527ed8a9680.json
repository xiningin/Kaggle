{"cell_type":{"8bd0721d":"code","b5085693":"code","764f560d":"code","c01e19ee":"code","a9c13a76":"code","41064925":"code","274d0864":"code","75e5ec53":"code","6de936f3":"code","49ab3e13":"code","066816b4":"code","776927df":"code","e31f89c6":"code","9a4661ab":"code","a753618a":"code","d5ef387e":"code","ff5e52dc":"code","1fc249f5":"code","f60fcba6":"code","7f1263d8":"code","a27243d7":"code","d7d475d6":"code","e98b3dea":"code","1d0f662c":"code","dc4b665a":"code","a9c5a89c":"code","b4c85345":"code","c7584806":"markdown","8368b0f7":"markdown","57dbd9c2":"markdown","0773e8b8":"markdown","e0988a99":"markdown","c6972ea1":"markdown","a9697f41":"markdown","86afbd64":"markdown","06a87583":"markdown","a1aae244":"markdown"},"source":{"8bd0721d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# for keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Sequential\n\n# for pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","b5085693":"dataset = pd.read_csv('..\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv')\ndataset.head()","764f560d":"# select the data\nfeatures = dataset.values[:, :dataset.shape[1]-2]\nchars = dataset['character'].values\n\nfeatures.shape, chars.shape","c01e19ee":"# receives data\ndef process_data(x):\n    images = [] # all the images\n    # is each row in x, each image\n    for img in x:\n        # reshape the flatten data\n        image = img.reshape(64,64,1)\n        images.append(image)\n    # return the images in an apropiate format\n    return np.array(images).astype('float32')\/255\n\n# recieves labels\ndef process_target(chars, num_classes):\n    target = [] # is the result\n    class_names = {} # other result\n    count = count_values(chars) # count the characters\n    ###### add the labels for the dict\n    for key, i in zip(count.keys(), range(num_classes)):\n        class_names[key] = i\n    ###### create the labels data, the numbers\n    labs = class_names.keys()\n    for char in chars:\n        pos = class_names[char] # position of the 1\n        row = []\n        for i in range(num_classes):# create the target [0,0,0...,1,...]\n            if pos != i:\n                row.append(0)\n            else:\n                row.append(1)\n        target.append(row)\n    return np.array(target).astype('float32'), class_names\n\n\ndef count_values(arr):\n    dic = {}\n    for val in arr:\n        if val not in dic.keys():\n            dic[val] = 1\n        else:\n            dic[val] += 1\n    return dic\n\n# get the images from the df as arrays\nX = process_data(features)\n\n# and obtain the target data from the characters\nY, class_names = process_target(chars, num_classes=15)\n\nX.shape, Y.shape","a9c13a76":"# split the data, as keras training goes first, for now there won't be val set\nx_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, Y, test_size=.2, random_state=314)\ny_train_k.shape, y_test_k.shape","41064925":"#keras.backend.clear_session()\n\ninput_shape = (64,64,1) # the dimension of the data\nnum_classes = 15 # the number of classes\n\nkeras_model = Sequential([\n\n    layers.InputLayer(input_shape=input_shape),\n\n    # convolutional part with relu and later pooling\n    layers.Conv2D(filters=32, kernel_size=5, activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n\n    # flatten the data, as it comes with (64,64,1) shape\n    layers.Flatten(),\n    \n    # dense part, with neurons\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=.4), # it helps to prevent overfitting\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=.4),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=.4),\n    layers.Dense(num_classes, activation='softmax')\n])\n\n# compile the model\nkeras_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics = ['accuracy'],\n)\n\nkeras_hist = keras_model.fit(\n    x_train_k,\n    y_train_k,\n    batch_size=128,\n    epochs=20,\n    validation_split=.1,# as there's no val data\n)","274d0864":"plt.figure(figsize=(15, 10))\n\n# plot the loss function\nplt.subplot(1,2,1)\nplt.plot(keras_hist.history['loss'], label='train')\nplt.plot(keras_hist.history['val_loss'], label='validation')\nplt.title('Loss Function')\nplt.grid(True)\nplt.legend()\n\n# and the accuracy\nplt.subplot(1,2,2)\nplt.plot(keras_hist.history['accuracy'], label='train')\nplt.plot(keras_hist.history['val_accuracy'], label='validation')\nplt.grid(True)\nplt.title('Accuracy')\nplt.legend()\n\nplt.show()","75e5ec53":"# First split the data for pytorch\nx_train_p, x_test_p, y_train_p, y_test_p = train_test_split(features.astype('float32'), chars, test_size=.2, random_state=314)\nx_train_p, x_val_p, y_train_p, y_val_p = train_test_split(x_train_p, y_train_p, test_size=.1, random_state=314)\n\ny_train_p.shape, y_test_p.shape, y_val_p.shape","6de936f3":"class CustomDataset(Dataset):\n    # data will be a numpy array, in this case: features\n    # labels will be tha characters, in this case: labels\n\n    ######################### THE \"BASIC\" NEEDED FUNCTIONS\n    \n    def __init__(self, data, labels, labels_ids=[]):\n        # process the labels and the data\n        self.process_data(data)\n        self.process_labels(labels, labels_ids) # this labels ids is too important, so\n                                                # read bellow why and check the function\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels_n[idx]\n    \n    ######################### THE PROCESSING FUNCTIONS\n    \n    # function inside the class for process the data\n    def process_data(self, data):\n        # convert data to a torch tensor\n        self.data = torch.from_numpy(data)\n        # normalize the data\n        self.data = self.data\/255\n        # reshape the tensor, keep the batch and chanel dim and reshape the images\n        self.data = self.data.view(self.data.shape[0], 1, 64, 64)\n        \n    # function inside the class for process the target\n    def process_labels(self, labels, labels_ids):\n        # count all the different values in labels\n        self.distrib, self.num_labels = self.__count_values(labels)\n        \n        # verify if there are existant ids !important\n        if labels_ids == []:\n            # set an id for each label if there's no labels\n            self.id_labels = {}\n            for label, i in zip(self.distrib.keys(), range(self.num_labels)):\n                self.id_labels[label] = i\n\n        # if there are existant labels use them\n        else:\n            self.id_labels = labels_ids   \n            \n        # then create an array with the ids\n        ids = [] # will be converted in an array\n        for label in labels:\n            # append the id of the label\n            ids.append(self.id_labels[label])\n        # use one hot encoding for the labels\n        self.labels = []\n        self.labels_n = [] # labels without encoding\n        for i in ids:\n            # append the normal label\n            self.labels_n.append(i)\n            # append the one hot encoded label\n            self.labels.append(self.__one_hot(i))\n        # convert to a numpy array, this is because the final dtype\n        # was torch.float64 and the data is torch.float32\n        self.labels = np.array(self.labels).astype('float32')\n        # and convert to torch tensor\n        self.labels = torch.from_numpy(self.labels)\n        self.labels_n = torch.tensor(self.labels_n, dtype=torch.int64)\n        \n    \n    ######################### EXTRA FUNCTIONS\n    \n    # extra function to count the different items from an array\n    def __count_values(self, arr):\n        dic = {}\n        for val in arr:\n            if val not in dic.keys():\n                dic[val] = 1\n            else:\n                dic[val] += 1\n        return dic, len(dic.keys())\n    \n    # extra function to make one hot encoding for the labels\n    def __one_hot(self, label):\n        res = np.zeros((self.num_labels))\n        res[label]+=1\n        return res\n    \n    # extra function to decode from one hot to numbers\n    def decode(self, labels): # recieve a torch tensor, a prediction\n        # select the indexs of the max elements in each label\n        decoded = torch.argmax(labels, dim=1).numpy()\n        return decoded\n        \n        \n\n#### DATASET CLASS ATRIBUTES: (the ones I use)\n# data: all the tensors (images) with shape (bacth,1,64,64), pytorch tensor\n# labels: all the labels encoded with one hot encoding, pytorch tensor\n# distrib: a dict that contains how many data there are from each label {character: number}\n# id_labels: a dict with and id for each label or character {character:id}\n## as this has to be the same for every dataset it will be passed as a param\n## for the test and val datasets. The data was shuffled, not being passing the dict\n## as a param for the others will cause diferent ids and LOW val and test accuracy.\n# labels_n: a list the labels but without one hot encoding, are the ids from id_labels\n# num_labels: how many different labels or classes there are\n\n\n# use the class for create dataset objects\n# since the keras processing has created a dict of ids, these classes\n# are going to use the same ids so the models predict \"the same\"\ntrain_set = CustomDataset(x_train_p, y_train_p, class_names)\n# the next ones will contain the same ids of train_set\ntest_set = CustomDataset(x_test_p, y_test_p, train_set.id_labels)\nval_set = CustomDataset(x_val_p, y_val_p, train_set.id_labels)\n\n\n# see some data of the datasets\n# the lengths of each dataset, the dtypes of data and labels, and an example of the labels\nlen(train_set), len(test_set), test_set[0][0].shape, test_set[0][0].dtype, test_set[0][1].dtype, test_set[1000][1]","49ab3e13":"# then use the module DataLoader to 'adapt' the datasets for the model\nbatch_size = 128\n\ntrain_loader = DataLoader(\n    dataset=train_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)\n\ntest_loader = DataLoader(\n    dataset=test_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)\n\nval_loader = DataLoader(\n    dataset=val_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)","066816b4":"class Network(nn.Module):\n    # in the init are going to be defined the layers\n    def __init__(self):\n        super(Network, self).__init__()\n        # define the layers, here the order is not important\n        # CONVOLUTIONAL LAYERS\n        # here there's no input shape, only the number of channels and the\n        # output number of chanels\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n        self.pool1 = nn.MaxPool2d(2)\n        # DENSE LAYERS\n        # the first para is like the input_shape, the seccond the number\n        # of outputs, like the number of neurons\n        self.dense1 = nn.Linear(32*30*30, 256)\n        self.dense2 = nn.Linear(256, 256)\n        self.dense3 = nn.Linear(256, 256)\n        self.dense4 = nn.Linear(256, 15)\n        # DROPOUT LAYER\n        self.dropout = nn.Dropout(0.3)\n        \n    # here we define the forward propagation of the net\n    def forward(self, x): # and use the not trainable layers\n        # convolutional process\n        x = F.relu(self.pool1(self.conv1(x)))\n        # apply the flatten process, conserving the batch dim\n        x = torch.flatten(x,1)\n        # dense process with droput layers\n        x = self.dropout(F.relu(self.dense1(x)))\n        x = self.dropout(F.relu(self.dense2(x)))\n        x = self.dropout(F.relu(self.dense3(x)))\n        x = self.dense4(x)\n        return x\n        \n        \npt_model = Network()\n\n# this is a prediction from the model, if we play and modificate the\n# forward function we will see how the x.shape is changing\npt_model(torch.randn((32,1,64,64)))\n\n# SET THE DEVICE\n# if we hace a gpu (with cuda) set the device as the gpu, else in cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npt_model = pt_model.to(device) # move the model to the selected device\ndevice","776927df":"lr = 0.002\n\n# this is like instance the loss function\ncriterion = nn.CrossEntropyLoss()\n# the optimizer receives the model weights and a learning rate\noptimizer = optim.Adam(pt_model.parameters(), lr=lr)","e31f89c6":"# define a metric\ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval() # is like swich the model mode, this changes the\n    # behave of layers like Dropouts Layers, BatchNorm Layers\n\n    with torch.no_grad(): # deactivcate the back propagation,\n    # it will reduce memory and speed up computations\n\n        for x,y in loader:\n            # move the data and targets to the device\n            x = x.to(device)\n            y = y.to(device)\n            # obtain the scores\n            scores = model(x)\n            # we ned the max from the second dim\n            _, preds = scores.max(1)\n            # select the correct preds and sum them\n            num_correct += (preds == y).sum()\n            # count the num of samples\n            num_samples += preds.shape[0]\n    \n    # calculate the accuracy, float since numbers are tensors\n    acc = float(num_correct) \/ float(num_samples)\n    print(f'Got {num_correct} \/ {num_samples} with accuracy {acc*100}%')\n\n    # switch the model to train mode\n    model.train()\n    \n    return acc","9a4661ab":"# train variables\nepochs = 24\npt_hist = {\n    'accuracy': [],\n    'val accuracy': []\n}\n\n# train the network\nfor epoch in range(epochs):\n    # this iters the the data and targets, and with an id\n    # data and targets are the batch for each train step\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # move the data and taregets to the model's device\n        data = data.to(device)\n        targets = targets.to(device)\n\n        # forward propagation, predict and measure the error\n        scores = pt_model(data)\n        loss = criterion(scores, targets)\n        \n        # backward propagation, use the error to fit the weights\n        optimizer.zero_grad() # clean the gradient, it's needed\n        # contains the directions to redice the error value of\n        # each prediction made, else is going to be adding value and fail\n        ## back propagation of the gradient and fit the weights\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n        \n    print(f'==> Epoch{epoch}')\n    # check the accuracy from train and val in each epoch\n    acc = check_accuracy(train_loader, pt_model)\n    val_acc = check_accuracy(val_loader, pt_model)\n    \n    # regist the accuacy values\n    pt_hist['accuracy'].append(acc)\n    pt_hist['val accuracy'].append(val_acc)","a753618a":"# then, freeze the convolutional part\n\n# iterate the model params\nfor param in pt_model.parameters():\n    print(param.shape)\n    break # only the first, the conv\n    \n# the printed shape is torch.Size([32, 1, 5, 5])\n# are 32 outs and a 5x5 kernel of 1 chanel, so that\n# are the params of the convolutional part\n\n# that property is like \"if the param is trainable\"\nif param.requires_grad == True:\n    # set as not trainable param\n    param.requires_grad = False","d5ef387e":"# check if it has the correct effect\nfor param in pt_model.parameters():\n    print(param.shape)\n    print(param.requires_grad)\n    break","ff5e52dc":"# this is like \"undo the train\"\n\n# redefine the layers\npt_model.dense1 = nn.Linear(32*30*30, 256)\npt_model.dense2 = nn.Linear(256, 256)\npt_model.dense3 = nn.Linear(256, 256)\npt_model.dense4 = nn.Linear(256, 15)\n\n# move the model to the device\npt_model = pt_model.to(device)","1fc249f5":"# redefine the optimizer for the new parameters\n\n# this optimizer receives the model trainable weights and a learning rate\nnew_optimizer = optim.Adam(filter(lambda p: p.requires_grad, pt_model.parameters()), lr=0.001)","f60fcba6":"# train variables\nepochs = 24\npt_hist = {\n    'accuracy': [],\n    'val accuracy': [],\n    'loss': []\n}\n\n# train the network\nfor epoch in range(epochs):\n    # this iters the the data and targets, and with an id\n    # data and targets are the batch for each train step\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # move the data and taregets to the model's device\n        data = data.to(device)\n        targets = targets.to(device)\n\n        # forward propagation, predict and measure the error\n        scores = pt_model(data)\n        loss = criterion(scores, targets)\n        \n        # backward propagation, use the error to fit the weights\n        new_optimizer.zero_grad() # clean the gradient, it's needed\n        # contains the directions to redice the error value of\n        # each prediction made, else is going to be adding value and fail\n        ## back propagation of the gradient and fit the weights\n        loss.backward()\n\n        # gradient descent or adam step\n        new_optimizer.step()\n        \n    print(f'==> Epoch{epoch}')\n    # check the accuracy from train and val in each epoch\n    acc = check_accuracy(train_loader, pt_model)\n    val_acc = check_accuracy(val_loader, pt_model)\n    \n    # regist the accuacy values\n    pt_hist['accuracy'].append(acc)\n    pt_hist['val accuracy'].append(val_acc)\n    pt_hist['loss'].append(loss.item())","7f1263d8":"# plot the accuracy increase\nx = np.arange(len(pt_hist['accuracy']))\nplt.figure(figsize=(10,6))\nplt.plot(x, pt_hist['accuracy'], label='train')\nplt.plot(x, pt_hist['val accuracy'], label='val')\nplt.title('Accuracy')\nplt.grid(True)\nplt.legend()\nplt.ylim(.7,1.1)\nplt.xlabel('Epoch')\nplt.show()","a27243d7":"results = keras_model.evaluate(x_test_k, y_test_k, batch_size=64)\nprint(\"test loss, test acc:\", results)","d7d475d6":"_ = check_accuracy(test_loader, pt_model)","e98b3dea":"# see the shapes of the predictions and how to predict\npt_model.eval()\npt_sample = torch.unsqueeze(test_set[0][0], 0).to(device)\npt_model(pt_sample).shape, type(pt_sample) == torch.Tensor, pt_sample.to('cpu').numpy().shape","1d0f662c":"ks_sample = x_test_k[0].reshape(1,64,64,1)\nkeras_model.predict(ks_sample).shape, type(ks_sample) == np.ndarray, ks_sample.shape","dc4b665a":"class Ensemble:\n    # models = [(model, 'keras' or 'pytorch')]\n    # datasets = [(test_loader, 'pt_loader'), ((x_test_k, y_test_k), 'numpy')]\n    def __init__(self, models=[], datasets=[]):\n        self.models = []\n        self.mtypes = []\n        self.sets = datasets\n\n        # read the models and the model types\n        for model, mtype in models:\n            self.models.append(model)\n            self.mtypes.append(mtype)  \n        \n    def add_model(self, model, mtype):\n        self.models.append(model)\n        self.mtypes.append(mtype)\n    \n    def add_set(self, dataset, datatype):\n        self.sets.append((dataset, datatype))\n    \n    # extra function to decode from one hot to numbers\n    def decode(self, x): # recieve a torch tensor, a prediction\n        # select the indexs of the max elements in each label\n        x = torch.tensor(x)\n        decoded = torch.argmax(x, dim=1).numpy()\n        return decoded\n    \n    def predict(self, x):        \n        # create a x for each mtype \n        x_dic = {}\n        if type(x) == np.ndarray:\n            x_dic['keras'] = x                     # keep the batch dim\n            x_dic['pytorch'] = torch.from_numpy(x).to(device).view(x.shape[0], 1, 64, 64)\n        else:\n            x_dic['pytorch'] = x.to(device)\n            x_dic['keras'] = x.to('cpu').numpy().reshape(x.shape[0], 64, 64, 1)\n        \n        # predict with the models\n        preds = []\n        for model, mtype in zip(self.models, self.mtypes):\n            if mtype == 'keras':\n                p = np.squeeze(model.predict(x_dic['keras']))\n                preds.append(p)\n            else:\n                with torch.no_grad():\n                    p = model(x_dic['pytorch']).to('cpu')\n                    p = np.squeeze(p.detach().numpy())\n                    preds.append(p)\n        \n        # sum the preds and obtain the mean\n        sum_preds = sum(preds)\/len(self.models)\n        # reshape for the decode\n        sum_preds = sum_preds.reshape(-1, 15)\n        # return the decoded labels\n        return self.decode(sum_preds)\n    \n    def evaluate(self):\n        num_correct = 0\n        num_samples = 0\n        preds, answers = [], []\n        for ds, datatype in self.sets:\n            print('evaluation on -> ' + datatype)\n            if datatype == 'numpy': # there are x_test and x_train\n                x_set, y_set = ds\n                y_set = self.decode(y_set)\n                for x, y in zip(x_set, y_set):\n                    # predict each image with both models\n                    pred = self.predict(x.reshape(1, 64, 64, 1))[0]\n                    # append the predictions and the answers\n                    preds.append(pred)\n                    answers.append(y)\n                    # count the answers\n                    if pred == y:\n                        num_correct += 1\n                    num_samples += 1\n                    \n            elif datatype == 'pt_loader': # the pytorch loader\n                for batch_x, batch_y in ds: # this will return a batch\n                    for x, y in zip(batch_x, batch_y):\n                        # predict each image with both models\n                        pred = self.predict(x.view(1, 1, 64, 64))[0]\n                        # append the predictions and the answers\n                        preds.append(pred)\n                        answers.append(y)\n                        # count the answers\n                        if pred == y.item():\n                            num_correct += 1\n                        num_samples += 1\n        \n        return num_correct\/num_samples, answers, preds\n                    \n    \n\nmodel = Ensemble(\n    models = [(keras_model, 'keras'), (pt_model, 'pytorch')],\n    datasets = [((x_test_k, y_test_k), 'numpy'), (test_loader, 'pt_loader')]\n)\n\n# it will take a while\nacc, predictions, answers = model.evaluate()\nacc","a9c5a89c":"# define the matrix with the real classes and the predicted\nm = confusion_matrix(answers, predictions)\n# the labels for the plot\nlabels = list(class_names.values()) # the characters throw warnings\nplt.figure(figsize=(20, 8))\n# create the plot\nheatmap = sns.heatmap(m, xticklabels=labels, yticklabels=labels, annot=True, fmt='d', color='blue')\n# labels for the axes\nplt.xlabel('Predicted Class')\nplt.ylabel('True Class')\nplt.title('Confusion Matrix')\nplt.show()\n# print the ids and the labels\nprint('Labels and ids:')\nprint(test_set.id_labels.keys())\nprint(test_set.id_labels.values())","b4c85345":"# preds must be like [[real, pred]]\ndef plot_images(imgs, dims, figsize, title_size, preds=[]):\n    plt.figure(figsize=figsize)\n    for img, i, in zip(imgs, np.arange(imgs.shape[0])):\n        plt.subplot(dims[0], dims[1], i+1)\n        # plot in a different color if the prediction is wrong\n        plt.imshow(np.squeeze(img), cmap=('gray' if preds[i][1] == preds[i][0] else 'magma'))\n        plt.axis('off')\n        title = f'Image {i+1}'\n        if preds != []:\n            title = f'Real: {preds[i][0]}, Pred: {preds[i][1]}'\n        plt.title(title, fontsize=title_size)\n    plt.show()\n\n# select some images\nsample_images = x_test_k[:25]\npredicts = model.predict(sample_images)\nanswers = model.decode(y_test_k[:25])\n\n# adecuate the preds and answers for the plot\ndata = []\nfor p,a in zip(predicts, answers):\n    data.append((a,p))\n    \nplot_images(sample_images, dims=(5,5), figsize=(20,20), title_size=22, preds=data)","c7584806":"# Evaluate the Models Individually","8368b0f7":"# Conclusion\nThis model implementation was interesting. Is the first time I use **Transfer Learning** in PyTorch. Was good and fun trying to join to models of two different libraries.","57dbd9c2":"# MNIST Classifier Using Keras and PyTorch CNNs and Transfer Learning with PyTorch","0773e8b8":"# Restart the Dense Layers","e0988a99":"# Keras Model Training\n","c6972ea1":"# A confusion Matrix","a9697f41":"# Dataset Class PyTorch\n","86afbd64":"# Join the models\nMake a function to predict using the two models at the same time.","06a87583":"# Now Transfer Learning\nThats somthing new I want to try with Py\/torch. The idea is that once the model params has ben optimized, or trained, `The convolutional part has already been optimized`, so data that flows through that layer `isn't going to change any more` if I *freeze* the convolutional part for train. In other words, `convolutional layer is trained and I will train new linear neurons without changing the convolutional part`.","a1aae244":"# See some Predictions"}}