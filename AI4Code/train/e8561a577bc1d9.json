{"cell_type":{"d00eaa0f":"code","58ab15b3":"code","7ab72770":"code","c6b6ddde":"code","7a33a4ab":"code","044fa876":"code","88c42adc":"code","8aeaf8eb":"code","1a4f8044":"code","865ad76e":"code","244ebeb0":"code","53210b94":"code","3f459b57":"code","7d70fa96":"code","ee1bf07e":"code","c9eab557":"code","fc3e4d42":"code","9081a8f2":"code","50c7ea0d":"markdown"},"source":{"d00eaa0f":"import numpy as np \nimport pandas as pd \nimport os\nimport cv2\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\nprint(\"Imported Modules Successfully\")","58ab15b3":"#check input dir files\npath = \"..\/input\/diabetic-retinopathy-balanced\/content\/Diabetic_Balanced_Data\"\nprint(os.listdir(path))\ntrain_path = path + \"\/train\"\ntest_path = path + \"\/test\"\nprint(os.listdir(train_path))\n#classes: 0 - No DR, 1 - Mild, 2 - Moderate, 3 - Severe, 4 - Proliferative DR\nclasses_len = []\nfor i in range(len(os.listdir(train_path))):\n    classes_len.extend(len(os.listdir(train_path + \"\/\" + str(i))) * [i])\n\nax = sns.countplot(y = classes_len)\nax.set(xlabel='Number of Images', ylabel='Label', title = 'Class Distribution')\nplt.show()","7ab72770":"def get_class_name(i):\n    if i == 0:\n        return \"No DR\"\n    elif i == 1: \n        return \"Mild\"\n    elif i == 2: \n        return \"Moderate\"\n    elif i == 3: \n        return \"Severe\"\n    else:\n        return \"Proliferative DR\"","c6b6ddde":"num_classes = 5\n#unify images shape\nshape = (256, 256)\n# initialize number of epochs, learning rate and batch size\nepochs = 15\nlearning_rate = 0.001\nbatch_size = 32","7a33a4ab":"image_size = 256\ndef prepare_image(image, sigmaX = 10, do_random_crop = False):\n    #ben graham preparation\n    # import image\n    #image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # perform smart crops\n    image = crop_black(image, tol = 7)\n    if do_random_crop == True:\n        image = random_crop(image, size = (0.9, 1))\n    \n    # resize and color\n    image = cv2.resize(image, (int(image_size), int(image_size)))\n    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), sigmaX), -4, 128)\n    \n    # circular crop\n    image = circle_crop(image, sigmaX = sigmaX)\n    return image\n\n\n### automatic crop of black areas\ndef crop_black(img, tol = 7):\n    \n    if img.ndim == 2:\n        mask = img > tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    \n    elif img.ndim == 3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img > tol\n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        \n        if (check_shape == 0): \n            return img \n        else:\n            img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img  = np.stack([img1, img2, img3], axis = -1)\n            return img\n        \n                \n### circular crop around center\ndef circle_crop(img, sigmaX = 10):   \n        \n    height, width, depth = img.shape\n    \n    largest_side = np.max((height, width))\n    img = cv2.resize(img, (largest_side, largest_side))\n\n    height, width, depth = img.shape\n    \n    x = int(width \/ 2)\n    y = int(height \/ 2)\n    r = np.amin((x,y))\n    \n    circle_img = np.zeros((height, width), np.uint8)\n    cv2.circle(circle_img, (x,y), int(r), 1, thickness = -1)\n    \n    img = cv2.bitwise_and(img, img, mask = circle_img)\n    return img \n\n\n### random crop\ndef random_crop(img, size = (0.9, 1)):\n\n    height, width, depth = img.shape\n    \n    cut = 1 - random.uniform(size[0], size[1])\n    \n    i = random.randint(0, int(cut * height))\n    j = random.randint(0, int(cut * width))\n    h = i + int((1 - cut) * height)\n    w = j + int((1 - cut) * width)\n\n    img = img[i:h, j:w, :]    \n    \n    return img","044fa876":"def read_data(path):\n    datagen = ImageDataGenerator(rescale = 1.\/255, preprocessing_function=prepare_image)\n    return datagen.flow_from_directory(directory = path,\n                                       target_size = shape,\n                                       class_mode = 'categorical',\n                                       batch_size = batch_size,\n                                       shuffle=True\n                                      )","88c42adc":"train = read_data(train_path)\ntest = read_data(test_path)\nprint(set(train.classes))","8aeaf8eb":"def createModel():\n    model = Sequential()\n    #conv=> relu => max pool\n    model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (256, 256, 3)))\n    model.add(Conv2D(32, (3, 3), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(512, activation = 'relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation = 'softmax'))\n    #return fully constructed model \n    opt = Adam(learning_rate = learning_rate)\n    model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=[\"accuracy\"])\n    return model","1a4f8044":"image_batch, label_batch = next(iter(train))\n\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10, 10))\n    for n in range(15):\n        ax = plt.subplot(5, 5, n + 1)\n        plt.imshow(image_batch[n])\n        #print(list(label_batch[n]))\n        #print(list(label_batch[n]).index(1.0))\n        title = get_class_name(list(label_batch[n]).index(1))\n        plt.title(title)\n        plt.axis(\"off\")\n\nshow_batch(image_batch, label_batch)\n#maybe images should be processed so that all doesnt contain black space","865ad76e":"nb_train_samples = 34792 # number of training samples\nnb_test_samples = 4971 # number of test samples","244ebeb0":"model = createModel()\nmodel.summary()","53210b94":"history = model.fit(train, steps_per_epoch = nb_train_samples \/\/ batch_size, epochs = 15, validation_data = test)","3f459b57":"\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(15)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","7d70fa96":"#Transfer Learning Part\nimport tensorflow as tf\nbase_model = tf.keras.applications.VGG16(input_shape = (224, 224, 3),\n                                               include_top = False,\n                                               weights = \"imagenet\")\n#prevent weight update while training\nfor layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalAveragePooling2D(),\n                                 tf.keras.layers.Dropout(0.6),\n                                 tf.keras.layers.BatchNormalization(),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\nmodel.summary()","ee1bf07e":"#training\nopt = Adam(lr=1e-3)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","c9eab557":"history = model.fit(train, steps_per_epoch = nb_train_samples \/\/ batch_size, epochs = 5, validation_data = test)","fc3e4d42":"\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(5)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","9081a8f2":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test.classes, predictions)\nimport seaborn as sns\nsns.heatmap(cm, annot = True, cmap = \"Blues\")","50c7ea0d":"# **Computer Vision Workshop**"}}