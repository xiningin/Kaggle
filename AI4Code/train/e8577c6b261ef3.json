{"cell_type":{"668d6363":"code","6b9e1e6e":"code","e24d90ff":"code","3152776e":"code","1399178a":"code","d4e094ca":"code","aee54d8a":"code","2bc0df8c":"code","57797b4b":"code","691b5372":"code","cb7e6daf":"code","e97b9bcb":"code","8c4a3459":"code","d1514b0a":"code","50bddcc4":"code","327ae637":"code","68d847c0":"code","0f6780d5":"code","5c3638b3":"code","88ffaf44":"code","879ec560":"code","20341e66":"code","7016d043":"code","011af738":"code","f10144fd":"code","390888a0":"code","882b390c":"code","5e4bca05":"code","a401c1e5":"code","cbda4b10":"code","af1cfc87":"code","ddb6f6f2":"code","ef1e3070":"code","debba7a1":"code","69169d07":"code","0df574d5":"code","681d23ac":"code","b832e093":"code","a9d3df79":"code","500e5957":"code","004338d5":"code","2d66f2d8":"code","b0ff3afb":"code","2478e686":"code","0a689356":"code","0a1955fd":"code","4638e487":"code","2c163702":"code","82d8e080":"code","e05b212b":"code","15c5c3dc":"code","f7b6b3f4":"code","57264bbe":"code","c151c512":"code","32950d82":"code","add2be21":"code","c8c1b104":"code","95691ac3":"code","e8a68948":"code","519defe7":"code","8fbae87c":"code","4dacff89":"code","8b96fa6d":"code","4495f428":"code","aa6a086a":"code","f11c4607":"code","2311ad7c":"code","4c7b8170":"code","0b35c4b2":"markdown","3f397ab4":"markdown","c01c9105":"markdown","8d758686":"markdown","d2c9d1a8":"markdown","81a1731d":"markdown","73d3432b":"markdown","e42b6bf0":"markdown","dda45a9f":"markdown","c30dcfb5":"markdown","e3f6d67a":"markdown","e1ce61b2":"markdown","89d326dd":"markdown","56781f87":"markdown","cbaa9a4f":"markdown","ba07000b":"markdown","4cc53136":"markdown","d2265269":"markdown","983d3a65":"markdown","50e1f545":"markdown","415293d4":"markdown","b3333b92":"markdown","653ebb44":"markdown","8e3b455c":"markdown","2e4ea0ca":"markdown","87b1d545":"markdown","2e83dea0":"markdown","058bba31":"markdown","b0248ed7":"markdown","6ec50e39":"markdown","3006eb3d":"markdown","0aceacbb":"markdown","ed98e228":"markdown","ace85a3f":"markdown","b7142f0c":"markdown","59cc6c0b":"markdown","41f03e64":"markdown","6e38eb22":"markdown","75ea68ea":"markdown","2df12235":"markdown","eed6a26d":"markdown","3a2d0de3":"markdown","c2160c17":"markdown"},"source":{"668d6363":"%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model, datasets\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport statsmodels.formula.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import product\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom xgboost import XGBClassifier\nfrom numpy import loadtxt\nfrom sklearn.metrics import accuracy_score\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)","6b9e1e6e":"f1 = pd.read_csv('..\/input\/sales_train.csv')\nf1.head()","e24d90ff":"print(f1.shape)","3152776e":"f2 = pd.read_csv('..\/input\/items.csv')\nf2.head()","1399178a":"f3 = pd.read_csv('..\/input\/item_categories.csv')\nf3.head()","d4e094ca":"f4 = pd.read_csv('..\/input\/shops.csv')\nf4.head()","aee54d8a":"teste = pd.read_csv('..\/input\/test.csv').set_index('ID')\nteste","2bc0df8c":"print(f2.shape,f3.shape,f4.shape)","57797b4b":"f1['item_price'].describe()","691b5372":"%matplotlib notebook\n\nf1['item_price'].hist(bins=30,edgecolor='black')","cb7e6daf":"f1['item_price'].mean()","e97b9bcb":"%matplotlib notebook\n\nf1.boxplot(column='item_price')","8c4a3459":"%matplotlib notebook\n\nf1.boxplot(column='item_cnt_day')","d1514b0a":"f1['item_cnt_day'].mean()","50bddcc4":"print(f1.count())","327ae637":"%matplotlib notebook\nds1 = f1[f1['item_price'] < 50000]\nds1['item_price'].hist(bins=30,edgecolor='black')","68d847c0":"print(float(ds1['item_price'].shape[0])\/float(f1.shape[0]))","0f6780d5":"%matplotlib notebook\nds2 = f1[f1['item_price'] < 5000]\nds2['item_price'].hist(bins=30,edgecolor='black')","5c3638b3":"print(float(ds2['item_price'].shape[0])\/float(f1.shape[0]))","88ffaf44":"dataset=f1[f1['item_price'] < 50000]\nf1[f1['item_price'] > 50000]","879ec560":"dataset['item_price'].mean()","20341e66":"f1[f1['item_cnt_day'] > 2000]\ndataset=f1[f1['item_cnt_day'] < 2000]","7016d043":"dataset=dataset[dataset['item_price'] > 0]\nf1[f1['item_price'] <= 0]","011af738":"devolvidos = f1[f1['item_cnt_day'] <= 0]\nf1[f1['item_cnt_day'] <= 0]","f10144fd":"%matplotlib notebook\ndevolvidos['shop_id'].hist(bins=60,edgecolor='black')\n","390888a0":"%matplotlib notebook\ndevolvidos['item_id'].hist(bins=22170,edgecolor='black')","882b390c":"devolvidos['item_id'].value_counts()\n","5e4bca05":"print(dataset['item_cnt_day'].max(),)","a401c1e5":"%matplotlib notebook\nplt.scatter(y=dataset['item_cnt_day'], x=dataset['item_price'], color='blue', s=50, alpha=.5)\nX_plot = sp.linspace(min(dataset['item_price']), max(dataset['item_price']), len(dataset['item_price']))\nplt.ylim(-300,3000)\nplt.xlim(-1000,50000)\nplt.title('Gr\u00e1fico de dispers\u00e3o')\nplt.ylabel('$y$ - Quantidade de itens vendidos')\nplt.xlabel('$x$ - Pre\u00e7o do item')\nplt.show()","cbda4b10":"%matplotlib notebook\nplt.scatter(y=dataset['item_cnt_day'], x=dataset['item_price'], color='blue', s=50, alpha=.5)\nX_plot = sp.linspace(min(dataset['item_price']), max(dataset['item_price']), len(dataset['item_price']))\nplt.ylim(-300,3000)\nplt.xlim(-300,5000)\nplt.title('Gr\u00e1fico de dispers\u00e3o')\nplt.ylabel('$y$ - Quantidade de itens vendidos de uma s\u00f3 vez')\nplt.xlabel('$x$ - Pre\u00e7o do item')\nplt.show()","af1cfc87":"print((f1[f1['item_price'] <= 5000].shape[0])\/f1['item_price'].shape[0])","ddb6f6f2":"dataset.corr()","ef1e3070":"plt.figure(figsize=(7,4)) \nsns.heatmap(dataset.corr(),annot=True,cmap='cubehelix_r')\nplt.show()","debba7a1":"print(dataset['shop_id'].max())","69169d07":"%matplotlib notebook\ndataset['shop_id'].hist(bins=60,edgecolor='black')","0df574d5":"ano1=dataset[dataset['date_block_num'] <= 11]\nano2=dataset[dataset['date_block_num'] > 11]\nano2=ano2[ano2['date_block_num'] <= 23]\nano3=dataset[dataset['date_block_num'] > 23]\nprint(\"Minimo m\u00eas do ano 1:\",ano1['date_block_num'].min(),\"M\u00e1ximo m\u00eas do ano 1:\",ano1['date_block_num'].max())\nprint(\"Minimo m\u00eas do ano 2:\",ano2['date_block_num'].min(),\"M\u00e1ximo m\u00eas do ano 2:\",ano2['date_block_num'].max())\nprint(\"Minimo m\u00eas do ano 3:\",ano3['date_block_num'].min(),\"M\u00e1ximo m\u00eas do ano 3:\",ano3['date_block_num'].max())","681d23ac":"%matplotlib notebook\nano1['date_block_num'].hist(bins=12,edgecolor='black',alpha=0.5)","b832e093":"%matplotlib notebook\nano2['date_block_num'].hist(bins=12,edgecolor='black',alpha=0.5)","a9d3df79":"%matplotlib notebook\nano3['date_block_num'].hist(bins=10,edgecolor='black',alpha=0.5)","500e5957":"dataset=pd.merge(dataset, f2, how='inner')\ndataset.sort_values(by=['date'], inplace=True)\ndataset.head(3)","004338d5":"print(dataset['item_category_id'].max())","2d66f2d8":"%matplotlib notebook\ndataset['item_category_id'].hist(bins=84,edgecolor='black')","b0ff3afb":"'''\nv=[]\nw=[]\nx=[]\ny=[]\nz=[]\nfor i in range (33):\n    filtroMes = dataset[dataset['date_block_num'] == i]\n    for j in range (60):\n        filtroShop = filtroMes[filtroMes['shop_id'] == j]\n        for k in range (22170):\n            filtroItem = filtroShop[filtroShop['item_id'] == k]\n            if(filtroItem.count()[1] > 0):\n                v.append(filtroItem.iloc[0][1])\n                w.append(filtroItem.iloc[0][2])\n                x.append(filtroItem.iloc[0][3])\n                y.append(filtroItem['item_price'].mean())\n                z.append(filtroItem['item_cnt_day'].sum())\n\n\n#data = OrderedDict(\n#{\n#'date_block_num': [teste.iloc[0][1],teste.iloc[0][1]],\n#'shop_id': [teste.iloc[0][2],teste.iloc[0][2]],\n#'item_id': [teste.iloc[0][3],teste.iloc[0][3]],\n#'item_price': [teste['item_price'].mean(),teste.iloc[0][4]],\n#'item_cnt_month': [teste['item_cnt_day'].sum(),teste.iloc[0][5]]\n#})\n\ndata = OrderedDict(\n{\n'date_block_num': v,\n'shop_id': w,\n'item_id': x,\n'item_price': y,\n'item_cnt_month': z\n})\ndf = pd.DataFrame(data)\ndf.to_csv('filtro1.csv')\ndf\n'''","2478e686":"matrix = []\ncols = ['date_block_num','shop_id','item_id']\n\nfor i in range(34):\n    sales = dataset[dataset.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\n\n\ngroup = dataset.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)\n                                .astype(np.float16))\nmatrix","0a689356":"teste['date_block_num'] = 34\nteste['date_block_num'] = teste['date_block_num'].astype(np.int8)\nteste['shop_id'] = teste['shop_id'].astype(np.int8)\nteste['item_id'] = teste['item_id'].astype(np.int16)\nmatrix = pd.concat([matrix, teste], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True)","0a1955fd":"matrix['month'] = matrix['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","4638e487":"cache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num    \nmatrix","2c163702":"'''matrix['item_shop_last_sale'].describe()","82d8e080":"cache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num   \n            \nmatrix","e05b212b":"matrix=pd.merge(matrix, f2, how='inner')\nmatrix=pd.merge(matrix, f1, how='inner')\nmatrix\n","15c5c3dc":"'''matrix['item_category_id'].describe()","f7b6b3f4":"'''matrix['item_last_sale'].describe()","57264bbe":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","c151c512":"'''matrix = lag_feature(matrix, [1,2,3,6], 'item_cnt_month')\nmatrix","32950d82":"group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\nmatrix","add2be21":"group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\nmatrix","c8c1b104":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\nmatrix","95691ac3":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\nmatrix'''\n","e8a68948":"group = dataset.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = dataset.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\n","519defe7":"'''group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,4,5,6], 'item_avg_item_price')\nmatrix.drop(['item_avg_item_price'], axis=1, inplace=True)'''","8fbae87c":"'''group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","4dacff89":"'''matrix","8b96fa6d":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\nmatrix","4495f428":"matrix = matrix[matrix.date_block_num > 5]\ncolumns=['item_name','item_category_name','date','delta_price_lag']\nfor c in columns:\n    if c in matrix:\n        matrix.drop(c, axis = 1, inplace = True)\nmatrix","aa6a086a":"'''matrix['date_item_avg_item_cnt_lag_1'].describe()","f11c4607":"X_train = matrix[matrix.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = matrix[matrix.date_block_num < 33]['item_cnt_month']\nX_valid = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)","2311ad7c":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=5,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 4)\n    plot_importance(model)\n    pyplot.show()","4c7b8170":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\nsubmission = pd.DataFrame({\n    \"ID\": teste.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\nmatrix.to_csv('data.csv', index=False)\ndel matrix\nplot_importance(model)\npyplot.show()","0b35c4b2":"A proxima feature \u00e9 quatos meses desde a \u00fatima venda daquele item naquela loja","3f397ab4":"M\u00e9dia dos itens vendidos no m\u00eas passado, por categorias","c01c9105":"Perceba que 98,84766% dos itens tem pre\u00e7os abaixo de 5000\n","8d758686":"Antes de minerarmos poss\u00edveis caracteristicas para os dados, vamos sintetizar todo o aprendizado da an\u00e1lise:\n<li>A maior parte dos produtos vendidos tem pre\u00e7o abaixo de 5000<\/li> \n<li>A partir de um certo ponto, itens mais caros tendem a ser menos vendidos<\/li> \n<li>Durante 2 anos consecutivos o m\u00eas de dezembro foi o m\u00eas que houve o maior \u00edndice de vendas, provavelmente por conta do natal<\/li>\n<li>Os gr\u00e1ficos de vendas mesais por ano tem o comportamento de um vale, maiores nas extremidades e menores no centro <\/li>\n<li>Algumas lojas s\u00e3 respos\u00e1veis por quase todas as vendas","d2c9d1a8":"<h3>Popularidade das lojas<\/h3>","81a1731d":"<h4>Peneirando a base de dados:<\/h4>","73d3432b":"Todos os itens com pre\u00e7os acima de 50000 tiveram apenas 1 venda durante todo o periodo, pode-se ent\u00e3o considerar estas vendas como eventos isolados e retirar as linhas da base de dados. Em um \u00fanico dia foi vendida 2000 unidades de um mesmo item, como s\u00f3 existe ocorr\u00eancia dessa quantidade de vendas em uma \u00fanica linha, pode-se considerar como outro evento isolado.","e42b6bf0":"M\u00e9dia de itens vendidos no ultimo m\u00eas\n","dda45a9f":"<b>Pelo gr\u00e1fico no formato boxplot, encontramos 1 valor outlier extremamente longe da m\u00e9dia dos pre\u00e7os<\/b>","c30dcfb5":"Pode-se observar que o maior pre\u00e7o da coluna \"item_price\" est\u00e1 bem acima da m\u00e9dia, tem-se tamb\u00e9m um valor negativo entre os pre\u00e7os, o que \u00e9 bem estranho.","e3f6d67a":"Existe uma \u00fanica venda registrada com o \"item_price\" negativo, isso tamb\u00e9m pode ser considerado um evento isolado.","e1ce61b2":"Anexando o conjunto de teste ao dataset como o m\u00eas 34, isso facilitar\u00e1 na hora de criar as features.","89d326dd":"<h1>Predict Future Sales - An\u00e1lise explorat\u00f3ria de dados<\/h1>","56781f87":"<h3>An\u00e1lises gr\u00e1ficas<h3>","cbaa9a4f":"<b>Dimens\u00f5es das tabelas<\/b>","ba07000b":"Drop dos 6 primeiros meses e da coluna de nomes de itens","4cc53136":"<b>Perceba que n\u00e3o existe valores nulos em nenhuma linha do dataset<\/b>","d2265269":"<b>Correla\u00e7\u00e3o entre as vari\u00e1veis:<\/b>","983d3a65":"Um grande problema da base de dados s\u00e3o os itens com \"item_cnt_day\" < 0, muito provavelmente estas foram devolu\u00e7\u00f5es de itens. Naturalmente, produtos, sejam de qualquer \u00e1rea, podem vir defeituosos ou n\u00e3o suprirem as espectativas do cliente, por\u00e9m se h\u00e1 uma grande frequ\u00eancia de devolu\u00e7\u00e3o de um mesmo produto, com o passar do tempo, sua popularidade tender\u00e1 a cair.","50e1f545":"<h2>Entendendo a base de dados<\/h2>\n\n<h3>Descri\u00e7\u00e3o dos arquivos:<\/h3>\n<li><b>sales_train.csv<\/b> - o conjunto de treinamento. Dados hist\u00f3ricos di\u00e1rios de janeiro de 2013 a outubro de 2015.<\/li>\n<li><b>test.csv<\/b> - o conjunto de teste. O problema se baseia na predi\u00e7\u00e3o das vendas dessas lojas e de seus produtos para novembro de 2015.<\/li>\n<li><b>sample_submission.csv<\/b> - um arquivo com o modelo de envio no formato correto.<\/li>\n<li><b>items.csv<\/b> - informa\u00e7\u00f5es suplementares sobre os itens \/ produtos.<\/li>\n<li><b>item_categories.csv<\/b> - informa\u00e7\u00f5es suplementares sobre as categorias de itens.<\/li>\n<li><b>shops.csv<\/b> - informa\u00e7\u00f5es suplementares sobre as lojas.<\/li>\n\n<h3>Atributos dos dados:<\/h3>\n<li><b>ID<\/b> - um identificador que representa uma tupla dentro do conjunto de testes<\/li>\n<li><b>shop_id<\/b> - identificador \u00fanico de uma loja <\/li>\n<li><b>item_id<\/b> - identificador \u00fanico de um produto <\/li>\n<li><b>item_category_id<\/b> - identificador \u00fanico da categoria de um item<\/li>\n<li><b>item_cnt_day<\/b> - n\u00famero de produtos vendidos. Ser\u00e1 previsto um valor mensal dessa medida<\/li>\n<li><b>item_price<\/b> - pre\u00e7o atual de um item<\/li>\n<li><b>date<\/b> - data em formato dia\/m\u00eas\/ano<\/li>\n<li><b>date_block_num<\/b> - um n\u00famero consecutivo utilizado para representar um m\u00eas. Janeiro de 2013 \u00e9 0, fevereiro de 2013 \u00e9 1, ..., outubro de 2015 \u00e9 33<\/li>\n<li><b>item_name<\/b> - nome do item<\/li>\n<li><b>shop_name<\/b> - nome da loja<\/li>\n<li><b>item_category_name<\/b> - nome da categoria do item<\/li>\n\n\n\n","415293d4":"Perceba que um item muito comprado, ja que todo item pode apresentar defeito, pode ser um dos itens mais devolvidos. Portanto a an\u00e1lise de devolu\u00e7\u00e3o deve ser feita em termo de % e n\u00e3o de quantidade de devolu\u00e7\u00f5es, devemos analisar a quantidade de itens devolvidos\/quantidade de itens comprados. ","b3333b92":"Agora, todas as lag_features ser\u00e3o percorridas para que os valores nulos sejam corrigidos.\n","653ebb44":"<h3>Vedas mensais por ano<\/h3>","8e3b455c":"<b>Importando Bibliotecas<\/b>","2e4ea0ca":"A proxima feature \u00e9 quatos meses desde a \u00fatima venda daquele item.","87b1d545":"<b>Detalhes sobre os pre\u00e7os dos itens: <\/b>","2e83dea0":"Perceba que aproximadamente 99,99990% dos itens tem pre\u00e7os abaixo de 50000","058bba31":"<b>Dimens\u00f5es da tabela<\/b>","b0248ed7":"Antes de criar as fetures vamos separar a base de dados em conjuntos \u00fanicos de shop\/item para cada m\u00eas, dessa forma trabalharemos de forma similar ao conjuto de testes. Tamb\u00e9m limitaremos os valores de 'item_cnt_month' entre 0 e 20 ( como recomendado pelos moderadores da competi\u00e7\u00e3o).","6ec50e39":"Fu\u00e7\u00e3o para criar as lag_features\n****","3006eb3d":"   Este Problema de aprendizado de m\u00e1quina est\u00e1 sendo desenvolvido em Python 3, a competi\u00e7\u00e3o trabalha com um conjunto de dados de vendas di\u00e1rias de uma empresa de software da R\u00fassia - <b>1C Company.<\/b> O objetivo final \u00e9 prever as vendas totais de cada produto e de cada loja no pr\u00f3ximo m\u00eas.\n","0aceacbb":"M\u00e9dia dos pre\u00e7os dos itens vendidos nos ultimos meses.","ed98e228":"Perceba que existem 22170 itens diferentes, 84 categorias diferentes e 60 lojas diferentes.","ace85a3f":"M\u00e9dia de itens vendidos no m\u00eas passado, por lojas.","b7142f0c":"<h2>Features<\/h2>","59cc6c0b":"Criando as lag_features para: \n","41f03e64":"Criando 4 novas lag features, baseadas na quantidade dos itens vendidos no m\u00eas, para o ultimo m\u00eas, os ultimos 2 meses, os ultimos 3 meses e os ultimos 6 meses\n","6e38eb22":"<h2>Aplicando xgboost<\/h2>","75ea68ea":"Agora ser\u00e3o adicionadas 2 features importantes para s\u00e9ries temporais, o m\u00eas e o dia (perceba que os n\u00fameros simbolizam vari\u00e1veis categ\u00f3ricas).","2df12235":"<b>Pelo gr\u00e1fico percebemos que a maioria dos itens tem pre\u00e7o abaixo de 25000<\/b>","eed6a26d":"<h2>Lendo os dados<\/h2>","3a2d0de3":"M\u00e9dia dos itens vendidos nos ultimos meses, por itens.","c2160c17":"<b>Em termos de vari\u00e1veis quantitativas temos apenas 1 correla\u00e7\u00e3o v\u00e1lida,  \"item_price \" ---> \"item_cnt_day\"  = 0.011<\/b>\n"}}