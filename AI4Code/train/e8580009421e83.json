{"cell_type":{"9335323f":"code","f9ce8f60":"code","69e67507":"code","65050c36":"code","7547ad16":"code","f2e40bad":"code","af253a79":"code","5f99f0a3":"code","1c49781d":"code","40ceb845":"code","7f7c418d":"code","2070788b":"code","1a151335":"code","de4ee0ac":"code","ca6dea4b":"code","34455a97":"code","45bf7912":"code","d45a28f5":"code","30fe6557":"code","fa487c53":"code","27c4ecfa":"code","519da2fc":"code","1968bf15":"code","687bfe8b":"markdown","9457fa31":"markdown","aab4ff0f":"markdown","39d61bdb":"markdown","43797ef0":"markdown","d2b08c5c":"markdown","e7c81118":"markdown","0bbbccfa":"markdown"},"source":{"9335323f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9ce8f60":"import sys\nimport pickle\nsys.path.append(\"..\/tools\/\")\nimport pandas\nimport numpy as np\n#from tester import dump_classifier_and_data\nimport matplotlib.pyplot as plt\nimport scipy\n%matplotlib inline","69e67507":"def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n    return_list = []\n    if isinstance(sort_keys, str):\n        import pickle\n        keys = pickle.load(open(sort_keys, \"rb\"))\n    elif sort_keys:\n        keys = sorted(dictionary.keys())\n    else:\n        keys = dictionary.keys()\n\n    for key in keys:\n        tmp_list = []\n        for feature in features:\n            try:\n                dictionary[key][feature]\n            except KeyError:\n                print (\"error: key \", feature, \" not present\")\n                return\n            value = dictionary[key][feature]\n            if value==\"NaN\" and remove_NaN:\n                value = 0\n            tmp_list.append( float(value) )\n\n        # Logic for deciding whether or not to add the data point.\n        append = True\n        # exclude 'poi' class as criteria.\n        if features[0] == 'poi':\n            test_list = tmp_list[1:]\n        else:\n            test_list = tmp_list\n        ### if all features are zero and you want to remove\n        ### data points that are all zero, do that here\n        if remove_all_zeroes:\n            append = False\n            for item in test_list:\n                if item != 0 and item != \"NaN\":\n                    append = True\n                    break\n        ### if any features for a given data point are zero\n        ### and you want to remove data points with any zeroes,\n        ### handle that here\n        if remove_any_zeroes:\n            if 0 in test_list or \"NaN\" in test_list:\n                append = False\n        ### Append the data point if flagged for addition.\n        if append:\n            return_list.append( np.array(tmp_list) )\n\n    return np.array(return_list)","65050c36":"def targetFeatureSplit( data ):\n    target = []\n    features = []\n    for item in data:\n        target.append( item[0] )\n        features.append( item[1:] )\n\n    return target, features","7547ad16":"email_features_list=['from_messages',\n    'from_poi_to_this_person',\n    'from_this_person_to_poi',\n    'shared_receipt_with_poi',\n    'to_messages',\n    ]\n\nfinancial_features_list=['bonus',\n    'deferral_payments',\n    'deferred_income',\n    'director_fees',\n    'exercised_stock_options',\n    'expenses',\n    'loan_advances',\n    'long_term_incentive',\n    'other',\n    'restricted_stock',\n    'restricted_stock_deferred',\n    'salary',\n    'total_payments',\n    'total_stock_value',\n]\n\nfeatures_list = ['poi']+email_features_list + financial_features_list \n### Load the dictionary containing the dataset\nimport pickle\noriginal = \"\/kaggle\/input\/enron-person-of-interest-dataset\/final_project_dataset.pkl\"\ndestination = \"final_project_dataset_unix.pkl\"\n\ncontent = ''\noutsize = 0\nwith open(original, 'rb') as infile:\n    content = infile.read()\nwith open(destination, 'wb') as output:\n    for line in content.splitlines():\n        outsize += len(line) + 1\n        output.write(line + str.encode('\\n'))\n        \ndata_dict = pickle.load(open(\"final_project_dataset_unix.pkl\", \"rb\"))\ndata_dict","f2e40bad":"#Dataset exploration\nprint ('Exploratory Data Analysis')\ndata_dict.keys()\nprint ('Total number of data points= {0}'.format(len(data_dict.keys())))\n\ncount_poi=0\nfor name in data_dict.keys():\n    if data_dict[name]['poi']==True:\n        count_poi+=1\n\nprint ('Number of Persons of Interest: {0}'.format(count_poi))\nprint ('Number of Non-Person of Interest: {0}'.format(len(data_dict.keys())-count_poi))","af253a79":"all_features=data_dict['BAXTER JOHN C'].keys()\nprint ('Total Features everyone on the list has:', len(all_features))\n\nmissing={}\nfor feature in all_features:\n    missing[feature]=0\n\nfor person in data_dict:\n    records=0\n    for feature in all_features:\n        if data_dict[person][feature]=='NaN':\n            missing[feature]+=1\n        else:\n            records+=1\n\nprint ('Number of Missing Values for each Feature:')\nfor feature in all_features:\n    print (feature, missing[feature])","5f99f0a3":"def PlotOutlier(data_dict, ax, ay):\n    data = featureFormat(data_dict, [ax,ay,'poi'])\n    for point in data:\n        x = point[0]\n        y = point[1]\n        poi=point[2]\n        if poi:\n            color='blue'\n        else:\n            color='green'\n\n        plt.scatter( x, y, color=color )\n    plt.xlabel(ax)\n    plt.ylabel(ay)\n    plt.show()\nPlotOutlier(data_dict, 'from_poi_to_this_person','from_this_person_to_poi')\nPlotOutlier(data_dict, 'total_payments', 'total_stock_value')\nPlotOutlier(data_dict, 'from_messages','to_messages')\nPlotOutlier(data_dict, 'salary','bonus')\n\n","1c49781d":"\ndef remove_outliers(data_dict, outliers):\n    for outlier in outliers:\n        data_dict.pop(outlier, 0)\noutliers =['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHARD EUGENE E']#latter two has almost all nan values\nremove_outliers(data_dict, outliers)\ndata_dict","40ceb845":"my_dataset = data_dict\ndef computeFraction( poi_messages, all_messages ):\n    \"\"\" given a number messages to\/from POI (numerator) \n        and number of all messages to\/from a person (denominator),\n        return the fraction of messages to\/from that person\n        that are from\/to a POI\n   \"\"\"\n    fraction = 0.\n    if all_messages =='NaN':\n        return fraction\n    if poi_messages=='NaN':\n        return fraction\n        \n    fraction=float(poi_messages)\/float(all_messages)\n    return fraction\nsubmit_dict={}\nfor name in my_dataset:\n\n    data_point = my_dataset[name]\n    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n    to_messages = data_point[\"to_messages\"]\n    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n    data_point[\"fraction_from_poi\"] = fraction_from_poi\n\n\n    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n    from_messages = data_point[\"from_messages\"]\n    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n    submit_dict[name]={\"from_poi_to_this_person\":fraction_from_poi,\n                       \"from_this_person_to_poi\":fraction_to_poi}\n    data_point[\"fraction_to_poi\"] = fraction_to_poi\n","7f7c418d":"my_feature_list=features_list+['fraction_from_poi','fraction_to_poi']\n\nfor x in range(len(my_feature_list)): \n    print (my_feature_list[x])","2070788b":"from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import SVC\n\n\ndef getkbest(data_dict, features_list, k):\n    data=featureFormat(my_dataset, features_list)\n    labels, features = targetFeatureSplit(data)\n    selection=SelectKBest(k=k).fit(features,labels)\n    scores=selection.scores_\n    unsorted_pairs = zip(features_list[1:], scores)\n    sorted_pairs=list(reversed(sorted(unsorted_pairs, key=lambda x: x[1])))\n    selection_best = dict(sorted_pairs[:k])\n    return selection_best\nnum=12 \nbest_features = getkbest(my_dataset, my_feature_list, num)\nprint ('Selected features and their scores: ', best_features)\n","1a151335":"l=best_features\ndef getList(dict): \n    list = [] \n    for key in dict.keys(): \n        list.append(key) \n          \n    return list\nl=getList(l)      \nmy_feature_list = ['poi'] + l\n#type(l)\nprint (\"{0} selected features: {1}\\n\".format(len(my_feature_list) - 1, my_feature_list[1:]))","de4ee0ac":"data = featureFormat(my_dataset, features_list, sort_keys = True)\nlabels, features = targetFeatureSplit(data)\nfrom sklearn import preprocessing\nscaler=preprocessing.MinMaxScaler()\nfeatures=scaler.fit_transform(features)","ca6dea4b":"from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit","34455a97":"from sklearn.tree import DecisionTreeClassifier\n\nclf_d=Pipeline([\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_d',DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=2, min_samples_split=7, splitter='best',random_state=42))])\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf_p=Pipeline([\n    ('standardscaler', StandardScaler()),\n    ('classifier', LogisticRegression(penalty='l2', tol=0.001, C=0.0000001, random_state=42))])\n\nfrom sklearn.cluster import KMeans\nclf_k=Pipeline([\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_k',KMeans(n_clusters=2, random_state=42, tol=0.001))])\n\nfrom sklearn.svm import SVC\nclf_s=Pipeline([\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_s',SVC(kernel='rbf',C = 1000,random_state = 42))])\n\n\nfrom sklearn.naive_bayes import GaussianNB\nclf_g=Pipeline(steps=[\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_g',GaussianNB())])\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf =Pipeline( [\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_rf',RandomForestClassifier())])","45bf7912":"from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report,accuracy_score\ndef evaluate(clf, features, labels, num=1):\n    print (clf)\n    accuracy=[]\n    precision=[]\n    recall=[]\n    for trial in range(num):\n        features_train, features_test, labels_train, labels_test=train_test_split(features, labels, test_size=0.3, random_state=42)\n        clf=clf.fit(features_train, labels_train)\n        pred=clf.predict(features_test)\n        accuracy.append(accuracy_score(labels_test,pred))\n        precision.append(precision_score(labels_test, pred))\n        recall.append(recall_score(labels_test, pred))\n    print ('precision: {}'.format(np.mean(precision)))\n    print ('recall: {}'.format(np.mean(recall)))\n    print ('accuracy: {}'.format(np.mean(accuracy)))\n    return np.mean(precision), np.mean(recall), confusion_matrix(labels_test, pred),classification_report(labels_test, pred)","d45a28f5":"print ('KMeans: ',evaluate(clf_k, features, labels))\n","30fe6557":"print ('Gaussian: ',evaluate(clf_g, features, labels))\n","fa487c53":"print ('Linear Regression: ', evaluate(clf_p, features, labels))","27c4ecfa":"print ('Random Forest: ',evaluate(clf_rf, features, labels))","519da2fc":"print ('SVC: ', evaluate(clf_s, features, labels))","1968bf15":"print ('Decision Tree: ', evaluate(clf_d, features, labels))","687bfe8b":"Exploring the dataset and all the features  after loading the pickle file: ","9457fa31":"Extract features and lables","aab4ff0f":"We can see that random forest gave us the best accuracy but here precision and recall values are better estimators since ours is not a balanced dataset wrt poi. So, naive bayes does the job with >=0.5 in both percision and recall.","39d61bdb":"Checking the recall, precision and accuracy values averaged over number of cross validictions","43797ef0":"Selecting K best features(k=12)","d2b08c5c":"Removing outliers","e7c81118":"Adding new features","0bbbccfa":"Missing features"}}