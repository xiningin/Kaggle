{"cell_type":{"dad185c8":"code","c28a30b2":"code","9ecb1d0b":"code","2aa7ebc7":"code","bb86eda9":"code","3b899659":"code","3508f28f":"code","ae39032c":"code","617a5687":"code","11363bd0":"code","1b609972":"code","4abef15b":"code","a610fc1f":"code","f970993d":"code","5a119bb9":"code","4c92fc07":"code","ac7b2268":"code","d1cb346e":"code","0c6bc815":"markdown","4163e4fc":"markdown","a2960bb1":"markdown","de4d0832":"markdown","0df9c1f1":"markdown","adec2206":"markdown","99bb6ab4":"markdown","51df043a":"markdown","252f004d":"markdown","0d47ed3f":"markdown"},"source":{"dad185c8":"\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/brasilian-houses-to-rent\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c28a30b2":"from sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.set_cmap('Dark2')","9ecb1d0b":"df = pd.read_csv('\/kaggle\/input\/brasilian-houses-to-rent\/houses_to_rent_v2.csv')\ndf.info()","2aa7ebc7":"fig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].scatter(df['area'], df['total (R$)'])\nax[1].scatter(np.log(df['area']), np.log(df['total (R$)']))\n\nax[0].set_title('Base area and total $')\nax[1].set_title('Log scale')\n\nax[0].set_xlabel('Area');\nax[0].set_ylabel('Total $');\n\nax[1].set_xlabel('Area');\nax[1].set_ylabel('Total $');","bb86eda9":"# Prepare data for illustration\nx = np.random.normal(0, 1, 950)\nx_out = np.random.randint(6, 10, 50)\nx_final = np.append(x_out, x)\nx_final_std = np.std(x_final); x_final_mean = np.mean(x_final)","3b899659":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))\nax1.axvspan(x_final_mean + 3*x_final_std, \n            x_final_mean - 3*x_final_std, alpha=.2, color='r')\nax1.axvspan(x_final_mean + 2*x_final_std, \n            x_final_mean - 2*x_final_std, alpha=.2, color='g')\nax1.axvspan(x_final_mean + 1*x_final_std, \n            x_final_mean - 1*x_final_std, alpha=.2, color='blue')\nax1.axvspan(x_final_mean-.05, \n            x_final_mean+.05, color='navy')\nax1.text(8, .25, 'Outliers')\nax1.text(4.5, .25, '3 stds')\nax1.text(2.5, .25, '2 stds')\nax1.text(1, .25, '1 std')\nax1.set_title('Standart deviation approach')\nsns.distplot(x_final, ax=ax1, color='black')\n\nax2.axvspan(max(x_final[x_final < np.quantile(x_final, .05)]), \n            min(x_final[x_final > np.quantile(x_final, 1-.05)]), \n            alpha=.2, color='r')\nax2.axvspan(max(x_final[x_final < np.quantile(x_final, .15)]), \n            min(x_final[x_final > np.quantile(x_final, 1-.15)]), \n            alpha=.2, color='g')\nax2.axvspan(max(x_final[x_final < np.quantile(x_final, .25)]), \n            min(x_final[x_final > np.quantile(x_final, 1-.25)]), \n            alpha=.2, color='b')\n\nax2.text(8, .25, 'Outliers')\nax2.text(4, .25, '95% quantile')\nax2.text(1, .35, '85% quantile',rotation=90)\nax2.text(.3, .25, '75% quantile',rotation=90)\nax2.set_title('Quantile approach')\nsns.distplot(x_final, ax=ax2, color='black')","3508f28f":"def quantile_outlier(x, t=.25):\n    x = np.array(x)\n    low_thr = np.quantile(x, t)\n    upp_thr = np.quantile(x, 1-t)\n    return np.array((low_thr < x) & (x < upp_thr), dtype=int)\n\ndef std_outlier(x, t):\n    mu = x.mean()\n    low_thr = mu - t * x.std()\n    upp_thr = mu + t * x.std()\n    return np.array((low_thr < x) & (x < upp_thr), dtype=int)","ae39032c":"iso = IsolationForest(n_estimators=300, contamination=.01, bootstrap=True)\nlof = LocalOutlierFactor(n_neighbors=250, algorithm='brute', contamination=.005)\n\n# Make predictions\ny_pred_lof = lof.fit_predict(df[['area', 'total (R$)']])\ny_pred_iso = iso.fit_predict(df[['area', 'total (R$)']])\ny_pred_qua = quantile_outlier(df['total (R$)'], .01) & quantile_outlier(df['area'], .01)\ny_pred_std = std_outlier(df['total (R$)'], 3) & std_outlier(df['area'], 1)","617a5687":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n\nax[0, 0].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_iso, marker='.')\nax[0, 0].set_title('IsolationForest')\nax[0, 1].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_lof, marker='.')\nax[0, 1].set_title('LocalOutlierFactor')\nax[1, 0].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_qua, marker='.')\nax[1, 0].set_title('Quantile based')\nax[1, 1].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_std, marker='.')\nax[1, 1].set_title('Std based')\nplt.xlabel('area');\nplt.ylabel('total$');\n","11363bd0":"from sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler","1b609972":"df['total (R$)'] = np.log(df['total (R$)'])","4abef15b":"df_tr = pd.get_dummies(df, columns=['city', 'animal', 'furniture'], prefix='d')\ndf_tr['floor'].replace('0', 0, inplace=True)\ndf_tr.replace('-', 0, inplace=True)","a610fc1f":"X = df_tr.drop('total (R$)',axis=1)\ny = df['total (R$)']","f970993d":"# Remove ouliers and make filtered datasets\nlof = LocalOutlierFactor()\nmask = np.array(lof.fit_predict(X)+1, dtype='bool')\nX_filtered = X[mask]\ny_filtered = y[mask]","5a119bb9":"lr = LinearRegression()\nrid = Ridge()\nlas = Lasso()\nela = ElasticNet()\nrnf = RandomForestRegressor()\n\n\nparams_lr = {\n    'normalize' : [True, False]\n}\nparams_rid = {\n    'alpha' : [.0, .01, .05, .5, 1], \n    'normalize' : [True, False]\n}\nparams_las = {\n    'alpha' : [.0, .01, .05, .5, 1], \n    'normalize' : [True, False]\n}\nparams_ela = {\n    'alpha' : [.0, .01, .05, .5, 1], \n    'l1_ratio' : [.0, .01, .05, .5, 1, 2],\n    'normalize' : [True, False]\n}\nparams_rnf = {\n    #'n_estimators' : np.arange(1, 150, 10), #for time reasons\n    #'max_depth' : [1, 5, 10, 20, 35, 50]\n    'max_depth': [20, 50], 'n_estimators': [31, 141]\n}\n","4c92fc07":"def get_grid_results(estimator, params, X, y, ncv=6):\n    gs = GridSearchCV(estimator, params, cv=ncv, n_jobs=-1, \n                      return_train_score=True, \n                      scoring='neg_mean_squared_error')\n    gs.fit(X, y)\n    print(gs.best_params_)\n    return gs","ac7b2268":"clf_names = ['lr', 'rid', 'las', 'ela', 'rnf']\nmean_train_scores = []\nmean_test_scores = []\nbest_clfs = []\n\nmean_train_scores_filtered = []\nmean_test_scores_filtered = []\nbest_clfs_filtered = []\n\nfor c, p in zip([lr, rid, las, ela, rnf], \n                [params_lr, params_rid, params_las, params_ela, params_rnf]):\n    clf = get_grid_results(c, p, X, y)\n    mean_train_scores.append(-np.mean(clf.cv_results_['mean_train_score']))\n    mean_test_scores.append(-np.mean(clf.cv_results_['mean_test_score']))\n    best_clfs.append(clf.best_estimator_)\n    \n    clf_filtered = get_grid_results(c, p, X_filtered, y_filtered)\n    mean_train_scores_filtered.append(-np.mean(clf_filtered.cv_results_['mean_train_score']))\n    mean_test_scores_filtered.append(-np.mean(clf_filtered.cv_results_['mean_test_score']))\n    best_clfs_filtered.append(clf_filtered.best_estimator_)","d1cb346e":"fig, (ax1) = plt.subplots(1, 1, figsize=(14, 5))\nax1.bar(np.arange(len(mean_train_scores))-.3, \n        mean_train_scores, width=.2, \n        label='Mean train full', color='b')\nax1.bar(np.arange(len(mean_test_scores))-.1,\n        mean_test_scores, width=.2, \n        label='Mean test full', color='b',alpha=.5)\nax1.bar(np.arange(len(mean_train_scores_filtered))+.1, \n        mean_train_scores_filtered, width=.2, \n        label='Mean train filtered', color='r')\nax1.bar(np.arange(len(mean_test_scores_filtered))+.3, \n        mean_test_scores_filtered, \n        width=.2, label='Mean test filtered', color='r', alpha=.5)\n\nax1.set_title('Mean train and test scores on full dataset and filtered datasets')\nax1.set_xticks(np.arange(len(clf_names)));\nax1.set_xticklabels(clf_names, fontsize=16)\nax1.set_yticks(np.arange(0, .6, .1));\nax1.set_yticklabels([f'{i:.1f}' for i in np.arange(0, .6, .10, )], fontsize=16)\nax1.grid();\nax1.legend();","0c6bc815":"The figure below shows regions with 2 and 3 standart deviations in both \"sides\" around mean value (navy line near zero on xaxis). All values outside the colored areas can be considered as outliers. We can use the approach in the task. \n\nBoth approaches are highly depends on distribution of data. ","4163e4fc":"Both figures shows that dataset have outliers in both columns. Lets apply simplest methods to detect abnormal objects. Methods uses standart deviation and quantiles. ","a2960bb1":"Lets visualize `area` and `total` features in regular and log scales (for `total`). ","de4d0832":"The functions for filtering data are below. ","0df9c1f1":"Lets compare Isolation Forest, Local Oulier Factor with simple approaches above. ","adec2206":"## Some regression analysis","99bb6ab4":"Dataset is full and have no any NA values","51df043a":"As we can see on the figure below filtering data can be useful for making correct prediction. In this case I use LocalOutlierFactor model with default parameters, but tuning params can make prediction procedure more accurate. \nWe can see the minimum difference in the results of RandomForest algorithm which more robust to outliers. ","252f004d":"Import some methods for anomality detection: [LocalOulierFactor](https:\/\/en.wikipedia.org\/wiki\/Local_outlier_factor) and [IsolationForest](https:\/\/en.wikipedia.org\/wiki\/Isolation_forest). ","0d47ed3f":"In this part we can apply simple regression approaches to predict total price. Also we compare results on full dataset and filtered dataset. "}}