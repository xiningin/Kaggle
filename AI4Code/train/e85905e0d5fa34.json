{"cell_type":{"3e4ee25a":"code","02cf8b1d":"code","94f4974a":"code","3e5fd2e3":"code","1f8da3c4":"code","4f0503cc":"code","16640879":"code","a8b16957":"code","0da12555":"code","2fde176d":"code","d9415ec2":"code","4136084d":"code","265afbbc":"code","bac4dcd4":"code","831b882b":"code","0d2be8f5":"code","f1a888af":"code","5650f28f":"code","5eea4aae":"code","35b6f7cb":"code","e439b8d2":"code","1f229034":"code","2a901104":"code","19c3de54":"code","bfc11cc9":"code","94834fe6":"code","b9efc277":"code","52d3516a":"code","90a9f3ca":"code","648e8e7f":"code","32b6cdbb":"code","f64d5726":"code","7c452483":"code","3f3ddbb9":"code","f41eb7af":"code","f7271bc7":"code","7f9bb67f":"code","a2e686f7":"code","3226a599":"code","45564316":"code","e35018f1":"code","ee3c7774":"code","f30b0b99":"markdown","b44150b5":"markdown","4b6e5722":"markdown","72f23f88":"markdown","c31ee69c":"markdown","8e996020":"markdown","4981c34c":"markdown","1ee3585e":"markdown","c0b7dc49":"markdown","f92b9739":"markdown","5d7985af":"markdown","016908d7":"markdown","23a2d18f":"markdown","c9a81ca5":"markdown","2331fb43":"markdown","597ab410":"markdown","a319c053":"markdown","574965e5":"markdown","e0b8878f":"markdown","34d2967b":"markdown","f9a03cef":"markdown","4f513718":"markdown","3bd5cd14":"markdown","899f7fe5":"markdown","321bf495":"markdown","f239f374":"markdown","b55bb7ae":"markdown","aaa05675":"markdown","fc38029b":"markdown","1606ed64":"markdown","c76b4f48":"markdown","787bf054":"markdown","830306a4":"markdown"},"source":{"3e4ee25a":"import pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport nltk\nimport string\nimport sklearn\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")","02cf8b1d":"df_train=pd.read_excel('\/kaggle\/input\/mh-newspred\/Data_Train.xlsx')\ndf_test=pd.read_excel('\/kaggle\/input\/mh-newspred\/Data_Test.xlsx')\n\n#since it is large file we can't open '.xlsx files in kaggle use the following code lines after adding the data in your kernel, It will going to display the path of your data set, So copy the followoing paths and add to load the data. \n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n   # for filename in filenames:\n       # print(os.path.join(dirname, filename))\n        \ndf_train.head()","94f4974a":"df_test.head()","3e5fd2e3":"#creating a data frame and adding NEWS column  to it.\ndf=df_train[[\"STORY\"]]\ndf[\"STORY\"]=df[\"STORY\"].astype(str)\ndf.head()\n\n#coverting all the words to lower case for case sensitive\ndf[\"text_lower\"]=df[\"STORY\"].str.lower()\ndf.head()\n\n#converting to string format and adding that as one column in 'df' data frame, \ndf[\"text_lower\"]=df[\"STORY\"].str.lower()\ndf.head()\n\n\n#Removing all the special characters in the data which is not required. For instance (don't,wasn't she's, , . ! etc) \nimport warnings\nimport re\nPUNCT_TO_REMOVE = string.punctuation   \ndef remove_punctuation(text_lower):\n    return text_lower.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"removed_punctuations\"]=df[\"text_lower\"].apply(lambda text_lower: remove_punctuation(text_lower))\n#df.head()\ndf[\"removed_punctuations\"]=df[\"removed_punctuations\"].astype(str)\ntext=df[\"removed_punctuations\"]\n\n\n# for removing special characters from the data ((!,@,#,$,%,^,&,*,(,) \ntext = text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) \ndf[\"special_removed\"]=text.astype(str)\ndf.head()\n","1f8da3c4":"#for removing numbers in the text data  \ntext2= df[\"special_removed\"].apply((lambda x: re.sub(r\"\\d\", \"\", x)))   #df['special_removed']=re.sub(r\"\\d\", \"\", df['special_removed'])\ndf[\"numbers_removed\"]=text2.astype(str)\n\n#converting to string\ndf[\"special_removed\"]=text.astype(str)\ndf[\"special_removed\"]=df[\"special_removed\"].astype(str)\n#df.head()\n\n\n#for removing of white spaces \n#df['numbers_removed']=df['numbers_removed'].str.strip()\n\n# for removing extra white spaces\ntext3=df['numbers_removed'].apply((lambda x: re.sub(r\"\\s+\", \" \", x)))\ndf[\"whitespace_removed\"]=text3.astype(str)\n\n#removing of stop words\nfrom nltk.corpus import stopwords\n\", \" .join(stopwords.words('english'))\n\nstopwords=set(stopwords.words('english'))\n\ndef remove_stopwords(sent):\n    return \" \" .join([word for word in str(sent).split()\n                      if word not in stopwords])\n\n\ndf[\"stopwords_removed\"]=df[\"whitespace_removed\"].apply(lambda sent: remove_stopwords(sent))\ndf.head()","4f0503cc":"import nltk\nfrom nltk.corpus import wordnet \nfrom nltk.stem import WordNetLemmatizer\n#df.drop([\"lemmatize_words\"], axis=1, inplace=True) \nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(sent):\n    return \" \".join([lemmatizer.lemmatize(word) for word in sent.split()])\n\ndf[\"lemmatized\"] = df[\"stopwords_removed\"].apply(lambda sent: lemmatize_words(sent))\n#df.drop([\"text_lemmatized\"], axis=1, inplace=True) \n\n\n#removing of single characters\ntext4= df[\"lemmatized\"].apply((lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)))   #df['special_removed']=re.sub(r\"\\d\", \"\", df['special_removed'])\ndf[\"singlechar_removed\"]=text4.astype(str)\n\n#removing extra spaces\ntext5=df['lemmatized'].apply((lambda x: re.sub(r\"\\s+\", \" \", x)))\ndf[\"space_removed\"]=text5.astype(str)\n\n#removing letters below 3 (and,how, why, etc)\ntext6=df['space_removed'].apply((lambda x: re.sub(r'\\W*\\b\\w{1,3}\\b', \" \", x)))\ndf[\"singledouble\"]=text6.astype(str)\n\ntext7=df['space_removed'].apply((lambda x: re.sub(r\"\\s+\", \" \", x)))\ndf[\"singledouble\"]=text7.astype(str)\n\nextra=df[\"singledouble\"].str.strip()\ndf[\"strip\"]=extra.astype(str)\ndf.head()","16640879":"#Bag of words creation\nfrom sklearn.feature_extraction.text import CountVectorizer \nbow_vectorizer = CountVectorizer(max_df=0.70, min_df=2, max_features=1000, stop_words='english')\n#max_df= atleast the words needs to present in 70% of the documents\n#min_df=2 (at least 2 times the words need to preset among all the documents)\n#maxfeatures= set of unique words\n \n# bag-of-words feature matrix\nbow = bow_vectorizer.fit_transform(df['strip'])\nprint(bow)","a8b16957":"from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\nprocessed_features = vectorizer.fit_transform(df['strip'])\nprint(processed_features)","0da12555":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nfrom keras.utils import to_categorical\n\nX2=processed_features\ny=df_train[\"SECTION\"]\n\nprint(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=0)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nlogpredicted_classes = model.predict(X_test)\nprint(confusion_matrix(y_test,logpredicted_classes))\nprint(classification_report(y_test,logpredicted_classes))\nprint(accuracy_score(y_test, logpredicted_classes))","2fde176d":"X=bow\ny=df_train[\"SECTION\"]\nprint(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\ntext_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\ntext_classifier.fit(X_train, y_train)\n\npredictions = text_classifier.predict(X_test)\nprint(predictions)","d9415ec2":"print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint(accuracy_score(y_test, predictions))","4136084d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","265afbbc":"from gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\n#loading the downloaded model\nmodel = KeyedVectors.load_word2vec_format('\/kaggle\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin', binary=True)\n\n#the model is loaded. It can be used to perform all of the tasks mentioned above.\n\n# getting word vectors of a word\ndog = model['dog']\ndog","bac4dcd4":"#perfoming similiar words for the given word from pre trained vectors\nprint(model.most_similar('AVENGERS'))","831b882b":"#perfoming similiar words for the given word from pre trained vectors\nprint(model.most_similar('CRICKET'))","0d2be8f5":"#performing king queen magic\nprint(model.most_similar(positive=['woman', 'king'], negative=['man']))","f1a888af":"#picking odd one out\nprint(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))","5650f28f":"#printing similarity index\nprint(model.similarity('woman', 'man'))","5eea4aae":"train=df['strip']\n\n\n#NOTE: #concatenating the 'strip column' because it is sereis of line,But if we want to do word embeddings all the tokens to be in one row, because it checks by word by word relation\njoin = ','.join(str(v) for v in train)\nhelo=join\n\n#do this lines if you get get any error related objects \n#train.dropna\n#train.dropna(inplace=True)\n","35b6f7cb":"#IMPORT FROM NLTK \nfrom nltk.tokenize import sent_tokenize, word_tokenize\nall_sentences = nltk.sent_tokenize(helo)\nnewsent = nltk.sent_tokenize(helo)  #This line makes all the data set of 7269 lines into one row\n#print(newsent)","e439b8d2":"tokenwords = [nltk.word_tokenize(sent) for sent in newsent]   #This line makes all the data set of tokens into one row\n#print(tokenwords)","1f229034":"classification=df_train['SECTION']\nlabel=to_categorical([classification.values]) \nlabel","2a901104":"import gensim\nfrom gensim.models import Word2Vec\nword2vec = Word2Vec(tokenwords, min_count=2)  #min_count=2:atleast if the word repeated twice,than it will take into vector\nvocabulary = word2vec.wv.vocab\n#print(vocabulary)\n\n# save model in ASCII (word2vec) format\n#filename = 'embedding_word2vec.txt'\n#model.wv.save_word2vec_format(filename, binary=False)","19c3de54":"sim_words = word2vec.wv.most_similar(positive=['phone'])\nsim_words\n","bfc11cc9":"word2vec.wv.similarity('phone', 'selfies')","94834fe6":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding","b9efc277":"helo=df['strip']\nfrom keras.preprocessing.text import Tokenizer\nword_tokenizer=Tokenizer()\nword_tokenizer.fit_on_texts(helo)","52d3516a":"vocab_length = word_tokenizer.word_index\n#print(vocab_length[10])","90a9f3ca":"#shows the length of unique tokens and adding more space in vocab_length above 1\nvocab_length = len(word_tokenizer.word_index) + 1\n#print(vocab_length)","648e8e7f":"trainembedded_sentences = word_tokenizer.texts_to_sequences(helo)\n#print(trainembedded_sentences)","32b6cdbb":"#run each line and check\nword_count = lambda sentence: len(word_tokenize(sentence))\nlongest_sentence = max(helo, key=word_count)\nlength_long_sentence = len(word_tokenize(longest_sentence))\nlength_long_sentence","f64d5726":"#padded_sentences = pad_sequences(trainembedded_sentences, length_long_sentence, padding='post')\ntrainpadded_sentences = pad_sequences(trainembedded_sentences, maxlen=558)\n#print(trainpadded_sentences)","7c452483":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros","3f3ddbb9":"#here  we can load either pre-trained models or our own customized file. But make it as text file bcz to write a function for that\n#here i am loading google word embeddings \nembeddings_dictionary = dict()\ngoogle_file = open('\/kaggle\/input\/googleword2vec-as-text-file\/googleVec.txt',encoding=\"utf8\") \n\nfor line in google_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n\n\ngoogle_file.close()\nembedding_matrix = zeros((vocab_length, 300))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","f41eb7af":"#converting to matrix shape\nX2=np.matrix(trainpadded_sentences)\nY2=np.matrix(label)\n","f7271bc7":"from sklearn.model_selection import train_test_split\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X2, Y2, test_size=0.3)\n","7f9bb67f":"X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)","a2e686f7":"print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","3226a599":"Model = Sequential()\nembedding_layer = Embedding(vocab_length, 300, weights=[embedding_matrix], input_length=558, trainable=False)\nModel.add(embedding_layer)\nModel.add(Flatten())\nModel.add(Dense(4, activation='softmax'))\n","45564316":"Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nprint(Model.summary())","e35018f1":"history = Model.fit(X_train, Y_train,  batch_size=32, epochs=10,  validation_data=(X_val, Y_val))","ee3c7774":"Model.evaluate(X_test, Y_test)[1]","f30b0b99":"*CHALLENGING TASK: *\nHave you ever thought how to input this pre-trained and custom word embeddings to learn the model, Once think before going down\n\nNow lets learn how to input this word embeddings to learn the model using ","b44150b5":"First lets learn how to load a model by using pre-trained word embeddings.\n\nWe are going to use google\u2019s pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download. \n\n> you can download from this link   https:\/\/code.google.com\/archive\/p\/word2vec\/","4b6e5722":"The next step is to find the number of words in the longest sentence and then to apply padding to the sentences having shorter lengths than the length of the longest sentence. because if you have a long words in one row and less in another row, so the matrix would be in appropriate shape, so in order to make it equal we will pad the sentence nothing but adding zeros to the short sentences to make it equal to the longest sentence in the data set","72f23f88":"shows the each word count","c31ee69c":"### **Keras Model**  <a id='section3'><\/a>\n\nKeras can be used to either learn custom words embedding or it can be used to load pretrained word embeddings. In this section, we will see how the Keras Embedding Layer can be used to learn custom word embeddings.","8e996020":"## Word Embeddings <a id='section2.3'><\/a>\n\nWord Embeddings: word embeddings are nothing but they store the similar kind of words for instance(synonyms of the words like \n\n**model.similarity('beautiful')**\n\n*gorgeous 0.98, *\n\n*wonderful 0.72,*\n\n*amazing 0.65,*\n\n*fantastic 0.60*\n\nso all the kinds of same words will stored in one word vector. So when ever the new word we give which is not there in the training data but due to word embeddings it will find the related kinds of words for that particular word and classifies the model.\n\nexample: we will use word embeddings in Machine Translation models\n\n*Word embeddings are of two types:*\n \n** 1)Pre-trained word embeddings** : This word embeddings are already trained by experts we can load that models for training our data, \n\n***Word2Vec (by Google)***\n\n***GloVe (by Stanford)***\n\n***fastText (by Facebook)***\n\n**Word2Vec:**\n\nThis model is provided by Google and is trained on Google News data. This model has 300 dimensions and is trained on 3 million words from google news data.\nTeam used skip-gram and negative sampling to build this model. It was released in 2013.\n\n**GloVe:**\n\nGlobal Vectors for words representation (GloVe) is provided by Stanford. They provided various models from 25, 50, 100, 200 to 300 dimensions based on 2, 6, 42, 840 billion tokens\nTeam used word-to-word co-occurrence to build this model. In other words, if two words co-occur many times, it means they have some linguistic or semantic similarity.\n\n**fastText:**\n\nThis model is developed by Facebook. They provide 3 models with 300 dimensions each.\nfastText is able to achieve good performance for word representations and sentence classifications because they are making use of character level representations.\nEach word is represented as bag of characters n-grams in addition to the word itself. For example, for the word partial, with n=3, the fastText representation for the character n-grams is <pa, art, rti, tia, ial, al>. <and> are added as boundary symbols to separate the n-grams from the word itself.\n \n** 2)Customized word embeddings** : This word embeddings can be created for our own data sets.\n\n\n","4981c34c":"create target label column ","1ee3585e":"**We have converted our sentences into padded sequence of numbers. The next step is to load the google pre-trained word embeddings and then create our embedding matrix that contains the words in our corpus and their corresponding values from google embeddings. Run the following script:**","c0b7dc49":"## TF-IDF <a id='section2.2'><\/a>\n\n**TF-IDF stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d.**   \n\nThis is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining. If i give you a sentence for example \u201cThis building is so tall\u201d. Its easy for us to understand the sentence as we know the semantics of the words and the sentence. But how will the computer understand this sentence? The computer can understand any data only in the form of numerical value. So, for this reason we vectorize all of the text so that the computer can understand the text better. By vectorizing the documents we can further perform multiple tasks such as finding the relevant documents, ranking, clustering and so on. This is the same thing that happens when you perform a google search. The web pages are called documents and the search text with which you search is called a query. google maintains a fixed representation for all of the documents. When you search with a query, google will find the relevance of the query with all of the documents, ranks them in the order of relevance and shows you the top k documents, all of this process is done using the vectorized form of query and documents. Although Googles algorithms are highly sophisticated and optimized, this is their underlying structure. Now coming back to our TF-IDF,\n\nTF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)\n\n![image.png](attachment:image.png)","f92b9739":"### Lemmatisation \n\nLemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n\n**For instance:**\n\nThe word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n\nThe word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n\nThe word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.\n\nfor more about stemming and lemmatizer https:\/\/towardsdatascience.com\/stemming-lemmatization-what-ba782b7c0bd8","5d7985af":"Lets check how to load the pre trained model and find similar words.","016908d7":"### **If you feel Kernel useful Plz Upvote...!**","23a2d18f":"Dont worry about the accuracy it depends upon the pre-trained vectors.","c9a81ca5":"*Import the keras layers*","2331fb43":"The code above will split the val_and_test size equally to the validation set and the test set.\n\nIn summary, we now have a total of six variables for our datasets we will use:\n\nX_train (10 input features, 70% of full dataset)\n\nX_val (10 input features, 15% of full dataset)\n\nX_test (10 input features, 15% of full dataset)\n\nY_train (1 label, 70% of full dataset)\n\nY_val (1 label, 15% of full dataset)\n\nY_test (1 label, 15% of full dataset)\n\nIf you want to see how the shapes of the arrays are for each of them (i.e. what dimensions they are), simply run","597ab410":"## Predict The News Category\n\nIt was one of the best hackathon problem given by **CYPHER-2019 ** Team. It is very interesting problem we can learn around 70% of Natural Language Processing. By seeing the dataset I was amazed how to solve this? But while founding the solution I learned almost everything about the NLP.\n \n![image.png](attachment:image.png)\n\nBy end of this kernel, I will share my knowledge how to approach this kind of models by solving the present News category hackathon dataset.\n\n\n## TOPICS\n\n1) [Text Preprocessing](#section1)\n\n2) [Converting Text To numerical Machine Understandable format](#section2)\n\n2.1) [Bag of Words](#section2.1)\n\n2.2) [TF-IDF](#section2.2)\n\n2.3) [Word Embeddings](#section2.3)\n\n3)   [Loading Word Embeddings to Train the model using Keras](#section3)\n  ","a319c053":"Inorder to find the longest sentence ","574965e5":"Finally, to convert sentences to their numeric counterpart, call the texts_to_sequences function and pass it the whole corpus.","e0b8878f":"now lets check how this keras model makes us easier to do tokens and pre-processing tasks.","34d2967b":"## Text Preprocessing <a id='section1'><\/a>\n\nIn this kernel, we will talk about the basic steps of text preprocessing.\n\nThese steps are needed for transferring text from human language to machine-readable format for further processing. We will also discuss text preprocessing tools.\nAfter a text is obtained, we start with text normalization. Text normalization includes:\n\n**converting all letters to lower or upper case**\n\n**converting numbers into words or removing numbers**\n\n**removing punctuations, accent marks and other diacritics**\n\n**removing white spaces**\n\n**expanding abbreviations**\n\n**removing stop words, sparse terms, and particular words**\n\n**applying lemmatization**","f9a03cef":"In the last section, we used one_hot function to convert text to vectors. Another approach is to use Tokenizer function from keras.preprocessing.text library.\n\nYou simply have to pass your corpus to the Tokenizer's fit_on_text method.","4f513718":"To get the number of unique words in the text, you can simply count the length of word_index dictionary of the word_tokenizer object. Remember to add 1 with the vocabulary size. This is to store the dimensions for the words for which no pretrained word embeddings exist.","3bd5cd14":"Importing the packages required.","899f7fe5":"**HuHuHU Heyyyy It's Working :) :)**\n","321bf495":"Here in my data sets **I had uploaded google word2vec as text file** because to pass these pre trained word embeddings as a embedding layer for the model to learn.\n\n**NOTE:** I takes long time to convert **'GoogleNews-vectors-negative300.bin' to .txt file** so better to use my data set in your kernel\n\nDO THE BELOW LINES TO CHECK THE PATH OF YOUR FILES IN YOUR INPUT ","f239f374":"**Gensim** library has different models, from gensim we will going to perform Word2vec similary by using **spacy **library\n\n**NOTE: ** *if you are doing in your loacal machines its better to install with ANACONDA by using 'conda install gensim' or pip install gensim*","b55bb7ae":"### Creating pre trained word embeddings for our own dataset\n\n*NOTE: Your Local Machine Computation need be more, It takes more Long time to create word embeddings, so better to take small data set with 100 sequence of words*\n\n**continuation from the CELL NUMBER In[45]** ONCE CHECK In[45] and do the below code","aaa05675":"This tells scikit-learn that your val_and_test size will be 30% of the overall dataset. The code will store the split data into the first four variables on the left of the equal sign as the variable names suggest.\n\nUnfortunately, this function only helps us split our dataset into two. Since we want a separate validation set and test set, we can use the same function to do the split again on val_and_test:","fc38029b":"### heyyy HuuuHuuu Yayyyyyy....!! Model was working :) :)","1606ed64":"Now using **Bag of Words** as a training set and check how much accuracy it is showing.","c76b4f48":"## Metrics Evalutation","787bf054":"**now pad with zeros after each sentence which is lesser than 558**","830306a4":"## Converting Text To numerical Machine Understandable way <a id='section2'><\/a>\n\nThere three approaches to do this:\n\n1) Bag of Words\n\n2) TF-IDF\n\n3) Word Embeddings\n\n## 1) Bag of Words <a id='section2.1'><\/a>\n\n![image.png](attachment:image.png)\n\nwe will study another very useful model that converts text to numbers i.e. the Bag of Words (BOW).\n\nSince most of the statistical algorithms, e.g machine learning and deep learning techniques, work with numeric data, therefore we have to convert text into numbers. Several approaches exist in this regard. However, the most famous ones are Bag of Words, TF-IDF, and word2vec. Though several libraries exist, such as Scikit-Learn and NLTK, which can implement these techniques in one line of code, it is important to understand the working principle behind these word embedding techniques. The best way to do so is to implement these techniques from scratch in Python and this is what we are going to do today.\n\nIf you are getting confused, let me give you an example:\n\nLet us say that you dataset consist of only two sentences:\n\nI love coffee.\nI like dogs.\nVocab: {I, love, coffee, I, like, dogs}\n\nIntegers assigned:\n\nI: 0, love: 1, coffee: 2, like: 3, dogs: 4\n\nVector representations:\n\nI: [1,0,0,0,0]\n\nlove: [0,1,0,0,0]\n\ncoffee: [0,0,1,0,0]\n\nlike: [0,0,0,1,0]\n\ndogs: [0,0,0,0,1]\n\nThis type of word representation is called one-hot vector representation.\n\nBut, as you can see same words in the same sentences than instead of 1 it will add how many times words was repeated for instance (2,3)"}}