{"cell_type":{"3fd3c8b9":"code","2ccf9585":"code","2089bc9a":"code","72055ce6":"code","69e14ee7":"code","07eb2307":"code","b312d871":"code","cbbad9fa":"code","85d3ca99":"code","1f55fdd2":"code","cfde1339":"code","efab3dda":"code","853d7367":"code","ba2c5d5e":"code","b0fe26a3":"code","40c24666":"code","1e3069d1":"code","db032726":"code","c3401e34":"code","e5a94e1f":"code","231595c7":"code","b5496740":"code","c7ca667c":"code","a97a7b7f":"code","a172cc15":"code","c38fa169":"code","0e46a5ce":"code","e5e3f71b":"code","79e42368":"code","918b0ba0":"code","57ec9313":"code","4eb61ec4":"code","8049e796":"code","016ac158":"code","7a21bcf9":"code","dc0b1c08":"code","c5994d17":"code","7748a5ab":"code","57fc5982":"code","42ae19d2":"code","ce25cade":"code","c83211a0":"code","5dc1141a":"code","cfa95ef8":"code","fcc0af06":"code","fe932142":"code","88719b2f":"code","e2155436":"code","c2509768":"markdown","4b855fba":"markdown","12ff5384":"markdown","aea15f03":"markdown","ffda984a":"markdown","7649726a":"markdown","f9ff1adb":"markdown","efdf43b7":"markdown","47f08f1b":"markdown","2c0f6edb":"markdown","4e8ea8a9":"markdown","302d81dc":"markdown","d2e1e0ca":"markdown","9d2247b4":"markdown","24cda55e":"markdown","69cc778c":"markdown","c02b599c":"markdown","d0b4c758":"markdown","b3fdfb5d":"markdown","af575ae1":"markdown","e00b0481":"markdown","01138a46":"markdown","5b109f92":"markdown","7c5104cd":"markdown","c99c4aba":"markdown","274f01c0":"markdown","767e7ffe":"markdown","101d8a1d":"markdown","e62c3170":"markdown","851d556f":"markdown","e221c156":"markdown","6e6dc8fa":"markdown","b815b77c":"markdown"},"source":{"3fd3c8b9":"import pandas as pd #we use this to load, read and transform the dataset\nimport numpy as np #we use this for statistical analysis\nimport matplotlib.pyplot as plt #we use this to visualize the dataset\nimport seaborn as sns #we use this to make countplots\nimport sklearn.metrics as sklm #This is to test the models","2ccf9585":"#here we load the train data\ndata = pd.read_csv(r'\/kaggle\/input\/home-loan-predictions\/Train_Loan_Home.csv')\n\n#and immediately I would like to see how this dataset looks like\ndata.head()","2089bc9a":"#now let's look closer at the dataset we got\ndata.info()","72055ce6":"data.shape","69e14ee7":"data.describe()","07eb2307":"data.describe(include='O')","b312d871":"#Let's see what the options are in the text columns (the objects)\nprint('Gender: ' + str(data['Gender'].unique()))\nprint('Married: ' + str(data['Married'].unique()))\nprint('Dependents: '+ str(data['Dependents'].unique()))\nprint('Education: '+ str(data['Education'].unique()))\nprint('Self_Employed: '+ str(data['Self_Employed'].unique()))\nprint('Property_Area: '+ str(data['Property_Area'].unique()))","cbbad9fa":"#first let's count the number of loans approved and rejected\nApproved = data[data['Loan_Status'] == 'Y']['Loan_Status'].count()\nRejected = data[data['Loan_Status'] == 'N']['Loan_Status'].count()\n\n#now let's put these results in a dataframe to visualize them\ndf = {\"Count\" : [Approved, Rejected]} #this is for the legend to be clear that it is counts\nStatus = pd.DataFrame(df, index=[\"Approved\", \"Rejected\"])\n\n#let's visualize the bar plot\nax = Status.plot(kind = 'bar', title = 'Status of the loans')\n\n#here I want to add the labels to the bars and to make this more clear I've made them white of color\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() \/ 2, p.get_height() - 30), color = 'white', fontweight = 'bold')","85d3ca99":"#let's see the percentages of the status:\nprint('The percentage of approved loans : %.2f' % (data['Loan_Status'].value_counts()[0] \/ len(data)))\nprint('The percentage of rejected loans : %.2f' % (data['Loan_Status'].value_counts()[1] \/ len(data)))","1f55fdd2":"#let's look in what columns there are missing values \ndata.isnull().sum().sort_values(ascending = False)","cfde1339":"#Let's look at the credit history in more detail to see what the best way is to handle these missing values\n#I will use seaborn for the visualization\nsns.countplot(data['Loan_Status'],hue=data['Credit_History'])","efab3dda":"print(pd.crosstab(data['Credit_History'],data['Loan_Status']))","853d7367":"print('The percentage of credit history yes : %.2f' % (data['Credit_History'].value_counts()[1] \/ len(data)))\nprint('The percentage of credit history no : %.2f' % (data['Credit_History'].value_counts()[0] \/ len(data)))","ba2c5d5e":"data['Credit_History'] = data['Credit_History'].fillna(1)\ndata.isnull().sum().sort_values(ascending = False)","b0fe26a3":"#Continue with Self_Employed\nsns.countplot(data['Loan_Status'],hue=data['Self_Employed'])","40c24666":"data['Self_Employed'] = data['Self_Employed'].fillna('No')\ndata.isnull().sum().sort_values(ascending = False)","1e3069d1":"#Continue with LoanAmount, as this is a numeric, thus continous number, I will use a scatterplot to see if there is a pattern \/ correlation. \nplt.scatter(data['Loan_Status'], data['LoanAmount'])","db032726":"#As the patterns look similar for yes and no, I will fill the missing values with the mean of the column\ndata['LoanAmount'] = data['LoanAmount'].fillna( data['LoanAmount'].mean())\ndata.isnull().sum().sort_values(ascending = False)","c3401e34":"#Let's drop the rest of the missing values:\ndata.dropna(inplace = True)\ndata.shape","e5a94e1f":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (20,10))\nax1.boxplot(data['ApplicantIncome'])\nax2.boxplot(data['CoapplicantIncome'])\nax3.boxplot(data['LoanAmount'])\nplt.show()","231595c7":"#Look closely at the ApplicantIncome column.\nplt.boxplot(data['ApplicantIncome'])","b5496740":"#We see that there are two great outliers here. \n#let's look closer to these two outliers\noutliers = data[data['ApplicantIncome'] > 50000]\noutliers.head()","c7ca667c":"#As you can see that these are just two rows and the status is not for both approved, I will remove these two rows for the model. \ndata = data[data['ApplicantIncome'] < 50000]\n#let's plot the applicant income again in a boxplot\nplt.boxplot(data['ApplicantIncome'])","a97a7b7f":"#still a lot of outliers above the 25000. Let's look closer to those again to be sure we need to add them to get a good model performance\noutliers = data[data['ApplicantIncome'] > 25000]\noutliers.head()","a172cc15":"#Look closely at the CoApplicantIncome column.\nplt.boxplot(data['CoapplicantIncome'])","c38fa169":"#We see that there are three great outliers here. \n#let's look closer to these two outliers\noutliers = data[data['CoapplicantIncome'] > 25000]\noutliers.head()","0e46a5ce":"#As you can see that these are just two rows and the status is not approved, I will remove these two rows for the model. \ndata = data[data['CoapplicantIncome'] < 25000]\n#let's plot the applicant income again in a boxplot\nplt.boxplot(data['CoapplicantIncome'])","e5e3f71b":"#First make the target column (Loan_Status) numerical\ndata['Loan_Status'] = np.where((data['Loan_Status'] == 'Y'), 1, 0)","79e42368":"#Next we will drop the loan_ID column as this will only confuse the model later on\ndata.drop('Loan_ID', axis=1, inplace=True)\ndata.info()","918b0ba0":"#Next, make all other columns numerical as well. \ndata['Married'] = np.where((data['Married'] == 'Yes'), 1, 0)\ndata['Gender'] = np.where((data['Gender'] == 'Female'), 1, 0)\ndata['Education'] = np.where((data['Education'] == 'Graduate'), 1, 0)\ndata['Self_Employed'] = np.where((data['Self_Employed'] == 'Yes'), 1, 0)\ndata['Dependents'] = np.where((data['Dependents'] == '0'), 0, 1) #I saw that there was no big difference between the number of dependents if there are any. So I made no dependents = 0  and yes dependents = 1","57ec9313":"#Lastly I want to change the Property_Area column, but I want to keep all three options. Therefore this I will do differently. \n\ndef f(row):\n  if row['Property_Area'] == \"Rural\":\n    val = 1\n  elif row['Property_Area'] == \"Urban\":\n    val = 0\n  else:\n    val = 2\n  return val\n\ndata['Property_Area'] = data.apply(f, axis=1)","4eb61ec4":"data.info()","8049e796":"# Most important features\nLet's continue by looking at the most important features according to three different tests. \nThan we will use the top ones to train and test our first model. ","016ac158":"#First we need to split the dataset in the y-column (the target) and the components (X), the independent columns. \n#This is needed as we need to use the X columns to predict the y in the model. \n\nX = data.iloc[:,0:11]  #independent columns \ny = data.iloc[:,-1]    #target column = Status of the loan","7a21bcf9":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","dc0b1c08":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Name of the column','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","c5994d17":"#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","7748a5ab":"#Load the chosen models here\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#add the logistic regression for cross check\nfrom sklearn.linear_model import LogisticRegression","57fc5982":"from sklearn.model_selection import train_test_split\n\n#First try with the 4 most important features\nX_4 = data[['Credit_History', 'CoapplicantIncome', 'Married', 'Property_Area']] #independent columns chosen \ny = data.iloc[:,-1]    #target column = Status of the loan\n\n#I want to withhold 30 % of the trainset to perform the tests\nX_train, X_test, y_train, y_test= train_test_split(X_4,y, test_size=0.3 , random_state = 25)","42ae19d2":"print('Shape of X_train is: ', X_train.shape)\nprint('Shape of X_test is: ', X_test.shape)\nprint('Shape of Y_train is: ', y_train.shape)\nprint('Shape of y_test is: ', y_test.shape)","ce25cade":"#Let's confirm that we use the same number of status approved versus disapproved in the test and train data.\n#As approved is 1, this can be counted easily. \nprint('The % approved status versus not approved in original_data :',data['Loan_Status'].value_counts().values\/ len(data))\nprint('\\nThe % approved status versus not approved in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('\\nThe % approved status versus not approved in in y_test :',y_test.value_counts().values\/ len(y_test))","c83211a0":"#To check the models, I want to build a check matrix within two functions:\ndef score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('DETAILS ACCURACY, PRECISION AND RECALL')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))\/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))\/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])","5dc1141a":"#Start with the K-Nearest Neighbors\nK_n = KNeighborsClassifier()\nK_n.fit(X_train, y_train)","cfa95ef8":"#Now let's see how this model performs\nprob_K = K_n.predict_proba(X_test)\nprint_metrics(y_test, prob_K, 0.3) ","fcc0af06":"#Continue with the decision tree with a max number of layers of 3\nD_tree = DecisionTreeClassifier(max_depth = 3)\nD_tree.fit(X_train, y_train)","fe932142":"#let's see it's performance\nprob_D = D_tree.predict_proba(X_test)\nprint_metrics(y_test, prob_D, 0.3)","88719b2f":"# logistic_regression model\nlogistic_mod = LogisticRegression(C = 1.0, class_weight = {0:0.45, 1:0.55}) \nlogistic_mod.fit(X_train, y_train)","e2155436":"#Check the performance of the logistic regression model\nprobabilities = logistic_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.3) ","c2509768":"This model does not seem to predict well enough for the positives. The true positives are 6 versus 43 false negative. On the other hand the true negatives are 121 over 1 false positive.  ","4b855fba":"# Predicting Loan Egibility","12ff5384":"This looks about the same, let's continue. ","aea15f03":"# Take a closer look at some of the features\nlet's look at the outliers!","ffda984a":"Right so now all columns are numeric\n\n","7649726a":"It seems that we have a lot of text \/ category information (these are of the Dtype 'object') and a few numerical columns (Dtypes 'int64' and 'float64'). \n\nThe last column 'Loan_status' is the column we would like to predict. ","f9ff1adb":"I will look closely at the top 3 here (as these have the most missing values) and I will drop the other missing value rows. ","efdf43b7":"The dataset consists of 614 rows and 13 columns. ","47f08f1b":"![BAIME banner](https:\/\/user-images.githubusercontent.com\/47600826\/89530907-9b3f6480-d7ef-11ea-9849-27617f6025cf.png)","2c0f6edb":"![lening](https:\/\/images.financialexpress.com\/2020\/07\/HOME-LOAN-HIKE.jpg)","4e8ea8a9":"# Try and check the models ","302d81dc":"# Predict on the testset\n\nTest the model on the Test dataset","d2e1e0ca":"The accuracy seems to be higher (true positives better, 19 now), but still room for improvement","9d2247b4":"Looks like a good feature to use, as there is clearly a difference in the size of the columns for the yes and the no, so let's look deeper!","24cda55e":"# Decision Tree","69cc778c":"## The problem\nIn this notebook we look at the data we got via this [Kaggle dataset](https:\/\/www.kaggle.com\/gavincanacam\/home-loan-predictions). \n\nThis company called \"Housing Finance company\" wants to automate the loan eligibility process based on the customer information and identify the factors\/customer segments who are eligible for taking the loan.\n\nWe will explore the dataset given, check the various features we have and we will make an algorithm that can predict whether or not the loan would be approved in order to automate the process","c02b599c":"As this seems to have no effect on the outcome, I will fill these with the most frequent one (so No) ","d0b4c758":"These seem to be ok for the model as 75% is approved. So let's keep them for now. ","b3fdfb5d":"# K-Nearest Neighbors","af575ae1":"# Split the dataset in train and test\nBefore we are going to use the models choosen, we will first split the dataset in a train and test set.\nThis because we want to test the performance of the model on the training set and to be able to check it's accuracy. \n","e00b0481":"This model seems to perform less than the decision tree model. 16 true positives and 1 false positives. ","01138a46":"It looks like this is not well balanced in this set.\nBut as this is the only data we have, I will leave this as is for now. ","5b109f92":"# Handling missing values\nLet's continue with handling the missing values in this dataset. \nLet's see where and how many missing values there are in this dataset.  ","7c5104cd":"# Logistic regression","c99c4aba":"# Loan Status in this Dataset\n\n![approved or rejected](https:\/\/db3pap006files.storage.live.com\/y4pVnKKIPUMfGtdOP-mIsJIDFD6QD9mNmC5br03t9oSX6uCFHlSgyrzOKvkBvemfQbgGRltJXJI1DygwGgxBzszvmqoQtfMhbsE_Ajl8VAnNDIy3BIOXRlTJAB3jdnZYTPtQFmMkHmo74vxcBUc_JjX1kW47Rp33UKov0MllAFFuPU-lzJypcr-s05Yv1bCIpcC9bwZsareXmkMCxxmCZBS67Ya2zrP2Ac3z3F0enmC6qo\/stamp-2114884_1920.png?psid=1&width=192&height=65)\n\nAs Loan Status is the column we want to predict, let's explore this column in the training dataset. ","274f01c0":"# Make all columns numeric\nWe need to make all column input numeric to use them further on. \nThis is what I will do now. ","767e7ffe":"Seems that the three feature selection models differ in what feature is the most important.\nFor the first test I will keep:\n- Credit history (high in all three tests and the highest in the correlation)\n- Co Applicant Income (high in two tests, negative in the correlation, but this is explainable, as no income for the spous means more risk)\n- Property Area (high in two tests)\n- Married (mentioned in two tests)\n\nAfter a test, these 4 gave better results than using all features. ","101d8a1d":"Seems there are more categorical (binary) columns, such as Gender, Married and education","e62c3170":"It seems that we have some strange outliers for the income and loan amounts. We will look and handle these later on. ","851d556f":"Seems that if you have a credit history, it is more likely to get the loan approved. \n\nOptions in handling these missing values:\n- Drop all the rows with missing values\n- Handle the missing values with 0 (so no history) as there is nothing clear. \n- Or we use the most frequent number, which is 1 for the credit history. \n\nIn this case, I tend to go for the most frequent number, as this is 86% of the dataset, so most likely to be true.","e221c156":"# Import the important libraries \/ packages\nThese packages are needed to load and use the dataset","6e6dc8fa":"# Conclusion:\nWe would need more data to make the models perform better. \n\nFor now, The decision tree has the highest accuracy and precision scores with the 4 most important features. \nTherefore this would be the model to use for the prediction on the status","b815b77c":"# Machine learning Model\nAs this is a binary problem (so yes or no in the status), I choose for binary models:\n- Decision Tree\n- K-nearest Neighbors\n\nBut we can cross check it with a logistic regression model here.\n\nFor the record, I left out Random Forrest, as this is a random decision tree model, so not the same each time you run the model"}}