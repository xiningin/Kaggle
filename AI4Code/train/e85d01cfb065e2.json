{"cell_type":{"b01b9a2a":"code","b0527614":"code","f3eff1d2":"code","26865702":"code","a74248e3":"code","503937df":"code","89348e8b":"code","5d92484f":"code","cb67c22b":"markdown","0eb51874":"markdown","29a75a2d":"markdown","57ce3211":"markdown","7cf61335":"markdown","197e1d3d":"markdown","2fe4dad3":"markdown","ce7ee428":"markdown"},"source":{"b01b9a2a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0527614":"shufData = pd.read_csv(\"\/kaggle\/input\/184702-tu-ml-ws-20-amazon-commerce-reviews\/amazon_review_ID.shuf.sol.ex.csv\")\ntestData = pd.read_csv(\"\/kaggle\/input\/184702-tu-ml-ws-20-amazon-commerce-reviews\/amazon_review_ID.shuf.tes.csv\")\ntrainingData = pd.read_csv(\"\/kaggle\/input\/184702-tu-ml-ws-20-amazon-commerce-reviews\/amazon_review_ID.shuf.lrn.csv\")","f3eff1d2":"print(trainingData.shape)","26865702":"#print(trainingData.head())\n#print(trainingData.describe())\n#print(trainingData['Class'].unique())\n#print(trainingData['Class'].describe())\n#corrmat = trainingData.corr()\n#f, ax = plt.subplots(figsize=(12, 9))\n#sns.heatmap(corrmat, vmax=.8, square=True);\n#labels = np.array(trainingData['actual'])\n#print(trainingData.dtypes)\nprint(testData.dtypes)\n","a74248e3":"training_LEARN = trainingData.copy()\ntest_LEARN = testData.copy()","503937df":"# Choose parameters for the model\npN_estimators = 11000          # int\npRandom_state = 0             # int\npCritirion = \"gini\"           # or entropy?\npMin_samples_split = 2        # int or float\npMin_samples_leaf = 1         # int\npMin_weight_fraction_leaf = 0 # float\npMax_features = \"auto\"        # {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}\npBootstrap = False             # bool\n\n\n# For test\/train split\npSplitSize = 0.2\n\n# Predictor variables\nX = training_LEARN.iloc[:, 1:10000].values\n\n# Value to be predicted variables\ny = training_LEARN.iloc[:, 10001].values # Class\n\n# Split the training into train and test - for proper classificaion, entire training will be used.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=pSplitSize, random_state=0)\n\n\n# Scale the data - Not neccesary in binary classification\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n# Start running the models\n## Create the regressor\nregressor = RandomForestClassifier(n_estimators = pN_estimators\n                                   , random_state = pRandom_state\n                                   , criterion = pCritirion\n                                   #, max_depth #int or none\n                                   , min_samples_split = pMin_samples_split\n                                   , min_samples_leaf = pMin_samples_leaf\n                                   , min_weight_fraction_leaf = pMin_weight_fraction_leaf\n                                   , max_features = pMax_features\n                                   , bootstrap = pBootstrap\n                                  )\n## Fit the model\nregressor.fit(X_train, y_train)\n## Calculate the estimate class\ny_pred = regressor.predict(X_test)\n\nprint(\"\\n******************************************\")\nprint(\"******************************************\")\nprint(\"** Results for \" + \"ImputeMethod\" + \": **\")\n\nprint(\"Accuraccy Score:\")\nprint(accuracy_score(y_test, y_pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Classification Report:\")\nprint(classification_report(y_test,y_pred))\n\n# Used parameters\nprint(\"\\n Used Parameters:\")\nprint(\"n_estimators = \" + str(pN_estimators))\nprint(\"random_state = \" + str(pRandom_state))\nprint(\"critirion = \" + str(pCritirion))\nprint(\"min_samples_split = \" + str(pMin_samples_split))\nprint(\"min_samples_leaf = \" + str(pMin_samples_leaf))\nprint(\"min_weight_fraction_leaf = \" + str(pMin_weight_fraction_leaf))\nprint(\"max_features = \" + str(pMax_features))\nprint(\"bootstrap = \" + str(pBootstrap))\nprint(\"splitSize = \" + str(pSplitSize))\n# See sizes of datasets\nprint(\"\\n Split sizes:\")\nprint(\"X_Train: \" + str(X_train.shape))\nprint(\"y_train: \" + str(y_train.shape))\n\nprint(\"X_test: \" + str(X_test.shape))\nprint(\"y_test: \" + str(y_test.shape))\n\nprint(\"******************************************\")\nprint(\"******************************************\")","89348e8b":"# Choose parameters for the model\npN_estimators = 12500         # int\npRandom_state = 0             # int\npCritirion = \"gini\"           # or entropy?\npMin_samples_split = 2        # int or float\npMin_samples_leaf = 1         # int\npMin_weight_fraction_leaf = 0 # float\npMax_features = \"auto\"        # {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}\npBootstrap = False             # bool\n# Bootstrap = False gives slightly better when testing - Not sure why!?\n\n# Predictor variables\nX_train = training_LEARN.iloc[:, 1:10000].values\nX_test = test_LEARN.iloc[:, 1:10000].values ##### REMEMBER TO CHANGE THIS INDEX!! CLASS IS IN PLACE 1 _ THIS FUCKED US UP!\n\n# Value to be predicted variables\ny_train = training_LEARN.iloc[:, 10001].values # Class\n\n\n\n# Scale the data - Not neccesary in binary classification\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n# Scale the data - Not neccesary in binary classification\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n# Start running the models\n## Create the regressor\nregressor = RandomForestClassifier(n_estimators = pN_estimators\n                                   , random_state = pRandom_state\n                                   , criterion = pCritirion\n                                   #, max_depth #int or none\n                                   , min_samples_split = pMin_samples_split\n                                   , min_samples_leaf = pMin_samples_leaf\n                                   , min_weight_fraction_leaf = pMin_weight_fraction_leaf\n                                   , max_features = pMax_features\n                                   , bootstrap = pBootstrap\n                                  )\n## Fit the model\nregressor.fit(X_train, y_train)\n## Calculate the estimate class\ny_pred = regressor.predict(X_test)\n\n\n# Run a second model as well - This should be rewritten\n#regressor2 = RandomForestClassifier(n_estimators = pN_estimators\n#                                   , random_state = pRandom_state\n#                                   , criterion = 'entropy'\n#                                   #, max_depth #int or none\n#                                   , min_samples_split = pMin_samples_split\n#                                   , min_samples_leaf = pMin_samples_leaf\n#                                   , min_weight_fraction_leaf = pMin_weight_fraction_leaf\n#                                   , max_features = pMax_features\n#                                   , bootstrap = pBootstrap\n#                                  )\n## Fit the model\nregressor.fit(X_train, y_train)\n#regressor2.fit(X_train, y_train)\n## Calculate the estimate class\ny_pred = regressor.predict(X_test)\n#y_pred2 = regressor2.predict(X_test)","5d92484f":"res = testData.copy()\n#res2 = testData.copy()\nres[\"Class\"] = y_pred\n#res2[\"Class\"] = y_pred2\n#print(res2[{'ID', 'Class'}])\n\n\n#res.to_csv('SolutionAmazonGroup34v03_gini_WithoutBootstrap.csv', index = False, header=True, columns={'ID', 'Class'})\nres.to_csv('SolutionAmazonGroup34v03_gini_WithoutBootstrap12500.csv', index = False, header=True, columns={'ID', 'Class'})\n#res2.to_csv('SolutionAmazonGroup34v03_entropy.csv', index = False, header=True, columns={'ID', 'Class'})","cb67c22b":"# Export The Full Model","0eb51874":"## The shape","29a75a2d":"## Default settings and imports","57ce3211":"## Training and Test split\nBefore running the full model, let's estimate the model on our own split\n","7cf61335":"# Train the full model","197e1d3d":"# Random Forest - Amazon Dataset\nThis is a copy of the workbook stored on kaggle workbooks (https:\/\/www.kaggle.com\/gunnarsjurarsonknudsen\/group-34-amazon-dataset-data-exploration\/edit\/run\/45846222)","2fe4dad3":"# Data Exploration\nFirst step is to get a feel for the data","ce7ee428":"# Read the data "}}