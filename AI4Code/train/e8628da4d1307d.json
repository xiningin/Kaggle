{"cell_type":{"15b58ea8":"code","d8a62c11":"code","b5eeda65":"code","c1fe0c43":"code","d1b73723":"code","cde9158a":"code","1bb18518":"code","21b89d66":"code","59e1b785":"code","1c8aa096":"code","57a4c10b":"code","2c8721c1":"code","22f0d1f0":"code","0ccfc010":"code","78586c92":"code","fa6e3f0f":"code","491a6e1c":"code","866bd4c6":"code","85b76e08":"code","ba45116b":"code","6f4386bc":"code","7658ae03":"code","343039de":"code","e4936128":"code","59ae9da8":"code","cec4b18f":"code","b727bd4a":"code","067f35ec":"code","b71df8d7":"code","cc68a638":"code","d3a01406":"markdown","14b74582":"markdown","da0ad1fc":"markdown","9dbb3473":"markdown","d64584c1":"markdown","07910086":"markdown","e63fb5cb":"markdown","c3669111":"markdown","577a54d9":"markdown","5c62ad39":"markdown","53425f94":"markdown"},"source":{"15b58ea8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8a62c11":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV, RepeatedKFold, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, f1_score, classification_report","b5eeda65":"df = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndf2 = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","c1fe0c43":"df[\"city\"] = df['city'].replace('city_','',regex=True).astype('int')\ndf2[\"city\"] = df2['city'].replace('city_','',regex=True).astype('int')","d1b73723":"def count_percent(data):\n    df_cols = pd.DataFrame({'Count Missing': data.isnull().sum(),\n                        'Percent Missing': data.isnull().sum()*100\/data.shape[0]})\n    return df_cols\n\ndf_cols = count_percent(df)\nprint(df_cols[df_cols['Count Missing']>0],'\\n')","cde9158a":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","1bb18518":"sns.heatmap(df2.isnull(),yticklabels=False,cbar=False,cmap='viridis')","21b89d66":"print(df.shape)\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","59e1b785":"df = df.drop(columns=[\"enrollee_id\"])\ndf2 = df2.drop(columns=[\"enrollee_id\"])","1c8aa096":"df.head()","57a4c10b":"# perfect nothing is missing now\ndf_cols = count_percent(df)\nprint(df_cols[df_cols['Count Missing']>0],'\\n')\n\ndf_cols = count_percent(df2)\nprint(df_cols[df_cols['Count Missing']>0],'\\n')","2c8721c1":"# print(df.isnull().sum(),df2.isnull().sum())","22f0d1f0":"df.dtypes","0ccfc010":"df2.dtypes","78586c92":"train = df\ntest = df2","fa6e3f0f":"from sklearn.preprocessing import LabelEncoder\n\n# I do this manually to explicitly tell the model that a better education & experience serves well as a trustworthy input.\n\n# However, later we wil see the feature importanes report in SHAP and notice interesting results.\nexperience_dict = {'Has relevent experience' : 1,\n             'No relevent experience': 0}\n\neducation_dict = {'Graduate' : 2,\n             'Masters' : 1,\n             'Phd' : 0}\n\nenrollment_dict = {'no_enrollment' : 2,\n             'Full time course' : 1,\n             'Part time course' : 0}\n\ngender_dict = {'Male' : 2,\n             'Female' : 1,\n             'Other' : 0}\n\ndiscipline_dict = {'STEM' : 5,\n             'Humanities' : 4,\n             'Business Degree' : 3,\n             'Other' : 2,\n             'No Major' : 1,\n             'Arts' : 0 }\n\ncompany_dict = {'Pvt Ltd' : 5,\n             'Funded Startup' : 4,\n             'Public Sector' : 3,\n             'Early Stage Startup' : 2,\n             'NGO' : 1,\n             'Other' : 0 }\n\n\n# Train encoding\nle = LabelEncoder()\ntrain['gender'] = train['gender'].map(gender_dict)\ntrain['relevent_experience'] = train['relevent_experience'].map(experience_dict)\ntrain['education_level'] = train['education_level'].map(education_dict)\ntrain['enrolled_university'] = train['enrolled_university'].map(enrollment_dict)\ntrain['major_discipline'] = train['major_discipline'].map(discipline_dict)\ntrain['experience'] = le.fit_transform(train['experience'].astype(str))\ntrain['company_size'] = le.fit_transform(train['company_size'].astype(str))\ntrain['company_type'] = train['company_type'].map(company_dict)\ntrain['last_new_job'] = le.fit_transform(train['last_new_job'].astype(str))\n\n# Test encoding\ntest['gender'] = le.fit_transform(test['gender'].astype(str))\ntest['relevent_experience'] = test['relevent_experience'].map(experience_dict)\ntest['education_level'] = test['education_level'].map(education_dict)\ntest['enrolled_university'] = test['enrolled_university'].map(enrollment_dict)\ntest['major_discipline'] = test['major_discipline'].map(discipline_dict)\ntest['experience'] = le.fit_transform(test['experience'].astype(str))\ntest['company_size'] = le.fit_transform(test['company_size'].astype(str))\ntest['company_type'] = test['company_type'].map(company_dict)\ntest['last_new_job'] = le.fit_transform(test['last_new_job'].astype(str))","491a6e1c":"type(train)","866bd4c6":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\ntrain_col_names = list(train.columns.values.tolist())\ntest_col_names = list(test.columns.values.tolist())\n\nmice_imputer = IterativeImputer(random_state=42, estimator=lr, \n                                max_iter=10, n_nearest_features=2, imputation_order = 'roman')\ntrain = mice_imputer.fit_transform(train)\ntest = mice_imputer.fit_transform(test)\n\ntrain = pd.DataFrame(train)\ntest = pd.DataFrame(test)\n\ntrain.columns = train_col_names\ntest.columns = test_col_names","85b76e08":"train.head()","ba45116b":"X, y = train.loc[:, df.columns != \"target\"], train[\"target\"]\nX_pred = test.loc[:].to_numpy()","6f4386bc":"corr_matrix = train.corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nfig = plt.figure(figsize=[10,8])\nsns.heatmap(corr_matrix,cmap='seismic',linewidth=1.5,linecolor='white',vmax = 1, vmin=-1,mask=mask, annot=True,fmt='0.2f')\nplt.title('Correlation Heatmap', weight='bold',fontsize=20)","7658ae03":"from imblearn.over_sampling import SMOTE,ADASYN, SVMSMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.model_selection import train_test_split\n\nsvm_smote = SMOTE( random_state=101) #,sampling_strategy='minority', k_neighbors=5)\nX_svm_smote, y_svm_smote = svm_smote.fit_resample(X,y)\n\nX_train_svm, X_test_svm, y_train, y_test = train_test_split(X_svm_smote,y_svm_smote, test_size=0.20, random_state=101)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train_svm)\nX_test = sc.transform(X_test_svm)","343039de":"# model accuracy\nresult = []","e4936128":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    A =(((C.T)\/(C.sum(axis=1))).T)\n    B =(C\/C.sum(axis=0))\n    plt.figure(figsize=(20,4))\n\n    sns.set(font_scale=1)\n    labels = [0,1]\n    # representing A in heatmap format\n    cmap1=sns.light_palette(\"orange\")\n    cmap2=sns.light_palette(\"#34c3eb\")\n    cmap3=sns.light_palette(\"purple\")\n    \n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap1, fmt=\".3f\", xticklabels=labels, yticklabels=labels,annot_kws={\"size\":14})\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap2, fmt=\".3f\", xticklabels=labels, yticklabels=labels,annot_kws={\"size\":14})\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap3, fmt=\".3f\", xticklabels=labels, yticklabels=labels,annot_kws={\"size\":14})\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()\n\n\ndef plot_roc_auc(y_test,y_pred):\n    from plot_metric.functions import BinaryClassification\n    # Visualisation with plot_metric\n    bc = BinaryClassification(y_test, y_pred, labels=[\"Class 1\", \"Class 0\"])\n\n    # Figures\n    plt.figure(figsize=(8,7))\n    bc.plot_roc_curve()\n    \n    plt.show()","59ae9da8":"!pip3 install plot_metric","cec4b18f":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier(max_depth=2, random_state= 59)\nrandom_forest_model.fit(X_train, y_train)\ny_pred_random_forest = random_forest_model.predict(X_test)\ncm_random_forest = confusion_matrix(y_pred_random_forest,y_test)\nfscore = f1_score(y_test,y_pred_random_forest)\nacc_random_forest = accuracy_score(y_test, y_pred_random_forest)\nresult.append(acc_random_forest)\n\nprint(\"RESULTS :\")\nprint(\"Random Forest Model Accuracy : \",round(acc_random_forest,2))\nprint(\"Random Forest Model F1-score : \",round(fscore,2))\nprint(\"Classification Report :\",classification_report(y_test,y_pred_random_forest))\nprint('\\n')\nplot_confusion_matrix(y_test, y_pred_random_forest)\nprint('\\n')\nplot_roc_auc(y_test,y_pred_random_forest)","b727bd4a":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)\nmlp.fit(X_train,y_train)\n\npredict_train = mlp.predict(X_train)\ny_pred = mlp.predict(X_test)\n\nfscore = f1_score(y_test,y_pred)\nacc_mlp = accuracy_score(y_pred, y_test)\nresult.append(acc_mlp)\n\nprint(\"RESULTS :\")\nprint(\"ANN Accuracy : \",round(acc_mlp,2))\nprint(\"ANN F1-score : \",round(fscore,2))\nprint(\"Classification Report :\",classification_report(y_test,y_pred))\nprint('\\n')\nplot_confusion_matrix(y_test, y_pred)\nprint('\\n')\nplot_roc_auc(y_test,y_pred)","067f35ec":"knn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nknn = KNeighborsClassifier()\nclf = GridSearchCV(knn, param_grid=knn_param_grid, cv = StratifiedKFold(n_splits = 10), \n                   scoring = \"accuracy\", n_jobs = -1,verbose = 1)\nclf.fit(X_train,y_train)\ny_pred_knn = clf.predict(X_test)\ncm_random_knn = confusion_matrix(y_test,y_pred_knn)\nacc_random_knn = accuracy_score(y_test, y_pred_knn)\nfscore = f1_score(y_test,y_pred_knn)\nresult.append(acc_random_knn)\n\nprint('\\n')\nprint(\"RESULTS :\")\nprint(\"KNN Model Acc : \",round(acc_random_knn,2))\nprint(\"KNN Model F1-score : \",round(fscore,2))\nprint(\"Classification Report :\",classification_report(y_test,y_pred_knn))\nprint('\\n')\nplot_confusion_matrix(y_test, y_pred_knn)","b71df8d7":"import xgboost as xgb\nfrom sklearn.calibration import CalibratedClassifierCV\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.1], #so called `eta` value\n              'max_depth': [5],\n              'min_child_weight': [7],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [1000], #number of trees, change it to 1000 for better results\n              'seed': [1337]}\n\nXGB = xgb.XGBClassifier()\nXGB = GridSearchCV(XGB, parameters, cv = StratifiedKFold(5, shuffle=True), \n                   scoring='roc_auc',\n                   verbose=1, refit=True)\n# XGB = CalibratedClassifierCV(XGB, method=\"sigmoid\")\nXGB.fit(X_train, y_train)\ny_pred_xgb = XGB.predict(X_test)\ncm_xgb = confusion_matrix(y_pred_xgb,y_test)\nfscore = f1_score(y_test,y_pred_xgb)\nacc_xgb = accuracy_score(y_test, y_pred_xgb)\nresult.append(acc_xgb)\n\n\nprint(\"RESULTS : \")\nprint(\"XGBoost Model Accuracy : \",round(acc_xgb,2))\nprint(\"XGBoost Model F1-score : \",round(fscore,2))\nprint(\"Classification Report :\\n\",classification_report(y_test,y_pred_xgb))\nprint('\\n')\nplot_confusion_matrix(y_test, y_pred_xgb)\nprint('\\n')\nplot_roc_auc(y_test,y_pred_xgb)","cc68a638":"from lightgbm import LGBMClassifier\nimport lightgbm as lgb\nclf_lgb = lgb.LGBMClassifier()\nclf_lgb.fit(X_train, y_train)\ny_pred=clf_lgb.predict(X_test)\n\nfscore = f1_score(y_test,y_pred)\nacc_lgb = accuracy_score(y_pred, y_test)\nresult.append(acc_lgb)\n\nprint(\"RESULTS :\")\nprint(\"LGB Model Accuracy : \",round(acc_lgb,2))\nprint(\"LGB Model F1-score : \",round(fscore,2))\nprint(\"Classification Report :\",classification_report(y_test,y_pred))\nprint('\\n')\nplot_confusion_matrix(y_test, y_pred)\nprint('\\n')\nplot_roc_auc(y_test,y_pred)","d3a01406":"df = df.drop(columns=[\"city\"])\ndf2 = df2.drop(columns=[\"city\"])","14b74582":"## Light Gradient Boosting Model","da0ad1fc":"## Looking at missing data","9dbb3473":"## Missing Values Imputation\n\nA more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.","d64584c1":"## ANN","07910086":"## XGBoost","e63fb5cb":"## KNN ","c3669111":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","577a54d9":"## Import the modules","5c62ad39":"## Dataset is imbalanced\n\n[medium link](https:\/\/towardsdatascience.com\/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb)","53425f94":"## Random Forest"}}