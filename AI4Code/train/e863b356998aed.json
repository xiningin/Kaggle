{"cell_type":{"7d7e9275":"code","6bedfdfb":"code","cc0245b5":"code","627093ac":"code","4dd1d923":"code","a2802dc0":"code","91f3a2e3":"code","dd22d2bc":"code","2a2b0227":"code","4b8bebdb":"code","e119225c":"code","b3165fbe":"code","39be2a34":"code","35b7f308":"code","68869734":"code","e82a7126":"code","f34441d6":"code","25e8bea7":"code","cd338e21":"code","605cc3f2":"code","3cd19587":"code","a0a25479":"code","0841d558":"code","bcc9cede":"code","828e3af3":"code","71780a68":"code","cbb5928a":"code","b3b818bc":"code","6b0984c1":"code","2cc426bb":"code","cc490e8a":"code","2c0fef0b":"code","c375e0be":"code","1e9e267b":"code","aa233ac1":"code","c6dfad03":"code","ec2e56c1":"code","551e396d":"code","c3386e22":"markdown","061f54a1":"markdown","8759e621":"markdown","88cb2417":"markdown","86fba6c2":"markdown","dd5fe739":"markdown","d1f3426d":"markdown","542d4e27":"markdown","7f75d654":"markdown"},"source":{"7d7e9275":"# Extracting the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, roc_curve\nfrom xgboost import plot_importance\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6bedfdfb":"df1 = pd.read_csv('\/kaggle\/input\/retailtransactiondata\/Retail_Data_Response.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/retailtransactiondata\/Retail_Data_Transactions.csv',parse_dates=['trans_date'])","cc0245b5":"df1.head()","627093ac":"df2.head()","4dd1d923":"df1.describe(include='all')","a2802dc0":"df2.describe(include='all')","91f3a2e3":"print(df2['trans_date'].min())\nprint(df2['trans_date'].max())","dd22d2bc":"sd = dt.datetime(2015,3,17)\ndf2['recent']= sd - df2['trans_date']\ndf2['recent'].astype('timedelta64[D]')\ndf2['recent']=df2['recent'] \/ np.timedelta64(1, 'D')\ndf2.head()","2a2b0227":"data_rfm = df2.groupby('customer_id').agg({'recent': lambda x:x.min(), # Recency\n                                        'customer_id': lambda x: len(x),               # Frequency\n                                        'tran_amount': lambda x: x.sum()})          # Monetary Value\n\ndata_rfm.rename(columns={'recent': 'recency', \n                         'customer_id': 'frequency', \n                         'tran_amount': 'monetary_value'}, inplace=True)","4b8bebdb":"rfm = data_rfm.reset_index()","e119225c":"rfm.head()","b3165fbe":"rfm.describe(include='all')","39be2a34":"label = df1.groupby('response').agg({'customer_id': lambda x: len(x)})\nlabel.head()","35b7f308":"plt.figure(figsize=(5,5))\nx=range(2)\nplt.bar(x,label['customer_id'])\nplt.xticks(label.index)\nplt.title('Label Distribution')\nplt.xlabel('Convert or Not')\nplt.ylabel('total_user')\nplt.show()","68869734":"dataset = pd.merge(df1,rfm)\ndataset.head()","e82a7126":"# Define the minority data size and indices\nminority_class_len = len(dataset[dataset['response'] == 1])\nminority_index_list = dataset[dataset['response'] == 1].index\nprint(minority_index_list)","f34441d6":"#Define the majority data size and indices\nmajority_class_len = len(dataset[dataset['response'] == 0])\nmajority_index_list = dataset[dataset['response'] == 0].index\nprint(majority_index_list)","25e8bea7":"#Perform random undersampling\nrandom_majority = np.random.choice(majority_index_list,\n                                   minority_class_len,\n                                   replace = False)\nunder_sample_indexlist = np.concatenate([random_majority,minority_index_list])\nunder_sample = dataset.loc[under_sample_indexlist]\nunder_sample.reset_index(drop=True, inplace=True)\nunder_sample.head()","cd338e21":"label_us = under_sample.groupby('response').agg({'customer_id': lambda x: len(x)})\nlabel_us.head()","605cc3f2":"plt.figure(figsize=(5,5))\nx=range(2)\nplt.bar(x,label_us['customer_id'])\nplt.xticks(label_us.index)\nplt.title('Label Distribution')\nplt.xlabel('Convert or Not')\nplt.ylabel('total_user')\nplt.show()","3cd19587":"from sklearn.utils import resample","a0a25479":"# Separate input features and target\ny = dataset.response\nX = dataset.drop('response', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)","0841d558":"# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\nX.head()","bcc9cede":"# separate minority and majority classes\nnot_order = X[X.response==0]\norder = X[X.response==1]\n\n# upsample minority\norder_upsampled = resample(order,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_order), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_order, order_upsampled])\nupsampled.reset_index(drop=True, inplace=True)\nupsampled.head()","828e3af3":"label_os = upsampled.groupby('response').agg({'customer_id': lambda x: len(x)})\nlabel_os.head()","71780a68":"plt.figure(figsize=(5,5))\nx=range(2)\nplt.bar(x,label_os['customer_id'])\nplt.xticks(label_os.index)\nplt.title('Label Distribution')\nplt.xlabel('Convert or Not')\nplt.ylabel('total_user')\nplt.show()","cbb5928a":"x = under_sample.drop(columns=['response','customer_id'])\ny = under_sample['response']\nidentifier = under_sample['customer_id']\n\nfor i in range(0,100):\n    skf = StratifiedKFold(n_splits=5, random_state = i, shuffle = True)\n        \npredicted_y = []\nexpected_y = []\ncustomer_id = []\n\nfor train_index, test_index in skf.split(x, y):\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    id_train, id_test = identifier[train_index], identifier[test_index]\n    \n    xgb_model = xgb.XGBClassifier(objective='binary:logistic').fit(x.loc[train_index], y[train_index])\n    predictions = xgb_model.predict(x.loc[test_index])\n\n    predicted_y.extend(predictions)\n    expected_y.extend(y_test)\n    customer_id.extend(id_test)\n    \nresult = {'id': customer_id,'pred': predicted_y, 'exp': expected_y}    \nreport = classification_report(expected_y, predicted_y)\nprint(report)","b3b818bc":"score = pd.DataFrame(data=result)\nscore.head()","6b0984c1":"cf_matrix = confusion_matrix(expected_y,predicted_y)\nplt.figure(figsize=(10,9))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot = labels, fmt='', cmap='Blues')\nplt.show()","2cc426bb":"x = upsampled.drop(columns=['response','customer_id'])\ny = upsampled['response']\nidentifier = upsampled['customer_id']\n\n\nfor i in range(0,100):\n    skf = StratifiedKFold(n_splits=10, random_state = i, shuffle = True)\n         \npredicted_y = []\nexpected_y = []\ncustomer_id = []\n\nfor train_index, test_index in skf.split(x, y):\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    id_train, id_test = identifier[train_index], identifier[test_index]\n    \n    xgb_model = xgb.XGBClassifier(objective='binary:logistic').fit(x.loc[train_index], y[train_index])\n    predictions = xgb_model.predict(x.loc[test_index])\n\n    predicted_y.extend(predictions)\n    expected_y.extend(y_test)\n    customer_id.extend(id_test)\n    \nresult = {'id': customer_id,'pred': predicted_y, 'exp': expected_y}    \nreport = classification_report(expected_y, predicted_y)\nprint(report)","cc490e8a":"score = pd.DataFrame(data=result)\nscore.head()","2c0fef0b":"cf_matrix = confusion_matrix(expected_y,predicted_y)\nplt.figure(figsize=(10,9))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot = labels, fmt='', cmap='Blues')\nplt.show()","c375e0be":"x = upsampled.drop(columns=['response','customer_id'])\ny = upsampled['response']\nidentifier = upsampled['customer_id']\n\nfor i in range(0,100):\n    skf = StratifiedKFold(n_splits=10, random_state = i, shuffle = True)\n        \npredicted_y = []\nexpected_y = []\ncustomer_id = []\n\nfor train_index, test_index in skf.split(x, y):\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    id_train, id_test = identifier[train_index], identifier[test_index]\n    \n    xgb_model = xgb.XGBClassifier(objective='binary:logistic').fit(x.loc[train_index], y[train_index])\n    predictions = xgb_model.predict_proba(x.loc[test_index])[:,1]\n    \n    predicted_y.extend(predictions)\n    expected_y.extend(y_test)\n    customer_id.extend(id_test)\n    \n    \nresult = {'id': customer_id,'pred': predicted_y, 'exp': expected_y} \nprob_score = pd.DataFrame(data=result)\nprob_score.head()","1e9e267b":"prob_score.to_csv('xgboost_propensity_score.csv',index=False)","aa233ac1":"x = upsampled.drop(columns=['response','customer_id'])\ny = upsampled['response']\n\nlogreg = LogisticRegression(solver='liblinear', penalty='l1', C=0.1, class_weight='balanced')\n\nfor i in range(0,100):\n    skf = StratifiedKFold(n_splits=10, random_state = i, shuffle = True)\n        \npredicted_y = []\nexpected_y = []\n\nfor train_index, test_index in skf.split(x, y):\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    logreg_model = logreg.fit(x.loc[train_index], y[train_index])\n    predictions = logreg_model.predict(x.loc[test_index])\n\n    predicted_y.extend(predictions)\n\n    expected_y.extend(y_test)\n    \nreport = classification_report(expected_y, predicted_y)\nprint(report)","c6dfad03":"cf_matrix = confusion_matrix(expected_y,predicted_y)\nplt.figure(figsize=(10,9))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot = labels, fmt='', cmap='Blues')\nplt.show()","ec2e56c1":"x = under_sample.drop(columns=['response','customer_id'])\ny = under_sample['response']\n\nlogreg = LogisticRegression(solver='liblinear', penalty='l1', C=0.1, class_weight='balanced')\n\nfor i in range(0,100):\n    skf = StratifiedKFold(n_splits=10, random_state = i, shuffle = True)\n        \npredicted_y = []\nexpected_y = []\n\nfor train_index, test_index in skf.split(x, y):\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    logreg_model = logreg.fit(x.loc[train_index], y[train_index])\n    predictions = logreg_model.predict(x.loc[test_index])\n\n    predicted_y.extend(predictions)\n\n    expected_y.extend(y_test)\n    \nreport = classification_report(expected_y, predicted_y)\nprint(report)","551e396d":"cf_matrix = confusion_matrix(expected_y,predicted_y)\nplt.figure(figsize=(10,9))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot = labels, fmt='', cmap='Blues')\nplt.show()","c3386e22":"### Over Sample","061f54a1":"Based on information above that the transaction data were taken from 16 May 2011 - 16 March 2015, we will assume that campaign data in df1 were campaign conduct in 17 March 2014. So, the propensity Modelling will see the RFM value of user for last 4 years (we assume all transaction history of user since their first transaction)","8759e621":"### Under Sample","88cb2417":"## XGBoost Modelling","86fba6c2":"# **Propensity Analysis**\n\nRFM been a famous method to build a customer segmentation. We can easily generate segment based on user Recency, Frequency, and Monetary. This segment information very usefull for customer targeting, customer behavious analysis and understanding our customer tier. But sometimes, we need the exact number about our customer quality or we need to know the our customer score based on a view metrics. So basically Propensity Modelling is method to solve that problem. Propensity Modelling in a nutshell is a method to generate probability score on how customer do a particular action. In this project I will combine the Propensity Analysis using RFM metrics to show the probability score of how customer order activity after they get campaign\n\nThe dataset contain user RFM before they get campaign and we will predict using label in 'Retail_Data_Response.csv'","dd5fe739":"### **UnderSample dataset**","d1f3426d":"## Logistic Regression Model","542d4e27":"### Oversample dataset","7f75d654":"We can see that we have an imbalanced data here. In order to optimize the model, we can perform a view method to handle this imbalanced data. In this project I will try to use undersample and oversample method and see which method perform best."}}