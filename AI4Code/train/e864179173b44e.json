{"cell_type":{"56bf8130":"code","790e735a":"code","4cc1818a":"code","e5dc8ffe":"code","c4275254":"code","f41f3a4c":"code","e8a80530":"code","35edd474":"code","b9ece347":"code","119409c3":"code","250ac2ff":"code","6675a063":"code","4f4096c1":"code","54c1ba24":"code","f913ebde":"code","572f2b6b":"code","8f44f216":"code","f0f37d47":"code","c8efaa99":"code","49e55fd7":"code","25fc88e3":"markdown","e17f75ab":"markdown","8bac5aa1":"markdown","72db17b4":"markdown","88883f81":"markdown","912f8256":"markdown","2007ab60":"markdown","23b2eb3a":"markdown","fcc90228":"markdown","59ccc2b0":"markdown","4231ce33":"markdown","79e6640f":"markdown","bae92433":"markdown","a1457743":"markdown"},"source":{"56bf8130":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport altair as alt\nimport seaborn as sns\nfrom sklearn import preprocessing, impute, tree, model_selection, metrics\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","790e735a":"df = pd.read_csv('..\/input\/wine-reviews\/winemag-data-130k-v2.csv')\ndf.info()","4cc1818a":"df.isnull().any()","e5dc8ffe":"imputers = []\nfor column in df.columns:\n    if df[column].isnull().any() == True:\n        df['was_missing_'+column] = df[column].isna()\n        imputer = impute.SimpleImputer(strategy='most_frequent' if df[column].dtype == 'O' else 'median')\n        df[column+'imputed_value'] = imputer.fit_transform(df[column].values.reshape(-1,1))\n        imputers.append({column+'_imputer': imputer})","c4275254":"df.head()","f41f3a4c":"alt.Chart(df.sample(n=500, random_state=1)).mark_bar().encode(\n    x='count(country)',\n    y=alt.Y('country', sort='-x')\n)","e8a80530":"us_data = df.query(\"country == 'US'\")[['points','variety','price','taster_name']]\nagg_op1 = us_data.groupby('taster_name').agg({\n    'points': 'mean',\n    'price': 'mean',\n    'variety': 'count'\n})\nagg_op1.columns = ['mean_points','mean_price','frequency']\nagg_op1.sort_values(by='frequency', ascending=False)","35edd474":"agg_op2 = us_data.groupby(['taster_name','variety']).agg({\n    'points':'mean',\n    'price':'mean'\n})\nagg_op2.columns = ['mean_points','mean_price']\nidmaxmin = agg_op2.groupby(\"taster_name\").agg({\"mean_points\": ['idxmax', 'idxmin']})\n\ntastermaxmin = pd.DataFrame(columns=['taster_name'], data=idmaxmin.index.values)\ntastermaxmin['most_rated'] = idmaxmin.iloc[:,0].apply(lambda x: x[1]).values\ntastermaxmin['least_rated'] = idmaxmin.iloc[:,1].apply(lambda x: x[1]).values\ntastermaxmin","b9ece347":"corr = alt.Chart(\n    agg_op2.reset_index()\n).mark_point(opacity=0.2, color='red').encode(\n    alt.X('mean_points', type='quantitative'),\n    alt.Y('mean_price', type='quantitative'),\n)\nprefers = alt.Chart(\n    agg_op2.reset_index()\n).mark_point().encode(\n    alt.Y('mean_points', bin=True, type='quantitative'),\n    alt.X('mean_price', bin=True, type='quantitative'),\n    size='count()'\n)\n\ncorr.display()\nprefers.display()","119409c3":"points_dist = alt.Chart(agg_op2.reset_index()).mark_bar().encode(\n    alt.X('mean_points:Q',bin=True),\n    y='count()'\n)\nprice_dist = alt.Chart(agg_op2.reset_index()).mark_bar().encode(\n    alt.X('mean_price:Q',bin=True),\n    y='count()'\n)\npoints_dist.display()\nprice_dist.display()","250ac2ff":"# Looks like price was imputed, but rate isn't. We'll take priceimputed_value as price column and points to visualize the correlation\ndf.columns[df.columns.str.contains('was_missing_')]","6675a063":"# altair can't show more than 5,000 records, but pandas can help us to get a random sample of 5,000 to show them.\ncorr = alt.Chart(\n    df[['priceimputed_value','points']].sample(n=5000, random_state=1)\n).mark_point().encode(\n    alt.X('points', type='quantitative'),\n    alt.Y('priceimputed_value', type='quantitative')\n)\nprint(df[['priceimputed_value','points']].corr())\ncorr.display()","4f4096c1":"# These visualization is an extra, just to validate the assumption that imputing can alter the PDF and, in consequence, the correlation. \n# Nevertheless, it's not significant.\ncorr = alt.Chart(\n    df.query(\"was_missing_price == False\")[['price','points']].sample(n=5000, random_state=1)\n).mark_point().encode(\n    alt.X('points', type='quantitative'),\n    alt.Y('price', type='quantitative')\n)\nprint(df[['price','points']].corr())\ncorr.display()","54c1ba24":"todrop = ['country', 'designation', 'price',\n          'province', 'region_1', 'region_2',\n          'taster_name', 'taster_twitter_handle',\n          'variety','Unnamed: 0', 'points']\nX = df.drop(todrop, axis=1).copy()\ny = df.points.values\nencoders = []\nfor column in X.columns:\n    if X[column].dtype == 'O':\n        encoder = preprocessing.LabelEncoder()\n        X[column] = encoder.fit_transform(X[column])\n        encoders.append((column+'_encoder',encoder))","f913ebde":"regressor = tree.DecisionTreeRegressor(max_depth=24, random_state=1)\ntrain_sizes, train_scores, test_scores, fit_times, _ = \\\n    model_selection.learning_curve(regressor, X, y, cv=10, shuffle=True, random_state=1, return_times=True)","572f2b6b":"train_score = pd.DataFrame()\ntest_score = pd.DataFrame()\nscalability = pd.DataFrame()\nperformance = pd.DataFrame()\n\ntrain_score['scores'] = train_scores.reshape(1,-1)[0]\ntrain_score['size'] = 0\n\ntest_score['scores'] = test_scores.reshape(1,-1)[0]\ntest_score['size'] = 0\n\nscalability['scores'] = fit_times.reshape(1,-1)[0]\nscalability['size'] = 0\n\nperformance['fit'] = fit_times.reshape(1,-1)[0]\nperformance['test'] = test_scores.reshape(1,-1)[0]\n\nfor i in range(len(train_sizes)):\n    train_score.iloc[10*i:10*(i+1), 1] = train_sizes[i]\n    test_score.iloc[10*i:10*(i+1), 1] = train_sizes[i]\n    scalability.iloc[10*i:10*(i+1), 1] = train_sizes[i]\n    \ntrain_score_line = alt.Chart(train_score).mark_line(color='red').encode(\n    x=alt.X('size:Q', scale=alt.Scale(zero=False)),\n    y=alt.Y('mean(scores)', scale=alt.Scale(zero=False))\n)\ntrain_score_band = alt.Chart(train_score).mark_errorband(extent='ci').encode(\n    x=alt.X('size:Q', scale=alt.Scale(zero=False)),\n    y=alt.Y('scores', scale=alt.Scale(zero=False))\n)\n\ntest_score_line = alt.Chart(test_score).mark_line(color='green').encode(\n    x=alt.X('size:Q', scale=alt.Scale(zero=False)),\n    y=alt.Y('mean(scores)', scale=alt.Scale(zero=False))\n)\ntest_score_band = alt.Chart(test_score).mark_errorband(extent='ci').encode(\n    x=alt.X('size:Q', scale=alt.Scale(zero=False)),\n    y=alt.Y('scores', scale=alt.Scale(zero=False))    \n)\n\nscalability_line = alt.Chart(scalability).mark_line().encode(\n    x=alt.X('size:Q', scale=alt.Scale(zero=False)),\n    y=alt.Y('mean(scores)', scale=alt.Scale(zero=False))\n)\nscalability_band = alt.Chart(scalability).mark_errorband(extent='ci').encode(\n    x=alt.X('size:Q', scale=alt.Scale(zero=False)),\n    y=alt.Y('scores', scale=alt.Scale(zero=False))\n)\n\nperformance_line = alt.Chart(performance).mark_line().encode(\n    x=alt.X('mean(fit)',scale=alt.Scale(zero=False)),\n    y=alt.Y('mean(test)',scale=alt.Scale(zero=False))\n)\nperformance_band = alt.Chart(performance).mark_line().encode(\n    x=alt.X('fit',scale=alt.Scale(zero=False)),\n    y=alt.Y('test',scale=alt.Scale(zero=False))\n)\n\n(train_score_line + train_score_band).display()\n(test_score_line + test_score_band).display()\n(scalability_line + scalability_band).display()\n(performance_line + performance_band).display()","8f44f216":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.78, random_state=1, shuffle=True)\nregressor.fit(X_train, y_train)","f0f37d47":"features_importances = pd.DataFrame(columns=['features','importance'])\nfeatures_importances.features = X.columns.values\nfeatures_importances.importance = regressor.feature_importances_\nfeatures_importances.sort_values(by='importance', ascending=False, inplace=True)\nsns.barplot(data=features_importances, y='features', x='importance')","c8efaa99":"y_pred = regressor.predict(X_test)\nmetrics.mean_squared_error(y_test, y_pred, squared=True)","49e55fd7":"print(tree.export_text(regressor, max_depth=5, feature_names=features_importances.features.values.tolist()))","25fc88e3":"# Dealing with missing values #\n\nBut we're not done yet, finding them is just the first part, the second one is imputing them (filling the missing values with something). To impute them we have lot of alternatives like median, mean, mode, dropping them, or dealing with them using a predictor model.\n\nWe won't get into any troubles fitting a predictive model to impute those missing values, instead we'll use the `SimpleImputer` provided by sklearn. For those categorical values, the strategy is `most_frequent` (a.k.a mode), and for those quantitative values, the strategy is `mean`.\n\nNow, I'm inspiring the following pice of code on the course [Intermediate Machine Learning - Missing values](https:\/\/www.kaggle.com\/alexisbcook\/missing-values), creating a new column to identify those variables with missing values and imputing them into a new column (just in case we need the real ones)","e17f75ab":"# Data Exploration #\n\nIn this section, we'll be looking at the data for patterns that explain our predictive model, that is, correlations, probability distribution functions, and other characteristics we consider relevant for the analysis.\n\n## Most reviews come from... #\n\nIt's time to check where the reviews come from, as you may notice, I'm sampling the data to 500 records, not just altair, but most of the visualization libraries have a hard time when we ask them to deal with large data volumnes, so be kind with the libraries and your PC and limit your visualizations to some reasonable size.\n\n> Remember: sampling randomly your data could cause some data missing, but you'll get the essence of it. You're understanding it, not predicting it.","8bac5aa1":"When fitted, we can get the importance of every feature. Translating this into normal words, the most important characteristic to take into account when a tester gives reviwes is the price. Followed by a description (meaning we have to identify those with the best reviwes and try to do a wine that satisfy those descriptions), a good title (something atractive) and the vineyard.\n\nAnd the final MSE score is 7, not to good but not to bad.\n\n> This score can be improved by applying techniques to avoid overfitting and using a more sophisticated algorithm.","72db17b4":"If we take as reference the previous observation, saying that:\n\n> Virgine Boone is the taster with the most rated wines,\n\n> Matt Kettmann is the taster with the highest mean rate, and\n\n> Susan Kostrzewa is the taster with the lowest mean rate\n\nWe can observe that:\n\n> Vigine Boone prefer Moscato Giallo and dislike Abouriou,\n\n> Matt Kettmann prefer Grenache Blend and dislike Alvarinho, and\n\n> Susan Kostrzewa prefer Vidal Blanc and dislike Nebbiolo\n\nMy hypothesis was expecting to see somewhat a relation between the varieties, however it doesn't. I think a further analysis is required to make a conclusion.\n\nMoving on the analysis, we'll check if exists a correlation between rate and price as well as see the prefered wine price between the tasters.","88883f81":"The relation between them looks exponential, but not determinant. The price is not the factor that, by its own, can determine the rate of a wine. \n\nThis is confirmed by the graphic on its own, that looks like an exponential, but dispersed at the base; and by the correlation of 0.39. If correlation were bigger, then we would confirm a strong relationship, but it's not the case.\n\nWe need to go deeper, aiming to get some extra information that helps our final predictive model.\n\n> This behaviour could be made by the price imputing. Confirm these new assumption, we have to remove the missing records and plot again. Although the imputing is not supposed to modify the PDF, there can be a possibility","912f8256":"# Last Step #\n\nThe last step for this anlyasis is showing the rules the tasters follow to give his reviews. All values are encoded so when plotting the tree the rules will be based on numbers.\n\nI'm limiting the number of node to be shown to 5, because the trees tend to be a lot bigger and takes to much time to be printed.","2007ab60":"# Introduction #\n\nThis notebook is inspired on the dataset `Wine Reviwes` aiming to provide a process to predict the rating of a wine. In this notebook you'll see data exploration, data visualization, model fitting and predicting.\n\nThise notebook is mostly based on `sklearn` and `pandas`. Some visualizations are created with `altair` and `seaborn`.\n\n# Reading dataset #\n\nFirst thing's first, reading the data set (I mean, one won't be able to do anythong without reading it :P), but we continue getting dataset's information like its data types and null columns.\n\nAs you can see, `df.info` shows data types and the number of records in each row, but to know the columns containing null values, we use `df.isnull().any()`. How it works? Easy, the first part is the data frame (I named it `df` just to simplify), then the `isnull()` instruction validates into every column is there a null (the output is a matrix full of True or False), and the `any()` simplifies the matrix generated for the `isnull()` to says yes or no.","23b2eb3a":"As expected, a correlation between the price and rate is clear, some exponential relation appear, observing the highest the price the highest the rate. Considering, the most prefered wines are the cheper ones, at least 200 reviews came from those between 20 and 40 dollars.\n\nTo confirm the correlation between price and rate, we'll be using the full dataset, making a plot and a correlation. But before it, let's see if we have a probability distribution function (pdf) clear.","fcc90228":"As we can see, Virgine Boone is the taster with the most rated wines, giving a mean rating of 89 points over 100, and the wines' mean price is 46. In the other hand, the reviewer with the highest mean rating is Matt Kettmann with 90 points, meanwhile the lowest mean rating is given by Susan Kostrzewa.\n\nHowever, this is not enough information about tasters, let's see what are the top rated wine variety by organizing the data from the highest rate to the lowest rate and selecting the highest and the lowest.","59ccc2b0":"## Analyzing the most reviewed country ##\n\nWe just discover that the US has the most number of reviwes, so we'll be analyzing them getting information about the tasters suchlike likes and dislikes, mean rating, and mean wine prices.\n\n> We just analyze the US reviews because has a great part of the population, this sample will give us an approach of the population information.\n\n### About tasters ###\n\nGrouping the data using the `taster_name` provides a visibility on its perfomance reviewing wines. \n\n- `mean_points`: Shows the mean rating points the taster provides to a wine\n- `mean_price`: Show the mean price of the tasted wines\n- `frequency`: The number of wines every taster has been reviewing","4231ce33":"## Going deeper ##\n\nAfter looking at the testers likes and dislikes, it's time to go a little bit further and try to find rules that tasters follow to give their ratings. There are not better algorithm for this task than `Decision Trees` (this can or can't be our final predictor, in fact it will yelds the rules followed by the tasters, we can opt to use other algorithms for the prediction task).\n\nWe'll use the base algorihtm in sklearn `DecisionTreeRegressor` to see feature importance, rules, and final prediction.\n\n### Fittin Tree ###\n\nFitting a tree can be as complex as we want. For this example we'll explore the `learning_curve` alternative, by measuring its performance against the size of the training and testing data sets. There is a relative new functionality on `sklearn` for trees that is [cost_complexity_pruning_path](https:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py) this function will prune our tree, looking for the simpliest path to make a prediction. However, the time it takes to compleate the pruning depends on the number of `ccp_alphas` the function `cost_complexity_pruning_path` returns.\n\nFirst, let remove useless columns (those with missing values) and encode the categorical columns into numerical. I'm also storing the encoder as a tuple into a list if they usage will be required later on.\n\n>I'm not using the one-hot-encoder because I fell this encoding technique works better on Neural Networks","79e6640f":"The following graphics show the accuarcy of combining different training and testing data set sizes. While the training size increase the accuarcy decrease, although the accuarcy of the testing set increase while the size increase.\n\nIt is supposed to be a common point where both scores meet each other, but I'm assuming it's increasing the testing size and decreasing the training size to a point it won't be usefull. Nevertheless, the size of the dataset affect this results.","bae92433":"While the `mean_points` has a clear normal distribution, the `mean_price` has a positive skewed distribution. Somewhat easy ones.\n\nBut it isn't enough, time to check a correlation between price and rate. However, we don't have to forget that some columns had missing values, to confirm if any of our columns had been imputed, filtering `was_missing` columns is required.","a1457743":"Once interpretated the graphics above, then we can split our data more confident. The `learning_curve` has a parameter for the traning size, variyng depending on the one needs\n\n> train_sizes=array([0.1, 0.33, 0.55, 0.78, 1. ])\n\nThe first plot yells the model's accuarcy depending on the training size; the second plot shoes the model's accuarcy for unseen data; the third plot refers to model's scalability; and the fourth one the performance. At this points, we're interested just on the first and seconds plots, if we calculate the 78% of the total number of records in the dataset, we get that nearly 100,000 records. Leaving of testing data to the remaining 12% or almot 15,600 records.\n\n> Note: Seems that the model is overfitting while the number of records increase during the training, meaning we'd need either a more robust algorithm or a prune technique to improve the score.\n\nTaking action, we'll use the `train_test_split` function on `model_selection` to split the data set into training and testing, with the training size of 78% of the total number of records, shuffled to avoid any overfitting because of the data order."}}