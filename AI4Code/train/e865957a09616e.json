{"cell_type":{"c3345b21":"code","cfa178a8":"code","4dcb9536":"code","d038d15d":"code","ae907644":"code","bb3a45ea":"code","6ca697dc":"code","cff73a6f":"code","b665a48c":"code","6c8b7b57":"code","137a0461":"code","61a65d64":"code","9dca7b6b":"code","e4f12779":"code","2404eb06":"code","9123972d":"code","2c4c05d7":"code","aec3382a":"code","1b8e0aa4":"code","ea2879d4":"code","304df6f2":"markdown"},"source":{"c3345b21":"import os\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.datasets import mnist\n\nprint(os.getcwd())\nprint(\"Modules imported \\n\")\nprint(\"Files in current directory:\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","cfa178a8":"# LOAD DATA from Kaggle\n\ntrainRaw = pd.read_csv('..\/input\/train.csv')\ntestRaw = pd.read_csv('..\/input\/test.csv')","4dcb9536":"train = trainRaw.copy()\ntest_imagesKaggle = testRaw.copy()\ntrain_labelsKaggle = trainRaw['label']\n\nprint(\"train with Labels  \", train.shape)\nprint(\"train_labelsKaggle \", train_labelsKaggle.shape)\nprint(\"_\"*50)\ntrain.drop(['label'],axis=1, inplace=True)\ntrain_imagesKaggle = train\nprint(\"train_imagesKaggle without Labels \", train_imagesKaggle.shape)\nprint(\"_\"*50)\nprint(\"test_imagesKaggle  \", test_imagesKaggle.shape)","d038d15d":"# RESHAPE to 28 X 28 (Height, Width) which Kaggle has flattened in their file\n\ntrain4Display = np.array(train_imagesKaggle).reshape(42000,28,28)\ntest4Display = np.array(test_imagesKaggle).reshape(28000,28,28)\n\nz = 4056\n\nprint(\"train image\")\nprint(train_labelsKaggle[z])\ndigit = train4Display[z]\nplt.imshow(digit, cmap=plt.cm.binary)\nplt.show()\n\nprint(\"test image\")\ndigit = test4Display[z]\nplt.imshow(digit, cmap=plt.cm.binary)\nplt.show()\n","ae907644":"# NORMALIZE \/ SCALE and Prep for CNN in terms of number dimensions expected\n\ntrain_imagesKaggle = train4Display.reshape(42000,28,28,1)\ntest_imagesKaggle = test4Display.reshape(28000,28,28,1)\n\ntrain_imagesKaggle = train_imagesKaggle.astype('float32') \/ 255\ntest_imagesKaggle = test_imagesKaggle.astype('float32') \/ 255\nprint(\"train_imagesKaggle \",train_imagesKaggle.shape)\nprint(\"test_imagesKaggle \", test_imagesKaggle.shape)\nprint(\"_\"*50)\n\n# ONE HOT ENCODER for the labels\ntrain_labelsKaggle = to_categorical(train_labelsKaggle)\nprint(\"train_labelsKaggle \",train_labelsKaggle.shape)","bb3a45ea":"# Load Data from Keras MNIST\n\n(train_imagesRaw, train_labelsRaw), (test_imagesRaw, test_labelsRaw) = mnist.load_data()","6ca697dc":"# Normalize \/ Scale and One Hot encoder for the Keras dataset & Reshape for CNN\n\ntrain_imagesKeras = train_imagesRaw.copy()\ntrain_labelsKeras = train_labelsRaw.copy()\ntest_imagesKeras = test_imagesRaw.copy()\ntest_labelsKeras = test_labelsRaw.copy()\n\ntrain_imagesKeras = train_imagesKeras.reshape(60000,28,28,1)\ntest_imagesKeras = test_imagesKeras.reshape(10000,28,28,1)\n\nprint(\"train_imagesKeras \",train_imagesKeras.shape)\nprint(\"train_labelsKeras \",train_labelsKeras.shape)\nprint(\"test_imagesKeras \", test_imagesKeras.shape)\nprint(\"test_labelsKeras \", test_labelsKeras.shape)\n\n# NORMALIZE 0-255 to 0-1\ntrain_imagesKeras = train_imagesKeras.astype('float32') \/ 255\ntest_imagesKeras = test_imagesKeras.astype('float32') \/ 255\nprint(\"_\"*50)\n\n# ONE HOT ENCODER for the labels\ntrain_labelsKeras = to_categorical(train_labelsKeras)\ntest_labelsKeras = to_categorical(test_labelsKeras)\nprint(\"train_labelsKeras \",train_labelsKeras.shape)\nprint(\"test_labelsKeras \", test_labelsKeras.shape)","cff73a6f":"# CONCATENATE the training sets of Kaggle and Keras into final TRAIN and leave the test for CV\n\ntrain_images = np.concatenate((train_imagesKeras,train_imagesKaggle), axis=0)\nprint(\"new Concatenated train_images \", train_images.shape)\nprint(\"_\"*50)\n\ntrain_labels = np.concatenate((train_labelsKeras,train_labelsKaggle), axis=0)\nprint(\"new Concatenated train_labels \", train_labels.shape)","b665a48c":"# Initial model\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, kernel_size=(4, 4), strides=(2,2), input_shape=(28, 28, 1)))\nmodel.add(PReLU())\nmodel.add(layers.Dropout(0.5))\nmodel.add(BatchNormalization())\n#model.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (4, 4), strides=(2,2)))\nmodel.add(PReLU())\nmodel.add(layers.Dropout(0.5))\nmodel.add(BatchNormalization())\n#model.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), strides=(1,1)))\nmodel.add(PReLU())\nmodel.add(layers.Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512))\nmodel.add(PReLU())\nmodel.add(layers.Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dense(512))\nmodel.add(PReLU())\nmodel.add(layers.Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dense(256))\nmodel.add(PReLU())\nmodel.add(layers.Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dense(128))\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(layers.Dense(10, activation='softmax'))\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nprint(model.summary())","6c8b7b57":"# Initial fIT & Evaluate initial model\n\nnum_epochs = 80\nBatchSize = 2048\n\nmodel.fit(train_images, train_labels, epochs=num_epochs, batch_size=BatchSize, validation_data=(test_imagesKeras, test_labelsKeras))\ntest_loss, test_acc = model.evaluate(test_imagesKeras, test_labelsKeras)\nprint(\"_\"*80)\nprint(\"Accuracy on test \", test_acc)","137a0461":"# NN MODEL\n\ndef build_model():    \n    model = models.Sequential()\n    model.add(layers.Conv2D(32, kernel_size=(4, 4), strides=(2,2), input_shape=(28, 28, 1)))\n    model.add(PReLU())\n    model.add(layers.Dropout(0.5))\n    model.add(BatchNormalization())\n    #model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (4, 4), strides=(2,2)))\n    model.add(PReLU())\n    model.add(layers.Dropout(0.5))\n    model.add(BatchNormalization())\n    #model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), strides=(1,1)))\n    model.add(PReLU())\n    model.add(layers.Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(layers.Flatten())\n    model.add(layers.Dense(512))\n    model.add(PReLU())\n    model.add(layers.Dropout(0.25))\n    model.add(BatchNormalization())\n    model.add(layers.Dense(512))\n    model.add(PReLU())\n    model.add(layers.Dropout(0.25))\n    model.add(BatchNormalization())\n    model.add(layers.Dense(256))\n    model.add(PReLU())\n    model.add(layers.Dropout(0.25))\n    model.add(BatchNormalization())\n    model.add(layers.Dense(128))\n    model.add(PReLU())\n    model.add(BatchNormalization())\n    model.add(layers.Dense(10, activation='softmax'))\n    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model","61a65d64":"# Check some test vs pred\n#TestNum = 10\n#for t in range(100, 100+TestNum):\n#    print(predictions[t])\n#    digit = test_imagesRaw[t]\n#    plt.imshow(digit, cmap=plt.cm.binary)\n#    plt.show()","9dca7b6b":"# CHECK ALL the ERRORS\n#TestNum = test_labels.shape[0]\n#ErrCount = 0\n#for t in range(TestNum):\n#        if test_labelsRaw[t] != predictions[t]:\n#            ErrCount = ErrCount +1\n#            #print(\"True \", test_labelsRaw[t], \"Predicted \",predictions[t])\n#            #digit = test_imagesRaw[t]\n#            #plt.imshow(digit, cmap=plt.cm.binary)\n#            #plt.show()\n\n#print(\"Errors \", ErrCount, \" out of \", TestNum, \" = \", 100 * ErrCount\/TestNum)","e4f12779":"# CROSS VALIDATION k-fold\ntrain_data = train_images\ntrain_targets = train_labels\nk = 4\nnum_val_samples = len(train_data) \/\/ k\nall_mae_histories = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n    [train_data[:i * num_val_samples],\n    train_data[(i + 1) * num_val_samples:]],\n    axis=0)\n    partial_train_targets = np.concatenate(\n    [train_targets[:i * num_val_samples],\n    train_targets[(i + 1) * num_val_samples:]],\n    axis=0)\n    \n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets,\n    validation_data=(val_data, val_targets),\n    epochs=num_epochs, batch_size=BatchSize, verbose=0)\n    \n    mae_history = history.history['acc']\n    all_mae_histories.append(mae_history)\n    \nprint(\"Done CV k-fold\")","2404eb06":"# LOSS Learning curves\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, (len(history.history['acc']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9123972d":"# ACCURACY Learning Curves\n\nhistory_dict = history.history\nloss_values = history_dict['acc']\nval_loss_values = history_dict['val_acc']\nepochs = range(1, (len(history.history['acc']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training Acc')\nplt.plot(epochs, val_loss_values, 'b', label='Validation Acc')\nplt.title('Training and validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","2c4c05d7":"# CONCATENATE the train with test for FINAL FIT\n\ntrain_imagesFin = np.concatenate((train_images,test_imagesKeras), axis=0)\nprint(\"train_imagesFin \", train_imagesFin.shape)\nprint(\"_\"*50)\n\ntrain_labelsFin = np.concatenate((train_labels,test_labelsKeras), axis=0)\nprint(\"train_labelsFin \", train_labelsFin.shape)","aec3382a":"# FINAL FIT according to the above charts\n\nmodel = build_model()\nmodel.fit(train_imagesFin, train_labelsFin, epochs=num_epochs, batch_size=BatchSize)","1b8e0aa4":"# PREDICT & ARGMAX to get the digit from the probability of softmax layer\n\nRawPred = model.predict(test_imagesKaggle)\npred = []\nnumTest = RawPred.shape[0]\nfor i in range(numTest):\n    pred.append(np.argmax(RawPred[i])) \npredictions = np.array(pred)  ","ea2879d4":"# SUBMISSION\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\n#print(sample_submission.shape)\nresult=pd.DataFrame({'ImageId':sample_submission.ImageId, 'Label':predictions})\nresult.to_csv(\"submission.csv\",index=False)\nprint(result)","304df6f2":"* MNIST with Convoluted NN Keras\n* Train set is made of Kaggle 42k + Keras 60k = 102k\n* CV on train and then test on 10k from Keras\n* Final model is trained on train+test = 112k\n* CNN with adam ( vs rmsprop) and dropout against overfitting\n\n* It's a quick intro to the capabilities of CNN and Keras"}}