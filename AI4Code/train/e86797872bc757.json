{"cell_type":{"abd4a87f":"code","032292ba":"code","b6264a32":"code","1d751608":"code","4926c295":"code","a21e92c8":"code","62de3476":"code","cb8ac01b":"code","cfe819a3":"code","54df5972":"code","2a1e0f0a":"code","dff46a20":"code","a2588461":"code","7204f437":"code","92b7ff0e":"markdown","b4a40f30":"markdown","8ef2d9f6":"markdown","c9c504d4":"markdown","f1df604f":"markdown","24199df3":"markdown","51b6dcad":"markdown","08b25b5e":"markdown","f4c1e7af":"markdown","dbf8740e":"markdown","51d56bb0":"markdown","0c1de211":"markdown"},"source":{"abd4a87f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current sessionj","032292ba":"train = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat-ii\/train.csv\")\ntrain.head()\n                    ","b6264a32":"test = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat-ii\/test.csv\")\ntest.head()","1d751608":"train.info()\n","4926c295":"train_target = train.target\ntest_id = test.id\ntrain.drop([\"target\",\"id\"],1,inplace=True)\ntest.drop(\"id\",1,inplace = True)\n","a21e92c8":"# get list of numeric variables\nnum_vars = [] \nfor col in train:\n    if train[col].dtypes != 'O':\n        num_vars.append(col)\nnum_vars ","62de3476":"# converting numeric variables to categorical variables\nfor cols in num_vars:\n    train[cols] = train[cols].astype('category')\n    test[cols] = test[cols].astype('category')\n","cb8ac01b":"# verify the dataset \ntrain.info()","cfe819a3":"# number of unique variables for each of the column present in the dataset\nfor col in train.columns:\n    print(col,\":\\n\",train[col].nunique(),\"\\n\")\n    ","54df5972":"!pip install deeptables\n","2a1e0f0a":"from deeptables.models.deeptable import DeepTable, ModelConfig\nfrom tensorflow.keras.utils import plot_model\n","dff46a20":"n_folds=5\nepochs=1\nbatch_size=128","a2588461":"%%time\n\nconf = ModelConfig(\n    dnn_params={\n        'hidden_units':((300, 0.3, True),(300, 0.3, True),), #hidden_units\n        'dnn_activation':'relu',\n    },\n    fixed_embedding_dim=True,\n    embeddings_output_dim=20,\n    nets =['linear','cin_nets','dnn_nets'],\n    stacking_op = 'add',\n    output_use_bias = False,\n    cin_params={\n       'cross_layer_size': (200, 200),\n       'activation': 'relu',\n       'use_residual': False,\n       'use_bias': True,\n       'direct': True, \n       'reduce_D': False,\n    },\n)\n\ndt = DeepTable(config = conf)\noof_proba, eval_proba, test_prob = dt.fit_cross_validation(\n    train, train_target, X_eval=None, X_test=test, \n    num_folds=n_folds, stratified=False, iterators=None, \n    batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[], n_jobs=1)","7204f437":"# lets prepare for the prediction submission \nsub = pd.DataFrame() \nsub['Id'] = test_id \nsub['target'] = test_prob\nsub.to_csv('submission_dt.csv',index=False)","92b7ff0e":"We have successfully converted all the viables to categorical type.","b4a40f30":"Lets convert all numeric variables to categorical variables","8ef2d9f6":"DT follow these steps to build a neural network:\n\n1. Category features -> Embedding Layer.\n\n2. Continuous feature -> Dense Layer or to Embedding Layer after discretization\/categorization.\n\n3. Embedding\/Dense layers -> Feature Interactions\/Extractions nets.\n\n4. Stacking(add\/concat) outputs of nets as the output of the model.","c9c504d4":"Lets check the number of unique values for each variable ","f1df604f":"We will convert all the non-categorical variables to categorical variables, lets first get a list of numeric variables","24199df3":"Lets cross verify whether we have successfully converted the numeric fields to categorical fields or not","51b6dcad":"# ModelConfig\n\nModelConfig is the most important parameter in DT. It is used to set how to clean and preprocess the data automatically, and how to assemble various network components to building a neural nets for prediction tasks, as well as the setting of hyper-parameters of nets, etc. If you do not change any settings in ModelConfig, DT will work in most cases as well. However, you can get a better performance by tuning the parameters in ModelConfig.","08b25b5e":"# Model Based on DeepTables\n\nDeepTables(DT) is a easy-to-use toolkit that enables deep learning to unleash great power on tabular data.\n\nMLP (also known as Fully-connected neural networks) have been shown inefficient in learning distribution representation. The \u201cadd\u201d operations of the perceptron layer have been proven poor performance to exploring multiplicative feature interactions. In most cases, manual feature engineering is necessary and this work requires extensive domain knowledge and very cumbersome.","f4c1e7af":"# Kindly upvote if you liked my kernel!","dbf8740e":"Using the above model, I got Private Score of 0.78676 and Public Score of 0.78527.\nI used epoch = 1 and n_folds = 5 just to make this submission possible quickly.\n\nModel will produce much better results if you use n_folds = 50 and epoch = 100.\n\n","51d56bb0":"# Few important points about the data we have in the dataset\nGiven dataset contains Numeric as well as Categorical Variables:-\n\n1. Following kind of categorical variabels are present in the dataset:\n\n    1.1 Binary Variables - values like Male & Female.\n\n    1.2 Ordinal Variables (low and high cardinality) - ordered values like \"Good\" \"V Good\" \"Excellent\".\n\n    1.3 Nominal Variables (low and high cardinality) - do not have any intrinsic order, values like blood groups \"A\" \"B\" etc","0c1de211":"There are lots of high cardinality features in the dataset."}}