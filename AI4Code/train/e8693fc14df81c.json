{"cell_type":{"6cb2c0ec":"code","cc79b138":"code","11cba9c1":"code","baa8583f":"code","c39324b4":"code","90249b1f":"code","619d7dd0":"code","6faddeb5":"code","984f5ff5":"code","685f58d6":"code","3ace7457":"code","71d29b5e":"code","2b6629b5":"code","4a34f7a3":"markdown"},"source":{"6cb2c0ec":"# Import neccessary libraries\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding","cc79b138":"# define documents\ndocs = ['Well done!',\n        'Good work',\n        'Great effort',\n        'nice work',\n        'Excellent!',\n        'Weak',\n        'Poor effort!',\n        'not good',\n        'poor work',\n        'Could have done better.']","11cba9c1":"# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])","baa8583f":"# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1","c39324b4":"# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(encoded_docs)","90249b1f":"# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","619d7dd0":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove100\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","6faddeb5":"# create a weight matrix for words in training docs\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","984f5ff5":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","685f58d6":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","3ace7457":"# summarize the model\nprint(model.summary())\n","71d29b5e":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n","2b6629b5":"# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","4a34f7a3":"From jason Brownlee's work\nhttps:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/?unapproved=571491&moderation-hash=dbec3607a90eb769ee710fe61d43c1c9#comment-571491\n\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/"}}