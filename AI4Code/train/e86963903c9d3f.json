{"cell_type":{"b3d4bbca":"code","a4e9b83e":"code","15d70221":"code","e4557feb":"code","bf9d98ab":"code","5d0aef33":"code","86a38509":"code","4fca860b":"code","d6563da8":"code","fcfa1e24":"code","5eccf109":"markdown","95a1f145":"markdown","8981c705":"markdown","80ec1d25":"markdown","a50b4ab1":"markdown","d4db6b2e":"markdown","a06a9b24":"markdown","eafaf841":"markdown","6dcf6f70":"markdown","534a35b1":"markdown","501c9a69":"markdown"},"source":{"b3d4bbca":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import mean","a4e9b83e":"sample_subm = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\nX_test0 = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv', index_col='id')\nX_train0 = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv', index_col='id')","15d70221":"X_train0.describe()","e4557feb":"#Check correlations\nX_train1 = pd.DataFrame(X_train0)\n\nc = X_train1.corr().abs()\n\ns = c.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=False)\n\nprint(so[100:150])","bf9d98ab":"# Visualize relevant variables and outliers in Train set\nsns.pairplot(X_train0[['loss', 'f25', 'f52', 'f13', \n                    'f18', 'f2', 'f19','f77']])","5d0aef33":"from sklearn.model_selection import train_test_split\n#from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\n\nX = X_train0.drop(['loss'], axis = 1)\ny = X_train0.loss\n\nprint(\"Ok\")\n#train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=82)\n#train_data = lightgbm.Dataset(train_X, label=train_y)\n#test_data = lightgbm.Dataset(test_X, label=test_y)","86a38509":"import lightgbm\n\npp = 0\nn_b_r = 0\ne_s_r = 0\n\n#np.random.seed(seed=68)\n\n#Set the minimum error arbitrarily large\nminr = 9999999999999\n#How many runs to perform using randomly selected hyperparameters\niterations = 2 #50\nseeds = 2 #3\nfor i in range(iterations):\n    print('ITERATION: ', i+1)\n    try:\n        params = {} #initialize parameters\n        params['boosting_type'] = np.random.choice(['gbdt']) #, 'goss']) #, 'dart'])  #'boosting' \n        params['metric'] = 'RMSE'\n        params['learning_rate'] = np.random.uniform(0.0169, 0.0175)  #1st ROUND - (0.01, 0.9)        \n        params['num_leaves'] = np.random.randint(16, 20)  #1st ROUND - (10, 200)  #OLD -31\n        params['min_data'] = np.random.randint(14, 18)  #1st ROUND - (5, 50)\n        params['max_depth'] = np.random.randint(13,16)  #1st ROUND - (-1, 30)\n        params['feature_fraction'] = np.random.uniform(0.130, 0.145)  #1st ROUND (0.05, 0.95) \n        params['bagging_fraction'] = np.random.uniform(0.77, 0.9)  #1st ROUND (0.05, 0.95) \n        params['bagging_freq'] = np.random.randint(15, 18)   #1st ROUND \n        its = np.random.randint(3020, 3100)   #1st ROUND (1000, 8000) \n        e_stop = np.random.randint(65, 80)   #1st ROUND (50, 500) \n        params['verbose'] = -1\n        print(params, its, e_stop)     \n\n        train_data = lightgbm.Dataset(X, label=y)  \n        \n        rms_j = []\n        for j1 in range(seeds):\n            modelLGBM = lightgbm.cv(params,\n                                   train_data,\n                                    num_boost_round=its,\n                                    nfold=3,\n                                    early_stopping_rounds=e_stop,\n                                    seed=np.random.randint(1, 9999),\n                                    verbose_eval = -1 )  #silent evaluations\n            #print(modelLGBM)\n            a = map(lambda x: np.mean(modelLGBM[x]), modelLGBM)\n            rms0 = list(a)         \n            rms_j.append(rms0[0])\n        print(rms_j)\n        rms_j_aux = 0\n        for k in range(len(rms_j)):\n            rms_j_aux += rms_j[k]\n        rms3 = rms_j_aux \/ len(rms_j)\n        print(\"RMS average: \", rms3)\n\n        if rms3 < minr:\n            minr = rms3\n            pp = params \n            n_b_r = its\n            e_s_r = e_stop\n    except Exception as e: \n        print('***** Failed - ', e)\n        \nprint(\"*\" * 100)\nprint('Minimum RMS: ', minr)\nprint('parameters = ', pp)\nprint('num_boost_round =', n_b_r)\nprint('early_stopping_rounds = ', e_s_r)","4fca860b":"import lightgbm\n\nt_size = 0.25 # can be changed \n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=t_size)\ntrain_data = lightgbm.Dataset(train_X, label=train_y)\ntest_data = lightgbm.Dataset(test_X, label=test_y)\n\nparameters =  {'boosting_type': 'gbdt',    #'goss','dart'\n               'metric': 'RMSE',\n               'learning_rate': 0.01734225667880825, \n               'num_leaves': 17, \n               'min_data': 15,\n               'max_depth': 14,\n               'feature_fraction': 0.13301919009818083,  \n               'bagging_fraction': 0.8647874600606483,  \n               'bagging_freq': 16,  \n               'verbose': -1}  #silent errors\n#parameters = pp\nn_b_r =  3070\ne_s_r =  74\n\nmodelLGBM = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=n_b_r, \n                       early_stopping_rounds=e_s_r, \n                        verbose_eval = -1 )  #silent evaluations\n\ny_pred2 = modelLGBM.predict(test_X)\npreds3 = [round(value) for value in y_pred2]\n\n# evaluate predictions\nrms3 = mean_squared_error(test_y, preds3, squared=False)\nprint(\"RMS: \", rms3)\n      \ng2=plt.scatter(test_y, preds3)","d6563da8":"preds1F = modelLGBM.predict(X_test0)\n\nprint(preds1F)\n\npd.DataFrame(preds1F).boxplot(return_type='axes')","fcfa1e24":"output = pd.DataFrame({'id': X_test0.index,\n                      'loss': preds1F})\noutput.set_index('id', inplace=True)\nprint(len(output))\noutput.to_csv('submission.csv')","5eccf109":"# Load Data","95a1f145":"# LightGBM Model","8981c705":"# ML Model","80ec1d25":"# EDA","a50b4ab1":"I've seen it before, there's not much to do here... let the ML Boost model do the job.","d4db6b2e":"**Train and Test Split - this is not done, because it is incldued the cross validation procedure!**","a06a9b24":"# Playgroung Augut 2021 \n# LightGBM + Random search + k-fold Cross Validation","eafaf841":"**Check the \"optimal\" parameters.**","6dcf6f70":"**Random Search for hyperparameter tunning.** <br\/>\nIt is possible to Run a couple of rounds, reducing the variable space according to the primarly results. <br\/>\n**Makes use of k-fold cross validation.**","534a35b1":"# Final Predict","501c9a69":"# Submission "}}