{"cell_type":{"0705f8b6":"code","1c2c5265":"code","78eb0edb":"code","d3fff273":"code","968034f2":"code","8ac097a0":"code","a185ad31":"code","271aed75":"code","ad3ac652":"code","6ef38fda":"code","4e7a2977":"code","8fe8c632":"code","39e5f745":"code","6ea004b9":"code","c2d4be52":"code","d4d51ad6":"markdown","05ba38c5":"markdown","103c8848":"markdown","7740300a":"markdown","02bb3eb5":"markdown","8b3171e8":"markdown","07125372":"markdown","c60494a7":"markdown","b2dc9ee0":"markdown"},"source":{"0705f8b6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import KNNImputer\nfrom sklearn.tree import *\nfrom sklearn.metrics import f1_score","1c2c5265":"data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata.head()","78eb0edb":"data.hist(figsize=(10,10))","d3fff273":"data.nunique()","968034f2":"data.isna().sum()","8ac097a0":"data.stroke.value_counts()","a185ad31":"data.describe()","271aed75":"cat = ['gender','ever_married','Residence_type','smoking_status','work_type']\nfor i in cat:\n    dummy = pd.get_dummies(data[i],drop_first=True,prefix=f\"{i}_\")\n    data = pd.concat([data,dummy],axis=1)","ad3ac652":"data.head()","6ef38fda":"data = data.drop([*cat,'id'],axis=1)","4e7a2977":"data.head()","8fe8c632":"data.corrwith(data['stroke'])","39e5f745":"X = data.drop('stroke',axis=1).values\ny = data['stroke'].values","6ea004b9":"skf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(X, y)\n\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    imputer = KNNImputer(n_neighbors=2)\n    X_train = imputer.fit_transform(X_train)\n    X_test = imputer.fit_transform(X_test)\n    \n    clf = DecisionTreeClassifier()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    f = f1_score(y_true = y_test , y_pred = y_pred,average = 'weighted')\n    \n    print(f)","c2d4be52":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,10))\nplot_tree(clf,feature_names=data.drop('stroke',axis=1).columns,max_depth = 2,filled=True,class_names = True)\nplt.show()","d4d51ad6":"# Histogram Plots","05ba38c5":"# Decision Tree\nThis algorithm is amongst the simplest and fastest classification algorithm.  \nThe classification takes place in a series of decisions and the procedure will be clear to you by the end of the notebook.","103c8848":"# Other Properties","7740300a":"# One Hot Encoding Categorical Variables","02bb3eb5":"# Loading Dataset","8b3171e8":"# Train and Test","07125372":"You can learn aboout this graph and it's interpretation from [here](https:\/\/towardsdatascience.com\/understanding-decision-trees-once-and-for-all-2d891b1be579)  \nHappy Learning","c60494a7":"**Note** Preprocessing techniques are avoided here to represent True Decision Tree.  \nIt shall be done to increase overall accuracy","b2dc9ee0":"As the dataset is highly imbalanced, we'll go for startified split rather than train test split.  \nStartified Split, balances the ration of train and test target variables as in original dataset, to represent population"}}