{"cell_type":{"e73ead57":"code","548a4b84":"code","f9827f67":"code","57811bf4":"code","7535e276":"code","bfaa948f":"code","ae9f7fde":"code","4ff792e5":"code","9ef76cda":"code","f601c218":"code","a1066483":"code","0c996f00":"code","07465389":"code","3f8271d7":"code","60eb2a9e":"markdown","131e1661":"markdown","227ab86f":"markdown","57c187fb":"markdown","03747d19":"markdown","af30c9da":"markdown","450303f0":"markdown","450a9632":"markdown","c31dd1e8":"markdown","daf3750f":"markdown","7d51b878":"markdown","8a9defe4":"markdown","28c7c01a":"markdown","205ab518":"markdown","b78fdc71":"markdown","08f865fd":"markdown","000ff3d3":"markdown"},"source":{"e73ead57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","548a4b84":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","f9827f67":"df2017 = pd.read_csv('..\/input\/cicids2017\/Friday-WorkingHours-Afternoon-DDos.csv')\ndf2018 = pd.read_csv('..\/input\/ids-intrusion-csv\/02-14-2018.csv')","57811bf4":"temp, No_Col_2017 = df2017.shape\nprint(f'There are {No_Col_2017} columns in CICIDS2017')\ntemp, No_Col_2018 = df2018.shape\nprint(f'There are {No_Col_2018} columns in CICIDS2018')","7535e276":"df2017.head(10)","bfaa948f":"df2018.head(10)","ae9f7fde":"print(df2017.columns)","4ff792e5":"print(df2018.columns)","9ef76cda":"df2017.info()","f601c218":"df2018.info()","a1066483":"df_col_2017 = df2017.columns.tolist()\ndf_col_2018 = df2018.columns.tolist()\ni = 0\nprint(\"CICIDS2017   CICIDS2018\")\nwhile i < 80:\n    if i < 79:\n        print(f'{df_col_2017[i]}\\t\\t\\t\\t{df_col_2018[i]}')\n    else:\n        print(f'\\t\\t\\t\\t{df_col_2018[i]}')\n    i = i + 1\n","0c996f00":"d0_2017 = df2017 #the first row portion is already imported so we will just copy that\nd1_2017= pd.read_csv('..\/input\/cicids2017\/Friday-WorkingHours-Afternoon-PortScan.csv')\nd2_2017= pd.read_csv('..\/input\/cicids2017\/Friday-WorkingHours-Morning.csv')\nd3_2017= pd.read_csv('..\/input\/cicids2017\/Monday-WorkingHours.csv')\nd4_2017= pd.read_csv('..\/input\/cicids2017\/Thursday-WorkingHours-Afternoon-Infilteration.csv')\nd5_2017= pd.read_csv('..\/input\/cicids2017\/Thursday-WorkingHours-Morning-WebAttacks.csv')\nd6_2017= pd.read_csv('..\/input\/cicids2017\/Tuesday-WorkingHours.csv')\nd7_2017= pd.read_csv('..\/input\/cicids2017\/Wednesday-workingHours.csv')\n\n# And now cmbining the datasets\ndf2017 = pd.concat([d0_2017, d1_2017, d2_2017, d3_2017, d4_2017, d5_2017, d6_2017, d7_2017], ignore_index=True)","07465389":"df2017.shape ","3f8271d7":"# d0_2018 = df2018 #the first row portion is already imported so we will just copy that\n# dtypes_of_df2018 = df2018.dtypes.to_dict()\n# d1_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-15-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d2_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-16-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d3_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-20-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d4_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-21-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d5_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-22-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d6_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-23-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d7_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/02-28-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d8_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/03-01-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d9_2018= pd.read_csv('..\/input\/ids-intrusion-csv\/03-02-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n\n# # And now cmbining the datasets\n# df2018 = pd.concat([d0_2018, d1_2018, d2_2018, d3_2018, d4_2018, d5_2018, d6_2018, d7_2018, d8_2018, d9_2018], ignore_index=True)","60eb2a9e":"# 2. CICIDS2018","131e1661":"# First To see the number of column in both datasets\n","227ab86f":"# CICIDS2018 features","57c187fb":"# CICIDS2018 columns","03747d19":"# Lets see the fisrt 10 rows of each dataset","af30c9da":"# We will do analysis of features in depth but before that we will see which types of attacks are there in both dataset and what are the frequencies of it","450303f0":"# **Netwok Intrusion Detection System**\n\n# Analysis of the two recent dataset CICIDS2017 and CICIDS2018.\n \n# **The analysis is for the FYP(NIDS) from University of Engineering And Technology Mardan**\n","450a9632":"# 1. CICIDS2017","c31dd1e8":"# Here we will combine all the portions of dataset in to one datafram","daf3750f":"# No of rows and columns","7d51b878":"# CICIDS2017 features","8a9defe4":"# ***To see the column of both dataset samples***","28c7c01a":"# Comparing the features ","205ab518":"# CICIDS2017 columns","b78fdc71":"# > Analysis of the fisrt csv file of both the datasets\n","08f865fd":"# To see the detail information of features of the datasets samples","000ff3d3":"# Importing the libraries"}}