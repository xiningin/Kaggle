{"cell_type":{"55b4a40c":"code","6ed5be13":"code","5141f5c2":"code","7d808c93":"code","b1e6b34e":"code","94f28c60":"code","858f2034":"code","70c6095e":"code","e76f74c3":"code","87600b1d":"code","cf52acf1":"code","6316ef7d":"code","0577f485":"code","64b92785":"code","fde05b70":"code","d702c55e":"code","75a5d59d":"code","3cd52de9":"code","87729f02":"code","b99b452c":"code","170f64aa":"code","21842883":"code","4e64a6f3":"code","51e43bdd":"code","dcb157e3":"code","0fb1cd87":"code","903068a8":"code","b9400d53":"code","0b4249dc":"code","98af9135":"code","d403be7f":"code","5b5039a6":"code","4e2d133f":"code","ff7719b6":"code","a5ff7069":"code","ea08211f":"code","ebc81ded":"code","e1e8aaf0":"code","69cefd18":"code","07106725":"code","4c7fc74d":"code","9da56121":"code","afc78c48":"code","2c5d1a8a":"code","e8b8b1af":"code","f51c4097":"code","28eb1b14":"code","0b44d711":"code","43424f1a":"code","615119fb":"code","4b0eb73e":"code","f7eb5ebe":"code","16126479":"code","6c4fa71b":"code","562d4bff":"code","18d235e7":"code","2de63c8f":"code","f1fc31b4":"code","7b972b15":"code","9f21e963":"code","a10d45b1":"code","e65964de":"code","53d8fb3b":"code","a75b2299":"code","023bdbdc":"code","dd823a32":"code","79ac31db":"code","f910ae9f":"code","7f672484":"code","b2bff870":"code","2c9388b9":"code","c0470ee1":"code","10d409f1":"code","602fbc5b":"code","253a092e":"code","3101f7ca":"code","d40fdd50":"code","35f5fd98":"code","85a216db":"code","ba06d74c":"code","e49e49d5":"code","416dc59e":"code","d6683e10":"code","25140fa5":"code","503624bc":"code","86157d13":"code","6efd70fc":"code","61d429a8":"code","c0986adf":"code","9135fc9a":"code","b7b1591e":"code","a823f349":"code","c755a777":"code","68c64d4a":"code","3bd652c9":"code","92c66c1d":"code","9d710ef0":"code","34bea7d7":"code","79696300":"code","432a1cc8":"code","b566ce99":"code","095372e7":"code","5193316c":"code","35f94d0e":"code","f72a4373":"code","22876dd0":"code","6b9529cb":"code","b9e9048d":"code","83485436":"code","c91170ac":"code","4d36e3bd":"code","7920397a":"code","c799d839":"code","2eae1ca1":"code","461c2604":"code","9381ceca":"code","de5ecee5":"code","1ed6437d":"markdown","e969fa02":"markdown","20a07aed":"markdown","5c3253f8":"markdown","08260ad0":"markdown","a78fe0b7":"markdown","baed9c81":"markdown","f56fe861":"markdown","091fc454":"markdown","61e8eac9":"markdown","eb9f9317":"markdown","2aa84b45":"markdown","ad3fc0c0":"markdown","9d725e41":"markdown","78d12ff6":"markdown","46a0b4ec":"markdown","44c95b7a":"markdown","c370c3df":"markdown","d666d81b":"markdown","3a2d28af":"markdown","59573218":"markdown","88b73718":"markdown","c95921c8":"markdown","2b46e24f":"markdown","bef9eb02":"markdown","6e4ed18a":"markdown","e493a225":"markdown","dc63b802":"markdown","0dbb0547":"markdown","fccac90f":"markdown","ebcea61a":"markdown","cbbc1919":"markdown","e8c0005c":"markdown","d5956bb2":"markdown","8411eb91":"markdown","53f575bf":"markdown","da8beae5":"markdown","9994312a":"markdown","9b73ce59":"markdown","b4d0d6c7":"markdown","8f3e921f":"markdown","892bc494":"markdown","191e64dc":"markdown","c3520ae1":"markdown","7bd91886":"markdown","269e4e88":"markdown","783d59e9":"markdown","74ebf90d":"markdown","4945faf9":"markdown","b7b75260":"markdown","abdc7132":"markdown"},"source":{"55b4a40c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ed5be13":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config Completer.use_jedi = True\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 14\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (16,10)\nplt.rcParams[\"ps.useafm\"] = True\nimport random\nrandom.seed(21)\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)","5141f5c2":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf","7d808c93":"df.info()","b1e6b34e":"df.isna().sum()","94f28c60":"print(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","858f2034":"corr = df.corr()\nplt.figure(figsize=(10,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","70c6095e":"df.describe().T","e76f74c3":"plt.figure(figsize=(14,8))\nx = list(df.nunique().index)\ny = df.nunique().values\nfor i in range(len(x)):\n        plt.text(i-.15,y[i]+5,str(y[i]))\nplt.title(\"No. of unique values in every feature of the dataset\",fontdict={\"fontsize\":16})\nsns.barplot(x= x,y=y)\nplt.show();","87600b1d":"df.shape","cf52acf1":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df['Age'], bins = 20, ax=ax[0,0] , color=\"red\") \nsns.distplot(df['Pregnancies'], bins = 20, ax=ax[0,1], color=\"red\") \nsns.distplot(df['Glucose'], bins = 20, ax=ax[1,0], color=\"black\") \nsns.distplot(df['BloodPressure'], bins = 20, ax=ax[1,1], color=\"black\") \nsns.distplot(df['SkinThickness'], bins = 20, ax=ax[2,0], color=\"red\")\nsns.distplot(df['Insulin'], bins = 20, ax=ax[2,1], color=\"red\")\nsns.distplot(df['DiabetesPedigreeFunction'], bins = 20, ax=ax[3,0], color=\"red\") \nsns.distplot(df['BMI'], bins = 20, ax=ax[3,1],color=\"black\")","6316ef7d":"skewed_features = df.apply(lambda x:x.skew()).sort_values(ascending=False)\nskewed_features","0577f485":"df[df['Glucose'] == 0]","64b92785":"df[df['BloodPressure'] == 0]","fde05b70":"df[df['SkinThickness'] == 0]","d702c55e":"df[df['Insulin'] == 0]","75a5d59d":"df[df['BMI'] == 0]","3cd52de9":"df[['BMI','Insulin','SkinThickness','BloodPressure','Glucose']] = df[['BMI','Insulin','SkinThickness',\n                                                                      'BloodPressure','Glucose']].replace(0,np.nan)","87729f02":"## Checking for NULL values\ndf.isna().sum()","b99b452c":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df['Age'], bins = 20, ax=ax[0,0] , color=\"red\") \nsns.distplot(df['Pregnancies'], bins = 20, ax=ax[0,1], color=\"red\") \nsns.distplot(df['Glucose'], bins = 20, ax=ax[1,0], color=\"black\") \nsns.distplot(df['BloodPressure'], bins = 20, ax=ax[1,1], color=\"black\") \nsns.distplot(df['SkinThickness'], bins = 20, ax=ax[2,0], color=\"k\")\nsns.distplot(df['Insulin'], bins = 20, ax=ax[2,1], color=\"red\")\nsns.distplot(df['DiabetesPedigreeFunction'], bins = 20, ax=ax[3,0], color=\"red\") \nsns.distplot(df['BMI'], bins = 20, ax=ax[3,1],color=\"black\")","170f64aa":"skewed_features = df.apply(lambda x:x.skew()).sort_values(ascending=False)\nskewed_features","21842883":"corr = df.corr()\nplt.figure(figsize=(10,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","4e64a6f3":"fig = plt.figure(figsize=(16,16))\nfor i in range(len(df.columns)):\n    fig.add_subplot(3,3,i+1)\n    sns.boxplot(y=df.iloc[:,i])\nplt.tight_layout()\nplt.show()","51e43bdd":"sns.pairplot(df,hue='Outcome')","dcb157e3":"## Functions for creating barplot and for adding labels on the respective bars\ndef addlabels(x,y,df):\n    for i in range(len(x)):\n        plt.text(i-0.15,y[i]+5,str(np.round((y[i]\/df.shape[0])*100,1)) + \"%\")\ndef barplot(df,feature,fig_size):\n    plt.figure(figsize=fig_size)\n    y = df[feature].value_counts().sort_index(ascending=True).values\n    x = list(df[feature].value_counts().sort_index(ascending=True).index)\n    addlabels(x,y,df)\n    sns.countplot(df[feature])","0fb1cd87":"df.columns","903068a8":"df['Outcome'].value_counts()","b9400d53":"barplot(df,'Outcome',(8,4))","0b4249dc":"df['Pregnancies']","98af9135":"df['Pregnancies'].value_counts()","d403be7f":"barplot(df,'Pregnancies',(16,8))","5b5039a6":"sns.countplot(x=df['Pregnancies'],hue=df['Outcome'])","4e2d133f":"sns.boxplot(y='Pregnancies',x='Outcome',data=df)","ff7719b6":"df['Age']","a5ff7069":"sns.boxplot(x=df['Outcome'],y=df['Age'])","ea08211f":"age = df['Age'].value_counts().sort_index(ascending=True)","ebc81ded":"plt.figure(figsize = (15,5))\nplt.plot(age)\nplt.xlabel(\"Age of the person\")\nplt.ylabel(\"Frequency count of ages\")\nplt.title(\"Age distribution of whole dataset\")\nplt.show()","e1e8aaf0":"age_0 = df[df['Outcome'] == 0]['Age'].value_counts().sort_index(ascending=True)\nage_1 = df[df['Outcome'] == 1]['Age'].value_counts().sort_index(ascending=True)","69cefd18":"plt.figure(figsize = (15,5))\nplt.plot(age_0,c='k',label=\"Aren't diabetic patients\")\nplt.plot(age_1,c='r',label='Are diabetic patients ')\nplt.xlabel(\"Age of the person\")\nplt.ylabel(\"Frequency count of ages\")\nplt.title(\"Age distribution of whole dataset\")\nplt.legend()\nplt.show","07106725":"sns.countplot(x=df['Age'],hue=df['Outcome'])","4c7fc74d":"sns.lineplot(x='Age',y='Pregnancies',hue='Outcome',data=df)","9da56121":"df['Glucose']","afc78c48":"glucose = df['Glucose'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(glucose)\nplt.xlabel(\"Glucose level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"Glucose distribution of whole dataset\")\nplt.show()","2c5d1a8a":"glucose_0 = df[df['Outcome'] == 0]['Glucose'].value_counts().sort_index(ascending=True)\nglucose_1 = df[df['Outcome'] == 1]['Glucose'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(glucose_0,c='k',label=\"Aren't diabetic patients\")\nplt.plot(glucose_1,c='r',label='Are diabetic patients ')\nplt.xlabel(\"Glucose level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"Glucose distribution of whole the dataset\")\nplt.legend()\nplt.show()","e8b8b1af":"sns.boxplot(y=df['Glucose'],x=df['Outcome'])","f51c4097":"df['BloodPressure']","28eb1b14":"df['BloodPressure'].value_counts()","0b44d711":"sns.boxplot(x=df['Outcome'],y=df['BloodPressure'])","43424f1a":"sns.lineplot(x='Age',y='BloodPressure',hue='Outcome',data=df)","615119fb":"bloodPressure = df['BloodPressure'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(bloodPressure)\nplt.xlabel(\"Blood Pressure level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"Blood Pressure distribution of whole dataset\")\nplt.show()","4b0eb73e":"bloodPressure_0 = df[df['Outcome'] == 0]['BloodPressure'].value_counts().sort_index(ascending=True)\nbloodPressure_1 = df[df['Outcome'] == 1]['BloodPressure'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(bloodPressure_0,c='k',label=\"Aren't diabetic patients\")\nplt.plot(bloodPressure_1,c='r',label='Are diabetic patients ')\nplt.xlabel(\"Blood Pressure level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"Blood Pressure distribution of whole the dataset\")\nplt.legend()\nplt.show()","f7eb5ebe":"df['SkinThickness']","16126479":"df['SkinThickness'].value_counts()","6c4fa71b":"sns.boxplot(x=df['Outcome'],y=df['SkinThickness'])","562d4bff":"skinthickness = df['SkinThickness'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(skinthickness)\nplt.xlabel(\"SkinThickness level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"SkinThickness distribution of whole dataset\")\nplt.show()","18d235e7":"skinthickness_0 = df[df['Outcome'] == 0]['SkinThickness'].value_counts().sort_index(ascending=True)\nskinthickness_1 = df[df['Outcome'] == 1]['SkinThickness'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(skinthickness_0,c='k',label=\"Aren't diabetic patients\")\nplt.plot(skinthickness_1,c='r',label='Are diabetic patients ')\nplt.xlabel(\"SkinThickness level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"SkinThickness distribution of whole the dataset\")\nplt.legend()\nplt.show()","2de63c8f":"sns.lineplot(x='Age',y='SkinThickness',hue='Outcome',data=df)","f1fc31b4":"df['Insulin']","7b972b15":"df['Insulin'].value_counts()","9f21e963":"sns.boxplot(x=df['Outcome'],y=df['Insulin'])","a10d45b1":"sns.jointplot(x=df['Insulin'], y=df['Age'],hue=df['Outcome'],color=\"#4CB391\")","e65964de":"sns.scatterplot(x=df['Insulin'], y=df['Glucose'],hue=df['Outcome'],color=\"#4CB391\")","53d8fb3b":"insulin = df['Insulin'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(insulin)\nplt.xlabel(\"Insulin level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"Insulin distribution of whole dataset\")\nplt.show()","a75b2299":"insulin_0 = df[df['Outcome'] == 0]['Insulin'].value_counts().sort_index(ascending=True)\ninsulin_1 = df[df['Outcome'] == 1]['Insulin'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(insulin_0,c='k',label=\"Aren't diabetic patients\")\nplt.plot(insulin_1,c='r',label='Are diabetic patients ')\nplt.xlabel(\"Insulin level\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"Insulin distribution of whole the dataset\")\nplt.legend()\nplt.show()","023bdbdc":"df['BMI']","dd823a32":"df['BMI'].value_counts()","79ac31db":"sns.scatterplot(x=df['BMI'],y=df['SkinThickness'],hue=df['Outcome'])","f910ae9f":"sns.jointplot(x=df['BMI'],y=df['SkinThickness'],kind='reg',color='#fa3737')","7f672484":"sns.lineplot(x='Age',y='BMI',hue='Outcome',data=df)","b2bff870":"sns.boxplot(x=df['Outcome'],y=df['BMI'])","2c9388b9":"bmi = df['BMI'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(bmi)\nplt.xlabel(\"BMI value\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"BMI distribution of whole dataset\")\nplt.show()","c0470ee1":"bmi_0 = df[df['Outcome'] == 0]['BMI'].value_counts().sort_index(ascending=True)\nbmi_1 = df[df['Outcome'] == 1]['BMI'].value_counts().sort_index(ascending=True)\nplt.figure(figsize = (15,5))\nplt.plot(bmi_0,c='k',label=\"Aren't diabetic patients\")\nplt.plot(bmi_1,c='r',label='Are diabetic patients ')\nplt.xlabel(\"BMI value\")\nplt.ylabel(\"Frequency count of persons\")\nplt.title(\"BMI distribution of whole the dataset\")\nplt.legend()\nplt.show()","10d409f1":"df['DiabetesPedigreeFunction']","602fbc5b":"df['DiabetesPedigreeFunction'].value_counts()","253a092e":"sns.boxplot(y=df['DiabetesPedigreeFunction'],x=df['Outcome'])","3101f7ca":"df['Pregnancies'].plot(kind='box')","d40fdd50":"df[df['Pregnancies'] > 13]","35f5fd98":"df['Pregnancies'].replace([14,15,17],np.nan,inplace=True)","85a216db":"df['BloodPressure'].plot(kind='box')","ba06d74c":"df[df['BloodPressure'] < 40]","e49e49d5":"df['BloodPressure'].replace([30,24,38],np.nan,inplace=True)","416dc59e":"df['SkinThickness'].plot(kind='box')","d6683e10":"df[df['SkinThickness'] > 65]","25140fa5":"df['SkinThickness'].replace(99,np.nan,inplace=True)","503624bc":"df['Insulin'].plot(kind='box')","86157d13":"df[df['Insulin'] > 500]","6efd70fc":"df['Insulin'].replace(df[df['Insulin'] > 500]['Insulin'].values,np.nan,inplace=True)","61d429a8":"df.isna().sum()","c0986adf":"df['Pregnancies'].fillna(df['Pregnancies'].median(),inplace=True)\ndf['Glucose'].fillna(df['Glucose'].mean(),inplace=True)\ndf['BloodPressure'].fillna(df['BloodPressure'].mean(),inplace=True)\ndf['SkinThickness'].fillna(df['SkinThickness'].mean(),inplace=True)\ndf['Insulin'].fillna(df['Insulin'].mean(),inplace=True)\ndf['BMI'].fillna(df['BMI'].mean(),inplace=True)","9135fc9a":"X = df.drop(['Outcome'],axis=1)\ny = df['Outcome']\nprint(X.shape,y.shape)","b7b1591e":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,random_state=42)\nprint(\"The shape of training data : \",x_train.shape,y_train.shape)\nprint(\"The shape of testing data : \",x_test.shape,y_test.shape)","a823f349":"from sklearn.preprocessing import StandardScaler\nscaler =  StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)","c755a777":"print(\"The shape of training data : \",x_train_scaled.shape,y_train.shape)\nprint(\"The shape of testing data : \",x_test_scaled.shape,y_test.shape)","68c64d4a":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nx_sm, y_sm = smote.fit_resample(x_train_scaled, y_train)\nprint(\"The shape of training data : \",x_train_scaled.shape,y_train.shape)\nprint(\"The shape of testing data : \",x_sm.shape,y_sm.shape)","3bd652c9":"x_train_scaled = x_sm\ny_train = y_sm\nprint(\"The shape of training data : \",x_train_scaled.shape,y_train.shape)\nprint(\"The shape of testing data : \",x_test_scaled.shape,y_test.shape)","92c66c1d":"def draw_confusion_matrix(y_test,y_pred):\n    fig = plt.figure(figsize=(4, 4))\n    ax= plt.subplot()\n    cm = confusion_matrix(y_true=y_test,y_pred=y_pred)\n    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\n    ax.set_xlabel('Predicted label')\n    ax.set_ylabel('Actual label')\n    plt.show()","9d710ef0":"## To store the F1 scores of all models\nmodel_scores = []","34bea7d7":"from sklearn.metrics import classification_report,confusion_matrix,f1_score\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train_scaled,y_train)\ny_pred = logreg.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","79696300":"from sklearn.svm import LinearSVC\nlinearsvm = LinearSVC().fit(x_train_scaled,y_train)\nprint(\"Coefficient :\",linearsvm.coef_)\nprint(\"Intercept :\",linearsvm.intercept_)\ny_pred = linearsvm.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","432a1cc8":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=4)\ntree.fit(x_train_scaled,y_train)\ny_pred = tree.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)","b566ce99":"## Performing cross validation for hyperparamter tunning of DecisionTreeClassifier model.\nfrom sklearn.model_selection import GridSearchCV\nparam_test = {\n    'max_depth' : [2,3,4,5,6,7],\n    'min_samples_split' : [2,3,4,5],\n    'min_samples_leaf' :[2,3,4,5]\n}\ntree_cv = GridSearchCV(estimator= DecisionTreeClassifier(),\n                             param_grid = param_test,scoring='f1',\n                             n_jobs=-1,cv=5)\ntree_cv.fit(x_train_scaled,y_train)\n## This best score is the mean of five cross validation folds with the best hyperparamters. \nprint(tree_cv.best_params_, tree_cv.best_score_)\ny_pred = tree_cv.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","095372e7":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier().fit(x_train_scaled,y_train)\ny_pred = clf.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)","5193316c":"## Performing cross validation for hyperparamter tunning of MLPClassifier model.\nparam_test = {\n    'activation' : ['logistic', 'tanh', 'relu'],\n    'alpha' : [0.01,0.1,0.5,1],\n    'learning_rate' : ['adaptive']\n    \n}\nmlp_cv = GridSearchCV(estimator= MLPClassifier(),\n                             param_grid = param_test,scoring='f1',\n                             n_jobs=-1,cv=5)\nmlp_cv.fit(x_train_scaled,y_train)\n## This best score is the mean of five cross validation folds with the best hyperparamters. \nprint(mlp_cv.best_params_, mlp_cv.best_score_)\ny_pred = mlp_cv.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","35f94d0e":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500,max_depth=4).fit(x_train_scaled,y_train)\ny_pred = rf.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)","f72a4373":"## Performing cross validation for hyperparamter tunning of Random Forest Classifier model.\nparam_test = {\n    'max_depth' : [2,3,4,5,6,7],\n    'min_samples_split' : [2,3,4,5],\n    'min_samples_leaf' :[2,3,4,5]\n}\nrf_cv = GridSearchCV(estimator= RandomForestClassifier(n_estimators=500),\n                             param_grid = param_test,scoring='f1',\n                             n_jobs=-1,cv=5)\nrf_cv.fit(x_train_scaled,y_train)\n## This best score is the mean of five cross validation folds with the best hyperparamters. \nprint(rf_cv.best_params_, rf_cv.best_score_)\ny_pred = rf_cv.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","22876dd0":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier()\ngbrt.fit(x_train_scaled,y_train)\ny_pred = gbrt.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)","6b9529cb":"## Performing cross validation for hyperparamter tunning of GradientBoostingClassifier model.\nparam_test = {\n    'max_depth' : [2,3,4,5,6,7],\n    'min_samples_split': np.arange(2, 12, 3),\n    'min_samples_leaf': np.arange(1, 10, 3)\n}\ngbrt_cv = GridSearchCV(estimator= GradientBoostingClassifier(n_estimators=500),\n                             param_grid = param_test,scoring='f1',\n                             n_jobs=-1,cv=5)\ngbrt_cv.fit(x_train_scaled,y_train)\nprint(gbrt_cv.best_params_, gbrt_cv.best_score_)\ny_pred = gbrt_cv.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","b9e9048d":"from xgboost import XGBClassifier\nxgb = XGBClassifier(max_depth=1)\nxgb.fit(x_train_scaled,y_train)\ny_pred = xgb.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","83485436":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=1000,learning_rate=0.05)\nada.fit(x_train_scaled,y_train)\ny_pred = ada.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","c91170ac":"import lightgbm\nfrom lightgbm import LGBMClassifier\nlgbm = LGBMClassifier(max_depth=1,learning_rate=0.1,reg_alpha=0.05,reg_lambda=0.01)\nlgbm.fit(x_train_scaled, y_train)\ny_pred = lgbm.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","4d36e3bd":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(max_depth=3,n_estimators=1000,verbose=False)\ncat.fit(x_train_scaled, y_train)\ny_pred = cat.predict(x_test_scaled)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(\"F1 score : {:.3f}\".format(f1_score(y_true=y_test,y_pred=y_pred,average='macro')))\ndraw_confusion_matrix(y_test,y_pred)\nmodel_scores.append(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","7920397a":"import tensorflow as tf\ndef make_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(128,activation='elu',input_shape=(x_train_scaled.shape[1],)))\n    model.add(tf.keras.layers.Dense(64,activation='elu'))\n    model.add(tf.keras.layers.Dense(32,activation='elu'))\n    #model.add(tf.keras.layers.Dense(16,activation='elu'))\n    model.add(tf.keras.layers.Dense(4,activation='elu'))\n    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer='Adam',metrics=['AUC'])\n    return model","c799d839":"model = make_model()\nmodel.summary()","2eae1ca1":"model.fit(x_train_scaled,y_train,validation_data=(x_test_scaled,y_test),epochs=20,batch_size=32,\n          callbacks=[\n              tf.keras.callbacks.EarlyStopping(monitor='val_auc',patience=5),\n              tf.keras.callbacks.ModelCheckpoint('model_{val_auc:.3f}.h5',\n                                                save_best_only=True,save_weights_only=False,mode='auto',\n                                                monitor='val_auc')\n              \n    \n])","461c2604":"y_pre = model.predict(x_test_scaled)\nfor i,val in enumerate(y_pre):\n    if val >= 0.5:\n        y_pred[i] = 1\n    else:\n        y_pred[i] = 0\n        ","9381ceca":"draw_confusion_matrix(y_pred,y_test)\nmodel_scores.append(model.evaluate(x_test_scaled,y_test)[1])","de5ecee5":"print(\"The F1 score of Logistic Regression : {:.3f}\".format(model_scores[0]))\nprint(\"The F1 score of Linear SVC : {:.3f}\".format(model_scores[1]))\nprint(\"The F1 score of Decision Tree Classifier : {:.3f}\".format(model_scores[2]))\nprint(\"The F1 score of MLP Classifier : {:.3f}\".format(model_scores[3]))\nprint(\"The F1 score of Random Forest Classifier : {:.3f}\".format(model_scores[4]))\nprint(\"The F1 score of Gradient Boosting Classifier : {:.3f}\".format(model_scores[5]))\nprint(\"The F1 score of XGB Classifier : {:.3f}\".format(model_scores[6]))\nprint(\"The F1 score of Ada Boost Classifier : {:.3f}\".format(model_scores[7]))\nprint(\"The F1 score of LGBM Classifier : {:.3f}\".format(model_scores[8]))\nprint(\"The F1 score of CatBoost Classifier : {:.3f}\".format(model_scores[9]))\nprint(\"The F1 score of Dense layer based ANN Classifier : {:.3f}\".format(model_scores[10]))","1ed6437d":"From this plot, we can conclude that, generally chances of having diabetes increases with increase in number of pregancies.","e969fa02":"#### Dense layer based ANN Classifier","20a07aed":"### Final scores of all models","5c3253f8":"## Task 4 - Model Evaluation ","08260ad0":"#### Feature 'BMI'","a78fe0b7":"Some points to note here :-\n\n- Features like Age and pregnancies are having good correlation which is quite obvious.\n- Features like Glucose and Insulin are having good correlation.\n- Features Insulin and SkinThickness are having highest correlation value.\n- Our target Feature i.e., Outcome is having some correlation with glucose, Insulin and BMI.","baed9c81":"#### Univariate Statistics of the dataset","f56fe861":"#### AdaBoost Classifier","091fc454":"There are some outliers and therefore we have to takecare of that as well.","61e8eac9":"#### Feature 'Age'","eb9f9317":"#### MLP Classifier","2aa84b45":"#### Feature 'Outcome'","ad3fc0c0":"### Now, let's have some insights from every feature of the dataset.","9d725e41":"#### Random Forest Classifier","78d12ff6":"#### GradientBoosting Classifier","46a0b4ec":"#### DecisionTree Classifier","44c95b7a":"#### Feature 'Pregnancies'","c370c3df":"#### LightBGM","d666d81b":"#### Linear SVC","3a2d28af":"## Task - 3 - Data Preparation for model evaluation ","59573218":"#### Bar Plot to check for unique number of values in each feature of the dataset ","88b73718":"#### Firstly, we need to split the dataset into train and test set ","c95921c8":"Most of the features are loosely gaussian distributed, which is good for us.","2b46e24f":"#### Pairplot","bef9eb02":"#### Checking the skewness","6e4ed18a":"#### Checking for zero values of above features","e493a225":"If you carefully, then you would came to know that some of the features are having zero value which is unrealistic.\n\nThe features having zero value are :- \n- Glucose\n- BloodPressure \n- SkinThickness\n- Insulin\n- BMI\n\nFor these feature, the value as zero can be considered as missing value and therefore we will be replacing them to Nan and will do some arrangements to fill these missing values.","dc63b802":"#### Logistic Regression","0dbb0547":"### End Notes\n\nHere, in my case I got the maximum F1 score from Dense Layer Based ANN Classifier. The data was imbalanced, therefore I have to use SMOTE oversampling techinque to balance the positive and negative instances. I have tried varieties of model here. More Careful tunning and feature engineering may bring even better results. However, This is what I got. Feel free to give my suggestions and feedbacks on this notebook.\n\nHope you like this notebook, if you do then please upvote this notebook. Thank you......!!!!\n\nYou can also check this Hand Gesture Regonition Dataset from \nhttps:\/\/www.kaggle.com\/aryarishabh\/hand-gesture-recognition-dataset","fccac90f":"#### Correlation plot of the original dataset","ebcea61a":"#### After removing all zero values, the density plot will look like","cbbc1919":"Clearly, there is some imbalance in the dataset. So, we keep this thing in our mind and will continue some analysis.","e8c0005c":"Generally, model don't work well when data is skewed. However, the acceptable range is from -3 to +3. So, here everthing is good.","d5956bb2":"#### CatBoost Classifier","8411eb91":"## Task 1 - Importing libraries and dataset","53f575bf":"#### Feature 'DiabetesPedigreeFunction'","da8beae5":"From this plot, we get to know that chances of having diabetes increases with age (more nearly after 30 Years).","9994312a":"#### Correlation plot after updating the data","9b73ce59":"## Task 2 - Exploratory Data Analysis (EDA)","b4d0d6c7":"### Dealing with NULL values","8f3e921f":"#### Feature 'Insulin'","892bc494":"#### Feature 'BloodPressure'","191e64dc":"So, we have some NULL values. We will fill the missing values using some strategy","c3520ae1":"#### Density plot of all features","7bd91886":"There is an extreme outlier here, we'll take care of it.","269e4e88":"#### Feature 'SkinThickness'","783d59e9":"#### Box Plot of every feaure of dataset","74ebf90d":"#### XGB Classifier","4945faf9":"#### Feature 'Glucose'","b7b75260":"#### Now replacing the value zero with Nan","abdc7132":"### Checking the Outliers"}}