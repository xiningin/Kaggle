{"cell_type":{"4a0771b8":"code","1826c45c":"code","096fa9cb":"code","5d1223a0":"code","ab501ca2":"code","e60dba3c":"code","d3c1fedd":"code","5170d1a1":"code","593cb356":"code","c95125d4":"code","a42778d3":"code","69efebfa":"code","d6d616fa":"code","432768b4":"code","8b9efa97":"code","4333d1ca":"code","9c2dc210":"code","9357e0ed":"code","e4a28734":"code","1e4b66d4":"code","fce5d2a7":"code","7c4fe09b":"code","3c396cab":"code","c525005b":"code","0ee0c684":"code","6816d415":"code","97eef8ff":"code","5e038ddc":"code","37abdf17":"code","d9423e0b":"code","597dd8a3":"markdown","8eb6a864":"markdown","293e8194":"markdown","a55a1e6a":"markdown","e7150e28":"markdown","d5d7b3c2":"markdown","ea43fc76":"markdown","acf5c171":"markdown","8c0cddc7":"markdown","87fb059f":"markdown","5ef7b49d":"markdown","156b2159":"markdown","0a4d0d60":"markdown","4d2baf8e":"markdown","61ff1348":"markdown","1397ce01":"markdown","599768b4":"markdown","02084eee":"markdown","114f4f38":"markdown","eecfff67":"markdown","cc4ffe86":"markdown","03751e03":"markdown","cb8dbd31":"markdown","8757d25d":"markdown","d8dcd942":"markdown","35e6cf38":"markdown","4d64d1f8":"markdown","d03df73f":"markdown","c2bbbff4":"markdown","c95a5f51":"markdown","1c268aea":"markdown","be8fa6e4":"markdown","7bd73c4a":"markdown","f66aa9ed":"markdown","acb3e7b1":"markdown","fd4f41be":"markdown"},"source":{"4a0771b8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nfrom scipy.stats import skew","1826c45c":"X_train = pd.read_csv('..\/input\/train.csv')\nX_test = pd.read_csv('..\/input\/test.csv')","096fa9cb":"X_train.head()","5d1223a0":"X_train.info()","ab501ca2":"#the correlation matrix\ncorr_mat = X_train.corr()\ncorr_mat['SalePrice'].sort_values(ascending=False)\n","e60dba3c":"#get the variables that have a good correlation(>.7) with the target\nvalues = list(corr_mat['SalePrice'].values)\nkeys = list(corr_mat['SalePrice'].keys())\nvariables = [i for i in keys if values[keys.index(i)] > .7]\n\n#ploting scatters \nsns.set()\nsns.pairplot(X_train[variables],size=3)\nplt.show()","d3c1fedd":"X_train.drop(X_train[(X_train['GrLivArea']>4000) & (X_train['SalePrice']<300000)].index, inplace=True)","5170d1a1":"sns.distplot(X_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(X_train['SalePrice'], plot=plt)\nplt.show()","593cb356":"#saving the labels of our target variable \ny = X_train['SalePrice']\n\n#save the id column for later.\ntest_id = X_test['Id']\n\n#dropping column that will not be needed when training our model.\nX_train.drop('SalePrice', 1, inplace=True)\nX_train.drop(\"Id\", axis = 1, inplace = True)\nX_test.drop(\"Id\", axis = 1, inplace = True)\n\n#save indexes before concatenating\nX_train_idx = X_train.shape[0]\nX_test_idx = X_test.shape[0]\n\n#concatenate the data\ndata = pd.concat((X_train,X_test)).reset_index(drop=True)\nprint('Train index: %s \\nTest index: %s \\nShape of our whole data(rows,columns): %s'%(X_train_idx, X_test_idx, data.shape))","c95125d4":"#print the sum of the missing values of a column from the highest to the lowest.\nnumeric_cols = []\nnon_numeric_cols = []\nfor key,val in data.isnull().sum().sort_values(ascending=False).items():\n    if val > 0:\n        print(key, val, data[key].dtype)\n        if data[key].dtype != 'object':numeric_cols.append(key)\n        elif data[key].dtype == 'object':non_numeric_cols.append(key)\n        ","a42778d3":"features = data.isnull().sum().sort_values(ascending=False).keys()\nmissing_values = pd.DataFrame(data[numeric_cols])\nmissing_values.head()","69efebfa":"fill_zero_cols = ['BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF2', 'GarageCars']\nfill_median_cols = ['GarageArea','TotalBsmtSF', 'MasVnrArea', 'BsmtFinSF1', 'LotFrontage', 'BsmtUnfSF', 'GarageYrBlt']\n[data[i].fillna(0,inplace=True) for i in fill_zero_cols]\n[data[j].fillna(data[j].median(),inplace=True) for j in fill_median_cols];","d6d616fa":"features = data.isnull().sum().sort_values(ascending=False).keys()\nmissing_values = pd.DataFrame(data[non_numeric_cols])\nmissing_values.head()","432768b4":"data.fillna('none', inplace=True)","8b9efa97":"data.isnull().sum().sort_values(ascending=False)","4333d1ca":"#transforming our target labels(SalePrice).\ny = np.log1p(y)\n#transfomring the numeric features \nnumeric_feats = data.dtypes[data.dtypes != \"object\"].index\nskewed_feats = X_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\ndata[skewed_feats] = np.log1p(data[skewed_feats])","9c2dc210":"pd.get_dummies(data['PoolQC']).head()","9357e0ed":"data = pd.get_dummies(data)","e4a28734":"X_train = data[:X_train_idx]\nX_test = data[X_train_idx:] ","1e4b66d4":"from sklearn.linear_model import Lasso, ElasticNet, Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler","fce5d2a7":"#our cross function\ndef display_scores(scores):\n    print(\"Score:{:.4f}\".format(scores.mean()))\n#learning curves function \ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n        val_errors.append(mean_squared_error(y_val_predict, y_val))\n        plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n        plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")","7c4fe09b":"elastic_net = ElasticNet(alpha=.5, l1_ratio=.5)\nscores = cross_val_score(elastic_net, X_train, y, scoring='neg_mean_squared_error', cv=5)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)","3c396cab":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n        {'alpha': [.0004, .0005], 'l1_ratio': [.5, .8, 1]},\n]\n    \ngrid_search = GridSearchCV(ElasticNet(), param_grid, cv=5,\n                               scoring='neg_mean_squared_error',\n                               return_train_score=True)","c525005b":"grid_search.fit(X_train, y)","0ee0c684":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","6816d415":"lasso = Lasso(alpha=.1).fit(X_train,y)\nscores = cross_val_score(lasso,X_train,y,scoring='neg_mean_squared_error',cv=5)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)","97eef8ff":"plot_learning_curves(lasso,X_train,y)\nplt.show()","5e038ddc":"lasso = Lasso(alpha=.0005).fit(X_train,y)\nscores = cross_val_score(lasso,X_train,y,scoring='neg_mean_squared_error',cv=5)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)","37abdf17":"plot_learning_curves(lasso,X_train,y)\nplt.show()","d9423e0b":"preds = lasso.predict(X_test)\nlasso_predictions = pd.DataFrame(np.exp(preds), test_id, columns=['SalePrice']).to_csv('lasso_predictions.csv')","597dd8a3":"Hip hip! we're are done. NO MISSING VALUES! Now, we take care of our categorical columns.\n\nTime to bring justice to the East and West of our data, and handle the skewed data with the** Log** power! ","8eb6a864":"#### In this kernel, I summed up everything I've learnt to understand this competition and solve the problem. You will learn from this kernel how to visualize, clean and prepare the data to train you machine leanrnig model, and even select the best model for the problem with the help of the learning curves.","293e8194":"As you see above, each category will be a column(feature) and will be 1 if the following instance has that feature and 0 if it has not.\nSo, let's do this to all the data.","a55a1e6a":"# Data Analysis & Regularized Linear Regression ","e7150e28":"This is interesting. The model perfroms well when the ratio is 1, which means it is Lasso regression. \n\n\nLet's test a Lasso model with alpha equal to .1 and look at the learning curves. ","d5d7b3c2":"We have 81 features(columns) of types object(43), int(35), and float(3), and with a closer look you will notice that some of them have missing values(the non- null values aren't 1460). So, along with the **categorical columns**, we will need to handle the **missing values**.\n\nLet's see how our target variable, **SalePrice**,  is correlated with some other varibles in our data.","ea43fc76":"Before we carry on, let's delete our target **SalePrice**, and the **Id** column and save them for later, and concatenate the training data and test data\ntogether for a bit.","acf5c171":"Well, it seems that some of the columns are **numeric** and others not so, let's look at the **non-numeric** first.","8c0cddc7":"WOW! it improved! Let's look at the learning curve.","87fb059f":"Let's import here some needed libraries.","5ef7b49d":"Nice, we have the data. Let's have a quick look at it.","156b2159":"This time we trained our model with a **big alpha**(learning rate), so in our learning curve you see that our model is not fitting\nthe data well(the red line represents the train set, and it tells us whether our model is **overfitting or underfitting** the data).\n\nThe red line starts with error 0, meaning that our model fits the data well when it starts with some instances, but when it's given more instances,  it underfits the data. \n\nThe blue line shows how our model generalizes on the validation set, data it's never seen before. Also, in our curve we a big error on the validation set. So, our model performs poorly both on the training set and the validation set.\n\nThis happed beacuse the alpha the we trained our lasso model with is big. Alpha is resposible for this '**how much you want to regularize your model?**', so the bigger alpha, the more you model will decrease the features'coefficients to zero, and the model will be have then less features that will make it a **high bias model** or just simple model, model cannot fit the data.\n\n\nLet's train our model with very **small alpha** and see what get.","0a4d0d60":"\nTime to fill in the **non-numeric **ones.","4d2baf8e":"That was super fast LOL. Let's check again to make sure we don't have missing values.","61ff1348":"And load the data which is splitted into two files, one for** trianing** and another for **testing**.","1397ce01":"Let's import first some libraries.","599768b4":"Yay! we're done!...knowing how to clean our data for a machine learning model!\n\nHere what we're gonna do:\n \n*  **Handling the missing data.**\n*  **Transforming the data with Log.**\n*  **Getting Dummy variables. **","02084eee":"Well, not a bad score for the first attempt! Let's fine-tune the model and look at the **learning curve**.","114f4f38":"We will fill the missing values for **BsmtHalfBath, BsmtFullBath, BsmtFinSF2, GarageCars** with zero because I can see that some of the \ninstances have zero also which mean that instance(house) has no value for that feature(Bsmt)\n\nWe fill the missing values for **GarageArea,TotalBsmtSF, MasVnrArea, GarageYrBlt, BsmtFinSF1, LotFrontage, BsmtUnfSF** with the median of each column since the missing\nvalue could've just been a close number to its friends.","eecfff67":"#### Well, what then? we need to predict the house prices and all what we have is the data! So, let's look at it to familiarize ourselves and know what is needed to be done to prepare the data for the machine learning part. \n\n**Get motivated and believe me after this kernel you will have superpowers!... along with some fun! **","cc4ffe86":"Lots of figurs? Not really LOL. \n\nDon't get lost. Look at the** 3rd row, the plot in the middle** represents the scatter** between 'SalePrice', \nand 'GrLivArea'.** Noticed something? the two points** between 4000 and 6000 on the x-axis**? these two points probably think they are diffrent\nor even smarter from their mates, but they had a bad end. They were sold with a** low price** althoug they claim they are** big areas(houses)**!  \nSo, we can't have such arrogant points in our data! They do not even go with the flow like the other two points in the top right of the graph. We will delete them! why? because **outliers** cause problems when applying **statistical tests**('regression; in this problem), and they are probably errors.","03751e03":"We're finally have our data ready for our models, but we need to **get_dummies** for our **categorical features** first because the models need numbers not categories.\n\nFor Example, the feature 'PoolQC' is a categorical feature. it's filled with 4 values, (Ex, none,..etc)  when we get dummies for it. the following will happen:","cb8dbd31":"Well, that's not everything you get from these scatters. Look at the **skewed** data( 3rd row, top right). Wait! Lets Look at it closely.","8757d25d":"Before this, let us look first at an **ElasticNet** model which is a regularized model the lives in the middle between Ridge and Lasso.","d8dcd942":"The **skewed data **has no justice! it either gives more money(points) to the people of the East or the West! it gives one, \nand abuse another. Such injustice! We will force it to spread its points equally using some sort of powers(**Log Transformation**).\n\nHmm, you think we're nice, right? No, Not really. We did this for our problem's sake(**regression**) Since This can be valuable both \nfor making patterns in the data more interpretable, and for helping to** meet the assumptions** of some statistical test(Regression).","35e6cf38":"The kernel is over! I had fun writing this kernel and learned a lot. I hope you did as well. \n\nIf you want more if this, go check these kernels as well because they are really good and helped me a lot.\n\n*  [stacked-regressions-top-4-on-leaderboard](http:\/\/https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) \n* [comprehensive-data-exploration-with-python](http:\/\/https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n\nThank you! \n","4d64d1f8":"l1_ratio parameter is used to control the mix ratio of Ridge and Lasso. Let us do some grid search with some values. ","d03df73f":"Okay, so most of these features are yes-no questions, for instance, to fill the missing values for the feature 'PoolQC' you have \nto ask yourself this question '**Does the following house have a pool?**'. Therefore, We're gonna fill the** missing values** with 'none'\nmeaning that the following house doesn't have that feature. Fun! Let's none these values.","c2bbbff4":"So, we now know a little bit about our data(contains **categorical features**), and need to go deep! Let's get more info about our features(columns).","c95a5f51":"**Models**","1c268aea":"Very good!\n\nNow we handle the **missing data**.","be8fa6e4":"Now, lets predict the prices and save them into a file to be submitted.","7bd73c4a":"Okay, now our model fits and generalizes better. Still we have an error on both the training set and the validation set that can be improved **by feeding our model with more training examples**. ","f66aa9ed":"Awesome! let's pick the some of the **highly correlated** columns with our **SalePrice** column,  then have a scatter party, where we drink **outliers**! \n","acb3e7b1":"We've arrived safely to the second part. Congrats!\n\nTime to seprate out train and test set since we're done processing the data.","fd4f41be":"We're going to train a lasso model. **Lasso** is simply a regularized linear regression model. Lasso also selects\nthe most important features, so it does the **feature selection** process for us. Then we will evaluate it using cross validation, and look at some learning curves as well."}}