{"cell_type":{"d056489d":"code","57940d29":"code","8858345b":"code","7e58f9e7":"code","bd3ae13e":"code","ca716985":"code","3578bb48":"code","f82fa1a3":"code","75291e76":"code","fc5bf18a":"code","1aac0bb0":"code","97da03fc":"code","d725fb71":"code","117e3d4c":"code","805eccda":"code","c7c36efb":"code","02b0b9e9":"code","3a2821e9":"code","d6a64c32":"code","1ee55dc9":"code","56366e24":"code","13d32040":"code","31bb3e93":"code","e6ec1f59":"code","ba3d7f27":"code","179fb48b":"code","249c3435":"code","9f8e13f0":"code","554adbe0":"markdown","4533d86d":"markdown","8c396580":"markdown","d34c3bef":"markdown","84076bd1":"markdown","bc543ce7":"markdown","fe72e5e0":"markdown","bc560f04":"markdown","d6d0b983":"markdown","c9f8d46f":"markdown","37128fa5":"markdown","9a64a44d":"markdown","86918d90":"markdown","09d80a84":"markdown","03a1489a":"markdown","4df03104":"markdown","40278c8c":"markdown","beb89456":"markdown"},"source":{"d056489d":"#Keras library for CIFAR dataset\nfrom keras.datasets import cifar10","57940d29":"#Downloading the CIFAR dataset\n(x_train,y_train),(x_test,y_test)=cifar10.load_data()","8858345b":"#importing other required libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils.multiclass import unique_labels\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras import Sequential\nfrom keras.applications import VGG19,ResNet50 ##VGG19 and RsNet50 for Transfer Learning\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import SGD,Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\nfrom keras.utils import to_categorical","7e58f9e7":"W_grid=5\nL_grid=5\nfig,axes = plt.subplots(L_grid,W_grid,figsize=(10,10))\naxes=axes.ravel()\nn_training=len(x_train)\nfor i in np.arange(0,L_grid * W_grid):\n    index=np.random.randint(0,n_training) \n    axes[i].imshow(x_train[index])\n    axes[i].set_title(y_train[index]) \n    axes[i].axis('off')\nplt.subplots_adjust(hspace=0.4)","bd3ae13e":"#defining training and test sets\nx_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)","ca716985":"#Dimension of the dataset\nprint((x_train.shape,y_train.shape))\nprint((x_val.shape,y_val.shape))\nprint((x_test.shape,y_test.shape))","3578bb48":"#Onehot Encoding the labels.\n#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\ny_train=to_categorical(y_train)\ny_val=to_categorical(y_val)\ny_test=to_categorical(y_test)","f82fa1a3":"#Verifying the dimension after onehot encoding\nprint((x_train.shape,y_train.shape))\nprint((x_val.shape,y_val.shape))\nprint((x_test.shape,y_test.shape))","75291e76":"#Image Data Augmentation\ntrain_generator = ImageDataGenerator(\n                                    rotation_range=2, \n                                    horizontal_flip=True,\n                                    zoom_range=.1 )\n\nval_generator = ImageDataGenerator(\n                                    rotation_range=2, \n                                    horizontal_flip=True,\n                                    zoom_range=.1)\n\ntest_generator = ImageDataGenerator(\n                                    rotation_range=2, \n                                    horizontal_flip= True,\n                                    zoom_range=.1) ","fc5bf18a":"#Fitting the augmentation defined above to the data\ntrain_generator.fit(x_train)\nval_generator.fit(x_val)\ntest_generator.fit(x_test)","1aac0bb0":"#Learning Rate Annealer\nlrr= ReduceLROnPlateau(\n                       monitor='val_acc', #Metric to be measured\n                       factor=.01, #Factor by which learning rate will be reduced\n                       patience=3,  #No. of epochs after which if there is no improvement in the val_acc, the learning rate is reduced\n                       min_lr=1e-5) #The minimum learning rate ","97da03fc":"#Defining the VGG Convolutional Neural Net\nbase_model = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])","d725fb71":"#Adding the final layers to the above base models where the actual classification is done in the dense layers\n\nmodel= Sequential()\nmodel.add(base_model) #Adds the base model (in this case vgg19 to model)\nmodel.add(Flatten()) #Since the output before the flatten layer is a matrix we have to use this function to get a vector of the form nX1 to feed it into the fully connected layers","117e3d4c":"#Model summary\nmodel.summary()","805eccda":"#Adding the Dense layers along with activation and batch normalization\nmodel.add(Dense(1024,activation=('relu'),input_dim=512))\nmodel.add(Dense(512,activation=('relu'))) \nmodel.add(Dense(256,activation=('relu'))) \n#model.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\nmodel.add(Dense(128,activation=('relu')))\n#model.add(Dropout(.2))\nmodel.add(Dense(10,activation=('softmax'))) #This is the classification layer","c7c36efb":"#Checking the final model summary\nmodel.summary()","02b0b9e9":"#Defining the parameters\nbatch_size= 100\nepochs=20\nlearn_rate=.001\n\nsgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\nadam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)","3a2821e9":"#Compiling the model\n#During model compiling the 3 main things we specify are loss function,optimizer and the metrics that need to be evaluated during the test and train processes.\n#Lets start by using the SGD optimizer\n#We will specify the loss as categoricl crossentropy since the labels are 1 hot encoded. IF we had integer labels,we'd have to use sparse categorical crossentropy as loss function.\nmodel.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])","d6a64c32":"#Training the model\nmodel.fit_generator(train_generator.flow(x_train,y_train,batch_size=batch_size),\n                      epochs=epochs,\n                      steps_per_epoch=x_train.shape[0]\/\/batch_size,\n                      validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),validation_steps=250,\n                      callbacks=[lrr],verbose=1)","1ee55dc9":"#Plotting the training and valiation loss\n\nf,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n\n#Assigning the first subplot to graph training loss and validation loss\nax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\nax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n\n#Plotting the training accuracy and validation accuracy\nax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\nax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')","56366e24":"#Making prediction\ny_pred=model.predict_classes(x_test)\ny_true=np.argmax(y_test,axis=1)","13d32040":"L = 4\nW = 4\nfig, axes = plt.subplots(L, W, figsize = (12,12))\naxes = axes.ravel()\n\nfor i in np.arange(0, L * W):  \n    axes[i].imshow(x_test[i])\n    axes[i].set_title(f\"Prediction Class = {y_pred[i]:0.1f}\\n True Class = {y_true[i]:0.1f}\")\n    axes[i].axis('off')\n\nplt.subplots_adjust(wspace=0.5)","31bb3e93":"#Defining function for confusion matrix plot\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    fig, ax = plt.subplots(figsize=(7,7))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)","e6ec1f59":"#Plotting the confusion matrix\nconfusion_mtx=confusion_matrix(y_true,y_pred)","ba3d7f27":"class_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']","179fb48b":"# Plotting non-normalized confusion matrix\nplot_confusion_matrix(y_true, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')","249c3435":"# Plotting normalized confusion matrix\nplot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')","9f8e13f0":"from sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred))","554adbe0":"#Prepare the dataset","4533d86d":"#Transfer Learning on CIFAR-10 Classificatio\n\nIt will be implemented in the following steps:-\n* Downloading the CIFAR-10 dataset\n* Importing the required libraries\n* Preparing the dataset\n* Defining the VGG transfer learning model\n* Training the VGG model\n* Making Predictions\n* Evaluating performance","8c396580":"[ReduceLROnPlateau](https:\/\/keras.io\/api\/callbacks\/reduce_lr_on_plateau\/)","d34c3bef":"[Keras Deep (Transfer) Learning Models](https:\/\/keras.io\/api\/applications\/)","84076bd1":"Learning rate annealer to update the learning rate","bc543ce7":"[CIFAR-10 Dataset](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html)","fe72e5e0":"Classification Report","bc560f04":"#Evaluating the performance","d6d0b983":"#Making predictions","c9f8d46f":"[Keras CIFAR-10 Load Function](https:\/\/keras.io\/api\/datasets\/cifar10\/)","37128fa5":"![alt text](https:\/\/miro.medium.com\/max\/2268\/1*CrjJwSX9S7f759dK2EtGJQ.png)","9a64a44d":"#Train the model","86918d90":"Confusion matrix","09d80a84":"#Define the model","03a1489a":"#Import the required libraries","4df03104":"#Download the data","40278c8c":"#Reference Reading\n[Practical Comparison of Transfer Learning Models in Multi-Class Image Classification](https:\/\/analyticsindiamag.com\/practical-comparison-of-transfer-learning-models-in-multi-class-image-classification\/)","beb89456":"Visualize training performance"}}