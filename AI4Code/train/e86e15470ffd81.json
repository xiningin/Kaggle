{"cell_type":{"8329bc2c":"code","b1c43ca3":"code","09f2963c":"code","40795abf":"code","2a169a6b":"code","4aa56f69":"code","9c8489ef":"code","5ecb5c64":"code","7f832eea":"code","f6b804ac":"code","634fca05":"code","0a536b36":"code","5f71902d":"code","66d092d9":"code","1958ccb6":"code","ad88f6b7":"code","3fa1142c":"code","2a975936":"code","25eba837":"code","5ec26d6d":"code","789fefe2":"code","50d11bbf":"code","205eefed":"code","d95f91b4":"code","5550a791":"code","6f5dcdd3":"code","ec20286b":"code","29022959":"code","90ce1f82":"code","ab641c02":"code","bd5e117f":"code","56a61ced":"markdown","5dad987d":"markdown","74d6e358":"markdown","35c56f9c":"markdown","6465d16d":"markdown","3bd91fbb":"markdown","9f8651a1":"markdown","b6c263f1":"markdown","da2754e4":"markdown","1dd8c122":"markdown","03339f9a":"markdown","754ebde1":"markdown","4de6a10d":"markdown","07786c51":"markdown","6c37dc42":"markdown","afdaf90f":"markdown","e3225566":"markdown","c5c8dcdb":"markdown","3b9071e1":"markdown","e6873493":"markdown","15710f84":"markdown","a7424d9f":"markdown","6b420076":"markdown","ca145aee":"markdown","0a5ba907":"markdown","0b75b7fc":"markdown","c2b8227b":"markdown","23e24aee":"markdown"},"source":{"8329bc2c":"! pip install datasets transformers","b1c43ca3":"import transformers\n\nprint(transformers.__version__)","09f2963c":"from datasets import load_dataset\nfrom datasets import Dataset","40795abf":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import default_data_collator\nfrom transformers import BertTokenizer, pipeline","2a169a6b":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","4aa56f69":"model_checkpoint = \"mrm8488\/bert-tiny-5-finetuned-squadv2\"\nbatch_size = 4\n\nmax_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n","9c8489ef":"df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv').sample(frac = 1)\ndf","5ecb5c64":"df_hindi = df[df['language'] == 'hindi'].head(368)\ndf_hindi","7f832eea":"df_tamil = df[df['language'] == 'tamil']\ndf_tamil","f6b804ac":"df = pd.concat([df_hindi,df_tamil])\ndf","634fca05":"\ndef convert_to_dataset(DF): \n    data = {'answers':[],'context':[],'id':[],'question':[],'title':[]}\n\n    for i in range(len(DF)):\n\n        row = DF.iloc[i]\n        data['answers'].append({'answer_start': [row['answer_start']], 'text': [row['answer_text']]})\n        data['context'].append(row['context'])\n        data['id'].append(row['id'])\n        data['question'].append(row['question'])\n        data['title'].append('NA')\n\n\n    dataset = Dataset.from_dict(data)\n\n    return dataset\n","0a536b36":"train, test = train_test_split(df, test_size=0.1)\n\ntrain_dataset = convert_to_dataset(train)\ntest_dataset = convert_to_dataset(test)","5f71902d":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","66d092d9":"pad_on_right = tokenizer.padding_side == \"right\"","1958ccb6":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","ad88f6b7":"\ntrain_tokenized_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)","3fa1142c":"\ntest_tokenized_dataset = test_dataset.map(prepare_train_features, batched=True, remove_columns=test_dataset.column_names)","2a975936":"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","25eba837":"model_name = model_checkpoint.split(\"\/\")[-1]\n\nargs = TrainingArguments(\n    f\"test-squad\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=1, \n    weight_decay=0.01,\n)","5ec26d6d":"data_collator = default_data_collator","789fefe2":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=train_tokenized_dataset,\n    eval_dataset=train_tokenized_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","50d11bbf":"trainer.train()","205eefed":"trainer.save_model(\"test-squad-trained\")","d95f91b4":"!mkdir tokenizer \n!mkdir model","5550a791":"import shutil  \nshutil.move('.\/test-squad-trained\/config.json','.\/model\/config.json')\nshutil.move('.\/test-squad-trained\/pytorch_model.bin','.\/model\/pytorch_model.bin')","6f5dcdd3":"import os \nos.rename('.\/test-squad-trained','.\/tokenizer')","ec20286b":"tokenizer = BertTokenizer.from_pretrained(\".\/tokenizer\")","29022959":"model = AutoModelForQuestionAnswering.from_pretrained(\".\/model\")","90ce1f82":"nlp = pipeline('question-answering', model=model, tokenizer=tokenizer,device = 0)","ab641c02":"\ndata = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nSUB = pd.DataFrame(columns = ['id','PredictionString'])\n\nfor id_,C,Q,lan in data[[\"id\",\"context\", \"question\",\"language\"]].to_numpy():\n    \n    result = nlp(context=C, question=Q)    \n    SUB.loc[len(SUB.index)] = [id_,result['answer']] \n    \nSUB","bd5e117f":"SUB.to_csv('submission.csv', index=False)","56a61ced":"### In order to use the given data set we need to convert the pandas data frame into hugging faces dataset object, which is done by the convert_to_dataset function.\n* loop through the dataset \n* represent the datapoint in the required format ( data dict )\n* convert the data dict to hugging faces dataset object\n","5dad987d":"### As we know that there are a lot of data points of Hindi language and fewer data points for Tamil, here we will be using equal no. of data points for both the languages.\n","74d6e358":"### We first load the model using AutoModelForQuestionAnswering.from_pretrained function and then we fine-tune the model on our data.\n","35c56f9c":"## SETUP","6465d16d":"\n\n### In this notebook we will demostrate the process of fine tuneing the pretrained hugging faces transformer model, I hope this notebook will help to understand the use of pretrained transformer model in the context of this compitation.\n\n### In my previous notebook([here](https:\/\/www.kaggle.com\/vaibhavrmankar\/simple-start-eda-submission)) I have done the EDA, if you are new in the competition you might wanna look into the notebook for a the understanding of the problem statement and given data.","3bd91fbb":"### We need to chnage the file structure, for esly loading the model and tokenizer.","9f8651a1":"### This notebook is build using the example notebook provided by hugging faces.\n### Ref :[here](https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb)\n\n","b6c263f1":"### We use the trainer for model training.","da2754e4":"### Once we get to good enough accuracy we save the model for future use ","1dd8c122":"## Use fine-tuned model for Submission \n","03339f9a":"### Iterate through the submission dataset and add the predictions.\n","754ebde1":"## load the data","4de6a10d":"#### Saved model can be used offline for the submission.\n","07786c51":"## Thank you for reading, Happy to hear any thoughts\/suggestions :) \n","6c37dc42":"### Now we have prepared our data for the given task. We can start fine-tuning the model.\n","afdaf90f":"# **fine-tune the transformer model for our use case**\n","e3225566":"## Fine-tuning the model","c5c8dcdb":"### Split the data into train and test.","3b9071e1":"## Preprocess data","e6873493":"## Which Question answering model to fine-tune ? ","15710f84":"### After we have saved the model we need to use the model for generating the output.","a7424d9f":"### data preprocessing is the important step which includes the use of a tokenizer from the pre-trained model.\n\n### To do this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which ensures:\n\n* we get a tokenizer that corresponds to the model architecture we want to use,\n* we download the vocabulary used when pretraining this specific checkpoint.\n \n","6b420076":"### Note: if you get the error message after running the following cell, please restart the kernel.\n","ca145aee":"### Load the saved model useing from_pretrained method.","0a5ba907":"### There are several question-answering models to think of, We can try out different models and compare the results.\n","0b75b7fc":"### Setup the question-answering pipeline.","c2b8227b":"### we need to install the huggingface Datasets.","23e24aee":"### We use the default data collector to batch our processed examples together.\n"}}