{"cell_type":{"40bf3767":"code","f5f923f1":"code","c8e4a9e0":"code","c447f77d":"code","244074c7":"code","70f5d882":"code","eb43c058":"code","4014cd50":"code","4a037bcb":"code","33b08c6f":"code","8f9d19dc":"code","ecc0fef4":"code","a210712a":"code","d83ab1ac":"code","3adf75da":"code","b3139d9e":"code","5b949052":"code","2921e8f4":"code","d089bcaa":"code","3d904dbb":"code","9ad768ba":"code","f3b72a8f":"code","82b9b47b":"code","c4604939":"code","31570ad4":"code","534ec2fc":"code","68bde32b":"code","2365ca70":"code","5c4f346f":"code","92153693":"code","34acea35":"code","78dd614e":"code","9b19c235":"code","07f8828e":"code","7e6f641a":"code","76a12530":"code","73b916d0":"code","d018a927":"code","ee1552fe":"code","37569930":"code","d6040af6":"code","c55ac6d3":"code","a64cc7d9":"code","dbfb62e7":"code","d6646932":"code","dc21d4df":"code","b467089f":"code","806ffad3":"code","c83f0f2c":"code","fbff3bb4":"code","800a93ca":"code","fbd339cd":"code","9801f690":"code","31697d43":"code","ebb8613c":"code","dfcc0f1f":"code","d2a27ae8":"code","a9f448ae":"code","5471abce":"code","bd501da3":"code","52f18171":"code","780aea20":"code","1aae1472":"code","91c352b8":"code","bcc3eae0":"code","b40584da":"code","dcdd999e":"markdown","074779eb":"markdown","94f2a4f5":"markdown","3f26a3af":"markdown","fcb80cfc":"markdown","8662f20d":"markdown","e09e0557":"markdown","647d475e":"markdown","74ab2d76":"markdown","75e2be4d":"markdown","4642e490":"markdown","ba695b32":"markdown","697d83c4":"markdown","12797029":"markdown","cecee3b9":"markdown","d808d348":"markdown"},"source":{"40bf3767":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport seaborn as  sns\nfrom sklearn.metrics import r2_score\n\nimport tensorflow as tf\n\n\n","f5f923f1":"df0 = pd.read_csv(\"..\/input\/covid-19-cases\/Dataset\/data\/CONVENIENT_global_confirmed_cases.csv\")\ndf1 = pd.read_csv(\"..\/input\/covid-19-cases\/Dataset\/data\/CONVENIENT_global_deaths.csv\")","c8e4a9e0":"countries = df0.iloc[:,1:].columns\ncountries","c447f77d":"world = pd.DataFrame({\"Country\":[],\"Cases\":[]})\n# Pass all the countrie to the world dataframe\nworld['Country'] = df0.iloc[:,1:].columns\ncases = []\nfor i in world['Country']:\n    cases.append(pd.to_numeric(df0[i][1:]).sum())\nworld['Cases'] = cases\n\ncountry_list = list(world['Country'].values)\nidx = 0\nfor i in country_list:\n    sayac = 0\n    for j in i:\n        if j==\".\":\n            i = i[:sayac]\n            country_list[idx]=i\n        elif j==\"(\":\n            i = i[:sayac-1]\n            country_list[idx]=i\n        else:\n            sayac+=1\n    idx += 1\nworld['Country'] = country_list\nworld = world.groupby('Country')['Cases'].sum().reset_index()\nworld.head()","244074c7":"continent = pd.read_csv(\"..\/input\/covid-19-cases\/Dataset\/continents\/continents2.csv\")\ncontinent[\"name\"] = continent[\"name\"].str.upper()\ncontinent.head()","70f5d882":"world.head()","eb43c058":"world['Cases Range'] = pd.cut(world['Cases'],[10000,50000,200000,800000,1500000,15000000],labels=[\"U50K\",\"50kto200k\",\"200kto800k\",\"800kto1.5M\",\"1.5M+\"])\n","4014cd50":"alpha = []\nfor i in world['Country'].str.upper().values:\n    if i == \"BRUNEI\":\n        i = \"BRUNEI DARUSSALAM\"\n    elif i == \"US\":\n        i = \"UNITED STATES\"\n    if len(continent[continent[\"name\"] == i][\"alpha-3\"].values)==0:\n        alpha.append(np.nan)\n    else:\n        alpha.append(continent[continent[\"name\"]==i][\"alpha-3\"].values[0])\nworld[\"Alpha3\"]=alpha","4a037bcb":"world.head()","33b08c6f":"world['Country'] = world['Country'].str.upper()\nworld.head()","8f9d19dc":"world.isna().sum()","ecc0fef4":"fig = px.choropleth(world.dropna(),\n                   locations='Alpha3',\n                   color='Cases Range',\n                   projection='mercator',\n                   color_discrete_sequence=['khaki','yellow','lightblue','red','orange'])\nfig.update_geos(fitbounds='locations',visible=False)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\nplt.show()","a210712a":"# Daily cases all around the world\ncount = []\nfor i in range(1,len(df0)):\n    count.append(sum(pd.to_numeric(df0.iloc[i,1:].values)))\n\ndf = pd.DataFrame()\ndf['Date'] = df0['Country\/Region'][1:]\ndf['Cases'] = count\ndf = df.set_index('Date')\n\n# Daily death cases all around the world\ncount = []\nfor i in range(1,len(df1)):\n    count.append(sum(pd.to_numeric(df1.iloc[i,1:].values)))\n\ndf['Deaths'] = count\n\ndf.head()","d83ab1ac":"# remove decimal values\npd.set_option('precision',0)\ndf.head()","3adf75da":"# Daily covid19 cases\n\nplt.ticklabel_format(style='plain')\ndf.Cases.plot(title='Daily Covid19 Cases in World',marker=\".\",figsize=(10,8),label=\"Daily cases\")\ndf.Cases.rolling(window=5).mean().plot(figsize=(25,5),label='MovingAverage(5)')\nplt.ylabel(\"Cases\",fontsize=15)\nplt.xlabel(\"Date\",fontsize=15)\nplt.legend()\nplt.show();","b3139d9e":"fig = px.line(df, y='Cases',title='Daily Covid 19 Cases in World')\nfig.show();","5b949052":"# Daily covid19 Death Cases\ndf.Deaths.plot(title='Daily Covid19 Deaths in World', marker=\".\",label=\"Daily Deaths\")\ndf.Deaths.rolling(window=5).mean().plot(figsize=(25,5),label='MovingAverage(5)')\nplt.ylabel(\"Deaths\",fontsize=15)\nplt.xlabel(\"Date\",fontsize=15)\nplt.xticks(fontstyle='oblique',fontsize=10)\nplt.legend()\nplt.show();","2921e8f4":"fig = px.line(df, y='Deaths',title='Daily Covid 19 Death Cases in World')\nfig.show();","d089bcaa":"# parse dates from 'df' dataframe\nset_date = pd.to_datetime(df.index)\ndf.index = set_date","3d904dbb":"df.head()","9ad768ba":"# Get  data array\ntimesteps = df.index.to_numpy()\ncases = df['Cases'].to_numpy()\ndeaths = df['Deaths'].to_numpy()\n\ntimesteps[:10],cases[:10],deaths[:10]","f3b72a8f":"# Create train and test splits the right way for time series\nsplit_size = int(0.8 * len(df))\n\n# Create train data splits (everything before the split)\nX_train, y_train = timesteps[:split_size], cases[:split_size]\n\n# Create test data splits (everything after the split)\nX_test, y_test = timesteps[split_size:], cases[split_size:]\n\nlen(X_train),len(X_test), len(y_train), len(y_test)","82b9b47b":"# Plot correctly made splits\nplt.figure(figsize=(10, 7))\nplt.ticklabel_format(style='plain')\nplt.scatter(X_train, y_train, s=5, label=\"Train data\")\nplt.scatter(X_test, y_test, s=5, label=\"Test data\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cases\")\nplt.legend(fontsize=14)\nplt.show();","c4604939":"# Create a naive forecast\nnaive_forecast = y_test[:-1] # Naive forecast every value excluding the last value\nnaive_forecast[:10], naive_forecast[-10:]","31570ad4":"# Create a function to plot time series data\ndef plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):\n    \"\"\"\n    Plots a timesteps (a series of points in time) against values (a series of values across timesteps).\n\n    Parameters\n    ---------\n    timesteps : array of timesteps\n    values : array of values across time\n    format : style of plot, default \".\"\n    start : where to start the plot (setting a value will index from start of timesteps & values)\n    end : where to end the plot (setting a value will index from end of timesteps & values)\n    label : label to show on plot of values\n    \"\"\"\n    # Plot the series\n\n    plt.plot(timesteps[start:end], values[start:end], format, label=label)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Cases\")\n    if label:\n        plt.legend(fontsize=14) # make label bigger\n    plt.grid(True)","534ec2fc":"# Plot naive forecast\nplt.figure(figsize=(10, 7))\nplt.ticklabel_format(style='plain')\nplot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\nplot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")\nplot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\");","68bde32b":"import tensorflow as tf\n","2365ca70":"# MASE implemented\ndef mean_absolute_scaled_error(y_true, y_pred):\n    \"\"\"\n    Calculates the Mean Absolute Scaled Error\n    \"\"\"\n    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n\n    mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1]))\n\n    return mae\/mae_naive_no_season","5c4f346f":"# evaluate metrics function\ndef evaluate_preds(y_true, y_pred):\n    # Make sure float32 (for metric calculations)\n    y_pred = tf.cast(y_pred, dtype=tf.float64)\n\n    # Calculate various metrics\n    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n    rmse = tf.sqrt(mse)\n    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n    mase = mean_absolute_scaled_error(y_true, y_pred)\n\n    return {\"mae\": mae.numpy(),\n            \"mse\": mse.numpy(),\n            \"rmse\": rmse.numpy(),\n            \"mape\": mape.numpy(),\n            \"mase\": mase.numpy()}","92153693":"naive_results = evaluate_preds(y_true=y_test[1:],\n                               y_pred=naive_forecast)\nnaive_results","34acea35":"HORIZON = 7\nWINDOW_SIZE = 30\n","78dd614e":"# Create function to label windowed data\ndef get_labelled_windows(x, horizon=7):\n    \"\"\"\n    Creates an window and label for the data\n    \n    Example: Window: [1, 2, 3, 4,5, 6,7] -> [8]\n    \"\"\"\n    \n    return x[:,:-horizon], x[:,-horizon:]","9b19c235":"# Test out the window labelling function\ntest_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(30)+1,axis=0),horizon=HORIZON)\nprint(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")","07f8828e":"# Create function to view NumPy arrays as windows\ndef make_windows(x, window_size=30,horizon=7):\n    \"\"\"\n    Turns a 1D array into a 2D array of sequential windows of window size\n    \"\"\"\n    # Create a window of specific window_size(add the horizon on the end for later labelling)\n    window_step = np.expand_dims(np.arange(window_size+horizon),axis=0)\n    \n    # Create 2D array of multiple window steps (minus 1 to account for 0 indexing)\n    window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)),axis=0).T\n    \n    # Index on the target array(time series) with 2D array of multiple window steps\n    #\n    windowed_array = x[window_indexes]\n    \n    # Get the labelled windows\n    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n    \n    return windows, labels","7e6f641a":"full_windows, full_labels = make_windows(cases, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)","76a12530":"# View the first 3 windows\/labels\npd.set_option('precision',0)\nfor i in range(3):\n    print(f\"Window: {full_windows[i]} -> Label: {full_labels[i]}\")","73b916d0":"# make the train\/test splits\ndef make_train_test_splits(windows, labels, test_split=0.2):\n    \n    \"\"\"\n    Splits matching pairs of windows and labels into train and test splits\n    \"\"\"\n    split_size=int(len(windows)*(1-test_split))\n    train_windows = windows[:split_size]\n    train_labels = labels[:split_size]\n    test_windows = windows[split_size:]\n    test_labels = labels[split_size:]\n    return train_windows, test_windows, train_labels,test_labels","d018a927":"train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)","ee1552fe":"import os\n\n# Create a function to implement a ModelCheckpoint callback with a specific filename\ndef create_model_checkpoint(model_name, save_path='model_checkpoint'):\n    return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n                                             verbose=0,\n                                             save_best_only=True)","37569930":"from tensorflow.keras import layers, Sequential\n\n# set random seed for as reproducile results as possible\ntf.random.set_seed(42)\n\n# Model\nmodel_1 = Sequential(name='model_1_dense')\nmodel_1.add(layers.Dense(128, activation='relu'))\nmodel_1.add(layers.Dense(HORIZON,activation='linear'))\n\n\n# compile model\nmodel_1.compile(loss='mae',\n               optimizer=tf.keras.optimizers.Adam(),\n               metrics=['mae'])\n\n# Fit the model\nmodel_1.fit(x=train_windows,\n           y=train_labels,\n           epochs=100,\n           verbose=1,\n           batch_size=128,\n           validation_data=(test_windows,test_labels),\n           callbacks=[create_model_checkpoint(model_name=model_1.name)])","d6040af6":"# Evaluate modelon test data\nmodel_1.evaluate(test_windows, test_labels)","c55ac6d3":"# make preds\ndef make_preds(model,input_data):\n    \n    forecast = model.predict(input_data)\n    return tf.squeeze(forecast)","a64cc7d9":"# make prediction using model_1 on the test dataset\nmodel_1_preds = make_preds(model_1,test_windows)\nlen(model_1_preds), model_1_preds[:10]","dbfb62e7":"def evaluate_preds(y_true, y_pred):\n    \"\"\"\n    Calculates all the metrics used for Mean Absolute Error, Mean Squared Error,\n    Root Mean Squared Error, Mean Absolute Percentage Error, Mean Absolute Scaled Error\n    ----------------------------------------------------------------------------------\n    y_true = True labels \n    y_pred = Predicted labels\n    \n    \"\"\"\n    y_true = tf.cast(y_true,dtype=tf.float32)\n    y_pred = tf.cast(y_pred,dtype=tf.float32)\n    \n   # calculate various metric\n    mae = tf.keras.metrics.mean_absolute_error(y_true,y_pred)\n    mse = tf.keras.metrics.mean_squared_error(y_true,y_pred)\n    rmse = tf.sqrt(mse)\n    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n    mase = mean_absolute_scaled_error(y_true,y_pred)\n    \n    # Account for different sized metrics (for longer horizons, reduce to single number)\n    if mae.ndim > 0:\n        mae = tf.reduce_mean(mae)\n        mse = tf.reduce_mean(mse)\n        rmse = tf.reduce_mean(rmse)\n        mape = tf.reduce_mean(mape)\n        mase = tf.reduce_mean(mase)\n    return {'mae':mae.numpy(),\n           'mse':mse.numpy(),\n           'rmse':rmse.numpy(),\n           'mape':mape.numpy(),\n           'mase':mase.numpy()}","d6646932":"# Get model_3 results aggregated to single values\nmodel_1_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_1_preds)\nmodel_1_results","dc21d4df":"from tensorflow.keras.layers import Conv1D\n\n# set random seed\ntf.random.set_seed(42)\n\n# model_2\nmodel_2 = Sequential(name='model_2_conv1d')\nmodel_2.add(layers.Lambda(lambda x : tf.expand_dims(x,axis=1)) )\nmodel_2.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\nmodel_2.add(layers.Dense(HORIZON))\n\n# compile\nmodel_2.compile(loss='mae',\n               optimizer='adam',\n               metrics=['mae'])\n\n# fit\nmodel_2.fit(train_windows,\n           train_labels,\n           epochs=100,\n           validation_data=(test_windows,test_labels),\n           callbacks=[create_model_checkpoint(model_name=model_2.name)])","b467089f":"# evaluate on test data\nmodel_2.evaluate(test_windows,test_labels)","806ffad3":"# make prediction\nmodel_2_preds = make_preds(model_2,test_windows)\nmodel_2_preds[:10]","c83f0f2c":"# Evaluate metrics\nmodel_2_results = evaluate_preds(y_true=test_labels,\n                                y_pred=model_2_preds)\nmodel_2_results","fbff3bb4":"tf.random.set_seed(42)\n\n# Let's build an LSTM model with the Functional API\ninputs = layers.Input(shape=(WINDOW_SIZE))\nx = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) \nx = layers.LSTM(128, activation=\"relu\")(x) \noutput = layers.Dense(HORIZON)(x)\nmodel_3 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_3_lstm\")\n\n# Compile model\nmodel_3.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Seems when saving the model several warnings are appearing: https:\/\/github.com\/tensorflow\/tensorflow\/issues\/47554 \nmodel_3.fit(train_windows,\n            train_labels,\n            epochs=100,\n            verbose=0,\n            batch_size=128,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_3.name)])","800a93ca":"# Load in best version of model 5 and evaluate on the test data\nmodel_3 = tf.keras.models.load_model(\"model_checkpoint\/model_3_lstm\/\")\nmodel_3.evaluate(test_windows, test_labels)","fbd339cd":"# prediction\nmodel_3_preds = make_preds(model_3,test_windows)\nmodel_3_preds[:10]","9801f690":"# evaluate metrics\nmodel_3_results = evaluate_preds(y_true=test_labels,\n                               y_pred=model_3_preds)\nmodel_3_results","31697d43":"!pip install kats","ebb8613c":"# Importing the TimeseriesData class from kats\nfrom kats.consts import TimeSeriesData","dfcc0f1f":"# Utils to work with kats\nfrom dateutil import parser\nfrom datetime import datetime\n","d2a27ae8":"\n# Importing the things we need \nfrom kats.models.ensemble.ensemble import EnsembleParams , BaseModelParams \nfrom kats.models.ensemble.kats_ensemble import KatsEnsemble\nfrom kats.models import (\n    arima, \n    holtwinters , \n    linear_model , \n    prophet , \n    quadratic_model , \n    sarima , \n    theta\n)","a9f448ae":"\n# Defining the parameters of different models \nmodel_params = EnsembleParams(\n    [\n     BaseModelParams('arima' , arima.ARIMAParams(p = 1 , d=1 , q=1)) , \n     BaseModelParams('sarima' ,\n                     sarima.SARIMAParams(\n                         p = 2 , d= 2 , q =1 , trend = 'ct' , \n                     seasonal_order = (1, 0 ,1 ,12) , enforce_invertibility = False , \n                     enforce_stationarity = False),\n     ),\n     BaseModelParams('prophet' , prophet.ProphetParams()) , \n     BaseModelParams('linear' , linear_model.LinearModelParams()) , \n     BaseModelParams('quadratic' , quadratic_model.QuadraticModelParams()),\n     BaseModelParams('theta' , theta.ThetaParams()), \n    ]\n)","5471abce":"\n# Creating KatEnsembleParam with detailed configuration \nKatEnsembleParams = {\n    'models': model_params , \n    'aggregation': 'median' , \n    'seasonality_length': 7 , \n    'decomposition_method': 'multiplicative'\n}","bd501da3":"\n# Creating a Time Series dataset \ncovid_ts = TimeSeriesData(value = df.Cases,\n                            time = df.index , \n                            sort_by_time= True)","52f18171":"\n# Creating a KatEnsemble model (or) instantiating it \nensemble_model = KatsEnsemble(\n    data = covid_ts , \n    params = KatEnsembleParams\n)\n\n# Fitting the model \nensemble_model.fit()","780aea20":"# Making prediction for the next 60 days \nforecast = ensemble_model.predict(steps = 60)","1aae1472":"\n# Aggregate individual model results (we will get the predictions for 30 Days)\nensemble_model.aggregate()","91c352b8":"# Plotting the model \nensemble_model.plot()","bcc3eae0":"all_model_results = pd.DataFrame({\"Naive_Resutlts\":naive_results,\n                                 \"Simple Dense\": model_1_results,\n                                 \"Conv1D\":model_2_results,\n                                 \"RNN\":model_3_results}).T\nall_model_results","b40584da":"all_model_results['mae'].plot(figsize=(10,5),kind='barh',color='lightblue')\nplt.legend(fontsize=14)\nplt.yticks(fontsize=15)\nplt.grid(True)\nplt.show();","dcdd999e":"## Split dataset into Train and Test\n\nThe best way to split the time series data is to avoid the random_split\n","074779eb":"## Data Preparation","94f2a4f5":"### Introduction\n**Coronavirus disease 2019 (COVID-19)** is a contagious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first known case was identified in *Wuhan, China*, in December 2019. The disease has since spread worldwide, leading to an ongoing pandemic.\n\n### Symptoms\nSymptoms of COVID-19 are variable, but often include *fever, cough, headache, fatigue, breathing difficulties, and loss of smell and taste*.Symptoms may begin *one to fourteen days* after exposure to the virus. At least a third of people who are infected do not develop noticeable symptoms. Of those people who develop symptoms noticeable enough to be classed as patients, most (81%) develop mild to moderate symptoms (up to mild pneumonia), while 14% develop severe symptoms (dyspnea, hypoxia, or more than 50% lung involvement on imaging), and 5% suffer critical symptoms (respiratory failure, shock, or multiorgan dysfunction). Older people are at a higher risk of developing severe symptoms. Some people continue to experience a range of effects (long COVID) for months after recovery, and damage to organs has been observed. Multi-year studies are underway to further investigate the long-term effects of the disease.\n\n### Transmutation\nCOVID-19 transmits when people breathe in air contaminated by droplets and small airborne particles containing the virus. The risk of breathing these in is highest when people are in close proximity, but they can be inhaled over longer distances, particularly indoors. Transmission can also occur if splashed or sprayed with contaminated fluids in the eyes, nose or mouth, and, rarely, via contaminated surfaces. People remain contagious for up to 20 days, and can spread the virus even if they do not develop symptoms.\n\nSeveral testing methods have been developed to diagnose the disease. The standard diagnostic method is by detection of the virus' nucleic acid by real-time reverse transcription polymerase chain reaction (rRT-PCR), transcription-mediated amplification (TMA), or by reverse transcription loop-mediated isothermal amplification (RT-LAMP) from a nasopharyngeal swab.\n\nSeveral COVID-19 vaccines have been approved and distributed in various countries, which have initiated mass vaccination campaigns. Other preventive measures include physical or social distancing, quarantining, ventilation of indoor spaces, covering coughs and sneezes, hand washing, and keeping unwashed hands away from the face. The use of face masks or coverings has been recommended in public settings to minimize the risk of transmissions. While work is underway to develop drugs that inhibit the virus, the primary treatment is symptomatic. Management involves the treatment of symptoms, supportive care, isolation, and experimental measures.","3f26a3af":"## Import Data","fcb80cfc":"\n<div class=\"alert alert-block alert-success\">\n    <h1 align=\"center\">Covid-19 Cases<\/h1>\n    \n<\/div>","8662f20d":"## Make our evaluation function work for larger horizons","e09e0557":"## Data Viualization","647d475e":"## Model 3 : RNN(WINDOW=30,HORIZON=7)","74ab2d76":"**Create a dataframe containing `Country` and `Cases` columns in it**","75e2be4d":"## Model 4: Kats ","4642e490":"## Baseline Model : Naive Forecast\n\nAs usual, let's start with a baseline\n\nOne of the most common baseline models for time series forecasting, the naive model (also called the`naive forecast`), requires no training at all.\n\nThat's because all the naive model does is use the previous timestep value to predict the next timestep value\n\nThe formula looks like this\n","ba695b32":"## Import Libraries","697d83c4":"### Format Data : Windowing Dataset\n\nWindowing is a method to turn a time series dataset into **supervised learning problem**\n\nIn other words, we want to use windows of the past to predict the future\n\n```\nWindow for one month (univariate time series)\n\n[0, 1, 2, 3, 4, 5, 6] -> [7]\n[1, 2, 3, 4, 5, 6, 7] -> [8]\n[2, 3, 4, 5, 6, 7, 8] -> [9]\n\n```","12797029":"## Model 1: Dense Model(Window=30, horizon =7)","cecee3b9":"## Make a modelling checkpoint\n\nIn order for a fair comparison, we want to compare each model's best performance against each model's best performance against each model's best performance.\n\nFor example, if `model_1` performed incredibly well on epoch 55 but its performance fell off toward epoch 100, we want the version of the model's from epoch 55 to compare to other model's rahter than the version of the model from epoch 100.\n\nAnd the same goes for each of our other models:compare the best agoinst the best.\n\nTo take of this, we'll implement a `ModelCheckpoint` callback.\n\nThe `ModelCheckpoint callback` will monitor our model's performance during training and save the best model to file by setting `save_best_only=True`.\n\nThat way when evaluating our model we could restore its best performing configuration from file.\n\n\ud83d\udd11 **Note:** Because of the size of the dataset (smaller than usual), you'll notice our modelling experiment results fluctuate quite a bit during training (hence the implementation of the **ModelCheckpoint** callback to save the best model).","d808d348":"## Model 2 : Conv1D (WINDOW=30, HORIZON=7)"}}