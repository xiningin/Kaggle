{"cell_type":{"19b9a31c":"code","5e5fdae8":"code","a440717b":"code","3fa68f04":"code","d23d1512":"code","47b59901":"code","7891250c":"code","c0d30ecf":"code","f85d3535":"code","ae69dd86":"code","7b80e417":"code","3f02229b":"code","b4f3cb21":"code","59d928c3":"code","dffbeafb":"code","2d2cb134":"code","a417960c":"code","e5dd0446":"markdown","dc60f9dc":"markdown","a8bd85c0":"markdown","3bcd8005":"markdown","97084d3d":"markdown","dbbe1e57":"markdown","569ee0a9":"markdown","20bdea8d":"markdown","f9f1c504":"markdown","eb7ac764":"markdown"},"source":{"19b9a31c":"from sklearn import *\nimport pandas as pd\n\ndata_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ndata_test = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')","5e5fdae8":"data_train.head()","a440717b":"data_train.describe(include='all')","3fa68f04":"X_train = data_train.iloc[:, 1:-1].to_numpy()\ny_train = data_train.iloc[:, -1].to_numpy()\nX_test = data_test.iloc[:, 1:].to_numpy()\n\nfor i in (X_train, y_train, X_test): print(i.shape)  # sanity check","d23d1512":"scaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","47b59901":"def choose_model(models, X, y, verbose=False):\n    best_clf, best_score = None, None\n    for model in models:\n        clf = model_selection.GridSearchCV(model['clf'], model['params'], scoring='neg_log_loss', n_jobs=-1).fit(X, y)\n        if best_score is None or best_score < clf.best_score_: best_score, best_clf = clf.best_score_, clf.best_estimator_\n        if verbose: print(f\"{model['name']} got score {clf.best_score_}\" + (f\" with parameters {clf.best_params_}\" if model['params'] else \"\"))\n    return best_clf, best_score","7891250c":"models = (\n    {\n        'name'   : 'Gaussian Naive Bayes',\n        'clf'    : naive_bayes.GaussianNB(),\n        'params' : {}\n    },\n    {\n        'name'   : 'Logistic Regression',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('none',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Lasso',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('l1',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Ridge',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('l2',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Linear Discriminant Analysis',\n        'clf'    : discriminant_analysis.LinearDiscriminantAnalysis(),\n        'params' : {}\n    },\n    {\n        'name'   : 'Quadratic Discriminant Analysis',\n        'clf'    : discriminant_analysis.QuadraticDiscriminantAnalysis(),\n        'params' : {}\n    },\n#     {  # Takes forever!\n#         'name'   : 'K Nearest Neighbors',\n#         'clf'    : neighbors.KNeighborsClassifier(),\n#         'params' : {'algorithm' : ('ball_tree',), 'n_neighbors' : range(1, 10, 2)}  # Specifying the algorithm to prevent brute force from exploding.\n#                                                                                     # Selecting BallTree as it is considered better than KDTree in high dimensions.\n#     },\n    {\n        'name'   : 'Decision Tree',\n        'clf'    : tree.DecisionTreeClassifier(),\n        'params' : {'max_depth' : range(10, 101, 10)}\n    },\n    {\n        'name'   : 'Random Forests',\n        'clf'    : ensemble.RandomForestClassifier(),\n        'params' : {'max_depth' : range(10, 51, 10)}\n    },\n    {\n        'name'   : 'AdaBoost',\n        'clf'    : ensemble.AdaBoostClassifier(),\n        'params' : {}\n    },\n#     {  # Takes forever and ever!!! I guess SVM's bad reputation of bad scaling is well-earned!\n#         'name'   : 'Support Vector Machines',\n#         'clf'    : svm.SVC(),\n#         'params' : {'kernel' : ('linear', 'poly', 'rbf'), 'C' : range(1, 10, 2)}\n#     },\n    {\n        'name'   : 'Multi-layer Perceptron',\n        'clf'    : neural_network.MLPClassifier(),\n        'params' : {'hidden_layer_sizes' : range(10, 100, 10)}\n    }\n)","c0d30ecf":"best_clf, best_score = choose_model(models, X_train, y_train, verbose=True)","f85d3535":"# Re-tuning random forests as its optimum lies on the border.\nbest_clf, best_score = choose_model(\n    ({\n        'name'   : 'Random Forests',\n        'clf'    : ensemble.RandomForestClassifier(),\n        'params' : {'max_depth' : range(1, 11)}\n    },), X_train, y_train, verbose=True)","ae69dd86":"best_models = (\n    {\n        'name'   : 'Ridge',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('l2',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Linear Discriminant Analysis',\n        'clf'    : discriminant_analysis.LinearDiscriminantAnalysis(),\n        'params' : {}\n    },\n    {\n        'name'   : 'Random Forests',\n        'clf'    : ensemble.RandomForestClassifier(),\n        'params' : {'max_depth' : range(1, 11)}\n    }\n)","7b80e417":"pca = decomposition.PCA(n_components='mle').fit(X_train)","3f02229b":"pca.n_components_","b4f3cb21":"pca.explained_variance_ratio_","59d928c3":"pca.explained_variance_ratio_.sum()","dffbeafb":"X_reduced = pca.transform(X_train)\nbest_clf_reduced, best_score_reduced = choose_model(best_models, X_reduced, y_train, verbose=True)","2d2cb134":"# Choosing between the high- and low-dimensional settings automatically; for scripting purposes\nif best_score_reduced > best_score:\n    print('Choosing the low-dimensional setting...')\n    best_score = best_score_reduced\n    best_clf = best_clf_reduced\n    X_test = pca.transform(X_test)\nelse:\n    print('Sticking with the high-dimensional setting...')\n\nprint(f\"Estimated score = {best_score}\")","a417960c":"y_predict = best_clf.predict_proba(X_test)\ndata_predict = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\ndata_predict.iloc[:, 1:] = y_predict\ndata_predict.to_csv('submission.csv', index=False)","e5dd0446":"## Re-trying the best models in a lower-dimensional setting\n*MLP is excluded as it is probably best if tuned separately with dedicated neural network libraries supporting deeper models and accelerators (GPUs and TPUs).*","dc60f9dc":"## Streamlining models evaluations","a8bd85c0":"This is a classification problem with 200K samples, i.e. $n = 200,000$; each having 75 features (from `feature_0` to `feature_74`), i.e. $p = 75$.\nThere are $9$ possible classes for each observation.\n\nIn the following, the `neg_log_loss` score will be used as the competition evaluates submissions using multi-class logarithmic loss.","3bcd8005":"## Trying different models with all the features","97084d3d":"# CIE 632: Machine Learning Fundamentals - Spring 2021\n# Project - Due Date: June 26, 2021\n# Name: Muhammad Hamdy AlAref","dbbe1e57":"## Writing predictions","569ee0a9":"Evidently, Ridge (Logistic Regression with `l2` penalty), LDA, Random Forests and MLP (not-so-deep neural network) yield the best results with Random Forests being slightly better.","20bdea8d":"## Exploring the data","f9f1c504":"Looks like the variability in the data is spread across all the dimensions!","eb7ac764":"## Standardizing the features"}}