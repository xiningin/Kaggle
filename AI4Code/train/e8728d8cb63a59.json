{"cell_type":{"9a26aa8c":"code","ccb6af02":"code","a55e87dd":"code","a9274d08":"code","295a1d28":"code","f0840778":"code","8ee0a22c":"code","fa6726ce":"code","d1028a65":"code","2f0ca2c2":"code","bd6563c0":"code","9847f4d2":"code","bdaf45a4":"code","de976721":"code","63bd4c17":"code","f6936b6c":"code","1da9ab09":"code","b516fc8e":"code","3897ffa0":"code","490ac8c0":"code","8658f269":"code","75ad5aa3":"code","7bf23769":"code","0a5d7f2d":"code","08c73889":"code","e0b8617c":"code","b789a67e":"code","2a8d6ed6":"code","e5b98083":"code","1581fc9b":"code","d2361357":"code","2d6030f8":"code","5e99d62d":"code","1eee174f":"code","8a70e69d":"code","1ce560f6":"code","7f52a2c2":"code","3f59debf":"code","91d4cc15":"code","7eabb846":"code","83324083":"code","58df5345":"code","358f3e00":"markdown","5975ed71":"markdown","6677be53":"markdown","1ee3d066":"markdown","f180997a":"markdown","780052d9":"markdown","1020aae1":"markdown","9be1f6a4":"markdown","16c1a735":"markdown","7fa50c0a":"markdown","6272d3d0":"markdown","87c1908f":"markdown","5021223f":"markdown","16f899a3":"markdown","48d1ed21":"markdown","558a1fb1":"markdown","e4085a44":"markdown","a6ac33c5":"markdown","5ed73ff8":"markdown","be70995c":"markdown","b579b622":"markdown","fd47774d":"markdown","1896cdb0":"markdown","a4a2e663":"markdown"},"source":{"9a26aa8c":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","ccb6af02":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n#Looking at the closer details\ntrain.describe()","a55e87dd":"print(train.columns)","a9274d08":"#To see it on a graph\ntrain.sample(20)","295a1d28":"print(train.dtypes)","f0840778":"print(train.isnull().sum())","8ee0a22c":"sns.barplot(data= train, x=\"Sex\", y=\"Survived\")\nprint(\"Percentage of females surviving:\", train[\"Survived\"][train.Sex == \"female\"].value_counts(normalize=True)*100)\nprint(\"Number males surviving:\", train[\"Survived\"][train.Sex == \"male\"].value_counts(normalize = True)*100)","fa6726ce":"train[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"]= test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60,np.inf]\nlabels = [\"Unknown\",\"Small children\",\"Children\",\"Teens\",\"Adolescent\",\"Young Adults\",\"Adults\",\"Elders\"]\ntrain[\"AgeGroups\"] = pd.cut(train.Age,labels=labels, bins=bins)\ntest[\"AgeGroups\"] = pd.cut(test.Age,labels=labels, bins=bins)\nsns.barplot(data = train,x= \"AgeGroups\",y= \"Survived\")","d1028a65":"sns.barplot(data= train, x=\"Pclass\", y=\"Survived\")\nprint(\"Percentage of High earners surviving:\", train[\"Survived\"][train.Pclass == 1].value_counts(normalize=True)*100)\nprint(\"Percentage of middle class earners surviving:\", train[\"Survived\"][train.Pclass == 2].value_counts(normalize = True)*100)\nprint(\"Percentage of low class earners surviving:\", train[\"Survived\"][train.Pclass == 3].value_counts(normalize = True)*100)","2f0ca2c2":"#Family factors\nsns.barplot(data =train, x=\"SibSp\",y=\"Survived\")\nprint(\"Survivors with one sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Survivors with two sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"survivors with three sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\nprint(\"Survivors with four sibling\/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)","bd6563c0":"\nsns.barplot(data = train,x=\"Parch\",y=\"Survived\")","9847f4d2":"train[\"CabinBool\"] = (train.Cabin.notnull().astype(\"int\"))\ntest[\"CabinBool\"] =  (train.Cabin.notnull().astype(\"int\"))\nprint(\"Percentage of recorded Cabins that survived:\",train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of recorded Cabins that didn't survive:\",train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\nsns.barplot(data = train,x=\"CabinBool\",y=\"Survived\")","bdaf45a4":"test.describe(include=\"all\")","de976721":"train = train.drop([\"Ticket\"],axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n\ntrain = train.drop([\"Cabin\"],axis = 1)\ntest = test.drop([\"Cabin\"],axis = 1)\n\ntrain = train.drop([\"CabinBool\"],axis = 1)\ntest = test.drop([\"CabinBool\"],axis = 1)","63bd4c17":"print(\"Number of people embarking in Southampton (S):\",train[train[\"Embarked\"] == \"S\"].shape[0])\n\nprint(\"Number of people embarking in Cherbourg (C):\",train[train[\"Embarked\"] == \"C\"].shape[0])\n\nprint(\"Number of people embarking in Queenstown (Q):\",train[train[\"Embarked\"] == \"Q\"].shape[0])","f6936b6c":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","1da9ab09":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","b516fc8e":"#Replace the titles with numbers that would let out computer understand\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","3897ffa0":"title_map = {\"Master\":1,\"Miss\":2,\"Mr\":3,\"Mrs\":4,\"Rare\":5,\"Royal\":5}\nfor dataset in combine:\n    dataset[\"Title\"] = dataset[\"Title\"].map(title_map)\n    dataset[\"Title\"] = dataset[\"Title\"].fillna(0)\n\ntrain.head()","490ac8c0":"Master_age = train[train[\"Title\"]==1][\"AgeGroups\"].mode()#Small children (Somehow, don't ask me)\nMs_age = train[train[\"Title\"]==2][\"AgeGroups\"].mode() #Teens\nMr_age = train[train[\"Title\"]==3][\"AgeGroups\"].mode() #Young Adults\nMrs_age = train[train[\"Title\"]==4][\"AgeGroups\"].mode() #Adults\nRare_age = train[train[\"Title\"]==5][\"AgeGroups\"].mode() #Adult\nRoyal_age = train[train[\"Title\"]==6][\"AgeGroups\"].mode() #Adults\n#Demonstrating what I'm doing when using .mode\nprint(Master_age)\nprint(Ms_age)\nprint(Mrs_age)\ntrain.head()","8658f269":"title_age_map = {1: \"Small children\", 2: \"Teens\", 3: \"Young Adults\", 4: \"Adults\", 5: \"Adults\", 6: \"Adults\"}\n\nfor x in range(len(train[\"AgeGroups\"])):\n    if train[\"AgeGroups\"][x] == \"Unknown\":\n        train[\"AgeGroups\"][x] = title_age_map[train[\"Title\"][x]]\n\nfor x in range(len(test[\"AgeGroups\"])):\n    if test[\"AgeGroups\"][x] == \"Unknown\":\n        test[\"AgeGroups\"][x] = title_age_map[train[\"Title\"][x]]","75ad5aa3":"Age_map = {\"Small children\": 1, \"Children\": 2, \"Teens\": 3, \"Adolescent\": 4, \"Young Adults\": 5, \"Adults\": 6, \"Elders\": 7}\n\ntrain[\"AgeGroups\"] = train[\"AgeGroups\"].map(Age_map)\ntest[\"AgeGroups\"] = test[\"AgeGroups\"].map(Age_map)\ntrain.head()","7bf23769":"train = train.drop([\"Name\"], axis = 1)\ntest = test.drop([\"Name\"], axis = 1)\ntrain = train.drop([\"Age\"], axis = 1)\ntest = test.drop([\"Age\"], axis = 1)","0a5d7f2d":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","08c73889":"#map each Embarked value to a numerical value that can be read by the computer. Almost there!\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","e0b8617c":"print(train.Fare.max())\nprint(train.Fare.min())","b789a67e":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)\n","2a8d6ed6":"train.head()\n","e5b98083":"test.head()","1581fc9b":"from sklearn.model_selection import train_test_split\n\npredict = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predict, target, test_size = 0.22, random_state = 0)","d2361357":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","2d6030f8":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","5e99d62d":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","1eee174f":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","8a70e69d":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","1ce560f6":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","7f52a2c2":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","3f59debf":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","91d4cc15":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","7eabb846":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian ,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","83324083":"I decide to use the Gradient Boosting Classifier since it has a higher score overall.","58df5345":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","358f3e00":"**EMBARKED**","5975ed71":"Let's see if we have any missing values in any of the columns","6677be53":"Now that we are finally extracted everything we could from the name and the age columns, we can now get rid of them.","1ee3d066":"Finally, the fare feature.\nWe'll try and categorize this into more logical \"bins\"","f180997a":"# Contents\n\n1. Import Necessary Libraries\n2. Read In and Explore the Data\n3. Data Analysis\n4. Data Visualization\n5. Cleaning Data\n6. Choosing the Best Model\n7. Creating Submission File\n","780052d9":"We see that most embarked from Southhampton, so we can fill in the empty values with \"S\"","1020aae1":"**pclass Factor**","9be1f6a4":"**Time for predictions!**\nSex: Since people have the conception of \"Women and children first\", it's likely that more women survived than men.\nAge: Just thinking rationally would let us see that people who were younger (Not so young that they have to be carried by someone of course) than the most were likely to survive more aswell.\npclass: This is an interesting one. Higher fares may have let people get cabins from a higher part of the Titanic, which would've allowed easier acces to lifeboats.","16c1a735":"# Time to clean up data","7fa50c0a":"Now that we have all of the libraries in place, let's take a look at our data. We will first import it and read it with the commands: 'pd.csv_read' and 'describe()'","6272d3d0":"**Family factor**","87c1908f":"Embarked Feature","5021223f":"Testing Different Models\nI will be testing the following models with my training data (got the list from here):\n\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","16f899a3":"**Training Data**","48d1ed21":"**#Parch Factor**","558a1fb1":"As more siblings or spouses were present, the probability of survival was also lower. However it is interesting to note that people with 1 sibling\/spouse was mos likely to survive is a very interesting that people with no siblings or sposes were less likely to survive than the ones who had one or two.","e4085a44":"Sex Feature","a6ac33c5":"**Age Factor:**","5ed73ff8":"Let's take a look at how our values were stored shall we? Pandas has a simple built in function, 'dtypes', allowing us to very easily look at the data types.","be70995c":"# Data Analysis\n We can take a look at the graph as a whole to see the collumns and some variables with the 'head()' functions, or we can use the '.collumns' function to see the collumns on the screen","b579b622":"# Graphing\n\nIf you were bored from all the numbers and tables, this part may be more fun for you.","fd47774d":"Now that we filled in the missing values, let's turn those into numbers that our computer can process easily.","1896cdb0":"It seems like we have many missing values from our DataSet.\n16.38% of the age column is missing.\nA whooping 70% of the cabin values are missing.\nOnly 0.22% of the embarked column is missing which shouldn't hurt our graph too much. Age factor is important for the survival rate, so we should try and fill in those values as much as we can. Cabin values are mostly missing so dropping the table may be the wisest option. We can however figure that higher \"fares\" would equal to a higher cabin, therefore making a dependence graph can still give us an idea. It is not necessary but could be certainly interesting.\n\n","a4a2e663":"Cabin Feature Cabin features are a little bit tricky. They don't really mean anything, unless we assume that the ones recorded were people that were more important, or of a higher economic class."}}