{"cell_type":{"adf46152":"code","4f0ede45":"code","a348baf6":"code","76498b8a":"code","4bc244b2":"code","96c6c791":"code","ac710d43":"code","83d754eb":"code","e2e040d7":"code","480f4a57":"code","1d7b1727":"code","027004c5":"code","1a509135":"code","71e124c8":"code","9cde227c":"code","cb103ecb":"code","2f5d468d":"code","3df5026e":"markdown","c2e3430e":"markdown","898ec271":"markdown","cc38375b":"markdown"},"source":{"adf46152":"!pip uninstall -y kaggle\n!pip install --upgrade pip\n!pip install kaggle==1.5.6\n!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\/\n! chmod 600 ~\/.kaggle\/kaggle.json\n!ls -lha kaggle.json\n!kaggle -v\n!kaggle competitions download -c stay-or-leave\n","4f0ede45":"!unzip stay-or-leave.zip\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\nimport random\nfrom sklearn.preprocessing import MinMaxScaler","a348baf6":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nrandom.seed(777)\ntorch.manual_seed(777)\nif device == 'cuda':\n  torch.cuda.manual_seed_all(777)","76498b8a":"lr=1e-3\nepochs=50\nbatch_size=500","4bc244b2":"train_data=pd.read_csv('train_data.csv',header=None,skiprows=1,usecols=range(0,10))\ntest_data=pd.read_csv('test_data.csv',header=None,skiprows=1,usecols=range(0,9))","96c6c791":"x_train=train_data.loc[:,0:8]\ny_train=train_data.loc[:,[9]]\nx_train=np.array(x_train)\ny_train=np.array(y_train)\nprint(y_train)\ny_train[y_train == 0] = -1\nprint(y_train)\nscaler=MinMaxScaler()\nx_train=scaler.fit_transform(x_train)\n\nx_train=torch.FloatTensor(x_train)\ny_train=torch.LongTensor(y_train)","ac710d43":"train_dataset=torch.utils.data.TensorDataset(x_train,y_train)","83d754eb":"data_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n                                        batch_size=batch_size,\n                                        shuffle=True,\n                                        drop_last=True)","e2e040d7":"linear1=torch.nn.Linear(9,128,bias=True)\nlinear2=torch.nn.Linear(128,128,bias=True)\nlinear3=torch.nn.Linear(128,1,bias=True)\ntanh=torch.nn.Tanh()","480f4a57":"torch.nn.init.xavier_normal_(linear1.weight)\ntorch.nn.init.xavier_normal_(linear2.weight)\ntorch.nn.init.xavier_normal_(linear3.weight)","1d7b1727":"model=torch.nn.Sequential(linear1,tanh,\n                          linear2,tanh,\n                          linear3).to(device)","027004c5":"loss=torch.nn.MSELoss().to(device) \noptimizer=torch.optim.Adam(model.parameters(),lr=lr)","1a509135":"total_batch=len(data_loader)\nfor epoch in range(epochs):\n  avg_cost=0\n\n  for X,Y in data_loader:\n    X=X.to(device)\n    Y=Y.float().to(device)\n\n    optimizer.zero_grad()\n    hypothesis=model(X)\n    cost=loss(tanh(hypothesis), Y)\n    cost.backward()\n    optimizer.step()\n\n    avg_cost += cost\/total_batch\n\n  print('epoch:','%04d' % (epoch+1),'cost=','{:.9f}'.format(avg_cost))\nprint('learning finished')","71e124c8":"with torch.no_grad():\n  x_test=test_data.loc[:,:]\n  x_test=np.array(x_test)\n  x_test=scaler.transform(x_test)\n  x_test=torch.FloatTensor(x_test).to(device)\n\n  predict=model(x_test)","9cde227c":"predict[predict>0.5350]=1\npredict[predict<=0.5350]=0","cb103ecb":"prediction=predict.long().cpu().numpy().reshape(-1,1)\nid=np.array([i for i in range(len(predict))]).reshape(-1,1).astype(np.uint32)\n\nresult = np.hstack([id,prediction])\n\nsubmit= pd.DataFrame(result, columns=('Id','Expected'))\nsubmit.to_csv(\"submission.csv\",index=False,header=True)","2f5d468d":"!kaggle competitions submit -c stay-or-leave -f submission.csv -m \"Message\"","3df5026e":"# \ub370\uc774\ud130\ub85c\ub4dc","c2e3430e":"# \ub370\uc774\ud130\uc608\uce21","898ec271":"# \ubca0\uc774\uc2a4\ub77c\uc778 \ucf54\ub4dc \uc124\uba85\n- Learning rate\ub294 0.1, epochs\ub294 50, batchsize\ub294 500\uc73c\ub85c \uc124\uc815\n- Train data\uc640 Test data\ub97c \ub85c\ub4dc\ud558\uace0, MinMaxScaler\ub97c \uc774\uc6a9\ud574 x_train\uacfc x_test\ub97c \uc815\uaddc\ud654\uc2dc\ucf1c\uc90c\n- \ub808\uc774\uc5b4\ub294 3\uac1c\uc758 \uce35\uc744 \uc313\uc558\uace0, activation function\uc73c\ub85c sigmoid \ud568\uc218 \uc0ac\uc6a9\n- \uac00\uc911\uce58 \ucd08\uae30\ud654\uc5d0 xavier initialization \uc0ac\uc6a9\n- Binary classification \ubb38\uc81c\uc774\ubbc0\ub85c loss function\uc73c\ub85c BCELoss\ub97c \uc0ac\uc6a9\ud558\uc600\uace0, optimizer\ub294 Adam\uc744 \uc0ac\uc6a9\n- Test data\uc5d0 \uc758\ud55c \uc608\uce21\uac12\uc740 0.5350\uc744 \uae30\uc900\uc73c\ub85c \ud06c\uba74 1, \uc791\uac70\ub098 \uac19\uc73c\uba74 0\uc73c\ub85c \ubd84\ub958\n","cc38375b":"# \ubaa8\ub378\ud559\uc2b5"}}