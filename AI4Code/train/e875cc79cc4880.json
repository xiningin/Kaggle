{"cell_type":{"222bcdc7":"code","0327cd1d":"code","9e735d79":"code","62d23e56":"code","10cfb987":"code","c9df623f":"code","02103791":"code","330de274":"code","9df99b6e":"code","a729772e":"code","e43b95b2":"code","ccf1e9f2":"code","9147ea5b":"code","cc753a6f":"code","c0671662":"code","b10bd0df":"code","072b2efc":"code","bf8b63ff":"code","f578986e":"code","0f017be5":"code","9ce79a34":"code","1c5ced15":"code","52f5d362":"code","ec3b4bc9":"code","f1ff4608":"code","0285ff8c":"code","f064ff42":"code","1063d2a9":"code","e0caf8ab":"code","d992d6fc":"code","0001267e":"code","5ea53dd6":"code","5c45f6d0":"code","16bcf24f":"code","571679b9":"code","2822a5fe":"code","cd2a2ad2":"code","e1617b79":"code","7adee91f":"code","c62fc03e":"code","06a979e4":"code","de0f661d":"code","27ed352d":"code","f30da1a1":"code","2e5d892e":"code","c717ef38":"code","371085f4":"code","693dde05":"code","e0b0ad18":"code","c2876a1e":"code","1578480b":"code","fc61d9e5":"code","037b7aa1":"code","82bf16ae":"markdown","1e011f25":"markdown","b16cd4e5":"markdown","41b2574c":"markdown","c6fcdc36":"markdown","4af1a493":"markdown","b12dde99":"markdown","e164f1da":"markdown","30e869e4":"markdown","78f0e6f3":"markdown","ad05e3f9":"markdown","5a1037f4":"markdown","9b90bb92":"markdown","31a47540":"markdown","d8a31cf4":"markdown","b4bf03f2":"markdown","54840706":"markdown","aa1a3349":"markdown","55ddc42a":"markdown","6de05267":"markdown","16908a12":"markdown","e9cfd6bb":"markdown","4cdd1539":"markdown"},"source":{"222bcdc7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n","0327cd1d":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","9e735d79":"train.head()","62d23e56":"train.info()\n\nprint('*'*30)\n\ntest.info()","10cfb987":"# Lets find columns with null values\ntrain.isnull().sum()","c9df623f":"# Check missing value from dataset\nplt.figure(figsize=(18,18))\nplt.subplot(221)\nsns.heatmap(data=train.isnull())\nplt.subplot(222)\nsns.heatmap(data=test.isnull())","02103791":"# Lets drop Cabin column because it have alot of missing value that will effect accuracy of model\n\ntrain.drop('Cabin', axis=1, inplace=True)\ntest.drop('Cabin', axis=1, inplace=True)","330de274":"# Replace missing Age with mean value of Age column. Its best way to correct the dataset when percentage of missing value of are less\n\ntrain['Age'].fillna(train.Age.mean(), inplace=True)\ntest['Age'].fillna(test.Age.mean(), inplace=True)\n\ntrain['Fare'].fillna(train.Fare.mean(), inplace=True)\ntest['Fare'].fillna(test.Fare.mean(), inplace=True)","9df99b6e":"# lets see if there are any more columns with missing values \n\nplt.figure(figsize=(18,18))\nplt.subplot(221)\nsns.heatmap(data=train.isnull())\nplt.subplot(222)\nsns.heatmap(data=test.isnull())","a729772e":"# Now lets check howmany man\/female can survived\/not-survived using count plot\n\nplt.figure(figsize=(10,4))\nsns.set_style('whitegrid')\nsns.set(font_scale=1)\n\nsns.countplot(x='Survived', data=train,hue=\"Sex\")\nplt.xlabel('0=Not-Survived, 1=Survived')\nplt.ylabel('Number of Male and Female')\nplt.title('Gender wise Education visualization')\nplt.show()","e43b95b2":"outliers_features = [\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\noutlier_indices = []\nfor col in outliers_features:\n    # 1st quartile (25%)\n    Q1 = np.percentile(train[col], 25)\n    \n    # 3rd quartile (75%)\n    Q3 = np.percentile(train[col],75)\n    \n    # Interquartile range (IQR)\n    IQR = Q3 - Q1\n\n    # outlier step\n    outlier_step = IQR * 1.5\n\n    # Determine a list of indices of outliers for feature col\n    columns_idx = train[(train[col] < Q1 - outlier_step) | (train[col] > Q3 + outlier_step )].index\n\n    # append the found outlier indices for col to the list of outlier indices \n    outlier_indices.extend(columns_idx)\n\n# select observations containing more than 2 outliers\noutlier_indices = Counter(outlier_indices)        \noutliers_to_drop = list( k for k, v in outlier_indices.items() if v > 2 )","ccf1e9f2":"train.loc[outliers_to_drop] # Show the outliers rows","9147ea5b":"# Drop outliers\ntrain = train.drop(outliers_to_drop, axis = 0).reset_index(drop=True)","cc753a6f":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","c0671662":"# The Age is measured on very different scaling. So we need to do feature scaling before we start predictions\n\nplot = sns.FacetGrid(train, col=\"Sex\", margin_titles=True, row=\"Survived\")\nplot.map(plt.hist, \"Age\",color=\"blue\");","b10bd0df":"#Let's see Age distribution by classes\nplt.figure(figsize=(15,7))\nplt.xlabel(\"Age\")    \nplt.title(\"Class wise Age Distribution\")\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') ;\n\ntrain.Age[train['Pclass'] == 1].plot(kind=\"kde\")\ntrain.Age[train['Pclass'] == 2].plot(kind=\"kde\")\ntrain.Age[train['Pclass'] == 3].plot(kind=\"kde\")\n","072b2efc":"train","bf8b63ff":"## Separate train dataset and test dataset\n\ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","f578986e":"# We can define no of family members as new feature\n\ntrain['Family_members'] = train['SibSp'] + train['Parch'] \ntest['Family_members'] = test['SibSp'] + test['Parch'] ","0f017be5":"# We can also add title as new feature that will help to increase accuracy\n\ntrain['Title'] = train['Name'].map(lambda name: (name.split(',')[1].split('.')[0].strip()))\ntest['Title'] = test['Name'].map(lambda name: (name.split(',')[1].split('.')[0].strip()))","9ce79a34":"plt.figure(figsize=(17,7))\nsns.countplot(x='Title', data=train)","1c5ced15":"# Dropping PassengerId and Ticket column\n\n#train.drop(['PassengerId','Ticket', 'Name'], axis = 1, inplace = True)\n#test.drop(['Ticket', 'Name'], axis = 1, inplace = True)","52f5d362":"# Label encoding the categorical columns of dataset\n\nLabelEnco = LabelEncoder()\n\ncolumnNames = ['Embarked','Sex', 'Title']\nfor col in columnNames:\n    train[col] = LabelEnco.fit_transform(train[col])\n    test[col] = LabelEnco.fit_transform(test[col])\n\ntrain.head()","ec3b4bc9":"# Exploring Pclass and i created three new columns according to the Pclass column\n\ntrain['Pclass_3'] = train['Pclass'].map(lambda x: (x == 3))\ntrain['Pclass_2'] = train['Pclass'].map(lambda x: (x == 2))\ntrain['Pclass_1'] = train['Pclass'].map(lambda x: (x == 1))\n\ntest['Pclass_3'] = test['Pclass'].map(lambda x: (x == 3))\ntest['Pclass_2'] = test['Pclass'].map(lambda x: (x == 2))\ntest['Pclass_1'] = test['Pclass'].map(lambda x: (x == 1))","f1ff4608":"# Storing categorial values into separate columns with 1\/0 values\n\ntrain.Sex.value_counts()\ntrain['is_male'] = train['Sex'].map(lambda x: (x == 1))\ntrain['isfemale'] = train['Sex'].map(lambda x: (x == 0))\n\ntest['is_male'] = test['Sex'].map(lambda x: (x == 1))\ntest['isfemale'] = test['Sex'].map(lambda x: (x == 0))","0285ff8c":"# Prepare train and test data for modeling\n\nX_train = train.drop(['PassengerId','Ticket', 'Name', 'Survived', 'Pclass', 'Sex'], axis=1)\nY_train = train['Survived']\nX_test = test.drop(['Ticket', 'Name', 'PassengerId', 'Pclass', 'Sex'], axis=1)\n\nprint(\"shape of X_train\",X_train.shape)\nprint(\"Shape of Y_train\",Y_train.shape)\nprint(\"Shape of x_test\",X_test.shape)","f064ff42":"train.head()","1063d2a9":"# We shall now plot the numberical variables to look at the distribution\nnumerical = ['Pclass','Sex','Age']\nsns.pairplot(data=train, x_vars=numerical, hue='Survived', palette='Set1')","e0caf8ab":"# Import Libraries\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, mean_absolute_error, mean_squared_error,precision_recall_curve","d992d6fc":"# Create function to generate headmap of confusion matrix, we are defining the function so that we can resuse it in various model evalution\n\ndef show_matrix(x, y):\n    matrix = confusion_matrix(x, y)\n    sns.heatmap(matrix, annot = True,fmt = 'g')\n    plt.show()","0001267e":"from sklearn.linear_model import LogisticRegression\n\nlogisticModel = LogisticRegression(max_iter=200, solver='newton-cg')\nlogisticModel.fit(X_train, Y_train)","5ea53dd6":"logiPred = logisticModel.predict(X_train)\n\n# Plotting the confusion matrix in heatmap\nshow_matrix(Y_train, logiPred)","5c45f6d0":"# Check prediction score of Logistic model\nprecision_score(Y_train, logiPred)*100","16bcf24f":"logiAcScore = accuracy_score(Y_train, logiPred)*100\nlogiMse = mean_squared_error(Y_train, logiPred)*100\n\n# We will use this variables further in model comparison","571679b9":"# Precision-Recall vs Threshold Chart, explore the training result\n\nlogipp = logisticModel.predict_proba(X_train)\n\nprecision, recall, thresholds = precision_recall_curve(Y_train, logipp[:,1])\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])\n","2822a5fe":"# Import model\nfrom sklearn.neighbors import KNeighborsClassifier","cd2a2ad2":"knnModel = KNeighborsClassifier(n_neighbors=3, algorithm='auto', leaf_size=35)\nknnModel.fit(X_train, Y_train)","e1617b79":"# Prdict of KNN model\nknnPrediction = knnModel.predict(X_train)","7adee91f":"# Plotting the confusion matrix in heatmap\nshow_matrix(Y_train, knnPrediction)","c62fc03e":"np.round(precision_score(Y_train, knnPrediction)*100, 3)","06a979e4":"knnpp = knnModel.predict_proba(X_train)\n\nprecision, recall, thresholds = precision_recall_curve(Y_train, knnpp[:,1])\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])\n","de0f661d":"knnAcScore = accuracy_score(Y_train, knnPrediction)*100\nknnMse = mean_squared_error(Y_train, knnPrediction)*100","27ed352d":"from sklearn.tree import DecisionTreeClassifier\n\ndtcModel = DecisionTreeClassifier(max_depth=8)\ndtcModel.fit(X_train, Y_train)\nDTCPreds = dtcModel.predict(X_train)","f30da1a1":"# Plotting the confusion matrix in heatmap\nshow_matrix(Y_train, DTCPreds)\n\ndtcAcScore = accuracy_score(Y_train, DTCPreds)*100\ndtcMse = mean_squared_error(Y_train, DTCPreds)*100","2e5d892e":"np.round(precision_score(Y_train, DTCPreds)*100, 3)","c717ef38":"DTCpp = dtcModel.predict_proba(X_train)\n\nprecision, recall, thresholds = precision_recall_curve(Y_train, DTCpp[:,1])\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])\n","371085f4":"from sklearn.ensemble import RandomForestClassifier\nRMModel = RandomForestClassifier(max_features='sqrt', random_state=5, max_depth=100)\n# max_features='sqrt', random_state=5\nRMModel.fit(X_train, Y_train)\n\nRMPreds = RMModel.predict(X_train)\n\n# Plotting the confusion matrix in heatmap\nshow_matrix(Y_train, RMPreds)","693dde05":"rmpp = RMModel.predict_proba(X_train)\n\nprecision, recall, thresholds = precision_recall_curve(Y_train, rmpp[:,1])\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])\n","e0b0ad18":"rmAcScore = accuracy_score(Y_train, RMPreds)*100\nrmMse = mean_squared_error(Y_train, RMPreds)*100\n\nnp.round(precision_score(Y_train, RMPreds)*100, 3)","c2876a1e":"print(\"------------------------------\")\nprint(\"Logistic Regression\")\nprint(\"------------------------------\")\nprint(\"Accuracy score of KNN model is\", np.round(logiAcScore, 3))\nprint(\"Mean squared error of KNN model is\", np.round(logiMse, 3))\nprint(\"\")\nprint(\"------------------------------\")\nprint(\"KNN\")\nprint(\"------------------------------\")\nprint(\"Accuracy score of KNN model is\", np.round(knnAcScore, 3))\nprint(\"Mean squared error of KNN model is\", np.round(knnMse, 3))\nprint(\"\")\nprint(\"------------------------------\")\nprint(\"Decision Tree\")\nprint(\"------------------------------\")\nprint(\"Accuracy score of DecisionTree model is\", np.round(dtcAcScore, 3))\nprint(\"Mean squared error of DecisionTree model is\", np.round(dtcMse, 3))\nprint(\"\")\nprint(\"------------------------------\")\nprint(\"RandomForestClassifier\")\nprint(\"------------------------------\")\nprint(\"Accuracy score of RandomForestClassifier model is\", np.round(rmAcScore, 3))\nprint(\"Mean squared error of RandomForestClassifier model is\", np.round(rmMse, 3))","1578480b":"Y_test = RMModel.predict(X_test)\nacc_log = round(RMModel.score(X_train, Y_train) * 100, 2)\nacc_log","fc61d9e5":"X_test","037b7aa1":"submission = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": Y_test\n})\nprint(submission)\n\n# submission.to_csv('.\/submission.csv', index=False)","82bf16ae":"Here out of 891 total predictions, KNN did 139 wrong predictions","1e011f25":"# LogisticRegression\nIn statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.","b16cd4e5":"We can see maximum people between 25-30 year could not survived. That's also because of titanic has more young people","41b2574c":"I used Tukey method to find outliers. Tukey's rule says that the outliers are values more than 1.5 times the interquartile range from the quartiles \u2014 either below Q1 \u2212 1.5IQR, or above Q3 + 1.5 IQR","c6fcdc36":"# DecisionTree Classifier","4af1a493":"# KNeighbors Classifier\nIn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set.","b12dde99":"Cabin has 687 missing values which is 80% of data so we should drop cabin.","e164f1da":"# Missing Values Corrections\nWe will try to fill missing values by mean\/median\/mode. But note that it's also not 100% accurate. You can also use models like Decision Trees and Random Forest to handle missing values","30e869e4":"Now both datasets looks clean without missing values. We can proceed further cleaning and modeling process","78f0e6f3":"we found some alot of SibSp has high value. lets drop all of the outliers","ad05e3f9":"# Initial inspection of dataset","5a1037f4":"Number of death of man is very high comparing to females. More than 400 male passenger could not survived where as around 90 female could not survived","9b90bb92":"# Visualizations","31a47540":"# Feature Engineering (Convert Categorical variables into Numerical ones)","d8a31cf4":"As per wikipedia, Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean on 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking at the time one of the deadliest of a single ship[a] and the deadliest peacetime sinking of a superliner or cruise ship to date.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Titanic\n\n![image.png](attachment:b301dd4a-da16-4f8b-a746-608e262549e9.png)\n\n**Our main goal is who can survive and who can not?!**\n\nPlease like or comment if you like my work :)","b4bf03f2":"# Submission","54840706":"# Import libriaries","aa1a3349":"# Titanic: Exploring Survival","55ddc42a":"# RandomForestClassifier","6de05267":"**Continueing in newer version**","16908a12":"# Outlier detection with Interquartile Range(IQR)\n\nIdentifying outliers in data is an important part of statistical analysis. IQR is measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212  Q1. \n\nWhen a data set has outliers or extreme values, we summarize a typical value using the median as opposed to the mean.  When a data set has outliers, variability is often summarized by a statistic called the interquartile range, which is the difference between the first and third quartiles. The first quartile, denoted Q1, is the value in the data set that holds 25% of the values below it. The third quartile, denoted Q3, is the value in the data set that holds 25% of the values above it. The quartiles can be determined following the same approach that we used to determine the median, but we now consider each half of the data set separately. The interquartile range is defined as follows:\n\n\n![image.png](attachment:0d9bc4b7-4103-44fe-a909-5663d43a8b11.png)","e9cfd6bb":"# Model Comparison","4cdd1539":"As you can see here of RandomForestClassifier model's **accuracy** and **MSE** is batter than other. We will try out other models and evaluate their results"}}