{"cell_type":{"2b4e3896":"code","78bc9a27":"code","55af0683":"code","4a0e84c8":"code","68695bf7":"code","c8ec93a3":"code","f7a11e6f":"code","7b423ff2":"code","0181ce95":"code","35c92814":"code","6dae3616":"code","ed1e6f2d":"code","ee0fb867":"code","4faac6d0":"code","4298bc3c":"code","502bfeba":"code","b30438eb":"code","05344744":"code","7601ee19":"code","daec9bbb":"code","0513a965":"code","66a450c4":"code","c1372ba5":"code","a0bfde55":"code","860c32d0":"code","3a187f97":"code","acad54f5":"code","74f02ca7":"code","500afaa1":"code","030d6983":"code","ac1638d6":"code","e310a20b":"code","1d74703e":"code","546ea36a":"code","03c51f79":"code","dce8283a":"code","706e0ccb":"code","aa0ca499":"code","435f9da0":"markdown","119e7a3a":"markdown","a1beed10":"markdown","1d229358":"markdown","f20484e5":"markdown","530d442d":"markdown","30e9ca73":"markdown","b99e450c":"markdown","ffd7b551":"markdown","11d7e4ce":"markdown","e0c052d8":"markdown"},"source":{"2b4e3896":"import pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nnltk.download('wordnet')\nnltk.download('punkt')\nstemmer = SnowballStemmer('russian')\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.cluster import KMeans, DBSCAN\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics.cluster import fowlkes_mallows_score\n\nimport seaborn as sns\n\nfrom yellowbrick.text import FreqDistVisualizer, TSNEVisualizer\n\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV","78bc9a27":"data = pd.read_csv('..\/input\/russian-language-toxic-comments\/labeled.csv')","55af0683":"# Plotting the distribution of the labels\ntoxicc = 0\nokc = 0\nfor label in data['toxic']:\n    if label == 1:\n        toxicc += 1\n    if label == 0:\n        okc += 1","4a0e84c8":"plt.bar('0 - Non toxic comments', okc)\nplt.bar('1 - Toxic comments', toxicc, color = 'orange')\nplt.show()","68695bf7":"data2 = []\nfor i in range(0, len(data.index)):\n\n    # Punctuation removal\n    table = str.maketrans(dict.fromkeys(string.punctuation))                   \n    sentences = (data.comment[i].translate(table))\n\n    # \" '\\n \" removal\n    words = sentences[:-3] \n\n    # Tokenization\n    words = nltk.word_tokenize(words)\n\n    # shrt words removal & lemmatization & stemming\n    words_ = []\n    for word in words:\n        if len(word) > 2:   \n            if not word.isnumeric():                                                  \n                word1 = stemmer.stem(WordNetLemmatizer().lemmatize(word, pos='v'))          \n                words_.append(word1)\n    data2.append(words_)\n\ndf = pd.DataFrame({'comment':data2, 'toxic':data['toxic']})   ","c8ec93a3":"comparison = pd.DataFrame({'comments': data['comment'], 'preprocessed comments': data2, 'labels': data['toxic']})\ncomparison.head()","f7a11e6f":"train_data__, test_data__, train_labels, test_labels = train_test_split(df['comment'], df['toxic'], test_size = 0.2, random_state = 25)","7b423ff2":"# CountVectorizer & TermFrequencies\ncvect = CountVectorizer(ngram_range=(1, 1), lowercase='true')   \ntfidf_transformer = TfidfTransformer(norm= 'l2', use_idf= False)","0181ce95":"# Transforming the processed data to a list (for tfidf)\ndata4 = train_data__.astype(str).values.tolist()\n\ntrain_data1 = cvect.fit_transform(data4)\ntrain_data = tfidf_transformer.fit_transform(train_data1)","35c92814":"# Same procedure for the test data\ndata5 = test_data__.astype(str).values.tolist()\n\ntest_data1 = cvect.transform(data5)\ntest_data = tfidf_transformer.transform(test_data1)","6dae3616":"# Same procedure for the entire data set\ntrain_data6 = df['comment'].astype(str).values.tolist()\n\ndata1_ = cvect.fit_transform(train_data6)\ndata_ = tfidf_transformer.fit_transform(data1_)","ed1e6f2d":"#https:\/\/www.scikit-yb.org\/en\/latest\/api\/text\/freqdist.html\nfeatures = cvect.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(data1_)\nvisualizer.poof()","ee0fb867":"#https:\/\/www.scikit-yb.org\/en\/latest\/api\/text\/freqdist.html\n\ntsne = TSNEVisualizer()\ntsne.fit_transform(data_, df['toxic'])\ntsne.poof()","4faac6d0":"# Model fitting\nmodel = MultinomialNB(alpha = 0.1)\nmodel.fit(train_data, train_labels)","4298bc3c":"# Prediction\nprediction = model.predict(test_data)","502bfeba":"accuracy_score(test_labels, prediction) ","b30438eb":"print(fowlkes_mallows_score(prediction, test_labels))","05344744":"models = Pipeline([('CountVect', CountVectorizer()), \n                     ('TermFreq', TfidfTransformer()), \n                     ('NB', MultinomialNB())]) \n\nparameters = { 'CountVect__ngram_range': [(1, 1), (1, 2), (2, 2),(4,5)], \n              'TermFreq__use_idf': (True, False), \n              'TermFreq__norm': ('l1', 'l2'), \n              'NB__alpha': [1, 1e-1, 1e-2, 1e-3] } \n\nCrossValFolds = 5\ngrid_search= GridSearchCV(models, parameters, cv = CrossValFolds, n_jobs = -1) \ngrid_search.fit(data4, train_labels)\n\nprint(grid_search.best_score_) \nprint(grid_search.best_params_)","7601ee19":"print(classification_report(test_labels, prediction))","daec9bbb":"confusion_matrix(test_labels, prediction, )","0513a965":"ax = sns.heatmap(confusion_matrix(test_labels, prediction), annot = np.array([['1857', '78'],['350', '598']]), cmap=plt.cm.Blues, fmt = '')","66a450c4":"kmeans = KMeans(n_clusters=2, init='k-means++', random_state=0).fit(data_)","c1372ba5":"kmeans_pred = []\nfor label in kmeans.predict(data_):\n    if label == 0:\n        kmeans_pred.append(1)\n    else:\n        kmeans_pred.append(0)","a0bfde55":"fowlkes_mallows_score(kmeans_pred, df['toxic']) ","860c32d0":"def accuracy_score_(labels__, labels___):\n    score = 0\n    for idx, label in enumerate(labels__):\n        if label == labels___[idx]:\n            score = score + 1\n\n    return score\/len(labels___)","3a187f97":"accuracy_score_(kmeans_pred, df['toxic'])","acad54f5":"print(classification_report(df['toxic'], kmeans_pred))","74f02ca7":"confusion_matrix(df['toxic'], kmeans_pred)","500afaa1":"ax = sns.heatmap(confusion_matrix(test_labels, prediction), annot = np.array([['6878', '2708'],['3733', '1093']]), cmap=plt.cm.Blues, fmt = '')","030d6983":"Y = []\nfor k in range(1,10):\n    kmean_ = KMeans(n_clusters=k).fit(data_)\n    Y.append(kmean_.inertia_)","ac1638d6":"X = range(1,10)\n\nplt.figure(figsize=(12,6))\nplt.plot(X, Y)\nplt.plot(2, 7554, 'gD')\n\nplt.text(2.2, 14003, 'Elbow point', bbox=dict(color='green', alpha=0.8))\nplt.text(2.38, 13545, 'k = 2', fontsize = 12)\n\nplt.ylabel('Squared distances sum')\nplt.xlabel('No of clusters')\nplt.title('Elbow Method')\n\nplt.show()","e310a20b":"# Dimensionality reduction\nSVD = TruncatedSVD(100)\nPca = SVD.fit_transform(data_)\n\n# Makeing the data positive\nscaler = MinMaxScaler().fit(Pca)\ndata_ = scaler.transform(Pca)","1d74703e":"clustering = DBSCAN(eps=0.9, min_samples=2).fit(data_)","546ea36a":"fowlkes_mallows_score(clustering.labels_, data['toxic'])","03c51f79":"accuracy_score_(clustering.labels_, data['toxic'])","dce8283a":"# Computing a tabel for parameter comparison\n\nAcc = []\nParam = []\nX_ = []\nprint('Clusters \\t Acc \\t eps \\t min_samples')\nfor eps_ in [ 0.6, 0.8, 0.9]:\n    for min_samples_ in range(1, 10):\n        clustering = DBSCAN(eps=eps_, min_samples=min_samples_).fit(data_)\n        n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n        print( n_clusters,' \\t       ', round(accuracy_score_(clustering.labels_, data['toxic']), 3),'\\t ', eps_, '  \\t ', min_samples_)\n        \n        X_.append(accuracy_score_(clustering.labels_, data['toxic']))\n        if n_clusters == 2 or n_clusters == 3:\n            Acc.append(accuracy_score_(clustering.labels_, data['toxic']))\n            Param.append([eps_, min_samples_])","706e0ccb":"#print('Best score: ', max(Acc),'\\nBest Parameters: eps_ = ', Param[np.argmax(Acc, axis = 0)][0], ', min_sample = ', Param[np.argmax(Acc, axis = 0)][1])#","aa0ca499":"Y_ = range(0,27)\n\nplt.figure(figsize=(12,6))\nplt.plot(Y_, X_)\nplt.plot(22, 0.6602830974188176, 'gD')\n\nplt.text(20.2, 0.635, 'Maximum accuracy', bbox=dict(color='green', alpha=0.8))\nplt.text(19.4, 0.615, 'eps_ =  0.9 , min_sample =  1', fontsize = 10)\n\nplt.xlabel('Accuracy')\nplt.ylabel('Parameters')\nplt.title('Best Parameters DBSCAN')\n\nplt.show()","435f9da0":"### Libraries","119e7a3a":"## Supervised learning method - Naive Bayes","a1beed10":"# Unsupervised methods","1d229358":"## DBSCAN","f20484e5":"## K-means","530d442d":"## Data Preprocessing","30e9ca73":"We are using two scores to compare the results\n\nAccuracy score = $\\frac{TP + TN}{Total}$\n\nFowlkes Mallwows Score = $\\frac{TP}{\\sqrt(TP+FP)(TP+FN}$ ","b99e450c":"###Parameter Tunning","ffd7b551":"### Data visualization","11d7e4ce":"# **Second PML Project**\nGhadamiyan Lida, class 407 AI","e0c052d8":"### Elbow method"}}