{"cell_type":{"4c6ce1bb":"code","b9335bd4":"code","8a126e9c":"code","39dabe07":"code","960b12ec":"code","301af3fa":"code","4a3264c7":"code","5c470ab9":"code","61dcfad7":"code","5e74cdae":"code","dfc927e5":"code","a01740a5":"code","1809ffb1":"code","feac9537":"code","5302f209":"code","e1aa51e5":"code","7ecb3c77":"code","4eb1fb1a":"code","2a09c707":"code","442c57dc":"code","01b184b3":"code","b64af17e":"code","a29c9bc7":"code","e5cadffb":"code","d751e659":"code","4532c158":"code","7b91d411":"code","80e91e61":"code","65f9d487":"code","e7e5422e":"code","581668fa":"code","0130cbb9":"code","9f11cd73":"code","ec86506e":"code","7f33c926":"code","3e318fec":"code","854bce13":"code","133c517b":"code","e60fbf5b":"code","19aea49a":"code","225babb6":"code","996f149a":"code","a75a9ac0":"code","928b6d98":"code","5c27a9a2":"code","2a4baa65":"code","0936ed76":"code","3844e6bc":"code","a0894048":"code","e99e5ada":"code","5e982148":"code","64f4e4a5":"code","87a152c0":"code","5537c28a":"code","0d966841":"code","558711eb":"code","0097d359":"code","b73f0dff":"markdown","457801ca":"markdown","368efc1d":"markdown","48da25e6":"markdown","c17c469c":"markdown","dd4fa15a":"markdown","29cff331":"markdown","cae99967":"markdown","de9b3b1d":"markdown","c1b693a6":"markdown","b2631bd2":"markdown","2000d2e4":"markdown","a0596dfd":"markdown","87abcee4":"markdown","df55dab7":"markdown","08fc3e65":"markdown","15d029d2":"markdown","ff3cdb90":"markdown","eb3b5d80":"markdown","233e2b45":"markdown","995e6cea":"markdown","ee5ca5c6":"markdown","724827ed":"markdown","89295c0e":"markdown","ace8faf9":"markdown"},"source":{"4c6ce1bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nfrom pathlib import Path\nimport itertools\n\n\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12, 8\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9335bd4":"input_path = Path('\/kaggle\/input\/widsdatathon2021\/')","8a126e9c":"TARGET_COL = \"diabetes_mellitus\"\n\ndiab = pd.read_csv(input_path \/ 'TrainingWiDS2021.csv')\ndisplay(diab.head())","39dabe07":"diab.describe()","960b12ec":"test = pd.read_csv(input_path \/ 'UnlabeledWiDS2021.csv')\ndisplay(test.head())","301af3fa":"submission = pd.read_csv(input_path \/ 'SampleSubmissionWiDS2021.csv')\ndisplay(submission.head())","4a3264c7":"diab.isnull().sum()","5c470ab9":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in diab.columns if diab[feature].isna().sum()>1 and diab[feature].dtypes!='O']\nnumerical_nan","61dcfad7":"diab[numerical_nan].isna().sum","5e74cdae":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=diab[feature].median()\n    \n    diab[feature].fillna(median_value,inplace=True)\n    \ndiab[numerical_nan].isnull().sum()","dfc927e5":"# categorical features with missing values\ncategorical_nan = [feature for feature in diab.columns if diab[feature].isna().sum()>0 and diab[feature].dtypes=='O']\nprint(categorical_nan)","a01740a5":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    diab[feature] = diab[feature].fillna('None')","1809ffb1":"diab[categorical_nan].isna().sum()","feac9537":"# Count Plot\nplt.style.use(\"classic\")\nplt.figure(figsize=(8, 6))\nsns.countplot(diab['diabetes_mellitus'], palette='rainbow', **{'hatch':'\/','linewidth':3})\nplt.xlabel(\"diabetes_mellitus\")\nplt.ylabel(\"Count\")\nplt.title(\"Diabetes\")\nplt.xticks(rotation=45, fontsize=8)\nplt.show()","5302f209":"fig = go.Figure(data=[go.Bar(\n            x=diab['age'][0:10], y=diab['diabetes_mellitus'][0:10],\n            text=diab['diabetes_mellitus'][0:10],\n            textposition='auto',\n            marker_color='black'\n\n        )])\nfig.update_layout(\n    title='Diabetes Incidence by Age',\n    xaxis_title=\"Age\",\n    yaxis_title=\"Diabetes Mellitus\",\n)\nfig.show()","e1aa51e5":"grouped = diab.groupby('diabetes_mellitus').agg({'icu_id':['mean', 'std', min, max], \n                                       'd1_glucose_max':['mean', 'std', min, max],\n                                       'weight':['mean', 'std', min, max],\n                                       'apache_2_diagnosis':['mean', 'std', min, max],\n                                       'glucose_apache':['mean', 'std', min, max],\n                                       'bmi':['mean', 'std', min, max],\n                                       'heart_rate_apache':['mean', 'std', min, max],\n                                       'age':['mean', 'std', min, max]\n                                      })\ngrouped.columns = [\"_\".join(x) for x in grouped.columns.ravel()]\ngrouped # or grouped.T","7ecb3c77":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in diab.columns:\n    if diab[c].dtype=='float16' or  diab[c].dtype=='float32' or  diab[c].dtype=='float64':\n        diab[c].fillna(diab[c].mean())\n\n#fill in -999 for categoricals\ndiab = diab.fillna(-999)\n# Label Encoding\nfor f in diab.columns:\n    if diab[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(diab[f].values))\n        diab[f] = lbl.transform(list(diab[f].values))\n        \nprint('Labelling done.')","4eb1fb1a":"diab = pd.get_dummies(diab)","2a09c707":"columns=diab.columns[:8]\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(diab[columns]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(diab[cn][diab.diabetes_mellitus == 1], bins=50)\n    sns.distplot(diab[cn][diab.diabetes_mellitus == 0], bins=50)\n    ax.set_xlabel('')\n    plt.legend(diab[\"diabetes_mellitus\"])\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","442c57dc":"sns.heatmap(diab[diab.columns[:8]].corr(),annot=True,cmap='summer')\nfig=plt.gcf()\nplt.show()","01b184b3":"from sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')","b64af17e":"diabetes_mellitus=diab['diabetes_mellitus']\ndata=diab[diab.columns[:8]]\ntrain,test=train_test_split(diab,test_size=0.25,random_state=0,stratify=diab['diabetes_mellitus'])# stratify the outcome\ntrain_X=train[train.columns[:8]]\ntest_X=test[test.columns[:8]]\ntrain_Y=train['diabetes_mellitus']\ntest_Y=test['diabetes_mellitus']","a29c9bc7":"features = train_X.columns.values\n\nfor feature in features:\n    mean, std = diab[feature].mean(), diab[feature].std()\n    train_X.loc[:, feature] = (train_X[feature] - mean) \/ std\n    test_X.loc[:, feature] = (test_X[feature] - mean) \/ std","e5cadffb":"abc=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree', 'Random forest', 'Naive Bayes']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),\n        KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier(), \n        RandomForestClassifier(n_estimators=100,random_state=0), GaussianNB()]\nfor i in models:\n    model = i\n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    abc.append(metrics.accuracy_score(prediction,test_Y))\nmodels_dataframe=pd.DataFrame(abc,index=classifiers)   \nmodels_dataframe.columns=['Accuracy']\nmodels_dataframe.sort_values(['Accuracy'], ascending=[0])","d751e659":"modelRF= RandomForestClassifier(n_estimators=100,random_state=0)\nmodelRF.fit(train_X,train_Y)\npredictionRF=modelRF.predict(test_X)\npd.Series(modelRF.feature_importances_,index=train_X.columns).sort_values(ascending=False)","4532c158":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.preprocessing import StandardScaler #Standardisation","7b91d411":"kfold = KFold(n_splits=10, random_state=22) # k=10 splits the data into 10 equal parts","80e91e61":"# Starting with the original dataset and then doing centering and scaling\nfeatures=diab[diab.columns[:8]]\nfeatures_standard=StandardScaler().fit_transform(features)# Gaussian Standardisation\nX=pd.DataFrame(features_standard,columns=['icu_id', 'd1_glucose_max', 'weight', 'apache_2_diagnosis', \n                                          'glucose_apache', 'bmi', 'heart_rate_apache', 'age'])\nX['diabetes_mellitus']=diab['diabetes_mellitus']","65f9d487":"#xyz=[]\n#accuracy=[]\n#classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree', 'Random forest', 'Naive Bayes']\n#models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),\n #       KNeighborsClassifier(n_neighbors=3),DecisionTreeClassifier(), \n  #      RandomForestClassifier(n_estimators=100,random_state=0), GaussianNB()]\n\n#for i in models:\n #   model = i\n  #  cv_result = cross_val_score(model,X[X.columns[:8]], X['diabetes_mellitus'], cv = kfold, scoring = \"accuracy\")\n   # cv_result=cv_result\n    #xyz.append(cv_result.mean())\n    #accuracy.append(cv_result)\n\n#cv_models_dataframe=pd.DataFrame(xyz, index=classifiers)   \n#cv_models_dataframe.columns=['CV Mean']    \n#cv_models_dataframe\n#cv_models_dataframe.sort_values(['CV Mean'], ascending=[0])","e7e5422e":"#box=pd.DataFrame(accuracy,index=[classifiers])\n#boxT = box.T","581668fa":"#ax = sns.boxplot(data=boxT, orient=\"h\", palette=\"Set2\", width=.6)\n#ax.set_yticklabels(classifiers)\n#ax.set_title('Cross validation accuracies with different classifiers')\n#ax.set_xlabel('Accuracy')\n#plt.show()","0130cbb9":"#linear_svm=svm.SVC(kernel='linear',C=0.1,gamma=10, probability=True)\n#radial_svm=svm.SVC(kernel='rbf',C=0.1,gamma=10, probability=True)\n#lr=LogisticRegression(C=0.1)","9f11cd73":"#from sklearn.ensemble import VotingClassifier #for Voting Classifier","ec86506e":"#ensembleModel=VotingClassifier(estimators=[('Linear_svm',linear_svm), ('Radial_svm', radial_svm), ('Logistic Regression', lr)], \n #                                           voting='soft', weights=[2,1,3])\n\n#ensembleModel.fit(train_X,train_Y)\n#predictEnsemble = ensembleModel.predict(test_X)","7f33c926":"#print('Accuracy of ensembled model with all the 3 classifiers is:', np.round(ensembleModel.score(test_X,test_Y), 4))","3e318fec":"#from sklearn import metrics\n#from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, \n                           #  classification_report, f1_score, average_precision_score, precision_recall_fscore_support)","854bce13":"# Logistic regression\n#modelLR = LogisticRegression()\n#modelLR.fit(train_X,train_Y)\n#y_pred_prob_lr = modelLR.predict_proba(test_X)[:,1]\n#fpr_lr, tpr_lr, thresholds_lr = roc_curve(test_Y, y_pred_prob_lr)\n#roc_auc_lr = auc(fpr_lr, tpr_lr)\n#precision_lr, recall_lr, th_lr = precision_recall_curve(test_Y, y_pred_prob_lr)\n\n# SVM with rbf\n#modelSVMrbf=svm.SVC(kernel='rbf', probability=True)\n#modelSVMrbf.fit(train_X,train_Y)\n#y_pred_prob_SVMrbf = modelSVMrbf.predict_proba(test_X)[:,1]\n#fpr_SVMrbf, tpr_SVMrbf, thresholds_SVMrbf = roc_curve(test_Y, y_pred_prob_SVMrbf)\n#roc_auc_SVMrbf = auc(fpr_SVMrbf, tpr_SVMrbf)\n#precision_SVMrbf, recall_SVMrbf, th_SVMrbf = precision_recall_curve(test_Y, y_pred_prob_SVMrbf)\n\n# SVM with linear\n#modelSVMlinear=svm.SVC(kernel='linear', probability=True)\n#modelSVMlinear.fit(train_X,train_Y)\n#y_pred_prob_SVMlinear = modelSVMlinear.predict_proba(test_X)[:,1]\n#fpr_SVMlinear, tpr_SVMlinear, thresholds_SVMlinear = roc_curve(test_Y, y_pred_prob_SVMlinear)\n#roc_auc_SVMlinear = auc(fpr_SVMlinear, tpr_SVMlinear)\n#precision_SVMlinear, recall_SVMlinear, th_SVMlinear = precision_recall_curve(test_Y, y_pred_prob_SVMlinear)\n\n# KNN\n#modelKNN = KNeighborsClassifier(n_neighbors=3)\n#modelKNN.fit(train_X,train_Y)\n#y_pred_prob_KNN = modelKNN.predict_proba(test_X)[:,1]\n#fpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(test_Y, y_pred_prob_KNN)\n#roc_auc_KNN = auc(fpr_KNN, tpr_KNN)\n#precision_KNN, recall_KNN, th_KNN = precision_recall_curve(test_Y, y_pred_prob_KNN)\n\n\n# Decision Tree\n#modelTree=DecisionTreeClassifier()\n#modelTree.fit(train_X,train_Y)\n#y_pred_prob_Tree = modelTree.predict_proba(test_X)[:,1]\n#fpr_Tree, tpr_Tree, thresholds_Tree = roc_curve(test_Y, y_pred_prob_Tree)\n#roc_auc_Tree = auc(fpr_Tree, tpr_Tree)\n#precision_Tree, recall_Tree, th_Tree = precision_recall_curve(test_Y, y_pred_prob_Tree)\n\n# Random forest\n#modelRF= RandomForestClassifier(n_estimators=100,random_state=0)\n#modelRF.fit(train_X,train_Y)\n#y_pred_prob_rf = modelRF.predict_proba(test_X)[:,1]\n#fpr_rf, tpr_rf, thresholds_rf = roc_curve(test_Y, y_pred_prob_rf)\n#roc_auc_rf = auc(fpr_rf, tpr_rf)\n#precision_rf, recall_rf, th_rf = precision_recall_curve(test_Y, y_pred_prob_rf)\n\n\n# Naive Bayes\n#modelNB= GaussianNB()\n#modelNB.fit(train_X,train_Y)\n#y_pred_prob_nb = modelNB.predict_proba(test_X)[:,1]\n#fpr_nb, tpr_nb, thresholds_nb = roc_curve(test_Y, y_pred_prob_nb)\n#roc_auc_nb = auc(fpr_nb, tpr_nb)\n#precision_nb, recall_nb, th_nb = precision_recall_curve(test_Y, y_pred_prob_nb)\n\n# Ensemble \n#y_pred_prob_en = ensembleModel.predict_proba(test_X)[:,1]\n#fpr_en, tpr_en, thresholds_en = roc_curve(test_Y, y_pred_prob_en)\n#roc_auc_en = auc(fpr_en, tpr_en)\n#precision_en, recall_en, th_en = precision_recall_curve(test_Y, y_pred_prob_en)","133c517b":"# Plot ROC curve\n#plt.plot([0, 1], [0, 1], 'k--')\n#plt.plot(fpr_lr, tpr_lr, label='Log Reg (area = %0.3f)' % roc_auc_lr)\n#plt.plot(fpr_SVMrbf, tpr_SVMrbf, label='SVM rbf (area = %0.3f)' % roc_auc_SVMrbf)\n#plt.plot(fpr_SVMlinear, tpr_SVMlinear, label='SVM linear (area = %0.3f)' % roc_auc_SVMlinear)\n#plt.plot(fpr_KNN, tpr_KNN, label='KNN (area = %0.3f)' % roc_auc_KNN)\n#plt.plot(fpr_Tree, tpr_Tree, label='Tree (area = %0.3f)' % roc_auc_Tree)\n#plt.plot(fpr_rf, tpr_rf, label='RF (area = %0.3f)' % roc_auc_rf)\n#plt.plot(fpr_nb, tpr_nb, label='NB (area = %0.3f)' % roc_auc_nb)\n#plt.plot(fpr_en, tpr_en, label='Ensamble (area = %0.3f)' % roc_auc_en)\n#plt.xlabel('False Positive Rate')\n#plt.ylabel('True Positive Rate')\n#plt.title('ROC curves from the investigated models')\n#plt.legend(loc='best')\n#plt.show()","e60fbf5b":"#plt.plot([1, 0], [0, 1], 'k--')\n#plt.plot(recall_lr, precision_lr, label='Log Reg')\n#plt.plot(recall_SVMrbf, precision_SVMrbf, label='SVM rbf')\n#plt.plot(recall_SVMlinear, precision_SVMlinear, label='SVM linear')\n#plt.plot(recall_KNN, precision_KNN, label='KNN')\n#plt.plot(recall_Tree, precision_Tree, label='Tree')\n#plt.plot(recall_rf, precision_rf, label='RF')\n#plt.plot(recall_nb, precision_nb, label='NB')\n#plt.plot(recall_en, precision_en, label='Ensamble')\n#plt.title('Precision vs. Recall')\n#plt.xlabel('Recall')\n#plt.ylabel('Precision')\n#plt.legend(loc='best')\n#plt.show()","19aea49a":"#def plot_confusion_matrix(cm, classes,\n#                          normalize=False,\n #                         title='Confusion matrix',\n  #                        cmap=plt.cm.Blues):\n   # \"\"\"\n    #This function prints and plots the confusion matrix.\n    #Normalization can be applied by setting `normalize=True`.\n    #\"\"\"\n    #plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    #plt.title(title)\n    #plt.colorbar()\n    #tick_marks = np.arange(len(classes))\n    #plt.xticks(tick_marks, classes, rotation=45)\n    #plt.yticks(tick_marks, classes)\n\n    #if normalize:\n     #   cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    #thresh = cm.max() \/ 2.\n    #for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n     #   plt.text(j, i, cm[i, j],\n     #            horizontalalignment=\"center\",\n      #           color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    #plt.tight_layout()\n   # plt.ylabel('True label')\n    #plt.xlabel('Predicted label')","225babb6":"#class_names = test_Y.unique()\n#cmEnsamble = confusion_matrix(test_Y, predictEnsemble)\n#plot_confusion_matrix(cmEnsamble, classes=class_names, title='Confusion matrix with ensamble model, without normalization')","996f149a":"#print(metrics.classification_report(test_Y, predictEnsemble))","a75a9ac0":"#from sklearn.model_selection import train_test_split\n#from sklearn.metrics import confusion_matrix\n#import itertools\n\n#from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n#from keras.models import Sequential\n#from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n#from keras import optimizers\n#from keras.preprocessing.image import ImageDataGenerator\n#from keras.callbacks import ReduceLROnPlateau\n\n# fix random seed for reproducibility\n#np.random.seed(2)","928b6d98":"#train_Y = to_categorical(train_Y, num_classes = 2)\n#test_Y = to_categorical(test_Y, num_classes = 2)","5c27a9a2":"# Confirm the train-test split ratio\n#print(np.shape(train_X))\n#print(np.shape(train_Y))\n#print(np.shape(test_X))\n#print(np.shape(test_Y))","2a4baa65":"#model = Sequential()\n#model.add(Dense(8, input_dim=8, activation='relu'))\n#model.add(Dense(12, input_dim=8, activation='relu'))\n#model.add(Dense(12, input_dim=8, activation='relu'))\n#model.add(Dense(8, activation='relu'))\n#model.add(Dense(4, activation='relu'))\n#model.add(Dense(2, activation='sigmoid'","0936ed76":"#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","3844e6bc":"#epoch = 150\n#batch_size = 50\n\n#history = model.fit(train_X, train_Y, batch_size = batch_size, epochs = epoch, \n     #     validation_data = (test_X, test_Y), verbose = 2)","a0894048":"#score, acc = model.evaluate(test_X, test_Y)\n#print('Test score:', score)\n#print('Test accuracy:', acc)","e99e5ada":"# Plot the loss and accuracy curves for training and validation vs. epochs\n\n#fig, ax = plt.subplots(2,1)\n#ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n#ax[0].plot(history.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n#legend = ax[0].legend(loc='best', shadow=True)\n\n#ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n#ax[1].plot(history.history['val_acc'], color='r',label=\"Testing accuracy\")\n#legend = ax[1].legend(loc='best', shadow=True)\n\n#plt.show()","5e982148":"# Predict the values from the validation dataset\n#Y_pred = model.predict(test_X)\n# Convert predictions classes to one hot vectors \n#Y_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\n#Y_true = np.argmax(test_Y,axis = 1) \n# compute the confusion matrix\n#confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\n#plot_confusion_matrix(confusion_mtx, classes = range(2))\n\n#plt.show()","64f4e4a5":"#print(metrics.classification_report(Y_true, Y_pred_classes))","87a152c0":"#df_std = StandardScaler().fit_transform(diab)\n#y = diab.iloc[:,-1].values","5537c28a":"#from sklearn.manifold import TSNE\n\n#tsne = TSNE(n_components=2, random_state=0)\n#x_t = tsne.fit_transform(df_std)","0d966841":"#color_map = {0:'red', 1:'blue'}\n#plt.figure()\n#plt.figure()\n#plt.scatter(x_t[np.where(y == 0), 0], x_t[np.where(y == 0), 1], marker='x', color='g', \n#            linewidth='1', alpha=0.8, label='No diabetes')\n#plt.scatter(x_t[np.where(y == 1), 0], x_t[np.where(y == 1), 1], marker='v', color='r',\n#            linewidth='1', alpha=0.8, label='Diabetes')\n\n#plt.xlabel('X in t-SNE')\n#plt.ylabel('Y in t-SNE')\n#plt.legend(loc='upper left')\n#plt.title('t-SNE visualization of diabetes data')\n#plt.show()","558711eb":"#test[TARGET_COL] = modelRF.predict_proba(test[features].values)[:,1]","0097d359":"#test[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission.csv\",index=False)","b73f0dff":"#Create the model using Keras","457801ca":"The campaign is represented by a blue circle logo that was adopted in 2007 after the passage of the UN Resolution on diabetes. The blue circle is the global symbol for diabetes awareness. It signifies the unity of the global diabetes community in response to the diabetes epidemic.\n\nEvery year, the World Diabetes Day campaign focuses on a dedicated theme that runs for one or more years. The theme for World Diabetes Day 2020 is The Nurse and Diabetes. Messaging and materials will start to be made available during the second quarter of 2020.https:\/\/worlddiabetesday.org\/about\/\n\n![](https:\/\/cdn.diabetesselfmanagement.com\/2019\/10\/shutterstock_1214470630.jpg)https:\/\/cdn.diabetesselfmanagement.com\/2019\/10\/shutterstock_1214470630.jpg","368efc1d":"#Confusion matrix using this model","48da25e6":"#Ensembling","c17c469c":"### <b><mark style=\"background-color: #9B59B6\"><font color=\"white\">Managing Diabetes in Pregnancy Before, During, and After COVID-19<\/font><\/mark><\/b>\n\nAuthor: Helen R Murphy\n\nDiabetes Technol Ther . 2020 Jun;22(6):454-461. doi: 10.1089\/dia.2020.0223. Epub 2020 May 18.\nPMID: 32396397 DOI: 10.1089\/dia.2020.0223\n\nPregnant women with diabetes are identified as being more vulnerable to the severe effects of COVID-19 and advised to stringently follow social distancing measures. The author reviewed the management of diabetes in pregnancy before and during the lockdown.\n\nMethods: Majority of antenatal diabetes and obstetric visits are provided remotely, with pregnant women attending hospital clinics only for essential ultrasound scans and labor and delivery. Online resources for supporting women planning pregnancy and for self-management of pregnant women with type 1 diabetes (T1D) using intermittent or continuous glucose monitoring are provided.\n\nRetinal screening procedures, intrapartum care, and the varying impact of lockdown on maternal glycemic control are considered. Alternative screening procedures for diagnosing hyperglycemia during pregnancy and gestational diabetes mellitus (GDM) are discussed. Case histories describe the remote initiation of insulin pump therapy and automated insulin delivery in T1D pregnancy.\n\nResults: Initial feedback suggests that video consultations are well received and that the patient experiences for women requiring face-to-face visits are greatly improved. As the pandemic eases, formal evaluation of remote models of diabetes education and technology implementation, including women's views, will be important.\n\nConclusions: Research and audit activities will resume and they will find new ways for supporting pregnant women with diabetes to choose their preferred glucose monitoring and insulin delivery.\nhttps:\/\/pubmed.ncbi.nlm.nih.gov\/32396397\/","dd4fa15a":"### <b><mark style=\"background-color: #9B59B6\"><font color=\"white\">Effects of COVID-19 pandemic lockdown on gestational diabetes mellitus: A retrospective study<\/font><\/mark><\/b>\n\nAuthors: L. Ghesqui\u00e8re, C. Garabedian, E. Drumez, M. Lema\u00eetre, M. Cazaubiel, C. Bengler, and A. Vambergue\n\nDiabetes Metab. 2020 Oct 15 - doi: 10.1016\/j.diabet.2020.09.008\n\nThe objective of their study was to evaluate the impact of the lockdown period on the glycemic balance in patients with GDM (gestational diabetes mellitus).\n\nGestational diabetes mellitus (GDM) is the most common medical complication of pregnancy, with a prevalence varying from 5.8% (1.8\u201322.3%) in Europe to 12.9% (8.4\u201324.5%) in the Middle East and North Africa . Maintaining adequate blood glucose levels in GDM reduces morbidity for both the mother and infant. The authors hypothesis was that GDM control would be lower during the coronavirus disease 2019 (COVID-19) pandemic lockdown due to a decrease in physical activity and changes in patients\u2019 dietary habits. Thus, the objective of their study was to evaluate the impact of the lockdown period on glycaemic balance in patients with GDM.\n\nThis retrospective study compared two periods: the COVID-19 lockdown from 18 March to 7 May 2020 and the same period in 2019. All pregnant patients who were followed for GDM during those periods were included. Standard-of-care treatment involves a multidisciplinary lifestyle approach defined by diet and exercise interventions: women undertake home blood glucose monitoring six times a day with the aim of achieving a capillary fasting glucose target of <5.1 mmol\/L and\/or a 2-h postprandial capillary glucose <6.6 mmol\/L. If these targets are not achieved after at least 10 days of dietary and lifestyle measures, then the women are given either prandial or basal insulin.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7557293\/","29cff331":"#Fit the Model","cae99967":"<i><p style=\"font-size:18px; background-color: #32a8a2; border: 2px solid black; margin: 20px; padding: 20px;\">World Diabetes Day (WDD) was created in 1991 by IDF and the World Health Organization in response to growing concerns about the escalating health threat posed by diabetes. World Diabetes Day became an official United Nations Day in 2006 with the passage of United Nation Resolution 61\/225. It is marked every year on 14 November, the birthday of Sir Frederick Banting, who co-discovered insulin along with Charles Best in 1922.\nWDD is the world\u2019s largest diabetes awareness campaign reaching a global audience of over 1 billion people in more than 160 countries. The campaign draws attention to issues of paramount importance to the diabetes world and keeps diabetes firmly in the public and political spotlight.https:\/\/worlddiabetesday.org\/about\/","de9b3b1d":"#Cross validation","c1b693a6":"#Evaluate model","b2631bd2":"#ROC curve with AUC","2000d2e4":"Let us have look at the distribution of the features grouping them by the diabetes_mellitus column. diabetes_mellitus 0 is no diabetes, 1 is with diabetes.","a0596dfd":"#Model report with ensemble model","87abcee4":"\n\n<div class=\"alert alert-block alert-success\">\n    Now wait, this is getting too long! The snippet below simply stopped and the Notebook is still running, more than 24 hours. Since women have so many things to do, like cook, sweep\/clean the house, go to Supermarket, change diapers, feed someone or simply, do Nothing. I commented all the rest of the script. Maybe, when and if I have a stronger Hardware\/GPU, and mostly TIME. Time is so precious, like health is too. Probably, that's\nthe major reason for Women not being so into Competitions.    \n\n<\/div>","df55dab7":"#Model building","08fc3e65":"#Compare al the models for their accuracy. It takes more than 1 hour.","15d029d2":"#Codes by Manjit Pathak https:\/\/www.kaggle.com\/mnpathak1\/model-comparison-with-roc-curves-and-more\/notebook","ff3cdb90":"<font color=\"#EC7063\">Health Promotion and Compliance<\/font>\n\nAt their centre, online demonstrations, educational videos and teleconsultations were all made available to patients. This management decision was appreciated by our patients, contributed to good compliance with diabetes monitoring and did not change rates of follow-up. In addition, a recent study of diabetes management in pregnancy before and during the lockdown showed that video consultations are well received and that they greatly improve the experience of women who require face-to-face visits.\n\n<font color=\"#EC7063\">Diabetes control was lower during the COVID-19 Pandemic Lockdown<\/font>\n\nIn conclusion, diabetes control was lower during the COVID-19 pandemic lockdown. This may be explained by reduced physical activity, modified dietary habits and greater anxiety during this period. It will now be of interest to follow these patients to evaluate the impact on maternal and neonatal morbidity, including macrosomia, induction and caesarean rates.\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7557293\/","eb3b5d80":"#Compile the Model","233e2b45":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">World Diabetes Day (WDD), Gestational Diabetes Mellitus, WiDS 2021<\/h3>","995e6cea":"#Prediction using Neural Network with Keras","ee5ca5c6":"That snippet above stopped the Notebook, therefore I commented everything after it.","724827ed":"#Confusion matrix with ensemble model","89295c0e":"#Training and validation curves vs. epoch","ace8faf9":"\n\n<div class=\"alert alert-block alert-success\">\n    It is a WiDS (Women in Data Science) Competition, in which the target is Diabetes. And there is no PREGNANCY data? Am I missing something? Gestational Diabetes is diabetes while pregnant. Besides, it increases the risk for type 2 diabetes later in life. Sorry I could not get that lack of data. \n\n<\/div>"}}