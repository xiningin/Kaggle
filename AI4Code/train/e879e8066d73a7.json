{"cell_type":{"77bee596":"code","76261b5f":"code","465788e4":"code","46f23403":"code","c60a1938":"code","3fddb2bb":"code","344e8511":"code","a2bc9342":"code","360f661d":"code","3c86a0a5":"code","6ef0ae3a":"code","83ce87a7":"code","ee557db5":"code","920347d1":"code","5f1aa416":"code","0758f5e9":"code","ef537115":"code","4f506ef7":"code","e8b305b9":"markdown","58f1e29e":"markdown","647d7918":"markdown","93c11b4e":"markdown","14a51084":"markdown","630b4c57":"markdown","91f9d4e6":"markdown","7d305e78":"markdown","a6600e8f":"markdown","498c908f":"markdown","f6f45fd5":"markdown","9d8f3fcb":"markdown","f71c3d25":"markdown","d60427d8":"markdown","8d0901ef":"markdown","a5d3012d":"markdown","ee6c6f16":"markdown"},"source":{"77bee596":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","76261b5f":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","465788e4":"df_train.info()","46f23403":"df_test","c60a1938":"df_train['text'].notnull().all()","3fddb2bb":"y_train = df_train['target'].values","344e8511":"import re\nimport nltk\n\nnltk.download('stopwords') \n\nfrom nltk.corpus import stopwords\n\n#For Stemmer\nfrom nltk.stem.porter import PorterStemmer\n\n#For Lemmatizer\nfrom nltk.stem import WordNetLemmatizer ","a2bc9342":"def clean_text_stemmer(data):\n\n  corpus = []\n\n  for i in range(0, len(data['text'].values)):\n    review = re.sub('[^a-z-A-Z]', ' ', data['text'][i])\n\n    review = review.lower()\n    review = review.split()\n\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    review = ' '.join(review)\n\n    corpus.append(review)\n\n  return corpus","360f661d":"stemmer_corpus_train = clean_text_stemmer(df_train)\nstemmer_corpus_test = clean_text_stemmer(df_test)","3c86a0a5":"from sklearn.feature_extraction.text import CountVectorizer\n\nstemmer_cv = CountVectorizer()\n\nX_train = stemmer_cv.fit_transform(stemmer_corpus_train).toarray()\nX_test = stemmer_cv.transform(stemmer_corpus_test).toarray()","6ef0ae3a":"#For lemmatizer\nnltk.download('wordnet') ","83ce87a7":"def clean_text_lemmatizer(data):\n\n  corpus = []\n\n  for i in range(0, len(data['text'].values)):\n\n    review = re.sub('[^a-z-A-Z]', ' ', data['text'][i])\n    review = review.lower()\n    review = review.split()\n\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n\n    lemmatizer = WordNetLemmatizer()\n    review = [lemmatizer.lemmatize(word) for word in review if not word in set(all_stopwords)]\n    review = ' '.join(review)\n\n    corpus.append(review)\n    \n  return corpus\n","ee557db5":"lem_corpus_train = clean_text_lemmatizer(df_train)\nlem_corpus_test = clean_text_lemmatizer(df_test)","920347d1":"from sklearn.feature_extraction.text import CountVectorizer\n\nlem_cv = CountVectorizer()\n\nX_train_lem = lem_cv .fit_transform(lem_corpus_train ).toarray()\nX_test_lem = lem_cv .transform(lem_corpus_test).toarray()","5f1aa416":"from sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)","0758f5e9":"naive_bayes_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nnaive_bayes_submission['target'] = y_pred\nnaive_bayes_submission.to_csv('naive_bayes_submission.csv', sep = ',', index = False)","ef537115":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nlog_reg_stem_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nlog_reg_stem_submission['target'] = y_pred\nlog_reg_stem_submission.to_csv('log_reg_stem_submission.csv', sep = ',', index=False)","4f506ef7":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train_lem, y_train)\n\ny_pred = classifier.predict(X_test_lem)\n\nlog_reg_lem_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nlog_reg_lem_submission['target'] = y_pred\nlog_reg_lem_submission.to_csv('log_reg_lem_submission.csv', sep = ',', index=False)","e8b305b9":"\n### Import Necessary Libraries","58f1e29e":"<a id= 11> <\/a>\n### Lemmatizer","647d7918":"I will use 2 clean text techniques.\n\nOne of them is stemmer, other is lemmatizer method.\n\nIn stemmer method, leaf and leaves refer to different words. But in lemmatizer method, leaf and leaves stand for same word.","93c11b4e":"<a id= 1> <\/a>\n# Import Libraries","14a51084":"<a id= 10> <\/a>\n### Stemmer","630b4c57":"<a id= 4> <\/a>\n## Handle Missing Values","91f9d4e6":"<a id= 6> <\/a>\n### Clean Texts with Stemmer","7d305e78":"<a id= 8> <\/a>\n# Naive Bayes Model\n\nFor Naive Bayes model, I will only use stemmer.","a6600e8f":"Lemmatizer gives better F1 accuracy which is 0.80018.","498c908f":"<a id= 2> <\/a>\n# Load the dataset","f6f45fd5":"# Introduction\nLooking at twitter dataset, We are expected to predict if a tweet is about disaster or not.\n\n## Content:\n1. [Import Libraries](#1)\n2. [Load the dataset](#2)\n3. [Data Preprocessing](#3)  \n  * [Handle Missing Values](#4)\n  * [Clean Texts](#5)\n    * [Clean Texts with Stemmer](#6)\n    * [Clean Texts with Lemmatizer](#7)\n4. [Naive Bayes](#8)\n5. [Logistic Regression](#9)\n  * [Stemmer](#10)\n  * [Lemmatizer](#11)  ","9d8f3fcb":"<a id= 5> <\/a>\n## Clean Texts","f71c3d25":"<a id= 7> <\/a>\n### Clean Texts with Lemmatizer","d60427d8":"<a id= 9> <\/a>\n## Logistic Regression","8d0901ef":"As you can see, there is no missing value in train set.","a5d3012d":"For logistic regression, I will use 2 different methods that I show above.","ee6c6f16":"<a id= 3> <\/a>\n# Data Preprocessing"}}