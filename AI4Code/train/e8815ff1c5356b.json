{"cell_type":{"e8e6e101":"code","498e8b3f":"code","a4cf7b76":"code","589136f0":"code","687b7e63":"code","18b422d7":"code","19d4b508":"code","917f2fe2":"code","faf49034":"code","ade67a5c":"code","b3f81745":"code","53db9374":"code","7d7d9821":"code","9d3a0bdd":"code","0fac0f31":"code","278b1a89":"code","a46ea23b":"code","47ff9318":"code","f7888d81":"code","3cd87f35":"code","4cc82bae":"code","324069f8":"code","cda8cada":"markdown","f213015b":"markdown"},"source":{"e8e6e101":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling","498e8b3f":"path = '..\/input\/100-bird-species'\n\ndef get_file_number(folder):\n    return sum([len(files) for r, d, files in os.walk(folder)])\n\n\nprint(get_file_number(os.path.join(path, 'train')))\nprint(get_file_number(os.path.join(path, 'valid')))\nprint(get_file_number(os.path.join(path, 'test')))","a4cf7b76":"train_dir = '..\/input\/100-bird-species\/train'\nvalid_dir = '..\/input\/100-bird-species\/valid'\ntest_dir = '..\/input\/100-bird-species\/test'","589136f0":"batch_size = 32 \n\n# instead of using ImageDataGenerator - image_dataset_from_directory() generates a tf.data.Dataset(usually faster than the ImageDataGenerator) from image files in a directory\n# image_size must be provided, here: reducing intital size of (224, 224) to (128, 128)\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(train_dir, label_mode='categorical', image_size=(128, 128), batch_size=batch_size)\nvalid_dataset = tf.keras.utils.image_dataset_from_directory(valid_dir, label_mode='categorical', image_size=(128, 128), batch_size=batch_size)\ntest_dataset = tf.keras.utils.image_dataset_from_directory(test_dir, label_mode='categorical', image_size=(128, 128), batch_size=batch_size)","687b7e63":"label_names = train_dataset.class_names\nprint(label_names)","18b422d7":"plt.figure(figsize=(25, 20))\n\nfor images, labels in train_dataset.take(1):\n    for i in range(6):\n        ax = plt.subplot(1, 6, i+1)\n        plt.imshow(images[i].numpy().astype('int'))\n        plt.title(train_dataset.class_names[tf.argmax(labels[i])]) # this works to display the labels according to the images\n        #plt.title(label_name[labels[i]]) doesnt work\n        plt.grid()\n        plt.axis('on')","19d4b508":"'''If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding images.\nOtherwise, it yields a tuple (images, labels), where images has shape (batch_size, image_size[0], image_size[1], num_channels).'''\nfor image_batch, labels_batch in train_dataset:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","917f2fe2":"# normalize the dataset to get (input) values between 0-1\ndef normalize_data(dataset):\n    # To rescale an input in the [0, 255] range to be in the [0, 1] range, you would pass scale=1.\/255.\n    normalization_layer = Rescaling(1.\/255)\n    normalized_dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n    \n    return normalized_dataset\n\n# normalize every dataset\nnormalized_training_data = normalize_data(train_dataset)\nnormalized_validation_data = normalize_data(valid_dataset)\nnormalized_testing_data = normalize_data(test_dataset)","faf49034":"# testing out the noramlization results\nimage_batch, labels_batch = next(iter(normalized_training_data))\nfirst_image = image_batch[0]\nprint(np.min(first_image), np.max(first_image)) # print scaled(!) minimum value (0-1)","ade67a5c":"'''Configure the dataset for performance\nLet's make sure to use buffered prefetching so you can yield data from disk without having I\/O become blocking. These are two important methods you should use when loading data:\n\nDataset.cache keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. \nIf your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\nDataset.prefetch overlaps data preprocessing and model execution while training.\n\nMost dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. \nThis often improves latency and throughput, at the cost of using additional memory to store prefetched elements.'''\n\nAUTOTUNE = tf.data.AUTOTUNE\nnormalized_training_data = normalized_training_data.cache().prefetch(buffer_size=AUTOTUNE)\nnormalized_validation_data = normalized_validation_data.cache().prefetch(buffer_size=AUTOTUNE)","b3f81745":"# some random model\nnum_classes = 315\n\nmodel = Sequential()\nmodel.add(Conv2D(32, 3, activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(64, 3, activation='relu'))\nmodel.add(Conv2D(64, 3, activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))","53db9374":"# here: loss ='categorical_crossentropy' because in the dataset generator label_mode='categorical'\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","7d7d9821":"history = model.fit(normalized_training_data, epochs=5, validation_data=normalized_validation_data)","9d3a0bdd":"plt.figure(figsize=(10,8))\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.grid(True)\nplt.legend(['accuracy', 'val_accuracy'])\nplt.xlabel('Epochs')\nplt.title('1st Model Accuracy')\nplt.show()\n\nplt.figure(figsize=(10,8))\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend(['loss', 'val_loss'])\nplt.grid()\nplt.xlabel('Epochs')\nplt.title('1st Model Loss')","0fac0f31":"model.evaluate(test_dataset, verbose=1)","278b1a89":"test_labels = test_dataset.class_names\n\nplt.figure(figsize=(25, 20))\n\nfor image, label in normalized_testing_data.take(1):\n    model_prediction = model.predict(image)\n    for i in range(6):\n        plt.subplot(1, 6, i+1)\n        plt.imshow(image[i].numpy().astype(\"float\")) # dtype should be float after normalization!\n        plt.title(f\"Prediction: {test_labels[tf.argmax(tf.round(model_prediction[i]))]}\\n Original Labels : {test_labels[tf.argmax(label[i])]}\")\n        plt.grid(True)\n        plt.axis(\"on\")\n    plt.show()","a46ea23b":"# loading the already built model (efficientNetB3 that is stored inside the folder) - get some pretty good score :D\nnew_model = tf.keras.models.load_model('..\/input\/100-bird-species\/EfficientNetB3-birds-98.92.h5')","47ff9318":"#new_model.summary()","f7888d81":"new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","3cd87f35":"new_model.fit(normalized_training_data, epochs=5, validation_data=normalized_validation_data)","4cc82bae":"new_model_evaluation = new_model.evaluate(normalized_testing_data, verbose=1)","324069f8":"test_labels = test_dataset.class_names\n\nplt.figure(figsize=(25, 20))\n\nfor image, label in normalized_testing_data.take(1):\n    new_model_prediction = new_model.predict(image)\n    for i in range(6):\n        plt.subplot(1, 6, i+1)\n        plt.imshow(image[i].numpy().astype(\"float\")) # dtype should be float after normalization!\n        plt.title(f\"Prediction: {test_labels[tf.argmax(tf.round(new_model_prediction[i]))]}\\n Original Labels : {test_labels[tf.argmax(label[i])]}\")\n        plt.grid(True)\n        plt.axis(\"on\")\n    plt.show()","cda8cada":"**Loading the existing model (EfficientNetB3)**","f213015b":"Well, the above model seems to be not very good and there is definetly room for improvement. Now, loading the provided pre-built model (EfficientNet) should get better results.\n\nNotes on first model:\n- More Epochs\n- Preprocessing could also be extended"}}