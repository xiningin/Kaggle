{"cell_type":{"e4b3538b":"code","3fce3329":"code","cb25f857":"code","1866ef59":"code","00f26e59":"code","62227ef1":"code","9bd930b2":"code","0a362ffb":"code","43ad00e3":"code","abee8d84":"code","081f0b35":"code","9bdedf19":"code","ca881b96":"code","8b382536":"code","c1a86f6b":"code","d669f20b":"code","4f26594a":"code","3b1d5758":"code","dc406718":"code","c38faf4a":"code","ded4774f":"code","6de0ba64":"code","97948dc4":"code","f89603e5":"code","3a1721e3":"code","a41e78dc":"code","ece8988e":"code","90c886f4":"code","fbd69326":"markdown","1422c041":"markdown","0ba856ae":"markdown","f892517c":"markdown","7f5ad93c":"markdown","30f681bb":"markdown","ad762b0e":"markdown","9d0620e6":"markdown","a8e3db05":"markdown","96acb679":"markdown"},"source":{"e4b3538b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport regex\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3fce3329":"train_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain_data.head()","cb25f857":"print('The train set contains {0} rows and {1} columns '.format(train_data.shape[0],train_data.shape[1]))","1866ef59":"ax=sns.countplot(data=train_data,x=train_data['target'])\nplt.xlabel('Target Variable- Disaster or not disaster tweet')\nplt.ylabel('Count of tweets')\nplt.title('Count of disaster and non-disaster tweets')\ntotal = len(train_data)\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\n\n#https:\/\/stackoverflow.com\/questions\/33179122\/seaborn-countplot-with-frequencies\n# Study the document for matplotlib","00f26e59":"train_data['keyword']=train_data.keyword.str.replace('%20','_')\ntrain_data['keyword'] = train_data['keyword'].replace(np.nan, '', regex=True)","62227ef1":"#plt.figure(figsize=(100,50))\n\n#sns.catplot(data=train_data,y='keyword',col='target',height=200,aspect=0.5,kind='count')","9bd930b2":"#pd.set_option('display.max_rows' ,None)\nkeyword_count=pd.DataFrame(train_data.groupby(['keyword','target']).agg(['count']).sort_values(by=('id', 'count'),ascending=False)[('id', 'count')])","0a362ffb":"keyword_count.columns","43ad00e3":"\n#keyword_count['keyword_value']=keyword_count.keyword[0][0]\n'''\nkeyword_count['keyword']=keyword_count.index\nkeyword_count.columns=keyword_count.columns.droplevel()\nkeyword_count.columns=['count','keyword']\n\nfor value in range(0,len(keyword_count)):\n    #print(keyword_count.keyword[value][0])\n    if 'keyword_value' not in keyword_count.columns:\n        keyword_count['keyword_value']=keyword_count.keyword[0][0]\n    else:\n        keyword_count['keyword_value'][value]=keyword_count.keyword[value][0]\n    #print(keyword_count.keyword[value][1])\n    if 'target_value' not in keyword_count.columns:\n        keyword_count['target_value']=keyword_count.keyword[0][1]\n    else:\n        keyword_count['target_value'][value]=keyword_count.keyword[value][1]\n\nif 'keyword' in keyword_count.columns:\n    keyword_count=keyword_count.drop(['keyword'],axis=1)\n#Index(['count', 'keyword', 'keyword_value', 'target_value'], dtype='object')\n'''","abee8d84":"keyword_count","081f0b35":"wordcloud = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=80, \n                          random_state=42,\n    collocations=False,\n    colormap=\"Oranges_r\"\n                         ).generate(' '.join(train_data[train_data['target']==1]['keyword']))\n#.join(text2['Crime Type']))\n\nplt.figure(figsize=(10,10))\nplt.title('Major keywords for disaster tweets', fontsize=30)\nplt.imshow(wordcloud)\n\nplt.axis('off')\nplt.show()","9bdedf19":"wordcloud = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=40, \n                            collocations=False,\n    colormap=\"PuOr\"\n                         ).generate(' '.join(train_data[train_data['target']==0]['keyword']))\n#.join(text2['Crime Type']))\n\nprint(wordcloud)\nplt.figure(figsize=(10,25))\nplt.imshow(wordcloud)\nplt.title('Major keywords for non-disaster tweets', fontsize=30)\nplt.axis('off')\nplt.show()","ca881b96":"train_data['location'].value_counts()","8b382536":"#pd.set_option('display.max_rows' ,None)\ntrain_data['location']","c1a86f6b":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    stem_words=[stemmer.stem(w) for w in filtered_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    return \" \".join(filtered_words)","d669f20b":"train_data['location_cleaned']=train_data['location'].map(lambda s:preprocess(s))","4f26594a":"#train_data['location_cleaned']","3b1d5758":"train_data[\"location_cleaned\"].replace({\"united states\": \"usa\", \n                                        \"world\": \"worldwide\",\n                                        \"nyc\":\"new york\",\n                                       \"california usa\":\"california\",\n                                        \"new york city\":\"new york\",\n                                        \"california united states\":\"california\",\n                                        \"mumbai\":\"india\"\n                                       }, inplace=True)","dc406718":"train_data['location_cleaned'].value_counts().nlargest(20)","c38faf4a":"#Preprocessing text\ntrain_data['text_cleaned']=train_data['text'].map(lambda s:preprocess(s)) \ntest_data['text_cleaned']=test_data['text'].map(lambda s:preprocess(s))","ded4774f":"#train_data\ntrain_text = train_data['text_cleaned']\ntest_text = test_data['text_cleaned']\ntrain_target = train_data['target']\nall_text = train_text.append(test_text)","6de0ba64":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(all_text)\n\ncount_vectorizer = CountVectorizer()\ncount_vectorizer.fit(all_text)\n\ntrain_text_features_cv = count_vectorizer.transform(train_text)\ntest_text_features_cv = count_vectorizer.transform(test_text)\n\ntrain_text_features_tf = tfidf_vectorizer.transform(train_text)\ntest_text_features_tf = tfidf_vectorizer.transform(test_text)","97948dc4":"train_text.head()","f89603e5":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 2018)\ntest_preds = 0\noof_preds = np.zeros([train_data.shape[0],])\n\nfor i, (train_idx,valid_idx) in enumerate(kfold.split(train_data)):\n    x_train, x_valid = train_text_features_tf[train_idx,:], train_text_features_tf[valid_idx,:]\n    y_train, y_valid = train_target[train_idx], train_target[valid_idx]\n    classifier = LogisticRegression()\n    print('fitting.......')\n    classifier.fit(x_train,y_train)\n    print('predicting......')\n    print('\\n')\n    oof_preds[valid_idx] = classifier.predict_proba(x_valid)[:,1]\n    test_preds += 0.2*classifier.predict_proba(test_text_features_tf)[:,1]","3a1721e3":"pred_train = (oof_preds > .25).astype(np.int)\nf1_score(train_target, pred_train)","a41e78dc":"#submission1 = pd.DataFrame.from_dict({'id': test['id']})\n#submission1['prediction'] = (test_preds>0.25).astype(np.int)\n#submission1.to_csv('submission.csv', index=False)\n#submission1['prediction'] = (test_preds>0.25)","ece8988e":"submission1=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission1['target'] = (test_preds>0.25).astype(np.int)\nsubmission1.to_csv('submission.csv', index=False)\nsubmission1['target'] = (test_preds>0.25)\nsubmission1['target']=submission1['target'].map(lambda x:int(x==True))","90c886f4":"submission1.head()","fbd69326":"Above code represents the code for the non-disaster tweets.As one can see, the list is quite different from the disaster tweets.\n# Also, the level of words is weaker than the tweets keywords given in the disaster tweets.These keywords are more of generalised words taht have been used like **body bag,death,siren,wrecked etc**.","1422c041":"Above code specifies that these are the major words with the highest no. of frequencies when it has been mentioned for disaster tweets.\n# \n# I will list out some of them :\n#     **forest fire,death,flood,derailment,outbreak,typhoon**","0ba856ae":"Since train set contains 5 columns with target variables, I can do preliminary analysis by using the following method :\n#     \n#     1. Understand on the no. of tweets which are disaster related and which are not.Bar plots would do in this case.\n#     2. How keyword and location helps in the tweets and would they help in prediction or not.\n#     3. Filtering out the stop words(words that can be filtered out as they may not be needed for the analysis)\n#     4. Checking the words present in the tweet and then analysing based on the target variable.","f892517c":"1. \/kaggle\/input\/nlp-getting-started\/train.csv\n# 2. \/kaggle\/input\/nlp-getting-started\/test.csv\n# 3. \/kaggle\/input\/nlp-getting-started\/sample_submission.csv","7f5ad93c":"## Tweet analysis based on Location\n \nWe need to first clean the data for the location variable leaving out all the un-necessary words.","30f681bb":"Seeing from the tweets we can do an analysis as it can be construed an imbalanced dataset.In the initial analysis where I have went through the dataset I found out that the data needs to be cleaned **a lot** .For this, I am going column wise i.e. \n \n **keyword --> location --> tweet **\n \nIn the keywords column I found out that there are %20 added between words .I am thinking this has been fetched from the search strings hence I was thinking I remove it and replace it with *_* to make it look presentable.","ad762b0e":"Most of the location tweets weren't part of the tweet.That amounts to be around **2700 tweets** that have no location. Apart from that USA based location tweets for USA,New York,Washington,California,Chicago that amounts to be around **450 tweets**. London has approx. **40 tweets**.India has over **50 tweets**.","9d0620e6":"1. 1. Above graph is not that clear, I will try to analyse in a tabular manner","a8e3db05":"Important Docs:\n\n Thanking the below people to help me understand how to approach the problem.\n \n     1. (Beginner's guide)(https:\/\/www.kaggle.com\/frtgnn\/beginner-s-stop-to-text-data-a-simple-intro)\n     2. https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n     3. https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model\n     4.https:\/\/stackoverflow.com\/questions\/16645799\/how-to-create-a-word-cloud-from-a-corpus-in-python\n     5.https:\/\/www.geeksforgeeks.org\/generating-word-cloud-in-python-set-2\/\n     6. https:\/\/stackoverflow.com\/questions\/54396405\/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb\/54398984","96acb679":"Location"}}