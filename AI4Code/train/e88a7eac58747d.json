{"cell_type":{"daaf1079":"code","020be54c":"code","61b02f43":"code","4ec59caf":"code","26260bb9":"code","f5f4eed9":"code","bfd58fa1":"markdown","c888e4d5":"markdown","5b826156":"markdown","4291c0b0":"markdown","08368e65":"markdown","371d051d":"markdown","1518bf5b":"markdown"},"source":{"daaf1079":"!pip install transformers datasets accelerate ","020be54c":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\nmlm_data = train[['excerpt']]\nmlm_data = mlm_data.rename(columns={'excerpt':'text'})\nmlm_data.to_csv('mlm_data.csv', index=False)\n\nmlm_data_val = test[['excerpt']]\nmlm_data_val = mlm_data_val.rename(columns={'excerpt':'text'})\nmlm_data_val.to_csv('mlm_data_val.csv', index=False)","61b02f43":"import argparse\nimport logging\nimport math\nimport os\nimport random\n\nimport datasets\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\nfrom accelerate import Accelerator\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING, \n    MODEL_MAPPING, \n    AdamW, \n    AutoConfig, \n    AutoModelForMaskedLM, \n    AutoTokenizer, \n    DataCollatorForLanguageModeling, \n    SchedulerType, \n    get_scheduler, \n    set_seed\n)\n\nlogger = logging.getLogger(__name__)\nMODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n# from pprint import pprint\n# pprint(MODEL_TYPES, width=3, compact=True)","4ec59caf":"class TrainConfig:\n    train_file= 'mlm_data.csv'\n    validation_file = 'mlm_data.csv'\n    validation_split_percentage= 5\n    pad_to_max_length= True\n    model_name_or_path= 'roberta-base'\n    config_name= 'roberta-base'\n    tokenizer_name= 'roberta-base'\n    use_slow_tokenizer= True\n    per_device_train_batch_size= 8\n    per_device_eval_batch_size= 8\n    learning_rate= 5e-5\n    weight_decay= 0.0\n    num_train_epochs= 1 # change to 5\n    max_train_steps= None\n    gradient_accumulation_steps= 1\n    lr_scheduler_type= 'constant_with_warmup'\n    num_warmup_steps= 0\n    output_dir= 'output'\n    seed= 2021\n    model_type= 'roberta'\n    max_seq_length= None\n    line_by_line= False\n    preprocessing_num_workers= 4\n    overwrite_cache= True\n    mlm_probability= 0.15\n\nconfig = TrainConfig()\n\nif config.train_file is not None:\n    extension = config.train_file.split(\".\")[-1]\n    assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\nif config.validation_file is not None:\n    extension = config.validation_file.split(\".\")[-1]\n    assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\nif config.output_dir is not None:\n    os.makedirs(config.output_dir, exist_ok=True)","26260bb9":"def main():\n    args = TrainConfig()\n    accelerator = Accelerator()\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m\/%d\/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    data_files = {}\n    if args.train_file is not None:\n        data_files[\"train\"] = args.train_file\n    if args.validation_file is not None:\n        data_files[\"validation\"] = args.validation_file\n    extension = args.train_file.split(\".\")[-1]\n    if extension == \"txt\":\n        extension = \"text\"\n    raw_datasets = load_dataset(extension, data_files=data_files)\n    \n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name)\n    elif config.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n    \n    if args.model_name_or_path:\n        model = AutoModelForMaskedLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMaskedLM.from_config(config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    if args.max_seq_length is None:\n        max_seq_length = tokenizer.model_max_length\n        if max_seq_length > 1024:\n            logger.warning(\n                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n            )\n            max_seq_length = 1024\n    else:\n        if args.max_seq_length > tokenizer.model_max_length:\n            logger.warning(\n                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n            )\n        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function,\n        batched=True,\n        num_proc=args.preprocessing_num_workers,\n        remove_columns=column_names,\n        load_from_cache_file=not args.overwrite_cache,\n    )\n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length \/\/ max_seq_length) * max_seq_length\n        result = {\n            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n            for k, t in concatenated_examples.items()\n        }\n        return result\n\n    tokenized_datasets = tokenized_datasets.map(\n        group_texts,\n        batched=True,\n        num_proc=args.preprocessing_num_workers,\n        load_from_cache_file=not args.overwrite_cache,\n    )\n    train_dataset = tokenized_datasets[\"train\"]\n    eval_dataset = tokenized_datasets[\"validation\"]\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) \/ args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps \/ num_update_steps_per_epoch)\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss \/ args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        losses = losses[: len(eval_dataset)]\n        perplexity = math.exp(torch.mean(losses))\n\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)","f5f4eed9":"if __name__ == \"__main__\":\n    main()","bfd58fa1":"#### Code Reference: \n`Transformer Examples` - https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/pytorch\/language-modeling\/run_mlm_no_trainer.py\n\n> 90-95% of the code is from this great `run_mlm_no_trainer.py script` from `HuggingFace Examples Repository`. I have merely `changed few lines to adjust the code according to my task`. \n    \n    P.S. Make sure to understand everything instead of blindly copying the code.","c888e4d5":"### Install Dependencies","5b826156":"### Load CommonLit Readability Data","4291c0b0":"### Import Dependencies","08368e65":"### Run","371d051d":"## Further Pre-training\n\nBesides the training data of a target task, we can further pre-train a transformer on the data from the same domain.\n\n![image.png](https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-32381-3_16\/MediaObjects\/489562_1_En_16_Fig1_HTML.png)\n\nThe Transformer models are pre-trained on the general domain corpus. For a text classification task \/ regression task in a specific domain, such as Readability Assesment, its data\ndistribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n\n1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n\n2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n\n3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n\n#### Reference: [How to finetune BERT for Text Classification ?](https:\/\/arxiv.org\/pdf\/1905.05583.pdf)\n\n> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies.","1518bf5b":"### Config"}}