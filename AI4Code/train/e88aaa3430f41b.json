{"cell_type":{"8364ff3a":"code","4fb7a453":"code","c04979ee":"code","0f2276dc":"code","e13b10bc":"code","7ccb57b5":"code","7ef4c55f":"code","990802b1":"code","b5bf9f7d":"code","4c2e598f":"code","45b1ad13":"code","d62e81a5":"code","8f1535cf":"code","0e8f9d49":"code","1be7b425":"code","6f33feee":"code","88a201d9":"code","5df8ed86":"code","f9a4d2b3":"code","2315ad08":"code","40b1aa9a":"markdown","7b21ee65":"markdown","0977372c":"markdown","4cecc585":"markdown","e72f466d":"markdown","88f0ac6b":"markdown","c4c19f07":"markdown","1cfefc41":"markdown","b34039d8":"markdown","b8d2396f":"markdown","4721cf83":"markdown","70086a1c":"markdown"},"source":{"8364ff3a":"import pandas as pd\nimport numpy as np\nimport pickle as pk\nimport torch\n\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n!pip install deepctr_torch\n","4fb7a453":"\nfrom deepctr_torch.models import DeepFM, xDeepFM, AutoInt, PNN\nfrom deepctr_torch.inputs import  SparseFeat, DenseFeat, get_feature_names\n\npd.set_option('display.max_columns', 500)\n\nimport warnings\nwarnings.filterwarnings('ignore')","c04979ee":"\nwith open('..\/input\/ieeefraud\/train.pickle', 'rb') as file:\n    train =pk.load(file)\n    \nwith open('..\/input\/ieeefraud\/test.pickle', 'rb') as file:\n    test =pk.load(file)\n    \nfile.close()","0f2276dc":"target = ['isFraud']\n\nsparse_features = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',  'addr1', \n'addr2', 'P_emaildomain', 'R_emaildomain',  'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', \n'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', \n'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', \n'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',]\n\nnotfeature = [\"TransactionID\"]\n\ndense_features = [i for i in train.columns if i not in sparse_features+target+notfeature]\n\ntest['isFraud'] = 0\ndata = pd.concat([train, test], axis=0, sort=False)\nprint(train.shape, test.shape, data.shape)","e13b10bc":"# train.fillna(-10, inplace=True)\n# test.fillna(-10, inplace=True)\ndata[sparse_features] = data[sparse_features].fillna(\"null\",)\ndata[dense_features] = data[dense_features].fillna(-999,)","7ccb57b5":"data[sparse_features] = data[sparse_features].astype(\"str\")\n\n# for feat in dense_features:\n#     print(\"start pro \", feat)\n#     if data[feat].dtype == 'O':\n#         print(\"object -> float\", feat)\n#         data[feat] = data[feat].astype(\"float\")","7ef4c55f":"#del train, test","990802b1":"# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n\nfor feat in sparse_features:\n    # print(\"Process: \", feat)\n    lbe = LabelEncoder()\n    data[feat] = lbe.fit_transform(data[feat])\nprint(\"Process sparse finish \")\n\nmms = MinMaxScaler(feature_range=(0, 1))\nfor feat in dense_features:\n    print(\"Process: \", feat)\n    data[feat] = mms.fit_transform(np.array(data[feat]).reshape(-1, 1))\nprint(\"Process dense finish \")\n\n# 2.count #unique features for each sparse field,and record dense feature field name\n\nfixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n                       for feat in sparse_features] + [DenseFeat(feat, 1,)\n                      for feat in dense_features]\n\ndnn_feature_columns = fixlen_feature_columns\nlinear_feature_columns = fixlen_feature_columns\n\nfixlen_feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n","b5bf9f7d":"# 3.generate input data for model\n\n#train, test = train_test_split(data, test_size=0.2)\ntrain = data[:590540]\ntest = data[590540:]\nprint(train.shape, test.shape, data.shape)\n\ntrain_model_input = [train[name] for name in fixlen_feature_names]\ntest_model_input = [test[name] for name in fixlen_feature_names]","4c2e598f":"# import warnings\n# warnings.filterwarnings('ignore')\n\nbs = 1024\ndevice = 'cpu'\nuse_cuda = True\nif use_cuda and torch.cuda.is_available():\n    print('cuda ready...')\n    device = 'cuda:0'\n\nmodel = AutoInt(dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\",\n              metrics=['auc'], )\n\nhistory = model.fit(train_model_input, train[target].values,\n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nAI_pred_trn = model.predict(train_model_input, batch_size=bs)\nAI_pred_ans = model.predict(test_model_input, batch_size=bs)","45b1ad13":"# import warnings\n# warnings.filterwarnings('ignore')\n\n\nmodel = xDeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\",\n              metrics=['auc'], )\n\nhistory = model.fit(train_model_input, train[target].values,\n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nXD_pred_trn = model.predict(train_model_input, batch_size=bs)\nXD_pred_ans = model.predict(test_model_input, batch_size=bs)","d62e81a5":" \nbs = 1024\nmodel = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\", \n              metrics=['auc', 'logloss'], )\n\nhistory = model.fit(train_model_input, train[target].values, \n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nDF_pred_trn = model.predict(train_model_input, batch_size=bs)\nDF_pred_ans = model.predict(test_model_input, batch_size=bs)\n\n","8f1535cf":"bs = 1024\nmodel = PNN(dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\", \n              metrics=['auc', 'logloss'], )\n\nhistory = model.fit(train_model_input, train[target].values, \n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nPN_pred_trn = model.predict(train_model_input, batch_size=bs)\nPN_pred_ans = model.predict(test_model_input, batch_size=bs)","0e8f9d49":"trn_stack = train[['TransactionID','isFraud']]\ntest['isFraud'] = 0\ntet_stack = test[[\"TransactionID\", \"isFraud\"]]\n\ntrn_stack['AI_pred'] = AI_pred_trn\ntet_stack['AI_pred'] = AI_pred_ans\n\ntrn_stack['XD_pred'] = XD_pred_trn\ntet_stack['XD_pred'] = XD_pred_ans\n\ntrn_stack['DF_pred'] = DF_pred_trn\ntet_stack['DF_pred'] = DF_pred_ans\n\ntrn_stack['PN_pred'] = PN_pred_trn\ntet_stack['PN_pred'] = PN_pred_ans","1be7b425":"train_fe = [i for i in trn_stack.columns if i not in ['TransactionID', 'isFraud']]\ntrn_stack.describe()","6f33feee":"tet_stack.describe()","88a201d9":"from sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold\nimport lightgbm as lgb\nimport gc\n\ndef make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=3):\n    folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True)\n    # tr_df['VLABEL'] = 0\n\n    X, y = tr_df[features_columns], tr_df[target]\n    P, P_y = tt_df[features_columns], tt_df[target]\n\n    tt_df = tt_df[['TransactionID', target]]\n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros((len(tr_df),1))\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:', fold_)\n        tr_x, tr_y = X.iloc[trn_idx, :], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx, :], y[val_idx]\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)\n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets=[vl_data],\n            verbose_eval=200,\n        )\n        \n        oof[val_idx] = estimator.predict(vl_x).reshape(-1, 1)\n        pp_p = estimator.predict(P)\n        # Y_label = estimator.predict(X)\n        predictions += pp_p \/ NFOLDS\n        # tr_df['VLABEL'] += Y_label \/ NFOLDS\n\n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(), X.columns)),\n                                       columns=['Value', 'Feature'])\n        print(feature_imp)\n\n    # tr_df[['TransactionID', 'VLABEL']].to_csv('submission.csv', index=False)\n    tt_df['prediction'] = predictions\n\n    return tt_df, oof","5df8ed86":"lgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'n_estimators':800,\n                    'verbose':-1,\n                }\n \ntest_predictions, oof = make_predictions(trn_stack, tet_stack, train_fe, 'isFraud', lgb_params, NFOLDS=2)","f9a4d2b3":"test_predictions['isFraud'] = test_predictions['prediction']\ntest_predictions.sort_values('TransactionID', inplace=True)\ntest_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","2315ad08":"test_predictions.head()","40b1aa9a":"## PNN","7b21ee65":"Just use origin feature, dataset is created by merge identity.csv and transaction.csv from train and test ","0977372c":"# Stack\n\nstack these for model, and use feature importance to judge good and bad","4cecc585":"# Single Model","e72f466d":"# Some models can make more useful feature\nIn this kernel. I will try DeepCtr model such as DeepFm, Xdeepfm, AutoInt and PNN model.\n- fm component of Deepfm can create cross feature\uff1a order-2 feature interactions\n- WHile Product component of PNN can create: feature interactions viewd as a series of multiplication operations\n\nIt means PNN can create group feature in this competition.\n- group feature such like \n    ```python\n    newfeature =  df['feature'].groupby(by = ['card1','card2','card3']).count()\n    ```\n\nAnd now, let us show PNN whether is better than other model?<br>\nAnd then, discover whether group feature is more useful than order-2 feature interactions.","88f0ac6b":"# Conclusion\nYou can find that PNN model can get the highest importance among these four models","c4c19f07":"## DeepFM\n- 0.87","1cfefc41":"## auto int\n\n- use self-attention to learn feature iteraction ","b34039d8":"## xDeepFM","b8d2396f":"# BTW \nOf course, you can make more feature you create, and use my backbone to train.<br>\nI just use origin feature, so the score will not good enough","4721cf83":"I will use deepctr-torch , because it is really easy to use","70086a1c":"> # Prepocess Data\n- make data can be fed into NN model"}}