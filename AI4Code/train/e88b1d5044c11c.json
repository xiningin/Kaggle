{"cell_type":{"170cc198":"code","29c0dce4":"code","8e3c6e2b":"code","2c62d855":"code","9c59ebdb":"code","f0fc44a7":"code","43eb15d7":"code","e1455138":"code","6e736b90":"code","dc790823":"code","ec70828c":"code","7aabe1c5":"code","599c5d46":"code","635f4592":"code","7c37442b":"code","fbab2484":"code","f2873b49":"code","af1b20b9":"code","5562be5c":"code","b97e04bd":"code","9b7d156b":"code","51e77b77":"code","93f3a033":"code","296092e8":"code","b6684830":"code","638d62bf":"code","d4f0cb09":"code","3b74c5a2":"code","07d25cc0":"code","e0137a7c":"code","2ceea8a0":"code","7f13b188":"code","c602f0dd":"markdown","6ab75350":"markdown","f391a9a8":"markdown","45066207":"markdown","1f089727":"markdown"},"source":{"170cc198":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","29c0dce4":"\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n#from nltk.corpus import stopwords\n#from nltk.stem.porter import PorterStemmer\n\nimport tokenization\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8e3c6e2b":"BASE_PATH = \"\/kaggle\/input\/vso-closed\/\"","2c62d855":"train =pd.read_csv(BASE_PATH + \"VSO_Closed_subcategory.csv\")\ntrain.head()","9c59ebdb":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","f0fc44a7":"vocab_file = \"\/kaggle\/input\/vocabline\/vocab1.txt\"\ndo_lower_case = True\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","43eb15d7":"text = \"DLP Policy is not working\"\ntokenize_ = tokenizer.tokenize(text)\nprint(\"Text after tokenization: \")\nprint(tokenize_)\nmax_len = 512\n\ntext = tokenize_[:max_len-2]\ninput_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\npad_len = max_len - len(input_sequence)\n\nprint(\"After adding [CLS] and [SEP]: \")\nprint(input_sequence)\ntokens = tokenizer.convert_tokens_to_ids(input_sequence)\nprint(\"After converting Tokens to Id: \")\nprint(tokens)\ntokens += [0] * pad_len\nprint(\"tokens: \")\nprint(tokens)\npad_masks = [1] * len(input_sequence) + [0] * pad_len\nprint(\"Pad Masking: \")\nprint(pad_masks)\nsegment_ids = [0] * max_len\nprint(\"Segment Ids: \")\nprint(segment_ids)","e1455138":"def pre_Process_data(documents, tokenizer, max_len=512):\n    '''\n    For preprocessing we have regularized, transformed each upper case into lower case, tokenized,\n    Normalized and remove stopwords. For normalization, we have used PorterStemmer. Porter stemmer transforms \n    a sentence from this \"love loving loved\" to this \"love love love\"\n    \n    '''\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    print(\"Pre-Processing the Data.........\\n\")\n    for data in documents:\n        review = re.sub('[^a-zA-Z]', ' ', data)\n        url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n        review = url.sub(r'',review)\n        html=re.compile(r'<.*?>')\n        review = html.sub(r'',review)\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        review = emoji_pattern.sub(r'',review)\n        text = tokenizer.tokenize(review)\n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","6e736b90":"input_word_id = Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\nsegment_id = Input(shape=(max_len,), dtype=tf.int32, name = \"segment_id\")\n\n_, sequence_output = bert_layer([input_word_id, input_mask, segment_id])\nclf_output = sequence_output[:, 0, :]\nmodel = Model(inputs=[input_word_id, input_mask, segment_id],outputs=clf_output)\nmodel.compile(Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\nprint(\"shape of _ layer of BERT: \"+str(_.shape))\nprint(\"shape of last layer of BERT: \"+str(sequence_output.shape))","dc790823":"def build_model(bert_layer, max_len=512):\n    input_word_id = Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_id = Input(shape=(max_len,), dtype=tf.int32, name = \"segment_id\")\n    \n    _, sequence_output = bert_layer([input_word_id, input_mask, segment_id])\n    clf_output = sequence_output[:, 0, :]\n    dense_layer1 = Dense(units=256,activation='relu')(clf_output)\n    dense_layer1 = Dropout(0.4)(dense_layer1)\n    dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n    dense_layer2 = Dropout(0.4)(dense_layer2)\n    out = Dense(7, activation='sigmoid')(dense_layer2)\n    \n    model = Model(inputs=[input_word_id, input_mask, segment_id],outputs=out)\n    model.compile(Adam(lr=2e-5), loss='categorical_crossentropy',metrics=['accuracy'])\n    \n    return model","ec70828c":"import keras\ntrain['merged_clean'].fillna(\" \",inplace=True)\ntrain_input = pre_Process_data(train.merged_clean.values, tokenizer, max_len=512)\nx_train = list(train_input)\nx_train = np.asarray(x_train)\ny_train = list(train['BroadClassification'])\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(y_train)\n\ndef encode(le, labels):\n    enc = le.transform(labels)\n    return keras.utils.to_categorical(enc)\n\ny_enc = encode(le, y_train)\ny_train = np.asarray(y_enc)","7aabe1c5":"print(y_train.shape)","599c5d46":"model = build_model(bert_layer, max_len=512)\nmodel.summary()","635f4592":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\ntrain_history = model.fit(\n    train_input, y_train,\n    validation_split=0.2,\n    epochs=12,\n    callbacks=[checkpoint],\n    batch_size=16\n)","7c37442b":"submission = pd.read_csv(\"\/kaggle\/input\/vsoactive\/VSOActive.csv\")\nsubmission.head()","fbab2484":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport time\nfrom nltk import FreqDist\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\ndef initial_clean(text):\n    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n    text = text.lower() # lower case the text\n    text = nltk.word_tokenize(text)\n    return text\n\nstop_words = stopwords.words('english')\ndef remove_stop_words(text):\n    return [word for word in text if word not in stop_words]\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\ndef stem_words(text):\n    try:\n        text = [stemmer.stem(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except IndexError: # the word \"oed\" broke this, so needed try except\n        pass\n    return text\n\ndef apply_all(text):\n    ans=\"\"\n    for x in stem_words(remove_stop_words(initial_clean(text))):\n      ans = ans + x +' '\n    return ans","f2873b49":"model.load_weights('.\/model.h5')\nsubmission['merged']=submission['Title']+\" \"+submission['Description']\nsubmission['merged'].fillna(\" \",inplace=True)\nsubmission['merged_clean']=submission['merged'].apply(apply_all)\ntrain_input = pre_Process_data(submission.merged_clean.values, tokenizer, max_len=512)\ntest_pred = model.predict(train_input)\ndef decode(le, one_hot):\n    dec = np.argmax(one_hot, axis=1)\n    return le.inverse_transform(dec)\npred=decode(le, test_pred)","af1b20b9":"p=submission['BroadClassification'].values\ncount=0\nfor i in range(len(p)):\n    if(p[i]==pred[i]):\n        count=count+1\n    else:\n        print(p[i],\"-->\",pred[i])\nprint(count\/len(pred))","5562be5c":"print(count)","b97e04bd":"for layers in model.layers:\n    print(layers.name)","9b7d156b":"layer_name = 'tf_op_layer_strided_slice_1'\n#model.load_weights('.\/model.h5')  \nmodel.output_hidden_states=True\n# with tf.Session() as session:\n#     tf.keras.backend.get_session(session)\n#     session.run(tf.global_variables_initializer())\n#     session.run(tf.tables_initializer())\nintermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\nintermediate_output8 = intermediate_layer_model.predict(train_input)","51e77b77":"print(intermediate_output8.shape)","93f3a033":"submission=pd.read_csv('\/kaggle\/input\/vso-closed\/VSO_Closed_subcategory.csv')\nsubmission['merged_clean'].fillna(\" \",inplace=True)\ntrain_input = pre_Process_data(submission.merged_clean.values, tokenizer, max_len=512)","296092e8":"intermediate_output1 = intermediate_layer_model.predict(train_input)","b6684830":"print(intermediate_output1.shape)\nnp.save('.\/embeddings.npy',intermediate_output1)","638d62bf":"def function(text):\n    train_input = pre_Process_data([apply_all(text)], tokenizer, max_len=512)\n    return intermediate_layer_model.predict(train_input)","d4f0cb09":"print(function(\"dlp policy\").shape)","3b74c5a2":"submission['Title_clean']=submission['Title'].apply(apply_all)\nsubmission['embeddings_title']=submission['Title_clean'].apply(function)\nsubmission['Description_clean']=submission['Description'].apply(apply_all)\nsubmission['embeddings_desc']=submission['Description_clean'].apply(function)","07d25cc0":"submission.to_csv('.\/VSO_Closed_subcategory_withembeddings.csv')","e0137a7c":"submission.head()","2ceea8a0":"from IPython.display import FileLink\nFileLink('.\/model.h5') ","7f13b188":"!ls","c602f0dd":"## What is bert? (According to me)\nIn 2019 a model ruled over the entire NLP dimension and that is bert. Bert is not the solution to very NLP problem, but yes it does gives you a upper hand when it comes down to accuracy.<br>\nTo understand bert you need to understand certain basic terms sunch as bidirectional encoder, transformer, look-ahead masking, transformer, and attention making(both multihead and self) in the world of NLP.\nFor better understanding I would recommend you this article at Analytics Vidya(Link - https:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/understanding-transformers-nlp-state-of-the-art-models\/)","6ab75350":"## Notes\nSo bert actually takes three inputs, first is id of tokenized text, second is padding ID and third is Segmentation ID. We have added the tokens \"[CLS]\" and \"[SEP]\" at the beginning and end of a sentence maker it easier for the model to understand the beginning and end of a senntence.<br>\nIn the cell below the entire pre-processing of text before feeding to the bert is explained with code.","f391a9a8":"On the Internet and kaggle there are various pre-trained model of bert in keras and pytorch. But, I choose these kernels because I found them easy to understand and implement. For understanding purpose,I basically prefered the kernel **BERT for Humans: Tutorial+Baseline**, but for implementation purpose I found the kernel **Disaster NLP: Keras BERT using TFHub** to be very useful. I would be explaining in sort but for detailed understanding plese visit their beautiful kernels.","45066207":"## Thank You","1f089727":"## This is the very first time I would be implementing BERT.\nThe Kernels which I fould very helpful for implementing BERT are given below. These kernels really helped to understand and implement the model. You might find my kernel almost similar to them, and I openly accent this fact that what ever is there in this kernel is 99% of their contribution.\n* [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) by [xhlulu](https:\/\/www.kaggle.com\/xhlulu)\n* [BERT for Humans: Tutorial+Baseline](https:\/\/www.kaggle.com\/abhinand05\/bert-for-humans-tutorial-baseline) by [Abhinand](https:\/\/www.kaggle.com\/abhinand05)"}}