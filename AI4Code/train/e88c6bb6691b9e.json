{"cell_type":{"5fc7bdc3":"code","6c782fe6":"code","9d105e79":"code","0d3a061a":"code","f7c39591":"code","b058d4d9":"code","6f3adfe0":"code","9cd8f39a":"code","46ab922a":"code","22121ab6":"code","e1cdc767":"code","9df90b3a":"code","b1c21085":"code","a59727d3":"code","44ca42c7":"code","8a83683f":"code","80ea3094":"code","c5bfd9fa":"code","2b0dc717":"code","c23de7a9":"code","634b4e30":"code","4bd03c6b":"code","18ec2ca9":"code","df886c96":"code","9b45667a":"code","c7343add":"code","6e8c870a":"code","2e0ac38c":"code","7826209d":"code","5ef8f2d5":"code","accd9942":"code","beadc3b9":"code","0fe90fb0":"code","3bc6e2be":"code","80bda317":"code","39df66a6":"code","faa2edf1":"code","0dc4ac14":"code","92195a89":"code","d46dc4eb":"code","f794fb44":"code","67619094":"code","efa6eb30":"code","8fe628ae":"code","5f3e6360":"code","b5babffe":"code","b86a77a4":"code","5b9c97f2":"code","c9b87855":"code","d580b8db":"code","6f741509":"code","63e9c103":"code","c7564a07":"code","b5734b5d":"code","46b4bacc":"code","feaad424":"code","ba22df85":"code","b20f1f56":"code","e11c69e2":"code","5487b661":"code","97bbcdb9":"code","c89c6e41":"code","abfac0c3":"code","b4a12ded":"code","f8de0c46":"code","5c6f5676":"code","e52d215e":"markdown","9eba1d92":"markdown","d49431e2":"markdown","9d67788e":"markdown","b25c58ce":"markdown","95e8f1f9":"markdown","6799e54b":"markdown","feb07751":"markdown","e3d0d98d":"markdown","b45b0391":"markdown","7aad4bda":"markdown","d67cd51f":"markdown","1b8e14f3":"markdown","97bfd7cb":"markdown","31e5ea60":"markdown","6514cfde":"markdown","41af1119":"markdown","338485e4":"markdown","351020f5":"markdown","1d06a3e3":"markdown","c3ef22b6":"markdown","ef9992cd":"markdown","6d58f5f6":"markdown","f81290b9":"markdown","4ff17c0a":"markdown","49a45b75":"markdown","cf1494a7":"markdown","4a370437":"markdown","da63b6ac":"markdown"},"source":{"5fc7bdc3":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline\n\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets\nfrom keras.datasets import mnist\n\nfrom keras.models import Sequential, load_model\n\nfrom keras.layers import Dense, Dropout, Flatten\n\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\n\nfrom keras.utils.np_utils import to_categorical","6c782fe6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d105e79":"import cv2\nimport os\nimport glob","0d3a061a":"monkeys_labels = {0: 'n0', 1: 'n1', 2: 'n2', 3: 'n3', 4: 'n4', 5: 'n5', 6:'n6', 7:'n7', 8:'n8', 9:'n9'}\npic_size = 64\nbatch_size = 32\nepochs = 200\nnum_classes = len(monkeys_labels)\n\nprint (num_classes)","f7c39591":"def load_train_set(path): \n    X, Y= [], []\n    for class_pictures in monkeys_labels:\n        img_dir = path+ monkeys_labels[class_pictures]\n        print (img_dir)\n        data_path = os.path.join(img_dir,'*g')\n        files = glob.glob(data_path)\n        for f1 in files:\n            img = cv2.imread(f1)\n            img = cv2.resize(img, (100,100))\n            X.append(np.array(img))\n            Y.append(class_pictures)\n        length= len(X)\n        #Connaitre le nombre d'image pour chaque type\n        print(\"Nombre total d'image apr\u00e8s insetion du type : \", monkeys_labels[class_pictures] , length)\n    X = np.array(X)\n    Y = np.array(Y)\n    return X, Y","b058d4d9":"X_train, Y_train = load_train_set(\"\/kaggle\/input\/10-monkey-species\/training\/training\/\")","6f3adfe0":"n_n0 = 105\nprint(\"Nombre n0 = \", n_n0)\nn_n1 = 216 - n_n0\nprint(\"Nombre n1 = \", n_n1)\nn_n2 = 326 - n_n1 - n_n0\nprint(\"Nombre n2 = \", n_n2)\nn_n3 = 448 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n3 = \", n_n3)\nn_n4 = 553 - n_n3 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n4 = \", n_n4)\nn_n5 = 666 - n_n4 - n_n3 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n5 = \", n_n5)\nn_n6 = 772 - n_n5 - n_n4 - n_n3 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n6 = \", n_n6)\nn_n7 = 886 - n_n6 - n_n5 - n_n4 - n_n3 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n7 = \", n_n7)\nn_n8 = 992 - n_n7 - n_n6 - n_n5 - n_n4 - n_n3 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n8 = \", n_n8)\nn_n9 = 1098 - n_n8 - n_n7 - n_n6 - n_n5 - n_n4 - n_n3 - n_n2 - n_n1 - n_n0\nprint(\"Nombre n9 = \", n_n9)\n\n","9cd8f39a":"plt.figure(figsize=(10,20))\nfor i in range(0,49) :\n    plt.subplot(10,5,i+1)\n    plt.axis('off')\n    plt.imshow(X_train[i])\n    plt.title('Label: %s' % monkeys_labels[Y_train[i]])","46ab922a":"X_train.shape","22121ab6":"X_test, Y_test = load_train_set(\"\/kaggle\/input\/10-monkey-species\/validation\/validation\/\")","e1cdc767":"plt.figure(figsize=(10,20))\nfor i in range(0,20) :\n    plt.subplot(10,5,i+1)\n    plt.axis('off')\n    plt.imshow(X_test[i])\n    plt.title('Label: %s' % monkeys_labels[Y_test[i]])","9df90b3a":"# Normalisation entre 0 et 1\nX_train = X_train \/ 255\nprint(X_train[0][0])\n# Normalisation entre 0 et 1\nX_test = X_test \/ 255\nprint(X_test[0][0])","b1c21085":"Y_train1 = to_categorical(Y_train)\nY_test1 = to_categorical(Y_test)","a59727d3":"# R\u00e9seau convolutionnel simple\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(100, 100, 3), activation='relu'))\n#model.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\n#model.add(Dense(128, activation='relu'))\nmodel.add(Dense(1)) #1 car on a que 2 sortie possible => malade ou pas malade\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])","44ca42c7":"model.summary()","8a83683f":"# Apprentissage\ntrain = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=200, verbose=1)","80ea3094":"# Test\nscores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","c5bfd9fa":"print(train.history['accuracy'])","2b0dc717":"print(train.history['val_accuracy'])","c23de7a9":"def plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()","634b4e30":"plot_scores(train)","4bd03c6b":"# Prediction\nY_cnn = model.predict_classes(X_test)","18ec2ca9":"cm = confusion_matrix(Y_cnn,Y_test)\nprint(cm)\nplt.figure(figsize = (12,10))","df886c96":"plt.figure(figsize=(15,25))\nn_test = X_test.shape[0]\ni=1\nfor j in range(len(X_test)) :\n    if (Y_cnn[j] != Y_test[j]) & (i<50):\n        plt.subplot(10,5,i)\n        plt.axis('off')\n        plt.imshow(X_test[j])\n        pred_classe = Y_cnn[j].argmax(axis=-1)\n        plt.title('%s \/ %s' % (monkeys_labels[int(Y_cnn[j])], monkeys_labels[int(Y_test[j])]))\n        i+=1","9b45667a":"# R\u00e9seau convolutionnel simple\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(100, 100, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n#model.add(Dense(128, activation='relu'))\nmodel.add(Dense(1)) #1 car on a que 2 sortie possible => malade ou pas malade\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])","c7343add":"model.summary()","6e8c870a":"# Apprentissage\ntrain = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=200, verbose=1)","2e0ac38c":"# Test\nscores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","7826209d":"# Mod\u00e8le CNN plus profond\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #divise les images par 2 (taille)\nmodel.add(Dropout(0.2)) #20% des neurones (al\u00e9atoire) ne vont pas apprendre ==> r\u00e9duit le sur-apprentissage\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten()) #applati\nmodel.add(Dense(512, activation='relu')) #ici neurone ordinaire et pas convolutionnel\nmodel.add(Dense(num_classes, activation='softmax')) #pour classer les images avec le nombre de classe\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","5ef8f2d5":"model.summary()","accd9942":"# Apprentissage\ntrain = model.fit(X_train, Y_train1, validation_data=(X_test, Y_test1), epochs=50, batch_size=2000, verbose=1)\n\n# Test\nscores = model.evaluate(X_test, Y_test1, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","beadc3b9":"plot_scores(train)","0fe90fb0":"# Prediction\nY_cnn = model.predict_classes(X_test)","3bc6e2be":"cm = confusion_matrix(Y_cnn,Y_test)\nprint(cm)\nplt.figure(figsize = (12,10))","80bda317":"from keras.applications import VGG16","39df66a6":"vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(100,100,3))\nvgg16.trainable = False","faa2edf1":"vgg16.summary()","0dc4ac14":"model = Sequential()\nmodel.add(vgg16)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))","92195a89":"model.summary()","d46dc4eb":"# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","f794fb44":"# Apprentissage\ntrain = model.fit(X_train, Y_train1, validation_data=(X_test, Y_test1), epochs=20, batch_size=2000, verbose=1)\n\n# Test\nscores = model.evaluate(X_test, Y_test1, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","67619094":"for layer in vgg16.layers[15:]:\n    layer.trainable=True\nfor layer in vgg16.layers[0:15]:\n    layer.trainable=False","efa6eb30":"# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Apprentissage\ntrain = model.fit(X_train, Y_train1, validation_data=(X_test, Y_test1), epochs=20, batch_size=2000, verbose=1)\n\n# Test\nscores = model.evaluate(X_test, Y_test1, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","8fe628ae":"for layer in vgg16.layers[10:]:\n    layer.trainable=True\nfor layer in vgg16.layers[0:10]:\n    layer.trainable=False\n    \n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Apprentissage\ntrain = model.fit(X_train, Y_train1, validation_data=(X_test, Y_test1), epochs=20, batch_size=2000, verbose=1)\n\n# Test\nscores = model.evaluate(X_test, Y_test1, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","5f3e6360":"plot_scores(train)","b5babffe":"from keras.applications import InceptionV3, ResNet50V2","b86a77a4":"ResNet50V2 = ResNet50V2(weights='imagenet', include_top=False, input_shape=(100,100,3))\nResNet50V2.trainable = False","5b9c97f2":"ResNet50V2.summary()","c9b87855":"model = Sequential()\nmodel.add(ResNet50V2)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))","d580b8db":"# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Apprentissage\ntrain = model.fit(X_train, Y_train1, validation_data=(X_test, Y_test1), epochs=20, batch_size=2000, verbose=1)\n\n# Test\nscores = model.evaluate(X_test, Y_test1, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","6f741509":"plot_scores(train)","63e9c103":"for i in range (len(ResNet50V2.layers)):\n    print (i,ResNet50V2.layers[i])","c7564a07":"for layer in vgg16.layers[126:]:\n    layer.trainable=True\nfor layer in vgg16.layers[0:126]:\n    layer.trainable=False","b5734b5d":"# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Apprentissage\ntrain = model.fit(X_train, Y_train1, validation_data=(X_test, Y_test1), epochs=20, batch_size=2000, verbose=1)\n\n# Test\nscores = model.evaluate(X_test, Y_test1, verbose=0)\nprint(\"Score : %.2f%%\" % (scores[1]*100))","46b4bacc":"plot_scores(train)","feaad424":"import cv2\nimport datetime as dt\nimport glob\nimport itertools\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nfrom keras import models, layers, optimizers\nfrom keras.applications import Xception\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom pathlib import Path\nfrom sklearn.metrics import confusion_matrix","ba22df85":"weights = Path('..\/input\/xception\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\ntrain_dir = Path('..\/input\/10-monkey-species\/training\/training\/')\ntest_dir = Path('..\/input\/10-monkey-species\/validation\/validation\/')","b20f1f56":"height=150\nwidth=150\nchannels=3\nbatch_size=32\nseed=1337\n\n\n# Training generator\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_generator = train_datagen.flow_from_directory(train_dir, \n                                                    target_size=(height,width),\n                                                    batch_size=batch_size,\n                                                    seed=seed,\n                                                    class_mode='categorical')\n\n# Test generator\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_generator = test_datagen.flow_from_directory(test_dir, \n                                                  target_size=(height,width), \n                                                  batch_size=batch_size,\n                                                  seed=seed,\n                                                  class_mode='categorical')","e11c69e2":"# Initialize the base model\nbase_model = Xception(weights=weights,\n                      include_top=False,\n                      input_shape=(height, width, channels))\nbase_model.summary()","5487b661":"def extract_features(sample_count, datagen):\n    start = dt.datetime.now()\n    features =  np.zeros(shape=(sample_count, 5, 5, 2048))\n    labels = np.zeros(shape=(sample_count,10))\n    generator = datagen\n    i = 0\n    for inputs_batch,labels_batch in generator:\n        stop = dt.datetime.now()\n        time = (stop - start).seconds\n        print('\\r',\n              'Extracting features from batch', str(i+1), '\/', len(datagen),\n              '-- run time:', time,'seconds',\n              end='')\n        \n        features_batch = base_model.predict(inputs_batch)\n        \n        features[i * batch_size : (i + 1) * batch_size] = features_batch\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        i += 1\n        \n        if i * batch_size >= sample_count:\n            break\n            \n    print(\"\\n\")\n    \n    return features,labels","97bbcdb9":"train_features, train_labels = extract_features(1098, train_generator)\ntest_features, test_labels = extract_features(272, test_generator)","c89c6e41":"flat_dim = 5 * 5 * 2048\ntrain_features = np.reshape(train_features, (1098, flat_dim))\ntest_features = np.reshape(test_features, (272, flat_dim))","abfac0c3":"reduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n                                         factor=0.1,\n                                         patience=2,\n                                         cooldown=2,\n                                         min_lr=0.00001,\n                                         verbose=1)\n\ncallbacks = [reduce_learning_rate]","b4a12ded":"model = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_dim=flat_dim))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(10, activation='softmax'))\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc'])\nmodel.summary()","f8de0c46":"history = model.fit(train_features, \n                    train_labels, \n                    epochs=30,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_split=0.1,\n                    callbacks=callbacks)","5c6f5676":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","e52d215e":"On remarque que la m\u00e9thode CNN indique tout les types de singes (presque en totalit\u00e9) comme \u00e9tant des n1 (\u00e0 tord)","9eba1d92":"# Une couche convutionnelle","d49431e2":"On affiche 50 n0 de validation :","9d67788e":"# Transfer learning","b25c58ce":"La pr\u00e9cision n'est pas vraiment au rendez-vous, essayons de d\u00e9gel\u00e9 1\/3 des couches","95e8f1f9":"# Mod\u00e8le CNN plus profond","6799e54b":"Essayons d'affiner","feb07751":"## VGG16\n","e3d0d98d":"Cette m\u00e9thode n'aura vraisemblablement pas de meilleurs r\u00e9sultats","b45b0391":"Clairement sur-apprentissage","7aad4bda":"C'est pire, regardons la courbes","d67cd51f":"# Initialisation","1b8e14f3":"Diff\u00e9rence entre l'entrainement et la validation assez importante (+ 20 %)","97bfd7cb":"https:\/\/www.kaggle.com\/crawford\/monkey-classifier-cnn-xception-0-90-acc","31e5ea60":"On note qu'il sagit ici d'un ph\u00e9nom\u00e8ne de sur-apprentissage.","6514cfde":"On va d\u00e9geler 5 couches","41af1119":"On affiche les 50 premiers n0","338485e4":"## ResNet50v2","351020f5":"# M\u00e9thode Xception ","1d06a3e3":"Le resultat est similaire, essayons 10 couches","c3ef22b6":"Cette fois la pr\u00e9cision est de 90%, avec un l\u00e9ger sur apprentissage \u00e0 priori.","ef9992cd":"On retrouve une pr\u00e9cision qui est n\u00e9ttement sup\u00e9rieur au m\u00e9thode CNN (+20%)","6d58f5f6":"# Lecture des images","f81290b9":"On remarque directement que l'accuracy s'est am\u00e9lior\u00e9 (enfin pas vraiment vu que l'on se rapproche du 50%)  \nAnalysons :","4ff17c0a":"Pas beaucoup mieux","49a45b75":"On va utiliser utiliser une couche convolutionnelle pour l'extraction des caract\u00e9ristiques, et une couche dense pour la classification :","cf1494a7":"On remarque tout de suite que cette m\u00e9thode ne fonctionne pas bien avec si peu d'image","4a370437":"On affiche 50 images o\u00f9 l'algorithme s'est tromp\u00e9 :","da63b6ac":"Au moins cette m\u00e9thode ne dit pas que tout les singes sont les m\u00eames !"}}