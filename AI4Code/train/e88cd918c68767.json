{"cell_type":{"59948df8":"code","b1908166":"code","d6d1fdf4":"code","8024ff8c":"code","eebfad6f":"code","4da27955":"code","545fb75f":"code","cf75afe1":"code","8b38bca8":"code","f414e9fd":"code","b93fe613":"code","902d84ce":"code","c881b8cb":"code","c5f7acc3":"code","5932f328":"code","515421ac":"code","2238f913":"code","e8449947":"code","c0dd3482":"code","105d3398":"code","d819af71":"code","9f13d17e":"code","3d65a819":"code","1798675a":"code","d9cdeeb5":"code","5cd627d6":"code","63db70e7":"code","da96fcb8":"code","5b8f2c60":"code","f96859e2":"code","7e185eec":"code","bda6f401":"code","47985a41":"code","7d7acab6":"code","a9b70fd3":"code","f3eeaa9b":"code","fffdb0c1":"code","cbfc8013":"code","86bdcfc8":"code","7593762c":"markdown","997dd8a7":"markdown","64dba1b6":"markdown","68825f31":"markdown","b761d81f":"markdown","02d5cca3":"markdown","06fcf6d2":"markdown","6272bca4":"markdown","9d2c853d":"markdown","4caa0f98":"markdown","c50b3542":"markdown","07a68aa7":"markdown","a2b39ef2":"markdown","d28a3256":"markdown","67e5f33b":"markdown","2ef1898e":"markdown","1e8927cf":"markdown","af87c727":"markdown","e8e48db6":"markdown","fcb0b73c":"markdown","1c04bf2f":"markdown","3216b13b":"markdown","dcab50b6":"markdown","e44c13d3":"markdown"},"source":{"59948df8":"import pandas as pd\nimport numpy as np\nimport math\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA \nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn import tree\n\nfrom xgboost import XGBClassifier\n\n% matplotlib inline\n\n# Set Random Seed\nnp.random.seed(42)\nnp.random.RandomState(42)","b1908166":"# Read csv\ndata = pd.read_csv(\"..\/input\/data 2.csv\")\ndata.head()","d6d1fdf4":"print (\"Number of data points :\", len(data))","8024ff8c":"# Describe data\ndata.describe()","eebfad6f":"data.info()","4da27955":"# Save labels in y\ny = data[\"diagnosis\"]","545fb75f":"# Drop columns\nX = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)","cf75afe1":"y_pos = [yy for yy in y if yy == 'M']\ny_neg = [yy for yy in y if yy == 'B']\nprint('y_pos: ', len(y_pos))\nprint('y_neg: ', len(y_neg))","8b38bca8":"# Plot a Correlation chart\ncorr = X.corr() # .corr is used for find corelation\n#plt.figure(figsize=(20,15))\nsns.set(rc={'figure.figsize':(25,20)})\n# plot a heatmap\nsns.heatmap(corr, cbar = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           xticklabels= X.columns, yticklabels= X.columns,\n           cmap= 'viridis') ","f414e9fd":"# Drop columns\nX = X.drop([\"perimeter_mean\", \"area_mean\", \"radius_worst\", \"area_worst\", \"perimeter_worst\"], axis=1)\nX = X.drop([\"texture_worst\", \"perimeter_se\", \"area_se\"], axis=1)\n\nX.info()","b93fe613":"# Plot a countplot\nsns.set(rc={'figure.figsize':(8,5)})\nsns.countplot(y) ","902d84ce":"# Print count\ncount = y.value_counts()\nprint('Number of Benign : ',count[0] )\nprint('Number of Malignant : ',count[1]) ","c881b8cb":"# Creating a empty list\nmean_volume = []\n# defining pi\npi = 3.1415\n\n# calculatin mean volume for each mean radius and saving result in mean_volume list\nfor i in range(len(X)):\n    #aving result in mean_volume list\n    mean_volume.append((math.pow(X[\"radius_mean\"][i], 3)*4*pi)\/3)\n\n# Creating a new feature\nX[\"mean_volume\"]= mean_volume    ","c5f7acc3":"# Creating a new feature adding up some phisical measuraments\n# X[\"mesuraments_sum_mean\"] = X[\"radius_mean\"] + X[\"perimeter_mean\"] + X[\"area_mean\"]","5932f328":"X.head()","515421ac":"# Define a scaler function\ndef scaler(df):\n    \"\"\"The Function receive a Dataframe and return a Scaled Dataframe\"\"\"\n    scaler = preprocessing.MinMaxScaler()\n    scaled_df = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n    \n    return scaled_df\n\n# testing scaler\nscaled_df = scaler(X)\n\nscaled_df.head()","2238f913":"# Define a function to detect outliers\ndef remove_outliers(X, y, f=2, distance=1.5):\n    \n    \"\"\"The Function receive Features (X) and Label (y) a frequency (f) and Inter-Quartile distance (distance),  \n    and return features and labels without outliers (good_X, good_y)\"\"\"\n    \n    outliers  = []\n\n    # For each feature find the data points with extreme high or low values\n    for feature in X.keys():\n\n        # Calculate Q1 (25th percentile of the data) for the given feature\n        Q1 = np.percentile(X[feature], 25)\n\n        # Calculate Q3 (75th percentile of the data) for the given feature\n        Q3 = np.percentile(X[feature], 75)\n\n        # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n        step = (Q3 - Q1) * distance\n\n        outliers.append(X[~((X[feature] >= Q1 - step) & (X[feature] <= Q3 + step))].index.values)\n\n    # Select the indices for data points you wish to remove\n    flat_list = [item for sublist in outliers for item in sublist]\n\n    # importing Counter\n    from collections import Counter\n    \n    freq = Counter(flat_list)\n    # Create a list to store outliers to remove\n    outliers_to_remove = []\n    \n    for key, value in freq.items():\n        if value > f:\n            outliers_to_remove.append(key)\n\n    # Remove the outliers, if any were specified\n    good_X = X.drop(X.index[outliers_to_remove]).reset_index(drop = True)\n    good_y    = y.drop(y.index[outliers_to_remove]).reset_index(drop = True)\n    # Sort list\n    outliers_to_remove.sort()\n    # Print outliers founded\n    for i in range(len(outliers_to_remove)):\n        print( \"data point: \", outliers_to_remove[i], \"is considered outlier to more than \", f, \" feature\" )\n\n    print( \"All \", len(outliers_to_remove), \"were removed!\" )\n    # return data without outliers\n    return good_X, good_y \n\n\ngood_X, good_y = remove_outliers(scaled_df, y, f=2, distance=1.5)","e8449947":"good_X.head()","c0dd3482":"sns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(good_y) ","105d3398":"count = y.value_counts()\ncount2 = good_y.value_counts()\n\nprint('Number of Benign removed: ',count[0] - count2[0])\nprint('Number of Malignant removed: ',count[1] - count2[1])","d819af71":"def tsne_plot(good_X, good_y):\n    tsne = TSNE(n_components=2, init='random', random_state=0, perplexity=100)\n    X_original_SNE = tsne.fit_transform(good_X)\n\n    X_original_SNE_df = pd.DataFrame(X_original_SNE, columns=[\"d1\", \"d2\"])\n    good_y_df = pd.DataFrame(good_y, columns=[\"diagnosis\"])\n\n    X_original_SNE_df = pd.concat([good_y_df, X_original_SNE_df.iloc[:,0:]],axis=1)\n    X_original_SNE_df.head()\n\n    fig = plt.figure(figsize=(12,8))\n    ax = fig.add_subplot(111)\n\n    m_SNE = X_original_SNE_df.loc[X_original_SNE_df['diagnosis'] == 'M']\n    b_SNE = X_original_SNE_df.loc[X_original_SNE_df['diagnosis'] == 'B']\n\n    ax.scatter(m_SNE['d1'], m_SNE['d2'], c='darkorange', s=100)\n    ax.scatter(b_SNE['d1'], b_SNE['d2'], c='blue', s=100)\n    \n    ax.set_xlim([-10, 10])\n    ax.set_ylim([-10, 10])\n\n    plt.show()","9f13d17e":"tsne_plot(scaled_df, y)","3d65a819":"tsne_plot(good_X, good_y)","1798675a":"X_resampled, y_resampled = ADASYN().fit_sample(good_X, good_y)\nprint(sorted(Counter(y_resampled).items()))\n\nsns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(y_resampled) ","d9cdeeb5":"tsne_plot(X_resampled, y_resampled)","5cd627d6":"def selector(X, y, k=12):\n    \n    \"\"\"The function receive features and labels (X, y) and a target number to select features (k)\n    and return a new dataset wiht k best features\"\"\"\n    \n    X = pd.DataFrame(X)\n    \n    selector = SelectKBest(chi2, k)\n    \n    X_new = selector.fit_transform(X, y)\n    \n    return pd.DataFrame(X_new, columns=X.columns[selector.get_support()])\n\nX_select = selector(X_resampled, y_resampled)\n\nX_select.head()","63db70e7":"# Support Vector Machine Classifier\nSV_clf = svm.SVC()\n# Parameters to tune\nSV_par = {'kernel': ['rbf'], 'C': [1]}\n\n# Logistic Regression\nLR_clf = LogisticRegression()\n# Parameters to tune\nLR_par= {'penalty':['l1','l2'], 'C': [0.5, 1, 5, 10], 'max_iter':[50, 100, 150, 200]}\n\nclassifiers = [SV_clf, LR_clf]\n\nclassifiers_names = ['Support Vector Machine', 'Logistic Regression']\n\nparameters = [SV_par, LR_par]","da96fcb8":"def tune_compare_clf(X, y, classifiers, parameters, classifiers_names):\n    \n    '''The function receive Data (X, y), a classifiers list, \n    a list of parameters to tune each chassifier (each one is a dictionary), \n    and a list with classifiers name. \n    \n    The function split data in Train and Test data, \n    train and tune all algorithms and print results using F1 score.\n    \n    The function also returns a Dataframe with predictions, each row is a classifier prediction,\n    and X_test and y_test.\n    '''\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n    \n\n    print(\"\\n\" \"Train size : \", X_train.shape, \" and Train labels : \", y_train.shape, \"\\n\")\n\n    print(\"Test size: \", X_test.shape, \" and Test labels : \", y_test.shape, \"\\n\", \"\\n\")\n    \n    results = []\n    \n    print(\"  ---- F1 Score  ----  \", \"\\n\")\n\n    for clf, par, name in zip(classifiers, parameters, classifiers_names):\n        # Store results in results list\n        clf_tuned = GridSearchCV(clf, par).fit(X_train, y_train)\n        y_pred = clf_tuned.predict(X_test)\n        results.append(y_pred)   \n\n        print(name, \": %.2f%%\" % (f1_score(y_test, y_pred, average='weighted') * 100.0))\n\n    result = pd.DataFrame.from_records(results)   \n    \n    return result, X_test,  y_test","5b8f2c60":"# result, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)\n# result, X_test, y_test = tune_compare_clf(X_resampled, y_resampled, classifiers, parameters, classifiers_names)","f96859e2":"# result, X_test, y_test = tune_compare_clf(X_select, y_resampled, classifiers, parameters, classifiers_names)","7e185eec":"from sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\ndef mesh(f1, f2, expend):\n    x_min, x_max = f1.min() - expend, f1.max() + expend\n    y_min, y_max = f2.min() - expend, f2.max() + expend\n    resolution = 80\n    hx = (x_max - x_min)\/resolution\n    hy = (y_max - y_min)\/resolution\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n    return xx, yy\n\ndef knn(X, y):\n#     X = X.values\n#     y = y.values\n    \n    acc = 0\n    for i in range(100):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True) # 70% training and 30% test\n\n        # we create an instance of Neighbours Classifier and fit the data.\n        clf = KNeighborsClassifier(n_neighbors=29)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n\n#         print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n        acc += metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", acc\/100)\n\nknn(X_resampled, y_resampled)","bda6f401":"knn(X_select, y_resampled)","47985a41":"from sklearn import svm\n\ndef svm_fit(X, y):    \n    acc = 0\n    for i in range(100):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True) # 70% training and 30% test\n\n        clf = svm.SVC(gamma='scale')\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n    \n        acc += metrics.accuracy_score(y_test, y_pred)\n        \n    print(\"Accuracy:\", acc\/100)\n\nsvm_fit(X_resampled, y_resampled)","7d7acab6":"svm_fit(X_select, y_resampled)","a9b70fd3":"import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport operator \nimport csv\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\n\ndef euclideanDistance(instance1, instance2, length):\n    distance = 0\n    for x in range(length):\n        distance += pow((instance1[x] - instance2[x]), 2)\n    return math.sqrt(distance)\n\ndef getNeighbors(trainingSet, testInstance, k):\n    distances = []\n    length = len(testInstance)\n    for x in range(len(trainingSet)):\n        dist = euclideanDistance(testInstance, trainingSet[x], length)\n        distances.append((trainingSet[x], dist))\n    distances.sort(key=operator.itemgetter(1))\n    neighbors = []\n    for x in range(k):\n        neighbors.append(distances[x][0])\n    return neighbors\n\ndef count(df, threshold):\n    df = pd.DataFrame(df)\n    data_0 = df.loc[df[0] == 0]\n    data_1 = df.loc[df[0] == 1]\n\n    data_0_strip = data_0.loc[data_0[1] > threshold]\n    data_1_strip = data_1.loc[data_1[1] > threshold]\n\n    pfa = len(data_0_strip) \/ len(data_0)\n    pde = len(data_1_strip) \/ len(data_1)\n\n    return pfa, pde\n\ndef fit(thresholds, data):\n    fpr, tpr = [], []\n    pfa, pde = count(data, -math.inf)\n    fpr.append(pfa)\n    tpr.append(pde)\n\n    for thres in thresholds:\n        pfa, pde = count(data, thres)\n        fpr.append(pfa)\n        tpr.append(pde)\n\n    pfa, pde = count(data, math.inf)\n    fpr.append(pfa)\n    tpr.append(pde)\n\n    return fpr, tpr\n\ndef operatingpoint(ph0, ph1, pfa, pde):\n    max_pcd = 0\n    for idx, pfaa in enumerate(pfa):\n        pcd = ph0 * (1 - pfaa) + ph1 * (pde[idx])\n        if pcd > max_pcd:\n            max_pcd = pcd\n            point = (pfaa, pde[idx])\n    return point, max_pcd\n\ndef predict(train_point_list, target, label, k):\n    dist_list = np.array([np.linalg.norm(p - target) for p in train_point_list])\n    ind = np.argpartition(dist_list, k)[:k]\n    return(sum(label[ind])\/k)\n\ndef roc_curve(label, preds):\n    beta = np.unique(preds)\n    beta = np.sort(beta)\n    beta = np.insert(beta, 0, -np.inf)\n    beta = np.insert(beta, beta.size, np.inf)\n    H0 = preds[label == 0]\n    H1 = preds[label == 1]\n    pfa = np.array([sum(H0 > b)\/H0.size for b in beta])\n    pd = np.array([sum(H1 > b)\/H1.size for b in beta])\n    return pfa, pd\n\ndef max_Pcd(pfa, pd, label):\n    ph1 = sum(label == 1)\/len(label)\n    ph0 = 1-ph1\n    pcd = pd*ph1-ph0*pfa+ph0\n    return np.max(pcd)\n\ndef calpe(df, df_label, testing, testing_label, K):\n    roc = []\n    Z = np.array([predict(df, pt, df_label, K) for pt in testing])\n    pfa, pd = roc_curve(testing_label, Z)\n    return 1 - max_Pcd(pfa, pd, testing_label)","f3eeaa9b":"def knn_pe(df, df_label, testing, testing_label):\n    trainPE, testPE = [], []\n    xaxisTrain, xaxisTest = [], []\n    for K in range(1, 400, 4):\n        minpeTraining = calpe(df, df_label, df, df_label, K)\n        minpeTesting = calpe(df, df_label, testing, testing_label, K)\n\n        trainPE.append(minpeTraining)\n        xaxisTrain.append(len(df) \/ K)\n        \n        testPE.append(minpeTesting)\n        xaxisTest.append(len(testing) \/ K)\n\n    # PLOTTING\n    plt.title('min Pe with N\/K Testing on Training Data')\n    plt.xlabel('N \/ K', ha='center', va='center')\n    plt.ylabel('min Pe', ha='center', va='center', rotation='vertical')\n\n    plt.plot(xaxisTrain, trainPE, color='darkorange', lw=3, label='Training Data')\n    plt.plot(xaxisTest, testPE, color='g', lw=3, label='Testing Data')\n\n    plt.legend(loc=\"upper right\")\n    plt.grid()\n\n    plt.show()\n    \ny_label = []\nfor x in y_resampled:\n    if x == 'M':\n        y_label.append(1)\n    else:\n        y_label.append(0)\ny_label = np.array(y_label)\n\nX_resampled_train, X_resampled_test, y_label_train, y_label_test = train_test_split(X_resampled, y_label, test_size=0.15, random_state=42, shuffle=True)\n\nknn_pe(X_resampled_train, y_label_train, X_resampled_test, y_label_test)","fffdb0c1":"# Scale, Outliers Remove and Resample\n    \nX_scaled = scaler(X)\nX_good, y_good = remove_outliers(X_scaled, y, f=2, distance=2)\nX_new, y_new = resample(X_good, y_good, method=\"RandomOverSampler\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","cbfc8013":"y_pred_votes = result.describe().iloc[[2]]","86bdcfc8":"print(\"Accuracy: %.2f%%\" % (f1_score(y_test, y_pred_votes.T, average='weighted') * 100.0))\n\nsns.set(rc={'figure.figsize':(5,5)})\ncm = confusion_matrix(y_test,y_pred_votes.T)\nsns.heatmap(cm,annot=True,fmt=\"d\")","7593762c":"### Feature Selection using [Scikit Learn](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n\nFeature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. SelectKBest removes all but the k highest scoring features","997dd8a7":"The highest and more consistents results across all classifiers was achieved using Scale, Removing Outiliers and Balancing data. The higher score was reached by Gradient Boosting Algorithm (F1 Score = 99.28) and lower was Decision Tree (F1 Score 94,20) which even been the lower is a very descent result. ","64dba1b6":"Data ins't balenced, there is more case of benigns tumors that malignant. Later we'll use methods to balance data and analyze if results get better. ","68825f31":"### Feature Scaling\n\nSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n\nAnother reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.[1]\n\n[Feature Scaling - Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling)","b761d81f":"### Testing Algorithms with Different Data Manipulation Techniques\n\nBefore in this project we define and test differents aproachs to use our original dataset, and create some functions:\n\n* scaler(X)\n\n* selector(X, y, k)\n\n* remove_outliers(X, y, f, distance)\n\n* resample(X, y, method)\n\nNow we'll test using: \n\n* tune_compare_clf(X, y, classifiers, parameters, classifiers_names) - a function that tune each algorith to given data and print F1 Scores. \n\nWith Technique or set of techniques is more effective for this dataset to minimize error when classifies Breast Cancer. \n","02d5cca3":"### Plotting a [Confusion Matrix](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html) with best results\n\n","06fcf6d2":"### Detect Outliers using [Tukey Method](http:\/\/datapigtechnologies.com\/blog\/index.php\/highlighting-outliers-in-your-data-with-the-tukey-method\/)","6272bca4":"### Selecting Features\n\nWe don't need pacient \"id\", \"diagnosis\" is our labels and \"Unnamed: 32\" have only NaNs. Let exclude this tree columns.","9d2c853d":"Creating a simple new feature, measuraments_sum_mean just adding feature relatade with cell size","4caa0f98":"### Analyzing Data","c50b3542":"\n\n\n|Classifier          | Scaled + Outiliers Removed | Scaled + Resampled | Scaled + Outiliers Removed+Resampled | Scaled + Feature + Out. Rem + Resampled |\n|--------------------|----------------------------|--------------------|--------------------------------------|-----------------------------------------|\n|Random Forest       |96.14%                      |**97.90%**          |97.83%                                |96.47%                                   |\n|Extra DecisionTrees |94.21%                      |96.50%              |97.10%                                |96.47%                                   |\n|Decision Tree       |91.24%                      |97.20%              |94.20%                                |94.74%                                   |\n|Support Vector      |**98.05%**                  |95.81%              |97.83%                                |47.80%                                   |\n|AdaBoost Classifier |96.14%                      |95.11%              |97.10%                                |**96.49%**                               |\n|Gradient Boosting   |95.20%                      |96.50%              |**99.28%**                            |95.60%                                   |\n|SGD Classifier      |**98.05%**                  |93.68%              |97.83%                                |85.07%                                   |\n|Logistic Regression |97.08%                      |95.80%              |97.83%                                |96.47%                                   |\n|XGB Classifier      |96.16%                      |97.20%              |98.55%                                |96.47%                                   |\n\n","07a68aa7":"## Test, Tune and Compare Classifiers with different parameters and data settings","a2b39ef2":"### Importing Data","d28a3256":"### Preparing Data","67e5f33b":"### Looking for correlation between features","2ef1898e":"Many malignant were considered outilier, this make data even more unbalanced. Later we'll understand if remove outlier improve results in this Dataset.","1e8927cf":"### Detect Outliers","af87c727":"### Importing Libraries","e8e48db6":"### Importing Classifiers Algorithms and set parameters\n\n** [Grid Search](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)**\n\nExhaustive search over specified parameter values for an estimator. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.","fcb0b73c":"** From random over-sampling to SMOTE and ADASYN**  \n\nApart from the random sampling with replacement, there is two popular methods to over-sample minority classes: (i) Synthetic Minority Oversampling Technique (SMOTE) and (ii) Adaptive Synthetic (ADASYN) sampling method. These algorithm can be used in the same manner:","1c04bf2f":"### Feature Engineering\n\nCreating a Volume Mean Feature using radius_mean","3216b13b":"## [Imbalanced Learning](http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/)\n\n*** Naive random over-sampling***  \nOne way to fight this issue is to generate new samples in the classes which are under-represented. The most naive strategy is to generate new samples by randomly sampling with replacement the current available samples. The RandomOverSampler offers such scheme:","dcab50b6":"### KNN Pe","e44c13d3":"### Results with set of techniques"}}