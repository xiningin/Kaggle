{"cell_type":{"9d85a8d9":"code","eef741d5":"code","6eef160d":"code","e9349e80":"code","66d5ea58":"code","b5e73c73":"code","cd52b0fb":"code","eda8423f":"code","c09c343f":"code","8ca67404":"code","30aa1edd":"code","048fa5ac":"code","43e6c8a4":"code","1c48811a":"code","5138cecf":"code","147b8f64":"code","8cb940df":"markdown","75725130":"markdown","ca876fd6":"markdown","4304a974":"markdown"},"source":{"9d85a8d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eef741d5":"from keras.datasets import imdb\n\n\n(x_train,y_train), (x_test,y_test) = imdb.load_data(num_words=10000)","6eef160d":"print(max([max(sequence) for sequence in x_train]))","e9349e80":"# if you want to see this English words:\nword_index = imdb.get_word_index()","66d5ea58":"# First way\n# manual way\ndef vectorize_sequences(sequences, dimension = 10000):\n    results = np.zeros((len(sequences),dimension))\n    \n    for i,sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\nx_train = vectorize_sequences(x_train)\nx_test = vectorize_sequences(x_test)","b5e73c73":"# automatic way\nfrom keras.utils.np_utils import to_categorical\n\none_hot_train_labels = to_categorical(x_train)\none_hot_test_labels = to_categorical(x_test)","cd52b0fb":"x_train[0]","eda8423f":"# Second way \n\n# y_train = np.array(y_train)\n# y_test = np.array(y_test) \n\n# If you want to use it this way, you should use sparse_categorical_crossentropy instead of categorical_crossentropy.","c09c343f":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16,activation= 'relu',input_shape=(10000,)))\nmodel.add(layers.Dense(16,activation = 'relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))","8ca67404":"model.compile(optimizer = 'rmsprop',\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])","30aa1edd":"x_val = x_train[:10000]\nx_train = x_train[10000:]\n\ny_val = y_train[:10000]\ny_train = y_train[10000:]","048fa5ac":"history = model.fit(x_train,\n                   y_train,\n                   epochs = 20,\n                   batch_size = 512,\n                   validation_data=(x_val,y_val))","43e6c8a4":"history_dict = history.history\nhistory_dict.keys()","1c48811a":"import matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values)+1)\n\nplt.plot(epochs, loss_values, 'bo')\nplt.plot(epochs,val_loss_values,'b')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","5138cecf":"acc = history_dict['accuracy']\nval_acc= history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo')\nplt.plot(epochs,val_acc,'b')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","147b8f64":"results = model.evaluate(x_test,y_test)\nresults","8cb940df":"### Summary\n* **'binary_crossentropy' should be used as loss function in binary classification problems**\n","75725130":"## IMDB Dataset \n\n","ca876fd6":"### Data Preprocessing\n\n+ **We need to send data as Tensors to artificial neural networks**\n+ **There are 2 way for this process.** \n\n+ **The most logical is to do 'One hot encoding'**\n","4304a974":"## Reference : https:\/\/tanthiamhuat.files.wordpress.com\/2018\/03\/deeplearningwithpython.pdf"}}