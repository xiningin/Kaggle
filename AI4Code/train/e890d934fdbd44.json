{"cell_type":{"f1df4bb7":"code","73cdf590":"code","9314187e":"code","d057bd7a":"code","ed25db38":"code","a33a8730":"code","e6ce476a":"code","ba196c16":"code","573c2898":"code","3bcf0961":"code","a176a586":"code","1beee81c":"code","9c44484a":"code","e317dc48":"code","a8ec7ca1":"code","7fda2986":"code","f4b6d046":"code","fc769af5":"code","774bf0e0":"code","6a7a8504":"code","2f6b0b97":"markdown","8dee57b9":"markdown","f059f9d3":"markdown","8be27cc9":"markdown","3eb4bc52":"markdown","87bc32c9":"markdown","f8737c0d":"markdown","9a701961":"markdown","8c7eb4fc":"markdown","25748832":"markdown","7f2073e3":"markdown","79f16199":"markdown","c79d68dc":"markdown","cd7a9f40":"markdown","454b452e":"markdown"},"source":{"f1df4bb7":"# we import Basic ML Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","73cdf590":"df= pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","9314187e":"df.head()","d057bd7a":"df.info()","ed25db38":"df.describe()","a33a8730":"sns.pairplot(df)","e6ce476a":"df.corr()","ba196c16":"#we can delet 'No' column to increase power of model.\n\n#X : Features\n#y : Target variable\n\nX = df.drop('Y house price of unit area', axis= 1)\ny = df['Y house price of unit area']","573c2898":"#for considering interaction between features we use polynomial features\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_converter= PolynomialFeatures(degree=2, include_bias=False)\npoly_features= polynomial_converter.fit_transform(X)\n\npoly_features.shape","3bcf0961":"#we use train_test_split from sklearn.model_selection to devide dataset to train and test set.\n\nfrom sklearn.model_selection import train_test_split\n\n#train set in a bigger sample of dataset that model uses to learn.\n#test set in smaller sample of dataset that model should be evaluated in.\n\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)\n\n#random_state before spliting controls shuffling dataset","a176a586":"#we use StandardScaler from sklearn.preprocessing to Standard all features set\n \nfrom sklearn.preprocessing import StandardScaler \n\nscaler = StandardScaler() #scaler is StandardScaler Class \nscaler.fit(X_train) \n\nX_train_standard= scaler.transform(X_train)\nX_test_standard = scaler.transform(X_test)","1beee81c":"a = {'X_train':X_train[1], 'X_train_standard': X_train_standard[1]}\npd.DataFrame(a)","9c44484a":"from sklearn.linear_model import Ridge\n\n#step 1: Choose the model type\nridge_model = Ridge (alpha= 6)\n\n#step 2: Train the model with fit\nridge_model.fit(X_train_standard, y_train)\n\n#step 3: Predecting model\ny_pred= ridge_model.predict(X_test_standard)\n\n#step 4: Evaluating the model\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nRidge_MAE = mean_absolute_error(y_test, y_pred)\nRidge_MSE = mean_squared_error(y_test, y_pred)\nRidge_RMSE = np.sqrt(Ridge_MSE)\n\npd.DataFrame([Ridge_MAE, Ridge_MSE, Ridge_RMSE], index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","e317dc48":"from sklearn.linear_model import RidgeCV\n\nridge_cv_model=RidgeCV(alphas=(0.1, 1.0, 10.0), scoring='neg_mean_absolute_error')\n\nridge_cv_model.fit(X_train_standard, y_train)\n\ny_pred_ridge= ridge_cv_model.predict(X_test_standard)\n\nRidgeCV_MAE= mean_absolute_error(y_test, y_pred_ridge)\nRidgeCV_MSE= mean_squared_error(y_test, y_pred_ridge)\nRidgeCV_RMSE= np.sqrt(RidgeCV_MSE)\n\npd.DataFrame([RidgeCV_MAE, RidgeCV_MSE, RidgeCV_RMSE], index=['MAE', 'MSE', 'RMSE'], columns=['Ridge Metrics'])","a8ec7ca1":"from sklearn.linear_model import LassoCV\n\n#step 1: Choose the model type\nlasso_cv_model= LassoCV(eps=0.01, n_alphas=100, cv=5)\n\n#step 2: Train the model with fit\nlasso_cv_model.fit(X_train_standard, y_train)\n\n#step 3: Predecting model\ny_pred_lasso= lasso_cv_model.predict(X_test_standard)\n\n#step 4: Evaluating the model\n\nLasso_MAE= mean_absolute_error(y_test, y_pred_lasso)\nLasso_MSE= mean_squared_error(y_test, y_pred_lasso)\nLasso_RMSE= np.sqrt(Lasso_MSE)\n\npd.DataFrame([Lasso_MAE, Lasso_MSE, Lasso_RMSE], index=['MAE', 'MSE', 'RMSE'], columns=['Lasso Metrics'])","7fda2986":"from sklearn.linear_model import ElasticNetCV\n\n#step 1: Choose the model type\nelastic_model= ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],cv=5, max_iter=100000)\n\n#step 2: Train the model with fit\nelastic_model.fit(X_train_standard, y_train)\n\n#step 3: Predecting model\ny_pred_elastic=elastic_model.predict(X_test_standard)\n\n#step 4: Evaluating the model\n\nMAE_Elastic= mean_absolute_error(y_test, y_pred_elastic)\nMSE_Elastic= mean_squared_error(y_test, y_pred_elastic)\nRMSE_Elastic= np.sqrt(MSE_Elastic)\n\npd.DataFrame([MAE_Elastic, MSE_Elastic, RMSE_Elastic], index=['MAE', 'MSE', 'RMSE'], columns=['Elastic Metrics'])","f4b6d046":"np.count_nonzero(elastic_model.coef_)","fc769af5":"metrics_compare = pd.DataFrame({'Model':['Ridge','RidgeCV','LassoCV','Elastic Net'],\n                                'MAE':[Ridge_MAE,RidgeCV_MAE,Lasso_MAE,MAE_Elastic],\n                                'MSE':[Ridge_MSE,RidgeCV_MSE,Lasso_MAE,MSE_Elastic],\n                                'RMSE': [Ridge_RMSE,RidgeCV_RMSE,Lasso_RMSE,RMSE_Elastic]})\nmetrics_compare","774bf0e0":"from math import ceil\n\ndata = metrics_compare\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n\naxs[0].bar(metrics_compare['Model'], metrics_compare['MAE'])\naxs[1].scatter(metrics_compare['Model'], metrics_compare['MAE'])\naxs[2].plot(metrics_compare['Model'], metrics_compare['MAE'])\nfig.suptitle('MAE Compare')\n\nplt.ylim([4.3000, 4.5000])","6a7a8504":"print('Ridge model has {} zero Coefficient.'.format(list(ridge_model.coef_).count(0)))\nprint('Ridge with cross validation model has {} zero Coefficient.'.format(list(ridge_cv_model.coef_).count(0)))\nprint('Lasso Regression model has {} zero Coefficient.'.format(list(lasso_cv_model.coef_).count(0)))\nprint('Elastic net model has {} zero Coefficient.'.format(list(elastic_model.coef_).count(0)))","2f6b0b97":"we can compare one of X_train column with same column of X_train_standard to see differences.","8dee57b9":"# 6. Regularization\nAfter preprocessing data we can build models we want. In the following we build 3 models with 3 Algorithms.\n\n1. Ridge Regression\n2. Lasso Regression\n3. Elastic Net","f059f9d3":"# 5. Scaling the data","8be27cc9":"# 7. Compare models","3eb4bc52":"# 2. Define features and Target Variable\nnow we have to determine which columns are features and which one is target variable. ","87bc32c9":"# 3. Preprocessing Data ( for Polynomial Conversion )\n\nIn my last notebook (checking same dataset for polynomial regression) we find out the best degree for these model is \"2\". You can check that here: https:\/\/www.kaggle.com\/javadmaddah\/polynomial-regression-on-real-estate-week-3-2","f8737c0d":"### 6-3. Elastic Net with Cross-validation\n![image.png](attachment:image.png)","9a701961":"### 6-1. Ridge Regression\n\n![image.png](attachment:image.png)","8c7eb4fc":"as we can see at first we have 8 columns (7 features) and after Polynomial conversion we have 35 features for building model.","25748832":"Wehn you work with big datasets you maybe face overfitting! Overfitting happend when the model is too complex. when it happends, train test work properly but model does not match the test set. \n\nIn linear regression all features have a non-zero coefficient in model, even if they are not actully effects!\n\n![image.png](attachment:image.png)\n\nFor Reducing the effect of overfitting we can reduce Dimensions of the dataset. There are some methods for regularization that make a function to increas cost of increasing model coefficients, so model try to remove some features. \n\nIn this notebook we try to use some of these methods for getting better results.\n\nbefore that once-over we review the steps of building model and evaluating it after preprocessing with skleran!\n\n1. Choose the model type (LinearRegression, Ridge, RidgeCv,...)\n2. train model with model.fit\n3. predicting model\n4. evaluate with Indicators","7f2073e3":"# 4. Split data to train - test set","79f16199":"As we could see, there are 5 factors that maybe have effects on price. Transaction date, house age, distance to the nearest MRT station, number of convenience stores, latitude and longitude.\n\nwith correlation analysis we can figure out the basis information about dependence between features.","c79d68dc":"### 6-2. Lasso Regression with Cross-validation\n\n![image.png](attachment:image.png)","cd7a9f40":"# 1. Import Data and first review\nIn first step we should import dataset and check it for basic information and EDA.","454b452e":"#### 6-1-1. Ridge Regression with Cross-validation"}}