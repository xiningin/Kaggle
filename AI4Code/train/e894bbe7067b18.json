{"cell_type":{"74b5029b":"code","da49128c":"code","53263c54":"code","a393b06f":"code","3edc7044":"code","f0cdd26d":"code","d32b9ea2":"code","b271e875":"code","9b57c7d4":"code","85096690":"code","3952fdf9":"code","485d01b8":"code","ac1cbd09":"code","328d7213":"code","40b2975b":"code","bbac4f8d":"code","868fd428":"code","9efa2110":"code","448b0ae6":"code","0e7ddf6f":"code","013a28cf":"code","81cccef9":"code","860f3348":"code","96972c2e":"code","63186fea":"code","8327bda1":"code","efe1037b":"code","eafa4373":"code","62099dc0":"code","8f2bef64":"code","ff9b6802":"code","2a6d9586":"code","e796feaa":"code","f7617ed9":"code","e6491591":"code","192abaef":"code","ef2492e0":"code","23ec3929":"code","6b986d9f":"code","b0be0b4f":"code","62343a1f":"code","a1037ad6":"code","138902a6":"code","f636d442":"markdown","b39c8bd8":"markdown","b0d53693":"markdown","06d9aaa9":"markdown","3a707f55":"markdown","09e2ffc2":"markdown","c14d93b4":"markdown","50add2d1":"markdown","10eb43cd":"markdown","355a06ed":"markdown","ec8fd413":"markdown","58847a8d":"markdown","b951f66b":"markdown","6baa39b0":"markdown","798cb4ee":"markdown","c572b687":"markdown","b2628dfd":"markdown","56afe10b":"markdown","0f6c7add":"markdown","260d24f8":"markdown","d4bece25":"markdown","253f5d7f":"markdown","d6073440":"markdown","ebc2a7ea":"markdown","03f8a808":"markdown"},"source":{"74b5029b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da49128c":"import sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","53263c54":"# datasets path and df names\npath = [\n    '\/kaggle\/input\/national-health-and-nutrition-examination-survey\/demographic.csv',\n    '\/kaggle\/input\/national-health-and-nutrition-examination-survey\/examination.csv',\n    '\/kaggle\/input\/national-health-and-nutrition-examination-survey\/questionnaire.csv',\n    '\/kaggle\/input\/national-health-and-nutrition-examination-survey\/labs.csv',\n    '\/kaggle\/input\/national-health-and-nutrition-examination-survey\/diet.csv'  \n]\n\ndfname = [\n    'dm',\n    'exam',\n    'qs',\n    'lab',\n    'diet'\n]","a393b06f":"# import datasets as dfs\n\ndf = {}\ndfn = dict(zip(dfname, path))\ndf = {key: pd.read_csv(value) for key, value in dfn.items()}","3edc7044":"Xs = {k: v for k, v in df.items() if k in ['dm', 'exam', 'labs']}\n\ndfs = Xs.values()\n\nfrom functools import partial, reduce \ninner_merge = partial(pd.merge, how='inner', on='SEQN') \n\nc = reduce(inner_merge, dfs)\nc\n\n# check if there are duplicated SEQN\nc.SEQN.duplicated().value_counts()\n","f0cdd26d":"# show combined df\nqs = df['qs'][['SEQN','MCQ160F']]\nqs","d32b9ea2":"c = pd.merge(c,qs, how='left', on='SEQN')\nc","b271e875":"c.MCQ160F.value_counts()","9b57c7d4":"# MCQ160F (target feature): exclude null values and NA\nc = c[(c.MCQ160F.notnull()) & (c.MCQ160F != 9)]\n\n# check MCQ160F\nc.MCQ160F.describe()","85096690":"# target varoable counts\nc.MCQ160F.value_counts()","3952fdf9":"# import code book\ncbook = pd.read_csv('\/kaggle\/input\/nhanes-2013-2014-codebook-with-sas-label\/nhanes_2013_2014_codebook.csv')\n\ncbook","485d01b8":"# exclude non-numeric values\nd = c.select_dtypes(['number'])\n\n# exclue columns that have over 50% NaN\nd = d.dropna(thresh = 0.5*len(d), axis =1)\n\nprint(len(d.columns), 'columns left')","ac1cbd09":"# changing target variable coding from 1, 2 to 0 (Negative), 1 (Positive)\nd['MCQ160F']=d.apply(lambda x: 1 if x.MCQ160F == 1 else 0, axis='columns')\nd.MCQ160F.value_counts()","328d7213":"vals = d.MCQ160F.value_counts()\n\nplt.figure(figsize=(8,6))\nplt.rc('font', size=12)\n\nax = vals.plot.bar(rot=0, color='#4B4E6D')\n\nfor i in range(len(vals)):\n    ax.annotate(vals[i], xy=[vals.index[i], vals[i]], ha='center', va='bottom')","40b2975b":"from sklearn.impute import SimpleImputer\nimp_mode=SimpleImputer(strategy='most_frequent')\n\nd = pd.DataFrame(imp_mode.fit_transform(d), columns=d.columns)","bbac4f8d":"X = d.loc[:, d.columns != 'MCQ160F']\ny = d.MCQ160F","868fd428":"print('X shape:', X.shape)\nprint('y shape:', y.shape)","9efa2110":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)","448b0ae6":"from xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\ndef confusion(y_test, y_pred):\n    conf = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['True[0]', 'True[1]'], columns=['Predict[0]', 'Predict[1]'])\n    print('Confusion Matrix:')\n    print(conf)\n    return conf\n\nconfusion(y_test, y_pred)","0e7ddf6f":"y_test.value_counts()\n","013a28cf":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)\nX_test_sm, y_test_sm = smote.fit_sample(X_test, y_test)\n\nX_train_sm = pd.DataFrame(X_train_sm, columns=X.columns)\nX_test_sm = pd.DataFrame(X_test_sm, columns=X.columns)","81cccef9":"print(y_train_sm.value_counts())\nprint(y_test_sm.value_counts())","860f3348":"model = XGBClassifier()\nmodel.fit(X_train_sm, y_train_sm)\ny_pred_sm = model.predict(X_test_sm)\n\naccuracy = accuracy_score(y_test_sm, y_pred_sm)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nconf = pd.DataFrame(confusion_matrix(y_test_sm, y_pred_sm), index=['True[0]', 'True[1]'], columns=['Predict[0]', 'Predict[1]'])\nconf","96972c2e":"from xgboost import XGBClassifier\nfrom matplotlib import pyplot\n\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X_train_sm, y_train_sm)\n\n# Features selected by XGBoost\nkeys = list(model.get_booster().feature_names)\nvalues = list(model.feature_importances_)\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 24 features\nxgbfs_ = data[:24]\n\n# Plot feature score\nxgbfs_.sort_values(by='score').plot(kind='barh', figsize=(10, 8), color='#4B4E6D')\n","63186fea":"xgbfs_.reset_index()","8327bda1":"xgbfs = xgbfs_.reset_index()\nxgbfs.columns=['variable', 'score']\n\nxgbfs['variable'] = xgbfs['variable'].apply(lambda x: x.upper())\n\nxgbfs = pd.merge(left=xgbfs, right=cbook, left_on='variable', right_on='variable', how='left')\nxgbfs","efe1037b":"# final variables\nvar_list = xgbfs.variable.tolist()\nvar_list.append('MCQ160F')\nprint(var_list)","eafa4373":"#final df\ndf_final = d.filter(var_list)\ndf_final","62099dc0":"# sns.axes_style(\"white\")\nax = plt.subplots(figsize=(20,20))\n\ncorr = df_final.corr()\n\n# sns.heatmap(corr, vmin=-1, vmax=1, cmap=sns.cm.rocket_r)\nsns.heatmap(corr, cmap=sns.cm.rocket_r)","8f2bef64":"X_ = df_final.loc[:, df_final.columns != 'MCQ160F']\ny = df_final.MCQ160F","ff9b6802":"from sklearn.preprocessing import MinMaxScaler\n\nminmax=MinMaxScaler()\nX = pd.DataFrame(minmax.fit_transform(X_), columns=X_.columns)\nX","2a6d9586":"from sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)","e796feaa":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)\n# X_test_sm, y_test_sm = smote.fit_sample(X_test, y_test)\n\nX_train_sm = pd.DataFrame(X_train_sm, columns=X.columns)\n# X_test_sm = pd.DataFrame(X_test_sm, columns=X.columns)","f7617ed9":"print('X train shape: ',X_train_sm.shape)\nprint('y train values: \\n', y_train_sm.value_counts())\nprint()\nprint('X test shape: ',X_test.shape)\nprint('y test values: \\n', y_test.value_counts())","e6491591":"mscore=[]","192abaef":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(max_iter=100, solver='lbfgs', class_weight='balanced', random_state=11).fit(X_train_sm, y_train_sm)\ny_pred = clf.predict(X_test)\n\nprint('Accuracy Score:', clf.score(X_test, y_test))\nprint('Prediction:', y_pred)\n\nmscore.append(['Logistic Regression', clf.score(X_test, y_test)])\n\nprint(classification_report(y_test, y_pred))\nconfusion(y_test, y_pred)","ef2492e0":"from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=150, criterion='gini', max_depth=5, random_state=11)\nrnd_clf.fit(X_train_sm, y_train_sm)\n\ny_pred = rnd_clf.predict(X_test)\n\nprint('Accuracy Score:', rnd_clf.score(X_test, y_test))\nprint('Prediction:', y_pred)\n\nmscore.append(['Random Forest', rnd_clf.score(X_test, y_test)])\n\nprint(classification_report(y_test, y_pred))\nconfusion(y_test, y_pred)","23ec3929":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(learning_rate=0.1, n_estimators=10, random_state=11)\ngbc.fit(X_train_sm, y_train_sm)\n\ny_pred = gbc.predict(X_test)\n\nprint('Accuracy Score:', gbc.score(X_test, y_test))\nprint('Prediction:', y_pred)\n\nmscore.append(['GradientBoosting', gbc.score(X_test, y_test)])\n\nprint(classification_report(y_test, y_pred))\nconfusion(y_test, y_pred)","6b986d9f":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(learning_rate=0.01, n_estimators=30, random_state=11)\nada.fit(X_train_sm, y_train_sm)\n\ny_pred = ada.predict(X_test)\n\nprint('Accuracy Score:', ada.score(X_test, y_test))\nprint('Prediction:', y_pred)\n\nmscore.append(['Adaptive Boosting', ada.score(X_test, y_test)])\n\n# from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\nconfusion(y_test, y_pred)","b0be0b4f":"from sklearn.svm import SVC\nsvm_clf = SVC(kernel='sigmoid', gamma='auto', random_state=11)\nsvm_clf.fit(X_train_sm, y_train_sm)\ny_pred = svm_clf.predict(X_test)\n\nprint('Accuracy Score:', svm_clf.score(X_test, y_test))\nprint('Prediction:', y_pred)\n\nmscore.append(['SVM', svm_clf.score(X_test, y_test)])\n\n# from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\nconfusion(y_test, y_pred)","62343a1f":"from xgboost import XGBClassifier\n\nxgbc = XGBClassifier(eta=0.01, max_depth=3)\n# xgbc = XGBClassifier(eta=0.01)\nxgbc.fit(X_train_sm, y_train_sm)\n\ny_pred = xgbc.predict(X_test)\n\nprint('Accuracy Score:', svm_clf.score(X_test, y_test))\nprint('Prediction:', y_pred)\n\nmscore.append(['XGBoost', xgbc.score(X_test, y_test)])\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\nconfusion(y_test, y_pred)","a1037ad6":"mscore.sort(key=lambda x: x[1], reverse=True)\nmscore","138902a6":"model = list(i[0] for i in mscore)\nscore = list(round(i[1]*100,2) for i in mscore)\n\nprint('Accracy Score: \\n')\nfor m,s in zip(model, score):\n    print(f'{m}: {s}%')\n  \n \n# creating horizontal bar plot\nplt.barh(model, score, height = 0.5, color='#4B4E6D') # this color is called independence, how cool!\n \nplt.xlabel(\"Accuracy Score\")\nplt.ylabel(\"Model\")\nplt.title(\"Model Comparison\")\nplt.gca().invert_yaxis()\nplt.show()","f636d442":"## Stroke Prediction Using Machine Learning\n### Li-Chia Chen\n\n## I. Introduction\nStroke is the fifth cause of death in the United States, according to the Heart Disease and Stroke Statistics 2020 report. Those who suffer from stroke, if luckily survived, may also suffer from expensive medical bills and even disability. Foreseeing the underlying risk factors of stroke is highly valuable to stroke screening and prevention. In this project, the National Health and Nutrition Examination Survey (NHANES) data from the National Center for Health Statistics (NCHS) is used to develop machine learning models. The NHANES dataset holds an abundance of variables, ranging from demographics, medical history, physical examinations, biochemistry to dietary and lifestyle questionnaires. Known features contributing to stroke, such as blood pressure, serum cholesterol level, alcohol consumption, weight, etc., and additional features will be selected for correlation evaluation and machine learning model development. \n\nMain elements include data cleaning, imbalance dataset processing, feature selection from over 1,000 variables, and model training.","b39c8bd8":"#### Oversampling with SMOTE","b0d53693":"### Upsampling Minoroty Class with SMOTE\nNote that it is important to only oversample after train\/test split, so the testing data will not leak.","06d9aaa9":"## IV. Model Training","3a707f55":"### Gradient Boosting Decision Trees","09e2ffc2":"## V. Model Comparison","c14d93b4":"## VI. Future Work\n1. Model optimization\/hyperparameter tuning\n2. Ensamble\n3. ROC\/AUC comparison\n","50add2d1":"### Merge SAS Labels from the codebook","10eb43cd":"### Train\/Test Split","355a06ed":"### Data Cleaning","ec8fd413":"### Logistic Regression","58847a8d":"### AdaBoostClassifier","b951f66b":"## II. Data Loading and Cleaning\n### Datasets\n- NHANES Datasets from 2013-2014<br><br>\nhttps:\/\/www.kaggle.com\/cdc\/national-health-and-nutrition-examination-survey\n<br><br>\n- Variable Search<br><br>\nhttps:\/\/wwwn.cdc.gov\/nchs\/nhanes\/search\/default.aspx\n<br><br>\n- Codebook<br><br>\nhttps:\/\/wwwn.cdc.gov\/nchs\/nhanes\/Search\/DataPage.aspx?Component=Demographics&CycleBeginYear=2013\n<br><br>\n","6baa39b0":"## III. Feature Selection\nReference: \nA data-driven approach to predicting diabetes and cardiovascular disease with machine learning<br><br>\nhttps:\/\/pubmed.ncbi.nlm.nih.gov\/31694707\/","798cb4ee":"### Upsampling minority class\nIn the following cells the minority class is upsampled with SMOTE (Synthetic Minority Oversampling Technique).","c572b687":"### Codebook\n- Codebook with variable and sas label parsed with Beautiful Soup:\nhttps:\/\/www.kaggle.com\/lcchennn\/nhanes-2013-2014-codebook-with-sas-label","b2628dfd":"### XGBClassifier for feature selection\nFrom the classificaiton result, the accuracy is pretty decent. However once look into the confusion matrix we can see there is only 1 correct prediction for class 1 (recall for class 1 is bad). This is commonly seen for imbalanced dataset, which the classes in the dataset have highly uneven sample sizes. Imbalanced problems is often seen in healthcare  datasets.","56afe10b":"### SVM","0f6c7add":"### Data Normalization","260d24f8":"### Random Forest","d4bece25":"### Train\/Test Split","253f5d7f":"### XGBoost","d6073440":"#### Check the Classification Result Again\nAfter oversampling, the classification result is more reasonable.","ebc2a7ea":"### Feature Selection with XGBoost","03f8a808":"### Exclude rows with null values or NA for MCQ160F\nThe prediction target in the dataset is MCQ160F, a questionnaire question \"Has a doctor or other health professional ever told you that you had a stroke?\"\n"}}