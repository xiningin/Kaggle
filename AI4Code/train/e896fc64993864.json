{"cell_type":{"09acbaeb":"code","af37b690":"code","15ba7854":"code","cb9b7129":"code","d7b96971":"code","0dc7828b":"code","6d6f63c6":"code","55569a4f":"code","79bb7e2f":"code","99cab3b2":"code","5f37ebdf":"code","52590cf7":"code","8fbeb193":"code","fa16c81d":"code","e6751285":"code","6cc85c69":"code","76caf1c6":"code","3d1686a5":"code","a6bcdc6d":"code","8cfcf186":"code","deca22aa":"code","3a5471cd":"code","f99f406b":"code","45def44c":"code","b53049be":"code","bc909780":"code","181208cc":"code","731f8359":"code","3cc69468":"code","b2845ee5":"code","48e3080b":"code","db0552e5":"code","817ff92b":"code","f623c4a4":"code","386f438f":"code","5f9d2686":"code","2908e164":"code","486cb45d":"code","73f5c170":"code","9518ca06":"code","dc491b2e":"code","7660d98f":"code","7ab6c092":"code","b491f25f":"code","68c6329f":"code","3d2d40c8":"code","b6fb9b41":"code","85ccf3ed":"code","a9257bd2":"code","e88e28f0":"code","693a2c35":"code","b1aac536":"code","3e359bed":"code","bdb33042":"code","075484cf":"code","ff7237d3":"code","65db532a":"code","b598f8f7":"code","478e7cc4":"code","8d7c6dc3":"code","5a59cdfa":"code","683f7063":"code","6078e5d0":"code","e8f1a57b":"code","8435cd2b":"code","ffdabe5b":"code","62982dc7":"code","bbce37c7":"code","ad069a0b":"code","33901508":"code","e77eb563":"code","fb4f071e":"code","119d92a0":"code","c4030ad0":"code","82a6c3b1":"code","70a85395":"code","bd3053ae":"code","0e6c8ed2":"code","8865bd6d":"code","da31846e":"code","a4f06912":"code","431209a4":"code","c1b78475":"code","6827e5b5":"code","fb0fb985":"code","2949c038":"code","2b197532":"code","f08d991e":"code","b486b9d5":"code","1c5837c2":"code","ef958e74":"code","bc9db998":"code","958d819f":"code","bd254339":"code","c6cc0221":"code","f8859995":"code","fcc9eef6":"code","e9d381f6":"code","de764fe9":"code","104b8bc7":"code","e512a9a0":"markdown","cc5547df":"markdown","93db8a12":"markdown","8d9d3795":"markdown","c95d5e14":"markdown","40433909":"markdown","88e0c1dc":"markdown","f2a7579b":"markdown","31312c14":"markdown","9b421e7a":"markdown","e073a274":"markdown","faefa5ca":"markdown","aa5d92a0":"markdown","b369e5b0":"markdown","544462da":"markdown","c1e4ebf8":"markdown","d2583ad7":"markdown","83f84954":"markdown","d1bd9fab":"markdown","86b5459d":"markdown","84a8408d":"markdown","0fbfb1bd":"markdown","a3f3e6d1":"markdown","c68fb67e":"markdown","485ded1f":"markdown","3a5c5df3":"markdown","a210c543":"markdown","428d5e35":"markdown","3f880c18":"markdown","6bcfc03c":"markdown","2cc91060":"markdown","eac1f808":"markdown","9a86e979":"markdown","ad0b9840":"markdown","5dcc1240":"markdown","efba2b2c":"markdown"},"source":{"09acbaeb":"from __future__ import print_function  # Compatability with Python 3\nprint( 'Print function ready to serve.' )","af37b690":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\npd.set_option('display.max_columns', 100)\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline \n\n# Seaborn for easier visualization\nimport seaborn as sns\n\nfrom scipy import stats\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"\/\"]).decode(\"utf8\"))","15ba7854":"# Load real estate data from CSV\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","cb9b7129":"# setting the number of cross validations used in the Model part \nnr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# only columns with correlation above this threshold value  \n# are used for the ML Regressors in Part 3\nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1","d7b96971":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","0dc7828b":"def print_cols_large_corr(df, nr_c, targ) :\n    corr = df.corr()\n    corr_abs = corr.abs()\n    print (corr_abs.nlargest(nr_c, targ)[targ])","6d6f63c6":"def plot_corr_matrix(df, nr_c, targ) :\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()","55569a4f":"# Dataframe dimensions\nprint(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)","79bb7e2f":"# Column datatypes\nprint(df_train.dtypes)\nprint(\"*\"*50)\nprint(df_test.dtypes)","99cab3b2":"# Type of df.types\ntype(df_train.dtypes)","5f37ebdf":"# Display first 5 rows of df_train\ndf_train.head()","52590cf7":"# Display first 5 rows of df_test\ndf_test.head()","8fbeb193":"# Filter and display only df.dtypes that are 'object'\ndf_train.dtypes[df_train.dtypes == 'object']","fa16c81d":"# Loop through categorical feature names and print each one\nfor feature in df_train.dtypes[df_train.dtypes == 'object'].index:\n    print(feature)","e6751285":"# Display the first 10 rows of data\ndf_train.head(10)","6cc85c69":"# Display last 5 rows of data\ndf_train.tail()","76caf1c6":"# Plot histogram grid\ndf_train.hist(figsize=(20,20), xrot=-45)\n\n# Clear the text \"residue\"\nplt.show()","3d1686a5":"# Summarize numerical features\ndf_train.describe()","a6bcdc6d":"df_test.describe()","8cfcf186":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","deca22aa":"df_train['SalePrice_Log'] = np.log1p(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())","3a5471cd":"# Summarize categorical features\ndf_train.describe(include=['object'])","f99f406b":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","45def44c":"print(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train[categorical_feats].columns)","b53049be":"# Plot bar plot for each categorical feature\n\nfor feature in df_train.dtypes[df_train.dtypes == 'object'].index:\n    sns.countplot(y=feature, data=df_train)\n    plt.show()","bc909780":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","181208cc":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","731f8359":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","3cc69468":"# fillna with mean or mode for the remaining values\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)\ndf_train.fillna(df_train.mode(), inplace=True)\ndf_test.fillna(df_test.mode(), inplace=True)","b2845ee5":"df_train.isnull().sum().sum()","48e3080b":"df_test.isnull().sum().sum()","db0552e5":"for col in numerical_feats:\n    print(col)\n    print(\"Skewness: %f\" % df_train[col].skew())\n    print(\"Kurtosis: %f\" % df_train[col].kurt())\n    print(\"*\"*50)","817ff92b":"sns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","f623c4a4":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","386f438f":"for df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index","5f9d2686":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","2908e164":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","486cb45d":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()","73f5c170":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n\ncols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)","9518ca06":"print(ser_corr)\nprint(\"*\"*30)\nprint(\"List of numerical features with r above min_val_corr :\")\nprint(cols_abv_corr_limit)\nprint(\"*\"*30)\nprint(\"List of numerical features with r below min_val_corr :\")\nprint(cols_bel_corr_limit)","dc491b2e":"for catg in list(categorical_feats) :\n    print(df_train[catg].value_counts())\n    print('#'*50)","7660d98f":"li_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()   ","7ab6c092":"catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', \n                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncatg_weak_corr = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                  'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', \n                  'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', \n                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', \n                  'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                  'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                  'SaleCondition' ]\n      ","b491f25f":"nr_feats = len(cols_abv_corr_limit)","68c6329f":"plot_corr_matrix(df_train, nr_feats, target)","3d2d40c8":"id_test = df_test['Id']\n\nto_drop_num  = cols_bel_corr_limit\nto_drop_catg = catg_weak_corr\n\ncols_to_drop = ['Id'] + to_drop_num + to_drop_catg \n\nfor df in [df_train, df_test]:\n    df.drop(cols_to_drop, inplace= True, axis = 1)","b6fb9b41":"catg_list = catg_strong_corr.copy()\ncatg_list.remove('Neighborhood')\n\nfor catg in catg_list :\n    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')\n    sns.violinplot(x=catg, y=target, data=df_train)\n    plt.show()\n    #sns.boxenplot(x=catg, y=target, data=df_train)\n    #bp = df_train.boxplot(column=[target], by=catg)","85ccf3ed":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y=target, data=df_train, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","a9257bd2":"for catg in catg_list :\n    g = df_train.groupby(catg)[target].mean()\n    print(g)","e88e28f0":"# 'MSZoning'\nmsz_catg2 = ['RM', 'RH']\nmsz_catg3 = ['RL', 'FV'] \n\n\n# Neighborhood\nnbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']\nnbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']\n\n# Condition2\ncond2_catg2 = ['Norm', 'RRAe']\ncond2_catg3 = ['PosA', 'PosN'] \n\n# SaleType\nSlTy_catg1 = ['Oth']\nSlTy_catg3 = ['CWD']\nSlTy_catg4 = ['New', 'Con']\n\n#[]","693a2c35":"for df in [df_train, df_test]:\n    \n    df['MSZ_num'] = 1  \n    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2    \n    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3        \n    \n    df['NbHd_num'] = 1       \n    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2    \n    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3    \n\n    df['Cond2_num'] = 1       \n    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2    \n    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3    \n    \n    df['Mas_num'] = 1       \n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2 \n    \n    df['ExtQ_num'] = 1       \n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2     \n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3     \n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4     \n   \n    df['BsQ_num'] = 1          \n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2     \n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3     \n \n    df['CA_num'] = 0          \n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1    \n\n    df['Elc_num'] = 1       \n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2 \n\n\n    df['KiQ_num'] = 1       \n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2     \n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3     \n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4      \n    \n    df['SlTy_num'] = 2       \n    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1  \n    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3  \n    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4  ","b1aac536":"new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']\n\nnr_rows = 4\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(new_col_num):\n            sns.regplot(df_train[new_col_num[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","3e359bed":"catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncorr1 = df_train.corr()\ncorr_abs_1 = corr1.abs()\n\nnr_all_cols = len(df_train)\nser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_1)\ncols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n\n\nfor df in [df_train, df_test] :\n    df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n    df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1)    ","bdb33042":"corr2 = df_train.corr()\ncorr_abs_2 = corr2.abs()\n\nnr_all_cols = len(df_train)\nser_corr_2 = corr_abs_2.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_2)","075484cf":"df_train.head()","ff7237d3":"df_test.head()","65db532a":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_all_cols = len(df_train)\nprint (corr_abs.nlargest(nr_all_cols, target)[target])","b598f8f7":"nr_feats=len(df_train.columns)\nplot_corr_matrix(df_train, nr_feats, target)","478e7cc4":"cols = corr_abs.nlargest(nr_all_cols, target)[target].index\ncols = list(cols)\n\nif drop_similar == 1 :\n    for col in ['GarageArea','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n        if col in cols: \n            cols.remove(col)","8d7c6dc3":"cols = list(cols)\nprint(cols)","5a59cdfa":"feats = cols.copy()\nfeats.remove('SalePrice_Log')\nfeats.remove('SalePrice')\n\nprint(feats)","683f7063":"df_test.head()","6078e5d0":"df_train_ml = df_train[feats].copy()\ndf_test_ml  = df_test[feats].copy()\n\ny = df_train[target]\n\nprint(target)","e8f1a57b":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline \n\n# Seaborn for easier visualization\nimport seaborn as sns\n\n# Scikit-Learn for Modeling\nimport sklearn\n\n# Import Elastic Net, Ridge Regression, and Lasso Regression\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso\n\n# Import Random Forest and Gradient Boosted Trees\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","8435cd2b":"X = df_train_ml.copy()\ny = df_train[target]\nX_test = df_test_ml.copy()\n#y_test = df_train[target]\n\nX.info()\nX_test.info()","ffdabe5b":"# Split X and y into train and test sets\nX_train = df_train[feats].copy()\nX_test = df_test[feats].copy()\ny_train = df_train[target]\n#y_test = df_test[target]","62982dc7":"X.head()","bbce37c7":"X_test.head()","ad069a0b":"print( len(X_train), len(X_test), len(y_train) )","33901508":"# Summary statistics of X_train\nX_train.describe()","e77eb563":"# Standardize X_train\nX_train_new = (X_train - X_train.mean()) \/ X_train.std()","fb4f071e":"# Summary statistics of X_train_new\nX_train_new.describe()","119d92a0":"# Function for creating model pipelines\nfrom sklearn.pipeline import make_pipeline","c4030ad0":"# For standardization\nfrom sklearn.preprocessing import StandardScaler","82a6c3b1":"make_pipeline(StandardScaler(), Lasso(random_state=123))","70a85395":"# Create pipelines dictionary\npipelines = {\n    'lasso' : make_pipeline(StandardScaler(), Lasso(random_state=123)),\n    'ridge' : make_pipeline(StandardScaler(), Ridge(random_state=123)),\n    'enet'  : make_pipeline(StandardScaler(), ElasticNet(random_state=123))\n}","bd3053ae":"# Add a pipeline for 'rf'\npipelines['rf'] = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=123))\n\n# Add a pipeline for 'gb'\npipelines['gb'] = make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state=123))","0e6c8ed2":"# Check that we have all 5 algorithms, and that they are all pipelines\nfor key, value in pipelines.items():\n    print( key, type(value) )","8865bd6d":"# List tuneable hyperparameters of our Lasso pipeline\npipelines['lasso'].get_params()","da31846e":"# Lasso hyperparameters\nlasso_hyperparameters = { \n    'lasso__alpha' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] \n}\n\n# Ridge hyperparameters\nridge_hyperparameters = { \n    'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]  \n}","a4f06912":"# Elastic Net hyperparameters\nenet_hyperparameters = { \n    'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],                        \n    'elasticnet__l1_ratio' : [0.1, 0.3, 0.5, 0.7, 0.9]  \n}","431209a4":"# Random forest hyperparameters\nrf_hyperparameters = { \n    'randomforestregressor__n_estimators' : [100, 200],\n    'randomforestregressor__max_features': ['auto', 'sqrt', 0.33],\n}","c1b78475":"# Boosted tree hyperparameters\ngb_hyperparameters = { \n    'gradientboostingregressor__n_estimators': [100, 200],\n    'gradientboostingregressor__learning_rate' : [0.05, 0.1, 0.2],\n    'gradientboostingregressor__max_depth': [1, 3, 5]\n}","6827e5b5":"# Create hyperparameters dictionary\nhyperparameters = {\n    'rf' : rf_hyperparameters,\n    'gb' : gb_hyperparameters,\n    'lasso' : lasso_hyperparameters,\n    'ridge' : ridge_hyperparameters,\n    'enet' : enet_hyperparameters\n}","fb0fb985":"for key in ['enet', 'gb', 'ridge', 'rf', 'lasso']:\n    if key in hyperparameters:\n        if type(hyperparameters[key]) is dict:\n            print( key, 'was found in hyperparameters, and it is a grid.' )\n        else:\n            print( key, 'was found in hyperparameters, but it is not a grid.' )\n    else:\n        print( key, 'was not found in hyperparameters')","2949c038":"# Helper for cross-validation\nfrom sklearn.model_selection import GridSearchCV","2b197532":"# Create cross-validation object from Lasso pipeline and Lasso hyperparameters\nmodel = GridSearchCV(pipelines['lasso'], hyperparameters['lasso'], cv=10, n_jobs=-1)","f08d991e":"type(model)","b486b9d5":"# Fit and tune model\nmodel.fit(X_train, y_train)","1c5837c2":"# Create empty dictionary called fitted_models\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and saving it to fitted_models\nfor name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n    # Fit model on X_train, y_train\n    model.fit(X_train, y_train)\n    \n    # Store model in fitted_models[name] \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, 'has been fitted.')","ef958e74":"# Check that we have 5 cross-validation objects\nfor key, value in fitted_models.items():\n    print( key, type(value) )","bc9db998":"from sklearn.exceptions import NotFittedError\n\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))","958d819f":"# Display best_score_ for each fitted model\nfor name, model in fitted_models.items():\n    print( name, model.best_score_ )","bd254339":"# Import r2_score and mean_absolute_error functions\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error","c6cc0221":"# Display fitted random forest object\nfitted_models['rf']","f8859995":"# Predict test set using fitted random forest\npred = fitted_models['rf'].predict(X_test)","fcc9eef6":"# Calculate and print R^2 and MAE, for training data set\npred_train = fitted_models['rf'].predict(X_train)\nprint( 'R^2:', r2_score(y_train, pred_train ))\nprint( 'MAE:', mean_absolute_error(y_train, pred_train))","e9d381f6":"print(pred)","de764fe9":"df_test.head()","104b8bc7":"# Create final table\n\npred_pd = pd.DataFrame()\npred_pd['Id'] = id_test\npred_pd['SalePrice'] = np.exp(pred)\n\npred_pd.head\n#pred_pd.to_csv('submission_mrig_v2.csv',index=False)","e512a9a0":"### import the libraries we'll need","cc5547df":"## Distributions of numeric features","93db8a12":"Display summary statistics for categorical features.","8d9d3795":"Outliers\n\nFind columns with strong correlation to target\nOnly those with r > min_val_corr are used in the ML Regressors in Part 3\nThe value for min_val_corr can be chosen in global settings","c95d5e14":"##### import the libraries we'll need","40433909":"### The target variable : Distribution of SalePrice","88e0c1dc":"Conclusion from EDA on categorical columns:\n\nFor many of the categorical there is no strong relation to the target.\nHowever, for some fetaures it is easy to find a strong relation.\nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType' Also for the categorical features, I use only those that show a strong relation to SalePrice. So the other columns are dropped when creating the ML dataframes in Part 2 :\n'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition'","f2a7579b":"Check for Multicollinearity\n\n<br> Strong correlation of these features to other, similar features:\n\n<br> 'GrLivArea_Log' and 'TotRmsAbvGrd'\n\n<br> 'GarageCars' and 'GarageArea'\n\n<br> 'TotalBsmtSF' and '1stFlrSF'\n\n<br> 'YearBuilt' and 'GarageYrBlt'\n\n<br> Of those features we drop the one that has smaller correlation coeffiecient to Target.","31312c14":"List of features with missing values","9b421e7a":"## Basic information","e073a274":"Convert categorical columns to numerical\n<br> For those categorcial features where the EDA with boxplots seem to show a strong dependence of the SalePrice on the \n<br> category, we transform the columns to numerical. To investigate the relation of the categories to SalePrice in more detail, we \n<br> make violinplots for these features Also, we look at the mean of SalePrice as function of category.","faefa5ca":"Correlation matrix 1\n<br> Features with largest correlation to SalePrice_Log\n<br> all numerical features with correlation coefficient above threshold","aa5d92a0":"the target variable SalePrice is not normally distributed.\nThis can reduce the performance of the ML models because they assume normal distribution, see sklearn info on preprocessing\n\nTherfore we make a log transformation, the resulting distribution looks much better.","b369e5b0":"#### List of features used for the Regressors in Part 3","544462da":"# Relation of features to target (SalePrice_log)","c1e4ebf8":"Of those features with the largest correlation to SalePrice, some also are correlated strongly to each other.\n\n<br> To avoid failures of the ML regression models due to multicollinearity, these are dropped in part 2.\n\n<br> This is optional and controlled by the switch drop_similar (global settings)","d2583ad7":"columns and correlation after dropping","83f84954":"Creating Datasets for ML algorithms","d1bd9fab":"Conclusion from EDA on numerical columns:\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.\nFor other features like 'MSSubClass' the correlation is very weak.\nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.\nThis threshold value can be choosen in the global settings : min_val_corr\n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:\n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',\n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.\nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)","86b5459d":"Number of Numerical and Categorical features","84a8408d":"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).\n<br> These will probably be useful for optimal performance of the Regressors in part 3.\n\n<br> Dropping the converted categorical columns and the new numerical columns with weak correlation\n\n<br> columns and correlation before dropping\n\n","0fbfb1bd":"## Part 2: Data wrangling\n<br> Drop all columns with only small correlation to SalePrice\n<br> Transform Categorical to numerical \n<br> Handling columns with missing data\n<br> Log values\n<br> Drop all columns with strong correlation to similar features\n\n<br> Numerical columns : drop similar and low correlation\n\n<br> Categorical columns : Transform to numerical\n\n<br> Dropping all columns with weak correlation to SalePrice","a3f3e6d1":"# Exploratory Analysis","c68fb67e":"List of numerical features and their correlation coefficient to target","485ded1f":"Columns of Numerical and Categorical features","3a5c5df3":"new dataframes","a210c543":"log transform\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:","428d5e35":"Filling missing values\nFor a few columns there is lots of NaN entries.\nHowever, reading the data description we find this is not missing data:\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.","3f880c18":"#### Correlation Matrix 2 : All features with strong correlation to SalePrice","6bcfc03c":"Relation to SalePrice for all categorical features","2cc91060":"### Plots of relation to target for all numerical features","eac1f808":"List of categorical features and their unique values","9a86e979":"### shape, info, head and describe","ad0b9840":"Some useful functions","5dcc1240":"Checking correlation to SalePrice for the new numerical columns","efba2b2c":"### List of all features with strong correlation to SalePrice_Log\n<br> after dropping all coumns with weak correlation"}}