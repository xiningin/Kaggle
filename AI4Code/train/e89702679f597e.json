{"cell_type":{"ad4909ad":"code","8f60a886":"code","3d2b8a84":"code","f304724d":"code","87a174ab":"code","5a72523a":"code","e978fd5a":"code","342241c2":"code","338bad7c":"code","bc2e8400":"code","4dab7de0":"code","885f71fc":"code","35543709":"code","6ef456a0":"code","86b38814":"code","59d00f49":"code","a548f39f":"code","2c32e3d0":"code","2f93e4ef":"code","294930bc":"code","0cbd9faa":"code","3330d4fc":"code","5bc10950":"code","15bc0ab2":"code","ae1c8f59":"code","d448174f":"code","72ea9686":"code","d30d78e7":"code","2f6d85bc":"code","a7f988cf":"code","4eafd104":"code","d0e55b84":"markdown","5b7fd938":"markdown","d0c110a6":"markdown","65da5213":"markdown","b72f9775":"markdown","652e9c28":"markdown","e254f2ae":"markdown","6e6db7ad":"markdown","9ef376ff":"markdown","22528aec":"markdown","915c56c7":"markdown","3758a79a":"markdown","a235c648":"markdown","739e950b":"markdown","1ebe0e06":"markdown","8ac5a355":"markdown","ee08ec4e":"markdown","787c246a":"markdown","bfd3ad7a":"markdown","16ee87fc":"markdown","ede82e0b":"markdown","24ad49d5":"markdown","51b9295a":"markdown","d27099b7":"markdown"},"source":{"ad4909ad":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\n\nimport gc\nimport random\nimport transformers\nimport warnings\n\nimport tensorflow.keras.backend as K\n\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, fbeta_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import AutoTokenizer, TFAutoModel\n\n# print(f\"TensorFlow version: {tf.__version__}\")\n# print(f\"Transformers version: {transformers.__version__}\")\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","8f60a886":"def print_cm(cm, labels, counts=True, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n    # https:\/\/gist.github.com\/zachguo\/10296432\n    \"\"\"pretty print for confusion matrixes\"\"\"\n    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n    empty_cell = \" \" * columnwidth\n    \n    fst_empty_cell = (columnwidth-3)\/\/2 * \" \" + \"T\\P\" + (columnwidth-3)\/\/2 * \" \"\n    \n    if len(fst_empty_cell) < len(empty_cell):\n        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n    # Print header\n    print(\"    \" + fst_empty_cell, end=\" \")\n    \n    for label in labels:\n        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n        \n    print()\n    # Print rows\n    for i, label1 in enumerate(labels):\n        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n        for j in range(len(labels)):\n            if counts:\n                cell = \"%{0}.1d\".format(columnwidth) % cm[i, j]\n            else:\n                cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n            if hide_zeroes:\n                cell = cell if float(cm[i, j]) != 0 else empty_cell\n            if hide_diagonal:\n                cell = cell if i != j else empty_cell\n            if hide_threshold:\n                cell = cell if cm[i, j] > hide_threshold else empty_cell\n            print(cell, end=\" \")\n        print()","3d2b8a84":"fake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ntrue = true.assign(fake = 0)\nfake = fake.assign(fake = 1)\nfull = pd.concat([true,fake],axis=0)","f304724d":"true.head()","87a174ab":"fake.head()","5a72523a":"true.text.str.contains(\"\\(Reuters\\)\").mean(), fake.text.str.contains(\"\\(Reuters\\)\").mean()","e978fd5a":"non_reuters_tag = ~full.text.str.contains(\"\\(Reuters\\)\")\nlabels = ['True','Fake']\nprint_cm(confusion_matrix(full['fake'],non_reuters_tag),labels=labels,counts=True)","342241c2":"accuracy_score(full['fake'],non_reuters_tag)","338bad7c":"def drop_prefix(text,prefix='(Reuters)',n=5):\n    ts = str.split(text,' ')\n    if prefix in ts[:n]:\n        return str.split(text,prefix)[-1]\n    else:\n        return text","bc2e8400":"full = full.assign(text = full.text.apply(lambda x: drop_prefix(x,'(Reuters)')))\nfull = full.assign(text = full.text.str.strip(' -'))","4dab7de0":"true.subject.value_counts()","885f71fc":"fake.subject.value_counts()","35543709":"true.date.value_counts().sort_index()","6ef456a0":"fake.date.value_counts().sort_index().tail(15)","86b38814":"tct = pd.to_datetime(true.date,errors='coerce').value_counts().sort_index()\nfct = pd.to_datetime(fake.date,errors='coerce').value_counts().sort_index()\n\nplt.plot(tct.index,tct.values,label='True',alpha=0.4)\nplt.plot(fct.index,fct.values,'red',label='Fake',alpha=0.4)\nplt.xticks(rotation=45)\nplt.ylabel(\"Document count\")\nplt.legend(loc='upper left')\nplt.title(\"Documents published per day\")","59d00f49":"full.shape[0] - full.drop_duplicates().shape[0]","a548f39f":"true = true.drop(columns=['date','subject'])\nfake = fake.drop(columns=['date','subject'])\nfull = full.drop(columns=['date','subject'])","2c32e3d0":"full.shape[0], full.shape[0] - full.drop_duplicates().shape[0], 44898\/5","2f93e4ef":"(full.shape[0] - full.drop_duplicates().shape[0])\/full.drop_duplicates().shape[0]","294930bc":"def first_n_words_of_text(df,n=2):\n    word_list_series = df.text.str.strip(' -').str.split(' ').apply(lambda x: x[:n]).astype(str)\n    return word_list_series","0cbd9faa":"first_2_words = first_n_words_of_text(full)\nfirst_2_words.value_counts().head(20)","3330d4fc":"def prefix_classifier_performance(df,prefix):\n    # Return the fraction of documents in each class which begin with prefix\n    n = len(prefix)\n    return df.groupby('fake')['text'].agg(lambda x: x.apply(lambda y: y[:n] == prefix).mean())","5bc10950":"prefix_classifier_performance(full, 'Donald Trump')","15bc0ab2":"prefix_classifier_performance(full, '21st Century Wire')","ae1c8f59":"full = full.assign(text = full.text.apply(lambda x: drop_prefix(x,'21st Century Wire')))","d448174f":"full = full.assign(ncw = full.title.str.split(' ').apply(lambda x: np.sum([y==str.upper(y) for y in x])))\n\nplt.hist(full.loc[full.fake.astype(bool),'ncw'],color='red',label='fake',alpha=0.4)\nplt.hist(full.loc[~full.fake.astype(bool),'ncw'],color='blue',label='true',alpha=0.4)\nplt.xlabel(\"Number of fully capitalized words in title\")\n_=plt.legend()","72ea9686":"# Average class label by number of capitalized words in title.\nfull.groupby('ncw')['fake'].mean()","d30d78e7":"yhat_capital_classifier = full.ncw.ge(2)\n(yhat_capital_classifier == full['fake']).mean()","2f6d85bc":"## defining configuration\n\nclass Configuration():\n    \"\"\"\n    All configuration for running an experiment\n    \"\"\"\n    def __init__(\n        self,\n        model_name,\n        train,\n        test,\n        max_length = 64,\n        padding = True,\n        batch_size = 128,\n        epochs = 5,\n        learning_rate = 1e-5,\n        metrics = [\"binary_accuracy\"],\n        verbose = 1,\n        train_splits = 4,\n        accelerator = \"TPU\",\n        target_col = 'fake',\n        seed = 13\n    ):\n        # seed and accelerator\n        self.SEED = seed\n        self.ACCELERATOR = accelerator\n\n        self.TRAIN = train\n        self.TEST = test\n        self.TARGET_COL = target_col\n        \n        # splits\n        self.TRAIN_SPLITS = train_splits\n        \n        # model configuration\n        self.MODEL_NAME = model_name\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n\n        # model hyperparameters\n        self.MAX_LENGTH = max_length\n        self.PAD_TO_MAX_LENGTH = padding\n        self.BATCH_SIZE = batch_size\n        self.EPOCHS = epochs\n        self.LEARNING_RATE = learning_rate\n        self.METRICS = metrics\n        self.VERBOSE = verbose\n        \n        # initializing accelerator\n        self.initialize_accelerator()\n\n    def initialize_accelerator(self):\n        \"\"\"\n        Initializing accelerator\n        \"\"\"\n        # checking TPU first\n        if self.ACCELERATOR == \"TPU\":\n            print(\"Connecting to TPU\")\n            try:\n                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n                print(f\"Running on TPU {tpu.master()}\")\n            except ValueError:\n                print(\"Could not connect to TPU\")\n                tpu = None\n\n            if tpu:\n                try:\n                    print(\"Initializing TPU\")\n                    tf.config.experimental_connect_to_cluster(tpu)\n                    tf.tpu.experimental.initialize_tpu_system(tpu)\n                    self.strategy = tf.distribute.experimental.TPUStrategy(tpu)\n                    self.tpu = tpu\n                    print(\"TPU initialized\")\n                except _:\n                    print(\"Failed to initialize TPU\")\n            else:\n                print(\"Unable to initialize TPU\")\n                self.ACCELERATOR = \"GPU\"\n\n        # default for CPU and GPU\n        if self.ACCELERATOR != \"TPU\":\n            print(\"Using default strategy for CPU and single GPU\")\n            self.strategy = tf.distribute.get_strategy()\n\n        # checking GPUs\n        if self.ACCELERATOR == \"GPU\":\n            print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n\n        # defining replicas\n        self.AUTO = tf.data.experimental.AUTOTUNE\n        self.REPLICAS = self.strategy.num_replicas_in_sync\n        print(f\"REPLICAS: {self.REPLICAS}\")\n        \n        \n        \ndef encode_text(df, tokenizer, max_len, padding):\n    \"\"\"\n    Preprocessing textual data into encoded tokens.\n    \"\"\"\n    text = df[[\"title\", \"text\"]].values.tolist()\n\n    # encoding text using tokenizer of the model\n    text_encoded = tokenizer.batch_encode_plus(\n        text,\n        pad_to_max_length = padding,\n        max_length = max_len\n    )\n\n    return text_encoded\n\n\ndef get_tf_dataset(X, y, auto, labelled = True, repeat = False, shuffle = False, batch_size = 128):\n    \"\"\"\n    Creating tf.data.Dataset for TPU.\n    \"\"\"\n    if labelled:\n        ds = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\n    else:\n        ds = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(2048)\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(auto)\n\n    return ds\n\n\n\n## building model\ndef build_model(model_name, max_len, learning_rate, metrics):\n    \"\"\"\n    Building the Deep Learning architecture\n    \"\"\"\n    # defining encoded inputs\n    input_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\n    \n    # defining transformer model embeddings\n    transformer_model = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings = transformer_model(input_ids)[0]\n\n    # defining output layer\n    output_values = Dense(1, activation = \"sigmoid\")(transformer_embeddings[:, 0, :])\n\n    # defining model\n    model = Model(inputs = input_ids, outputs = output_values)\n    opt = Adam(learning_rate = learning_rate)\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n    metrics = metrics\n\n    model.compile(optimizer = opt, loss = loss, metrics = metrics)\n\n    return model\n\ndef run_model(config):\n    \"\"\"\n    Running the model\n    \"\"\"\n    ## reading data\n    df_train = config.TRAIN\n    df_test = config.TEST\n    \n    # stratified K-fold on language and label\n    skf = KFold(n_splits = config.TRAIN_SPLITS, shuffle = True, random_state = config.SEED)\n\n    # initializing predictions\n    preds_oof = np.zeros((df_train.shape[0], 1))\n    preds_test = np.zeros((df_test.shape[0], 1))\n    acc_oof = []\n\n    # iterating over folds\n    for (fold, (train_index, valid_index)) in enumerate(skf.split(df_train)):\n        # initializing TPU\n        if config.ACCELERATOR == \"TPU\":\n            if config.tpu:\n                config.initialize_accelerator()\n\n        # building model\n        K.clear_session()\n        with config.strategy.scope():\n            model = build_model(config.MODEL_NAME, config.MAX_LENGTH, config.LEARNING_RATE, config.METRICS)\n            if fold == 0:\n                print(model.summary())\n\n        print(\"\\n\")\n        print(\"#\" * 19)\n        print(f\"##### Fold: {fold + 1} #####\")\n        print(\"#\" * 19)\n\n        # splitting data into training and validation\n        X_train = df_train.iloc[train_index]\n        X_valid = df_train.iloc[valid_index]\n\n        y_train = X_train[config.TARGET_COL].values\n        y_valid = X_valid[config.TARGET_COL].values\n        \n        print(\"\\nTokenizing\")\n\n        # encoding text data using tokenizer\n        X_train_encoded = encode_text(df = X_train, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n        X_valid_encoded = encode_text(df = X_valid, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # creating TF Dataset\n        ds_train = get_tf_dataset(X_train_encoded, y_train, config.AUTO, repeat = True, shuffle = True, batch_size = config.BATCH_SIZE * config.REPLICAS)\n        ds_valid = get_tf_dataset(X_valid_encoded, y_valid, config.AUTO, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        n_train = X_train.shape[0]\n\n        if fold == 0:\n            X_test_encoded = encode_text(df = df_test, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # saving model at best accuracy epoch\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            \"model.h5\",\n            monitor = \"binary_accuracy\",\n            verbose = 0,\n            save_best_only = True,\n            save_weights_only = True,\n            mode = \"max\",\n            save_freq = \"epoch\"\n        )\n\n        print(\"\\nTraining\")\n\n        # training model\n        model_history = model.fit(\n            ds_train,\n            epochs = config.EPOCHS,\n            callbacks = [sv],\n            steps_per_epoch = n_train \/ config.BATCH_SIZE \/\/ config.REPLICAS,\n            validation_data = ds_valid,\n            verbose = config.VERBOSE\n        )\n\n        print(\"\\nValidating\")\n\n        # scoring validation data\n        model.load_weights(\"model.h5\")\n        ds_valid = get_tf_dataset(X_valid_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        preds_valid = model.predict(ds_valid, verbose = config.VERBOSE)\n        acc = accuracy_score(y_valid, np.argmax(preds_valid, axis = 1))\n\n        preds_oof[valid_index] = preds_valid\n        acc_oof.append(acc)\n\n        print(\"\\nInferencing\")\n\n        # scoring test data\n        ds_test = get_tf_dataset(X_test_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n        preds_test += model.predict(ds_test, verbose = config.VERBOSE) \/ config.TRAIN_SPLITS\n\n        print(f\"\\nFold {fold + 1} Accuracy: {round(acc, 4)}\\n\")\n\n        g = gc.collect()\n\n    # overall CV score and standard deviation\n    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\n    print(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\\n\")\n\n    return preds_oof, preds_test","a7f988cf":"TEST_FRAC = 0.2\n\ntrain, test = train_test_split(full,random_state=32094,test_size=TEST_FRAC)","4eafd104":"# # Model: Bert Base Cased\n# config_1 = Configuration(\"bert-base-cased\", train=train, test=test, max_length = 32, batch_size = 8, epochs = 5, train_splits = 4)\n# preds_train_1, preds_test_1 = run_model(config_1)\n\n# # Model: Bert Base Uncased\n# config_2 = Configuration(\"bert-base-uncased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n# preds_train_2, preds_test_2 = run_model(config_2)\n\n# Model: Bert Large Cased\n#config_3 = Configuration(\"bert-large-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_3, preds_test_3 = run_model(config_3)\n\n# Model: Bert Large Uncased\n#config_4 = Configuration(\"bert-large-uncased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_4, preds_test_4 = run_model(config_4)\n\n# Model: Bert Multilingual Base Cased\n#config_5 = Configuration(\"bert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_5, preds_test_5 = run_model(config_5)\n\n# Model: Distilbert Base Cased\n#config_6 = Configuration(\"distilbert-base-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_6, preds_test_6 = run_model(config_6)\n\n# Model: Distilbert Base Uncased\n#config_7 = Configuration(\"distilbert-base-uncased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_7, preds_test_7 = run_model(config_7)\n\n# Model: Distilbert Multilingual Base Cased\n#config_8 = Configuration(\"distilbert-base-multilingual-cased\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_8, preds_test_8 = run_model(config_8)\n\n# Model: Roberta Base\n#config_9 = Configuration(\"roberta-base\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_9, preds_test_9 = run_model(config_9)\n\n# Model: Roberta Large\n#config_10 = Configuration(\"roberta-large\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_10, preds_test_10 = run_model(config_10)\n\n# Model: XLM Roberta Base\n#config_11 = Configuration(\"jplu\/tf-xlm-roberta-base\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_11, preds_test_11 = run_model(config_11)\n\n# Model: XLM Roberta Large\n#config_12 = Configuration(\"jplu\/tf-xlm-roberta-large\", translation = False, max_length = 32, batch_size = 32, epochs = 2, train_splits = 4)\n#preds_train_12, preds_test_12 = run_model(config_12)","d0e55b84":"<a id=\"information-leak-2\"><\/a>\n# Information leak 2: Subject column","5b7fd938":"* The date column format completely determines whether the item is real or fake. \n* The fake news date column sometimes contains URLs instead (which sound like fake news sites)!","d0c110a6":"<a id=\"intro\"><\/a>\n# *What is information\/data leak?*\n\n* <h4> Information leak is when training and\/or validation data contains information about the target which would not realistically be available at the time of prediction. It is a common form of \"cheating\" in data science (often accidental). <\/h4>\n* <h4> For example, if a variable has a perfect correlation of 1 with the target, you probably have an information leak.  I found a lot of this in the data!<\/h4>","65da5213":"<a id=\"cost\"><\/a>\n# Scoping: Cost of an error\n\nIn order to guide further analysis, **we should consider what we might want to use this model for**. Here are a few possible use cases:\n\n1. An automated black box fake news filter, which censors fake news.\n2. A model to guide construction of a (rule-based) policy for censoring fake news (similar to 1, but interpretable).\n3. A tool which ranks news stories by their likelihood of being fake for review by human censors.\n\n**We also should consider *who* may be using the model** in order to effectively guide its design and evaluation. Let's assume that the user is a large social media platform acting as a news aggregator. On a given social media platform, the formula determining how many people see a given story is proprietary. One could imagine using collaborative filtering, for example, to \"recommend\" posts to users. \n\nMetrics to use in evaluation of our fake news detector would depend on the use case and user because the errors have different costs.\n\nLet $TP$, $TN$, $FP$, and $FN$ denote the number of true positives, true negatives, false positives, and false negatives, respectively.\n\n### False negatives\n\nIn a false negative, a fake news story will slip by and people may read and believe it. The cost of a single false negative may be minimal. However, the average cost of a large number of false negatives may be high. \n\n<!-- Let the function $C_{FN}(n)$ denote the cost of $n$ false negatives. If a prediction is made every time a story is shared, then the cost of a false negative is independent of the users and post involved. However, if that's the case,  -->\n\n<!-- given false negative rate varies a lot by user, but not much by use case. Weibo or Twitter failing to detect a fake news story may carry a lesser cost than, say, SmartNews or NBC. -->\n\nIn production false negatives may indicate <b>adversarial examples<\/b>, i.e. entities constructed to try to fool the model. Since there was no model at the time the data were created, there are no adversarial examples in our data.\n\n### False positives\nAn erroneously censored news story (false positive) may\n   * lead to backlash from the author\/publisher and their readers who trust them, resulting in distrust of the filter and their administrator \/ developer (me!!  :O).\n   * censor facts which may be important for a wide array of different reasons\n\n### Empirical cost\n\nIdeally, we would like to assign \"costs\" \n\\begin{align*}\nC_{TP}, C_{TN}\\leq 0 \\\\ C_{FP}, C_{FN}\\geq 0\n\\end{align*}\n\nto each mode of classification and choose a model to minimize the *empirical cost*\n\n$$C_{TP} \\times TP + C_{TN} \\times TN + C_{FP}\\times FP + C_{FN} \\times FN$$\n\n### Empirical cost vs. no model\n\n* In absence of a model, every case is let by; by comparison, the benefit from a true negative is 0. Let us therefore choose $C_{TN}=0$ and adopt the convention that costs will be considered relative to the absence of any filter.\n* The benefit conferred from detecting a fake news story is likely the opposite the cost of accidentally letting that story through. Therefore $C_{TP} = - C_{FN}$.\n\nSo our total cost reduces therefore to\n\n$$ C_{FP} \\times FP + C_{FN} \\times (FN - TP) $$\n\nThere are many possible relationships between $C_{FP}$ and $C_{FN}$ in our three use cases, and depending on the user.\n\nWe will examine the cost surface for some models, as well as\n* the precision-recall curve\n* $F_{\\beta}$ score for some different values of $\\beta$\n* ROC","b72f9775":"<a id=\"information-leak-1\"><\/a>\n## Information leak 1: \"(Reuters)\" in text\n\nReuters is one of four news organizations at the vanguard of journalistic integrity, together with the Associated Press, Agence France-Presse, and Agencia EFE. Read more here: https:\/\/en.wikipedia.org\/wiki\/Journalistic_objectivity ","652e9c28":"<a id=\"information-leak-5\"><\/a>\n# Information leak 5: duplicates\n\n* If two duplicates make their way into different folds, the same example will be in the training and the validation\/test set, so the label will be leaked to the training set. \n\n* Several duplicate articles were published (same title, text, and label, different dates).\n\n* In production, it may happen that a previously seen article is predicted upon. For example, if predictions are made every time an article is shared, then an article whose label is known to the algorithm may be reposted and evaluated by the fake news detector. **Therefore in this case the appropriate method of evaluation is to use a time-series split to sequester the train\/valid\/test sets or CV folds by date.** In other words, the training set should contain examples preceding some date $d_0$, the validation set examples between $d_0$ and another, later date $d_1$, and the test set examples dated after $d_1$.","e254f2ae":"* For an $n$-fold CV, the probability that they are all in the same fold is the probability that a hypergeometric draw with population $44898$, $5795$ successes in the popluation, and a sample of size $44898\/n$ has all $5795$ successes in the sample. \n* This probability is basically $0$ for all values of $n$. \n* Therefore two duplicates will almost certainly be put in different CV folds.","6e6db7ad":"These don't have the Reuters tag.","9ef376ff":"<a id=\"imports\"><\/a>\n## Imports and utilities","22528aec":"<a id=\"tf-framework\"><\/a>\n## Tensorflow framework\n\nTHANK YOU immensely to @rohanrao for the very convenient and well-constructed framework I have adapted for use below. I'm just putting it here so more people can use it.\n\nhttps:\/\/www.kaggle.com\/rohanrao\/tpu-sherlocked-one-stop-for-with-tf","915c56c7":"<a id=\"cheaty-model\"><\/a>\n## Cheaty model: Text contains (Reuters)\n\nThe simple rule-based model which predicts `True` in presence of the string `(Reuters)` has <b>99.6% accuracy.<b>","3758a79a":"## Setup\n* [Imports and utilities](#imports)\n* [Load data](#load)\n\n## Data cleaning: Information leak\n* [Introduction: What is information leak?](#intro)\n* [Information leak 1: \"(Reuters)\" in text](#information-leak-1)\n    - [Cheaty model: Text contains (Reuters)](#cheaty-model)\n* [Information leak 2: Subject column](#information-leak-2)\n* [Information leak 3: Date format, URLs in date column](#information-leak-3)\n* [Information leak 4: Dates](#information-leak-4)\n* [Information leak 5: Duplicates](#information-leak-5)\n* [Other dead giveaways and interesting features](#dead-giveaways)\n    - [First few words of text](#first-few-words)\n    - [Number of all-caps words in title](#all-caps)\n\n## Prediction\n* [Scoping: Cost of an error](#cost)\n* [Tensorflow framework](#tf-framework)\n* [Run 4-fold CV](#cv)","a235c648":"<a id=\"information-leak-3\"><\/a>\n# Information leak 3: URLs in date column","739e950b":"* Only 170 (0.38%) of true news documents do not contain the text `(Reuters)`, and only 9 (0.02%) fake news documents do contain the text `(Reuters)`. \n\n* If we accept the data as-is, we have an excellent model which is useless in practice: an attacker can easily discern the rule and inject the token `(Reuters)` into their text to fool the model.\n\n#### If `(Reuters)` occurs in the first 5 words of the text, drop it and everything preceding it.","1ebe0e06":"# Data issues\n\n* <h3>Problems with the data explain why everyone's accuracy on Kaggle is so high (>99%).<\/h3>\n* <h3>Using the subject column alone or the formatting of the date column, we easily get 100% accuracy. <\/h3>\n* <h3>Using presence of the string `(Reuters)` gets us 99.6% accuracy.<\/h3>","8ac5a355":"<a id=\"information-leak-4\"><\/a>\n# Information leak 4: Dates\n\n* Date is irrelevant if our goal is to detect fake news in real time. \n* Today's date will probably not tell us any information about whether a news story published today is real or fake. ","ee08ec4e":"Notice anything? The text of these all contain a `(Reuters)` tag and preceding location.","787c246a":"<a id=\"dead-giveaways\"><\/a>\n# Other features and dead giveaways\n\n<a id=\"first-few-words\"><\/a>\n### First few words of text","bfd3ad7a":"* The subject completely determines whether the news is fake or not. \n* Obviously in a real-world situation we couldn't hope for such absolutely classifying information at prediction time.","16ee87fc":"* I'd like to drop `21st Century Wire` along with `(Reuters)`, as it too appears to be a header for the article text, and is perfectly correlated with the target. ","ede82e0b":"## Load data","24ad49d5":"* So if the document was published before 2016, predict fake. Clearly, this does not represent the state of fake news in the real world.","51b9295a":"<a id='cv'><\/a>\n## 4-fold CV","d27099b7":"<a id=\"all-caps\"><\/a>\n### Number of fully capitalized words in title"}}