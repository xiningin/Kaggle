{"cell_type":{"3ad8ad63":"code","6ba80cdf":"code","29a04a2b":"code","62d50d0a":"code","644d402f":"code","9237d2ab":"code","b19189de":"code","0ef4d7c7":"code","0ddbedbb":"code","87ce237c":"code","c7a3cae5":"code","c0435f36":"code","86cef474":"code","6b2bba74":"code","e2890e5d":"code","35b34d18":"code","6f7f6af4":"code","0bb0b856":"code","982cc9e3":"code","bb135889":"code","48465f87":"code","9b0ecf5a":"code","9d89aed5":"code","5bd46ceb":"code","03493b5a":"code","70a322c5":"code","cc0b3444":"code","9ee59a34":"markdown","5c382ddd":"markdown","fd5ab702":"markdown","1c72a7ea":"markdown","2e7eaf6f":"markdown","ca039d7d":"markdown","ed9587e3":"markdown","c72ca546":"markdown","5a14cfd8":"markdown","c691f678":"markdown","b354e400":"markdown"},"source":{"3ad8ad63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ba80cdf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","29a04a2b":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head()","62d50d0a":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","644d402f":"plt.figure(figsize=(8,4))\nsns.heatmap(train_data.isna(), cmap='viridis', yticklabels=False, cbar=False) # heatmap for showing missing values","9237d2ab":"sns.countplot(x='Survived',data=train_data, palette= 'Set1')\n# 1 -> passenger survived\n# 0 -> passenger deceased","b19189de":"sns.countplot(x='Survived', hue='Sex', data=train_data, palette='Set1')\n\n# from this plot, it's visible that many males were survied and many female died","0ef4d7c7":"sns.countplot(x='Survived', hue='Pclass', data=train_data, palette= 'Set1')\n# here we observe that passengers in class 3 were mostly survived as compared to other classes in Titanic","0ddbedbb":"sns.displot(train_data['Age'].dropna(), bins=30)","87ce237c":"sns.displot(train_data['Fare'], bins=40)","c7a3cae5":"sns.boxplot(x='Pclass', y='Age', data=train_data)","c0435f36":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isna(Age):\n        if Pclass == 1:     # from the boxplot we are taking meadian values from the Pclass\n            return 37\n        if Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","86cef474":"train_data['Age'] = train_data[['Age','Pclass']].apply(impute_age, axis=1)\ntest_data['Age'] = test_data[['Age','Pclass']].apply(impute_age, axis=1)","6b2bba74":"train_data.drop('Cabin',axis=1, inplace= True) # this feature is categorical andf doesn't help as feature for predictions\ntest_data.drop('Cabin',axis=1, inplace= True)","e2890e5d":"sns.heatmap(train_data.isna(),cmap='viridis', yticklabels=False, cbar=False)\n# me made sure that there are no missing values present in the Age column","35b34d18":"sex= pd.get_dummies(train_data['Sex'], drop_first=True)\nembark = pd.get_dummies(train_data['Embarked'], drop_first=True)\n\ntrain_data = pd.concat([train_data, sex, embark], axis=1)\ntrain_data.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace= True)\ntrain_data.head()","6f7f6af4":"sex= pd.get_dummies(test_data['Sex'], drop_first=True)\nembark = pd.get_dummies(test_data['Embarked'], drop_first=True)\n\ntest_data = pd.concat([test_data, sex, embark], axis=1)\ntest_data.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace= True)\ntest_data.head()","0bb0b856":"mean = test_data['Fare'].mean()\ntest_data['Fare'].fillna(mean, inplace= True)","982cc9e3":"test_data.isna().sum()","bb135889":"X_train = train_data.drop(['Survived','PassengerId'],axis=1)\nX_test = test_data.drop('PassengerId', axis=1)","48465f87":"y_train = train_data['Survived']","9b0ecf5a":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)  # initializing the model","9d89aed5":"model.fit(X_train, y_train)","5bd46ceb":"prediction = model.predict(X_test)","03493b5a":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nada = AdaBoostClassifier()\n\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100, 500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=ada, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","70a322c5":"import xgboost as xgb\ngbm = xgb.XGBClassifier(learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n nthread= -1,\n scale_pos_weight=1).fit(X_train, y_train)\nxgb_predictions = gbm.predict(X_test)","cc0b3444":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': xgb_predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9ee59a34":"<h3><b> The number of people survived and deceased <\/b> <\/h3>","5c382ddd":"* We see that wealthier people of class 1, class 2 are tend to be old as compared to the class 3 people\n* Which indicates that older people are tend to be rich and chose either of Class 1 & 2\n* Whereas younger people chose class 3","fd5ab702":"<h2><b><u>Handling Missing Values <\/u> <\/b> <\/h2>","1c72a7ea":"<h3><b> Age Distribution of the passengers in titanic <\/b> <\/h3>","2e7eaf6f":"<h2> Loading the data <\/h2>","ca039d7d":"<h2><b><u>Exploratory Data Analysis <\/u><\/b>","ed9587e3":"<h2><b><u>Creating dummy variable of categorical variables <\/u> <\/b> <\/h2>","c72ca546":"**<h3>Check whether there are any missing values present in the data** <\/h3>","5a14cfd8":"* From the above heatmap we can see that there are missing values present in the feature Age, Cabin","c691f678":"<h2><b><u> Training and Prediciting the model <\/u> <\/b> <\/h2>","b354e400":"<h3><b> Distribution of Fare amount in Titanic <\/b> <\/h3>"}}