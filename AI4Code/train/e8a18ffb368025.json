{"cell_type":{"e4e578fa":"code","4b26cce4":"code","3fa5321a":"code","3a418369":"code","4c484b3d":"code","833c0a44":"code","483659ba":"code","449d17c1":"code","04407e5f":"code","ac9c962d":"code","f96fc5ed":"code","b45d8fe4":"code","58aeb9e4":"code","d4e95356":"code","4bc0894b":"code","8a91cefa":"code","3f8e106c":"code","730dc252":"code","8729c7e0":"code","78b569b9":"code","bd549346":"code","2e359a14":"code","e7502f16":"code","f748e90f":"code","ea1d8712":"code","30cc92c0":"code","5735b341":"code","ba1b90ef":"code","680214a9":"code","480d4dfd":"code","d4ef8212":"code","98da695c":"markdown","90a47102":"markdown","a92a64e8":"markdown"},"source":{"e4e578fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b26cce4":"import matplotlib.pyplot as plt\nimport seaborn as sns","3fa5321a":"data = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","3a418369":"data.head()","4c484b3d":"data.isna().sum()\n# No nans, it is great","833c0a44":"sns.pairplot(data)","483659ba":"data.columns","449d17c1":"drop_cols = ['integration_type_id', 'revenue_share_percent']","04407e5f":"# Let's generate target value CPM\ndata['cpm'] = np.where(data.measurable_impressions > 0, data.total_revenue * 100 \/ data.measurable_impressions * 1000, 0)","ac9c962d":"data.head()","f96fc5ed":"# Let's drop target and useless cols\ndrop_cols += ['measurable_impressions', 'total_revenue']\ndata.drop(columns=drop_cols, inplace=True)","b45d8fe4":"data.head()","58aeb9e4":"# Continue Data exploring\ndata.info()","d4e95356":"# Mark all data that have less than 255 unique values as categorical\nCAT_THRESHOLD = 255\ncat_features = set()\nfor col in data:\n    if data[col].nunique() <= CAT_THRESHOLD:\n        cat_features.add(col)\nprint(cat_features)","4bc0894b":"# Let's drop date from cat values\ncat_features.remove('date')","8a91cefa":"# Transform date to datetime and get some features\ndata['date'] = pd.to_datetime(data['date'])\ndata['day'] = data['date'].dt.day\ndata['month'] = data['date'].dt.month\ndata['dayofweek'] = data['date'].dt.dayofweek\ndata['dayofyear'] = data['date'].dt.dayofyear","3f8e106c":"# add some datetime features as categorical\ncat_features |= {'dayofweek', 'month', 'day'}","730dc252":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom scipy import sparse\n\nimport datetime","8729c7e0":"cpm_threshold = data['cpm'].quantile(0.95)\ndata_raw = data.copy()\ndata = data.query('cpm < @cpm_threshold')","78b569b9":"date_th = pd.to_datetime('2019-06-21')\ntrain = data.query('date <= @date_th')\ntest = data.query('date > @date_th')","bd549346":"y_train = train.pop('cpm')\ny_test = test.pop('cpm')","2e359a14":"ohe = OneHotEncoder(handle_unknown='ignore')\nX_cat_train = ohe.fit_transform(train[cat_features])\nX_cat_test = ohe.transform(test[cat_features], )","e7502f16":"# Let's take a look on the rest features\nnp.setdiff1d(train.columns.values, list(cat_features))","f748e90f":"useful_features = list(np.setdiff1d(train.columns.values, list(cat_features)))\nuseful_features.remove('date')","ea1d8712":"useful_features","30cc92c0":"X_int_train = train[useful_features]\nX_int_test = test[useful_features]","5735b341":"X_train = sparse.hstack([X_cat_train, X_int_train])\nX_test = sparse.hstack([X_cat_test, X_int_test])","ba1b90ef":"xgb_model = XGBRegressor(\n#     predictor='gpu_predictor',\n    objective='reg:squarederror',\n#     cu\n    n_estimators=850, # best estimators num\n    verbosity=1,\n    reg_alpha=0.23,\n    reg_lambda=0.1,\n    n_jobs=6,\n    max_depth=9,\n    eta=0.25,\n    colsample_bytree=0.7\n)","680214a9":"xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])","480d4dfd":"# Calc train loss\ny_pred_train = xgb_model.predict(X_train)\nprint(f'Train MSE: {mean_squared_error(y_train, y_pred_train)}')","d4ef8212":"# Calc train loss\ny_pred_test = xgb_model.predict(X_test)\nprint(f'Test MSE: {mean_squared_error(y_test, y_pred_test)}')","98da695c":"### Data exploration","90a47102":"### Data Preparation","a92a64e8":"`integration_type_id` and `revenue_share_percent` - has one unique value, we should drop it\n"}}