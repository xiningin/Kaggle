{"cell_type":{"070746f9":"code","1baaf9a7":"code","52df109d":"code","6845aefd":"code","d3e480ef":"code","58ecc157":"code","a1a4dcab":"code","636e3215":"code","8767a872":"code","8caa5ace":"code","7a132941":"code","563a4817":"code","9dbf9210":"code","da988ce2":"code","784c13d6":"code","e9b97ee3":"code","0504402e":"code","8fe93dfd":"code","5f390425":"code","6540ab21":"code","6c451afe":"code","cebb998e":"code","d3810a88":"markdown","8b7855ef":"markdown","c602ba6f":"markdown","18a17329":"markdown","0948e675":"markdown","25c4523f":"markdown","e22c7a2a":"markdown","ea3ab0f9":"markdown","07aa67cd":"markdown","dede9a77":"markdown","6ed5c0db":"markdown","a6d02aa3":"markdown","dcb37add":"markdown","59d68f3e":"markdown","6d5208ac":"markdown","27501128":"markdown","41c9fdc3":"markdown"},"source":{"070746f9":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1baaf9a7":"# read in all our data\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.set_index(\"Id\")\n\nprint(\"The dataset has {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]))\ndisplay(df_train.dtypes.value_counts())\ndf_train.head(3)\n","52df109d":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test.set_index(\"Id\")\n\nprint(\"The dataset has {} rows and {} columns\".format(df_test.shape[0], df_test.shape[1]))\ndisplay(df_test.dtypes.value_counts())\ndf_test.head(3)","6845aefd":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(16, 14))\nsns.heatmap(corrmat, vmax=.8, square=True);","d3e480ef":"#saleprice correlation \nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","58ecc157":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], height = 2.5)\nplt.show();","a1a4dcab":"# get the number of missing data points per column\n# missing_values_count_train = df_train.isnull().sum()\nmissing_values_count_train = df_train.isnull().sum().sort_values(ascending=False)\npercent_train = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data_train = pd.concat([missing_values_count_train, percent_train], axis=1, keys=['Total', 'Percent'])\n\n\n\n# how many total missing values do we have?\ntotal_cells_train = np.product(df_train.shape)\ntotal_missing_train = missing_values_count_train.sum()\n\n# percent of data that is missing\npercent_missing_train = (total_missing_train*100)\/total_cells_train\nprint('Total missing data in Train dataset {:.2f}%'.format(percent_missing_train))\n\nprint(\"Missing data of the total of each column:\")\nmissing_data_train.head(20)","636e3215":"# get the number of missing data points per column\n\nmissing_values_count_test = df_test.isnull().sum().sort_values(ascending=False)\npercent_test = (df_test.isnull().sum()\/df_test.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([missing_values_count_test, percent_test], axis=1, keys=['Total', 'Percent'])\n\n# how many total missing values do we have?\ntotal_cells_test = np.product(df_test.shape)\ntotal_missing_test = missing_values_count_test.sum()\n\n# percent of data that is missing\npercent_missing_test = (total_missing_test*100)\/total_cells_test\nprint('Total missing data in Train dataset {:.2f}%'.format(percent_missing_test))\n\nprint(\"Missing data of the total of each column:\")\nmissing_data_test.head(33)","8767a872":"df_train_columns_w_mising_values =  df_train[df_train.columns[df_train.isnull().any()]]\ndf_train_columns_w_mising_values","8caa5ace":"df_test_columns_w_mising_values =  df_test[df_test.columns[df_test.isnull().any()]]\ndf_test_columns_w_mising_values","7a132941":"#dealing with missing data\ndf_train = df_train.drop((missing_data_train[missing_data_train['Total'] > 1]).index,1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max() #just checking that there's no missing data missing...","563a4817":"df_train['GrLivArea'] = np.log(df_train['GrLivArea'])","9dbf9210":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1","da988ce2":"#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])","784c13d6":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","e9b97ee3":"#convert categorical variable into dummy\ndf_train['SalePrice']","0504402e":"x_train = df_train.drop(['SalePrice'], axis=1)\ny_train = df_train['SalePrice']\n\nx_test = df_test\n# x_test\n","8fe93dfd":"features = pd.concat([x_train, x_test]).reset_index(drop=True)\nfeatures","5f390425":"df_train['SalePrice'].describe()","6540ab21":"# Selecting only data with numbers\ndata_corr = df_train.select_dtypes(include=[np.number])\ndata_corr.head()","6c451afe":"corr = data_corr.corr().style.background_gradient(cmap='coolwarm')\ncorr\n\n# corr.sort_values(['SalePrice'], ascending=False, inplace=True)\n# pd.DataFrame(corr['SalePrice'])","cebb998e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(20, 12))\nsns.heatmap(features.isnull())\nplt.show()","d3810a88":"# Correlation between salesprice and the columns with highest correlation heat\nHere I display the most correlated columns to have a better view and make conclusions.","8b7855ef":"# Concatinating X_train and X_test","c602ba6f":"# Spliting the Train and Test Data","18a17329":"# Data transformation","0948e675":"# Relationshipt between SalePrice and other numerical columns\n## Correlation \n\nMaking a correlation with seaborn and mathplotlib to see how each column is correlated to target variable (SalePrice)","25c4523f":"## Creating a ataframe with the columns with missing values and the values\n## Train Dataset","e22c7a2a":"## Test Dataset","ea3ab0f9":"# Remove all columns with at least one missing value (I don't want to do this)\n\nAfter careful consideration, of all the columns with missin data only **TotalBsmtSF, GarageCars and BsmtFullBath** have missing vaues in the test dataset.\n\nSo far, we used the train data set for our analisis since this data is the one we will use to train our model.\n\nAll the columns with missing data including columns with missing data that are one of the most correlated to saleprice in the testing dataset, will be removed.\n\n**Note**: the 'Electrical' columns has only one missing cell, so we just delete that one in particular.","07aa67cd":"# Data Standarization","dede9a77":"* **OverallQual** = Rates the overall material and finish of the house\n* **GrLivArea** = Above grade (ground) living area square feet\n* **TotalBsmtSF** = Total square feet of basement area\n* **GarageCars** = Size of garage in car capacity\n* **GarrageArea** = Size of garage in square feet\n* **1stFlrSF** = First Floor square feet\n* **FullBath** = Full bathrooms above grade\n* **TotRmsAbvGrd** = Total rooms above grade (does not include bathrooms)\n* **YearBuilt** = Original construction date","6ed5c0db":"# Scatter plots between 'SalePrice' and correlated variables ","a6d02aa3":"Moust correlated variables description:\nAccording to the dataset description:\n\n* **OverallQual** = Rates the overall material and finish of the house\n* **GrLivArea** = Above grade (ground) living area square feet\n* **TotalBsmtSF** = Total square feet of basement area\n* **GarageCars** = Size of garage in car capacity\n* **GarrageArea** = Size of garage in square feet\n* **TotalBsmtSF** = Total square feet of basement area\n* **1stFlrSF** = First Floor square feet\n* **FullBath** = Full bathrooms above grade\n* **TotRmsAbvGrd** = Total rooms above grade (does not include bathrooms)\n* **YearBuilt** = Original construction date\n\nHaving the variables with the most correlation with Saleprice we can make the following conlusions:\n* The first 3 variables (**OverallQual**, **GrLivArea**, **TotalBsmtSF**) are the most correlated to salePrice \u2705\n* **GarageCars** and **GarrageArea** are basically the same, de amount of cars will vary on the total size are of the garage, so we can select one of these columns for and drop the other one.\n* **TotalBsmtSF** and **1stFlrSF** also looks very similar, we will keep **TotalBsmtSF**\n* **YearBuilt** it is wierd how the correlation shows the contruction year not so correlated to the salePrice, some reasons could be the fact that olds hourses might be renovated or located in better areas.","dcb37add":"# Loading the Datasets\n## Train","59d68f3e":"## Test","6d5208ac":"# Feature Engineering on Train Dataset\n## Train Dataset\n\n### How many missing data we have on each dataset","27501128":"# Correlation with the target variable","41c9fdc3":"## Test Dataset"}}