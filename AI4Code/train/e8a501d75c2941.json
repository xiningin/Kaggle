{"cell_type":{"b56a7591":"code","e7eb194e":"code","420d1b54":"code","e34ebb38":"code","e9e68463":"code","fd84be87":"code","83a523a9":"code","ca56a7fb":"code","2c3a64d7":"code","67c7287e":"code","601cbfbf":"code","e59e0a45":"code","468c4a2b":"code","a1805e89":"code","05ad5e45":"code","3e532c9a":"code","dcb14062":"code","76c449a3":"code","bea3c214":"code","f63e98b7":"code","45336142":"code","3c971cbf":"code","29c6c5cb":"code","19040c86":"code","b4796749":"code","7cada72c":"code","7c0ea17e":"markdown","1b73419b":"markdown","f1eddd14":"markdown","9e9a7e3d":"markdown","ab4a4dec":"markdown","8435c4b2":"markdown","cb61cf18":"markdown","70ffcb88":"markdown","3695fe7b":"markdown"},"source":{"b56a7591":"# Importing the libraries \nimport pandas as pd\nfrom pandas import read_csv\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error","e7eb194e":"import os\nprint(os.listdir(\"..\/input\"))","420d1b54":"filename = (\"..\/input\/boston-house-prices\/housing.csv\")\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndataset = read_csv(filename, delim_whitespace=True, names=names)","e34ebb38":"dataset.shape","e9e68463":"dataset.describe()","fd84be87":"from scipy import stats\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataset.items():\n    sns.boxplot(y=k, data=dataset, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","83a523a9":"for k, v in dataset.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(dataset)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n    \n    ","ca56a7fb":"corr = dataset.corr()\ncorr.shape","2c3a64d7":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')","67c7287e":"prices = dataset['MEDV']\nfeatures = dataset.drop('MEDV', axis = 1)","601cbfbf":"plt.figure(figsize=(20, 5))\n\n# i: index\nfor i, col in enumerate(features.columns):\n    # 3 plots here hence 1, 3\n    plt.subplot(1, 6, i+1)\n    x = dataset[col]\n    y = prices\n    plt.plot(x, y, 'o')\n    # Create regression line\n    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('prices')","e59e0a45":"from scipy import stats\n#histogram and normal probability plot\nsns.distplot(dataset['MEDV'], hist=True);\nfig = plt.figure()\nres = stats.probplot(dataset['MEDV'], plot=plt)","468c4a2b":"from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    \n    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n    score = r2_score(y_true, y_predict)\n    \n    # Return the score\n    return score","a1805e89":"from sklearn.model_selection import train_test_split\narray = dataset.values\nX = array[:,0:6]\nY = array[:,6]\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.20, random_state=7)","05ad5e45":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","3e532c9a":"# Test options and evaluation metric using Root Mean Square error method\nRMS = 'neg_mean_squared_error'\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state = 7, shuffle = True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","dcb14062":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","76c449a3":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n        kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","bea3c214":"fig = plt.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","f63e98b7":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=10, random_state=7, shuffle = True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\ndataset.shape","45336142":"ensembles = []\nensembles.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n        kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\ndataset.shape","3c971cbf":"fig = plt.figure()\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","29c6c5cb":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=np.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=7)\nkfold = KFold(n_splits=10, random_state=7, shuffle = True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","19040c86":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=7, n_estimators=400)\nmodel.fit(rescaledX, Y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))\n","b4796749":"print('R^2:',metrics.r2_score(Y_validation, predictions))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(Y_validation, predictions))*(len(Y_validation)-1)\/(len(Y_validation)-X_validation.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(Y_validation, predictions))\nprint('MSE:',metrics.mean_squared_error(Y_validation, predictions))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(Y_validation, predictions)))\n","7cada72c":"predictions=predictions.astype(int)\nsubmission = pd.DataFrame({\n        \"Org House Price\": Y_validation,\n        \"Pred House Price\": predictions\n    })\n\nsubmission.to_csv(\"PredictedPrice.csv\", index=False)","7c0ea17e":"We can see that GBM has better accuracy than KNN","1b73419b":"Ensembles","f1eddd14":"It shows 'peakedness', positive skewness and does not follow the diagonal line. A simple data transformation can solve the problem. Will do in by standardizing the data\n","9e9a7e3d":"### Make Prediction on Validation Dataset","ab4a4dec":"KNN algorithm Tuning","8435c4b2":"Tuning scaled GBM ","cb61cf18":"From my analysis, Price increases with RM and Price decreases with increase in PTRATO and LSTAT","70ffcb88":"Standardize the dataset","3695fe7b":"I observed that INDUS, RM, TAX, PTRATIO and LSTAT shows some good correaltion with MEDV and I am interested to know more about them.\nHowever I noticed that INDUS shows good correlation with TAX and LSAT which is a pain point for us :(\n\nbecause it leads to Multicollinearity. "}}