{"cell_type":{"c7fa29e0":"code","b37d1e17":"code","8e7430ab":"code","e401f6c6":"code","0d67aed2":"code","3125f8a2":"code","0da4f9c6":"code","89ec7e6b":"code","3cd02b37":"code","9b7aa683":"code","9831e7e2":"code","e36fe535":"code","5a267ebb":"code","7b930364":"code","31dc1084":"code","dd53372c":"code","71be99f4":"markdown","774b2e21":"markdown","9aacf6b0":"markdown","0f22cb1d":"markdown","d6e6da74":"markdown","2e4d2f8c":"markdown","5e326c3e":"markdown","a4f15632":"markdown","74d0cb63":"markdown","6297c325":"markdown"},"source":{"c7fa29e0":"import numpy as np \nimport pandas as pd ","b37d1e17":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt","8e7430ab":"import re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nimport emoji\nimport string\n\ndef preprocess_data(data, remove_stop = True):\n    \n    data = re.sub('https?:\/\/\\S+|www\\.\\S+', '', data)\n    data = re.sub('<.*?>', '', data)\n    emoj = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    emoj.sub(r'', data)\n    data = data.lower()\n    data = data.translate(str.maketrans('','', string.punctuation))\n    data = re.sub(r'\\[.*?\\]', '', data)\n    data = re.sub(r'\\w*\\d\\w*','', data)\n    \n    words= data.split()\n    \n    if(remove_stop):\n        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n        words = [w for w in words if len(w) > 2]  # remove a,an,of etc.\n    \n    words= ' '.join(words)\n    \n    return words","e401f6c6":"from nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import RegexpTokenizer\n\n\ndef tokenizer(words):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    words= tokenizer.tokenize(words)\n    \n    return words\n\ndef lemmatize(words):\n    lemmatizer = WordNetLemmatizer() \n    lem= []\n    for w in words:\n        lem.append(lemmatizer.lemmatize(w))\n    return lem\n\ndef stemming(words):\n    ps = PorterStemmer() \n    stem= []\n    for w in words:\n        stem.append(ps.stem(w))\n    return stem  ","0d67aed2":"stemming(tokenizer(preprocess_data(\"@water #dream hi 19 :) hello where are you going be there tomorrow happening\")))","3125f8a2":"import spacy\n\nnlp = spacy.load('en_core_web_lg')\n\ntext = \"\"\"London is the capital and most populous city of England and \nthe United Kingdom.  Standing on the River Thames in the south east \nof the island of Great Britain, London has been a major settlement \nfor two millennia. It was founded by the Romans, who named it Londinium.\nThe City of Westminster is also an Inner London borough holding city status.\nLondon is governed by the mayor of London and the London Assembly.\nLondon has a diverse range of people and cultures, and more than 300 languages are spoken in the region.\n\"\"\"\ndoc = nlp(text)\n","0da4f9c6":"from spacy import displacy\n\nfor entity in doc.ents:\n    print(f\"{entity.text} ({entity.label_})\")\n\ndisplacy.render(doc, style=\"ent\") #this needs to be closed","89ec7e6b":"i=0\nfor token in doc:\n    if i<10:\n        print(token.text, token.pos_)\n    i+=1","3cd02b37":"doc1 = nlp(\"Gandhiji was born in Porbandar in 1869.\")\n\ndisplacy.render(doc1, style=\"dep\")\n\n#sentence_spans = list(doc.sents) # To show large text dependency parsing, uncomment this.\n","9b7aa683":"def worldcloud(word_list):\n    #wordcloud = WordCloud()\n    #wordcloud.fit_words(dict(count(word_list).most_common(40)))\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                \n                min_font_size = 10).generate(word_list)\n\n    fig=plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","9831e7e2":"worldcloud(' '.join([token.text for token in doc if token.pos_ in ['NOUN']]))","e36fe535":"import gensim\n\nword2vec_path = \"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","5a267ebb":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n   \n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged","7b930364":"tokens_list= stemming(tokenizer(preprocess_data(\"@water #dream hi 19 :) hello where are you going be there tomorrow happening\")))","31dc1084":"tokens_list","dd53372c":"get_average_word2vec(tokens_list, word2vec) #This will give an average of all of the token","71be99f4":"### Pre Processing data\n#### The following function ***preprocess_data*** covers all the preprocessing steps like removing any link in the data, emoji, hashtag, @ and stop words. It returns a sentence devoid of all the non-processable data. ","774b2e21":"### POS Tagging\n#### Categorizing each token according its appropriate Part of Speech could help in better understanding of data and can help in drawing accurate relations with other tokens. This process is called POS Tagging. You can see each tag's meaning [here](https:\/\/universaldependencies.org\/docs\/u\/pos\/).","9aacf6b0":"### In this notebook, I've tried to cover the very basic steps that are required during data preprocessing and have elaborated the NLP pipeline to be used while approaching for any Natural Language problem. \n### Here is a pipeline shown that should be followed for any NLP problem and once you're thorough with the concepts of these underrated topics then you must proceed for trying BERT and other Transformers based models.","0f22cb1d":"### Using Spacy\n#### Spacy provides simple ways to perform complex operations like Dependency parsing, POS tagging, NER, Coreference resolution etc. on data. Here I'll demonstrate its usage via an example.","d6e6da74":"#### Tokenizer function splits the sentence into tokens which can then be used for vector representation of words.\n#### Lemmatizer reduces the word to its root form. For eg: Corpora : Corpus\n#### Stemming merely deletes the suffix of the root word. For eg: Programer : Program , Programing : Program. Stemming could be sometimes inaccurate while deduction.","2e4d2f8c":"![image.png](attachment:image.png)\nCredits: [Adam Geitgey](https:\/\/medium.com\/@ageitgey)","5e326c3e":"### Vectorization\n#### We will be using Word2vec pretrained model to get the vector for our text. We can also use spacy's vectorizer but just wanted to demonstrate word2vec.\n\n#### An average of all the token embeddings would be really helpful while doing further solution.","a4f15632":"### Word Cloud\n#### Word cloud can be used to see how frequently a particular category of words appear in our text. We will test it for seeing unique nouns as an example.","74d0cb63":"### Named Entity Recognition (NER)\n#### Each entity is given a category tag viz Person, Organization, Brand etc according to the spacy's dictionary. NER can easily enable machine to distinguish between \"James\" and \"James Square\". You can find out the meaning of each annotation on spacy website [here](https:\/\/spacy.io\/api\/annotation#named-entities).","6297c325":"### Dependency Parsing diagram\nI won't be covering Dependency Parsing topic in detail as it requires a separate notebook since it is an enormous topic in itself."}}