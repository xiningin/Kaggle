{"cell_type":{"9c3b68fc":"code","938c32d7":"code","b16b56f5":"code","3dc940ca":"code","42315cf0":"code","930b61de":"code","f4b697f1":"code","94cb5c22":"code","68b002eb":"code","64c589c1":"code","97f85c9b":"code","85877507":"code","1d0d170a":"code","6d79a404":"code","f3fc2351":"code","334e5ae2":"code","05570f91":"code","deb43055":"code","ef56c5ad":"code","944c9a5c":"code","15f4ca33":"code","99b401d7":"code","14b3cd46":"code","126c3149":"code","7dddbbd7":"code","2322d7db":"code","720c7980":"code","e0cdeda2":"code","cc933677":"code","c6973ac9":"code","7c172970":"code","8dbd524a":"code","564f7a2d":"code","a125cc44":"code","889006c1":"code","d3e3564c":"code","953970a6":"code","816a9093":"code","ab1b7834":"code","c55e3eda":"code","abbcf104":"code","cdccda20":"code","2bbce743":"code","7a51c1f3":"code","d2b1b304":"code","e45816c9":"code","0771a2ac":"code","32f20a18":"code","ec0fb9ca":"code","d8a0e259":"code","cf1ca00e":"code","23b931c5":"code","4ddfaa93":"code","5baf9ebd":"code","25cb68dd":"code","c596ef98":"code","f0a5a003":"code","1ccbe453":"code","f409da37":"markdown","4c5413b3":"markdown","c0196b89":"markdown","e6d20196":"markdown","5c128fe6":"markdown","461f6fdb":"markdown","8d7a1652":"markdown","ab3168e1":"markdown","4a3e45f7":"markdown","064b0c25":"markdown","e0bf508d":"markdown","4c2c68d6":"markdown","1bf42a3f":"markdown","cd98547d":"markdown","320944d1":"markdown"},"source":{"9c3b68fc":"# Python 3 environment defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nfrom datetime import datetime\nfrom time import time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\nimport warnings\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.utils.mem import *","938c32d7":"from fastai.utils.show_install import show_install; show_install()","b16b56f5":"!nvidia-smi","3dc940ca":"def fmt_now():\n    return datetime.today().strftime('%Y%m%d-%H%M%S')","42315cf0":"path = Path(\"\/kaggle\/input\/understanding_cloud_organization\")\npath.ls()","930b61de":"path_img = path\/\"train_images\"\n\nfnames_train = get_image_files(path_img)\nfnames_train[:3]\nprint(len(fnames_train))","f4b697f1":"path_test = path\/\"test_images\"\n\nfnames_test = get_image_files(path_test)\nfnames_test[:3]\nprint(len(fnames_test))","94cb5c22":"img_f = fnames_train[1]\nimg = open_image(img_f)\nimg.show(figsize=(10, 10))","68b002eb":"def split_img_label(img_lbl):\n    \"\"\"Return image and label from file name like '0011165.jpg_Flower'\"\"\"\n    s = img_lbl.split(\"_\")\n    assert len(s) == 2\n    return s[0], s[1]","64c589c1":"train = pd.read_csv(f'{path}\/train.csv')\n\n# split Image_Label\ntrain[\"Image\"] = train[\"Image_Label\"].apply(lambda img_lbl: split_img_label(img_lbl)[0])\ntrain[\"Label\"] = train[\"Image_Label\"].apply(lambda img_lbl: split_img_label(img_lbl)[1])\ndel train[\"Image_Label\"]\n\ntrain.head()","97f85c9b":"train_with_mask = train.dropna(subset=[\"EncodedPixels\"])\nax = train_with_mask[\"Label\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', title=\"Shares of each classes\", figsize=(10, 6))","85877507":"class_counts = train.dropna(subset=[\"EncodedPixels\"]).groupby(\"Image\")[\"Label\"].nunique()\nax = class_counts.plot(kind=\"hist\", title=\"Number of classes per image\")","1d0d170a":"# pivot to have one row per image and masks as columns\ntrain = train.pivot(index='Image', columns='Label', values='EncodedPixels')\nassert len(train) == len(fnames_train) # sanity check\ntrain.head()","6d79a404":"def show_img_fn(fname, figsize=(10, 10)):\n    img = open_image(fname)\n    img.show(figsize=figsize)    ","f3fc2351":"def show_img_info(fname):\n    show_img_fn(path_img\/fname)\n    display(train.loc[[fname]])   ","334e5ae2":"unusual_imgs = [\"1588d4c.jpg\", \"c0306e5.jpg\", \"c26c635.jpg\", \"fa645da.jpg\", \"41f92e5.jpg\", \"e5f2f24.jpg\"]","05570f91":"for fname in unusual_imgs:\n    img = open_image(path_img\/fname)\n    img.show(figsize=(5, 5), title=fname)     ","deb43055":"train_img_dims = (1400, 2100)  # Train and test images are 1400x2100 pixels","ef56c5ad":"def rle_to_mask(rle, shape):\n    mask_img = open_mask_rle(rle, shape)\n    mask = mask_img.px.permute(0, 2, 1)\n    return mask","944c9a5c":"def mask_to_rle(mask):\n    \"\"\"Convert binary `mask` to RLE string\"\"\"\n    return rle_encode(mask.numpy().T)","15f4ca33":"def test_mask_rle():\n    \"\"\"test case for mask RLE encode\/decode\"\"\"\n    mask_rle = train.iloc[0][\"Fish\"]    \n    mask = rle_to_mask(mask_rle, train_img_dims)\n    mask_rle_enc = mask_to_rle(mask)\n    assert mask_rle_enc == mask_rle\n    \n    print(mask.shape)\n    Image(mask).show()\n    \ntest_mask_rle()","99b401d7":"# TODO remove use_partial_data()\nitem_list = (SegmentationItemList.\n             from_df(df=train.reset_index(), path=path_img, cols=\"Image\")\n             .use_partial_data(sample_pct=0.1)  # use only a subset of data to speedup tests\n             .split_by_rand_pct(0.2))","14b3cd46":"class MultiLabelImageSegment(ImageSegment):\n    \"\"\"Store overlapping masks in separate image channels\"\"\"\n\n    def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,\n        cmap:str='tab20', alpha:float=0.5, class_names=None, **kwargs):\n        \"Show the masks on `ax`.\"\n             \n        # put all masks into a single channel\n        flat_masks = self.px[0:1, :, :].clone()\n        for idx in range(1, self.shape[0]): # shape CxHxW\n            mask = self.px[idx:idx+1, :, :] # slice tensor to a single mask channel\n            # use powers of two for class codes to keep them distinguishable after sum \n            flat_masks += mask * 2**idx\n        \n        # use same color normalization in image and legend\n        norm = matplotlib.colors.Normalize(vmin=0, vmax=2**self.shape[0]-1)\n        ax = show_image(Image(flat_masks), ax=ax, hide_axis=hide_axis, cmap=cmap, norm=norm,\n                        figsize=figsize, interpolation='nearest', alpha=alpha, **kwargs)\n        \n        # custom legend, see https:\/\/matplotlib.org\/3.1.1\/gallery\/text_labels_and_annotations\/custom_legends.html\n        cm = matplotlib.cm.get_cmap(cmap)\n        legend_elements = []\n        for idx in range(self.shape[0]):\n            c = 2**idx\n            label = class_names[idx] if class_names is not None else f\"class {idx}\"\n            line = Line2D([0], [0], color=cm(norm(c)), label=label, lw=4)\n            legend_elements.append(line)\n        ax.legend(handles=legend_elements)\n        \n        # debug info\n        # ax.text(10, 10, f\"px={self.px.size()}\", {\"color\": \"white\"})\n        \n        if title: ax.set_title(title)\n\n    def reconstruct(self, t:Tensor): \n        return MultiClassImageSegment(t)","126c3149":"# source: https:\/\/forums.fast.ai\/t\/unet-how-to-get-4-channel-output\/54674\/4\ndef bce_logits_floatify(input, target, reduction='mean'):\n    return F.binary_cross_entropy_with_logits(input, target.float(), reduction=reduction)","7dddbbd7":"class MultiLabelSegmentationLabelList(SegmentationLabelList):\n    \"\"\"Return a single image segment with all classes\"\"\"\n    # adapted from https:\/\/forums.fast.ai\/t\/how-to-load-multiple-classes-of-rle-strings-from-csv-severstal-steel-competition\/51445\/2\n    \n    def __init__(self, items:Iterator, src_img_size=None, classes:Collection=None, **kwargs):\n        super().__init__(items=items, classes=classes, **kwargs)\n        self.loss_func = bce_logits_floatify\n        self.src_img_size = src_img_size\n        # add attributes to copy by new() \n        self.copy_new += [\"src_img_size\"]\n    \n    def open(self, rles):        \n        # load mask at full resolution\n        masks = torch.zeros((len(self.classes), *self.src_img_size)) # shape CxHxW\n        for i, rle in enumerate(rles):\n            if isinstance(rle, str):  # filter out NaNs\n                masks[i] = rle_to_mask(rle, self.src_img_size)\n        return MultiLabelImageSegment(masks)\n    \n    def analyze_pred(self, pred, thresh:float=0.0):\n        # binarize masks\n        return (pred > thresh).float()\n    \n    def reconstruct(self, t:Tensor): \n        return MultiLabelImageSegment(t)","2322d7db":"class_names = [\"Fish\", \"Flower\", \"Gravel\", \"Sugar\"]","720c7980":"def get_masks_rle(img):\n    \"\"\"Get RLE-encoded masks for this image\"\"\"\n    img = img.split(\"\/\")[-1]  # get filename only\n    return train.loc[img, class_names].to_list()","e0cdeda2":"# reduce image size\n# img_size = tuple(v \/\/ 16 for v in train_img_dims)\nimg_size = (84, 132)  # use multiple of 4\nimg_size","cc933677":"classes = [0, 1, 2, 3] # no need for a \"void\" class: if a pixel isn't in any mask, it is not labelled\nitem_list = item_list.label_from_func(func=get_masks_rle, label_cls=MultiLabelSegmentationLabelList, \n                                      classes=classes, src_img_size=train_img_dims)","c6973ac9":"# add unlabelled test images\n# set empty RLE string as label to produce empty multi-label masks and allow reconstruct() and show()\nitem_list = item_list.add_test_folder(path_test, label=\"\")","7c172970":"batch_size = 8\n\n# TODO add data augmentation\ntfms = ([], [])\n# tfms = get_transforms()\n\nitem_list = item_list.transform(tfms, tfm_y=True, size=img_size)","8dbd524a":"data = (item_list\n        .databunch(bs=batch_size)\n        .normalize(imagenet_stats) # use same stats as pretrained model\n       )  \nassert data.test_ds is not None","564f7a2d":"data.show_batch(2, figsize=(15, 10), class_names=class_names)","a125cc44":"# adapted from: https:\/\/www.kaggle.com\/iafoss\/unet34-dice-0-87\n# can use sigmoid on the input too, in this case the threshold would be 0.5\ndef dice_metric(pred, targs, threshold=0):\n    pred = (pred > threshold).float()\n    targs = targs.float()  # make sure target is float too\n    return 2.0 * (pred*targs).sum() \/ ((pred+targs).sum() + 1.0)","889006c1":"metrics = [dice_metric]\n\ncallback_fns = [\n    # update a graph of learner stats and metrics after each epoch\n    ShowGraph,\n\n    # save model at every metric improvement\n    partial(SaveModelCallback, every='improvement', monitor='dice_metric', name=f\"{fmt_now()}_unet_resnet18_stage1_best\"),\n    \n    # stop training if metric no longer improve\n    partial(EarlyStoppingCallback, monitor='dice_metric', min_delta=0.01, patience=2),\n]\n\nlearn = unet_learner(data, models.resnet18, metrics=metrics, wd=1e-2, callback_fns=callback_fns)\nlearn.model_dir = \"\/kaggle\/working\/\"  # point to writable directory","d3e3564c":"learn.loss_func","953970a6":"learn.summary()","816a9093":"learn.lr_find()\nlearn.recorder.plot()","ab1b7834":"defaults.device = torch.device('cuda')\nlearn.fit_one_cycle(5, max_lr=1e-4)","c55e3eda":"learn.recorder.plot_metrics()","abbcf104":"learn.save(f\"{fmt_now()}_unet_resnet18_stage1\", return_path=True)","cdccda20":"!ls -lth {learn.model_dir}","2bbce743":"# learn = learn.load(Path(learn.model_dir)\/\"20190924-095959_unet_resnet18_stage1_best\")","7a51c1f3":"learn.unfreeze()","d2b1b304":"learn.lr_find()\nlearn.recorder.plot()","e45816c9":"# slice(start,end) syntax: the first group's learning rate is start, the last is end, and the remaining are evenly geometrically spaced\nlearn.fit_one_cycle(15, max_lr=slice(1e-6, 1e-5))","0771a2ac":"learn.save(f\"{fmt_now()}_unet_resnet18_stage2\", return_path=True)","32f20a18":"learn.show_results(imgsize=8, class_names=class_names)","ec0fb9ca":"preds, _ = learn.get_preds(ds_type=DatasetType.Test, with_loss=False)","d8a0e259":"preds.shape","cf1ca00e":"learn.show_results(ds_type=DatasetType.Test, imgsize=8, class_names=class_names)","23b931c5":"for i in range(3):\n    pimg = MultiLabelImageSegment(preds[i] > 0)\n    pimg.show(figsize=(6, 6), class_names=class_names)   ","4ddfaa93":"def resize_pred_masks(preds, shape=(4, 350, 525)):\n    \"\"\"Resize predicted masks and return them as a generator\"\"\"\n    for p in range(preds.shape[0]):\n        mask = MultiLabelImageSegment(preds[p])\n        yield mask.resize(shape)","5baf9ebd":"pred_masks = resize_pred_masks(preds)","25cb68dd":"test_fnames = [p.name for p in data.test_dl.items]\nlen(test_fnames)","c596ef98":"def write_submission_file(filename, test_fnames, preds, threshold=0):\n    with open(filename, mode='w') as f:\n        f.write(\"Image_Label,EncodedPixels\\n\")\n\n        for img_name, masks in zip(tqdm(test_fnames), resize_pred_masks(preds)):\n            binary_masks = masks.px > threshold # TODO use activation instead\n            \n            for class_idx, class_name in enumerate(class_names):\n                rle = mask_to_rle(binary_masks[class_idx])\n                f.write(f\"{img_name}_{class_name},{rle}\\n\")\n\n    print(f\"Wrote '{f.name}'.\")","f0a5a003":"submission_file = f\"{fmt_now()}_submission.csv\"","1ccbe453":"write_submission_file(submission_file, test_fnames, preds)","f409da37":"# Initialization","4c5413b3":"## Code utils","c0196b89":"### Write submission file","e6d20196":"## EDA","5c128fe6":"## Train","461f6fdb":"### Unfreeze and differential learning rate","8d7a1652":"From [evaluation](https:\/\/www.kaggle.com\/c\/understanding_cloud_organization\/overview\/evaluation):\n> start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\n> The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.","ab3168e1":"## Load images","4a3e45f7":"### Broken images\n\nImages which look incorrect.","064b0c25":"### Get predictions for test images","e0bf508d":"As per competition rules, predicted masks must have 1\/4 of original size for submission","4c2c68d6":"### Convert masks from\/to RLE","1bf42a3f":"### Resize predictions to submission size","cd98547d":"# Segmentation using fast.ai","320944d1":"## Predictions"}}