{"cell_type":{"43806892":"code","f6fdbce5":"code","659b930f":"code","dfcf14c1":"code","b11ad712":"code","ba37da37":"code","afdf3000":"code","dd318599":"code","4e2a3234":"code","dd18f80e":"code","fd47b0a8":"code","83dbae9c":"code","0afaf40c":"code","40b2eea0":"code","55c820d9":"code","d9811ef3":"code","62f0f681":"code","5b267776":"markdown","d0f5d8ac":"markdown","4d67c133":"markdown","8a12634a":"markdown","3a879e95":"markdown","3f97b1ee":"markdown","1bc82ab8":"markdown","761663e2":"markdown","d76dfa11":"markdown","353d3f42":"markdown"},"source":{"43806892":"!pip install ..\/input\/mtcnn-package\/mtcnn-0.1.0-py3-none-any.whl","f6fdbce5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom time import time\nimport os\nfrom mtcnn import MTCNN\nfrom tqdm import tqdm_notebook\nimport time\nimport gc\nimport random","659b930f":"train_dir = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/'\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\ntest_dir = '\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/'\ntest_video_files = [test_dir + x for x in os.listdir(test_dir)]","dfcf14c1":"import pandas as pd\ndf_train = pd.read_json('\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/metadata.json').transpose()\ndf_train=dict(df_train)['label']\ndf_train.head()","b11ad712":"LABELS = ['FAKE','REAL']","ba37da37":"detector = MTCNN()","afdf3000":"THRESHOLD=0.7\ndef detect_face(img,ratio):\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    final = []\n    detected_faces_raw = detector.detect_faces(img)\n    if detected_faces_raw==[]:\n        print('no faces found')\n        return []\n    for x in detected_faces_raw:\n        if x['confidence']<THRESHOLD:\n            continue\n        x,y,w,h=x['box']\n        #x,y,w,h=ratio*x,ratio*y,ratio*w,ratio*h\n        final.append([x,y,w,h])\n    return final","dd318599":"RESIZING_RATIO=2\nFACES_TAKE = 1\ndef detect_video(video):\n    face_frames=[]\n    cap = cv2.VideoCapture(video)\n    cap.set(cv2.CAP_PROP_FPS, 24)\n    ret,frame = cap.read()\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    t=tqdm_notebook(total=total)\n    while True:\n        t.update()\n        #cap.set(cv2.CAP_PROP_POS_MSEC,(count*1000))   \n        ret,frame = cap.read()\n        if ret!=True:\n            break\n        frame=cv2.resize(frame,(int(1920\/RESIZING_RATIO),int(1024\/RESIZING_RATIO)))\n        faces = detect_face(frame,RESIZING_RATIO)\n        if faces==[]:\n            face_frames.append([])\n            continue\n        if FACES_TAKE==1:\n            x,y,w,h=faces[0]\n            face_frames.append(frame[y:h+y,x:w+x])\n            continue\n        face_frames.append([])\n        count=0\n        for (x,y,w,h) in faces:\n            count+=1\n            croped_face = frame[y:h+y,x:w+x]\n            face_frames[-1].append(croped_face)\n            if count==FACES_TAKE:\n                break\n    return face_frames","4e2a3234":"def video(video):\n    cap = cv2.VideoCapture(video)\n    cap.set(cv2.CAP_PROP_FPS, 24)\n    ret,frame = cap.read()\n    final=[]\n    while True:\n        if ret!=True:\n            break\n        ret,frame = cap.read()\n        final.append(frame)\n    return final","dd18f80e":"def random_choice(face_frames,num):\n    return [random.choice(face_frames) for _ in range(num)]","fd47b0a8":"def show(face_frames,num_faces):\n    if FACES_TAKE==1:\n        for x in random_choice(face_frames,num_faces):\n            try:\n                if type(x)!=np.ndarray:\n                    x=np.zeros(face_frames[0].shape)\n                else:\n                    x=cv2.cvtColor(x,cv2.COLOR_BGR2RGB)\n                plt.imshow(x)\n                plt.show()\n            except:\n                pass\n    else:\n        pass #currently working on it","83dbae9c":"face_frames=detect_video(train_video_files[0])","0afaf40c":"show(face_frames,5)","40b2eea0":"gc.collect()","55c820d9":"def pipeline(video_files):\n    X_train=[]\n    y_train=[]\n    for video_file in video_files:\n        X=detect_video(video_file)\n        y=labels.index(df_train[video_files])\n        X_train.append(X)\n        y_train.append(y)\n    return X_train,y_train","d9811ef3":"#pipeline(train_video_files)","62f0f681":"submission = pd.read_csv(\"\/kaggle\/input\/deepfake-detection-challenge\/sample_submission.csv\")\nsubmission['label'] = 0.45\nsubmission.to_csv('submission.csv', index=False)","5b267776":"**<a id=\"5\"><\/a> <br>**\n## 5- Example of Submission","d0f5d8ac":"**<a id=\"2\"><\/a> <br>**\n## 2- Load Data","4d67c133":"**<a id=\"3\"><\/a> <br>**\n## 3- Define Functions","8a12634a":"**<a id=\"4\"><\/a> <br>**\n## 4- Build pipeline","3a879e95":"The variable  **THRESHOLD**  means the confidence of detected face have to be more than the value or else they will be ignored.","3f97b1ee":"It will take too long to execute this so you can try it on your own.","1bc82ab8":"**<a id=\"1\"><\/a> <br>**\n## 1- Import","761663e2":"**Further More**\n1. try to do this on a better local machine in order to preprocess the whole dataset.\n2. try building a model, for example BiLSTM, CNN, LSTM-CNN.","d76dfa11":"The variable  **RESIZING_RATIO**  means the multiplicative inverse of the resizing rate. Making it bigger will make the process faster.\n\nThe variable  **FACE_TAKE**  means how many of the face will be kept.","353d3f42":"This notebook is forked from Marco Vasquez E's wonderful notebook([here](https:\/\/www.kaggle.com\/marcovasquez\/basic-eda-face-detection-split-video-and-roi)). This notebook is a baseline of preprocessing. If you find this helpful, please *upvote* this notebook and the associated dataset."}}