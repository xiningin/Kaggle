{"cell_type":{"74d0541f":"code","cb2c5539":"code","17ef9182":"code","dd9b542c":"code","5fea1e4b":"code","c8d6c5f7":"code","afe42b52":"code","8e58d372":"code","01b94bd2":"code","b08f5124":"code","9e709ec4":"code","b9a18785":"code","9162efd3":"code","fab47435":"code","8bef9bfa":"code","8d63dc01":"code","01b5ed60":"code","193a301b":"code","953817f1":"code","75460662":"code","f690e446":"code","b3248a1a":"code","1405455a":"code","2e909ca9":"code","a60c423c":"code","d869a616":"code","952281ef":"code","338f9e0d":"code","c12c9f3a":"code","50826a0e":"code","d05cf911":"code","2b8a9b62":"code","3016acec":"code","c626e6bb":"code","90bf5610":"code","b8f58203":"code","08c83836":"code","6576eb7a":"code","6f7335a8":"code","e726ace4":"code","de431dc9":"code","69c8c81f":"code","149fcac6":"code","3fbeb098":"code","1916d88a":"code","5590f0e6":"code","73088294":"code","d5faa84f":"code","307f6169":"code","3672873b":"code","5623f542":"code","f84324ec":"code","53e1d044":"code","edbe8001":"code","7ad02031":"code","80a34c18":"code","47986ce2":"code","248109c0":"code","390c49f2":"code","b3f17138":"code","354e60c3":"code","924b1fb7":"code","fe6c03ec":"code","4dfa8808":"code","976029ee":"code","98885695":"code","e0ce7ae9":"code","a276c1ea":"code","09e8911f":"code","1a499fda":"code","efe0f80e":"code","084130b7":"code","bb5c6923":"code","d6a1f544":"code","252f76d3":"code","d40a3048":"code","af2eb7a4":"code","82cdf3c3":"code","41a72755":"code","6b94b2d0":"code","f50bb3a1":"code","b328ffdf":"code","98f0e5b7":"code","abcd0c41":"code","24522089":"code","1c0aaa6b":"code","46a53c12":"code","34a84cd9":"code","fac746bb":"code","16b6b356":"code","3992b5af":"code","7605eed9":"code","0b108577":"code","8cf8e375":"code","525a6d0d":"code","60b046cc":"code","b9cb2f88":"code","5d289fe5":"code","598c5d8d":"code","13568215":"code","6c1cf8ce":"code","c51ffc21":"markdown","0bd6d128":"markdown","46b5ca29":"markdown","d32da460":"markdown","cd0f7678":"markdown","8e2b728a":"markdown","30ffbe9e":"markdown","b18d06ee":"markdown","3e9dd5a5":"markdown","4f618ce4":"markdown","21072a5a":"markdown","53606dd5":"markdown","6871f5c6":"markdown","a86928aa":"markdown","7c5dd057":"markdown","1c4372b8":"markdown","701f287b":"markdown","6959a71a":"markdown","0e5810ff":"markdown","05b993a9":"markdown","cd5dd41f":"markdown","2482b6fb":"markdown","2aa0818f":"markdown","56454745":"markdown","76800e08":"markdown","afae49cc":"markdown","8e0412f6":"markdown","e5d8c576":"markdown","8327128c":"markdown","5706305d":"markdown","893f547a":"markdown","6eb95214":"markdown","fd500573":"markdown","f82489b3":"markdown","2f04c370":"markdown","88ae123b":"markdown","5ba994a6":"markdown","51d375d4":"markdown","6441e1f0":"markdown","003d467a":"markdown","fa30d9d8":"markdown","e789db7c":"markdown","6f17eb77":"markdown","f2137664":"markdown","d8f9a9e2":"markdown","c13b707f":"markdown","93d63072":"markdown","bafd7394":"markdown","30cd1126":"markdown","a1fc4ffc":"markdown","783ab107":"markdown","7583e4ae":"markdown","d1bce5cf":"markdown","65ada8c1":"markdown","7e77835c":"markdown","3a81c636":"markdown","190211e9":"markdown"},"source":{"74d0541f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\npd.options.display.max_rows = 10","cb2c5539":"path_data_folder = '..\/input'\nname_anime_data = r'anime.csv'\nname_rating_data = r'rating.csv'","17ef9182":"df_anime = pd.read_csv(path_data_folder+'\/'+name_anime_data,index_col='anime_id', sep=',')\ndf_rating = pd.read_csv(path_data_folder+'\/'+name_rating_data, sep=',')","dd9b542c":"df_anime.shape","5fea1e4b":"df_anime.dtypes","c8d6c5f7":"df_anime.episodes.value_counts()","afe42b52":"df_anime.loc[df_anime.episodes == 'Unknown', 'episodes'] = -1","8e58d372":"df_anime.episodes = pd.to_numeric(df_anime.episodes)","01b94bd2":"df_anime.dtypes","b08f5124":"df_anime.describe()","9e709ec4":"df_anime.episodes.describe()","b9a18785":"df_anime.episodes.hist()","9162efd3":"df_anime.loc[df_anime.genre.isnull()]","fab47435":"df_anime.genre.fillna('No Genre',inplace=True)","8bef9bfa":"df_anime.count()","8d63dc01":"df_anime.loc[df_anime.type.isna()]","01b5ed60":"df_anime.members.hist()","193a301b":"pd.Series(df_anime.members.describe(),dtype='int64')","953817f1":"df_anime = df_anime.dropna(0, subset=['type'])","75460662":"pd.isnull(df_anime).sum()","f690e446":"df_anime.describe()","b3248a1a":"df_anime.count()","1405455a":"df_anime.loc[pd.isna(df_anime.rating)]","2e909ca9":"df_anime.count()","a60c423c":"nro_total_ratings = df_anime.rating.shape[0]\nnro_total_ratings","d869a616":"df_anime.loc[df_anime.rating.isnull()].shape","952281ef":"nro_nulls = pd.isna(df_anime.rating).sum()\nnro_nulls","338f9e0d":"impr = [\"validos\", \"null\"]\nvol = [nro_total_ratings-nro_nulls, nro_nulls]\nexpl =(0, 0.05)\nplt.pie(vol, explode=expl, labels=impr, autopct='%1.1f%%', shadow=True)\nplt.title(\"Impresi\u00f3n\", bbox={\"facecolor\":\"0.8\", \"pad\":10})\nplt.legend()","c12c9f3a":"df_anime = df_anime.dropna(0, subset=['rating']) # borramos las filas con valores de rating na","50826a0e":"pd.isna(df_anime).sum()","d05cf911":"df_anime.index.is_unique # nos aseguramos que el \u00edndice sea \u00fanico","2b8a9b62":"df_anime.shape","3016acec":"df_anime.count()","c626e6bb":"df_anime[['episodes', 'rating']].describe()","90bf5610":"df_anime['rating'].hist()","b8f58203":"df_anime.loc[df_anime.rating > 9]","08c83836":"df_rating.shape","6576eb7a":"df_rating.count()","6f7335a8":"df_rating.dtypes","e726ace4":"pd.isnull(df_rating).sum()","de431dc9":"df_rating.head()","69c8c81f":"np.round(df_rating[['rating']].describe(), 2)","149fcac6":"df_rating.rating.hist()","3fbeb098":"df_rating[['user_id', 'rating']].groupby(['user_id']).mean().merge(\n    df_rating[['user_id', 'rating']].groupby(\n    ['user_id']).count(),left_on='user_id', right_on='user_id') # veamos el rating promedio y el numero de personas","1916d88a":"df_rating.loc[(df_rating.user_id == 73515)]['rating'].hist()","5590f0e6":"df_rating.loc[(df_rating.user_id == 73515)&(df_rating.rating>=1)]['rating'].hist()","73088294":"df_rating.loc[(df_rating.rating < 0)&(df_rating.rating > -1)|(df_rating.rating > 10)]","d5faa84f":"df_rating","307f6169":"df_rating = df_rating.merge(df_rating[['user_id', 'rating']].loc[df_rating.rating >=1].groupby(['user_id']).mean(), on='user_id',\n               suffixes=('', '_prom'))","3672873b":"df_rating","5623f542":"df_rating['rating'].loc[df_rating.rating == -1] = df_rating['rating_prom']","f84324ec":"df_rating.loc[df_rating.user_id == 73515].rating.value_counts()","53e1d044":"df_rating[['user_id', 'rating']].groupby(['user_id']).mean().hist()","edbe8001":"df_rating['rating'].hist()","7ad02031":"df_inner = df_rating.merge(df_anime.sample(n=100),on='anime_id', how='inner')","80a34c18":"df_inner","47986ce2":"df_inner.shape","248109c0":"pd.isna(df_inner).sum()","390c49f2":"df_inner","b3f17138":"df_inner_sample = df_inner","354e60c3":"df_inner_sample","924b1fb7":"df_inner_sample.shape","fe6c03ec":"df_rating_animes = pd.crosstab(df_inner_sample['user_id'], df_inner_sample['name'], values=df_inner_sample['rating_x'], aggfunc=['mean'])","4dfa8808":"df_rating_animes","976029ee":"df_rating_animes = df_rating_animes.fillna(value=0.0)","98885695":"df_rating_animes","e0ce7ae9":"df_rating_animes.shape","a276c1ea":"scaler = MinMaxScaler(feature_range=[0, 1])\ndata_rescaled = scaler.fit_transform(df_rating_animes)","09e8911f":"data_rescaled","1a499fda":"data_rescaled.shape","efe0f80e":"pca = PCA().fit(data_rescaled)","084130b7":"#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('N\u00famero de Componentes')\nplt.ylabel('Varianza (%)') #for each component\nplt.title('Explained Variance')\nplt.show()","bb5c6923":"np.cumsum(pca.explained_variance_ratio_)[49]","d6a1f544":"pca = PCA(n_components=49)\ndataset = pca.fit_transform(data_rescaled)","252f76d3":"pca_deg = PCA(n_components=3)\ndataset_deg = pca_deg.fit_transform(data_rescaled)","d40a3048":"dataset.shape","af2eb7a4":"X = dataset\nX_deg = dataset_deg\n#y = np.array(df_completo['anime_id'])\nX.shape\n","82cdf3c3":"X_deg.shape","41a72755":"def elbow_curve(X):\n    n_clusters = range(1, 10)\n    kmeans = [KMeans(n_clusters=i) for i in n_clusters]\n    kmeans\n    score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n    score\n    plt.plot(n_clusters,score)\n    plt.xlabel('N\u00famero de cl\u00fasters')\n    plt.ylabel('Score')\n    plt.title('Elbow Curve')\n    plt.show()","6b94b2d0":"elbow_curve(X)","f50bb3a1":"elbow_curve(X_deg)","b328ffdf":"def silhouette_score_sk(X):\n    # EJEMPLO EXTRA\u00cdDO DE SKLEARN POR EL TIPO DE GR\u00c1FICO, cr\u00e9ditos a sklearn.\n    range_n_clusters = [2, 3, 4, 5, 6]\n    for n_clusters in range_n_clusters:\n        # Create a subplot with 1 row and 2 columns\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig.set_size_inches(18, 7)\n\n        # The 1st subplot is the silhouette plot\n        # The silhouette coefficient can range from -1, 1 but in this example all\n        # lie within [-0.1, 1]\n        ax1.set_xlim([-0.1, 1])\n        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n        # plots of individual clusters, to demarcate them clearly.\n        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n        # Initialize the clusterer with n_clusters value and a random generator\n        # seed of 10 for reproducibility.\n        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n        cluster_labels = clusterer.fit_predict(X)\n\n        # The silhouette_score gives the average value for all the samples.\n        # This gives a perspective into the density and separation of the formed\n        # clusters\n        silhouette_avg = silhouette_score(X, cluster_labels)\n        print(\"For n_clusters =\", n_clusters,\n              \"The average silhouette_score is :\", silhouette_avg)\n\n        # Compute the silhouette scores for each sample\n        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n        y_lower = 10\n        for i in range(n_clusters):\n            # Aggregate the silhouette scores for samples belonging to\n            # cluster i, and sort them\n            ith_cluster_silhouette_values = \\\n                sample_silhouette_values[cluster_labels == i]\n\n            ith_cluster_silhouette_values.sort()\n\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n\n            color = cm.nipy_spectral(float(i) \/ n_clusters)\n            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                              0, ith_cluster_silhouette_values,\n                              facecolor=color, edgecolor=color, alpha=0.7)\n\n            # Label the silhouette plots with their cluster numbers at the middle\n            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n            # Compute the new y_lower for next plot\n            y_lower = y_upper + 10  # 10 for the 0 samples\n\n        ax1.set_title(\"The silhouette plot for the various clusters.\")\n        ax1.set_xlabel(\"The silhouette coefficient values\")\n        ax1.set_ylabel(\"Cluster label\")\n\n        # The vertical line for average silhouette score of all the values\n        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n        ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n        # 2nd Plot showing the actual clusters formed\n        colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                    c=colors, edgecolor='k')\n\n        # Labeling the clusters\n        centers = clusterer.cluster_centers_\n        # Draw white circles at cluster centers\n        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                    c=\"white\", alpha=1, s=200, edgecolor='k')\n\n        for i, c in enumerate(centers):\n            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                        s=50, edgecolor='k')\n\n        ax2.set_title(\"The visualization of the clustered data.\")\n        ax2.set_xlabel(\"Feature space for the 1st feature\")\n        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                      \"with n_clusters = %d\" % n_clusters),\n                     fontsize=14, fontweight='bold')\n\n    plt.show()","98f0e5b7":"silhouette_score_sk(X)","abcd0c41":"silhouette_score_sk(X_deg)","24522089":"kmeans = KMeans(n_clusters=3).fit(X)\nkmeans_3comp = KMeans(n_clusters=3).fit(X_deg)","1c0aaa6b":"kmeans.cluster_centers_","46a53c12":"kmeans.cluster_centers_.shape","34a84cd9":"pd.Series(kmeans.predict(X)).value_counts()","fac746bb":"pd.Series(kmeans_3comp.predict(X_deg)).value_counts()","16b6b356":"pd.Series(kmeans.predict(X))","3992b5af":"pd.Series(kmeans_3comp.predict(X_deg))","7605eed9":"df_rating_animes.shape","0b108577":"df_rating_animes['cluster_n_comp'] = kmeans.predict(X)\ndf_rating_animes['cluster_3_comp'] = kmeans_3comp.predict(X_deg)","8cf8e375":"df_rating_animes.groupby(['cluster_n_comp']).mean()","525a6d0d":"df_rating_animes.groupby(['cluster_3_comp']).mean()","60b046cc":"print(df_rating_animes.loc[df_rating_animes.cluster_n_comp == 0].mean().sort_values(ascending=False)[:10].to_string())","b9cb2f88":"print(df_rating_animes.loc[df_rating_animes.cluster_3_comp == 0].mean().sort_values(ascending=False)[:10].to_string())","5d289fe5":"print(df_rating_animes.loc[df_rating_animes.cluster_n_comp == 1].mean().sort_values(ascending=False)[:10].to_string())","598c5d8d":"print(df_rating_animes.loc[df_rating_animes.cluster_3_comp == 1].mean().sort_values(ascending=False)[:10].to_string())","13568215":"print(df_rating_animes.loc[df_rating_animes.cluster_n_comp == 2].mean().sort_values(ascending=False)[:10].to_string())","6c1cf8ce":"print(df_rating_animes.loc[df_rating_animes.cluster_3_comp == 2].mean().sort_values(ascending=False)[:10].to_string())","c51ffc21":"A simple vista se ven cambios muy bruscos al reducir tan dr\u00e1sticamente el n\u00famero de componentes","0bd6d128":"# Join","46b5ca29":"## Carga de datos","d32da460":"## Determinemos el n\u00famero de cl\u00fasters mediante el m\u00e9todo *Elbow Curve*","cd0f7678":"Si bien una gran parte de este dataframe se compone de valores -1(usuario vi\u00f3 pero no evalu\u00f3) estos estan distribuidos por usuario en peque\u00f1as porciones. Tomando en cuenta que el objetivo es encontrar similitudes entre los gustos de los usuarios, se toma le decisi\u00f3n de rellenar con el promedio de sus ratings **sin** considerar los valores -1 para dicho c\u00e1lculo.","8e2b728a":"Teniendo nuestro dataset reescalado y reducido, podemos comenzar con el algoritmo de clustering **K-Means**","30ffbe9e":"Dado que el objetivo es ejecutar el algoritmo **K-Means** con la valoraci\u00f3n que le dio cada usuario a cada anime es que hacemos una tabla cruzada de la siguiente manera.","b18d06ee":"# Sistema de recomendaci\u00f3n de anime","3e9dd5a5":"Para el dataset resultante de PCA con 3 componentes.","4f618ce4":"Dada la gran cantidad de columnas se procede a ejecutar *PCA* con el objetivo de reducir la dimensionalidad de los datos teniendo periodos de entrenamiento m\u00e1s cortos.","21072a5a":"### Caracter\u00edstica de los datos del dataframe *df_rating*","53606dd5":"El cl\u00faster 3 si bien tiene coincidencias entre ambos, los resultados son muy distintos.","6871f5c6":"Como podemos ver, en vez de usar todas las columnas, podemos usar directamente considerablemente menos teniendo una m\u00ednima degradaci\u00f3n del dataset.","a86928aa":"Obtenemos los gr\u00e1ficos con ambos dataset con el fin de comparar.","7c5dd057":"Los episodios tambi\u00e9n deben ser tratados como n\u00fameros enteros. Veamos los posibles valores de esta columna","1c4372b8":"**IMPORTANTE**: Un **m\u00ednimo** de registros tiene valor NaN en *type* y tambi\u00e9n en *episodes* y *rating*.","701f287b":"Como podemos ver, existen valores 'Unknown', para los cuales daremos un valor entero negativo constante. En este caso **-1**. A pesar de ser una minima parte que conforma este grupo con respecto al total, se prefiere no borrar, ni tampoco utilizar media, mediana o moda, puesto que existen valores at\u00edpicos, por lo que se genera un nuevo grupo distintivo.","6959a71a":"### Tratamiento datos columna *rating*","0e5810ff":"### Tratamiento columna genre","05b993a9":"Como podemos ver, en los registros(o rows) con valor NaN en la columna *type* coincide que en todas existe falta de informaci\u00f3n en las columnas *episodes* y *rating*, siendo esta \u00faltima raz\u00f3n para ser eliminada siguiendo la estrategia. Sin embargo compararemos el n\u00famero de miembros de cada uno de estos registros con respecto a los dem\u00e1s.","cd5dd41f":"## Tratamiento de datos para el dataframe *df_anime*","2482b6fb":"Para ejecutar el algoritmo de clustering se har\u00e1 lo mismo, utilizar ambos datasets para comparar los grupos finales.","2aa0818f":"Como podemos ver, sin ning\u00fan problema la columna fue *casteada* a valor entero.\n- NOTA: Casting es convertir el tipo de dato que posee una variable.","56454745":"Los animes correspondientes a cada cl\u00faster son los animes que se le pueden recomendar al usuario perteneciente al cl\u00faster. Adem\u00e1s con estos cl\u00fasters obtenidos se pueden hacer m\u00e1s estudios con respecto a los datos presentes en cada cl\u00faster al relacionarlo con la tabla(o dataframe) *df_anime* tales como:\n   - Ideintificar tipos de anime.\n   - Identificar g\u00e9neros.\n   - Hacer un extenso an\u00e1lisis de los episodios en los animes de los diferentes cl\u00fasters.\n   - Clasificar otros usuarios.","76800e08":"Borramos los valores NaN","afae49cc":"Como podemos ver, lo m\u00e1s probable es que estos anime a\u00fan no sean vistos, por lo que podr\u00edan no ser recomendamos simplemente (dado que no tienen rating) o ser considerados por su g\u00e9nero, puesto que claramente los g\u00e9neros pertenecientes a un anime son una relaci\u00f3n con otros. Sin embargo estos valores NaN en la columna rating sean tratados o no, ser\u00e1n eliminados al ejecutar la operaci\u00f3n **INNER JOIN** con la tabla(o dataframe) rating, puesto que no existen usuarios en la tabla rating que hayan evaluado al anime de la tabla anime.\nPara esta ocasi\u00f3n excepcional, tomo la decisi\u00f3n de no considerar a los valores NaN en rating, dado la m\u00ednima parte que conforman con respecto al total. Sin embargo, si fuesen una gran parte(o mayor), utilizar\u00eda la estrategia de ejecutar una operaci\u00f3n **OUTER JOIN** entre ambas tablas y tratar\u00eda de otra manera los valores NaN en rating.","8e0412f6":"Creo que el hecho de utilizar como valor 0 el no haber visto un anime en la misma variable en donde se indica el gusto(rating) por el anime es incorrecto, puesto que al indicar 0 estamos indicando que el gusto por el anime es p\u00e9simo, esto aunque sea en escala del 1 al 10, ya que el modelo no tiene conocimiento de esta escala que es una convenci\u00f3n humana, por lo que creo que este valor 0 distorciona este modelo basado en distancias.","e5d8c576":"## Caracter\u00edsticas datos del dataframe *df_anime*","8327128c":"# Determinemos el n\u00famero de componentes para PCA","5706305d":"Para este problema se propone el algoritmo de cl\u00fastering **K-Means** dada su extensa aplicaci\u00f3n y su r\u00e1pida ejecuci\u00f3n. Adem\u00e1s se propone antes de utilizar K-Means, reducir la dimensionalidad de los datos con el fin de disminuir los tiempos de entrenamiento.","893f547a":"Los resultados mediante *Elbow Curve* no son muy claros, por lo que se utilizar\u00e1 el m\u00e9todo *Silhouette Score*","6eb95214":"## Tratamiento de datos del dataframe *df_rating*","fd500573":"# Introducci\u00f3n\n\nEsta propuesta se basa en **clustering** dado que se busca encontrar grupos con similitudes por sus gustos, para lo cual es vital el rating para cada anime por cada usuario. El objetivo final es que los usuarios queden agrupados seg\u00fan como valoraron los distintos animes.","f82489b3":"# Falencias del modelo","2f04c370":"En esta problem\u00e1tica de **aprendizaje no supervisado**, se buscar juntar ambas tablas mediante el *anime_id* ejecutando una operaci\u00f3n de **INNER-JOIN**. Luego a partir de esta tabla se busca generar una cross-table obteniendo una tabla de tantas filas como usuarios existan, y tantas columnas como animes se encuentren en los datos, teniendo como dato en cada coordenada *i,j* la valoraci\u00f3n del usuario *i* para el anime *j*.","88ae123b":"### Tratamiento columna rating","5ba994a6":"Una vez tratados los datos de ambos dataframe se ejecuta la operaci\u00f3n **INNER-JOIN** entre ambos con el objetivo de relacionar el anime con la valoraci\u00f3n de cada usuario.","51d375d4":"El cl\u00faster n\u00famero 1 queda muy parecido entre ambos resultados, incluso en el mismo orden","6441e1f0":"Inicialmente borraria los anime sin g\u00e9nero, ya que considero que es demasiado importante para la recomendaci\u00f3n\n - Sin g\u00e9nero no existe relaci\u00f3n directa con los gustos del usuario\n - Sin embargo podr\u00eda darse una relaci\u00f3n impl\u00edcita en el rating de otros usuarios con el fin de categorizar a dichos anime. As\u00ed mismo de acuerdo al gusto de los usuarios podr\u00edan caer en alguna categor\u00eda y obtener dicha etiqueta.","003d467a":"De las 89 dimensiones, podemos reducir a 49 componentes principales sin degradar considerablemente el dataset.","fa30d9d8":"## Determinemos el n\u00famero de cl\u00fasters mediante el m\u00e9todo *Silhouette Score*","e789db7c":"El nuevo dataframe contiene las valoraciones que dio cada usuario para cada anime.","6f17eb77":"### Tratamiento columna *type*","f2137664":"## Interesante ver donde est\u00e1 concentrada la mayor\u00eda de los ratings","d8f9a9e2":"# Conclusi\u00f3n","c13b707f":"### Tratamiento columna episodes","93d63072":"# Decisi\u00f3n para la columna *rating*","bafd7394":"A pesar de ser una cantidad tan baja de valores null, como antes ya se explic\u00f3, con la estrategia seguida se eliminan si o s\u00ed dado que se busca generar un dataset mediante una operaci\u00f3n **INNER JOIN** entre ambas tablas.","30cd1126":"De ahora en adelante para reducir los tiempos para las pruebas trabajaremos con un dataframe de muestra obtenido aleatoriamente.","a1fc4ffc":"Verificamos que no existan valores no v\u00e1lidos como menores 0(pero mayores a -1) o mayores a 10.","783ab107":"# Estrategia","7583e4ae":"Adem\u00e1s con el fin de comparar resultados finales, aplicaremos PCA para 3 componentes.","d1bce5cf":"Ambas tablas contienen informaci\u00f3n de preferencias de 73.516 usuario en 12.294 anime (series de dibujos animados orientales). Cada usuario puede agregar un anime a su lista y darle un rating (de 0 a 10). La descripci\u00f3n de la data es la siguiente:\n\nAnime.csv \n- anime_id : id \u00fanico del anime (de la p\u00e1gina myanimelist.net) \n- name : nombre del anime \n- genre : lista de generos separados por coma del anime \n- type : TV, movie (de pel\u00edcula), OVA, etc\u2026 \n- episodes : cantidad de episodios del show (1 si es pel\u00edcula) \n- rating : rating promedio (de 1-10) para este anime \n- members : numero de miembros de la comunidad que est\u00e1n en el grupo del anime\n\nRating.csv \n- user_id : id del usuario generado aleatoriamente \n- anime_id : el anime que el usuario rankeo \n- rating : el rating entre 1 y 10 que el usuario asign\u00f3 al anime ( -1 si el usuario vio el anime pero no le asign\u00f3 puntaje)\n\nEl objetivo del desaf\u00edo es desarrollar un sistema de recomendaci\u00f3n que permita sugerir anime que los usuarios no han visto, en base a sus ratings previos. Es muy importante justificar la elecci\u00f3n del sistema (o modelo), el trabajo previo de la data (EDA) y la documentaci\u00f3n de lo que se hizo (no es necesario un informe, pero si comentar porqu\u00e9 se tomaron las decisiones que se tomaron; por ej eliminar una variable o eliminar registros missing, etc).","65ada8c1":"Para el dataset resultante de PCA con bajo degradamiento.","7e77835c":"- Para ambos casos 2 y 3 cl\u00fasters cumplen la tarea de clustering, aunque no de forma satifactor\u00eda dado que a simple vista no est\u00e1n bien definidos los cl\u00fasters.","3a81c636":"## Gr\u00e1fico ratings v\u00e1lidos vs nulls","190211e9":"Si bien la cantidad de registros en la tabla anime con valores null es m\u00ednima, de igual forma estos tienen un rating asignado y es importante considerarlos por lo que se toma la decisi\u00f3n de crear un nuevo valor **'No Genre'** para estos valores."}}