{"cell_type":{"bc7cf77b":"code","d126314f":"code","0ae627a8":"code","9ec7f9d0":"code","9184212b":"code","2ba801c5":"code","09784942":"code","1a022acf":"code","e0ef00bd":"code","c40b476b":"code","f1122dc4":"code","dcfbe6bb":"code","bf73a69a":"code","19034b92":"code","ffacfbe5":"code","0b3d3c80":"code","b1854da4":"code","0eaf0ed7":"code","f73b0d25":"code","7e6adc62":"code","855c7688":"code","c6ec77f2":"code","919a4c80":"code","665fded1":"code","3fcfba52":"code","490d5bcd":"code","24902679":"code","4ee5595d":"code","01a166fd":"code","bb825899":"code","7243a0e7":"code","73ba39d1":"code","42c769b7":"code","38b7f4f7":"code","553aa6eb":"code","a008fe9f":"code","8047a303":"code","68ecd1f6":"code","2a9402a0":"code","c5428899":"code","d46a88e2":"code","7bd8d92f":"code","66d21fbc":"code","486fc254":"code","d175628e":"code","171726a8":"code","4d350f2a":"markdown","c14e328b":"markdown","cefdce9e":"markdown","5c5d0193":"markdown","4dcf1500":"markdown","f15dfda2":"markdown","3dda8f55":"markdown","a4f382a1":"markdown","682bbaed":"markdown","10211283":"markdown","79a44526":"markdown","c6cc71f2":"markdown","74770486":"markdown","46e73267":"markdown","5a803011":"markdown","90a526a6":"markdown","8a8f8724":"markdown","c6d46321":"markdown","6bd08824":"markdown","c9953866":"markdown","5fc2e121":"markdown","d983f980":"markdown","8127f4c7":"markdown","df7a15b6":"markdown","47abf12d":"markdown","71076073":"markdown","fc046e02":"markdown","39757c34":"markdown","69e4c550":"markdown","e8f3b04d":"markdown","5bc77c46":"markdown","8a1cc0c3":"markdown","d154bb08":"markdown","5563adf0":"markdown","5a0e735e":"markdown","537fbcfa":"markdown","4c3ab5af":"markdown","716867bd":"markdown","fa7892d8":"markdown","0cec619c":"markdown","b898a1f2":"markdown","7995417a":"markdown"},"source":{"bc7cf77b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nplt.style.available\nplt.style.use('seaborn-darkgrid')\nimport scipy.stats as stats\nimport pylab \nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom bayes_opt import BayesianOptimization\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import LinearSVR\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Any results you write to the current directory are saved as output.","d126314f":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","0ae627a8":"fig, ax = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(train.SalePrice, ax=ax[0])\nsns.boxplot(y= train.SalePrice,ax=ax[1], color='green')\nstats.probplot(train.SalePrice, dist=\"norm\", plot=pylab)\npylab.show()\n\nprint('Skewness: ',train.SalePrice.skew())\nprint('Kurtosis: ',train.SalePrice.kurt())","9ec7f9d0":"model = LinearRegression().fit(train.GrLivArea.values.reshape(-1,1), train.SalePrice.values.reshape(-1,1))\n\n\nfig, ax = plt.subplots(1,2, figsize=(16,6))\nsns.scatterplot(train.GrLivArea, train.SalePrice, ax=ax[0], color='blue')\nax[0].set_title('Regression Plot')\nfor i in range(len(train.SalePrice)):\n    ax[0].plot((train.GrLivArea.iloc[i], train.GrLivArea.iloc[i]), (train.SalePrice.values[i], model.predict(train.GrLivArea.values.reshape(-1,1))[i]), 'r-', color='red', alpha=0.4)\nax[0].plot(train.GrLivArea, model.predict(train.GrLivArea.values.reshape(-1,1)), color='red')\nax[1].set_title('Residual Plot')\nsns.residplot(train.GrLivArea, train.SalePrice, ax=ax[1], color='green', scatter_kws={'alpha':0.3})\nplt.show()","9184212b":"fig, ax = plt.subplots(1,3, figsize=(20,6))\n\nsns.residplot(train.GrLivArea, np.log1p(train.SalePrice), lowess=True, color=\"red\", ax=ax[0], scatter_kws={'alpha': 0.1, 'color':'blue'})\nax[0].set_title('Log Transformation')\nsns.residplot(train.GrLivArea, np.sqrt(train.SalePrice), lowess=True, color=\"red\", ax=ax[1], scatter_kws={'alpha': 0.1, 'color':'blue'})\nax[1].set_title('Square Root Transformation')\nsns.residplot(train.GrLivArea, boxcox1p(train.SalePrice, boxcox_normmax(train.SalePrice + 1)), lowess=True, color=\"red\", ax=ax[2], scatter_kws={'alpha': 0.1, 'color':'blue'})\nax[2].set_title('Box-Cox Transformation')\nplt.show()","2ba801c5":"fig, ax = plt.subplots(2, 2, figsize=(20,10))\n\nsns.distplot(train.SalePrice, color=\"red\", ax=ax[0][0])\nax[0][0].set_title('Sale Prices')\nsns.distplot(np.log1p(train.SalePrice), ax=ax[0][1])\nax[0][1].set_title('Log-transformed Sale Prices')\nsns.boxplot(y= train.SalePrice, color=\"red\", ax=ax[1][0])\nsns.boxplot(y= np.log1p(train.SalePrice), ax=ax[1][1])\nplt.show()","09784942":"from collections import Counter\ndef detect_outliers(data, features):\n    indices = {}\n    fo = {}    \n    for f in features:\n        outliers = []\n        median = np.median(data[f])\n        \n        q1 = np.nanpercentile(data[f], 25)\n        q3 = np.nanpercentile(data[f], 75)\n        iqr = q3 - q1\n        \n        low  = q1 - 1.5*iqr\n        high = q3 + 1.5*iqr\n        \n        outlier_indices = data[(data[f] < low) | (data[f] > high)].index\n        outliers.extend(outlier_indices)\n        \n        if outliers: \n            fo.update({f : len(outliers)})\n            indices.update({f : outliers})\n    return fo\n\n#     indices = [i for i,j in Counter(outliers).items() if j > 0]\n#     return indices\n\nprint('Number of Outliers for SalePrice: ',detect_outliers(train, ['SalePrice']))\ntrain['LogSalePrice'] = np.log1p(train.SalePrice)\nprint('Number of Outliers for Log-transformed SalePrice: ',detect_outliers(train, ['LogSalePrice']))\ntrain.drop(['LogSalePrice'], axis=1, inplace=True)","1a022acf":"fig, ax = plt.subplots(1, 2, figsize=(20,6))\nsns.scatterplot(train.MoSold, train.SalePrice, ax=ax[0])\nsns.scatterplot(train.YrSold, train.SalePrice, ax=ax[1])\nax[0].set_xticks(train.MoSold.unique())\nax[1].set_xticks(train.YrSold.unique())\nplt.show()","e0ef00bd":"plt.figure(figsize=(17,6))\nsns.swarmplot(train.YearBuilt, train.SalePrice)\nplt.xticks(rotation=90)\n\ntrain[['SalePrice', 'YearBuilt']].corr()","c40b476b":"plt.figure(figsize=(8,7))\nsns.boxplot(train.Street, train.SalePrice)\n\nprint('Street value counts\\n')\nprint(train.Street.value_counts())","f1122dc4":"plt.figure(figsize=(8,7))\n\nsns.boxplot(train.Utilities, train.SalePrice)\nprint(train.Utilities.value_counts())","dcfbe6bb":"plt.figure(figsize=(10,7))\n\nsns.boxplot(train.PoolQC.fillna('No Pool'), train.SalePrice)\nprint(train.PoolQC.value_counts())","bf73a69a":"plt.figure(figsize=(10,7))\n\nsns.boxplot(train.Fireplaces, train.SalePrice)\nprint(train.Fireplaces.value_counts())","19034b92":"num_features = [f for f in train.columns if train[f].dtype != 'O']\nnumerical = ['SalePrice','LotArea','OverallQual','MasVnrArea','TotalBsmtSF','1stFlrSF','GrLivArea','FullBath','BsmtFullBath','GarageArea','PoolArea']\n\nsns.pairplot(train[numerical])\nplt.show()","ffacfbe5":"plt.subplots(figsize = (20,15))\nmask = np.zeros_like(train[num_features[1:(len(num_features)-1)]].corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train[num_features[1:(len(num_features)-1)]].corr(), mask = mask, annot=True, center = 0, cmap='Reds', fmt='.2f')\n\nd = train.corr()[(train.corr().index=='SalePrice')].T.reset_index()\nd[d.SalePrice > 0.6].reset_index(drop=True).iloc[:6].sort_values(by='SalePrice', ascending=False).reset_index(drop=True)","0b3d3c80":"print('High Correlations Between Features\\n\\n')\n\nv=[]\nfor i in train[num_features[1:(len(num_features)-1)]]:\n    for idx,j in enumerate(train.corr()[i][train.corr()[i].values > 0.7]):\n        if j<1 and j not in v:\n            print(i, '-', train.corr()[i][train.corr()[i].values > 0.7].index[idx],': ', j,'\\n')\n            v.append(j)","b1854da4":"data = pd.concat([train, test], axis=0).reset_index(drop=True)\ndata.tail()","0eaf0ed7":"nulls = pd.Series(data.isna().sum()[(data.isna().sum()>0) & (data.isna().sum().index !='SalePrice')].sort_values(ascending=False).values, index=data.isna().sum()[(data.isna().sum()>0) & (data.isna().sum().index !='SalePrice')].sort_values(ascending=False).index)\ndtypes = pd.Series([data[i].dtype for i in nulls.index], index=nulls.index)\n\nnulld = pd.concat([nulls, dtypes], axis=1, keys=['# of Nulls', 'Dtype'])\nnulld","f73b0d25":"idx= []\nidx.extend(data[data.GarageType.isnull()].index)\nidx2 = []\nidx2.extend(data[data.GarageCond.isnull()].index)\nprint(set(idx2) - set(idx))\n\ndata[['GarageType','GarageFinish','GarageYrBlt','GarageCond','GarageYrBlt']].iloc[[2126, 2576]]","7e6adc62":"for i in nulld[(nulld.Dtype == 'object')].index:\n    \n    if  nulld.loc[i]['# of Nulls']>20:\n        data[i] = data[i].fillna('None')\n        \n    else:\n        data[i] = data[i].fillna(data[i].mode()[0])","855c7688":"for i in data[data.LotFrontage.isna()].index:\n    data.LotFrontage.iloc[i] = data.LotFrontage[data.Neighborhood == data.Neighborhood.iloc[i]].median()\n    \nfor i in nulld[(nulld.Dtype != 'object')].index:\n    data[i] = data[i].fillna(data[i].mode()[0])","c6ec77f2":"print('Number of Missing Values: ',data[~data.SalePrice.isna()].isna().sum().values.sum())","919a4c80":"data['TotalSF'] = data['1stFlrSF'] + data['2ndFlrSF'] + data['TotalBsmtSF']\ndata['TotalBath'] = data.BsmtFullBath + data.BsmtHalfBath + data.FullBath + data.HalfBath\ndata['TotalPorchSF'] = data['OpenPorchSF'] + data['3SsnPorch'] + data['EnclosedPorch'] + data['ScreenPorch'] + data['WoodDeckSF']\n\ndata['hasPool'] = [1 if i >0 else 0 for i in data['PoolArea']]\ndata['has2ndfloor'] = [1 if i >0 else 0 for i in data['2ndFlrSF']]\ndata['hasGarage'] = [1 if i >0 else 0 for i in data['GarageArea']]\ndata['hasBsmt'] = [1 if i >0 else 0 for i in data['TotalBsmtSF']]\ndata['hasFireplace'] = [1 if i >0 else 0 for i in data['Fireplaces']]","665fded1":"for i in ['MSSubClass', 'YrSold', 'MoSold']:\n    data[i] = data[i].apply(lambda x: str(x))","3fcfba52":"test_id = data[data.SalePrice.isna()]['Id']\n\ndata.drop(['Street', 'Utilities', 'Id'], axis=1, inplace=True)","490d5bcd":"data = pd.get_dummies(data).reset_index(drop=True)","24902679":"# test\ntest = data[data.SalePrice.isna()].drop(['SalePrice'], axis=1)\n\n# train\ntrain = data[~data.SalePrice.isna()]","4ee5595d":"[{i:train[i].value_counts().iloc[0]} for i in train.columns if train[i].value_counts().iloc[0] == len(train)]","01a166fd":"# drop -->  MSSubClass_150\n\ntrain = train.drop(['MSSubClass_150'], axis=1)\ntest  = test.drop(['MSSubClass_150'], axis=1)","bb825899":"x = train[~train.SalePrice.isna()].drop(['SalePrice'], axis=1)\ny = train[~train.SalePrice.isna()].SalePrice\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.3, random_state=3)","7243a0e7":"def gbrf(learning_rate, n_estimators, max_depth):\n    params = dict(learning_rate=learning_rate, n_estimators=int(n_estimators), max_depth=int(max_depth))\n    \n    gbr = GradientBoostingRegressor()\n    return min(cross_val_score(gbr, x, y, cv=StratifiedKFold(5, shuffle=True, random_state=3), scoring='neg_root_mean_squared_error'))","73ba39d1":"bounds = {'n_estimators':(100,3000), 'max_depth':(2,5), 'learning_rate':(0,1)}\n\noptimizer = BayesianOptimization(f= gbrf,\n                                 pbounds= bounds,\n                                 random_state=3)\n\noptimizer.maximize(n_iter=10, init_points=8, acq='ei')","42c769b7":"gbr_params = optimizer.max\nprint('Optimum parameters for Gradient Boost\\n\\n',gbr_params['params'])","38b7f4f7":"# import xgboost as xgb\n\n# def black_box_function(n_estimators, max_depth, learning_rate):\n#     params = dict(n_estimators= int(n_estimators), max_depth= int(max_depth), learning_rate= learning_rate, eval_metrics='rmse')\n#     model = xgb.XGBModel(objective='reg:squarederror', **params)\n\n#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=0)\n#     return -model.evals_result()['validation_0']['rmse'][-1]","553aa6eb":"def xgbf(n_estimators, gamma, max_depth, learning_rate):\n    d_train = xgb.DMatrix(x, y)\n    params = dict(n_estimators= int(n_estimators), gamma= gamma, max_depth= int(max_depth), learning_rate= learning_rate, eval_metrics='rmse')\n    \n    return -xgb.cv(dtrain = d_train, params= params, folds=StratifiedKFold(5, shuffle=True, random_state=3))['test-rmse-mean'].iloc[-1]","a008fe9f":"bounds = {'n_estimators':(100,3000), 'gamma':(0,1), 'max_depth':(2,5), 'learning_rate':(0,1)}\n\noptimizer = BayesianOptimization(f= xgbf,\n                                 pbounds= bounds,\n                                 random_state=3)\n\noptimizer.maximize(n_iter=8, init_points=5, acq='ei')","8047a303":"xgb_params = optimizer.max\nprint('Optimum parameters for XG Boost\\n\\n',xgb_params['params'])","68ecd1f6":"def lgbf(learning_rate, max_depth, num_leaves, n_estimators, reg_lambda):\n    \n    train_data = lgb.Dataset(x, label=y)\n    params = dict(learning_rate= learning_rate, max_depth=int(max_depth), num_leaves=int(num_leaves), n_estimators=int(n_estimators), reg_lambda=int(reg_lambda), metrics='rmse', objective='regression')\n    \n    return -lgb.cv(train_set=train_data, params=params, nfold=5)['rmse-mean'][-1]","2a9402a0":"bounds = dict(learning_rate= (0,1), max_depth=(-1,3), num_leaves=(20,50), n_estimators=(100,3000), reg_lambda=(0,3))\n\noptimizer = BayesianOptimization(f= lgbf,\n                                 pbounds= bounds,\n                                 random_state=3)\n\noptimizer.maximize(n_iter=8, init_points=5, acq='ei')","c5428899":"lgb_params = optimizer.max\nprint('Optimum parameters for LightGBM\\n\\n',lgb_params['params'])","d46a88e2":"from catboost import Pool, cv\n\n\ndef cbf(learning_rate, reg_lambda):\n    \n    cv_data = Pool(data=x, label=y)\n    params = dict(learning_rate= learning_rate, reg_lambda=reg_lambda, loss_function ='RMSE', verbose=False, eval_metric='RMSE', iterations=400)\n    \n    return -cv(cv_data, params, fold_count=5, plot=False)['test-RMSE-mean'].iloc[-1]","7bd8d92f":"bounds = dict(learning_rate= (0,1), reg_lambda=(2,5))\n\noptimizer = BayesianOptimization(f= cbf,\n                                 pbounds= bounds,\n                                 random_state=3)\n\noptimizer.maximize(n_iter=8, init_points=5, acq='ei')","66d21fbc":"cb_params = optimizer.max\nprint('Optimum parameters for CatBoost\\n\\n',cb_params['params'])","486fc254":"final_estimators = [Lasso(), Ridge(), ElasticNet(), LinearSVR(), RandomForestRegressor()]\n\nestimators = [('xgb', XGBRegressor(n_estimators=2981, learning_rate=0.5002050589158064, gamma=0.20137108244848478)),\n              ('gbc', GradientBoostingRegressor(n_estimators=554, max_depth=3, learning_rate=0.6931379183129963)),\n              ('lgb', LGBMRegressor(n_estimators=1982, num_leaves=28, reg_lambda=2.028764705940394, learning_rate=0.029876210878566956)),\n              ('catb', CatBoostRegressor(learning_rate=0.051867793738858414, reg_lambda=3.3120792921610147, verbose=False))]\n\nfor i in final_estimators:\n    sc = StackingRegressor(estimators = estimators,\n                           final_estimator= i,\n                           cv = StratifiedKFold(5, shuffle=True, random_state=3)).fit(x_train, y_train)\n    \n    print('Meta Regressor: ',i, '\\n\\n')\n    print('R2 Score = ', sc.score(x_val, y_val))\n    print('RMSE = ', np.sqrt(mean_squared_error(y_val, sc.predict(x_val))), '\\n\\n')","d175628e":"def blend_models(x_train, y_train, x_val):\n    sr_r = StackingRegressor(estimators = estimators,final_estimator= Ridge()).fit(x_train, y_train).predict(x_val) * 0.3\n    se_r = StackingRegressor(estimators = estimators,final_estimator= ElasticNet()).fit(x_train, y_train).predict(x_val) * 0.3\n    cb_r = CatBoostRegressor(learning_rate=0.051867793738858414, reg_lambda=3.3120792921610147, verbose=False).fit(x_train, y_train).predict(x_val) * 0.3\n    lg_r = LGBMRegressor(n_estimators=1982, num_leaves=28, reg_lambda=2.028764705940394, learning_rate=0.029876210878566956).fit(x_train, y_train).predict(x_val) * 0.1\n    \n    return sr_r + se_r + lg_r + cb_r\n    \n    \nprint('RMSE:', np.sqrt(mean_squared_error(y_val, blend_models(x_train, y_train, x_val))))\nprint()\nprint('R2 Score: ', r2_score(y_val, blend_models(x_train, y_train, x_val)))","171726a8":"test_predictions = pd.Series(blend_models(x, y, test), name='SalePrice')\n\nids = test_id.reset_index(drop=True)\nresults = pd.concat([ids, test_predictions], axis=1)\nresults.to_csv(\"houseprices.csv\", index = False)","4d350f2a":"Null categorical features means there aren't any of it.\n<font color=blue>\n* **PoolQC:** 2909 houses don't have any pools\n* **Fence:** 2814 houses don't have any fences.\n* **GarageType:** 157 houses definitely have no garages, <font color=red>because all other related features such as GarageFinish and GarageCond have also same missed indices.\n    \n<font color=black>\nThe other 2 indices are [2126 and 2576], <font color=red>which means these houses have actually garages but other related features are unrecorded.\n    \n<font color=black>\n\nThe indices are from test data, let's skip them and just impute with 'None'.","c14e328b":"<b id=26>\n    \n## Blended Models\n<font color=blue>\n* The weights of the models have been determined according to the single RMSE obtained in the Bayesian Optimization and Stacking processes.\n    \n<font color=black>\nWeights\n\n\n<font color=green>\n* Stacked Model with Meta-Regressor Ridge Weight: 0.3\n* Stacked Model with Meta-Regressor ElasticNet: RMSE Weight: 0.3\n* CatBoost: RMSE Weight: 0.3\n* LightGBM: RMSE Weight: 0.1","cefdce9e":"### Thanks for reading this kernel.","5c5d0193":"<b id=16>\n\n# Preprocessing","4dcf1500":"<b id=23>\n## CatBoost","f15dfda2":"#### Imputing Categorical Features\n<font color=blue>\n* Impute categorical feature with 'None' if it has more than 20 null values.\n\n* Else, impute with mode value.","3dda8f55":"<font color=red>\nAlthough frequentist statistics(hypothesis tests) and descriptive statistics(mean, variance etc.) require the data to be normally distributed, it's not a vital process to make all numerical independent variables normal.\n    \nSo, it can be skipped for this time.","a4f382a1":"<b id=10>\n\n### Pair Relationships\n<font color=blue>\n* The features related to area size and quality have the best relationship with SalePrice.","682bbaed":"<b id =1>\n# Univariate Analysis: SalePrice","10211283":"It stems from the variability in the measurement, therefore<font color= red> the outliers **should NOT** be removed from the dataset, since they may carry vital information.\n<font color= blue>\n    \n    \n* A Robust Linear Model (RLM) can be used to deal with them.","79a44526":"<b id=14>\n\n## Imputation of Missing Values","c6cc71f2":"#### Drop features that cause overfitting\n\n<font color=blue>\n\n* In train dataset, MSSubClass_150 consists of all zeros. Therefore, it's not a useful feature and may cause overfitting,","74770486":"<b id=13>\n\n# Feature Engineering","46e73267":"Plots show that;\n<font color= blue>* Dependent variable is highly skewed to the <font color= red> right.\n<font color= blue>* There are <font color= red> probably **OUTLIERS** <font color= black>\n<font color= blue>* Distribution is<font color= red> NOT normal. \n<font color=black>\n    \n    \n    \n    \n","5a803011":"### Drop Unnecessary Features","90a526a6":"### Missing Values","8a8f8724":"This situation causes one of the assumptions of regression to fail;\n\n*        <font color=blue>Non-constant variance of error terms aka Heteroscedasticity\n* Non-linearity\n\n<font color=black>\n    \nDeviations of true values from regression line gradually increases, accordingly the residual plot has a shape of <font color=red> **FUNNEL!**","c6d46321":"<b id=2>\n### Outliers\n<font color=blue>\nNotice that there is a drop in the number of outliers of SalePrice after transformation.","6bd08824":"<b id=12>\n\n### High Correlations and Multicollinearity\n    \n<font color=blue>\n* There is no correlation as high as it will cause multicollinearity and fail one of the assumptions of regression.\n\n<font color=black>\nFor a better inference, <font color=red>**VIF(variation inflation factor)**<font color=black> must be examined.","c9953866":"<b id=9>\n\n### Fireplaces vs SalePrice\n\n<font color=blue>\n\nThere is a gradual effect of the number of fireplaces on sale prices.\n\nSince having one or more fireplaces doesn't have a significant effect on medians, 1 or more fireplaces can be grouped and created a binary feature.","5fc2e121":"<b id=3>\n\n## Bivariate Analysis\n","d983f980":"<b id=21>\n## XGBoost","8127f4c7":"<b id=17>\n## One-hot Encoding","df7a15b6":"<b id=27>\n    \n# Submission\n\n<font color=green>\n* Blended models have yold the best scores so far. Therefore, the predictions for test data will be submitted with them.","47abf12d":"<b id=7>\n\n### Utilities vs SalePrice\n\n<font color=blue>\nUtilities also isn't a useful feature.","71076073":"<b id=8>\n\n### PoolQC vs SalePrice\n\n<font color=blue>\nAlthough there are only a few houses having pools, there seems to be a relationship between the quality of pool and sale prices.","fc046e02":"<b id=25>\n## Stacked Models","39757c34":"<b id=22>\n## LightGBM","69e4c550":"<b id=18>\n## Train Test Split","e8f3b04d":"1. [Univariate Analysis: SalePrice](#1)\n    * [Outliers](#2)\n    \n    \n2. [Bivariate Analysis](#3)\n    * [MoSold-YrSold vs SalePrice](#4)\n    * [YearBuilt vs SalePrice](#5)\n    * [Street vs SalePrice](#6)\n    * [Utilities vs SalePrice](#7)\n    * [PoolQC vs SalePrice](#8)\n    * [Fireplaces vs SalePrice](#9)\n    * [Pair Relationships](#10)\n    \n    \n3. [Correlation Between Features](#11)\n    * [High Correlations and Multicollinearity](#12)\n    \n    \n4. [Feature Engineering](#13)\n    * [Imputation of Missing Values](#14)\n    * [Creating New Features](#15)\n    \n    \n5. [Preprocessing](#16)\n    * [One-hot Encoding](#17)\n    * [Train Test Split](#18)\n    \n    \n6. [Bayesian Hyperparameter Optimization](#19)\n    * [Gradient Boosting](#20)\n    * [XGBoost](#21)\n    * [LightGBM](#22)\n    * [CatBoost](#23)\n  \n  \n7. [Regression Analysis with Ensemble Modeling](#24)\n    * [Stacked Models](#25)\n    * [Blended Models](#26)\n    \n    \n8. [Submission](#27)","5bc77c46":"#### Imputing Numerical Features\n<font color=blue>\n* Impute LotFrontage according to the corresponding Neighborhood's median value.\n* Impute rest with mode value.","8a1cc0c3":"<font color=blue>\n* The highest R2 score and the lowest RMSE have been yold by Meta Regressor: <font color=green>ElasticNet","d154bb08":"<b id=24>\n    \n# Regression Analysis with Ensemble Modeling\n\n<font color=blue>\n* Stacked Models\n\n* Blended Models","5563adf0":"<b id=11>\n\n### Correlation Between Features\n<font color=blue>\n* Correlation coefficients for SalePrice verifies what has been said above.","5a0e735e":"When the dependent variable in a regression analysis is not normally distributed, it is common practice to perform a power transformation on that variable. Some of them are;\n<font color=blue>\n* Log transformation\n* Square root transformation\n* Box-Cox transformations\n\n","537fbcfa":"<b id=5>\n### YearBuilt vs SalePrice\n\n<font color=blue>\n    \n* Unlike MoSold\/YrSold, there is a relationship between YearBuilt and SalePrice that can be counted as monotonic. So, it can be stay as a numerical feature.","4c3ab5af":"<b id=19>\n\n# Bayesian Hyperparameter Optimization\n<font color=blue>\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives.\n\n<font color=black>\n\nMethods to be optimized:\n<font color=green>\n* Gradient Boosting\n* XGBoost\n* LightGBM\n* CatBoost","716867bd":"<b id=20>\n\n## Gradient Boosting","fa7892d8":"<font color=blue> \n\nObvious to see that each method enhanced the **Heteroscedasticity** even if just a bit.\n\nAlso of the data has been made more **normal distribution-like.**\n<font color=black> \n\nWe will continue with the log-transformed output.","0cec619c":"<b id=6>\n\n### Street vs SalePrice\n\n<font color=blue>\nStreet doesn't seem to be a meaningful feature to determine the SalePrice in regression.","b898a1f2":"<b id=15>\n\n## Creating New Features","7995417a":"<b id=4>\n\n### MoSold-YrSold vs SalePrice\n\n<font color=blue>\n\n* There is not a monotonic relation between months\/years(discrete variables) houses sold and sale prices. Therefore, they can be labeled as categorical."}}