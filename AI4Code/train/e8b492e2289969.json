{"cell_type":{"7e2ec329":"code","57f26391":"code","85c38d82":"code","423d338d":"code","5228fcbc":"code","a0a7fb38":"code","89196e63":"code","63e1c8c6":"code","046e13bf":"code","aa8af0e2":"code","8e6ef3b3":"code","9a2c2c26":"code","2d6278f8":"code","26e2331f":"code","5574a476":"code","63415b37":"code","96711f80":"code","94684d7e":"code","5bc6c0a9":"code","1c7e9989":"code","b004db1b":"code","9c7baa9f":"code","b50c4e8c":"code","1fe190fa":"code","4fc9dc27":"code","d994c631":"code","36bc21d8":"code","6238e7da":"code","7a96cefd":"code","c67bd559":"code","1cbe0d91":"code","7957d7f6":"code","0f70d817":"code","2519c9c4":"code","fa3cd184":"code","191a8a63":"code","353c927a":"code","18f95533":"code","1f5ff688":"code","c4513e18":"code","cc959f38":"code","6f9bd123":"code","9435b21d":"code","a48650b4":"code","26db0caa":"code","fa7b8ce9":"code","5592b2b4":"code","1b18a676":"code","5f489426":"code","1848fc0b":"code","e1c09319":"code","968f0173":"code","8aa17d61":"code","3909a126":"code","4f67bada":"code","87de2796":"code","707f48f0":"code","365cd688":"code","cda400a4":"code","a6df936c":"code","173672aa":"code","415ea251":"code","5ae4ce5e":"code","84060157":"code","806e40c2":"code","ec0bc00b":"code","3045ed0e":"code","07ad266f":"code","e3a784a0":"code","a94f61ca":"code","5ac36e16":"code","a98c8d72":"code","1e88dead":"code","77181abc":"code","512cb632":"code","55da98da":"code","fb4aba9f":"markdown","4ff78de5":"markdown","2cb6cb65":"markdown","19021b9b":"markdown","d6e728fc":"markdown","778a0f57":"markdown","336230f0":"markdown","fe9bbe17":"markdown","9db39f33":"markdown","7785ffcd":"markdown","af05b5e2":"markdown","34b1fffa":"markdown"},"source":{"7e2ec329":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","57f26391":"training_data = pd.read_csv('..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv',header=0,delimiter='\\t',quoting=3)","85c38d82":"training_data.head()","423d338d":"training_data['review'][0]","5228fcbc":"#it is not considered a reliable practice to remove markup using regular expressions,\n#so even for an application as simple as this, it's usually best to use a package like BeautifulSoup.\n#removing HTML Markups and Tags like \"<br>\"\nfrom bs4 import BeautifulSoup  ","a0a7fb38":"example1 = BeautifulSoup(training_data[\"review\"][0]) ","89196e63":"example1.get_text()","63e1c8c6":"#creating a function to clean the reviews\ndef review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    \n    #6.Lemmatization\n    for word in meaningful_words:\n        word = wordnet_lemmatizer.lemmatize(word,'v')\n    \n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))  ","046e13bf":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","aa8af0e2":"num_reviews = training_data['review'].size\nprint(\"Cleaning and parsing the training set movie reviews...\\n\")\nclean_train_reviews = []\nfor i in range( 0, num_reviews ):\n    # If the index is evenly divisible by 1000, print a message\n    if( (i+1)%1000 == 0 ):\n        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ))                                                                    \n    clean_train_reviews.append( review_to_words( training_data[\"review\"][i] ))\n","8e6ef3b3":"clean_train_reviews[0]","9a2c2c26":"from sklearn.feature_extraction.text import CountVectorizer","2d6278f8":"# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \nvectorizer = CountVectorizer(analyzer = \"word\",\n                             tokenizer = None, \n                             preprocessor = None,\n                             stop_words = None,  \n                             max_features = 5000)","26e2331f":"train_data_features = vectorizer.fit_transform(clean_train_reviews)","5574a476":"# Numpy arrays are easy to work with, so convert the result to an array\ntrain_data_features = train_data_features.toarray()","63415b37":"#25000 reviews and 5000 unique words(5000 was the number we set in max_features attribute in the CountVectorizer function\n#to limit the size of dictionary of unique words to 5000)\ntrain_data_features.shape","96711f80":"# Take a look at the words in the vocabulary\nvocab = vectorizer.get_feature_names()\nprint(vocab)","94684d7e":"#Using RandomForest Classifier to classify reviews\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","5bc6c0a9":"X_train, X_test, y_train, y_test = train_test_split(train_data_features, training_data[\"sentiment\"], \n                                                    test_size=0.2)","1c7e9989":"RF = RandomForestClassifier(n_estimators = 100)\nRF.fit( X_train, y_train )","b004db1b":"predictions = RF.predict(X_test)","9c7baa9f":"from sklearn.metrics import confusion_matrix,classification_report","b50c4e8c":"print(confusion_matrix(y_test,predictions))","1fe190fa":"print(classification_report(y_test,predictions))","4fc9dc27":"test_data = pd.read_csv('..\/input\/word2vec-nlp-tutorial\/testData.tsv',header=0,delimiter='\\t',quoting=3)","d994c631":"test_data.head()","36bc21d8":"num_reviews = len(test_data[\"review\"])\nclean_test_reviews = [] ","6238e7da":"print(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    if( (i+1) % 1000 == 0 ):\n        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n    clean_review = review_to_words( test_data[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()","7a96cefd":"# Use the random forest to make sentiment label predictions\nresult = RF.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )\n","c67bd559":"import tensorflow as tf","1cbe0d91":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python import keras","7957d7f6":"#building the model and compiling it\nmodel = keras.Sequential([\n    keras.layers.Dense(128,input_shape=(5000,),activation=tf.nn.relu),\n    keras.layers.Dense(128,activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metrics=['accuracy'])","0f70d817":"model.fit(X_train,y_train,epochs=10)","2519c9c4":"predictions_ANN = model.predict(X_test)","fa3cd184":"#to convert the output into binary True False form so that I can be compared with y_test\npredictions_ANN = (predictions_ANN>0.9)","191a8a63":"predictions_ANN","353c927a":"print(confusion_matrix(y_test,predictions_ANN))","18f95533":"print(classification_report(y_test,predictions_ANN))","1f5ff688":"result_ANN = model.predict(test_data_features)\nresult_ANN = result_ANN>0.9","c4513e18":"result_ANN","cc959f38":"result_ANN = [1 if res==True else 0 for res in result_ANN]","6f9bd123":"result_ANN","9435b21d":"# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result_ANN} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"Bag_of_Words_model_ANN.csv\", index=False, quoting=3 )\n","a48650b4":"unlabelled_data = pd.read_csv('..\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv',delimiter='\\t',quoting=3,header=0)","26db0caa":"unlabelled_data.head()","fa7b8ce9":"unlabelled_data['review'][0]","5592b2b4":"def review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)","1b18a676":"# Download the punkt tokenizer for sentence splitting\nimport nltk.data\nimport nltk\nnltk.download('punkt')\n\nfrom nltk import word_tokenize,sent_tokenize\n\n# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a list of sentences, where each sentence is a list of words\n    \n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    \n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","5f489426":"sentences = []  # Initialize an empty list of sentences\n\nprint(\"Parsing sentences from training set\")\nfor review in training_data[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint(\"Parsing sentences from unlabeled set\")\nfor review in unlabelled_data[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n","1848fc0b":"# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint(\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","e1c09319":"model.wv.doesnt_match(\"man woman child kitchen\".split())","968f0173":"model.wv.doesnt_match(['boy','girl','sun','child'])","8aa17d61":"model.wv.most_similar(\"man\")","3909a126":"model.wv.most_similar(\"queen\")","4f67bada":"model.wv.most_similar(\"awful\")","87de2796":"model.wv.most_similar(\"great\")","707f48f0":"model.wv.most_similar('dog')","365cd688":"# Load the model that we created\nfrom gensim.models import Word2Vec\nmodel = Word2Vec.load(\"300features_40minwords_10context\")","cda400a4":"type(model.wv.syn0)","a6df936c":"model.wv.syn0.shape","173672aa":"model['great']","415ea251":"def makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given paragraph\n    \n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    \n    nwords = 0.\n     \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            #here we are adding the feature vectors of all the words that were in the review\n            featureVec = np.add(featureVec,model[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in reviews:\n        # Print a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Review %d of %d\" % (counter, len(reviews)))\n        # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n        # Increment the counter\n        counter = counter + 1\n    return reviewFeatureVecs","5ae4ce5e":"# ****************************************************************\n# Calculate average feature vectors for training and testing sets,\n# using the functions we defined above. Notice that we now use stop word\n# removal.\n\nclean_train_reviews = []\nfor review in training_data[\"review\"]:\n    clean_train_reviews.append( review_to_wordlist( review,remove_stopwords=True ))\n\ntrainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n\nprint (\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test_data[\"review\"]:\n    clean_test_reviews.append( review_to_wordlist( review,remove_stopwords=True ))\n\ntestDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )","84060157":"X_train_vec,X_test_vec,y_train_vec,y_test_vec = train_test_split(trainDataVecs,training_data['sentiment'])","806e40c2":"from sklearn.ensemble import RandomForestClassifier\nforest_train = RandomForestClassifier( n_estimators = 100 )\n\nprint (\"Fitting a random forest to labeled training data...\")\nforest_train.fit( X_train_vec, y_train_vec )\n\n# Test & extract results \nresult = forest_train.predict( X_test_vec )","ec0bc00b":"print(confusion_matrix(y_test_vec,result))\nprint('\\n')\nprint(classification_report(y_test_vec,result))","3045ed0e":"# Fit a random forest to the training data, using 100 trees\nforest = RandomForestClassifier( n_estimators = 100 )\n\nforest.fit( trainDataVecs, training_data[\"sentiment\"] )\n\n# Test & extract results \nresult_to_be_submitted = forest.predict( testDataVecs )\n\n# Write the test results \noutput = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result_to_be_submitted} )\noutput.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n","07ad266f":"from sklearn.cluster import KMeans\nimport time\n\nstart = time.time() # Start time\n\n# Set \"k\" (num_clusters) to be 1\/5th of the vocabulary size, or an\n# average of 5 words per cluster\nword_vectors = model.wv.syn0\nnum_clusters = int(word_vectors.shape[0] \/ 5)\n\n# Initalize a k-means object and use it to extract centroids\nkmeans_clustering = KMeans( n_clusters = num_clusters )\nidx = kmeans_clustering.fit_predict( word_vectors )\n\n# Get the end time and print how long the process took\nend = time.time()\nelapsed = end - start\nprint (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")","e3a784a0":"# Create a Word \/ Index dictionary, mapping each vocabulary word to a cluster label                                                                                            \nword_centroid_map = dict(zip( model.wv.index2word, idx ))","a94f61ca":"values_word_centroid = list(word_centroid_map.values())","5ac36e16":"values_word_centroid[0]","a98c8d72":"keys_word_centroid = list(word_centroid_map.keys())","1e88dead":"# For the first 10 clusters\nfor cluster in range(0,10):\n    #\n    # Print the cluster number  \n    print (\"\\nCluster %d\" % cluster)\n    #\n    # Find all of the words for that cluster number, and print them out\n    words = []\n    for i in range(0,len(word_centroid_map.values())):\n        if( values_word_centroid[i] == cluster ):\n            words.append(keys_word_centroid[i])\n    print (words)","77181abc":"def create_bag_of_centroids( wordlist, word_centroid_map ):\n    #\n    # The number of clusters is equal to the highest cluster index\n    # in the word \/ centroid map\n    num_centroids = max( word_centroid_map.values() ) + 1\n    #\n    # Pre-allocate the bag of centroids vector (for speed)\n    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n    #\n    # Loop over the words in the review. If the word is in the vocabulary,\n    # find which cluster it belongs to, and increment that cluster count \n    # by one\n    for word in wordlist:\n        if word in word_centroid_map:\n            index = word_centroid_map[word]\n            bag_of_centroids[index] += 1\n    #\n    # Return the \"bag of centroids\"\n    return bag_of_centroids","512cb632":"# Pre-allocate an array for the training set bags of centroids (for speed)\ntrain_centroids = np.zeros( (training_data[\"review\"].size, num_clusters), \\\n    dtype=\"float32\" )\n\n# Transform the training set reviews into bags of centroids\ncounter = 0\nfor review in clean_train_reviews:\n    train_centroids[counter] = create_bag_of_centroids( review, \\\n        word_centroid_map )\n    counter += 1\n\n# Repeat for test reviews \ntest_centroids = np.zeros(( test_data[\"review\"].size, num_clusters), \\\n    dtype=\"float32\" )\n\ncounter = 0\nfor review in clean_test_reviews:\n    test_centroids[counter] = create_bag_of_centroids( review, \\\n        word_centroid_map )\n    counter += 1","55da98da":"# Fit a random forest and extract predictions \nforest = RandomForestClassifier(n_estimators = 100)\n\n# Fitting the forest may take a few minutes\nprint (\"Fitting a random forest to labeled training data...\")\nforest = forest.fit(train_centroids,training_data[\"sentiment\"])\nresult = forest.predict(test_centroids)\n\n# Write the test results \noutput = pd.DataFrame(data={\"id\":test_data[\"id\"], \"sentiment\":result})\noutput.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )","fb4aba9f":"I tried to use Neural Network to see if there is any improvement in accuracy","4ff78de5":"Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.","2cb6cb65":"A minor detail to note is the difference between the \"+=\" and \"append\" when it comes to Python lists. In many applications the two are interchangeable, but here they are not. If you are appending a list of lists to another list of lists, \"append\" will only append the first list; you need to use \"+=\" in order to join all of the lists at once.","19021b9b":"NOTE - Some changes have been made in the syntaxes - \nKeyedVectors.load_word2vec_format (instead ofWord2Vec.load_word2vec_format)\nword2vec_model.wv.save_word2vec_format (instead of  word2vec_model.save_word2vec_format)\nmodel.wv.syn0norm instead of  (model.syn0norm)\nmodel.wv.syn0 instead of  (model.syn0)\nmodel.wv.vocab instead of (model.vocab)\nmodel.wv.index2word instead of (model.index2word)","d6e728fc":"The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector.Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:","778a0f57":"In this Notebook, I will be classifying reviews as positive or negative using 2 different concepts of Natural Language Procession - \n1. The first one involves creating the classical bag of words model\n2. The second one involves involves creating a bag of vectors model i.e. each word is converted into a vector(Word to Vec)\n\nThe advantage of converting words to vector is that semantically similar words are placed near to each other and words opposite in meaning are placed further apart. \n\nIn the project, the accuracy in predicting the sentiment of the reviews remains almost the same with both the approaches but it is found that when the amount of training data is increased, the word2vec approach performs better. \nP.S. - Google Search utlises the word2vec approach.","336230f0":"To train Word2Vec it is better not to remove stop words or numbers because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors.","fe9bbe17":"One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n\nSince each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n\nThe following code averages the feature vectors","9db39f33":"Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.","7785ffcd":"It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting. In order to use this, you will need to install NLTK and use nltk.download() to download the relevant training file for punkt.","af05b5e2":"Now using the 2nd Approach i.e Word to Vec. It is an unsupervised approach i.e. it doesnt involve using labels, it just places the similar words together and dissimilar words far apart.","34b1fffa":"the Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\""}}