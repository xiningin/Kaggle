{"cell_type":{"7bba82a8":"code","72603a89":"code","66926e2a":"code","fce01a2e":"code","524d7971":"code","b18661ee":"code","8146807f":"code","8e3c5830":"code","01207d2a":"code","defdeb72":"code","d41da9bd":"code","8aec3485":"markdown"},"source":{"7bba82a8":"import pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\nimport tqdm \nimport numpy as np\nfrom sklearn.linear_model import Ridge\nimport dask","72603a89":"fnc = pd.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv')\nloading = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv')\nlabels = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv')","66926e2a":"fnc_features, loading_features = list(fnc.columns[1:]), list(loading.columns[1:])\ndf = fnc.merge(loading, on=\"Id\")\n\nlabels[\"is_train\"] = True","fce01a2e":"df = df.merge(labels, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","524d7971":"FNC_SCALE = 1\/600\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","b18661ee":"def metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)\/np.sum(y_true, axis=0))","8146807f":"def svm_ridge(target, c, ww, ff):\n\n    NUM_FOLDS = 7\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\n\n    features = loading_features + fnc_features\n\n    overal_score = 0\n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    cnt = 0\n    for f, (train_ind, val_ind) in (enumerate(kf.split(df, df))):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        # train both models at once\n        pipelines = [SVR(C=c), Ridge(alpha=0.0001)]  \n        pipelines_ = [dask.delayed(pl).fit(train_df[features], train_df[target]) for pl in pipelines]\n        fit_pipelines = dask.compute(*pipelines_)\n        \n        # inference for val\n        inference_ = [dask.delayed(m).predict(val_df[features]) for m in fit_pipelines]\n        preds = (dask.compute(*inference_))\n        val_pred_1 = preds[0]\n        val_pred_2 = preds[1]\n        \n        # inference for test\n        inference_ = [dask.delayed(m).predict(test_df[features]) for m in fit_pipelines]\n        preds = (dask.compute(*inference_))\n        test_pred_1 = preds[0]\n        test_pred_2 = preds[1]\n        \n        val_pred = np.array((1-ff)*val_pred_1+ff*val_pred_2)\n        val_pred = val_pred.flatten()\n        \n        test_pred = np.array((1-ff)*test_pred_1+ff*test_pred_2)\n        test_pred = test_pred.flatten()\n        \n        y_oof[val_ind] = val_pred\n        y_test[:, f] = test_pred\n        \n        \n        print(target + \" iter \" + str(cnt))\n        cnt+=1\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    \n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    print(target + \" \" + str(score))\n    return ww*score, test_df[target]","8e3c5830":"%%time \nresults = []\nL = [(\"age\", 60, 0.3, 0.5), (\"domain1_var1\", 12, 0.175, 0.2), (\"domain1_var2\", 8, 0.175, 0.2), (\"domain2_var1\", 9, 0.175, 0.22), (\"domain2_var2\", 15, 0.175, 0.22)]\nfor x in L:\n    print(x)\n    y = dask.delayed(svm_ridge)(x[0], x[1], x[2], x[3])\n    results.append(y)\n\nresults = dask.compute(*results)","01207d2a":"r_sum = 0\nfor i in range(0, 5):\n    r_sum += results[i][0]\nr_sum\n\npreds = pd.concat([results[0][1], results[1][1], results[2][1], results[3][1], results[4][1]], axis=1)","defdeb72":"# Function from https:\/\/www.kaggle.com\/nischaydnk\/beginners-trends-neuroimaging-decent-score\ndef make_sub(predictions):\n    features = ('age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2')\n    _columns = (0,1,2,3,4)\n    tests = predictions.rename(columns=dict(zip(features, _columns)))\n    tests = tests.melt(id_vars='Id',value_vars=_columns,value_name='Predicted')\n    tests['target'] = tests.variable.map(dict(zip(_columns, features)))\n    tests['Id_'] = tests[['Id', 'target']].apply(lambda x: '_'.join((str(x[0]), str(x[1]))), axis=1)\n  \n    return tests.sort_values(by=['Id', 'variable'])\\\n              .drop(['Id', 'variable', 'target'],axis=1)\\\n              .rename(columns={'Id_':'Id'})\\\n              .reset_index(drop=True)\\\n              [['Id', 'Predicted']]","d41da9bd":"preds = pd.concat([test_df['Id'], preds], axis=1)\npreds = make_sub(preds)\npreds.to_csv('dask.csv', index=False)","8aec3485":"This is a dask implementation of @tunguz Rapids ai Ensemble code: https:\/\/www.kaggle.com\/tunguz\/rapids-ensemble-for-trends-neuroimaging. \n\nNot having acess to GPUs (and tired of paying for AWS), I still wanted train my models in parallel while training it on my local machine (i9 processor, 8 cores). \n\nThis implementation uses between 500-600% of the CPU and 2gb of memory on my personal machine. However, on the Kaggle kernels it uses ~400 CPU and 3.6gb of Ram. \n\nIf this can be parrelized anymore for CPUs please let me know!"}}