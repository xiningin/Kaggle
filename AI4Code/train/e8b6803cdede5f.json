{"cell_type":{"8d178036":"code","4b95826f":"code","f76a8884":"code","f6f1b421":"code","cd1bd1b5":"code","5e9748de":"code","e902a0f2":"code","4976917d":"code","1c0130cf":"code","9b585181":"code","1f368cb8":"code","436d8c65":"code","982b0956":"markdown","7e2cb3cb":"markdown"},"source":{"8d178036":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_path=\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\n\n%matplotlib inline \n\n# Any results you write to the current directory are saved as output.","4b95826f":"df = pd.read_csv(data_path+\"train.csv\")","f76a8884":"null_val = df.isnull().sum()\npercent = 100 * df.isnull().sum() \/ len(df)\nmissing_table = pd.concat([null_val, percent], axis=1)\nmissing_table=missing_table.rename(columns={0: 'missing_num', 1: 'missing_rate'})\nmissing_table[missing_table['missing_num']!=0]","f6f1b421":"# test missing\ndf_test=pd.read_csv(data_path+\"test.csv\")\nnull_val = df_test.isnull().sum()\npercent = 100 * df_test.isnull().sum() \/ len(df)\nmissing_table = pd.concat([null_val, percent], axis=1)\nmissing_table=missing_table.rename(columns={0: 'missing_num', 1: 'missing_rate'})\nmissing_table[missing_table['missing_num']!=0]","cd1bd1b5":"df.describe()","5e9748de":"sns.distplot(df.SalePrice)","e902a0f2":"df.corr()","4976917d":"# \u6570\u5b57\u30920\u3067\u88dc\u5b8c\ndef comp(df):\n    na_col_list = df.isnull().sum(\n    )[df.isnull().sum() > 0].index.tolist()  # \u6b20\u640d\u3092\u542b\u3080\u30ab\u30e9\u30e0\u3092\u30ea\u30b9\u30c8\u5316\n    na_float_cols = df[na_col_list].dtypes[df[na_col_list].dtypes ==\n                                           'float'].index.tolist()  \n    na_int_cols = df[na_col_list].dtypes[df[na_col_list].dtypes ==\n                                           'int'].index.tolist()\n    for na_float_col in na_float_cols:\n        df.loc[df[na_float_col].isnull(), na_float_col] = 0.0\n    for na_int_col in na_int_cols:\n        df.loc[df[na_int_col].isnull(), na_int_col] = 0\n    return df\ndf=comp(df)\ndf","1c0130cf":"# SalesPrice\u3068\u306e\u76f8\u95a2\u304c\u9ad8\u3044\u9806\u306b\u4e0a\u4f4d\u306e\u30c7\u30fc\u30bf\u3092\u8868\u793a\nnum = 60\ncolumn=\"SalePrice\"\ndf = df.select_dtypes(include=[int, float])\ncorrmat = df.corr()\ncols = corrmat.abs().nlargest(num, column)[column].index\nprint(cols)\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.5)\nfig, ax = plt.subplots(figsize=(num, num))\nax.set_ylim(len(cm), 0)\nsns.heatmap(cm, cbar=True, annot=True, square=True, ax=ax, fmt='.2f', \n            annot_kws={'size': 15}, yticklabels=cols.values, xticklabels=cols.values)","9b585181":"def zscore_normalization(df, column, mean, std):\n    '''\n    Zscore normalization column data with argment(mean,std)\n    '''\n    def zscore(x): return (x - mean) \/ std\n    df[column] = df[column].map(zscore)\n    # all element 0\n    if std == 0:\n        df[column] = 0\n    return df\n\n\ndef zscore_normalization_describe(df, df_describe):\n    '''\n    Zscore normalization df with df_describe(mean,std)\n    '''\n    columns = df.columns.tolist()\n    for c in columns:\n        df = zscore_normalization(\n            df, c, df_describe.at[\"mean\", c], df_describe.at[\"std\", c])\n    return df","1f368cb8":"# train data set\n# \u6570\u5b57\u306e\u307f\u6b8b\u3059\ndf_train=df.select_dtypes(include=['float', 'int'])\n\n# \u6b63\u89e3\u30c7\u30fc\u30bf\ndf_teach=df_train['SalePrice']\n\n# \u76f8\u95a2\u4fc2\u65700.4\u4ee5\u4e0a\u306ecol\u3092\u4f7f\u7528\n# cols=['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n#        'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n#        'YearRemodAdd', 'MasVnrArea', 'Fireplaces']\n# df_train=df_train.loc[:,cols]\n\n# Id\u4ee5\u4e0b\u306e\u76f8\u95a2\u306e\u5217\u524a\u9664\ncols=['Id', 'MiscVal', 'BsmtHalfBath', 'BsmtFinSF2']\ndf_train=df_train.drop(cols,axis=1)\n\n# 0\u3067\u88dc\u5b8c\ndf_train=comp(df_train)\n\n#train\u3067\u6a19\u6e96\u5316\ndf_train_describe=df_train.describe()\ndf_train=zscore_normalization_describe(df_train, df_train_describe)\n\n# \u6b63\u89e3\u9023\u7d50\ndf_train['SalePrice']=df_teach\n\ndf_train.to_csv(\"train_dataset.csv\",index=False)\ndf_train","436d8c65":"# test dataset\n\ndf_test=pd.read_csv(data_path+\"test.csv\")\n\ndf_test=df_test.select_dtypes(include=['float', 'int'])\n# cols=['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n#        'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n#        'YearRemodAdd', 'MasVnrArea', 'Fireplaces']\n# df_test=df_test.loc[:,cols]\n\n# Id\u4ee5\u4e0b\u306e\u76f8\u95a2\u306e\u5217\u524a\u9664\ncols=['Id', 'MiscVal', 'BsmtHalfBath', 'BsmtFinSF2']\ndf_test=df_test.drop(cols,axis=1)\n\n# \u6b20\u640d\u88dc\u5b8c\ndf_test=comp(df_test)\n\n#train\u3067\u6a19\u6e96\u5316\ndf_test=zscore_normalization_describe(df_test, df_train_describe)\n\ndf_test.to_csv(\"test_dataset.csv\",index=False)\ndf_test","982b0956":"* 150000$\u4f4d\u306e\u5272\u5408\u304c\u6700\u3082\u591a\u3044\n* \u6975\u7aef\u306b\u9ad8\u3044\u91d1\u984d\u306e\u3082\u306e\u304c\u3042\u308b","7e2cb3cb":"Predict Local NeuralNet "}}