{"cell_type":{"9db805e7":"code","fcb8d4d9":"code","b7960fd5":"code","8baff3d6":"code","b71974d6":"code","14355d22":"code","f406df23":"code","0c16263e":"code","8d44751c":"code","97e2fb5f":"code","aa3bff64":"code","2ef00913":"code","3273d1e9":"code","d8f6ee9a":"code","14a14fb4":"code","55e3eb5e":"code","fff928bf":"code","4166f802":"code","e40caf7d":"code","f826e34c":"code","e7cbbdf0":"code","eed72265":"code","d901ab98":"code","eebfe463":"code","471a9516":"code","432ca9c5":"code","09df11ca":"markdown","1c5791ce":"markdown","18f99c84":"markdown","65afde77":"markdown"},"source":{"9db805e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fcb8d4d9":"#libraries\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport re\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression","b7960fd5":"data=pd.read_csv('\/kaggle\/input\/emotions-in-text\/Emotion_final.csv')","8baff3d6":"data.head()","b71974d6":"#checking for null velue\ndata.isnull().sum()","14355d22":"plt.figure(figsize = (20,7))\n#sns.set_theme(style=\"darkgrid\")\n\nax = sns.countplot(x=\"Emotion\", data=data, palette=\"Set3\", dodge=False)","f406df23":"data.Emotion = pd.Categorical(data.Emotion)\ndata['Emotion_categorical'] = data.Emotion.cat.codes\ndata.head()","0c16263e":"plt.figure(figsize = (20,7))\n#sns.set_theme(style=\"darkgrid\")\n\nax = sns.countplot(x=\"Emotion_categorical\", data=data, palette=\"Set3\", dodge=False)","8d44751c":"# cleaning data \ndef cleantext(data):\n    \n    data = re.sub(r'@[A-Za-z0-9]+', '', data) # remove @mentions\n    data = re.sub(r'#', '', data)# remove # tag\n    data = re.sub(r'RT[\\s]+', '', data) # remove the RT\n    data = re.sub(r'https?:\\\/\\\/\\S+', '', data) # remove links\n    data = re.sub('(\\\\\\\\u([a-z]|[0-9])+)', ' ', data) # remove unicode characters\n    data = re.sub(r'\"', '', data)\n    data = re.sub(r':', '', data)\n    return data","97e2fb5f":"data['Text']=data['Text'].apply(cleantext)","aa3bff64":"#finding most common words in the dataset\n\n\nword= ' '.join([twts for twts in data['Text']])\nclude= WordCloud(width=1000, height=1000, random_state= 21,min_font_size=15,max_font_size=119).generate(word)\nplt.figure(figsize = (15,15))\nplt.imshow(clude,interpolation='bilinear')\nplt.axis('off')\nplt.show()","2ef00913":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(data.Text).toarray()\nlabels = data.Emotion_categorical\nfeatures.shape","3273d1e9":"#for 1st 1o row \ni=0\nN=2\nfor Emotion_categorical, Emotion  in sorted(data.Emotion_categorical.items()):\n  features_chi2 = chi2(features, labels == Emotion_categorical)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  #print(\"# \"+Emotion+\":\")\n  print(\"# '{}':\".format(Emotion))\n  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n  i=i+1\n  if(i==10):\n    break","d8f6ee9a":"import nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nvectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\nX = vectorizer.fit_transform(data['Text']).toarray()","14a14fb4":"tfidfconverter = TfidfTransformer()\nX = tfidfconverter.fit_transform(X).toarray()","55e3eb5e":"X_train, X_test, y_train, y_test = train_test_split(X, data['Emotion'], test_size=0.2, random_state=0)","fff928bf":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=100, random_state=0)\nclassifier.fit(X_train, y_train) ","4166f802":"y_pred_r = classifier.predict(X_test)","e40caf7d":"import sklearn.metrics as metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_r))","f826e34c":"print(classifier.predict(vectorizer.transform([\"my pc is broken but i am really very happy as i got a new pc from my wife \"])))","e7cbbdf0":"lr=LogisticRegression(max_iter=1000, multi_class='multinomial')\nlem=lr.fit(X_train, y_train)","eed72265":"y_pred_l  = lem.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_l))","d901ab98":"print(lem.predict(vectorizer.transform([\"my pc is broken but i am really very happy as i got a new pc from my wife \"])))","eebfe463":"clf = MultinomialNB().fit(X_train, y_train)","471a9516":"y_pred_m  = clf.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_m))","432ca9c5":"#count_vect = Vectorizer()\nprint(clf.predict(vectorizer.transform([\"my pc is broken but i am really very happy as i got a new pc from my wife \"])))","09df11ca":"**piam emrul hasan **","1c5791ce":"> **LogisticRegression******","18f99c84":"**# Multinomial Naive Bayes**","65afde77":"**RandomForestClassifier**"}}