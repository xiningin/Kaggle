{"cell_type":{"e8bbdffd":"code","e8119294":"code","cfc1603c":"code","600afdb3":"code","ba51bfa1":"code","f2c3e6af":"code","cbe9e397":"code","3fbebef4":"code","701f0b48":"code","3525bd43":"code","58e8e72d":"code","a5d03c7a":"code","f3af469b":"code","b1208a29":"code","fdd2d7bb":"code","45b61626":"code","53a237dd":"code","1cb8cc99":"code","3c4caf0d":"code","a3a1c31f":"code","1d5704fd":"code","0cbed75a":"code","7062a6ae":"code","e5a1551b":"code","d2f69fd7":"code","db4d5ce2":"code","a51af387":"code","023f9795":"code","ea207e3d":"code","2cb57ca7":"code","05989204":"code","4281de22":"code","593af0c9":"code","aa872f6f":"code","5a749e1f":"code","e77b81d6":"code","0c91fea6":"code","1ebcb4c4":"code","163cad9d":"code","8c7b8717":"code","b7a80036":"code","cc031e92":"markdown","f8170fed":"markdown","a42e4e0b":"markdown","0e8f575a":"markdown","7f387008":"markdown","35089506":"markdown","f5f05d9c":"markdown","75c4c4c3":"markdown","31c50b6c":"markdown","7850f62d":"markdown","e4298ac5":"markdown","1ef25c01":"markdown","16bd3398":"markdown","e845a627":"markdown","4288d953":"markdown","1ce3f5f5":"markdown","0e84079d":"markdown","168c941f":"markdown","fdf487ca":"markdown","c2c5abb6":"markdown","bc6aa501":"markdown","28a1a310":"markdown","e0856dc6":"markdown","656857fe":"markdown","ccad4277":"markdown","3d8193a5":"markdown"},"source":{"e8bbdffd":"import pandas as pd\nimport numpy as np","e8119294":"import os\npaths = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\npaths.sort()\npaths","cfc1603c":"track_cols = [\"track ID\", \"track title\", \"artist name\"]\ntrack_set = pd.read_table(paths[0], header=None, names=track_cols)","600afdb3":"user_cols = [\"user ID\", \"track ID\", \"listen time\"]\nuser_set = pd.read_table(paths[1], header=None, names=user_cols)","ba51bfa1":"track_set.head()","f2c3e6af":"track_set.shape","cbe9e397":"user_set.head()","3fbebef4":"user_set.shape","701f0b48":"track_set.isnull().sum()","3525bd43":"track_set[\"artist name\"].value_counts().head(15)","58e8e72d":"track_set.shape","a5d03c7a":"(track_set[\"artist name\"].str.find(\"Unknown\") != -1).sum()","f3af469b":"track_set[track_set[\"track title\"] == \"[Unknown]\"]","b1208a29":"track_set = track_set[track_set[\"artist name\"].str.find(\"Unknown\") == -1]","fdd2d7bb":"(track_set[\"track ID\"].str.find(\"Unknown\") != -1).sum()","45b61626":"user_set.isnull().sum()","53a237dd":"((user_set[\"track ID\"].str.find(\"Unknown\") != -1) & (user_set[\"user ID\"].str.find(\"Unknown\") != -1)).sum()","1cb8cc99":"top5_songs = user_set[\"track ID\"].value_counts().iloc[0:5]\ntop5_songs","3c4caf0d":"for i, song in enumerate(top5_songs.index):\n    print(i+1, track_set[track_set[\"track ID\"] == song].values[0][1:3])\n    ","a3a1c31f":"top10_users = user_set[\"user ID\"].value_counts().iloc[0:10]\nfor i, user_id in enumerate(top10_users.index):\n    print(i+1, user_id)","1d5704fd":"# lets count how many times every song appear in user_set\nsong_counts = user_set[\"track ID\"].value_counts()\n\n# create a dataframe from value_counts()\nsong_counts = pd.DataFrame({\"track ID\":song_counts.index,\"count\":song_counts.values})\nsong_counts","0cbed75a":"# I will store all artist and informations about them in dictionary. Key is the artist name, value is a list: list[0] tells us how many users listened to artist's songs,\n# list[1] is the artist's song list\n# Created dictionary with following template : {artist_name: [number_of_songs_plays, [songs_titles]]}\nartist_dict = {artist : [0, list] for artist in track_set[\"artist name\"].value_counts().index.values}","7062a6ae":"for i, (artist, informations) in enumerate(artist_dict.items()):\n    \n    # first I collect all songs of current artist from track_set and store it in artist_dict\n    # this information will be helpfull in the next task\n    artist_dict[artist][1] = track_set[track_set[\"artist name\"] == artist][\"track ID\"].tolist()\n    \n    # I need info how many times every song appears, i take it from song_counts, I store this value in list and make a sum of this values\n    artist_dict[artist][0] = sum(song_counts[song_counts[\"track ID\"].isin(artist_dict[artist][1])][\"count\"].tolist())\n    \n    # I wrote a blockade because counting all artists takes too long :(\n    if i == 100:\n        break\n\n# I need to sort artist_dict by value\nsorted_artists = {key: value for key, value in sorted(artist_dict.items(), reverse=True, key=lambda item: item[1][0])}\n\n# Printing result\nfor i, (k, v) in enumerate(sorted_artists.items()):\n    if i == 5: break\n    print(i+1, k, v[0])","e5a1551b":"# I should take the Pink Floyd song list from the previous task but since I couldn't count all I will do this especially for Pink Floyd just in caset \nartist = \"Pink Floyd\"\nartist_dict[artist][1] = track_set[track_set[\"artist name\"] == artist][\"track ID\"].tolist()\nartist_dict[artist][0] = sum(song_counts[song_counts[\"track ID\"].isin(artist_dict[artist][1])][\"count\"].tolist())\n    \nsongs_from_Pink_Floyd = artist_dict[\"Pink Floyd\"]","d2f69fd7":"# To find 3 most listened songs I will borrow line of code from task 6 \ntop_3 = song_counts[song_counts[\"track ID\"].isin(artist_dict[artist][1])][\"track ID\"].iloc[0:3].values.tolist()\ntop_3","db4d5ce2":"# lets search by users who listened even 1 of top3 songs of Pink Floyd\nusers_listened_to_top3 = user_set[user_set['track ID'].isin(top_3)]\nusers_listened_to_top3","a51af387":"users = user_set['user ID'].unique()","023f9795":"boolean_table=[]\nfor user in users:\n    \n    # I return true if user listened to all three songs, false otherwise\n    boolean_table.append(len(users_listened_to_top3.loc[users_listened_to_top3['user ID'] == user, \"track ID\"].unique()) == 3)\n    \n# then return filtered list\nusers_listened_to_top_3 = users[boolean_table]","ea207e3d":"users_listened_to_top_3.size","2cb57ca7":"import pandas as pd\nimport numpy as np\nimport sklearn \nimport matplotlib.pyplot as plt","05989204":"user_events = user_set[user_set['user ID'] == '0478c8abd9327b47848aa71c46112192'].sort_values('listen time')\nuser_events","4281de22":"user_events[\"shifted time\"] = user_events[\"listen time\"].shift(-1)\nuser_events[\"duration time\"] = user_events[\"shifted time\"] - user_events[\"listen time\"]\nuser_events","593af0c9":"user_events = user_events[user_events['duration time'] < 60*60]","aa872f6f":"user_events.drop(user_events.loc[:, ['listen time', 'shifted time']], axis=1, inplace=True)\nuser_events","5a749e1f":"All_I_Need_ID = track_set.loc[track_set['track title'] == 'All I Need', 'track ID'].values[0]\ntime_duration = user_events.loc[user_events[\"track ID\"] == All_I_Need_ID, 'duration time']\nAll_I_Need_ID","e77b81d6":"n, bins, _ = plt.hist(time_duration, bins=20)","0c91fea6":"max_n = max(n)\nmax_n_index = np.where(n==max_n)[0][0]\nduration_time = (bins[max_n_index] + bins[max_n_index+1]) \/ 2","1ebcb4c4":"from time import gmtime\nfrom time import strftime\nstrftime(\"%H:%M:%S\", gmtime(duration_time))","163cad9d":"listen_sorted = user_set.copy()\nlisten_sorted.sort_values(\"listen time\", inplace=True)\nlisten_sorted","8c7b8717":"samples = []\nfor i, us in enumerate(users):\n    # I create condition table which will be userd to search and drop\n    condition = listen_sorted['user ID'] == us\n    \n    # same as counting for one user\n    u_events = listen_sorted.loc[condition]\n    u_events[\"shifted time\"] = u_events[\"listen time\"].shift(-1)\n    u_events[\"duration time\"] = u_events[\"shifted time\"] - u_events[\"listen time\"]\n    samples = np.concatenate((samples ,u_events.loc[(u_events[\"track ID\"] == All_I_Need_ID) & (u_events['duration time'] < 60*60), 'duration time'].values))\n    \n    # I remove user from the searched dataset because I will no longer need him and it. This reduces the search set.\n    listen_sorted = listen_sorted[~condition]\n    if i == 5:\n        break\n\nprint(samples)","b7a80036":"n, bins = np.histogram(samples, bins = 200)\nmax_n = max(n)\nmax_n_index = np.where(n==max_n)[0][0]\nduration_time = (bins[max_n_index] + bins[max_n_index+1]) \/ 2\nstrftime(\"%H:%M:%S\", gmtime(duration_time))","cc031e92":"Form Spotify we can find that duration of \"All I Need\" from Radiohead is 3 minutes and 49 seconds. Find out if we can retreive this information from dataset. \n\nA1. First get all events from user with id `0478c8abd9327b47848aa71c46112192`.","f8170fed":"**User which have listened to most number of unique songs means how many times user appears in user_set** ","a42e4e0b":"### track_set","0e8f575a":"Data from one user tells that one song is around 3min 47sec","7f387008":"### user_set","35089506":"4. Find 5 most listened songs.","f5f05d9c":"5. Find 10 users which have listened to most number of unique songs.","75c4c4c3":"2. Check if data has been loaded properly.","31c50b6c":"**the more bins we set the better result we get**","7850f62d":"Below there are two tasks `A` and `B`. Choose and __do only one__ of these.   \nFirst is related with extracting information about particular song duration.  \nIn second you will be asked to make visualization of top100 artist from dataset.  \nIn cell below we imported modules which we advise to use. Of course, you can use different libraries that you are familliar with.","e4298ac5":"A4. Draw a chart with durations of \"All I Need\" song. Choose chart type which will help us find correct song duration.","1ef25c01":"6. Find 5 most listened artists according to dataset.","16bd3398":"7. Count all users that have listened to __all__ 3 most popular song from `Pink Floyd`.","e845a627":"**First, I sort user_set, then I will the same with all users. It involves many iterations. I did a blockade again because for 10 users it takes around 1.5 min. Then I just make histogram of samples and choose most frequent value.**","4288d953":"Download this notebook with outputs and send to us.","1ce3f5f5":"**Seems we have complete data**","0e84079d":"**It is possible but I cant figure out how to do it faster. We can retrive how many times user lesiten to one song, what is the total time user spend on listening songs.**","168c941f":"**I decide not to touch the song title because we have all the song IDs, and it's the song title in some way. I removed the Unknown from artist name because it creates an \"Unknown Artist\" and match +1500 songs with him which is not true.**","fdf487ca":"A5. For this time we only used data from one user. Can you do the same reseach with whole dataset? How long \"All I Need\" song is? You don't have to care about removing last single users event duration as it will not affect results.\n\nAlso instead of drawing chart use [np.histogram](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.histogram.html) function to find \"All I Need\" duration.","c2c5abb6":"A6. Write small summary if it worked and what other ideas about inforamtions can we retrive from events time.","bc6aa501":"### Task A","28a1a310":"**Most listened song means: how many users listen to this song, it means: how many times this song apear in user_set** ","e0856dc6":"3. Deal with missing data if any.","656857fe":"A3. Discard all values longer than 60 minutes as we can assume that song is for sure shorter.","ccad4277":"A2. Most often users are listening song one after one and rarely are skipping songs. Can you calculate time between listening songs for this user. Most often this will exact duration of song? \n\nTip: Use [shift](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.shift.html)","3d8193a5":"Files which you have added contain dataset from LastFm service. You are provided with two files:\n* track_title_artist.txt -  informations about tracks: track ID, track title and artist name,\n* user_track_time.txt - data about service usage: user ID, track ID, listen time in unix timestamp format.  \n\n\n1. Load datasets from `kaggle\/input` directory."}}