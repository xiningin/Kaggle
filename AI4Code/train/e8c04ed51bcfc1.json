{"cell_type":{"48a75846":"code","b9fd90ae":"code","33c2e6cb":"code","dc7f5ca8":"code","a6a6269b":"code","d3c30893":"code","5b748be8":"code","52fe1f33":"code","73958b10":"code","630a8af6":"code","827f7d62":"code","e06cbc47":"code","1cb7f958":"code","6d8d92ba":"code","94f2e43b":"code","1ee37f6b":"code","73e84597":"code","d7dd6272":"code","eab24707":"code","86e7baf8":"code","664de960":"code","104d3751":"code","f7cce454":"code","aea07d95":"code","4df0c8d0":"code","d3515a82":"code","ab7f3ce4":"code","f57658a1":"code","87858c03":"code","1c7ebb43":"code","69c5f2c0":"code","594d0eb6":"code","d118427c":"code","038449c2":"code","5fce4742":"code","a008a681":"code","bc37d502":"code","bfb6b596":"code","8898fdc5":"code","ed9354f2":"code","9d7f88e5":"code","93f68518":"code","767ffad0":"code","eac6e081":"code","2d75a8b6":"code","c4a4d82d":"code","22a06115":"code","967d4b30":"code","e83a9677":"code","b2ff0373":"code","7194a153":"code","fe5c6156":"code","691407d9":"code","1a3dddd3":"code","9db00021":"code","ffa2a37a":"markdown","45f18841":"markdown","aef582e0":"markdown","64bfa8a9":"markdown","b8e364bb":"markdown","3b489d39":"markdown","db814589":"markdown","b02b3dcb":"markdown","253d8dad":"markdown","2bcf2cdf":"markdown","3b251815":"markdown","89b90c55":"markdown","56cf085b":"markdown","faf18199":"markdown","a02b3d77":"markdown","67d216b3":"markdown","e1a4ee77":"markdown","574d7938":"markdown","763ffa22":"markdown","e6d08e02":"markdown"},"source":{"48a75846":"!pip install lightgbm\n!pip install xgboost","b9fd90ae":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport lightgbm as lgb\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","33c2e6cb":"train = pd.read_csv('..\/input\/taxi-pricing-with-mobility-analytics\/sigma_cabs.csv')\ntrain.head()","dc7f5ca8":"train.describe()","a6a6269b":"train.shape","d3c30893":"train.info()","5b748be8":"train.isnull().sum()","52fe1f33":"train.replace(r'^\\s*$', np.nan, regex=True)","73958b10":"train['Type_of_Cab'] = train['Type_of_Cab'].fillna(train['Type_of_Cab'].mode()[0])\n\ntrain['Customer_Since_Months'] = train['Customer_Since_Months'].fillna(train['Customer_Since_Months'].mean())\n\ntrain['Life_Style_Index'] = train['Life_Style_Index'].fillna(train['Life_Style_Index'].mean())\n\ntrain['Confidence_Life_Style_Index'] = train['Confidence_Life_Style_Index'].fillna(train['Confidence_Life_Style_Index'].mode()[0])\n\ntrain['Var1'] = train['Var1'].fillna(train['Var1'].mean())","630a8af6":"train.isnull().sum()","827f7d62":"train = train.drop(['Trip_ID'], axis = 1)","e06cbc47":"train.Destination_Type.unique()","1cb7f958":"cleanup_nums = {\"Type_of_Cab\": {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5},\n                \"Confidence_Life_Style_Index\": {\"A\": 1, \"B\": 2, \"C\": 3},\n                \"Destination_Type\": {'A': 1, 'E': 5, 'B': 2, 'C': 3, 'G': 7, 'D': 4, 'F': 6, 'K': 11, 'L': 12, 'H': 8, 'I': 9, 'J': 10, 'M': 13,'N': 14},\n                \"Gender\" :{'Male': 1, \"Female\": 2}}","6d8d92ba":"train = train.replace(cleanup_nums)","94f2e43b":"train.info()","1ee37f6b":"train.head()","73e84597":"# import seaborn as sns\n# sns.pairplot(data =train, hue = 'Surge_Pricing_Type')\n# plt.show()","d7dd6272":"train.plot()","eab24707":"train.groupby('Surge_Pricing_Type').count().plot(kind='bar')","86e7baf8":"train.hist(bins = 20, figsize = (15, 10))","664de960":"#sns.pairplot(train, hue = 'Surge_Pricing_Type')","104d3751":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(12,10))\nsns.heatmap(train.corr(), annot = True, cmap = \"YlGnBu\")\nplt.show()","f7cce454":"import matplotlib.pyplot as plt\n\nplt.matshow(train.corr())\nplt.show()","aea07d95":"X = train.iloc[:, 0:12].values\ny = train.iloc[:, 12].values\n\nX_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.2,random_state=0)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","4df0c8d0":"from sklearn.preprocessing import StandardScaler\nsc_X= StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)","d3515a82":"#Decision Tree\n\nmodel_tree = DecisionTreeClassifier(random_state=0)\nmodel_tree.fit(X_train, y_train)","ab7f3ce4":"Y_pred1 = model_tree.predict(X_test)\n\nprint(confusion_matrix(y_test, Y_pred1))","f57658a1":"print(accuracy_score(y_test, Y_pred1))","87858c03":"print(classification_report(y_test, Y_pred1))","1c7ebb43":"model_linear = LogisticRegression()\nmodel_linear.fit(X_train, y_train)","69c5f2c0":"Y_pred2 = model_linear.predict(X_test)\n\nprint(confusion_matrix(y_test, Y_pred2))","594d0eb6":"print(accuracy_score(y_test, Y_pred2))","d118427c":"print(classification_report(y_test, Y_pred2))","038449c2":"model_random = RandomForestClassifier(random_state=0)\nmodel_random.fit(X_train, y_train)","5fce4742":"Y_pred3 = model_random.predict(X_test)\n\nprint(confusion_matrix(y_test, Y_pred3))","a008a681":"print(accuracy_score(y_test, Y_pred3))","bc37d502":"print(classification_report(y_test, Y_pred3))","bfb6b596":"model_knn = KNeighborsClassifier(n_neighbors=3,metric='minkowski',p=2)\nmodel_knn.fit(X_train, y_train)","8898fdc5":"Y_pred4 = model_knn.predict(X_test)\n\nprint(confusion_matrix(y_test, Y_pred4))","ed9354f2":"print(accuracy_score(y_test, Y_pred4))","9d7f88e5":"print(classification_report(y_test, Y_pred4))","93f68518":"model_nb = GaussianNB()\nmodel_nb.fit(X_train,y_train)","767ffad0":"Y_pred5 = model_nb.predict(X_test)\n\nprint(confusion_matrix(y_test, Y_pred5))","eac6e081":"print(accuracy_score(y_test, Y_pred5))","2d75a8b6":"print(classification_report(y_test, Y_pred5))","c4a4d82d":"xg_reg = GradientBoostingClassifier()","22a06115":"xg_reg.fit(X_train,y_train)\n\nY_pred6 = xg_reg.predict(X_test)\nprint(confusion_matrix(y_test, Y_pred6))","967d4b30":"print(accuracy_score(y_test, Y_pred6))","e83a9677":"print(classification_report(y_test, Y_pred6))","b2ff0373":"clf = lgb.LGBMClassifier()\nclf.fit(X_train, y_train)","7194a153":"Y_pred7 = clf.predict(X_test)\n\nprint(confusion_matrix(y_test, Y_pred7))","fe5c6156":"print(accuracy_score(y_test, Y_pred7))","691407d9":"print(classification_report(y_test, Y_pred7))","1a3dddd3":"result = {'Model Name':['Decision Tree', 'Logistic Regression', 'Random Forest', 'KNN', 'Naive Bayes', 'XGB', 'LGBM'], \n          'Accuracy Score': [0.5667413511563437*100, 0.6380207344396764*100, 0.685679565564121*100, 0.5788174533854858*100, 0.6469069228724414*100, 0.688983404853226*100, 0.6936163748908214*100]}","9db00021":"res = pd.DataFrame.from_dict(result)\nres","ffa2a37a":"## Logistic Regression","45f18841":"## LGBM Classifier","aef582e0":"# Data Preprocessing","64bfa8a9":"## KNN","b8e364bb":"## XGB Boost","3b489d39":"# Heatmap","db814589":"##### Since the Type_of_cab have empty columns we will replace them by the mode of the Type_of_cab column.\n\n##### Since the Customer_Since_Months have empty columns we will replace them by the mean of the Customer_Since_Months column.\n\n##### Since the Life_Style_Index have empty columns we will replace them by the mean of the Life_Style_Index column.\n\n##### Since the Confidence_Life_Style_Index have empty columns we will replace them by the mean of the Confidence_Life_Style_Index column.\n\n##### Since the Var1 have empty columns we will replace them by the mean of the Var1 column.","b02b3dcb":"# Null Values","253d8dad":"# Visualizing Data","2bcf2cdf":"# Importing Libraries","3b251815":"## Decision Tree","89b90c55":"Model Building\n\n    --> Decision Tree\n    --> Logistic Regression\n    --> Random Forest\n    --> KNN\n    --> Naive Bayes\n    --> XGB\n    --> LGBM","56cf085b":"## Naive Bayes","faf18199":"# Installing Dependencies","a02b3d77":"# Importing the Dataset","67d216b3":"## Random Forest Classifier","e1a4ee77":"## Converting Categorical values to Numerical values","574d7938":"# Conclusion\n\n## Hence, LGBM gives the Highest Accuracy Score","763ffa22":"# Model Building","e6d08e02":"# Histogram"}}