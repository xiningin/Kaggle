{"cell_type":{"b236ae2a":"code","511fb16c":"code","5fd1baba":"code","9b07d911":"code","6014af2f":"code","6136561e":"code","c09bf4c5":"code","44f0e911":"code","061d9501":"code","dcb9d0ed":"code","2d98c907":"code","3a7c22f4":"code","ebefb354":"code","5f2291ee":"code","a53cf8e8":"code","e94ceeb1":"code","8d0b8fb4":"code","83338f0a":"code","f0123945":"code","d34a47d8":"code","d1b4c3d7":"code","95ad13ae":"code","4c70e238":"code","1083473c":"code","39d477ad":"code","5c44a30f":"code","0b04fef9":"code","250dde02":"code","27418ded":"code","9ab0e12f":"code","56a34da4":"code","6665dac9":"code","85491e79":"code","2073ea6d":"code","f2bf5b79":"code","80375c17":"code","cbc8bfc7":"code","7300135f":"code","1351cbc9":"code","dd305eae":"code","f46b7feb":"code","0ff2a64f":"code","85ad93a4":"code","c4f06bcc":"code","16bcc34e":"code","9329aa74":"markdown","28875eb1":"markdown","c290b06a":"markdown","350385be":"markdown"},"source":{"b236ae2a":"! pip install seqeval","511fb16c":"! pip install pytorch-transformers","5fd1baba":"import pandas as pd\nimport math\nimport numpy as np\nfrom seqeval.metrics import f1_score\nfrom seqeval.metrics import classification_report,accuracy_score,f1_score\nimport torch.nn.functional as F\nimport torch\nimport os\nfrom tqdm import tqdm,trange\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_transformers import BertTokenizer, BertConfig\nfrom pytorch_transformers import BertForTokenClassification, AdamW","9b07d911":"! ls ..\/input","6014af2f":"df_data = pd.read_csv(\"..\/input\/ner_dataset.csv\",sep=\",\",encoding=\"latin1\").fillna(method='ffill')\ndf_data.head()","6136561e":"df_data.POS.unique()\n","c09bf4c5":"df_data.Tag.value_counts()","44f0e911":"class SentenceGetter(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","061d9501":"getter = SentenceGetter(df_data)\nsentences = [[s[0] for s in sent] for sent in getter.sentences]\nprint(sentences[0])","dcb9d0ed":"poses = [[s[1] for s in sent] for sent in getter.sentences]\nlabels = [[s[2] for s in sent] for sent in getter.sentences]\n\nprint (sentences[0])\nprint (poses[0])\nprint(labels[0])","2d98c907":"tags_vals = list(set(df_data[\"Tag\"].values))\n","3a7c22f4":"# Add X  label for word piece support\n# Add [CLS] and [SEP] as BERT need\ntags_vals.append('X')\ntags_vals.append('[CLS]')\ntags_vals.append('[SEP]')\ntags_vals = set(tags_vals)\ntags_vals","ebefb354":"# Set a dict for mapping id to tag name\n#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n\n# Recommend to set it by manual define, good for reusing\ntag2idx={'B-art': 14,\n 'B-eve': 16,\n 'B-geo': 0,\n 'B-gpe': 13,\n 'B-nat': 12,\n 'B-org': 10,\n 'B-per': 4,\n 'B-tim': 2,\n 'I-art': 5,\n 'I-eve': 7,\n 'I-geo': 15,\n 'I-gpe': 8,\n 'I-nat': 11,\n 'I-org': 3,\n 'I-per': 6,\n 'I-tim': 1,\n 'X':17,\n 'O': 9,\n '[CLS]':18,\n '[SEP]':19}","5f2291ee":"tag2idx\n","a53cf8e8":"# Mapping index to name\ntag2name={tag2idx[key] : key for key in tag2idx.keys()}\n","e94ceeb1":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","8d0b8fb4":"n_gpu","83338f0a":"# Len of the sentence must be the same as the training model\n# See model's 'max_position_embeddings' = 512\nmax_len  = 45\n# load tokenizer, with manual file address or pretrained address\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","f0123945":"tokenized_texts = []\nword_piece_labels = []\ni_inc = 0\nfor word_list,label in (zip(sentences,labels)):\n    temp_lable = []\n    temp_token = []\n    \n    # Add [CLS] at the front \n    temp_lable.append('[CLS]')\n    temp_token.append('[CLS]')\n    \n    for word,lab in zip(word_list,label):\n        token_list = tokenizer.tokenize(word)\n        for m,token in enumerate(token_list):\n            temp_token.append(token)\n            if m==0:\n                temp_lable.append(lab)\n            else:\n                temp_lable.append('X')  \n                \n    # Add [SEP] at the end\n    temp_lable.append('[SEP]')\n    temp_token.append('[SEP]')\n    \n    tokenized_texts.append(temp_token)\n    word_piece_labels.append(temp_lable)\n    \n    if 5 > i_inc:\n        print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n        print(\"texts:%s\"%(\" \".join(temp_token)))\n        print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n        print(\"lables:%s\"%(\" \".join(temp_lable)))\n    i_inc +=1","d34a47d8":"# Make text token into id\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\nprint(input_ids[0])\n","d1b4c3d7":"# Make label into id, pad with \"O\" meaning others\ntags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n                     maxlen=max_len, value=tag2idx[\"O\"], padding=\"post\",\n                     dtype=\"long\", truncating=\"post\")\nprint(tags[0])","95ad13ae":"# For fine tune of predict, with token mask is 1,pad token is 0\nattention_masks = [[int(i>0) for i in ii] for ii in input_ids]\nattention_masks[0];","4c70e238":"# Since only one sentence, all the segment set to 0\nsegment_ids = [[0] * len(input_id) for input_id in input_ids]\nsegment_ids[0];","1083473c":"\ntr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(input_ids, tags,attention_masks,segment_ids, \n                                                            random_state=4, test_size=0.3)","39d477ad":"tr_inputs = torch.tensor(tr_inputs)\nval_inputs = torch.tensor(val_inputs)\ntr_tags = torch.tensor(tr_tags)\nval_tags = torch.tensor(val_tags)\ntr_masks = torch.tensor(tr_masks)\nval_masks = torch.tensor(val_masks)\ntr_segs = torch.tensor(tr_segs)\nval_segs = torch.tensor(val_segs)","5c44a30f":"batch_num = 32\n","0b04fef9":"# Only set token embedding, attention embedding, no segment embedding\ntrain_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\ntrain_sampler = RandomSampler(train_data)\n# Drop last can make batch training better for the last one\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)","250dde02":"model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\",num_labels=len(tag2idx))\n","27418ded":"model;\n","9ab0e12f":"model.cuda();\n","56a34da4":"# Add multi GPU support\nif n_gpu >1:\n    model = torch.nn.DataParallel(model)\n","6665dac9":"# Set epoch and grad max num\nepochs = 5\nmax_grad_norm = 1.0","85491e79":"# Cacluate train optimiazaion num\nnum_train_optimization_steps = int( math.ceil(len(tr_inputs) \/ batch_num) \/ 1) * epochs","2073ea6d":"# True: fine tuning all the layers \n# False: only fine tuning the classifier layers\nFULL_FINETUNING = True","f2bf5b79":"if FULL_FINETUNING:\n    # Fine tune model all layer parameters\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0}\n    ]\nelse:\n    # Only fine tune classifier parameters\n    param_optimizer = list(model.classifier.named_parameters()) \n    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\noptimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)","80375c17":"# TRAIN loop\nmodel.train();","cbc8bfc7":"print(\"***** Running training *****\")\nprint(\"  Num examples = %d\"%(len(tr_inputs)))\nprint(\"  Batch size = %d\"%(batch_num))\nprint(\"  Num steps = %d\"%(num_train_optimization_steps))\nfor _ in trange(epochs,desc=\"Epoch\"):\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(train_dataloader):\n        # add batch to gpu\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # forward pass\n        outputs = model(b_input_ids, token_type_ids=None,\n        attention_mask=b_input_mask, labels=b_labels)\n        loss, scores = outputs[:2]\n        if n_gpu>1:\n            # When multi gpu, average it\n            loss = loss.mean()\n        \n        # backward pass\n        loss.backward()\n        \n        # track train loss\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        \n        # update parameters\n        optimizer.step()\n        optimizer.zero_grad()\n        \n    # print train loss per epoch\n    print(\"Train loss: {}\".format(tr_loss\/nb_tr_steps))","7300135f":"bert_out_address = 'models\/bert_out_model\/en09'\n# Make dir if not exits\nif not os.path.exists(bert_out_address):\n        os.makedirs(bert_out_address)","1351cbc9":"model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","dd305eae":"# If we save using the predefined names, we can load using `from_pretrained`\noutput_model_file = os.path.join(bert_out_address, \"pytorch_model.bin\")\noutput_config_file = os.path.join(bert_out_address, \"config.json\")\n\n# Save model into file\ntorch.save(model_to_save.state_dict(), output_model_file)\nmodel_to_save.config.to_json_file(output_config_file)\ntokenizer.save_vocabulary(bert_out_address)","f46b7feb":"model = BertForTokenClassification.from_pretrained(bert_out_address,num_labels=len(tag2idx))\n\nmodel.cuda(); # Set model to GPU\n\nif n_gpu >1:\n    model = torch.nn.DataParallel(model)\n","0ff2a64f":"model.eval();\n","85ad93a4":"eval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\ny_true = []\ny_pred = []\n\nprint(\"***** Running evaluation *****\")\nprint(\"  Num examples ={}\".format(len(val_inputs)))\nprint(\"  Batch size = {}\".format(batch_num))\nfor step, batch in enumerate(valid_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    input_ids, input_mask, label_ids = batch\n    \n#     if step > 2:\n#         break\n    \n    with torch.no_grad():\n        outputs = model(input_ids, token_type_ids=None,\n        attention_mask=input_mask,)\n        # For eval mode, the first result of outputs is logits\n        logits = outputs[0] \n    \n    # Get NER predict result\n    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n    logits = logits.detach().cpu().numpy()\n    \n    \n    # Get NER true result\n    label_ids = label_ids.to('cpu').numpy()\n    \n    \n    # Only predict the real word, mark=0, will not calculate\n    input_mask = input_mask.to('cpu').numpy()\n    \n    # Compare the valuable predict result\n    for i,mask in enumerate(input_mask):\n        # Real one\n        temp_1 = []\n        # Predict one\n        temp_2 = []\n        \n        for j, m in enumerate(mask):\n            # Mark=0, meaning its a pad word, dont compare\n            if m:\n                if tag2name[label_ids[i][j]] != \"X\" and tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n                    temp_1.append(tag2name[label_ids[i][j]])\n                    temp_2.append(tag2name[logits[i][j]])\n            else:\n                break\n        \n            \n        y_true.append(temp_1)\n        y_pred.append(temp_2)\n\n        \n\nprint(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\nprint(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n\n# Get acc , recall, F1 result report\nreport = classification_report(y_true, y_pred,digits=4)\n\n# Save the report into file\noutput_eval_file = os.path.join(bert_out_address, \"eval_results.txt\")\nwith open(output_eval_file, \"w\") as writer:\n    print(\"***** Eval results *****\")\n    print(\"\\n%s\"%(report))\n    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n    \n    writer.write(\"f1 socre:\\n\")\n    writer.write(str(f1_score(y_true, y_pred)))\n    writer.write(\"\\n\\nAccuracy score:\\n\")\n    writer.write(str(accuracy_score(y_true, y_pred)))\n    writer.write(\"\\n\\n\")  \n    writer.write(report)","c4f06bcc":"import nltk\nsentences = \"I bought a new iphone and its Iphone X from Apple\"\nword_tokens = nltk.word_tokenize(sentences)\npos_tags = nltk.pos_tag(word_tokens)\ntokenized_texts = []\nword_piece_labels = []\ni_inc = 0\ntemp_token = []\n# Add [CLS] at the front \ntemp_token.append('[CLS]')\nfor word,lab in pos_tags:\n    token_list = tokenizer.tokenize(word)\n    for m,token in enumerate(token_list):\n        temp_token.append(token)\n# Add [SEP] at the end\ntemp_token.append('[SEP]')\ntokenized_texts.append(temp_token)\nprint(\"texts:%s\"%(\" \".join(temp_token)))\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\nprint(input_ids[0])\nb_input_mask = \"\"","16bcc34e":"# source: https:\/\/github.com\/billpku\/NLP_In_Action\/blob\/master\/NER_with_BERT.ipynb","9329aa74":"The process of doing NER with BERT contains 4 steps:\n1. Load data\n2. Set data into training embeddings\n3. Train model\n4. Evaluate model performance","28875eb1":"f1 socre: 0.816981\n\nAccuracy score: 0.967118","c290b06a":"**TRAIN THE MODEL**","350385be":"**PREDICT**"}}