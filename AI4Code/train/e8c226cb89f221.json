{"cell_type":{"a4ec01da":"code","9847e03b":"code","bb5e9da5":"code","e144d526":"code","6b4f4ae4":"markdown","b7c2b98e":"markdown","31170ec7":"markdown","b0d1762b":"markdown"},"source":{"a4ec01da":"import numpy as np\nimport pandas as pd\nimport json\nimport os\n\ndatafiles = []\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\/CORD-19-research-challenge\/\"):\n    for filename in filenames:\n        ifile = os.path.join(dirname, filename)\n        if ifile.split(\".\")[-1] == \"json\":\n            datafiles.append(ifile)\ndoc_ids = []\ntitles = []\nabstracts = []\nbodytexts = []\nid2title = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    doc_ids.append(doc['paper_id']) \n    titles.append(doc['metadata']['title'])\n    abstract = ''\n    for item in doc['abstract']:\n        abstract = abstract + item['text']\n    abstracts.append(abstract)\n    bodytext = ''\n    for item in doc['body_text']:\n        bodytext = bodytext + item['text']\n    bodytexts.append(bodytext)\ntexts = np.array([t + ' ' + a for t, a in zip(titles, abstracts)], dtype='object')","9847e03b":"from nltk.tokenize import word_tokenize\nfrom gensim.sklearn_api.phrases import PhrasesTransformer\n\nbigrams = PhrasesTransformer(min_count=20, threshold=100)\nprocessed_texts = bigrams.fit_transform([word_tokenize(text) for text in texts])","bb5e9da5":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nvectorizer = CountVectorizer(stop_words=\"english\", lowercase=True, max_df=0.1, min_df=20)\ndtm = vectorizer.fit_transform([\" \".join(text) for text in processed_texts])\nk = 8\nlda = LatentDirichletAllocation(n_components=k, \n                                learning_method=\"batch\", \n                                doc_topic_prior=1\/k,\n                                topic_word_prior=0.1,\n                                n_jobs=-1,\n                                random_state=0)\nlda.fit(dtm)","e144d526":"from pyLDAvis import enable_notebook, display, sklearn\n\nenable_notebook()\ntopic_model_info = sklearn.prepare(lda, dtm, vectorizer, mds='PCoA')\ndisplay(topic_model_info)","6b4f4ae4":"# Analyzing the COVID-19 corpus with LDA and PCoA \n\nIn this notebook, we perform Latent Dirichlet Allocation to infer high-level topics and visualize them <br>\nto get some understanding of the general structure of the CORD19 corpus. <br>\nFollowing the principal coordinate analysis based on the Jensen-Shannon divergence between topics, <br>\nit seems that the two main axes along which the topics are organized are:\n- PC1: To which extent the topic focuses on the propagation of the virus vs. the virus itself;\n- PC2: To which extent the topic is related to some individuals.\n\n## Reading the data","b7c2b98e":"# Fitting LDA\n\nWe fit LDA with k=8 topics, with alpha=1\/k and beta=0.1, using the batch implementation in scikit learn.","31170ec7":"# Preprocessing the text\n\nWe merge 2-word phrases based on the rough approximation of the pointwise mutual information proposed by Mikolov <br>\net al. in \"Mikolov, et. al: \u201cDistributed Representations of Words and Phrases and their Compositionality\".","b0d1762b":"# Visualizing the topics\n\nWe compute a 2D map of the topics by multidimensional scaling, based on the Jensen-Shannon divergence between the <br>\nper topic distributions over words, using the implementation in pyLDAVis.\n\n- PC1: To which extent the topic focuses on the propagation of the virus vs. the virus itself;\n- PC2: To which extent the topic is related to individuals."}}