{"cell_type":{"1e6b316c":"code","fea4e734":"code","b96fec8c":"code","8cbc8423":"code","7120b289":"code","1b111ae2":"code","edfdb50b":"code","16e93d98":"code","829e3cfa":"code","c6829c5d":"code","824f697e":"code","0a0f3cf9":"code","40d3b478":"code","1f2097e4":"code","a2f0a537":"code","7b672d25":"code","583485a9":"code","579aac75":"code","02b74833":"code","7f056a1a":"code","3f69f786":"code","34c9f4ed":"code","baafeea5":"code","71a02d3a":"code","c13d408c":"code","ffd7699f":"code","4c610189":"code","6047907b":"code","ebb896b5":"code","355750b2":"code","082535b3":"code","59dfd1dc":"code","d41e3455":"code","3cb41485":"code","68102606":"code","350b1c6a":"code","32f7308d":"code","d5ef1b1c":"code","505458ac":"code","c2d8d912":"code","62ebdee5":"code","93450b3b":"code","72c6b38b":"code","de76e08d":"code","6c9cdd5c":"code","04783bb7":"code","c0d973f4":"code","5ffa79c4":"code","abce2f58":"code","0f51ac45":"code","d75abeda":"code","3186bf12":"code","ffc828af":"code","8f071a0e":"code","1cdb2fe6":"code","f52db64e":"code","96c0547f":"code","0f0bde35":"code","433005d9":"code","e3f32cb6":"code","19cc6e54":"code","4ef0fb53":"code","8b8fabb8":"code","9d42383d":"code","34f70cab":"code","e078e0cf":"code","52b61d57":"code","87d813b7":"code","bcae2d46":"code","a0c6b7b0":"code","ab21172a":"code","f9f0cfe4":"code","b2fa3083":"code","77423fce":"code","cc6b63cb":"code","9995f6b4":"code","afb41444":"code","8bb3a4bf":"code","aa7244b5":"code","a1156f16":"code","2e108553":"code","0a22fd60":"code","b771c8a0":"code","84a4299b":"code","c3f18919":"code","586b7bb9":"code","5168a151":"code","51212fb7":"code","896d1c84":"code","e4b98377":"code","633d2398":"code","24216d82":"code","83eeed6d":"code","8ec4464b":"code","c99aab6c":"code","f9d85b16":"code","38706255":"code","7ba943ed":"code","e4505887":"code","b9bb7053":"code","283899ca":"code","bcd95d7e":"code","c350f1df":"code","eeaf40da":"code","09595c48":"code","18862bc2":"code","c6d2c2b6":"code","6300041d":"code","60349f60":"code","7bf2c7e4":"code","c3378bdb":"code","48be987a":"code","793dde0a":"code","f36d2e28":"code","e4ff82be":"code","df0d467a":"markdown","cad1373b":"markdown","f6ab0ea8":"markdown","8f5186c7":"markdown","02bc532d":"markdown","ed3e240b":"markdown","5a9bd23e":"markdown","b9f11e11":"markdown","1c7f62e2":"markdown","98f16a16":"markdown","287505b8":"markdown","ee1b0856":"markdown","5de33054":"markdown","4e1eedd6":"markdown","b2c772a5":"markdown","0c937981":"markdown","3c84f31c":"markdown","aed22eea":"markdown","359465ca":"markdown","4aa25b30":"markdown","cf11664e":"markdown","32d609cf":"markdown","b4979165":"markdown","b1d54b7c":"markdown","02e16073":"markdown","e3e2229a":"markdown","40382429":"markdown","5d3194e9":"markdown","71ea1e64":"markdown","5934674a":"markdown","b8710b68":"markdown","29f05a92":"markdown","c234fb44":"markdown","c83e55ea":"markdown","d6c57d68":"markdown","513f0390":"markdown","d992cbe4":"markdown","b1bbb635":"markdown","bf1abee2":"markdown","dbfb864a":"markdown","8c1ac968":"markdown","bd848fbd":"markdown","1b1c7cf1":"markdown","59b102a9":"markdown","417919bb":"markdown","199bc266":"markdown","4c169bc9":"markdown","c1c7d8d7":"markdown","1be8dc77":"markdown","7a221450":"markdown","585587ac":"markdown","cf35f0d8":"markdown","07c5368a":"markdown","b75ba8c7":"markdown","700bd93e":"markdown","b808e451":"markdown","9b585d65":"markdown","77c3cd37":"markdown","ec9eb6d0":"markdown","478d5f30":"markdown","21e9fb31":"markdown","1cc8a892":"markdown","5962f849":"markdown","e2900094":"markdown","3faf9ade":"markdown","e50ee3a4":"markdown","b64cb80d":"markdown","65e31f66":"markdown","e7fd39d6":"markdown","01894ae8":"markdown","6c0e64d6":"markdown","91d42d3b":"markdown","cf159f49":"markdown","aa647e43":"markdown","62ae73a6":"markdown","cd24b2ac":"markdown","5b7f7980":"markdown"},"source":{"1e6b316c":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.base import TransformerMixin\n\nimport os\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","fea4e734":"print(df_train.shape)\ndf_train.head()","b96fec8c":"print(df_test.shape)\ndf_test.head()","8cbc8423":"#Info on our target variable\ndf_train.SalePrice.describe()","7120b289":"# function to check distribution\n\ndef skew_distribution(data, col='SalePrice'):\n    fig, ax1 = plt.subplots()\n    sns.distplot(data[col], ax=ax1, fit=stats.norm)\n    (mu, sigma) = stats.norm.fit(data[col])\n    ax1.set(title='Normal distribution ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma))\n\n    fig, ax2 = plt.subplots()\n    stats.probplot(data[col], plot=plt)\n\n    print('The {} skewness is {:.2f}'.format(col, stats.skew(data[col])))","1b111ae2":"# distribution of the Price and fit of normal distribution\nskew_distribution(df_train, 'SalePrice')","edfdb50b":"df_train['SalePrice'] = np.log1p(df_train['SalePrice'])\n\nskew_distribution(df_train, 'SalePrice')","16e93d98":"#Finding the correlations in numeric features\ncorr = df_train.corr()   # or df_train[num_columns].corr()\ntop_corr_feat = corr['SalePrice'].sort_values(ascending=False)[:25]\nprint(top_corr_feat)","829e3cfa":"# Most correlated variables\nthreshold = 0.51\ntop_corr = corr.index[np.abs(corr[\"SalePrice\"]) > threshold]\n\nplt.figure(figsize=(10,8))\nsns.heatmap(df_train[top_corr].corr(),annot=True,cmap=\"RdBu_r\")","c6829c5d":"# Inspect numeric \/ categorical correlated features\nfor col in top_corr_feat.index[:15]:\n    print('{} - unique values: {} - mean: {:.2f}'.format(col, df_train[col].unique()[:5], np.mean(df_train[col])))","824f697e":"# we prefer select non categorical values for a scatter matrix plot\ncols = 'SalePrice GrLivArea GarageArea TotalBsmtSF YearBuilt 1stFlrSF MasVnrArea TotRmsAbvGrd'.split()\n\nwith plt.rc_context(rc={'font.size':14}): \n    fig, ax = plt.subplots(figsize=(16,13), tight_layout=True)    \n    pd.plotting.scatter_matrix(df_train[cols], ax=ax, diagonal='kde', alpha=0.8)","0a0f3cf9":"cut_area = 4600\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.scatter(df_train['SalePrice'], df_train['GrLivArea'], s=18)\nax.set(xlabel='SalePrice', ylabel='GrLivArea')\nax.axhline(cut_area, ls='--', lw=2.5, c='green', alpha=0.5)   \nax.grid()","40d3b478":"# remove points in the SalePrice - GrLivArea scatter plot\nprint('size of train dataset {}'.format(df_train.shape))\ndf_train = df_train.loc[df_train['GrLivArea'] < cut_area]\nprint('size of train dataset {} after removing outliers'.format(df_train.shape))","1f2097e4":"# Let's dig now into some of the most interpretable correlated features\ndecades = (df_train['YearBuilt'] \/\/ 10) * 10   # construct decade construction\ndecades.name = 'Decades'\n\nwith plt.rc_context(rc={'font.size':14}): \n    fig, ((ax1,ax2),(ax3,ax4), (ax5,ax6)) = plt.subplots(3,2, figsize=(20,12), tight_layout=True)\n    \n    sns.violinplot(x=df_train['OverallQual'], y=df_train['SalePrice'], ax=ax1)\n    sns.violinplot(x=df_train['GarageCars'], y=df_train['SalePrice'], ax=ax2)\n    sns.violinplot(x=df_train['FullBath'], y=df_train['SalePrice'], ax=ax3)\n    sns.violinplot(x=df_train['Fireplaces'], y=df_train['SalePrice'], ax=ax4)\n    sns.violinplot(x=df_train['TotRmsAbvGrd'], y=df_train['SalePrice'], ax=ax5)\n    sns.violinplot(x=decades, y=df_train['SalePrice'], ax=ax6)","a2f0a537":"months = 'Jan. Feb. March April May June July Aug. Sept. Oct. Nov. Dec.'.split()\nwith plt.rc_context(rc={'font.size':14}): \n    fig, ax = plt.subplots(figsize=(8,4))\n    df_train.groupby('MoSold')['SalePrice'].count().plot(kind='bar', alpha=0.3, ax=ax)\n    ax.set(xlabel='Month Sold', ylabel='Number of houses sold \/ month', \n           xticklabels=months)\n    ax.tick_params(axis='x', rotation=15)","7b672d25":"fig, ax = plt.subplots(figsize=(10,4))\nsns.violinplot(x=df_train['MoSold'], y=df_train['SalePrice'], ax=ax)\n_ = ax.set(xticklabels=months)","583485a9":"# We don't need the Id column so we save it for final submission\ndf_train_id = df_train['Id']\ndf_test_id = df_test['Id']\n\ndf_train.drop(\"Id\", axis=1, inplace=True)\ndf_test.drop(\"Id\", axis=1, inplace=True)","579aac75":"# same transformation to the train \/ test datasets to avoid irregularities\nsize_train = len(df_train.index)\nsize_test = len(df_test.index)\n\ndf_tot = pd.concat([df_train, df_test], sort=False).reset_index(drop=True)\ndf_tot.drop(['SalePrice'], axis=1, inplace=True)\n\ny_train = df_train['SalePrice'].values","02b74833":"df_na = (df_tot.isnull().sum()) \/ len(df_tot) * 100\ndf_na = df_na.drop(df_na[df_na==0].index).sort_values(ascending=False)\ndf_na.head(15)","7f056a1a":"with plt.rc_context(rc={'font.size':14}): \n    fig, ax = plt.subplots(figsize=(16, 6))\n\n    sns.barplot(df_na.index, df_na, palette=\"pastel\", ax=ax)\n    ax.set(xlabel='Features', ylabel='Missing values percentages')\n    ax.tick_params(axis='x', rotation=55)","3f69f786":"# According to the data description\nfor col in 'PoolQC MiscFeature Alley Fence FireplaceQu'.split():\n    df_tot[col].fillna('None', inplace=True)","34c9f4ed":"# Get the LotFrontage from its median values from the Neighborhood\nprint(df_tot.groupby(\"Neighborhood\")[\"LotFrontage\"].agg(np.median).head())\n\ndf_tot[\"LotFrontage\"] = df_tot.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","baafeea5":"# Inspect Garage properties columns \nfor col in df_tot.columns:\n    if col.startswith('Garage'):\n        print('{} - unique values: {}'.format(col, df_tot[col].unique()[:5]))","71a02d3a":"# Replace Garage categorical values by None\nfor col in 'GarageType GarageFinish GarageQual GarageCond'.split():\n    df_tot[col].fillna('None', inplace=True)\n    \n# Replace Garage numeric values by 0\nfor col in 'GarageYrBlt GarageCars GarageArea'.split():\n    df_tot[col].fillna(0, inplace=True)","c13d408c":"# Inspect Basement properties columns \nfor col in df_tot.columns:\n    if 'Bsmt' in col:\n        print('{} - unique values: {}'.format(col, df_tot[col].unique()[:5]))","ffd7699f":"# Same replacements (that Garage columns)\nfor col in 'BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2'.split():\n    df_tot[col].fillna('None', inplace=True)\n    \n# Replace numeric values by 0\nfor col in 'BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath'.split():\n    df_tot[col].fillna(0, inplace=True)","4c610189":"df_tot['MasVnrArea'].fillna(0, inplace=True)\ndf_tot['MasVnrType'].fillna('None', inplace=True)","6047907b":"# The most frequent value is RL\ndf_tot['MSZoning'].value_counts() \/ len(df_tot) * 100","ebb896b5":"df_tot['MSZoning'].fillna('RL', inplace=True)","355750b2":"print(df_tot['Utilities'].value_counts())\n\n# Since the values are almost only AllPub (except 1 line) this column is useless for SalePrice prediction\ndf_tot.drop(columns='Utilities', inplace=True)","082535b3":"print(df_tot['Functional'].value_counts() \/ len(df_tot) * 100)\ndf_tot['Functional'].fillna('Typ', inplace=True)","59dfd1dc":"print('{} missing values from {} column'.format(df_tot['Exterior1st'].isnull().sum(), 'Exterior1st'))\n\n# Fill in with the most common value --> 'Vinyl1Sd'\ndf_tot['Exterior1st'].fillna(df_tot['Exterior1st'].mode()[0], inplace=True)\n","d41e3455":"print('{} missing values from {} column'.format(df_tot['Exterior2nd'].isnull().sum(), 'Exterior2nd'))\n\n# Fill in with the most common value\ndf_tot['Exterior2nd'].fillna(df_tot['Exterior2nd'].mode()[0], inplace=True)","3cb41485":"# Fill in with the most common value\ndf_tot['KitchenQual'].fillna(df_tot['KitchenQual'].mode()[0], inplace=True)\ndf_tot['Electrical'].fillna(df_tot['Electrical'].mode()[0], inplace=True)\ndf_tot['SaleType'].fillna(df_tot['SaleType'].mode()[0], inplace=True)\ndf_tot['MSSubClass'].fillna('None', inplace=True)","68102606":"df_na = (df_tot.isnull().sum()) \/ len(df_tot) * 100\ndf_na = df_na.drop(df_na[df_na==0].index).sort_values(ascending=False)\ndf_na.head()","350b1c6a":"num_cols = df_tot.select_dtypes(exclude='object').columns\nprint('{} Numeric columns \\n{}'.format(len(num_cols), num_cols))\n\ncateg_cols = df_tot.select_dtypes(include='object').columns\nprint('\\n{} Categorical columns \\n{}'.format(len(categ_cols), categ_cols))","32f7308d":"# Basic cleaning of the data\n# numeric cols --> mean value and non-numeric cols --> most frequent value\n# Inspired from the nice code by 'sveitser' at http:\/\/stackoverflow.com\/a\/25562948\n\nclass DataImputer(TransformerMixin):\n    \"\"\"First data cleaning operation.\"\"\"\n    \n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') \n                               else X[c].median() for c in X], index=X.columns)\n        return self\n    \n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\n### same transformation to the train \/ test datasets to avoid irregularities\n#tot_X = df_train.append(df_test, sort=False)\n#tot_X_imputed = DataImputer().fit_transform(tot_X)\n\n#le = LabelEncoder()\n#for feat in object_cols:\n#    tot_X_imputed[feat] = le.fit_transform(tot_X_imputed[feat])","d5ef1b1c":"# Inspect numeric \/ categorical correlated features\nfor col in top_corr_feat.index[:25]:\n    print('{} - unique values: {} - mean: {:.2f}'.format(col, df_train[col].unique()[:5], np.mean(df_train[col])))","505458ac":"cols = 'MSSubClass OverallQual GarageCars YrSold MoSold Fireplaces HalfBath'.split()\n\nfor col in cols:\n    df_tot[col] = df_tot[col].astype(str)","c2d8d912":"cols_le = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', \n           'GarageQual', 'GarageCond', 'GarageFinish', 'GarageType', \n           'FireplaceQu', 'ExterQual', 'ExterCond', \n           'HeatingQC', 'PoolQC', 'KitchenQual', \n           'Functional', 'Fence', 'LandSlope',\n           'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', \n           'MSSubClass', 'OverallCond', 'GarageCars', 'YrSold', 'MoSold', 'Fireplaces', 'HalfBath'] \n\nle = LabelEncoder() \nfor col in cols_le:\n    df_tot[col] = le.fit_transform(df_tot[col])","62ebdee5":"# Combine total square foot area\ndf_tot['TotalSF'] = df_tot['TotalBsmtSF'] + df_tot['1stFlrSF'] + df_tot['2ndFlrSF'] + df_tot['GrLivArea'] + df_tot['GarageArea']\n\n# Combine the bathrooms\ndf_tot['Bathrooms'] = df_tot['FullBath'] + df_tot['HalfBath']* 0.5 \n\n# Combine Year built, Garage Year Built and Year Remod \n# (with a coeff 0.5 since it's less correlated to Year Built than the Garage year built).\ndf_tot['YearMean'] = df_tot['YearBuilt'] + df_tot['YearRemodAdd'] * 0.5 + df_tot['GarageYrBlt']","93450b3b":"# Compute the skew of all numerical features\nnew_num_cols = df_tot.select_dtypes(exclude='object').columns\n\nfeat_skews = df_tot[new_num_cols].apply(stats.skew).sort_values(ascending=False)\nskew_df = pd.DataFrame({'skewness' :feat_skews})\nskew_df.head()","72c6b38b":"# Check MiscVal distribution before transformation\nskew_distribution(df_tot, 'MiscVal')","de76e08d":"from scipy.special import boxcox1p\n\ncols = skew_df[np.abs(skew_df['skewness']) > 0.8].index\nprint('We use the boxcop1p transformation on {} numeric features'.format(len(cols)))\nfor col in cols:\n    df_tot[col] = boxcox1p(df_tot[col], 0.15)","6c9cdd5c":"skew_distribution(df_tot, 'MiscVal')","04783bb7":"print(df_tot.shape, 'before dummy categories')\ndf_tot = pd.get_dummies(df_tot)\nprint(df_tot.shape, 'after dummy categories')","c0d973f4":"df_train = df_tot[:size_train]\ndf_test = df_tot[size_train:]","5ffa79c4":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom mlxtend.regressor import StackingRegressor\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor","abce2f58":"def rmse(ypred, ytrue):\n    \"\"\"\n    Compute the RMSE between true labels and predictions.\n    \"\"\"\n    return np.sqrt(mean_squared_error(ypred, ytrue))","0f51ac45":"#train and test (for validation) from the train dataset\nXtrain, Xtest, ytrain, ytest = train_test_split(df_train, y_train, shuffle=True, \n                                                test_size=0.3, random_state=28)\n\nscale = RobustScaler()\nkf = KFold(n_splits=5, shuffle=True, random_state=28)","d75abeda":"# Function to compute the RMSE\ndef result_GridCV(name, model):\n    \"\"\"\n    Display the results on the RMSE after the GridSearchCV.\n    \"\"\"\n    model.fit(Xtrain, ytrain)\n    ytrain_pred = model.predict(Xtrain)\n    ytest_pred = model.predict(Xtest)\n    \n    rmse_train = rmse(ytrain, ytrain_pred)\n    rmse_test = rmse(ytest, ytest_pred)\n\n    print(\"{} - TRAIN score: {:.4f}\" .format(name, rmse_train))\n    print(\"{} - TEST score: {:.4f}\" .format(name, rmse_test))","3186bf12":"score = 'neg_mean_squared_error'","ffc828af":"# First a fast GridSearchCV to find the optimal alpha\nparam_grid = {'alpha': np.logspace(0, 2, 50)}\n\ngrid_ridge = GridSearchCV(Ridge(), param_grid, cv=40, scoring=score, verbose=0, n_jobs=-1)  \ngrid_ridge.fit(Xtrain, ytrain)\nprint(grid_ridge.best_params_)","8f071a0e":"# Compute RMSE with best Ridge\nridge = make_pipeline(RobustScaler(), grid_ridge.best_estimator_)\nresult_GridCV('Ridge', ridge)","1cdb2fe6":"# First a fast GridSearchCV to find the optimal alpha\nparam_grid = {'alpha': np.logspace(-4, 0, 30)}\nscore = 'neg_mean_squared_error'\n\ngrid_lasso = GridSearchCV(Lasso(), param_grid, cv=30, scoring=score, verbose=0, n_jobs=-1)       \ngrid_lasso.fit(Xtrain, ytrain)\nprint(grid_lasso.best_params_)","f52db64e":"print('{}\/{} coefficients not null with the Lasso method'.format((grid_lasso.best_estimator_.coef_ !=0).sum(), len(df_train.columns)))\n#df_train.columns[grid_lasso.best_estimator_.coef_ !=0]","96c0547f":"# Compute RMSE with best Lasso\nlasso = make_pipeline(RobustScaler(), grid_lasso.best_estimator_)\nresult_GridCV('Lasso', lasso)","0f0bde35":"grid_net = {'alpha': np.logspace(-4, -3, 20),\n            'l1_ratio': [0.5,0.55,0.6,0.65]}\nscore = 'neg_mean_squared_error'\n\ngrid_net = GridSearchCV(ElasticNet(), grid_net, cv=20, scoring=score, verbose=0, n_jobs=-1)       \ngrid_net.fit(Xtrain, ytrain)\nprint(grid_net.best_params_)","433005d9":"# Compute RMSE with best Ridge\nenet = make_pipeline(RobustScaler(), grid_net.best_estimator_)\nresult_GridCV('ElasticNET', enet)","e3f32cb6":"# SVR kernel - \n#params_svr = {'kernel': ['rbf', 'poly'], 'gamma': np.logspace(-3,2,10),\n#                     'C': [1, 10, 100]},\n#score = 'neg_mean_squared_error'\n\n#grid_svr = GridSearchCV(SVR(), params_svr, scoring=score)\n#grid_svr.fit(df_train, y_train)\n#print(grid_svr.best_params_)\n\n# Compute RMSE with best SVR\n#svr_score = rmse_CV(grid_svr.best_estimator_)\n#print(\"SVR score: {:.4f} +\/- {:.4f}\\n\" .format(svr_score.mean(), svr_score.std()))","19cc6e54":"#grid_forr = {'n_estimators': [1000,1500,2000], 'max_features': ['auto'],\n#             'min_samples_leaf': [4,8,10,20], 'min_samples_split': [4,8,16], \n#             'max_depth': [4,8,10,20]\n#            }\n#score = 'neg_mean_squared_error'\n\n#grid_forrest = GridSearchCV(RandomForestRegressor(), grid_forr, cv=5, scoring=score, verbose=1, n_jobs=-1)\n#grid_forrest.fit(df_train, y_train)\n#print(grid_forrest.best_params_)","4ef0fb53":"# Because the above GridSeachCV took several minutes, we dit get the optimal hyperparameters to run the score\nrfc = RandomForestRegressor(n_estimators=2000, n_jobs=-1, random_state=28,\n                           max_features='auto', min_samples_leaf=10, \n                           min_samples_split=8, max_depth=20, \n                           bootstrap=True)\n\nresult_GridCV('RandomForrest Regr.', rfc)","8b8fabb8":"top_n_feat = 24\n\nfeat_imp = pd.Series(rfc.feature_importances_, index=df_train.columns)[:top_n_feat]     # random forrest regr.\ncoefs = pd.Series(grid_lasso.best_estimator_.coef_, index=df_train.columns)     # lasso \nfeat_coeff = pd.concat([coefs.sort_values().head(top_n_feat\/\/2), coefs.sort_values().tail(top_n_feat\/\/2)])\n\ncolors, alpha = sns.color_palette('bright'), 0.6\nwith plt.rc_context(rc={'font.size':14}): \n    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,6), tight_layout=True)\n    feat_imp.sort_values().plot(kind='barh', color=colors, ax=ax1, alpha=alpha) \n    #feat_coeff.reindex(feat_imp.sort_values().index).plot(kind='barh', color=colors, ax=ax2, alpha=alpha)  # if same cols. as forrest\n    feat_coeff.plot(kind='barh', color=colors, ax=ax2, alpha=alpha) \n    \n    ax1.set(xlabel='Feature importances', title='Random Forrest Regression - feature importances')    \n    ax2.set(xlabel='Lasso coeff.', title='Lasso - feature coeff. weights')\n    for ax in [ax1,]:\n        ax.set_xscale('log')","9d42383d":"gboost = GradientBoostingRegressor(n_estimators=800, learning_rate=0.05,\n                                   max_depth=4, max_features='auto',\n                                   min_samples_leaf=10, min_samples_split=10, \n                                   loss='huber', random_state=28)\nresult_GridCV('GBoost', gboost)","34f70cab":"#params_lgb = {'num_leaves': [4,8,32], 'max_depth': [4,8], 'reg_alpha': [0,0.2], 'reg_lambda': [0.5,1], \n#              'n_estimators': [750], 'learning_rate': [0.02,0.04,0.08]}\n\n#grid_lgb = GridSearchCV(LGBMRegressor(n_jobs=-1), params_lgb, cv=5, scoring=score, verbose=1, n_jobs=-1)\n#grid_lgb.fit(Xtrain, ytrain)\n#print(grid_lgb.best_params_)","e078e0cf":"params_lgb = {'learning_rate': 0.04, 'max_depth': 4, 'n_estimators': 750, 'num_leaves': 4, 'reg_alpha': 0, 'reg_lambda': 1}\nlgb = LGBMRegressor(n_jobs=-1, **params_lgb)\nresult_GridCV('LBGMRegressor', lgb)","52b61d57":"#params_xgb = {'gamma': [0.03,0.04,0.05,], 'max_depth': [3,4,8], 'reg_alpha': [0,], 'reg_lambda': [1,], \n#              'n_estimators': [500,1000,1500], 'learning_rate': [0.02,0.04,0.08], 'subsample': [0.5,], \n#              'min_child_weight': [1,2,4,8]}\n\n#params_xgb = {'gamma': [0.025,0.03,0.035,], 'max_depth': [3,4,8]}\n\n#grid_xgb = GridSearchCV(XGBRegressor(n_jobs=-1, random_state=28, n_estimators=1500, objective='reg:squarederror'), \n#                        params_xgb, cv=3, scoring=score, verbose=1, n_jobs=-1)\n#grid_xgb.fit(Xtrain, ytrain)\n#print('XGBoosting best RMSE: %f using %s\\n' % (-grid_xgb.best_score_, grid_xgb.best_params_))","87d813b7":"# Because the above GridSeachCV took quite a few time, we dit get the optimal hyperparameters to run the score.\nxgb = XGBRegressor(base_score=0.5, colsample_bylevel=1,\n                   colsample_bynode=1, colsample_bytree=0.5, \n                   learning_rate=0.02, gamma=0.025,\n                   max_depth=4, n_estimators=1500, min_child_weight=2,\n                   nthread=-1, reg_alpha=0., reg_lambda=1, subsample=0.5, \n                   objective='reg:squarederror', random_state=28)\n                    \nresult_GridCV('XGBoost', xgb)\n\n# Avoid overfitting with early-stops method\n#pred = model.predict(Xtest, ntree_limit=model.best_ntree_limit)","bcae2d46":"eval_set = [(Xtrain, ytrain), (Xtest, ytest)]\nxgb.fit(Xtrain, ytrain, early_stopping_rounds=100, eval_metric='rmse', eval_set=eval_set, verbose=None)","a0c6b7b0":"# Check evolution of the loss function\n\nresults = xgb.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\nfig, ax = plt.subplots(figsize=(8,6))\nax.plot(x_axis, results['validation_0']['rmse'], label='Train')\nax.plot(x_axis, results['validation_1']['rmse'], label='Test')\nax.set(title='XGBoost Regression Error', ylabel='RMSE', ylim=([0,1]))\nax.legend()","ab21172a":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # clones of the actual model \n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Training of the clone model\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    \n    # prediction and average of the trained clone models\n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)   ","f9f0cfe4":"averaged_models = AveragingModels(models=(ridge, lasso, enet, gboost, lgb, xgb))\nresult_GridCV('Averaged models', averaged_models)","b2fa3083":"stack = StackingRegressor(regressors=[ridge, lasso, enet, averaged_models], \n                          meta_regressor=xgb, \n                          use_features_in_secondary=True)\n\n# Training the stacking regr.\nresult_GridCV('Stack regressors', stack)","77423fce":"# We can try to average the models\npred_net = enet.predict(Xtest)\npred_averaged = averaged_models.predict(Xtest)\npred_stack = stack.predict(Xtest)","cc6b63cb":"def weighted_average(pred1, pred2, pred3, weights=[0.5, 0.4,0.1]):\n    \"\"\"\n    Compute the average with weights (a+b+c=1) of the top 3 predictions.\n    \"\"\"\n    \n    a, b, c = weights\n    return pred1 * a + pred2 * b + pred3 * c","9995f6b4":"pred_tot = weighted_average(pred_net, pred_averaged, pred_stack, [0.5,0.4,0.1])\nRMSE = rmse(ytest, pred_tot)\nprint('RMSE of average models: {:.4f}'.format(RMSE))","afb41444":"# Check what is the optimal combinaison of weights\ndelta = 0.1\nc = 0.05\nfor a,b in zip(np.arange(0,1-c,delta), 1-c - np.arange(0,1-c,delta)):\n    pred = weighted_average(pred_net, pred_averaged, pred_stack, [a,b,c])\n    RMSE = rmse(ytest, pred)\n    print('RMSE (a={:.2f}, b={:.2f}) : {:.4f}'.format(a, b, RMSE))","8bb3a4bf":"pred_tot = weighted_average(pred_net, pred_averaged, pred_stack, [0.05,0.90,0.05])\nRMSE = rmse(ytest, pred_tot)\nprint('RMSE of average models: {:.4f}'.format(RMSE))","aa7244b5":"# Final prediction on the SalePrice\n\nenet.fit(df_train, y_train)\naveraged_models.fit(df_train, y_train)\nstack.fit(df_train, y_train)","a1156f16":"enet_final = np.expm1(enet.predict(df_test))\naverage_final = np.expm1(averaged_models.predict(df_test))\nstack_final = np.expm1(stack.predict(df_test))","2e108553":"pred_final = weighted_average(enet_final, average_final, stack_final, [0.05,0.9,0.05])","0a22fd60":"df_sub = pd.DataFrame({'Id': df_test_id, 'SalePrice': pred_final})\nprint(df_sub.head())\ndf_sub.to_csv('submission.csv',index=False)","b771c8a0":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam, RMSprop, Adagrad, Nadam, SGD, Adadelta, Adamax\nfrom keras.constraints import maxnorm\nfrom keras.regularizers import l2\nnp.random.seed(28)    # for reproductibily","84a4299b":"#train, validation and test from the train dataset\nXtrain, Xtest, ytrain, ytest = train_test_split(df_train, y_train, shuffle=True, \n                                                test_size=0.3, random_state=28)\n\nXval, Xtest, yval, ytest = train_test_split(Xtest, ytest, test_size=0.5, \n                                            shuffle=True, random_state=28)","c3f18919":"# function to plot the evolution of loss (use of it later)\ndef loss_plot(info_model, ymax=5):\n    train_loss = info_model.history['loss']\n    val_loss = info_model.history['val_loss']    \n    epochs = range(len(train_loss))\n    \n    with plt.rc_context({'font.size':13}):\n        fig, ax = plt.subplots(figsize=(8,5))\n        ax.plot(epochs, train_loss, label='Train')\n        ax.plot(epochs, val_loss, label='Validation')\n        ax.set(xlabel='Epochs', ylabel='Loss - RMSE', \n               title='Model loss', ylim=(0,ymax))\n        ax.legend()       ","586b7bb9":"model1 = Sequential()\n\n# First layer\nmodel1.add(Dense(128, input_dim=Xtrain.shape[1], activation='relu'))\n\n# Hidden layers\nmodel1.add(Dense(256, activation='relu'))\nmodel1.add(Dense(256, activation='relu'))\n\n# Final layer\nmodel1.add(Dense(1, activation='linear'))","5168a151":"model1.summary()","51212fb7":"# Optimizer\nopt1 = Adam(lr=0.001, beta_1=0.95)    # default values\n\n# Since the metrics RMSE does not exists in Keras, we need to implement it\nimport keras.backend as K\n\ndef rmse_keras(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))","896d1c84":"# Let's compile our model\nmodel1.compile(optimizer=opt1, loss=rmse_keras) ","e4b98377":"def init_callback(model_id='1', verbose=0):\n    \"\"\"\n    Create a callback per model id. \n    \"\"\"\n\n    filepath = 'weights{}.best.hdf5'.format(model_id)\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=verbose, \n                                 save_best_only=True, mode='auto')\n    return [checkpoint]\n\n# Callback MODEL 1\ncallbacks1 = init_callback(model_id='1', verbose=0)","633d2398":"info_model1 = model1.fit(Xtrain, ytrain, epochs=2000, \n                       batch_size=32, verbose=0,         # default values\n                       validation_data=(Xval,yval), \n                       use_multiprocessing=True, \n                       shuffle=True, \n                       callbacks=callbacks1)","24216d82":"# Evolution of the training\nloss_plot(info_model1, 0.8)","83eeed6d":"# Load weights file of the best model :\n\n#val_loss = info_model1.history['val_loss']\n#best_weight = np.argmin(val_loss) + 1\n#best_val_loss = np.min(val_loss)\n#file_w = 'weights-{:03d}--{:.5f}.hdf5'.format(best_weight, best_val_loss) # best checkpoint\n\ndef load_best_weights(model, model_id='1', opt='adam'):\n    \"\"\"\n    Load the model with best weight during the training and compile it.\n    \"\"\"\n    model.load_weights('weights{}.best.hdf5'.format(model_id))\n    model.compile(loss=rmse_keras, optimizer=opt)\n    \n    return model","8ec4464b":"# Load best model\nmodel1 = load_best_weights(model1, model_id='1', opt=opt1)\nloss1 = model1.evaluate(Xtest, ytest)\nprint('MODEL 1  --  RMSE (test data): {:.4f}'.format(loss1))","c99aab6c":"# Optimizer\nopt2 = Adam(lr=0.001, beta_1=0.9)    # higher learning rate (dropping)","f9d85b16":"# kernel contraints\nweight_constraint = maxnorm(3)\nactivation = 'relu'\nweight_init = 'normal'\n\n\nmodel2 = Sequential()\n# First layer\nmodel2.add(Dropout(0.2, input_shape=(Xtrain.shape[1],))) \nmodel2.add(Dense(128, activation=activation, kernel_constraint=weight_constraint, \n                 kernel_initializer=weight_init, ))\n\n# Hidden layers\nmodel2.add(Dense(256, activation=activation, kernel_constraint=weight_constraint, \n                 kernel_initializer=weight_init))\nmodel2.add(Dropout(0.3))\nmodel2.add(Dense(256, activation=activation, kernel_constraint=weight_constraint, \n                kernel_initializer=weight_init))\nmodel2.add(Dropout(0.3))\n\n# Final layer\nmodel2.add(Dense(1, kernel_initializer='normal', activation='linear'))","38706255":"# Let's compile our model\nmodel2.compile(optimizer=opt2, loss=rmse_keras) \n\n# Callback MODEL 2\ncallbacks2= init_callback(model_id='2', verbose=0)","7ba943ed":"info_model2 = model2.fit(Xtrain, ytrain, epochs=2000, \n                       batch_size=32, verbose=0,         # default values\n                       validation_data=(Xval,yval), \n                       use_multiprocessing=True, \n                       shuffle=True, \n                       callbacks=callbacks2)","e4505887":"# Evolution of the training\nloss_plot(info_model2, 1)","b9bb7053":"# Load best model\nmodel2 = load_best_weights(model2, model_id='2', opt=opt2)\nloss2 = model2.evaluate(Xtest, ytest)\nprint('MODEL 2  --  RMSE (test data): {:.4f}'.format(loss2))","283899ca":"from sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef create_mlp(lr=0.001, beta1=0.9, beta2=0.999, \n              activation='relu',\n              dropout_input=0.2, \n              dropout_rate=0.3,\n              weight_constraint=3,\n              weight_init='normal',\n              input_neurons=128,\n              hidden_neurons=256):\n    \"\"\"\n    Function to create a Keras model that can be used with sklearn methods.\n    \"\"\"\n    \n    model = Sequential()\n    # First layer\n    model.add(Dropout(dropout_input, input_shape=(Xtrain.shape[1],))) \n    model.add(Dense(input_neurons, activation=activation, \n                    kernel_constraint=maxnorm(weight_constraint), \n                    kernel_initializer=weight_init, \n                    ))   \n    # Hidden layers\n    model.add(Dense(hidden_neurons, activation=activation, \n                    kernel_constraint=maxnorm(weight_constraint), \n                    kernel_initializer=weight_init))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(hidden_neurons, activation=activation, \n                    kernel_constraint=maxnorm(weight_constraint), \n                    kernel_initializer=weight_init)) \n    model.add(Dropout(dropout_rate))   \n    # Final layer\n    model.add(Dense(1, kernel_initializer=weight_init, activation='linear'))\n\n    # Compile model\n    optimizer = Adam(learning_rate=lr, beta_1=beta1, beta_2=beta2)\n    model.compile(loss=rmse_keras, optimizer=optimizer)\n    return model","bcd95d7e":"# Scoring for the GridSearchCV\nscore = 'neg_mean_squared_error'\n\nfit_grid = False         # avoid the notebook to take 1 day to run","c350f1df":"cv = 3  # only cross-val=3 for now since it takes time \nmodel = KerasRegressor(build_fn=create_mlp, epochs=1500, batch_size=32, \n                       verbose=1, shuffle=True, use_multiprocessing=True)\n\n# Grid search params \nlr = [0.001, 0.01, 0.1]\nbeta1 = [0., 0.4, 0.9, 0.95]\nparam_grid = dict(lr=lr, beta1=beta1)\n\nif fit_grid:\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, n_jobs=-1, cv=cv, verbose=0)   \n    grid_result = grid.fit(Xtrain, ytrain)\n    print('Adam GridSearch best MSE: {:.4f} using {}'.format(-grid_result.best_score_, grid_result.best_params_))","eeaf40da":"cv = 3\nmodel = KerasRegressor(build_fn=create_mlp, epochs=1500, batch_size=32, \n                       verbose=1, shuffle=True, use_multiprocessing=True)\n\n# Grid search params \nweight_constraint = [2,3,4] \ndropout_rate = np.arange(0,0.5,0.1)  # hidden layers\nparam_grid = dict(weight_constraint=weight_constraint, \n                  dropout_rate=dropout_rate)\n\nif fit_grid:\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, n_jobs=-1, cv=cv, verbose=0)   \n    grid_result = grid.fit(Xtrain, ytrain)\n    print('MaxNorm constraint GridSearch best MSE: %f using %s\\n' % (-grid_result.best_score_, grid_result.best_params_))","09595c48":"#means = grid_result.cv_results_['mean_test_score']\n#stds = grid_result.cv_results_['std_test_score']\n#params = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print('{:.4f} +\/- {:.4f} with: {}'.format(-mean, stdev, param))","18862bc2":"cv = 3\nmodel = KerasRegressor(build_fn=create_mlp, epochs=1500, batch_size=32, \n                       verbose=1, shuffle=True, use_multiprocessing=True)\n\n# Grid search params \nweight_init = ['uniform', 'normal', 'zero', 'he_normal', 'he_uniform']\nparam_grid = dict(weight_init=weight_init)\n\nif fit_grid:\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, n_jobs=-1, cv=cv, verbose=0)   \n    grid_result = grid.fit(Xtrain, ytrain)\n    print('Weight initialization GridSearch best MSE: %f using %s\\n' % (-grid_result.best_score_, grid_result.best_params_))","c6d2c2b6":"cv = 3\nmodel = KerasRegressor(build_fn=create_mlp, epochs=1500, batch_size=32, \n                       verbose=1, shuffle=True, use_multiprocessing=True)\n\n# Grid search params \nactivation = ['softplus', 'softsign', 'relu', 'tanh', 'linear']\nparam_grid = dict(activation=activation)\n\nif fit_grid:\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, n_jobs=-1, cv=cv, verbose=0)   \n    grid_result = grid.fit(Xtrain, ytrain)\n    print('Activation functions GridSearch best MSE: %f using %s\\n' % (-grid_result.best_score_, grid_result.best_params_))","6300041d":"cv = 3\nmodel = KerasRegressor(build_fn=create_mlp, epochs=1500, batch_size=32, \n                       verbose=1, shuffle=True, use_multiprocessing=True)\n\n# Grid search params \ndropout_input = [0.1,0.2,0.3]\nparam_grid = dict(dropout_input=dropout_input)\n\nif fit_grid:\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, n_jobs=-1, cv=cv, verbose=0)   \n    grid_result = grid.fit(Xtrain, ytrain)\n    print('Activation functions GridSearch best MSE: %f using %s\\n' % (-grid_result.best_score_, grid_result.best_params_))","60349f60":"# Get the best model and then fit and select best weight\nbest_model = create_mlp(lr=0.001, beta1=0.95, \n                        weight_constraint=4, \n                        weight_init='uniform', \n                        activation='linear', \n                        dropout_input=0.1, \n                        dropout_rate=0.4)\n\n# Callback best MODEL\ncallbacks = init_callback(model_id='0', verbose=0)\n\n# Train\ninfo = best_model.fit(Xtrain, ytrain, epochs=4000, \n                       batch_size=32, verbose=0,         \n                       validation_data=(Xval,yval), \n                       use_multiprocessing=True, \n                       shuffle=True, \n                       callbacks=callbacks)","7bf2c7e4":"# Evolution of the training\nloss_plot(info, 2)","c3378bdb":"# Load best model\nbest_model = load_best_weights(best_model, model_id='0', opt=Adam(0.001,0.95))\nloss = best_model.evaluate(Xtest, ytest)\nprint('FINAL MODEL  --  RMSE (test data): {:.4f}'.format(loss))","48be987a":"# Get the best model and then fit and select best weight\nbest_model = create_mlp(lr=0.001, beta1=0.99, \n                        weight_constraint=4, \n                        weight_init='uniform', \n                        activation='linear', \n                        dropout_input=0.1, \n                        dropout_rate=0.4)\n\n# Callback best MODEL\ncallbacks99 = init_callback(model_id='99', verbose=0)\n\n# Train\ninfo99 = best_model.fit(Xtrain, ytrain, epochs=4000, \n                       batch_size=32, verbose=0,         \n                       validation_data=(Xval,yval), \n                       use_multiprocessing=True, \n                       shuffle=True, \n                       callbacks=callbacks99)","793dde0a":"loss_plot(info99, 2)\n\n# Load best model\nbest_model = load_best_weights(best_model, model_id='99', opt=Adam(0.001,0.99))\nloss = best_model.evaluate(Xtest, ytest)\nprint('FINAL MODEL  --  RMSE (test data): {:.4f}'.format(loss))","f36d2e28":"#model.add(BatchNormalization())   # use_biais=False into Dense layer before","e4ff82be":"# Test to prediction\n\n#best_model.fit(df_train, y_train)\n#prediction = np.expm1(best_model.predict(df_test)).flatten()\n\n#df_sub = pd.DataFrame({'Id': df_test_id, 'SalePrice': prediction})\n#print(df_sub.head())\n#df_sub.to_csv('submission.csv',index=False)","df0d467a":"## Second try with some more complex architecture","cad1373b":"To correct these feature, since some of them have high skewness we're gonna use a generalisation of the `np.log1p` transformation which is called the [boxcop1p transformation](https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html) (`scipy.special.boxcox1p`). The `np.log1p` is then the particular case when $\\lambda \\rightarrow 0$ in the boxcop1p function.","f6ab0ea8":"### Elastic Net","8f5186c7":"For all the above ensemblists methods we should also *in theory* do a GridSearchCV for tuning hyperparameters\nbut it does take a lot of time given all differents hyperparams to adjust. Therefore, I already did it but comment some.","02bc532d":"Now we're gonna LabelEncode all **quality** features (where the order matters) and the precedents columns.","ed3e240b":"We define also a callback to check the model after every epoch with `ModelCheckpoint`. It will ensure that we have a snapshot of the best model discovered during the training run.","5a9bd23e":"## Third model with GridSearch on our MLP","b9f11e11":"### XGBoost Regressor","1c7f62e2":"We're gonna remove the principal outliers in the scatter plots of (GrLivRea - GarageArea - TotalBsmtSF - 1stFlrSF - MasVnrArea - TotRmsAbvGrd) vs **SalePrice**. (TODO).","98f16a16":"### LightGBM Regressor","287505b8":"***Select train and test dataset***","ee1b0856":"**Apply dummy categories to the rest**","5de33054":"We can see that we get a lesser good RMSE (0.149) $\\rightarrow$ now is the time to do some hyperpameters tuning ;-)","4e1eedd6":"#### Combine features!\nIt's very useful and important if we want to improve our model predictions.\n- all area features: basement, 1st and 2nd floor, ground live area and garage.\n- bathrooms: fullbath + half-bath\n- Year: year built + Garage year built + Year Remod","b2c772a5":"### Lasso model","0c937981":"## Test with higher beta1=0.99","3c84f31c":"# Submission ","aed22eea":"### GridSearch and Keras","359465ca":"Let's have a look at the architecture and the number of parameters.","4aa25b30":"### Transform dataset to only numeric values\nWe will use **Label Encoding** for categories with apparent order and **One-Hot Encoding** (or **pd.get_dummies**) for the others categories.\n\nFirstly, there are a few numeric features which are categorical (as we seen it at the beginning). We will convert them into categorical columns.","cf11664e":"For the **linear regression models**, we will use both a *GridSearchCV* for tuning the hyperparameters and compute the best score.","32d609cf":"## First try with a basic MLP\nWe construct a basic architecture for regression: \n- 1 **input** `Dense` (=*fully-connected*) layer with an ReLU activation function\n- 2 **hidden** `Dense` layers (ReLU)\n- 1 **final** `Dense` layer with a linear activation function","b4979165":"### Replace NaN values","b1d54b7c":"Let's have a closer first look at the data.","02e16073":"##\u00a0Stacking Regressor","e3e2229a":"Adam GridSearch best MSE: 0.0518 using {'beta1': 0.95, 'lr': 0.001}.","40382429":"We now have a slight better result by averaging our models.","5d3194e9":"Inspect the top correlated features values","71ea1e64":"best with -- dropout=0.4 -- MaxDrop=4","5934674a":"### Correlation matrix\nBefore we do some features engineering, let's have a look on the most correlate features with the target (SalePrice).","b8710b68":"These scatter plots give us some insights on a few outliers i.e. values which posses incoherent\/huge values and which will impact the models. \n\nThus, we will only remove a few of them since these ones are really important. These outliers correspond for exemple to big houses not that expensive (therefore it is safe to remove these points). \n\nWe note that remove outliers only in the train dataset can cause bad performance on the model applied on the test dataset (if this one contains outliers as well).","29f05a92":"Activation functions GridSearch best MSE: 0.049348 using {'dropout_input': 0.1}","c234fb44":"## Averaging models\nThanks to this awesome [notebook (by Serigne)](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) which shows a way to do that.","c83e55ea":"**Not bad at all for a first try (0.127), we get a similar value compared to machine learning methods without tuning hyperparameters and so on!**\n\nNow, let's try to tune our model more precisely with dropout, regularization, weight constraints etc.","d6c57d68":"**Select the numeric and categoricals columns**","513f0390":"**Dropout**: refers to *dropping* out units in the neural network i.e. the neurons in the layer before have a probability of p (between 0 and 1) in dropping out during the training phase (`Dropout`). This technique is efficient to prevent overfitting. More details can be found on this paper: [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http:\/\/jmlr.org\/papers\/volume15\/srivastava14a\/srivastava14a.pdf). They suggest to put a dropout of 0.2 for the input layer and between 0.2 and 0.5 for hidden ones.\n\nThere also other techniques which reduce overfitting such as **weight constraints and regulatization (l2 or l1)**: `kernel_constraint` `kernel_regularizer`.\n\nWe initialize the weight layers with a normal distribution (`kernel_initializer`).\n\nIt is also nice to preselect an optimizer to train the neural network and tune its parameters (we'll do it after this section). Since we are adding dropping, we'll need to change the default values of the optimization algorithm (Adam): its learning rate and momentum. The learning rate is controlling how much to update the weight at the end of each batch and the momentum how much to let the previous update influence the current weight update. Adam works well in practice and compares favorably to other adaptive learning-method \nalgorithms. It can be viewed as a combination of RMSprop and momentum. With dropping, we need to set largest learning rate and momentum. More details on the different algorithms can be found on: [this overview of gradient descent optimization\nalgorithms](https:\/\/arxiv.org\/pdf\/1609.04747.pdf).","d992cbe4":"# Modeling \n(i.e. the fun part)","b1bbb635":"Activation functions GridSearch best MSE: 0.040397 using {'activation': 'linear'}","bf1abee2":"We can now distinguish the common features that are both *relevant* using the Lasso and RandomForrest methods. \n\nSince the RandomForrestRegressor do not perform as well as the lasso, we should trust more the features importances based on the Lasso coeffs. We can for exemple see that the most important features are **MSZoning - OverallQuality - TotalSF (which is good since we ingineered it) - GrLivArea - YearBuilt)**. For instance, we should try to engineer also the YearBuild column with another one (TODO).","dbfb864a":"### Tuning on activation function","8c1ac968":"Hey everyone ! So this is my first project on kaggle. I applied basic feature engineering methods and test differents models such as Lasso, ElasticNet, RandomForrest, GradientBoosting and the ***powerful*** XGBoosting. \n\nI also added a section on how to solve this problem using Deep Learning with a *Multi Layer Perceptron*.\n\nAs a first try on this project, I'm sure that there are a lot of things that I can improve. So please don't hesitate to give some comments and ways to improve my predictions. \n\nEnjoy !","bd848fbd":"Thanks to a wrapper of sklearn in Keras, we are able to do GridSearch on our neural network.","1b1c7cf1":"### Use best hyperpameters and train best model !\n\nOf course, in theory we should do a GridSearchCV with all the hyperpameters which vary but it would take at least a day or 2 (that's a downside of neural network in a way).","59b102a9":"To improve further our prediction, we can stack the different *top* regressors !","417919bb":"\n#### Basic Class `DataImputer` to pre-clean the dataset (not used here) but it will be interesting to compare the performances of our approach with this simpler one (TODO).","199bc266":"### Tuning on Adam optimizer params","4c169bc9":"# Data exploration","c1c7d8d7":"### I hope this notebook gave you an idea of what can be done with classic Machine Learning algorithms but also with more advanced ones using a Deep Learning approach.","1be8dc77":"**The pros of using Lasso and RandomForrest algorithms are to get insight on the coefficient weights and feature importances.**","7a221450":"### Ridge model","585587ac":"OK, now every feature is filled with values ! ","cf35f0d8":"## Test basic models such as RandomForrest & Lasso Regression \nThese algorithms should help us to select the most important features. We need to automatically remove the outliers that we maybe not seen: we make use of the `RobustScaler` method for regression models that don't perform well with outliers.","07c5368a":"RMSE = 0.1216","b75ba8c7":"No sign of overfitting by the evolution of the validation vs. train loss !","700bd93e":"Much better ! Indeed the SalePrice is now more gaussian and the second plot which represents the probability plot (i.e. the quantile of a distribution vs. in this case the ones of a theoritical gaussian distribution) shows that the distribution follows almost a normal distribution.","b808e451":"Then, we need to specify:\n- the algorithm to do the the optimization\n- the loss function to use\n- the other metrics we want to analyse \n\nWe are firstly using a *classic* optimimizer: `Adam` (more details on the [optimizers on Keras here](https:\/\/keras.io\/optimizers\/)).\nThe loss function is the root mean squared error (RMSE).","9b585d65":"### Check the model loss(=score here) evolution: RMSE","77c3cd37":"### Tuning on input dropout","ec9eb6d0":"Weight initialization GridSearch best MSE: 0.037718 using {'weight_init': 'uniform'}","478d5f30":"I will try to improve my predictions by working on the data cleaning, feature enginnering and the modeling (stacking ...).\n#### If you had a good time and liked this notebook let me know with a +1 or a comment or both *:-)*","21e9fb31":"### Tuning on weight constraints and dropout","1cc8a892":"### Check the missing values","5962f849":"The target column is skewed. Therefore we need to transform it into a more normal distribution since ***linear models*** will perform better.\n#### **Log-transformation of the Price column**","e2900094":"# Using Regression, XGBoosting and Neural Network to predict house prices","3faf9ade":"### Check remaining missing values","e50ee3a4":"That's it for the Deep Learning part. Of course there are a lots a things to do\/improve such as:\n- test a l2 or l1 regularization on the weights instead of applying a MaxNorm.\n- test more precisely with GridSearch the optimal numbers of neurons in the hidden layers.\n- rethink the architecture of this basic neural network: I'm not an expert but I won't say the more hidden layers the better but there is an optimum number of those! I'll work on that.\n- what is the optimal number of neurons in the input layers according to the dataset shape ?\n\nAt the end, we did get a worst score with the MLP but it is interesting to search for the best configuration and I'm sure that with the right architecture we can out-best the machine learning predictions. Nonetheless, \nwe see here that for this type of regression predictions, standard machine learning give nice score without 'thinking\/searching to much' let's say ; )","b64cb80d":"### Tuning on weight initialization","65e31f66":"Better : ) ","e7fd39d6":"It seems that the houses are mainly sold\/bought during the summer. Concerning the prizes, they are slightly higher during the winter (maybe because the market is less *hot*).","01894ae8":"We're gonna use the wrapper `wrappers.scikit_learn` from Keras to use sklearn search methods to optimize our model prediction. \nUsing a grid search we're gonna test a lot of hyperparameters:\n- the learning rate (and the beta parameters of Adam) \n- the activation function \n- the dropout rate in the hidden layers \n- the weight contraints \n- the weight initialization.\n\nWe note that since these grid search took a quite a time (~30 min for 45 fits approximatively), we did it once and we commented these for the sake of the commiting time. Of course, you can paste these commented sections and test yourself the different gre","6c0e64d6":"### Random Forrest Regressor","91d42d3b":"### Check the skewed features (other than SalePrice which was already corrected)","cf159f49":"The best hyperparams for the XGBoosting model are:\n\n{'gamma': 0.025, 'learning_rate': 0.02, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 1500, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.5}","aa647e43":"# A Deep Learning approach: Multilayer Perceptron (MLP) for regression","62ae73a6":"That's a little better for this feature for instance !","cd24b2ac":"### Gradient Boosting Regressor","5b7f7980":"# Features engineering"}}