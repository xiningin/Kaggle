{"cell_type":{"86f67d5a":"code","672fc15b":"code","57f386c9":"code","66526bbc":"code","965ad824":"code","9e054cf6":"code","f492dbff":"code","8a758f20":"code","bfc654ee":"code","40b9277d":"code","c8437488":"code","43398cf1":"code","7526dfb7":"code","a0ca977a":"code","b69b8c4b":"code","7749c72f":"markdown","cf87957e":"markdown","32b59dcd":"markdown","864efe78":"markdown","372fdbc7":"markdown","a38271dd":"markdown","63f29bda":"markdown","164d4380":"markdown","7f748b75":"markdown","1fb37737":"markdown","704e26a4":"markdown","52e65f34":"markdown","f43daffa":"markdown","a79397f2":"markdown","fc0faa8d":"markdown","86366252":"markdown","55df73ef":"markdown","c83b3cf4":"markdown","426d435c":"markdown","d57df54c":"markdown","da3eaf20":"markdown","d715a9a1":"markdown"},"source":{"86f67d5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","672fc15b":"import numpy as np\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\n","57f386c9":"iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n\n# SVM Classifier model\nsvm_clf = SVC(kernel=\"linear\",C=10)\nsvm_clf.fit(X, y)","66526bbc":"print(svm_clf.support_vectors_)","965ad824":"def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n    w = svm_clf.coef_[0]\n    b = svm_clf.intercept_[0]\n\n    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n    # => x1 = -w0\/w1 * x0 - b\/w1\n    x0 = np.linspace(xmin, xmax, 200)\n    decision_boundary = -w[0]\/w[1] * x0 - b\/w[1]\n\n    margin = 1\/w[1]\n    gutter_up = decision_boundary + margin\n    gutter_down = decision_boundary - margin\n\n    svs = svm_clf.support_vectors_\n    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n","9e054cf6":"plot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\nplt.ylabel(\"Petal Width\", fontsize=14)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\nplt.show()","f492dbff":"svm_clf = SVC(kernel=\"linear\",C=0.1)\nsvm_clf.fit(X, y)\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\nplt.ylabel(\"Petal Width\", fontsize=14)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\nplt.show()","8a758f20":"from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n\ndef plot_dataset(X, y, axes):\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n    plt.axis(axes)\n    plt.grid(True, which='both')\n    plt.xlabel(r\"$x_1$\", fontsize=20)\n    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.show()","bfc654ee":"# SVM Classifier model\nsvm_clf = SVC(kernel=\"linear\")\nsvm_clf.fit(X, y)","40b9277d":"plot_svc_decision_boundary(svm_clf, -1.5, 3.5)\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\nplt.ylabel(\"X2\", fontsize=14)\nplt.xlabel(\"X1\", fontsize=14)\nplt.axis([-1.5, 3.5, -1, 1.5])\nplt.show()","c8437488":"def plot_predictions(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)","43398cf1":"svm_clf2 = SVC(kernel=\"poly\", degree=2, coef0=1, C=5)\nsvm_clf2.fit(X, y)\nplot_predictions(svm_clf2, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])","7526dfb7":"from sklearn.svm import SVR\ndataset = pd.read_csv(\"\/kaggle\/input\/advtlr\/Advertising.csv\")\n# Selecting the Second, Third and Fouth Column\nX= dataset.iloc[:,1:4]\n# Selecting Fouth Columnn\ny=dataset.iloc[:,4]","a0ca977a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nsvr_lin = SVR(kernel='linear', C=150, gamma='auto',epsilon=.1)\nsvr_lin.fit(X_train,y_train)","b69b8c4b":"from sklearn.metrics import r2_score\ny_pred = svr_lin.predict(X_test)\nr2_score(y_pred, y_test)","7749c72f":"# Support Vector Parameters <a id='1.5'> <\/a>","cf87957e":"# Contents\n\n* [<font size=4>Getting Started<\/font>](#1)\n    * [Kernel](#1.1)\n    * [Linear Kernel](#1.1)\n    * [Polynomial Kernel](#1.2)\n    * [Support Vector Regression](#1.3)\n    * [Support Vector Parameters](#1.4)","32b59dcd":"* kernelstring, optional (default=\u2019rbf\u2019) It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019,\n* degree Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n* loss str, \u2018hinge\u2019 or \u2018squared_hinge\u2019\n* For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy\n* gamma parameter defines how far the influence of a single training example reaches, with low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019.","864efe78":"# Kernel <a id=\"1.1\"><\/a>","372fdbc7":"# A Small value of C","a38271dd":"# Polynomial Kernel <a id=\"1.2\"><\/a>","63f29bda":"** Nodel Evalutation**","164d4380":"# The decison boundary of iris","7f748b75":"![image.png](attachment:image.png)","1fb37737":"![image.png](attachment:image.png)","704e26a4":"# Support Vector Regression <a id=\"1.4\"><\/a>","52e65f34":"![image.png](attachment:image.png)","f43daffa":"![image.png](attachment:image.png)","a79397f2":"# Hinge Loss","fc0faa8d":"![image.png](attachment:image.png)","86366252":"** Fittig the model**","55df73ef":"![image.png](attachment:image.png)   Wikipedia","c83b3cf4":"# Getting Started <a id=\"1\"><\/a>\nHere we describe importing the library, impoting the datset and some basic checks on the dataset","426d435c":"# Linear Kernel <a id=\"1.2\"><\/a>","d57df54c":"Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.**","da3eaf20":"** Getting the model and the data**","d715a9a1":"# SVC with Linear Kernel Versus Linerar SVC"}}