{"cell_type":{"ce51a20a":"code","f65bcf98":"code","1391d6e0":"code","49e34ff3":"code","2bdefe46":"code","443b35da":"code","8f89e138":"code","c8e33694":"code","c5cb48d9":"code","ddab9cfa":"code","43fd3747":"code","16fbf430":"code","0b259525":"code","71fd5ce7":"code","6eb56e0a":"code","359c5b0f":"code","bd510bba":"code","c96f8e04":"code","26c31942":"code","5e450896":"code","d10df99f":"code","6d4779d8":"code","ad27c29c":"markdown","b649056a":"markdown","046e2eee":"markdown","24b15021":"markdown","e7f0a27e":"markdown","8a6ff091":"markdown","86752050":"markdown","757bdb48":"markdown","57d268f6":"markdown","0e3d67c7":"markdown","d25e8ffa":"markdown","73d1b667":"markdown","16a10598":"markdown"},"source":{"ce51a20a":"!pip install fairseq\n!pip install fastBPE","f65bcf98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch\nimport argparse\nfrom sklearn import model_selection\nimport tokenizers\n\nfrom transformers import RobertaConfig\nfrom transformers import RobertaModel\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport random\nimport gc\n\nfrom types import SimpleNamespace\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\nfrom fairseq import options  \n\npath='..\/input\/tweet-sentiment-extraction\/'\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1391d6e0":"df = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ndf=df.dropna()\n\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold\n\n\ndf.to_csv(\"train_folds.csv\", index=False)","49e34ff3":"!tail train_folds.csv","2bdefe46":"parser = options.get_preprocessing_parser()  \nparser.add_argument('--bpe-codes', type=str,default=\"..\/input\/bertweet-base-transformers\/bpe.codes\")  ","443b35da":"class BERTweetTokenizer():\n    \n    def __init__(self,pretrained_path = '..\/input\/bertweet-base-transformers\/',parser=parser):\n        \n\n        self.bpe = fastBPE(args=parser.parse_args(args=[]))\n        self.vocab = Dictionary()\n        self.vocab.add_from_file(pretrained_path + \"dict.txt\")\n        self.cls_token_id = 0\n        self.pad_token_id = 1\n        self.sep_token_id = 2\n        self.pad_token = '<pad>'\n        self.cls_token = '<s> '\n        self.sep_token = ' <\/s>'\n        \n    def bpe_encode(self,text):\n        return self.bpe.encode(text)\n    \n    def encode(self,text,add_special_tokens=False):\n        subwords = self.cls_token + self.bpe.encode(text) + self.sep_token\n        input_ids = self.vocab.encode_line(subwords, append_eos=False, add_if_not_exist=True).long().tolist()\n        return input_ids\n    \n    def tokenize(self,text):\n        return self.bpe_encode(text).split()\n    \n    def convert_tokens_to_ids(self,tokens):\n        input_ids = self.vocab.encode_line(' '.join(tokens), append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n\n    \n    def decode_id(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@')","8f89e138":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    config = RobertaConfig.from_pretrained(\n    \"..\/input\/bertweet-base-transformers\/config.json\")\n    config.output_hidden_states = True\n\n    BERTweet = RobertaModel.from_pretrained(\n    \"..\/input\/bertweet-base-transformers\/model.bin\",\n    config=config)\n    BERTweetpath=\"..\/input\/bertweet-base-transformers\/\"\n    TRAINING_FILE = \"train_folds.csv\"\n    TOKENIZER = BERTweetTokenizer('..\/input\/bertweet-base-transformers\/',parser=parser)","c8e33694":"config.TOKENIZER.encode('positive negative neutral'),config.TOKENIZER.decode_id([1809,4])","c5cb48d9":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \n    tweet_orig = \" \".join(str(tweet).split())\n    selected_text = \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text)\n\n\n    idx = tweet_orig.find(selected_text)\n    char_targets = np.zeros((len(tweet_orig)))\n    char_targets[idx:idx+len(selected_text)]=1\n\n    tok_tweet = config.TOKENIZER.encode(tweet_orig)\n\n    # Convert into torch tensor\n    all_input_ids = torch.tensor([tok_tweet], dtype=torch.long)\n\n    tok_tweet=tok_tweet[1:-1]\n\n    # ID_OFFSETS\n    offsets = []; idx=0\n    \n    try:\n        for t in tok_tweet:\n            ix=0\n            w = config.TOKENIZER.decode_id([t])\n\n            #print(\"==\",w,len(w))\n            if tweet[tweet.find(w)-1]==' ':   #to consider spaces in the offsets\n                offsets.append((idx+1,idx+1+len(w)))\n                idx =idx+1+ len(w)\n                ix=ix+1+ len(w)\n            else:\n                offsets.append((idx,idx+len(w)))\n                idx += len(w)\n                ix+=len(w)\n\n            tweet=tweet[ix:]\n    except:\n        print(\"***\",tweet_orig)\n        pass\n\n    input_ids_orig = tok_tweet\n    tweet_offsets = offsets\n\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if np.sum(char_targets[offset1:offset2])> 0:\n            target_idx.append(j)\n\n    if  len(target_idx)>0:\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n\n    else:\n        targets_start = 0\n        targets_end= len(char_targets)\n\n\n    #print(targets_start,targets_end)    \n    sentiment_id = {\n        'positive': 1809,\n        'negative': 3392,\n        'neutral': 14058\n    }\n\n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet_orig,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","ddab9cfa":"class TweetDataset:\n    \n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","43fd3747":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = RobertaModel.from_pretrained(\"..\/input\/bertweet-base-transformers\/model.bin\",config=conf)\n        \n        self.drop_out = nn.Dropout(0.1)\n        self.Cov1S = nn.Conv1d(768 , 128 , kernel_size = 2 ,stride = 1 )\n        self.Cov1E = nn.Conv1d(768, 128, kernel_size = 2 ,stride = 1 )\n        self.Cov2S = nn.Conv1d(128 , 64 , kernel_size = 2 ,stride = 1)\n        self.Cov2E = nn.Conv1d(128 , 64 , kernel_size = 2 ,stride = 1)\n        self.lS = nn.Linear(64 , 1)\n        self.lE = nn.Linear(64 , 1)\n        \n        #self.lstm = nn.LSTM(1536,768,1, batch_first=True)\n        \n        torch.nn.init.normal_(self.lS.weight, std=0.02)\n        torch.nn.init.normal_(self.lE.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        #out = torch.cat((out[-1], out[-2]), dim=-1)\n        \n        out = self.drop_out(out[-1])\n        out = out.permute(0,2,1)     #m,768,192\n        \n        same_pad1 = torch.zeros(out.shape[0] , 768 , 1).cuda()  #m,768,1\n        same_pad2 = torch.zeros(out.shape[0] , 128 , 1).cuda()    #m,128,1\n\n        out1 = torch.cat((same_pad1 , out), dim = 2)   #m,768,193\n        \n        \n        out1 = self.Cov1S(out1)              #m,128,192\n        \n        out1 = torch.cat((same_pad2 , out1), dim = 2)   #m,128,193\n        out1 = self.Cov2S(out1)     #m,64,192\n        out1 = F.leaky_relu(out1)\n        out1 = out1.permute(0,2,1)  #m,192,64\n        \n        \n        start_logits = self.lS(out1).squeeze(-1) #m,192\n        #print(start_logits.shape)\n\n        out2 = torch.cat((same_pad1 , out), dim = 2)\n        out2 = self.Cov1E(out2)\n        out2 = torch.cat((same_pad2 , out2), dim = 2)\n        out2 = self.Cov2E(out2)\n        out2 = F.leaky_relu(out2)\n        out2 = out2.permute(0,2,1)\n        end_logits = self.lE(out2).squeeze(-1)\n\n        return start_logits, end_logits","16fbf430":"def dist_between(start_logits, end_logits, device='cuda', max_seq_len=128):\n    \"\"\"get dist btw. pred & ground_truth\"\"\"\n\n    linear_func = torch.tensor(np.linspace(0, 1, max_seq_len, endpoint=False), requires_grad=False)\n    linear_func = linear_func.to(device)\n    assert start_logits.shape == start_logits.shape\n\n    start_pos = (start_logits*linear_func).sum(axis=1)\n    end_pos = (end_logits*linear_func).sum(axis=1)\n\n    diff = end_pos-start_pos\n\n    return diff.sum(axis=0)\/diff.size(0)\n\n\ndef dist_loss_fn(start_logits, end_logits, start_positions, end_positions, device, max_seq_len=128, scale=1):\n    \"\"\"\n    calculate distance loss between prediction's length & GT's length\n    \n    Input\n    - start_logits ; shape (batch, max_seq_len{128})\n        - logits for start index\n    - end_logits\n        - logits for end index\n    - start_positions ; shape (batch, 1)\n        - start index for GT\n    - end_positions\n        - end index for GT\n    \"\"\"\n    start_logits = torch.nn.Softmax(1)(start_logits) # shape ; (batch, max_seq_len)\n    end_logits = torch.nn.Softmax(1)(end_logits)\n    \n    # one hot encoding for GT (start_positions, end_positions)\n    start_logits_gt = torch.zeros([len(start_positions), max_seq_len], requires_grad=False).to(device)\n    end_logits_gt = torch.zeros([len(end_positions), max_seq_len], requires_grad=False).to(device)\n    for idx, _ in enumerate(start_positions):\n        _start = start_positions[idx]\n        _end = end_positions[idx]\n        start_logits_gt[idx][_start] = 1\n        end_logits_gt[idx][_end] = 1\n\n    pred_dist = dist_between(start_logits, end_logits, device, max_seq_len)\n    gt_dist = dist_between(start_logits_gt, end_logits_gt, device, max_seq_len) # always positive\n    diff = (gt_dist-pred_dist)\n\n    rev_diff_squared = 1-torch.sqrt(diff*diff) # as diff is smaller, make it get closer to the one\n    loss = -torch.log(rev_diff_squared) # by using negative log function, if argument is near zero -> inifinite, near one -> zero\n\n    return loss*scale","0b259525":"def loss_fn(start_logits, end_logits, start_positions, end_positions,device):\n    loss_fct = nn.CrossEntropyLoss()\n    \n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    \n    dist_loss = dist_loss_fn(start_logits, end_logits,start_positions, end_positions,device, config.MAX_LEN) \n    \n    total_loss = (start_loss + end_loss)\n    \n    return total_loss+dist_loss\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \n        \nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        \n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            \n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","71fd5ce7":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    \n    model.train()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n        #print(\"(())\",bi,d['orig_selected'])\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        \n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        \n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end,device)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        \n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            jaccard_scores.append(jaccard_score)\n\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)","6eb56e0a":"def eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end,device)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    \n    print(f\"Jaccard = {jaccards.avg}\")\n    return jaccards.avg","359c5b0f":"def run(fold,seed=None):\n    dfx = pd.read_csv(config.TRAINING_FILE)\n    \n    dfx['text']=dfx['text'].apply(lambda x: x.strip())\n    dfx['selected_text']=dfx['selected_text'].apply(lambda x: x.strip())\n    \n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=2\n    )\n\n    device = torch.device(\"cuda\")\n#     model_config = config.config\n#     model_config.output_hidden_states = True\n    model = TweetModel(config.config)\n    model.to(device)\n\n    num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=4e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = EarlyStopping(patience=3, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(config.EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n        \n        if es.early_stop:\n            print(\"Early stopping\")\n            break","bd510bba":"run(fold=0)\nrun(fold=1)\nrun(fold=2)\nrun(fold=3)\nrun(fold=4)","c96f8e04":"gc.collect()","26c31942":"# def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \n#     tweet_orig = \" \".join(str(tweet).split())\n#     selected_text = \" \".join(str(selected_text).split())\n\n#     len_st = len(selected_text)\n\n\n#     idx = tweet_orig.find(selected_text)\n#     char_targets = np.zeros((len(tweet_orig)))\n#     char_targets[idx:idx+len(selected_text)]=1\n\n#     tok_tweet = config.TOKENIZER.encode(tweet_orig)\n\n#     # Convert into torch tensor\n#     all_input_ids = torch.tensor([tok_tweet], dtype=torch.long)\n\n#     tok_tweet=tok_tweet[1:-1]\n\n#     # ID_OFFSETS\n#     offsets = []; idx=0\n    \n#     try:\n#         for t in tok_tweet:\n#             ix=0\n#             w = config.TOKENIZER.decode_id([t])\n\n#             print(\"==\",w,len(w))\n#             if tweet[tweet.find(w)-1]==' ':\n#                 offsets.append((idx+1,idx+1+len(w)))\n#                 idx =idx+1+ len(w)\n#                 ix=ix+1+ len(w)\n#             else:\n                \n#                 offsets.append((idx,idx+len(w)))\n#                 idx += len(w)\n#                 ix+=len(w)\n\n#             tweet=tweet[ix:]\n#     except:\n#         print(\"***\",tweet_orig)\n        \n\n#     input_ids_orig = tok_tweet\n#     tweet_offsets = offsets\n\n\n#     target_idx = []\n#     for j, (offset1, offset2) in enumerate(tweet_offsets):\n#         if np.sum(char_targets[offset1:offset2])> 0:\n#             target_idx.append(j)\n\n#     if  len(target_idx)>0:\n#         targets_start = target_idx[0]\n#         targets_end = target_idx[-1]\n\n#     else:\n#         targets_start = 0\n#         targets_end= len(char_targets)\n\n\n#     #print(targets_start,targets_end)    \n#     sentiment_id = {\n#         'positive': 1809,\n#         'negative': 3392,\n#         'neutral': 14058\n#     }\n\n#     input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n#     token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n#     mask = [1] * len(token_type_ids)\n#     tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n#     targets_start += 4\n#     targets_end += 4\n\n#     padding_length = max_len - len(input_ids)\n#     if padding_length > 0:\n#         input_ids = input_ids + ([1] * padding_length)\n#         mask = mask + ([0] * padding_length)\n#         token_type_ids = token_type_ids + ([0] * padding_length)\n#         tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n#     return {\n#         'ids': input_ids,\n#         'mask': mask,\n#         'token_type_ids': token_type_ids,\n#         'targets_start': targets_start,\n#         'targets_end': targets_end,\n#         'orig_tweet': tweet_orig,\n#         'orig_selected': selected_text,\n#         'sentiment': sentiment,\n#         'offsets': tweet_offsets\n#     }","5e450896":"# dfx = pd.read_csv(config.TRAINING_FILE)\n# #dfx['text']=dfx['text'].apply(lambda x: x.strip())\n# dfx=dfx[dfx['textID']==\"77e645b46c\"]\n# dfx\n# #,dfx[dfx['selected_text']=='found any decently priced breakfast yet? i hope you do']['selected_text']","d10df99f":"#a=process_data(dfx.loc[287]['text'],dfx.loc[287]['selected_text'],dfx.loc[287]['sentiment'],config.TOKENIZER,config.MAX_LEN)","6d4779d8":"# filtered_output  = \"\"\n# for ix in range(a['targets_start'], a['targets_end'] + 1):\n#     filtered_output += a['orig_tweet'][a['offsets'][ix][0]: a['offsets'][ix][1]]\n#     if (ix+1) < len(a['offsets']) and a['offsets'][ix][1] < a['offsets'][ix+1][0]:\n#         filtered_output += \" \"\n        \n# filtered_output","ad27c29c":"<font size=4 color='green'>Loss function and helpers<\/font>","b649056a":"<font size=4 color='green'>Eval loop<\/font>","046e2eee":"<font size=4 color='green'>Folds creation:<\/font>","24b15021":"<font size=4 color='green'>Test runs<\/font>","e7f0a27e":"<p>This is a simple intro for using Bertweet without changing much in the existing pipeline.<\/p>\n\n<p>\nNote:\nThe offset has a bug in extracting few observations, nonetheless its worth a shot! Go for it <\/p>","8a6ff091":"<font size=4 color='green'>Bertweet tokenizer class<\/font>","86752050":"<font size=4 color='green'>Starting the engine<\/font>","757bdb48":"<font size=4 color='green'>Dataloader<\/font>","57d268f6":"<font size=4 color='green'>Model class<\/font>","0e3d67c7":"<font size=4 color='green'>Unfolding<\/font>","d25e8ffa":"<font size=4 color='green'>References:<\/font>\n\n<p>1)https:\/\/github.com\/VinAIResearch\/BERTweet<\/p>\n<p>2)https:\/\/www.kaggle.com\/christofhenkel\/setup-tokenizer<\/p>","73d1b667":"<font size=4 color='green'>Configs<\/font>","16a10598":"<font size=4 color='green'>Train loop<\/font>"}}