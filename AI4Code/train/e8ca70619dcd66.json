{"cell_type":{"5dd3ac7f":"code","3c8bba8d":"code","f1b0038a":"code","478018ad":"code","513ca9f0":"code","6868e193":"code","29853230":"code","edf861f8":"code","45b12dc4":"code","f6415dea":"code","b7c2bf8d":"code","083e08a8":"code","a039988d":"code","70fe7468":"code","83b28605":"code","d426ab8b":"code","b8f6d49f":"code","535d075c":"markdown","bcd8218b":"markdown","ea6b654b":"markdown","a7ce66ed":"markdown","a56bfe32":"markdown","50d63f45":"markdown","ab03ceaa":"markdown","767f8de4":"markdown","f377e3c7":"markdown","73769ecf":"markdown"},"source":{"5dd3ac7f":"import tensorflow as tf\nimport numpy as np\nimport os","3c8bba8d":"# Read text\npath_to_file = '\/kaggle\/input\/hp1txt\/hp1.txt'\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')","f1b0038a":"# Build a vocabulary of unique characters in the text\nvocab = sorted(set(text))\n\n# Map each unique char to a different index\nchar2idx = {u: i for i, u in enumerate(vocab)}\n# Map the index to the respective char\nidx2char = np.array(vocab)\n# Convert all the text to indices\ntext_as_int = np.array([char2idx[c] for c in text])\n\n# Maximum length sentence we want for a single input\nseq_length = 100\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_length + 1, drop_remainder=True)","478018ad":"def split_input_target(chunk):\n    ''' Creates an input and target example for each sequence'''\n    input_text = chunk[:-1]  # Removes the last character\n    target_text = chunk[1:]  # Removes the first character\n    return input_text, target_text","513ca9f0":"# Get inputs and targets for each sequence\ndataset = sequences.map(split_input_target)","6868e193":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","29853230":"# Batch size\nBATCH_SIZE = 64\nBUFFER_SIZE = 10000\n# Suffle the dataset and get batches\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","edf861f8":"vocab_size = len(vocab)\nembedding_dim = 256\nrnn_units = 1024","45b12dc4":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    ''' Builds a simple sequencial 3 layers model '''\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.GRU(rnn_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)])\n    return model","f6415dea":"model = build_model(vocab_size=vocab_size,\n                    embedding_dim=embedding_dim,\n                    rnn_units=rnn_units,\n                    batch_size=BATCH_SIZE)","b7c2bf8d":"model.summary()","083e08a8":"def loss(labels, logits):\n    ''' Performs Sparce Caterorical Crossentropy Loss '''\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nmodel.compile(optimizer='adam', loss=loss)","a039988d":"# Define checkpoint path for each batch\ncheckpoint_path = \"\/kaggle\/working\/training_checkpoints\/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    verbose=1,\n    save_weights_only=True)","70fe7468":"# Number of epochs (full training pass over the entire dataset)\nEPOCHS = 10\n# Train the model\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","83b28605":"def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n    '''Generates text using the learned model'''\n\n    # Converting our start string to numbers\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our result\n    text_generated = []\n    # Resets the state of metrics\n    model.reset_states()\n\n    for _ in range(num_generate):\n        predictions = model(input_eval)\n        # Remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # Using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(\n            predictions, num_samples=1)[-1, 0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","d426ab8b":"# Build network structure\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n# Load the weights of our latest learned model\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n# Build the learned model\nmodel.build(tf.TensorShape([1, None]))","b8f6d49f":"# Make predictions\npredicted_text = generate_text(\n    model, start_string='Harry ', num_generate=1000, temperature=1.0)\n\nprint(predicted_text)","535d075c":"## Build the model","bcd8218b":"### Create input and target examples","ea6b654b":"The `start_string` parameter lets you define the first word of the paragraph. And the `tempeture` parameter lets you control the weirdness of the results.","a7ce66ed":"## Create training batches","a56bfe32":"## Generate text","50d63f45":"## Train the model","ab03ceaa":"## Process text: Build vocabulay and convert char to indices","767f8de4":"### Load latest checkpoints and restore model","f377e3c7":"## Read data","73769ecf":"### Make Predictions"}}