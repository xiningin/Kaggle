{"cell_type":{"e1f22dde":"code","89391ad7":"code","abe06527":"code","08d2dedd":"code","79746e29":"code","a80b5932":"code","025cad9f":"code","027ac2c0":"code","edf2ecab":"code","f58c75bc":"code","188b9d51":"code","6df8ffab":"code","d8c014e7":"code","3fb708ee":"code","61250576":"code","ce599dfc":"code","8d932667":"code","7dc6be0a":"code","e778e04a":"code","ceb5a6ca":"code","15b26977":"code","0fe786d7":"code","3c020a10":"code","2627625f":"code","49a4c873":"code","0980c476":"code","6f463c2a":"code","bcad4e10":"code","d149f21b":"code","8a6a63a7":"code","e8e582c6":"code","2fb5af19":"code","b91214ac":"markdown","cfadbd59":"markdown","d2095d6b":"markdown","4381a6b8":"markdown","a99decf2":"markdown","1ee8a2c8":"markdown","63407d96":"markdown"},"source":{"e1f22dde":"import os\nimport time \nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import models, utils, transforms\n\nfrom glob import glob\nfrom PIL import Image","89391ad7":"data_path = os.path.join(\"\/kaggle\", \"input\", \"style-transfer\")\ncontent_path = os.path.join(data_path, \"content\")\nstyle_path = os.path.join(data_path, \"style\")\ncontent_fps = sorted(glob(os.path.join(content_path, \"*\")))\nstyle_fps = sorted(glob(os.path.join(style_path, \"*\")))\nprint(content_fps)\nprint(style_fps)","abe06527":"transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\ninverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])","08d2dedd":"content_images = []\nfor content_fp in content_fps:\n    image = Image.open(content_fp) \n    image = transform(image) # Size([3, 512, 512])\n    content_images.append(image)\n    \nstyle_images = []\nfor style_fp in style_fps:\n    image = Image.open(style_fp) \n    image = transform(image) # Size([3, 512, 512])\n    style_images.append(image)","79746e29":"def get_grid(images):\n\n    grid = utils.make_grid(images)\n    grid = grid.transpose(0, 2).transpose(0 ,1)\n\n    return grid","a80b5932":"content_grid = get_grid(content_images)\nstyle_grid = get_grid(style_images)\nfig, axes = plt.subplots(2, 1, figsize=(20, 10))\naxes[0].imshow(content_grid)\naxes[0].set_title(\"Content Images Normalized\")\naxes[1].imshow(style_grid)\naxes[1].set_title(\"Style Images Normalized\")\nplt.show()","025cad9f":"content_images_raw = []\nfor image in content_images:\n    image_raw = inverse_transform(image)\n    content_images_raw.append(image_raw)\n    \nstyle_images_raw = []\nfor image in style_images:\n    image_raw = inverse_transform(image)\n    style_images_raw.append(image_raw)","027ac2c0":"content_grid = get_grid(content_images_raw)\nstyle_grid = get_grid(style_images_raw)\nfig, axes = plt.subplots(2, 1, figsize=(20, 10))\naxes[0].imshow(content_grid)\naxes[0].set_title(\"Content Images\")\naxes[1].imshow(style_grid)\naxes[1].set_title(\"Style Images\")\nplt.show()","edf2ecab":"vgg = models.vgg19(pretrained=True)","f58c75bc":"for p in vgg.parameters():\n    p.requires_grad_(False)","188b9d51":"vgg","6df8ffab":"for i, feature in enumerate(vgg.features):\n    if isinstance(feature, nn.MaxPool2d):\n        vgg.features[i] = nn.AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)","d8c014e7":"vgg.features","3fb708ee":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","61250576":"vgg = vgg.to(device).eval()","ce599dfc":"content_layers = ['conv4_2']\nstyle_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n\nid_to_layer = {\n    '0': 'conv1_1', \n    '5': 'conv2_1', \n    '10': 'conv3_1', \n    '19': 'conv4_1',\n    '21': 'conv4_2',  \n    '28': 'conv5_1'\n}\nlayer_to_id = {v:k for k, v in id_to_layer.items()}\nprint(id_to_layer)\nprint(layer_to_id)\n\ncontent_layers_id = [int(layer_to_id[l]) for l in content_layers]\nstyle_layers_id = [int(layer_to_id[l]) for l in style_layers]\nprint(content_layers_id)\nprint(style_layers_id)","8d932667":"def extract_content_features(image):\n    \"\"\"\n    image -- ImageTensor of Size([n_c, n_h, n_w])\n    \"\"\"\n    X = image.unsqueeze(0)\n    content_features = []\n    for i, feature in enumerate(vgg.features):\n        X = feature(X)\n        if i in content_layers_id:\n            content_feature = X.view(-1)\n            content_features.append(content_feature)\n        \n    return content_features","7dc6be0a":"def extract_style_features(image):\n    \"\"\"\n    image -- ImageTensor of Size([n_c, n_h, n_w])\n    \"\"\"\n    X = image.unsqueeze(0)\n    style_features = []\n    for i, feature in enumerate(vgg.features):\n        X = feature(X)\n        if i in style_layers_id:\n            out = X.view(X.size(1), -1)\n            style_feature = torch.mm(out.clone(), out.t().clone())\n            style_feature = style_feature.view(-1)\n            style_feature = style_feature \/ len(style_feature)\n            style_features.append(style_feature)\n        \n    return style_features","e778e04a":"def compute_content_loss(content_features_c, content_features_g, weights):\n    \n    weights = torch.Tensor(weights)\n    weights = weights \/ weights.sum()\n    content_loss = 0\n    for feature_c, feature_g, weight in zip(content_features_c, content_features_g, weights):\n        loss = nn.MSELoss(reduction=\"mean\")(feature_c, feature_g)\n        loss = loss * weight\n        content_loss = content_loss + loss\n    content_loss = content_loss \/ len(weights)\n    \n    return content_loss","ceb5a6ca":"def compute_style_loss(style_features_c, style_features_g, weights):\n    \n    weights = torch.Tensor(weights)\n    weights = weights \/ weights.sum()\n    style_loss = 0\n    for feature_c, feature_g, weight in zip(style_features_c, style_features_g, weights):\n        loss = nn.MSELoss(reduction=\"mean\")(feature_c, feature_g)\n        loss = loss * weight\n        style_loss = style_loss + loss\n    style_loss = style_loss \/ len(weights)\n    \n    return style_loss","15b26977":"def compute_loss(content_loss, style_loss, content_weight, style_weight):\n    \n    return (content_weight * content_loss) + (style_weight * style_loss)","0fe786d7":"def generate_image(content_image, style_image, content_weight=100, style_weight=1, epochs=200, lr=0.1, print_steps=1, display_plot=True):\n    \n    print_every = epochs \/\/ print_steps\n    \n    weight_sum = content_weight + style_weight\n    content_weight \/= weight_sum\n    style_weight \/= weight_sum\n    print(\"Weight[content]:{}    Weight[style]:{}\".format(content_weight, style_weight))\n    \n    content_features_c = extract_content_features(content_image.to(device))\n    style_features_s = extract_style_features(style_image.to(device))\n    \n    generated_image = torch.randn((3, 512, 512)).to(device).requires_grad_(True)\n    optimizer = optim.Adam([generated_image], lr=lr)\n    \n    content_losses, style_losses, losses = [], [], []\n    for epoch in tqdm(range(1, epochs+1)):\n        \n        optimizer.zero_grad()\n        \n        content_features_g = extract_content_features(generated_image.to(device))\n        content_loss = compute_content_loss(content_features_c, content_features_g, [1])\n        \n        style_features_g = extract_style_features(generated_image.to(device))\n        style_loss = compute_style_loss(style_features_s, style_features_g, [0.75, 0.5, 0.2, 0.2, 0.2])\n        \n        loss = compute_loss(content_loss, style_loss, content_weight, style_weight)\n        loss.backward()\n        \n        optimizer.step()\n        \n        content_losses.append(content_loss.item())\n        style_losses.append(style_loss.item())\n        losses.append(loss.item())\n        \n        if epoch == 1 or epoch % print_every == 0:\n            message = \"Epoch:{}    Loss:{}    ContentLoss:{}    StyleLoss:{}\".format(epoch,\n                                                                                    loss.item(),\n                                                                                    content_loss.item(),\n                                                                                    style_loss.item())\n            print(message)\n            \n            \n    if display_plot:\n        fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n        axes[0].plot(losses)\n        axes[0].set_title(\"Loss\")\n        axes[1].plot(content_losses)\n        axes[1].set_title(\"Content Loss\")\n        axes[2].plot(style_losses)\n        axes[2].set_title(\"Style Loss\")\n    \n    images = [content_image, style_image, generated_image]\n    images = [inverse_transform(image).detach().to(\"cpu\") for image in images]\n    grid = get_grid(images)\n    plt.figure(figsize=(20, 5))\n    plt.imshow(grid)\n    plt.title(\"Content Image;    Style Image;    Generated Image\")\n    \n    return plt.show()","3c020a10":"generate_image(content_images[0], style_images[0])","2627625f":"generate_image(content_images[0], style_images[3], display_plot=False)","49a4c873":"generate_image(content_images[1], style_images[3], display_plot=False)","0980c476":"generate_image(content_images[1], style_images[4], display_plot=False)","6f463c2a":"generate_image(content_images[2], style_images[0], display_plot=False)","bcad4e10":"generate_image(content_images[2], style_images[3], display_plot=False)","d149f21b":"generate_image(content_images[3], style_images[1], display_plot=False)","8a6a63a7":"generate_image(content_images[3], style_images[2], display_plot=False)","e8e582c6":"generate_image(content_images[4], style_images[0], display_plot=False)","2fb5af19":"generate_image(content_images[4], style_images[4], display_plot=False)","b91214ac":"**Project Homepage:** https:\/\/github.com\/GokulKarthik\/deep-learning-projects-pytorch","cfadbd59":"## 6. Define style transfer","d2095d6b":"## 3. Define utilities","4381a6b8":"## 5. Define losses","a99decf2":"## 1. Load Content and Style Images","1ee8a2c8":" ## 4. Define feature extraction ","63407d96":"## 2. Load model"}}