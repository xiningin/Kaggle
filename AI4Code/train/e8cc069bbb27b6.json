{"cell_type":{"6e50f2c4":"code","e7a040d9":"code","810595e8":"code","543cd9ac":"code","654ef5df":"code","f5c00404":"code","d10fbdae":"code","6d8df7ba":"code","35f65fd8":"code","667c5adf":"code","d76fdeb3":"code","cb7dbea4":"code","870028c4":"code","ef0b83ff":"code","ff7cbe42":"code","bb2b2739":"code","149f5126":"code","74c00fab":"code","bbb5d48e":"code","5c9a51e0":"code","0e5363e8":"code","f150962a":"code","e83a465f":"code","d40f0cf2":"code","4ef4d326":"code","1e0086cb":"code","7188f313":"code","601ca8dc":"code","59f05c58":"code","61a61891":"code","c011d50d":"code","766b38e0":"code","4fa4b01e":"code","96afcdd5":"code","98f5f5fb":"code","2527f0b1":"code","94bb3551":"code","319fd9f4":"code","842f6a6b":"code","1063cd94":"code","dccaefc0":"code","96325de4":"code","70bc3727":"code","9f237b43":"code","869e066a":"code","efaf739a":"code","63a3ef71":"code","68616261":"code","ec496683":"code","0100e322":"code","953ecde7":"code","2f84af1f":"markdown","41771e4e":"markdown","32189c53":"markdown","ff11b967":"markdown","bd93afde":"markdown","1b942dbb":"markdown","992bfd38":"markdown","a352a119":"markdown","52a853a8":"markdown","d6182c58":"markdown","7946fcf1":"markdown","9705d318":"markdown","3170e7da":"markdown","21162e4f":"markdown","5ac95e78":"markdown","d302b4b0":"markdown","6bc423a2":"markdown","a0593f7a":"markdown","a7e4d1c7":"markdown","cce3b1d8":"markdown","42678bf7":"markdown","a5aa9251":"markdown"},"source":{"6e50f2c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7a040d9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Perceptron","810595e8":"df = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\ndf_eval = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/sample_submission.csv')","543cd9ac":"df_eval.describe()","654ef5df":"df.describe()","f5c00404":"df_submission.describe()","d10fbdae":"df.info","6d8df7ba":"# droping columns from data frames (2 cols from dataset and 1 col from evaluation sets)\n#ids = df_eval['id']\nX = df.drop(['Eat','pubchem_id'],axis=1)\ndf_eval = df_eval.drop('pubchem_id',axis=1)\n# array of label from dataset only\ny = df['Eat']\n","35f65fd8":"plt.plot(y)\nplt.show()","667c5adf":"plt.hist(y)\nplt.show()","d76fdeb3":"np.mean(y)","cb7dbea4":"sample = df_eval[:100]\nsample2 = df_eval[:100]","870028c4":"sample.info()","ef0b83ff":"sample2.info()","ff7cbe42":"plt.hist(sample,bins=2)\nplt.show()","bb2b2739":"plt.hist(sample2,bins=2)\nplt.show()","149f5126":"X = np.array(X)","74c00fab":"print(type(X),type(y))","bbb5d48e":"X.shape","5c9a51e0":"df_eval.shape","0e5363e8":"# Import the keras module from tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.layers import Dense, Dropout\nfrom  sklearn.model_selection import train_test_split","f150962a":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=63)","e83a465f":"# set aside 20% of train and test data for evaluation\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n    test_size=0.1, shuffle = True, random_state = 8)\n\n# Use the same function above for the validation set\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n    test_size=0.25, random_state= 8) # 0.25 x 0.9 = 0.225\n\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"y val shape: {}\".format(y_val.shape))","d40f0cf2":"#Feature Scaling\n#from sklearn.preprocessing import StandardScaler\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.transform(X_test)\n#X_val = sc.transform(X_val)","4ef4d326":"X_train","1e0086cb":"y_train","7188f313":"model = tf.keras.Sequential([  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=False),\n  layers.Dense(1024, activation='LeakyReLU', input_shape=(1276,)),\n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=False),\n  layers.Dropout(rate=0.50),  \n  layers.Dense(512, activation='LeakyReLU'), \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,beta_initializer='zeros', gamma_initializer='ones' ,center=True, scale=False),\n  layers.Dropout(rate=0.50),\n  layers.Dense(1024, activation='LeakyReLU'),\n  #layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,beta_initializer='zeros', gamma_initializer='ones' ,center=True, scale=False),\n  layers.Dense(1),\n])\n\nmodel.compile(loss = tf.losses.MeanSquaredError(),\n                      optimizer = tf.optimizers.Adam(learning_rate=0.005), metrics=['mse','RootMeanSquaredError'])","601ca8dc":"model2 = tf.keras.Sequential([ \n  layers.Dense(256, activation='elu', input_shape=(1276,), kernel_initializer='he_normal'),\n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=True), \n  layers.Dropout(rate=0.25),  \n  layers.Dense(128, activation='elu', kernel_initializer='he_normal'),  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=True),\n  layers.Dropout(rate=0.25),      \n  layers.Dense(64, activation='elu',kernel_initializer='he_normal'),  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=True),  \n  layers.Dropout(rate=0.25),  \n  layers.Dense(1)\n])\n\n\n#opt = SGD(lr=0.01, momentum=0.9)\nmodel2.compile(loss = tf.losses.MeanSquaredError(),\n                      optimizer = tf.optimizers.RMSprop(learning_rate=0.005), metrics=['mse','RootMeanSquaredError'])","59f05c58":"model3 = tf.keras.Sequential([ \n  layers.Dense(117, activation='ReLU', input_shape=(1276,)),  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=False),\n  layers.Dropout(rate=0.10),\n  layers.Dense(117, activation='ReLU'),  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=False),\n  layers.Dropout(rate=0.10),  \n  layers.Dense(117, activation='ReLU'),  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=False),\n  layers.Dropout(rate=0.10),  \n  layers.Dense(117, activation='ReLU'),  \n  layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',center=True, scale=False),\n  layers.Dropout(rate=0.10),    \n  layers.Dense(1)\n])\n\nmodel3.compile(loss = tf.losses.MeanSquaredError(),\n                      optimizer = tf.optimizers.Adam(learning_rate=0.005), metrics=['mse','RootMeanSquaredError'])","61a61891":"# note there is some default values fro epochs and batch size)\n# two callback check points (early stop, save best)\n# the function return the trained models\n\ndef my_models(model, train_X, train_y, model_name, epochs=100, batch=100):\n    \n    check_best_point = keras.callbacks.ModelCheckpoint(model_name + \"_my_best_model.h5\", save_best_only=True)\n    training = model.fit(train_X, train_y, validation_split=0.1,validation_data=[X_val, y_val],epochs=epochs, batch_size=batch,callbacks=[keras.callbacks.EarlyStopping(patience=10),check_best_point])\n\n    plt.plot(training.history['loss'], label='loss')\n    plt.plot(training.history['val_loss'],label='val_loss')\n    plt.plot(training.history['mse'], label='mse')\n    plt.plot(training.history['val_mse'],label='val_mse')\n    plt.plot(training.history['root_mean_squared_error'], label='RMSE')\n    plt.plot(training.history['val_root_mean_squared_error'],label='val_RMSE')\n    plt.legend()\n    plt.title(model_name)\n    plt.show()\n    return model","c011d50d":"model_1 = my_models(model,X_train,y_train,'Model1',200, 160)\nmodel_1.save('my_model.h5')","766b38e0":"model_2 = my_models(model2,X_train,y_train,'Model2',200,160)\nmodel_2.save('my_model2.h5')","4fa4b01e":"model_3 = my_models(model3 ,X_train,y_train,'Model3',200,160)\nmodel_3.save('my_model3.h5')","96afcdd5":"model_1 = keras.models.load_model('Model1_my_best_model.h5')","98f5f5fb":"#model_1.weights","2527f0b1":"model_1.evaluate(X_test,y_test,callbacks=[keras.callbacks.EarlyStopping(patience=10),])\nmodel_1.summary()\nmodel_1_predictions = model_1.predict(df_eval)","94bb3551":"model_2 = keras.models.load_model('Model2_my_best_model.h5')","319fd9f4":"#model_2.weights","842f6a6b":"model_2.evaluate(X_test,y_test,callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nmodel_2.summary()\nmodel_2_predictions = model_2.predict(df_eval)","1063cd94":"model_3 = keras.models.load_model('Model3_my_best_model.h5')","dccaefc0":"#model_3.weights","96325de4":"model_3.evaluate(X_test,y_test,callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nmodel_3.summary()\nmodel_3_predictions = model_3.predict(df_eval)","70bc3727":"plt.plot(model_1_predictions)","9f237b43":"plt.plot(model_2_predictions)","869e066a":"plt.plot(model_3_predictions)","efaf739a":"model_1_eval = df_eval.copy()\nmodel_1_eval['Eat'] = model.predict(model_1_eval, callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nmodel_1_eval[['id','Eat']].to_csv('\/kaggle\/working\/submission1.csv', index=False)\nnp.mean(model_1_eval['Eat'])","63a3ef71":"plt.plot(model_1_eval['Eat'])","68616261":"model_2_eval = df_eval.copy()\nmodel_2_eval['Eat'] = model.predict(model_2_eval, callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nmodel_2_eval[['id','Eat']].to_csv('\/kaggle\/working\/submission.csv', index=False)\nnp.mean(model_2_eval['Eat'])","ec496683":"plt.plot(model_2_eval['Eat'])","0100e322":"model_3_eval = df_eval.copy()\nmodel_3_eval['Eat'] = model.predict(model_3_eval, callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nmodel_3_eval[['id','Eat']].to_csv('\/kaggle\/working\/submission3.csv', index=False)\nnp.mean(model_3_eval['Eat'])","953ecde7":"plt.plot(model_3_eval['Eat'])","2f84af1f":"# defining models (model1, model2, model3)","41771e4e":"**Model 2**","32189c53":"# EDA (graphics)","ff11b967":"# checking data type for split method and shapes","bd93afde":"#END OF PREDICTIONS ALL MODELS","1b942dbb":"# basic data analysis (describe, info)","992bfd38":"**end**","a352a119":"# features and label separation...","52a853a8":"# Importing basic modules (libraries)","d6182c58":"**Model 3**","7946fcf1":"***Model 1***","9705d318":"# reading datasets using pandas","3170e7da":"# preparing submmition files  (predictions file creation)","21162e4f":"# importing modules or libtaries for tensorflow (python)","5ac95e78":"# the following flow cells will load the best save model and print model weights follow by evaluation of best model","d302b4b0":"** split data into training, test, and validation sets **","6bc423a2":"# the following flow cells will load the best save model and print model weights follow by evaluation of best model","a0593f7a":"# my_models function (parameter model, train data, model_name, epochs, batch size)","a7e4d1c7":"# calling function and saving model after fitting data and getting return model","cce3b1d8":"# ploting test data evaluation #","42678bf7":"# Sample of instances 100","a5aa9251":"# the following flow cells will load the best save model and print model weights follow by evaluation of best model"}}