{"cell_type":{"ac4c27b9":"code","7a0110a3":"code","f7786a6c":"code","09090a22":"code","663821d0":"code","ee363ddc":"code","5f77d748":"code","883becd4":"code","5c7070c9":"code","ea8291c9":"code","db31ac93":"code","68e81969":"code","265436de":"code","e50047d4":"code","fd57ee57":"code","783e3d43":"code","40945b1a":"code","c0f4c196":"code","3ffcd797":"code","ad8969db":"code","f830a34f":"code","c2d2adb8":"code","706d0219":"code","8313bf2c":"code","b6459bb6":"code","ef8f715f":"code","efa67aab":"code","f6a26d4c":"code","355446a2":"code","1bbf463e":"code","d1a55e4a":"code","a61d8db2":"code","0f046383":"code","6b8ce723":"code","7b2e257a":"code","45ef4b42":"code","cfe0cf55":"code","4a962a01":"code","c56a5a9d":"code","d306c64e":"code","14f0b015":"code","93458f1d":"code","cdbdcfc5":"code","63454b59":"code","da6a2b92":"code","5bb5fd5f":"code","c16df186":"code","26648ac6":"code","58e772a3":"code","9acbcb3a":"markdown","edecbfae":"markdown","1a6b6364":"markdown","beddb6f7":"markdown","ecf176de":"markdown","1fc93b4e":"markdown","9bf79e49":"markdown","8c306fa1":"markdown","cc7b511c":"markdown","7ab2b8aa":"markdown","77e0068b":"markdown","20dc6012":"markdown","65aa8a2f":"markdown","8ad38709":"markdown","54cd39ce":"markdown","9429a081":"markdown","da963803":"markdown","818dff5e":"markdown","dfe1a4a1":"markdown","3fdebc71":"markdown","e9e500b8":"markdown","6620cf61":"markdown","a2a9522f":"markdown","494abc71":"markdown","36573688":"markdown","742e7c79":"markdown","273becfe":"markdown","31c3e84a":"markdown","69b1fd23":"markdown","bf815654":"markdown","6151e42e":"markdown","7533c208":"markdown","4d60981f":"markdown","aed6cc9f":"markdown","9721b14f":"markdown","42a595ce":"markdown","c5b947cf":"markdown","31e66caa":"markdown","e8cc2a84":"markdown","53261bd6":"markdown","6ba1ac00":"markdown","cb254b09":"markdown","4e749682":"markdown","912ab47e":"markdown","b8aeda53":"markdown","5b7b4fe9":"markdown","ed8c17ac":"markdown","4eecae95":"markdown","d8c6cc72":"markdown","8531fba2":"markdown","d6f0a835":"markdown"},"source":{"ac4c27b9":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport datetime\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom math import sqrt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping\n\nnp.random.seed(234)\ntf.random.set_seed(234)","7a0110a3":"print(tf.__version__)","f7786a6c":"monthly_raw = pd.read_csv('..\/input\/usa-key-macroeconomic-indicators\/macro_monthly.csv',parse_dates=True)\nmonthly_raw.shape","09090a22":"monthly_raw.dropna(inplace=True)\nmonthly_raw.head(5)","663821d0":"monthly_raw.dtypes\n\nmonthly_raw.DATE = pd.to_datetime(monthly_raw.DATE)","ee363ddc":"monthly_raw['DATE'].nunique()","5f77d748":"monthly_df = monthly_raw.copy()","883becd4":"monthly_df['cpi_pct_mom'] = round((monthly_df['ccpi'].pct_change().fillna(0))*100,2)\nmonthly_df['cpi_pct_yoy'] = round((monthly_df['ccpi'].pct_change(12).fillna(0))*100,2)\n\nmonthly_df.iloc[:, 1:13].plot(kind ='line',\n            subplots = True,\n            figsize = (16,16),\n            title = ['Unemployment Rate', 'Personal Saving Rate','M2','Disposable Income','Personal Consumption Expenditure','Real Effective Exchange Rate',\n                     '10Y Treasury Yield','Fed Rate','Construction Spending','Industrial Production Index','Core CPI','Core CPI % Change MoM'],\n            legend = False,\n            layout = (4,3),\n            sharex = True,\n            style = ['midnightblue', 'steelblue', 'dodgerblue', 'slateblue','mediumblue','darkslateblue','red','salmon','brown','maroon','tomato'])\n\nplt.suptitle('27 Year Macroeconomic Indicators for the United States', fontsize = 24)","5c7070c9":"monthly_df['year'] = monthly_df['DATE'].apply(lambda x: x.year)\nmonthly_df['quarter'] = monthly_df['DATE'].apply(lambda x: x.quarter)\nmonthly_df['month'] = monthly_df['DATE'].apply(lambda x: x.month)","ea8291c9":"fig = px.box(monthly_df[12:], x=\"month\", y=\"cpi_pct_yoy\", points = \"all\", template = \"presentation\",)\nfig.update_layout(\n    xaxis = dict(\n        tickmode = 'linear',))\n\nfig = px.box(monthly_df[12:], x=\"quarter\", y=\"cpi_pct_yoy\", points = \"all\", template = \"presentation\")\n\nfig.show()","db31ac93":"fig = px.bar(\n    data_frame=monthly_df.groupby(['month']).std().reset_index(), \n    x=\"month\", \n    y=\"cpi_pct_yoy\", text=\"cpi_pct_yoy\"\n).update_traces(texttemplate='%{text:0.3f}', textposition='outside').update_xaxes(nticks=13)\nfig.show()\n\nfig = px.bar(\n    data_frame=monthly_df.groupby(['quarter']).std().reset_index(), \n    x=\"quarter\", \n    y=\"cpi_pct_yoy\", text=\"cpi_pct_yoy\").update_traces(texttemplate='%{text:0.3f}', textposition='outside').update_xaxes(nticks=5)\nfig.show()","68e81969":"monthly_raw.shape\nmonthly_raw.head(5)","265436de":"df_cpi = monthly_raw.set_index('DATE')","e50047d4":"df_cpi['ccpi'].plot()\nfig = seasonal_decompose(df_cpi['ccpi'], model='additive').plot()","fd57ee57":"split_point = len(df_cpi) - 12\ntrain, test = df_cpi[0:split_point], df_cpi[split_point:]\nprint('Training dataset: %d, Test dataset: %d' % (len(train), len(test)))\ntrain['ccpi'].plot()\ntest['ccpi'].plot()","783e3d43":"diff = train['ccpi'].diff()\nplt.plot(diff)\nplt.show()\ndiff = diff.dropna()","40945b1a":"def adf_test(df):\n    result = adfuller(df.values)\n    if result[1] > 0.05:\n        print(\"Series is not stationary\")\n    else:\n        print(\"Series is stationary\")\n\nadf_test(diff)","c0f4c196":"plot_pacf(diff.values)","3ffcd797":"plot_acf(diff.values)","ad8969db":"arima_model = ARIMA(np.log(train['ccpi']), order = (1,1,1))\n\narima_fit = arima_model.fit()\narima_fit.summary()","f830a34f":"forecast = arima_fit.forecast(steps=12)\nforecast = np.exp(forecast)\n\nplt.plot(forecast, color = 'red')\n\npct_chg = ((forecast[-1] - df_cpi.iloc[-12]['ccpi'])\/df_cpi.iloc[-12]['ccpi']) * 100\nprint('The forecasted U.S. Core Consumer Price Index (CPI) YoY is ' , round(pct_chg,2))","c2d2adb8":"mse = mean_squared_error(test['ccpi'].values, forecast[:12])\nprint('MSE: ', mse)\nmodel_error = test['ccpi'] - forecast\nprint('Mean Model Error: ', model_error.mean())","706d0219":"arima_model = ARIMA(np.log(test['ccpi']), order = (1,1,1),freq=test.index.inferred_freq)\n\narima_fit = arima_model.fit()\n\nforecast = arima_fit.forecast(steps=1)\nforecast = np.exp(forecast)\n\nprint('The Core CPI value for the month November 2021 predicted by ARIMA model is', round(forecast[0],2))","8313bf2c":"scaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(ccpi.reshape(-1,1))\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)\n","b6459bb6":"n_steps_in = 12\n\ntrain, test = dataset[0:310], dataset[310:len(dataset),:]\n\ntrainX, trainY = split_sequence(train, n_steps_in)\ntestX, testY = split_sequence(test, n_steps_in)","ef8f715f":"n_features = trainX.shape[2]\n\nuni_model = Sequential()\n\n# Adding the LSTM layer\nuni_model.add(LSTM(64, input_shape=(trainX.shape[1], n_features)))\n\n# Adding the output layer\nuni_model.add(Dense(1))\n\nuni_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = 'mean_squared_error', metrics=['mean_absolute_error'])\n\nfit = uni_model.fit(trainX, \n          trainY, validation_data = (testX, testY),   \n          epochs = 100, batch_size=1,\n          verbose = 0)\n\n\n# Check for overfitting\nplt.plot(fit.history['loss'], label = 'training', color = 'Blue')\nplt.plot(fit.history['val_loss'], label = 'validation', color = 'Red')\nplt.legend()\nplt.show","efa67aab":"trainPredict = uni_model.predict(trainX)\ntestPredict = uni_model.predict(testX)\n\nYtrain_hat = scaler.inverse_transform(trainPredict)\nYtrain_actual = scaler.inverse_transform(trainY)\nYtest_hat = scaler.inverse_transform(testPredict)\nYtest_actual = scaler.inverse_transform(testY)","f6a26d4c":"trainScore = mean_squared_error(Ytrain_actual, Ytrain_hat[:,0])\nprint('Train Score: %.2f MSE' % (trainScore))\ntestScore = mean_squared_error(Ytest_actual, Ytest_hat[:,0])\nprint('Test Score: %.2f MSE' % (testScore))\n\nmodel_error = Ytest_actual - Ytest_hat[:,0]\nprint('Mean Model Error: ', model_error.mean())","355446a2":"observed = df_cpi.loc['2020-11-01':'2021-10-01',['ccpi']]\nobserved.plot(color = 'SteelBlue', title = 'Actual', legend = False)\nplt.show()\n\npredicted = pd.DataFrame(Ytest_hat, index=pd.date_range('2020-11-01',periods=12,freq='M'))\npredicted.plot(color = 'Firebrick', title = 'Forecasted', legend = False)\nplt.show()","1bbf463e":"x_input = np.array(dataset[-12:])\nx_input = x_input.reshape((1, n_steps_in, n_features))\n\nforecast_normalized = uni_model.predict(x_input)\n\nforecast = scaler.inverse_transform(forecast_normalized)\nprint('The Core CPI value for the month Nov 2021 predicted by LSTM is ', forecast[0][0])\n\npct_chg = ((forecast[0][0] - df_cpi.iloc[-12]['ccpi'])\/df_cpi.iloc[-12]['ccpi']) * 100\nprint('The forecasted U.S. Core Consumer Price Index (CPI) YoY is ' , round(pct_chg,2))\n","d1a55e4a":"monthly_df_stationary = df_cpi.diff().dropna()\nmonthly_df_stationary = monthly_df_stationary.rename_axis('indicator', axis=1)\nfig = px.line(monthly_df_stationary.iloc[:,0:10], facet_col=\"indicator\", facet_col_wrap=1) \nfig.update_yaxes(visible=False)","a61d8db2":"for indi in monthly_df_stationary:\n    print('ADF Test: ', indi)\n    adf_test(monthly_df_stationary[[indi]])\n    \nmonthly_df_stationary[['m2']] = monthly_df_stationary[['m2']].diff().dropna()\nmonthly_df_stationary[['tcs']] = monthly_df_stationary[['tcs']].diff().dropna()\nmonthly_df_stationary[['ccpi']] = monthly_df_stationary[['ccpi']].diff().dropna()","0f046383":"# Drop any NaNs first\nmonthly_df_stationary = monthly_df_stationary.dropna()\n\nfor indi in monthly_df_stationary:\n    print('ADF Test: ', indi)\n    adf_test(monthly_df_stationary[[indi]])","6b8ce723":"maxlag=12\ntest = 'ssr_chi2test'\n\ndef grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n   \n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + '_x' for var in variables]\n    df.index = [var + '_y' for var in variables]\n    return df\n\ngrangers_causation_matrix(monthly_df_stationary, variables = monthly_df_stationary.columns)","7b2e257a":"feat_df = df_cpi.drop(['reer', 'ir','ffer'], axis = 1)","45ef4b42":"scaled = scaler.fit_transform(feat_df)","cfe0cf55":"scaled_df = pd.DataFrame(scaled, columns=feat_df.columns, index=feat_df.index)\nscaled_df.head(5)","4a962a01":"def split_sequences(sequences, n_steps_in, n_steps_out):\n    X, y = list(), list()    \n    for i in range(len(sequences)):\n        #find end of pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out - 1\n        \n        #check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        \n        #gather input and output\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix - 1:out_end_ix, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    \n    return np.array(X), np.array(y)","c56a5a9d":"#inputs\nin_cpi = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['ccpi']]) \nin_ur = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['unrate']])\nin_m2  = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['m2']])\nin_pce = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['pce']])\nin_dspic  = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['dspic']])\nin_tcs = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['tcs']])\nin_psr = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['psr']])\nin_ind = np.array(scaled_df.loc['1994-01-01':'2019-11-01', ['indpro']])\n\ntest_cpi = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['ccpi']]) \ntest_ur = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['unrate']])\ntest_pce = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['pce']])\ntest_dspic  = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['dspic']])\ntest_m2  = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['m2']])\ntest_tcs = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['tcs']])\ntest_psr = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['psr']])\ntest_ind = np.array(scaled_df.loc['2019-12-01':'2021-10-01', ['indpro']])\n\n#output\ntrainoutput_cpi = in_cpi\ntestoutput_cpi = test_cpi","d306c64e":"in_cpi = in_cpi.reshape((len(in_cpi), 1))\nin_ur = in_ur.reshape((len(in_ur), 1))\nin_pce = in_pce.reshape((len(in_pce), 1))\nin_dspic = in_dspic.reshape((len(in_dspic), 1))\nin_m2  = in_m2.reshape((len(in_m2), 1))\nin_tcs = in_tcs.reshape((len(in_tcs), 1))\nin_psr = in_psr.reshape((len(in_psr), 1))\nin_ind = in_ind.reshape((len(in_ind), 1))\n\ntest_cpi = test_cpi.reshape((len(test_cpi), 1))\ntest_ur = test_ur.reshape((len(test_ur), 1))\ntest_pce = test_pce.reshape((len(test_pce), 1))\ntest_dspic = test_dspic.reshape((len(test_dspic), 1))\ntest_m2  = test_m2.reshape((len(test_m2), 1))\ntest_tcs = test_tcs.reshape((len(test_tcs), 1))\ntest_psr = test_psr.reshape((len(test_psr), 1))\ntest_ind = test_ind.reshape((len(test_ind), 1))\n\ntrainoutput_cpi = trainoutput_cpi.reshape((len(trainoutput_cpi), 1))\ntestoutput_cpi = testoutput_cpi.reshape((len(testoutput_cpi), 1))","14f0b015":"trainset = np.hstack((in_cpi, in_ur, in_pce, in_dspic, in_m2, in_tcs, in_psr, in_ind, trainoutput_cpi))\ntestset = np.hstack((test_cpi, test_ur, test_pce, test_dspic, test_m2, test_tcs, test_psr, test_ind, testoutput_cpi))\n\nn_steps_in = 12\nn_steps_out = 1\n\ntrainX, trainy = split_sequences(trainset, n_steps_in, n_steps_out)\n\ntestX, testy = split_sequences(testset, n_steps_in, n_steps_out)\n\ntrainX.shape, trainy.shape","93458f1d":"n_features = trainX.shape[2]\n\nmulti_model = Sequential()\n\n# Adding the LSTM layer and dropout regularizaiton\nmulti_model.add(LSTM(100, return_sequences = True, input_shape=(n_steps_in, n_features)))\nmulti_model.add(LSTM(100))\nmulti_model.add(Dropout(0.2))\n\n# Adding output layer\nmulti_model.add(Dense(n_steps_out))\n\nmulti_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = 'mean_squared_error')\n\nearlystop = EarlyStopping(monitor = 'val_loss', patience =50,\n                  mode = 'min',\n                  verbose = 0)\n\nfit = multi_model.fit(trainX, \n          trainy, validation_data = (testX, testy),   \n          epochs = 500, verbose=0, callbacks = [earlystop])\n\n\n# Check for overfitting\nplt.plot(fit.history['loss'], label = 'training', color = 'Blue')\nplt.plot(fit.history['val_loss'], label = 'validation', color = 'Red')\nplt.legend()\nplt.show()","cdbdcfc5":"def feature_importance(model, g):\n    random_ind = np.random.choice(g.shape[0], 100, replace=False) # Randomly generate 100 numbers arange(218) \n    x = g[random_ind] #  Take 100 random sample from training set\n    orig_out = model.predict(x)\n    for i in range(8):  # iterate over the 7 features\n        new_x = x.copy()\n        perturbation_in = np.random.normal(0.0, 0.7, size=new_x.shape[:2]) # Draw random samples from normal distribution with sd = 0.7, this value is arbitary and would not affect the order of effect as its just introducing noise.\n        new_x[:, :, i] = new_x[:, :, i] + perturbation_in\n        perturbed_out = model.predict(new_x)\n        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n        print(f'Variable {i+1}, Perturbation Effect: {effect:.3f}')\n        \nfeature_importance(multi_model,trainX)","63454b59":"testPredict = multi_model.predict(testX)","da6a2b92":"testX = testX.reshape((testX.shape[0], testX.shape[2]*testX.shape[1]))","5bb5fd5f":"# Invert scaling for Predicted\ntestY_hat = np.concatenate((testX[:, 1:8], testPredict), axis=1)\ntestY_hat = scaler.inverse_transform(testY_hat)\n\ntestY_hat = testY_hat[:,7]\n\n# Invert scaling for Actual\ntestY_actual = np.concatenate((testX[:,1:8], testy), axis=1)\ntestY_actual = scaler.inverse_transform(testY_actual)\n\ntestY_actual = testY_actual[:,7]","c16df186":"mse = mean_squared_error(testY_actual, testY_hat)\nprint('Test MSE: %.3f' % mse)\n\nmodel_error = testY_actual - testY_hat\nprint('Mean Model Error: ', model_error.mean())","26648ac6":"observed = df_cpi.loc['2020-11-01':'2021-10-01',['ccpi']]\nobserved.plot(color = 'SteelBlue', title = 'Actual', legend = False)\nplt.show()\n\npredicted = pd.DataFrame(testY_hat, index=pd.date_range('2020-11-01',periods=12,freq='M'))\npredicted.plot(color = 'Firebrick', title = 'Forecasted', legend = False)\nplt.show()","58e772a3":"x_input = np.array(scaled[-12:])\nx_input = x_input.reshape((1, n_steps_in, n_features))\n\nforecast_normalized = multi_model.predict(x_input)\n\n# Manually inverse Min-max normalization\nmax_cpi = df_cpi['ccpi'].max()\nmin_cpi = df_cpi['ccpi'].min()\nforecast =  max_cpi-forecast_normalized[0][0]\/(max_cpi-min_cpi)\nprint('The Core CPI value for the month Nov 2021 predicted by LSTM is ', forecast)\n\npct_chg = ((forecast - df_cpi.iloc[-12]['ccpi'])\/df_cpi.iloc[-12]['ccpi']) * 100\nprint('The forecasted U.S. Core Consumer Price Index (CPI) YoY is ' , round(pct_chg,2))","9acbcb3a":"## Forecasting Inflation","edecbfae":"M2, TCS and CCPI are now stationary after second differencing. We can proceed to use Grangers Causality Test to investigate causality between our features and Core CPI. Granger causality is a statistical concept of causality that is based on prediction and is highly relevant in financial economics.\n\nThe code below is taken from [stackoverflow](https:\/\/stackoverflow.com\/questions\/58005681\/is-it-possible-to-run-a-vector-autoregression-analysis-on-a-large-gdp-data-with).","1a6b6364":"Now, we stack all the columns horizontally using numpy hstack to get it ready to be passed into the split sequences function. And pass the defined steps in and the single-step forecast.","beddb6f7":"### Training the Model\n\nBuild vanilla model with a single layer, no dropout regularization and no Early Stopping. The number of neurons that will be used is 100, for high dimensionality (so the model can capture the trends). Most parameters are chosen through a fine-tuning and trial-and-error approach.\n\nThe number of layer, 1 is chosen because of the complexity of the problem. <br>No dropout regularization due to the simple complexity nature (there is 1 layer only, and adding dropout would cause important information to be lost) of the built neural network and dataset size. Dropout regularization at optimal levels 0.1\/0.2 for LSTM were trialed and significantly decreased the performance of prediction on the test set. Dropout erase important context information, especially in this problem with limited timesteps and 1 layer. Furthermore, train loss and validation loss are carefully monitored for any overfitting.<br>\nSimilarly, a small learning rate of 0.001 is used due to size of the neural network and small data size.\n\nThe batch size is set as 100 epochs. Batch size fine-tuning is done based on the observation of model's peformance.\n","ecf176de":"From the plot, we can see that 2020 showed a dip due to the pandemic restrictions. The orange line indicates the Test set.\n\n### Take first differences\nHere, we are finding the number of optimal differencing to remove unit root so that the time-series is stationary. This is done by using diff() function and testing with Augmented Dickey-Fuller test.","1fc93b4e":"\n\nNow, we can plot the actual 2020 Core CPI against the forecasted 2020 Core CPI.","9bf79e49":"Overall, we can see that ARIMA performed badly as there was a greater increase in the later months of 2021. We can improve the model by including other features and using multivariate forecasting.\n","8c306fa1":"## ARIMA Implementation\n<a id = \"arima\"><\/a>\nAs the problem is a time-series forecasting, Autoregressive Integrated Moving Average (ARIMA) that combines AR and MA models can be used to forecast future trends\/values.<br> Some upsides of using ARIMA include its interpretability, ease of implementation and may even work better for relatively short series such as this case where the number of observation is not sufficient to apply more sophiscated models.<br> On the other hand, one limitation of ARIMA models is the assumption of constant variance and in financial time-series, most data exhibit volatility, asymmetries, irregular time intervals, sudden outbreaks, thus this model usually perform poorly on financial time series data (Petrica et al., 2016).\n\n**References**:<br>\n- <a src=\"https:\/\/www.datasciencecentral.com\/profiles\/blogs\/arima-sarima-vs-lstm-with-ensemble-learning-insights-for-time-ser\"> ARIMA\/SARIMA vs LSTM with Ensemble learning Insights for Time Series Data by Sharmistha Chatterjee \n-Limitation of ARIMA models in financial and monetary economics by Petrica et al., (2016)\n\n\n### Time Series Decomposition\nDecompose the data into trend, seasonal and residual components","cc7b511c":"Finding the order of Moving Average Term (q)<br>\n- q = 1 and 2 is significant, try conservative take of q = 1.","7ab2b8aa":"### Evaluating the univariate LSTM\n\nSimilar to the ARIMA, we will use MSE and Mean Model Error to evaluate the model's forecasting power.","77e0068b":"Set date as the dataframe's index.","20dc6012":"### Building the model\nSince CPI exhibits exponential growth (variance increases), we build the model on the ln(CPI) e.g. converting the raw values to log values.<br>\nAs earlier discovered, the ARIMA model parameters will be set as 1,1,1.","65aa8a2f":"## Multivariate Time Series Forecasting with LSTMs\n<a id = \"mlstm\"><\/a>\n### Data Preparation\n- Feature Selection with Granger Causality Test\n- Scaling with Min-Max normalization\n- Split multivariate sequence into samples with 12 steps in and 1 step out (<i>Code referenced from machinelearningmastery<\/i>)\n","8ad38709":"### Plot ACF and PACF\nNow, we need to find the optimal p and q using acf and pacf plot. Where p is the number of lags and q is the order of the MA term. <br><br>\nFinding the order of Auto Regressive Term (p)\n- PACF lag 1 is significant","54cd39ce":"From the above line chart, we can see that the univariate LSTM is able to \"approximate\" the shape of the Core CPI (though lagging) and underestimates the value.\n\n### Forecast for November 2021\n\nWe input the last 12 observations (12 steps in) into the model for it to forecast 1 step out which will be the Core CPI value for the month of November 2021.\n\nThen, calculate the year-on-year percentage change manually.","9429a081":"#  Starter Notebook on Forecasting Inflation","da963803":"With the perturbation effect results, we could say that the important features for forecasting Core CPI in this model\/dataset is past **Core CPI**, **Personal Consumption Expenditure**, and **M2**. Personal Consumption is one of the most important feature and this is obvious because PCE is also an important metric in determining inflation thus it should be closely correlated with Core CPI.\n\n\n### Predictions on Test Set\n\nAfter prediction, we need to invert the min-max normalization. To do so, we reshape the data back to the original form before the normalization.\n- Reshape the testX and concatenate with the y-hat (prediction) at the correct positions\n- Reshape the testX and concatenate with the actual y at the correct positions\n\nTo match the order of the original dataframe, we concatenate in the order of unemployment_rate, m2, pce, dspic, ffr, psr and cpi.","818dff5e":"Looking at the Core CPI change in an annualized basis, we can see that the earlier parts of the year showed more significant increases in Core CPI. While in the last few months, there are more outliers (below the lower whiskers) meaning that they showed little change from the previous year.<br> We can further explore the volatility of the change in Core CPI YoY.","dfe1a4a1":"#### Feature Selection\nIn order to see which features is useful for forecasting core CPI, we will be using the Granger Causality test.\nOne of the key assumptions before using this test requires the data to be stationary. Thus, we would take the first differences for each features and use the same ADF test function to check for stationarity.","3fdebc71":"#### Scaling with Min-Max Normalization","e9e500b8":"## Data Understanding\nQuick check on data types and convert realtime_start and date to datetime.","6620cf61":"### 1-Step Forecast for November 2021","a2a9522f":"Similar to the univariate model, we will leave the last 12 months as the test set. We configure the inputs to be from 1994 to 2019 (where 2000 will contain the first 12 time steps in) for the train set and 2019 to 2020 (where 2019 will contain the first 12 steps in) for the test set.","494abc71":"**Note**: The model was early stopped close to the 170th epoch.","36573688":"## Univariate Time Series Forecasting with LSTM\n<a id = \"ulstm\"><\/a>\nLong Short-Term Memory (LSTM) Recurrent Neural Network are popular in making predictions based on time series data, due to the lags of unknown duration. LSTM addresses that issue of vanishing gradient with multiple switch gates to remember longer time steps. Hence the past inputs to the model leaves a footprint. With LSTMs, there is no need to keep a finite number of states beforehand as required in the Hhidden Markov model. Common limitations (risk) of LSTM is that it is easy to overfit and hard to train in a sense that it takes alot of resources (computing power) to train these models fast, requiring memory-bandwidth-bound computation.\n\nAs the problem requires foreccasting the Core CPI value of the January 2021, we can use LSTM to predict a one-step out forecast, using 12 input time steps.\n\nThe LSTM architecture will be a simple, vanilla LSTM with one hidden layer with default activation tanh.\n\n**References**:<br>\n[The fall of RNN \/ LSTM](https:\/\/towardsdatascience.com\/the-fall-of-rnn-lstm-2d1594c74ce0) by Eugenio Culurciello <br>\n[Essentials of Deep Learning : Introduction to Long Short Term Memory](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/fundamentals-of-deep-learning-introduction-to-lstm\/)","742e7c79":"### Scaling and Data Preparation\n- Scaling with Min-Max Normalization\n- Split the univariate sequence into samples with 12 steps in 1 step out \n\n**References**<br>\nThe split sequence code is based on Machine Learning Mastery's code, available [here](https:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/).","273becfe":"Convert the array back to dataframe for resuability purposes","31c3e84a":"### Training the Model\nThe initial model was trialed on this multivariate with similar parameters as the univariate model but resulted in poor performance. The model was further tuned into a 500 epochs with default batch size of 32 and stacking an additional LSTM layer. A GridSearchCV attempt was done before to find the optimal epochs and batch size.\n\nThe model showed significant overfitting, thus regularization with EarlyStopping and Dropout were used. The final Dropout was set at 20% and added between the rec and Dense fully output layer. The EarlyStopping patience was set at 50.\n\nThe hyperparameters were also tuned through manual experimental runs observing the performance.\n\n**References**:<br>\n<li><a src=\"https:\/\/stackoverflow.com\/questions\/48714407\/rnn-regularization-which-component-to-regularize\/58868383#58868383\"> A comprehensive answer to RNN Regularization","69b1fd23":"### Macroeconomic Indicator Trend\n<a id = \"trend\"><\/a>\nWe can perform feature generation first and create percentage change Month-over-month and Year-over-year.","bf815654":"### Forecast for November 2020 - October 2021\nForecast for the next 12 months (12 out-of-sample)","6151e42e":"### Splitting the Data\nAs the dataset is small, we will use the <b>last 12 months as the out of sample test dataset<\/b>.","7533c208":"As compared to the univariate LSTM, the model had a bad start in January was unable to 'approximate' the actual shape of the Core CPI in 2020.","4d60981f":"Here, we can focus on just the **last row**, and we will be using a significance level of 0.05 like earlier, hence any p-values that are less than 0.05, we can reject the null hypothesis and conclude that the feature granger causes Core CPI.<br><br> Real Effective Exchange Rate, 10Y Treasury Yield and Fed Rate are not significant, thus we can first proceed to exclude them from our future models, dropping them from the dataframe.","aed6cc9f":"As above, we create a data structure with 12 timesteps and 1 output.","9721b14f":"This model showed significant improvement compared to ARIMA model with MSE of 1.70 and Mean Model Error of 0.78 which indicates the model gives a slight underestimation. Based on the MSE, the model has an average error of sqrt(1.70) = 1.30.","42a595ce":"We can see a slight overfit.\n\n**Note**: The validation loss shows significant spike and fluctuations due to the small batch_size passed in.\n\n### Predictions on Test Set\n- Make predictions on both the train set and test set\n- Inverse transform from normalized back to the original value.","c5b947cf":"Then, we reshape the input and output data into rows, columns format.","31e66caa":"### Forecast for November 2021","e8cc2a84":"### Evaluating the ARIMA model\n- with RMSE and\n- Mean of observed y - predicted y\n\nBased on the Mean Error, the ARIMA model overestimates the Core CPI value by an average of 0.25.","53261bd6":"#### Splitting Multivariate Sequences into Samples","6ba1ac00":"Create a copy","cb254b09":"### Evaluating the multivariate LSTM\n\nSimilar to the previous models, we will use the same metrics: MSE and Mean Model Error.","4e749682":"After first differencing, M2, TCS and CCPI are still not stationary, so we will take the second differencing for these indicators and check if it still contains unit root.","912ab47e":"Check number of month\/year in dataset","b8aeda53":"Lets take from 1994 onwards, as there are some NaNs for some of the indicators.\nWe can do so by simply dropping the NaNs.","5b7b4fe9":"The data shows a clear upward trend and is not stationary. As one of the key assumptions of the ARIMA model is that the time-series is stationary, we need to correct the non-stationarity later.","ed8c17ac":"## Navigation\nI. <b>Import Libraries and Data Loading<\/b><br>\na) [Import Libraries and Data Loading](#libraries)<br><br>\n\nII. <b> Data Understanding<\/b><br>\na) [Macroeconomic Trends](#trend)<br>\nb) [Core CPI trend by Month and Quarter](#mq)<br><br>\n\nIII. <b> Forecast<\/b><br>\na) [ARIMA](#arima)<br>\nb) [Univariate Forecasting with LSTM](#ulstm)<br>\nc) [Multivariate Forecasting with LSTM](#mlstm)<br>\n","4eecae95":"##  Import Libraries and Data Loading\n<a id=\"libraries\"><\/a>","d8c6cc72":"### Core CPI trend by Month and Quarter\n<a id = \"mq\"><\/a>","8531fba2":"### Feature Importance\n\nThere are a few ways to quantify and identify feature importance that influenced the recurrent neural network. Some of which include Pertubations, [Masking](https:\/\/stackoverflow.com\/questions\/44119207\/is-there-any-way-to-get-variable-importance-with-keras)\/[LIME](https:\/\/arxiv.org\/abs\/1606.05386), [Permutation Importance](https:\/\/www.kaggle.com\/cdeotte\/lstm-feature-importance)  and [SHAP](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html) (however, this led to many compatibility issues with tensorflow 2.7.0 on my end).\n\nHere, we will be using pertubation effect which is quite similar to Masking and LIME. The idea here involves introducing noise\/perturbing each variable with a random normal distribution then caclulate the difference between the perturbed predicted y and original predicted y.\n\n**References**<br>\nMore information on pertubation on Neural Networks available [here](https:\/\/towardsdatascience.com\/perturbation-theory-in-deep-neural-network-dnn-training-adb4c20cab1b) and [here](https:\/\/stats.stackexchange.com\/questions\/191855\/variable-importance-in-rnn-or-lstm).","d6f0a835":"### Augmented Dickey\u2013Fuller test\nWith the small p-value, a 1 differencing is enough to remove unit root and make the series stationary."}}