{"cell_type":{"2d6c7e1f":"code","41cfbb47":"code","c1850d20":"code","adc970c4":"code","a0fa6982":"code","f36a2d66":"code","2b91a0ab":"code","fa4c6e20":"code","b86f1612":"code","2fbd06e7":"code","5492c225":"code","0d7530e2":"code","cf0ad458":"code","d5bb754f":"code","fad79064":"code","d6b7175d":"code","e08a47d4":"code","61d5b1b2":"code","3e713e8f":"code","13b70322":"code","d839d711":"code","a02e5243":"code","4ba2b1b0":"markdown","96d436bd":"markdown","07be8a80":"markdown","d6a5c62d":"markdown","25a83cff":"markdown","244b773b":"markdown","fab1e514":"markdown","d627d26b":"markdown","d3dc0688":"markdown"},"source":{"2d6c7e1f":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler","41cfbb47":"data = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","c1850d20":"data.head()","adc970c4":"data.info()","a0fa6982":"# \uacb0\uce21\uac12 \ud770\uc0c9\uc73c\ub85c \ube44\uc5b4\uc788\uc74c \nimport missingno as msno\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmsno.matrix(data)\nplt.show()","f36a2d66":"# 1. \uacb0\uce21\ub41c \ub370\uc774\ud130\uac00 \ub108\ubb34 \ub9ce\uc740 \uacbd\uc6b0 \ud574\ub2f9 \uc5f4 \uc804\uccb4 \uc0ad\uc81c\ndata1 = data.dropna()\n\nmsno.matrix(data1)\nplt.show()","2b91a0ab":"\n# 2. \uacb0\uce21\ub41c \ub370\uc774\ud130\uac00 \uc77c\ubd80\uc77c \uacbd\uc6b0 \uadf8\ub7f4\ub4ef\ud55c \uac12\uc73c\ub85c \ub300\uccb4\ud558\uae30\n# sklearn.SimpleImputer(mean, median, most_frequent)\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy  = 'most_frequent')\ndata = pd.DataFrame(imputer.fit_transform(data))\ndata","fa4c6e20":"msno.matrix(data)\nplt.show()","b86f1612":"x = data.iloc[:,:-1].values\ny = data.iloc[:,-1:].values\n\ny","2fbd06e7":"data.columns= [['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value','ocean_proximity']]","5492c225":"data.head()","0d7530e2":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(y)\nlabels = encoder.transform(y)\ndata = data.drop('ocean_proximity', axis=1)\ndata['ocean'] = labels","cf0ad458":"data.tail()\ndata.values[:,:-1]","d5bb754f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() \nscaler.fit(data.values[:,:-1])\ndf = scaler.transform(data.values[:,:-1])","fad79064":"df = pd.DataFrame(df)\ndf['9'] = labels\ndf.head()","d6b7175d":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","e08a47d4":"data = torch.from_numpy(df.values).float()\n\ndata.shape","61d5b1b2":"x = data[:, :-1]\ny = data[:, -1:]\n\nprint(x.shape, y.shape)\nprint(x.size(0))","3e713e8f":"n_epochs = 4000\nbatch_size = 256\nprint_interval = 200\n#learning_rate = 1e-2","13b70322":"# Build Model\n\nmodel = nn.Sequential(\n    nn.Linear(x.size(-1), 6), # 8->6\n    nn.LeakyReLU(),\n    nn.Linear(6, 5),\n    nn.LeakyReLU(),\n    nn.Linear(5, 4),\n    nn.LeakyReLU(),\n    nn.Linear(4, 3),\n    nn.LeakyReLU(),\n    nn.Linear(3, y.size(-1)),\n\n)\n\nmodel\n\noptimizer = optim.Adam(model.parameters())","d839d711":"# |x| = (total_size, input_dim)\n# |y| = (total_size, output_dim)\n\n\nfor i in range(n_epochs) :\n  # shuffle the index to feed-forward.\n  # 20640\uac1c \ub370\uc774\ud130 \uc154\ud50c\ub9c1\ud574\uc11c \ub79c\ub364\ud558\uac8c \uc0c8\ub85c \uc778\ub371\uc2a4 \uc124\uc815\ud574\uc8fc\uae30 \n  indices = torch.randperm(x.size(0)) #x.size(0) = 20640\n  x_ = torch.index_select(x, dim=0, index=indices)\n  y_ = torch.index_select(y, dim=0, index=indices)\n\n  x_ = x_.split(batch_size, dim=0)\n  y_ = y_.split(batch_size, dim=0)\n  # |x_[i]| = (batch_size, input_dim)\n  # |y_[i]| = (batch_size, output_dim)\n\n  y_hat = []\n  total_loss = 0\n\n  for x_i, y_i in zip(x_, y_):\n    y_hat_i = model(x_i)\n    loss = F.mse_loss(y_hat_i, y_i)\n\n    optimizer.zero_grad()\n    loss.backward()\n\n    optimizer.step()\n\n    total_loss += float(loss) #this is very important to prevent memory leark.\n    y_hat += [y_hat_i]\n\n  total_loss = total_loss \/ len(x_)\n  if (i + 1) % print_interval == 0:\n    print('Epoch %d : loss=%.4e' % (i+1, total_loss))\n\ny_hat = torch.cat(y_hat, dim=0)\ny = torch.cat(y_, dim=0)\n\n","a02e5243":"df = pd.DataFrame(torch.cat([y, y_hat], dim=1).detach().numpy(),\n                  columns=[\"y\", \"y_hat\"])\n\nsns.pairplot(df, height=4)\nplt.show()\n\n# 5\uac00 \uc798 \uc608\uce21\ub418\uc9c0 \uc54a\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","4ba2b1b0":"### missing values \n\n\n```\n(1) data.info()\ub97c \ud1b5\ud574, total_bedrooms\uc758 \ub370\uc774\ud130\uac00 \uacb0\uce21\ub428\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.\n\n(2) \uacb0\uce21\ub41c \ub370\uc774\ud130 \ucc98\ub9ac \ubc29\ubc95\n  -  \uacb0\uce21\ub41c \ub370\uc774\ud130\uac00 \ub108\ubb34 \ub9ce\uc740 \uacbd\uc6b0 \ud574\ub2f9 \uc5f4 \uc804\uccb4 \uc0ad\uc81c\n  -  \uacb0\uce21\ub41c \ub370\uc774\ud130\uac00 \uc77c\ubd80\uc77c \uacbd\uc6b0 \uadf8\ub7f4\ub4ef\ud55c \uac12\uc73c\ub85c \ub300\uccb4\ud558\uae30\n```\n\n","96d436bd":"### Data load","07be8a80":"### LabelEncoder","d6a5c62d":"### To Model use Pytorch","25a83cff":"### reference\n\n* Missing value : https:\/\/continuous-development.tistory.com\/165\n\n* LabelEncoder : https:\/\/nicola-ml.tistory.com\/62","244b773b":"### Data preprocessing","fab1e514":" - missingno\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacb0\uce21\uac12 \uc704\uce58 \uc2dc\uac01\ud654\ub85c \ud655\uc778\n","d627d26b":"## Adam optimizer  - example \n\n* Data : california-housing-prices.csv\n* The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. \n\n* reference : https:\/\/www.kaggle.com\/camnugent\/california-housing-prices?select=housing.csv","d3dc0688":"### Let's see the result!"}}