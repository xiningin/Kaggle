{"cell_type":{"684e5309":"code","ffa18df0":"code","4ef3ad81":"code","9f6fa830":"code","36b75ee3":"code","741397e9":"code","1d64d2f1":"code","94cde41d":"code","00d4c5dd":"code","b0a93f23":"code","9635e9a8":"code","3c7415a4":"code","37546548":"code","7fae554d":"code","cb94465a":"code","c805e661":"code","1acbf347":"code","085d4026":"code","ab3d4d1e":"code","9de67805":"code","96521781":"code","67b58dce":"code","f91c941f":"code","562ba5fb":"code","75b9c31c":"code","b200af2b":"code","f4bd1d4d":"code","fdbb36fe":"code","06fb0664":"code","45a682b3":"code","57edc2f6":"code","0eef710e":"code","d0efdb2c":"code","d0392eb4":"code","ae49b0e8":"code","82c860dc":"code","b8c381de":"code","f51db446":"code","9f5733e7":"code","3d7bbecd":"code","67ae3d8a":"code","8cf290b9":"code","fe04c731":"code","80e67e22":"code","6d9aa236":"code","492f2677":"code","13c01265":"code","b3f0dbbd":"code","9c4379a2":"code","61d2cf8d":"code","0c656ea4":"code","46918717":"code","f170f74f":"code","fd1d95d6":"code","61097594":"code","5113ecfb":"code","ab962bb9":"code","7c520d3b":"code","ed7e8843":"code","da757a63":"code","cbb05a1e":"code","213aba9b":"code","c50d42a3":"code","223970e0":"code","5afdf7e8":"code","050d132d":"code","a7760992":"code","5586040e":"code","ff8cecbf":"code","28e2f747":"code","a8223793":"code","6dde9730":"code","90ccdf8e":"code","657fa0ab":"code","8ca89ebc":"code","783e6c9d":"code","717142eb":"code","ea2f47bc":"code","41ffaebb":"code","6490e16b":"code","f43ec201":"code","9645b226":"code","570069d0":"code","56f13624":"code","ea97b1f7":"code","1114c289":"code","1efa615b":"code","881dac22":"code","f11c9cb8":"code","08c04fb0":"code","65311439":"code","b81c205f":"code","7ea3f975":"code","b5484749":"code","ce211df2":"code","a32d66b9":"code","5d691622":"code","54ebaf14":"code","cfbf725b":"code","2896aa2a":"code","e2bd4abe":"code","8c722a18":"code","e4cc2c41":"code","aa10f3b6":"code","991ff214":"code","7fcf21d2":"code","634953cf":"code","dc751a84":"code","751f132b":"code","d41901a3":"code","8fb61aae":"code","e21da9fb":"code","99086d42":"code","8f5f1b2d":"code","1cb42c29":"code","51178279":"code","0f0a4675":"code","a6861c91":"code","f2eb1178":"code","22965860":"code","3821bd00":"code","137ed9bf":"code","9fa31def":"code","5c830ff7":"code","dfa179f8":"code","41137197":"code","9f021e75":"code","695ef368":"markdown","be9fd107":"markdown","7bc9f273":"markdown","a19729ec":"markdown","343ca4c5":"markdown","d91806d5":"markdown","8bd7e2b1":"markdown","3de2d8da":"markdown","22aeb471":"markdown","539adf4a":"markdown","5d2ec07e":"markdown","9c9e886e":"markdown","6ce364f5":"markdown","2f8aed89":"markdown","db4f6ac0":"markdown","8c16db9e":"markdown","d4cb1e36":"markdown","0f06171d":"markdown","fe499a85":"markdown","a9566164":"markdown","60ce9fb9":"markdown","b29e75d6":"markdown","3de8bfd4":"markdown","25d86eb2":"markdown","28f70686":"markdown","726f5a0d":"markdown","5588995a":"markdown","b0ba562e":"markdown","0721d0e1":"markdown","358cd029":"markdown","10226e4e":"markdown","20ff248e":"markdown","c2e31675":"markdown","eea3935a":"markdown","fb667cfb":"markdown","9f5b7841":"markdown"},"source":{"684e5309":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffa18df0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","4ef3ad81":"pip install openpyxl","9f6fa830":"#load the dataset\n\ndf = pd.read_excel(\"\/kaggle\/input\/online-retail-customer-segmentation\/Online Retail.xlsx\")\ndf.head()","36b75ee3":"# customer segmentation is performed on available data (remove the data where the transactions were cancelled or returned)","741397e9":"#shape of the data\ndf.shape","1d64d2f1":"df.info()","94cde41d":"print('Number of unique invoice available :', df['InvoiceNo'].nunique())","00d4c5dd":"print('Number of unique stock code available :', df['StockCode'].nunique())","b0a93f23":"df['Quantity'].describe()\n\n#the minimum quantity is a negative value ==> which is not possible we can check that further and rectify","9635e9a8":"df['InvoiceDate'].min(), df['InvoiceDate'].max()\n\n#dataset contains data from 1st December 2010 to 9th December 2011\n#one year data","3c7415a4":"df['UnitPrice'].describe()\n\n#Unitprice is also in negative, we can further check and rectify","37546548":"print('Number of unique stock code available :', df['CustomerID'].nunique())\n\n#there are 4372 unique customer data available","7fae554d":"print('Number of unique stock code available :', df['Country'].nunique())\n\n#there are around 38 countries, we can check the country wise customer coun","cb94465a":"plt.figure(figsize=(15,10))\nsns.countplot(data=df, y='Country')\nplt.show()","c805e661":"df['Country'].value_counts(normalize=True) * 100\n\n#Around 91.5% of the data are from customers in UK\n# We can consider the UK data for further analysis","1acbf347":"df[df['Country'] == 'United Kingdom']['CustomerID'].nunique()\n\n# total customers 4372 - 3950, there is no much loss of the data\n#only in UK there are around 3950, where other 37 countries customers are 400, therefore we will consider only UK data","085d4026":"df = df[df['Country'] == 'United Kingdom']","ab3d4d1e":"df.shape","9de67805":"df_copy = df.copy()","96521781":"#as we are considering only one country we can delete COUNTRY column\n\ndf.drop('Country', axis=1, inplace=True)","67b58dce":"df.info()","f91c941f":"# Null value\n\ndf.isnull().sum()","562ba5fb":"#percentage of null values\ndf.isnull().sum() \/ df.shape[0] * 100\n\n#there are more null values in CustomerID, we cannot impute any value to that because it's mnipulating the data","75b9c31c":"#we will drop those rows with null value\n\ndf.dropna(subset=['CustomerID'], inplace=True)","b200af2b":"df.shape","f4bd1d4d":"#Null values after dropping those rows\ndf.isnull().sum()\n\n#all the null values have been removed","fdbb36fe":"# we have to remove data where the products were returned or cancelled\n#Invoice starts with 'C'\n# Negative quantity\n# negative Unitprice","06fb0664":"#We can remove the Invoice starting with 'C'\n\ndf[df['InvoiceNo'].str.startswith('C', na=False)]\n#7533 transactions has been cancelled\n\ndf = df[~df['InvoiceNo'].str.startswith('C', na=False)]\n\ndf.shape","45a682b3":"#check whether there any negative quantity or unit price\n\ndf[['Quantity', 'UnitPrice']].describe()\n\n#there are no negative values but the unit price for some products are 0\n#its not possible or might be given in discounts\n# we can consider only the products purchased and not the products given for free","57edc2f6":"df = df[~(df['UnitPrice'] == 0)]\ndf.shape","0eef710e":"df.info()","d0efdb2c":"df[df.duplicated() == True].shape\n\n#There are 5118 dupicated values but we are not gonna remove those because there might be chances that instead of adding \n# the product in quantity they might have added a new transaction","d0392eb4":"df.shape","ae49b0e8":"df = df[df['InvoiceDate'] >= '2010-12-09']\n\ndf.shape","82c860dc":"max_date = df['InvoiceDate'].max()\nmax_date","b8c381de":"rfm_df = df.groupby('CustomerID', as_index=False)['InvoiceDate'].max()\n\nrfm_df.head()","f51db446":"#we can find out the recency by subtracting it with the max date\n\n(rfm_df['InvoiceDate'] - max_date)","9f5733e7":"rfm_df['Recency']  = rfm_df['InvoiceDate'].apply(lambda x: (max_date - x).days)","3d7bbecd":"rfm_df.head()","67ae3d8a":"#we can drop invoice date columns\n\nrfm_df.drop('InvoiceDate', axis=1, inplace=True)","8cf290b9":"rfm_df['Frequency'] = df.groupby(['CustomerID'], as_index=False)['InvoiceNo'].nunique()['InvoiceNo']","fe04c731":"rfm_df.head()","80e67e22":"df['Total_amount'] = df['UnitPrice'] * df['Quantity']","6d9aa236":"rfm_df['Monetary'] = df.groupby('CustomerID', as_index=False)['Total_amount'].sum()['Total_amount']","492f2677":"rfm_df.head()","13c01265":"df[df['CustomerID'] == 12346]","b3f0dbbd":"#get the 80% of the revenue\npareto_cutoff = rfm_df['Monetary'].sum() * 0.8\nprint(\"The 80% of total revenue is: \",round(pareto_cutoff,2))","9c4379a2":"customers_rank = rfm_df\n# Create a new column that is the rank of the value of coverage in ascending order\ncustomers_rank['Rank'] = customers_rank['Monetary'].rank(ascending=0)\n#customers_rank.drop('RevenueRank',axis=1,inplace=True)\ncustomers_rank.head()","61d2cf8d":"customers_rank.sort_values('Rank',ascending=True)","0c656ea4":"#get top 20% of the customers\ntop_20_cutoff = 3863 *20 \/100\ntop_20_cutoff","46918717":"#sum the monetary values over the customer with rank <=773\nrevenueByTop20 = customers_rank[customers_rank['Rank'] <= 772]['Monetary'].sum()\nrevenueByTop20","f170f74f":"quantiles = rfm_df.quantile(q=[0.25,0.5,0.75])\nquantiles.drop('CustomerID', axis=1, inplace=True)","fd1d95d6":"quantiles.to_dict()","61097594":"# Arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\ndef RScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1\n# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict)\ndef FMScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4","5113ecfb":"#create rfm segmentation table\nrfm_segmentation = rfm_df\nrfm_segmentation['R_Quartile'] = rfm_segmentation['Recency'].apply(RScore, args=('Recency',quantiles,))\nrfm_segmentation['F_Quartile'] = rfm_segmentation['Frequency'].apply(FMScore, args=('Frequency',quantiles,))\nrfm_segmentation['M_Quartile'] = rfm_segmentation['Monetary'].apply(FMScore, args=('Monetary',quantiles,))","ab962bb9":"rfm_segmentation.head()","7c520d3b":"rfm_segmentation['RFMScore'] = rfm_segmentation.R_Quartile.map(str) \\\n                            + rfm_segmentation.F_Quartile.map(str) \\\n                            + rfm_segmentation.M_Quartile.map(str)\nrfm_segmentation.head()","ed7e8843":"rfm_segmentation[rfm_segmentation['RFMScore']=='444'].sort_values('Monetary', ascending=False).head(10)","da757a63":"print(\"Best Customers: \",len(rfm_segmentation[rfm_segmentation['RFMScore']=='444']))\nprint('Loyal Customers: ',len(rfm_segmentation[rfm_segmentation['F_Quartile']==4]))\nprint(\"Big Spenders: \",len(rfm_segmentation[rfm_segmentation['M_Quartile']==4]))\nprint('Almost Lost: ', len(rfm_segmentation[rfm_segmentation['RFMScore']=='244']))\nprint('Lost Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='144']))\nprint('Lost Cheap Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='111']))","cbb05a1e":"rfm_data = rfm_df.drop(['R_Quartile','F_Quartile','M_Quartile','RFMScore','Rank'],axis=1)\nrfm_data.head()","213aba9b":"rfm_data.index = rfm_data['CustomerID']","c50d42a3":"rfm_data = rfm_data.drop('CustomerID', axis=1, inplace=False)","223970e0":"rfm_data.head()","5afdf7e8":"#check for outliers\nfig, ax = plt.subplots(3, 1, figsize=(15,12))\n\nsns.boxplot(rfm_data['Recency'], ax= ax[0] )\nsns.boxplot(rfm_data['Frequency'], ax=ax[1] )\nsns.boxplot(rfm_data['Monetary'], ax=ax[2])\n\nplt.show()","050d132d":"sns.pairplot(rfm_data, diag_kind='kde')","a7760992":"#instead of removing outliers, we can powertransform the data\n\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()","5586040e":"features = rfm_data.columns","ff8cecbf":"rfm_data_pt = pd.DataFrame(pt.fit_transform(rfm_data), columns=features, index=rfm_data.index)","28e2f747":"rfm_data_pt.describe()","a8223793":"sns.pairplot(rfm_data_pt, diag_kind='kde')","6dde9730":"from sklearn.cluster import KMeans","90ccdf8e":"#elbow method\n\ncluster_errors = []\n\nn_clusters = list(range(2,15))\n\nfor n in n_clusters:\n    kmeans = KMeans(n_clusters=n, n_init=10)\n    kmeans.fit(rfm_data_pt)\n    cluster_errors.append(kmeans.inertia_)\n\ndf_elbow = pd.DataFrame({'No_cluster' : n_clusters, 'WCSS_errors' :cluster_errors })","657fa0ab":"df_elbow","8ca89ebc":"plt.figure(figsize=(10,10))\nplt.plot(df_elbow['No_cluster'], df_elbow['WCSS_errors'], marker='*')\n\nplt.show()\n\n#we can consider n=3, we can recheck with sil","783e6c9d":"from sklearn.metrics import silhouette_score","717142eb":"#silhouette_score\n\nsil_score = []\n\nn_clusters = list(range(2,15))\n\nfor n in n_clusters:\n    kmeans = KMeans(n_clusters=n, n_init=10)\n    kmeans.fit(rfm_data_pt)\n    labels = kmeans.labels_\n    sil_score.append(silhouette_score(rfm_data_pt, labels ))\n\ndf_sil = pd.DataFrame({'No_cluster' : n_clusters, 'Silhouette_score' :sil_score })","ea2f47bc":"df_sil\n\n#we can consider n=2, silhouette_score is high for n=2","41ffaebb":"#Clustering\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(rfm_data_pt)\n\nlabels = kmeans.labels_","6490e16b":"rfm_data_pt['K_labels'] = labels","f43ec201":"rfm_data_pt.head()","9645b226":"sns.pairplot(rfm_data_pt, hue='K_labels')","570069d0":"sns.countplot(rfm_data_pt['K_labels'])","56f13624":"X = rfm_data_pt.iloc[:, 0:3]\nX.head()","ea97b1f7":"from sklearn.cluster import AgglomerativeClustering ","1114c289":"model2 = AgglomerativeClustering(n_clusters=2, affinity='euclidean',  linkage='ward')\n\nmodel2.fit(X)","1efa615b":"rfm_data_pt['Aglo_Labels'] = model2.labels_","881dac22":"from scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\nplt.figure(figsize=(18, 16))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\nZ = linkage(X, 'ward')\ndendrogram(Z,leaf_rotation=90.0,p=25,color_threshold=80,leaf_font_size=10,truncate_mode='level')\nplt.tight_layout()","f11c9cb8":"X_df = pd.DataFrame(X, columns= X.columns)\nX_df[\"group\"] = rfm_data_pt['Aglo_Labels']","08c04fb0":"sns.pairplot(X_df, hue='group')","65311439":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.countplot(rfm_data_pt['K_labels'], ax=ax[0])\nsns.countplot(rfm_data_pt['Aglo_Labels'], ax=ax[1])","b81c205f":"from sklearn.decomposition import PCA","7ea3f975":"X = rfm_data_pt.iloc[:, 0:3]\nX.head()","b5484749":"pca = PCA()\nX = pca.fit_transform(X)","ce211df2":"sns.pairplot(pd.DataFrame(X),diag_kind='kde')","a32d66b9":"sns.heatmap(rfm_data_pt.iloc[:, 0:3].corr(), annot=True)","5d691622":"sns.heatmap(pd.DataFrame(X).corr(), annot=True)","54ebaf14":"pca_var = list(pca.explained_variance_ratio_)\npca_var","cfbf725b":"from itertools import accumulate","2896aa2a":"list(accumulate(pca_var, lambda x,y : x+y ))\n\n#Top 2 components are giving us most of the variance, we can consider PCA=2","e2bd4abe":"X = rfm_data_pt.iloc[:, 0:3]\n\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X)\n\npca_df = pd.DataFrame(pca_X, columns=['PC1', 'PC2'])","8c722a18":"pca.explained_variance_ratio_","e4cc2c41":"pca.components_","aa10f3b6":"#elbow method to find out no of clusters\n\ncluster_errors = []\n\nn_clusters = list(range(2,15))\n\nfor n in n_clusters:\n    kmeans = KMeans(n_clusters=n, n_init=10)\n    kmeans.fit(pca_df)\n    cluster_errors.append(kmeans.inertia_)\n\ndf_elbow = pd.DataFrame({'No_cluster' : n_clusters, 'WCSS_errors' :cluster_errors })","991ff214":"df_elbow","7fcf21d2":"plt.figure(figsize=(10,10))\nplt.plot(df_elbow['No_cluster'], df_elbow['WCSS_errors'], marker='*')\n\nplt.show()\n\n#we can consider n=3, we can recheck with sil","634953cf":"from sklearn.metrics import silhouette_score","dc751a84":"#silhouette_score\n\nsil_score = []\n\nn_clusters = list(range(2,15))\n\nfor n in n_clusters:\n    kmeans = KMeans(n_clusters=n, n_init=10)\n    kmeans.fit(pca_df)\n    labels = kmeans.labels_\n    sil_score.append(silhouette_score(pca_df, labels ))\n\ndf_sil = pd.DataFrame({'No_cluster' : n_clusters, 'Silhouette_score' :sil_score })","751f132b":"df_sil\n\n#we can consider n=2, silhouette_score is high for n=2","d41901a3":"#Clustering\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(pca_df)\n\nlabels = kmeans.labels_","8fb61aae":"X1 = rfm_data_pt.iloc[:, 0:3]\n\nX_df = pd.DataFrame(X1, columns= X1.columns)\nX_df[\"group\"] = labels","e21da9fb":"X_df.head()","99086d42":"pca_df['K_label'] = labels","8f5f1b2d":"sns.pairplot(pca_df, hue='K_label')","1cb42c29":"model2 = AgglomerativeClustering(n_clusters=2, affinity='euclidean',  linkage='ward')\nmodel2.fit(pca_X)","51178279":"label = model2.labels_","0f0a4675":"pca_df['Aglo_label'] = label","a6861c91":"fig, ax = plt.subplots(1,2, figsize=(10,6))\n\nsns.scatterplot(pca_df['PC1'], pca_df['PC2'], hue=pca_df['K_label'], ax=ax[0])\n\nsns.scatterplot(pca_df['PC1'], pca_df['PC2'], hue=pca_df['Aglo_label']   ,ax=ax[1])","f2eb1178":"#We will consider Kmeans with PCA as out best clustering, the shape is clearly separated.","22965860":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import classification_report","3821bd00":"y = pca_df['K_label']\nX = pca_df[['PC1', 'PC2']]","137ed9bf":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","9fa31def":"rfc = RandomForestClassifier()\n\nrfc.fit(X_train, y_train)","5c830ff7":"y_train_proba = rfc.predict_proba(X_train)\ny_train_pred = rfc.predict(X_train)\n\ny_test_proba = rfc.predict_proba(X_test)\ny_test_pred = rfc.predict(X_test)","dfa179f8":"print('Classiifcation report for train data :')\nprint(classification_report(y_train, y_train_pred))","41137197":"print('Classiifcation report for test data :')\nprint(classification_report(y_test, y_test_pred))","9f021e75":"# The labels formed by Kmeans clustering are being classified correctly by RandomForest model.","695ef368":"RFM (**Recency, Frequency, Monetary**) analysis is a customer segmentation technique that uses past purchase **behavior** to divide customers into groups. <br> RFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.\n- RECENCY (R): Days since last purchase \n- FREQUENCY (F): Total number of purchases \n- MONETARY VALUE (M): Total money this customer spent.\n\nWe will create those 3 customer attributes for each customer.\n","be9fd107":"#### RFM Quartiles","7bc9f273":"In ecommerce companies like online retails, customer segmentation is necessary in order to understand customers behaviors. It leverages aqcuired customer data like the one we have in our case, **transactions data** in order to divide customers into groups. \n\nOur goal in this Notebook is to cluster our customers to get insights in:\n- Increasing **revenue** (Knowing customers who present most of our revenue)\n- Increasing customer **retention**\n- Discovering **Trends and patterns** \n- Defining **customers at risk**\n\nWe will do **RFM Analysis** as a first step and then **combine RFM with predictive algorithms (k-means)**. \n\nRFM Analysis answers these questions:\n- Who are our best customers?\n- Who has the potential to be converted in more profitable customers?\n- Which customers we must retain?\n- Which group of customers is most likely to respond to our current campaign?\n","a19729ec":"To calculate recency, we need to choose a date point from which we evaluate **how many days ago was the customer's last purchase**.","343ca4c5":"### Top Customers","d91806d5":"# Applying PCA","8bd7e2b1":"#### Creation of RFM segmentation table","3de2d8da":"We will create two segmentation classes since, high recency is bad, while high frequency and monetary value is good.","22aeb471":"Before moving to customer segments, Let's see the application of Pareto Principle \u2013 commonly referred to as the 80-20 rule on our dataset by applying it to our RFM variables.\n\nPareto\u2019s rule says **80% of the results come from 20% of the causes**.\n\nSimilarly, **20% customers contribute to 80% of your total revenue**. Let's verify that because that will help us know which customers to focus on when marketing new products.","539adf4a":"In our case, the 80% of total revenue is not achieved by the 20% of TOP customers but approximately, it does, because they are less than our 20% TOP customers who achieve it. It would be interesting to study this group of customers because they are those who make our most revenue.","5d2ec07e":"### AgglomerativeClustering","9c9e886e":"Best Recency score = 4: most recently purchase.\nBest Frequency score = 4: most quantity purchase.\nBest Monetary score = 4: spent the most.\n\nLet's see who are our **Champions** (best customers).","6ce364f5":"## Recency\n","2f8aed89":"Frequency helps us to know **how many times a customer purchased from us**. To do that we need to check how many invoices are registered by the same customer.","db4f6ac0":"### Preprocess Data","8c16db9e":"### Classification model, Random forest classification","d4cb1e36":"Now that we have the score of each customer, we can represent our customer segmentation.<br>\nFirst, we need to combine the scores (R_Quartile, F_Quartile,M_Quartile) together.","0f06171d":"## Customer segments with RFM Model","fe499a85":"### K-means Implementation","a9566164":"### Applying 80-20 rule","60ce9fb9":"Monetary attribute answers the question: **How much money did the customer spent over time?**\n\nTo do that, first, we will create a new column total cost to have the total price per invoice.","b29e75d6":"**How many customers do we have in each segment?**","3de8bfd4":"## Attribute Information:\n\nInvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. <br>\nStockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.<br>\nDescription: Product (item) name. Nominal.<br>\nQuantity: The quantities of each product (item) per transaction. Numeric.<br>\nInvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.<br>\nUnitPrice: Unit price. Numeric, Product price per unit in sterling.<br>\nCustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.<br>\nCountry: Country name. Nominal, the name of the country where each customer resides.<br>","25d86eb2":"A common challenge with k-means is that you must tell it how many clusters you expect. Figuring out how many clusters we need is not obvious from data, thus we will try different clusters numbers and check their [silhouette coefficient](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). The [elbow](https:\/\/en.wikipedia.org\/wiki\/Determining_the_number_of_clusters_in_a_data_set#The_elbow_method) method can be used to determine the number of clusters as well.\n\n**Note:** K-means is sensitive to initializations because those initializations are critical to quality of optima found. Thus, we will use smart initialization called ***k-means++***.","28f70686":"## Monetary","726f5a0d":"### UK data","5588995a":"Now that we knew our customers segments we can choose how to target or deal with each segment.\n\nFor example:\n\n**Best Customers - Champions**: Reward them. They can be early adopters to new products. Suggest them \"Refer a friend\".\n\n**At Risk**: Send them personalized emails to encourage them to shop.\n\nMore ideas about what actions to perform in [Ometria](http:\/\/54.73.114.30\/customer-segmentation#).","b0ba562e":"# RFM Analysis","0721d0e1":"## Applying K-means clustering on RFM variables","358cd029":"The simplest way to create customers segments from RFM Model is to use **Quartiles**. We assign a score from 1 to 4 to Recency, Frequency and Monetary. Four is the best\/highest value, and one is the lowest\/worst value. A final RFM score is calculated simply by combining individual RFM score numbers.\n\nNote: Quintiles (score from 1-5) offer better granularity, in case the business needs that but it will be more challenging to create segments since we will have 5*5*5 possible combinations. So, we will use quartiles.","10226e4e":"# Introduction","20ff248e":"## Frequency","c2e31675":"### Conclusion - perspective from this level of customer segmentation\nTo gain even further insight into customer behavior, we can dig deeper in the relationship between RFM variables.  \n\nRFM model can be used in conjunction with certain predictive models like **k-means clustering**, **Logistic Regression** and **Recommendation** to produce better informative results on customer behavior.\n\nWe will go for k-means since it has been widely used for Market Segmentation and it offers the advantage of being simple to implement, following Andrew Ng who advice in his Machine Learning course, start with a dirty and simple model then move to more complex models because simple implementation helps having a first glance at the data and know where\/how to exploit it better.","eea3935a":"### Applying RFM score formula","fb667cfb":"We can find [here](http:\/\/www.blastam.com\/blog\/rfm-analysis-boosts-sales) a suggestion of key segments and then we can decide which segment to consider for further study.\n\n**Note:** the suggested link use the opposite valuation: 1 as highest\/best score and 4 is the lowest.","9f5b7841":"# Import modules"}}