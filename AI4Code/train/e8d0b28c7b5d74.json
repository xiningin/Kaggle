{"cell_type":{"376d7bbe":"code","be6a5da1":"code","ebec0b41":"code","dad6c797":"code","fee55990":"code","6ca59709":"code","9b33cfd2":"code","4521f977":"code","a41c5f2a":"code","99a92828":"code","fb6d5ce1":"code","750fb808":"code","0fbd0b99":"code","0a64136d":"code","bf3fa16b":"code","17920a3c":"code","8d2500b0":"code","9462c02f":"code","37b1a4d3":"code","29660b1f":"code","0cb0e332":"code","8afcf3b6":"code","f8d0f6ac":"code","1259b815":"code","2531822f":"code","13c841d7":"code","7c7eca26":"code","52702666":"code","66afcfba":"code","9ac8abaa":"code","811f81df":"code","f4769b86":"code","235504ca":"code","387da5db":"code","7b4b04c7":"code","d0328efc":"code","aa01d5b4":"markdown","cf1e4932":"markdown","73499b86":"markdown","b757375c":"markdown","0a83f442":"markdown","1d6b54e4":"markdown","52ed2948":"markdown","e43b2fdd":"markdown","915eb198":"markdown","b88f084d":"markdown","03b7ae35":"markdown","5102926f":"markdown","53f8f9a2":"markdown","a9489778":"markdown","9dd9abb2":"markdown","f3f5dcc0":"markdown","cab02520":"markdown","24118aad":"markdown","41875173":"markdown","c65b630b":"markdown","30ced0be":"markdown","22c16a2e":"markdown","b4f6867f":"markdown","396c067d":"markdown","de98d6f1":"markdown","3f9da6b7":"markdown","a3176a2d":"markdown","9688e9cc":"markdown","00e44464":"markdown","5fe72d38":"markdown","4cf36610":"markdown","47cfe921":"markdown","57cf3dc5":"markdown","23ce798b":"markdown","5be0593d":"markdown","3a185408":"markdown","9454a38c":"markdown","786ee058":"markdown","fbb0a46d":"markdown","f1e3f15e":"markdown","8d318d21":"markdown","f6ab1d2d":"markdown","70367d5c":"markdown","d0fec4f2":"markdown"},"source":{"376d7bbe":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#I am setting a random seed to ensure reproducibility of results\nrandom.seed(42)","be6a5da1":"df = pd.read_csv('..\/input\/ab-data\/ab_data.csv', encoding='utf8', engine='python')\ndf.head()","ebec0b41":"df.info()","dad6c797":"# convert 'timestamp' to datetime for easier manipulation\ndf.timestamp = pd.to_datetime(df.timestamp)","fee55990":"# confirm that worked\ndf.dtypes","6ca59709":"print('The website was visited', df.shape[0], 'times.')","9b33cfd2":"print('There are', df.user_id.nunique(), 'unique users.')","4521f977":"print('The A\/B test was conducted for', len(df.timestamp.dt.floor('d').value_counts()), 'days.')","a41c5f2a":"df[df.converted == 1].shape[0]\/df.shape[0]","99a92828":"df[((df.group=='treatment') & (df.landing_page!='new_page')) | \n   ((df.group!='treatment') & (df.landing_page=='new_page'))].shape[0]\n# OR\n# df[((df['group'] == 'treatment') ^ (df['landing_page'] == 'new_page'))].shape[0]\n# OR\n# df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]","fb6d5ce1":"# Make use of exclusive OR (XOR) to find disalignment between page and corresponding group\ndf2 = df.drop(df[((df['group'] == 'treatment') ^ (df['landing_page'] == 'new_page'))].index, axis=0)","750fb808":"# Double Check all of the correct rows were removed - this should return 0\ndf2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]","0fbd0b99":"df2.user_id.nunique()","0a64136d":"df2.user_id.shape","bf3fa16b":"df2[df2.user_id.duplicated(keep=False)]","17920a3c":"# drop the first duplicate with index 1899\ndf2.drop([1899], inplace=True)","8d2500b0":"df2.converted.sum()\/df2.shape[0]","9462c02f":"# control group conversion rate\nctrl = df2[df2.group=='control']\np_ctrl = ctrl.converted.sum()\/ctrl.shape[0]\np_ctrl","37b1a4d3":"# treatment group conversion rate\ntreat = df2[df2.group=='treatment']\np_treat = treat.converted.sum()\/treat.shape[0]\np_treat","29660b1f":"(df2.landing_page=='new_page').sum()\/df2.shape[0]","0cb0e332":"# set conversion rates and sample sizes for both groups under the null\np_old = df2.converted.mean()\np_new = df2.converted.mean()\nn_old = df2[df2.landing_page == 'old_page'].shape[0]\nn_new = df2[df2.landing_page == 'new_page'].shape[0]\nprint(f\"p_old: {p_old}\\np_new: {p_new}\\nn_old: {n_old}\\nn_new: {n_new}\")","8afcf3b6":"# simulation of the two binomial distributions and the difference in their conversion rates\nold_page_converted = np.random.binomial(1, p=p_old, size=n_old)\nnew_page_converted = np.random.binomial(1, p=p_new, size=n_new)\ndiff = new_page_converted.mean() - old_page_converted.mean()\ndiff","f8d0f6ac":"# simulate the difference between the conversion rate for new and old pages\n# make use of binomial distribution since that fits our scenario\nnew_page_converted = np.random.binomial(n_new, p_new, 10000) #returns no. of successes from n_new trials,performed 10000 times\nold_page_converted = np.random.binomial(n_old, p_old, 10000) #returns no. of successes from n_old trials,performed 10000 times\n#NB: we cannot use new_page_converted.mean() as above since our simulation returns the no. of successes and not 0s and 1s\np_diffs = new_page_converted\/n_new - old_page_converted\/n_old\np_diffs = np.array(p_diffs)","1259b815":"plt.hist(p_diffs);","2531822f":"# get observed difference first, then determine the more extreme values in favour of the alternative\nobs_diff = (df2[df2.group=='treatment'].converted.mean()) - (df2[df2.group=='control'].converted.mean())\np_val = (p_diffs > obs_diff).mean()\np_val","13c841d7":"import statsmodels.api as sm\n\nconvert_old = df2[df2.landing_page=='old_page'].converted.sum()\nconvert_new = df2[df2.landing_page=='new_page'].converted.sum()\nn_old = df2[df2.landing_page=='old_page'].shape[0]\nn_new = df2[df2.landing_page=='new_page'].shape[0]","7c7eca26":"test_stat, p_value = sm.stats.proportions_ztest(np.array([convert_old, convert_new]), np.array([n_old, n_new]), alternative='smaller')\nprint(f\"z-score: {test_stat}\\np-value: {p_value}\") # test_stat is the z-score for our p-value","52702666":"df2['intercept'] = 1\ndf2['ab_page'] = pd.get_dummies(df2.group)['treatment']\ndf2.head()","66afcfba":"log_mod_1 = sm.Logit(df2.converted, df2[['intercept', 'ab_page']])\nresult_1 = log_mod_1.fit()","9ac8abaa":"result_1.summary()","811f81df":"countries_df = pd.read_csv('..\/input\/countries-dataset\/countries.csv', encoding='utf8', engine='python')\ndf_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')\ndf_new.head()","f4769b86":"# check unique entries in 'country' column\ndf_new.country.unique()","235504ca":"### Create the necessary dummy variables.\n### I'll create dummies for UK and US alone, leaving CA as the baseline.\ndf_new[['UK', 'US']] = pd.get_dummies(df_new.country)[['UK','US']]\n\nlog_mod_2 = sm.Logit(df_new.converted, df_new[['intercept', 'ab_page', 'UK', 'US']])\nresult_2 = log_mod_2.fit()\n# get model summary\nresult_2.summary()","387da5db":"# create the additional columns for the interactions\ndf_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page']\ndf_new['US_ab_page'] = df_new['US'] * df_new['ab_page']","7b4b04c7":"df_new.head()","d0328efc":"### Fit the Linear Model And Obtain the Results\nlog_mod_3 = sm.Logit(df_new.converted, df_new[['intercept','ab_page', 'UK', 'US', 'UK_ab_page', 'US_ab_page']])\nresult_3 = log_mod_3.fit()\nresult_3.summary()","aa01d5b4":"<a id='ab_test'><\/a>\n### A\/B Test\n\nNow for the A\/B test analysis!\n\nFirst, I'll set up my null and alternative hypotheses.\n\n**Null:** For the null, I want to assume that the old page performs better (i.e. it has a higher conversion rate) than the new page.\n\n**Alternative:** There is enough evidence to show that the old page performs worse, in which case I'll suggest the company opts for the new page. \n","cf1e4932":"Now let's get some preliminary stats about the results presented in the dataset.\n\nFirst, let's see the number of visits that were made to the website.","73499b86":"Not very many compared to the total entries in the dataset.","b757375c":"I would like to also make use of a built-in to ensure I achieve similar results.\n\nI'll be making use of the `statsmodels` library in Python, and the same parameters as before.","0a83f442":"### Some essential probabilities\n\nNext, I am going to find the probability of an individual converting regardless of the page they received?","1d6b54e4":"As expected, the plot shows that the sampling distribution of the differences `p_diffs`, is normally distributed.\n\n<br>\n\n#### P-value\nTo get the p-value, I'll find the proportion of difference values in `p_diffs` that are equal to, or more extreme than (i.e. in favour of the alternative) the actual difference observed from the original data. This would mean differences which are greater than (in accordance with the null hypothesis) the observed difference.\n\n**NB:** The observed difference is the conversion rate for both the old and new pages as observed from the data.","52ed2948":"Import relevant libraries.","e43b2fdd":"Firstly, I'll check the number of times the `new_page` and `treatment` don't line up. Hopefully it isn't the majority of the dataset.","915eb198":"Now, for the rows where `treatment` is not aligned with `new_page` or `control` is not aligned with `old_page`, we cannot be sure if this row truly received the new or old page. So we drop all the rows that don't meet the specifications.\nI assign the result to a new dataframe variable.","b88f084d":"<a id='regression'><\/a>\n#### A regression approach\n\nIn this final part, I will be using regression in order to, hopefully, achieve similar results as in the previous methods.\n\nA logistic regression would be the appropriate kind of regression in this case, as it involves the determination of which category a given dataset falls into (i.e. conversion or no conversion).\n\nThe goal here is to use `statsmodels` to fit a logistic regression model to see if there is a significant difference in conversion as a consequence of which page a customer receives.\n\nTo use `statsmodels`, I'll first need to create a column for the intercept, and create a dummy variable column for which page each user received.\n\nIn essence, I'll add an `intercept` column with all `1`'s, as well as an `ab_page` column, which is 1 when an individual receives the new page (**treatment**) and 0 if they recieved the the old page(**control**).","03b7ae35":"Now, I am considering adding other variables that might influence whether or not an individual converts. Would this be a good idea?\n\nFrom the summary table above, the independent variable, `ab_page`, has a p-value of `0.190`, which implies that the variable is not statistically significant (since p>$\\alpha$). This means that there is no observable change in the conversion rate that may be as a consequence of a change in the webpage that a user is shown. As a result, it would make sense to consider other factors that might influence a change in the conversion rate. Also, from the `McFadden pseudo-R squared` value in the summary, it is quite clear that the `ab_page` feature accounts for a very meager amount(practically zero) of the variability in the dependent variable (i.e. the conversion rate), hence, the need to explore other variables for better explainability.\n\nThe effect of inserting an additional feature into the model would depend partly (among other factors) on whether the variable is correlated with any of the already existing features, and\/or with the response variable. In a case where there is a correlation, the added variable would definitely have a negative effect on the overall quality of the model. In the alternate scenario in which absolutely no correlation is present (which never really happens in practice: we generally decide on what level of collinearity is tolerable), then the additional variable contributes positively to the explainability of the model.\n\nSo, in the quest for better explainability, I'll be adding a new feature: **the country in which a user lives**. I'll read in the **countries.csv** dataset and merge it to the original dataset on the `user_id`. Again I'll create dummy variables for the `country` column. Let's find out if this added variable is a better predictor of conversion rate.So, in the quest for better explainability, I'll be adding a new feature: **the country in which a user lives**. I'll read in the **countries.csv** dataset and merge it to the original dataset on the `user_id`. Again I'll create dummy variables for the `country` column. Let's find out if this added variable is a better predictor of conversion rate.","5102926f":"Approximately 12% of the visits to the website resulted in conversions.","53f8f9a2":"Here's a histogram of the sampling distribution above (`p_diffs`).","a9489778":"##### Model summary","9dd9abb2":"#### Recommendation\n\nBased on all the evidence from the A\/B test and logistic regression, there is no evidence that the new page would improve the conversion rate of the e-commerce company. In that light, I would recommend that the company choose to stick to the old page for the mean time, while researching on website features that would positively influence the chosen metric, and then another A\/B test could be run to measure that influence.","f3f5dcc0":"## A\/B Test Results Analysis\n\n### Table of Contents\n- [Introduction](#intro)\n- [Part I - Probability](#probability)\n- [Part II - A\/B Test](#ab_test)\n- [Part III - Regression](#regression)","cab02520":"From the calculations of the values for `p_treat`(conversion rate for treatment group) and `p_ctrl`((conversion rate for control group)) above, we see that the difference between the conversion rate for individuals who were shown the old page and those shown the new page (i.e. `p_ctrl` - `p_treat`) is approximately `0.00` (since both values are about the same). Hence, there doesn't seem to be sufficient evidence to suggest that the new treatment page leads to more conversions. Instead, the conversion rate for the new page appears to be slightly smaller than that of the old page.","24118aad":"Now I'll check what proportion of users converted","41875173":"This value is pretty close to the one we got from the previous unfiltered dataset: approximately `12%`.\n\nNow I want to find the conversion rate or probability of conversion, given that an individual was in the `control` group, and also given that an individual was in the `treatment` grouup.","c65b630b":"###### References\n* https:\/\/www.intechopen.com\/books\/bayesian-inference\/bayesian-hypothesis-testing-an-alternative-to-null-hypothesis-significance-testing-nhst-in-psycholog\n* http:\/\/www.real-statistics.com\/hypothesis-testing\/null-hypothesis\/\n* https:\/\/stats.stackexchange.com\/questions\/52067\/does-adding-more-variables-into-a-multivariable-regression-change-coefficients-o\n* https:\/\/thestatsgeek.com\/2014\/02\/08\/r-squared-in-logistic-regression\/\n* https:\/\/www.pluralsight.com\/guides\/interpreting-data-using-statistical-models-python\n* https:\/\/www.displayr.com\/how-to-interpret-logistic-regression-coefficients\/","30ced0be":"<a id='intro'><\/a>\n### Introduction\nIn this project, I'll be analysing the results from an A\/B test for a company's e-commerce website, working to understand those results, and proffer helpful suggestions. The company has developed a new web page in order to try and increase the number of users who \"convert\" (i.e. the number of users who decide to pay for the company's product). The goal here is to help the company decide whether to implement the new page, keep the old page, or perhaps run the experiment longer before making a decision.","22c16a2e":"### Analysis of results\n\nOur A\/B test is contigent on all members of the control group having viewed the old page, and those in the treatment group having viewed the new page. Hence, we must ensure that this is the case in our dataset, and get rid of any entries that do not satisfy this criterion for whatever reason (could be due to the mismatching of group members to their corresponding pages, or simply an error occurred during data gathering).","b4f6867f":"From the summary above, we see that the p-values for both UK and US (`0.074` and `0.130` respectively) are less than the chosen $\\alpha$-value of 0.05 and are thus not significant covariates of the model. Also, the pseudo R-squared value (though not a good estimate of explainability) indicates that the  added country variables did not substantially contribute to the explainability of the model. In sum, the `country` variable does not seem to have any impact on conversion.","396c067d":"<a id='probability'><\/a>\n#### Part I - Probability","de98d6f1":"The z-score and p-value obtained above imply that there is no statistical evidence to support the change in the webpage, since there is no increase in the conversion rate for the new page (i.e. both pages have similar conversion rates, or old is greater than new) according to the data.\n\nIndeed, these results are in agreement with the findings from the simulated distributions.","3f9da6b7":"From the hypothesis definitions above, the **metric** I am measuring is the **difference between the conversion rates for the old and new pages.**\n\nI assume, under the null hypothesis, that $p_{new}$ and $p_{old}$ both have conversion rates equal to the overall conversion rate in the dataset (regardless of the page a user was shown). By consequence, $p_{new}$ and $p_{old}$ are equal, and their difference is `0`.\n\nMost probably, the data in the original dataset was gathered such that it is sufficiently random, and represents the population fairly well. Hence, I'll use the sample-sizes for both treatment and control groups in the sampling distribution simulation.\n\nNow, I'm going to simulate a sampling distribution for both the old and new pages, having conversion rates of $p_{old}$ and $p_{new}$, and sample sizes of $n_{old}$ and $n_{new}$ respectively. And then I'll take the difference between the conversion rates of these two distribution, to ensure they correspond to the null hypothesis (i.e. the difference is close to `0`), in which case we can be more or less confident that the chosen sample sizes are adequate.","a3176a2d":"Instantiate the model, and fit the model using the two columns earlier created to predict whether or not an individual converts.","9688e9cc":"Next, I want to ensure that the new dataset does not contain repeated entries for a given user, in order to avoid misleading results.\n\nCompare number of unique ids to total number of rows in df2","00e44464":"Load in the `ab_data.csv` dataset and take a look at the top few rows.","5fe72d38":"###### Conclusions\n* From the summary above, the interactions between the page shown to a user and the country that user comes from does not influence the conversion of the user. This is because the p-values for the interaction variables with respect to the dependent variable are all $\\gt \\alpha$ (0.05), meaning they are not statistically significant.\n* Practically speaking, the country from which a user visits a webpage is a large categorisation, and, hence, isn't likely to have much predictive power on its own, except perhaps more related information is given (like government policy vis-a-vis the web, internet censorship, etc).\n* In sum, statistically speaking, the webpage, country, and the interaction between both of these variables, do not appear to have any impact on the conversion of users.","4cf36610":"From the regression summary above, the p-value associated with the **ab_page** variable is `0.190`. This value differs from the p-value obtained using A\/B testing, which is as a result of the difference in how the hypotheses were set up.\n\nIn the A\/B testing scenario, I used a `one-tailed test`: we were interested in an observed change in just one direction away from our metric i.e.\n\n$H_{o}$: $p_{new}$ - $p_{old}$ $\\leq$ 0,\n\n$H_{1}$: $p_{new}$ - $p_{old}$ $\\gt$ 0;\n\nwhereas, in the regression case above, I make use of a `two-tailed test`: we are concerned with a change in either direction of the metric being measured i.e.\n\n$H_{o}$: $p_{new}$ - $p_{old}$ = 0,\n\n$H_{1}$: $p_{new}$ - $p_{old}$ $\\neq$ 0).","47cfe921":"Check for inconvenient data types and missing values","57cf3dc5":"As we would expect, an individual receives the new page half of the time, which might be why the conversion rates for both pages are quite similar.","23ce798b":"Above, I've computed the probability of obtaining our observed statistic, `obs_diff`, or a more extreme value (in favour of the alternative hypothesis) based on the premise that the null hypothesis is true. This probability is the **p-value** for the sampling distribution of differences.\n\nSince this value is greater than the our designated significance level of **alpha=0.05**, then we may conclude that, based on the data, we do not have sufficient evidence in favour of the alternative hypothesis, hence, the result is not statistically significant and we do not reject the null hypothesis. In other words, the difference between the conversion rate for the old page and that of the new page is less than or equal to `0`, or otherwise put: **the conversion rate for the old page is either greater than or equal to that of the new page.**","5be0593d":"There appears to be one duplicate row","3a185408":"We seem to be getting quite similar values in all cases: roughly `12%`.\n\nSo, let's see the probability that an individual actually received the new page.","9454a38c":"Now, to compute the test statistic (z-score) and the p-value, I'll use the `stats.proportions_ztest` method.","786ee058":"For how long was the test conducted?","fbb0a46d":"$H_{o}$:&emsp; $p_{new}$ - $p_{old}$ $\\leq$ 0\n\n$H_{1}$:&emsp; $p_{new}$ - $p_{old}$ $\\gt$ 0\n\n_where $p_{old}$ is the conversion rate for the old page, while $p_{new}$ is the conversion rate for the new page_","f1e3f15e":"#### Info on datasets\n1. **`ab_data.csv` dataset**\n\nThis dataset contains information about the `200K+` users involved in the A\/B test.\n* `user_id` - unique identifier for each user\n* `timestamp` - associated date and time for each visit to the website by a given user\n* `group` - the category a user was grouped into pre-A\/B test (`control` or `treatment` groups)\n* `landing_page` - the page that was displayed to a user when they visited the company website (`new_page` or `old_page`)\n* `converted` - whether a user converted or not (`0` or `1`)\n**NB:** Users in the control group ought to be displayed the old page, while those in the treatment group ought to see the new page.","8d318d21":"The `timestamp` column is an `object` type, a `datetime` tyope would be more appropriate.\n\nThere are no missing values.","f6ab1d2d":"Then the number of unique users in the dataset.","70367d5c":"I have now looked at the individual factors of **country** and **page** on conversion. I would now like to look at an interaction between page and country to see if that would have a significant effect on conversion.","d0fec4f2":"From the difference value above (approximately `0.00`).\nSo it seems the sample sizes would do the trick.\n\nNext, I'll be simulating a sampling distribution for the difference in conversion rates between the old and new pages (just as was done above), but this time over 10,000 iterations, to be certain the above results aren't merely due to chance. I'll store this distribution in a numpy array. All previous parameters hold constant."}}