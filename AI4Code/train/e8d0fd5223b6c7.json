{"cell_type":{"6572cd17":"code","f637dfa9":"code","d36abeb3":"code","2b4e3a73":"code","a1edd822":"code","f924be21":"code","99a74d7c":"code","559301cc":"code","a6626a9c":"code","09f9f654":"code","77df0035":"code","97787750":"code","210e523b":"code","3112b250":"code","e9195ef9":"code","389392e0":"code","bacb7080":"code","44c5c760":"markdown","aae7e48b":"markdown","e7d9fe14":"markdown","0e32962c":"markdown","8fe4fe5b":"markdown","be4a14c4":"markdown","dc68ab54":"markdown","edfb4303":"markdown"},"source":{"6572cd17":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\n","f637dfa9":"data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv', index_col = 'CLIENTNUM')\ndrop_columns = ['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']\ndata.drop(columns = drop_columns, inplace = True)","d36abeb3":"data.info()","2b4e3a73":"num_columns = list(data.columns)\ncat_columns = [ 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category','Gender']\nfor col in cat_columns:\n    num_columns.remove(col)\n\nnum_columns.remove('Attrition_Flag')","a1edd822":"figsize = (15, 12)\nrows = 4\ncolumns = 4\nfig, axes = plt.subplots(rows, columns, figsize = figsize)\n\nfor i, col in enumerate(num_columns):\n    axrow, axcol = int(np.floor(i\/rows)), i%columns\n    ax_active = axes[axrow, axcol]\n    sns.distplot(data[data['Attrition_Flag'] == 'Attrited Customer'][col], ax = ax_active, kde= False)\n    sns.distplot(data[data['Attrition_Flag'] == 'Existing Customer'][col], ax = ax_active, kde= False)\n    ax_active.set_title(col)\n    ax_active.set_xlabel(None)\n    ax_active.legend(['Attrited', 'Existing'])\n\nplt.subplots_adjust(hspace = .3)","f924be21":"figsize = (15, 12)\nrows = 3\ncolumns = 3\nfig, axes = plt.subplots(rows, columns, figsize = figsize)\n\nfor i, col in enumerate(cat_columns):\n    axrow, axcol = int(np.floor(i\/rows)), i%columns\n#     print(axrow, axcol)\n    ax_active = axes[axrow, axcol]\n    sns.countplot(data = data, x = col, hue = 'Attrition_Flag', ax = ax_active)\n    ax_active.set_title(col)\n    ax_active.set_xlabel(None)\n    xlabels = ax_active.get_xticklabels()\n    ax_active.set_xticklabels(xlabels, Rotation = 20)\n    ax_active.legend(['Attrited', 'Existing'])\n\nplt.subplots_adjust(hspace = .3)","99a74d7c":"dummies = pd.get_dummies(data[cat_columns])\ndata_basic = pd.merge(data.copy(), dummies, on = 'CLIENTNUM')\ndata_basic.drop(columns = cat_columns, inplace = True)","559301cc":"attrition_dict = {'Existing Customer':0, 'Attrited Customer':1}\ndata_basic.loc[:, 'Attrition_Flag'] = data_basic.loc[:, 'Attrition_Flag'].map(attrition_dict)\ndata_basic.head()\n","a6626a9c":"X = data_basic.drop(columns = ['Attrition_Flag']).copy()\ny = data_basic['Attrition_Flag']\nminmax = preprocessing.MinMaxScaler()\n\nX = minmax.fit_transform(X)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify = y, test_size = .25)\nminmax.scale_","09f9f654":"LogClassifier = LogisticRegression(random_state = 3157, max_iter = 300)\nLogClassifier.fit(X_train, y_train)\ny_pred_log = LogClassifier.predict(X_test)\ntarget_names = ['Existing Customer', 'Attrited Customer']\n\nprint(metrics.classification_report(y_pred_log, y_test))","77df0035":"Classifier_rf = RandomForestClassifier()\nClassifier_rf.fit(X_train, y_train)\ny_pred_rf = Classifier_rf.predict(X_test)\n\nprint(metrics.classification_report(y_pred_rf, y_test))","97787750":"Classifier_xgb = GradientBoostingClassifier(random_state = 2290)\nClassifier_xgb.fit(X_train, y_train)\ny_pred_xgb = Classifier_xgb.predict(X_train)\nprint(metrics.classification_report(y_pred_rf, y_test))","210e523b":"Classifier_rf.feature_importances_\nfeature_importances = pd.Series(index = data_basic.columns[1:], data = Classifier_rf.feature_importances_)\nfeature_importances = feature_importances.sort_values(ascending = False)\nax = feature_importances[:10].plot.bar()\nax.set_title('Relative Feature Importance')","3112b250":"feature_importances","e9195ef9":"top_ten = list(feature_importances.index[:10])\ndata_basic[top_ten]","389392e0":"figsize = (15, 12)\nrows = 3\ncolumns = 4\nfig, axes = plt.subplots(rows, columns, figsize = figsize)\n\nfor i, col in enumerate(top_ten):\n    axrow, axcol = int(np.floor(i\/(rows+1))), i%columns\n#     print(axrow, axcol)\n    ax_active = axes[axrow, axcol]\n    sns.distplot(data_basic[data_basic['Attrition_Flag']==0][col], \n                 hist = False, kde = True, label = 'Existing',\n                ax = ax_active)\n    sns.distplot(data_basic[data_basic['Attrition_Flag']==1][col], \n                 hist = False, kde = True, label = 'Attrited',\n                ax = ax_active)\n    ax_active.set_title(col)\n    ax_active.set_xlabel(None)","bacb7080":"rows = 7\ncolumns = 7\n\n# sns.pairplot(data = data_basic[top_ten:8])\n\nfig, axes = plt.subplots(rows, columns, figsize = figsize)\n\nfor i, xcol in enumerate(top_ten[:7]):\n    for j, ycol in enumerate(top_ten[:7]):\n#         print(i, j, xcol, ycol)\n        ax_active = axes[i, j]\n        x = data_basic[data_basic['Attrition_Flag']==0][xcol]\n        y = data_basic[data_basic['Attrition_Flag']==0][ycol]\n        x2 = data_basic[data_basic['Attrition_Flag']==1][xcol]\n        y2 = data_basic[data_basic['Attrition_Flag']==1][ycol]\n        if xcol != ycol:\n            ax_active.scatter(x, y, s = 1, alpha = .5)\n            ax_active.scatter(x2, y2, s = 1, alpha = .5)\n            ax_active.set_xticks([])\n            ax_active.set_yticks([])\n            ax_active.set_xlabel(xcol, fontsize = 8)\n            ax_active.set_ylabel(ycol, fontsize = 8)\n\n# fig.legend(['existing', 'attrited'])","44c5c760":"# Classifiers\nI created some classifiers using easy classifiers from sklearn, including logrithmic, Random Forest, and Gradient Boosting. Random Forest and Gradient Boosting performed about the same, which isn't too surprising since they're related. \n<br>\n* Logrithmic did not do very well, with about .53 precision and .80 recall\n* Random Forest and Gradient Boost had about .78 precision and .97 recall. <br><br><br><br>\n\nEither the GB or RF classifiers are fairly useful in and of themselves. They promise to identify all but 3% of the customers about to churn, with 22% of the identifications being false positives, so for every 8 (actually closer to 7) customers these classifiers say need attention, there are 2 they make recommendations for that probably will not churn.\n\nWhat I'm most interested in is the feature importances attribute of the rf classifier, but more on that later in the notebook.","aae7e48b":"# Encoding Features\n\nThere is nothing terribly interesting or clever done here. Just one-hot encoding of the features in my list of categorical features and label encoding of the target.\n\nI just used one-hot encoding for all of them because, based on the plots above, none of the features has a terribly high cardinality. In the end, I have fewer than 50 features total, which should fit fairly quickly for most models.\n\nI'm also going to include the cell in which I did the test train split in this section. Before the split, I applied a minmax scaler so that linear models would be able to work with the data as well.","e7d9fe14":"# Introduction\n\nIn this notebook...\n* creating some models to analyze customer attrition\n* looking at feature importances to see if some intuative relationships between features can be gleaned. (not really)\n* I guess that useful relationships can't be visualized in two dimensions, maybe?\n* I can't work on this project more today.","0e32962c":"I write this shortly after I got that subplotted figure above to work out. I'm not super happy about how this turned out. I'm glad that I finally got all of these subplots and ax functions to compile, but I don't think that I'm getting any super simple relationships out of this. Probably because there are none in two dimensions.<br><br>\nI thought for a second that it might be interesting to add a third axis (requiring me to go and learn lots more code) so that I could visualize the relationships between three variables, but then I remembered that my monitor is two dimensional, so there would be no good way to look at the plots in relation to each other. Down this path lies madness. Anyway, that's what the ML algorithms are for, isn't it? To find patterns in higher dimensions that my monkey brain can't visualize?<br><br>\nI think that there's just no easy answer to the question \"how important is each feature?\". Clearly there is some clustering in higher dimensions that a decision tree can pick up on, since the random forest classifier was able to predict with such high recall, if not precision. I think that I just have to conclude that there is no terribly helpful information in each feature looked at in isolation, or even in pairs.<br><br>\nThis plot helped me see that there's a pretty interesting relationship for any of the features involving Total_Amt_Chng_Q4_Q1. It looks like there's a pretty stark threshold for that feature that virtually all of the churning customers fall on one side of. Looking back at the plots of the distributions of each variable in isolation, I can see WHY this is. It looks like both the churned customers and non churned customers fall into normal-ish curves for both, and a good portion of the not-churning customers are not in a point of overlap with the churning curve.<br><br>\nAnd then I realize that this makes a lot of sense. Total_Amt_Chng_Q1_Q4 is itself a combination of two variables. Clearly there is some clustering pattern on a plot of amtchgQ1 and amtchgQ4, and the financial institution this data comes from has already recognized it. Good on them, but I'm left here thinking that there (probably) remains no decisive clustering in 2 dimensions. When looking at any two of the features I worked with on that plot, the churning customers simply have a lot of overlap with not-churning customers, and no pair can by itself identify the churners.<br><br>\nAlso, a point of clarification: on those plots, orange dots represent churning customers and blue ones represent not-churning ones. Individual dots are too small to show up clearly, which made the overal legend I had here earlier unhelpful. I don't care to try and work out a way to improve the plots, and I don't see anything promising to use in these plots, so I'm staying out of that rabbit hole, thank you.","8fe4fe5b":"# Feature Importance\nI know that there's a better way to do this (probably PCA), but I'm just making it up as I go along for now. I don't feel like reading through the documentation right this moment. Sorry.<br><br>\nFirst, I'm just looking at the feature importances as they are in the RandomForestClassifier object. I admit that I don't know exactly what these values represent, but they should at least give me an idea of the importance of the features relative to each other.<br><br>\nIt surprised me that none of the categorical features were in the top ten; all were numeric columns in the original dataframe. I should probably aggregate the categorical features for a better picture, but I'm fairly sure that no sum of categorical feature importances would break the top ten. I'm going to focus on the top ten features for now, not because this is a good operating procedure, but because that's how I feel like doing it (for now).","be4a14c4":"# Data Analysis\n\nIt looks like this dataset is not missing any values, so no cleaning is necessary.<br><br>\nFirst, I created num_columns and cat_columns, lists of the names of the feature names, the categorical and numeric. There's probably a more elegant way to do this, but I like for loops, ok?<br><br>\nI then created plots to help visualize each feature in isolation, first the numeric and then the categorical. These plots are histograms and the number of non-attrited customers is far greater than that of the attrited customers. I'll have better plots for comparing attrited and existing customers later in the notebook. This was just to get me an idea of their distributions.\n","dc68ab54":"## Distribution Plots\nAgain, I'm just looking at the 10 most important features (all numeric) according to my random forest classifier so that I can wrap my head around the data. The first thing I did was plot the distribution of each variable, kde enabled, so the curves were scaled. Unlike my distribution plot earlier in this notebook, this helps me see differences in the distributions for churning and non-churning customers. The farther down the top ten list we go, the more the churning and non-churning curves look like each other. That makes sense.<br><br>\nNext I'm doing 2-D kde plots for the top ten features. This should make any simple relationships between 2 variable and churning easily visible. It's here that I'm glad I didn't go with all features because a ten by ten subplot is bad enough to look at as it is. In fact, since the last three features have curves that look so much like each other, I think that I'll just drop those for the kde plots to make things more visible. That may bite me later, but I'm not going to worry about it for now.","edfb4303":"# Importing Libraries and Loading Data\n\nAll of my library imports should be in the next cell"}}