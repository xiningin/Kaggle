{"cell_type":{"83cc3f94":"code","10e69573":"code","0ba68d7e":"code","327fa62f":"code","dca5a8b9":"code","f94372f8":"code","a252556c":"code","c21bd021":"code","9621abb2":"code","3f7186c9":"code","0616fd22":"code","28d111a1":"code","277557ad":"code","03d2a536":"code","d9e6f40f":"code","31fb1c5b":"code","952b496c":"code","9cb477c7":"code","0cf32fe5":"code","b22a51b8":"code","fc9e03d0":"code","0f8e3b74":"markdown","2e2dbd2f":"markdown","9cd1813d":"markdown","3084ad62":"markdown","53ef9671":"markdown","021255b1":"markdown","a56911ad":"markdown","1d378b0d":"markdown","caf3b231":"markdown","9d7b1139":"markdown","1d67a127":"markdown","977aac01":"markdown","bb9cb6c4":"markdown","1b42c53d":"markdown","9268ce26":"markdown","1c2be546":"markdown","1057a583":"markdown","9c167f88":"markdown","cd3d9974":"markdown","cab26c61":"markdown","fed5e3a5":"markdown","43358443":"markdown","c74f05fe":"markdown","5d0f4e08":"markdown","0f8a4770":"markdown","e354e6e1":"markdown","7b9809c6":"markdown","b54959a9":"markdown","f7b2a398":"markdown"},"source":{"83cc3f94":"import os\nimport warnings\nimport numpy as np \nimport pandas as pd \nimport pandas_profiling as pp\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')","10e69573":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","0ba68d7e":"prof = pp.ProfileReport(train, title=\"Pandas Profiling Report\")","327fa62f":"prof","dca5a8b9":"prof.to_file('profile_report.html')","f94372f8":"y = train[['Id','SalePrice']]\ntrain = train.drop('SalePrice',axis=1)","a252556c":"all_dfs = [train,test]\nall_df = pd.concat(all_dfs).reset_index(drop=True);","c21bd021":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","9621abb2":"display_all(all_df.isnull().sum())","3f7186c9":"all_df.drop(['Alley','PoolQC','MiscFeature','Fence','FireplaceQu','Utilities'],axis=1,inplace=True)","0616fd22":"all_df['LotFrontage'].fillna(value=all_df['LotFrontage'].median(),inplace=True)\nall_df['MasVnrType'].fillna(value='None',inplace=True)\nall_df['MasVnrArea'].fillna(0,inplace=True)\nall_df['BsmtCond'].fillna(value='TA',inplace=True)\nall_df['BsmtExposure'].fillna(value='No',inplace=True)\nall_df['Electrical'].fillna(value='SBrkr',inplace=True)\nall_df['BsmtFinType2'].fillna(value='Unf',inplace=True)\nall_df['GarageType'].fillna(value='Attchd',inplace=True)\nall_df['GarageYrBlt'].fillna(value=all_df['GarageYrBlt'].median(),inplace=True)\nall_df['GarageFinish'].fillna(value='Unf',inplace=True)\nall_df['GarageQual'].fillna(value='TA',inplace=True)\nall_df['GarageCond'].fillna(value='TA',inplace=True)\nall_df['BsmtFinType1'].fillna(value='NO',inplace=True)\nall_df['BsmtQual'].fillna(value='No',inplace=True)\nall_df['BsmtFullBath'].fillna(value=all_df['BsmtFullBath'].median(),inplace=True)\nall_df['BsmtFinSF1'].fillna(value=all_df['BsmtFinSF1'].median(),inplace=True)\nall_df['BsmtFinSF2'].fillna(value=0,inplace=True)\nall_df['BsmtUnfSF'].fillna(value=0,inplace=True)\nall_df['TotalBsmtSF'].fillna(value=all_df['TotalBsmtSF'].median(),inplace=True)\nall_df['BsmtHalfBath'].fillna(value=0,inplace=True)\nall_df['GarageCars'].fillna(value=all_df['GarageCars'].median(),inplace=True)\nall_df['GarageArea'].fillna(value=all_df['GarageArea'].median(),inplace=True)","28d111a1":"labelencoder=LabelEncoder()\n\nall_df['MSZoning']      = labelencoder.fit_transform(all_df['MSZoning'].astype(str))\nall_df['Exterior1st']   = labelencoder.fit_transform(all_df['Exterior1st'].astype(str))\nall_df['Exterior2nd']   = labelencoder.fit_transform(all_df['Exterior2nd'].astype(str))\nall_df['KitchenQual']   = labelencoder.fit_transform(all_df['KitchenQual'].astype(str))\nall_df['Functional']    = labelencoder.fit_transform(all_df['Functional'].astype(str))\nall_df['SaleType']      = labelencoder.fit_transform(all_df['SaleType'].astype(str))\nall_df['Street']        = labelencoder.fit_transform(all_df['Street'])   \nall_df['LotShape']      = labelencoder.fit_transform(all_df['LotShape'])   \nall_df['LandContour']   = labelencoder.fit_transform(all_df['LandContour'])   \nall_df['LotConfig']     = labelencoder.fit_transform(all_df['LotConfig'])   \nall_df['LandSlope']     = labelencoder.fit_transform(all_df['LandSlope'])   \nall_df['Neighborhood']  = labelencoder.fit_transform(all_df['Neighborhood'])   \nall_df['Condition1']    = labelencoder.fit_transform(all_df['Condition1'])   \nall_df['Condition2']    = labelencoder.fit_transform(all_df['Condition2'])   \nall_df['BldgType']      = labelencoder.fit_transform(all_df['BldgType'])   \nall_df['HouseStyle']    = labelencoder.fit_transform(all_df['HouseStyle'])   \nall_df['RoofStyle']     = labelencoder.fit_transform(all_df['RoofStyle'])   \nall_df['RoofMatl']      = labelencoder.fit_transform(all_df['RoofMatl'])    \nall_df['MasVnrType']    = labelencoder.fit_transform(all_df['MasVnrType'])   \nall_df['ExterQual']     = labelencoder.fit_transform(all_df['ExterQual'])  \nall_df['ExterCond']     = labelencoder.fit_transform(all_df['ExterCond'])   \nall_df['Foundation']    = labelencoder.fit_transform(all_df['Foundation'])   \nall_df['BsmtQual']      = labelencoder.fit_transform(all_df['BsmtQual'])   \nall_df['BsmtCond']      = labelencoder.fit_transform(all_df['BsmtCond'])   \nall_df['BsmtExposure']  = labelencoder.fit_transform(all_df['BsmtExposure'])   \nall_df['BsmtFinType1']  = labelencoder.fit_transform(all_df['BsmtFinType1'])   \nall_df['BsmtFinType2']  = labelencoder.fit_transform(all_df['BsmtFinType2'])   \nall_df['Heating']       = labelencoder.fit_transform(all_df['Heating'])   \nall_df['HeatingQC']     = labelencoder.fit_transform(all_df['HeatingQC'])   \nall_df['CentralAir']    = labelencoder.fit_transform(all_df['CentralAir'])   \nall_df['Electrical']    = labelencoder.fit_transform(all_df['Electrical'])    \nall_df['GarageType']    = labelencoder.fit_transform(all_df['GarageType'])  \nall_df['GarageFinish']  = labelencoder.fit_transform(all_df['GarageFinish'])   \nall_df['GarageQual']    = labelencoder.fit_transform(all_df['GarageQual'])  \nall_df['GarageCond']    = labelencoder.fit_transform(all_df['GarageCond'])   \nall_df['PavedDrive']    = labelencoder.fit_transform(all_df['PavedDrive'])  \nall_df['SaleCondition'] = labelencoder.fit_transform(all_df['SaleCondition'])  ","277557ad":"Scaler = StandardScaler()\nall_scaled = pd.DataFrame(Scaler.fit_transform(all_df))\n\ntrain_scaled = pd.DataFrame(all_scaled[:1460])\ntest_scaled = pd.DataFrame(all_scaled[1460:2920])","03d2a536":"X = train_scaled\nX_train, X_test, y_train, y_test = train_test_split(X, y['SalePrice'], test_size=0.1, random_state=42)","d9e6f40f":"from xgboost import XGBRegressor\nXGB = XGBRegressor(max_depth=2,learning_rate=0.1,n_estimators=1000,reg_alpha=0.001,reg_lambda=0.000001,n_jobs=-1,min_child_weight=3)\nXGB.fit(X_train,y_train)","31fb1c5b":"from lightgbm import LGBMRegressor\nLGBM = LGBMRegressor(n_estimators = 1000)\nLGBM.fit(X_train,y_train)","952b496c":"print (\"XGB Training score:\",XGB.score(X_train,y_train),\"Test Score:\",XGB.score(X_test,y_test))\nprint (\"LGBM Training score:\",LGBM.score(X_train,y_train),\"Test Score:\",LGBM.score(X_test,y_test))","9cb477c7":"y_pred_xgb  = pd.DataFrame( XGB.predict(test_scaled))\ny_pred_lgbm = pd.DataFrame(LGBM.predict(test_scaled))\n\ny_pred=pd.DataFrame()\ny_pred['SalePrice'] = 0.5 * y_pred_xgb[0] + 0.5 * y_pred_lgbm[0]\ny_pred['Id'] = test['Id']","0cf32fe5":"y_pred.to_csv('submission.csv',index=False)","b22a51b8":"from yellowbrick.regressor import ResidualsPlot\nfrom yellowbrick.regressor import PredictionError\nfrom yellowbrick.model_selection import ValidationCurve","fc9e03d0":"model = Lasso(alpha=10)\nvisualizer = PredictionError(model)\n\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)","0f8e3b74":"*Standard Scalar: Standardize features by removing the mean and scaling to unit variance*\n*The standard score of a sample x is calculated as:*\n\n    z = (x - u) \/ s\n\n*where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.*","2e2dbd2f":"### Competition Description\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","9cd1813d":"if you're complete beginner, then I suggest you to check out the following documentations. It contains a nice intro about boosting algorithms.\n\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/\n- https:\/\/lightgbm.readthedocs.io\/en\/latest\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2015\/11\/quick-introduction-boosting-algorithms-machine-learning\/","3084ad62":"You can find additional information of Yellowbricks  through its website: https:\/\/www.scikit-yb.org\/en\/latest\/","53ef9671":"## Some Regression Visuals to help us understand the current state!","021255b1":"## Dropping Irrelevant Columns","a56911ad":"*pandas_profiling -> Generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.*","1d378b0d":"## LightGBM","caf3b231":"# Data Preprocessing & Feature Engineering","9d7b1139":"### Displaying Sum of Null Values","1d67a127":"*Label Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.*","977aac01":"# Thanks A Lot For Your Time ! Please Upvote if you Like it.\n\n<p align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https:\/\/media.giphy.com\/media\/CHmwA02GQ6aTS\/giphy.gif\">\n<\/p>","bb9cb6c4":"## Encoding All Object Datatypes with Label Enconding","1b42c53d":"## Prediction Using Ensemble Technique With XGB and LGBM ! (50-50) split","9268ce26":"## Filling Null values with specific values","1c2be546":"# Modeling","1057a583":"## Train - Test split !","9c167f88":"## Scaling the dataset to use Linear Models","cd3d9974":"## Scores of the Models","cab26c61":"### Concatenating Train and Test datasets vertically.","fed5e3a5":"## But Why Ensemble ?\n\nA common practice nowadays is to check the reviews of items before buying them. And when checking reviews, you often look for the items with a large number of reviews so you could know for sure about its rating. After going through the reviews from multiple people you decide whether to buy the item or not.\n\nEnsemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This approach allows for better predictive performance compared to a single model. This is the reason why ensemble methods were placed first in many prestigious machine learning competitions, such as the Netflix Competition, KDD 2009, and Kaggle.Ensemble models can help tackle some complex machine learning problems such as overfitting and underfitting.","43358443":"### Initialising Train and Target","c74f05fe":"# Submission","5d0f4e08":"# Exploratory Data Analysis","0f8a4770":"## XGBoost","e354e6e1":"## Mixing 50% + 50% of Both the Models to get an Optimum Model.","7b9809c6":"# Getting the data","b54959a9":"*Prediction Error Plot*\n\n*A prediction error plot shows the actual targets from the dataset against the predicted values generated by our model. This allows us to see how much variance is in the model. Data scientists can diagnose regression models using this plot by comparing against the 45 degree line, where the prediction exactly matches the model.*","f7b2a398":"# Imports"}}