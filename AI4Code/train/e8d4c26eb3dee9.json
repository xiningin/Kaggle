{"cell_type":{"df84fa44":"code","9ba74bce":"code","c7e5b107":"code","0f5e3bf6":"code","98ac00d1":"code","dc6bee29":"code","0ed8033e":"code","51173623":"code","6f35e47d":"code","7611763c":"code","0487b577":"code","5a78122c":"code","03a45e6d":"code","6a4e0beb":"code","e65b9662":"code","f43aa4f4":"code","eb7e9cec":"code","acb4f3d6":"code","9803c9c4":"code","371c2c85":"code","d9b4a493":"code","7ab0f84b":"code","369dcb40":"code","028966e8":"code","22b11085":"code","f4c3a9ee":"code","6ad79b3b":"markdown","0f0b9cfd":"markdown","17759eea":"markdown","c28c085e":"markdown","5714fa14":"markdown"},"source":{"df84fa44":"import math\nimport sys\nimport pathlib\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras import Model, layers\nfrom tensorflow.keras.applications import EfficientNetB3, efficientnet\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, Input\n\nAUTO = tf.data.experimental.AUTOTUNE","9ba74bce":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","c7e5b107":"IMG_SIIZE = 512\nIMG_CHANNEL = 3\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nEPOCHS = 1","0f5e3bf6":"def get_layer_index(model, layer_name, not_found=None):\n    \"\"\"get model's layer index by layer's name\"\"\"\n    for i, layer in enumerate(model.layers):\n        if layer.name == layer_name:\n            return i\n    return not_found","98ac00d1":"tfrecords_path = [ str(i) for i in list(pathlib.Path('..\/input\/shoppee-tfrecord\/tfrecords').glob('**\/*.tfrecord'))]\ndf = pd.read_csv('..\/input\/shoppee-tfrecord\/tfrecords\/train_fold.csv')","dc6bee29":"df.head()","0ed8033e":"N_CLASSES = len(df.label_group.unique())\nDATA_SIZE = len(df)","51173623":"def data_augment(image):\n\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_hue(image, 0.01)\n    image = tf.image.random_saturation(image, 0.70, 1.30)\n    image = tf.image.random_contrast(image, 0.80, 1.20)\n    image = tf.image.random_brightness(image, 0.10)\n\n    return image","6f35e47d":"def load_tfrecords(paths, is_augment=True):\n    \"\"\"load tfrecords\"\"\"\n\n    raw_dataset = tf.data.TFRecordDataset(paths, num_parallel_reads=AUTO)\n\n    feature_description = {\n        'label_group': tf.io.FixedLenFeature([], tf.int64),\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'posting_id': tf.io.FixedLenFeature([], tf.string),\n        'title': tf.io.FixedLenFeature([], tf.string),\n    }\n\n    def _parse_function(example):\n\n        feature = tf.io.parse_single_example(example, feature_description)\n\n        image = tf.image.decode_jpeg(feature['image'], channels=3)\n        image = tf.image.resize(image, [IMG_SIIZE, IMG_SIIZE])\n        image = tf.cast(image, tf.float32) \/ 255.0\n        if is_augment:\n            image = data_augment(image)\n        label = feature['label_group']\n\n        return (image, label), label\n\n    parsed_dataset = raw_dataset.map(_parse_function)\n\n    return parsed_dataset","7611763c":"train_paths, val_paths = train_test_split(tfrecords_path, test_size=3, random_state=1)","0487b577":"train = load_tfrecords(train_paths)\nval = load_tfrecords(val_paths, is_augment=False)","5a78122c":"train_data_index = [ pathlib.Path(i).stem.replace('train_', '') for i in train_paths]\nval_data_index = [ pathlib.Path(i).stem.replace('train_', '') for i in val_paths]","03a45e6d":"df_train = df.query(f'fold == {train_data_index}')\ndf_val = df.query(f'fold == {val_data_index}')","6a4e0beb":"train = (train.shuffle(1012)\n         .repeat()\n         .batch(BATCH_SIZE)\n         .prefetch(buffer_size=AUTO))\n\nval = (val.repeat()\n       .batch(BATCH_SIZE)\n       .prefetch(buffer_size=AUTO))","e65b9662":"class BatchNormalization(tf.keras.layers.BatchNormalization):\n    \"\"\"Make trainable=False freeze BN for real (the og version is sad).\n       ref: https:\/\/github.com\/zzh8829\/yolov3-tf2\n    \"\"\"\n    def call(self, x, training=False):\n        if training is None:\n            training = tf.constant(False)\n        training = tf.logical_and(training, self.trainable)\n        return super().call(x, training)","f43aa4f4":"def ArcHead(num_classes, margin=0.5, logist_scale=64, name='ArcHead'):\n    \"\"\"Arc Head\"\"\"\n    def arc_head(x_in, y_in):\n        x = inputs1 = Input(x_in.shape[1:])\n        y = Input(y_in.shape[1:])\n        x = ArcMarginPenaltyLogists(num_classes=num_classes,\n                                    margin=margin,\n                                    logist_scale=logist_scale)(x, y)\n        return Model((inputs1, y), x, name=name)((x_in, y_in))\n    return arc_head","eb7e9cec":"class ArcMarginPenaltyLogists(tf.keras.layers.Layer):\n    \"\"\"ArcMarginPenaltyLogists\"\"\"\n    def __init__(self, num_classes, margin=0.5, logist_scale=64, **kwargs):\n        super(ArcMarginPenaltyLogists, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.margin = margin\n        self.logist_scale = logist_scale\n\n    def build(self, input_shape):\n        self.w = self.add_variable(\n            \"weights\", shape=[int(input_shape[-1]), self.num_classes])\n        self.cos_m = tf.identity(math.cos(self.margin), name='cos_m')\n        self.sin_m = tf.identity(math.sin(self.margin), name='sin_m')\n        self.th = tf.identity(math.cos(math.pi - self.margin), name='th')\n        self.mm = tf.multiply(self.sin_m, self.margin, name='mm')\n\n    def call(self, embds, labels):\n        normed_embds = tf.nn.l2_normalize(embds, axis=1, name='normed_embd')\n        normed_w = tf.nn.l2_normalize(self.w, axis=0, name='normed_weights')\n\n        cos_t = tf.matmul(normed_embds, normed_w, name='cos_t')\n        sin_t = tf.sqrt(1. - cos_t ** 2, name='sin_t')\n\n        cos_mt = tf.subtract(\n            cos_t * self.cos_m, sin_t * self.sin_m, name='cos_mt')\n\n        cos_mt = tf.where(cos_t > self.th, cos_mt, cos_t - self.mm)\n\n        mask = tf.one_hot(tf.cast(labels, tf.int32), depth=self.num_classes,\n                          name='one_hot_mask')\n\n        logists = tf.where(mask == 1., cos_mt, cos_t)\n        logists = tf.multiply(logists, self.logist_scale, 'arcface_logist')\n\n        return logists","acb4f3d6":"def _regularizer(weights_decay=5e-4):\n    return tf.keras.regularizers.l2(weights_decay)","9803c9c4":"preprocess_input = efficientnet.preprocess_input\n\nbase_model = EfficientNetB3(input_shape=[IMG_SIIZE, IMG_SIIZE, 3],\n                              include_top=False,\n                              weights='imagenet')\n\nbase_model.trainable = True\n\ndef build_model(w_decay=5e-4, embd_shape=256, is_training=True, num_classes=None, margin=0.5, logist_scale=32):\n    with strategy.scope():\n\n        inputs = Input([IMG_SIIZE, IMG_SIIZE, IMG_CHANNEL], name='input_image')\n\n        x = preprocess_input(inputs)\n\n        x = base_model(x)\n        \n        x = BatchNormalization()(x)\n        \n        x = Dropout(rate=0.3)(x)\n        \n        x = GlobalAveragePooling2D()(x)\n        \n        x = Dense(embd_shape, kernel_regularizer=_regularizer(w_decay))(x)\n        \n        embds = BatchNormalization(name='embs')(x)\n\n        if is_training:\n            assert num_classes is not None\n            labels = Input([], name='label')\n            logist = ArcHead(num_classes=num_classes, \n                            margin=margin,\n                            logist_scale=logist_scale)(embds, labels)\n            return Model((inputs, labels), logist)\n        else:\n            return Model(inputs, embds)","371c2c85":"model = build_model(num_classes=N_CLASSES, is_training=True)","d9b4a493":"model.summary()","7ab0f84b":"learning_rate = tf.constant(1e-4)\n\nmodel.compile(\n            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n            ) ","369dcb40":"checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB3.h5', \n                                                monitor = 'val_loss', \n                                                verbose = 2, \n                                                save_best_only = True,\n                                                save_weights_only = True, \n                                                mode = 'min')","028966e8":"steps_per_epoch = len(df_train) \/\/ BATCH_SIZE\nval_steps_per_epoch = len(df_val) \/\/ BATCH_SIZE\n\nmodel.fit(train,\n          epochs=EPOCHS,\n          steps_per_epoch=steps_per_epoch,\n          validation_data=val,\n          callbacks = [checkpoint],\n          validation_steps=val_steps_per_epoch)","22b11085":"model = Model(inputs=model.layers[0].input, outputs=model.layers[get_layer_index(model, 'embs')].output)","f4c3a9ee":"model.save('model')","6ad79b3b":"# Data Loading","0f0b9cfd":"I implemented the model with reference to the following notebook.  \n[Shopee EfficientNetB3 ArcMarginProduct | Kaggle](https:\/\/www.kaggle.com\/ragnar123\/shopee-efficientnetb3-arcmarginproduct)  \nThank you very much. @ ragnar123\n\nThe dataset I'm using is created using the following notebook.  \n[Shopee data to TFRecord | Kaggle](https:\/\/www.kaggle.com\/yukiohkawa\/shopee-data-to-tfrecord)","17759eea":"# Setting","c28c085e":"# Build Dataset","5714fa14":"# Build Model"}}