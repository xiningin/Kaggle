{"cell_type":{"98a39919":"code","abe6b422":"code","a8e2db20":"code","279ba165":"code","85def426":"code","3b13e53e":"code","8f242504":"code","70e38b59":"code","ee528ae1":"code","7612295c":"code","362fc610":"code","2ba1ef91":"code","06c52930":"code","71c431b3":"code","88ed8b1b":"code","b793e9da":"code","f637ec52":"code","bbef359a":"markdown","23c8e376":"markdown","23fb4e85":"markdown","00d43df9":"markdown","5b24d291":"markdown","1734e595":"markdown","cf56151f":"markdown","cd6a9bc5":"markdown","86152f2e":"markdown","a782fe7f":"markdown","71b7b506":"markdown","1bbda3bd":"markdown","13e274b2":"markdown","42f62eaf":"markdown","2b261b81":"markdown","db82e81e":"markdown","94b632e7":"markdown"},"source":{"98a39919":"#importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#modeling\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","abe6b422":"#set color palette\nsns.set_palette(\"Spectral_r\")","a8e2db20":"#import dataset \ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n\n#output file \nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\nFEATURES = train.columns[:-1]\nTARGET = train.columns[-1]","279ba165":"#Overview of train dataset\ntrain.head()","85def426":"#dimensions of the dataset\nprint(f'The shape of the train dataset {train.shape}')","3b13e53e":"train.describe()","8f242504":"#overview of test data\ntest.head()","70e38b59":"print(f'The shape of the test data is {test.shape}')","ee528ae1":"submission.head()","7612295c":"#missing values\nmissing = train.isnull().sum()\nmissing","362fc610":"#missing values plot\nmissing\/len(train)","2ba1ef91":"#checking for imbalance in the dataset\ncount = train['claim'].value_counts().values\nsns.barplot(x = [0,1], y = count)\nplt.title('Target variable count')","06c52930":"#distribution of features in train dataset\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(train.columns):\n    fig.add_subplot(np.ceil(len(train.columns)\/4), 4, idx+1)\n    train.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.show()","71c431b3":"#ditribution of features in test data\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(test.columns):\n    fig.add_subplot(np.ceil(len(test.columns)\/4), 4, idx+1)\n    test.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.show()","88ed8b1b":"#correlation between features\ncorr = train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 16))\n    ax = sns.heatmap(corr, mask=mask, cmap = 'Spectral_r', vmax=.3, square=True)","b793e9da":"#modeling\nX = train.loc[:, FEATURES]\ny = train.loc[:, TARGET]\n\nfinal_predictions = []\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X, y)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    model = XGBClassifier(random_state=42, verbosity=0, tree_method='gpu_hist')\n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_predictions.append(preds_test)\n    print(fold, roc_auc_score(y_valid, preds_valid))","f637ec52":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Make predictions\ny_pred = pd.DataFrame({'id': submission['id'], 'claim': preds})\n\n# Create submission file\ny_pred.to_csv(\"submission.csv\")","bbef359a":"<a id = \"9\" ><\/a>\n## Submission\nThis is my final submission file. ","23c8e376":"### Quick summary statistics of the data\nThe summary statistics shows the min, max, mean, standard deviation and quartile infomation for each feature column","23fb4e85":"<a id = \"5\" ><\/a>\n### Missing values \nWe will check if there are missing values in our dataset","00d43df9":"<a id = \"2\" ><\/a>\n## Importing the dataset\nWe are using three different files in this notebook and we will import all three files before starting our analysis.\n\n* `train.csv` - the training data with the target claim column\n* `test.csv` - the test set; you will be predicting the claim for each row in this file\n* `sample_submission.csv` - a sample submission file in the correct format","5b24d291":"### Imbalance in the distribution of Target variable\nFrom the plot below, we see that the target variable `claim` is fairly balanced","1734e595":"<a id = \"8\" ><\/a>\n## Modeling\nIn this notebook, I will be using XGBoost Classifier. I base my model on this notebook given by the [kaggle competition team](https:\/\/www.kaggle.com\/hsuchialun\/tps-xgboost-kfold-with-gpu#Step1:-Import-Helpful-Libraries). I changed few parameters in my model. ","cf56151f":"As shown above, our dataset contains missing values. Now, I will check the proportion of missing values in each column.\n\nIt can be noted that, on an average the proportion of missing values ranges between **(1.60 - 1.65)%**","cd6a9bc5":"# Thanks for reading! Upvote if you find this notebook useful ","86152f2e":"<a id = \"7\" ><\/a>\n### Correlations\nThere seem to be a very little or no correlation between features as well as feature-to-target correlation.","a782fe7f":"# Introduction\nTabular Playground Series are a month-long competions that are released on 1st of every month. These are designed to be beginner friendly and help bridge the gap between inclass competition and featured competition.\n\nThe aim of TPS September 2021  is to predict if the customer will claim a insurance policy or not. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nThis table of contents gives an overview about different sections in the notebook.\n\n1. [Load Required Libraries](#1)\n2. [Import the Dataset](#2)\n3. [Exploratory Data Analysis](#3)\n    * [Train Dataset](#3)\n    * [Test Dataset](#4)\n    * [Missing Values](#5)\n    * [Distributions](#6)\n    * [Correlations](#7)\n4. [Modeling](#8)\n5. [Submission](#9)","71b7b506":"### Submission File\nThe format of the output submission file is shown below: \n* It contains only two columns namely, the `id` column and the `claim` column","1bbda3bd":"### Dataframe dimensions\n* The `train` dataset contains 957919 rows of data and 120 features","13e274b2":"<a id = \"6\" ><\/a>\n### Feature Distributions\nShowing distribution on each feature that are available in train and test dataset. We observe that all features distribution on train and test dataset are almost similar.","42f62eaf":"<a id = \"1\" ><\/a>\n## Loading Required Libraries","2b261b81":"<a id = \"4\" ><\/a>\n### Test Data","db82e81e":"### Dataset Dimensions\n* The `test` dataset contains 493474 rows of data and 119 features","94b632e7":"<a id = \"3\" ><\/a>\n## Exploratory Data Analysis\nThe aim of this step is to explore the dataset a bit to get insights about the shape of the data, datatypes of the feature columns, missing values and so on.\n\n### Train Dataset"}}