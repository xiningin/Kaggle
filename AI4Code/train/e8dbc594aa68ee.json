{"cell_type":{"301be78b":"code","b4c5855b":"code","c0b61e9d":"code","262c72fc":"code","2b1f77a5":"code","b348cdd0":"code","ec17d140":"code","ef8b54b4":"code","73ea4520":"code","e858f39f":"code","e2b1ae77":"code","5601a705":"code","d3363729":"code","ea59255b":"code","c436a625":"code","c8bc8457":"code","7e8d8850":"code","cd5311db":"code","6788af93":"code","09dbd449":"code","afb7be11":"code","8a70ec45":"code","1ce9cc85":"code","99589a9c":"code","a4dcdf28":"code","5dfa414f":"code","2f7a9797":"code","1ef41df0":"code","655c3311":"code","05613754":"code","fe1a896f":"code","bc5c351e":"code","50cc7d24":"code","59e98c10":"code","cca55c64":"code","a0313db6":"code","5c495f96":"code","1af818c9":"code","6e312ae9":"code","b3c7d471":"code","732890d7":"code","2dfbab09":"code","395365f7":"code","98282a66":"code","7bcc0d25":"code","1eb932a5":"code","5eda0bc0":"code","bb4e9c0c":"code","8bf0d60c":"code","158d4d47":"code","680254ac":"code","e0dc61bc":"code","569d7e12":"code","e5855d97":"code","133b63d4":"code","f738db5a":"code","3c5efc7b":"code","a0301bbc":"code","ce28fa66":"code","f1d3d93a":"code","0a321433":"code","a7b31820":"code","1b7b4eab":"code","438c87c0":"code","88c4ebcf":"code","24107531":"code","abd2c408":"code","fdf06295":"code","a2f0599f":"code","9a68fde0":"code","1c59a8d7":"code","28678a88":"code","6a36ef2c":"code","825bc6be":"code","9e2da6d7":"code","fa618149":"code","d6973213":"code","3e1c4406":"code","dca8d539":"code","63c007b4":"code","936cd951":"code","a9d8d726":"code","60fff2d2":"code","a518f2af":"code","e139c6d2":"code","746d3a8a":"code","01439433":"code","1213887d":"markdown","51a7e20e":"markdown","48d6ad3f":"markdown","74fa826c":"markdown","409cb61a":"markdown","b0592326":"markdown","af09f1ab":"markdown","5c2332af":"markdown","bf91ccf9":"markdown","5428f4b0":"markdown","97c9829e":"markdown","fb28d866":"markdown","4fa42250":"markdown","49b691aa":"markdown","81581838":"markdown"},"source":{"301be78b":"INPUT_DATASET_PATH = \"..\/input\/imdb-review-dataset\/imdb_master.csv\"","b4c5855b":"# !pip install nltk","c0b61e9d":"import nltk\nfrom collections import Counter\nimport itertools\nfrom typing import List","262c72fc":"import torch","2b1f77a5":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids: List[int], label_id: int):\n        self.input_ids = input_ids\n        self.label_id = label_id","b348cdd0":"class Vocab:\n    def __init__(self, itos: List[str], unk_index: int):\n        self._itos = itos \n        # \u0441\u0442\u0440\u043e\u0438\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u0438\u043d\u0434\u0435\u043a\u0441 - \u0441\u043b\u043e\u0432\u043e - \u043d\u043e\u043c\u0435\u0440\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]","ec17d140":"from tqdm.notebook import tqdm\n","ef8b54b4":"class TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()","73ea4520":"class SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '<\/UNK>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size # \n        self._tokenizer = nltk.tokenize.TweetTokenizer()\n        \n    def tokenize(self, text):\n        return self._tokenizer.tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        # \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435 \u0431\u0443\u0434\u0443\u0442 max_vocab_size - 2 \u0441\u0430\u043c\u044b\u0445 \u0447\u0430\u0441\u0442\u044b\u0445 \u0441\u043b\u043e\u0432\n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform_single_text(self, text):\n        tokens =  self.tokenize(text)\n        ids = [self.vocab.word2id(token) for token in tokens]\n        return ids\n        \n    def transform(self, texts):\n        result = []\n        for text in tqdm(texts):\n            result.append(self.transform_single_text(text))\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in tqdm(texts)]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tqdm(tokenized_texts):\n            tokens = tokens\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","e858f39f":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])","e2b1ae77":"def features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","5601a705":"from sklearn import model_selection\nimport pandas as pd","d3363729":"imdb_df = pd.read_csv(INPUT_DATASET_PATH, encoding='latin-1')\ntrain_val_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(train_val_df, test_size=0.05, stratify=train_val_df.label)","ea59255b":"train_df.shape","c436a625":"val_df.shape","c8bc8457":"train_df.sample(5)","7e8d8850":"text2id = SimpleTextTransformer(max_vocab_size=10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])","cd5311db":"print(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])\nprint([text2id.vocab.id2word(x) for x in train_ids[0][:30]])","6788af93":"max_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}","09dbd449":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]","afb7be11":"print(train_features[3].__dict__)","8a70ec45":"train_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)","1ce9cc85":"print(train_tensor.size())","99589a9c":"print(len(text2id.vocab))","a4dcdf28":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression","5dfa414f":"count_vectorizer = CountVectorizer(max_features=10_000)","2f7a9797":"X_train = count_vectorizer.fit_transform(train_df['review'])\nX_val = count_vectorizer.transform(val_df['review'])","1ef41df0":"import numpy as np","655c3311":"y_train = np.array([classes[c] for c in train_df['label']])\ny_val = np.array([classes[c] for c in val_df['label']])","05613754":"y_train[:10]","fe1a896f":"print(X_train[0])","bc5c351e":"for i in X_train[0].indices:\n    print(count_vectorizer.get_feature_names()[i], X_train[0,i])","50cc7d24":"log_reg = LogisticRegression(max_iter=500)","59e98c10":"log_reg.fit(X_train,y_train,)","cca55c64":"from sklearn import metrics","a0313db6":"lr_pred = log_reg.predict(X_val)","5c495f96":"print(metrics.classification_report(y_val, lr_pred))","1af818c9":"print(log_reg.coef_.shape)","6e312ae9":"sorted_weights_indices = log_reg.coef_.argsort()[0]","b3c7d471":"sorted_weights_indices[:10]","732890d7":"cv_feature_names = count_vectorizer.get_feature_names()","2dfbab09":"for index in sorted_weights_indices[:20]:\n    print(cv_feature_names[index], log_reg.coef_[0,index])","395365f7":"for index in sorted_weights_indices[-20:]:\n    print(cv_feature_names[index], log_reg.coef_[0,index])","98282a66":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset","7bcc0d25":"train_dataset = TensorDataset(train_tensor, train_labels.type(torch.float32))\nval_dataset = TensorDataset(val_tensor, val_labels.type(torch.float32))","1eb932a5":"class BILSTM_Network(nn.Module):\n    def __init__(self, vocab_size, pad_index, embedding_size=300, hidden_size=512):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embedding_size, pad_index)\n        self.drop1 = nn.Dropout(p=0.5)\n        self.rnn = nn.LSTM(300, hidden_size, bidirectional=True, num_layers=1, batch_first=True)\n        self.drop2 = nn.Dropout(p=0.5)\n        self.fc = nn.Linear(2 * hidden_size,1)\n        self.pad_index = 0\n        \n    def forward(self, x):\n        pad_mask = (x == 0).type(torch.float32).view(x.size(0), x.size(1), 1)\n        # x: B x N\n        batch_size = x.size(0)\n        x = self.emb(x)\n        x = self.drop1(x)\n        # x: B, N, C\n        \n        # x: B x N x 2h\n        seq,_ = self.rnn(x)\n        \n        x = self.drop2(seq)\n        x = pad_mask * -1e9 + x * (1 - pad_mask)\n        \n        #x: B x 2h\n        x,_ = torch.max(x, dim=1)\n        x = self.fc(x)\n        #x: B x 1\n        x = torch.sigmoid(x)\n        return x.view(-1)\n    \n    def to_prediction(self, output):\n        return output > 0.5\n        ","5eda0bc0":"class BestModel:\n    def __init__(self, path, initial_criterion):\n        self.path = path\n        self.criterion = initial_criterion\n        \n    def update(self, model, optimizer, criterion):\n        self.criterion = criterion\n        torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'criterion': criterion}, self.path)\n        \n    def load_model_data(self):\n        return torch.load(self.path)\n    \n    def restore(self, model, optimizer):\n        model_data = self.load_model_data()\n        model.load_state_dict(model_data['model_state'])\n        optimizer.load_state_dict(model_data['optimizer_state'])","bb4e9c0c":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","8bf0d60c":"import numpy as np","158d4d47":"def train_model(epochs, model, optimizer, criterion, loaders, device, best_model, lr_scheduler=None, n_prints=1, clip=None):\n    print_every = len(loaders['train']) \/\/ n_prints\n    for epoch in range(epochs):\n        model.train()\n        running_train_loss = 0.0\n        \n        for iteration, (xx, yy) in enumerate(loaders['train']):\n            optimizer.zero_grad()\n            xx, yy = xx.to(device), yy.to(device)\n            out = model(xx)\n            loss = criterion(out, yy)\n            running_train_loss += loss.item()\n            loss.backward()\n            \n            if clip is not None:\n                nn.utils.clip_grad_norm_(model.parameters(),clip)\n            \n            optimizer.step()\n            \n            if(iteration % print_every == print_every - 1):\n                running_train_loss \/= print_every\n                print(f\"Epoch {epoch}, iteration {iteration} training_loss {running_train_loss} lr={np.round(get_lr(optimizer),6)}\")\n                running_train_loss = 0.0\n                \n        if lr_scheduler:\n            lr_scheduler.step()\n            \n        with torch.no_grad():\n            model.eval()\n            running_corrects = 0\n            running_total = 0\n            running_loss = 0.0\n            for xx, yy in loaders['validation']:\n                batch_size = xx.size(0)\n                xx, yy = xx.to(device), yy.to(device)\n\n                out = model(xx)\n                \n                loss = criterion(out, yy)\n                running_loss += loss.item()\n                \n                predictions = model.to_prediction(out).type(torch.float32)\n                running_corrects += (predictions == yy).sum().item()\n                running_total += batch_size\n            \n            mean_val_loss = running_loss \/ len(loaders['validation'])\n            accuracy = running_corrects \/ running_total\n            \n            if accuracy > best_model.criterion:\n                best_model.update(model, optimizer, accuracy)\n            \n            print(f\"Epoch {epoch}, val_loss {mean_val_loss}, accuracy = {accuracy}, lr={get_lr(optimizer)}\")\n    best_model.restore(model, optimizer)","680254ac":"device = torch.device('cuda') if torch.cuda.is_available else 'cpu'\nnetwork = BILSTM_Network(len(text2id.vocab), pad_index=0, embedding_size=300, hidden_size=1024\/\/2).to(device)","e0dc61bc":"print(device)","569d7e12":"optimizer = torch.optim.Adam(network.parameters(),lr=3e-4)\ncriterion = nn.BCELoss()\nbest_model = BestModel(\"best_model.md\", 0)\nscheduler= None\n# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,0.9,-1)","e5855d97":"train_loader = DataLoader(train_dataset,32,shuffle=True)\nval_loader = DataLoader(val_dataset,64, shuffle=False)","133b63d4":"train_model(20, network,\n            optimizer,\n            criterion,\n            {'train': train_loader, 'validation': val_loader},\n            device,\n            best_model, \n            n_prints=5,\n            lr_scheduler=scheduler,\n            clip=1.0)","f738db5a":"best_model.restore(network, optimizer)","3c5efc7b":"def evaluate(model, loader, device):\n    all_preds = []\n    correct_preds = []\n    with torch.no_grad():\n        model.eval()\n        for xx, yy in loader:\n            xx = xx.to(device)\n            output = model(xx)\n            all_preds.extend((output > 0.5).tolist())\n            correct_preds.extend(yy.type(torch.int8).tolist())\n            \n    return all_preds, correct_preds","a0301bbc":"model_preds, correct_preds = evaluate(network, val_loader, device)","ce28fa66":"from sklearn import metrics\nprint(metrics.classification_report(correct_preds, model_preds))","f1d3d93a":"!pip install transformers","0a321433":"import gc\ndel network\ndel optimizer\ngc.collect()\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()","a7b31820":"from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer","1b7b4eab":"selected_model = \"bert-base-multilingual-cased\"","438c87c0":"bert_tokenizer = AutoTokenizer.from_pretrained(selected_model)","88c4ebcf":"bert_model = AutoModel.from_pretrained(selected_model)","24107531":"print(bert_tokenizer.tokenize(train_df['review'].iloc[0]))","abd2c408":"bert_max_length = 300","fdf06295":"bert_train = [ bert_tokenizer.encode(t, return_tensors='pt',\n                                     max_length=bert_max_length,\n                                     truncation=True,\n                                     padding='max_length').view(-1) for t in tqdm(train_df['review'])]","a2f0599f":"bert_val = [ bert_tokenizer.encode(t, return_tensors='pt',\n                                     max_length=bert_max_length,\n                                     truncation=True,\n                                     padding='max_length').view(-1) for t in tqdm(val_df['review'])]","9a68fde0":"bert_train = torch.stack(bert_train)\nbert_val  = torch.stack(bert_val)","1c59a8d7":"bert_model.eval()\npass","28678a88":"with torch.no_grad():\n    res = bert_model.forward(bert_train[0:1],output_hidden_states=True)","6a36ef2c":"for l, h in enumerate(res.hidden_states):\n    print(l)\n    print(h.size())","825bc6be":"\nbert_model = bert_model.to(device)","9e2da6d7":"next(iter(DataLoader(bert_train,batch_size=16))).size()","fa618149":"def get_bert_layers(bert: BertModel, inputs_ids_batch, selected_layers: List[int]):\n    output = bert_model(inputs_ids_batch,output_hidden_states=True)\n    if len(selected_layers) > 1:\n        concatenated = torch.cat([output.hidden_states[l] for l in selected_layers], dim=-1)\n    else:\n        concatenated = output.hidden_states[selected_layers[0]]\n    \n    return concatenated","d6973213":"def pool_bert(input_ids, selected_layers, batch_size=16):\n    \n    pooled = np.zeros((len(input_ids), 768 * len(selected_layers)), dtype=np.float32)\n    offset = 0\n    for batch in tqdm(DataLoader(input_ids,batch_size=batch_size, shuffle=False)):\n        batch = batch.to(device)\n        with torch.no_grad():\n            concatenated = get_bert_layers(bert_model, batch, selected_layers)\n\n        non_padding_mask = (batch > 0.5).view(batch.size(0), batch.size(1), 1).type(torch.float32)\n        summed = (non_padding_mask * concatenated).sum(dim=-2)\n        \n        lengths = non_padding_mask.squeeze(-1).sum(dim=-1,keepdim=True)\n        mean_pooled = summed \/ lengths\n\n        pooled[offset:offset + batch.size(0)] = mean_pooled.cpu().numpy()\n        offset += batch.size(0)\n    return pooled","3e1c4406":"bert_train_pooled = pool_bert(input_ids=bert_train, selected_layers=[10,11,12],batch_size=32)","dca8d539":"bert_val_pooled = pool_bert(input_ids=bert_val, selected_layers=[10,11,12],batch_size=32)","63c007b4":"bert_log_reg = LogisticRegression(max_iter=500)\nbert_log_reg.fit(bert_train_pooled, y_train)","936cd951":"bert_log_pred = bert_log_reg.predict(bert_val_pooled)","a9d8d726":"print(metrics.classification_report(y_val, bert_log_pred))","60fff2d2":"class LSTMOverBERT(nn.Module):\n    def __init__(self, bert: BertModel, lstm_hidden_size=512):\n        super().__init__()\n        self.bert = bert\n        \n    def bert_encode(self, bert_input):\n        # ?\n        pass\n    \n    def lstm_encode(self, bert_output):\n        # ?\n        pass\n    \n    def classify(self, lstm_encoded, padding_mask):\n        pass\n    \n    \n    def forward(self,bert_input):\n        padding_mask = bert_input == 0\n        \n        # BATCH_SIZE x TEXT_LENGTH x BERT_HIDDEN\n        encoded = self.bert_encode(bert_input)\n        \n        # BATCH_SIZE x TEXT_LENGTH x LSTM_HIDDEN\n        lstm_encoded = self.lstm_encode(encoded)\n        \n        logits = self.classify(lstm_encoded, padding_mask)\n        \n        return torch.sigmoid(logits).view(-1)\n        ","a518f2af":"# network = LSTMOverBERT(bert_model, lstm_hidden_size=384)\n# optimizer = torch.optim.Adam(network.parameters(),lr=3e-4)\n# criterion = nn.BCELoss()\n# best_model = BestModel(\"best_model_over_bert.md\", 0)\n# scheduler= None","e139c6d2":"# bert_train_dataset = TensorDataset(bert_train, train_labels.type(torch.float32))\n# bert_val_dataset = TensorDataset(bert_val, val_labels.type(torch.float32))","746d3a8a":"# train_loader = DataLoader(bert_train_dataset,24,shuffle=True)\n# val_loader = DataLoader(bert_val_dataset,32, shuffle=False)","01439433":"# train_model(...)","1213887d":"\u0421\u0442\u0440\u043e\u0438\u043c \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445. \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u043c \u0434\u043b\u0438\u043d\u0443 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043d\u043e\u043c\u0435\u0440\u043e\u0432 \u0440\u0430\u0432\u043d\u043e\u0439 max_seq_len. ","51a7e20e":"\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043a\u0430\u0436\u0434\u044b\u0439 \u0432\u0435\u0441 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c\u0443 \u0441\u043b\u043e\u0432\u0443, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043d\u044f\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0447\u0438\u0442\u0430\u0435\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0435\u0441\u043e\u043c\u044b\u043c\u0438 \u043f\u0440\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u0438 \u0437\u0430\u0434\u0430\u0447\u0438","48d6ad3f":"<img src=\"https:\/\/raw.githubusercontent.com\/huggingface\/transformers\/master\/docs\/source\/imgs\/transformers_logo_name.png\">","74fa826c":"## \u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u044e BERT","409cb61a":"\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0432 \u0443\u0434\u043e\u0431\u043e\u0432\u0430\u0440\u0438\u043c\u044b\u0439 \u0434\u043b\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0432\u0438\u0434.<br>\n\u0410 \u0438\u043c\u0435\u043d\u043d\u043e:\n\n- \u0422\u0435\u043a\u0441\u0442 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 (\u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f, \u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u043d\u0438\u044f \u0441\u0447\u0438\u0442\u0430\u044e\u0442\u0441\u044f \u0441\u043b\u043e\u0432\u0430\u043c\u0438)\n- \u0421\u043b\u043e\u0432\u0430 \u043f\u043e\u0434\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\u0440\u044f. \u041a\u0430\u0436\u0434\u043e\u043c\u0443 \u0441\u043b\u043e\u0432\u0443 \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043d\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 (\u0438\u043d\u0434\u0435\u043a\u0441, \u0430\u0439\u0434\u0438) \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435. \u0420\u0435\u0434\u043a\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u043c \u043d\u0430\u0437\u043d\u0430\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 (\u044d\u043a\u0432\u0438\u0432\u0430\u043b\u0435\u043d\u0442\u043d\u043e \u0437\u0430\u043c\u0435\u043d\u0435 \u0440\u0435\u0434\u043a\u0438\u0445 \u0441\u043b\u043e\u0432 \u043d\u0430 \u0441\u043f\u0435\u0446. \u0441\u043b\u043e\u0432\u043e **\\<UNK\\>** (\u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e)). \n- \u041f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044e\u0442\u0441\u044f \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043d\u043e\u043c\u0435\u0440\u043e\u0432 \u0441\u043b\u043e\u0432.\n- \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043f\u043e \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0439 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u0435 \u0447\u0435\u0440\u0435\u0437 \u043e\u0431\u0440\u0435\u0437\u0430\u043d\u0438\u0435 \u0438\u043b\u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043d\u043e\u043c\u0435\u0440\u043e\u043c \u0441\u043f\u0435\u0446.\u0441\u0438\u043c\u0432\u043e\u043b\u0430 **\\<PAD\\>**","b0592326":"### \u0414\u0435\u043b\u0438\u043c \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u044b","af09f1ab":"\u041f\u043e\u043b\u0443\u0447\u0438\u043c sparse-\u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0438\u0437 scipy","5c2332af":"\u041a\u043b\u0430\u0441\u0441 \u0441\u043b\u043e\u0432\u0430\u0440\u044f. \u041c\u0435\u0442\u043e\u0434 word2id \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043d\u043e\u043c\u0435\u0440 \u0441\u043b\u043e\u0432\u0430, id2word - \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442, \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u043e.\nunk_index - \u043d\u043e\u043c\u0435\u0440 \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0431\u0443\u0434\u0443\u0442 \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0435\u043d\u044b \u0432\u0441\u0435 \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430","bf91ccf9":"### \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","5428f4b0":"\u041f\u0440\u043e\u0441\u0442\u0430\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430. \u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 NLTK. \u0412 \u0441\u043b\u043e\u0432\u0430\u0440\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u0441\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043f\u0435\u0446. \u0441\u043b\u043e\u0432. ","97c9829e":"\u0412\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u0435, \u0447\u0442\u043e\u0431\u044b \u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u043f\u0430\u043c\u044f\u0442\u044c \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b","fb28d866":"\u0418\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441 \u043e\u0431\u044a\u0435\u043a\u0442\u0430, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044e\u0449\u0435\u0433\u043e \u0442\u0435\u043a\u0441\u0442\u044b \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043d\u043e\u043c\u0435\u0440\u043e\u0432. \n\n**fit_transform** \u0432\u044b\u0443\u0447\u0438\u0432\u0430\u0435\u0442 \u043d\u043e\u0432\u044b\u0439 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0442\u0435\u043a\u0441\u0442 \u0441 \u0435\u0433\u043e \u043f\u043e\u043c\u043e\u0449\u044c\u044e. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432.\n\n**transform** \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0443\u0436\u0435 \u0432\u044b\u0443\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\u0440\u044f. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0442\u0435\u043a\u0441\u0442\u043e\u0432","4fa42250":"### \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u0440\u043e\u0441\u0442\u0443\u044e Bag-of-words \u043c\u043e\u0434\u0435\u043b\u044c (\u0441 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0451\u043c)","49b691aa":"### \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0438 \u043d\u0430\u0442\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c LSTM \u043f\u043e\u0432\u0435\u0440\u0445 BERT","81581838":"\u041a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0432\u0438\u0434\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u0441\u043b\u043e\u0432 \u0438 \u0435\u0433\u043e \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043c\u0435\u0442\u043a\u0438 (\u043a\u043b\u0430\u0441\u0441\u0430)"}}