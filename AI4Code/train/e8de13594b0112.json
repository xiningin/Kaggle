{"cell_type":{"ffb3ed94":"code","86b107a6":"code","5ebbbe73":"code","659ed060":"code","51fff1ef":"code","fa2cd93a":"code","f37a17bf":"code","3072dbb2":"code","ac7e449a":"code","48a24d52":"code","a99fe35e":"code","da0c441e":"code","ad0c02d8":"code","307710ac":"code","c48d136f":"code","0f8ae935":"code","969a69a6":"code","97f97670":"code","d00df19d":"code","6191ba4b":"code","2ab78646":"code","dea2ee57":"code","11cb8d04":"code","8bbcc93c":"code","840c33af":"code","053d4a67":"code","90dad453":"code","c939afa3":"code","63938461":"code","260fb0bb":"code","6f8185b8":"code","c659da9a":"code","f958bcf9":"code","8a144a33":"code","49fc1555":"code","828b785e":"code","27144340":"code","e1145119":"code","f63cdcd1":"code","251fe150":"code","ce535849":"code","29596217":"code","706da8d2":"code","ad6d762b":"code","f00959e7":"code","36b37778":"code","8247571b":"code","4c30bf25":"code","2ffde89a":"markdown","817ac78c":"markdown","18e9658a":"markdown","eaa081db":"markdown","49b78181":"markdown","06723963":"markdown","27be2b8e":"markdown","d47fba6b":"markdown","50c3d76e":"markdown"},"source":{"ffb3ed94":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport imblearn\nimport warnings\nwarnings.filterwarnings('ignore')","86b107a6":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf.head()","5ebbbe73":"df.info() # Checking infos","659ed060":"df.isna().sum()   #checking for missing values","51fff1ef":"sns.set_style(\"darkgrid\")\nsns.countplot(data = df , x = \"stroke\");","fa2cd93a":"for i in df.columns:\n    print(f\"The number of unique values in {i} column is\/are {len(df[i].unique())}\")\n    print(\"\\n\")\n    print(f\"The unique values in {i} column is\/are {df[i].unique()}\")\n    print(\"\\n\")\n    print(f\"The value counts for each value in {i} column is\/are :  \\n{df[i].value_counts()}\")\n    print(\"\\n\\n\")\n    print(\"*\"*100)\n    print(\"\\n\\n\")","f37a17bf":"df.drop([\"id\"] , axis = 1 , inplace = True)  # Droping id columns as it has all unique data\ndf.info()","3072dbb2":"cats = df.select_dtypes(include = [\"object\"]).columns  # Categorical columns\ncats","ac7e449a":"ints = df.select_dtypes(exclude = [\"object\"]).columns  # Integer columns\nints","48a24d52":"df.head()","a99fe35e":"sns.pairplot(df)","da0c441e":"g = sns.PairGrid(df)\ng.map_diag(plt.hist)\ng.map_upper(plt.scatter)\ng.map_lower(sns.kdeplot)","ad0c02d8":"fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n  \nfig.suptitle('Count plots of categorical columns')\n  \nsns.countplot(ax = axes[0, 0], data = df, x = 'gender')\nsns.countplot(ax = axes[0, 1], data = df, x = 'ever_married')\nsns.countplot(ax = axes[0, 2], data = df, x = 'work_type')\nsns.countplot(ax = axes[1, 0], data = df, x = 'Residence_type')\nsns.countplot(ax = axes[1, 1], data = df, x = 'smoking_status')","307710ac":"for i , r in enumerate(cats):\n    plt.figure()\n    plt.title(r)\n    sns.countplot(data = df , x = r)","c48d136f":"for i , r in enumerate(ints):\n    plt.figure()\n    plt.title(r)\n    sns.histplot(df[r] , kde = True)","0f8ae935":"sns.heatmap(df.corr() , annot = True , cmap = \"coolwarm\") ","969a69a6":"cats","97f97670":"ints","d00df19d":"sns.violinplot(x = \"gender\", y = \"bmi\", data = df , hue = \"ever_married\" , palette = 'rainbow' , split = True)","6191ba4b":"g = sns.FacetGrid(df, col=\"work_type\",  row=\"Residence_type\" , hue = \"hypertension\")\ng = g.map(plt.hist, \"bmi\")","2ab78646":"sns.histplot(data = df , x = \"bmi\" , bins = 30 , hue = \"gender\")","dea2ee57":"for i , r in enumerate(cats):\n        plt.figure()\n        plt.title(r)\n        sns.boxplot(x = r , y = \"bmi\" , data = df)","11cb8d04":"ints = df.select_dtypes(exclude = [\"object\"]).columns\nints","8bbcc93c":"df[\"bmi\"].median() , df[\"bmi\"].mean()","840c33af":"df[\"bmi\"].fillna(df[\"bmi\"].median() , inplace = True)\ndf.isna().sum()","053d4a67":"df.isna().sum()","90dad453":"df.loc[df[\"gender\"] == \"Other\" , \"gender\"] = \"Female\"","c939afa3":"df[\"gender\"].value_counts()","63938461":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\n\ndf[\"gender\"] = lb.fit_transform(df[\"gender\"])\ndf[\"ever_married\"] = lb.fit_transform(df[\"ever_married\"])\ndf[\"Residence_type\"] = lb.fit_transform(df[\"Residence_type\"])","260fb0bb":"df.head()","6f8185b8":"for i in df.columns:\n    print(f\"The number of unique values in {i} column is\/are {len(df[i].unique())}\")\n    print(\"\\n\")\n    print(f\"The unique values in {i} column is\/are {df[i].unique()}\")\n    print(\"\\n\")\n    print(f\"The value counts for each value in {i} column is\/are :  \\n{df[i].value_counts()}\")\n    print(\"\\n\\n\")\n    print(\"*\"*100)\n    print(\"\\n\\n\")","c659da9a":"df.isna().sum()","f958bcf9":"df = pd.get_dummies(df , columns = [\"work_type\" , \"smoking_status\"] , prefix = [\"work_type\" , \"smoking_status\"])\ndf.drop([\"work_type_Govt_job\" , \"smoking_status_Unknown\"] , axis = 1 , inplace = True)\ndf.head()","8a144a33":"from sklearn.model_selection import train_test_split\nX = df.drop([\"stroke\"] , axis = 1 )\ny = df[\"stroke\"]\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)","49fc1555":"len(X_train) , len(X_test)","828b785e":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncols = [\"age\" , \"avg_glucose_level\" , \"bmi\"]\nX_train[cols] = scaler.fit_transform(X_train[cols])\nX_test[cols] = scaler.transform(X_test[cols])","27144340":"X_train.head()","e1145119":"X_test.head()","f63cdcd1":"from imblearn.over_sampling import SMOTE\ny_train.value_counts()","251fe150":"sm = SMOTE(random_state = 42)\nX_train_1, y_train_1 = sm.fit_resample(X_train , y_train.ravel())\nsum(y_train_1 == 1) , sum(y_train == 0)","ce535849":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.metrics import confusion_matrix , roc_auc_score , precision_score , recall_score , f1_score , accuracy_score , classification_report , roc_curve , auc\nfrom sklearn.model_selection import cross_val_score","29596217":"models = []\nmodels.append(['XGBClassifier', XGBClassifier(learning_rate = 0.1 , objective = 'binary:logistic' , random_state = 42 , eval_metric='mlogloss')])\nmodels.append(['Logistic Regression', LogisticRegression(random_state = 42)])\nmodels.append(['SVM', SVC(random_state = 42)])\nmodels.append(['KNeigbors', KNeighborsClassifier()])\nmodels.append(['RandomForest', RandomForestClassifier(random_state = 42)])\nmodels.append(['AdaBoostClassifier', AdaBoostClassifier()])","706da8d2":"sam1 = []\nfor i in range(len(models)):\n    sam2 = []\n    model = models[i][1]\n    model.fit(X_train_1 , y_train_1)\n    \n    \n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test , y_pred)\n    \n    \n    accuracies = cross_val_score(estimator = model, X = X_train_1 , y = y_train_1, cv = 5)\n    roc = roc_auc_score(y_test , y_pred)\n    precision = precision_score(y_test , y_pred)\n    recall = recall_score(y_test , y_pred)\n    f1 = f1_score(y_test , y_pred)\n    \n    \n    \n    print(models[i][0],':')\n    print(cm)\n    print('Accuracy Score: ' , accuracy_score(y_test,y_pred))\n    print('\\n')\n    print('K-Fold Validation Mean Accuracy: {:.2f} %'.format(accuracies.mean()*100))\n    print('\\n')\n    print('Standard Deviation: {:.2f} %'.format(accuracies.std()*100))\n    print('\\n')\n    print('ROC AUC Score: {:.2f} %'.format(roc))\n    print('\\n')\n    print('Precision: {:.2f} %'.format(precision))\n    print('\\n')\n    print('Recall: {:.2f} %'.format(recall))\n    print('\\n')\n    print('F1 Score: {:.2f} %'.format(f1))\n    print(\"\\n\")\n    print('*'*40)\n    print('\\n\\n\\n')\n    \n    \n    \n    sam2.append(models[i][0])\n    sam2.append(accuracy_score(y_test , y_pred)*100)\n    sam2.append(accuracies.mean()*100)\n    sam2.append(accuracies.std()*100)\n    sam2.append(roc)\n    sam2.append(precision)\n    sam2.append(recall)\n    sam2.append(f1)\n    sam1.append(sam2)","ad6d762b":"df2 = pd.DataFrame(sam1 , columns = ['Model','Accuracy','K-Fold Mean Accuracy','Std.Deviation','ROC_AUC','Precision','Recall','F1 Score'])\n\ndf2.sort_values(by = [\"F1 Score\" , \"ROC_AUC\" , 'K-Fold Mean Accuracy' , \"Accuracy\"] , inplace = True , ascending = False)\ndf2","f00959e7":"sns.barplot(x = \"Model\" , y = \"ROC_AUC\" , data = df2)\nplt.title(\"Model Compare\");","36b37778":"linear = LogisticRegression(random_state = 42)\nlinear.fit(X_train_1, y_train_1)\ny_pred = linear.predict(X_test)\ny_prob = linear.predict_proba(X_test)[ : , 1]\ncm = confusion_matrix(y_test , y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(\"\\n\\n\\n\")\nprint(f'ROC AUC score: {roc_auc_score(y_test , y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test , y_pred))\nprint(\"\\n\\n\\n\")\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'coolwarm', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate , true_positive_rate , color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","8247571b":"preds = linear.predict(X_test)\nsum(preds == 0) , sum(preds == 1)","4c30bf25":"predicted_probab = linear.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\n\nplt.plot(fpr, tpr, marker='.', color='green',label=\"Logistic Regressor\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","2ffde89a":"## Model Fitting and Evaluation","817ac78c":"## Model Selection","18e9658a":"## Upsampling data with SMOTE ","eaa081db":"## EDA (Exploratory data analysis)","49b78181":"## Label encoding","06723963":"## Importing Required Libraries","27be2b8e":"# Logistic Regressor Model with AUC of 80.96 percent","d47fba6b":"## Feature Scaling","50c3d76e":"## Train Test Split"}}