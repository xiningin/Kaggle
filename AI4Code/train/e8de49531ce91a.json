{"cell_type":{"58042a9b":"code","d7c5ca8b":"code","ed07a066":"code","108b433d":"code","1f50148a":"code","b1af71e3":"code","395e494f":"code","06bf057e":"code","fc93b2ed":"code","71b60a1c":"code","7a9d8a4e":"code","0753c88c":"code","c568197e":"code","20aac513":"code","dffc500a":"code","4b83bc5f":"code","7ac9140d":"code","89c83122":"code","1c6b6f6f":"markdown","4ff2e1f7":"markdown","8f175d1e":"markdown","f1eda3c3":"markdown","37ec7fcf":"markdown","25c374b4":"markdown","2d4860f9":"markdown","0f1a4083":"markdown","d76601db":"markdown","992503fd":"markdown","01956a7e":"markdown","5f1fc2b4":"markdown","9a278671":"markdown","3ba0415b":"markdown","bb7ca420":"markdown","65bfebd1":"markdown","0e2fdd51":"markdown"},"source":{"58042a9b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Read raw training data\noriginal_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\n\n# Choose only numerical data\nnumerical_cols = {cname:len(original_data[cname].unique())  for cname in original_data.columns if original_data[cname].dtype in ['int64', 'float64']}\nnumerical_data = original_data[numerical_cols]\nnumerical_data.head()","d7c5ca8b":"# Display numerical variables with missing values\ntotal = numerical_data.isnull().sum().sort_values(ascending=False)\npercent = (numerical_data.isnull().sum()\/numerical_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data[missing_data['Total']>0]","ed07a066":"# Imputation\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_data = pd.DataFrame(my_imputer.fit_transform(numerical_data))\n\n# Imputation removed column names; put them back\nimputed_data.columns = numerical_data.columns\nimputed_data.head()","108b433d":"#Display Pearson correlation HeatMap\nplt.figure(figsize=(30,20))\ncor = imputed_data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","1f50148a":"# Keep only interesting variables, which reduces the dimensionality of the problem\ninteresting_cols = ['OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'GarageCars', 'SalePrice']\nreduced_data = imputed_data[interesting_cols]\nreduced_data.head()","b1af71e3":"#scatterplot\nsns.set()\nsns.pairplot(reduced_data, size = 2.5)\nplt.show();","395e494f":"sns.regplot(x=reduced_data['1stFlrSF'], y=reduced_data['SalePrice'])","06bf057e":"sns.regplot(x=reduced_data[reduced_data['1stFlrSF']<4000]['1stFlrSF'], y=reduced_data[reduced_data['1stFlrSF']<4000]['SalePrice'])","fc93b2ed":"\ninteresting_cols = ['OverallQual', 'YearBuilt', 'YearRemodAdd', '1stFlrSF', 'GrLivArea', 'FullBath', 'GarageCars', 'SalePrice']\n\nreduced_data = reduced_data[reduced_data['1stFlrSF']<4000][interesting_cols]\nreduced_data = reduced_data[interesting_cols]\nreduced_data.head()","71b60a1c":"from scipy.stats import norm\nfrom scipy import stats\n#histogram and normal probability plot\nsns.distplot(reduced_data['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(reduced_data['SalePrice'], plot=plt)","7a9d8a4e":"#applying log transformation\nreduced_data['SalePrice'] = np.log(reduced_data['SalePrice'])","0753c88c":"#histogram and normal probability plot\nsns.distplot(reduced_data['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(reduced_data['SalePrice'], plot=plt)","c568197e":"#histogram and normal probability plot\nsns.distplot(reduced_data['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(reduced_data['GrLivArea'], plot=plt)","20aac513":"#applying log transformation\nreduced_data['GrLivArea'] = np.log(reduced_data['GrLivArea'])","dffc500a":"#histogram and normal probability plot\nsns.distplot(reduced_data['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(reduced_data['GrLivArea'], plot=plt)","4b83bc5f":"#histogram and normal probability plot\nsns.distplot(reduced_data['1stFlrSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(reduced_data['1stFlrSF'], plot=plt)","7ac9140d":"#applying log transformation\nreduced_data['1stFlrSF'] = np.log(reduced_data['1stFlrSF'])","89c83122":"#histogram and normal probability plot\nsns.distplot(reduced_data['1stFlrSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(reduced_data['1stFlrSF'], plot=plt)","1c6b6f6f":"**So far, what variables we will use to build our models?**\n\nTo summarize, those are the most interesting variables we found that we will consider in building our models of the problem:\n\n<table>\n<tbody>\n<tr><th><b>Variable<\/b><\/th><th><b>Definition<\/b><\/th><\/tr>\n<tr>\n<td>OverallQual<\/td>\n<td>Overall material and finish quality<\/td>\n<\/tr>\n<tr>\n<td>YearBuilt<\/td>\n<td>Original construction date<\/td>\n<\/tr>\n<tr>\n<td>YearRemodAdd<\/td>\n<td>Remodel date<\/td>\n<\/tr>\n<tr>\n<td>TotalBsmtSF<\/td>\n<td>Total square feet of basement area<\/td>\n<\/tr>\n<tr>\n<td>1stFlrSF<\/td>\n<td>First Floor square feet<\/td>\n<\/tr>\n<tr>\n<td>GrLivArea<\/td>\n<td>Above grade (ground) living area square feet<\/td>\n<\/tr>\n<tr>\n<td>FullBath<\/td>\n<td>Full bathrooms above grade<\/td>\n<\/tr>\n<tr>\n<td>GarageCars<\/td>\n<td> Size of garage in car capacity<\/td>\n<\/tr>\n<tr>\n<td>SalePrice<\/td>\n<td>the property's sale price in dollars. This is the target variable that we're trying to predict.<\/td>\n<\/tr>\n<\/tbody>\n<\/table>","4ff2e1f7":"The goal of this notebook is to provide clear and thorough data analysis methodology to deal with structural data. In the proposed methodology we will use intuition and common sense to draw conclusions and then test them using statistical tools. Also, we will try to explain statistical results intuitively.\n\n**For the sake of simplifying the notebook, we will only consider numerical variables in our analysis**","8f175d1e":"Let's first plot 1stFlrSF against SalePrice with regression line","f1eda3c3":"## 4. Normality \n\nFinally, we want to investigate an important statistical property of the data, which is normality. It means that data should follow a normal distribution, and we are interested in normal distributions because it has some nice properties like its symmetry.\n\nLet's start by investigating the dependent (target) variable SalePrice.\nWe will do that using two methods:\n- **Histogram** - Kurtosis and skewness.\n- **Normal probability plot** - Data distribution should closely follow the diagonal that represents the normal distribution.","37ec7fcf":"That looks great! As expected, now it has a normal distribution, and all points fall on the diagonal line!\n\n\nNow let's try this on GrLivArea and 1stFlrSF","25c374b4":"Right away we can see that SalePrice does not follow a normal distribution and has positive skewness, but luckily we know a simple solution for that, we can simply take the log transformations. ","2d4860f9":"Luckily only three variables have missing values with a low percentage. A general and safe method is to replace the missing values in each variable with the mean of values in that variable using Imputation.","0f1a4083":"**I just wanna recommend a great notebook by [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) that inspired me organize this notebook.**","d76601db":"## 3. Scatter plot\n\nScatter plots are the best tool to look for trends in the dataset. The correlation matrix told us which variables are worth looking into, and now we want to see a visualization of the interesting things happening between the selected variables.","992503fd":"## 2. Correlation matrix \n\nThe Pearson correlation matrix is a great tool to take a first look at your data. It's the best way to start looking for interesting things. It displays the correlation coefficient between any two variables in the dataset. The correlation coefficient takes values between -1 and 1. A value of 1 indicating a very strong positive relation between the two variables, while a value of -1 indicating a very strong negative relation, and a value of 0 indicates no relation.\n\nWe are interested in looking into two types of relations:\n- First, strong relations between the dependent variable (SalePrice) and the independent variables. A strong relation of this type does not necessary mean causality, since it could be a third hidden variable is causing this relation. However, it means that a particular independent variable has a strong predictive power of the dependent variable, and we can further investigate this relation.\n\n- Second, strong relations between the independent variables. If two independent variables have a strong relation it means that both are giving us the same information, thus we only need to keep one of them. Usually a good practice it to keep the variable with the better relation with the dependent variable.","01956a7e":"## 5. What next?\n\nNow after this thorough analysis we've identified the most influential variables in predicting the behavior of the target variable SalePrice, and now what left is for you to use them to build a machine learning model of your choice that predicts house prices!","5f1fc2b4":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)","9a278671":"## 1. Missing values\n\nSince we don't want to waste your time analysing variables with a lot of missing observations, exploring the missing values seems like a good place to start our analysis.\n\nWe should be careful when dealing with missing values. Things to consider are the number of missing values for each variable, and the nature of the values. If the a variable has too little observations (lots of missing values), we would not trust any information it provides about the dependent variable. Otherwise, if we have only a small portion of the observations missing, we could use a suitable technique to replace the missing values.","3ba0415b":"Let's start by looking into the two controversial variables from the last step, namely, TotalBsmtSF and 1stFlrSF. As expected, they both have a clear trend with the dependent variable (SalePrice). However, we can see that TotalBsmtSF has a set of observations with a value of zero, which means that those houses do not have a basement, and the variable 1stFlrSF does not suffer from this issue. Since both variables provide us with the same information, we will keep 1stFlrSF so we do not face troubles when transforming zero observations in the next steps (It will make sense soon).\n\nAn interesting scatterplot to look at is the one between 1stFlrSF on the x-axis and GrLivArea on the y-axis. We can see dots forming a straight line, and this line is where 1stFlrSF = GrLivArea. No dots are below that line because the living area above ground is always equal or bigger than the area of the first floor.\n\n### outliers!\nWe are mainly interested here in studying trends between the independent variables and the dependent variable, and we can see that all independent variables have a clear trend with the dependent variable, which explains the strong correlation between them. However, we can see a few observations in 1stFlrSF and GrLivArea do not follow the main trend, which could be meant that we got outliers. Outliers are extreme values in our observations that could be harmful or could carry useful information, so we need to look further into them.","bb7ca420":"Now let's plot 1stFlrSF against SalePrice without the extreme values (1stFlrSF > 4000 and low price)","65bfebd1":"The extreme values represents houses with large areas and low prices, and this could be explained by assuming they are located in agricultural areas. And since we can identify only one datapoint with a First Floor area larger than 4000, we will delete it.  ","0e2fdd51":"We already see interesting things happening. The following variables have a strong relation (correlation coefficient >=0.5) with the dependent variable:\n- OverallQual\n- YearBuilt\n- YearRemodAdd\n- TotalBsmtSF\n- 1stFlrSF\n- GrLivArea\n- FullBath\n- TotRmsAbvGrd\n- GarageCars\n- GarageArea\n\n### Interesting relations to look into\n\n**GarageArea Vs. GarageCars**\n\nWe can notice a strong relation (correlation coefficient = 0.88) between the size of the garage in square feet (GaragArea) and the size of the garage in car capacity (GarageCars). It's intuitively clear that both represent the same thing, and that is the size of the garage. We only need to keep GarageCars since it has a higher correlation with the dependent variable.\n\n\n**TotalBsmtSF Vs. 1stFlrSF**\n\nAnother interesting relation between the total square feet of the basement area (TotalBsmtSF) and the first floor square feet (1stFlrSF) with correlation = 0.82. They are basically the same in most houses, and since they have the same correlation with the dependant variable we keep them both for further investigation.\n\n**GrLivArea Vs. TotRmsAbvGrd**\n\nTotal rooms above ground (TotRmsAbvGrd) and the above-ground living area square feet (GrLivArea) have a strong correlation, and again they represent the same thing. We only need to keep GrLivArea, can you guess why?\n"}}