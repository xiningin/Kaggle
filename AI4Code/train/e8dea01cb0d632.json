{"cell_type":{"f1870580":"code","adbb9264":"code","e73c2f2c":"code","cb026f36":"code","84022327":"code","e29977c9":"code","602ef5af":"code","76e71d54":"code","bdf7458c":"code","c91217a8":"code","5a5ce201":"code","e429e4c8":"code","6f299498":"code","f7b7c3d9":"code","be5a7c52":"code","f10f42bb":"code","f2715ea2":"code","549f4599":"code","0c313087":"code","72b98adf":"code","dc1f98f9":"code","f163dbe6":"code","30fb0598":"code","0105e5e1":"code","3e266150":"code","eed8272e":"code","2ae28b91":"code","1b24e8fa":"code","9b83c0ec":"code","cc312c4e":"code","87b782ee":"code","8c2b5a5d":"code","235755a8":"code","dc79dab7":"code","2b81c1b3":"code","3996429d":"code","445ce0b2":"code","ec952557":"code","a565d574":"code","c607ab33":"code","7f6c35df":"code","4b3d2492":"code","a6fde53b":"code","941c6c4f":"code","9fc6caab":"code","2a625700":"code","6910cd08":"code","e53f3be6":"code","da188537":"code","869f77bb":"code","906901aa":"code","bb9502a7":"code","9a74081d":"code","a5b5c226":"markdown","c627e550":"markdown","65884fd2":"markdown","51402c85":"markdown","2df13952":"markdown","dcab7548":"markdown","75ca3242":"markdown","c81ead8b":"markdown","20322c07":"markdown","b20120a1":"markdown","78aac879":"markdown","35fd3255":"markdown","069b4290":"markdown","ef1ee58a":"markdown","404cbb4e":"markdown","b414befe":"markdown","57ae036e":"markdown","a2aa48c9":"markdown"},"source":{"f1870580":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n%matplotlib inline","adbb9264":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e73c2f2c":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","cb026f36":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","84022327":"sns.distplot(train.SalePrice)","e29977c9":"train.SalePrice.describe()","602ef5af":"train.SalePrice.skew()","76e71d54":"train.SalePrice.isna().sum()","bdf7458c":"train.SalePrice.isnull().sum()","c91217a8":"ntrain = train.shape[0]\nntest = test.shape[0]\n#y_train = train.SalePrice\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","5a5ce201":"def check_nas():    \n    sample_values = pd.DataFrame(index=all_data.columns,columns=['SampleValue'])\n    for i in all_data.columns:\n        sample_values.loc[i].SampleValue = all_data[i].value_counts().index[1]\n    nas = pd.DataFrame(all_data.isnull().sum(),columns=['SumOfNA'])\n    types = pd.DataFrame(all_data.dtypes,columns=['Type'])\n    sample_values.sort_index(inplace=True)\n    nas.sort_index(inplace=True)\n    types.sort_index(inplace=True)\n    alls=pd.concat([sample_values,nas,types],axis=1)\n    return(alls[alls.SumOfNA>0].sort_values('SumOfNA',ascending=False))","e429e4c8":"check_nas()","6f299498":"# Most of the NAs are probably because the property does not contain the specific thing (eg. No Pool)\nnone_feats = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu',\n              'GarageFinish','GarageQual','GarageType','GarageCond',\n              'BsmtFinType2','BsmtExposure','BsmtFinType1','BsmtQual',\n              'BsmtCond','MasVnrType','MSZoning']\nzero_feats = ['LotFrontage','GarageYrBlt','MasVnrArea']","f7b7c3d9":"for i in none_feats:\n    all_data[i].fillna('None',inplace=True)","be5a7c52":"for i in zero_feats:\n    all_data[i].fillna(0,inplace=True)","f10f42bb":"all_data.drop(['MasVnrArea','MasVnrType','Electrical'],axis=1,inplace=True)","f2715ea2":"check_nas()","549f4599":"all_data['BsmtFullBath'].fillna(0,inplace=True)\nall_data['BsmtHalfBath'].fillna(0,inplace=True)\nall_data['Functional'].fillna('Typ',inplace=True)\nall_data['Utilities'].fillna('AllPub',inplace=True)\nall_data['BsmtFinSF1'].fillna(0,inplace=True)\nall_data['BsmtFinSF2'].fillna(0,inplace=True)\nall_data['BsmtUnfSF'].fillna(0,inplace=True)\nall_data['Exterior1st'].fillna('VinylSd',inplace=True)\nall_data['Exterior2nd'].fillna('VinylSd',inplace=True)\nall_data['GarageArea'].fillna(0,inplace=True)\nall_data['GarageCars'].fillna(0,inplace=True)\nall_data['KitchenQual'].fillna('None',inplace=True)\nall_data['SaleType'].fillna('WD',inplace=True)\nall_data['TotalBsmtSF'].fillna(0,inplace=True)\n","0c313087":"check_nas()","72b98adf":"fig = plt.figure(figsize=(12,9))\nsns.heatmap(train.corr())","dc1f98f9":"imp_feat=abs(train.corr()['SalePrice']).sort_values(ascending=False).head(11).index","f163dbe6":"fig = plt.figure(figsize=(10,5))\nsns.heatmap(train[imp_feat].corr(),annot=True)","30fb0598":"imp_feat_2 = imp_feat.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd'])","0105e5e1":"# sub 5\nall_data.drop(['GarageArea','1stFlrSF','TotRmsAbvGrd'],axis=1,inplace=True)","3e266150":"imp_feat_2","eed8272e":"sns.pairplot(train[imp_feat_2])","2ae28b91":"fig = plt.figure(figsize=(10,5))\nplt.scatter(x=train.GrLivArea,y=train.SalePrice)","1b24e8fa":"x=abs(train[imp_feat_2].skew()).sort_values(ascending=False)\nx","9b83c0ec":"def check_skew(column,with_log):\n    feat = train[column]\n    if (with_log==False):\n        fig,ax = plt.subplots(figsize = (5,3))\n        ax = sns.distplot(feat,fit=norm)\n        fig,ax = plt.subplots(figsize = (5,3))\n        ax = stats.probplot(feat, plot=plt)\n        (mu, sigma) = norm.fit(feat)\n        print( 'The normal dist fit has the following parameters: \\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    elif (with_log==True):\n        feat = np.log1p(feat)\n        fig,ax = plt.subplots(figsize = (5,3))\n        ax = sns.distplot(feat,fit=norm)\n        fig,ax = plt.subplots(figsize = (5,3))\n        ax = stats.probplot(feat, plot=plt)\n        (mu, sigma) = norm.fit(feat)\n        print( 'The normal dist fit has the following parameters: \\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    ","cc312c4e":"check_skew(column='SalePrice',with_log=False)\ncheck_skew(column='SalePrice',with_log=True)","87b782ee":"train['SalePrice'] = np.log1p(train['SalePrice'])","8c2b5a5d":"check_skew(column='TotalBsmtSF',with_log=False)\ncheck_skew(column='TotalBsmtSF',with_log=True)","235755a8":"all_data['TotalBsmtSF'][all_data['TotalBsmtSF']!=0] =  np.log1p(all_data['TotalBsmtSF'][all_data['TotalBsmtSF']!=0])","dc79dab7":"fig,ax = plt.subplots(figsize = (5,3))\nax = sns.distplot(train['TotalBsmtSF'][train['TotalBsmtSF']!=0],fit=norm)\nfig,ax = plt.subplots(figsize = (5,3))\nax = stats.probplot(train['TotalBsmtSF'][train['TotalBsmtSF']!=0], plot=plt)\n(mu, sigma) = norm.fit(train['TotalBsmtSF'][train['TotalBsmtSF']!=0])\nprint( 'The normal dist fit has the following parameters: \\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","2b81c1b3":"x=abs(train[imp_feat_2].skew()).sort_values(ascending=False)\nx","3996429d":"# for sub 5\n#num_nonzero = list(set(all_data.dtypes[all_data.dtypes != \"object\"].index) & set([i for i in all_data.columns.values if sum(all_data[i]==0)==0]))\n#skewed = abs(all_data[num_nonzero].skew()).sort_values(ascending=False)\n#skewed_feats = skewed[skewed > 0.75].index\n#all_data[skewed_feats] = np.log1p(all_data[skewed_feats])","445ce0b2":"# for sub 4\n#numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n#numeric_feats\n#skewed_feats=abs(all_data[numeric_feats].skew()).sort_values(ascending=False)\n#skewed_feats = skewed_feats[skewed_feats > 0.75].index\n#all_data[skewed_feats] = np.log1p(all_data[skewed_feats])","ec952557":"all_data = pd.get_dummies(all_data)","a565d574":"all_data.shape","c607ab33":"# sub 6\n#from sklearn.decomposition import PCA\n#pca = PCA(n_components=2)\n#pca.fit(all_data)\n#x_pca = pca.transform(all_data)\n#x_pca = pd.DataFrame(x_pca, columns=['pca1','pca2'])","7f6c35df":"#all_data = pd.concat([all_data,x_pca],axis=1)","4b3d2492":"y_train = train.SalePrice\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","a6fde53b":"train[train.GrLivArea >= 4600].index","941c6c4f":"# drop outliers from train and y_train\n# for sub 3\ny_train.drop(train[train.GrLivArea >= 4600].index,inplace=True)\ntrain.drop(train[train.GrLivArea >= 4600].index,inplace=True)","9fc6caab":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","2a625700":"gb = make_pipeline(RobustScaler(), GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.5, random_state=1,max_iter=5000))\nmodel_xgb = make_pipeline(RobustScaler(), xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","6910cd08":"param_grid = {'learning_rate':[0.04,0.05,0.06],'max_depth':[2,4,6]}\nsearch_gb = GridSearchCV(GradientBoostingRegressor(n_estimators=3000,loss='huber', \n                                                   min_samples_leaf=15,\n                                                   min_samples_split=10, \n                                                   random_state =5),\n                       param_grid = param_grid\n                       ,cv=3)\ngb_pipe_search = make_pipeline(RobustScaler(),search_gb)\nsearch_gb.fit(X,y)\nsearch_gb.best_params_","e53f3be6":"param_grid = {'learning_rate':[0.04,0.05,0.06],'max_depth':[2,4,6]\n             }\nsearch_xgb =GridSearchCV(xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1),param_grid=param_grid,cv=3)\nxgb_pipe_search = make_pipeline(RobustScaler(),search_xgb)\nsearch_xgb.fit(X,y)\nsearch_xgb.best_params_","da188537":"param_grid = {'alpha':np.linspace(0.0001,0.01,100),'l1_ratio':[0.3,0.5,0.9]}\nsearch_enet = GridSearchCV(ElasticNet(random_state=3),param_grid=param_grid,cv=3)\nenet_pipe_search = make_pipeline(RobustScaler(),search_enet)\nsearch_enet.fit(X,y)\nsearch_enet.best_params_","869f77bb":"gb_opt = make_pipeline(RobustScaler(), GradientBoostingRegressor(n_estimators=3000, learning_rate=0.04,\n                                   max_depth=2, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5))\nmodel_xgb_opt = make_pipeline(RobustScaler(), xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=2, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1))\nENet_opt = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","906901aa":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","bb9502a7":"avg_opt = AveragingModels(models = [gb_opt, model_xgb_opt, ENet_opt])\navg.fit(X,y)","9a74081d":"#sub = pd.DataFrame()\n#sub['Id'] = test_ID\n#sub['SalePrice'] = np.expm1(avg_opt.predict(test))\n#sub.to_csv('submission8.csv',index=False)","a5b5c226":"We first see form the pairplot two outliars in SalePrice vs GrLivArea, let's identify and eliminate those","c627e550":"## Normalising Features and Target","65884fd2":"## Outliars","51402c85":"## Features Importance and Correlation","2df13952":"Zero values ruin log method, must not include them in log","dcab7548":"SalePrice is skewed to the right, will have to be normalised.\n\nThe maximum seems to be very far from 75% percentile, shows evidence of outliers.\n\nNo 0 value\n\nNo NA or null","75ca3242":"# Grid Search ","c81ead8b":"## Analyse Target ##","20322c07":"GarageCars and GarageArea very correlated --> keep GarageCars\n\nTotalBsmtSF and 1stFlrSF very correlated --> keep TotalBsmtSF\n\nGrLivArea and TotRmsAbvGrd very correlated --> keep GrLivArea","b20120a1":"# Principal Component Analysis","78aac879":"# Averaging Models Class","35fd3255":"# Modelling","069b4290":"# 1st Round of Models","ef1ee58a":"## Missing Data ##","404cbb4e":"# Fitting Model","b414befe":"# Write Submission","57ae036e":"# Data Processing for the House Price Kaggle Competition\n\nMy work has been inspired by the following kernels:\n\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#3.-Keep-calm-and-work-smart\n\nhttps:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\nhttps:\/\/www.kaggle.com\/apapiu\/regularized-linear-models","a2aa48c9":"# Optimized Models"}}