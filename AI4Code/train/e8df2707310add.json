{"cell_type":{"17dda2ca":"code","ac384dc9":"code","b4954722":"code","52e8ce0f":"code","ae01a979":"code","1d4ca06c":"code","8b9d8c7a":"code","4c589f26":"code","9d1b731b":"code","a29aec74":"code","178eca59":"code","ee866e94":"code","b825fe9b":"code","e83ab311":"code","baeff1c1":"code","5a21747d":"code","1682806f":"code","b5ecf53e":"code","4f68647f":"markdown","cf8cf35d":"markdown","b177c3c8":"markdown","e89c11cd":"markdown","c28d7a68":"markdown","b1318cbb":"markdown","f00282c9":"markdown","fe31849d":"markdown","e656377e":"markdown","4d2877ac":"markdown","34ce55d8":"markdown","fb6646ce":"markdown","3ca88c26":"markdown","2464ba73":"markdown","20610721":"markdown","1bd742ea":"markdown","e0066c34":"markdown","d4ca00c8":"markdown","24fc9371":"markdown"},"source":{"17dda2ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac384dc9":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","b4954722":"df.shape","52e8ce0f":"df.columns","ae01a979":"df.describe()","1d4ca06c":"df[[\"Time\",\"Amount\",\"Class\"]].describe()","8b9d8c7a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"Distribuition of Normal(0) and Frauds(1): \")\nprint(df[\"Class\"].value_counts())\n\nplt.figure(figsize=(7,5))\nsns.countplot(df['Class'])\nplt.title(\"Class Count\", fontsize=18)\nplt.xlabel(\"Is fraud?\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.show()","4c589f26":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=False)\nplt.show();\n","9d1b731b":"tmp = df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","a29aec74":"class_1.describe()","178eca59":"import plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nfraud = df.loc[df['Class'] == 1]\n\ntrace = go.Scatter(\n    x = fraud['Time'],y = fraud['Amount'],\n    name=\"Amount\",\n     marker=dict(\n                color='rgb(238,23,11)',\n                line=dict(\n                    color='blue',\n                    width=1),\n                opacity=0.5,\n            ),\n    text= fraud['Amount'],\n    mode = \"markers\"\n)\ndata = [trace]\nlayout = dict(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest'\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='fraud-amount')","ee866e94":"Fraud = df[df['Class']==1]\n\nNormal = df[df['Class']==0]\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(Fraud.Amount, bins = bins , color ='red')\nax1.set_title('Fraud')\nax2.hist(Normal.Amount, bins = bins, color = 'green')\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","b825fe9b":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","e83ab311":"timedelta = pd.to_timedelta(df['Time'], unit='s')\ndf['Time_hour'] = (timedelta.dt.components.hours).astype(int)\n\n#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(df[df['Class'] == 0][\"Time_hour\"], \n             color='g')\nsns.distplot(df[df['Class'] == 1][\"Time_hour\"], \n             color='r')\nplt.title('Fraud x Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","baeff1c1":"df.drop(['Time_hour'], axis=1, inplace=True)\n\ndf.hist(figsize=(20,20))\nplt.show()","5a21747d":"correlation_matrix = df.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","1682806f":"# Scaling\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\ndf.head()","b5ecf53e":"# df.isnull().sum()\ndf.isnull().values.any()","4f68647f":"Scaling and Distributing\n\nIn this phase of our kernel, we will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n\nWhat is a sub-Sample?\nIn this scenario, our subsample will be a dataframe with a 50\/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n\nWhy do we create a sub-Sample?\nIn the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe will cause the following issues:\n\n* Overfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n* Wrong Correlations: Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.\n\nSummary:\n\nScaled amount and scaled time are the columns with scaled values.\nThere are 492 cases of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe.\nWe concat the 492 cases of fraud and non fraud, creating a new sub-sample.","cf8cf35d":"We can clearly see that the Data is imbalanced. Since it is a fraud detection dataset having imbalance is already given because in real time the ratio of fraud transactions to the legimate transactions is very less.\n\nTo get to know more about imbalanced data and how it can affect the analysis you can read \nhttps:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/","b177c3c8":"We can see most of the fraud transactions has happened in the day time.","e89c11cd":"Now that we are ready with the data set. Lets get into the model building part.\nLets split the model building into two part\n* Model building before sampling\n* Model building after sampling","c28d7a68":"The above correlation matrix shows that none of the V1 to V28 PCA components have any correlation to each other however if we observe Class has some form positive and negative correlations with the V components but has no correlation with Time and Amount.\n\n","b1318cbb":"> Domain Knowledge : Fraud is as old as humanity itself and can take an unlimited variety of different forms. Moreover, the development of new technologies provides additional ways in which criminals may commit fraud.\n\n                                                            \n![image.png](attachment:image.png)\n\n\n\nfor instance in e-commerce the information about the card is sufficient to perpetrate a fraud. The use of credit cards is prevalent in modern day society and credit card fraud has kept on growing in recent years. Financial losses due to fraud affect not only merchants and banks (e.g. reimbursements), but also individual clients.\nIf the bank loses money, customers eventually pay as well through higher interest rates, higher membership fees, etc.\nFraud may also affect the reputation and image of a merchant causing non-financial losses that, though difficult to quantify in the short term, may become visible in the long period. For example, if a cardholder is victim of fraud with a certain company, he may no longer trust their business and choose a competitor.\n\nThe actions taken against fraud can be divided into fraud prevention, which attempts to block fraudulent transactions at source, and fraud detection, where successful fraud transactions are identified a posteriori.","f00282c9":"From the above table we can see that time ,money and class are the only unscaled featuures with names. Fow now lets look into them closely.\n","fe31849d":"**Appeal**\n\nIf you liked my work, please upvote this kernel since it will keep me motivated to work on many other more sought datasets in different domains and to come out with beginner use-friendly note books.","e656377e":"**Introduction**\n\nIn this kernel our main aim is to detect whether a transaction is a normal payment or a fraud. \n","4d2877ac":"**Data Set Info:**\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n\nIf you have already seen the dataset then by now you should be knowing that the dataset contains 284807 rows(i.e, details of 284807 transactions) and 31 features. Excluding amount,time and the target columns,the names of rest of the features are not shown due to privacy reasons. All these features are already scaled. \nThe dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nNow,lets analyze some important aspects of the dataset.\nLet's get going!! Shall we?!","34ce55d8":"# Credit Card Fraud Detection","fb6646ce":"**Feature Engineering on Time feature**\n\nAs our Time feature are in seconds lets transform it to hours to get a better understanding of the patterns.","3ca88c26":"We can see most of the fraud transactions are below 500 and most of them are 0. i.e we can say that they were not successfull in there effort of doing a fraud transaction.","2464ba73":"The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.\n\nLet's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days).","20610721":"We cant see much visualization for fraud part because its highly imbalance and the entries for Fraud is considerably very less.","1bd742ea":"The Data set has 31 features with 284807 rows.","e0066c34":"Outline:\n\nI. Understanding our data\n\na) Gather Sense of our data\n\nII. Preprocessing\n\na) Scaling and Distributing\n\nb) Splitting the Data\n\n\nIII. Random UnderSampling and Oversampling\n\na) Distributing and Correlating\n","d4ca00c8":"Lets see amount per transaction by class","24fc9371":"We can clearly see that the Data is imbalanced. To get to know more about imbalanced data and how it can affect the analysis you can read "}}