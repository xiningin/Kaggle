{"cell_type":{"9bf00796":"code","ed249561":"code","634584e5":"code","fbaadecd":"code","fdcbbe32":"code","05ebe602":"code","d1a83df0":"code","a5459ee4":"code","68603594":"code","1d28fd87":"code","994c91ed":"code","fe84d324":"code","769ee4d0":"code","163476c1":"code","aa2472dc":"code","88e43557":"code","7028905e":"code","0d0d03e2":"code","3a22dd43":"code","b8c8c923":"code","982960b6":"code","207e592b":"code","d26bc697":"code","170402bd":"code","b8332120":"code","d4b50e7c":"code","c14bae68":"code","d524f7ef":"code","47a7af8f":"code","bcbdf280":"markdown","f3564e68":"markdown","4bace0c3":"markdown","f261d55c":"markdown","54e843cc":"markdown","4d817b0f":"markdown","2edfae93":"markdown","5853ac8e":"markdown","fd98dc81":"markdown","98e48ea1":"markdown","740ee2c4":"markdown","57ab50da":"markdown","d426b2a0":"markdown","d8d85224":"markdown","72ba3f1f":"markdown","8c194304":"markdown","807720fa":"markdown","dca75dd1":"markdown","05eda358":"markdown","51854f8a":"markdown","7c669b7b":"markdown","c47dedc9":"markdown","fab27c6e":"markdown","a5c17eb4":"markdown"},"source":{"9bf00796":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport sqlite3\nimport re\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\nimport gc\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n# Converting to CSR_Matrix..\nfrom scipy.sparse import csr_matrix","ed249561":"db = '\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite'\nconnection = sqlite3.connect(db)\n\n\ndf_filtered = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\",connection)\n\n\nprint(\"Number of data points in our data\", df_filtered.shape)\n# df_filtered = df_filtered.head(3000)","634584e5":"df_filtered['Score'] = df_filtered['Score'].apply(lambda x: 1 if x>3 else 0)\ndf_filtered['Score'].head(3)","fbaadecd":"#Sorting data according to ProductId in ascending order\ndf_sorted=df_filtered.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n#Deduplication of entries\ndf = df_sorted.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nprint(df.shape)\ndf.head(3)","fdcbbe32":"df = df[df['HelpfulnessNumerator'] <= df['HelpfulnessDenominator']]\ndf.shape","05ebe602":"#checking how much data still remains\n\nprint(f'{round((df.shape[0]\/df_filtered.shape[0])*100,2)}%')","d1a83df0":"print(df['Score'].value_counts())\nvalues = df['Score'].value_counts().values\nsns.barplot(x=['Positive','Negative'],y=values)\nplt.show()","a5459ee4":"# replacing some phrases like won't with will not\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\n\n","68603594":"preprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(df['Text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    # removing html tags\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    # removing extra spaces and numbers\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    # removing non alphabels\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentance.strip())","1d28fd87":"#combining required columns\ndf['clean_text'] = preprocessed_reviews\ndf = df[['Time','clean_text','Score']]\n#reseting index\ndf = df.reset_index(drop=True)","994c91ed":"#sampling 100k points \ndf_10k = df.sample(50000)\n#sorting 100kpoints based on time\ndf_10k['Time'] = pd.to_datetime(df_10k['Time'],unit='s')\ndf_10k = df_10k.sort_values('Time')\n#reseting index\ndf_10k = df_10k.reset_index(drop=True)","fe84d324":"df_10k['Score'].value_counts()","769ee4d0":"#splitting data to train.cv and test\nfrom sklearn.model_selection import train_test_split\nx = df_10k['clean_text']\ny = df_10k['Score']\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.3,stratify=y)\nX_tr,X_cv,y_tr,y_cv = train_test_split(X_train,y_train,test_size=0.3,stratify=y_train)","163476c1":"bow = CountVectorizer()\nbow.fit(X_tr)\nX_train_bow = bow.transform(X_tr)\nX_cv_bow = bow.transform(X_cv)\nX_test_bow = bow.transform(X_test)\n\nprint('shape of X_train_bow is {}'.format(X_train_bow.get_shape()))\nprint('shape of X_cv_bow is {}'.format(X_cv_bow.get_shape()))\nprint('shape of X_test_bow is {}'.format(X_test_bow.get_shape()))","aa2472dc":"\nC = [0.001,0.01,0.1,1,10,100]\ntrain_auc = []\ncv_auc = []\n\nfor c in C:\n    model = LogisticRegression(penalty='l2',C=c,solver='liblinear')\n    model.fit(X_train_bow,y_tr)\n    y_tr_pred = model.predict(X_train_bow)\n    y_cv_pred = model.predict(X_cv_bow)\n    train_auc.append(roc_auc_score(y_tr,y_tr_pred))\n    cv_auc.append(roc_auc_score(y_cv,y_cv_pred))\nplt.grid(True)\nplt.plot(np.log(C),train_auc,label='Train AUC')\nplt.plot(np.log(C),cv_auc,label='CV AUC')\nplt.scatter(np.log(C),train_auc)\nplt.scatter(np.log(C),cv_auc)\nplt.legend()\nplt.xlabel(\"C: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","88e43557":"model = LogisticRegression(penalty='l2',C=0.1,solver='liblinear')\nmodel.fit(X_train_bow,y_tr)\ny_tr_pred = model.predict(X_train_bow)\ny_cv_pred = model.predict(X_cv_bow)\ntrain_fpr, train_tpr, thresholds = roc_curve(y_tr, model.predict_proba(X_train_bow)[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test_bow)[:,1])\n\nplt.grid(True)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC CURVE FOR OPTIMAL K\")\nplt.show()\n\n#Area under ROC curve\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))\n","7028905e":"feats = bow.get_feature_names()\ncoefs = model.coef_.reshape(-1,1)\ndff = pd.DataFrame(coefs,columns=['coef'],index=feats)\ntop_neg = dff.sort_values(ascending=True,by='coef').head(10)\ntop_pos = dff.sort_values(ascending=False,by='coef').head(10)\nprint('Top 10 Positive features')\nprint(top_pos)\nprint('-'*50)\nprint('Top 10 Negative features')\nprint(top_neg)","0d0d03e2":"W = model.coef_","3a22dd43":"#noise\nepsilon = 0.00005\n# adding noise X_ = X + epsilon\nX_ = X_train_bow.data + epsilon \nprint(X_.shape)\nX_train_bow_dash = csr_matrix((X_, X_train_bow.indices, X_train_bow.indptr), shape=X_train_bow.shape)\nprint(X_train_bow_dash.shape)","b8c8c923":"model = LogisticRegression(penalty='l2',C=0.1,solver='liblinear')\nmodel.fit(X_train_bow_dash,y_tr)\nW_ = model.coef_","982960b6":"\nepsilon2 = 0.000006\nW = W + epsilon2\nW_ = W_ + epsilon2","207e592b":"change = abs((W - W_)\/(W))\npercentage_change = change*100\npercentage_change = percentage_change[0]\n\n\n# Printing Percentiles :\nfor i in range(10, 101, 10):\n    print(\"{}th Percentile value : {}\".format(i, np.percentile(percentage_change, i)))\n    \nprint('--'*50)\n\nfor i in range(90, 101):\n    print(\"{}th Percentile value : {}\".format(i, np.percentile(percentage_change, i)))\n\nprint('--'*50)\n\nfor i in range(1, 11):\n    print(\"{}th Percentile value : {}\".format((i*1.0\/10 + 99), np.percentile(percentage_change, i*1.0\/10 + 99)))","d26bc697":"feats = bow.get_feature_names()\nchange_ = percentage_change.reshape(-1,1)\npertub_df = pd.DataFrame(change_,columns=['change'],index=feats)\npertub_df.reset_index(inplace=True)\npertub_df = pertub_df.rename(columns={'index':'features'})\nprint(pertub_df.shape)\n# pertub_df_sorted = pertub_df.sort_values(ascending=False,by=['change'])\npertub_df.tail(3)","170402bd":"#removing features with high change (> 99.9th percentile value)\npertub_df = pertub_df[pertub_df['change'] < 0.5838385159697708]\nprint(pertub_df.shape)\npertub_df.tail(3)","b8332120":"import gc\nidx = pertub_df.index.to_list()\n# our features get reduced from 43064 to 43020. We will pick only those columns\n\nX_train_d = X_train_bow.todense()[:,idx]\ngc.collect()\nX_test_d = X_test_bow.todense()[:,idx]\nX_cv_d = X_cv_bow.todense()[:,idx]\nprint(X_train_d.shape)\nprint(X_cv_d.shape)\nprint(X_test_d.shape)","d4b50e7c":"from scipy import sparse\nX_train_d = sparse.csr_matrix(X_train_d)\nX_cv_d = sparse.csr_matrix(X_cv_d)\nX_test_d = sparse.csr_matrix(X_test_d)","c14bae68":"#hyperparameter tuning\n\nC = [0.001,0.01,0.1,1,10,100]\ntrain_auc = []\ncv_auc = []\n\nfor c in C:\n    model = LogisticRegression(penalty='l2',C=c,solver='liblinear')\n    model.fit(X_train_d,y_tr)\n    y_tr_pred = model.predict(X_train_d)\n    y_cv_pred = model.predict(X_cv_d)\n    train_auc.append(roc_auc_score(y_tr,y_tr_pred))\n    cv_auc.append(roc_auc_score(y_cv,y_cv_pred))\nplt.grid(True)\nplt.plot(np.log(C),train_auc,label='Train AUC')\nplt.plot(np.log(C),cv_auc,label='CV AUC')\nplt.scatter(np.log(C),train_auc)\nplt.scatter(np.log(C),cv_auc)\nplt.legend()\nplt.xlabel(\"C: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","d524f7ef":"model = LogisticRegression(penalty='l2',C=1,solver='liblinear')\nmodel.fit(X_train_d,y_tr)\ny_tr_pred = model.predict(X_train_d)\ny_cv_pred = model.predict(X_cv_d)\ntrain_fpr, train_tpr, thresholds = roc_curve(y_tr, model.predict_proba(X_train_d)[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test_d)[:,1])\n\nplt.grid(True)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC CURVE FOR OPTIMAL K\")\nplt.show()\n\n#Area under ROC curve\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))\n","47a7af8f":"feats = pertub_df.features.to_list()\ncoefs = model.coef_.reshape(-1,1)\ndff = pd.DataFrame(coefs,columns=['coef'],index=feats)\ntop_neg = dff.sort_values(ascending=True,by='coef').head(10)\ntop_pos = dff.sort_values(ascending=False,by='coef').head(10)\nprint('Top 10 Positive features')\nprint(top_pos)\nprint('-'*50)\nprint('Top 10 Negative features')\nprint(top_neg)","bcbdf280":"## Train-test split","f3564e68":"We found that after 99.9th percentile there is significiant rise in weight difference value. It shows existance of multicollinearity. If we remove those weights it will be better","4bace0c3":"## Pertubation test","f261d55c":"### Top features","54e843cc":"## Exploratory Data Analysis","4d817b0f":"## Loading the data\n\nThe dataset is available in two forms\n1. .csv file\n2. SQLite Database\n\nIn order to load the data, We have used the SQLITE dataset as it is easier to query the data and visualise the data efficiently.\n<br> \n\nHere as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score is above 3, then the recommendation will be set to \"positive\". Otherwise, it will be set to \"negative\".","2edfae93":"In this kernel we will go through **Perturbation test** which is used to check multicollinearity Here we use Logistic regression model and in addition to this we will also see how we can get the top features that influence positive class and negative class.","5853ac8e":"step 1 : Get weights W after we fit our model with data X.","fd98dc81":"Step 4: Add a small eps value(to eliminate the divisible by zero error) to W and W_","98e48ea1":"### we will use bag of wors approach to vectorize text features","740ee2c4":"Step 5:   find the % change between W and W_ (| (W-W_) \/ (W) |)*100)","57ab50da":"It is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.  Following is an example:","d426b2a0":"<b>Observation:-<\/b> It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions<br>\n\n* Helpfulness Numerator: Number of users who found the review helpful <br>\n* Helpfulness Denominator: Number of users who indicated whether they found the review helpful or not","d8d85224":"Step 3: Fit the model again in data X_ and get new weights W_","72ba3f1f":"Step 2: Add noise to X and get new data X' ie X'= X + e ","8c194304":"Now we can fit a logistic regression model using these features and so there exists no(little) multicollinearity","807720fa":"Observation: We have clearly an imbalenced dataset\n","dca75dd1":"### Top features","05eda358":"Clearly we have an imbalanced dataset. So It's better to use metrics like f1 score, AUC as perfomance metric","51854f8a":"Now we will analyse target values","7c669b7b":"### Preprocessing Review Text\n\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews","c47dedc9":"We will make all reviews with score greater than 3 as 1(positive) and less than 3 as 0(negative)","fab27c6e":"For the ease of computation we will sample just 100k points","a5c17eb4":"Step6: Remove those features showing high change"}}