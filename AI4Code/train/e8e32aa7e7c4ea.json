{"cell_type":{"a916bcd3":"code","19623b2a":"code","781176b1":"code","1f182457":"code","593a6856":"code","b6598f90":"code","8e0b7778":"code","541940fe":"code","be8de446":"code","0f1d27c0":"code","5f016b72":"code","bf43a903":"code","0f5d4930":"code","b221839e":"code","af6b7acb":"code","e4ad57f3":"code","ed20d21d":"code","3037cdaf":"code","c4cc2936":"code","301db132":"code","1d1fac3a":"code","cbd0619f":"markdown","b9281174":"markdown","c076a807":"markdown","f7d524a8":"markdown","b217a596":"markdown","e70a88d6":"markdown","eb8d0779":"markdown","7c5edd97":"markdown"},"source":{"a916bcd3":"# Sync Notebook with VS Code\n!git clone https:\/\/github.com\/sarthak-314\/fast-nlp\nimport sys; sys.path.append('fast-nlp')\n\nfrom src import *\nfrom src.tflow import *","19623b2a":"%%hyperparameters \n\n## MODEL ARCHITECTURE ## \nbackbone: roberta-large\nattention_dropout: 0.10\nfrom_pytorch: False\n\nmax_seq_len: 128\nbatch_size: 256\n\nconcat_abs_difference: True\ninter_hidden_layers: [256, 64, 16, 4]\nhidden_dropout: 0.25\n\n# backbone_weights: ['oldval771_muppetL_ep12.h5', 'uncategorized\/runs\/xar80du8'] # LB: 0.844\nbackbone_weights: [\"oldval770_muppetL_ep7.h5\", \"uncategorized\/runs\/3c9wpx0l\"] # LB: 0.844\n\n    \n## MODEL COMPILE ## \noptimizer:\n    _target_: AdamW\n    weight_decay: 1e-4\n    epsilon: 1e-6\n    beta_1: 0.9\n    beta_2: 0.99\n    max_grad_norm: 1.0\n    use_swa: False\n    use_lookahead: False\n    average_decay: 0.999\n    dynamic_decay: True\n\nwarmup_epochs: 0.25\nwarmup_power: 2.0\nlr_cosine:\n    max_lr: 4e-5\n    min_lr: 0\n    lr_gamma: 0.75\n    num_cycles: 2\n    step_gamma: 2\n\n# Loss Function # \nany_in_old: 2\nboth_in_old: 4\n\nmanual_label_smooth: 0.05\npseudo_label_smooth: 0.10\npseudo_weight: 0.25\n\nrmse_weights: {\n    'pseudo_lb833': 0.1, # TODO: Add more\n}\n\n## Model Training ## \nmax_epochs: 100\ncheckpoints_per_epoch: 4\n\n## Data Factory ## \nfold: 3\nadd_tags: True\nrandom_state: 69420\n\ntrain_repeat: 8\npseudo_examples: 10\nmin_rank_difference: 5000","781176b1":"!wandb login '00dfbb85e215726cccc6f6f9ed40e618a7cf6539'\nSTRATEGY = tf_accelerator(bfloat16=False, jit_compile=False)\nset_seed(HP.random_state)\nrandom.seed(HP.random_state)\n\nwith STRATEGY.scope(): \n    backbone = TFAutoModel.from_pretrained(\n        HP.backbone, \n        attention_probs_dropout_prob=HP.attention_dropout, \n        # from_pt=HP.from_pytorch,\n    )","1f182457":"def build_scorer(backbone): \n    input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True,\n    )\n    x = backbone_outputs.pooler_output\n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid') \n    model = tf.keras.Model([input_ids, attention_mask],  outputs=score_layer(x))\n    return model\n\nwith STRATEGY.scope(): \n    weights, run = HP.backbone_weights\n    wandb.restore(weights, run)\n    scorer = build_scorer(backbone)\n    scorer.load_weights(weights)","593a6856":"TOXIC_FEATURES = [\n    'severe_toxic', 'identity_hate', 'threat', \n    'toxic', 'insult', 'obscene', \n]\nTAGS = [\n    'Severe Toxicity', 'Identity Hate', 'Threat',\n    'Toxic', 'Insult', 'Obscene', \n]\nFEATURE_TO_TAG = {feat: tag for feat, tag in zip(TOXIC_FEATURES, TAGS)}\n\ntokenizer = AutoTokenizer.from_pretrained(\n    HP.backbone, additional_special_tokens=['[PSEUDO_LABELLED]'], \n)","b6598f90":"def add_feature_values(df, old): \n    old_dict = old.set_index('comment_text').to_dict()\n    for feat in TOXIC_FEATURES: \n        df[f'MT_{feat}'] = df.more_toxic.map(old_dict[feat])\n        df[f'LT_{feat}'] = df.less_toxic.map(old_dict[feat])\n    \n    df['MT_annotated'] = df.MT_toxic.apply(lambda label: np.isnan(label))\n    df['LT_annotated'] = df.LT_toxic.apply(lambda label: np.isnan(label))\n    df = df.fillna(1)\n    return df\n\ndef build_test_hard_pos(trials=HP.pseudo_examples, min_distance=HP.min_rank_difference):\n    test = pd.read_csv('..\/input\/toxic-dataframes\/test_comments.csv')\n    \n    id_to_score = pd.read_csv('..\/input\/toxic-pseudo-labels\/test_linear_lb869.csv').set_index('comment_id').to_dict()['score']\n    id_to_score2 = pd.read_csv('..\/input\/toxic-pseudo-labels\/perspective_api_lb771.csv').set_index('comment_id').to_dict()['score']\n    \n    test['score'] = test.comment_id.map(id_to_score)\n    dd = {'more_toxic': [], 'less_toxic': [], 'weights': []}\n    \n    test_comments = test.comment_text.values\n    test_scores = test.score.values\n    test_papi_scores = test.comment_id.map(id_to_score2).values\n    for _ in tqdm(range(trials)):\n        mt_i, lt_i = random.randint(0, len(test)-1), random.randint(0, len(test)-1)\n        if test_scores[mt_i] - test_scores[lt_i] < min_distance: \n            continue\n        if test_papi_scores[mt_i] < test_papi_scores[lt_i]: \n            continue\n        dd['more_toxic'].append(test_comments[mt_i])\n        dd['less_toxic'].append(test_comments[lt_i])\n        \n        wt = (test_scores[mt_i]-test_scores[lt_i]) \/ 5000 * HP.pseudo_weight\n        dd['weights'].append(wt)\n    \n    df = pd.DataFrame(dd)\n    print('Num examples from test:', len(df))\n    df['type'] = 'auto'\n    df['label_smooth'] = HP.pseudo_label_smooth\n    return df","8e0b7778":"%%time\ndf = pd.read_csv('\/kaggle\/input\/toxic-dataframes\/valid.csv')\nmanual = pd.read_csv('..\/input\/toxic-dataframes\/manual_pair_labels (1).csv')\ndfc = pd.read_csv('\/kaggle\/input\/toxic-dataframes\/comments.csv')\ntest = pd.read_csv('\/kaggle\/input\/toxic-dataframes\/test_comments.csv')\nold = pd.read_csv('..\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\n# manual_manual = manual[manual['type']!='auto'] # 709 examples\n\ndf['type'] = 'manual'\nmanual['fold'] = -1\ndf = pd.concat([df, manual])\ndf['weights'] = 1.0\ndf['label_smooth'] = 0.0\n\ndef normalize(preds): \n    return (preds-preds.min()) \/ (preds.max()-preds.min())\n \ntrain, valid = df[df.fold!=HP.fold], df[df.fold==HP.fold]\ntrain['weights'] = [\n    HP.any_in_old if x else 1\n    for x in train.more_toxic.isin(old.comment_text) | train.less_toxic.isin(old.comment_text)\n]\ntrain['weights'] = [\n    HP.both_in_old if x else train.weights.iloc[i]\n    for i, x in enumerate(train.more_toxic.isin(old.comment_text) & train.less_toxic.isin(old.comment_text))\n]\ntrain['label_smooth'] = HP.manual_label_smooth\ntrain_focus = train.copy()\n\ntest_hard = build_test_hard_pos()\ntrain = pd.concat([test_hard]+[train]*HP.train_repeat).sample(frac=1.)\n\ntrain_freeze = pd.concat([build_test_hard_pos()]+[train_focus]*HP.train_repeat).sample(frac=1.)\ntrain","541940fe":"%%time\ndef tokenize_text(text): \n    return tokenizer(\n        text, \n        max_length=HP.max_seq_len, \n        padding='max_length', \n        truncation=True, \n    )\n\ndef convert_to_features(example_batch): \n    M_tokenized = tokenize_text(example_batch['more_toxic'])\n    L_tokenized = tokenize_text(example_batch['less_toxic'])\n    return {\n        'MT_ids': M_tokenized['input_ids'], \n        'MT_mask': M_tokenized['attention_mask'], \n        'LT_ids': L_tokenized['input_ids'], \n        'LT_mask': L_tokenized['attention_mask'], \n    }\n\ndef dataset_to_tfds(dataset):\n    dataset.set_format(type='numpy')\n    \n    model_inputs = {\n        'MT_ids': dataset['MT_ids'].astype(np.int32), \n        'MT_mask': dataset['MT_mask'].astype(np.int32), \n        'LT_ids': dataset['LT_ids'].astype(np.int32), \n        'LT_mask': dataset['LT_mask'].astype(np.int32), \n        'weight': dataset['weights'].astype(np.float32),\n        'label_smooth': dataset['label_smooth'].astype(np.float32), \n    }\n    ds = tf.data.Dataset.from_tensor_slices(model_inputs)\n    return ds\n\ndef df_to_tfds(df, is_valid=False, batch_size=HP.batch_size): \n    raw_dataset = datasets.Dataset.from_pandas(df[['less_toxic', 'more_toxic', 'weights', 'label_smooth']])\n    processed_dataset = raw_dataset.map(\n        convert_to_features, \n        batched=True, \n        batch_size=8192, \n        num_proc=4, \n        desc='Running tokenizer on the dataset', \n    )\n    ds = dataset_to_tfds(processed_dataset)\n    if not is_valid:\n        ds = ds.shuffle(len(processed_dataset), reshuffle_each_iteration=True).repeat()\n    ds = ds.batch(batch_size)\n    if is_valid: \n        ds = ds.cache()\n    steps = len(processed_dataset)\/\/batch_size + 1\n    return ds.prefetch(tf.data.AUTOTUNE), steps\n\ntrain_ds, train_steps = df_to_tfds(train, is_valid=False)\nvalid_ds, valid_steps = df_to_tfds(valid, is_valid=True)\ntrain_freeze_ds, train_freeze_steps = df_to_tfds(train_freeze, is_valid=False, batch_size=4096)\ntrain_focus_ds, train_focus_steps = df_to_tfds(train_focus, is_valid=False)\n\ndel train, valid, df, manual, dfc, test, old, train_freeze, test_hard\ngc.collect()","be8de446":"class ToxicModel(tf.keras.Model):     \n    @tf.function\n    def train_step(self, data): \n        with tf.GradientTape() as tape:\n            batch_size = tf.shape(data['weight'])[0]\n            A_pred = self((data['MT_ids'], data['MT_mask'], data['LT_ids'], data['LT_mask']), training=True)\n            B_pred = self((data['LT_ids'], data['LT_mask'], data['MT_ids'], data['MT_mask']), training=True)\n            \n            loss = self.compiled_loss(\n                tf.ones(batch_size)-data['label_smooth'], A_pred, regularization_losses=self.losses\n            ) + self.compiled_loss(\n                tf.zeros(batch_size)+data['label_smooth'], B_pred, regularization_losses=self.losses\n            )\n            loss = data['weight'] * loss\n\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.compiled_metrics.update_state(tf.ones(batch_size), A_pred)\n        return {m.name: m.result() for m in self.metrics}\n    \n    @tf.function\n    def test_step(self, data):\n        batch_size = tf.shape(data['weight'])[0]\n        A_pred = self((data['MT_ids'], data['MT_mask'], data['LT_ids'], data['LT_mask']), training=False)\n        B_pred = self((data['LT_ids'], data['LT_mask'], data['MT_ids'], data['MT_mask']), training=False)\n        self.compiled_loss(tf.ones(batch_size), A_pred, regularization_losses=self.losses) \n        self.compiled_metrics.update_state(tf.ones(batch_size), A_pred)\n        return {m.name: m.result() for m in self.metrics}\n    \ndef bert_initalizer(initializer_range = 0.02):\n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\n\ndef build_hidden_layer(hidden_layer_units, hidden_dropout, name='hidden_layer'): \n    layers = []\n    for units in hidden_layer_units: \n        layers.append(tf.keras.layers.Dropout(hidden_dropout))\n        layers.append(tf.keras.layers.Dense(\n            units, \n            activation=tfa.activations.mish, \n            kernel_initializer=bert_initalizer(backbone.config.initializer_range)\n        ))\n    return tf.keras.Sequential(layers, name=name)\n    \n    \ndef build_model(backbone): \n    A_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    A_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    B_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    B_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    \n    A_x = backbone(input_ids=A_ids, attention_mask=A_mask).pooler_output\n    B_x = backbone(input_ids=B_ids, attention_mask=B_mask).pooler_output\n    if HP.concat_abs_difference: \n        abs_diff = tf.math.abs(A_x-B_x)\n        x = tf.concat([A_x, B_x, abs_diff], axis=-1)\n    else: \n        x = tf.concat([A_x, B_x], axis=-1)\n    \n    I = build_hidden_layer(HP.inter_hidden_layers, HP.hidden_dropout, name='inter_hidden_layer')\n    x = I(x)\n    \n    score_layer = tf.keras.layers.Dense(\n        1, \n        kernel_initializer=bert_initalizer(backbone.config.initializer_range), \n        activation='sigmoid', \n        name='score', \n    )\n    x = score_layer(x)\n \n    model = ToxicModel([A_ids, A_mask, B_ids, B_mask],  outputs=x)\n    model.trainable = True\n    return model\n\nwith STRATEGY.scope(): \n    model = build_model(backbone)","0f1d27c0":"from src.tflow.factory import lr_scheduler_factory, plot_first_epoch\nHP.steps_per_execution = 1024\n# HP.steps_per_execution = None\n\nHP.optimizer.betas = [HP.optimizer.beta_1, HP.optimizer.beta_2]\n\ndef loss_fn(y_true, y_pred): \n    y_true, y_pred = tf.cast(y_true, tf.float32), tf.cast(y_pred, tf.float32)\n    return tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM)(y_true, y_pred)\n\ndef model_compile(): \n    lr_scheduler = lr_scheduler_factory(HP.warmup_epochs, HP.warmup_power, HP.lr_cosine, train_steps+4)\n    optimizer = optimizer_factory(HP.optimizer, lr_scheduler)\n    \n    model.compile(\n        optimizer=optimizer, \n        loss=loss_fn, \n        metrics=[\"accuracy\"], \n        steps_per_execution=HP.steps_per_execution, \n        run_eagerly=HARDWARE == 'CPU', \n    )    \n\ndef quick_compile(lr=1e-3): \n    optimizer = optimizer_factory(HP.optimizer, lr)\n    model.compile(\n        optimizer=optimizer, \n        loss=loss_fn, \n        steps_per_execution=HP.steps_per_execution, \n        metrics=['accuracy'], \n    )    ","5f016b72":"HP.hidden_dropout = 0.25\nwith STRATEGY.scope(): \n    model = build_model(backbone)\n    backbone.trainable = False\n    quick_compile(1e-3)\n\naccuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint('freeze_checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps, epochs=100, \n    validation_data=valid_ds, validation_steps=valid_steps-1, \n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.50, patience=1, verbose=1, mode='max'), \n        accuracy_checkpoint, \n    ],\n)\ndel train_freeze_ds\n\nmodel.evaluate(train_focus_ds, steps=100, verbose=1)\nmodel.evaluate(valid_ds, verbose=1)","bf43a903":"with STRATEGY.scope(): \n    model.load_weights('freeze_checkpoint_acc.h5')\nmodel.evaluate(train_focus_ds, steps=100, verbose=1)\nmodel.evaluate(valid_ds, verbose=1)","0f5d4930":"Qqq","b221839e":"HP.hidden_dropout = 0.25\nHP.lr_cosine.decay_epochs = 1.0\nEPOCHS, CPE = 100, 4\n\nwith STRATEGY.scope(): \n    model = build_model(backbone)\n    backbone.trainable = True\n    model_compile()\n    model.load_weights('freeze_checkpoint_acc.h5')\n\naccuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\nloss_checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint_loss.h5', monitor='val_loss', mode='min', save_weights_only=True, save_best_only=True, verbose=1)\nhistory = model.fit(\n   train_focus_ds, steps_per_epoch=train_focus_steps\/\/CPE+1, epochs=EPOCHS*CPE, \n   validation_data=valid_ds, validation_steps=valid_steps-1, \n   callbacks=[accuracy_checkpoint, loss_checkpoint], \n)","af6b7acb":"# HP.hidden_dropout = 0.25\n# with STRATEGY.scope():\n#     model.load_weights('checkpoint_loss.h5')\n\n# accuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\n# loss_checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint_loss.h5', monitor='val_loss', mode='min', save_weights_only=True, save_best_only=True, verbose=1)\n# history = model.fit(\n#    train_focus_ds, steps_per_epoch=train_focus_steps, epochs=100, \n#    validation_data=valid_ds, validation_steps=valid_steps-1, \n#    callbacks=[accuracy_checkpoint, loss_checkpoint], \n# )","e4ad57f3":"# with STRATEGY.scope(): \n#     model.load_weights('checkpoint_loss.h5')\n#     model.evaluate(train_focus_ds, steps=100, verbose=1)\n#     model.evaluate(valid_ds, verbose=1)\ndel train_ds, train_focus_ds, valid_ds\ngc.collect()","ed20d21d":"# 6-7 loss. ","3037cdaf":"def build_interaction_model(model):\n    # x_dim = HP.repr_hidden_layers[-1]\n    x_dim = 1024\n    A_x = tf.keras.Input((x_dim,), dtype=tf.float32)\n    B_x = tf.keras.Input((x_dim,), dtype=tf.float32)\n    \n    if HP.concat_abs_difference: \n        abs_diff = tf.math.abs(A_x-B_x)\n        x = tf.concat([A_x, B_x, abs_diff], axis=-1)\n    else: \n        x = tf.concat([A_x, B_x], axis=-1)\n    inter_hidden_layer = model.get_layer('inter_hidden_layer')\n    x = inter_hidden_layer(x)\n    x = model.get_layer('score')(x)\n    return tf.keras.Model([A_x, B_x], outputs=x)\n\n\nwith STRATEGY.scope(): \n    model.load_weights('freeze_checkpoint_acc.h5') ######\u2116\n    inter_model = build_interaction_model(model)\n    inter_model.save_weights('inter_model.h5')\n    backbone.save_weights('backbone.h5')\n    \n    #wandb.init('Temp')\n    #wandb.save('backbone.h5')\n    #wandb.save('inter_model.h5')","c4cc2936":"# class DataGen(tf.keras.utils.Sequence):\n#     def __init__(self, test): \n#         self.test = test\n        \n#     def __len__(self):\n#         return len(self.test)**2\n\n#     def __getitem__(self, index):\n#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n#         X, y = self.__data_generation(list_IDs_temp)\n#         return X, y\n\n#     def on_epoch_end(self):\n#         'Updates indexes after each epoch'\n#         self.indexes = np.arange(len(self.list_IDs))\n#         if self.shuffle == True:\n#             np.random.shuffle(self.indexes)\n\n#     def __data_generation(self, list_IDs_temp):\n#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n#         # Initialization\n#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n#         y = np.empty((self.batch_size), dtype=int)\n\n#         # Generate data\n#         for i, ID in enumerate(list_IDs_temp):\n#             # Store sample\n#             X[i,] = np.load('data\/' + ID + '.npy')\n\n#             # Store class\n#             y[i] = self.labels[ID]\n\n#         return X, keras.utils.to_categorical(y, num_classes=self.n_classes)","301db132":"\ntest_comments = test.comment_text.values\ndd = {'comment_text': [], 'score': []}\nfor i in tqdm(range(len(test))): \n    \n    input_ids, attention_masks = [], []\n    for j in range(len(test)): \n        A_comment = test_comments[i]\n        B_comment = test_comments[j]\n        tokenized = tokenize_text(\n            A_comment, B_comment \n        )\n        input_ids.append(tokenized['input_ids'])\n        attention_masks.append(tokenized['attention_mask'])\n    \n    id_ds = tf.data.Dataset.from_tensor_slices(np.array(input_ids).astype(np.int32))\n    mask_ds = tf.data.Dataset.from_tensor_slices(np.array(attention_mask).astype(np.int32))\n    ds = tf.data.Dataset.zip((id_ds, mask_ds))\n    ds = ds.batch(1024).prefetch(tf.data.AUTOTUNE)\n    \n    preds = model.predict(ds, verbose=1)\n    preds = np.squeeze(np.array(preds))\n    score = preds.mean(preds)\n    \n    dd['comment_text'].append(test_comments[i])\n    dd['score'].append(scpre)\n    \ndf = pd.DataFrame(dd)\n","1d1fac3a":"# test = pd.read_csv('..\/input\/toxic-dataframes\/test_comments.csv')\n\n# i = 0\n# j = 0\n# pairs = []\n# def generator(): \n#     tokenized = tokenizer(\n#         test.iloc[i].comment_text, \n#         test.iloc[j].comment_text, \n#         max_length=HP.max_seq_len, \n#         padding='max_length', \n#         truncation=True,     \n#         return_tensors='tf', \n#     )\n#     yield tokenized['input_ids'], tokenized['attention_mask']\n\n# test_ds = tf.data.Dataset.from_generator(\n#     generator, \n#     output_signature=(\n#          tf.TensorSpec(shape=(HP.max_seq_len,), dtype=tf.int32),\n#          tf.TensorSpec(shape=(HP.max_seq_len,), dtype=tf.int32)\n#     )\n# )\n    \n# test_ds = tf.data.Dataset.zip((test_ds, test_ds))\n# test_ds = test_ds.batch(1024).prefetch(tf.data.AUTOTUNE)\n\n# preds = model.predict(test_ds, verbose=1, steps=10)","cbd0619f":"# \ud83e\udde0 Model Factory \ud83e\udde0\n---\n#### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#training'> \u26a1 Training <\/a>\n\n<a name='model-factory'\/>","b9281174":"## Inference","c076a807":"# \u2692 Data Factory \u2692\n\n#### <a href='#dataframes'> Dataframes <\/a> | <a href='#huggingface-data-module'> Huggingface Data Module <\/a> | <a href='#tensorflow-data-module'> Tensorflow Data Module <\/a>\n\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#training'> \u26a1 Training <\/a>\n\n\n\n\n<a name='data-factory'>\n   ","f7d524a8":"# \u2699\ufe0f Hyperparameters \u2699\ufe0f\n---\n### <a href='#training'> \u26a1 Training <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a>\n\n<a name='hyperparameters'>","b217a596":"## \ud83e\udd17 Huggingface Data Module\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a>\n\n<a name='huggingface-data-module'\/>","e70a88d6":"## Model Training \n---\n#### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a>\n\n<a name='training'>","eb8d0779":"# __Toxic Valid Cross Encoder with Aux Outputs__\n\nTrain a Cross Encoder on a Mix of Pairwise BCE Loss and Human Annotations\n\n---\n### <a href='#hyperparameters'> \u2699\ufe0f Hyperparameters <\/a> | <a href='#training'> \u26a1 Training <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a>\n\n","7c5edd97":"## Build Dataframes\n---\n##### <a href='#hyperparameters'> \u2699 Hyperparameters <\/a> | <a href='#model-factory'> \ud83e\udde0 Model Factory <\/a> | <a href='#data-factory'> \u2692 Data Factory <\/a>\n\n<a name='huggingface-data-module'\/>"}}