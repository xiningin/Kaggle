{"cell_type":{"8705d3ac":"code","f6799d95":"code","d8fe9aea":"code","bbcdfdac":"code","062e5b56":"code","c908ea22":"code","7409a3bf":"code","05a08e16":"code","7ee250f3":"code","f5235215":"code","d7cad73c":"code","fad0215e":"code","3fa0d278":"code","94ec509d":"code","66f6fb19":"code","0b699c97":"code","21facb47":"code","6ec03f12":"code","da48d138":"code","ecca612d":"code","d6c27d21":"code","0a52fe1e":"code","7b00cf01":"code","088d441e":"code","5cf237dc":"code","521e0a45":"code","acb7bd8c":"code","7f232953":"code","6e66b0f6":"code","91a64358":"code","94c7f656":"code","813632e2":"code","42de0cf9":"code","2df685c5":"code","0a4713fe":"code","f86dcfc1":"code","8eba5e1d":"code","61ea89c3":"code","b50f73d4":"code","ae03fb3a":"code","9faeb6d9":"code","78b9fd7e":"code","9dcba5c3":"code","84887468":"code","f733c6dd":"code","5e2d3fc3":"code","201176c7":"code","429ac2e6":"code","d9ec13cf":"code","a24dd79b":"code","c029b679":"code","425a244c":"code","45f6e900":"code","66fe7e34":"code","804a5487":"code","a8a55d30":"code","f14b97bf":"code","97458ee0":"markdown","b6cca6db":"markdown","42796d02":"markdown","966a9501":"markdown","f4752603":"markdown","02cf0084":"markdown","27ecf8d6":"markdown","8f9fdc38":"markdown","1ed71b49":"markdown","87e72d90":"markdown","69928647":"markdown","8e3e4c4b":"markdown","4f4ec9f3":"markdown","f856a023":"markdown","45d9ecb4":"markdown","d4b090bb":"markdown","e4f9a9fd":"markdown","365d202c":"markdown","064b29ad":"markdown","ee522def":"markdown","b7367783":"markdown","0c1c112b":"markdown","6e369c55":"markdown","651a16bc":"markdown","57b18667":"markdown","f442be48":"markdown","fbc42bb8":"markdown","366300d8":"markdown","8e5f4f60":"markdown","4bc35329":"markdown","20370c2a":"markdown","d10044dd":"markdown","67c87093":"markdown","541ba8e0":"markdown","910dd584":"markdown","d66cdd92":"markdown","6c731215":"markdown","b4cf7b79":"markdown","a92c5b1c":"markdown","a32d7e24":"markdown","c5c0a14d":"markdown","829d3aa3":"markdown","3e6d44a3":"markdown","bba9bb29":"markdown","89db0829":"markdown","80343b56":"markdown","98d7aa1a":"markdown","253ff652":"markdown","2468213c":"markdown","67fe5f8e":"markdown","13dc352a":"markdown","f32715d1":"markdown","49cce881":"markdown","a540f69e":"markdown","af3a80cb":"markdown","15eab29f":"markdown","1fc19717":"markdown","df09ef09":"markdown","a91242d8":"markdown","cb8e3d49":"markdown","42fdcbd4":"markdown","99e36792":"markdown","3335daad":"markdown"},"source":{"8705d3ac":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.dummy import DummyRegressor\n\nplt.style.use('seaborn')","f6799d95":"df = pd.read_csv(\"..\/input\/vehicle-dataset-from-cardekho\/car data.csv\")\ndf.rename(columns = {'Owner':'Past_Owners'},inplace = True)\ndf","d8fe9aea":"df.info()","bbcdfdac":"df.isnull().any()","062e5b56":"df.describe(include='all')","c908ea22":"atttibutes_hist = df[[\"Kms_Driven\", \"Present_Price\", \"Selling_Price\", \"Year\"]].hist(bins=50, figsize=(20,15))\natttibutes_hist","7409a3bf":"fig, ax = plt.subplots(2,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4)) = ax\n\nlabels = df['Fuel_Type'].value_counts().index.tolist()\nvalues = df['Fuel_Type'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2, 0.2])\nax1.set_title(\"Fuel Type:\", fontdict={'fontsize': 14})\n\nlabels = df['Seller_Type'].value_counts().index.tolist()[:2]\nvalues = df['Seller_Type'].value_counts().tolist()[:2]\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax2.set_title(\"Seller Type:\", fontdict={'fontsize': 14})\n\nlabels = df['Transmission'].value_counts().index.tolist()[:2]\nvalues = df['Transmission'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax3.set_title(\"Transmission:\", fontdict={'fontsize': 14})\n\nlabels = df['Past_Owners'].value_counts().index.tolist()\nvalues = df['Past_Owners'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2, 0.2])\nax4.set_title(\"number of Past Owners:\", fontdict={'fontsize': 20})\n","05a08e16":"print(df['Fuel_Type'].value_counts())","7ee250f3":"df = df[df['Fuel_Type'] != \"CNG\"]\nprint(df['Fuel_Type'].value_counts())","f5235215":"df_copy = df.copy() # save for later use\ndf = pd.get_dummies(df, columns=['Fuel_Type', 'Seller_Type', 'Transmission'])\ndf","d7cad73c":"df['Car_Age']= 2019-df['Year'] # the dataset is from 2019","fad0215e":"df.drop(columns=['Car_Name'], inplace=True)\ndf.drop(columns=['Year'], inplace=True)","3fa0d278":"df.head(3)","94ec509d":"pd.DataFrame(data={'features': df.columns})","66f6fb19":"cmap = sns.diverging_palette(30, 230, 90, 20, as_cmap=True)\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(df.corr(),annot=True, cmap=cmap)\nsns.set(font_scale=1)","0b699c97":"corr_matrix = df.corr()\ncorralations = corr_matrix['Selling_Price'].sort_values(ascending = False) \nhigh_corr = (corralations > 0.2)|(corralations < -0.2)\npd.DataFrame(corralations[high_corr])\ncorralations[high_corr].index","21facb47":"print(\"heatmap of the high correlations with Selling Price:\")\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(df[corralations[high_corr].index].corr(),annot=True, cmap=cmap)\nsns.set(font_scale=1)","6ec03f12":"fig, axes = plt.subplots(2, 2, figsize=(15, 15))\nfig.suptitle('categorial features:')\n\nsns.swarmplot(ax=axes[0,0], x=\"Fuel_Type\", y=\"Selling_Price\", data=df_copy)\nsns.swarmplot(ax=axes[0,1], x=\"Seller_Type\", y=\"Selling_Price\", data=df_copy)\nsns.swarmplot(ax=axes[1,0], x=\"Transmission\", y=\"Selling_Price\", data=df_copy)\nsns.swarmplot(ax=axes[1,1], x=\"Past_Owners\", y=\"Selling_Price\", data=df_copy)","da48d138":"sns.pairplot(df[['Selling_Price', 'Present_Price', 'Kms_Driven', 'Car_Age']], kind='reg')","ecca612d":"print('This bar plot represents an estimate of central tendency for a Selling-Price with the height of each rectangle and provides some indication of the uncertainty around that estimate price using error bars.')\nfig = plt.figure(figsize=(10,5))\nsns.barplot('Car_Age','Selling_Price',data=df).set_title('Selling Price range by Car Age')","d6c27d21":"X = df.drop(columns=['Selling_Price'])\nY = df['Selling_Price']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 42)","0a52fe1e":"scaler_standard = StandardScaler()\nscaler_MinMax = MinMaxScaler()\n\nX_train_standardized = X_train.copy()\nX_test_standardized = X_test.copy()\nX_train_normalized = X_train.copy()\nX_test_normalized = X_test.copy()\n\nnumerical_features = ['Present_Price', 'Kms_Driven', 'Past_Owners', 'Car_Age']\n\n# Standardization:\nscaler_standard.fit(X_train[numerical_features])\nX_train_standardized[numerical_features] = scaler_standard.transform(X_train_standardized[numerical_features])\n\n# the scaling is with the the same fitted scaler (by the train data)\nX_test_standardized[numerical_features] = scaler_standard.transform(X_test_standardized[numerical_features])\n\n# Normalization:\nscaler_MinMax.fit(X_train[numerical_features])\nX_train_normalized[numerical_features] = scaler_MinMax.transform(X_train_normalized[numerical_features])\n\n# the scaling is with the the same fitted scaler (by the train data)\nX_test_normalized[numerical_features] = scaler_MinMax.transform(X_test_normalized[numerical_features])\n","7b00cf01":"print('note: the mean is 0 and std is 1')\nX_train_standardized.describe()[numerical_features].iloc[[1, 2]]","088d441e":"print('note: the min is 0 and max is 1')\nindexes = [False, False, False, True, False, False, False, True]\nX_train_normalized.describe()[numerical_features].iloc[indexes]","5cf237dc":"dummy_regr = DummyRegressor(strategy=\"mean\")\ndummy_regr.fit(X_train, y_train)\nR2_score = dummy_regr.score(X_test, y_test)\ny_predict = dummy_regr.predict(X_test)\nmae = MAE(y_test, y_predict)\nprint('The dummy model have a R2 score of ' + str(R2_score)[:6] + \" as expected (around 0), and mean absolute error of \" + str(mae)[:4])","521e0a45":"LR = LinearRegression()","acb7bd8c":"kf = KFold(n_splits=10, random_state=42, shuffle=True)\n\nR2_scores_standardized = cross_val_score(LR, X_train_standardized, y_train, cv=kf)\ny_predict_standardized = cross_val_predict(LR, X_train_standardized, y_train, cv=kf)\nmae_standarsized = MAE(y_train, y_predict_standardized)\n\nR2_scores_normalized = cross_val_score(LR, X_train_normalized, y_train, cv=kf)\ny_predict_normalized = cross_val_predict(LR, X_train_normalized, y_train, cv=kf)\nmae_normalized = MAE(y_train, y_predict_normalized)","7f232953":"fig, axes = plt.subplots(1,2)\n((ax1, ax2)) = axes\n\ny_predicted = cross_val_predict(LR, X_train_standardized, y_train, cv=kf)\nax1.scatter(y_train, y_predicted, alpha=0.3, color='orange')\nax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\nax1.set_xlabel('Actual')\nax1.set_ylabel('Predicted')\nax1.set_title('standardized:')\n\ny_predicted = cross_val_predict(LR, X_train_normalized, y_train, cv=kf)\nax2.scatter(y_train, y_predicted, alpha=0.3, color='red')\nax2.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\nax2.set_xlabel('Actual')\nax2.set_ylabel('Predicted')\nax2.set_title('normalized:')\n\nplt.show()","6e66b0f6":"print(\"the scores of cross validation are:\")\nprint(R2_scores_standardized)\nprint()\nprint(\"mean R2 is: \" + str(R2_scores_standardized.mean())[:5] + \" with std of  \" + str(R2_scores_standardized.std())[:5] + \" and MAE of \" + str(mae_standarsized)[:6])","91a64358":"print(\"the scores of cross validation are:\")\nprint(R2_scores_normalized)\nprint()\nprint(\"mean R2 is: \" + str(R2_scores_normalized.mean())[:5] + \" with std of  \" + str(R2_scores_normalized.std())[:5] + \" and MAE of \" + str(mae_normalized)[:6])","94c7f656":"X_train_normalized","813632e2":"X_train_standardized","42de0cf9":"# create 3 more features:\ndf['KMs_Per_year'] = df['Kms_Driven']\/df['Car_Age']\ndf['Present_Price_Age_ratio'] = df['Present_Price']\/df['Car_Age']\ndf['Present_Price_KMs_ratio'] = df['Present_Price']\/df['Kms_Driven']\ndf.describe()[['KMs_Per_year', 'Present_Price_Age_ratio', 'Present_Price_KMs_ratio']]","2df685c5":"corr_matrix = df.corr()\ncorr_matrix['Selling_Price'].sort_values(ascending=False)","0a4713fe":"sns.pairplot(df[['Selling_Price', 'Present_Price_Age_ratio', 'Present_Price_KMs_ratio', 'KMs_Per_year']], kind='reg')","f86dcfc1":"# test train split\nX = df.drop(columns=['Selling_Price'])\nY = df['Selling_Price']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 42)\n\n# scaling the numerical features\nscaler_standard = StandardScaler()\nscaler_MinMax = MinMaxScaler()\n\nX_train_standardized = X_train.copy()\nX_test_standardized = X_test.copy()\nX_train_normalized = X_train.copy()\nX_test_normalized = X_test.copy()\n\nnumerical_features = ['Present_Price', 'Kms_Driven', 'Past_Owners', 'Car_Age', 'KMs_Per_year', 'Present_Price_Age_ratio', 'Present_Price_KMs_ratio']\n\n# Standardization:\nscaler_standard.fit(X_train[numerical_features])\nX_train_standardized[numerical_features] = scaler_standard.transform(X_train_standardized[numerical_features])\n\n# the scaling is with the the same fitted scaler (by the train data)\nX_test_standardized[numerical_features] = scaler_standard.transform(X_test_standardized[numerical_features])\n\n# Normalization:\nscaler_MinMax.fit(X_train[numerical_features])\nX_train_normalized[numerical_features] = scaler_MinMax.transform(X_train_normalized[numerical_features])\n\n# the scaling is with the the same fitted scaler (by the train data)\nX_test_normalized[numerical_features] = scaler_MinMax.transform(X_test_normalized[numerical_features])\n","8eba5e1d":"kf = KFold(n_splits=10, random_state=42, shuffle=True)\n\nR2_standardized = cross_val_score(LR, X_train_standardized, y_train, cv=kf)\ny_predict_standardized = cross_val_predict(LR, X_train_standardized, y_train, cv=kf)\nmae_standarsized = MAE(y_train, y_predict_standardized)\n\nprint('standartize:')\nprint('R2 score: ' + str(R2_standardized.mean())[:6])\nprint('R2 std: ' + str(R2_standardized.std())[:6])\nprint('MAE: ' + str(mae_standarsized)[:6])\n\nprint()\n\nR2_normalized = cross_val_score(LR, X_train_normalized, y_train, cv=kf)\ny_predict_normalized = cross_val_predict(LR, X_train_standardized, y_train, cv=kf)\nmae_normalized = MAE(y_train, y_predict_normalized)\n\nprint('normalize: ')\nprint('R2 score: ' + str(R2_normalized.mean())[:6])\nprint('R2 std: ' + str(R2_normalized.std())[:6])\nprint('MAE: ' + str(mae_normalized)[:6])","61ea89c3":"# I will use this function for make a copy of\n# train set by specific correlation limit.\n\n# copy X with columns which grater than specific limit:\ndef copy_by_corr_limit(X, lim, limits):\n    X_copy = X.copy()\n    s = (limits < lim)\n    X_copy = X_copy[X_copy.columns[~s]]\n    return X_copy","b50f73d4":"print('I will use the next list to select features by correlations')\nprint('correlations (without the sign+-):')\ncorrelations = abs(corr_matrix['Selling_Price']).sort_values(ascending=False)\ncorrelations.drop('Selling_Price', inplace=True)\ncorrelations","ae03fb3a":"corr_limits = [0, 0.03, 0.09, 0.25, 0.35, 0.40, 0.55, 0.552, 0.555, 0.9]\n\nmean_scores = []\nstd_scores = []\nmae_scores = []\n\nfor limit in corr_limits:\n    X_train_copy = copy_by_corr_limit(X_train_standardized, limit, correlations)\n    R2_scores = cross_val_score(LR, X_train_copy, y_train, cv=kf)\n    y_predict = cross_val_predict(LR, X_train_copy, y_train, cv=kf)\n    mae_score = MAE(y_train, y_predict)\n\n    \n    mean_scores.append(R2_scores.mean())\n    std_scores.append(R2_scores.std())\n    mae_scores.append(mae_score)\n    \npd.DataFrame(data={'lim correlation:':corr_limits, 'R2_score': mean_scores, 'R2_std': std_scores, 'MAE score': mae_scores}) ","9faeb6d9":"X_train_copy = copy_by_corr_limit(X_train_standardized, 0.00, correlations)\nX_test_copy = copy_by_corr_limit(X_test_standardized, 0.00, correlations)\n\nLR.fit(X_train_copy, y_train)\nR2_score = LR.score(X_test_copy, y_test)\ny_predict = LR.predict(X_test_copy)\nmae_score = MAE(y_test, y_predict)\n\nindexes = list(range(1, len(y_predict)+1))\nfig, axs = plt.subplots(1, 1, figsize=(9, 3), sharey=True)\naxs.plot(indexes, y_predict, label='target_predicted', color='orange')\naxs.plot(indexes, y_test, label='target_value', color='purple')\naxs.legend()\naxs.set_xlabel('targes indexes')\naxs.set_ylabel('Selling Price')\nfig.suptitle('Predicted values VS True Values:')\nplt.show()\n\npd.DataFrame(index=['test LR model'], data={'R2_score': R2_score, 'MAE score': mae_score})","78b9fd7e":"RFR = RandomForestRegressor()","9dcba5c3":"kf = KFold(n_splits=10, random_state=42, shuffle=True)\n\nR2_scores_standardized = cross_val_score(RFR, X_train_standardized, y_train, cv=kf)\ny_predict_standardized = cross_val_predict(RFR, X_train_standardized, y_train, cv=kf)\nmae_standarsized = MAE(y_train, y_predict_standardized)\n\nR2_scores_normalized = cross_val_score(RFR, X_train_normalized, y_train, cv=kf)\ny_predict_normalized = cross_val_predict(RFR, X_train_normalized, y_train, cv=kf)\nmae_normalized = MAE(y_train, y_predict_normalized)","84887468":"fig, axes = plt.subplots(1,2)\n((ax1, ax2)) = axes\n\ny_predicted = cross_val_predict(RFR, X_train_standardized, y_train, cv=kf)\nax1.scatter(y_train, y_predicted, alpha=0.3, color='orange')\nax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\nax1.set_xlabel('Actual')\nax1.set_ylabel('Predicted')\nax1.set_title('standardized:')\n\ny_predicted = cross_val_predict(RFR, X_train_normalized, y_train, cv=kf)\nax2.scatter(y_train, y_predicted, alpha=0.3, color='red')\nax2.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\nax2.set_xlabel('Actual')\nax2.set_ylabel('Predicted')\nax2.set_title('normalized:')\n\nplt.show()","f733c6dd":"print(\"the scores of cross validation are:\")\nprint(R2_scores_standardized)\nprint()\nprint(\"mean R2 is: \" + str(R2_scores_standardized.mean())[:5] + \" with std of  \" + str(R2_scores_standardized.std())[:5] + \" and MAE of \" + str(mae_standarsized)[:6])","5e2d3fc3":"print(\"the scores of cross validation are:\")\nprint(R2_scores_normalized)\nprint()\nprint(\"mean R2 is: \" + str(R2_scores_normalized.mean())[:5] + \" with std of  \" + str(R2_scores_normalized.std())[:5] + \" and MAE of \" + str(mae_normalized)[:6])","201176c7":"RFR = RandomForestRegressor()\n\nRFR.fit(X_train_standardized, y_train)\nR2_score = RFR.score(X_test_standardized, y_test)\ny_predict = RFR.predict(X_test_standardized)\nmae_score = MAE(y_test, y_predict)\n\nindexes = list(range(1, len(y_predict)+1))\nfig, axs = plt.subplots(1, 1, figsize=(9, 3), sharey=True)\naxs.plot(indexes, y_predict, label='target_predicted', color='orange')\naxs.plot(indexes, y_test, label='target_value', color='purple')\naxs.legend()\naxs.set_xlabel('targes indexes')\naxs.set_ylabel('Selling Price')\nfig.suptitle('Predicted values VS True Values:')\nplt.show()\n\npd.DataFrame(index=['test LR model'], data={'R2_score': R2_score, 'MAE score': mae_score}) \n","429ac2e6":"RFR = RandomForestRegressor()\n\nRFR.fit(X_train_normalized, y_train)\nR2_score = RFR.score(X_test_normalized, y_test)\ny_predict = RFR.predict(X_test_normalized)\nmae_score = MAE(y_test, y_predict)\n\nindexes = list(range(1, len(y_predict)+1))\nfig, axs = plt.subplots(1, 1, figsize=(9, 3), sharey=True)\naxs.plot(indexes, y_predict, label='target_predicted', color='orange')\naxs.plot(indexes, y_test, label='target_value', color='purple')\naxs.legend()\naxs.set_xlabel('targes indexes')\naxs.set_ylabel('Selling Price')\nfig.suptitle('Predicted values VS True Values:')\nplt.show()\n\npd.DataFrame(index=['test LR model'], data={'R2_score': R2_score, 'MAE score': mae_score}) \n","d9ec13cf":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","a24dd79b":"# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = RFR, param_distributions = random_grid, n_iter = 100, cv = kf, verbose=2, random_state=42, n_jobs = -1, scoring='r2')\n# Fit the random search model\nrf_random.fit(X_train_normalized, y_train)","c029b679":"print(\"best R2 score is:\")\nprint(rf_random.best_score_)","425a244c":"rf_random.best_params_","45f6e900":"rf = rf_random.best_estimator_\nrf.fit(X_train_normalized, y_train)\n\nR2_score = rf.score(X_test_normalized, y_test)\ny_predict = rf.predict(X_test_normalized)\nmae_score = MAE(y_test, y_predict)\n\nindexes = list(range(1, len(y_predict)+1))\nfig, axs = plt.subplots(1, 1, figsize=(9, 3), sharey=True)\naxs.plot(indexes, y_predict, label='target_predicted', color='orange')\naxs.plot(indexes, y_test, label='target_value', color='purple')\naxs.legend()\naxs.set_xlabel('targes indexes')\naxs.set_ylabel('Selling Price')\nfig.suptitle('Predicted values VS True Values:')\nplt.show()\n\npd.DataFrame(index=['test RFR model'], data={'R2_score': R2_score, 'MAE score': mae_score}) \n","66fe7e34":"rf.feature_importances_\nfeature_imp = pd.Series(rf.feature_importances_,index=X_train_normalized.columns).sort_values(ascending=False)\nprint(\"feature importances list:\")\nfeature_imp","804a5487":"imp_limits = [0, 0.0001, 0.017, 0.02, 0.03, 0.035, 0.036, 0.037, 0.04, 0.07, 0.08, 0.1, 0.3]\n\nmean_scores = []\nstd_scores = []\nmae_scores = []\n\nfor limit in imp_limits:\n    X_train_copy = copy_by_corr_limit(X_train_normalized, limit, feature_imp)\n    R2_scores = cross_val_score(rf, X_train_copy, y_train, cv=kf)\n    y_predict = cross_val_predict(rf, X_train_copy, y_train, cv=kf)\n    mae_score = MAE(y_train, y_predict)\n\n    mean_scores.append(R2_scores.mean())\n    std_scores.append(R2_scores.std())\n    mae_scores.append(mae_score)\n    \npd.DataFrame(data={'lim importance:':imp_limits, 'R2_score': mean_scores, 'R2_std': std_scores, 'MAE score': mae_scores}) ","a8a55d30":"rf_random.best_params_","f14b97bf":"X_train_copy = copy_by_corr_limit(X_train_standardized, 0.00, correlations)\nX_test_copy = copy_by_corr_limit(X_test_standardized, 0.00, correlations)\n\nLR.fit(X_train_copy, y_train)\nR2_score = LR.score(X_test_copy, y_test)\ny_predict = LR.predict(X_test_copy)\nmae_score = MAE(y_test, y_predict)\n\nindexes = list(range(1, len(y_predict)+1))\nfig, axs = plt.subplots(1, 1, figsize=(9, 3), sharey=True)\naxs.plot(indexes, y_predict, label='target_predicted', color='orange')\naxs.plot(indexes, y_test, label='target_value', color='purple')\naxs.legend()\naxs.set_xlabel('targes indexes')\naxs.set_ylabel('Selling Price')\nfig.suptitle('Predicted values VS True Values:')\nplt.show()\n\npd.DataFrame(index=['test LR model'], data={'R2_score': R2_score, 'MAE score': mae_score}) \n","97458ee0":"### numerical features correlations:","b6cca6db":"### - Unique values\n#### as we can see, in the 'Fuel type' attribute, there is only 2 observation that is uniqe. because it's just one I will remove this observation.","42796d02":"MAE:\n\nfrom these 3 metrics:\n- MAE is the easiest to understand, because it's the average error.\n- MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\nAll of these are loss functions, because we want to minimize them.\n\nI chose MAE because it gives a basic, simple-to-understand assessment of the error that the model has.","966a9501":"<br>\n<br>\n<br>","f4752603":"### The scores are:","02cf0084":"#### Although the high score with the cross validation, the test is not so good.\n#### I will check the feature importances list, and if I can use feature selection to improve the model.\n## feature selection:","27ecf8d6":"### I will make new featurs to use the data more efficiently:","8f9fdc38":"I will use 2 performance measurements: R2 and MAE.\n\n<br>R2:\nThe coefficient of determination, R2 (\"R squared\"), is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\nIt is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.\n\n( source - https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination )\n\nin simple words, R2 is the percentage of the explained variance from the general variance.\n<br>The percentage of explained variance allows us to know how much of the variance of the dependent variable is explained by the independent variables.<br>\nThe higher the percentage of explained variance, the more it means that X helps us predict Y.","1ed71b49":"## feature selection:","87e72d90":"## Regression task - Predict Car Selling Price","69928647":"### imports","8e3e4c4b":"<br>\n\n# <ins>D. Dig into the DATA - correlations and patterns<\/ins>","4f4ec9f3":"- this is preaty good, but I think that I can improve that with a creating new features!\n- note: it looks like the scaling method isn't matter, I will check that later also. but you can see below that the values of the features are different.","f856a023":"#### standadized train set cross validation:","45d9ecb4":"### Linear Regression with the params below:","d4b090bb":"#### to get a better understanding of the Age affect I will plot it another way:","e4f9a9fd":"### Now we have scaled test set and train set, we can continue to find a good model!\n<br> <br>","365d202c":"The best score is without any limit. this is what we did earlier, so it didn't help us as I hoped.","064b29ad":"### Basic information about the data:","ee522def":"### what can I do better?\n\n- handle outliers.\n- grid search for the RFR model.\n- check more algoritems (more models types).","b7367783":"\n# <ins>I. My best model:<\/ins>","0c1c112b":"I will repeat the previous steps to include the new features in the train and test sets. \n<br>note: the test and train sets rows will not change because Im using the same random_state. so, the rows will remain the same in each set.","6e369c55":"this is a minority improvment, so it is not significant. that is why I will go with 0.00 correlation limit.\n## Testing our best linear regression model:","651a16bc":"# <ins> F. Test Set and Train Test + Scaling<\/ins>","57b18667":"### let's take a look at the numeric attributes histograma:","f442be48":"#### normalized train set cross validation:","fbc42bb8":"#### standadized train set cross validation:","366300d8":"#### Massive improvement:","8e5f4f60":"## attributes information","4bc35329":"#### this score is pretty good!\n\n- I will try to increase the R2 and the MAE of the Random Forest Regressor model by choosing the best hyperparams.\n- I will use only the normalized data because it has a better scores.\n\n## Randomized Search:","20370c2a":"- here we can see the features wich have a significant linear correlation with Selling Price:","d10044dd":"# _Regression notebook_","67c87093":"# <ins> A. Task definition and general information <\/ins> ","541ba8e0":"### Random Forest Regressor normalized data","910dd584":"# <ins> B. Basic familiarity with the Dataset <\/ins>","d66cdd92":"# <ins> E. Select a Performance Measure <\/ins>","6c731215":"### heatmap correlations which is greater then -+0.2:","b4cf7b79":"#### check the limits scores:","a92c5b1c":"### general knowledge about the dataset:\nThis dataset contains information about used cars listed on www.cardekho.com and published on Kaggle.\n\nThe columns in the given dataset is as follows:\n\n- Car_Name\n- Year (of manufacture)\n- Selling_Price\n- Present_Price\n- Kms_Driven\n- Fuel_Type\n- Seller_Type\n- Transmission\n- Owner","a32d7e24":"### This is a big improvment!\nnote that the results are still the same, it seems that normalization and standatization have the same affect on linear regression models.\n<br>I will use only one of them the next step.\n\n<br> I will try to increase the R2 with feature selection. \n\n\n","c5c0a14d":"### categorial features correlations:","829d3aa3":"<br>\n<br>\n\n#### In conclusion:","3e6d44a3":"#### high correlations with selling price:","bba9bb29":"- Present_Price_Age_ratio is very significant.\n- Present_Price_KMs_ratio more significant than KMS alone.\n- KMs_Per_year is more significant than KMS ang Car Age separately.","89db0829":"### I would love to get comments, reviews and suggestions for improvement!\nI would especially love to get ideas why there are significant differences between the training (including cross-validation) and the testing with the Random Forest Regressor model.","80343b56":"### but first, let's see the dummy model:","98d7aa1a":"# H. Random Forest Regressor model ","253ff652":"#### I will drop the names and year columns.\nIt is true that in theory the names can give us a good information, but we have only 300 rows and 98 uniqe names, so, not in this case.","2468213c":"# <ins> G. Linear-Regression Model <\/ins>","67fe5f8e":"# <ins> C. Clean and prepare the data <\/ins>  ","13dc352a":"#### The year coulmn is not generalize, so I will generate it to Age. this is a better information.","f32715d1":"- I will scale the data in 2 ways To see if there are significant differences\u05e5\n### scaling the numerical features with StandardScaler and MinMax functions:","49cce881":"#### normalized train set cross validation:","a540f69e":"##### Let's see those correlations:","af3a80cb":"# This _linear regression_ model has R2 score which is a 10% improvement compared to  top-5 most voted notebooks at kaggle for this DATASET. The main reason is because I added 3 new features.\nyou can check here the most voted notebooks: https:\/\/www.kaggle.com\/nehalbirla\/vehicle-dataset-from-cardekho\/code?datasetId=33080&sortBy=voteCount","15eab29f":"## - Handeling text and categorial attributes\n\n#### first of all, I will use \"get_dummies\" function to \"convert\" every categorial attribute.","1fc19717":"### Scaling the numerical features (including the new ones) with StandardScaler and MinMax functions:","df09ef09":"#### let's look at the categorial attributes histograma (as pies):\n","a91242d8":"## Let's try so uncover some patterns.\n\n#### although linear correlations are not the only correlations we can find, it can gives us a good start. I will use Pearson\u2019s correlation coefficient in the next matrixes.\n\n### Correlation Matrix:","cb8e3d49":"## Testing best Random forest Regressor model:","42fdcbd4":"#### Those are great scores! let's see the score with the test set:","99e36792":"### Random Forest Regressor standardized data Test:","3335daad":"<br>\nlet's take a look at the attributes categories:"}}