{"cell_type":{"d74a7a00":"code","e430024d":"code","8002c9a8":"code","d0b801e8":"code","e44a6a91":"code","084063c8":"code","64a2e853":"code","a922cb02":"code","c67cd2dc":"code","ccc09565":"code","e24c2b35":"code","35a3d45b":"markdown","93eff6ea":"markdown","62d8c6e3":"markdown"},"source":{"d74a7a00":"!pip install sentence-transformers","e430024d":"from torch.utils.data import DataLoader\nimport math\nfrom sentence_transformers import LoggingHandler, util\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\nfrom sentence_transformers.readers import InputExample\nimport logging\nfrom datetime import datetime\nimport os\nimport gzip\nimport csv\nimport re\nfrom zipfile import ZipFile\n\nimport pandas as pd\nimport numpy as np","8002c9a8":"#### Just some code to print debug information to stdout\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nlogger = logging.getLogger(__name__)\n#### \/print debug information to stdout","d0b801e8":"# dataset_path = '..\/input\/cleaning-paraphrase-data\/'\n# train_path = '..\/input\/cleaning-paraphrase-data\/paraphrase_train.csv'\n# dev_path = '..\/input\/cleaning-paraphrase-data\/paraphrase_dev.csv'","e44a6a91":"# # Read the quora dataset split for classification\n# logger.info(\"Read train dataset\")\n# train_samples = []\n# with open(os.path.join(dataset_path, 'paraphrase_train.csv'), 'r', encoding='utf8') as fIn:\n#     reader = csv.DictReader(fIn, delimiter=',')\n#     for row in reader:\n#         train_samples.append(InputExample(texts=[row['q1'], row['q2']], label=int(row['score'])))\n#         train_samples.append(InputExample(texts=[row['q2'], row['q1']], label=int(row['score'])))\n\n\n# logger.info(\"Read dev dataset\")\n# dev_samples = []\n# with open(os.path.join(dataset_path, 'paraphrase_dev.csv'), 'r', encoding='utf8') as fIn:\n#     reader = csv.DictReader(fIn, delimiter=',')\n#     for row in reader:\n#         dev_samples.append(InputExample(texts=[row['q1'], row['q2']], label=int(row['score'])))","084063c8":"dataset_path = '..\/input\/paws-indonesia\/PAWS (Paraphrase Adversaries from Word Scrambling)\/final'\ntrain_path = '..\/input\/paws-indonesia\/PAWS (Paraphrase Adversaries from Word Scrambling)\/final\/train.tsv'\ndev_path = '..\/input\/paws-indonesia\/PAWS (Paraphrase Adversaries from Word Scrambling)\/final\/dev.tsv'","64a2e853":"df = pd.read_csv(train_path, sep='\\t')\ndf.isna().sum()","a922cb02":"def preprocessing(s):\n    s = re.sub(r\"(<br\\s*\/><br\\s*\/>)|(\\-)|(\\\/)\", ' ', s)\n    s = re.sub(r'<([A-Z][A-Z0-9]*)\\b[^>]*>(.*?)<\/\\1>', ' ', s) # html tag\n    s = s.strip().lower() # case folding dan menghilangkan new line\n    s = s.replace(\"\\n\", \" \") # menggantikan \\n dengan spasi\n    s = re.sub(r'[^a-zA-Z0-9 ]', ' ', s) # menghapus simbol\n    s = re.sub(' +', ' ', s) # menghapus repetitive space\n\n    return s","c67cd2dc":"# Read the quora dataset split for classification\nlogger.info(\"Read train dataset\")\ntrain_samples = []\nwith open(os.path.join(dataset_path, 'train.tsv'), 'r', encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter='\\t')\n    for row in reader:\n        sentence1 = preprocessing(row['sentence1'])\n        sentence2 = preprocessing(row['sentence2'])\n        \n        train_samples.append(InputExample(texts=[sentence1, sentence2], label=int(row['label'])))\n        train_samples.append(InputExample(texts=[sentence2, sentence1], label=int(row['label'])))\n\n\nlogger.info(\"Read dev dataset\")\ndev_samples = []\nwith open(os.path.join(dataset_path, 'dev.tsv'), 'r', encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter='\\t')\n    for row in reader:\n        sentence1 = preprocessing(row['sentence1'])\n        sentence2 = preprocessing(row['sentence2'])\n        dev_samples.append(InputExample(texts=[sentence1, sentence2], label=int(row['label'])))","ccc09565":"#Configuration\ntrain_batch_size = 16\nnum_epochs = 4\nmodel_save_path = 'output\/training_PAWS-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n\n#We use distilroberta-base with a single label, i.e., it will output a value between 0 and 1 indicating the similarity of the two questions\n# model = CrossEncoder('cross-encoder\/ms-marco-TinyBERT-L-6', num_labels=1)\n# model = CrossEncoder('indobenchmark\/indobert-base-p2', num_labels=1)\nmodel = CrossEncoder('cross-encoder\/quora-roberta-large', num_labels=1)\n# model = CrossEncoder('..\/input\/transfer-learning-multilingual-sbert-cross-encode\/output\/quora-indo-cross-encoder-v1-en-id-2021-06-25_03-54-49\/', num_labels=1)\n\n\n\n# We wrap train_samples (which is a List[InputExample]) into a pytorch DataLoader\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n\n\n# We add an evaluator, which evaluates the performance during training\nevaluator = CEBinaryClassificationEvaluator.from_input_examples(dev_samples, name='crossEncoder-dev')\n\n\n# Configure the training\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\nlogger.info(\"Warmup-steps: {}\".format(warmup_steps))","e24c2b35":"# Train the model\nmodel.fit(train_dataloader=train_dataloader,\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=5000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)","35a3d45b":"# Cleaning PAWS Dataset","93eff6ea":"# Cross Encoder - Question Pair\nThis examples trains a CrossEncoder for the Quora Duplicate Questions Detection task. A CrossEncoder takes a sentence pair\nas input and outputs a label. Here, it output a continious labels 0...1 to indicate the similarity between the input pair.\nIt does NOT produce a sentence embedding and does NOT work for individual sentences.","62d8c6e3":"https:\/\/www.sbert.net\/docs\/pretrained-models\/msmarco-v3.html\nMS MARCO is a large scale information retrieval corpus that was created based on real user search queries using Bing search engine. The provided models can be used for semantic search, i.e., given keywords \/ a search phrase \/ a question, the model will find passages that are relevant for the search query.\n\nhttps:\/\/www.sbert.net\/examples\/training\/ms_marco\/README.html\nMS MARCO Passage Ranking is a large dataset to train models for information retrieval. It consists of about 500k real search queries from Bing search engine with the relevant text passage that answers the query.\n\n\n"}}