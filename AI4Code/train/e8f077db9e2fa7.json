{"cell_type":{"84bd9ecb":"code","f51de179":"code","799153ed":"code","1ccf6579":"code","53a82f65":"code","af149b59":"code","4bda38d6":"code","f3445bb8":"code","577a71b5":"code","2215da26":"code","1c2336ae":"code","1df5f7ad":"code","8bfe5a4b":"code","6f4f391d":"code","298a459f":"code","e21468af":"code","4c32f6bb":"code","696d2706":"code","0a615bb8":"code","1ee3a077":"code","6a4da4be":"code","5b81a129":"code","4bda25a7":"code","692123c2":"code","8f0cd808":"code","0a510428":"code","4d96d57e":"code","0ac48b58":"code","02b83ad0":"code","79ed6d62":"code","67cfda17":"code","9166f052":"code","5150b8c2":"markdown","f5965e80":"markdown","9904d813":"markdown"},"source":{"84bd9ecb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVR, NuSVR, LinearSVR\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\nfrom sklearn.metrics import *\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython.display import display\n\n\ndef set_seed(seed=2121):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","f51de179":"train = pd.read_csv('..\/input\/taxi-pricing-with-mobility-analytics\/sigma_cabs.csv')\ntest = pd.read_csv('..\/input\/taxi-pricing-with-mobility-analytics\/test.csv')\ndisplay(train.head())\ndisplay(test.head())\nprint(train.shape)\ntest.shape","799153ed":"plt.style.use('fivethirtyeight')\nsns.countplot(train.Surge_Pricing_Type)","1ccf6579":"train.describe(include=['O'])","53a82f65":"train.info()","af149b59":"cats = [c for c in train.columns if train[c].dtypes == 'object']\nnums = [c for c in train.columns if c not in cats]\ncats","4bda38d6":"nums","f3445bb8":"missingTr = train.isnull().sum()##\/len(train)\nmissingTr = missingTr[missingTr>0]\nmissingTr = missingTr.sort_values()\n\nmissingTs = test.isnull().sum()#\/len(test)\nmissingTs = missingTs[missingTs>0]\nmissingTs = missingTs.sort_values()\n\nplt.style.use('fivethirtyeight')\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\nmissingTr.plot.bar(color='k', ax=axes[0])   \nmissingTs.plot.bar(color='k', ax=axes[1])   \n\naxes[0].set_title('train');\naxes[1].set_title('test');","577a71b5":"sns.set(font_scale=2.1)\nint_flat = pd.melt(train, value_vars=nums)\ng = sns.FacetGrid(int_flat, col='variable', col_wrap=4, sharex=False, sharey=False, height=10, aspect=1.2)\ng = g.map(sns.distplot, 'value', color='teal', kde=True, fit=norm)\nplt.style.use('fivethirtyeight')\nplt.xticks(rotation=90)\nplt.show()","2215da26":"plt.style.use('seaborn-talk')\nimport seaborn as sns\nfig, ((a,b,c),(d,e,f),(g,h,k)) = plt.subplots(3,3,figsize=(20,12))\nsns.kdeplot(train['Customer_Since_Months'][train.Surge_Pricing_Type == 1], color=\"coral\", shade=True, ax=a)\nsns.kdeplot(train[\"Customer_Since_Months\"][train.Surge_Pricing_Type == 2], color=\"teal\", shade=True, ax=a)\nsns.kdeplot(train[\"Customer_Since_Months\"][train.Surge_Pricing_Type == 3], color=\"grey\", shade=True, ax=a)\nsns.countplot(train['Customer_Since_Months'],hue=train['Surge_Pricing_Type'],palette='coolwarm',ax=b)\nsns.distplot(train.Customer_Since_Months, kde = False, fit=norm, color= 'grey', ax=c)\nsns.distplot(test.Customer_Since_Months, kde = False, fit=norm, color = 'teal', ax=c)\n\nsns.kdeplot(train['Customer_Rating'][train.Surge_Pricing_Type == 1], color=\"coral\", shade=True, ax=d)\nsns.kdeplot(train[\"Customer_Rating\"][train.Surge_Pricing_Type == 2], color=\"teal\", shade=True, ax=d)\nsns.kdeplot(train[\"Customer_Rating\"][train.Surge_Pricing_Type == 3], color=\"grey\", shade=True, ax=d)\nsns.countplot(train['Customer_Rating'],hue=train['Surge_Pricing_Type'],palette='coolwarm', ax=e)\nsns.distplot(train.Customer_Rating, kde = False, fit=norm, color= 'grey', ax=f)\nsns.distplot(test.Customer_Rating, kde = False, fit=norm, color = 'teal', ax=f)\n\nsns.kdeplot(train['Cancellation_Last_1Month'][train.Surge_Pricing_Type == 1], color=\"coral\", shade=True, ax=g)\nsns.kdeplot(train[\"Cancellation_Last_1Month\"][train.Surge_Pricing_Type == 2], color=\"teal\", shade=True, ax=g)\nsns.kdeplot(train[\"Cancellation_Last_1Month\"][train.Surge_Pricing_Type == 3], color=\"grey\", shade=True, ax=g)\nsns.countplot(train['Cancellation_Last_1Month'],hue=train['Surge_Pricing_Type'],palette='coolwarm',ax=h)\nsns.distplot(train.Cancellation_Last_1Month, kde = False, fit=norm, color= 'grey', ax=k)\nsns.distplot(test.Cancellation_Last_1Month, kde = False, fit=norm, color = 'teal', ax=k)","1c2336ae":"plt.style.use('seaborn-talk')\nimport seaborn as sns\nfig, ((a,b,c),(d,e,f),(g,h,k)) = plt.subplots(3,3,figsize=(20,12))\nsns.kdeplot(train['Var1'][train.Surge_Pricing_Type == 1], color=\"cyan\", shade=True, ax=a)\nsns.kdeplot(train[\"Var1\"][train.Surge_Pricing_Type == 2], color=\"teal\", shade=True, ax=a)\nsns.kdeplot(train[\"Var1\"][train.Surge_Pricing_Type == 3], color=\"grey\", shade=True, ax=a)\nsns.countplot(train['Var1'],hue=train['Surge_Pricing_Type'],palette='bone',ax=b)\nsns.distplot(train.Var1, kde = False, fit=norm, color= 'grey', ax=c)\nsns.distplot(test.Var1, kde = False, fit=norm, color = 'teal', ax=c)\n\nsns.kdeplot(train['Var2'][train.Surge_Pricing_Type == 1], color=\"cyan\", shade=True, ax=d)\nsns.kdeplot(train[\"Var2\"][train.Surge_Pricing_Type == 2], color=\"teal\", shade=True, ax=d)\nsns.kdeplot(train[\"Var2\"][train.Surge_Pricing_Type == 3], color=\"grey\", shade=True, ax=d)\nsns.countplot(train['Var2'],hue=train['Surge_Pricing_Type'],palette='bone', ax=e)\nsns.distplot(train.Var2, kde = False, fit=norm, color= 'grey', ax=f)\nsns.distplot(test.Var2, kde = False, fit=norm, color = 'teal', ax=f)\n\nsns.kdeplot(train['Var3'][train.Surge_Pricing_Type == 1], color=\"cyan\", shade=True, ax=g)\nsns.kdeplot(train[\"Var3\"][train.Surge_Pricing_Type == 2], color=\"teal\", shade=True, ax=g)\nsns.kdeplot(train[\"Var3\"][train.Surge_Pricing_Type == 3], color=\"grey\", shade=True, ax=g)\nsns.countplot(train['Var3'],hue=train['Surge_Pricing_Type'],palette='bone',ax=h)\nsns.distplot(train.Var3, kde = False, fit=norm, color= 'grey', ax=k)\nsns.distplot(test.Var3, kde = False, fit=norm, color = 'teal', ax=k)","1df5f7ad":"cats.remove('Trip_ID')\ndef analyse_cats(df, cat_cols):\n    d = pd.DataFrame()\n    cl = [];u = [];s =[];nans =[]\n    for c in cat_cols:\n        #print(\"column:\" , c ,\"--Uniques:\" , train[c].unique(), \"--Cardinality:\", train[c].unique().size)\n        cl.append(c); u.append(df[c].unique()); s.append(df[c].unique().size); nans.append(df[c].isnull().sum())\n        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\n        sns.countplot(train[c], ax=axes[0], label='train', palette='Set2');\n        sns.countplot(test[c],  ax=axes[1], label='test', palette='Set2');\n        axes[0].set_title('train');\n        axes[1].set_title('test');\n    d['\"feat\"'] = cl;d[\"uniques\"] = u;d[\"cardinality\"] = s;d[\"nans\"] = nans\n    return d\nsns.set()\nplt.style.use('seaborn')\ncatanadf = analyse_cats(train, cats)\ncatanadf","8bfe5a4b":"sns.set()\nplt.style.use('seaborn-poster')\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\nsns.distplot(train.Life_Style_Index, bins=50, fit=norm, kde=True, color='teal', ax=axes[0])\nsns.distplot(test.Life_Style_Index, bins=50, fit=norm, kde=True, color='darkred', ax=axes[1])\naxes[0].set_title('train');\naxes[1].set_title('test');","6f4f391d":"\nfor c in cats:\n    le = LabelEncoder()\n    le.fit(list(train[c].astype(str)) + list(test[c].astype(str)))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    print('target mapping :  ',c ,  le_name_mapping)\n","298a459f":"plt.style.use('fivethirtyeight')\nsns.catplot(x=\"Confidence_Life_Style_Index\", y=\"Surge_Pricing_Type\", hue=\"Gender\", kind=\"bar\", data=train, aspect=2.5)","e21468af":"plt.figure(figsize=(10, 7))\nsns.catplot(data=train[cats], orient=\"h\", kind=\"box\", aspect=2)\nplt.show()","4c32f6bb":"plt.figure(figsize=(10, 7))\nsns.catplot(data=train[['Var1', 'Var2', 'Var3']], orient=\"h\", kind=\"box\", aspect=2)\nplt.show()","696d2706":"plt.style.use('seaborn')\ng = sns.catplot(x=\"Type_of_Cab\", y=\"Surge_Pricing_Type\", row=\"Confidence_Life_Style_Index\",\n                kind=\"box\", orient=\"h\", height=2, aspect=4,\n                data=train)\ng.set(xscale=\"log\")","0a615bb8":"plt.figure(figsize=(15, 7))\nsns.scatterplot(x=\"Customer_Rating\", y=\"Life_Style_Index\", hue=\"Type_of_Cab\", size=\"Trip_Distance\" , data=train, palette='vlag')","1ee3a077":"plt.figure(figsize=(15, 7))\nsns.scatterplot(x=\"Trip_Distance\", y=\"Life_Style_Index\", hue=\"Type_of_Cab\", size=\"Customer_Rating\" , data=train, palette='BuGn_r')","6a4da4be":"plt.figure(figsize=(12, 5))\nsns.scatterplot(x=\"Cancellation_Last_1Month\", y=\"Life_Style_Index\", hue=\"Confidence_Life_Style_Index\", size=\"Customer_Rating\" , data=train, palette='vlag')","5b81a129":"plt.figure(figsize=(15, 7))\nsns.scatterplot(x=\"Var1\", y=\"Life_Style_Index\", hue=\"Confidence_Life_Style_Index\", size=\"Customer_Rating\" , data=train, palette='BuGn_r')","4bda25a7":"plt.figure(figsize=(15, 7))\nsns.scatterplot(x=\"Var1\", y=\"Var2\", hue=\"Confidence_Life_Style_Index\", size=\"Customer_Rating\" , data=train, palette='bone')","692123c2":"plt.figure(figsize=(15, 10))\nsns.heatmap(train[nums].corr(), annot=True, center=True)","8f0cd808":"target = train.pop('Surge_Pricing_Type')\ntarget = target.map({1:0, 2:1, 3:2})\n\n\nfor df in [train, test]:\n    del df['Trip_ID']","0a510428":"params = {\n    \n    'objective': 'multiclass',\n    'boosting': 'gbdt',\n    'metric': 'multi_logloss',\n    'max_depth': -1,\n    'num_leaves': 20,\n    'learning_rate': 0.1,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n       \n    'lambda_l2': 2.0,\n    'lambda_l1': 2.0,\n   # 'min_gain_to_split': 0,\n    'num_class': len(np.unique(target)),\n     }\n\nimport lightgbm as lgb\n\nscores = []\n\n\noof = np.zeros(len(train))\npreds_lgb = np.zeros(len(test))\n\nfeature_importances_gain = pd.DataFrame()\nfeature_importances_gain['feature'] = train.columns\n\nfeature_importances_split = pd.DataFrame()\nfeature_importances_split['feature'] = train.columns\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=4242)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print(\"fold :::::::: \" , fold_)\n    trn_data = lgb.Dataset(train.iloc[train_ind], target.iloc[train_ind])\n    val_data = lgb.Dataset(train.iloc[val_ind], target.iloc[val_ind])\n    \n    model = lgb.train(params, trn_data, valid_sets=(trn_data, val_data), num_boost_round=1000, verbose_eval=100, early_stopping_rounds=100)\n    oof[val_ind] = np.argmax(model.predict(train.iloc[val_ind], num_iteration=model.best_iteration), axis=1)\n    \n        \n    print('f1 :', f1_score(target.iloc[val_ind], oof[val_ind], average='micro'))\n    scores.append(f1_score(target.iloc[val_ind], oof[val_ind], average='micro'))\n    \n    feature_importances_gain['fold_{}'.format(fold_ + 1)] = model.feature_importance(importance_type='gain')\n    feature_importances_split['fold_{}'.format(fold_ + 1)] = model.feature_importance(importance_type='split')\n    \n    preds_lgb += np.argmax(model.predict(test, num_iteration=model.best_iteration), axis=1)\/folds.n_splits\n    \npreds_lgb = np.round(preds_lgb)\nprint('f1 micro mean ---->',np.mean(scores))\n","4d96d57e":"np.unique(preds_lgb)","0ac48b58":"plt.figure(figsize=(10, 5))\npd.Series(preds_lgb).hist(color='teal')","02b83ad0":"feature_importances_gain['average'] = feature_importances_gain[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances_gain.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(15, 10))\nsns.barplot(data=feature_importances_gain.sort_values(by='average', ascending=False).head(100),color='teal',  x='average', y='feature');\nplt.title('TOP feature importance over {} folds average'.format(folds.n_splits));","79ed6d62":"target_names = target.unique()\n\n\ndef plot_cm(y_true, y_pred, title):\n    figsize=(12,8)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='vlag',  annot=annot, fmt='', ax=ax)\nsns.set(font_scale=1.2)\nplot_cm(target, oof, 'cm')","67cfda17":"\nxgb_params = {\n    \n    'objective':'multi:softmax', \n    'max_depth': 5, \n    'learning_rate': 0.1, \n    'booster':'gbtree', \n    'eval_metric': 'mlogloss', \n    'max_leaves': 20, \n    'colsample_bytree': 0.8,\n    'num_class': len(np.unique(target)),\n    'subsample':0.8, \n    'lambda':2, \n    'alpha': 1.2\n   \n}\n\n\nxgb_scores = []\n\noof_xgb = np.zeros(len(train))\npred_xgb = np.zeros(len(test))\n\nimportances = pd.DataFrame()\n\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=4242)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold : ----------------------------------------', fold_)\n    trn_data = xgb.DMatrix(data=train.iloc[train_ind], label=target.iloc[train_ind])\n    val_data = xgb.DMatrix(data= train.iloc[val_ind], label=target.iloc[val_ind])\n    \n       \n    xgb_model = xgb.train(xgb_params, trn_data, num_boost_round=3000, evals=[(trn_data, 'train'), (val_data, 'test')], verbose_eval=100, early_stopping_rounds=100)\n    oof_xgb[val_ind] = xgb_model.predict(xgb.DMatrix(train.iloc[val_ind]),  ntree_limit= xgb_model.best_ntree_limit)\n    \n    print(f1_score(target.iloc[val_ind], oof_xgb[val_ind],average='micro' ))\n    xgb_scores.append(f1_score(target.iloc[val_ind], oof_xgb[val_ind], average='micro'))\n        \n    importance_score = xgb_model.get_score(importance_type='gain')\n    importance_frame = pd.DataFrame({'Importance': list(importance_score.values()), 'Feature': list(importance_score.keys())})\n    importance_frame['fold'] = fold_ +1\n    importances = pd.concat([importances, importance_frame], axis=0, sort=False)\n    \n    pred_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit= xgb_model.best_ntree_limit)\/folds.n_splits\n    \n\nprint('model f1:------------------>', np.mean(xgb_scores))","9166f052":"\nmean_gain = importances[['Importance', 'Feature']].groupby('Feature').mean()\n\nmean_gain = mean_gain.reset_index()\nplt.figure(figsize=(17, 10))\nsns.barplot(x='Importance', y='Feature', data=mean_gain.sort_values('Importance', ascending=False), palette='bone')\n","5150b8c2":"## **EDA**","f5965e80":"## XGBoost ","9904d813":"### Missing data"}}