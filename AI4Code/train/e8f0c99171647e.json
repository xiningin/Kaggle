{"cell_type":{"b0f41a20":"code","3b496a23":"code","c6d84a87":"code","61bfcdb9":"code","2d2a785e":"code","ed795103":"code","c5697042":"code","04424011":"code","c0f38db2":"code","d2996daa":"code","591b8c01":"code","cf8b854d":"code","77376ed8":"code","90efb043":"code","8ffb8d3c":"code","d7126529":"code","d72bef3f":"code","f29d69ff":"code","ef3f949c":"code","c2a7c69e":"code","252d8b07":"code","4a3032a7":"code","ac24327c":"code","8ff897f8":"code","77922e8d":"code","83b30313":"code","fd34baaf":"code","9912a9a8":"code","43093082":"code","a5f10b72":"code","0c2ce821":"code","dea3fc9b":"code","7ed4de59":"code","b5d53a4d":"code","cbd809e5":"code","d93ff649":"code","a7a09622":"code","e530d204":"code","1428fe7e":"code","a7d08ffd":"code","12f1fbbc":"code","be4298c5":"code","674a1b26":"code","8a563644":"code","1f0132e3":"code","96b4ba9d":"code","6f5d474f":"code","cca48c8c":"code","1f55d046":"code","96e7b9c7":"code","82863795":"code","9f924551":"markdown","67209389":"markdown","ec252426":"markdown","1cd01cf9":"markdown","c5941323":"markdown","cd647b4d":"markdown","82e80ec4":"markdown","f644ebec":"markdown","4d2be76c":"markdown","2310afab":"markdown","5c148d65":"markdown","fbd9d797":"markdown","d9e4b4cd":"markdown","d11ab422":"markdown","062fd824":"markdown","676cf926":"markdown","00a89703":"markdown","a6abaf3b":"markdown","a281e60a":"markdown","5f712019":"markdown"},"source":{"b0f41a20":"###############################################################################\n#                       Load Library                                          #\n###############################################################################\n\n#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\n# Preparation  \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\n# Import StandardScaler from scikit-learn\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score,recall_score\nfrom sklearn.metrics import make_scorer,mean_absolute_error\nfrom sklearn.metrics import mean_squared_error,classification_report,f1_score\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.linear_model import LogisticRegression\n\n#import tensorflow as tf \n#from tensorflow.keras import layers\n#from tensorflow.keras.callbacks import EarlyStopping\n#from tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone,ClassifierMixin\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif,chi2\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom xgboost import XGBClassifier,XGBRegressor\nfrom sklearn import set_config\nfrom itertools import combinations\n# Cluster :\nfrom sklearn.cluster import MiniBatchKMeans\n#from yellowbrick.cluster import KElbowVisualizer\n#import smong \nimport category_encoders as ce\nimport warnings\n#import optuna \nfrom joblib import Parallel, delayed\nimport joblib \nfrom sklearn import set_config\nfrom typing import List, Optional, Union\nimport itertools\n# Imbalanced data \nfrom imblearn.datasets import fetch_datasets\n# to correctly set up the cross-validation\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import (\n    RandomUnderSampler,\n    CondensedNearestNeighbour,\n    TomekLinks,\n    OneSidedSelection,\n    EditedNearestNeighbours,\n    RepeatedEditedNearestNeighbours,\n    AllKNN,\n    NeighbourhoodCleaningRule,\n    NearMiss,\n    InstanceHardnessThreshold\n)\nfrom imblearn.over_sampling import (\n    RandomOverSampler,\n    SMOTE,\n    ADASYN,\n    BorderlineSMOTE,\n    SVMSMOTE,\n)\n\nset_config(display='diagram')\nwarnings.filterwarnings('ignore')","3b496a23":"%%time \n###############################################################################\n#                        Read train data                                      #\n###############################################################################\n\ntrain = pd.read_csv('..\/input\/GiveMeSomeCredit\/cs-training.csv')\ntest = pd.read_csv('..\/input\/GiveMeSomeCredit\/cs-test.csv')\ntrain.head(3)","c6d84a87":"###############################################################################\n#                       Cast dtypes                                           #\n###############################################################################\n\n# Convert Dtypes :\ntrain[train.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns] = train[train.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','category']).columns] = train.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))\n# Convert Dtypes :\ntest[test.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns] = test[test.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntest[test.select_dtypes(['object','category']).columns] = test.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))","61bfcdb9":"###############################################################################\n#                        Reduce Memory                                        #\n###############################################################################\n\n# Author : https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        name =df[col].dtype.name \n        \n        if col_type != object and col_type.name != 'category':\n        #if name != \"category\":    \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\ntrain= reduce_mem_usage(train)\ntest= reduce_mem_usage(test)","2d2a785e":"# Cardinality : \n# - RevolvingUtilizationOfUnsecuredLines :125728, high Outlier\n# - DebtRatio :114194 , high Outlier \n# deal with outlier + bin \nPERCENTAGE = [\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\"]\n# MonthlyIncome:13594 , high outlier +bin \nREAL= [\"MonthlyIncome\"]\n# Can be considred as cat \nNUMERIC_DISCRET_low = [\"NumberOfDependents\",\n                       \"NumberOfTime60-89DaysPastDueNotWorse\",\n                       \"NumberRealEstateLoansOrLines\",\n                       \"NumberOfTimes90DaysLate\",\n                       \"NumberOfOpenCreditLinesAndLoans\",\n                       \"NumberOfTime30-59DaysPastDueNotWorse\",\n                       \"age\"]\nLate_Pay_Cols = ['NumberOfTime30-59DaysPastDueNotWorse',\n                 'NumberOfTimes90DaysLate',\n                 'NumberOfTime60-89DaysPastDueNotWorse']\nTARGET = [\"SeriousDlqin2yrs\"]\n\n#also change the type for TARGET to categorical\n#df[TARGET] = df[TARGET].astype('category')","ed795103":"y = train['SeriousDlqin2yrs']","c5697042":"%%time \n###############################################################################\n#                        Impute Data                                          #\n###############################################################################\n\nparam =  {   \"verbosity\": 0,\n            #\"objective\": \"binary:logistic\",\n            #\"eval_metric\": \"auc\",\n            'random_state': 42,\n            # regression\n            'objective':'reg:squarederror', \n             'eval_metric': 'mae',\n            #early_stopping_rounds=100 ,\n           # 'gpu_id':0, \n           # 'predictor':\"gpu_predictor\",\n            # use exact for small dataset.\n            #\"tree_method\": \"exact\",\n            # big data :\n            # 'tree_method': 'gpu_hist',\n            # defines booster, gblinear for linear functions.\n             'booster': 'gbtree', \n            'lambda': 8.544792472633987e-07,\n            'alpha': 0.31141671752487043,\n            'subsample': 0.8779467596981366, \n            'colsample_bytree': 0.9759532762677546,\n            'learning_rate': 0.008686087328805853, \n            'n_estimators': 6988,\n            'max_depth': 9,\n            'min_child_weight': 2, \n            'eta': 3.7603213457541647e-06,\n            'gamma': 2.1478058456847449e-07,\n            'grow_policy': 'lossguide'}\n                \n\n#model_xgb = XGBRegressor(\n       #objective=\"mae\",\n #   **xgb_params2)\n\nnumeric_transformer1 = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                           # ('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\nnumeric_transformer2 = Pipeline(\n                            steps=[\n                            #('imputer', SimpleImputer(strategy='median'\n                             #                         ,add_indicator=True)),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                           # ('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\n\npipe_xgbr1 = Pipeline(\n                    steps=[\n                        ('preprocessor', numeric_transformer1),\n                        ('classifier', XGBRegressor(\n                      #objective=\"mae\",\n                       **param))\n                    ]\n                )\npipe_xgbr2 = Pipeline(\n                    steps=[\n                        ('preprocessor', numeric_transformer2),\n                        ('classifier', XGBRegressor(\n                      #objective=\"mae\",\n                       **param))\n                    ]\n                )","04424011":"%%time \ntrain=train.drop(['Unnamed: 0','SeriousDlqin2yrs'], axis=1)\ntest=test.drop(['Unnamed: 0','SeriousDlqin2yrs'], axis=1)\ntrain_final= pd.concat( [train, test], axis=0) \n#testdf_income= train_final[train_final['MonthlyIncome'].isnull()==True]\ntraindf_income = train_final[train_final['MonthlyIncome'].isnull()==False]\ny_income = traindf_income['MonthlyIncome']\nX_income=traindf_income.drop([\"MonthlyIncome\"],axis=1)\npipe_xgbr1.fit(X_income, y_income)\ntrain_income_missing=train[train['MonthlyIncome'].isnull()==True].drop([\"MonthlyIncome\"],axis=1)\ntest_income_missing=test[test['MonthlyIncome'].isnull()==True].drop([\"MonthlyIncome\"],axis=1)\ntrain_predicted = pipe_xgbr1.predict(train_income_missing)\ntest_predicted = pipe_xgbr1.predict(test_income_missing)\ntrain.loc[(train.MonthlyIncome.isnull()), 'MonthlyIncome'] = train_predicted\ntest.loc[(test.MonthlyIncome.isnull()), 'MonthlyIncome'] = test_predicted","c0f38db2":"%%time \ntraindf_NumberOfDependents = train_final[train_final['NumberOfDependents'].isnull()==False]\ny_NumberOfDependents = traindf_NumberOfDependents['NumberOfDependents']\nX_NumberOfDependents=traindf_NumberOfDependents.drop([\"NumberOfDependents\"],axis=1)\npipe_xgbr2.fit(X_NumberOfDependents, y_NumberOfDependents)\ntrain_NumberOfDependents_missing=train[train['NumberOfDependents'].isnull()==True].drop([\"NumberOfDependents\"],axis=1)\ntest_NumberOfDependents_missing=test[test['NumberOfDependents'].isnull()==True].drop([\"NumberOfDependents\"],axis=1)\ntrain_predicted = pipe_xgbr2.predict(train_NumberOfDependents_missing)\ntest_predicted = pipe_xgbr2.predict(test_NumberOfDependents_missing)\ntrain.loc[(train.NumberOfDependents.isnull()), 'NumberOfDependents'] = train_predicted\ntest.loc[(test.NumberOfDependents.isnull()), 'NumberOfDependents'] = test_predicted","d2996daa":"###############################################################################\n#                        Add Bin Features                                     #\n###############################################################################\n\n# Add bin data \n# initializing append_str\nappend_str = 'cat_'\n# Append suffix \/ prefix to strings in list\nnum_features1=[\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\",\"MonthlyIncome\"]\nnum_features2=[\"NumberOfDependents\",\n                       \"NumberOfTime60-89DaysPastDueNotWorse\",\n                       \"NumberRealEstateLoansOrLines\",\n                       \"NumberOfTimes90DaysLate\",\n                       \"NumberOfOpenCreditLinesAndLoans\",\n                       \"NumberOfTime30-59DaysPastDueNotWorse\",\n                       \"age\"]\ncat_features1 = [append_str + sub for sub in num_features1]\ncat_features2 = [append_str + sub for sub in num_features2]\n\n# create the discretizer object with strategy quantile and 1000 bins\ndiscretizer1 = KBinsDiscretizer(n_bins=40, encode='ordinal',strategy='quantile')\ndiscretizer2 = KBinsDiscretizer(n_bins=4, encode='ordinal',strategy='quantile')\n\npipeline1 = Pipeline([\n        ('imputer', SimpleImputer( strategy='median')),\n        ('bin', discretizer1)\n    ])\n# fit the discretizer to the train set\npipeline1.fit(train.loc[:,num_features1])\n# apply the discretisation\ntrain_cat1 = pipeline1.transform(train.loc[:,num_features1])\ntest_cat1 = pipeline1.transform(test.loc[:,num_features1])\ntrain_df1=pd.DataFrame(train_cat1,columns=cat_features1).astype('category')\ntest_df1=pd.DataFrame(test_cat1,columns=cat_features1).astype('category')\ntrain_final1= pd.concat( [train.loc[:,num_features1], train_df1], axis=1) \ntest_final1= pd.concat( [test.loc[:,num_features1], test_df1], axis=1) \n\npipeline2 = Pipeline([\n        ('imputer', SimpleImputer( strategy='median')),\n        ('bin', discretizer2)\n    ])\n# fit the discretizer to the train set\npipeline2.fit(train.loc[:,num_features2])\n# apply the discretisation\ntrain_cat2 = pipeline2.transform(train.loc[:,num_features2])\ntest_cat2 = pipeline2.transform(test.loc[:,num_features2])\ntrain_df2=pd.DataFrame(train_cat2,columns=cat_features2).astype('category')\ntest_df2=pd.DataFrame(test_cat2,columns=cat_features2).astype('category')\ntrain_final2= pd.concat( [train.loc[:,num_features2], train_df2], axis=1) \ntest_final2= pd.concat( [test.loc[:,num_features2], test_df2], axis=1) \ntrain_final= pd.concat( [train_final1, train_final2], axis=1) \ntest_final= pd.concat( [test_final1, test_final2], axis=1) ","591b8c01":"###############################################################################\n#                        Final X and Y                                    #\n###############################################################################\n\n# Pour le train test\ntarget= \"SeriousDlqin2yrs\"\nX = train_final# axis=1\nX_test_final =test_final# axis=1","cf8b854d":"del train\ndel test \ndel train_final\ndel test_final","77376ed8":"###############################################################################\n#                        Select Dtypes                                         #\n###############################################################################\n\n# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','int16','float16','float32','float64','int8']).columns\ncat_columns","90efb043":"###############################################################################\n#                        Select Dtypes                                         #\n###############################################################################\n\n# select the float columns\nnum_columns = X.select_dtypes(include=['int64','int16','float16','float32','float64','int8']).columns\nnum_columns","8ffb8d3c":"###############################################################################\n#                       Fe Class                                              #\n###############################################################################\n\nclass MiniKmeansTransformerEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, num_clusters = 11, encoder=ce.woe.WOEEncoder()):\n        self.num_clusters = num_clusters\n        self.encoder= encoder\n        if self.num_clusters > 0:\n            self.kmeans = MiniBatchKMeans(n_clusters=self.num_clusters, random_state=0)\n    \n    def fit(self, X, y=None):\n        if self.num_clusters > 0:\n            self.kmeans.fit(X)\n            preds=self.kmeans.predict(X)\n            preds=pd.DataFrame(preds, columns=['kmeans']).astype('category')\n            self.encoder.fit(preds,y)\n        return self\n    \n    def transform(self, X, y=None):\n        pred_classes = self.kmeans.predict(X)\n        pred_classes=pd.DataFrame(pred_classes, columns=['kmeans']).astype('category')\n        pred_encoded = self.encoder.transform(pred_classes)\n        return np.hstack((X, pred_encoded))\n        #return pred_encoded","d7126529":"###############################################################################\n#                       Final Selected Pipes                                          #\n###############################################################################\n\n# Cat pipeline\ncategorical_transformer = Pipeline(\n                    steps=[\n                        ('imputer', SimpleImputer(strategy='most_frequent',\n                                                  fill_value='missing',\n                                                  add_indicator=True)),\n                        ('encoder',  ce.woe.WOEEncoder()),#(Numerical Input, Categorical Output)\n                        #('sparse_features', SparseInteractions(degree=2)),\n                        #('reducedim',  SelectPercentile( mutual_info_classif, percentile=90))\n\n                    ]\n                    ) \n#Define vnum pipeline\nnumeric_transformer = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            #('general_features',FeaturesEngineer()),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                            ('kmeans',MiniKmeansTransformerEncoder()),\n                            ('polynominal_features', PolynomialFeatures(degree=2)),\n                            #('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\n# Features union cat + num \n# WOE+PowerTransformer\npreprocessor_woe_powertransformer = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer, num_columns),\n               # ('categorical', categorical_transformer, cat_columns)\n            ])","d72bef3f":"params_hgbc = {'l2_regularization': 2.940296779699346e-10, \n         'early_stopping': 'False',\n         'learning_rate': 0.015318591316397998, \n         'max_iter': 1537, \n         'max_depth': 19, \n         'max_bins': 137, \n         'min_samples_leaf': 140,\n         'max_leaf_nodes': 23}\nmodel_hgbc = HistGradientBoostingClassifier(**params_hgbc)\n\n #Private Score 0.86875  Public Score 0.86245\npipe1_model_hgbc_woe_powertransformer = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        #('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n        #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',model_hgbc)\n    ]\n)\npipe1_model_hgbc_woe_powertransformer2 = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor_woe_powertransformer),\n            # ('dim_red', SelectKBest(f_regression, k=20)),\n            #('Sparseinteractions', SparseInteractions(degree=2)),\n             #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n            #('PCAreducer', PCA(n_components=0.8)),\n            ('classifier',model_hgbc)\n        ]\n    )\n# Private Score 0.86861  Public Score 0.86236\npipe1_model_hgbc_woe_powertransformer2 \n#pipe1_model_hgbc_woe_powertransformer\n","f29d69ff":"params_catboost = { \"random_state\": 42,\n                    \"loss_function\":\"Logloss\",\n                    \"eval_metric\":\"AUC\",\n                    #\"task_type\" : \"GPU\",\n                   'learning_rate': 0.03174251209279996,\n                   'iterations': 1957,\n                   'objective': 'CrossEntropy',\n                   'colsample_bylevel': 0.05457006237382004,\n                   'depth': 4,\n                   'boosting_type': 'Ordered',\n                   'bootstrap_type': 'MVS',\n                   'verbose':False\n                  }\nmodel_catboost = CatBoostClassifier(\n                              # use it  only on gpu\n                            # rsm=np.nan\n                              #cat_features=cat_features\n                             **params_catboost)\npipe1_model_catboost_woe_powertransformer = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',model_catboost)\n    ]\n)\n# Private Score 0.86903  Public Score 0.86217\n#pipe1_model_catboost_woe_powertransformer","ef3f949c":"params_xgbr = { \"verbosity\": 0,\n                \"objective\": \"binary:logistic\",\n                \"eval_metric\": \"auc\",\n                'random_state': 42,\n                # regression\n                #'objective':'reg:squarederror', \n                # 'eval_metric': 'mae',\n                #early_stopping_rounds=100 ,\n                'gpu_id':0, \n                'predictor':\"gpu_predictor\",\n                # use exact for small dataset.\n                #\"tree_method\": \"exact\",\n                # big data :\n                 'tree_method': 'gpu_hist',\n                # defines booster, gblinear for linear functions.\n               'booster': 'gbtree',\n               'lambda': 0.05718879852112006,\n               'alpha': 0.3744455966091751, \n               'subsample': 0.4600950043573827, \n               'colsample_bytree': 0.4826795171274075, \n               'learning_rate': 0.04028462871213848, \n               'n_estimators': 6258, \n               'max_depth': 3,\n               'min_child_weight': 9,\n               'eta': 1.0243140309668855e-07,\n               'gamma': 0.0002882867942209271, \n               'grow_policy': 'lossguide'\n              }\nmodel_xgbr = XGBClassifier(\n                              # use it  only on gpu\n                            # rsm=np.nan\n                              #cat_features=cat_features\n                             **params_xgbr)\npipe1_model_xgbr_woe_powertransformer = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',model_xgbr)\n    ]\n)\n#pipe1_model_xgbr_woe_powertransformer","c2a7c69e":"params_lgbm =  {\n               \"objective\": \"binary\",\n                'random_state': 42,\n                'metric': 'auc',\n                \"verbosity\": -1,\n                \"boosting_type\": \"gbdt\",\n                \"device\": \"gpu\",\n                #\"early_stopping_rounds\":100,\n                'learning_rate': 0.005282912028625591, \n                'n_estimators': 1491,\n                'lambda_l1': 2.5825901359648176e-05, \n                'lambda_l2': 4.490094872871969,\n                'num_leaves': 76,\n                'feature_fraction': 0.7099515479738928,\n                'bagging_fraction': 0.8225459856082522,\n                'bagging_freq': 3,\n                'min_child_samples': 35\n               }\nmodel_lgbm = lgbm.LGBMClassifier(**params_lgbm,\n                              # use it  only on gpu\n                            # rsm=np.nan\n                              #cat_features=cat_features\n                             )\npipe1_model_lgbm_woe_powertransformer = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',model_lgbm)\n    ]\n)\n#pipe1_model_lgbm_woe_powertransformer","252d8b07":"lgbm_param2={'learning_rate': 0.0018069834369607075,\n                                 'max_depth': 8,\n                                 #'max_features': 4,\n                                 'min_samples_leaf': 47,\n                                 #'min_samples_split': 389,\n                                 'subsample': 0.8573598985000007,\n                                 #'n_iter_no_change': 300,\n                                 'n_estimators': 5000,\n                                 'verbose': -1,\n                                 'random_state': 42,\n                                 'metric': 'auc',\n                               # \"device_type\" : \"gpu\",\n                                'boosting_type': 'gbdt',\n                                #'tree_method': \"gpu_hist\"\n                               }\n\nmodel_lgbm2 = lgbm.LGBMClassifier(**lgbm_param2,\n                              # use it  only on gpu\n                            # rsm=np.nan\n                              #cat_features=cat_features\n                             )\npipe1_model_lgbm_woe_powertransformer2 = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',model_lgbm2)\n    ]\n)\n#pipe1_model_lgbm_woe_powertransformer2","4a3032a7":"pipe1_GaussianNB_woe_powertransformer= Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',GaussianNB())\n    ]\n)\n","ac24327c":"###############################################################################\n#                        Best  Pipes                                          #\n###############################################################################\n\npipe1_lgbm_woe_powertransformer93 = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor_woe_powertransformer),\n            ('reducedim',  SelectPercentile( f_classif, percentile=93)),\n            ('classifier',\n             lgbm.LGBMClassifier( \n                #n_jobs=-1,\n                                             \n                                               **lgbm_param2))\n        ]\n    )\n\npipe1_lgbm_woe_powertransformer = Pipeline(\n                                steps=[\n                                    ('preprocessor', preprocessor_woe_powertransformer),\n                                    ('reducedim',  SelectPercentile( f_classif, percentile=98)),\n                                    ('classifier',\n                                     lgbm.LGBMClassifier( \n                                        #n_jobs=-1,\n                                                                    \n                                                                       **lgbm_param2))\n                                ]\n                                )\npipe1_lgbm_woe_powertransformer90 = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor_woe_powertransformer),\n            ('reducedim',  SelectPercentile( f_classif, percentile=90)),\n            ('classifier',\n             lgbm.LGBMClassifier( \n                #n_jobs=-1,\n                                           \n                                               **lgbm_param2))\n        ]\n    )\npipe1_lgbm_woe_powertransformer95 = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor_woe_powertransformer),\n            ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n            ('classifier',\n             lgbm.LGBMClassifier( \n                #n_jobs=-1,\n                                           \n                                               **lgbm_param2))\n        ]\n    )\npreprocessor_woe_powertransformer100 = ColumnTransformer(\n                transformers=[\n                    ('numerical', numeric_transformer, num_columns),\n                   # ('categorical', categorical_transformer, cat_columns)\n                ])\npipe_lgbm_woe_powertransformer100 = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer100),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',\n         lgbm.LGBMClassifier( \n            #n_jobs=-1,\n                                         \n                                           **lgbm_param2))\n    ]\n    ) \n## Pipe_12\n#Define vnum pipeline\nnumeric_transformer_12 = Pipeline(\n                                steps=[\n                                ('imputer', SimpleImputer(strategy='median'\n                                                          ,add_indicator=True)),\n                                #('general_features',FeaturesEngineer()),\n                                ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                                # Create an SelectKBest object to select features with two best ANOVA F-Values\n                                #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                                ('polynominal_features', PolynomialFeatures(degree=2)),\n                                ('kmeans',MiniKmeansTransformerEncoder()),\n                                #('reducedim',  SelectPercentile(f_classif,percentile=90))\n                                ]\n                                )\n\npreprocessor_woe_powertransformer_12 = ColumnTransformer(\n                transformers=[\n                    ('numerical', numeric_transformer_12, num_columns),\n                   # ('categorical', categorical_transformer, cat_columns)\n                ])\npipe1_lgbm_woe_powertransformer_12 = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor_woe_powertransformer_12),\n            ('classifier',\n             lgbm.LGBMClassifier( \n                #n_jobs=-1,\n                                               \n                                               **lgbm_param2))\n        ]\n    )\n# Pipe 2: \n# Cat pipeline\ncategorical_transformer2 = Pipeline(\n                    steps=[\n                        ('imputer', SimpleImputer(strategy='most_frequent',\n                                                  fill_value='missing',\n                                                  add_indicator=True)),\n                        ('encoder',  ce.woe.WOEEncoder()),#(Numerical Input, Categorical Output)\n                        #('sparse_features', SparseInteractions(degree=2)),\n                        #('reducedim',  SelectPercentile( mutual_info_classif, percentile=90))\n\n                    ]\n                    ) \n#Define vnum pipeline\nnumeric_transformer2 = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            #('general_features',FeaturesEngineer()),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                            ('polynominal_features', PolynomialFeatures(degree=2)),\n                            ('kmeans',MiniKmeansTransformerEncoder()),\n                            #('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\n# Features union cat + num \n# WOE+PowerTransformer\npreprocessor_woe_powertransformer2 = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer2, num_columns),\n                ('categorical', categorical_transformer2, cat_columns)\n            ])\npipe2_lgbm_woe_powertransformer = Pipeline(\n        steps=[\n            ('preprocessor', preprocessor_woe_powertransformer2),\n            ('classifier',\n             lgbm.LGBMClassifier( \n                #n_jobs=-1,\n                                              \n                                               **lgbm_param2))\n        ]\n    )\npipe2_model_catboost_woe_powertransformer = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor_woe_powertransformer2),\n         ('reducedim',  SelectPercentile( f_classif, percentile=95)),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        #('Sparseinteractions', SparseInteractions(degree=2)),\n         #('PolynomialFeatures', PolynomialFeatures(degree=2)),\n        #('PCAreducer', PCA(n_components=0.8)),\n        ('classifier',model_catboost)\n    ]\n)","8ff897f8":"###############################################################################\n#                        Best pipes                                           #\n###############################################################################\n\n# Pipe 1\n# Private Score 0.86914 Public Score 0.86245\npipe1_lgbm_woe_powertransformer95 \n# Private Score 0.86911 Public Score 0.86230\npipe_lgbm_woe_powertransformer100  \n# Private Score 0.86903  Public Score 0.86217\npipe1_model_catboost_woe_powertransformer\n#Private Score 0.86875  Public Score 0.86245\npipe1_model_hgbc_woe_powertransformer\n## Pipe_12\n#  Private Score 0.86895  Public Score 0.86231\npipe1_lgbm_woe_powertransformer_12\n# Private Score 0.86898 Public Score 0.86230\npipe2_lgbm_woe_powertransformer\n# Private Score 0.86903  Public Score 0.86215\npipe2_model_catboost_woe_powertransformer  \nbest_pipes=[pipe1_lgbm_woe_powertransformer95,\n            pipe1_lgbm_woe_powertransformer_12,\n            pipe1_model_catboost_woe_powertransformer,\n            #pipe1_model_hgbc_woe_powertransformer,\n            pipe2_lgbm_woe_powertransformer,\n            pipe2_model_catboost_woe_powertransformer\n           ]","77922e8d":"###############################################################################\n#                        Complete mlextend mdoels                             #\n###############################################################################\n\n##### from mlxtend.plotting import plot_confusion_matrix\nfrom mlxtend.classifier import StackingCVClassifier\n\nmlxtend_sclf1 = StackingCVClassifier(classifiers=best_pipes,use_probas=False,\n                            meta_classifier=model_xgbr,cv=10,stratify=True,\n                            random_state=42)\n\nmlxtend_sclf2 = StackingCVClassifier(classifiers=best_pipes,use_probas=True,\n                            meta_classifier=model_xgbr,cv=10,stratify=True,\n                            random_state=42)\n\nmlxtend_sclf3 = StackingCVClassifier(classifiers=best_pipes,use_probas=True,\n                              use_features_in_secondary=True ,\n                              meta_classifier=model_xgbr,cv=10,stratify=True,\n                            random_state=42)\nmlxtend_sclf32 = StackingCVClassifier(classifiers=best_pipes,use_probas=True,\n                              use_features_in_secondary=True ,\n                              meta_classifier=LogisticRegression(),cv=10,stratify=True,\n                            random_state=42)\nmlxtend_sclf4_pipe = StackingCVClassifier(classifiers=best_pipes,use_probas=True,\n                              use_features_in_secondary=True ,\n                              meta_classifier=model_xgbr,cv=10,stratify=True,\n                            random_state=42)\nmlxtend_sclf3","83b30313":"#X=X.iloc[0:10000,:].copy()\n#y=y.iloc[0:10000].copy()","fd34baaf":"###############################################################################\n#                        Define Models                                          #\n###############################################################################\n\nparams1 = {\"random_state\": 42,\n         \"solver\": 'newton-cg',\n         \"max_iter\": 10,\n        #\"meta_classifier__C\": np.logspace(-4, 4, 10),\n        #\"meta_classifier__C\": np.logspace(-4, 4, 10),\n         \"C\": .001,\n         #\"meta_classifier__penalty\":  ['l2', 'l1']\n         \"penalty\":  'l2'\n         }\nparams2={'C': 0.009,\n 'penalty': 'l2',\n 'random_state': 42,\n 'solver': 'liblinear'}\nlr1=LogisticRegression(**params1)\nlr2=LogisticRegression(**params2)\n# Initializing the StackingCV classifier\n# Create list to store classifiers\nclassifiers = {\"pipe1\":pipe1_lgbm_woe_powertransformer95,\n            #\"pipe2\":pipe_lgbm_woe_powertransformer100,\n            \"pipe2\": pipe1_lgbm_woe_powertransformer_12,\n           # \"pipe3\":pipe1_model_catboost_woe_powertransformer,\n           # \"pipe5\":pipe1_model_hgbc_woe_powertransformer,\n            \"pipe3\":pipe2_lgbm_woe_powertransformer,\n            \"pipe4\":pipe2_model_catboost_woe_powertransformer\n           }\nclassifier_combo1=[value for value in classifiers.values()]\n#classifier_combo1","9912a9a8":"###############################################################################\n#                       Stacking Models                                       #\n###############################################################################\n\nsclf1 = StackingCVClassifier(classifiers = classifier_combo1,\n                                shuffle = False,\n                                use_probas = True,\n                                stratify=True,\n                                cv = 10,\n                                meta_classifier =lr1 ,\n                                n_jobs = -1)\nsclf2 = StackingCVClassifier(classifiers = classifier_combo1,\n                                shuffle = False,\n                                use_probas = True,\n                                stratify=True,\n                                cv = 10,\n                                meta_classifier =lr2 ,\n                                n_jobs = -1)","43093082":"###############################################################################\n#                        Final Pipe                                          #\n###############################################################################\n\n#Define vnum pipeline\nnumeric_transformer_final = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            #('general_features',FeaturesEngineer()),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                            ('kmeans',MiniKmeansTransformerEncoder()),\n                            ('polynominal_features', PolynomialFeatures(degree=2)),\n                            ('reducedim',  SelectPercentile(f_classif,percentile=95)),\n                            #('scaler_minmax',MinMaxScaler())\n                            ]\n                            )\n# Features union cat + num \n# WOE+PowerTransformer\npreprocessor_woe_powertransformer_final = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer_final, num_columns),\n               # ('categorical', categorical_transformer, cat_columns)\n               \n            ])\npreprocessor_woe_powertransformer_final","a5f10b72":"preprocessor_woe_powertransformer_final","0c2ce821":"%%time \n###############################################################################\n#                         Undersample                      #\n###############################################################################\n\nundersampler_enn = make_pipeline(\n            #MinMaxScaler(),\n            EditedNearestNeighbours(\n        sampling_strategy='auto',\n        n_neighbors=3,\n        kind_sel='all',\n        n_jobs=4),\n           # model_lgbm2,\n    #lr1\n       )\nundersampler_enn1 = make_pipeline(\n            #MinMaxScaler(),\n            EditedNearestNeighbours(\n        sampling_strategy='majority',\n        n_neighbors=3,\n        kind_sel='all',\n        n_jobs=4),\n           # model_lgbm2,\n    #lr1\n)\nundersampler_oss = make_pipeline(\n          #  MinMaxScaler(),\n          OneSidedSelection(\n        sampling_strategy='auto',\n        random_state=42,\n        n_neighbors=1,\n        n_jobs=-1),\n           # model_lgbm2,\n    #lr1\n       )\nundersampler_tomek = make_pipeline(\n          #  MinMaxScaler(),\n          TomekLinks(\n        sampling_strategy='auto',\n        n_jobs=-1),\n           # model_lgbm2,\n    #lr1\n       )\n\n###############################################################################\n#                         Oversample                                          #\n###############################################################################\n\noversample_random= RandomOverSampler(\n        sampling_strategy='auto',\n        random_state=42)\n\noversample_smote= SMOTE(\n        sampling_strategy='auto',  # samples only the minority class\n        random_state=42,  # for reproducibility\n        k_neighbors=5,\n        n_jobs=4)\n\noversample_adasyn= ADASYN(\n        sampling_strategy='auto',  # samples only the minority class\n        random_state=42,  # for reproducibility\n        n_neighbors=5,\n        n_jobs=4)\n\noversample_border1= BorderlineSMOTE(\n        sampling_strategy='auto',  # samples only the minority class\n        random_state=42,  # for reproducibility\n        k_neighbors=5,\n        m_neighbors=10,\n        kind='borderline-1',\n        n_jobs=4)\n\noversample_border2= BorderlineSMOTE(\n        sampling_strategy='auto',  # samples only the minority class\n        random_state=42,  # for reproducibility\n        k_neighbors=5,\n        m_neighbors=10,\n        kind='borderline-2',\n        n_jobs=4)\n\noversample_svm= SVMSMOTE(\n        sampling_strategy='auto',  # samples only the minority class\n        random_state=42,  # for reproducibility\n        k_neighbors=5,\n        m_neighbors=10,\n        n_jobs=4,\n        svm_estimator=svm.SVC(kernel='linear'))\n###############################################################################\n#                         Cost-Sensitive                                         #\n###############################################################################\n# Not balanced lgbm2 \n# Balanced \nmodel_lgbm_balanced = lgbm.LGBMClassifier(**lgbm_param2,\n                              # use it  only on gpu\n                            # rsm=np.nan\n                              #cat_features=cat_features\n                            class_weight='balanced' # weights \/ cost\n                             )\n# Manual wieght : \nmodel_lgbm_manual = lgbm.LGBMClassifier(**lgbm_param2,\n                              # use it  only on gpu\n                            # rsm=np.nan\n                              #cat_features=cat_features\n                           class_weight={0:1, 1:10}) # weights \/ cost\n# Sample weights : \nscale_pos_weight1 = 139974\/(10026)\nscale_pos_weight2_smooth = np.sqrt(139974\/(10026))                           \n###############################################################################\n#                         Preprocess                                          #\n###############################################################################\npreprocessor_woe_powertransformer_final.fit(X,y)\nx_pre = preprocessor_woe_powertransformer_final.transform(X)\nx_test_final_pre = preprocessor_woe_powertransformer_final.transform(X_test_final)\n#x_rus, y_rus = undersampler_enn.fit_resample(x_pre, y)\n#x_rus, y_rus = undersampler_enn1.fit_resample(x_pre, y)\n#x_rus, y_rus = undersampler_oss.fit_resample(x_pre, y)\n#x_rus, y_rus = undersampler_tomek.fit_resample(x_pre, y)\n    \n# x_rus, y_rus = oversample_svm.fit_resample(x_pre, y)\n#x_rus, y_rus = oversample_border2.fit_resample(x_pre, y)\n#x_rus, y_rus = oversample_border1.fit_resample(x_pre, y)# not done wihthoutminmax\n#x_rus, y_rus = oversample_adasyn.fit_resample(x_pre, y)\n#x_rus, y_rus = oversample_smote.fit_resample(x_pre, y)\nx_rus, y_rus = oversample_random.fit_resample(x_pre, y)\n###############################################################################\n#                         Preprocess    Result                                #\n###############################################################################\nprint('original dataset shape:', y.shape)\n#print('Resample dataset shape', y_rus.shape)\nprint('original dataset shape:', X.shape)\nprint('Train dataset shape', x_pre.shape)\nprint('Test dataset shape', x_test_final_pre.shape)\n\n#del x_final\n#del x_test_final","dea3fc9b":"##############################################################################\n#                         Preprocess                                          #\n###############################################################################\n\nx_rus, y_rus = oversample_random.fit_resample(x_pre, y)\n###############################################################################\n#                         Preprocess    Result                                #\n###############################################################################\nprint('original dataset shape:', y.shape)\n#print('Resample dataset shape', y_rus.shape)\nprint('original dataset shape:', X.shape)\nprint('Train dataset shape', x_pre.shape)\nprint('Test dataset shape', x_test_final_pre.shape)\n\n#del x_final\n#del x_test_final","7ed4de59":"###############################################################################\n#                         Shape before sampling                               #\n###############################################################################\n\nunique, counts = np.unique(y, return_counts=True)\nprint( np.asarray((unique, counts)).T)","b5d53a4d":"###############################################################################\n#                         Shape after  sampling                               #\n###############################################################################\n\nunique, counts = np.unique(y_rus, return_counts=True)\nprint( np.asarray((unique, counts)).T)","cbd809e5":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy='auto')\nX_sm, y_sm = smote.fit_resample(x_pre, y)\ndf = pd.DataFrame(X_sm)\ndf['target'] = y_sm\n\ndf['target'].value_counts().plot(kind='bar', title='Count (target)')","d93ff649":"%%time \ndef plot_roc(y,y_pred):\n    from sklearn.metrics import auc\n    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_pred)\n    auc = auc(fpr, tpr)\n    plt.figure(1)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.plot(fpr, tpr, label='Model(area = {:.3f})'.format(auc))\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc='best')\n    plt.show()","a7a09622":"%%time \n###############################################################################\n#                         CV                                                 #\n###############################################################################\n\n# evaluate each strategy on the dataset\nresults = list()\n# Setting a 10-fold stratified cross-validation (note: shuffle=True)\nSEED = 42\nFOLDS = 10\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n# CV interations\n# Create arrays for the features and the response variable\nroc_auc = list()\naverage_precision = list()\n#X1=X.iloc[0:2000,:].copy()\n#y1=y[0:2000].copy()\noof = np.empty((X.shape[0],))\n#oof = np.empty((x_rus.shape[0],))\n#oof_bin = np.empty((X.shape[0],))\npredictions=[]\nmean_auc = 0\nmean_ap=0\nF1 = list()\nRecall=list()\nbest_iteration = list()\nfor fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):    \n    X_train, y_train =X.iloc[list(train_idx),:], y[list(train_idx)]\n    X_test, y_test = X.iloc[list(test_idx),:],y[list(test_idx)]\n    pipe1_model_lgbm_woe_powertransformer2.fit(X_train,y_train,\n                                                  #classifier__verbose=False\n                                                  #,verbose=False\n              #sample_weight=np.where(y_train==1,1,4)\n              #sample_weight=np.where(y_train==1,4,1)\n              #sample_weight=np.where(y_train==1,14,1)      \n                                                 )\n    preds = pipe1_model_lgbm_woe_powertransformer2.predict_proba(X_test)[:,1]\n    oof[test_idx] = preds\n    auc_score= roc_auc_score(y_true=y_test, y_score=preds)\n    roc_auc.append(auc_score)\n    ap=average_precision_score(y_true=y_test, y_score=preds)\n    plot_roc(y_test,preds)\n    mean_auc += auc_score \/ FOLDS\n    mean_ap+=ap\/FOLDS\n    predictions.append(pipe1_model_lgbm_woe_powertransformer2.predict_proba(X_test_final)[:,1]) \n    y_predicted =pipe1_model_lgbm_woe_powertransformer2.predict(X_test)\n    recall = recall_score(y_test, y_predicted)\n    f1= f1_score(y_test, y_predicted)\n    print('Confusion_matrix:\\n',confusion_matrix(y_test,y_predicted))\n    print(f\"Fold {fold} | AUC: {auc_score}\")\n    print(f\"Fold {fold} | F1: {f1}\")\n    print(f\"Fold {fold} | Avergae_precesion: {ap}\")\n    print(f\"Fold {fold} | recall: {recall}\")\n    print('Classification report:\\n',classification_report(y_test,y_predicted))\n    roc_auc.append(auc_score)\n    F1.append(f1)\n    Recall.append(recall)\n    #Mean of the predictions\nprint('-' * 80)  \nprint(f\"\\nOverall mean AUC score : {mean_auc}\")\nprint(f\"\\nOverall mean F1 score : {np.mean(F1)}\")\nprint(f\"\\nOverall mean average precision score : {mean_ap}\")\nprint(f\"\\nOverall mean recall score : {np.mean(Recall)}\")","e530d204":"'''\nUse shap to build an a explainer.\n:parameter\n    :param model: model instance (after fitting)\n    :param X_names: list\n    :param X_instance: array of size n x 1 (n,)\n    :param X_train: array - if None the model is simple machine learning, if not None then it's a deep learning model\n    :param task: string - \"classification\", \"regression\"\n    :param top: num - top features to display\n:return\n    dtf with explanations\n'''\ndef explainer_shap(model, X_names, X_instance, X_train=None, task=\"classification\", top=10):\n    ## create explainer\n    ### machine learning\n    if X_train is None:\n        explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(X_instance)\n    ### deep learning\n    else:\n        explainer = shap.DeepExplainer(model, data=X_train[:100])\n        shap_values = explainer.shap_values(X_instance.reshape(1,-1))[0].reshape(-1)\n\n    ## plot\n    ### classification\n    if task == \"classification\":\n        shap.decision_plot(explainer.expected_value, shap_values, link='logit', feature_order='importance',\n                           features=X_instance, feature_names=X_names, feature_display_range=slice(-1,-top-1,-1))\n    ### regression\n    else:\n        shap.waterfall_plot(explainer.expected_value[0], shap_values, \n                            features=X_instance, feature_names=X_names, max_display=top)","1428fe7e":"###############################################################################\n#                         Preprocess                                          #\n###############################################################################\nimport shap\npreprocessor_woe_powertransformer_final.fit(X,y)\nx_pre = preprocessor_woe_powertransformer_final.transform(X)\nx_test_final_pre = preprocessor_woe_powertransformer_final.transform(X_test_final)\nmodel_lgbm2.fit(x_pre,y)\ni = 1\nlist_feature_names =['num_'+str(i) for i in range(int(x_pre.shape[1])) ]\n","a7d08ffd":"f, ax = plt.subplots(figsize=(6, 15))\nfeature_importances = (model_lgbm2.feature_importances_ \/ sum(model_lgbm2.feature_importances_)) * 100\nresults = pd.DataFrame({'Features': list_feature_names,\n                        'Importances': feature_importances})\nresults.sort_values(by='Importances', inplace=True)\n\nax = plt.barh(results['Features'], results['Importances'])\nplt.xlabel('Importance percentages')\nplt.show()\n#########################################","12f1fbbc":"df_feature_importance = (\n    pd.DataFrame({\n        'feature': list_feature_names,\n        'importance': model_lgbm2.feature_importances_,\n    })\n    .sort_values('importance', ascending=False)\n)\ndf_feature_importance","be4298c5":"# best link : https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d\n#https:\/\/www.kaggle.com\/dansbecker\/shap-values\n\nx_pre_df = pd.DataFrame(data=x_pre[0:500],columns=list_feature_names)\nexplainer = shap.TreeExplainer(model_lgbm2)\nshap_values = explainer.shap_values(x_pre_df)\nexplainer.expected_value","674a1b26":"shap.summary_plot(shap_values [1],x_pre_df, max_display=10)","8a563644":"# Get the index of elements with value 1\nresult = np.where(y == 0)\nfraudvalue=result[0][0]\nx_pre_df.iloc[fraudvalue,:][0:2]","1f0132e3":"print(\"Test data (actual observation): {}\".format(y[fraudvalue]))\npreds=model_lgbm2.predict(x_pre[fraudvalue].reshape(1, -1))\nprint(\"Model's prediction: {}\".format(preds))","96b4ba9d":"# print the JS visualization code to the notebook\nshap.initjs()\nshap.force_plot(explainer.expected_value[1],\n                shap_values[1][fraudvalue,:], \n                x_pre_df.iloc[fraudvalue,:],link='logit'\n)","6f5d474f":"shap.decision_plot(explainer.expected_value[1]\n                   , shap_values[1][fraudvalue,:], x_pre[fraudvalue],\n                   link='logit', highlight=0,feature_names=list_feature_names)","cca48c8c":"shap.summary_plot(shap_values, x_pre, plot_type='bar')","1f55d046":"def ABS_SHAP(df_shap,df):\n    #import matplotlib as plt\n    # Make a copy of the input data\n    shap_v = pd.DataFrame(df_shap)\n    feature_list = df.columns\n    shap_v.columns = feature_list\n    df_v = df.copy().reset_index().drop('index',axis=1)\n    \n    # Determine the correlation in order to plot with different colors\n    corr_list = list()\n    for i in feature_list:\n        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n        corr_list.append(b)\n    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n    corr_df.columns  = ['Variable','Corr']\n    corr_df['Sign'] = np.where(corr_df['Corr']>0,'red','blue')\n    \n    # Plot it\n    shap_abs = np.abs(shap_v)\n    k=pd.DataFrame(shap_abs.mean()).reset_index()\n    k.columns = ['Variable','SHAP_abs']\n    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n    colorlist = k2['Sign']\n    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(5,6),legend=False)\n    ax.set_xlabel(\"SHAP Value (Red = Positive Impact)\")\n    \n#ABS_SHAP(shap_values,x_pre[0:200])","96e7b9c7":"predictions = np.mean(np.column_stack(predictions), axis=1)","82863795":"# Save the predictions to a CSV file\nsub = pd.read_csv('..\/input\/GiveMeSomeCredit\/sampleEntry.csv')\nsub['Probability']=predictions\nsub.to_csv('sclf1.csv', index=False)\nsub","9f924551":"**4-lgbm:**","67209389":"# Complete Pipe \n\n1. Features Engineer\n\n1. Kmeans\n\n1. Sparse\n\n1. Poly\n\n1. Scaler\/Transformer\n\n1. imput\n\n1. Add bin data \n\n## Pipe : \n","ec252426":"# Final predictions","1cd01cf9":"# Best Pipes : \n","c5941323":"# Selected Models :","cd647b4d":"## Final X and y","82e80ec4":"# UnderSample : \n    all data: \n\n    model_lgbm2 roc_auc: 0.8625688276997037 +\/- 0.0035080236891726173\n   \n    \n    enn\n\n    model_lgbm2 roc_auc: 0.8630909788287942 +\/- 0.0038026055124852757\n    \n    oss\n\n    model_lgbm2 roc_auc: 0.8627229021333344 +\/- 0.003784828443950451\n    \n    tomek\n\n    model_lgbm2 roc_auc: 0.8627041498279308 +\/- 0.003584230592089494\n    \n    ncr\n\n    model_lgbm2 roc_auc: 0.8621691036387884 +\/- 0.003973914786246707\n    \n    renn\n\n    model_lgbm2 roc_auc: 0.8621382365638033 +\/- 0.003615991141402151\n    \n    allknn\n\n    model_lgbm2 roc_auc: 0.8621110226669315 +\/- 0.0040351051926343535\n    \n\n\n    random\n\n    model_lgbm2 roc_auc: 0.858181429188133 +\/- 0.004612587761779285\n\n\n    iht\n\n    model_lgbm2 roc_auc: 0.7991996119828295 +\/- 0.010744213905743065\n    \n    \n    \n # OverSample   \n \n\n    model_lgbm2 roc_auc: 0.8627374534105787 +\/- 0.002681755319299279\n\n    random\n    model_lgbm2 roc_auc: 0.8627941543070008 +\/- 0.0031255721275641867\n    \n    svm\n    model_lgbm2 roc_auc: 0.8581355858562725 +\/- 0.0032732419156711657\n\n    border2\n    model_lgbm2 roc_auc: 0.8580089216588368 +\/- 0.0031155380536501227\n\n    border1\n    model_lgbm2 roc_auc: 0.8575183488597625 +\/- 0.0033858176289283228\n\n    smote\n    model_lgbm2 roc_auc: 0.8572738779017686 +\/- 0.003559254566375173\n\n    adasyn\n    model_lgbm2 roc_auc: 0.8562332256549373 +\/- 0.003528177645756943\n\n\n\n\n\n\n","f644ebec":"# 1-Preprocessing","4d2be76c":"# Final Pipe : ","2310afab":"## Reduce Memory","5c148d65":"### Num Features\n\n","fbd9d797":"**-3 XGBR**","d9e4b4cd":"## Convert Dtypes :","d11ab422":"# FE","062fd824":"## Imput before Descritaziation : \n\n### IterativeImputer\n","676cf926":"## X and Y","00a89703":"The red features in the force plot drives our prediction to be 1 ","a6abaf3b":"\n# Load the data","a281e60a":"**2-CatBoost**","5f712019":"# Explanation : "}}