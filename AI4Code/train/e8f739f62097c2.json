{"cell_type":{"64153aaa":"code","72922a79":"code","75a542ec":"code","af60fb7b":"code","410782a7":"code","3c649d51":"code","9051976d":"code","4f40595d":"code","9c205bb8":"code","4d7b7fba":"code","cc32ca2d":"code","5e8de12b":"code","f9da9dc4":"code","bd8a4a34":"code","7d0a3e9e":"code","0ea69fd3":"code","5e94107f":"code","dde6b6ca":"code","5524c5fa":"markdown","9e1abe26":"markdown","7b02288d":"markdown","aa0bb0e3":"markdown","a598677f":"markdown","e930cd6c":"markdown","68312816":"markdown","6bc6329f":"markdown","c436eda1":"markdown","9cc475ee":"markdown","bc78f3c3":"markdown","6e18f1db":"markdown","b4caefcb":"markdown","ce4870f9":"markdown","47246f10":"markdown","191f8dbf":"markdown","806c30fa":"markdown","e76c1901":"markdown","a942a443":"markdown","125faf01":"markdown","8d04415d":"markdown","6c17f629":"markdown","784e7919":"markdown"},"source":{"64153aaa":"import numpy as np\nimport pandas as pd\nimport os\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom numpy.random import seed\nseed(17)","72922a79":"train_raw = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/train.json.zip').reset_index(drop = True)\ntest_raw = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/test.json.zip').reset_index(drop = True)\nfull_raw = pd.concat([train_raw, test_raw])\n\nprint(train_raw.shape, test_raw.shape, full_raw.shape)\ntrain_raw.head(1)","75a542ec":"#Drop unused\nfull = full_raw.drop(['building_id', 'listing_id'], axis=1)\n\n#Extract from list\nfull['features'] = [','.join(map(str, i)) for i in full['features']]\n\n#Convert dtypes\nfull[['bathrooms', 'bedrooms']] = full[['bathrooms', 'bedrooms']].astype(int)\n\n#Extract day\/month and drop original date\nfull['day'] = pd.to_datetime(full.created).dt.day.astype('object')\nfull['month'] = pd.to_datetime(full.created).dt.day.astype('object')\nfull = full.drop('created', axis=1)\n\n#Count web links\nfull['photos'] = full.photos.apply(len)\n\n#Round price\nfull.price = full.price \/\/ 50 * 50\nfull.loc[full.price > 10000, 'price'] = full.loc[full.price > 10000, 'price'] \/\/ 500 * 500\n\n#Replace classes by labels\nfull = full.replace({'interest_level' : { 'high' : 0, 'medium' : 1, 'low' : 2 }})","af60fb7b":"#Columns to clean text\ncols_text = ['description', 'display_address', 'street_address', 'features']\n#To string\nfull[cols_text] = full[cols_text].astype(str)\n#To lower case\nfull[cols_text] = full[cols_text].apply(lambda x: x.str.lower())\n\n#Remove punctuation\nwith_whitespace = ['&', '(', ')', \"-\", \"_\", ':', '=', '\"', ',']\nwith_empty = ['.', \"'\", '`', '!', '*', '#', '\/', '<', '>', 'br',\n              ';', '$', '%', '|', '+', '?']\n\n\ndef replace_symbol(df, to_replace, replace_by):\n    for symbol in to_replace:\n        df = df.apply(lambda x: x.str.replace(symbol, replace_by, regex = True)) \n    return df\n\nfull[cols_text] = replace_symbol(full[cols_text], with_whitespace, ' ')\nfull[cols_text] = replace_symbol(full[cols_text], with_empty, '')","410782a7":"adr_feat = ['display_address', 'street_address']\n\n#Correct 'street' and 'avenue' cuts\nfull[adr_feat] = full[adr_feat].replace(['\\sst\\s', '\\sst$'], ' street', regex = True)\nfull[adr_feat] = full[adr_feat].replace(['\\save\\s', '\\save$'], ' avenue', regex = True)\n\n#Correct 'east' and 'west' cuts\nfull[adr_feat] = full[adr_feat].replace(['\\se\\s', '^e\\s'], ' east ', regex = True)\nfull[adr_feat] = full[adr_feat].replace(['\\sw\\s', '^w\\s'], ' west ', regex = True)\nfull[adr_feat].sample()","3c649d51":"desc_feat = ['description', 'features']\n#web links\nfull[desc_feat] = full[desc_feat].replace('\\swww.\\S*', ' weblink ', regex = True)\n#emails\nfull[desc_feat] = full[desc_feat].replace('\\s\\S*@\\S*', ' emailaddress', regex = True)\n#time\nfull[desc_feat] = full[desc_feat].replace('\\s\\d{1,2}\\s\\d\\d[ap]m', ' ampmtime', regex = True)\n#phone numbers\nfull[desc_feat] = full[desc_feat].replace('\\s\\d{2,4}\\s\\d{2,4}\\s\\d{2,4}', ' phonenumber', regex = True)","9051976d":"#Reduce multiple whitespaces\nfull[cols_text] = full[cols_text].replace('\\s+', ' ', regex = True)\n#Trim leading and tailing whitespaces\nfull[cols_text] = full[cols_text].replace(['^\\s', '\\s$'], '', regex = True)","4f40595d":"X = full.iloc[:-1 * len(test_raw)].drop('interest_level', axis=1)\ny = full.iloc[:-1 * len(test_raw)].interest_level\ntest = full.iloc[-1 * len(test_raw):].drop('interest_level', axis=1)\nprint(full.shape, '->', X.shape, y.shape, test.shape)","9c205bb8":"X_train, X_valid, y_train, y_valid =  train_test_split(\n                                      X, y, test_size=0.2, stratify=y, random_state=17)\n\ncat_features = ['day', 'month', 'manager_id', 'display_address']\ntext_features = ['description','street_address' , 'features']\n\nTrain = Pool(data=X_train,\n             label=y_train,\n             cat_features=cat_features,\n             text_features = text_features)\n            \nValid = Pool(data=X_valid,\n             label=y_valid,\n             cat_features=cat_features,\n             text_features = text_features)","4d7b7fba":"model = CatBoostClassifier( random_seed = 17,     \n                            thread_count = -1, \n                            verbose = 100,  \n                            loss_function='MultiClass',\n                            task_type = \"GPU\" )\n# Fit model\nmodel.fit(Train, eval_set=Valid)\npreds_class = model.predict(Valid)\npreds_proba = model.predict_proba(Valid)","cc32ca2d":"FE = model.get_feature_importance(data=Valid,\n                       thread_count=-1,\n                       verbose=False)\nFEG = pd.DataFrame(FE, index = X_valid.columns ).sort_values(0, ascending = False)\nFEG.plot.bar(figsize = (20,5), rot = 0)","5e8de12b":"result = pd.DataFrame()\nresult['Real'] = y_valid.values\nresult['Pred'] = preds_class[:,0]\n\nfor i in range(3):\n    print('Accuracy for Class', str(i), ':', \"%.4f\" %\n          (result.loc[result.Real == i, 'Real'] ==\n           result.loc[result.Real == i, 'Pred']).mean())\n    print('Class', str(i), 'in observations:', \"%.0f\" %\n          (result.loc[result.Real == i].shape[0] \/ len(result) * 100),\n          '%', \"\\n\")\nprint ('Mean accuracy:',  \"%.4f\" % (result.Real == result.Pred).mean(), \"\\n\")\nfor i in range(3):\n    print('Class', str(i), 'observations count:', \"%.0f\" % y[y==i].shape[0])","f9da9dc4":"model.get_all_params()","bd8a4a34":"#Generate new pool from all train data\nTest = Pool(data=test,\n            cat_features=cat_features,\n            text_features = text_features)\n\npreds_proba = model.predict_proba(Test)\npredictions = pd.DataFrame(preds_proba)\n\nsub = pd.read_csv('..\/input\/two-sigma-connect-rental-listing-inquiries\/sample_submission.csv.zip')\n\nsub['high'] = predictions[0]\nsub['medium'] = predictions[1]\nsub['low'] = predictions[2]\n\nsub.to_csv('submission_baseline.csv', index = False)","7d0a3e9e":"Train_gs = Pool(data=X,\n             label=y,\n             cat_features=cat_features,\n             text_features = text_features)\n\nmodel_gs = CatBoostClassifier(random_seed = 17,     \n                            thread_count = -1, \n                            verbose = 1000,  \n                            loss_function='MultiClass',\n                            task_type = \"GPU\",\n                            )\nparams_gs = {\n            'iterations': [10000],    #  [1000,2500,5000, 10000]\n            #'learning_rate': [0.01, 0.1, 0.15, 0.3, 0.5], \n            #'auto_class_weights': ['None', 'Balanced', 'SqrtBalanced']\n            #'depth': [4, 6, 8, 10]\n            #'l2_leaf_reg': [2,3,4]\n            #'min_data_in_leaf': [1, 2, 3,4]\n            }\ngs_result = model_gs.grid_search(params_gs, \n                              Train_gs, \n                              partition_random_seed = 17,\n                              stratified = True,\n                              verbose = 1000,\n                              plot=False)","0ea69fd3":"model_gs.get_all_params()","5e94107f":"Test = Pool(data=test,\n            cat_features=cat_features,\n            text_features = text_features)\n\npreds_proba_gs = model_gs.predict_proba(Test)\npredictions_gs = pd.DataFrame(preds_proba_gs)\n\nsub_gs = pd.read_csv('..\/input\/two-sigma-connect-rental-listing-inquiries\/sample_submission.csv.zip')\n\nsub_gs['high'] = predictions_gs[0]\nsub_gs['medium'] = predictions_gs[1]\nsub_gs['low'] = predictions_gs[2]\n\nsub_gs.head()\nsub_gs.to_csv('submission_final.csv', index = False)","dde6b6ca":"# !pip uninstall googletrans -y\n# !pip install googletrans==4.0.0rc1\n# import googletrans\n# from googletrans import Translator\n\n# data = X_train.join(y_train)\n\n# data_0 = data[data.interest_level == 0]\n# data_1 = data[data.interest_level == 1]\n\n# data_0_aug = data_1_aug = pd.DataFrame()\n\n# data_0_aug = pd.concat([data_0]*(Class_0_add-1))\n# data_1_aug = pd.concat([data_1]*(Class_1_add-1))\n\n# data_aug = pd.concat([data_0_aug, data_1_aug]).reset_index(drop = True)\n# for i in range(3):\n#     print('Class', str(i), 'count:', \"%.0f\" % data_aug[data_aug.interest_level == i].shape[0])\n\n\n# length = data_aug.shape[0]\n\n# rand_price = np.random.uniform(0.98, 1.02, length)\n# rand_long = np.random.uniform(0.999, 1.001, length)\n# rand_lat = np.random.uniform(0.9995, 1.0005, length)\n\n# data_aug['price'] = (data_aug['price'] * rand_price).astype(int) \/\/ 10 * 10\n# data_aug['longitude'] = round(data_aug['longitude'] * rand_long,4)\n# data_aug['latitude'] = round(data_aug['latitude'] * rand_lat,4)\n\n# train_new = pd.concat([data, data_aug])\n# print('New train set:')\n# for i in range(3):\n#     print('Class', str(i), 'count:', \"%.0f\" % train_new[train_new.interest_level == i].shape[0])\n\n\n# #*****Text Data distortion*****\n\n# languages = list(googletrans.LANGUAGES.keys())\n# def text_aug (text):\n#     rand = np.random.randint(0, len(languages))\n#     translated = translator.translate(text, dest = languages[rand])\n#     return str.lower(translator.translate\n#                     (translated.text, dest = 'en').text)\n\n# # Can be used only column by column, like this:\n# for col in df.columns:\n#     df[col] = df[col].apply(text_aug)","5524c5fa":"Common things for all text features","9e1abe26":"#### Grid search","7b02288d":"#### Augmentation (optional reading)\nOne of the ways to manage imbalanced data is data augmentation - adding synthetic data for minor classes. The issue is that in this competition you need to augment text data as well, which takes a lot of time, which I did not have. *(in my algorithm, the augmentation of 10 text cells took about a minute due to google translate lag and slow pandas.apply() method)*<br>\nBy the way  in the hidden cell you can find my algorithm, maybe it will be useful for your experiments. The algorithm uses translation to random language with further retranslation to English. I tried augmenting the numeric data only, but it expectedly worsened the situation.","aa0bb0e3":"### Text cleaning","a598677f":"Trim leading, tailing and multiple whitespaces","e930cd6c":"Here is the **main problem**: the data set is very imbalanced, this is why scores for minority classes is too low.<br>\nOne of possible solution is to set weights for classes, I will try it in grid search.","68312816":"Final submission score is 0.55789.<br>\n**Thank you for your attention !**","6bc6329f":"Text cleaning in ```description``` and ```features```","c436eda1":"Split back concatenated dataframe to train \/ test and additionally split train for ```X``` and ```y```","9cc475ee":"Baseline model score on validation was 0.5593, when I made submissions I got 0.56646 in private LB.<br>\nIt is not good score compare to others...","bc78f3c3":"### Catboost","6e18f1db":"#### Baseline model predictions","b4caefcb":"After a quick vivew at the data I decided to make the following features transformations:\n\n\n```bathrooms```       convert to int<br>\n```bedrooms```        no changes<br>\n```building_id```     drop<br>\n```created```         extract day and month<br>\n```description```     clean text<br>\n```display_address``` clean text<br>\n```features```        extract from list and clean text<br>\n```latitude```        no changes<br>\n```listing_id```      drop<br>\n```longitude```       no changes<br>\n```manager_id```      no changes<br>\n```photos```          count links (as images itself are not availble)<br>\n```price```           round<br>\n```street_address```  drop<br>\n```interest_level```  convert to labels\n\nI do not care about outliers and frequency encoding as Catboost has perfect built-in [algorithms](https:\/\/catboost.ai\/en\/docs\/concepts\/algorithm-main-stages_cat-to-numberic) for this, especially for GPU.<br>And the best thing is that it process [text](https:\/\/catboost.ai\/en\/docs\/concepts\/algorithm-main-stages_text-to-numeric) data automatically as well, it makes life much more easier. The only thing I have to do is to set feature types.<br>\n*Honestly I tried to remove outliers manually, but result was worse...*","ce4870f9":"### Content\n1. Data loading\n1. Defining pipeline\n1. Playing with features\n1. Catboost baseline\n1. Catbost grid search\n1. Augmentation (not used)","47246f10":"### Process data","191f8dbf":"Here are my trying with the grid search. Although it is possible to use sklearn GridSearchCV with Catboost models I kindly recommend you to use built-in ```grid_search``` as it works much faster. The main reason that built-in module support GPU.\nTo save notebook commiting time I kept only the best parameters, but you can see all other values I played with commented.<br>\nMy major disappointment is that ```auto_class_weights``` did not work as I hoped. Class manual setting (Catboost allows to do this) dicreased scores as well.<br>\nActually the main thing I could play with is number of trees, all other parameters Catboost picks up well automatically, depending on what I set manually.","806c30fa":"#### Final model predictions and submissions","e76c1901":"#### Baseline\nSimple out-of-the-box Catboost model.<br>\nThere is not log_loss **metric** in Catboost, but as it has the same nature as MultiClass **loss_function** I didn't set any metric and left it by default.","a942a443":"Text cleaning in ```display_address``` and ```street_address```","125faf01":"#### Validation scores","8d04415d":"#### Feature importances\nYou can see how it was imprtant to manage text features","6c17f629":"## Two Sigma Connect: Rental Listing Inquiries\n\nI got this exercise as a hiring test.<br>\nThe limitations I have:\n* images are not availble anymore (no peers)\n* for some resons I had only two days\n\nConsidering the above I decided to use Catboost as multipurpose tool with powerful built-in modules for categorical and text data preprocessing.","784e7919":"Train \/ validation split and create data pools.<br>\n*[Pool](https:\/\/catboost.ai\/en\/docs\/concepts\/python-reference_pool) is a special Catboost data constructor that increases training performance*<br>\nAs you can see, I set ```display_address``` as a category and the ```street_address``` as a text. Experiments have demonstrated that this was the right approach."}}