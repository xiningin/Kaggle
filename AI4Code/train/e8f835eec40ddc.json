{"cell_type":{"408f04a5":"code","7fe32970":"code","fdb15e83":"code","ef1fc6fa":"code","90c61294":"code","af6e4606":"code","cdd80360":"code","7602fde7":"code","800f5ae1":"code","82fdf62a":"code","110146cc":"code","c34a712a":"markdown","1821fed2":"markdown"},"source":{"408f04a5":"!cp ..\/input\/bert_files\/* .","7fe32970":"import collections\nimport os\nimport re\nimport pandas as pd\nimport modeling\nimport optimization\nimport tokenization\nimport tensorflow as tf\ntf.reset_default_graph()","fdb15e83":"df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n# Remove next line for inference on whole set\ndf = df[:1000]","ef1fc6fa":"class FLAGS(object):\n    bert_config_file = '..\/input\/bert_files\/bert_config.json'\n    vocab_file = '..\/input\/bert_files\/vocab.txt'\n    model_dir = \"..\/input\/toxic1\"\n    do_lower_case = True,\n    max_seq_length = 128\n\nclass InputFeatures(object):\n    def __init__(self, input_ids, input_mask, segment_ids, label_id, is_real_example=True):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id","90c61294":"def convert_single_example(ex_index, text, label_list, max_seq_length, tokenizer):\n\n    tokens_a = tokenizer.tokenize(text)\n\n    if len(tokens_a) > max_seq_length - 2:\n        offset = int(max_seq_length \/ 2) - 1\n        tokens_a = tokens_a[:offset] + tokens_a[-offset:]\n\n    tokens = [\"[CLS]\"] + tokens_a + ['[SEP]']\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_mask = [1] * len(input_ids)\n\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n    segment_ids = [0] * len(input_ids)\n\n    if ex_index < 3:\n        tf.logging.info(\"*** Examples ***\")\n        tf.logging.info(\"tokens: %s\" % \" \".join([tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n\n    feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=0\n    #, is_real_example=True\n    )\n    return feature","af6e4606":"def file_based_convert_examples_to_features(df, label_list, max_seq_length, tokenizer, output_file):\n\n    writer = tf.python_io.TFRecordWriter(output_file)\n    print(len(df))\n    for ex_index in range(len(df)):\n        if ex_index % 5000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(df)))\n        row = df.iloc[ex_index]\n        feature = convert_single_example(ex_index, row['comment_text'], label_list,\n                                         max_seq_length, tokenizer)\n\n        def create_int_feature(values):\n            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n            return f\n\n        features = collections.OrderedDict()\n        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n        features[\"label_ids\"] = create_int_feature([feature.label_id])\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n    writer.close()","cdd80360":"def file_based_input_fn_builder(input_file, seq_length):\n\n    name_to_features = {\n      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n    }\n\n    def input_fn(params):\n        d = tf.data.TFRecordDataset(input_file)\n\n        d = d.apply(\n            tf.contrib.data.map_and_batch(\n                lambda record: tf.parse_single_example(record, name_to_features),\n                batch_size=8,\n                drop_remainder=False))\n        return d\n\n    return input_fn\n","7602fde7":"def create_infer_model(input_ids, input_mask, segment_ids, labels, num_labels):\n\n    model = modeling.BertModel(\n      config=modeling.BertConfig.from_json_file(FLAGS.bert_config_file),\n      is_training=False,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=False)\n\n    output_layer = model.get_pooled_output()\n    hidden_size = output_layer.shape[-1].value\n\n    output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.sigmoid(logits)\n    return probabilities","800f5ae1":"def infer_fn_builder(num_labels):\n\n    def model_fn(features):\n        input_ids = features[\"input_ids\"]\n        input_mask = features[\"input_mask\"]\n        segment_ids = features[\"segment_ids\"]\n        label_ids = features[\"label_ids\"]\n        is_real_example = None\n\n        probabilities = create_infer_model(\n            input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n        output_spec = tf.estimator.EstimatorSpec(\n            predictions={\"probabilities\": probabilities},\n            mode='infer'\n        )\n        return output_spec\n\n    return model_fn","82fdf62a":"import time\ntic = time.time()\ntf.logging.set_verbosity(tf.logging.INFO)\nlabel_list = [\"0\", \"1\"]\n\ntokenizer = tokenization.FullTokenizer(\n  vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\nmodel_fn = infer_fn_builder(num_labels=len(label_list))\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=tf.estimator.RunConfig(model_dir=FLAGS.model_dir)\n)\n\nif not os.path.isfile(\"predict.tf_record\"):\n    file_based_convert_examples_to_features(df, label_list, FLAGS.max_seq_length, \n                                            tokenizer, \"predict.tf_record\")\n\npredict_input_fn = file_based_input_fn_builder(\"predict.tf_record\", FLAGS.max_seq_length)\n\nresult = estimator.predict(input_fn=predict_input_fn)\n\npredictions = []\nfor pred in result:\n    predictions.append(pred['probabilities'])\n\nprint(f'{len(predictions)} records done in {time.time() - tic}s')\nprint(predictions[:10])","110146cc":"out = pd.DataFrame(predictions)\nout.columns = ['civil','toxic']\ndf['prediction'] = out['toxic']\nsubmission = df[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","c34a712a":"This kernel is purely to provide a platform for doing the inference stage. The trained model I use here is based on training BERT large for a single epoch on the training set provided for this competition. The training was done using `run_classifier.py` from the BERT team, which can be downloaded [here](https:\/\/github.com\/google-research\/bert).\n\nOnce you have trained your own model, you would have to upload it as a private dataset. Then this kernel can be used to do the predictions and prepare a submission file.\n\nThe inference takes 30 minutes using the gpu in this kernel. BERT base will be quicker. It is possible to speed things up by uploading only the predictions, but for the final submission, you would have to use akernel similar to this, as explained in [this thread](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/87719#latest-530119).\n\n","1821fed2":"## BERT Model Inference and Submission File"}}