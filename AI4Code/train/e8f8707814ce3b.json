{"cell_type":{"53ab2f3c":"code","e9c75ac1":"code","2eef8ccd":"code","6c8129ab":"code","95e1407f":"code","36890364":"code","64eee236":"code","c9532fe5":"code","a3f3716e":"code","7c09504a":"code","eff4c1ec":"code","ac4138c5":"code","681a5dfc":"code","c2539b28":"code","398a1771":"code","6ec0855c":"code","58358bb6":"code","b5e051f0":"code","bf80a3a1":"code","b1effd9f":"code","c39d1718":"code","53b0e36f":"code","fdf4efc7":"code","17d62d05":"code","c36114f9":"code","545a081c":"code","f758e8c9":"code","4eb0c90c":"code","1fee272b":"code","8bb528d2":"code","09b5b70a":"code","0eb9fe83":"code","ebc0bf51":"code","7fdf831a":"code","4efcd16a":"code","9cdf7d0b":"code","9e027587":"code","5448baf9":"code","8c575491":"code","ea65754e":"code","f749feaa":"code","523709d5":"code","627d7489":"code","29cbf557":"code","f425813c":"code","7ab76120":"code","d52d23b0":"code","2b4f7bc9":"code","a57c3684":"code","7f58e739":"code","fff6af84":"code","c8ebfe99":"code","a2fade9b":"code","89ece91b":"code","0b1490bb":"code","276738c9":"code","1d54f4d1":"markdown","be9bd86b":"markdown"},"source":{"53ab2f3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nimport gc\nimport time","e9c75ac1":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n#\ndef autocorrelation(ys, t=1):\n    return np.corrcoef(ys[:-t], ys[t:])","2eef8ccd":"#==========================================================================\ndef preprocess_sales(sales, start=1400, upper=1970):\n    if start is not None:\n        print(\"dropping...\")\n        to_drop = [f\"d_{i+1}\" for i in range(start-1)]\n        print(sales.shape)\n        sales.drop(to_drop, axis=1, inplace=True)\n        print(sales.shape)\n    #=======\n    print(\"adding...\")\n    new_columns = ['d_%i'%i for i in range(1942, upper, 1)]\n    for col in new_columns:\n        sales[col] = np.nan\n    print(\"melting...\")\n    sales = sales.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\",\"scale\",\"start\"],\n                        var_name='d', value_name='demand')\n\n    print(\"generating order\")\n    if start is not None:\n        skip = start\n    else:\n        skip = 1\n    sales[\"nb\"] =sales.index \/\/ 42840 + skip\n    return sales\n#===============================================================\ndef preprocess_calendar(calendar):\n    global maps, mods\n    calendar[\"event_name\"] = calendar[\"event_name_1\"]\n    calendar[\"event_type\"] = calendar[\"event_type_1\"]\n\n    map1 = {mod:i for i,mod in enumerate(calendar['event_name'].unique())}\n    calendar['event_name'] = calendar['event_name'].map(map1)\n    map2 = {mod:i for i,mod in enumerate(calendar['event_type'].unique())}\n    calendar['event_type'] = calendar['event_type'].map(map2)\n    calendar['nday'] = calendar['date'].str[-2:].astype(int)\n    maps[\"event_name\"] = map1\n    maps[\"event_type\"] = map2\n    mods[\"event_name\"] = len(map1)\n    mods[\"event_type\"] = len(map2)\n    calendar[\"wday\"] -=1\n    calendar[\"month\"] -=1\n    calendar[\"year\"] -= 2011\n    mods[\"month\"] = 12\n    mods[\"year\"] = 6\n    mods[\"wday\"] = 7\n    mods['snap_CA'] = 2\n    mods['snap_TX'] = 2\n    mods['snap_WI'] = 2\n\n    calendar.drop([\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\", \"date\", \"weekday\"], \n                  axis=1, inplace=True)\n    return calendar\n#=========================================================\ndef make_dataset(categorize=False ,start=1400, upper= 1970):\n    global maps, mods\n    print(\"loading calendar...\")\n    calendar = pd.read_csv(\"..\/input\/m5-forecasting-uncertainty\/calendar.csv\")\n    print(\"loading sales...\")\n    sales = pd.read_csv(\"..\/input\/walmartadd\/sales.csv\")\n    cols = [\"item_id\", \"dept_id\", \"cat_id\",\"store_id\",\"state_id\"]\n    if categorize:\n        for col in cols:\n            temp_dct = {mod:i for i, mod in enumerate(sales[col].unique())}\n            mods[col] = len(temp_dct)\n            maps[col] = temp_dct\n        for col in cols:\n            sales[col] = sales[col].map(maps[col])\n        #\n\n    sales =preprocess_sales(sales, start=start, upper= upper)\n    calendar = preprocess_calendar(calendar)\n    calendar = reduce_mem_usage(calendar)\n    print(\"merge with calendar...\")\n    sales = sales.merge(calendar, on='d', how='left')\n    del calendar\n\n    print(\"reordering...\")\n    sales.sort_values(by=[\"id\",\"nb\"], inplace=True)\n    print(\"re-indexing..\")\n    sales.reset_index(inplace=True, drop=True)\n    gc.collect()\n\n    sales['n_week'] = (sales['nb']-1)\/\/7\n    sales[\"nday\"] -= 1\n    mods['nday'] = 31\n    sales = reduce_mem_usage(sales)\n    gc.collect()\n    return sales\n#===================","6c8129ab":"%%time\nCATEGORIZE = True;\nSTART = 1400; UPPER = 1970;\nmaps = {}\nmods = {}\nsales = make_dataset(categorize=CATEGORIZE ,start=START, upper= UPPER)","95e1407f":"sales.info()","36890364":"sales[\"x\"] = sales[\"demand\"] \/ sales[\"scale\"]","64eee236":"LAGS = [28, 29, 30, 31, 32, 33]\nFEATS = []\nfor lag in tqdm(LAGS):\n    sales[f\"x_{lag}\"] = sales.groupby(\"id\")[\"x\"].shift(lag)\n    FEATS.append(f\"x_{lag}\")","c9532fe5":"sales.info()","a3f3716e":"#sales.loc[(sales.start>1844)&(sales.nb>1840)&(sales.nb<1850), ['id','start','nb','demand']]\n#sales.start.max() #1845","7c09504a":"print(sales.shape)\nsales = sales.loc[sales.nb>sales.start]\nprint(sales.shape)","eff4c1ec":"sales.head()","ac4138c5":"########################### Apply on grid_df\nTARGET      = 'demand'\ngrid_df1 = sales[['id','d','demand']]\nSHIFT_DAY = 14\nSHIFT_DAY1 = 1\n\n# Lags\n# with 28 day shift\nstart_time = time.time()\nprint('Create lags')\n\n#LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\nLAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+13)]\ngrid_df1 = grid_df1.assign(**{\n        '{}_lag_{}'.format(col, l): grid_df1.groupby(['id'])[col].transform(lambda x: x.shift(l))\n        for l in LAG_DAYS\n        for col in [TARGET]\n    })\n\n# Minify lag columns\nfor col in list(grid_df1):\n    if 'lag' in col:\n        grid_df1[col] = grid_df1[col].astype(np.float16)\n\nprint('%0.2f min: Lags' % ((time.time() - start_time) \/ 60))\n\n\n# Rollings\n# with 28 day shift\nstart_time = time.time()\nprint('Create rolling aggs')\n\n\n\n#for i in [7,14,30,60,180]:\n#for i in [7,14,35,63,168]:\nfor i in [7,14,30]:\n    print('Rolling period:', i)\n    grid_df1['rolling_mean_'+str(i)] = grid_df1.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n    grid_df1['rolling_std_'+str(i)]  = grid_df1.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n\n\n    \nprint('%0.2f min: Lags' % ((time.time() - start_time) \/ 60))","681a5dfc":"grid_df1=grid_df1.drop(['demand'],axis=1)","c2539b28":"########################### Merge prices and save part 2\n#################################################################################\nprint('Merge prices and save part 2')\n# Merge Prices\ngrid_df = sales.merge(grid_df1, on=['id','d'], how='left')\n#keep_columns = [col for col in list(sales) if col not in original_columns]\n#grid_df = grid_df[MAIN_INDEX+keep_columns]\n#grid_df = reduce_mem_usage(grid_df)\n\n# Safe part 2\n#grid_df.to_pickle('State_Item_1.pkl')\nprint('Size:', sales.shape)\n\n# We don't need prices_df anymore\ndel grid_df1\n\n\n# We can remove new columns\n# or just load part_1\n#grid_df = pd.read_pickle('grid_part_1.pkl')","398a1771":"nb = sales['nb'].values\nMAX_LAG = max(LAGS)\n#tr_mask = np.logical_and(nb>START + MAX_LAG, nb<=1913)\ntr_mask = np.logical_and(nb>START + MAX_LAG, nb<=1941) \nval_mask = np.logical_and(nb>1913, nb<=1941)\nte_mask = np.logical_and(nb>1941, nb<=1969)","6ec0855c":"nb","58358bb6":"scale = sales['scale'].values\nids = sales['id'].values\n#y = sales['demand'].values\n#ys = y \/ scale\nys = sales['x'].values\nZ = sales[FEATS].values","b5e051f0":"sales['x']","bf80a3a1":"scale","b1effd9f":"sales.info()","c39d1718":"sv = scale[val_mask]\nse = scale[te_mask]\nids = ids[te_mask]\nids = ids.reshape((-1, 28))","53b0e36f":"se","fdf4efc7":"ca = sales[['snap_CA']].values\ntx = sales[['snap_TX']].values\nwi = sales[['snap_WI']].values\nwday = sales[['wday']].values\nmonth = sales[['month']].values\nyear = sales[['year']].values\nevent = sales[['event_name']].values\nnday = sales[['nday']].values\nlag1=sales[['x_28']].values","17d62d05":"item = sales[['item_id']].values\ndept = sales[['dept_id']].values\ncat = sales[['cat_id']].values\nstore = sales[['store_id']].values\nstate = sales[['state_id']].values","c36114f9":"Z","545a081c":"sales[FEATS]","f758e8c9":"ca","4eb0c90c":"def make_data(mask):\n    x = {\"snap_CA\":ca[mask], \"snap_TX\":tx[mask], \"snap_WI\":wi[mask], \"wday\":wday[mask], \n         \"month\":month[mask], \"year\":year[mask], \"event\":event[mask], \"nday\":nday[mask], \n         \"item\":item[mask], \"dept\":dept[mask], \"cat\":cat[mask], \"store\":store[mask], \n         \"state\":state[mask],\"num\":Z[mask]}\n    t = ys[mask]\n    return x, t","1fee272b":"sales.info()","8bb528d2":"xt, yt = make_data(tr_mask) #train\nxv, yv = make_data(val_mask) # val\nxe, ye = make_data(te_mask) # test","09b5b70a":"import tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf","0eb9fe83":"qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]","ebc0bf51":"q = tf.constant(np.array([qs]), dtype=tf.float32)\nq","7fdf831a":"ca = L.Input((1,), name=\"snap_CA\")\nca","4efcd16a":"#=====\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n#============================#\ndef make_model(n_in):\n    \n    num = L.Input((n_in,), name=\"num\")\n    \n    ca = L.Input((1,), name=\"snap_CA\")\n    tx = L.Input((1,), name=\"snap_TX\")\n    wi = L.Input((1,), name=\"snap_WI\")\n    wday = L.Input((1,), name=\"wday\")\n    month = L.Input((1,), name=\"month\")\n    year = L.Input((1,), name=\"year\")\n    event = L.Input((1,), name=\"event\")\n    nday = L.Input((1,), name=\"nday\")\n    item = L.Input((1,), name=\"item\")\n    dept = L.Input((1,), name=\"dept\")\n    cat = L.Input((1,), name=\"cat\")\n    store = L.Input((1,), name=\"store\")\n    state = L.Input((1,), name=\"state\")\n    inp = {\"snap_CA\":ca, \"snap_TX\":tx, \"snap_WI\":wi, \"wday\":wday, \n           \"month\":month, \"year\":year, \"event\":event, \"nday\":nday,\n           \"item\":item, \"dept\":dept, \"cat\":cat, \"store\":store, \n           \"state\":state, \"num\":num} \n    #\n    ca_ = L.Embedding(mods[\"snap_CA\"], mods[\"snap_CA\"], name=\"ca_3d\")(ca)\n    tx_ = L.Embedding(mods[\"snap_TX\"], mods[\"snap_TX\"], name=\"tx_3d\")(tx)\n    wi_ = L.Embedding(mods[\"snap_WI\"], mods[\"snap_WI\"], name=\"wi_3d\")(wi)\n    wday_ = L.Embedding(mods[\"wday\"], mods[\"wday\"], name=\"wday_3d\")(wday)\n    month_ = L.Embedding(mods[\"month\"], mods[\"month\"], name=\"month_3d\")(month)\n    year_ = L.Embedding(mods[\"year\"], mods[\"year\"], name=\"year_3d\")(year)\n    event_ = L.Embedding(mods[\"event_name\"], mods[\"event_name\"], name=\"event_3d\")(event)\n    nday_ = L.Embedding(mods[\"nday\"], mods[\"nday\"], name=\"nday_3d\")(nday)\n    item_ = L.Embedding(mods[\"item_id\"], 10, name=\"item_3d\")(item)\n    dept_ = L.Embedding(mods[\"dept_id\"], mods[\"dept_id\"], name=\"dept_3d\")(dept)\n    cat_ = L.Embedding(mods[\"cat_id\"], mods[\"cat_id\"], name=\"cat_3d\")(cat)\n    store_ = L.Embedding(mods[\"store_id\"], mods[\"store_id\"], name=\"store_3d\")(store)\n    state_ = L.Embedding(mods[\"state_id\"], mods[\"state_id\"], name=\"state_3d\")(state)\n    \n    p = [ca_, tx_, wi_, wday_, month_, year_, event_, nday_, item_, dept_, cat_, store_, state_]\n    emb = L.Concatenate(name=\"embds\")(p)\n    context = L.Flatten(name=\"context\")(emb)\n    \n    x = L.Concatenate(name=\"x1\")([context, num])\n    x = L.Dense(500, activation=\"relu\", name=\"d1\")(x)\n    x = L.Dropout(0.3)(x)\n    x = L.Concatenate(name=\"m1\")([x, context])\n    x = L.Dense(500, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dropout(0.3)(x)\n    x = L.Concatenate(name=\"m2\")([x, context])\n    x = L.Dense(500, activation=\"relu\", name=\"d3\")(x)\n    preds = L.Dense(9, activation=\"linear\", name=\"preds\")(x)\n    model = M.Model(inp, preds, name=\"M1\")\n    model.compile(loss=qloss, optimizer=\"adam\")\n    return model\n","9cdf7d0b":"net = make_model(len(FEATS))\nckpt = ModelCheckpoint(\"w.h5\", monitor='val_loss', verbose=1, save_best_only=True,mode='min')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\nes = EarlyStopping(monitor='val_loss', patience=3)\nprint(net.summary())","9e027587":"net.fit(xt, yt, batch_size=50_000, epochs=20, validation_data=(xv, yv), callbacks=[ckpt, reduce_lr, es])","5448baf9":"nett = make_model(len(FEATS))\nnett.load_weights(\"w.h5\")","8c575491":"pv = nett.predict(xv, batch_size=50_000, verbose=1)\npe = nett.predict(xe, batch_size=50_000, verbose=1)","ea65754e":"pv","f749feaa":"nett.evaluate(xv, yv, batch_size=50_000)","523709d5":"pv = pv.reshape((-1, 28, 9))\npe = pe.reshape((-1, 28, 9))","627d7489":"sv = sv.reshape((-1, 28))\nse = se.reshape((-1, 28))","29cbf557":"Yv = yv.reshape((-1, 28))","f425813c":"k = np.random.randint(0, 42840)\n#k = np.random.randint(0, 200)\nprint(ids[k, 0])\nplt.plot(np.arange(28, 56), Yv[k], label=\"true\")\nplt.plot(np.arange(28, 56), pv[k ,:, 3], label=\"q25\")\nplt.plot(np.arange(28, 56), pv[k ,:, 4], label=\"q50\")\nplt.plot(np.arange(28, 56), pv[k, :, 5], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","7ab76120":"#Prediction","d52d23b0":"names = [f\"F{i+1}\" for i in range(28)]","2b4f7bc9":"names","a57c3684":"piv = pd.DataFrame(ids[:, 0], columns=[\"id\"])","7f58e739":"piv.head(50)","fff6af84":"sv","c8ebfe99":"QUANTILES = [\"0.005\", \"0.025\", \"0.165\", \"0.250\", \"0.500\", \"0.750\", \"0.835\", \"0.975\", \"0.995\"]\nVALID = []\nEVAL = []\n\nfor i, quantile in tqdm(enumerate(QUANTILES)):\n    t1 = pd.DataFrame(pv[:,:, i]*sv, columns=names)\n    t1 = piv.join(t1)\n    t1[\"id\"] = t1[\"id\"] + f\"_{quantile}_validation\"\n    t2 = pd.DataFrame(pe[:,:, i]*se, columns=names)\n    t2 = piv.join(t2)\n    t2[\"id\"] = t2[\"id\"] + f\"_{quantile}_evaluation\"\n    VALID.append(t1)\n    EVAL.append(t2)\n#============#","a2fade9b":"sv","89ece91b":"sub = pd.DataFrame()\nsub = sub.append(VALID + EVAL)\ndel VALID, EVAL, t1, t2","0b1490bb":"sub.head()","276738c9":"sub.to_csv(\"submission.csv\", index=False)","1d54f4d1":"As we are asked to predict a time window of 28 days, the easiest way to go now is to use the last 28 days for validation:","be9bd86b":"**Variables to help with aggregation**"}}