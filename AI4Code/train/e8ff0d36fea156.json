{"cell_type":{"88d53e49":"code","956e3b30":"code","a57e45cf":"code","ba219a6f":"code","3e04ba1f":"code","4ccf922b":"code","82598060":"code","bfebccc6":"code","c60a1cf3":"code","a945f844":"code","7a514d26":"code","5ecaec8f":"code","0387350b":"code","aaabce97":"code","9b7ad347":"code","e9513295":"code","e95ab692":"code","43b4124f":"code","1b47c4e2":"code","37468ddc":"code","677da02c":"code","a37792a6":"code","65e83279":"code","bf51e81a":"code","19a1b441":"code","8665ebe7":"code","97af4d6a":"code","61003560":"code","eae1250c":"code","b4a30049":"code","94a37f5b":"code","a787abd3":"code","938eddd4":"code","14024cc3":"code","8b22c89d":"code","499b549e":"code","898544fa":"code","ff84cf85":"code","c7e5d643":"code","fb32496f":"code","9be14632":"code","a7444699":"code","2efcbe10":"markdown","e54c2d31":"markdown","bbdf565f":"markdown","760f5f80":"markdown","3291ba1f":"markdown","f347b375":"markdown","8b5f8132":"markdown","7ff51960":"markdown","fe2f46ba":"markdown","9de51bb2":"markdown","34d6d853":"markdown","de0810f4":"markdown","3ed6b6ad":"markdown","cc6d4a12":"markdown","49bee7b4":"markdown","ef9e2261":"markdown","46c5a1de":"markdown","d3c627e8":"markdown","510edfa9":"markdown","fd93ad82":"markdown","c5227b4e":"markdown","0d55587a":"markdown","1f885c25":"markdown","f81080d2":"markdown"},"source":{"88d53e49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","956e3b30":"df = pd.read_csv('..\/input\/amazon_alexa.tsv',sep='\\t')\ndf.head()","a57e45cf":"df.info()","ba219a6f":"# Percent of Each Rating\nprint('5 Star Rating Percent is {0} %'.format((2286\/3150)*100))\nprint('4 Star Rating Percent is {0} %'.format((455\/3150)*100))\nprint('3 Star Rating Percent is {0} %'.format((152\/3150)*100))\nprint('2 Star Rating Percent is {0} %'.format((96\/3150)*100))\nprint('1 Star Rating Percent is {0} %'.format((161\/3150)*100))","3e04ba1f":"df['review_length'] = df['verified_reviews'].apply(len)","4ccf922b":"df.head()","82598060":"sns.barplot(x='rating',y='rating',hue='feedback',data=df);","bfebccc6":"plt.figure(figsize= (20,8))\ndf.variation.value_counts().plot(kind='bar');","c60a1cf3":"plt.figure(figsize= (30,8))\nsns.barplot(x='variation',y='review_length',data=df);","a945f844":"df.tail(25)","7a514d26":"df.loc[(df.verified_reviews == ' '),]","5ecaec8f":"df.review_length.describe()","0387350b":"df[df.review_length == 2851]['verified_reviews'].iloc[0]","aaabce97":"plt.figure(figsize= (10,7))\nsns.scatterplot(x='review_length',y='rating',data=df,hue='rating',palette='viridis');","9b7ad347":"# Below 3 Rating\nbelow_3 = df.loc[(df.rating < 3),]\nbelow_3.head()","e9513295":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\nbelow_counts = cv.fit_transform(below_3['verified_reviews'])\nprint(below_counts.shape)\nprint(cv.get_feature_names())","e95ab692":"sum_below_1 = below_counts.sum(axis=0)\nbelow_words_freq1 = [(word, sum_below_1[0, idx]) for word, idx in cv.vocabulary_.items()]\nbelow_words_freq1 =sorted(below_words_freq1, key = lambda x: x[1], reverse=True)\nbelow_freq_df1 = pd.DataFrame(below_words_freq1, columns=['word', 'freq'])\nbelow_freq2 = below_words_freq1[8:]","43b4124f":"below_freq2","1b47c4e2":"from wordcloud import WordCloud\nwordcloud = WordCloud(background_color='black',width=1000, height=700).generate_from_frequencies(dict(below_freq2))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Most Frequent Words Below Rating 3\", fontsize=32);","37468ddc":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\noverall_counts = cv.fit_transform(df['verified_reviews'])\nprint(overall_counts.shape)\nprint(cv.get_feature_names())","677da02c":"sum_overall_1 = overall_counts.sum(axis=0)\noverall_words_freq1 = [(word, sum_overall_1[0, idx]) for word, idx in cv.vocabulary_.items()]\noverall_words_freq1 =sorted(overall_words_freq1, key = lambda x: x[1], reverse=True)\noverall_freq_df1 = pd.DataFrame(overall_words_freq1, columns=['word', 'freq'])","a37792a6":"from wordcloud import WordCloud\nwordcloud = WordCloud(background_color='black',width=1000, height=700).generate_from_frequencies(dict(overall_words_freq1))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Most Frequent Words Overall\", fontsize=32);","65e83279":"empty_reviews = df.loc[(df.verified_reviews == \" \"),'feedback'].value_counts()","bf51e81a":"empty_reviews","19a1b441":"# Changing the Loan_Amount_Term Column\ndf['verified_reviews'] = np.where((df.review_length == 1),\"love it echo\",df.verified_reviews)","8665ebe7":"df.tail(25)","97af4d6a":" df['review_length'] = df['verified_reviews'].apply(len)","61003560":"df.tail(25)","eae1250c":"df.loc[(df.verified_reviews == \" \"),]","b4a30049":"features = pd.DataFrame(overall_counts.toarray(), columns=list(sorted(cv.vocabulary_)))","94a37f5b":"features.head()","a787abd3":"features = features.join(df[['review_length','variation']], rsuffix='_base')\nfeatures = pd.get_dummies(features)","938eddd4":"target = df[['feedback']].astype(int)","14024cc3":"from sklearn.model_selection import train_test_split","8b22c89d":"x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","499b549e":"from xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(x_train, y_train)","898544fa":"feature_imp_df = pd.DataFrame([classifier.feature_importances_], columns=list(x_train.columns)).T\nfeature_imp_df.columns = ['imp']\nfeature_imp_df.sort_values('imp', ascending=False, inplace=True)","ff84cf85":"feature_imp_df.head(15).plot.barh(figsize=(16, 9))\nplt.title(\"15 Most Important Features\");","c7e5d643":"y_test['pred'] = classifier.predict(x_test)","fb32496f":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy Score for Test Set : {}\".format(accuracy_score(y_test.feedback, y_test.pred)*100))","9be14632":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test.feedback,y_test.pred))","a7444699":"print(confusion_matrix(y_test.feedback,y_test.pred))","2efcbe10":"**Thanks for Reading .. If you Liked it Kindly give an Upvote.**","e54c2d31":"**Let's Just fill the Reviews with \"love it echo\" as most people who have reviews missing are of feedback 1 and the most frequent words for a Positive Feedback are love,echo, etc. I know it's not fully correct but from the past data we can fill it with such values and it's better to fill them rather than Keeping it as Space. At least the Model will have seperate Vector Columns for these words as they have some Previous Records.**","bbdf565f":"**We have Created a Below_3 Rating Variable to see the most Frequent Words of people who have hated to Product.**","760f5f80":"***We have 2 Numeric Columns and 3 Object Variables including the Date Variable and a Total Of 3150 values.*\n*According to this we don't have any NULL Values in it.***","3291ba1f":"**Now we don't have any Missing Values and can proceed towards Modelling Part.**","f347b375":"*We can see Most of the Widely used Variations are **Black Dot** and **Charcoal Fabric** *","8b5f8132":"**And Thanks to Hancel PV For his wonderful Kernel.**","7ff51960":"**We can Clearly See Words Like \"Disappointed\",'Speaker\",\"Did'nt\",etc....**","fe2f46ba":"**Let's Have a Look at Rating Column.**","9de51bb2":"**We Can See Most of the People have given 5 star rating , Thus the Overall Review of Amazon Echo is Great.**","34d6d853":"** First Let's make a new variable review_length to count the review length**","de0810f4":"**We have a total of 79 Values which have Missing reviews.**","3ed6b6ad":"**We have a review of 2851 Characters . Let's Explore that.**","cc6d4a12":"**Here We can see that Most of the Reviews were of 5 star Rating or Either 2 Star . This Means it might the ones who have loved it  have written about the Pros of the Product as we have seen in the above one or Either the people might have written Cons about the Product. The Middle ones don't play a Significant role.**","49bee7b4":"**Now let's again Redo the Review_length column so that new values get properly fit in.**","ef9e2261":"**We can see Variation does'nt have any Relation with Review_length.**","46c5a1de":"***We get to know that 3 or above is termed as a 1 Feedback and Below it are considered 0 Feedback. This can also be understood that above 3 is a Positive Feedback and Below 3 is a Negative Feedback. ***","d3c627e8":"**Here People love Echo a lot. Let's Fill the Missing Reviews and Move towards Modelling Part.**","510edfa9":"**Here we can see that we have few NULL values or Empty Reviews in the Verified_Reviews Column. But the machine might have considered empty spaces as a Characters. **","fd93ad82":"**Now Let's Make a WordCloud to Summarize.**","c5227b4e":"**We can see Review_Length is an Important Feature. We know also that it should have been the Most Significant Feature in our Dataset.**","0d55587a":"**So for 630 Values our Model has an Accuracy of (567+9)\/630 * 100 = 91.42 % \nWith Precison of right prediction as  91% **","1f885c25":"**Here the Person has Really Liked the Product and have Expressed his\/her Feelings. So it is a Big Postive Feedback**","f81080d2":"**We have got a Good Accuracy of 91.42% . Let's Check out the Classification Report.**"}}