{"cell_type":{"86697a67":"code","77044c7e":"code","15b2fdef":"code","de205a3b":"code","10a818e6":"code","a2af67e7":"code","835a2ab3":"code","643e30c8":"code","10cba0c8":"code","12a57507":"code","198ce4cd":"code","f6ed5ac6":"code","5adabe7a":"code","5286fa96":"code","51b48386":"code","128ef043":"code","897fa086":"code","1f315c2e":"code","c3c2d868":"code","354d325e":"code","2da54057":"code","e490aa17":"code","f86dbce7":"code","a20a04d6":"code","3caab077":"code","006434eb":"code","fee14d9d":"code","6cf70a9e":"code","0117fd98":"code","0e9f2a57":"code","ef2c5fb8":"code","c7002e74":"code","3e172403":"code","c11ffb66":"code","33e82ebe":"code","90b82407":"code","52560e94":"code","7e2c7602":"code","80e67cfd":"code","35e7280a":"code","4731280e":"markdown","c27851f2":"markdown","e464eee5":"markdown","9c30ca53":"markdown","08985598":"markdown","0e8f43db":"markdown","ab728eab":"markdown","3fd86c96":"markdown","4aee6ed2":"markdown","019c1e6b":"markdown","58f54a8a":"markdown","881338e7":"markdown","3f8d8d22":"markdown","23e13502":"markdown","2a32b851":"markdown","f6f68016":"markdown","c1838e65":"markdown","07803715":"markdown","a9cf2c7d":"markdown","35fada2a":"markdown","6e6fb558":"markdown","0dc63607":"markdown","027e839e":"markdown"},"source":{"86697a67":"!pip install datatable","77044c7e":"# Load libraries\nimport datatable as dt\nfrom datatable.models import Ftrl\nprint(dt.__version__)\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# to print all outputs of a cell\nfrom IPython.core.interactiveshell import InteractiveShell  \nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')","15b2fdef":"## Data Table Reading\nstart = time.time()\ndata_dir = Path('..\/input\/tabular-playground-series-nov-2021\/')\ndt_train = dt.fread(data_dir \/ \"train.csv\")\ndt_test = dt.fread(data_dir \/ \"test.csv\")\ndt_submission = dt.fread(data_dir \/ \"sample_submission.csv\")\nend = time.time()\nprint(end - start)","de205a3b":"dt_train.head(5)","10a818e6":"# number of rows and columns in training dataset\ndt_train.shape","a2af67e7":"# number of rows and columns in test dataset\ndt_test.shape","835a2ab3":"for i in range(len(dt_train.names)):\n    print(dt_train.names[i], \":\", dt_train.stypes[i])","643e30c8":"dt_submission.head(5)","10cba0c8":"dt_submission.shape","12a57507":"# mean\ndt_train.mean()","198ce4cd":"# max\ndt_train.max()","f6ed5ac6":"# min\ndt_train.min()","5adabe7a":"# standard deviation\ndt_train.sd()","5286fa96":"dt_train.countna()","51b48386":"class_counts = dt_train.to_pandas().groupby('target').size()\nprint(class_counts)","128ef043":"dt_train[dt.f.target == 1, :].mean()","897fa086":"dt_train[dt.f.target == 1, :].min()","1f315c2e":"dt_train[dt.f.target == 1, :].max()","c3c2d868":"dt_train[dt.f.target == 0, :].mean()","354d325e":"dt_train[dt.f.target == 0, :].min()","2da54057":"dt_train[dt.f.target == 0, :].max()","e490aa17":"start = time.time()\n\n# Pairwise Pearson correlations\ncorrelations = dt_train.to_pandas().corr(method='pearson')\nprint(correlations)\n\nend = time.time()\nprint(end - start)","f86dbce7":"dt_train.skew()","a20a04d6":"dt_train.kurt()","3caab077":"dt_train.nunique()","006434eb":"from sklearn.model_selection import train_test_split\n\nX = dt_train[:, [col for col in dt_train.names if col != 'target']]\ny = dt_train[:, -1]\n\nX = X.to_numpy()\ny = y.to_numpy()\n\n# dt_df = dt_train.to_numpy()\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3)\n\nX_train = dt.Frame(X_train)\nX_validation = dt.Frame(X_validation)\ny_train = dt.Frame(y_train)\ny_validation = dt.Frame(y_validation)","fee14d9d":"from datatable.models import Ftrl\n\nmodel_ftrl_1 = Ftrl()\nmodel_ftrl_1.fit(X_train, y_train)\nmodel_ftrl_1","6cf70a9e":"prediction_validation_1 = model_ftrl_1.predict(X_validation)\nprediction_validation_1.head()","0117fd98":"X_test = dt_test[:,:]\nX_test = X_test.to_numpy()\nX_test = dt.Frame(X_test)\n\nprediction_test_1 = model_ftrl_1.predict(X_test)\nprediction_test_1.head()","0e9f2a57":"# Display the feature importances of model_ftrl_1 in descending order and calculate the logloss of y_validation and prediction_validation_1\nmodel_ftrl_1.feature_importances[:, :, dt.sort(-dt.f.feature_importance)]","ef2c5fb8":"preds = dt.cbind(y_validation, prediction_validation_1)\n# print(preds) very important to print pred because we will come to know that target has been renamed to C0\npreds[:, -dt.mean(dt.f.C0 * dt.math.log(dt.f['True']) + (1-dt.f.C0) * dt.math.log(dt.f['False']))][0, 0]","c7002e74":"submission_ids = dt_submission['id']\nprint(submission_ids)","3e172403":"# Create submission_1 in the submission format of the competition, write it as submission_1.csv and submit it on Kaggle\nsubmission_1 = dt.Frame(id=submission_ids, target=prediction_test_1['True'])\nsubmission_1.to_csv('submission_1.csv')\nsubmission_1.head()","c11ffb66":"# Train another FTRL model model_ftrl_2 with nepochs=3, `nbins=10 8, display it's feature importances, score & evaluate it's logloss onvalid_dataand submit the predictionspreds_test_2oftestassubmission_2`**\nmodel_ftrl_2 = Ftrl(nepochs=3, nbins=10**8)\nmodel_ftrl_2.fit(X_train, y_train)\nmodel_ftrl_2","33e82ebe":"model_ftrl_2.feature_importances[:, :, dt.sort(-dt.f.feature_importance)]","90b82407":"prediction_validation_2 = model_ftrl_2.predict(X_validation)\nprediction_validation_2.head()","52560e94":"prediction_test_2 = model_ftrl_2.predict(X_test)\nprediction_test_2.head()","7e2c7602":"preds = dt.cbind(y_validation, prediction_validation_2)\npreds[:, -dt.mean(dt.f.C0 * dt.math.log(dt.f['True']) + (1-dt.f.C0) * dt.math.log(dt.f['False']))][0, 0]","80e67cfd":"submission_2 = dt.Frame(id=submission_ids, target=prediction_test_2['True'])\nsubmission_2.to_csv('submission_2.csv')\nsubmission_2.head()","35e7280a":"# Submit a ensemble of model_ftrl_1 and model_ftrl_2 by averaging the predictions as submission_ensemble\nsubmission_ensemble = dt.cbind(submission_1, submission_2)\nsubmission_ensemble[:, dt.update(target = 0.5 * dt.f.target + 0.5 * dt.f.target)]\ndel submission_ensemble[:, ['id.0', 'target.0']]\nsubmission_ensemble.to_csv('submission_ensemble.csv')\nsubmission_ensemble.head()","4731280e":"Let's take a peek at the first 5 rows.","c27851f2":"Let's have a look at the skewness and kurtosis of all the attributes in the training dataset.","e464eee5":"# Submission using Model 1.","9c30ca53":"# Feature Importance of Model 1.","08985598":"We can see that the submission file has 540,000 rows and two columns, which are 'id' and 'target'.\n\n# Exploratory Data Analysis\n\nLet's find out the mean, maximum, minimum, standard deviation and the number of missing values in the training dataset.","0e8f43db":"We don't have any missing values, hence imputation is not required.","ab728eab":"# Model 2\n\nWe use the Ftrl model again, but with nepochs=3, nbins=10**8.","3fd86c96":"### Load data","4aee6ed2":"# Model 1\n\nIn this section, we use an [Ftrl](https:\/\/keras.io\/api\/optimizers\/ftrl\/) model, which stands for 'Follow The Regularized Leader'.","019c1e6b":"- The training dataset appears to be balanced.\n- We have 296,394 \/ 600,000 = 49% cases in which the email is not spam.\n- We have 303,606 \/ 600,000 = 51% cases in which the email is spam.\n\nLet's have a look at the mean, minimum and maximum values for cases in which the email is spam.","58f54a8a":"### Correlation","881338e7":"- We have 600,000 rows and 102 attributes in the training dataset.\n- 102 attributes include one id attribute of type integer, 100 attributes (f0 to f99) of type float and the target of type bool.\n\nNow, let's find out the dimensions of the test dataset.","3f8d8d22":"- we have 540,000 rows and 101 attributes in the test dataset\n- 101 attributes include one id attribute of type integer and 100 attributes (f0 to f99) of type float.\n\nLet's have a look at the data types of all the attributes in the training dataset.","23e13502":"Let's have a look at the dimensions of the submission file.","2a32b851":"There is no point in checking the number of unique values since attributes f0 to f99 are all of type float.\n\n# Prepare Validation Dataset","f6f68016":"# References\n\nThank you to vopani for these [datatable exercises](https:\/\/github.com\/vopani\/datatableton#set-04--frame-operations--beginner--exercises-31-40).","c1838e65":"### Load Libraries\n\nLet's start off by loading the libraries required.","07803715":"# Problem Statement\n\nIn this notebook, we <b>predict whether or not an email is spam<\/b>. It is a binary (2-class) classification problem. We have 600,000 rows in the training dataset with 102 attributes which include id, f0 to f99 and the target. We have 540,000 rows in the test dataset with 101 attributes which include id and f0 to f99. We have 540,000 rows in the submission file with 2 attributes, id and target. The id attribute is of type integer and f0 to f99 are of type float. The target is of type boolean having values of 0 which indicate not spam and 1 which indicates spam email. We do not have any missing values and the training dataset appears to be balanced.\n\nWe are going to cover the following steps:\n1. Load data\n2. Exploratory Data Analysis\n3. Prepare Validation Dataset\n4. Model 1 using Ftrl\n5. Feature Importance for Model 1\n6. Submission using Model 1\n7. Model 2\n8. Feature Importance for Model 2\n9. Submission using Model 2\n10. Ensemble of Model 1 and Model 2\n11. References\n\nLet's get started.\n\n# Load Data\n\n### Install Libraries","a9cf2c7d":"Now, let's have a look at the mean, minimum and maximum values for cases in which the email is not spam.","35fada2a":"# Ensemble of Model 1 and Model 2","6e6fb558":"# Feature Importance for Model 2","0dc63607":"# Submission using Model 2","027e839e":"Let's find out the dimensions of the training dataset."}}