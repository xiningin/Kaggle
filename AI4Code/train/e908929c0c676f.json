{"cell_type":{"b187ff26":"code","a439c64d":"code","6d30fe2c":"code","27abde2c":"code","d35bf7f4":"code","afebf6f0":"code","907a4112":"code","e03998dd":"code","b74f7631":"code","67aa2349":"code","44461e1a":"code","a916e0b5":"code","42ca46a2":"code","7b89cdc0":"code","ded4b020":"code","f7969c30":"markdown","1b08448d":"markdown","dd7179c9":"markdown","86848962":"markdown","15b70e3b":"markdown","b258a2b9":"markdown"},"source":{"b187ff26":"import numpy as np\nimport pandas as pd\nimport itertools\nimport gc\n\nimport tensorflow as tf","a439c64d":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","6d30fe2c":"train = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_sub = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')","27abde2c":"test.sample(3)","d35bf7f4":"def get_encoder():\n    seq = list('AGUC')\n    stru = list('.()')\n    loop = list('SEHIXMB')\n    encoder = dict()\n    for i, prod in enumerate(itertools.product(seq, stru, loop)):\n        concat_prod = prod[0] + prod[1] + prod[2]\n        encoder[concat_prod] = i\n    return encoder    \n\ndef get_bpp_seqs(id_seqs):\n    bpp = [np.load(f'..\/input\/stanford-covid-vaccine\/bpps\/{seq}.npy') \n           for seq in id_seqs]\n    # get quantiles\n    bpp = [(1 - np.floor(bi.sum(axis=1)) * 100) for bi in bpp]\n    return np.array(bpp)\n\ndef preprocess_train(df_train, cols=['sequence', 'structure', 'predicted_loop_type'],\n                     targets=['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']):\n    # transform into list\n    vecs = np.array(df_train[cols].applymap(list).values.tolist()).transpose(0, 2, 1)\n    # concatenate all 3 positions\n    seqs = np.array([[vii[0] + vii[1] + vii[2] for vii in vi]\n                     for vi in vecs])\n    # encode\n    assert df_train.seq_length.max() == df_train.seq_length.min()\n    seq_len = df_train.seq_length.max()\n    X = np.array([[encoder[sii] for sii in si] for si in seqs]).reshape(-1, seq_len, 1)\n    \n    # bpp\n    Xbpp = get_bpp_seqs(df_train.id.values)\n    \n    # target\n    y = np.array(df_train[targets].values.tolist()).transpose(0, 2, 1)\n    \n    return X, Xbpp, y, encoder\n\ndef preprocess_test(df_test, cols=['sequence', 'structure', 'predicted_loop_type']):\n    # transform into list\n    vecs = np.array(df_test[cols].applymap(list).values.tolist()).transpose(0, 2, 1)\n    # concatenate all 3 positions\n    seqs = np.array([[vii[0] + vii[1] + vii[2] for vii in vi]\n                     for vi in vecs])\n    # encode\n    assert df_test.seq_length.max() == df_test.seq_length.min()\n    seq_len = df_test.seq_length.max()\n    X = np.array([[encoder[sii] for sii in si] for si in seqs]).reshape(-1, seq_len, 1)\n\n    # bpp\n    Xbpp = get_bpp_seqs(df_test.id.values)\n    \n    return X, Xbpp","afebf6f0":"# preprocess\nencoder = get_encoder()\nX_train, Xbpp_train, y_train, encoder = preprocess_train(train.loc[train.signal_to_noise >= 1])\nX_test_pub, Xbpp_test_pub = preprocess_test(test.query(\"seq_length == 107\"))\nX_test_pvt, Xbpp_test_pvt = preprocess_test(test.query(\"seq_length == 130\"))","907a4112":"# https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183211\ndef mcrmse(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)","e03998dd":"# adapted from https:\/\/levelup.gitconnected.com\/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb\ndef get_model(seq_len=107, pred_len=68, loss='mse', opt='adam', emb_dim_seq=32, emb_dim_bpp=32):\n    # input concat sequences\n    input_seq = tf.keras.Input(shape=(seq_len, 1))\n    \n    # embedding\n    x_seq = tf.keras.layers.Embedding(input_dim=len(encoder), output_dim=emb_dim_seq, input_length=seq_len)(input_seq)\n    x_seq = tf.keras.layers.Reshape((x_seq.shape[1], x_seq.shape[2] * x_seq.shape[3]))(x_seq)\n    x_seq = tf.keras.layers.SpatialDropout1D(0.2)(x_seq)\n    # lstm\n    x_seq = tf.keras.layers.Bidirectional(\n                tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, \n                                     return_sequences=True))(x_seq)\n    x_seq = tf.keras.layers.Bidirectional(\n                tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, \n                                     return_sequences=True))(x_seq)\n    x_seq = tf.keras.layers.Attention()([x_seq, x_seq])\n    \n    \n    # bpp\n    input_bpp = tf.keras.Input(shape=(seq_len, 1))\n    \n    # embedding\n    x_bpp = tf.keras.layers.Embedding(input_dim=100, output_dim=emb_dim_bpp, input_length=seq_len)(input_bpp)\n    x_bpp = tf.keras.layers.Reshape((x_bpp.shape[1], x_bpp.shape[2] * x_bpp.shape[3]))(x_bpp)\n    x_bpp = tf.keras.layers.SpatialDropout1D(0.2)(x_bpp)\n    \n    # lstm\n    x_bpp = tf.keras.layers.Bidirectional(\n                tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2,\n                                     return_sequences=True))(x_bpp)\n    x_bpp = tf.keras.layers.Bidirectional(\n                tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, \n                                     return_sequences=True))(x_bpp)\n    \n    x_bpp = tf.keras.layers.Attention()([x_bpp, x_bpp])\n    \n    # concat\n    x = tf.keras.layers.Concatenate(axis=-1)([x_seq, x_bpp])\n    \n    # truncate\n    x = x[:, :pred_len, :]\n    \n    # dense\n    x = tf.keras.layers.Dropout(0.3)(x)\n    \n    x = tf.keras.layers.Dense(32)(x)\n    x = tf.keras.layers.LeakyReLU()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    \n    x = tf.keras.layers.Dense(5, activation='linear')(x)\n    \n    model = tf.keras.Model(inputs=[input_seq, input_bpp], outputs=x)\n    \n    model.compile(optimizer=opt, loss=loss, metrics=[mcrmse])\n    return model\n    \n    ","b74f7631":"get_model().summary()","67aa2349":"from sklearn.model_selection import KFold","44461e1a":"# params\nn_folds = 5\nseed = 12\nepochs = 50\nbatch_size = 32\nloss = mcrmse\nemb_dim_seq = 64\nemb_dim_bpp = 64\n\n# placeholders\nmodels = list()\nval_metrics = list()\nhistories = list()\npublic_preds = np.zeros((test.query(\"seq_length == 107\").shape[0], 107, 5))\nprivate_preds = np.zeros((test.query(\"seq_length == 130\").shape[0], 130, 5))\n\n\n# folds\nfolds = KFold(n_folds, shuffle=True, random_state=seed).split(X_train, y_train)\nfor fold, (train_ix, test_ix) in enumerate(folds):\n    print('-'*30, f'Fold {fold+1}\/{n_folds}', '-'*30, '\\n\\n')\n    tf.keras.backend.clear_session()\n    \n    # splits data\n    xtrain_fold, xbpp_train_fold, y_train_fold = X_train[train_ix], Xbpp_train[train_ix], y_train[train_ix]\n    xval_fold, xbpp_val_fold, y_val_fold = X_train[test_ix], Xbpp_train[test_ix], y_train[test_ix]\n    # model\n    opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n    with tpu_strategy.scope():\n        m = get_model(emb_dim_seq=emb_dim_seq, emb_dim_bpp=emb_dim_bpp, opt=opt, loss=loss)\n        # callback\n        lr_callback = tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.2, verbose=1,\n                                                           min_delta=0.007, monitor='val_loss')\n        ckpt = tf.keras.callbacks.ModelCheckpoint(f'model-{fold}.h5')\n    # fit\n    gc.collect()\n    h = m.fit((xtrain_fold, xbpp_train_fold), y_train_fold, \n              validation_data=((xval_fold, xbpp_val_fold), y_val_fold),\n              epochs=epochs, batch_size=batch_size,\n              callbacks=[lr_callback, ckpt])\n    # save\n    histories.append(h)\n    \n    # predict test\n    model_short = get_model(seq_len=107, pred_len=107, emb_dim_seq=emb_dim_seq, \n                            emb_dim_bpp=emb_dim_bpp, opt=opt, loss=loss)\n    model_short.load_weights(f'model-{fold}.h5')\n    model_public_pred = model_short.predict((X_test_pub, Xbpp_test_pub)) \/ (n_folds)\n\n    model_long = get_model(seq_len=130, pred_len=130, emb_dim_seq=emb_dim_seq, \n                           emb_dim_bpp=emb_dim_bpp, opt=opt, loss=loss)\n    model_long.load_weights(f'model-{fold}.h5')\n    model_private_pred = model_long.predict((X_test_pvt, Xbpp_test_pvt)) \/ (n_folds)\n\n    public_preds += model_public_pred\n    private_preds += model_private_pred\n","a916e0b5":"print(f\" CV-mean fold loss: {np.mean([min(history.history['val_loss']) for history in histories])}\")","42ca46a2":"# https:\/\/www.kaggle.com\/tuckerarrants\/openvaccine-gru-lstm\n\ntargets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ndef format_predictions(public_preds, private_preds):\n    preds = []\n    \n    for df, preds_ in [(test.query(\"seq_length == 107\"), public_preds), \n                       (test.query(\"seq_length == 130\"), private_preds)]:\n        for i, uid in enumerate(df.id):\n            single_pred = preds_[i]\n\n            single_df = pd.DataFrame(single_pred, columns=targets)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n            preds.append(single_df)\n\n    return pd.concat(preds)","7b89cdc0":"preds = format_predictions(public_preds, private_preds)\nsubmission = sample_sub[['id_seqpos']].merge(preds, on=['id_seqpos'])\nsubmission.head()","ded4b020":"submission.to_csv('submission.csv', index=False)","f7969c30":"We are going to merge Sequence, Structure and Predicted Loop Type as one sequence and then do one hot enconding from the combinations (e.g. the sequence could be somenthing like ['A.E', 'A.E', 'C(V', ...])","1b08448d":"# Modelling","dd7179c9":"# Preprocess","86848962":"# Submission","15b70e3b":"# Cross-Val","b258a2b9":"# Load data"}}