{"cell_type":{"10f8f8b3":"code","aee10c8e":"code","09e683b0":"code","b90fef79":"code","d2f55917":"code","b0b3f611":"code","a4024bc8":"code","2a2fed4e":"code","813aef28":"code","0dab2789":"code","fee7dfd3":"code","7e1fe8b2":"code","5e111fec":"code","c66490ea":"code","5e50df15":"code","be5800cc":"markdown","721fac31":"markdown","bb9b6398":"markdown","78e5adca":"markdown","db00d1b9":"markdown","ac275255":"markdown"},"source":{"10f8f8b3":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Conv2DTranspose, \n    LeakyReLU, Activation, \n    Concatenate, Dropout, BatchNormalization\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nfrom tensorflow_addons.layers.normalizations import InstanceNormalization\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom scipy.linalg import sqrtm","aee10c8e":"# define hardcoded values used throughout this notebook\nIMAGE_SIZE = (256, 256)\nIMAGE_SHAPE = (256, 256, 3)\nDATASET_PATH = \"\/kaggle\/input\/pix2pix-dataset\/edges2shoes\/edges2shoes\"\nTRAIN_DIR = DATASET_PATH + \"\/train\/\"\nVAL_DIR = DATASET_PATH + \"\/val\/\"\nNBR_EPOCHS = 1\nBATCH_SIZE = 5\nMAX_TRAIN_SAMPLES = 200  # restricts training data to this many samples - use for testing and for limited resources","09e683b0":"# get the file names in each directory, and count how many files there are\n\ndef only_images(ls):\n    return [f for f in ls if f.endswith('.jpg')]\n\ntrain_files = os.listdir(TRAIN_DIR)\nval_files = os.listdir(VAL_DIR)\n\ntrain_files = only_images(train_files)\nval_files = only_images(val_files)\n\ntrain_files = [TRAIN_DIR + \"\/\" + file_path for file_path in train_files]\nval_files = [VAL_DIR + \"\/\" + file_path for file_path in val_files]\n\nprint(f\"{len(train_files)} training files\")\nprint(f\"{len(val_files)} validation files\")","b90fef79":"# show the pixel dimensions and value ranges of the first 10 training samples\nsample_images = []\nfor i in train_files[:10]:\n    img = cv2.imread(i)\n    print(\"Shape:\", img.shape)\n    print(\"Pixel Value Range:\", np.min(img), np.max(img))\n    sample_images.append(img)\n    \n# visually inspect the images\nfig, axs = plt.subplots(nrows=2, ncols=5, figsize=(30, 10))\nfor idx, img in enumerate(sample_images):\n    axs[idx\/\/5, idx%5].imshow(img, cmap='gray')\n    axs[idx\/\/5, idx%5].axis('off')\nplt.show()","d2f55917":"# ensure that edges and image are separable at pixel 256 in the x-dimension\nsample0_edges, sample0_shoe = sample_images[0][:, :256], sample_images[0][:, 256:]\nplt.imshow(sample0_edges)\nplt.show()\nplt.imshow(sample0_shoe)\nplt.show()","b0b3f611":"def resize_image(img, size=(28,28)):\n    \"\"\"\n    Resizes an image to the given size.  \n    Code inspired by: https:\/\/stackoverflow.com\/questions\/44650888\/resize-an-image-without-distortion-opencv\n    \"\"\"\n    # get the image's original height and width\n    h, w = img.shape[:2]\n    \n    # get the image's original number of color channels\n    c = img.shape[2] if len(img.shape) > 2 else 1\n\n    # If the image is square already, simply resize it.\n    #   Otherwise find which is longer (h or w), and create a square \n    #   image of zeros with the length equal to the larger dimension.\n    #   Then paste the image into the square and resize the new image.\n    if h == w: \n        return cv2.resize(img, size, cv2.INTER_AREA)\n    else:\n        # find the longer dimension\n        longer_dim = h if h > w else w\n\n        # Determine which interpolation method to use.  If the longer \n        #   dimension is larger than half the sum of the new image's height and width, \n        #   then use INTER_AREA.  Otherwise use INTER_CUBIC (bicubic interpolation).\n        #   These methods take longer than others but ensure the least information loss.\n        if longer_dim > (size[0] + size[1]) \/\/ 2:\n            interpolation_method = cv2.INTER_AREA\n        else:\n            interpolation_method = cv2.INTER_CUBIC\n\n        # get the coordinates of the original image\n        x_pos = (longer_dim - w)\/\/2\n        y_pos = (longer_dim - h)\/\/2\n\n        # create an image of zeros, then copy the original\n        #   image into position\n        if len(img.shape) == 2:\n            mask = np.zeros((longer_dim, longer_dim), dtype=img.dtype)\n            mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n        else:\n            mask = np.zeros((longer_dim, longer_dim, c), dtype=img.dtype)\n            mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n\n        return cv2.resize(mask, size, interpolation_method)\n\n\n# test the resizing function on a single image\nimg = cv2.imread(train_files[0])[:, 256:]\nnew_img = resize_image(img=img, size=IMAGE_SIZE)\nplt.imshow(img, cmap='gray')\nplt.show()\nplt.imshow(new_img, cmap='gray')\nplt.show()","a4024bc8":"class DataLoader():\n    def __init__(self, dataset_folder_path, img_size=(256, 256)):\n        \"\"\"\n        Constructs a DataLoader object\n        \n        :params:\n            dataset_folder_path (str): The path to the folder of \n                images to be loaded.  This is the name of the Pix2Pix dataset, \n                like edges2shoes.\n            image_shape (tuple): Tuple of the image dimensions like (x, y).\n        \"\"\"\n        self.dataset_folder_path = dataset_folder_path\n        self.img_size = img_size\n        self.nbr_batches = None\n\n    def load_data(self, batch_size=1, use_for_training=True):\n        \"\"\"\n        Loads data\n        \"\"\"\n        if use_for_training:\n            image_folder_path = self.dataset_folder_path + \"\/train\"\n        else:\n            image_folder_path = self.dataset_folder_path + \"\/val\"\n        \n        # get a list of the files in the given path\n        image_files = os.listdir(image_folder_path)\n        image_files = [image_folder_path + \"\/\" + i for i in image_files]\n        if MAX_TRAIN_SAMPLES is not None:\n            image_files = image_files[:MAX_TRAIN_SAMPLES]\n\n        # randomly sample batch_size number of images from the given path\n        batch_images = np.random.choice(image_files, size=batch_size)\n\n        source_images = []\n        target_images = []\n        \n        for img_path in batch_images:\n            img = cv2.imread(img_path)\n            \n            # split the source from the target image at the pixel in the \n            #   width dimension that is 1\/2 the total width (that's just \n            #   the way the images were created)\n            h, w, channels = img.shape\n            half_w = int(w\/2)\n            source_img = img[:, :half_w, :]\n            target_img = img[:, half_w:, :]\n            \n            # resize images\n            source_img = resize_image(source_img, self.img_size)\n            target_img = resize_image(target_img, self.img_size)\n\n            # if training, randomly flip 1\/2 the images\n            if use_for_training and np.random.random() < 0.5:\n                source_img = np.fliplr(source_img)\n                target_img = np.fliplr(target_img)\n\n            source_images.append(source_img)\n            target_images.append(target_img)\n\n        # stack the batch images into a numpy array and scale pixels \n        #   to range from -1 to 1\n        source_images = np.array(source_images)\/127.5 - 1.\n        target_images = np.array(target_images)\/127.5 - 1.\n\n        return source_images, target_images\n\n    def load_batch(self, batch_size=1, use_for_training=True):\n        \"\"\"\n        A batch load generator\n        \"\"\"\n        if use_for_training:\n            image_folder_path = self.dataset_folder_path + \"\/train\"\n        else:\n            image_folder_path = self.dataset_folder_path + \"\/val\"\n\n        # get a list of the files in the given path\n        image_files = os.listdir(image_folder_path)\n        image_files = [image_folder_path + \"\/\" + i for i in image_files]\n        if MAX_TRAIN_SAMPLES is not None:\n            image_files = image_files[:MAX_TRAIN_SAMPLES]\n        \n        # determine how many batches, based on the number of files \n        #   and the batch size\n        self.nbr_batches = int(len(image_files) \/ batch_size)\n\n        for i in range(self.nbr_batches-1):\n            batch = image_files[i*batch_size:(i+1)*batch_size]\n\n            source_images = []\n            target_images = []\n            \n            for img in batch:\n                img = cv2.imread(img)\n                \n                # split the source from the target image at the pixel in the \n                #   width dimension that is 1\/2 the total width (that's just \n                #   the way the images were created)\n                h, w, channels = img.shape\n                half_w = int(w\/2)\n                source_img = img[:, :half_w, :]\n                target_img = img[:, half_w:, :]\n\n                # resize images\n                source_img = resize_image(source_img, self.img_size)\n                target_img = resize_image(target_img, self.img_size)\n\n                # if training, randomly flip 1\/2 the images\n                if use_for_training and np.random.random() < 0.5:\n                    source_img = np.fliplr(source_img)\n                    target_img = np.fliplr(target_img)\n\n                source_images.append(source_img)\n                target_images.append(target_img)\n\n            # stack the batch images into a numpy array and scale pixels \n            #   to range from -1 to 1\n            source_images = np.array(source_images)\/127.5 - 1.\n            target_images = np.array(target_images)\/127.5 - 1.\n            \n            yield source_images, target_images\n\n\n# test the data loader on a single image\nloader = DataLoader(\n    dataset_folder_path=DATASET_PATH,\n    img_size=IMAGE_SIZE\n)\ntrain_source, train_target = loader.load_data(batch_size=1, use_for_training=True)\ntrain_source.shape","2a2fed4e":"# test the data loader on a batch\nloader = DataLoader(\n    dataset_folder_path=DATASET_PATH,\n    img_size=IMAGE_SIZE\n)\nfor batch_nbr, batch in enumerate(loader.load_batch(batch_size=10, use_for_training=False)):\n    print('batch nbr', batch_nbr, batch[0].shape, batch[1].shape)","813aef28":"def load_real_samples(batch_size, nbr_patches):\n    \"\"\"\n    Uses a DataLoader object to load a batch of real image\n    samples.  Returns the source and target images, along with the\n    label (label = all ones because they are all real images).\n    \"\"\"\n    loader = DataLoader(\n        dataset_folder_path=DATASET_PATH,\n        img_size=IMAGE_SIZE\n    )\n    real_source_images, real_target_images = loader.load_data(\n        batch_size=batch_size,\n        use_for_training=True\n    )\n    y = np.ones((len(real_source_images), nbr_patches, nbr_patches, 1))\n    return [real_source_images, real_target_images], y\n\n\n# sense check: should see images here\n# [x_real_source, x_real_target], y_real = load_real_samples(batch_size=10, nbr_patches=16)\n# plt.imshow(x_real_source[0]) # source image (edges)\n# plt.show()\n# plt.imshow(x_real_target[0]) # target image (shoe)\n# plt.show()\n\n\ndef generate_fake_samples(generator, real_source_image_samples, nbr_patches):\n    \"\"\"\n    Uses a generator model to generate a batch of fake target image\n    samples.  Returns the generated images and their label (label = all\n    zeros because they are all fake images).\n    \"\"\"\n    fake_target_images = generator.predict(real_source_image_samples)\n    y = np.zeros((len(fake_target_images), nbr_patches, nbr_patches, 1))\n    return fake_target_images, y","0dab2789":"class Pix2Pix:\n    def __init__(self, input_image_shape):\n        \"\"\"\n        Pix2Pix constructor.\n\n        :params:\n            input_image_shape: Tuple of (rows, cols, channels),\n                where rows and cols are measured in pixels.  Pix2Pix\n                was trained on 256x256x3 shape images, originally.\n        \"\"\"\n        self.image_shape = input_image_shape\n        self.data_loader = DataLoader(\n            dataset_folder_path=DATASET_PATH,\n            img_size=input_image_shape[:2]\n        )\n        # attributes that are defined by methods\n        self.discriminator = None\n        self.generator = None\n        self.discriminator_nbr_patches = None\n        self.gan = None\n\n    def _build_discriminator(self):\n        \"\"\"\n        The discriminator is a deep CNN that takes the source image\n        and target image, and predicts whether the target image is a\n        real or fake translation of the source image.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        source_image = Input(shape=self.image_shape)\n        target_image = Input(shape=self.image_shape)\n\n        combined_images = Concatenate()([source_image, target_image])\n\n        # C64\n        x = Conv2D(\n            filters=64, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(combined_images)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C128\n        x = Conv2D(\n            filters=128, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C256\n        x = Conv2D(\n            filters=256, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C512\n        x = Conv2D(\n            filters=512, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # second last output layer\n        x = Conv2D(\n            filters=512, kernel_size=(4, 4),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # patch output\n        x = Conv2D(\n            filters=1, kernel_size=(4, 4),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        patch_out = Activation('sigmoid')(x)\n        # define model\n        model = Model([source_image, target_image], patch_out)\n        # compile model\n        opt = Adam(lr=0.0002, beta_1=0.5)\n        model.compile(\n            loss='binary_crossentropy',\n            optimizer=opt,\n            loss_weights=[0.5]\n        )\n\n        return model\n\n    @staticmethod\n    def encoder_block(layer_in, nbr_filters, batch_norm=True):\n        \"\"\"\n        Encoder block in the U-net architecture that downsamples\n        the input (layer_in) using strided convolution, optionally\n        applying batch normalization, and using leaky relu activation.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        # downsampling layer\n        x = Conv2D(\n            filters=nbr_filters, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(layer_in)\n\n        if batch_norm:\n            x = BatchNormalization()(x, training=True)\n\n        x = LeakyReLU(alpha=0.2)(x)\n\n        return x\n\n    @staticmethod\n    def decoder_block(layer_in, skip_in, nbr_filters, dropout=True):\n        \"\"\"\n        Decoder block in the U-net architecture that upsamples the\n        the input (layer_in) using strided convolution, batch\n        normalization, optional dropout, and relu activation.  The\n        skip connection from the encoder must also be added, right\n        before the activation.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        # add upsampling layer\n        x = Conv2DTranspose(\n            filters=nbr_filters, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(layer_in)\n\n        # add batch normalization\n        x = BatchNormalization()(x, training=True)\n\n        # conditionally add dropout\n        if dropout:\n            x = Dropout(0.5)(x, training=True)\n\n        # merge with skip connection\n        x = Concatenate()([x, skip_in])\n\n        # relu activation\n        x = Activation('relu')(x)\n\n        return x\n\n    def _build_generator(self):\n        \"\"\"\n        The generator is an encoder-decoder model with a U-net\n        architecture.  It takes an image from the source domain\n        as input, and generates an image from the target domain\n        as output.  The tanh activation produces an output with\n        values ranged from -1 to 1.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        input_image = Input(shape=self.image_shape)\n\n        # encoder model\n        e1 = Pix2Pix.encoder_block(\n            layer_in=input_image,\n            nbr_filters=64,\n            batch_norm=False\n        )\n        e2 = Pix2Pix.encoder_block(e1, 128)\n        e3 = Pix2Pix.encoder_block(e2, 256)\n        e4 = Pix2Pix.encoder_block(e3, 512)\n        e5 = Pix2Pix.encoder_block(e4, 512)\n        e6 = Pix2Pix.encoder_block(e5, 512)\n        e7 = Pix2Pix.encoder_block(e6, 512)\n\n        # bottleneck layer\n        b = Conv2D(\n            filters=512, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(e7)\n        b = Activation('relu')(b)\n\n        # decoder model\n        d1 = Pix2Pix.decoder_block(\n            layer_in=b,\n            skip_in=e7,\n            nbr_filters=512,\n            dropout=True\n        )\n        d2 = Pix2Pix.decoder_block(d1, e6, 512)\n        d3 = Pix2Pix.decoder_block(d2, e5, 512)\n        d4 = Pix2Pix.decoder_block(d3, e4, 512, dropout=False)\n        d5 = Pix2Pix.decoder_block(d4, e3, 256, dropout=False)\n        d6 = Pix2Pix.decoder_block(d5, e2, 128, dropout=False)\n        d7 = Pix2Pix.decoder_block(d6, e1, 64, dropout=False)\n\n        # output\n        x = Conv2DTranspose(\n            filters=3, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(d7)\n        output_image = Activation('tanh')(x)\n\n        # define model\n        model = Model(input_image, output_image)\n\n        return model\n\n    def _build_gan(self):\n        \"\"\"\n        Builds the Pix2Pix GAN such that the discriminator trains on\n        the real and generated images, while the generator trains\n        by means of the discriminator.  The generator updates to minimize\n        the L1 loss (error between real and fake images), and the\n        adversarial loss (error of the discriminator).  The updates\n        weight these losses at a 100:1 ratio in favor of the L1 loss,\n        in accordance with the Pix2Pix paper.\n\n        Balancing both losses requires specifying a separate model that\n        stacks the generator on the discriminator and feeds a source\n        image to both.  Since the output of the generator is also\n        the discriminator's target image (which is concatenated with the\n        source image as input), the discriminator can predict\n        whether the generated image is a real or fake translation of the\n        source image.\n        \"\"\"\n        source_image = Input(shape=self.image_shape)\n\n        # build discriminator and generator\n        self.discriminator = self._build_discriminator()\n        self.generator = self._build_generator()\n\n        # store the discriminator output number of patches\n        self.discriminator_nbr_patches = self.discriminator.output_shape[1]\n\n        # freeze discriminator weights for this combined model\n        self.discriminator.trainable = False\n\n        # map inputs and outputs\n        generator_output = self.generator(source_image)\n        discriminator_output = self.discriminator([source_image, generator_output])\n\n        # model takes source image as input,\n        # produces generated image and real\/fake prediction as output\n        model = Model(source_image, [discriminator_output, generator_output])\n\n        model.compile(\n            loss=['binary_crossentropy', 'mae'],\n            optimizer=Adam(lr=0.0002, beta_1=0.5),\n            loss_weights=[1, 100]\n        )\n\n        return model\n    \n    def save_samples(self, epoch, batch_i):\n        \"\"\"\n        Saves samples of source, target, and generated images to the \n        generated_images directory.\n        \"\"\"\n        os.makedirs('generated_images\/', exist_ok=True)\n        r, c = 3, 3\n        \n        # load source and target images, and generate a fake image\n        source_imgs, target_imgs = self.data_loader.load_data(batch_size=3, use_for_training=False)\n        fake_imgs = self.generator.predict(target_imgs)\n        \n        # combine them and rescale them to range from 0 to 1\n        gen_imgs = np.concatenate([target_imgs, fake_imgs, source_imgs])\n        gen_imgs = 0.5 * gen_imgs + 0.5\n        \n        titles = ['Condition', 'Generated', 'Original']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[i])\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(\"generated_images\/%d_%d.png\" % (epoch, batch_i))\n        plt.close()\n\n    def train(self, nbr_epochs, batch_size, sample_interval=100):\n        \"\"\"\n        Trains the GAN.  In each iteration, the discriminator is first\n        updated with real samples, and then with fake\/generated samples.\n        Then the generator is updated.\n        \"\"\"\n        # build the GAN\n        self.gan = self._build_gan()\n        \n        for epoch in range(nbr_epochs):\n\n            print(\"Epoch\", epoch, \"of\", nbr_epochs)\n            d_loss_real_hist, d_loss_fake_hist, g_loss_hist = [], [], []\n\n            for batch_i, (x_real_source, x_real_target) in enumerate(\n                self.data_loader.load_batch(batch_size)\n            ):\n                # create corresponding real labels\n                y_real = np.ones((\n                    len(x_real_source), \n                    self.discriminator_nbr_patches,\n                    self.discriminator_nbr_patches,\n                    1\n                ))\n                \n                # generate a batch of fake samples\n                x_fake_target, y_fake = generate_fake_samples(\n                    generator=self.generator, \n                    real_source_image_samples=x_real_source, \n                    nbr_patches=self.discriminator_nbr_patches\n                )\n                \n                # update discriminator for real samples\n                d_loss_real = self.discriminator.train_on_batch(\n                    [x_real_source, x_real_target], \n                    y_real\n                )\n                d_loss_real_hist.append(d_loss_real)\n                \n                # update discriminator for generated samples\n                d_loss_fake = self.discriminator.train_on_batch(\n                    [x_real_source, x_fake_target], \n                    y_fake\n                )\n                d_loss_fake_hist.append(d_loss_fake)\n                \n                # update the generator\n                g_loss, _, _ = self.gan.train_on_batch(\n                    x_real_source, \n                    [y_real, x_real_target]\n                )\n                g_loss_hist.append(g_loss)\n                \n                # store sample images\n                if batch_i % sample_interval == 0:\n                    self.save_samples(epoch=epoch, batch_i=batch_i)\n\n            # show performance on epoch (indent this block for batch level performance)\n            print(\n                f\"Epoch {epoch+1} batch {batch_i+1} performance:\\n\", \n                f\"Discriminator real loss: {d_loss_real} \\n\",\n                f\"Discriminator fake loss: {d_loss_fake} \\n\",\n                f\"Generator loss: {g_loss} \\n\"\n            )\n\n        # return the loss hist for the final epoch\n        return d_loss_real_hist, d_loss_fake_hist, g_loss_hist\n    \n    def save_generator(self):\n        \"\"\"\n        Saves a trained Pix2Pix generator model to a h5 file.\n        \"\"\"\n        filename = \"pix2pix_generator_model.h5\"\n        self.generator.save(filename)\n\n\np2p = Pix2Pix(input_image_shape=IMAGE_SHAPE)\nd_loss_real, d_loss_fake, g_loss = p2p.train(\n    nbr_epochs=NBR_EPOCHS, \n    batch_size=BATCH_SIZE,\n    sample_interval=10\n)\np2p.save_generator()","fee7dfd3":"# plot the loss by training iteration\nplt.plot(d_loss_real, label=\"Discriminator Loss\")\nplt.plot(g_loss, label=\"Generator Loss\")\nplt.title(\"Loss by Iteration\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Iteration\")\nplt.legend()\nplt.show()","7e1fe8b2":"# load the model and make prediction on a given image\np2p_model = load_model('pix2pix_generator_model.h5')\nsource_img = cv2.imread(train_files[0])[:, :256]\ntarget_img = cv2.imread(train_files[0])[:, 256:]\n\n# resize\nsource_img = resize_image(img=source_img, size=IMAGE_SIZE)\ntarget_img = resize_image(img=target_img, size=IMAGE_SIZE)\n\n# scale pixel values\nsource_img = np.array(source_img)\/127.5 - 1.\ntarget_img = np.array(target_img)\/127.5 - 1.\n\n# reshape\nsource_img = np.expand_dims(source_img, 0)\ntarget_img = np.expand_dims(target_img, 0)\n\n# make prediction, re-scale and reshape\ngen_img = p2p_model.predict(source_img)[0]\ngen_img = (gen_img + 1) \/ 2.0\ngen_img = np.expand_dims(gen_img, 0)\n\n# combine and re-scale to (0, 1)\nall_imgs = np.concatenate([target_img, gen_img, source_img])\nall_imgs = 0.5 * all_imgs + 0.5\n\ntitles = ['Target', 'Generated', 'Source']\nfor idx, i in enumerate(all_imgs):\n    plt.imshow(i)\n    plt.title(titles[idx])\n    plt.show()","5e111fec":"class CycleGAN:\n    def __init__(self, input_image_shape):\n        \"\"\"\n        CycleGAN constructor.\n\n        :params:\n            input_image_shape: Tuple of (rows, cols, channels),\n                where rows and cols are measured in pixels.  A\n                256x256x3 shape is a good starting point, but the patch\n                architecture makes it agnostic to image size.\n        \"\"\"\n        self.image_shape = input_image_shape\n        self.data_loader = DataLoader(\n            dataset_folder_path=DATASET_PATH,\n            img_size=input_image_shape[:2]\n        )\n        # Calculate output shape of D (PatchGAN)\n        patch = int(input_image_shape[0] \/ 2 ** 4)\n        self.discriminator_patch_size = (patch, patch, 1)\n        # attributes that are defined by methods\n        self.generator_AtoB = None\n        self.generator_BtoA = None\n        self.discriminator_A = None\n        self.discriminator_B = None\n        self.combined_AtoB = None\n        self.combined_BtoA = None\n\n    def _build_discriminator(self):\n        \"\"\"\n        The discriminator is a deep CNN that takes the source image as\n        input and predicts whether the target image is a real or fake\n        translation of the source image.  The output is a patch that\n        maps to 70x70 pixels in the source image.  This allows it to\n        work with images of different sizes.\n\n        The MSE loss uses a weighting scheme of 0.5 to slow the updates\n        to the discriminator during GAN training.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        source_image = Input(shape=self.image_shape)\n\n        # C64\n        x = Conv2D(\n            filters=64, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(source_image)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C128\n        x = Conv2D(\n            filters=128, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C256\n        x = Conv2D(\n            filters=256, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C512\n        x = Conv2D(\n            512, (4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # second last output layer\n        x = Conv2D(\n            512, (4, 4),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # patch output\n        patch_out = Conv2D(\n            1, (4, 4),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        # define model\n        model = Model(source_image, patch_out)\n        # compile model\n        model.compile(\n            loss='mse',\n            optimizer=Adam(lr=0.0002, beta_1=0.5),\n            loss_weights=[0.5]\n        )\n\n        return model\n\n    @staticmethod\n    def resnet_block(n_filters, input_layer):\n        \"\"\"\n        A ResNet block with two 3x3 convolutional layers (1x1 stride).\n        This block will be used with skip connections in the generator.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        # first layer convolutional layer\n        x = Conv2D(\n            filters=n_filters, kernel_size=(3, 3),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(input_layer)\n        x = InstanceNormalization(axis=-1)(x)\n        x = Activation('relu')(x)\n        # second convolutional layer\n        x = Conv2D(\n            filters=n_filters, kernel_size=(3, 3),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        # concatenate merge channel-wise with input layer\n        x = Concatenate()([x, input_layer])\n\n        return x\n\n    def _build_generator(self, nbr_resnet_blocks=9):\n        \"\"\"\n        The generator is a ResNet model with a U-net architecture.\n        It takes an image from the source domain as input, and generates\n        an image from the target domain as output.  This is accomplished\n        by downsampling the source image to the bottleneck in the U-net\n        architecture, then interpreting the downsampled encoding with a\n        ResNet block that use skip connections, and then upsampling the\n        representation back to the original size.  The tanh activation\n        in the final layer produces an output with values ranged from -1 to 1.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        source_image = Input(shape=self.image_shape)\n\n        # c7s1-64\n        x = Conv2D(\n            filters=64, kernel_size=(7, 7),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(source_image)\n        x = InstanceNormalization(axis=-1)(x)\n        x = Activation('relu')(x)\n        # d128\n        x = Conv2D(\n            filters=128, kernel_size=(3, 3),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = Activation('relu')(x)\n        # d256\n        x = Conv2D(\n            filters=256, kernel_size=(3, 3),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = Activation('relu')(x)\n        # R256\n        for _ in range(nbr_resnet_blocks):\n            x = CycleGAN.resnet_block(256, x)\n        # u128\n        x = Conv2DTranspose(\n            filters=128, kernel_size=(3, 3),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = Activation('relu')(x)\n        # u64\n        x = Conv2DTranspose(\n            filters=64, kernel_size=(3, 3),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        x = Activation('relu')(x)\n        # c7s1-3\n        x = Conv2D(\n            filters=3, kernel_size=(7, 7),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = InstanceNormalization(axis=-1)(x)\n        output_image = Activation('tanh')(x)\n        # define model\n        model = Model(source_image, output_image)\n\n        return model\n\n    def _build_combined(self, generator_1, generator_2, discriminator):\n        \"\"\"\n        Builds the combined model for the provided generators and discriminator.\n        This function sets up the linking between various generators.  See the\n        _build_gan method docstring for a complete mapping.\n\n        Only the weights of generator A are updated in the combined model,\n        using the weighted sum of all 4 loss functions.  The cycle loss is\n        given 10 times more weight than the adversarial loss, and the identity\n        loss is half the weight of the cycle loss.  This is consistent with\n        the original CycleGAN paper.\n        \"\"\"\n        # freeze discriminator and generator_2 weights for this combined model,\n        #   but keep the generator_1 trainable\n        generator_1.trainable = True\n        generator_2.trainable = False\n        discriminator.trainable = False\n\n        # discriminator element\n        input_gen = Input(shape=self.image_shape)\n        generator_1_output = generator_1(input_gen)\n        discriminator_output = discriminator(generator_1_output)\n\n        # identity element\n        identity_input = Input(shape=self.image_shape)\n        identity_output = generator_1(identity_input)\n\n        # forward cycle\n        forward_output = generator_2(generator_1_output)\n\n        # backward cycle\n        generator_2_output = generator_2(identity_input)\n        backward_output = generator_1(generator_2_output)\n\n        # define model\n        model = Model(\n            [input_gen, identity_input],\n            [discriminator_output, identity_output, forward_output, backward_output]\n        )\n        # compile model with the following weighting scheme:\n        #   discriminator_output = MSE = 1\n        #   identity_output = MAE = 5 (1\/2 the weight of the cycle loss)\n        #   forward_output = MAE = 10\n        #   backward_output = MAE = 10\n        model.compile(\n            loss=['mse', 'mae', 'mae', 'mae'],\n            loss_weights=[1, 5, 10, 10],\n            optimizer=Adam(lr=0.0002, beta_1=0.5)\n        )\n\n        return model\n\n    def _build_gan(self):\n        \"\"\"\n        Builds the CycleGAN such that the discriminator trains on\n        the real and generated images, while the generators train\n        by means of their associated discriminators.  The generators\n        update to minimize the L2 loss (adversarial loss, or MSE - the\n        loss predicted by the discriminator for generated images labeled\n        as real).  The generators also update to minimize the L1 loss\n        (MAE - the loss between the original source image and its\n        regeneration, when both generators are combined).  The generators\n        also update to minimize the L1 loss of the identity (how well they\n        produce an image from the target domain).\n\n        Balancing all 4 losses requires specifying two generators, 1 for\n        each domain (source and target).  The generators take inputs\n        from the opposite domain:\n\n            Input    |    Model    |    Output\n            ----------------------------------\n            domain B | generator A |  domain A\n            domain A | generator B |  domain B\n\n        Each generator is paired with a discriminator like so:\n\n\n            Inputs             |      Model      |         Outputs\n            -------------------------------------------------------------\n            domain A           | discriminator A |   real\/fake prediction\n            generator A output |                 |\n            -------------------------------------------------------------\n            domain B           | discriminator B |   real\/fake prediction\n            generator B output |                 |\n\n        This allows the GAN to learn to create new images in the target domain.\n        However, the generators are also regularized to translate reconstructed\n        versions of the images from the source domain.  So each generator also\n        takes generated images from the other to compare to the original images.\n        The passing of an image through both generators is called a cycle.\n        The forward cycle is in the reconstruction of the source image\n        (domain A), and the backward cylce is the reconstruction of the\n        target image (domain B).\n\n            Input    |    Model    | Output\/Input |    Model    |  Output\n            --------------------------------------------------------------\n            domain B | generator A |  domain A    | generator B | domain B\n            domain A | generator B |  domain B    | generator A | domain A\n\n        Finally, the generators must take input from the target domain and\n        generate the same image, without change.  This allows for better\n        matching of the target domain's color profile, and it is called an\n        identity mapping:\n\n            Input    |    Model    |    Output\n            ----------------------------------\n            domain A | generator A |  domain A\n            domain B | generator B |  domain B\n\n        This function does not return anything.  It just updates self.\n        \"\"\"\n        # build the generators and discriminators\n        self.generator_AtoB = self._build_generator()\n        self.generator_BtoA = self._build_generator()\n        self.discriminator_A = self._build_discriminator()\n        self.discriminator_B = self._build_discriminator()\n        self.combined_AtoB = self._build_combined(\n            generator_1=self.generator_AtoB,\n            generator_2=self.generator_BtoA,\n            discriminator=self.discriminator_B\n        )\n        self.combined_BtoA = self._build_combined(\n            generator_1=self.generator_BtoA,\n            generator_2=self.generator_AtoB,\n            discriminator=self.discriminator_A\n        )\n\n    def save_samples(self, epoch, batch_i, generator):\n        \"\"\"\n        Saves samples of source, target, and generated images to the\n        generated_images directory.\n        \"\"\"\n        os.makedirs('cyclegan_generated_images\/', exist_ok=True)\n        r, c = 3, 3\n        \n        if generator is None:\n            generator = self.generator_BtoA\n        \n        # load source and target images, and generate a fake image\n        source_imgs, target_imgs = self.data_loader.load_data(batch_size=3, use_for_training=False)\n        fake_imgs = generator.predict(target_imgs)\n\n        # combine them and rescale them to range from 0 to 1\n        gen_imgs = np.concatenate([target_imgs, fake_imgs, source_imgs])\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        titles = ['Target', 'Generated', 'Source']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i, j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[i])\n                axs[i, j].axis('off')\n                cnt += 1\n        fig.savefig(\"cyclegan_generated_images\/%d_%d.png\" % (epoch, batch_i))\n        plt.close()\n\n    def train(self, nbr_epochs, batch_size, sample_interval=100):\n        \"\"\"\n        Trains the GAN.  In each iteration, the discriminator is first\n        updated with real samples, and then with fake\/generated samples.\n        Then the generator is updated.\n\n        Remember that:\n            A = source domain\n            B = target domain\n        \"\"\"\n        # build the GAN\n        self._build_gan()\n\n        for epoch in tqdm(range(nbr_epochs)):\n            g_loss1_hist, g_loss2_hist = [], []\n            d_A_loss1_hist, d_A_loss2_hist, d_B_loss1_hist, d_B_loss2_hist  = [], [], [], []\n            for batch_i, (x_real_source, x_real_target) in enumerate(\n                    self.data_loader.load_batch(batch_size)\n            ):\n                # create the corresponding adversarial loss ground truth labels for real images\n                y_real = np.ones((len(x_real_source),) + self.discriminator_patch_size)\n\n                # generate a batch of fake samples\n                x_fake_target, y_fake = generate_fake_samples(\n                    generator=self.generator_AtoB,\n                    real_source_image_samples=x_real_source,\n                    nbr_patches=self.discriminator_patch_size[1]\n                )\n                x_fake_source, _ = generate_fake_samples(\n                    generator=self.generator_BtoA,\n                    real_source_image_samples=x_real_target,\n                    nbr_patches=self.discriminator_patch_size[1]\n                )\n\n                # update generator for target -> source via adversarial and cycle loss\n                g_loss2, _, _, _, _ = self.combined_BtoA.train_on_batch(\n                    [x_real_target, x_real_source],\n                    [y_real, x_real_source, x_real_target, x_real_source]\n                )\n                g_loss2_hist.append(g_loss2)\n\n                # update discriminator for source -> real\/fake prediction\n                d_A_loss1 = self.discriminator_A.train_on_batch(x_real_source, y_real)\n                d_A_loss2 = self.discriminator_A.train_on_batch(x_fake_source, y_fake)\n                d_A_loss1_hist.append(d_A_loss1)\n                d_A_loss2_hist.append(d_A_loss2)\n\n                # update generator for source -> target via adversarial and cycle loss\n                g_loss1, _, _, _, _ = self.combined_AtoB.train_on_batch(\n                    [x_real_source, x_real_target],\n                    [y_real, x_real_target, x_real_source, x_real_target]\n                )\n                g_loss1_hist.append(g_loss1)\n\n                # update discriminator for target -> real\/fake prediction\n                d_B_loss1 = self.discriminator_B.train_on_batch(x_real_target, y_real)\n                d_B_loss2 = self.discriminator_B.train_on_batch(x_fake_target, y_fake)\n                d_B_loss1_hist.append(d_B_loss1)\n                d_B_loss2_hist.append(d_B_loss2)\n\n                # store sample images\n                if batch_i % sample_interval == 0:\n                    self.save_samples(\n                        epoch=epoch, batch_i=batch_i, \n                        generator=self.generator_AtoB\n                    )\n\n            # show performance on epoch (indent this block for batch level performance)\n            print(\n                f\"Epoch {epoch + 1} batch {batch_i + 1} performance:\\n\",\n                f\"Discriminator A real loss: {d_A_loss1} \\n\",\n                f\"Discriminator A fake loss: {d_A_loss2} \\n\",\n                f\"Discriminator B real loss: {d_B_loss2} \\n\",\n                f\"Discriminator B fake loss: {d_B_loss2} \\n\",\n                f\"Generator source to target weighted avg loss: {g_loss1} \\n\",\n                f\"Generator target to source weighted avg loss: {g_loss2} \\n\",\n            )\n\n        return (\n            g_loss1_hist, g_loss2_hist,\n            d_A_loss1_hist, d_A_loss2_hist, d_B_loss1_hist, d_B_loss2_hist\n        )\n    \n    def save_generator(self):\n        \"\"\"\n        Saves a trained CycleGAN generator model to a h5 file.\n        \"\"\"\n        filename = \"cyclegan_generator_AtoB_model.h5\"\n        self.generator_AtoB.save(filename)\n        filename = \"cyclegan_generator_BtoA_model.h5\"\n        self.generator_BtoA.save(filename)\n\n\ncg = CycleGAN(input_image_shape=IMAGE_SHAPE)\nlosses = cg.train(\n    nbr_epochs=NBR_EPOCHS, \n    batch_size=BATCH_SIZE,\n    sample_interval=10\n)\ncg.save_generator()","c66490ea":"# load the model and make prediction on a given image\ncyclegan_model = load_model('cyclegan_generator_AtoB_model.h5')\nsource_img = cv2.imread(train_files[0])[:, :256]\ntarget_img = cv2.imread(train_files[0])[:, 256:]\n\n# resize\nsource_img = resize_image(img=source_img, size=IMAGE_SIZE)\ntarget_img = resize_image(img=target_img, size=IMAGE_SIZE)\n\n# scale pixel values\nsource_img = np.array(source_img)\/127.5 - 1.\ntarget_img = np.array(target_img)\/127.5 - 1.\n\n# reshape\nsource_img = np.expand_dims(source_img, 0)\ntarget_img = np.expand_dims(target_img, 0)\n\n# make prediction, re-scale and reshape\ngen_img = cyclegan_model.predict(source_img)[0]\ngen_img = (gen_img + 1) \/ 2.0\ngen_img = np.expand_dims(gen_img, 0)\n\n# combine and re-scale to (0, 1)\nall_imgs = np.concatenate([target_img, gen_img, source_img])\nall_imgs = 0.5 * all_imgs + 0.5\n\ntitles = ['Target', 'Generated', 'Source']\nfor idx, i in enumerate(all_imgs):\n    plt.imshow(i)\n    plt.title(titles[idx])\n    plt.show()","5e50df15":"class EvalGAN:\n    def __init__(self, real_images, fake_images):\n        self.real_images = preprocess_input(\n            np.array([\n                resize_image(i, size=(299, 299)) for i in real_images\n            ])\n        )\n        self.fake_images = preprocess_input(\n            np.array([\n                resize_image(i, size=(299, 299)) for i in fake_images\n            ])\n        )\n        self.model = InceptionV3(\n            include_top=False, \n            pooling='avg', \n            input_shape=(299, 299, 3)\n        )\n    \n    def calculate_fid(self):\n        \"\"\"\n        Calculates the Frechet Inception Distance (FID) score.\n        Implemented from: https:\/\/machinelearningmastery.com\/how-to-implement-the-frechet-inception-distance-fid-from-scratch\/\n        \"\"\"\n        # calculate activations\n        act1 = self.model.predict(self.real_images)\n        act2 = self.model.predict(self.fake_images)\n        # calculate mean and covariance statistics\n        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n        mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n        # calculate sum squared difference between means\n        ssdiff = np.sum((mu1 - mu2)**2.0)\n        # calculate sqrt of product between cov\n        covmean = sqrtm(sigma1.dot(sigma2))\n        # check and correct imaginary numbers from sqrt\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n        # calculate score\n        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fid\n\n\n[real_source_images, real_target_images], _ = load_real_samples(\n    batch_size=BATCH_SIZE, \n    nbr_patches=1\n)\n\n# Evaluate Pix2Pix\nfake_target_images, _ = generate_fake_samples(\n    generator=p2p_model, \n    real_source_image_samples=real_source_images, \n    nbr_patches=1\n)\n\ngeval = EvalGAN(real_target_images, fake_target_images)\nfid = geval.calculate_fid()\nprint(\"Pix2Pix FID:\", fid)\n\n# Evaluate CycleGAN\nfake_target_images, _ = generate_fake_samples(\n    generator=cyclegan_model, \n    real_source_image_samples=real_source_images, \n    nbr_patches=1\n)\n\ngeval = EvalGAN(real_target_images, fake_target_images)\nfid = geval.calculate_fid()\nprint(\"CycleGAN FID:\", fid)","be5800cc":"# Build Pix2Pix Model","721fac31":"# Utility Functions","bb9b6398":"# Data Exploration","78e5adca":"# Background and Purpose\n\nGANs are hard to evaluate.  The images they generate are often judged subjectively, based on how real they look.  The Frechet Inception distance (FID) was proposed as a way to more objectively evaluate GANs.  This notebook compares Pix2Pix and CycleGAN for the same image to image translation task, using the Edges2Shoes image set from the Pix2Pix dataset.  The FID is used to score the models.  Due to memory constraints, CycleGAN is run on a very small batch size, and the load_batch generator in the dataloader is restricted to 200 samples.  These restrictions should be removed for a fair comparison, but this would require more memory.","db00d1b9":"# GAN Evaluation using Frechet Inception Distance","ac275255":"# Build CycleGAN"}}