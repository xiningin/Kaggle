{"cell_type":{"1f57f2c1":"code","28110067":"code","9b64672b":"code","68d89db9":"code","3f5c4a9c":"code","24b03e39":"code","c872034d":"code","9ff84903":"code","4857f6d1":"code","2f7eb76b":"code","65bd6cae":"code","55c6eb3c":"code","7382f515":"code","0d976fce":"code","e8079f08":"code","b3759a2e":"code","a2a01f30":"code","cd889f3a":"code","8b515dad":"code","9383c8fc":"code","3d699502":"code","96d642f5":"code","49af526c":"code","b02ca36a":"code","21be03bc":"code","e573d62f":"code","97ec3184":"code","f4dab668":"code","1a3422b0":"code","1f43b8c9":"code","4418f8a3":"code","a3f7319a":"code","ac2e344c":"code","c829b840":"code","b8fb0887":"code","a0c06731":"code","be4f28c2":"code","f8bfc8b3":"code","134ea6b6":"code","eecce038":"code","d97edfdd":"code","7992f6e5":"markdown","b12e22c8":"markdown","c4cc863f":"markdown","56e49885":"markdown","b74804d9":"markdown","96bb6c00":"markdown","4a194d8c":"markdown","45dfaa4c":"markdown","db77cabd":"markdown","e641db9e":"markdown","aada18dc":"markdown","7af60c2c":"markdown"},"source":{"1f57f2c1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom inspect import signature\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n   \n    \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","28110067":"heart_dm= pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\nheart_dm.head()","9b64672b":"# Sex Dictionary\nsex_dict = { 0: 'Female',1: 'Male' }\n\n# Chest Pain Dictionary\nchest_pain_dict = { 0:'Typical Angina', 1:'Atypical Angina', 2:'Non-Anginal Pain', 3:'Asymptomatic'}\n\n# Blood Sugar  Dictionary\nfbs_dict = { 0: 'No Blood Sugar', 1: 'Blood Sugar'}\n\n# Rest ECG\nrestecg_dict = { 0:'Normal restecg', 1:'ST-T wave abnormality restecg', 2:'ventricular hypertrophy restecg'}\n\n# Exercise Induced Angina\nexang_dict = { 0:'Exang No', 1:'Exang Yes'}\n\n# Slope of the peak exercise ST segment \nslope_dict = { 0:'Upsloping', 1:'flat', 2:'Downsloping'}\n\n# Number of Major Vessels (0-3) Colored by Flourosopy\nca_dict = { 0:'Major vessel 0', 1:'Major vessel 1', 2:'Major vessel 2', 3:'Major vessel 3', 4:'Major vessel 4'}\n\n# Thalassemia Dictionary\nthal_dict = { 0: 'None', 1: 'Normal', 2:'Fixed Defect',3:'Reversable Defect'}\n\n# Target Dictionary\ntarget_dict = { 0: 'Not Present', 1: 'Present'}","68d89db9":"df = heart_dm[['age']]\n\n# trestbps as Resting Blood Pressure\ndf['Resting Blood Pressure'] = heart_dm['trestbps']\n\n# chol as Serum Cholestoral\ndf['Serum Cholestoral'] = heart_dm['chol']\n\n# thalach as Max. Heart Rate\ndf['Max. Heart Rate'] = heart_dm['thalach']\n\n# old peak ST Depression\ndf['ST Depression'] = heart_dm['oldpeak']\n\n# sex as Sex\ndf['Sex'] = heart_dm['sex'].apply(lambda x:sex_dict[x])\n\n# thal as Thalassemia\ndf['Thalassemia'] = heart_dm['thal'].apply(lambda x:thal_dict[x])\n\n# fbs as Thalassemia\ndf['Fasting Blood Sugar'] = heart_dm['fbs'].apply(lambda x:fbs_dict[x])\n\n# cp as Chest Pain\ndf['Chest Pain'] = heart_dm['cp'].apply(lambda x:chest_pain_dict[x])\n\n# target as Heart Disease\ndf['Heart Disease'] = heart_dm['target'].apply(lambda x:target_dict[x])","3f5c4a9c":"df.head()","24b03e39":"sns.set(style = \"darkgrid\")\nsns.countplot(x = \"Heart Disease\", data = df, palette = \"bwr\")","c872034d":"countNoDisease = len(df[df['Heart Disease'] == 'Not Present'])\ncountHaveDisease = len(df[df['Heart Disease'] == 'Present'])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df['Heart Disease'])) * 100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df['Heart Disease'])) * 100)))","9ff84903":"\n\npd.crosstab(df['Sex'], df['Heart Disease']).plot(kind = 'bar', figsize = (13,5), color = ['#0000ff', '#ff0000'])\n\n","4857f6d1":"pd.crosstab(df['age'], df['Heart Disease']).plot(kind = \"bar\", figsize = (20,6))","2f7eb76b":"pd.crosstab(df['ST Depression'], df['Heart Disease']).plot(kind = \"bar\", figsize = (15,6), color = ['#0000ff','#FF5733' ])","65bd6cae":"\n\npd.crosstab(df['Fasting Blood Sugar'], df['Heart Disease']).plot(kind = \"bar\", figsize = (15,6), color = ['#FFC300','#581845' ])\n\n","55c6eb3c":"pd.crosstab(df['Chest Pain'], df['Heart Disease']).plot(kind = \"bar\", figsize = (15,6), color = ['#11A5AA','#AA1190' ])","7382f515":"pd.crosstab(heart_dm.thal, df['Heart Disease']).plot(kind = \"bar\", figsize = (15,6), color = ['#99A6BB','#AA4510' ])","0d976fce":"#get correlations of each features in dataset\ncorrmat = heart_dm.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize = (20,20))\n\n#plot heat map\nsns.heatmap(heart_dm[top_corr_features].corr(), annot = True, cmap = \"Blues\")","e8079f08":"\n\n#Correlation with output variable\ncor_target = abs(corrmat[\"target\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target > 0.10]\nrelevant_features\n\n","b3759a2e":"X = heart_dm.drop(['fbs','chol','target'], 1)\nY = heart_dm['target']","a2a01f30":"#standardizing the input feature\n\nsc = StandardScaler()\nX = sc.fit_transform(X)","cd889f3a":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)","8b515dad":"def plt_conf_matrix(pred):\n  # Confusion Matrix\n  cm = confusion_matrix(Y_test, pred)\n  plt.figure(figsize = (5, 4))\n  sns.heatmap(cm, xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative'], annot = True, fmt = 'd', cmap=\"Blues\")\n  plt.title('Confusion Matrix')\n  plt.ylabel('Actual Values')\n  plt.xlabel('Predicted Values')\n  plt.show()\n  \ndef plt_var_dev(scores):\n  #Variance & Dev. standard\n  print('\\nVariance: {}'.format(round(np.var(scores) * 100, 2)))\n  print('Dev. standard: {}\\n'.format(round(np.std(scores) * 100, 2)))\n  data = {'variance': np.var(scores), 'standard dev': np.std(scores)}\n  names = list(data.keys())\n  values = list(data.values())\n  fig,axs = plt.subplots(1, 1, figsize = (5, 3), sharey = True)\n  axs.bar(names, values)\n  plt.show()\n\ndef plt_auc(model):\n  #AUC\n  probs = model.predict_proba(X_test)\n  # keep probabilities for the positive outcome only\n  probs = probs[:, 1]\n  auc = roc_auc_score(Y_test, probs)\n  print('\\nAUC: %.2f\\n' % auc)\n  # calculate roc curve\n  fpr, tpr, thresholds = roc_curve(Y_test, probs)\n  plt.figure(figsize = (5, 4))\n  # plot no skill\n  plt.plot([0, 1], [0, 1], linestyle = '--')\n  # plot the roc curve for the model\n  plt.plot(fpr, tpr, marker = '.')\n  plt.xlabel('FP RATE')\n  plt.ylabel('TP RATE')\n  plt.show()\n  \ndef plt_prec_rec(pred):\n  #Precision-Recall Curve\n  average_precision = average_precision_score(Y_test, pred)\n  precision, recall, _ = precision_recall_curve(Y_test, pred)\n  print('\\nAP = {0:0.2f}\\n'.format(average_precision))\n  step_kwargs = ({'step': 'post'}\n                 if 'step' in signature(plt.fill_between).parameters\n                 else {})\n  plt.figure(figsize = (5, 4))\n  plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n  plt.fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n  plt.xlabel('Recall')\n  plt.ylabel('Precision')\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.title('Precision-Recall curve')","9383c8fc":"scores = []\nmodels_name = []","3d699502":"sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\nsgd.fit(X_train, Y_train)\ny_pred_sgd = sgd.predict(X_test)\nscore_sgd = round(accuracy_score(y_pred_sgd,Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Stochastic Gradient Discent is: \" + str(score_sgd) + \" %\")\n\nscores.append(score_sgd)\nmodels_name.append('SGD')","96d642f5":"\nlr = LogisticRegression()\nlr.fit(X_train,Y_train)\ny_pred_lr = lr.predict(X_test)\nscore_lr = round(accuracy_score(y_pred_lr,Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \" + str(score_lr) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_lr))\n\nplt_conf_matrix(y_pred_lr)\nplt_auc(lr)\nplt_prec_rec(y_pred_lr)\n\nscores.append(score_lr)\nmodels_name.append('LGR')","49af526c":"cv = ShuffleSplit(n_splits = 5, test_size = 0.15, random_state = 0)\nscores_lr = cross_val_score(lr, X, heart_dm.target, cv = cv)\nscore_lr = round(scores_lr.mean() * 100, 2)\n\nprint(\"The accuracy score achieved using LR & Cross-Validation technique is: \" + str(score_lr) + \" %\")\n\nscores.append(score_lr)\nmodels_name.append('LGR')\n\nplt_var_dev(scores_lr)","b02ca36a":"k_range = range(1,10)\nerror = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train,Y_train)\n    y_pred_knn = knn.predict(X_test)\n    score_knn = round(accuracy_score(y_pred_knn,Y_test) * 100, 2)\n    error.append(np.mean(y_pred_knn != Y_test))\n    \nplt.figure(figsize=(4,4))\nplt.plot(range(1, 10), error)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error')\nplt.show()\n\nscores.append(max(scores_list))\nmodels_name.append('KNN')","21be03bc":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train,Y_train)\ny_pred_knn = knn.predict(X_test)\nscore_knn = round(accuracy_score(y_pred_knn, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using KNN is: \" + str(score_knn) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_knn))\n\nplt_conf_matrix(y_pred_knn)\nplt_auc(knn)\nplt_prec_rec(y_pred_knn)\n\n","e573d62f":"cv = ShuffleSplit(n_splits = 5, test_size = 0.15, random_state = 0)\nscores_knn = cross_val_score(knn, X, heart_dm.target, cv = cv)\nscore_knn = round(scores_knn.mean() * 100, 2)\n\nprint(\"The accuracy score achieved using KNN & Cross-Validation technique is: \" + str(score_knn) + \" %\")\n\nscores.append(score_knn)\nmodels_name.append('KNN')\n\nplt_var_dev(scores_knn)","97ec3184":"\n\nnb = GaussianNB()\nnb.fit(X_train,Y_train)\ny_pred_nb = nb.predict(X_test)\nscore_nb = round(accuracy_score(y_pred_nb,Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Naive Bayes is: \" + str(score_nb) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_nb))\n\nplt_conf_matrix(y_pred_nb)\nplt_auc(nb)\nplt_prec_rec(y_pred_nb)\n\nscores.append(score_nb)\nmodels_name.append('GNB')","f4dab668":"cv = ShuffleSplit(n_splits = 5, test_size = 0.15, random_state = 0)\nscores_nb = cross_val_score(nb, X, heart_dm.target, cv = cv)\nscore_nb = round(scores_nb.mean() * 100, 2)\n\nprint(\"The accuracy score achieved using Gaussian Naive Bayes & Cross-Validation technique is: \" + str(score_nb) + \" %\")\n\nscores.append(score_nb)\nmodels_name.append('GNB')\n\nplt_var_dev(scores_nb)","1a3422b0":"dt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\ny_pred_dt = dt.predict(X_test)\nscore_dt = round(accuracy_score(y_pred_dt, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \" + str(score_dt) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_dt))\n\nscores.append(score_dt)\nmodels_name.append('DT')\n\nplt_conf_matrix(y_pred_dt)\nplt_auc(dt)\nplt_prec_rec(y_pred_dt)","1f43b8c9":"\n\nsv = SVC(kernel = 'sigmoid')\nsv.fit(X_train, Y_train)\ny_pred_svm = sv.predict(X_test)\nscore_svm = round(accuracy_score(y_pred_svm, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Linear SVM is: \" + str(score_svm) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_svm))\n\nplt_conf_matrix(y_pred_svm)\nplt_prec_rec(y_pred_svm)\n\nscores.append(score_svm)\nmodels_name.append('SVM')\n","4418f8a3":"cv = ShuffleSplit(n_splits = 5, test_size = 0.15, random_state = 0)\nscores_svm = cross_val_score(sv, X, heart_dm.target, cv = cv)\nscore_svm = round(scores_svm.mean() * 100, 2)\n\nprint(\"The accuracy score achieved using SVM & Cross-Validation technique is: \" + str(score_svm) + \" %\")\n\nscores.append(score_svm)\nmodels_name.append('SVM')\n\nplt_var_dev(scores_svm)","a3f7319a":"rf = RandomForestClassifier(n_estimators = 100, bootstrap = True)\nrf.fit(X_train, Y_train)\ny_pred_rf = rf.predict(X_test)\nscore_rf = round(accuracy_score(y_pred_rf, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Random Forest is: \" + str(score_rf) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_rf))\n\n\nscores.append(score_rf)\nmodels_name.append('RF')\n\nplt_conf_matrix(y_pred_rf)\nplt_auc(rf)\nplt_prec_rec(y_pred_rf)","ac2e344c":"classifier = Sequential()\n# First Hidden Layer\nclassifier.add(Dense(4, activation = 'relu', kernel_initializer = 'random_normal', input_dim = 11))\n# Second  Hidden Layer\nclassifier.add(Dense(4, activation = 'relu', kernel_initializer = 'random_normal'))\n# Output Layer\nclassifier.add(Dense(1, activation = 'sigmoid', kernel_initializer = 'random_normal'))","c829b840":"# Compiling the neural network\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","b8fb0887":"# Fitting the data to the training dataset\nhistory = classifier.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 100, batch_size = 16, verbose = 2)","a0c06731":"# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()\n\n","be4f28c2":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()","f8bfc8b3":"\n\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n","134ea6b6":"cm = confusion_matrix(Y_test, y_pred)\ntrue_pos = np.diag(cm)\nfalse_pos = np.sum(cm, axis = 0) - true_pos\nfalse_neg = np.sum(cm, axis = 1) - true_pos\nscore_nn = round(np.sum(true_pos)\/(np.sum(true_pos) + np.sum(false_pos)) * 100, 2)\n\nprint(\"\\nThe accuracy score achieved using Neural Network is: \" + str(score_nn) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred))\n\n\nplt_conf_matrix(y_pred)\nplt_prec_rec(y_pred)\n\nscores.append(score_nn)\nmodels_name.append('NN')","eecce038":"true_pos = np.diag(cm)\nfalse_pos = np.sum(cm, axis = 0) - true_pos\nfalse_neg = np.sum(cm, axis = 1) - true_pos\nscore_nn = round(np.sum(true_pos)\/(np.sum(true_pos) + np.sum(false_pos)) * 100, 2)\n\nprint(\"The accuracy score achieved using Neural Network is: \" + str(score_nn) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred))\n\nplt_conf_matrix(y_pred)\nplt_prec_rec(y_pred)\n\nscores.append(score_nn)\nmodels_name.append('NN')","d97edfdd":"\n\nprint(scores)\n\ncolors = [\"purple\", \"green\", \"orange\", \"magenta\", \"red\", \"yellow\", \"grey\", \"blue\"]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize = (8, 5))\nplt.yticks(np.arange(0, 100, 10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x = scores, y = models_name, palette = colors)\nplt.show()\n\n","7992f6e5":"# Neural Network","b12e22c8":"# Gaussian Naive Bayes - Probabilistic Classifier","c4cc863f":"# Support Vector Machine","56e49885":"1.Age\n\n2.Sex\n\n3.Chest Pain Type (4 values)\n\n4.Resting Blood Pressure\n\n5.Serum Cholestoral in mg\/dl\n\n6.Fasting Blood Sugar > 120 mg\/dl\n\n7.Resting Electrocardiographic results (values 0, 1, 2)\n\n8.Maximum Heart Rate Achieved\n\n9.Exercise Induces Angina\n\n10.Oldpeak = ST depression induced by exercise relative to rest\n\n11.The Slope Of The Peak Exercise ST segment\n\n12.Number Of Major Vessels (0-3) Colored By Flourosopy\n\n13.Thal: 3=normal; 6=fixed defect; 7=reversable defect","b74804d9":"# K-Nearest Neighbor","96bb6c00":"# Logistic Regression\n","4a194d8c":"# Decision Tree - Probabilistic Classifier","45dfaa4c":"# Creating new DataFrame","db77cabd":"# Supervised Algorithms","e641db9e":"Stocastich Gradient Descent","aada18dc":"We plot the heatmap by using the correlation for the dataset. This helps us eliminate any features that may not help with prediction.","7af60c2c":"# Random Forest"}}