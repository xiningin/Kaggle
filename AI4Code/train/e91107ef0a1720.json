{"cell_type":{"d3ab9ca3":"code","fcadfcef":"code","57ec157f":"code","d00c844a":"code","2dafea76":"code","b7461ca4":"markdown","bd867f1c":"markdown"},"source":{"d3ab9ca3":"import gym\nimport numpy as np\nfrom collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nimport random\n\nclass DQLAgent:\n    def __init__(self, env):\n        # hiperparametre\n        self.state_size = env.observation_space.shape[0]\n        self.action_size = env.action_space.n\n        \n        self.gamma = 0.95\n        self.learning_rate = 0.001\n        \n        self.epsilon = 1 #explore\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n        \n        self.memory = deque(maxlen = 1000)\n        \n        self.model = self.build_model()\n        \n    def build_model(self):\n        # NN for q-learning\n        model = Sequential()\n        model.add(Dense(48, input_dim = self.state_size, activation = \"tanh\"))\n        model.add(Dense(self.action_size, activation= \"linear\"))\n        model.compile(loss = \"mse\", optimizer=Adam(lr = self.learning_rate))\n        return model\n    \n    def remember(self, state, action, reward, next_state,done):\n        self.memory.append((state, action, reward, next_state,done))\n    \n    def act(self,state):\n        #acting explore or expoit\n        if random.uniform(0,1) <= self.epsilon:\n            return env.action_space.sample()\n        else:\n            act_values = self.model.predict(state)\n            return np.argmax(act_values[0])\n    \n    def replay(self, batch_size):\n        # training\n        if len(self.memory) < batch_size:\n            return\n        else:\n            minibatch = random.sample(self.memory, batch_size)\n            for state, action, reward, next_state, done in minibatch:\n                if done:\n                    target = reward\n                else:\n                    target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n                train_target = self.model.predict(state)\n                train_target[0][action] = target\n                self.model.fit(state, train_target, verbose=0)\n                \n    def adaptiveEGreedy(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","fcadfcef":"if __name__ == \"__main__\":\n    \n    #initilaze env and agent\n    env = gym.make(\"CartPole-v0\")\n    agent = DQLAgent(env)\n    \n    batch_size = 16\n    episodes = 40\n    for e in range(episodes):\n        \n        #initilaze env\n        state = env.reset()\n        state = np.reshape(state,[1,4])\n        time = 0\n        while(True):\n            \n            # act\n            action = agent.act(state) # select an action \n            # step\n            next_state, reward, done, _ = env.step(action)\n            next_state = np.reshape(next_state,[1,4])\n            \n            # remember \/ storage\n            agent.remember(state, action, reward, next_state, done)\n            # update step\n            state = next_state\n            # replay\n            agent.replay(batch_size)\n            # adjust epsilon\n            agent.adaptiveEGreedy()\n            time = time + 1\n            \n            if done:\n                print(\"Episode: {},  Time: {}\".format(e,time))\n                break","57ec157f":"!apt-get install python-opengl -y","d00c844a":"!pip install pyvirtualdisplay","2dafea76":"import time\n\ntrained_model = agent\nstate = env.reset()\nstate = np.reshape(state,[1,4])\ntime_t = 0\nwhile True:\n    env.render()\n    action = trained_model.act(state)\n    next_state, reward, done, _ = env.step(action)\n    next_state = np.reshape(next_state, [1,4])\n    state= next_state\n    time_t += 1\n    print(time_t)\n    #time.sleep(0.1)\n    if done:\n        break\n        \nprint(\"Done\")","b7461ca4":"## Hi fellas. In this notebook, I will show you have to keep upright","bd867f1c":"## A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."}}