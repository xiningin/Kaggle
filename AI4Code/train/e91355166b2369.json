{"cell_type":{"e43eb325":"code","ae51334f":"code","aa7e3891":"code","a0468e52":"code","f0b7b3c2":"code","52dfc37f":"code","07fa14b3":"code","15489564":"code","ab5d2181":"code","50255b8e":"code","1eb043a1":"code","7fdb9a3a":"markdown","90b6a4e6":"markdown","e7048b95":"markdown","e1fdd37d":"markdown","42f696e6":"markdown","7e8a4268":"markdown","23cfbd7a":"markdown","19cfecfd":"markdown","a4e3e544":"markdown","cf9784cc":"markdown"},"source":{"e43eb325":"import numpy as np\nimport matplotlib.pyplot as plt","ae51334f":"x = np.array(12)\nx","aa7e3891":"x = np.array([12, 4, 6, 8, 14])\nx\nx.ndim","a0468e52":"x = np.array([[5, 78, 2, 34, 0],\n[6, 79, 3, 35, 1],\n[7, 80, 4, 36, 2]])\nx.ndim","f0b7b3c2":"from numpy import array\n\nT = array([\n  [\n  [1,2,3],    [4,5,6],    [7,8,9]],\n  [[11,12,13], [14,15,16], [17,18,19]],\n  [[21,22,23], [24,25,26], [27,28,29]\n  ],\n  ])\nprint(T.shape)\nprint(\"3D Tensor T is: \", T)\nprint(\"Dimension of 3D Tensor T is: \", T.ndim)","52dfc37f":"from keras.datasets import mnist\n(X_train_images, Y_train), (X_test_images, Y_test) = mnist.load_data()\n\n# To display the number of axes of the tensor X_train_images , the ndim attribute:\nprint(X_train_images.ndim)\n\n# Here\u2019s its shape:\nprint('shape of X_train_images', X_train_images.shape)\n\n# And this is its data type, the dtype attribute:\nprint('dtypes ', X_train_images.dtype)","07fa14b3":"digit = X_train_images[4]\nplt.imshow(digit, cmap=plt.cm.binary)\n# plt.imshow just finishes drawing a picture instead of printing it. If you want to print the picture, you just need to add plt.show.\nplt.show()","15489564":"fig = plt.figure()\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    plt.tight_layout()\n    plt.imshow(X_test_images[i], cmap='gray', interpolation='none')\n    plt.title(\"Digits: {}\".format(Y_train[i]))\n    plt.xticks([])\n    plt.yticks([])","ab5d2181":"my_slice = X_train_images[10:100]\nprint(my_slice.shape)","50255b8e":"# The below slice notation with colon (:) is equivalent to the previous implementation\nmy_slice = X_train_images[10:100, :, :]\nmy_slice.shape","1eb043a1":"# Even the below slice notation with colon (:) is also equivalent to the previous implementation\nmy_slice = X_train_images[10:100, 0:28, 0:28]\nmy_slice.shape","7fdb9a3a":"---\n\n## Vectors (1D tensors)\n\nAn array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly one axis. Following is a Numpy vector:\n","90b6a4e6":"## Data representations for neural networks - Scaler, Vector, Tensor\n\nIn general, all current machine-learning systems use tensors as their basic data structure. Tensors are fundamental to the field\u2014so fundamental that Google\u2019s TensorFlow was named after them. Even the text data or image data are converted to Numerical features for processing.\n\n## So what\u2019s a tensor?\n\nAt its core, a tensor is a container for data\u2014almost always numerical data. So, it\u2019s a container for numbers. You may be already familiar with matrices, which are 2D tensors: tensors are a generalization of matrices to an arbitrary number of dimensions\n(note that in the context of tensors, a dimension is often called an axis).\n\n\n## Scalars (0D tensors)\n\nA tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional tensor, or 0D tensor). In Numpy, a float32 or float64 number is a scalar tensor (or scalar array). You can display the number of axes of a Numpy tensor via the ndim attribute; a scalar tensor has 0 axes (ndim == 0 ). The number of axes of a tensor is also called its rank. Here\u2019s a Numpy scalar:\n","e7048b95":"The MNIST dataset will be loaded as a set of training and test inputs (X) and outputs (Y). The imputs are samples of digit images while the outputs contain the numerical value each input represents.\n\nSo what we have here is a 3D tensor of 8-bit integers. More precisely, it\u2019s an array of 60,000 matrices of 28 \u00d7 8 integers. Each such matrix is a grayscale image, with coefficients between 0 and 255.\n\nLet\u2019s display the fourth digit in this 3D tensor, using Matplotlib's **[imshow](https:\/\/matplotlib.org\/3.3.3\/api\/_as_gen\/matplotlib.pyplot.imshow.html)** function. Below are some details on this function from its official docs.\n\nThe input may either be actual RGB(A) data, or 2D scalar data, which will be rendered as a pseudocolor image. For displaying a grayscale image set up the color mapping using the parameters cmap='gray', vmin=0, vmax=255.\n\nThe number of pixels used to render an image is set by the axes size and the dpi of the figure.\n\n### A note on image size, pixel and numpy array\n\nImages are made up of pixels and each pixel is a dot of color. Lets say I have a cat image of 1200\u00d7800 pixels. When an image is loaded into a computer, it is saved as an array of numbers. Each pixel in a color image is made up of a Red, Green and Blue (RGB) part. It can take any value between 0 and 255 with 0 being the darkest and 255 being the brightest. In a grayscale image, each pixel is represented by just a single number between 0 and 1. If a pixel is 0, it is completely black, if it is 1 it is completely white. Everything in between is a shade of gray.\n\nSo, if the cat image was black and white, it would be a 2D numpy array with shape (800, 1200). As it is a color image, it is in fact a 3D numpy array (to represent the three different color channels) with shape (800, 1200, 3).\n\nNow to display our mnist image data as an image, i.e., on a 2D regular raster.\n\n","e1fdd37d":"#### The parameter tight-layout to fit plots within your figure cleanly.\n\ntight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area. This is an experimental feature and may not work for some cases. It only checks the extents of ticklabels, axis labels, and titles.\n\nAn alternative to tight_layout is constrained_layout.\n\n## Manipulating tensors in Numpy\n\nIn the previous example, we selected a specific digit alongside the first axis using the syntax X_train_images[i] . Selecting specific elements in a tensor is called tensor slicing.\nLet\u2019s look at the tensor-slicing operations you can do on Numpy arrays. The following example selects digits #10 to #100 (#100 isn\u2019t included) and puts them in an array of shape (90, 28, 28):","42f696e6":"#### Let's inspect a few examples of this MNIST dataset, containing only grayscale images.\n\nI will use the subplot() funtion - The Matplotlib subplot() function can be called to plot two or more plots in one figure. Matplotlib supports all kind of subplots including 2x1 vertical, 2x1 horizontal or a 2x2 grid.\n\nHere is the arguments for matplotlib.pyplot.subplots:\n\n*args, (int, int, index)\n\nThree integers (nrows, ncols, index). The subplot will take the index position on a grid with nrows rows and ncols columns. index starts at 1 in the upper left corner and increases to the right.\n\nThere is also an important difference between `plt.subplots()` and `plt.subplot()`, notice the missing `'s'` at the end.\n\nWe can use `plt.subplots()` to make all their subplots at once and it returns the figure and axes (plural of axis) of the subplots as a tuple. A figure can be understood as a canvas where you paint your sketch.\n\n```python\n    # create a subplot with 2 rows and 1 columns\n    fig, ax = plt.subplots(2,1)\n```\nWhereas, you can use `plt.subplot()` if you want to add the subplots separately. It returns only the axis of one subplot.\n\n```python\n    fig = plt.figure() # create the canvas for plotting\n    ax1 = plt.subplot(2,1,1)\n    # (2,1,1) indicates total number of rows, columns, and figure number respectively\n    ax2 = plt.subplot(2,1,2)\n```\n\nIn many cases, `plt.subplots()` is preferred because it gives you easier options to directly customize your whole figure\n\n```python\n    # for example, sharing x-axis, y-axis for all subplots can be specified at once\n    fig, ax = plt.subplots(2,2, sharex=True, sharey=True)\n```\n\nwhereas, with `plt.subplot()`, one will have to specify individually for each axis which can become cumbersome.","7e8a4268":"## A note on Python slice notation (:)\n\n#### So what does the 3 mean in somesequence[::3] ?\n\n`:` is the delimiter of the slice syntax to 'slice out' sub-parts in sequences , `[start:end]`\n\n    [1:5] is equivalent to \"from 1 to 5\" (5 not included)\n    [1:] is equivalent to \"1 to end\"\n    [len(a):] is equivalent to \"from length of a to end\"\n\n    Remember that [1:5] starts with the object at index 1, and the object at index 5 is not included.\n\nThe third parameter is the step. So, it means 'nothing for the first argument, nothing for the second, and jump by three'. It gets every third item of the sequence sliced.\n\n[::3] just means that you have not specified any start or end indices for your slice. Since you have specified a step, 3, this will take every third entry of something starting at the first index. For example:\n\n```python\n'123123123'[::3]\n# '111'\n```\n\n[source](https:\/\/stackoverflow.com\/a\/509295\/1902852)\n\n    a[start:stop]  # items start through stop-1\n    a[start:]      # items start through the rest of the array\n    a[:stop]       # items from the beginning through stop-1\n    a[:]           # a copy of the whole array\n\nThere is also the `step` value, which can be used with any of the above:\n\n    a[start:stop:step] # start through not past stop, by step\n\nThe key point to remember is that the `:stop` value represents the first value that is _not_ in the selected slice. So, the difference between `stop` and `start` is the number of elements selected (if `step` is 1, the default).\n\nThe other feature is that `start` or `stop` may be a _negative_ number, which means it counts from the end of the array instead of the beginning. So:\n\n    a[-1]    # last item in the array\n    a[-2:]   # last two items in the array\n    a[:-2]   # everything except the last two items\n\nSimilarly, `step` may be a negative number:\n\n    a[::-1]    # all items in the array, reversed\n    a[1::-1]   # the first two items, reversed\n    a[:-3:-1]  # the last two items, reversed\n    a[-3::-1]  # everything except the last two items, reversed\n\nPython is kind to the programmer if there are fewer items than you ask for. For example, if you ask for `a[:-2]` and `a` only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.\n\n### Relation to `slice()` object\n\nThe slicing operator `[]` is actually being used in the above code with a `slice()` object using the `:` notation (which is only valid within `[]`), i.e.:\n\n    a[start:stop:step]\n\nis equivalent to:\n\n    a[slice(start, stop, step)]\n\nSlice objects also behave slightly differently depending on the number of arguments, similarly to `range()`, i.e. both `slice(stop)` and `slice(start, stop[, step])` are supported.\nTo skip specifying a given argument, one might use `None`, so that e.g. `a[start:]` is equivalent to `a[slice(start, None)]` or `a[::-1]` is equivalent to `a[slice(None, None, -1)]`.\n\nWhile the `:`-based notation is very helpful for simple slicing, the explicit use of `slice()` objects simplifies the programmatic generation of slicing.\n\n---\n\n## The notion of data batches\n\n\nIn general, the first axis (axis 0, because indexing starts at 0) in all data tensors you\u2019ll come across in deep learning will be the samples axis (sometimes called the samples dimension). In the MNIST example, samples are images of digits. In addition, deep-learning models don\u2019t process an entire dataset at once; rather, they break the data into small batches. Concretely, here\u2019s one batch of our MNIST digits, with batch size of 128:\n\n`batch = X_train_images[:128]`\n\nAnd here\u2019s the next batch:\n\n`batch = X_train_images[128:256]`\n\nAnd the n th batch:\n\n`batch = X_train_images[128 * n:128 * (n + 1)]`\n\nWhen considering such a batch tensor, the first axis (axis 0) is called the batch axis or batch dimension. This is a term we will frequently encounter when using Keras and other deep-learning libraries.\n\n## Real-world examples of data tensors\n\nLet\u2019s make data tensors more concrete with a few examples similar to what we will encounter generally. The data we will manipulate will almost always fall into one of the following categories:\n\n- **Vector data** \u20142D tensors of shape (samples, features)\n\n- **Timeseries data or sequence data**\u20143D tensors of shape (samples, timesteps, features)\n\n- **Images\u20144D tensors of shape** (samples, height, width, channels) or (samples, channels, height, width)\n\n- **Video\u20145D tensors** of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)\n\n---\n\n## Vector data\n\nThis is the most common case. In such a dataset, each single data point can be encoded as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of vectors), where the first axis is the samples axis and the second axis is the features axis.\n\nLet\u2019s take a look at two examples:\n\n\uf0a1 An actuarial dataset of people, where we consider each person\u2019s age, ZIP code, and income. Each person can be characterized as a vector of 3 values, and thus an entire dataset of 100,000 people can be stored in a 2D tensor of shape (100000, 3).\n\n\uf0a1 A dataset of text documents, where we represent each document by the counts of how many times each word appears in it (out of a dictionary of 20,000 common words). Each document can be encoded as a vector of 20,000 values (one count per word in the dictionary), and thus an entire dataset of 500 documents can be stored in a tensor of shape (500, 20000).\n\n## Timeseries data or sequence data\n\nWhenever time matters in your data (or the notion of sequence order), it makes sense to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D tensor\n\n![img](https:\/\/i.imgur.com\/rg4iG2p.png)\n\n\n**The time axis is always the second axis (axis of index 1), by convention.**\n\n Let\u2019s look atfew examples:\n- **A dataset of stock prices**. Every minute, we store the current price of the stock, the highest price in the past minute, and the lowest price in the past minute. Thus every minute is encoded as a 3D vector, an entire day of tradingencoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading day), and 250 days\u2019 worth of data can be stored in a 3D tensor of shape (250, 390, 3). Here, each sample would be one day\u2019s worth of data.\n\n- **A dataset of tweets**, where we encode each tweet as a sequence of 280 characters out of an alphabet of 128 unique characters. In this setting, each character can be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry at the index corresponding to the character). Then each tweet can be encoded as a 2D tensor of shape (280, 128), and a dataset of 1 million tweets can be stored in a tensor of shape (1000000, 280, 128).\n\n---\n\n## Image data\n\nImages typically have three dimensions: height, width, and color depth. Although grayscale images (like our MNIST digits) have only a single color channel and could thus be stored in 2D tensors, by convention image tensors are always 3D, with a one-dimensional color channel for grayscale images. A batch of 128 grayscale images of size 256 \u00d7 256 could thus be stored in a tensor of shape (128, 256, 256, 1), andbatch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)\n\n![img](https:\/\/i.imgur.com\/yEUXMdw.png)\n\nThe above is a  A 4D image data tensor (channels-first convention)\n\nThere are two conventions for shapes of images tensors: the channels-last convention (used by TensorFlow) and the channels-first convention (used by Theano). TensorFlow, places the color-depth axis at the end: (samples, height, width, color_depth). Meanwhile, Theano places the color depth axis right after the batch axis: (samples, color_depth, height, width). With the Theano convention, the previous examples would become (128, 1, 256, 256) and (128, 3, 256, 256). The Keras framework provides support for both formats.\n\n##  Video data\nAs we mentioned eariler in this article, Video data is one of the few types of real-world data for which you\u2019ll need 5D tensors.\n\n##### A video can be understood as a sequence of frames, each frame being a color image. Because each frame can be stored in a 3D tensor (height, width, color_depth), a sequence of frames can be stored in a 4D tensor (frames, height, width, color_depth), and thus a batch of different videos can be stored in a 5D tensor of shape (samples, frames, height, width, color_depth).\n\nFor instance, a 60-second, 144 \u00d7 256 YouTube video clip sampled at 4 frames per second would have 240 frames. A batch of four such video clips would be stored in a tensor of shape (4, 240, 144, 256, 3). That\u2019s a total of 106,168,320 values! If the dtype of the tensor was float32, then each value would be stored in 32 bits, so the tensor would represent 405 MB. Heavy! Videos we encounter in real life are much lighter, because they aren\u2019t stored in float32, and they\u2019re typically compressed by a large factor (such as in the MPEG format).\n\n\n","23cfbd7a":"It\u2019s equivalent to this more detailed notation, which specifies a start index and stop index for the slice along each tensor axis. Note that : is equivalent to selecting the entire axis:\n","19cfecfd":"See the output of the above, to understand the structure of a 3-D tensor. It is a collection of matrices. Thus unlike a single matrix with two axes a 3-D tensor has three axes.\n\n![img](https:\/\/i.imgur.com\/lbAGWug.png)\n\nBy packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learning, you\u2019ll generally manipulate tensors that are 0D to 4D, although you may go up to 5D if you process video data.\n\n\n## 4-D tensors\nThe same way we get a 3-D tensor, if some of such 3-D tensors are to be grouped then another dimension gets created making the tensor a 4-D tensor. [See the image for a hypothetical 4-D tensor](https:\/\/dibyendudeb.com\/tensors-in-deep-learning-an-introduction\/#unique4d). Here you can see three cubes are clubbed. Such 4-D tensors are very useful for storing images for image recognition in deep learning.\n\n![img](https:\/\/i.imgur.com\/LgtygJo.png)\n\nIn the same fashion we can have more higher dimension tensors. Though tensors up to 4 dimension are more common, some times to sore videos 5-D tensors are also used. Theoretically there is no such limitation in dimension. For the sake of data storage in an organized manner any n number of dimensions can be used.\n\n\n## 5-D tensors\n\nThis is the type of tensors when we need to store data with yet another dimension. Video data can be an ideal example where 5-D tensors are used.\n\nIf we take an example of a 5-minute video of 1080 HD resolution, then what will be the dimension of its data structure? Let\u2019s calculate in a simple way. The pixel size will be 1080 x 1920 pixels. The time duration of the video in seconds is 5 x 60=300 seconds.\n\nNow if the video is sampled by 10 frames\/second then a total number of frames will be 300 x 10=3000. Suppose the colour depth of the video is 3. So for this video, the tensor should have 4 dimensions, the shape is (3000, 1080,1920,3).\n\nSo, the single video clip is a 4-D tensor. Now if we want to store multiple videos, say 10 video clips with 1080 HD resolution, then we need 5-D tensors. The shape of this 5-D tensor will be (3000, 1080,1920,3,10).\n\n---\n\n## Key attributes of Tensors\nA tensor is defined by three key attributes:\n\n**Number of axes (rank)** \u2014For instance, a 3D tensor has three axes, and a matrix has two axes. This is also called the tensor\u2019s ndim in Python libraries such as Numpy.\n\n**Shape** \u2014This is a tuple of integers that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape with a single element, such as (5,), whereas a scalar has an empty shape, ().\n\n**Data type** (usually called dtype in Python libraries)\u2014This is the type of the data contained in the tensor; for instance, a tensor\u2019s type could be float32, uint8, float64, and so on. On rare occasions, you may see a char tensor. Note that string tensors don\u2019t exist in Numpy (or in most other libraries), because tensors live in preallocated, contiguous memory segments: and strings, being variable length, would preclude the use of this implementation. You can see all supported dtypes at [tf.dtypes.DType](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/dtypes\/DType).\n\nTo make this more concrete, let\u2019s look back at the data we processed in the MNIST\nexample. First, we load the [MNIST dataset](https:\/\/keras.io\/api\/datasets\/mnist\/): This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.","a4e3e544":"This vector has five entries and so is called a 5-dimensional vector. Don\u2019t confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its axis, whereas a 5D tensor has five axes (and may have any number of dimensions along each axis). Dimensionality can denote either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor), which can be confusing at times. In the latter case, it\u2019s technically more correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes), but the ambiguous notation 5D tensor is common regardless.\n\n![img](https:\/\/i.imgur.com\/5kDiQyA.png)\n\nIn another visual representation of Tensor\n\n![img](https:\/\/i.imgur.com\/x830zja.png)\n\n---\n\n## Matrices (2D tensors)\n\nAn array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to rows and columns). You can visually interpret a matrix as a rectangular grid of numbers. This is a Numpy matrix:","cf9784cc":"The entries from the first axis are called the rows, and the entries from the second axis are called the columns. In the previous example, [5, 78, 2, 34, 0] is the first row of x, and [5, 6, 7] is the first column.\n\n## 3D tensors and higher-dimensional tensors\n\nIf you pack such matrices in a new array, you obtain a 3D tensor, which you can visually\ninterpret as a cube of numbers. Following is a Numpy 3D tensor:"}}