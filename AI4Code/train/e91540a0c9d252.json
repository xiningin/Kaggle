{"cell_type":{"1209593c":"code","1cfd43c8":"code","4eb6632d":"code","1fd584cb":"code","dd7f4376":"code","507a8f42":"code","0fb1bc3e":"markdown","a456ab8b":"markdown","98ba1fe0":"markdown","1cea357e":"markdown","bf539ee3":"markdown","3b1c38e3":"markdown","e5633fd2":"markdown","60aff585":"markdown","c2addd29":"markdown","4f63e7fe":"markdown","b0522584":"markdown","e2dbc139":"markdown","d69aa94b":"markdown"},"source":{"1209593c":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image, ImageColor, ImageDraw , ImageFont , ImageOps\nfrom IPython.display import display, Markdown ,clear_output\n\nimport tensorflow as tf \nimport tensorflow_hub as hub\n\nimport tempfile\nfrom six.moves.urllib.request import urlopen\nfrom six import BytesIO\nimport time","1cfd43c8":"ssd_url = \"https:\/\/tfhub.dev\/google\/openimages_v4\/ssd\/mobilenet_v2\/1\"\ninception_resnet_v2_url = \"https:\/\/tfhub.dev\/google\/faster_rcnn\/openimages_v4\/inception_resnet_v2\/1\"","4eb6632d":"ssd_model = hub.load(ssd_url)\nssd_detector = ssd_model.signatures['default']\n\nicp_resv2_model = hub.load(inception_resnet_v2_url)\nicp_res2_detector = icp_resv2_model.signatures['default']\nclear_output()","1fd584cb":"def resize_image(url , img_height = 256, img_width = 256):\n    '''\n    Fetches an image online, resizes it and saves it locally.\n    \n    Args:\n        url (string) -- link to the image\n        new_width (int) -- size in pixels used for resizing the width of the image\n        new_height (int) -- size in pixels used for resizing the length of the image\n        \n    Returns:\n        (string) -- path to the saved image\n    '''\n    response = urlopen(url)\n    img = response.read()\n    img = BytesIO(img)\n    pil_image = Image.open(img)\n    pil_image = ImageOps.fit(pil_image, (img_height, img_width), Image.ANTIALIAS)\n    pil_image_rgb= pil_image.convert(\"RGB\")\n    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n    pil_image_rgb.save(filename, format=\"JPEG\", quality=90) # save images to a temporary location\n    #print(\"Image downloaded to %s.\" % filename)\n    return filename\n\n\ndef display_image(image , detector_name):\n    \"\"\"\n    Displays an image inside the notebook.\n    \"\"\"\n    fig = plt.figure(figsize = (15,10))\n    plt.grid(False)\n    plt.title(detector_name , fontdict = {'fontsize' : 20} )\n    plt.imshow(image)\n    \n    \ndef load_image(img_path):\n    '''\n    Loads a JPEG image and converts it to a tensor.\n    \n    Args:\n        path-- path to a locally saved JPEG image\n    \n    Returns:\n        an image tensor\n    '''\n    image = tf.io.read_file(img_path)\n    image = tf.image.decode_jpeg(image,channels = 3 )\n    return image\n\n\ndef draw_bb(image , ymin ,xmin , ymax , xmax , color , font , thickness = 4 , display_str_list = ()):\n    \"\"\"\n    Adds a bounding box to an image.\n    Args:\n        image -- the image object\n        ymin -- bounding box coordinate\n        xmin -- bounding box coordinate\n        ymax -- bounding box coordinate\n        xmax -- bounding box coordinate\n        color -- color for the bounding box edges\n        font -- font for class label\n        thickness -- edge thickness of the bounding box\n        display_str_list -- class labels for each object detected\n    Returns:\n        No return.  The function modifies the `image` argument \n                    that gets passed into this function\n    \n    \"\"\"\n    draw = ImageDraw.Draw(image)\n    width , height = image.size\n    # scale the bounding box coordinates to the height and width of the image\n    (left , right , top , bottom ) = (xmin * width , xmax * width, \n                                     ymin * height , ymax * height) \n    # define the four edges of the detection box\n    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),(left, top)], \n            width=thickness,\n            fill=color)\n    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n    # If the total height of the display strings added to the top of the bounding\n    # box exceeds the top of the image, stack the strings below the bounding box\n    # instead of above.\n    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n    if top > total_display_str_height:\n        text_bottom = top\n    else:\n        text_bottom = top + total_display_str_height\n    for display_str in display_str_list[::-1]:\n        text_width , text_height = font.getsize(display_str)\n        margin = np.ceil(0.05 * text_height)\n        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n                        (left + text_width, text_bottom)],\n                       fill=color)\n        draw.text((left + margin, text_bottom - text_height - margin),\n                  display_str,\n                  fill=\"black\",\n                  font=font)\n        text_bottom -= text_height - 2 * margin\n        \n        \ndef draw_boxes(image , boxes , class_names , scores , max_boxes = 10 , min_score = 0.1):\n    \"\"\"\n    Overlay labeled boxes on an image with formatted scores and label names.\n    \n    Args:\n        image -- the image as a numpy array\n        boxes -- list of detection boxes\n        class_names -- list of classes for each detected object\n        scores -- numbers showing the model's confidence in detecting that object\n        max_boxes -- maximum detection boxes to overlay on the image (default is 10)\n        min_score -- minimum score required to display a bounding box\n    \n    Returns:\n        image -- the image after detection boxes and classes are overlaid on the original image.\n    \"\"\"\n    colors = list(ImageColor.colormap.values())\n    font = ImageFont.load_default()\n    \n    for i in range(min(boxes.shape[0] ,max_boxes)):\n        \n        if scores[i] >= min_score:# only display detection boxes that have the minimum score or higher\n            ymin , xmin , ymax , xmax = tuple(boxes[i])\n            display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),int(100 * scores[i]))\n            color = colors[hash(class_names[i]) % len(colors)]\n            image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n            draw_bb(image_pil, ymin ,xmin , ymax ,xmax, color , font ,display_str_list=[display_str])\n            np.copyto(image, np.array(image_pil))\n    return image\n\n\ndef run_detector(detector,detector_name , image_path):\n    '''\n    Runs inference on a local file using an object detection model.\n    Args:\n        detector (model) -- an object detection model loaded from TF Hub\n        path -- path to an image saved locally\n    '''\n    image = load_image(image_path) # loads an image tesnor \n    converted_image = tf.image.convert_image_dtype(image ,tf.float32)[tf.newaxis,...] # adds a batch dimension \n    # Running Inference on image \n    start_time = time.time()\n    result = detector(converted_image)\n    end_time = time.time()\n    \n    result = {key:value.numpy() for key,value in result.items()}\n    \n    print(detector_name + \" Found %d objects.\" %len(result[\"detection_scores\"]))\n    print(detector_name + \" Inference Time:\" , end_time - start_time)\n    #Drawing bounding box on image\n    image_with_bb = draw_boxes(image.numpy() , result[\"detection_boxes\"],\n      result[\"detection_class_entities\"], result[\"detection_scores\"])\n    \n    display_image(image_with_bb, detector_name)\n    \ndef object_detector(detector , detector_name , image_url , img_height = 640 , img_width = 480):\n    image_path = resize_image(image_url , img_height ,img_width)\n    run_detector(detector ,detector_name, image_path)","dd7f4376":"image_urls = [\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/0d\/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg\/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg\",\n             \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1b\/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg\",\n             \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/02\/Paso_de_cebra_en_el_centro_de_Ciudad_de_M%C3%A9xico.jpg\",\n             \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3c\/Peak_hour_traffic_in_melbourne.jpg\",\n             \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/a1\/Aepyceros_melampus_petersi_female_8014.jpg\"]","507a8f42":"for index, url in enumerate(image_urls):\n    display(Markdown('## Inference on image '+ str(index +1)))\n    object_detector(ssd_detector,\"SSD + MobileNetV2 Detector\" ,  url)\n    plt.show()\n    object_detector(icp_res2_detector,\"FasterRCNN + InceptionResNet V2 Detector\" ,  url )\n    plt.show()","0fb1bc3e":"##### There are 60+ pre-trained models available on Tensorflow-Hub for object detection in images. Check them [here](https:\/\/tfhub.dev\/s?module-type=image-object-detection).\n##### You can select any model for inference by just copying the URL of that model from TF-HUB.\n##### Each model differs in size , accuracy, loading and inferencing time.","a456ab8b":"**Created by Sanskar Hasija**\n\n**\ud83e\uddd0Object detection using TFhub\ud83e\uddd0**\n\n**18 NOVEMBER 2021**\n","98ba1fe0":"#### In this notebook, I will use and compare 2 different models.\n\n#### 1. [SSD + MobileNet V2](https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/2): Small and fast.\n#### 2. [FasterRCNN + InceptionResNet V2](https:\/\/tfhub.dev\/google\/faster_rcnn\/openimages_v4\/inception_resnet_v2\/1): High Accuracy","1cea357e":"# <center>\ud83e\uddd0OBJECT DETECTION USING TFHUB\ud83e\uddd0 <\/center>\n## <center>If you find this notebook useful, support with an upvote\ud83d\udc4d<\/center>","bf539ee3":"### Model's URL","3b1c38e3":"<a id=\"3\"><\/a>\n# <center>Utility Functions <\/center>","e5633fd2":"### URL of 5 different images","60aff585":"<a id=\"1\"><\/a>\n# <center>Imports<\/center>","c2addd29":"### This notebook is an end-to-end notebook for objection detection in images using pre-trained models from TensorFlow Hub.\n### Check out Tensorflow-hub [here](https:\/\/www.tensorflow.org\/hub).\n\n### [1. Imports](#1) ###\n### [2. Loading Models](#2) ##\n### [3. Utility Functions](#3) ###\n### [4. Inference on different images](#4) ###\n","4f63e7fe":"### Checking results on all 5 images","b0522584":"<a id=\"4\"><\/a>\n# <center>Inference on different images<\/center>","e2dbc139":"### Loading both models","d69aa94b":"<a id=\"2\"><\/a>\n# <center>Loading Models<\/center>"}}