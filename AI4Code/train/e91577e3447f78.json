{"cell_type":{"604c453c":"code","92240142":"code","e2229778":"code","f51ea69a":"code","4d732c2d":"code","08664f39":"code","a826d953":"code","672764ac":"code","cee3e870":"code","615b1910":"code","7318a5e4":"code","4c5f81cc":"code","73185589":"code","2034e21f":"code","57672beb":"code","86a692b3":"code","af85052d":"code","b169dd2c":"code","be72486c":"code","befcf08d":"code","cc12ebd9":"code","949d8613":"markdown","4bdfabc5":"markdown","b1dcb80a":"markdown","95a55727":"markdown","88053f54":"markdown","f58415e1":"markdown","31ac517e":"markdown","5c0eb249":"markdown"},"source":{"604c453c":"#!pip install efficientnet -q","92240142":"# Libraries\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsys.path = ['..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master',] + sys.path\n\n\nimport glob\nimport numpy as np\nimport pandas as pd\nimport math\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\nimport random\n\n# tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import models\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import metrics\n\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torchvision.transforms as transforms\n\n\nfrom tqdm import tqdm\n\nfrom efficientnet_pytorch import model as enet\n\n#albumentations\nimport albumentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau","e2229778":"data_dir = '..\/input\/seti-breakthrough-listen'\ntrain_merger = os.path.join(data_dir,'train_labels.csv')\ntrain_labels = pd.read_csv(train_merger)\nprint('train_label_csv : ' +str(train_labels.shape[0]))\n#adding the path for each id for easier processing\ntrain_labels['path'] = train_labels['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')\ntrain_labels.head()","f51ea69a":"from kaggle_datasets import KaggleDatasets\n\ndata_dir ='..\/input\/seti-breakthrough-listen'\npath_dir = KaggleDatasets().get_gcs_path('seti-breakthrough-listen')\n\ntrain = os.path.join(data_dir, 'train_labels.csv')\ntrain_df = pd.read_csv(train)  \nprint(\"data_train_csv : \" + str(train_df.shape[0]))\ndisplay(train_df.head(5))\n\n\nsub = os.path.join(data_dir,'sample_submission.csv')\nsub_df = pd.read_csv(sub)\nprint(\"data_submission_csv : \" + str(sub_df.shape[0]) )\ndisplay(sub_df.head(5))\n\nlabel_cols = sub_df.columns[1:]\n#label_cols.values\nlabels = train_df[label_cols].values","4d732c2d":"from typing import *\n\nclass Transform:\n    def __init__(self, aug_kwargs: Dict):\n        albumentations_aug = [getattr(A, name)(**kwargs)\n                            for name, kwargs in aug_kwargs.items()]\n        albumentations_aug.append(ToTensorV2(p=1))\n        self.transform = A.Compose(albumentations_aug)\n    \n    def __call__(self, image):\n        image = self.transform(image = image)['image']\n        return image","08664f39":"class ModeTransform():\n    def __init__(self, df_frame, config,type_mode,mode,target,type_spatial,type_channel,transform):\n        self.df_frame = df_frame\n        self.type_mode = type_mode\n        self.config = config\n        self.target = target\n        self.file_names = df_frame['path'].values\n        self.labels = df_frame['target'].values\n        self.transform = transform\n        self.mode = mode\n        self.type_spatial = type_spatial\n        self.type_channel = type_channel\n        \n    def __len__(self):\n        return len(self.df_frame)\n\n    def __getitem__(self, idx):\n        image = np.load(self.file_names[idx])\n        # print(image.shape) -> (6, 273, 256)\n        \n        if self.type_spatial:\n            if self.type_mode == 'spatial_6ch':\n                image = image.astype(np.float32)\n                image = np.vstack(image) # no transpose here (1638, 256) \n                #image = np.vstack(image).transpose((1, 0))\n                # print(image.shape) -> (256, 1638)\n            elif self.type_mode == 'spatial_3ch':\n                image = image[::2].astype(np.float32)\n                image = np.vstack(image).transpose((1, 0))\n        \n        elif self.type_channel:\n            if self.type_mode == '6_channel':\n                image = image.astype(np.float32)\n                image = np.transpose(image, (1,2,0))\n            elif self.type_mode == '3_channel':\n                image = image[::2].astype(np.float32)\n                image = np.transpose(image, (1,2,0))\n        \n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image).float()\n\n        if self.mode == 'test':\n            return image    \n        else:\n            label = torch.tensor(self.labels[idx]).float()\n            return image, label","a826d953":"CONFIG = { \n    \"TRAIN_TRANSFORMS\": {        \n        \"VerticalFlip\": {\"p\": 0.5},\n        \"HorizontalFlip\": {\"p\": 0.5},\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    }}\nconfig = CONFIG\n\n# Parameters\nparams_train  = {'mode'            : 'train',\n                 'type_mode'       : 'spatial_6ch',\n                 'target'          : True,\n                 'type_spatial'    : True,\n                 'type_channel'    : True}\n\ntrain_dset = ModeTransform(train_labels,config,\n                           **params_train,\n                           transform=Transform(config[\"TRAIN_TRANSFORMS\"]))\n\nfor i in range(2):\n    image, label = train_dset[i]\n    plt.imshow(image[0])\n    plt.title(f'label: {label}')\n    plt.show()\nimage.shape","672764ac":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results \n     are the same every time we run.This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nseed = 42\nrandom_state = set_seed(seed)","cee3e870":"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print('GPU is available')\nelse:\n    device = torch.device('cpu')\n    print('GPU not available, CPU used')","615b1910":"class CustomDataset:\n    \n    def __init__(self, image_dir, targets, isTrain=True): \n        self.image_dir = image_dir\n        self.targets = targets\n        self.isTrain = isTrain\n\n    def __len__(self):\n        return len(self.image_dir)\n    \n    def __getitem__(self, idx):      \n        image = np.load(self.image_dir[idx])\n        image1 = np.vstack(image).transpose((1, 0)).astype(np.float32)[np.newaxis, ]\n                \n        return {\n            \"image_trnspose\": torch.tensor(image1, dtype=torch.float),\n            \"targets\": torch.tensor(self.targets[idx], dtype=torch.long),\n        }   ","7318a5e4":"df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\nprint (df.shape)\ndf['img_path'] = df['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')\ndf.head()","4c5f81cc":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n        \n        \n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False)\n\n    def extract(self, x):\n        return self.enet(x)\n      \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.extract(x)\n        x = self.myfc(x)\n        \n        return x                               ","73185589":"class MixUpAugmentation():\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    def __init__(self, x, y, alpha, use_cuda):\n        self.x = x\n        self.y = y\n        self.alpha = alpha\n        self.use_cuda = use_cuda\n        \n    def __getitem__(self):\n        if self.alpha > 0:\n            lmbda = np.random.beta(self.alpha, self.alpha)\n        else:\n            lmbda = 1\n         \n        batch_size = self.x.size()[0]\n        if self.use_cuda:\n            index = torch.randperm(batch_size).cuda()\n        else:\n            index = torch.randperm(batch_size)\n\n        mixed_x = lmbda * self.x + (1 - lmbda) * self.x[index, :]\n        y_a, y_b = self.y, self.y[index]\n        return mixed_x, y_a, y_b, lmbda    ","2034e21f":"class MixUpCriterion():\n     def __init__(self, criterion, pred, y_a, y_b, lmbda):\n        self.criterion = criterion\n        self.pred = pred\n        self.y_a = y_a\n        self.y_b = y_b\n        self.lmbda = lmbda\n      \n     def __getitem__(self):\n        return self.lmbda * self.criterion(self.pred, self.y_a) + (1 - self.lmbda) * self.criterion(self.pred, self.y_b)    ","57672beb":"'''\nParameters\n'''\nparams_MixUpAug  = {'alpha'            : 1.0,\n                   'use_cuda'         : True} ","86a692b3":"def train(data_loader, model, optimizer, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        \n        inputs1 = data[\"image_trnspose\"]\n        targets = data['targets']\n        \n        inputs1, targets_a, targets_b, lam = MixUpAugmentation(inputs1, \n                                                               targets.view(-1, 1), **params_MixUpAug)\n\n        inputs1 = inputs1.to(device, dtype=torch.float)\n        targets_a = targets_a.to(device, dtype=torch.float)\n        targets_b = targets_b.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs1)\n        loss = MixUpCriterion(criterion, outputs, targets_a, targets_b, lam)\n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image_trnspose\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","af85052d":"paths = [\n 'efficientnet-b0-08094119.pth',\n 'efficientnet-b1-dbc7070a.pth',\n 'efficientnet-b2-27687264.pth',\n 'efficientnet-b3-c8376fa2.pth',\n 'efficientnet-b4-e116e8b3.pth',\n 'efficientnet-b5-586e6cc6.pth',\n 'efficientnet-b6-c76e70fd.pth',\n 'efficientnet-b7-dcc49843.pth',\n]\npretrained_model = {\n    'efficientnet-b0': '..\/input\/efficientnet-pytorch\/' + paths[0],\n    'efficientnet-b1': '..\/input\/efficientnet-pytorch\/' + paths[1],\n    'efficientnet-b2': '..\/input\/efficientnet-pytorch\/' + paths[2],\n    'efficientnet-b3': '..\/input\/efficientnet-pytorch\/' + paths[3],\n    'efficientnet-b4': '..\/input\/efficientnet-pytorch\/' + paths[4],\n    'efficientnet-b5': '..\/input\/efficientnet-pytorch\/' + paths[5],\n    'efficientnet-b6': '..\/input\/efficientnet-pytorch\/' + paths[6],\n    'efficientnet-b7': '..\/input\/efficientnet-pytorch\/' + paths[7],\n}","b169dd2c":"device = \"cuda\"\nmodel_name = 'efficientnet-b4'\nmodel = enetv2(model_name,out_dim = 1)\nmodel.to(device)\nmodel.load_state_dict(torch.load('..\/input\/eff-mixup-v98\/aug_training_0_10.pt'))","be72486c":"submission = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsubmission['img_path'] = submission['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/test\/{x[0]}\/\/{x}.npy')","befcf08d":"test_dataset = CustomDataset(image_dir=submission.img_path.values, \n                                     targets=submission.target.values,\n                                      )\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, \n                                          batch_size=16, \n                                          shuffle=False, \n                                          num_workers=4)\n\ntest_predictions, test_targets = evaluate(test_loader,model, device)","cc12ebd9":"test_predictions = np.array(test_predictions)\nsubmission.target = test_predictions[:, 0]\nsubmission.drop(['img_path'], axis=1, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","949d8613":"# \ud83d\uddc3 <span style=\"font-family:cursive;\">Custom Dataset<\/span>","4bdfabc5":"# \ud83e\uddea<span style=\"font-family:cursive;\">Define Model<\/span>","b1dcb80a":"# \ud83d\udcda<span style = 'font-family:cursive;'> Libraries <\/span> ","95a55727":"# <span style=\"font-family:cursive;\"> if it's notebook useful for you come on upvote \ud83d\ude00\u2b06\ud83d\udd1d<\/span>","88053f54":"# \ud83d\udd04<span style=\"font-family:cursive;\"> MixUp Augmentation<\/span>","f58415e1":"# \ud83d\udcdd<span style=\"font-family:cursive;\"> Data Preparation<\/span>\n","31ac517e":"# \ud83d\udcd1<span style=\"font-family:cursive;\"> Train & Evaluate Loader<\/span>","5c0eb249":"# \ud83d\udcd7<span style=\"font-family:cursive;\"> Albumentations<\/span>"}}