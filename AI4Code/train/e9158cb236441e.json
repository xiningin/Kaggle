{"cell_type":{"e17816e6":"code","57e34d90":"code","11338e88":"code","f7deae74":"code","ddd3d48f":"code","c0a38ef7":"code","6936063e":"code","8b38b80f":"code","ce20ae29":"code","7d4446ed":"code","7e571d7c":"code","4e1dc058":"code","4e136ac0":"code","4c8063a9":"code","d80099f8":"code","ea0e53b0":"code","5ce89a14":"code","c472c5af":"code","abef5a38":"code","c3b0a515":"code","eabd32a6":"code","7d3744d4":"code","37117b10":"markdown","339598bb":"markdown","12420cae":"markdown","9dcb1b4d":"markdown","d3bea20e":"markdown","8a90e679":"markdown","c64efafa":"markdown","f6098512":"markdown","ed6ed2b1":"markdown","11e360d8":"markdown","16cff541":"markdown","fed76b02":"markdown","dd9df510":"markdown","bf5e3b1d":"markdown","266596fc":"markdown","64a3265c":"markdown","8366b7f3":"markdown","bdeb12a9":"markdown"},"source":{"e17816e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57e34d90":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom pprint import pprint\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis  \n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","11338e88":"medium_articles = pd.read_csv(\"..\/input\/medium-articles\/articles.csv\")\nmedium_articles.head()","f7deae74":"article_text = medium_articles.text\n# visualise first article print few words from it\n#print(\"First 1000 words in first article are : \\n\",article_text[0][:1000])\n\n#total number of author\nprint(f\"Total Number of unique authors : {len(medium_articles.author.unique())}\")\n\n#counting the articles of different authors\narticle_counts = dict()\nfor author in medium_articles.author:\n    article_count = len(medium_articles[medium_articles[\"author\"]==author])\n    article_counts[author] = article_count\n\n#sorting the article_counts on the basis of article count\narticle_counts = dict(sorted(article_counts.items(), key=lambda x: x[1], reverse=True))\n\n\n#plot histogram of top 10 authors and their article counts\nplt.figure(figsize=(14,7))\nplt.subplot(1, 2, 1)\nsns.barplot(x = list(article_counts.keys())[:10],y=list(article_counts.values())[:10])\nplt.title(\"Authors and Number of Articles\")\nplt.xlabel(\"Authors\")\nplt.ylabel(\"No. of Articles\")\nplt.xticks(rotation = 90) \n\n\n\n#counting the total number of claps for each author\ndef convert_to_num(clap_count):\n    if \"K\" in clap_count:\n        if \".\" in clap_count:\n            clap_count = re.sub(\"\\.\",\"\",clap_count[:-1])+\"00\"\n        else:\n            clap_count = clap_count[:-1]+\"000\"\n    return(int(clap_count))\n\nmedium_articles.claps = medium_articles.claps.apply(convert_to_num)\n\n#couting total number of claps for each author\nclap_counts = dict()\nfor author in medium_articles.author:\n    clap_count = medium_articles[medium_articles[\"author\"]==author][\"claps\"]\n    clap_counts[author] = sum(clap_count)\n\n#sort clap_counts on the basis of counting of claps\nclap_counts = dict(sorted(clap_counts.items(), key=lambda x: x[1], reverse=True))\n\n#plot histogram of top 10 authors and their article counts\nplt.subplot(1, 2, 2)\nsns.barplot(x = list(clap_counts.keys())[:10],y=list(clap_counts.values())[:10])\nplt.title(\"Authors and Number of Claps\")\nplt.xlabel(\"Authors\")\nplt.ylabel(\"No. of Claps\")\nplt.xticks(rotation = 90) \nplt.show()","ddd3d48f":"#lowercase the articles \nmedium_articles.text = medium_articles.text.apply(lambda t : t.lower())","c0a38ef7":"# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the text data\nmedium_articles.text = medium_articles.text.apply(lambda x:expand_contractions(x))","6936063e":"print(stopwords.words('english'))","8b38b80f":"print(f\"These are the pucntions which string.punctuation consist :  {string.punctuation}\")","ce20ae29":"stop_words = set(stopwords.words('english')) \n\ndef remove_stopwords(article):\n    \"Return the articel after remvoing stopwords\"\n    article_tokens = word_tokenize(article) \n    filtered_article = [word for word in article_tokens if not word in stop_words] \n    return \" \".join(filtered_article)\n\n\n#removing stopwords\nmedium_articles.text = medium_articles.text.apply(remove_stopwords)\n\n#removing Punctuations \nmedium_articles.text = medium_articles.text.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n\n#removing digits\nmedium_articles.text = medium_articles.text.apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n\nmedium_articles.text[50]","7d4446ed":"def remove_extra_marks(article):\n    extra_keys = [\"\u2019\",\"\u2014\",\"\u201d\",\"\u201c\"]\n    article_tokens = word_tokenize(article) \n    filtered_article = [word for word in article_tokens if not word in extra_keys] \n    return \" \".join(filtered_article)\n    \nmedium_articles.text = medium_articles.text.apply(remove_extra_marks)","7e571d7c":"#printing arbitrary example to visulise clean data\nmedium_articles.text[50]","4e1dc058":"import spacy\nnlp = spacy.load('en_core_web_sm')\nprint(f\"\"\" \"helps\" after lemmztization :  {nlp(\"helps\")[0].lemma_}\"\"\")\nprint(f\"\"\" \"helping\" after lemmztization :  {nlp(\"helping\")[0].lemma_}\"\"\")","4e136ac0":"def lemmatize(text):\n    \"\"\"Return text after performing the lemmztiztion\"\"\"\n    doc = nlp(text)\n    tokens = [token for token in doc]\n    return  \" \".join([token.lemma_ for token in doc])\n\n#lemmatize the articles\nmedium_articles.text = medium_articles.text.apply(lemmatize)","4c8063a9":"import wordcloud\nfrom wordcloud import WordCloud\n\n#combine all the articles\narticle_data = \"\"\nfor article in medium_articles.text:\n    article_data = article_data+\" \"+article\n\n#ploting the word cloud\nplt.figure(figsize=(10, 10))\nwordcloud = WordCloud(width = 500, height = 500, background_color='#40E0D0', colormap=\"ocean\",  random_state=10).generate(article_data)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","d80099f8":"#tokenize articles\ntokeize_article = medium_articles.text.apply(lambda x : x.split())\nid2word = corpora.Dictionary(tokeize_article)\n\n# Create Corpus\ntexts = tokeize_article\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n\n#printing 50 words from the text corpus\ncorpus_example = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]\ncorpus_example[0][:50]","ea0e53b0":"# build LDA model for 10 topic\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           per_word_topics=True,\n                                           eta = 0.6)","5ce89a14":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","c472c5af":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = gensimvis.prepare(lda_model, corpus, id2word)\nvis","abef5a38":"# Compute Perplexity\nprint('\\nPerplexity : ', lda_model.log_perplexity(corpus)) \n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","c3b0a515":"def calculate_coherence_score(n, alpha, beta):\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=n, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=alpha,\n                                           per_word_topics=True,\n                                           eta = beta)\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n    return coherence_lda\n\n#list containing various hyperparameters\nno_of_topics = [2,5,7,10,12,14]\nalpha_list = ['symmetric',0.3,0.5,0.7]\nbeta_list = ['auto',0.3,0.5,0.7]\n\n\nfor n in no_of_topics:\n    for alpha in alpha_list:\n        for beta in beta_list:\n            coherence_score = calculate_coherence_score(n, alpha, beta)\n            print(f\"n : {n} ; alpha : {alpha} ; beta : {beta} ; Score : {coherence_score}\")","eabd32a6":"n = 14\nalpha = 0.3\nbeta = \"auto\"\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=n, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=alpha,\n                                           per_word_topics=True,\n                                           eta = beta)\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=tokeize_article, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","7d3744d4":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = gensimvis.prepare(lda_model, corpus, id2word)\nvis","37117b10":"## Preprocessing Text Data","339598bb":"**Lemmaztisation:** Lemmatization process Topic modeling algorithm to avoid making mistakes by converting the word in its root from like lemmatization prevents counting \"help\", \"helps\", \"helping\" as three different words. Stemming can also be used, but it performs the task more forcefully, and sometimes the word loses its meaning, so I prefer lemmatization for the topic modeling task.\n\nHere I am using **spacy** library to perform the lemmatizaton task.","12420cae":"### Tuning hyperparameters:\n\nWe can try out different number of topics, different values of alpha and beta(eta) to increse the conharence score. High conherence score is good for our model.","9dcb1b4d":"The highest coherence score is : 0.4962 for number of topics = 14  alpha = 0.3 and beta = auto","d3bea20e":"To measure how good is our model we can use **perplexity score** and **coherence socre**. The lower score of perplexity is better for the model.","8a90e679":"This notebook is mainly focused on LDA implementation for coherence score you can read this [medium article](https:\/\/towardsdatascience.com\/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0).","c64efafa":"**Expanding contractions:** Contractions are the shortened form of the words like it's, hasn't. We expand them for better analysis of our text data. I have taken these contractions from [Analytics Vidhay's article](http:\/\/https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/beginners-guide-exploratory-data-analysis-text-data\/).","f6098512":"### Visualising Topics:","ed6ed2b1":" ##Topic Modelling: \nTopic modeling is an unsupervised learning technique used to extract topics from a large corpus of text automatically. Topics can be defined as the co-occurring terms that are frequently repeating in the text corpus. Topic modeling is helpful in various tasks such as clustering documents, understanding and summarizing an extensive collection of textual documents and retrieving information from an extensive collection of data, etc. In this notebook I will perform topic modelling using LDA(Latent Dirichlet Allocation) and BERT. \n\nHere I am using medium article dataset to perfom Topic Modelling. The dataset consist 350 articles related to AI, Machine Learning and Data Science.","11e360d8":"Now we have finished with all the data preprocessing steps, and let's plot a word cloud to visualize the most frequently used words and see the most commonly occurred word in the articles.","16cff541":"There are still some marks like quotation marks, hypen and apostrophe still remaining in the article, we also need to remove them.","fed76b02":"Here, we can see that Adam Geitgey has written highest number of articles.","dd9df510":"## Exploring the dataset :","bf5e3b1d":"## Latent Dirichlet Allocation[LDA]:  \n\nLDA algorithm is an unsupervised learning algorithm that works on a probabilistic statistical model to discover topics that the document contains automatically. <br> \nThis algorithm works on certain assumptions. The assumption is that the document was generated by picking a set of topics and picking a set of words for each topic. In other words, we can say that the document is a probability distribution over the latent topics, and topics are probability distribution over the words.\n\nNow let's understand the work methodology of LDA using Plate Diagram:\n<br>\n\n![Smoothed_LDA.png](attachment:f941a0a0-3132-4dd6-88d8-2930d44b78c5.png)\n             \n               **Plate Diagram of LDA model**\n\nThe above image is taken from [Wikipedia](http:\/\/https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4d\/Smoothed_LDA.png\/377px-Smoothed_LDA.png)\n\nHere, $\\alpha$ and $\\beta$ are two hyperparameters that we have to initialise manually, and they symbolize per topic distribution($\\alpha$) and per topic word distribution($\\beta$), respectively. And **Z** is the topic for the **N**-th word in document **M**, and **W** is the specific word. We can only see W(specific words) in the documents because it is only an observable variable, and all others are latent.\n\n$\\theta$ is the matrix where rows are the documents; columns are topics, and $\\theta(i,j)$ represents the probability of $ith$ document containing $jth$ topic. Similarly, $\\phi$ is the matrix where rows are the topics; columns are words, and $\\phi(i,j)$ represents the probability of $ith$ topic containing $jth$ word. According to the distribution of $\\phi$; **K** individual words are generated for the topics.\n\nWith the help of this LDA, try to estimate the words that belong to each topic and find the topics in documents accordingly.","266596fc":"**The above graphs show that Adam Getigey has written most articles and has the highest number of claps. But when it comes to the 2nd, 3rd positions, the author's name is not the same in the graphs. It shows that either the reach of articles or the quality of articles is inadequate for the authors.**","64a3265c":"**Removing Stopwords and Puctuations**: Stopwords are the words that are highly occurred in a language; they do not add any significance to the sentence. \"a,\" \"an,\" \"the,\" \"in\" are some examples of stopwords. So we generally ignore stop words during our NLP task. In some NLP tasks, stop words are also important, but stopwords are unnecessary for topic modeling. We also remove punctuation marks because they are also unnecessary and do not contribute anything.","8366b7f3":"Final LDA Model :","bdeb12a9":"We can interpret this output as for topic 0, word, vector,ai, make, cluster, go, system, use, one are 10 most important keywords. The weight of \"ai\" on topic 0 is 0.008."}}