{"cell_type":{"0ae3d8ad":"code","29cabd68":"code","9cd7a382":"code","c9082240":"code","3531cf46":"code","c88dee52":"code","9796fd61":"code","d0e2bd84":"code","a4a6d7e8":"code","3c828ebf":"code","aa302ade":"code","fe520087":"code","fca4fa3b":"code","410b58ea":"code","ef5d2595":"code","4c02e92e":"code","c8a867ce":"code","0b74934c":"code","5646c59d":"code","0b65952b":"code","dcef361c":"code","8a20d023":"markdown","40f4e745":"markdown","4d5d07eb":"markdown","0a9e0f79":"markdown","5013ae84":"markdown","41cb1ec5":"markdown","996af77d":"markdown","cf93a1fe":"markdown","2387ac50":"markdown","8fd0aaff":"markdown","6320afb3":"markdown","5c76e5c2":"markdown","85270750":"markdown","dd65668d":"markdown","b1b1b321":"markdown","68b3517d":"markdown","719f44ee":"markdown","91d6db02":"markdown","0c729236":"markdown","44b3ab3d":"markdown","cb4c1e99":"markdown","1f0fc3f8":"markdown","a3c69b62":"markdown"},"source":{"0ae3d8ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.core.display import display, HTML\nimport ipywidgets as widgets\nfrom IPython.display import display,clear_output\nfrom ipywidgets import Output, Button\nfrom matplotlib import pyplot as plt\nimport matplotlib.pyplot as plt\nimport networkx as nx # create network\nimport plotly.express as px\nimport plotly.graph_objects as go\n# make sure the code in plotly is able to run properly\nfrom plotly.offline import plot, iplot, init_notebook_mode\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\ninit_notebook_mode(connected=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom umap import UMAP\n# remove unnecessary warnings in the output\npd.options.mode.chained_assignment = None  # default='warn'","29cabd68":"#sample = pd.read_csv('..\/input\/feedback-prize-2021\/train\/007ACE74B050.txt')\n# f = open('..\/input\/feedback-prize-2021\/train\/007ACE74B050.txt','r')\nwith open('..\/input\/feedback-prize-2021\/train\/007ACE74B050.txt') as f:\n    contents = f.read()\n    print(contents)","9cd7a382":"# load data\ntrain = pd.read_csv(\"\/kaggle\/input\/feedback-prize-2021\/train.csv\")\n\n# taking 007ACE74B050.txt as an example\nsample = train.loc[train['id'] == '007ACE74B050'] # you can change the id to whatever article you want to see","c9082240":"# iterate each discourse type and print it in the color as we wished\ndef color_article(id):\n    sample = train.loc[train['id'] == id]\n    for i in range(sample.shape[0]):\n        text = sample['discourse_text'].iloc[i]\n        discourse = sample['discourse_type'].iloc[i]\n        if discourse == 'Lead':\n            color = '(255, 102, 204)'\n        elif discourse == 'Position':\n            color = '(0, 0, 102)'\n        elif discourse == 'Claim':\n            color = '(51, 102, 255)'\n        elif discourse == 'Counterclaim':\n            color = '(153, 102, 51)'\n        elif discourse == 'Rebuttal':\n            color = '(102, 204, 255)'\n        elif discourse == 'Evidence':\n            color = '(0, 0, 0)'\n        elif discourse == 'Concluding Statement':\n            color = '(51, 51, 153)'\n\n        sample_html_text = '<p style=\"color:rgb' + str(color) + ';\">' + str(text) +'  (' +str(discourse)+') ' + '<\/p>'\n        display(HTML(sample_html_text))\ncolor_article(\"007ACE74B050\")","3531cf46":"# unique id of each article\nids = train['id'].unique()\n\n# create a dropdown list containing unique ids\n# create a button to get input\noutput = Output()\nstart = Button(description=\"Start\")\nplays_widget = widgets.Dropdown(\n    options=list(ids),\n    value=list(list(ids))[0],\n    description='Essay id:',\n    disabled=False,\n)\n\ndef click_start(b):\n    with output:\n        clear_output()\n        #print(\"Essay id: \" + str(plays_widget.value))\n        color_article(plays_widget.value)\n       \nstart.on_click(click_start)\n\n# once click the start button, the corresponding article will be displayed\ndisplay(plays_widget, start, output)","c88dee52":"# count number of elements per discouse type in each essay\ndiscourse_type_number = train.groupby(['id','discourse_type']).nunique().reset_index()[['id','discourse_type','discourse_id']]\n\n# plot a box chart to show quantiles, mean and median of nr of elements per discourse type\nfig = go.Figure()\ndiscourse = train['discourse_type'].unique()\n\nfor d in discourse:\n    dataset = discourse_type_number.loc[discourse_type_number['discourse_type'] == d]    \n    fig.add_trace(go.Box(y=dataset['discourse_id'], \n                         text = dataset['id'],\n                         hovertemplate='<b>id: %{text} - <br>Nr of discourse elements - %{y:.1f}<\/b>',\n                         name=d))\n    \nfig.update_xaxes(showspikes=True)\nfig.update_yaxes(showspikes=True)\nfig.update_layout(showlegend = False,\n                  title = 'Nr of discouse per types in an essay <sup><br>Claims and Evidence have the highest medians at 3 elements in an essay<\/br><\/sup>',\n                  yaxis_title=\"Nr. discourse elements\")\nfig.show()","9796fd61":"# avgerage word length = sum of word length \/ number of words in an element\ndef average_word_length(text):\n    words = text.split()\n    word_count = len(words)\n    word_length = 0\n    for word in words:\n        word_length += len(word)\n    avg_word_length = round(word_length\/word_count,2)\n    return avg_word_length\n\n# create a new column avg_word_length to store the data\ntrain['avg_word_length'] = train['discourse_text'].apply(average_word_length)\n\n# plot a box chart to show quantiles, mean and median of average word length per discourse type\nfig = go.Figure()\ndiscourse = train['discourse_type'].unique()\n\nfor d in discourse:\n    dataset = train.loc[train['discourse_type'] == d]    \n    fig.add_trace(go.Box(y=dataset['avg_word_length'], \n                         text = dataset['discourse_id'],\n                         hovertemplate='<b>discourse_id: %{text} - <br>Average word length - %{y:.1f}<\/b>',\n                         name=d))\n    \nfig.update_xaxes(showspikes=True)\nfig.update_yaxes(showspikes=True)\nfig.update_layout(showlegend = False,\n                  title = 'Average word length in each discourse element per type<sup><br>* Average word length = Sum of each word length in an element \/ Number of words in an element<\/br><\/sup>',\n                  yaxis_title=\"Avg. word length\")\nfig.show()","d0e2bd84":"# nr of unique words \/ number of total words\ndef unique_word_share(text):\n    words = text.split()\n    word_count = len(words)\n    unique_word_count = len(set([w.lower() for w in words]))\n    word_uniqueness = round(unique_word_count * 100\/word_count,2)\n    return word_uniqueness\n\ntrain['uniqueness'] = train['discourse_text'].apply(unique_word_share)\n\n# plot a box chart to show quantiles, mean and median of lexical uniqueness per discourse type\nfig = go.Figure()\ndiscourse = train['discourse_type'].unique()\n\nfor d in discourse:\n    dataset = train.loc[train['discourse_type'] == d]    \n    fig.add_trace(go.Box(y=dataset['uniqueness'], \n                         text = dataset['discourse_id'],\n                         hovertemplate='<b>discourse_id: %{text} - <br> lexical uniqueness - %{y:.1f}%<\/b>',\n                         name=d))\n    \nfig.update_xaxes(showspikes=True)\nfig.update_yaxes(showspikes=True)\nfig.update_layout(showlegend = False,\n                  title = 'Lexical uniqueness in each discourse element per type<sup><br>* lexical uniqueness = Number of unique words in an element * 100 \/ Number of words in an element<\/br><\/sup>',\n                  yaxis_title=\"Lexical uniqueness (%)\")\nfig.show()","a4a6d7e8":"# get the last position of each essay\ntrain['last_position'] = train['predictionstring'].apply(lambda x: x.split(' ')[-1])\ntrain['final_position'] = train['last_position'].astype(float).groupby(train['id']).transform('max')\n\n# number of discourse elements per essay\ntrain_discourse_aggr = train.groupby(['id']).nunique().reset_index()[['id','discourse_id']]\n\n# length of each essay\ntrain_discourse_aggr['length_essay'] = train.groupby(['id']).mean()[['final_position']].reset_index()['final_position']","3c828ebf":"# the correlation coefficient between essay length and number of its discourse elements\n# is ~0.6, strong positive\ntrain_discourse_aggr.corr()","aa302ade":"# plot the scatter chart showing the correlation between essay length and number of its discourse elements\nfig = go.Figure(data=go.Scatter(x=train_discourse_aggr['length_essay'], \n                                y=train_discourse_aggr['discourse_id'], \n                                text=train_discourse_aggr['id'],\n                                opacity = 0.85,\n                                name = \"\",\n                                hovertemplate = \"Essay id: %{text} <br>Essay length: %{x} <br>Nr of discourse: %{y}\",\n                                mode='markers')\n               )\nfig.update_xaxes(showgrid=False)\nfig.update_layout(showlegend = False,\n                  title = 'A strong positive correlation between the length of an essay and the number of discourse elements <sup><br>The correlation coefficient is 0.6<\/sup>',\n                  xaxis_title=\"Essay length\",\n                 yaxis_title=\"Nr of discourse elements\")\n\nfig.show()","fe520087":"# get the length of each discourse type element\ntrain['discourse_text_length'] = train['discourse_text'].apply(lambda x: len(x.split(' ')))\n\n# plot the length of each element into a histogram chart by type\nimport plotly.graph_objects as go\nfig = go.Figure()\n\nx0 = np.random.randn(500)\n# Add 1 to shift the mean of the Gaussian distribution\nx1 = np.random.randn(500) + 1\ndiscourse = train['discourse_type'].unique()\n\nfor d in discourse:\n    \n    fig.add_trace(go.Histogram(\n        x=train.loc[train['discourse_type'] == d]['discourse_text_length'],\n        name = d\n                    )\n                  )\n\n# Overlay both histograms\nfig.update_layout(\n    title = \"Histogram of discourse element length per type <br><sup>Evidence and Concluding Statement have longer tails than the others. <\/sup>\",\n    barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","fca4fa3b":"# get the start and end position of each element\ntrain['first_position'] = train['predictionstring'].apply(lambda x: x.split(' ')[0])\ntrain['last_position'] = train['predictionstring'].apply(lambda x: x.split(' ')[-1])\n\n# normalize the two values in percentiles as start_perc and last_perc respectively\ntrain['start_perc'] = round(train['first_position'].astype(float) * 100 \/ train['final_position'].astype(float),0)\ntrain['last_perc'] = round(train['last_position'].astype(float) * 100\/ train['final_position'].astype(float),0)\n\n# get the median value of start_perc and last_perc per discourse type\ntrain_type_position_aggr = train.groupby('discourse_type').median().reset_index().sort_values(by = 'start_perc', ascending = False)\n\n# plot the dumbbell chart showing the relative median position of each discourse type\nfig=go.Figure()\nfor i in range(train_type_position_aggr.shape[0]):\n    fig.add_shape(\n            type='line',\n            x0=train_type_position_aggr['start_perc'].iloc[i], \n            y0=train_type_position_aggr['discourse_type'].iloc[i], \n            x1=train_type_position_aggr['last_perc'].iloc[i], \n            y1=train_type_position_aggr['discourse_type'].iloc[i],\n            line_color=\"#cccccc\"\n        )\n    fig.add_trace(go.Scatter(x=train_type_position_aggr[\"start_perc\"], \n                             y=train_type_position_aggr[\"discourse_type\"], \n                             #hovertemplate='<b>%{x:.2f}%<\/b> of participants earning <b>LESS<\/b> than 70K chose <b>%{y}<\/b>',\n                             hovertemplate='<b>%{y} usually starts at %{x:.1f}% of the essay<\/b>',\n                             mode='markers',\n                             name = \"\",\n                             marker=dict(size=[10] * train_type_position_aggr.shape[0], color=[\"#DEBAE6\"] * train_type_position_aggr.shape[0]),\n                            ))\n    \n    fig.add_trace(go.Scatter(x=train_type_position_aggr[\"last_perc\"], \n                             y=train_type_position_aggr[\"discourse_type\"], \n                             hovertemplate='<b>%{y} usually ends at %{x:.1f}% of the essay<\/b>',\n                             mode='markers',\n                             name = \"\",\n                             marker=dict(size=[10] * train_type_position_aggr.shape[0],color=[\"#C54DFD\"] * train_type_position_aggr.shape[0])\n                             #visible=(question_name==default_state)\n                            ))\n\nfig.update_xaxes(showspikes=True)\nfig.update_yaxes(showspikes=True)\nfig.update_layout(showlegend = False,\n                  title = 'Position of the discourse type in essays <sup><br>The position is calculated as the median value of normalized percentile of each element<\/br><\/sup>',\n                  xaxis_title=\"% of the essay\")\nfig.show()","410b58ea":"# get the next type\ntrain['next_discourse_type'] = train.groupby(['id'])['discourse_type'].shift(-1)\n\n# select relevant colums\ndiscourse_network = train[['id','discourse_type','next_discourse_type']]\n\n# get the occurrence of each pair\ndiscourse_network_aggr = discourse_network.groupby(['discourse_type','next_discourse_type']).nunique().reset_index()\ndiscourse_network_aggr = discourse_network_aggr.sort_values(by=['discourse_type','id'], ascending = False)\n\n# get the chance of each pair\ndiscourse_network_aggr['sum'] = discourse_network_aggr['id'].groupby(discourse_network_aggr['discourse_type']).transform('sum')\ndiscourse_network_aggr['share'] = round(discourse_network_aggr['id']*100\/discourse_network_aggr['sum'],1)\n\n# plot a bar chart showing the % of each discourse type coming next to each type\ndiscourse=discourse_network_aggr['discourse_type'].unique()\n\nfig = go.Figure()\n\nfor d in discourse:\n    sample = discourse_network_aggr.loc[discourse_network_aggr['next_discourse_type'] == d]\n    fig.add_trace(\n        go.Bar(name=d, \n               x=discourse, \n               y=sample['share'],\n               text = d,\n               hovertemplate = \"Share (%): %{y}\"\n              )\n    )\n\n# Change the bar mode\nfig.update_layout(\n    title = 'What is the discouse type coming next?',\n    xaxis_title=\"Discourse type\",\n    yaxis_title=\"next discourse element (%)\",\n    barmode='stack')\nfig.show()","ef5d2595":"def contain_phrase(phrase):\n# add a new column if_contain indicating if the phrase \n# I think exists in the column discourse_text\n    train['if_contain'] = np.where(train['discourse_text'].str.find(phrase) != -1, True, False)\n\n    # get the share of the elements that contain target phrase\n    train_if_contain_aggr = train.groupby(['discourse_type','if_contain']).nunique().reset_index()[['discourse_type','if_contain','discourse_id']]\n    train_if_contain_aggr['sum'] = train_if_contain_aggr['discourse_id'].groupby(train_if_contain_aggr['discourse_type']).transform('sum')\n    train_if_contain_aggr['share'] = round(train_if_contain_aggr['discourse_id'] * 100 \/ train_if_contain_aggr['sum'],1)\n    train_if_contain_aggr = train_if_contain_aggr.loc[train_if_contain_aggr['if_contain'] == True].sort_values(by = 'share', ascending = False)\n    \n    # plot a bar chart \n    colors = ['lightslategray',] * 7\n    colors[0] = \"#DEBAE6\"\n    \n    fig = go.Figure(data=[go.Bar(\n        x=train_if_contain_aggr['discourse_type'],\n        y=train_if_contain_aggr['share'],\n        name = \"\",\n        hovertemplate='<b>%{y:.1f}% of elements in %{x} does<\/b>',\n        marker_color=colors # marker color can be a single color value or an iterable\n    )])\n    fig.update_layout(title_text='How much % of the elements contain the phrase \"{}\" per discourse type?'.format(phrase),\n                     yaxis_title=\"% of the elements\")\n    fig.show()\ncontain_phrase(\"I think\")","4c02e92e":"# create a input box \n# create a button start the program\noutput = Output()\nstart = Button(description=\"Start\")\n\n\nphrase_widget = widgets.Textarea(\n    #value='Hello World',\n    placeholder='Type something',\n    description='Input:',\n    disabled=False\n)\n\ndef click_start(b):\n    with output:\n        clear_output()\n        contain_phrase(phrase_widget.value)\n       \nstart.on_click(click_start)\n\n# once click the start button, the corresponding article will be displayed\ndisplay(phrase_widget, start, output)","c8a867ce":"# tokenize each discourse element \n# count the frequency of each token and sum them up per discourse type\nunique_discourse = train['discourse_type'].unique()\ndiscourse_freq = []\nfor d in unique_discourse:\n    dataset = train.loc[train['discourse_type'] == d]\n    discourse_text = dataset.discourse_text\n    word_freq = dict(FreqDist(word.lower() for word in word_tokenize(''.join(discourse_text))))\n    table = pd.DataFrame(list(word_freq.items()))\n    table['type'] = d\n    table = table.rename(columns={0: \"word\", 1: \"frequency\"})\n    #table = table.sort_values(by='frequency',ascending = False)\n    table['freq_rank'] = table['frequency'].rank(ascending=False)\n    #table = table.head(1000)\n    discourse_freq.append(table)\n    total_table = pd.concat(discourse_freq)\n\n# count the occurrance probability of each token i.e. number of frequency per token \/ total frequency\ntotal_table['sum'] = total_table['frequency'].groupby(total_table['type']).transform('sum')\ntotal_table['occur_probability'] = total_table['frequency'] \/ total_table['sum']\ntotal_table['mean'] = total_table['occur_probability'].groupby(total_table['word']).transform('mean')\n\n# count the variability of frequency among discourse types for each token\n# e.g. the word 'example' occurres in Evidence at a probability of 0.001428%, and such value is 245% of its average occurrance probablity among discouse types  \n# Based on that, if we see 'example' in a piece of text, our first reasonable guess of the text discourse type should be 'Evidence'.\ntotal_table['change_over_mean'] = round((total_table['occur_probability'] - total_table['mean']) * 100 \/ total_table['mean'],0)","0b74934c":"total_table.loc[total_table['word'] == 'example'][['word','type','occur_probability','change_over_mean']]","5646c59d":"# get the top 20 tokens for each discourse\ntotal_table['type_rank'] = total_table.groupby(\"type\")[\"change_over_mean\"].rank(\"dense\", ascending=False)\ntotal_table_100 = total_table.loc[total_table['type_rank']<=100]\n\n# get the most occurred type of each token\n# the dataset for visualization is ready!\ntotal_table_max = total_table_100.groupby(['word']).max().reset_index()[['word','change_over_mean']]\ndata_aggr = pd.merge(\n    total_table_max,\n    total_table,\n    how=\"inner\",\n    on=None,\n    left_on=['word','change_over_mean'],\n    right_on=['word','change_over_mean'],\n    left_index=False,\n    right_index=False,\n    sort=True,\n    suffixes=(\"_x\", \"_y\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n)\n\n\ndata_aggr = data_aggr.rename(columns={'frequency': \"Frequency\", \n                                      'occur_probability': \"Probability of occurance\"})\n# plot scatter chart per discourse\n# place the tokens as points\nfig = px.scatter(data_aggr, \n                 x=\"Frequency\", \n                 y=\"Probability of occurance\", \n                 size = 'Probability of occurance',\n                 text = 'word',\n                 facet_col=\"type\", \n                 hover_data=['word']\n                 #facet_col_wrap= 3\n                )\n\nfig.update_traces(textposition='top center')\n\nfig.update_layout(\n    title_text='Top 100 discourse signal token <sup><br> If a token occurs at a larger probability in the discourse type X than any other one, we would regard this token as a signal for discourse X.<\/sup>'\n)\n\nfig.show()","0b65952b":"punctuations = ['.',',',':','-','?','!',';','\"']\ndef count_punt(text, p):\n    return text.count(p)\nfor p in punctuations:\n    train[p] = train['discourse_text'].apply(count_punt,args=(p))\ntrain_punct = train.groupby('discourse_type').sum().reset_index()\ntrain_punct['sum'] = train_punct['!'] + train_punct[','] + train_punct['.'] + train_punct[':'] + train_punct['-'] + train_punct[';'] + train_punct['\"'] + train_punct['?']\np_share = []\nfor p in punctuations:\n    train_punct[p + \" percentage\"] = round(train_punct[p] * 100 \/ train_punct['sum'],1)\n    p_share.append(p + \" percentage\")\n\nfig = px.bar(train_punct, x=\"discourse_type\", y=p_share, title=\"% of punctuation occurring in each discourse type\")\nfig.show()","dcef361c":"tfidf = TfidfVectorizer() \n\ntrain_sample = train.sample(10000)\ndocs = tfidf.fit_transform(train_sample['discourse_text'])\n\n# create a list of our conditions\nconditions = [\n    (train_sample['discourse_type'] == \"Lead\"),\n    (train_sample['discourse_type'] == \"Position\"),\n    (train_sample['discourse_type'] == \"Counterclaim\"),\n    (train_sample['discourse_type'] == 'Claim'),\n    (train_sample['discourse_type'] == 'Evidence'),\n    (train_sample['discourse_type'] == 'Concluding Statement'),\n    (train_sample['discourse_type'] == 'Rebuttal')\n    ]\n\nvalues = [0, 1, 2, 3, 4, 5, 6]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntrain_sample['discourse_type_transform'] = np.select(conditions, values)\ntarget = train_sample['discourse_type_transform']\n\numap = UMAP(random_state=0)\ndr = umap.fit_transform(docs, target)\n\n# for visualization\ndark_palette = ['green','red','yellow','blue','purple','pink','orange']\n\ncolor = [dark_palette[0] if i==0 else dark_palette[1] if i==1 else dark_palette[2] if i==2 else dark_palette[3] if i==3 else dark_palette[4] if i==4 else dark_palette[5] if i==5 else dark_palette[6] for i in target]\n\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(111)\nax.axis('off')\n\nax.scatter(x=dr[:,0], y=dr[:,1], s=10, alpha=0.25, c=color)\nax.set_title('TFIDF Dimension Reduction', loc='left', fontsize=20, fontweight='bold')\n\nfig.tight_layout()\nplt.show()","8a20d023":"# The chance of discourse type coming next to each other\n\nFrom the chart below, we can see in most cases -\n\n* Lead always sits at the beginning of the essay. \n* Position follows a Lead or a Concluding Statement.\n* Claims come after Poitions \n* Evidence follows either a cliam or a Rebuttal.","40f4e745":"# Top 100 signal tokens in each discourse type","4d5d07eb":"# Introduction\n\nThis notebook aims at creating **interactive visualizations** for readers to grasp the data easily at the first glance.\n\nIt includes:\n\n* Paint the discourse type with different colors\n* Number of discourse elements per type in each essay\n* A histogram showing the length of discourse type\n* Average word length in each element per discourse type\n* Lexical uniqueness per discourse type\n* The correlation between essay length and its number of discourse elements\n* Relative median position of each discourse type in essays\n* The chance of discourse type coming next to each other\n* Does the text contain \"I think\"?\n* Top 100 signal tokens in each discourse type\n* Does discourse type favor specific punctuation?\n* Text clustering visualization\n* What is the next step?","0a9e0f79":"# Number of discourse elements per type in each essay\n\nWhen I was a student at primary or middle school, my writing teach suggested us writing only one Lead for one article, since it won't let the content distracting to the users. \n\nThen what about our datasets? \n\n* Do stutdents usually write one Lead as I did?\n\n* How many evidences do they usually write in an essay?\n\n* Does they usually include at least one counterclaim or rebuttal as they wrote the Lead?\n\nLet's figure it out!","5013ae84":"Ok, if we see 'I think', we are pretty confident the Position of the arthur will show up later on.\n\nThen what about when we see the word 'example'? It looks easy as well - The author must state an evidence most likely!\n\nBut what about 'may', 'however', or a single question mark '?'?\n\nI want to know do we have some data-driven signal words for one specific discourse. i.e. if a word occurs at a very larger probability in discourse X than any other one, I would regard it as a signal for the discourse X.\n\nBelow are my code counting this -","41cb1ec5":"# The correlation between essay length and its number of discourse elements\n\nDoes a long essay usually have more discourse elements than a shorter one?\n\n","996af77d":"# Paint the discourse type with different colors\n\nWhy do we need it? If we open a text file **007ACE74B050.txt** as an example -","cf93a1fe":"# A histogram showing the length of discourse type","2387ac50":"# Text clustering visualization\n\nAs inspired by the interesting work - [how to visualize text data](https:\/\/www.kaggle.com\/subinium\/how-to-visualize-text-dataset) from SUBIN AN, I created some cluster visualization for our dataset.","8fd0aaff":"# Does the text contain \"I think\"?\n\nWhen I read an essay and come across a phrase such as 'I think', without any hesitation, I know that the author started to state the position thereafter. Though curiosity drives me do check if it is still the case in our dataset.\n\nI check the discourse element one by one that whether it contains the phrase 'I think'.\n\nThe outcome is no surprise - 12.7% of the elements in the type **Position** does and the next one following up is **Concluding Statement**, with 8.5%.","6320afb3":"**I also add the type name at the end of each segment. Hope it makes each one looks more clearly to you.** ","5c76e5c2":"# What is the next step?\n\nThis notebook is an ongoing work. \n\nDo you have any other idea? Just let me know in the comment if you do or like the notebook (or not).","85270750":"# Does discourse type favor specific punctuation?\n\nFrom the Top 100 signal tokens chart above, I see one interesting fact that in Lead type, the most outstanding token is the question mark.\n\nInspired from it, let's check does each discourse type have specific favored punctuation?","dd65668d":"# Lexical uniqueness per discourse type\n\nI come up with a term probaly made myself - lexical uniqueness, which indicates the variability of the word usage in a sentence. In other words, if an essay has used words repeatedly, its lexical uniqueness will be low.\n\nTo calculate it, I use this equation below\n\n> Lexical uniqueness = number of unique words * 100 \/ number of total words\n\nFrom the chart, **Evidence** seems to have the **lowest** lexical uniqueness on average. \n\nWhy is that?","b1b1b321":"**The text itself doesn't show the discourse type on sentences.**\n\nSo let's use some colors to make each part more outstanding.\n\nFirstly, let's give each discourse type a color, as shown below:\n","68b3517d":"Thanks for reading!","719f44ee":"# Average word length in each element per discourse type\n\nDo people usually use longer words when they state Claims or Position?\n\nFrom this chart, it looks Claims and Postions both have the highest means (6.5 letters per word on average), however, Claims has a wider range of the value, indicating that it can have extremely long words in some sentences.","91d6db02":"### Choose whatever article you are interested\n\nSo what if you're interested in any other article? Or just intend to play around with the data by randomly clicking an id?\n\nSure. Here I've created a dropdown list where you can choose an article id, and once you click the **Start** button, the screen will present an article in colors by discouse type.\n\n**Please be aware that, for now, the interactive dropdown list can be working only in the edit mode** - You can edit\/copy the notebook and test it yourself. Since the dataset has around 15k articles in total, it might take 3~5 seconds to load the data.\n\nHope you like it!","0c729236":"Here you can input whatever string you want to check how much each discourse type contains.\n\nPlease be aware that for now the string is case sensitive, which means if you input 'For example' or 'for example', it will return different outcomes.\n\n**Running the notebook is required, so you may copy\/fork the notebook and play with it on your own.**","44b3ab3d":"# Relative median position of each discourse type in essays\n\nHow usually does each discourse type be placed in an essay in general. Here I get the median value of both the element starts and ends per discourse type, and plot a dumbbell chart of it.","cb4c1e99":"<h3 style=\"color:rgb(255, 102, 204);\">Lead<\/h3>\n<h3 style=\"color:rgb(0, 0, 102);\">Position<\/h3>\n<h3 style=\"color:rgb(51, 102, 255);\">Claim<\/h3>\n<h3 style=\"color:rgb(153, 102, 51);\">Counterclaim<\/h3>\n<h3 style=\"color:rgb(102, 204, 255);\">Rebuttal<\/h3>\n<h3 style=\"color:rgb(0, 0, 0);\">Evidence<\/h3>\n<h3 style=\"color:rgb(51, 51, 153);\">Concluding Statement<\/h3>","1f0fc3f8":"e.g. the word 'example' occurres in Evidence at a probability of 0.001428%, and such value is 245% of its average occurrance probablity among discouse types  \nBased on that, if we see 'example' in a piece of text, our first reasonable guess of the text discourse type should be 'Evidence'.","a3c69b62":"Then, taking the same passage as an example, let's see how it looks like after we've painted it."}}