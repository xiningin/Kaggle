{"cell_type":{"068bf591":"code","9ad47a91":"code","809de494":"code","bffc24e3":"code","11efd47d":"code","c89f13b5":"code","182cb61c":"code","137c07fd":"code","334b4d56":"code","7f22615d":"code","ac888fff":"code","a9c9d071":"code","a4213e48":"code","f8e83d4d":"code","87465e09":"code","05b85c34":"code","d5f456e5":"code","6a84d47c":"markdown","ef255b33":"markdown"},"source":{"068bf591":"!pip install -qqq -U git+https:\/\/github.com\/stalkermustang\/nuscenes-devkit.git","9ad47a91":"from pathlib import Path\nfrom PIL import Image","809de494":"# dir with all input data from Kaggle\nINP_DIR = Path('\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/')","bffc24e3":"# dir with index json tables (scenes, categories, logs, etc...)\nTABLES_DIR = INP_DIR.joinpath('train_data')","11efd47d":"# Adjust the dataroot parameter below to point to your local dataset path.\n# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps\n!ln -s {INP_DIR}\/train_images images\n!ln -s {INP_DIR}\/train_maps maps\n!ln -s {INP_DIR}\/train_lidar lidar","c89f13b5":"DATA_DIR = Path().absolute() \n# Empty init equals '.'.\n# We use this because we link train dirs to current dir (cell above)","182cb61c":"# dir to write KITTY-style dataset\nSTORE_DIR = DATA_DIR.joinpath('kitti_format')","137c07fd":"!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti -h","334b4d56":"# convertation to KITTY-format\n!python -m lyft_dataset_sdk.utils.export_kitti nuscenes_gt_to_kitti \\\n        --lyft_dataroot {DATA_DIR} \\\n        --table_folder {TABLES_DIR} \\\n        --samples_count 20 \\\n        --parallel_n_jobs 2 \\\n        --get_all_detections True \\\n        --store_dir {STORE_DIR}","7f22615d":"# check created (converted) files. velodyne = LiDAR poinclouds data (in binary)\n!ls {STORE_DIR}\/velodyne | head -2","ac888fff":"# render converted data for check. Currently don't support multithreading :(\n!python -m lyft_dataset_sdk.utils.export_kitti render_kitti \\\n        --store_dir {STORE_DIR}","a9c9d071":"# Script above write images to 'render' folder\n# in store_dir (where we have converted dataset)\nRENDER_DIR = STORE_DIR.joinpath('render')","a4213e48":"# get all rendered files\nall_renders = list(RENDER_DIR.glob('*'))\nall_renders.sort()","f8e83d4d":"# render radar data (bird view) and camera data with bboxes","87465e09":"Image.open(all_renders[0])","05b85c34":"Image.open(all_renders[1])","d5f456e5":"!rm -rf {STORE_DIR}","6a84d47c":"## In this kernel we convert LEVEL5 Lyft data (NuScenes format) to KITTI format, which is usually used in public repositories. After this you can search for repos, that solve KITTI 3d-detection task.","ef255b33":"## I'm use rendering only for check success converting. \n\n## Can be used to visualize NN predictions for test lyft set (visual metric estimation :D)"}}