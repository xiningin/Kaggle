{"cell_type":{"6850667b":"code","2c23b51a":"code","dc2f04bd":"code","4cf77691":"code","f670fc3f":"code","37d4e9a5":"code","1220b851":"code","8106e61b":"code","49a7092a":"code","0381230e":"code","84bebfea":"code","b0bfc370":"code","618a3fd9":"code","11f99315":"code","fe440eca":"code","9fa59b25":"code","937b3aaa":"code","433ff459":"code","929ec248":"code","f0a5acbc":"code","4a6009f4":"markdown","ec5677d8":"markdown","a23dab3e":"markdown","f2364a16":"markdown","b31cd8a4":"markdown","19ccf24b":"markdown","f66f0d37":"markdown"},"source":{"6850667b":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2c23b51a":"df = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")","dc2f04bd":"df.describe()","4cf77691":"df.columns","f670fc3f":"df.dropna(inplace=True)\ndf.drop(axis=1, columns=\"Id\", inplace=True)","37d4e9a5":"df.head()","1220b851":"from sklearn import preprocessing\nqualiVars = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\ndf[qualiVars] = df[qualiVars].apply(preprocessing.LabelEncoder().fit_transform)","8106e61b":"Xtrain = df.iloc[:,0:-1]\nYtrain = df.income","49a7092a":"from sklearn.model_selection import train_test_split","0381230e":"X_train, X_test, y_train, y_test = train_test_split(Xtrain,Ytrain,test_size=0.20)","84bebfea":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score","b0bfc370":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","618a3fd9":"rfc_pred = rfc.predict(X_test)","11f99315":"print(\"Random forest metrics\")\nprint(confusion_matrix(y_test, rfc_pred))\nprint(classification_report(y_test, rfc_pred))\nprint('Accuracy: ',accuracy_score(y_test, rfc_pred))","fe440eca":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","9fa59b25":"log_pred = logmodel.predict(X_test)","937b3aaa":"print(\"Logistic regression metrics\")\nprint(confusion_matrix(y_test, log_pred))\nprint(classification_report(y_test, log_pred))\nprint('Accuracy: ',accuracy_score(y_test, log_pred))","433ff459":"from sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train,y_train)","929ec248":"svm_pred = svm.predict(X_test)","f0a5acbc":"print(\"SVM metrics\")\nprint(confusion_matrix(y_test, svm_pred))\nprint(classification_report(y_test, svm_pred))\nprint('Accuracy: ',accuracy_score(y_test, svm_pred))","4a6009f4":"### SVM","ec5677d8":"### -----","a23dab3e":"# ADULT","f2364a16":"'''\nForam usadas as tr\u00eas t\u00e9cnicas acima (random forest, logistic regression e SVM) para classifica\u00e7\u00e3o na base Adult. Os \u00fanicos tratamentos feitos nos dados foram a remo\u00e7\u00e3o de valores inexistentes e a transforma\u00e7\u00e3o de atributos categ\u00f3ricos em num\u00e9ricos (tratamento feito em comum para os tr\u00eas m\u00e9todos). Ap\u00f3s esse tratamento, os modelos foram treinados e testados na base de dados, com a sele\u00e7\u00e3o de par\u00e2metros sendo arbitr\u00e1ria.\nEm termos de acur\u00e1cia, o random forest obteve o melhor resultado, enquanto SVM e Logistic regression ficaram bem pr\u00f3ximos. Sobre interpreta\u00e7\u00e3o do funcionamento do classificador, o random forest tamb\u00e9m leva vantagem, por ser mais simples de se explicar. Por\u00e9m, na tentativa de testar diversos valores para o parametro n_estimators, o tempo de processamento se tornou muito longo. Ainda assim, na tratativa simples que foi adotada, o random forest se mostrou o melhor classificador.\nCom um tratamento mais criterioso dos dados e outras estrat\u00e9gias para a sele\u00e7\u00e3o dos par\u00e2metros, \u00e9 esperado que os resultados obtidos se mostrem um pouco melhores.\n'''","b31cd8a4":"### RANDOM FOREST","19ccf24b":"### LOGISTIC REGRESSION","f66f0d37":"### -----"}}