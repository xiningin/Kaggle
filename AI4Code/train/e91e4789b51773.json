{"cell_type":{"44e40155":"code","235a4338":"code","71dccf0f":"code","f97c28f1":"code","445ab83e":"code","b4232911":"code","29b77058":"code","7bd56fae":"code","7d6fba1d":"code","aef79b3b":"code","03715f0c":"code","5024a0ce":"code","d812a903":"code","35fdf514":"code","b0490ec9":"code","dc4498d0":"code","62486eff":"code","8fb5b8b4":"code","db2fd12c":"code","60719450":"code","877749bf":"code","8c3b5a21":"code","c6cda1ed":"code","ab778307":"code","2e8f8b9b":"code","67f79e77":"code","16a2bf37":"code","d0fc3f8e":"code","b7d9b0ae":"code","60655c9e":"code","67868801":"code","45284a69":"code","01d9e1ca":"code","f5046520":"code","b566de34":"code","e9ee1ddb":"code","be32ef80":"code","ad61a3fc":"code","f22ad4c2":"code","ebbfaba5":"code","52cf8b67":"code","b6b23c4d":"code","2419f729":"code","217f2e7c":"code","7e1c7020":"code","1db7cd29":"code","27f3d5c3":"code","a4f0100c":"code","c14ccd3c":"code","f9568f8e":"code","22b7cdbb":"code","7b36ff52":"code","bd1362f7":"code","b02febdf":"code","e97edebe":"code","f1561e4d":"code","6b7dc9b2":"markdown","0f69b472":"markdown","efc7f605":"markdown","b0bb8ee6":"markdown","0be59d52":"markdown","c564ac98":"markdown","ec338fe1":"markdown","2292053b":"markdown","41fa3bfa":"markdown","92b77669":"markdown","42ac2a1e":"markdown","8bc9c8cd":"markdown","4c81d02d":"markdown","23d16cf3":"markdown","8bdf00aa":"markdown","f9eede10":"markdown","7c9b0c17":"markdown","342a4c0e":"markdown","f4a2fc3f":"markdown","7333d1a6":"markdown","71940544":"markdown","30c68dc5":"markdown","a5defcb4":"markdown","fbd4bc51":"markdown","e272ad15":"markdown","fb373a4b":"markdown","6a3acb5f":"markdown","5226362d":"markdown","eca01b67":"markdown","9cc1285b":"markdown","eb71ffb6":"markdown","1db676e9":"markdown","c7c88590":"markdown","82fe8ccc":"markdown"},"source":{"44e40155":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, cross_val_predict,RandomizedSearchCV, KFold\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport optuna","235a4338":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","71dccf0f":"features = [feature for feature in train_df.columns if feature not in ['id', 'target']]\nX_train = train_df[features]\ny_train = train_df['target']\nX_test = test_df[features]","f97c28f1":"print('Rows and Columns in train dataset:', train_df.shape)\nprint('Rows and Columns in test dataset:', test_df.shape)","445ab83e":"print('First 5 data in the train dataset:')\ntrain_df.head()","b4232911":"print('First 5 data in the test dataset:')\ntest_df.head()","29b77058":"print('Missing value in train dataset:', sum(train_df.isnull().sum()))\nprint('Missing value in test dataset:', sum(test_df.isnull().sum()))","7bd56fae":"print('Statistics on Train dataset')\ntrain_df.describe()","7d6fba1d":"print('Statistics on Test dataset')\ntest_df.describe()","aef79b3b":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(train_df[feature] ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-0.2, 5, 'Features Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.2, 4.5, 'All features have bimodal or multimodal distribution', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.get_xaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.get_xaxis().set_visible(False)","03715f0c":"fig = plt.figure(figsize=(7.5, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\nsns.kdeplot(train_df['target'], ax=ax0, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nax0.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_xlabel('target')\n\nbackground_color = \"#f6f6f6\"\n\nax0.text(-0.9, 0.74, 'Target Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.9, 0.68, 'Target feature has bimodal distribution', fontsize=13, fontweight='light', fontfamily='serif')\nax0.tick_params(axis='y', left=False)\nax0.get_yaxis().set_visible(False)\nax0.set_facecolor(background_color)\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)","5024a0ce":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(test_df[feature] ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-0.2, 5.6, 'Features Distribution on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.2, 5, 'Test features distribution resemble train features distribution', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.get_xaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.get_xaxis().set_visible(False)","d812a903":"X_all = pd.concat([X_train, X_test], axis=0)\n\nfig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(X_all[feature] ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-0.2, 5.2, 'Combined Distribution of Train & Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.2, 4.6, 'Combined features between train & test dataset resemble individual distributions', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.get_xaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.get_xaxis().set_visible(False)","35fdf514":"fig = plt.figure(figsize=(18, 8), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ncolors = [\"#ffd514\", \"#f6f6f6\",\"#ffd514\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(0, -1, 'Features Correlation on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(0, -0.4, 'Some features have a high correlation', fontsize=13, fontweight='light', fontfamily='serif')\n\nax1.set_facecolor(background_color)\nax1.text(-0.1, -1, 'Features Correlation on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax1.text(-0.1, -0.4, 'Features in test dataset resemble features in train dataset ', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nsns.heatmap(X_train.corr()[X_train.corr() >= 0.7], ax=ax0, linewidths=.1, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g')\n\nsns.heatmap(X_test.corr()[X_test.corr() >= 0.7], ax=ax1, linewidths=.1, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g')\n\nplt.show()","b0490ec9":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.5, hspace=0.5)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.scatterplot(train_df[feature], train_df['target'] ,ax=locals()[\"ax\"+str(run_no)], color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\n        locals()[\"ax\"+str(run_no)].grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        run_no += 1\n        \nax0.text(-0.4, 13.8, 'Features and Target Relation', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.4, 12, 'cont2 and cont14 have a distinct separation', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.tick_params(axis='y', left=False)\nax14.get_xaxis().set_visible(False)\nax14.get_yaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.tick_params(axis='y', left=False)\nax15.get_xaxis().set_visible(False)\nax15.get_yaxis().set_visible(False)","dc4498d0":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(train_df['target'], ax=locals()[\"ax\"+str(run_no)], color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        sns.kdeplot(train_df[feature] * 10 ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-0.2, 0.9, 'Features and Target Distribution Comparison', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.2, 0.8, 'By multiplying features with 10, make the comparison between features and target distribution possible', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.get_xaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.get_xaxis().set_visible(False)","62486eff":"fig = plt.figure(figsize=(15, 30), facecolor='#f6f6f6')\ngs = fig.add_gridspec(15, 1)\ngs.update(wspace=0.5, hspace=0.5)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 15):\n    locals()[\"ax\"+str(row)] = fig.add_subplot(gs[row, 0])\n    locals()[\"ax\"+str(row)].set_facecolor(background_color)\n    for s in [\"top\",\"right\",\"left\"]:\n        locals()[\"ax\"+str(row)].spines[s].set_visible(False)\n    run_no += 1\n\nrun_no = 0\nfor feature in features:\n    sns.scatterplot(train_df['id'], train_df[feature],ax=locals()[\"ax\"+str(run_no)],  color='#ffd514', linewidth=0, zorder=3)\n    locals()[\"ax\"+str(run_no)].grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1\n        \nsns.scatterplot(train_df['id'], train_df['target'],ax=ax14, color='#ff819a', linewidth=0, zorder=3)\nax14.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nax0.text(-100, 1.6, 'Features and Target by Time Series on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-100, 1.3, 'cont10 has lower items on low and high value compared to other features', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax15.spines[s].set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.tick_params(axis='y', left=False)\nax15.get_xaxis().set_visible(False)\nax15.get_yaxis().set_visible(False)","8fb5b8b4":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.5, hspace=0.5)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.scatterplot(train_df['target']\/(train_df[feature] * 10), train_df['target'] ,ax=locals()[\"ax\"+str(run_no)], color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\n        locals()[\"ax\"+str(run_no)].grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-15000, 13.8, 'Division and Target Relation on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-15000, 12, 'There are feature division that has an upward diagonal cut ', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.tick_params(axis='y', left=False)\nax14.get_xaxis().set_visible(False)\nax14.get_yaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.tick_params(axis='y', left=False)\nax15.get_xaxis().set_visible(False)\nax15.get_yaxis().set_visible(False)","db2fd12c":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(train_df['target']\/(train_df[feature] * 10) ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-5000, 0.003, 'Division Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-5000, 0.0027, 'cont5 shows a clear separation at the amount around 2', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.get_xaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.get_xaxis().set_visible(False)","60719450":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.5, hspace=0.5)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.scatterplot(train_df['target'] - (train_df[feature] * 10), train_df['target'] ,ax=locals()[\"ax\"+str(run_no)], color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\n        locals()[\"ax\"+str(run_no)].grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-10, 13.5, 'Substraction and Target Relation on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-10, 12, 'Most of the substraction showing a parallelogram shape to the target', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.tick_params(axis='y', left=False)\nax14.get_xaxis().set_visible(False)\nax14.get_yaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.tick_params(axis='y', left=False)\nax15.get_xaxis().set_visible(False)\nax15.get_yaxis().set_visible(False)","877749bf":"fig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(train_df['target'] - (train_df[feature] * 10) ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n        \nax0.text(-6, 0.28, 'Substraction Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-6, 0.25, 'cont7 shows a one modal distribution', fontsize=13, fontweight='light', fontfamily='serif')\n\nfor s in [\"top\", \"bottom\", \"right\",\"left\"]:\n    ax14.spines[s].set_visible(False)\n    ax15.spines[s].set_visible(False)\n\nax14.tick_params(axis='x', bottom=False)\nax14.get_xaxis().set_visible(False)\n\nax15.tick_params(axis='x', bottom=False)\nax15.get_xaxis().set_visible(False)","8c3b5a21":"cv = KFold(n_splits=5, shuffle=True, random_state=42)","c6cda1ed":"%%time\nlin_reg = LinearRegression()\nscores = cross_val_score(lin_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nlin_rmse_scores = np.sqrt(-scores)\nprint('Linear Regression performance:', lin_rmse_scores)","ab778307":"%%time\ntree_reg = DecisionTreeRegressor(random_state=42)\nscores = cross_val_score(tree_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\ntree_rmse_scores = np.sqrt(-scores)\nprint('Decision Tree Regressor performance:', tree_rmse_scores)","2e8f8b9b":"%%time\nforest_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\nscores = cross_val_score(forest_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nforest_rmse_scores = np.sqrt(-scores)\nprint('Random Forest performance:', forest_rmse_scores)","67f79e77":"%%time\nlgbm_reg = LGBMRegressor(random_state=42)\nscores = cross_val_score(lgbm_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nlgbm_rmse_scores = np.sqrt(-scores)\nprint('LGBM performance:', lgbm_rmse_scores)","16a2bf37":"%%time\nxgb_reg = XGBRegressor(random_state=42)\nscores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nxgb_rmse_scores = np.sqrt(-scores)\nprint('XGBoost performance:', xgb_rmse_scores)","d0fc3f8e":"%%time\ncb_reg = CatBoostRegressor(random_state=42, verbose=False)\nscores = cross_val_score(cb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\ncb_rmse_scores = np.sqrt(-scores)\nprint('CatBoost performance:', cb_rmse_scores)","b7d9b0ae":"%%time\nab_reg = AdaBoostRegressor(random_state=42)\nscores = cross_val_score(ab_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nab_rmse_scores = np.sqrt(-scores)\nprint('AdaBoost performance:', ab_rmse_scores)","60655c9e":"def build_and_compile_model(norm):\n    model = keras.Sequential([\n        norm,\n        layers.Dense(64, activation='relu'),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(1)])\n    \n    model.compile(loss='mean_squared_error',\n                 optimizer=tf.keras.optimizers.Adam(0.001))\n    return model","67868801":"%%time\nnormalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(X_train))\ndnn_model = build_and_compile_model(normalizer)\nhistory = dnn_model.fit(X_train, y_train, validation_split=0.2,\n                       verbose=0, epochs=100)","45284a69":"fig = plt.figure(figsize=(7.5, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(2, 1)\nax0 = fig.add_subplot(gs[0, 0])\nsns.kdeplot(np.sqrt(history.history['val_loss']) ,ax=ax0, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nax0.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nbackground_color = \"#f6f6f6\"\n\nax0.tick_params(axis='y', left=False)\nax0.get_yaxis().set_visible(False)\nax0.set_facecolor(background_color)\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)","01d9e1ca":"%%time\nlin_reg = LinearRegression()\ny_predict = cross_val_predict(lin_reg, X_train, y_train, cv=cv, n_jobs=-1)","f5046520":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12, 'Linear Regression Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11, 'Most of the prediction concentrate in a range of 7 to 9', fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","b566de34":"%%time\ntree_reg = DecisionTreeRegressor(random_state=42)\ny_predict = cross_val_predict(tree_reg, X_train, y_train, cv=cv, n_jobs=-1)","e9ee1ddb":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12, 'Decision Tree Regressor Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11, 'Prediction distribution resemble the target distribution but the accuracy are way off ', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","be32ef80":"%%time\nforest_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\ny_predict = cross_val_predict(forest_reg, X_train, y_train, cv=cv, n_jobs=-1)","ad61a3fc":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12, 'Random Forest Regressor Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11, 'Most of the prediction concentrate in a range of 6.7 to 9.5', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","f22ad4c2":"%%time\nlgbm_reg = LGBMRegressor(random_state=42)\ny_predict = cross_val_predict(lgbm_reg, X_train, y_train, cv=cv, n_jobs=-1)","ebbfaba5":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12, 'LGBM Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11, 'Most of the prediction concentrate in a range of 7 to 9.5', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","52cf8b67":"%%time\nxgb_reg = XGBRegressor(random_state=42)\ny_predict = cross_val_predict(xgb_reg, X_train, y_train, cv=cv, n_jobs=-1)","b6b23c4d":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12.5, 'XGBoost Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11.5, 'Most of the prediction concentrate in a range of 6.5 to 10', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","2419f729":"%%time\ncb_reg = CatBoostRegressor(random_state=42, verbose=False)\ny_predict = cross_val_predict(cb_reg, X_train, y_train, cv=cv, n_jobs=-1)","217f2e7c":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12.5, 'CatBoost Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11.5, 'Most of the prediction concentrate in range of 7 to 9.5 resemble the result of LGBM', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","7e1c7020":"%%time\nab_reg = AdaBoostRegressor(random_state=42)\ny_predict = cross_val_predict(ab_reg, X_train, y_train, cv=cv, n_jobs=-1)","1db7cd29":"fig = plt.figure(figsize=(10, 4), facecolor='#f6f6f6')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ngs.update(wspace=0.2, hspace=0)\nbackground_color = \"#f6f6f6\"\n\nsns.scatterplot(y_train, y_train, ax=ax0, color='#ff819a', linewidth=0.3, edgecolor='#5a0012', zorder=3)\nsns.scatterplot(y_predict, y_train, ax=ax0, color='#ffd514', linewidth=0.3, edgecolor='#4f4100', zorder=3)\n\nsns.kdeplot(y_train, ax=ax1, color='#ff819a', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\nsns.kdeplot(y_predict, ax=ax1, color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n\nax0.text(-1.5, 12.5, 'AdaBoost Results', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.5, 11.5, 'Most of the prediction concentrate in a range of 7.3 to 8.7', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nax0.set_ylabel('target')\nax0.set_xlabel('prediction')\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\nax0.grid(which='major', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax1.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n\nfor s in [\"top\", \"right\", \"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)","27f3d5c3":"def objective(trial):    \n    params = {\n            'random_state': 42,\n            'max_depth': trial.suggest_int('max_depth', 1, 14),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0)\n            }\n    \n    lgbm_reg = LGBMRegressor()\n    lgbm_reg.set_params(**params)\n    scores = cross_val_score(lgbm_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n    rmse = np.sqrt(-scores)\n    return np.mean(rmse)","a4f0100c":"study = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 1)\nbest_params = study.best_trial.params","c14ccd3c":"best_params","f9568f8e":"%%time\nlgbm_reg = LGBMRegressor()\nlgbm_reg.set_params(**best_params)\nscores = cross_val_score(lgbm_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nlgbm_rmse_scores = np.sqrt(-scores)\nprint('LGBM performance:', lgbm_rmse_scores)","22b7cdbb":"def objective(trial):    \n    params = {\n            'random_state': 42,\n            'max_depth': trial.suggest_int('max_depth', 1, 14),\n            'eta': trial.suggest_float('eta', 0.01, 1.0),\n            }\n    xgb_reg = XGBRegressor()\n    xgb_reg.set_params(**params)\n    scores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n    rmse = np.sqrt(-scores)\n    return np.mean(rmse)","7b36ff52":"study = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 1)\nbest_params = study.best_trial.params","bd1362f7":"best_params","b02febdf":"%%time\nxgb_reg = XGBRegressor()\nxgb_reg.set_params(**best_params)\nscores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\nxgb_rmse_scores = np.sqrt(-scores)\nprint('XGBoost performance:', xgb_rmse_scores)","e97edebe":"X_train['below8'] = np.where(y_train < 8, 1, 0)","f1561e4d":"%%time\ncb_reg = CatBoostRegressor(random_state=42, verbose=False)\nscores = cross_val_score(cb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\ncb_rmse_scores = np.sqrt(-scores)\nprint('CatBoost performance:', cb_rmse_scores)","6b7dc9b2":"[back to top](#table-of-contents)\n<a id=\"adaboost\"><\/a>\n## 4.7 AdaBoost","0f69b472":"[back to top](#table-of-contents)\n<a id=\"adaboost_result\"><\/a>\n## 5.7 AdaBoost","efc7f605":"[back to top](#table-of-contents)\n<a id=\"random_forest_result\"><\/a>\n## 5.3 Random Forest","b0bb8ee6":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# 3. EDA\n\n<a id=\"general\"><\/a>\n## 3.1 General\n**Observations:**\n* There are 300,000 rows with 16 columns in the train dataset and 200,000 rows with 15 columns in test dataset. \n* There are 14 features (columns with prefix 'cont'), 1 id and 1 target column. Target column is not included in the test set.\n* There is no missing values in the train and test dataset.\n* cont1 to cont14 features roughly have a range from 0 to 1. \n* target variable roughly has a range from 0 to 10.","0be59d52":"[back to top](#table-of-contents)\n<a id=\"features_target_substraction\"><\/a>\n## 3.7 Features and Target by Substraction\nSubstract the target with the individual feature. Individual feature has been multiply to 10 to more resemble with the target.\n\n**Observations:**\n* Parallelogram is spotted on most of the substraction especially on `cont5`, `cont8` and `cont14`.\n* Substraction on `cont7` showing a one modal distribution.","c564ac98":"[back to top](#table-of-contents)\n<a id=\"features_target_division\"><\/a>\n## 3.6 Features and Target by Division\nDivide the target with the individual feature. Individual feature has been multiply to 10 to more resemble with the target.\n\n**Observations:**\n* Some features has a clear upward diagonal cut which can clearly be seen on `cont4`, `cont5`, `cont8`, `cont11`, `cont12` and `cont14`.\n* Due to small division between target and the feature there are some features that looks like exclamation mark as can be seen on `cont1`, `cont2`, `cont7`, `cont9` and `cont10`.\n\n**Ideas:**\n* Triangle shape between the target and the division can be further explored for feature engineering.\n* `cont5` division can be futher explored for feature engineering as there is a quite a clear cut in the distribution ","ec338fe1":"[back to top](#table-of-contents)\n<a id=\"below_result\"><\/a>\n## 7.2 Below 8 Result\n\nUsing the catboost without any hyperparameter tuning, RMSE can reach to around 0.37 to 0.38.","2292053b":"[back to top](#table-of-contents)\n<a id=\"features_correlation\"><\/a>\n## 3.3 Features Correlation\n\n**Observations:**\n\n* Correlation above 0.7 or below -0.7 are considered as high correlation. \n* Features `cont1`, `cont6` and `cont9` to `cont13` have a high correlation with each others.\n\n**Ideas:**\n* Consider to remove these features and retain 1 feature with the highest correlation with the target.","41fa3bfa":"[back to top](#table-of-contents)\n<a id=\"simple_models\"><\/a>\n# 4. Simple Models\nResults of models using default hyperparameters and without any feature engineering using 5 cross validations:\n* The best performance is **CatBoost**, the model will be used for the submission.\n* **Decision Tree Regressor** has the worst performance compared to others model.\n* The fastest model is **Linear Regression** and the performance is higher than **Decision Tree Regressor**. \n* The most time consuming model is **Random Forest** with wall time of around 20 minutes. The prediction result is also medicore compared to other models. \n* **AdaBoost** and **Linear Regression** performance are quite the same.\n* **LGBM** and **XGBoost** have quite the same performance with LGBM has the fastest time to process.\n* **Deep Neural Network** use 1 normalization layer, 2 hidden layers (64 and 64) and 1 output layer. It still can not beat plain vannila CatBoost.","92b77669":"[back to top](#table-of-contents)\n<a id=\"features_target\"><\/a>\n## 3.4 Features and Target\n**Observations:**\n* The relation between features and the target is following the distribution of the features. There are 2 features that are interesting, they are:\n    * Nine distinct separations can be seen in the `cont2` feature.\n    * There are 2 distinct separations in the `cont14` relative to the target.\n* By multiplying the features by 10 will approximate a range that resemble target variable which can be used to compare the distribution between the target and the features. \n\n**Ideas:**\n* It is possible to transformed `cont2` and `cont14` into categorical variables to be used for target encoding.","42ac2a1e":"[back to top](#table-of-contents)\n<a id=\"linear_regression\"><\/a>\n## 4.1 Linear Regression","8bc9c8cd":"[back to top](#table-of-contents)\n<a id=\"lgbm_hpt\"><\/a>\n## 6.1 LGBM","4c81d02d":"[back to top](#table-of-contents)\n<a id=\"target_splitting\"><\/a>\n# 7. Target Splitting\nBased on the distribution of the target, splitting the target into two distribution can be done. The distribution can be divided at 8, and a new categorical feature can be created in this case **below 8** feature. The performance of the feature is great, but reproducing the feature on the test will be a challenge. One way is to create a classification problem that predict the **below 8** feature.\n\n<a id=\"below_feature\"><\/a>\n## 7.1 Below 8 Feature\nProducing the feature using the Train set is easy but producing it with the test set will be a challenge.","23d16cf3":"Below are the RMSE distribution on validation dataset from Deep Neural Network","8bdf00aa":"[back to top](#table-of-contents)\n<a id=\"deep_neural_network\"><\/a>\n## 4.8 Deep Neural Network","f9eede10":"[back to top](#table-of-contents)\n<a id=\"introduction\"><\/a>\n# 1. Introduction\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. ","7c9b0c17":"[back to top](#table-of-contents)\n<a id=\"decision_tree_regressor\"><\/a>\n## 4.2 Decision Tree Regressor ","342a4c0e":"[back to top](#table-of-contents)\n<a id=\"features_distribution\"><\/a>\n## 3.2 Features and Target Distribution\n**Observations:**\n* Train and test dataset features have bimodal or multimodal distributions.\n* Train and test datset features distribution more or less are the same, there is no significant gap between each features in the test and train dataset.","f4a2fc3f":"[back to top](#table-of-contents)\n<a id=\"xgboost_hpt\"><\/a>\n## 6.2 XGBoost","7333d1a6":"[back to top](#table-of-contents)\n<a id=\"lgbm\"><\/a>\n## 4.4 LGBM","71940544":"[back to top](#table-of-contents)\n<a id=\"catboost_result\"><\/a>\n## 5.6 CatBoost","30c68dc5":"[back to top](#table-of-contents)\n<a id=\"lgbm_result\"><\/a>\n## 5.4 LGBM","a5defcb4":"[back to top](#table-of-contents)\n<a id=\"preparation\"><\/a>\n# 2. Preparation","fbd4bc51":"[back to top](#table-of-contents)\n<a id=\"xgboost_result\"><\/a>\n## 5.5 XGBoost","e272ad15":"[back to top](#table-of-contents)\n<a id=\"decision_tree_regressor_result\"><\/a>\n## 5.2 Decision Tree Regressor ","fb373a4b":"[back to top](#table-of-contents)\n<a id=\"xgboost\"><\/a>\n## 4.5 XGBoost","6a3acb5f":"[back to top](#table-of-contents)\n<a id=\"simple_models_result\"><\/a>\n# 5. Simple Models Results\nThis section shows the result of the simple model prediction with comparison with target:\n* Scatter plot shows the prediction relative to the target **<font color=\"#ffd514\">(yellow)<\/font>** and perfect prediction **<font color=\"#ff819a\">(pink)<\/font>** will look like.\n* Distribution plot shows the prediction distribution **<font color=\"#ffd514\">(yellow)<\/font>** and target distribution **<font color=\"#ff819a\">(pink)<\/font>**.\n\n**Observations:**\n* All of the model concentrate their predictions on 6.5 to 10 except for Decision Tree Regressor that have a more diverse result but not accurate\n* All of the model failed to follow the target bimodal distribution except for Decision Tree Regressor. It is follow the target distribution but the prediction is not accurate. \n* All of the model result (except for Distribution Tree Regressor) are a unimodal distribution and they look like an exclamation mark if it compared with a perfect prediction. This is due to a short range prediction compared to target.\n\n[back to top](#table-of-contents)\n<a id=\"linear_regression_result\"><\/a>\n## 5.1 Linear Regression","5226362d":"[back to top](#table-of-contents)\n<a id=\"features_target_time\"><\/a>\n## 3.5 Features and Target by Time Series\nAssuming id feature is a time feature\n\n**Observations:**\n* Features are consistently distributed around 0 - 1 across time (id).\n* `cont10` has lower items on low and high value compared to other features. It gives a sense that the variance is high.\n* `cont7` and `cont9` have lower items on lower value with cont9 has lower items than `cont7`.\n\n**Ideas:**\n* Removing records that has more variance in the `cont7`, `cont9` and `cont10`.","eca01b67":"# Table of Contents\n\n<a id=\"table-of-contents\"><\/a>\n1. [Introduction](#introduction)\n2. [Preparation](#preparation)\n3. [EDA](#eda)\n    * 3.1 [General](#general)\n    * 3.2 [Features Distribution](#features_distribution)\n    * 3.3 [Features Correlation](#features_correlation)\n    * 3.4 [Features vs Target](#features_target)\n    * 3.5 [Features and Target by Time](#features_target_time)\n    * 3.6 [Features and Target by Division](#features_target_division)\n    * 3.7 [Features and Target by Substraction](#features_target_substraction)\n4. [Simple Models](#simple_models)\n    * 4.1 [Linear Regression](#linear_regression)\n    * 4.2 [Decision Tree Regressor](#decision_tree_regressor)\n    * 4.3 [Random Forest](#random_forest)\n    * 4.4 [LGBM](#lgbm)\n    * 4.5 [XGBoost](#xgboost)\n    * 4.6 [CatBoost](#catboost)\n    * 4.7 [AdaBoost](#adaboost)\n    * 4.8 [Deep Neural Network](#deep_neural_network)\n5. [Simple Models Result](#simple_models_result)    \n    * 5.1 [Linear Regression](#linear_regression_result)\n    * 5.2 [Decision Tree Regressor](#decision_tree_regressor_result)\n    * 5.3 [Random Forest](#random_forest_result)\n    * 5.4 [LGBM](#lgbm_result)\n    * 5.5 [XGBoost](#xgboost_result)\n    * 5.6 [CatBoost](#catbosst_result)\n    * 5.7 [AdaBoost](#adaboost_result)\n6. [Optuna Hyperparameters Tuning](#hyperparameters)\n    * 6.1 [LGBM](#lgbm_hpt)\n    * 6.2 [XGBoost](#xgboost_hpt)\n7. [Target Splitting](#target_splitting)\n    * 7.1 [Below 8 Feature](#below_feature)\n    * 7.2 [Below 8 Result](#below_result)\n8. [Winners Solutions](#winners_solutions)","9cc1285b":"[back to top](#table-of-contents)\n<a id=\"hyperparameters\"><\/a>\n# 6. Optuna Hyperparameters Tuning\nThis section purpose is to demonstrate the hyperparameters tuning using Optuna on LGBM, XGBoost and CatBoost. The objective is to optimized average RMSE from 5 CVs and to speed up the process the number of trials is set to 1 which is not ideal. ","eb71ffb6":"[back to top](#table-of-contents)\n<a id=\"winners_solutions\"><\/a>\n# 8. Winners Solutions\nCongratulations for all the winners and thank you for sharing your solution. Below are the winners and their solutions:\n* 1st place position: [danzel](https:\/\/www.kaggle.com\/springmanndaniel) - [1st place solution](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/discussion\/216037)\n* 2nd place position: [Ren](https:\/\/www.kaggle.com\/ryanzhang) - [2nd solution write up.](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/discussion\/216070)\n* 3rd place position: [Fatih](https:\/\/www.kaggle.com\/fatihozturk) - [3rd place solution](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/discussion\/216087)","1db676e9":"Setting up the 5 cross validation using KFold to get a consistent validation dataset:","c7c88590":"[back to top](#table-of-contents)\n<a id=\"catboost\"><\/a>\n## 4.6 CatBoost","82fe8ccc":"[back to top](#table-of-contents)\n<a id=\"random_forest\"><\/a>\n## 4.3 Random Forest"}}