{"cell_type":{"003698fb":"code","027a1a4a":"code","bfa34a58":"code","d0f6b661":"code","6d9247b7":"code","08a06956":"code","47fecdaa":"code","1a6d107d":"code","a9afe9a7":"code","486729d9":"code","316ae762":"code","48c44588":"code","a5e6a09d":"code","76863e09":"code","afcfbefd":"code","0eb6ab30":"code","74ca10f6":"code","d71c0435":"code","9f67f2d2":"code","e05aa241":"code","e91fba03":"code","9e6366aa":"code","063fd781":"code","33891a5a":"code","fc00e3ef":"code","032f3e8a":"code","b8824ebb":"code","56f83d3d":"code","4d8fca8d":"code","a23a71a6":"code","d3563d6a":"code","b41333bf":"code","32c3b224":"code","b86d2e6f":"code","9ecc4289":"code","9cf3060b":"code","3e008c53":"code","fb5e783b":"code","7b15fb04":"code","fee2cdfa":"code","cd04444e":"code","4293ae7a":"code","23cbb0c0":"code","6a8c5b00":"code","13ef389a":"code","8320fead":"code","4674b245":"code","fb13513d":"code","7608d060":"code","9e9c3e42":"code","84cee00e":"code","349cfd8f":"code","a5a239d6":"code","81eecf44":"code","c8c83be6":"code","195381a0":"code","2bd114b8":"code","3f61510f":"code","7be7b38d":"code","95d9ff50":"code","0605aa73":"code","f2ec1c42":"code","6683c454":"code","e27d699d":"code","d0d48fba":"code","47049d9b":"code","b77f0a6a":"code","650d6535":"code","cbc995e3":"code","64526d14":"code","78037690":"markdown","a43f72d8":"markdown","2e334052":"markdown","e63ec8f6":"markdown","53ee490f":"markdown","1f59cff0":"markdown","cf6c67f7":"markdown","04dae98e":"markdown","264f29c7":"markdown","06a57c15":"markdown","591d8e35":"markdown","c21adb41":"markdown","d29d75cd":"markdown","3a2cf2e5":"markdown","f38ac441":"markdown","a1ec40f0":"markdown","daf4a672":"markdown","24862d28":"markdown","65c2f97e":"markdown","35ab78e3":"markdown","f4239551":"markdown","fdf7ba97":"markdown","ab3d185c":"markdown","0ea7edef":"markdown","4de54b27":"markdown","7b35c031":"markdown","daa100d8":"markdown","01e44864":"markdown","15eaa605":"markdown","f08b7609":"markdown","81dbade9":"markdown","e1460a28":"markdown","85dd61b1":"markdown","c5265dc8":"markdown","63dd31ab":"markdown","26684ba6":"markdown","b5a6e232":"markdown","b8f734b2":"markdown","cbb8b28e":"markdown","b48a42c3":"markdown","01e065b1":"markdown","dc3e5db9":"markdown","3de6d4e3":"markdown","7706ec8e":"markdown","7f6559ec":"markdown","57c38ad4":"markdown","63d05abc":"markdown","88eefb20":"markdown","1ff167d9":"markdown","2009157b":"markdown","5d0e4f7a":"markdown","06ccc960":"markdown","2256feef":"markdown"},"source":{"003698fb":"#data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\n# visualization\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#sklearn import\n#  Data Modelling Libraries\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n                             GradientBoostingClassifier, ExtraTreesClassifier,\n                             VotingClassifier)\n\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score, cross_val_predict,\n                                     StratifiedKFold, learning_curve,train_test_split)\n\nfrom sklearn.metrics import (confusion_matrix, accuracy_score) \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\n","027a1a4a":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ncombine = [train_df, test_df]\nprint(\"Our dataset feature are: \" ,train_df.columns.values)","bfa34a58":"# preview the start of the data\ntrain_df.head()","d0f6b661":"# preview the end of the data\ntrain_df.tail()","6d9247b7":"#Show information about the data checking the types of the features And if there are any null values in the data set\ntrain_df.info()\nprint(\"\\n\")\ntest_df.info()","08a06956":"train_df.describe()","47fecdaa":"#Checking the Probabilery between Pclass (ticket class) and the survival rate\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1a6d107d":"# Checking the Probabilery between the sex and the survival rate\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","a9afe9a7":"# Checking the Probabilery between SibSp(# of siblings \/ spouses aboard the Titanic) and the survival rate\ntrain_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","486729d9":"# Checkingthe Probabilery  between Parch( # of parents \/ children aboard the Titanic) and the Survived Rate\ntrain_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","316ae762":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain_df['Survived'].value_counts().plot.pie(ax=ax[0],explode=[0,0.1],shadow=True,autopct='%1.1f%%')\nax[0].set_title('Survived',fontsize=30)\nax[0].set_ylabel('Count')\nsns.set(font=\"Verdana\")\nsns.set_style(\"ticks\")\nsns.countplot('Survived',hue='Sex',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\nplt.ioff() # This removes the matplotlib notifications","48c44588":"# Visual the correlation bewteen Age and if survived ( 0 = did not survived & 1 = survived )\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","a5e6a09d":"#Visual the correlation bewteen Pclass and if survived ( 0 = did't survived - False  & 1 = survived - True )\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=3, aspect=2.5)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","76863e09":"grid = sns.FacetGrid(train_df, row='Embarked', height=3, aspect=2.5)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex' , palette = 'pastel')\ngrid.add_legend()","afcfbefd":"#Embarked places map - C = Cherbourg, Q = Queenstown, S = Southampton# \ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', height=3, aspect=2.5)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","0eb6ab30":"grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', height=3, aspect=2.5)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","74ca10f6":"# fill empty values in the dataframe\ndef fill_na_median(df, column_name):\n    df_not_null = df[~df[column_name].isnull()]\n    df[column_name].fillna(df_not_null[column_name].median(), inplace=True) \n\ndef fill_na_mean(df, column_name):\n    df_not_null = df[~df[column_name].isnull()]\n    df[column_name].fillna(df_not_null[column_name].mean(), inplace=True) \n\ndef fill_na_random_pick_column_distribution(df, column_name):\n    df_not_null = df[~df[column_name].isnull()]\n    df_null = df[df[column_name].isnull()]\n    options = np.random.choice(df_not_null[column_name])\n    df[column_name] = df[column_name].apply(lambda x: np.random.choice(df_not_null[column_name]) if pd.isnull(x) else x)","d71c0435":"fill_na_median(train_df, 'Fare')\nfill_na_median(test_df, 'Fare')\nfill_na_random_pick_column_distribution(train_df, 'Age')\nfill_na_random_pick_column_distribution(test_df, 'Age')\nfill_na_random_pick_column_distribution(train_df, 'Embarked')\nfill_na_random_pick_column_distribution(test_df, 'Embarked')","9f67f2d2":"\nprint(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","e05aa241":"# checking if there are any None values left\ntrain_df.isna().any()","e91fba03":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","9e6366aa":"# reduce the number of options in 'Title' attribute to get the data ready to modeling - by union the Rare title together\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","063fd781":"# PassengerId and the Name features aren't relevat to out modeling so lets drop them\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","33891a5a":"for dataset in combine:\n    # Number of brother and parntes on the boat . + 1 representing the current passenger\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","fc00e3ef":"\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","032f3e8a":"# Removing our old features\ntrain_df = train_df.drop(['Parch', 'SibSp'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","b8824ebb":"\nmostfreq = train_df.Embarked.dropna().mode()[0]\nmostfreq\nfor dataset in combine:\n\n    dataset['Embarked'] = dataset['Embarked'].fillna(mostfreq)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","56f83d3d":"# show absolute correlation between features in a heatmap\nplt.figure(figsize=(12,10))\ncor = np.abs(train_df.corr())\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds, vmin=0, vmax=1)\nplt.show()","4d8fca8d":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1)","a23a71a6":"# create dummy variables\n_feature = 'Sex'\n\n# train\ndummies = pd.get_dummies(X_train[_feature])\nX_train = X_train.join(dummies)\n\n# test\ndummies = pd.get_dummies(X_test[_feature])\nX_test = X_test.join(dummies)","d3563d6a":"feature = 'person'\n\n# It's likely the age threshold for adults was younger in the early 1900s.\n# Account of a 9 year-old boy almost getting refused a lifeboat:\n# https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/winnie-coutts.html\n# May want to try younger ages here.\nchild_age = 14\n\ndef get_person(passenger):\n    \"\"\"\n    Returns a person value of 'female_adult', 'male_adult', 'child'.\n    \"\"\"\n    age, sex = passenger\n    \n    if (age < child_age):\n        return 'child'\n    elif (sex == 'female'):\n        return 'female_adult'\n    else:\n        return 'male_adult'\n    \nX_train = X_train.join(pd.DataFrame(X_train[['Age', 'Sex']].apply(get_person, axis=1), columns=['person']))\nX_test = X_test.join(pd.DataFrame(X_test[['Age', 'Sex']].apply(get_person, axis=1), columns=['person']))\n\nX_train['person'].value_counts().sort_index()","b41333bf":"_feature = 'person'\n\n_columns = ['male_adult', 'female_adult', 'child']\n\n# train\ndummies = pd.get_dummies(X_train[_feature])\nX_train = X_train.join(dummies)\n\n# test\ndummies = pd.get_dummies(X_test[_feature])\nX_test = X_test.join(dummies)","32c3b224":"# create dummy variables for person column. \n_feature = 'Pclass'\n\n# train\ndummies = pd.get_dummies(X_train[_feature], prefix='class')\nX_train = X_train.join(dummies)\n\n# test\ndummies = pd.get_dummies(X_test[_feature], prefix='class')\nX_test = X_test.join(dummies)","b86d2e6f":" #Turn the 'sex' feature to numeric category ( 0 is refrence to male and 1 to female)\n\n\nX_train['Sex'] = X_train['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\nX_test['Sex'] = X_test['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntrain_df.head()","9ecc4289":"# Turn the 'Title' feature to numeric category\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nX_train['Title'] = X_train['Title'].map(title_mapping)\nX_train['Title'] = X_train['Title'].fillna(0) # when you see a nane value replace it with 0\n\n\nX_test['Title'] = X_test['Title'].map(title_mapping)\nX_test['Title'] = X_test['Title'].fillna(0) # when you see a nane value replace it with 0\n\n","9cf3060b":"\nX_train['Embarked'] = X_train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\nX_test['Embarked'] = X_test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n","3e008c53":"X_train= X_train.drop(['person', 'class_3','male_adult','male'], axis=1)\nX_test= X_test.drop(['person', 'class_3','male_adult','male'], axis=1)","fb5e783b":"X_test\n","7b15fb04":"X_train.info()","fee2cdfa":"'''\nt = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1)\ndef createTrainValidationSplit(t_size,x_df , t_vec ):\n    X_train, X_val, t_train, t_val = model_selection.train_test_split(x_df, t_vec, test_size=t_size, random_state=2)\n    return X_train, X_val, t_train, t_val\n\ntrain_df.columns\nloss_CE_arr = []\nval_size_array = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in (val_size_array):\n    X_train, X_val, t_train, t_val = createTrainValidationSplit(i,X,t)\n    SGD_cls = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.001)).fit(X_train, t_train)\n    y_train_prob = SGD_cls.predict_proba(X_train)\n    y_val_prob = SGD_cls.predict_proba(X_val)\n    y_train = SGD_cls.predict(X_train)\n    y_val = SGD_cls.predict(X_val)\n    \n    loss_CE_arr.append( metrics.log_loss(t_train, y_train_prob))\n   \n    #print the accuracy score and CE loss of the train and validation\n    print('Accuracy score on train', SGD_cls.score(X_train, t_train))\n    print('Accuracy score on validation', SGD_cls.score(X_val, t_val))\n    print()\n    print('CE on train', metrics.log_loss(t_train, y_train_prob))\n    print('CE on validation', metrics.log_loss(t_val, y_val_prob))\n    print('-'* 40)\n'''","cd04444e":"\n#plt.plot(val_size_array, loss_CE_arr, '+-r')\n\n#plt.show()","4293ae7a":"'''\n# # split the data to 70% train and 30% validation from the train test\nX = train_df.drop(\"Survived\", axis=1)\nt = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1)\nX_train, X_val, t_train, t_val = model_selection.train_test_split(X, t, test_size=0.3, random_state=2)\nX_train, X_val, t_train, t_val\ntrain_df.columns\n# # split the data to 80% train and 20% validation from the train test\nX = train_df_cp.drop(\"Survived\", axis=1)\nt = train_df_cp[\"Survived\"]\nX_test_cp  = test_df_cp.drop(\"PassengerId\", axis=1)\nX_train_cp, X_val_cp, t_train_cp, t_val_cp = model_selection.train_test_split(X, t, test_size=0.2, random_state=2)\n\ntrain_df.columns\n# import neural_network and run MLP on the data\nfrom sklearn import neural_network\nMLP_cls = pipeline.make_pipeline(preprocessing.StandardScaler(), neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=40000)).fit(X_train, t_train)\n\ny_train_prob = MLP_cls.predict_proba(X_train)\ny_val_prob = MLP_cls.predict_proba(X_val)\nY_predict_MLP = MLP_cls.predict(X_test)\nprint('Accuracy score on train', MLP_cls.score(X_train, t_train))\nprint('Accuracy score on validaion', MLP_cls.score(X_val, t_val))\nprint()\nprint('CE on train', metrics.log_loss(t_train, y_train_prob))\nprint('CE on validaion', metrics.log_loss(t_val, y_val_prob))\nSGD_cls = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.001)).fit(X_train, t_train)\ny_train_prob = SGD_cls.predict_proba(X_train)\ny_val_prob = SGD_cls.predict_proba(X_val)\ny_train = SGD_cls.predict(X_train)\ny_val = SGD_cls.predict(X_val)\n\n\n\n#print the accuracy score and CE loss of the train and validation\nprint('Accuracy score on train', SGD_cls.score(X_train, t_train))\nprint('Accuracy score on validation', SGD_cls.score(X_val, t_val))\nprint()\nprint('CE on train', metrics.log_loss(t_train, y_train_prob))\nprint('CE on validation', metrics.log_loss(t_val, y_val_prob))\nMLP_cls1 = pipeline.make_pipeline(preprocessing.StandardScaler(), neural_network.MLPClassifier(activation='logistic',  solver='adam', alpha=0, max_iter=100000)).fit(X_train_cp, t_train_cp)\n\ny_train_prob_cp = MLP_cls1.predict_proba(X_train_cp)\ny_val_prob_cp = MLP_cls1.predict_proba(X_val_cp)\nprint('Accuracy score on train', MLP_cls1.score(X_train_cp, t_train_cp))\nprint('Accuracy score on validaion', MLP_cls1.score(X_val_cp, t_val_cp))\nprint()\nprint('CE on train', metrics.log_loss(t_train_cp, y_train_prob_cp))\nprint('CE on validaion', metrics.log_loss(t_val_cp, y_val_prob_cp))\n\nSGD_cls1 = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.01)).fit(X_train_cp, t_train_cp)\ny_train_prob = SGD_cls1.predict_proba(X_train_cp)\ny_val_prob = SGD_cls1.predict_proba(X_val_cp)\ny_train = SGD_cls1.predict(X_train_cp)\ny_val = SGD_cls1.predict(X_val_cp)\n\n\n\n#print the accuracy score and CE loss of the train and validation\nprint('Accuracy score on train', SGD_cls1.score(X_train_cp, t_train_cp))\nprint('Accuracy score on validation', SGD_cls1.score(X_val_cp, t_val_cp))\nprint()\nprint('CE on train', metrics.log_loss(t_train_cp, y_train_prob))\nprint('CE on validation', metrics.log_loss(t_val, y_val_prob))\nY_predict1_NN = MLP_cls1.predict(X_test_cp)\nY_predict_NN = MLP_cls.predict(X_test)\nY_predict1 =SGD_cls1.predict(X_test_cp)\nY_predict = SGD_cls.predict(X_test)\n'''","23cbb0c0":"# find best subset of features on this dataset\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nnumerical_cols = X_train.select_dtypes(include=['int64', 'float64','uint8']).columns\ncategorical_cols = X_train.select_dtypes(include=['object', 'bool']).columns\nall_cols = categorical_cols.tolist() + numerical_cols.tolist()\nct_enc_std = ColumnTransformer([\n            (\"encoding\", OrdinalEncoder(), categorical_cols),\n            (\"standard\", StandardScaler(), numerical_cols)])\nX_encoded = pd.DataFrame(ct_enc_std.fit_transform(X_train, Y_train), columns=all_cols)\n\nselector = RFECV(GradientBoostingClassifier(random_state=1), cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=1)).fit(X_encoded, Y_train)\ndisplay(X_encoded.loc[:, selector.support_])\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[i for i in range(1, len(selector.grid_scores_) + 1)], y=selector.grid_scores_))\nfig.update_xaxes(title_text=\"Number of features selected\")\nfig.update_yaxes(title_text=\"Cross validation score (nb of correct classifications)\")\nfig.show()","6a8c5b00":"_features =['Sex','Age', 'Title', 'Fare', 'class_1','FamilySize' ,'Pclass','Embarked']\nX_train = X_train[_features]\nX_test =X_test[_features]","13ef389a":"X = X_train\nt = Y_train","8320fead":"\n# Cross validate model with Kfold stratified cross val\nK_fold = StratifiedKFold(n_splits=10)","4674b245":"# Modeling step Test differents algorithms \nrandom_state = 2\n\nmodels = [] # append all models or predictive models \ncv_results = [] # cross validation result\ncv_means = [] # cross validation mean value\ncv_std = [] # cross validation standard deviation\n\nmodels.append(KNeighborsClassifier())\nmodels.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nmodels.append(DecisionTreeClassifier(random_state=random_state))\nmodels.append(RandomForestClassifier(random_state=random_state))\nmodels.append(ExtraTreesClassifier(random_state=random_state))\nmodels.append(SVC(random_state=random_state))\nmodels.append(GradientBoostingClassifier(random_state=random_state))\nmodels.append(LogisticRegression(random_state = random_state))\nmodels.append(LinearDiscriminantAnalysis())\nmodels.append(MLPClassifier(random_state=random_state))\n\n\nfor model in models :\n    cv_results.append(cross_val_score(model, X_encoded, Y_train, \n                                      scoring = \"accuracy\", cv = K_fold, n_jobs=4))\n\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_frame = pd.DataFrame(\n    {\n        \"CrossValMeans\":cv_means,\n        \"CrossValErrors\": cv_std,\n        \"Algorithms\":[\n                     \"KNeighboors\",\n                     \"AdaBoost\", \n                     \"DecisionTree\",   \n                     \"RandomForest\",\n                     \"ExtraTrees\",\n                     \"SVC\",\n                     \"GradientBoosting\",                      \n                     \"LogisticRegression\",\n                     \"LinearDiscriminantAnalysis\",\n                     \"MultipleLayerPerceptron\"]\n    })\n\ncv_plot = sns.barplot(\"CrossValMeans\",\"Algorithms\", data = cv_frame,\n                palette=\"husl\", orient = \"h\", **{'xerr':cv_std})\n\ncv_plot.set_xlabel(\"Mean Accuracy\")\ncv_plot = cv_plot.set_title(\"CV Scores\")","fb13513d":"\nfrom sklearn.metrics import confusion_matrix\ndef confusion_matrix_function(classifier,X_train, X_val, t_train, t_val):\n    # calculate cm for train and val\n    cls = classifier.fit(X_train, t_train)\n    y_train = cls.predict(X_train)\n    y_val = cls.predict(X_val)\n    cm_train = confusion_matrix(t_train, y_train)\n    cm_val = confusion_matrix(t_val, y_val)\n\n    print('cm_train')\n    print(cm_train)\n    print()\n    print('cm_val')\n    print(cm_val)\n    # show tn, fp, fn and tp for train and val\n    tn_train, fp_train, fn_train, tp_train = cm_train.ravel()\n    tn_val, fp_val, fn_val, tp_val = cm_val.ravel()\n\n    print(f'Train: TN {tn_train:4}, FP {fp_train:4}, FN {fn_train:4}, TP {tp_train:4}')\n    print(f'Val:   TN {tn_val:4}, FP {fp_val:4}, FN {fn_val:4}, TP {tp_val:4}')\n    \n    # lot confusion matrix of train and val as heatmaps in seaborn\n    cm_train_df = pd.DataFrame(cm_train, index=['actual_e', 'actual_p'], columns=['predicted_e', 'predicted_p'])\n    cm_val_df = pd.DataFrame(cm_val, index=['actual_e', 'actual_p'], columns=['predicted_e', 'predicted_p'])\n\n    sns.set(font_scale=2)\n    plt.figure(figsize = (15,12))\n    fig = sns.heatmap(cm_train_df, annot=True, cmap=plt.cm.Pastel1, fmt='g')\n    fig.set_title(\"cm_train\")\n    plt.show()\n    print()\n    plt.figure(figsize = (15,12))\n    fig = sns.heatmap(cm_val_df, annot=True, cmap=plt.cm.Pastel2, fmt='g')\n    fig.set_title(\"cm_val\")\n    plt.show()","7608d060":"# split the data to train and validation\nX_train, X_val, Y_train, t_val = train_test_split(X, t, test_size=0.3, random_state=1)","9e9c3e42":"# GBC Classifier\nGBC_Model = GradientBoostingClassifier()\n\nscores = cross_val_score(GBC_Model, X_train, Y_train, cv = K_fold,\n                       n_jobs = 4, scoring = 'accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\nconfusion_matrix_function(GBC_Model,X_train, X_val, Y_train, t_val)","84cee00e":"# Linear Discriminant Analysis \nLDA_Model= LinearDiscriminantAnalysis()\n\nscores = cross_val_score(LDA_Model, X_train, Y_train, cv = K_fold,\n                       n_jobs = 4, scoring = 'accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\nconfusion_matrix_function(LDA_Model,X_train, X_val, Y_train, t_val)","349cfd8f":"# Logistic Regression\n#\nLog_Model = LogisticRegression(C=1)\nscores = cross_val_score(Log_Model, X_train, Y_train, cv=K_fold, \n                        n_jobs=4, scoring='accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\n\nconfusion_matrix_function(Log_Model,X_train, X_val, Y_train, t_val)","a5a239d6":"# Random Forest Classifier Model\n#\nRFC_model = RandomForestClassifier(n_estimators=10)\nscores = cross_val_score(RFC_model, X_train, Y_train, cv=K_fold, \n                        n_jobs=4, scoring='accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\n\nconfusion_matrix_function(RFC_model,X_train, X_val, Y_train, t_val)","81eecf44":"# Gaussian Naive Bayes\nGNB_Model = GaussianNB()\n\nscores = cross_val_score(GNB_Model, X_train, Y_train, cv=K_fold, \n                        n_jobs=4, scoring='accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\n\nconfusion_matrix_function(GNB_Model,X_train, X_val, Y_train, t_val)","c8c83be6":"# Support Vector Machine\nSVM_Model = SVC()\n\nscores = cross_val_score(SVM_Model, X_train, Y_train, cv=K_fold, \n                        n_jobs=4, scoring='accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\nconfusion_matrix_function(SVM_Model,X_train, X_val, Y_train, t_val)","195381a0":"# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {\n            'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01, 0.001],\n              'max_depth': [4, 8,16],\n              'min_samples_leaf': [100,150,250],\n              'max_features': [0.3, 0.1]\n              }\n\ngsGBC = GridSearchCV(GBC, param_grid = gb_param_grid, cv=K_fold, \n                     scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\nGBC_best = gsGBC.best_estimator_\nconfusion_matrix_function(GBC_best,X_train, X_val, Y_train, t_val)\n# Best score\ngsGBC.best_score_","2bd114b8":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"min_samples_split\": [2, 6, 20],\n              \"min_samples_leaf\": [1, 4, 16],\n              \"n_estimators\" :[100,200,300,400],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC, param_grid = rf_param_grid, cv=K_fold,\n                     scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\nRFC_best = gsRFC.best_estimator_\nconfusion_matrix_function(RFC_best,X_train, X_val, Y_train, t_val)\n# Best score\ngsRFC.best_score_","3f61510f":"# LogisticRegression Parameters tunning \nLRM = LogisticRegression()\n\n## Search grid for optimal parameters\nlr_param_grid = {\"penalty\" : [\"l2\"],\n              \"tol\" : [0.0001,0.0002,0.0003],\n              \"max_iter\": [100,200,300],\n              \"C\" :[0.01, 0.1, 1, 10, 100],\n              \"intercept_scaling\": [1, 2, 3, 4],\n              \"solver\":['liblinear'],\n              \"verbose\":[1]}\n\n\ngsLRM = GridSearchCV(LRM, param_grid = lr_param_grid, cv=K_fold,\n                     scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsLRM.fit(X_train,Y_train)\nLRM_best = gsLRM.best_estimator_\n\nconfusion_matrix_function(LRM_best,X_train, X_val, Y_train, t_val)\n\n# Best score\ngsLRM.best_score_","7be7b38d":"# Linear Discriminant Analysis - Parameter Tuning\nLDA = LinearDiscriminantAnalysis()\n\n## Search grid for optimal parameters\nlda_param_grid = {\"solver\" : [\"svd\"],\n              \"tol\" : [0.0001,0.0002,0.0003]}\n\n\ngsLDA = GridSearchCV(LDA, param_grid = lda_param_grid, cv=K_fold,\n                     scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsLDA.fit(X_train,Y_train)\nLDA_best = gsLDA.best_estimator_\n\n\nconfusion_matrix_function(LDA_best,X_train, X_val, Y_train, t_val)\n# Best score\ngsLDA.best_score_\n","95d9ff50":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100, 200, 300]}\n\ngsSVMC = GridSearchCV(SVMC, param_grid = svc_param_grid, cv = K_fold,\n                      scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\nconfusion_matrix_function(SVMC_best,X_train, X_val, Y_train, t_val)\n# Best score\ngsSVMC.best_score_","0605aa73":"# Plot learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n  \n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","f2ec1c42":"# Gradient boosting - Learning Curve \nplot_learning_curve(estimator = gsGBC.best_estimator_,title = \"GBC learning curve\",\n                    X = X_train, y = Y_train, cv = K_fold);","6683c454":"# Random Forest - Learning Curve\nplot_learning_curve(estimator = gsRFC.best_estimator_ ,title = \"RF learninc curve\",\n                    X = X_train, y = Y_train, cv = K_fold);","e27d699d":"# Logistic Regression - Learning Curve    gsLRM.best_estimator_\nplot_learning_curve(estimator = Log_Model ,title = \"Logistic Regression - Learning Curve\",\n                    X = X_train, y = Y_train, cv = K_fold);","d0d48fba":"# Linear Discriminant Analysis - Learning Curve\nplot_learning_curve(estimator = gsLDA.best_estimator_ ,title = \"Linear Discriminant - Learning Curve\",\n                    X = X_train, y = Y_train, cv = K_fold);","47049d9b":"# Support Vector Machine - Learning Curve\nplot_learning_curve(estimator = gsSVMC.best_estimator_,title = \"SVC learning curve\",\n                    X = X_train, y = Y_train, cv = K_fold);","b77f0a6a":"VotingPredictor = VotingClassifier(estimators =\n                           [('rfc', RFC_best), \n                            ('gbc', GBC_best)],\n                           voting='soft', n_jobs = 4)\n\n\nVotingPredictor = VotingPredictor.fit(X_train, Y_train)\n\nscores = cross_val_score(VotingPredictor, X_train, Y_train, cv = K_fold,\n                       n_jobs = 4, scoring = 'accuracy')\n\nprint(scores)\nprint(round(np.mean(scores)*100, 2))\n\nconfusion_matrix_function(VotingPredictor,X_train, X_val, Y_train, t_val)","650d6535":"from sklearn.model_selection import KFold\n# calculate score and loss from KFold and display graphs\ndef get_cv_score_and_loss(X, t, model, show_score_loss_graphs=False):\n    scores_losses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n\n    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n        X_train = X.loc[train_ids]\n        t_train = t.loc[train_ids]\n        X_val = X.loc[val_ids]\n        t_val = t.loc[val_ids]\n\n        model.fit(X_train, t_train)\n\n        y_train = model.predict(X_train)\n        y_val = model.predict(X_val)\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'train', model.score(X_train, t_train),log_loss(t_train, y_train)]\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'val', model.score(X_val, t_val),log_loss(t_val, y_val)]\n\n\n    val_scores_losses_df = scores_losses_df[scores_losses_df['split']=='val']\n    train_scores_losses_df = scores_losses_df[scores_losses_df['split']=='train']\n\n    mean_val_score = val_scores_losses_df['score'].mean()\n    mean_val_loss = val_scores_losses_df['loss'].mean()\n    mean_train_score = train_scores_losses_df['score'].mean()\n    mean_train_loss = train_scores_losses_df['loss'].mean()\n\n    if show_score_loss_graphs:\n        fig = px.line(scores_losses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n        fig.show()\n        fig = px.line(scores_losses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss: {mean_val_loss:.2f}, Mean Train Loss: {mean_train_loss:.2f}')\n        fig.show()\n\n    return mean_val_score, mean_val_loss, mean_train_score, mean_train_loss","cbc995e3":"\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X, t, VotingPredictor, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')","64526d14":"Predictive_Model = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": VotingPredictor.predict(X_test)})\n\nPredictive_Model.to_csv('VotingPredictor_model.csv', index=False)\n\nLDA_best_Model = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": LDA_best.predict(X_test)})\n\nPredictive_Model.to_csv('LDA_best_model.csv', index=False)\n\n\nRFC_best_Model = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": RFC_best.predict(X_test)})\n\nPredictive_Model.to_csv('RFC_best_model.csv', index=False)\n\nGBC_best_Model = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": GBC_best.predict(X_test)})\n\nPredictive_Model.to_csv('GBC_best_model.csv', index=False)","78037690":"## Workflow Stage\n\n1. general discreption\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Supply or submit the results.\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** As the feature values change does the solution state change as well\n\n**Converting.** For modeling stage, one needs to prepare the data. \n\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. \n\n**Creating.** Can we create new features based on an existing feature or a set of features\n\n\n","a43f72d8":"Let's create a method that gets: data and model and returns R2 score and MSE loss\n\n","2e334052":"## Let's explore following models separately:\n\n### GBC Classifier\n### Linear Discriminant Analysis\n### Logistic Regression\n### Random Forest Classifer\n### Gaussian Naive Bayes\n### Let's start with Gradient Boosting Classifier.","e63ec8f6":"* Conclusion :\n*    The cabin feature got alot of null values - So we wont consider it in the model\n*    Age feature also got alot of null values - we have to check it later\n*    All the other features have 8 null values in the train set and some except the Embarked feature\n","53ee490f":"# let's start with the feature engeeniring","1f59cff0":"* as we can see the passengers that been on the ship with a small number of famliy member\n* got the highest Probability Survived the most ( 2 - 4 Pepole)\n* let's dig a little bit deeper\n### from FamilySize feature we create a new one that we refer if the passenger abord alone to the titanic","cf6c67f7":"my last submissions at the current work\n![image.png](attachment:image.png)","04dae98e":"# Titanic - Machine Learning from Disaster\n\n* Name : Lior Toledano\n* Account URL: https:\/\/www.kaggle.com\/liortoledano\/account\n","264f29c7":"**Keys Definitions**\n\n* survival - Survival 0 = No, 1 = Yes \n* pclass - Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n* sex - Sex\n* Age - Age in years\n* sibsp - # of siblings \/ spouses aboard the Titanic\n* parch - # of parents \/ children aboard the Titanic\n* ticket - Ticket number\n* fare - Passenger fare\n* cabin - Cabin number\n* embarked - Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton","06a57c15":"### LDA","591d8e35":"### Random Forest and GradientBoosting classifiers tend to overfit the training set. One way to improve the overfitting model is to feed it more training data until the validation error reaches the training error.","c21adb41":"# my old modeling part","d29d75cd":"# Visual that correlate between the features & the target.\n* Thats help as see the correlation more brightly","3a2cf2e5":"## we want to drop the non numerical categoriy 'Ticket' and the alot of missing values 'Cabin' features","f38ac441":"* Observations:\n* we can see that most of the females on the board survived ( 74 % ) and most of the males didn't survived( 81 %)\n* Conclusion :\n* Sex feature goes to the model because its got a high correlation to the survived rate","a1ec40f0":"## Sex\n#### Accounts of the evacuation effort indicate the ship's officers applied the 'Women and children first' doctrine when organizing people in lifeboats. Simply asserting all women survived and all men died should improve our model. Of course we know some women died and some men survived. You can get details in this gender as a feature analysis. Meanwhile, we're starting with broad generalizations and will refine our features along the way.","daf4a672":"* Now lets work on the SibSp and Parch features\n* Because both of them talk about the the Family Size","24862d28":"* First of all let's give a general discreption on the Titanic Disasster\n- On April 15, 1912, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.","65c2f97e":"### Hyper-Parameter Tuning on LDA","35ab78e3":"# Summary:\nThe work was very instructive and we received a lot of tools during the course that helped me understand concepts from the world of machine learning. We have learned many techniques and ways to pinpoint our model percentages\n\nIt was a very good practice regarding our new subjects even though I was not able to increase the accuracy percentages of my model by much. I really enjoyed the process and experimenting with different functions and libraries\n\nIn the end I think the indemnity was minor because most of my data is not different then it was in my first ex, after further reading on the internet I realized that there were specific women who did not survive the Titanic at the test set and apparently my model predicts they survived which explains the relatively high FN percentage in my final model.\nIf i had more time to deal with it I would go back to the feature engineering phase again and try to create and play with the data more in depth to get a better result\n\n## References List : \n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/www.codementor.io\/@innat_2k14\/titanic-kaggle-machine-learning-from-disaster-part-1-10gbvqzguc\n* https:\/\/www.kaggle.com\/vinceallenvince\/titanic-guided-by-a-confusion-matrix\n* Fourth\/Fifth\/SixthPracticeML\n","f4239551":"## Hyper-Parameter Tuning on SVC\n\n","fdf7ba97":"This  subset only make a modle predict worth so i just use the holl set","ab3d185c":"# Analyze by describing data - Show a numerical categories statistic ","0ea7edef":"## Person\n#### We can account for the misclassified children if we engineer a 'Person' feature with the labels 'male_adult', 'female_adult' and 'child'. As a refinement to our model, we'll assert all adult females and all children survived while all adult males died.","4de54b27":"## Class\n##### Looking at survival rates, first and second class had much higher survival rates for women and children. With a class feature, our model will assert all female adults and children in classes 1 and 2 survived and everyone else died.","7b35c031":"### Support Vector Machine or SVM is pretty much promsing ML algorithm. It should perform well also.","daa100d8":"### According to our training set\n#### It can be seen that most of the passengers did not survive the cruise.\n#### Those who survived most were women and a few more men in relation to their quantity on the ship\n#### Those who did not survive, the vast majority are men and only less than a third of the women who were on the ship did not survive","01e44864":"### Get more information about the sex from the name feature and add a new feature that we called Title to our dataset\n","15eaa605":"# Hyperparameter Tuning\n#### I decided to choose this promising models of GradientBoosting, Linear Discriminant Analysis, RandomForest, Logistic Regression and SVM for the ensemble modeling. So, now we need to fine-tune them.\n\n#### One way to do that would be to fiddle with the hyperparameters manually until we find a great combination of hyperparamerter values. This would be very tedious work, and we may not have time to explore many combination. Instead we should get Scikit-Learn's GridSearchCV to search for us. All we need to do is tell it which hyperparameters we want it to experiment with, and what values to try out and it will evaluate all the possible combination of hyperparameter values, using cross-validation.","f08b7609":"* we can see that most of the passengers and the unsurvived are between age 18 to 40 years old\n* And that the age feature is super relevant to our prediction","81dbade9":"## But first talk about Confusion Matrix\n### We want to be able to explain the results of a classifier.\n#### When talking about binary classification, the classification type of a sample can be one out of four:\n\n* 1.TP (True Positive) - The model classified correctly that a sample is positive.\n* 2.TN (True Negative) - The model classified correctly that a sample is negative.\n* 3.FP (False Positive) - The model classified a sample as positive but the sample is actually negative.\n* 4.FN (False Negative) - The model classified a sample as negative but the sample is actually positive.\n* We can show these values in a matrix:\n![image.png](attachment:image.png)","e1460a28":"* When we use it on code, we can use Scikit-learn confusion_matrix.\n* Let's create a function that will calculate the confusion matrix for each of our classifiers that we will trained on the Train Data.","85dd61b1":"## The best subset of features is Pclass ,Sex,Age,Fare,Embarked,Title,FamilySize,FamilySize,class_1. ","c5265dc8":"# conclusions:\n*          The ticket feature will not goes to the model because its irrelevant to our prediction\n","63dd31ab":"## Hyper-Parameter Tuning on GBC","26684ba6":"## Hyper-Parameter Tuning on RFC\n\n","b5a6e232":"# Classifier\n### I compare 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n1. KNN\n2. AdaBoost\n3. Decision Tree\n4. Random Forest\n5. Extra Trees\n6. Support Vector Machine\n7. Gradient Boosting\n8. Logistic regression\n9. Linear Discriminant Analysis\n10. Multiple layer perceprton\n\n# Evaluation using Cross Validation\n###  great alternative is to use Scikit-Learn's cross-validation feature. The following performs K-fold cross validation; it randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Models 10 times, picking a different fold for evaluation every time and training on the other 9 folds.","b8f734b2":"### Gaussian NB performs pretty well on binary classification.\n\n","cbb8b28e":"# Using groupby function to find the mean survival by the other features","b48a42c3":"my last submissions at the first work\n![image.png](attachment:image.png)","01e065b1":"showing the graph of our loss CE on the validation and the Validation size\u00b6\n","dc3e5db9":"### Logistic Regression classifier.\n\n","3de6d4e3":"# Predictive Modeling\n### Here, we split our datasets according to the previous amounts and make test and train set. To avoid overfitting event we can create validation set but that's not effective. So, we use K-Fold approaches and use StratifiedKFold to split the train datasets into 10 (by default).","7706ec8e":"### As we can see the embarked feature got a conection with the survived feature \n##### we have to handle the missing values. we are going to do it in the feaures engeeniring part\n### We can infer some unexpected conclusions from the graphs\n#### Although in general most women survived and men did not\n#### It can be seen that passengers who Embarked from 'Cherbourg' happened the opposite\n","7f6559ec":"\n\n\n![image.png](attachment:image.png)","57c38ad4":"## Now let's handle the None values in age and Fare features\n* we saw earlier that we have got more then 200 none values in the 'Age' feature\n* the way that is chose to handle it its by puting the mean age in every none values exsist in the data","63d05abc":"### Random Forest is typically an ensemble of decesion tree classifer. ","88eefb20":"* As we saw on the plot above the CE minimized when 30% of the data goes to the validation set\n#### Classification - At my first work\n* We will use 2 ways to classify the data based on logistic regression:\n\n* The Optimizer - SGDClassifier (When using log loss)\n\n* The Estimator - LogisticRegression","1ff167d9":"## Feature Selection\nWe want to choose the best features for our use case.\nWe will use Scikit-learn RFECV to use CV and choose the best number of features on this dataset.\nWe will enter the Scikit-learn RepeatedKFold to repeat each KFold a few times with different splits.\nwe will do the feature selection on the GradientBoostingClassifier because he get me the best score","2009157b":"## Plot Learning Curves\n### Diagnose Bias and Variance to Reduce Error\n### Learning curves are a good way to see the overfitting and underfitting effect on the training set and the effect of the training size on the accuracy. Learning curves plots the model's performance on the training set and the validation set as a function of training set size.\n### To generate the plots, we simply train the model several times on different sized subsets of the training sets\n\n### Underfitting: If model is underfitting the training data, adding more training example will not help. We need to use more complex model or come up with better features.\n\n \n### Overfitting: One way to improve the overfitting model is to feed it more training data until the validation error reaches the training error.\n\n","5d0e4f7a":"## Hyper-Parameter Tuning on LR\n\n","06ccc960":"## in general most women paied higher fare then the men\n### And it's connect to the graph above that women that Embarked from 'Cherbourg' survived les then the men from that place","2256feef":"* Observations:\n1. *    most of the passengers in class 3 didn't survived ( the lowest level of class ticket)\n2. *    most of the passengers in class 1 survived\n3. *    most of the childerns in classes 2 and 1 under 18 years old had a high chance to survived\n   *    most of the unsurvived passengers was in class 3 and in age between 18 - 40\n* Conclusions:\n    * The pclass will goes to our model\n    \n    # Lets learn more about how the titanic build\n    * as we can see in the pic below the first class are in the higher position on the ship thats probably explain the high numbers of survivals\n    * in a similar way the third class passengers got the lowest chance to survived because they was on the lowest part in the ship\n    ![image.png](attachment:image.png)"}}