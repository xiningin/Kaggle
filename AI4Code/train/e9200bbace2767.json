{"cell_type":{"0dee146f":"code","918146c3":"code","9daf9bb1":"code","90c87ecd":"code","fc5d2239":"markdown","101ec2fe":"markdown","4a223846":"markdown","a67a9125":"markdown","a6b57a85":"markdown"},"source":{"0dee146f":"import sys\nsys.path.append('..\/input\/rapids-kaggle-utils')\n\nimport cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\nimport cu_utils.transform as cutran\n\nimport gc\nfrom joblib import Parallel, delayed","918146c3":"PATH = \"..\/input\/optiver-realized-volatility-prediction\"\norder_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\nlen(order_book_training)","9daf9bb1":"def fix_offsets(data_df):\n    \n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    \n    return data_df\n\ndef ffill(data_df):\n    # MultiIndex.from_product uses pandas in the background\n    # That's why we need to transform the data into pd dataframe\n    # Then the MultiIndex.from_product will return cudf dataframe \n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket']).to_pandas()\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\n\ndef rel_vol_fe(df, null_val=-9999):\n    \n    # compute wap\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[\"WAP\"] = (p1*s2 + p2*s1) \/ (s1 + s2)\n\n\n        df[\"log_wap\"] = df[\"WAP\"].log()\n        df[\"log_wap_shifted\"] = (df[[\"time_id\", \"log_wap\"]].groupby(\"time_id\", method='cudf')\n                                 .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                                incols={\"log_wap\": 'x'},\n                                                outcols=dict(y_out=cp.float32),\n                                                tpb=32)[\"y_out\"])\n        df = df[df[\"log_wap_shifted\"] != null_val]\n\n        df[\"diff_log_wap\"] = df[\"log_wap\"] - df[\"log_wap_shifted\"]\n        df[f\"diff_log_wap{n}\"] = df[\"diff_log_wap\"]**2\n    \n\n    \n    # Summary statistics for different 'diff_log_wap'\n    sum_df = df.groupby(\"time_id\").agg({\"diff_log_wap1\": {\"sum\", \"mean\", \"std\", \"median\", \"max\", \"min\"}, \n                                        \"diff_log_wap2\": {\"sum\", \"mean\", \"std\", \"median\", \"max\", \"min\"}}\n                                      ).reset_index()\n    \n    # Create wanted features for training\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    sum_df.columns = [f(x) for x in sum_df.columns]\n    sum_df[\"volatility1\"] = (sum_df[\"diff_log_wap1_sum\"])**0.5\n    sum_df[\"volatility2\"] = (sum_df[\"diff_log_wap2_sum\"])**0.5\n    sum_df[\"vol1_mean\"] = sum_df[\"diff_log_wap1_mean\"].fillna(0).values\n    sum_df[\"vol2_mean\"] = sum_df[\"diff_log_wap2_mean\"].fillna(0).values\n    sum_df[\"vol1_std\"] = sum_df[\"diff_log_wap1_std\"].fillna(0).values\n    sum_df[\"vol2_std\"] = sum_df[\"diff_log_wap2_std\"].fillna(0).values\n    sum_df[\"vol1_median\"] = sum_df[\"diff_log_wap1_median\"].fillna(0).values\n    sum_df[\"vol2_median\"] = sum_df[\"diff_log_wap2_median\"].fillna(0).values\n    sum_df[\"vol1_max\"] = sum_df[\"diff_log_wap1_max\"].fillna(0).values\n    sum_df[\"vol2_max\"] = sum_df[\"diff_log_wap2_max\"].fillna(0).values\n    sum_df[\"vol1_min\"] = sum_df[\"diff_log_wap1_min\"].fillna(0).values\n    sum_df[\"vol2_min\"] = sum_df[\"diff_log_wap2_min\"].fillna(0).values\n    sum_df[\"volatility_rate\"] = (sum_df[\"volatility1\"] \/ sum_df[\"volatility2\"]).fillna(0)\n    sum_df[\"mean_volatility_rate\"] = (sum_df[\"vol1_mean\"] \/ sum_df[\"vol2_mean\"]).fillna(0)\n    sum_df[\"std_volatility_rate\"] = (sum_df[\"vol1_std\"] \/ sum_df[\"vol2_std\"]).fillna(0)\n    sum_df[\"median_volatility_rate\"] = (sum_df[\"vol1_median\"] \/ sum_df[\"vol2_median\"]).fillna(0)\n    sum_df[\"max_volatility_rate\"] = (sum_df[\"vol1_max\"] \/ sum_df[\"vol2_max\"]).fillna(0)\n    sum_df[\"min_volatility_rate\"] = (sum_df[\"vol1_min\"] \/ sum_df[\"vol2_min\"]).fillna(0)\n    \n    return sum_df[[\"time_id\", \"volatility1\", \"volatility2\", \n                   \"volatility_rate\", \"vol1_std\", \"vol2_std\",\n                   \"vol1_mean\", \"vol2_mean\", \"vol1_median\", \"vol2_median\",\n                   \"vol1_max\", \"vol2_max\", \"vol1_min\", \"vol2_min\",\n                   \"mean_volatility_rate\", \"std_volatility_rate\",\n                   \"median_volatility_rate\", \"max_volatility_rate\",\n                   \"min_volatility_rate\"]]\n\ndef spread_fe(df):\n    \n    # Bid ask spread\n    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1)                               \n\n    # different spreads\n    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n    df['v_spread_b'] = df['bid_price1'] - df['bid_price2']\n    df['v_spread_a'] = df['ask_price1'] - df['ask_price2']\n    \n    # Summary statistics for different spread\n    sum_df = df.groupby(\"time_id\").agg({\"h_spread_l1\": { \"mean\", \"std\", \"median\", \"max\", \"min\"}, \n                                        \"h_spread_l2\": { \"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"v_spread_b\": {\"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"v_spread_a\": {\"mean\", \"std\", \"median\", \"max\", \"min\"},\n                                        \"bas\": {\"mean\"}}\n                                      ).reset_index()\n    \n    \n    # Create wanted features for training\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    sum_df.columns = [f(x) for x in sum_df.columns]\n\n    return sum_df\n    \n    \n\ndef get_stat(path):\n    \n    book = cudf.read_parquet(path)\n    stock_id = int(path.split(\"=\")[1].split(\"\/\")[0])\n    book = fix_offsets(book)\n    #book = cudf.DataFrame(ffill(book))\n    rel_vol_data = rel_vol_fe(book)\n    spread_data = spread_fe(book)\n    transbook = cudf.merge(rel_vol_data,\n                           spread_data,\n                           on = ['time_id'], how = 'left')\n    transbook['stock_id'] = stock_id\n    \n    return transbook\n\n\ndef process_data(order_book_paths):\n    \n    df = Parallel(n_jobs=-1, verbose=1)(delayed(get_stat)(path)\n                             for path in tqdm(order_book_paths))\n    \n    stock_dfs = cudf.concat(df, ignore_index=True)\n    return stock_dfs","90c87ecd":"%%time\ntrain_past_volatility = process_data(order_book_training)\nprint(train_past_volatility.shape)\ntrain_past_volatility.to_csv(\".\/train_past_volatility.csv\")\ntrain_past_volatility.columns","fc5d2239":"# Utils","101ec2fe":"# Acknowledgment\n* [we-need-to-go-deeper-and-validate](https:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper-and-validate)\n* [accelerating-trading-on-gpu-via-rapids](https:\/\/www.kaggle.com\/aerdem4\/accelerating-trading-on-gpu-via-rapids)\n* [Forward filling book data](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251277)\n* [Deep Learning approach with a CNN- inference](https:\/\/www.kaggle.com\/slawekbiel\/deep-learning-approach-with-a-cnn-inference)","4a223846":"# Library","a67a9125":"# Update \n* Rebase seconds_in_bucket because of the filler data \n* Forward fill of the `book_data` for train and test data","a6b57a85":"# Feature generation"}}