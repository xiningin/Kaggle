{"cell_type":{"551124d3":"code","e3227cc7":"code","4c941fbc":"code","80b2c6c4":"code","3ca2df3f":"code","ad3cf675":"code","f453e1f7":"code","891881bf":"code","1dbf4b32":"code","565e0005":"code","84dba25b":"code","b93039a8":"code","ecfa7ccf":"code","7525e6f0":"code","5ac55dec":"code","6ecc833b":"code","fd8e6de6":"code","5ae56da7":"code","66b1fce8":"code","54677d2a":"code","88a3aa9c":"code","5f821533":"code","5cb2fc39":"code","5a84da3d":"code","6be6f154":"code","78309c79":"code","0cba082c":"code","9182751a":"code","89a3bc7e":"code","862ab159":"code","e3777b73":"code","d08bfd4a":"code","ca75dbce":"code","6e8ccd8b":"code","67c5ec70":"code","66aa1649":"code","4dbebdfe":"code","b2df301a":"code","5f684568":"code","4b9ddedd":"code","f027a26e":"code","a299c7bb":"code","5d28086e":"code","d9f905ef":"code","a4a194b9":"code","b6906711":"code","9206c674":"code","19fef92c":"code","d4b84688":"code","18b3fa75":"code","c0791891":"code","f8553746":"code","cba4b640":"code","1bbe88cb":"code","699f5fb9":"code","8cf11fa1":"code","e9055ed9":"code","646b8821":"code","fbb6a939":"code","6948e687":"code","640ce7f1":"code","baa7c2ef":"code","fc38d734":"code","bb7ad469":"code","fb8a36e2":"code","fba52a13":"code","0eaaf9f4":"code","36a37956":"code","dfb5dba0":"code","1c3b0821":"code","b9869a70":"code","f18731ab":"code","ed3926fb":"code","c098a720":"code","ddaf8fbf":"code","4115edb9":"code","31709c11":"code","d34ddb3f":"code","3b4a516b":"code","e9f5c73a":"code","a1bdef43":"code","c75676c2":"code","5d8fc776":"code","e501c619":"code","63274096":"code","b68fc706":"code","de183891":"code","8d1ac62b":"code","2b16e535":"code","596e3ca8":"code","95772b88":"code","7761a77a":"code","af679dca":"code","395ff78d":"markdown","5443dad6":"markdown","ba189b98":"markdown","463a32ed":"markdown","53f9e074":"markdown","97decba5":"markdown","691f891d":"markdown","4f79ac22":"markdown","0c56b657":"markdown","ee9f6fed":"markdown","ec212f7f":"markdown","b5006864":"markdown","75707cde":"markdown","b0ffa5c1":"markdown","03ee9e90":"markdown","26da3f3e":"markdown","93dfc178":"markdown","97ae1967":"markdown","24f0b228":"markdown","be71d4f2":"markdown","11b4ced0":"markdown","5c752cf7":"markdown","f7ab0c5c":"markdown","32a6f700":"markdown","caeb5ab7":"markdown","96ced5d3":"markdown","a8c0a122":"markdown","a4133407":"markdown"},"source":{"551124d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3227cc7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4c941fbc":"import warnings\nwarnings.filterwarnings('ignore')","80b2c6c4":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime as dt\nimport seaborn as sns","3ca2df3f":"# dat = pd.read_csv(\"data.csv\", encoding='ISO-8859-1')\ndat = pd.read_csv('..\/input\/ecommerce-data\/data.csv',encoding=\"ISO-8859-1\")","ad3cf675":"dat['InvoiceDate'] = pd.to_datetime(dat['InvoiceDate'])\ndat.CustomerID = dat.CustomerID.astype(object)\ndat.info() \n","f453e1f7":"dat.isnull().sum()\ndat.isnull().sum() \/ dat.shape[0] * 100","891881bf":"dat = dat.dropna()\ndat.isnull().sum()","1dbf4b32":"#No of cancelled transactions\ndat[\"IsCancelled\"]=np.where(dat.InvoiceNo.apply(lambda l: l[0]==\"C\"), True, False)\ndat.IsCancelled.value_counts() \/ dat.shape[0] * 100\n","565e0005":"dat = dat.loc[dat.IsCancelled==False].copy()\ndat = dat.drop(\"IsCancelled\", axis=1)","84dba25b":"dat.StockCode.nunique(), dat.Description.nunique()","b93039a8":"stockcode_frequency = dat.StockCode.value_counts().sort_values(ascending=False)\ndescription_frequency = dat.Description.value_counts().sort_values(ascending=False)\nfig, ax = plt.subplots(2,1,figsize=(20,15))\nsns.barplot(stockcode_frequency.iloc[0:19].index,\n            stockcode_frequency.iloc[0:19].values,\n            ax = ax[0], palette=\"Blues_r\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_xlabel(\"Stockcode\")\nax[0].set_title(\"Which stockcodes are most common?\");\nsns.barplot(description_frequency.iloc[0:19].index,\n            description_frequency.iloc[0:19].values,\n            ax = ax[1], palette=\"Purples_r\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"Description\")\nax[1].tick_params(labelrotation=90)\n\nax[1].set_title(\"Which Descriptions are most common?\");","ecfa7ccf":"customer_frequency = dat.CustomerID.value_counts().sort_values(ascending=False).iloc[0:19] \nplt.figure(figsize=(19,10))\ncustomer_frequency.index = customer_frequency.index.astype('Int64') \nsns.barplot(customer_frequency.index, customer_frequency.values, order=customer_frequency.index, palette=\"Spectral_r\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"CustomerID\")\nplt.title(\"Which customers are most common?\");","7525e6f0":"country_frequency = dat.Country.value_counts().sort_values(ascending=False).iloc[0:20]\nplt.figure(figsize=(20,5))\nsns.barplot(country_frequency.index, country_frequency.values, palette=\"plasma_r\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Which countries made the most transactions?\");\nplt.xticks(rotation=90)","5ac55dec":"x = dat.groupby(['CustomerID','Country']).size().sort_values(ascending=False).iloc[0:19]\npd.DataFrame(x)","6ecc833b":"customer_frequency = dat.CustomerID.value_counts().sort_values(ascending=False).iloc[0:19] \nuk_customers = dat.groupby(dat['CustomerID']).size().where(dat['Country'] == 'United Kingdom').sort_values(ascending=False).iloc[0:19]\nfig, ax = plt.subplots(2,1,figsize=(20,20))\nsns.barplot(customer_frequency.index,\n            customer_frequency.values,\n            ax = ax[0], palette=\"Blues_r\", order=customer_frequency.index)\nax[0].set_ylabel(\"Frequency\")\nax[0].set_xlabel(\"CustomerID\")\nax[0].set_title(\"Which Customers are most common?\");\nsns.barplot(uk_customers.index,\n            uk_customers.values,\n            ax = ax[1], palette=\"cool\", order=uk_customers.index)\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"CustomerID\")\nax[1].set_title(\"Which Customers are most common in the United Kingdom?\");","fd8e6de6":"dat.UnitPrice.describe()","5ae56da7":"#taking out unit price less than 0\ndat = dat.loc[dat.UnitPrice > 0].copy()","66b1fce8":"fig, ax = plt.subplots(2,1,figsize=(15,15))\nsns.distplot(dat.UnitPrice, ax=ax[0])\nax[0].set_ylabel('Frequency')\nsns.distplot(np.log(dat.UnitPrice), ax=ax[1], bins=20)\nax[1].set_ylabel('Frequency')\nax[1].set_xlabel(\"Log-Unit-Price\");","54677d2a":"np.exp(-2),np.exp(3)","88a3aa9c":"dat = dat.loc[(dat.UnitPrice > 0.1) & (dat.UnitPrice < 20)].copy()","5f821533":"dat.UnitPrice.describe()\n","5cb2fc39":"fig, ax = plt.subplots(2,1,figsize=(15,15))\nsns.distplot(dat.UnitPrice, ax=ax[0])\nax[0].set_ylabel('Frequency')\nsns.distplot(np.log(dat.UnitPrice), ax=ax[1], bins=20)\nax[1].set_ylabel('Frequency')\nax[1].set_xlabel(\"Log-Unit-Price\");","5a84da3d":"dat.Quantity.describe()\n","6be6f154":"fig, ax = plt.subplots(2,1,figsize=(15,15))\nsns.distplot(dat.Quantity, ax=ax[0], kde=False)\nax[0].set_title(\"Quantity distribution\")\nax[0].set_ylabel('Frequency')\nax[0].set_yscale(\"log\")\nsns.distplot(np.log(dat.Quantity), ax=ax[1], bins=20, kde=False)\nax[0].set_title(\"Log-Quantity distribution\")\nax[1].set_ylabel('Frequency')\nax[1].set_xlabel(\"Log-Quantity\");","78309c79":"np.exp(4),np.quantile(dat.Quantity, 0.95)","0cba082c":"dat = dat.loc[dat.Quantity < 55].copy()","9182751a":"fig, ax = plt.subplots(2,1,figsize=(15,15))\nsns.distplot(dat.Quantity, ax=ax[0], kde=False)\nax[0].set_title(\"Quantity distribution\")\nax[0].set_ylabel('Frequency')\nax[0].set_yscale(\"log\")\nsns.distplot(np.log(dat.Quantity), ax=ax[1], bins=20, kde=False)\nax[0].set_title(\"Log-Quantity distribution\")\nax[1].set_ylabel('Frequency')\nax[1].set_xlabel(\"Log-Quantity\");","89a3bc7e":"dat[\"Revenue\"] = dat.Quantity * dat.UnitPrice\n\ndat[\"Month\"] = dat.InvoiceDate.dt.month\n\ndat.groupby('Month').sum().sort_values(by='Revenue', ascending=False)","862ab159":"plt.rcParams.update({'font.size': 12})\nz = dat.groupby('Month').sum().sort_values(by='Revenue',ascending=False)\nx = z.index\ny = z['Revenue'].sort_values(ascending=False)\nplt.figure(figsize=(10,10))\nsns.barplot(x, y, order=x)\nplt.ylabel(\"Revenue\", Size=14)\nplt.xlabel(\"Months\", Size=14)\nplt.title(\"Which Month had the highest Revenue?\", Size=14);","e3777b73":"#What product contributed the most to revenue? Why?\ndf = dat[['StockCode','Revenue']].groupby('StockCode').sum().sort_values(by='Revenue', ascending=False).iloc[0:9]\ndf.reset_index(inplace=True)","d08bfd4a":"z = df.groupby('StockCode').sum().sort_values(by='Revenue',ascending=False)\nx = z.index\ny = z['Revenue'].sort_values(ascending=False)\nplt.figure(figsize=(10,10))\nsns.barplot(x, y, order=x)\nplt.ylabel(\"Revenue\", Size=14)\nplt.xlabel(\"StockCode\", Size=14)\nplt.title(\"Which StockCodes gave the highest Revenue?\", Size=14);","ca75dbce":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import ceil\n\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.manifold import TSNE\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual,VBox,HBox,Layout\nimport ipywidgets as widgets\nimport datetime\nfrom datetime import date\n","6e8ccd8b":"data = dat.copy()","67c5ec70":"data['InvoiceDate']=pd.to_datetime(data['InvoiceDate'])\ndata['Sales'] = data.Quantity*data.UnitPrice\ndata['Year']=data.InvoiceDate.dt.year\ndata['Month']=data.InvoiceDate.dt.month\ndata['Week']=data.InvoiceDate.dt.week\n# data.InvoiceDate.dt.week\n# wk = dt.isocalendar()[1]\ndata['Year_Month']=data.InvoiceDate.dt.to_period('M')\ndata['Hour']=data.InvoiceDate.dt.hour\ndata['Day']=data.InvoiceDate.dt.day\ndata['is_cancelled']=data.InvoiceNo.apply(lambda x: 'Yes' if x[0]=='C' else 'No')\ndata['weekday'] = data.InvoiceDate.dt.day_name()\ndata['Quarter'] = data.Month.apply(lambda m:'Q'+str(ceil(m\/4)))\ndata['Date']=pd.to_datetime(data[['Year','Month','Day']])\ndata.head()","66aa1649":"df = px.data.gapminder()\ndf = df [['country','iso_alpha']]\ndata = pd.merge(data,df[['country','iso_alpha']],left_on='Country',right_on='country',how='left').drop(columns=['country'])\ndel df","4dbebdfe":"grp_data = data.groupby(by='Country')['Sales'].sum().sort_values(ascending=False).reset_index()\n\nfig = go.Figure(data=go.Choropleth(\n    locations = grp_data['Country'],\n    z = grp_data['Sales'],\n    text = grp_data['Country'],\n    colorscale = 'earth',\n    locationmode = 'country names',\n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = 'Sales',\n))\n\n\nfig.update_layout(\n    title_text='Sales by country',\n    geo=dict(showframe=False,showcoastlines=False,projection_type='equirectangular'),\n    annotations = [dict(x=0.55,y=0.1,xref='paper',yref='paper',showarrow = False)])\n\nfig.show()\n\ndel grp_data","b2df301a":"data_=data[data.is_cancelled=='No']\ndel data","5f684568":"\nsales_by_date = data_.groupby(by='Date')['Sales'].sum().reset_index()\nfig = go.Figure(data=go.Scatter(x=sales_by_date.Date,y=sales_by_date.Sales\n                                ,line = dict(color='black', width=1.5)))\nfig.update_layout(xaxis_title=\"Date\",yaxis_title=\"Sales\",title='Daily Sales',template='ggplot2')\nfig.show()","4b9ddedd":"sales_by_hour = data_.groupby(by='Hour')['Sales'].sum().reset_index()\nsales_by_weekday = data_.groupby(by='weekday')['Sales'].sum().reset_index()\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Total Hourly Sales\", \"Total Sales by Weekday\"))\nfig.add_trace(go.Bar(y=sales_by_hour.Hour, x=sales_by_hour.Sales,orientation='h'),row=1, col=1)\nfig.add_trace(go.Bar(x=sales_by_weekday.weekday, y=sales_by_weekday.Sales),row=1, col=2)\nfig.update_layout(height=700, width=800,template='ggplot2')\nfig.update_xaxes(title_text=\"Sales\", row=1, col=1)\nfig.update_xaxes(title_text=\"Weekday\", row=1, col=2)\nfig.update_yaxes(title_text=\"Hours\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales\", row=1, col=2)\nfig.show()\n\n\ndel [sales_by_hour,sales_by_weekday]","f027a26e":"customer_by_month1 = data_.groupby('CustomerID')['Date'].min().reset_index()\ncustomer_by_month1['days'] = pd.TimedeltaIndex(customer_by_month1.Date.dt.day,unit=\"D\")\ncustomer_by_month1['Month'] = customer_by_month1.Date- customer_by_month1.days+pd.DateOffset(days=1)\ncustomer_by_month1['Quarter_acquisition'] = customer_by_month1['Month'].dt.quarter.apply(lambda x:'Q'+str(x))\ncustomer_by_month1['Year_acquisition'] = customer_by_month1['Month'].dt.year\ncustomer_by_month = data_.groupby(by = customer_by_month1.Month)['CustomerID'].size().reset_index()\ncustomer_by_month.sort_values(by ='Month',ascending=True,inplace=True)\ncustomer_by_month['cum_customer'] = np.cumsum(customer_by_month.CustomerID)\ncustomer_by_month['Month_1'] = customer_by_month['Month'].dt.strftime('%b-%y')","a299c7bb":"plt.style.use('ggplot')\nplt.figure(figsize=(20,5))\nplt.plot(customer_by_month.Month_1,customer_by_month.cum_customer,'bo-',color='black')\n\n# zip joins x and y coordinates in pairs\nfor d,c in zip(customer_by_month['Month_1'],customer_by_month['cum_customer']):\n\n    label = \"{:.0f}\".format(c)\n\n    plt.annotate(label, \n                 (d,c), \n                 textcoords=\"offset points\"\n                 , bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\")\n                 #,arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"angle,angleA=0,angleB=10,rad=90\")\n                 ,xytext=(0,10),\n                 ha='center') \nplt.show()\n\ndel customer_by_month","5d28086e":"sales_by_hour = data_.groupby(by='Hour')['Sales'].mean().reset_index()\nsales_by_weekday = data_.groupby(by='weekday')['Sales'].mean().reset_index()\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Avg Hourly Sales\", \"Avg Sales by Weekday\"))\nfig.add_trace(go.Bar(y=sales_by_hour.Hour, x=sales_by_hour.Sales,orientation='h'),row=1, col=1)\nfig.add_trace(go.Bar(x=sales_by_weekday.weekday, y=sales_by_weekday.Sales),row=1, col=2)\nfig.update_layout(height=700, width=800,template='ggplot2')\nfig.update_xaxes(title_text=\"Sales\", row=1, col=1)\nfig.update_xaxes(title_text=\"Weekday\", row=1, col=2)\nfig.update_yaxes(title_text=\"Hours\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales\", row=1, col=2)\nfig.show()\n\ndel [sales_by_hour,sales_by_weekday]","d9f905ef":"LRFM = data_.groupby('CustomerID').agg(Frequency=pd.NamedAgg(column=\"InvoiceNo\", aggfunc=\"nunique\")\n                                        ,Monetary=pd.NamedAgg(column=\"Sales\", aggfunc=\"sum\"),\n                                         Recency = pd.NamedAgg(column='InvoiceDate',aggfunc='min')).reset_index()\n\nlength = data_.groupby('CustomerID')['Date'].max() - data_.groupby('CustomerID')['Date'].min()\nlength =  (length\/np.timedelta64(1, 'D')).reset_index()\nlength.columns = ['CustomerID','Length_of_stay']\nLRFM = LRFM.merge(length,on='CustomerID',how='inner')\ndel length\n\nLRFM.head()","a4a194b9":"e = data_['InvoiceDate'].min()\nprint('minimun :'+ str(e))","b6906711":"LRFM['Recency'] = LRFM['Recency'].apply(lambda x : (x - e).days)","9206c674":"LRFM","19fef92c":"### 3 D View of Sales vs Length of Stay vs Frequency\n### This chart shows that there are set of potential customers who has influential data points. \n### This gives an intuition of finding cluster in the data.\n\nfig = go.Figure(data=[go.Scatter3d(x=LRFM.Monetary,y=LRFM.Length_of_stay,z=LRFM.Frequency,mode='markers'\n                                   ,marker=dict(size=4,color='coral',colorscale='Viridis',opacity=0.8))])\nfig.update_layout(margin=dict(l=1, r=2, b=1, t=1)\n                  ,scene=dict(xaxis=dict(title='Sales')\n                              ,yaxis=dict(title='Lenght of Stay')\n                              ,zaxis=dict(title='Frequency')),width=800,height=500)\nfig.show()\n","d4b84688":"\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.manifold import TSNE","18b3fa75":"X=LRFM.drop(columns = 'CustomerID')\nerror = []\nsilhouette = []\nnp.random.seed(12)\nrng = range(2,20)\nfor i in rng:\n    km = KMeans(n_clusters=i, init='random',n_init=20, max_iter=200,tol=.0001, random_state=12)\n    km.fit(X)\n    error.append(km.inertia_)\n    lbls = km.fit_predict(X)\n    silhouette.append(silhouette_score(X, lbls))\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Number of Cluster vs Error\", \"Number of Cluster vs Silhouette\"))\nfig.add_trace(go.Scatter(x=list(rng), y=error),row=1, col=1)\nfig.add_trace(go.Scatter(x=list(rng), y=silhouette),row=1, col=2)\nfig.update_layout(height=500, width=800,template='ggplot2')\nfig.update_xaxes(title_text=\"Number of Clusters\", row=1, col=1)\nfig.update_xaxes(title_text=\"Number of Clusters\", row=1, col=2)\nfig.update_yaxes(title_text=\"Errors\", row=1, col=1)\nfig.update_yaxes(title_text=\"Silhouette Distance\", row=1, col=2)\nfig.show()\n\ndel [error,silhouette,rng,km,lbls,X]","c0791891":"from yellowbrick.cluster import KElbowVisualizer\nX=LRFM.drop(columns = 'CustomerID')\n\nmodel = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,20), timings= True)\nvisualizer.fit(X)        # Fit data to visualizer\nvisualizer.show()        # Finalize and render figure","f8553746":"X=LRFM.drop(columns = 'CustomerID')\ncluster_lbls = KMeans(n_clusters=5, random_state=12).fit_predict(X)\nX['cluster'] = cluster_lbls\nX['sample_silhouette_values'] = silhouette_samples(X, cluster_lbls)\nX['txt']=X.cluster.apply(lambda x:'Cluster '+str(x))","cba4b640":"df = X.groupby('cluster').agg({'cluster':'size', 'Monetary':'mean','Frequency':'mean','Length_of_stay':'mean'}) \\\n       .rename(columns={'cluster':'Size','Monetary':'Avg Sales','Frequency':'Avg Recency','Length_of_stay':'Avg Lenght of Stay'}) \\\n       .reset_index().sort_values(by = 'Avg Sales')\n\ncluster_map ={'Cluster 4':'lightskyblue','Cluster 0':'lightskyblue','Cluster 8':'lightskyblue'\n              ,'Cluster 6':'lightskyblue','Cluster 2':'lightskyblue',\n             'Cluster 3':'orange','Cluster 7':'orange','Cluster 9':'orange'\n              ,'Cluster 1':'olive','Cluster 5':'olive'}\n\ntxt =['Size = {0:.0f}'.format(i) for i in df.Size]\ndf['cluster']=df.cluster.apply(lambda x:'Cluster '+str(x))\ndf['Group']=df.cluster.map(cluster_map)\n\nfig = make_subplots(rows=1, cols=3,subplot_titles=(\"Avg Sales\", \"Avg Recency\",'Avg Lenght of Stay'))\n\nfig.add_trace(go.Bar(y=df.cluster, x=df['Avg Sales'],hovertext=txt\n                        ,text=txt,textposition='auto',marker_color=df.Group,orientation='h'),row=1, col=1)\nfig.add_trace(go.Bar(y=df.cluster, x=df['Avg Recency'],hovertext=txt\n                        ,text=txt,textposition='auto',marker_color=df.Group,orientation='h'),row=1, col=2)\nfig.add_trace(go.Bar(y=df.cluster, x=df['Avg Lenght of Stay'],hovertext=txt\n                        ,text=txt,textposition='auto',marker_color=df.Group,orientation='h'),row=1, col=3)\n\nfig.update_traces(marker_line_color='rgb(8,48,107)', marker_line_width=1.5, opacity=0.8)\nfig.update_layout(title_text='Cluster Size',width = 800,height=600,template='ggplot2'\n                  ,font=dict(family=\"Courier New, monospace\",size=10,color=\"RebeccaPurple\"))\n\nfig.show()","1bbe88cb":"#3 D view of clusters\nfig = go.Figure(data=[go.Scatter3d(x=X.Monetary,y=X.Length_of_stay,z=X.Frequency,mode='markers'\n                                   ,marker=dict(size=4,color=X.cluster\n                                                ,colorscale='Viridis',opacity=0.8))])\n\n# tight layout\nfig.update_layout(margin=dict(l=1, r=2, b=1, t=1)\n                  ,scene=dict(xaxis=dict(title='Sales')\n                              ,yaxis=dict(title='Lenght of Stay')\n                              ,zaxis=dict(title='Recency')),width=700,height=500)\nfig.show()\n","699f5fb9":"prpxlt = [10,15,30,40]\nfig = make_subplots(rows=2, cols=2,subplot_titles=['perplexity = %s'%i for i in prpxlt])\n\nfor i, prpxlt_ in enumerate(prpxlt):\n    X_tsne = TSNE(n_components=2, init='random',\n                         random_state=0, perplexity=prpxlt_).fit_transform(X[['Frequency', 'Monetary', 'Length_of_stay','Recency']])\n    if i%2==0:\n        c=2\n    else:\n        c=1\n    r = (i)\/\/2+1\n    fig.add_trace(go.Scatter(x=X_tsne[:,0],y=X_tsne[:,1]\n                                    , mode='markers',marker=dict(size=4,color=X.cluster\n                                                ,colorscale='Viridis',opacity=0.8),text=X.txt),row=r, col=c)\n\n    fig.update_xaxes(title_text=\"Component 1\", row=r, col=c)\n    fig.update_yaxes(title_text=\"Component 2\", row=r, col=c)\n    \nfig.update_layout(height=600, width=800)\nfig.show()\n\ndel [X_tsne,fig]\n","8cf11fa1":"from sklearn.preprocessing import MinMaxScaler\ntemp = X.drop(columns=['cluster','sample_silhouette_values','txt'])\nscaler1 = MinMaxScaler()\nscaled_data = pd.DataFrame(scaler1.fit_transform(temp),columns=['Frequency', 'Monetary', 'Recency','Length_of_stay'])\nscaled_data","e9055ed9":"def kmeans(normalised_df_rfm, clusters_number, original_df_rfm):\n    \n    kmeans = KMeans(n_clusters = clusters_number, random_state = 1)\n    kmeans.fit(normalised_df_rfm)\n\n    # Extract cluster labels\n    cluster_labels = kmeans.labels_\n        \n    # Create a cluster label column in original dataset\n    df_new = original_df_rfm.assign(Cluster = cluster_labels)\n    \n    # Initialise TSNE\n    model = TSNE(random_state=1)\n    transformed = model.fit_transform(df_new)\n    \n    # Plot t-SNE\n    plt.title('Flattened Graph of {} Clusters'.format(clusters_number))\n    sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"Set1\")\n    \n    return df_new","646b8821":"plt.figure(figsize=(10,5))\ndf_rfm_k4 = kmeans(scaled_data, 5, temp)","fbb6a939":"plt.figure(figsize=(10,5))\ndf_rfm_k4 = kmeans(scaled_data, 6, temp)","6948e687":"import ipyvolume as ipv\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom ipywidgets import ColorPicker, VBox, \\\n    interact, interactive, fixed\nimport struct","640ce7f1":"def handle_cp_change(labels, **groups):\n    group_ids = [int(g.split(' ')[1]) for g in groups.keys()]\n    group_color = {k: hex_to_rgb(get_cp_value(cp)) for k, cp in zip(group_ids, groups.values())}\n    colors = list(map(lambda x: group_color[x], labels))\n    scatter.color = colors\ndef get_cp_value(cp):\n    if type(cp) == ColorPicker:\n        return cp.value\n    else:\n        return cp\n","baa7c2ef":"def color_scatter_with_kmeans(n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans = kmeans.fit(X)\n    labels = kmeans.predict(X)\n    color_pickers = {f'group {k}': ColorPicker(value=avaliable_colors[k%7], description=f'group {k}') \n                     for k in range(n_clusters)}\n    handle_cp_change(labels=list(labels), **color_pickers)\n    return interact(handle_cp_change, labels=fixed(list(labels)) , **color_pickers)","fc38d734":"#Importing required modules\n \nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport numpy as np\n \n#Load Data\ndata = temp.copy()\npca = PCA(2)\n \n#Transform the data\ndf = pca.fit_transform(data)\n \n#Import KMeans module\nfrom sklearn.cluster import KMeans\n \n#Initialize the class object\nkmeans = KMeans(n_clusters= 4)\n \n#predict the labels of clusters.\nlabel = kmeans.fit_predict(df)\n \n#Getting unique labels\nu_labels = np.unique(label)\n \n#plotting the results:\nfor i in u_labels:\n    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\nplt.legend()\nplt.show()","bb7ad469":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_pacf,plot_acf\n","fb8a36e2":"ds_weekly = data_.groupby(by=['Year','Week'])['Sales'].sum().reset_index()\nds_daily = data_.groupby(by=['Date'])['Sales'].sum().reset_index()","fba52a13":"def plot_(t_train,t_test,x_train,x_test,x_train_pred,x_test_pred,forecast,title='Weekly'):\n    xt = (max(t_test)+np.arange(len(forecast)))+1\n    fig_train=go.Scatter(name='Train : Actual ',x=t_train,y=x_train,showlegend=True)\n    fig_trian_pred=go.Scatter(name='Train : Predict',x=t_train,y=x_train_pred,showlegend=True)\n    fig_test=go.Scatter(name='Test : Actual',x=t_test,y=x_test,showlegend=True)\n    fig_test_pred=go.Scatter(name='Test : Predict',x=t_test,y=x_test_pred,showlegend=True)\n    fig_forecast=go.Scatter(name='Forecast',x=xt,y=forecast,showlegend=True)\n\n    fig = go.Figure([fig_train,fig_trian_pred,fig_test,fig_test_pred,fig_forecast])\n    fig.update_layout(xaxis_title=title,yaxis_title=\"Sales\",title=title +' Trend'\n                      ,height=400,hovermode=\"x\",template='ggplot2')\n    fig.show()","0eaaf9f4":"fig = go.Figure(data=[go.Scatter(x=ds_weekly.index,y=ds_weekly.Sales)])\nfig.update_layout(xaxis_title=\"Week\",yaxis_title=\"Sales\",title='Weekly Trend',height=400,template='ggplot2')\nfig.show()","36a37956":"# Weekly: Test of Stationarity of Actual Series\noutput = adfuller(ds_weekly.Sales)\nprint('***************************Week*********************************')\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is extremely high which indicates that we are fail to reject null hypothesis \" \\\n      \"and can conclude that series is not stationary \")","dfb5dba0":"# Weekly: Test of Stationarity with 1 differencing of series\nd=1\nprint('***************************Week*********************************')\n# series = ds.Sales\nseries = ds_weekly.Sales.diff(d)# - ds.Sales.rolling(window=12).mean()\nseries = series.dropna()\noutput = adfuller(series)\n\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is close to zero which is less than .05 hence we reject null hypothesis and conclude \" \\\n      \" that series is stationary with rolling mean difference at lag of 12 \")","1c3b0821":"# Weekly: PACF & ACF\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nplot_acf(series, ax=ax[0])\nplot_pacf(series, ax=ax[1])\nplt.show()","b9869a70":"# Weekly: Train & Test Split\nseries=ds_weekly.Sales\nsplit_time = 45\ntime=np.arange(len(ds_weekly))\nxtrain=series[:split_time]\nxtest=series[split_time:]\ntimeTrain = time[:split_time]\ntimeTest = time[split_time:]\nprint('Full Set Size ',series.shape)\nprint('Training Set Size ',xtrain.shape)\nprint('Testing Set Size ',xtest.shape)","f18731ab":"# Weekly: ARIMA Model\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(xtrain, order=(1,1,2))\nmodel_fit = model.fit()\nprint(model_fit.summary())","ed3926fb":"ytrain_pred = model_fit.predict()\nytest_pred = model_fit.predict(start=min(timeTest),end=max(timeTest),dynamic=True)\nprint('MSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain)**2)))\nprint('MSE Test :',np.sqrt(np.mean((ytest_pred - xtest)**2)))\nforecast = model_fit.forecast(20, alpha=0.05)\nplot_(t_train = timeTrain,t_test = timeTest,x_train = xtrain,x_test = xtest,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Weekly')","c098a720":"# WEEKLY: SARIMAX Model\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\ns_model = SARIMAX(endog=xtrain , order=(1, 1, 2), seasonal_order=(1, 1, 1, 3), trend='c')\ns_model_fit=s_model.fit()\nprint(s_model_fit.summary())","ddaf8fbf":"ytrain_pred = s_model_fit.predict()\nytest_pred = s_model_fit.predict(start=min(timeTest),end=max(timeTest),dynamic=True)\nprint('RMSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain)**2)))\nprint('RMSE Test :',np.sqrt(np.mean((ytest_pred - xtest)**2)))\nforecast = s_model_fit.forecast(20, alpha=0.05)\nplot_(t_train = timeTrain,t_test = timeTest,x_train = xtrain,x_test = xtest,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Weekly')","4115edb9":"# Daily: Trend\nfig = go.Figure(data=[go.Scatter(x=ds_daily.Date,y=ds_daily.Sales)])\nfig.update_layout(xaxis_title=\"Date\",yaxis_title=\"Sales\",title='Daily Trend',height=400,template='ggplot2')\nfig.show()","31709c11":"# Daily: Test of Stationarity of Actual Series\nprint('\\n***************************Daily*********************************')\noutput = adfuller(ds_daily.Sales)\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is extreemly high which indicates that we are fail to reject null hypothesis \" \\\n      \"and can conclude that series is not stationary \")","d34ddb3f":"# Daily: Test of Stationarity with 1 differencing of series\nprint('\\n***************************Daily*********************************')\nseries_date = ds_daily.Sales.diff(d)\nseries_date = series_date.dropna()\noutput = adfuller(series_date)\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is close to zero which is less than .05 hence we reject null hypothesis and conclude \" \\\n      \" that series is stationary with rolling mean difference at lag of 12 \")","3b4a516b":"# Daily: Train & Test Split\nseries_date=ds_daily.Sales\nsplit_time = 250\ntime_d=np.arange(len(ds_daily))\nxtrain_d=series_date[:split_time]\nxtest_d=series_date[split_time:]\ntimeTrain_d = time_d[:split_time]\ntimeTest_d = time_d[split_time:]\nprint('Full Set Size ',series_date.shape)\nprint('Training Set Size ',xtrain_d.shape)\nprint('Testing Set Size ',xtest_d.shape)","e9f5c73a":"# Daily: ARIMA Model\ns_model = ARIMA(endog=xtrain_d , order=(1, 1, 1))\ns_model_fit=s_model.fit()\nprint(s_model_fit.summary())","a1bdef43":"ytrain_pred = s_model_fit.predict()\nytest_pred = s_model_fit.predict(start=min(timeTest_d),end=max(timeTest_d),dynamic=True)\n\nprint('MSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain_d)**2)))\nprint('MSE Test :',np.sqrt(np.mean((ytest_pred - xtest_d)**2)))\nforecast = s_model_fit.forecast(20, alpha=0.05)\n\nplot_(t_train = timeTrain_d,t_test = timeTest_d,x_train = xtrain_d,x_test = xtest_d,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Daily')","c75676c2":"# Daily: SARIMAX Model\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\ns_model = SARIMAX(endog=xtrain_d , order=(1, 1, 1), seasonal_order=(1, 1, 2, 12), trend='t')\ns_model_fit=s_model.fit()\nprint(s_model_fit.summary())","5d8fc776":"ytrain_pred = s_model_fit.predict()\nytest_pred = s_model_fit.predict(start=min(timeTest_d),end=max(timeTest_d),dynamic=True)\n\nprint('MSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain_d)**2)))\nprint('MSE Test :',np.sqrt(np.mean((ytest_pred - xtest_d)**2)))\nforecast = s_model_fit.forecast(30, alpha=0.05)\nxt = max(timeTest_d)+np.arange(len(forecast))\n\nplot_(t_train = timeTrain_d,t_test = timeTest_d,x_train = xtrain_d,x_test = xtest_d,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Daily')","e501c619":"from sklearn.linear_model import LinearRegression\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.stattools import durbin_watson\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import linear_rainbow\nfrom statsmodels.stats.api import het_goldfeldquandt\nfrom sklearn.metrics import r2_score","63274096":"#Split the data\nxtrain1,xtest1,ytrain1,ytest1 = train_test_split(LRFM[['Frequency', 'Length_of_stay']],LRFM.Monetary\n                                             ,test_size=.3,random_state=49,shuffle=True)\nprint(xtrain1.shape)\nprint(xtest1.shape)","b68fc706":"#Log Log Model\n#After several iteration I saw that Log-Log model seems better fit on the data which improves R square and following are all assumptions of regression. Please check chart of residual vs predicted for all assumption tests\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# poly = PolynomialFeatures(2)\n# xtest = poly.fit_transform(xtest1)\n# xtrain = poly.fit_transform(xtrain1)\n\ninp_test=sm.add_constant(np.log1p(xtest1))\ninp_train=sm.add_constant(np.log1p(xtrain1))\nytrain=np.log1p(ytrain1)\nytest=np.log1p(ytest1)\nols=sm.OLS(ytrain,inp_train)\nmodel=ols.fit()\nprint(model.summary())","de183891":"#Interpration\nprint('Below are the interprataion of all 2 coeficients')\nprint('if customers increase the frequency of purchase by 1 percent then we can expect that sales will increase by 1.3%')\nprint('As customers are staying longer with business then we can expect that sales will increase by 0.07%')","8d1ac62b":"#Get Error\nytrain_pred=model.predict(inp_train)\nytest_pred=model.predict(inp_test)\n\ntrain_res=ytrain-ytrain_pred\ntest_res=ytest-ytest_pred\n\ntrain_mse= np.mean(train_res**2)\ntest_mse= np.mean(test_res**2)\n\nprint('Train MSE:{0:.4f}'.format(train_mse))\nprint('Test MSE:{0:.4f}'.format(test_mse))","2b16e535":"#Get R square\nr2_train=r2_score(ytrain,ytrain_pred)\nr2_test=r2_score(ytest,ytest_pred)\nprint('R square Train:{0:.4f}'.format(r2_train))\nprint('R square Test:{0:.4f}'.format(r2_test))","596e3ca8":"#Linearrainbow test\n#H0 : Part of the data is Linear\n\n#Ha : Independent features does not have linear relation with target feature\n\npval=linear_rainbow(res=model,frac=0.5)[1]\nprint('P value {0:.2f}'.format(pval))","95772b88":"#Homoscadastic -- Uniform variance\n#H0 : model is Homoscadastic\n\n#Ha : model is Heteroskedastic\n\nhomoscad = het_goldfeldquandt(train_res,inp_train)[1]\nprint('P Value {0:.2f}'.format(homoscad))","7761a77a":"#Plot between Actual and predicted to check relationship\nfig = make_subplots(rows=1, cols=2,subplot_titles=['Train','Test'])\nfig.add_trace(go.Scatter(x=ytrain,y=ytrain_pred, mode='markers'\n                         ,marker=dict(size=4,colorscale='Viridis',opacity=0.8)),row=1, col=1)\nfig.add_trace(go.Scatter(x=ytest,y=ytest_pred, mode='markers'\n                         ,marker=dict(size=4,colorscale='Viridis',opacity=0.8)),row=1, col=2)\nfig.update_xaxes(title_text=\"Actual\", row=1, col=1)\nfig.update_yaxes(title_text=\"Predicted\", row=1, col=1)\nfig.update_xaxes(title_text=\"Actual\", row=1, col=2)\nfig.update_yaxes(title_text=\"Predicted\", row=1, col=2)\nfig.update_layout(height=400, width=800,template='ggplot2')\nfig.show()","af679dca":"textstr = \"<br>\".join((\"SKEW={:.2f}\".format(train_res.skew())\n                     ,'Linearity Test P value={:.2f}'.format(pval)\n                    ,'Homoscadastic Test P value={:.2f}'.format(homoscad)\n                    ,'Durbin Watson value = {:.2f}'.format(durbin_watson(train_res))\n                    ,'Train MSE ={:.3f}'.format(train_mse)\n                    ,'Test MSE ={:.3f}'.format(test_mse)\n                    ,'R square Train ={:.3f}'.format(r2_train)\n                    ,'R square Test ={:.3f}'.format(r2_test)))\n\nfig= px.scatter(x=ytrain,y=train_res,marginal_y=\"histogram\",\n               marginal_x=\"histogram\", trendline=\"ols\")\n\nfig.update_layout(title=\"Test of Assumptions\",xaxis_title=\"Predicted\",yaxis_title=\"Residual\",\n                            font=dict(family=\"Courier New, monospace\",size=10,color=\"RebeccaPurple\"))\n\nfig.add_annotation(x=4,y=4,xref=\"x\",yref=\"y\",text=textstr,showarrow=False,font=dict(size=10)\n                   ,align=\"left\",ax=20,ay=-30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"snow\",opacity=0.8)\nfig.show()","395ff78d":"Now are std is much smaller and we have a more evenly distributed graph. Our graph is still skewed to the right which is something to make note of for which we will deal with later. Now lets take a look at the Quantity column.","5443dad6":"## Time Series Forecasting","ba189b98":"It is clear that the vast majority of transactions take place in the United Kingdom. Lets see if our top 20 Customers purchase their items in the United Kingdom or in other countries.","463a32ed":"We can see that November is the highest revenue month for the company followed by October and then September. This could be because these are the months leading up to the holiday months where it is more likely people will be buying gifts and business increasing their inventory. It should be noted that the company considers many of their customers to be wholesalers indicating that customers are likely preparing for the holiday season by purchasing more products.","53f9e074":"### Geographic sales view","97decba5":"We have 3665 unique StockCodes and 3877 unique Descriptions which aligns with the fact that the retailer sells many different types of products. Lets take a look at the most common stockcodes and descriptions being sold","691f891d":"### Rpeat Purchase\n#### Most of customers made their next purchase after one or two months of first purchase\n#### There were 28 customers who made their first purchase in Dec 2010 also made their next purchase after 12 months\n#### 33 customers made their second purchase after 7 months of their first purchase in Mar 2011.","4f79ac22":"### This analysis exhibits the number of customer acquisition month over month and in the end of Nov 2011 there were approx. 4.2k customers who made their purchase during the given period.\n\n","0c56b657":"#### Below charts shows average sales by hours varies whereas average sales between 7AM to 10 AM is higher compare to other hours of the day. In right hand side chart, we can see that average sales on Sunday and Monday is low compare to other weekdays.","ee9f6fed":"Which months had the highest Revenue?\u00b6\n","ec212f7f":"## t-distributed Stochastic Neighbor Embedding.\n### t-SNE is a method to visualize high dimensional data in two dimensions if the value of perplexity (It is the value to select nearest neighbors which will generate different samples) is more than 30 then algorithm is able to separate the points clearly.","b5006864":"cust_date =data_.loc[~data_.CustomerID.isna(),['CustomerID','Date']].drop_duplicates()\ncust_date.sort_values(by=['CustomerID','Date'],inplace=True)\ncust_date['rnk'] = cust_date.groupby(by='CustomerID')['Date'].transform('rank', method='dense')\ncust_date = cust_date[cust_date.rnk<=2]\ncust_date['Purchase'] = cust_date.rnk.map({1:'First Purchase',2:'Second Purchase'})\n\n# cust_purchase= cust_date.pivot(index='CustomerID',columns=['Purchase'],values='Date').reset_index()\n# cust_date.pivot(index='CustomerID',values='Date',columns=['Purchase'],level=2).reset_index()\n# df.pivot(index ='A', columns ='B', values =['C', 'A']) \ncust_date.pivot(index='CustomerID',columns='Purchase',values='Date').reset_index()\n# cust_purchase['gap']=(cust_purchase['Second Purchase']- cust_purchase['First Purchase'])\/ np.timedelta64(30, 'D')\n# cust_purchase['gap'] = cust_purchase['gap'].fillna(0).apply(lambda x:ceil(x))\n\n# days = pd.TimedeltaIndex(cust_purchase['First Purchase'].dt.day,unit=\"D\")\n\n# cust_purchase['First_Purchase_month']=cust_purchase['First Purchase']- days+pd.DateOffset(days=1)\n# cust_purchase_grid = cust_purchase.pivot_table(index=['First_Purchase_month'],columns =['gap'],values='CustomerID',aggfunc='count').fillna(0)\n# cust_purchase_grid = cust_purchase_grid.sort_index( ascending=False)\n\n# fig = go.Figure(data=go.Heatmap(z=cust_purchase_grid\n#                                 ,y=cust_purchase_grid.index,\n#                    x=cust_purchase_grid.columns.tolist(),\n#                    hoverongaps = True,colorscale='balance'))\n# fig.update_xaxes(side=\"top\",showticklabels = True,ticktext= cust_purchase_grid.columns.tolist())\n# fig.update_layout(height=600,width=800,xaxis_title=\"Future Month of next Purchase after making first purchase\"\n#                   ,yaxis_title=\"Month of 1st Purchase\",\n#            yaxis=dict(autorange='reversed',showticklabels = True)\n#                   ,autosize=False,template='ggplot2')\n\n# fig.show()","75707cde":"### Due to presence of anomaly in the data, we can see some spike in the trend. There is no more detail available so it will be difficult to anticipate that this spike is due to any events or promotion. For now, lets assume that it is a random event.","b0ffa5c1":"### # Daily: Trend","03ee9e90":"We can see that our top 20 most frequent stockcodes and descriptions generally match up with eachother in terms of level of frequency so we can say it is true that majority of the descriptions are consistent with the stockcodes except for some exceptions causing slight differences in the amount of stockcodes vs descriptions","26da3f3e":"From the graphs we can see that a large portion of the prices are quite small and we have a few outliers that are very large. Due to the high frequency of small transactions I will focus on the transactions with prices in the log-unit-price graph. To find the prices i will take the exponent of -2 and the exponent of 3 as the majority of the price are between these two log units.","93dfc178":"It appears we have a few outliers in our top customers group where the top country is Ireland and the Netherlands.However, the majority are from the United Kingdom which makes sense due to the large difference in transactions between the United Kingdom and the rest of the countries in our dataset.","97ae1967":"We can see that the majority of our distribution lies between 0.1 and 20.1 so I will delete all outliers outside of this range","24f0b228":"#### Charts show the group vs average sales and number of customers belong to the group. Customers from Cluster 1 and 5 can be considered on high priority and can be offered some promotions so that they can continue their association with business. Whereas customers from cluster 3, 7 and 9 can be treated as medium size customer.\n\n#### For medium size customers we can understand their purchase pattern and provide some promotions accordingly.\n\n#### In the end we have list of customers from remaining clusters who can be taken as seriously and if we see their length of stay is better than medium size customers but their order size and order frequency is very low. So, we may understand their purchases pattern, type of product they select, and we can offer them some discounts or vouchers, or we can do some cross sales with them.","be71d4f2":"From the graphs it looks like we have a small amount of outliers greater than 70000. lets take the exponent where Log-Quantity=4 as most of our distribution lies within this region\n\n","11b4ced0":"We can see that StockCode 22423 contributed the most to revenue by over $60000. Lets take a look at a sample of the transactions for StockCode 22423 to see if there are any clues that can explain why it's contribution to revenue is so high --it's high contribution to revenue would be the relatively high UnitPrice compared to the rest of the products. This allows it to generate more revenue in lower quantities being sold. The other data features seem to be relatively random and so without further information we can make any more inferences.","5c752cf7":"# Initialise TSNE\nmodel = TSNE(random_state=1)\ntransformed = model.fit_transform(temp)\n    \n# Plot t-SNE\nplt.title('Flattened Graph of {} Clusters'.format(clusters_number))\nsns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"Set1\")","f7ab0c5c":"#### 1 Hourly sales are normally distributed and with mean 709k.\n#### 2 There is no pattern in day wise sales in a month.\n#### 3 There is no business on Saturday.\n#### 4 Average sales are consistent from Monday to Friday\n#### 5 Sales starts at 6 in the morning and ends at 8 PM.\n#### 6 Peak hours of sales are between 10 AM to 3 PM\n#### 7 Monday and Tuesday Sales starts at 7 AM and close at 6 PM\n#### 8 Wednesday and Friday sales start at 7 AM and close at 8 PM\n#### 9 Business hours for Sunday is between 9 to 4 PM","32a6f700":"#### United Kingdom has highest sales and it alone contributes 84% of total sales\n#### United Kingdom, Netherlands, EIRE (Republic of Ireland) , Germany, France, Australia, Spain are 7 countries which as 95% of total sales\n#### Netherlands has highest average sales per transaction compare to other countries\n#### United Kingdom has performed consistently each quarter followed by Netherlands whereas Germany which was on 6th Rank in Q1 2011 achieved 3rd rank in Q4 2011","caeb5ab7":"It looks like we will be able to keep more than 95% of our data with a max quantity set at 55. Lets take a look at our distribution after we drop the outliers","96ced5d3":"Test of Assumptions of Regression\nTo meet all assumptions of linear regression below are the criteria must be followed\n\nNormality - Error Must be normally distributed\n\nTo test this assumption, we can run statistical tests or simply can take skewness of residual and can accept it as normally distributed if skewness is -.5 to +.5\nIndependence - Residual must not be autocorrelated\n\nTo test this assumption, we can check Durbin Watson value\nif it is = 2, then there is no autocorrelation among residual points.\nif it is 0< Durbin Watson <2 then there is Positive autocorrelation\nif it is 2< Durbin Watson <4. Negative auto correlation\nFor standard practice Durbin Watson value between 1.5 and 2.5 is acceptable range to pass this test\nHomoscedasticity - There should be uniform variance in error with respect target variable\nLinearity -The relationship between X and the mean of Y is linear.","a8c0a122":"### Unit Price and Quantity","a4133407":"Customers and Countries"}}