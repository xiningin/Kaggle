{"cell_type":{"62118dda":"code","910273dc":"code","0ab8254a":"code","3e8b18e7":"code","38cd5e40":"code","822a1cb8":"code","b7aff4b2":"code","e727795c":"code","05da66df":"code","b2674e2f":"code","9c30ab29":"code","e7012274":"code","2c488a2e":"code","2d08a323":"code","903eab32":"code","adf6f214":"code","d2835c2b":"code","85f06cea":"code","b44ac7db":"code","afe2ffc1":"code","0b231896":"code","e381c74c":"code","d1230dae":"code","5145169e":"code","1fae9ad5":"code","f15cb9a0":"code","7af300b8":"code","702b8ad1":"code","527d876e":"code","6895a1c5":"code","658b0f1b":"code","808f0914":"code","e800bb60":"code","5eef0865":"markdown","e82196a8":"markdown","d23776c5":"markdown","cba3d545":"markdown","faa49b7b":"markdown","563a81d9":"markdown","3c79d2fe":"markdown","7fd6c73f":"markdown","9f815f67":"markdown","64718e49":"markdown","2cbfc7c3":"markdown","f9654c13":"markdown","f80e1dbb":"markdown","b1bec146":"markdown"},"source":{"62118dda":"import numpy as np\nimport pandas as pd\nimport os\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom numba import jit\nimport itertools\nfrom seaborn import countplot,lineplot, barplot\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport random\nimport math\n\nfrom numpy.fft import *\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.tools import FigureFactory as FF\n# import peakutils\n\nle = preprocessing.LabelEncoder()\nRAW_DATA_PATH = \"..\/input\/career-con-2019\"\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","910273dc":"data_target = pd.read_csv(\"{}\/y_train.csv\".format(RAW_DATA_PATH))\ntrain_X = pd.read_csv('{}\/X_train.csv'.format(RAW_DATA_PATH)).iloc[:,3:]\ntest_X  = pd.read_csv('{}\/X_test.csv'.format(RAW_DATA_PATH)).iloc[:,3:]\ntrain_X = train_X.append(test_X)\ntrain_X = train_X.values.reshape(-1,128,10)","0ab8254a":"# Originally from Markus F's kernel: https:\/\/www.kaggle.com\/friedchips\/the-missing-link\ndef sq_dist(a,b):\n    ''' the squared euclidean distance between two samples '''\n    \n    return np.sum((a-b)**2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left\/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n    \n    edge_list = []\n    linked_list = []\n    \n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4]) # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i   = np.argmin(dist_list) # this is i's closest neighbor\n        if closest_i == i: # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4]) # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list) # here it is\n        if closest_rev == closest_i: # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev): # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n            \n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n    \n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i: # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n    \n    return data_runs","3e8b18e7":"train_left_edges, train_left_linked  = find_run_edges(train_X, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(train_X, edge='right')\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","38cd5e40":"train_runs = find_runs(train_X, train_left_edges, train_right_edges)","822a1cb8":"lost_samples = np.array([ i for i in range(len(train_X)) if i not in np.concatenate(train_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))","b7aff4b2":"find_run_edges(train_X[lost_samples], edge='left')[1][0]\nlost_run = np.array(lost_samples[find_runs(train_X[lost_samples], [0], [5])[0]])\ntrain_runs.append(lost_run)","e727795c":"first_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==821:\n            print(i)\n            first_double_surface = i","05da66df":"train_runs[first_double_surface]","b2674e2f":"new_train_runs = [821,  974,  328, 1548,  172,\n        355,  957, 1481, 1046, 1650,  857,  724,  164, 1092, 1017, 1300,\n       1212,  536,  531, 1032,  994, 1501,  588,  579, 1177,  812, 1333,\n       1253]\n\ntrain_runs[first_double_surface] = train_runs[first_double_surface][0:-len(new_train_runs)]","9c30ab29":"train_runs.append(np.array(new_train_runs))","e7012274":"second_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==3055:\n            print(i)\n            second_double_surface = i","2c488a2e":"train_runs[second_double_surface]","2d08a323":"new_train_runs2 = [3055, 3360, 3662,\n       3780, 3663, 3091, 3769, 3175, 1957, 2712, 2063, 2708, 3139, 2722]\n\ntrain_runs[second_double_surface] = train_runs[second_double_surface][0:-len(new_train_runs2)]","903eab32":"train_runs.append(np.array(new_train_runs2))","adf6f214":"third_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==2484:\n            print(i)\n            third_double_surface = i","d2835c2b":"train_runs[third_double_surface]","85f06cea":"new_train_runs3 = [2484, 3062, 2290, 3517, 3293, 2651, 3767,\n       2029, 2558, 3580, 1874, 3373, 2514, 2308, 3160, 3161, 3613, 2511,\n       2469, 2990, 2780, 3756, 2376, 2616, 2540, 2039, 2219, 3743, 3198,\n       2584, 2752, 2304, 2887, 2841, 3480, 2517, 3020, 3424, 2027, 2652,\n       2648, 3433, 2359, 3392, 3164, 3798, 3642, 2713, 3405, 3673, 2369,\n       3411, 3595, 2242, 2307, 1897, 2834, 2350, 3795, 2948, 1856, 3486,\n       3353, 1966]\n\ntrain_runs[third_double_surface] = train_runs[third_double_surface][0:-len(new_train_runs3)]","b44ac7db":"train_runs.append(np.array(new_train_runs3))","afe2ffc1":"fourth_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==3501:\n            print(i)\n            fourth_double_surface = i","0b231896":"train_runs[fourth_double_surface]","e381c74c":"new_train_runs4 = [3501, 2785]\ntrain_runs[fourth_double_surface] = train_runs[fourth_double_surface][0:-len(new_train_runs4)]\ntrain_runs.append(np.array(new_train_runs4))","d1230dae":"df_train_y = pd.DataFrame()\ndf_train_y['run_id'] = 0\ndf_train_y['run_pos'] = 0\n\nfor run_id in range(len(train_runs)):\n    for run_pos in range(len(train_runs[run_id])):\n        series_id = train_runs[run_id][run_pos]\n        df_train_y.at[ series_id, 'run_id'  ] = run_id\n        df_train_y.at[ series_id, 'run_pos' ] = run_pos\n\ndf_train_y.tail()","5145169e":"df_train_y['index'] = df_train_y.index\ndf_train_y = df_train_y.sort_values('index')\ndf_train_y.rename(columns={'index':'series_id'}, inplace=True)\ndf_train_y['run_id'] = df_train_y['run_id'].apply(lambda x: int(x))\ndf_train_y['run_pos'] = df_train_y['run_pos'].apply(lambda x: int(x))\ndf_train_y.tail()","1fae9ad5":"run_id_train = df_train_y['run_id'][0:3810].values\nrun_id_test = df_train_y['run_id'][3810:7626].values","f15cb9a0":"data_target['run_id'] = run_id_train\nmapping_leak = {}\nfor i in range(0,3810):\n    cur_data = data_target.iloc[i]\n    mapping_leak.update({cur_data['run_id']: cur_data['surface']})\n    \nunknown_run = []\nans_test = []\nknown_series = []\nunknown_series = []\n\nfor i in range(0,3816):\n    if run_id_test[i] in mapping_leak:\n        ans_test.append(mapping_leak[run_id_test[i]])\n        known_series.append(i)\n    else:\n        ans_test.append('unknown')\n        unknown_series.append(i)\n        unknown_run.append(run_id_test[i])","7af300b8":"print(\"Number of known series:{}\\nNumber of unknown series:{}\\n\".format(len(known_series),len(unknown_series)))","702b8ad1":"sub = pd.read_csv(\"{}\/sample_submission.csv\".format(RAW_DATA_PATH))\nsub['surface'] = ans_test\nsub.head()","527d876e":"sub['surface'].head(10)","6895a1c5":"best = pd.read_csv('..\/input\/best-73\/submission_24.csv')\nmap_best_ans = {}\n\nfor i in range(0, best.shape[0]):\n    map_best_ans.update({best.iloc[i]['series_id'] : best.iloc[i]['surface'] })\n","658b0f1b":"result = []\nfor i in range(0, sub.shape[0]):\n    if (sub.surface[i] == 'unknown'):\n        result.append(map_best_ans[i])\n    else:\n        result.append(sub.surface[i])","808f0914":"sub.surface = result\nsub.to_csv('final_submission.csv', index=False)","e800bb60":"sub.surface.value_counts()","5eef0865":"Now let's separate train and test.","e82196a8":"[](http:\/\/)As what have been said in Markus F's kernel, there is some train runs (4) having more than 1 surface. We need to separate it manually. In order to do this, we need to track the runs in which the mis-allocated series lies.","d23776c5":"Rename index as series_id.","cba3d545":"After we remove the double surface runs, let's add this knowledge to our train_y. ","faa49b7b":"Load data and append test row to train data:","563a81d9":"Like what Markus F said in his kernel, we can reconstruct the original data before it's being splitted. Apparently, some of the original data is being splitted between train and test data. So here we go:","3c79d2fe":"# #16 Solution (Private 0.76)","7fd6c73f":"Then, I just combine these data with my best submission (LB 0.73). I keep the known series target values while replacing the unknown one with values from my best submission.","9f815f67":"Now find the missing samples from test data","64718e49":"Because some runs is linked between train and test data, we can essentially use train's target to map our test's target.","2cbfc7c3":"First, thank you Kaggle for hosting this awesome competition and congratulations to every one that participated & fought hard in this competition! Especially to [Markus F](https:\/\/www.kaggle.com\/friedchips), [Thomas Rohwer](https:\/\/www.kaggle.com\/trohwer64), and [Nanashi](https:\/\/www.kaggle.com\/jesucristo) for their incredibly helpful contribution in discussions and kernels! I personally would have not made it #16 if it wasnt for their contribution on [The Missing Link](https:\/\/www.kaggle.com\/friedchips\/the-missing-link), [\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239#latest-512162), and [Smart Robots. Complete Notebook](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73).\n\n\n\n\n\n","f9654c13":"Let's find the edges!","f80e1dbb":"As you can see, we automatically get more 69% of our test target classified correctly!","b1bec146":"Let's find the runs!"}}