{"cell_type":{"37a3d11c":"code","c199f05d":"code","8f06457c":"code","75a8a2cd":"code","3993a332":"code","db69ad60":"code","6242e16b":"code","ab64499b":"code","34295c11":"code","3bbcdcdf":"code","cd5cf919":"code","2960cb3b":"code","5f76d909":"code","1c941927":"markdown","408fd3ad":"markdown","71fdce56":"markdown","1ff995e9":"markdown","a34a3036":"markdown","d92c6d21":"markdown","ba0ffd7e":"markdown","07cf2881":"markdown","e0232aba":"markdown","d03b8ffe":"markdown","f402e4b4":"markdown","31baad5f":"markdown"},"source":{"37a3d11c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import text #text processing\nfrom sklearn.feature_extraction.text import CountVectorizer #spliting and numeric representation (Bag-of-words\/n-grams)\nfrom sklearn.feature_extraction.text import TfidfTransformer #calculating word importance score (TF\/IDF)","c199f05d":"#Import a Scikit-learn dataset - 20newsgroups dataset\nfrom sklearn.datasets import fetch_20newsgroups\n\n#There are 20 different groups there, we will select only 4 for the tutorial\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n\n#Download the newsgroup data - training subset (part of the data assigned for training the classifer) - check part 2 if this is confusing\n#Make sure your Internet is on in Kaggle!!!\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n\n#Download the newsgroup data - testing subset (part of the data assigned for testing the performance of the classifer) - check part 2 if this is confusing\ntwenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)","8f06457c":"#Display the length of the data\nprint(\"Number of documents in train:\", len(twenty_train.data))\nprint(\"Number of documents in test:\", len(twenty_test.data))\n\n#Display names of newsgroup categories (our classification labels)\nprint(\"Category names:\", twenty_train.target_names)\n\n#Each document has its category represented as a number\nprint(\"Document label ids:\", twenty_train.target[:10])\n\n#We need to look-up the category name for the category number for each documents to get the actual newsgroup name as text\nprint(\"Category names for the first few documents:\")\nfor t in twenty_train.target[:10]:\n    print(twenty_train.target_names[t])\n\n#Let's look at first few lines of some documents from the data\nprint(\"\\n--- Example document ---\")\nprint(\"\\n\".join(twenty_train.data[2000].split(\"\\n\")[:15]))","75a8a2cd":"#The object type for the dataset is: 'sklearn.utils.Bunch'\nprint(\"Type of the data object used:\", type(twenty_train)) \n\n#Turn the training subset of the data into a dataframe\ntrain_df = pd.DataFrame(data=np.c_[twenty_train.data, [twenty_train.target_names[t] for t in twenty_train.target]], \n                             columns=['text','label'])\ndisplay(train_df)","3993a332":"#Turn the testing subset of the data into a dataframe\ntest_df = pd.DataFrame(data=np.c_[twenty_test.data, [twenty_test.target_names[t] for t in twenty_test.target]], \n                             columns=['text','label'])\ndisplay(test_df)","db69ad60":"import nltk # NLP processing toolkit\nfrom nltk.stem.porter import * # Importing Porter Stemming from NLTK\nfrom nltk import word_tokenize # Importing Tokenizer from NLTK\nimport spacy # An NLP library for text processing\n\n#Create own tokenization (method of splitting text into individual word tokens)\nclass SpacyTokenizer:\n    def __init__(self):\n        self.sp = spacy.load('en_core_web_sm') #Load english data\n    def __call__(self, doc):\n        tokens = self.sp(doc) #Tokenize the sentence (doc)  - essentially split into words\n        return [t.lemma_ for t in tokens] #Look throgh the tokens and get their lemmatized representation (essentially normalized)\n    \n#Porter Stemming (chops off word endings to normalize representation, check the link above)\nclass PorterTokenizer:\n    def __init__(self):\n        self.stemmer = PorterStemmer() #Create Porter Stemmer object\n    def __call__(self, doc):\n        tokens = word_tokenize(doc) #Tokenize the sentence (doc) - essentially split into words\n        return [self.stemmer.stem(t) for t in tokens]  #Loop through the tokens and apply Porter Stemming","6242e16b":"from sklearn.pipeline import Pipeline #load the scikit-learn Pipeline module\n\n#Load some classifiers\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.svm import LinearSVC\n\n#Create the data processing pipeline - this does not process the data yet, just defines the processing steps we will execute later\nclf_pipe = Pipeline([\n    ('vect',  #Step 1: Split sentences into phrases and replace the phrases with numeric ids - check parts 1,2 \n         CountVectorizer(\n             ngram_range=(1,2), #how long sequences of words are we considering (1-only individual words)\n             stop_words=text.ENGLISH_STOP_WORDS #what common words do we remove in processing, check: text.ENGLISH_STOP_WORDS\n             #tokenize=PorterTokenizer() #uncomment to use own tokenizer (cell above)\n         )\n    ),\n    ('tfidf', TfidfTransformer()), #Step 2: Calculate word importance using TF\/IDF, emphasize unique words - check part 2\n    ('clf', LinearSVC()), #Step 3: An ML classifer to use\n])\n\n#print(text.ENGLISH_STOP_WORDS) #show english stop words","ab64499b":"#Pipeline based processing and training a classifer combined\n#Inputs are: \n#         sentencs as text - train_df['text'].values\n#         correct labels for traininf - train_df['label'].values\nclf_pipe.fit(train_df['text'].values, train_df['label'].values) ","34295c11":"cv = clf_pipe.named_steps['vect'] # get the object for pipeline step under 'vect' - CountVectorizer\ntfidf = clf_pipe.named_steps['tfidf'] # get the object for pipeline step under 'tfidf' - TfidfTransformer\n\nvocab = cv.get_feature_names() # get the vocabulary - list of phrases found in the data\nprint(\"Number of phrases in vocabulary:\", len(vocab)) # show how many there are\nprint(\"Id assigned to a phrase:\",cv.vocabulary_.get(u'algorithm')) # get the id assigned to a particular phrase\nprint(\"Show first few phrases in the vocabulary:\",vocab[20000:20050])\n\nsentence = 'You need to love others and be benevolent'\nbow = cv.transform([sentence]) # Transform an exacmple sentence to list of ids\nprint(\"\\nSentence text:\", sentence)\nprint(\"Sentence vector size:\",bow.shape) # display the size of the numeric representation of this sentence\ntf = tfidf.transform(bow) # calculate word importance (TF\/IDF) based representation of the sentence\n\n#display the non-zero (extracted) phrases for the above sentence\nfor w_idx in bow.nonzero()[1]:\n    print(vocab[w_idx],\"->\",bow.toarray()[0,w_idx],\"->\",tf.toarray()[0,w_idx])","3bbcdcdf":"#Let's try to classifier some examples\ndocs_new = ['God is love', 'OpenGL on the GPU is fast', \"These test results are worrysome\", 'graphic cards are expensive', \n           \"let's check the diagnosis\", 'how did the surgery go'] # list of sentences to classify\npredicted = clf_pipe.predict(docs_new) # predicting the labels (category) for our exemple sentences\n\n#Printing sentences and the predicted labels\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, category))","cd5cf919":"from sklearn import metrics  #classification evaluation metrics: accuracy score, confusion matrix, etc.\n\n#Predicting labels for all test subset documents (we loaded this at the beginning)\npredicted = clf_pipe.predict(test_df['text'].values)\n\n#Calculating the accuracy on the test subset\nprint(\"Mean accuracy on test data:\", metrics.accuracy_score(test_df['label'].values, predicted)) \n\n#Calculateing some more advances metrics for evaluation\nprint(\"More advanced metrics:\")\nprint(metrics.classification_report(test_df['label'].values, predicted))","2960cb3b":"#Calculating the confusion-matrix - allows us to see which categories are hard to distinguish\nmetrics.confusion_matrix(test_df['label'].values, predicted)","5f76d909":"#We can also plot it\nmetrics.plot_confusion_matrix(clf_pipe, #our classification pipline from above\n                    test_df['text'].values, #text sentences from test set\n                    test_df['label'].values, #correct labels from test set\n                    cmap=plt.cm.Blues, #color scheme\n                    normalize='pred', #whether to normalize everything (make it add up to 1.00), None if no normalization\n                    values_format = '.2f') #how to display the values (float\/integer), if not normalized counts use: 'd'\nplt.show()","1c941927":"## Converting everything to the familiar DataFrame format\nConverting 'sklearn.utils.Bunch' to DataFrame, based on google search: https:\/\/stackoverflow.com\/questions\/38105539\/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset","408fd3ad":"# Text Classification - part 3\n### Text classification on a large document set\nBased on: https:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html\n<ul>\n<li> Step 1: Loading the dataset\n<li> Step 2: Processing the data\n<li> Step 3: Training a text classifier\n<li> Step 4: Testing our classfier\n<\/ul>\n\n","71fdce56":"# Step 3: Training a text classifer","1ff995e9":"## Examples of accessing individual pipeline steps if needed","a34a3036":"## Exploring the data","d92c6d21":"# Step 2: Processing the data\nNLTK - https:\/\/www.nltk.org\/  <br \/>\nSpacy, Tokenization, Lemmatization & Porter Stemming - https:\/\/stackabuse.com\/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library\/","ba0ffd7e":"## Evaluating on the test portion of the dataset (data we have not trained on)\n* Overview of metrics in Scikit-learn documentation: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html \n* Classification report Scikit-learn documentation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html \n* Precission-Recall explained:\nhttps:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c\n* F1 score explained: https:\/\/deepai.org\/machine-learning-glossary-and-terms\/f-score \n\n<ul>\n    <li><b>Precision<\/b> - out of all the documents we categorized as being about e.g., 'atheism', what percentage was trully (correct label) about atheism? Precision of 0.96 means that 4% of the articles we predicted as being about atheism, were about something else)\n    <li><b>Recall<\/b> - out of all the articles about, e.g., 'atheism' in present in the dataset, what percentage we were able to correctly identify? Recall of 0.83 means that we missed (were not able to identify) 17% of the articles about atheism present in the data (they were predicted as being about something else)\n<\/ul>","07cf2881":"### Plotting confusion matrix:\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.plot_confusion_matrix.html\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html","e0232aba":"# Step 4: Testing our text classfier","d03b8ffe":"### Streamlining processing with a pipeline\nMore on pipelines: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline <br \/>\nMore on classifiers: https:\/\/stackabuse.com\/overview-of-classification-methods-in-python-with-scikit-learn\/\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html","f402e4b4":"# Step 1: Loading the dataset\nThe dataset is called \u201cTwenty Newsgroups\u201d. Here is the official description, quoted from the website:\n\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper \u201cNewsweeder: Learning to filter netnews,\u201d though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.","31baad5f":"Confusion matrix explained: \n* https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62\n* https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/\n* https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/"}}