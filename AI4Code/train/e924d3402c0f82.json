{"cell_type":{"658c91d4":"code","4aa28e3a":"code","c503c732":"code","a86bcb4e":"code","6c16c275":"code","42f71c78":"code","e11bc438":"code","8bdad457":"code","c6910c57":"code","fcb10855":"code","eacbe645":"code","0aab3fdb":"code","bd16e7be":"code","c7c3fb70":"code","e48e532c":"code","2f0a95cb":"code","ef519a64":"code","7170f2de":"code","155eb094":"code","d3f1863b":"code","8164d0fd":"code","277d34d8":"code","fd16682e":"code","90d91d16":"code","87bf5573":"code","0d1f0dcd":"code","00fe63a0":"code","5f4db671":"code","77f10bd5":"code","9ed6ac19":"code","40cd9731":"code","87903b77":"code","322a3225":"code","5450a44b":"code","7bd92369":"code","40fc5ce3":"code","19020d0a":"code","db482131":"code","23bcc2c3":"code","7fbbc9f4":"code","23fbb03b":"code","acc98124":"code","5f9d016b":"code","08e87823":"code","cff8e93f":"code","a53c0bfa":"code","a1c2c0e2":"code","d014b73d":"code","db907adf":"code","2f7e1405":"code","ee556368":"code","1392782f":"code","50136305":"code","98e6bea9":"code","4646697e":"code","84b738b6":"code","28e86772":"code","6135094c":"code","0b3e1cac":"code","1400c1c9":"code","15791e9b":"code","4756c7d9":"code","471b7228":"markdown","82e4a3d8":"markdown","83f0f3c4":"markdown","03dda0a1":"markdown","e7734e5d":"markdown","979807dc":"markdown","657b6cb2":"markdown","fda5d128":"markdown","fac344a3":"markdown","10fa1d72":"markdown","8e890f0c":"markdown","f860353f":"markdown","5cce4914":"markdown","b416b7d0":"markdown"},"source":{"658c91d4":"!wget -O - https:\/\/kheafield.com\/code\/kenlm.tar.gz |tar xz\n!mkdir kenlm\/build","4aa28e3a":"%cd kenlm\/build\n!cmake ..\n!make -j2","c503c732":"!conda install -y gdown","a86bcb4e":"!gdown --id 1Rtaqxq_ahaocqva2ar7bIcyt39zmU8ZQ #gigaword 3\/gigaword3_nyt_eng_2000.tar.gz","6c16c275":"!unzip \"\/kaggle\/working\/kenlm\/build\/gigaword3_nyt_eng_2000 (extract.me).zip\"","42f71c78":"ls","e11bc438":"!pip install https:\/\/github.com\/kpu\/kenlm\/archive\/master.zip\n","8bdad457":"!bin\/lmplz -o 6 --text nyt_eng_200012 --arpa sixgram.arpa","c6910c57":"!bin\/lmplz -o 6 --text nyt_eng_200001 --arpa sixgram.arpa","fcb10855":"ls","eacbe645":"import kenlm\nmodel = kenlm.Model('sixgram.arpa')","0aab3fdb":"import math\ndef print_score(model, s):\n  tokens = s.split(' ')\n  log_score = 0.0\n  for i, (logprob, length, oov) in enumerate(model.full_scores(s)):\n    #if i < len(tokens):\n      #print(tokens[i], math.exp(logprob), oov)\n    #else:\n      #print('END', math.exp(logprob), oov)\n  \n    log_score += logprob\n  return log_score","bd16e7be":"print_score(model, 'I look forward to meeting you')","c7c3fb70":"print_score(model, 'I look forward to meet you')","e48e532c":"import pandas as pd","2f0a95cb":"\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger') \nnltk.download('tagsets')\nfrom nltk.tokenize import word_tokenize\nsentence = \"My name is Jocelyn\"\ntoken = nltk.word_tokenize(sentence)\ntoken\n","ef519a64":"nltk.pos_tag(token)","7170f2de":"#test some tagsets and create lists\narticle = ['the', 'an', 'a']\nv_be = ['is', 'am', 'are', 'were', 'was', \"isn't\", \"ain't\", \"weren't\", \"wasn't\"]\ndo = ['do','does','did',\"don't\",\"doesn't\",\"didn't\"]\n","155eb094":"#noun --> noun.json \u0e14\u0e39\u0e1e\u0e2b\u0e39\u0e1e\u0e08\u0e19\u0e4c\n#prep.json\nprep = [\n    \"aboard\",\n    \"about\",\n    \"above\",\n    \"absent\",\n    \"across\",\n    \"after\",\n    \"against\",\n    \"along\",\n    \"alongside\",\n    \"amid\",\n    \"amidst\",\n    \"among\",\n    \"amongst\",\n    \"around\",\n    \"as\",\n    \"astride\",\n    \"at\",\n    \"atop\",\n    \"before\",\n    \"afore\",\n    \"behind\",\n    \"below\",\n    \"beneath\",\n    \"beside\",\n    \"besides\",\n    \"between\",\n    \"beyond\",\n    \"by\",\n    \"circa\",\n    \"despite\",\n    \"down\",\n    \"during\",\n    \"except\",\n    \"for\",\n    \"from\",\n    \"in\",\n    \"inside\",\n    \"into\",\n    \"less\",\n    \"like\",\n    \"minus\",\n    \"near\",\n    \"nearer\",\n    \"nearest\",\n    \"notwithstanding\",\n    \"of\",\n    \"off\",\n    \"on\",\n    \"onto\",\n    \"opposite\",\n    \"outside\",\n    \"over\",\n    \"past\",\n    \"per\",\n    \"save\",\n    \"since\",\n    \"through\",\n    \"throughout\",\n    \"to\",\n    \"toward\",\n    \"towards\",\n    \"under\",\n    \"underneath\",\n    \"until\",\n    \"up\",\n    \"upon\",\n    \"upside\",\n    \"versus\",\n    \"via\",\n    \"with\",\n    \"within\",\n    \"without\",\n    \"worth\",\n    \"according to\",\n    \"adjacent to\",\n    \"ahead of\",\n    \"apart from\",\n    \"as of\",\n    \"as per\",\n    \"as regards\",\n    \"aside from\",\n    \"astern of\",\n    \"back to\",\n    \"because of\",\n    \"close to\",\n    \"due to\",\n    \"except for\",\n    \"far from\",\n    \"inside of\",\n    \"instead of\",\n    \"left of\",\n    \"near to\",\n    \"next to\",\n    \"opposite of\",\n    \"opposite to\",\n    \"out from\",\n    \"out of\",\n    \"outside of\",\n    \"owing to\",\n    \"prior to\",\n    \"pursuant to\",\n    \"rather than\",\n    \"regardless of\",\n    \"right of\",\n    \"subsequent to\",\n    \"such as\",\n    \"thanks to\",\n    \"up to\",\n    \"as far as\",\n    \"as opposed to\",\n    \"as soon as\",\n    \"as well as\",\n    \"at the behest of\",\n    \"by means of\",\n    \"by virtue of\",\n    \"for the sake of\",\n    \"in accordance with\",\n    \"in addition to\",\n    \"in case of\",\n    \"in front of\",\n    \"in lieu of\",\n    \"in place of\",\n    \"in point of\",\n    \"in spite of\",\n    \"on account of\",\n    \"on behalf of\",\n    \"on top of\",\n    \"with regard to\",\n    \"with respect to\",\n    \"with a view to\"\n]\n","d3f1863b":"# noun prep verb p'no\n!git clone https:\/\/github.com\/nozomiyamada\/contest2","8164d0fd":"import json\nwith open('contest2\/verbs-dictionaries.json') as f:\n  verb = json.load(f)\n#verb","277d34d8":"f","fd16682e":"with open('contest2\/noun.json') as f:\n  noun = json.load(f)\n#noun","90d91d16":"def article_candi(sentence):\n  a_candi = []\n  no_candi = []\n  if sentence != None:\n    tokens = sentence.split(' ')\n    for i, (t) in enumerate(tokens):\n      if tokens[i] in article:\n        for a in article:\n          a_candi.append(tokens[0:i]+[a]+tokens[i+1:])\n  if a_candi == []:\n    s = nltk.word_tokenize(sentence)\n    no_candi.append(s)\n    return no_candi\n  else:\n    return a_candi","87bf5573":"a2 = article_candi('I love an kid') # in list\na2","0d1f0dcd":"a1= article_candi('I love this kid') #not in list\na1\n","00fe63a0":"#prototype for every pos finished!!!\ndef find_best_article(sentence):\n  scd = {}\n  all_cand = article_candi(sentence)\n  all_cand\n  s_cand = [' '.join(word) for word in all_cand]\n  for i in s_cand:\n    scores = print_score(model,i)\n    scd[i] = scores\n  max_key = max(scd, key=scd.get, default=None)\n  return max_key","5f4db671":"fa_1 = find_best_article('I love an dog')\nfa_1","77f10bd5":"fa_2 = find_best_article('I lave a dog')\nfa_2","9ed6ac19":"#prototype of func for lst of lst <works!!!>\ndef noun_candi(sentence): #noun is a list of lists\n  a_candi = []\n  no_candi = []\n  if sentence != None:\n    tokens = sentence.split(' ')\n    for i, (t) in enumerate(tokens):\n      for forms in noun:\n        if tokens[i] in forms:\n          for word in forms:\n            c = tokens[0:i]+[word]+tokens[i+1:] #\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19 list \u0e0b\u0e49\u0e33\n            if c not in a_candi:\n              a_candi.append(c)\n  if a_candi == []:\n    s = nltk.word_tokenize(sentence)\n    no_candi.append(s)\n    return no_candi\n  else:\n    return a_candi    \n          \n  return a_candi","40cd9731":"n_cand = noun_candi('accountants is smart')\nn_cand","87903b77":"n2 = noun_candi('Tu is dumb')\nn2","322a3225":"nc2 = noun_candi('your academics performance is excellent')\nnc2","5450a44b":"#prototype for every pos finished!!!\ndef find_best_noun(sentence):\n  scd = {}\n  all_cand = noun_candi(sentence)\n  all_cand\n  s_cand = [' '.join(word) for word in all_cand]\n  for i in s_cand:\n    scores = print_score(model,i)\n    scd[i] = scores\n  max_key = max(scd, key=scd.get, default=None)\n  return max_key","7bd92369":"eg = 'your academics performance is excellent' #but accountants is smart won = incorrect\nbfn = find_best_noun(eg)\nbfn","40fc5ce3":"def verb_candi(sentence): #noun is a list of lists\n  a_candi = []\n  no_candi = []\n  if sentence != None:\n    tokens = sentence.split(' ')\n    for i, (t) in enumerate(tokens):\n      for forms in verb:\n        if tokens[i] in forms:\n          for word in forms:\n            c = tokens[0:i]+[word]+tokens[i+1:] #\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19 list \u0e0b\u0e49\u0e33\n            if c not in a_candi:\n              a_candi.append(c)\n  if a_candi == []:\n    s = nltk.word_tokenize(sentence)\n    no_candi.append(s)\n    return no_candi\n  else:\n    return a_candi   \n          \n  return a_candi","19020d0a":"v1 = verb_candi('She swaggedd')\nv1\n","db482131":"def find_best_verb(sentence):\n  scd = {}\n  all_cand = verb_candi(sentence)\n  all_cand\n  s_cand = [' '.join(word) for word in all_cand]\n  for i in s_cand:\n    scores = print_score(model,i)\n    scd[i] = scores\n  max_key = max(scd, key=scd.get, default=None)\n  return max_key","23bcc2c3":"bfv = find_best_verb('I loves you')\nbfv","7fbbc9f4":"be = 'He is the one'\nvbb = find_best_verb(be)\nvbb","23fbb03b":"def prep_candi(sentence):\n  a_candi = []\n  no_candi = []\n  if sentence != None:\n    tokens = sentence.split(' ')\n    for i, (t) in enumerate(tokens):\n      if tokens[i] in prep:\n        for a in prep:\n          a_candi.append(tokens[0:i]+[a]+tokens[i+1:])\n  if a_candi == []:\n    s = nltk.word_tokenize(sentence)\n    no_candi.append(s)\n    return no_candi\n  else:\n    return a_candi\n      \n  return a_candi","acc98124":"p1 = prep_candi('I fall into love')\np1","5f9d016b":"p2 = prep_candi('I fall im love')\np2","08e87823":"def find_best_prep(sentence):\n  scd = {}\n  all_cand = prep_candi(sentence)\n  all_cand\n  s_cand = [' '.join(word) for word in all_cand]\n  for i in s_cand:\n    scores = print_score(model,i)\n    scd[i] = scores\n  max_key = max(scd, key=scd.get, default=None)\n  return max_key","cff8e93f":"s = 'We are afraid about climate change'\nbfp = find_best_prep(s)\nbfp","a53c0bfa":"p = 'I look down to you'\npp = find_best_prep(p)\npp","a1c2c0e2":"def tok_pos(max_key):\n  ans = nltk.word_tokenize(max_key)\n  answer = nltk.pos_tag(ans)\n  return answer\n#max_key = 'This is the fish'\n#answer = tok_pos(max_key)\n#print(answer)\n#>>>[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('fish', 'NN')]","d014b73d":"def find_best(sentence):\n  s1 = find_best_article(sentence)\n  s2 = find_best_noun(s1)\n  s3 = find_best_verb(s2)\n  s4 = find_best_prep(s3)\n  return s4","db907adf":"sen = 'I loves to ate a apple'\nfin = find_best(sen)\nfin","2f7e1405":"two = 'I loves to eat a apple'\nans_2 = find_best(two)\nans_2","ee556368":"q = 'I eats a chicken'\na = find_best(q)\na","1392782f":"def break_into_sen(lst):\n sentence_s =  ' '.join(word for word in lst)\n return sentence_s\n","50136305":"f = find_best('However , leading cancer specialists reviewed that animal test results do not necessarily apply to humans ( Lewan , 2007 ) .')\nprint(f)","98e6bea9":"g = 'she knew who I am'\nh = find_best(g)\nh","4646697e":"one = find_best('Humans have many basic needs and one of them is to have an environment that can sustain their lives .')\none","84b738b6":"!gdown --id 1CHnRNybDYbq9xTZNCxf22PIQIyBe0Yfz","28e86772":"!rm -r contest2\n!git clone https:\/\/github.com\/nozomiyamada\/contest2","6135094c":"with open('\/kaggle\/working\/kenlm\/build\/contest2\/dev_small.txt','r') as f: #encoding=\"utf-8\", errors='ignore') as f: #remove \u2018content\/\u2019 from path then use \n  lines = f.readlines()\n  dev_lst =[]\n  for line in lines:\n    fi = find_best(line)\n    dev_lst.append(fi)","0b3e1cac":"ans_dev = '\\n'\nans_dev = ans_dev.join(dev_lst)\nprint(ans_dev)","1400c1c9":"with open('\/kaggle\/working\/kenlm\/build\/contest2\/dev_small.txt') as f:\n  dev = f.readlines()\n  last = dev[-1]\n  with open('\/kaggle\/working\/ansdev_2.txt', 'w') as d:\n    for line in dev:\n      line = line.strip()\n      ans = find_best(line)\n      #last line\n      if ans == last:\n        d.write(ans)\n      else:\n        d.write(ans+'\\n')\n\n      ","15791e9b":"!wc '\/kaggle\/working\/kenlm\/build\/contest2\/dev_small.txt'\n!wc '\/kaggle\/working\/ansdev_2.txt'","4756c7d9":"!python \/kaggle\/working\/kenlm\/build\/contest2\/m2scorer\/scripts\/m2scorer.py \/kaggle\/working\/\/ansdev_2.txt \/kaggle\/working\/kenlm\/build\/contest2\/dev_small_answer.txt","471b7228":"# Train LM with KenLM","82e4a3d8":"**Verb func \u0e23\u0e27\u0e21\u0e23\u0e48\u0e32\u0e07**","83f0f3c4":"## Evaluate","03dda0a1":"# Install KenLM Library","e7734e5d":"# Download Data","979807dc":"# Grammatical Error Correction\n> My 2nd Contest Assigment from Introduction to Computational Linguistics Course\n> Chulalongkorn University","657b6cb2":"# **Functions for every** **pos**","fda5d128":"# **Test with bea19-sentences.txt (incorrect text)**\ndata = bea--","fac344a3":"**Article func ver \u0e23\u0e27\u0e21\u0e23\u0e48\u0e32\u0e07 = \u0e44\u0e14\u0e49\u0e15\u0e31\u0e27\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e2b\u0e31\u0e27\u0e02\u0e49\u0e2d\u0e19\u0e35\u0e49**","10fa1d72":"# Pos tag","8e890f0c":"**\u0e4c\u0e4cNoun** **\u0e23\u0e27\u0e21\u0e23\u0e48\u0e32\u0e07 **","f860353f":"!bin\/lmplz -o 5 --text 1b.txt --arpa fivegram.arpa","5cce4914":"# Install KenLM in Python","b416b7d0":"## **find_best \u0e23\u0e27\u0e21\u0e23\u0e48\u0e32\u0e07\u0e40\u0e23\u0e35\u0e22\u0e07\u0e25\u0e33\u0e14\u0e31\u0e1a** *article* >* noun* > *verb* > *prep*"}}