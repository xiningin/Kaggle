{"cell_type":{"49323aa1":"code","8429aa41":"code","110c3699":"code","53bc1283":"code","1d6678ff":"code","f9f6d00b":"code","5f7a45e0":"code","d70320b9":"code","38ac139e":"code","7f8f6234":"code","ff52908b":"code","512f2f80":"code","c48e2dd3":"code","729dd3dc":"code","60f59bf6":"code","2a768d23":"code","6b85eae4":"code","5952d970":"code","999ad7f2":"code","487ad3e2":"code","b2c7bea9":"code","2898a453":"code","ec7f7564":"code","2ebcc5b4":"code","20864d9d":"code","28c1f69b":"code","abb45b4e":"code","a1fa3351":"markdown","142c7130":"markdown","78bf60b7":"markdown","470ede00":"markdown","b55caacd":"markdown","81100f82":"markdown","25116a98":"markdown","13af157c":"markdown","032ec372":"markdown","887fca86":"markdown","79281ea1":"markdown","33a1025c":"markdown","0639b8fe":"markdown","22c0c450":"markdown","9a082209":"markdown","5d0d1330":"markdown","7e48f568":"markdown","fd711a2f":"markdown","4d376ff8":"markdown","fea0533d":"markdown","2a9191db":"markdown","5792454e":"markdown","838bf4a7":"markdown","2e86b889":"markdown"},"source":{"49323aa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8429aa41":"from sklearn.model_selection import train_test_split\n\ndef load():\n    \n    Train = pd.read_csv (r'\/kaggle\/input\/titanic\/train.csv')\n    test = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')\n    example_output= pd.read_csv(r'\/kaggle\/input\/titanic\/gender_submission.csv')\n\n    x_=Train\n    X =x_.drop(['Survived'], axis=1)\n    Y= Train['Survived']\n    x_train, x_test, y_train, y_test= train_test_split(X,Y,test_size=0.2,random_state=42)\n\n    train= x_train\n    train['Survived']= y_train\n    return x_train, x_test, y_train, y_test, train\n    \nx_train, x_test, y_train, y_test, train= load()\nplot_data=train","110c3699":"train.tail()\n","53bc1283":" train.isnull().sum()","1d6678ff":"def nan_cabin(train):\n    train[['Cabin']]=train[['Cabin']].replace(np.nan, 'H777')\n    \n        #Creating a new feature is a person has a cabin or not.\n    train.loc[train['Cabin']=='H777','Has_Cabin']=0\n    train.loc[train['Cabin']!='H777','Has_Cabin']=1\n    return train","f9f6d00b":"def nan_embarked(train):\n    train[['Embarked']]=train[['Embarked']].replace(np.nan, 'S')\n    return train\n    ","5f7a45e0":"def process_ticket(train):\n    train['Ticket_prefix']= train['Ticket'].str.split(\" \", n = 1, expand = True)[0] \n    train.loc[(train.Ticket == train.Ticket_prefix),'Ticket_prefix']='no-prefix'\n    train['Ticket'].loc[(train['Ticket_prefix']!= 'no-prefix' )] = train['Ticket'].str.split(\" \", n = 1, expand = True)[1]\n    train['Ticket']=pd.to_numeric(train['Ticket'], errors='coerce')\n    train['Ticket'] = train['Ticket'].replace(np.nan,0)\n    train['Ticket_3dig']= train['Ticket']%1000\n    return train","d70320b9":"def process_cabin(train):\n    train['Cabin_alpha'] = train['Cabin'].str[0]\n    train[['Cabin_alpha']]=train[['Cabin_alpha']].replace(np.nan, 'X') \n    train['Cabin_num'] = train ['Cabin'].str.replace(r'\\D', '')\n    train['Cabin_num']=pd.to_numeric(train['Cabin_num'], errors='coerce')\n    train['Cabin_num'] = train['Cabin_num'].replace(np.nan,0)\n    train['Cabin_num'] = train['Cabin_num']\n    return train\n    ","38ac139e":"def creat_titles(train):\n    train['Title'] = train['Name'].str.extract(r' ([A-Za-z]+)\\.')\n    train['Surname']=train['Name'].str.extract(r'([A-Za-z]+)\\,')\n    title=['Mrs','Mr','Miss','Master']\n    train.loc[~train[\"Title\"].isin(title), \"Title\"] = \"Rare\"\n    return train","7f8f6234":"def titles_to_fill_age(train):\n    age_mean={}\n    \n    Mrs_age=train.loc[train['Title']=='Mrs'].Age.mean()\n    train.loc[train['Title']=='Mrs']=train.loc[train['Title']=='Mrs'].replace(np.nan,Mrs_age)\n    age_mean['Mrs_age']= Mrs_age \n    \n    Mr_age=train.loc[train['Title']=='Mr'].Age.mean()\n    train.loc[train['Title']=='Mr']=train.loc[train['Title']=='Mr'].replace(np.nan,Mr_age)\n    age_mean['Mr_age']=Mr_age\n    \n    Miss_age=train.loc[train['Title']=='Miss'].Age.mean()\n    train.loc[train['Title']=='Miss']=train.loc[train['Title']=='Miss'].replace(np.nan,Miss_age)\n    age_mean['Miss_age']=Miss_age\n    \n    Master_age=train.loc[train['Title']=='Master'].Age.mean()\n    train.loc[train['Title']=='Master']=train.loc[train['Title']=='Master'].replace(np.nan,Master_age)\n    age_mean['Master_age']=Master_age\n    \n    Rare_age=train.loc[train['Title']=='Rare'].Age.mean()\n    train.loc[train['Title']=='Rare']=train.loc[train['Title']=='Rare'].replace(np.nan,Rare_age)\n    age_mean['Rare_age']=Rare_age\n    \n    return age_mean, train\n    ","ff52908b":"def catagorical_age_fare(train):\n    \n    cut_labels_age = ['Children', 'Teens', 'youngadults','adults', 'elderly']\n    cut_bins = [-1,10,20,30,40, 200]\n    train['CategoricalAge'] = pd.cut(train['Age'], bins=cut_bins, labels=cut_labels_age)\n\n    \n    cut_labels_fare = ['cheap', 'mid','slighly expensive', 'expensive','VeryExpensive']\n    cut_bins = [-100,10,50,100,500,10000]\n    train['CategoricalFare'] = pd.cut(train['Fare'], bins=cut_bins, labels=cut_labels_fare)\n    return train\n    ","512f2f80":"def process_family(train):\n    train['FamilySize']= train['SibSp']+train['Parch']\n    train['Alone']= 0\n    train['Alone'].loc[train['FamilySize']==0]=1\n    return train\n    ","c48e2dd3":"def embarked_sex(train):\n    train['Sex_Embarked']=train['Sex'].astype(str)+train['Embarked'].astype(str)\n    return train","729dd3dc":"from sklearn.preprocessing import LabelEncoder\nimport category_encoders as ce\nimport re\nx_train, x_test, y_train, y_test, train= load()\ndef preprocessing_train(train):\n    encoder={}\n    age_mean={}\n    #handling nan values in cabin and embarked\n    train = nan_cabin(train)\n    train= nan_embarked(train)\n    #Label encoding Sex\n    train= embarked_sex(train)\n    sexemb_encoder = LabelEncoder()\n    sexemb_encoder.fit(train[['Sex_Embarked']])\n    train['Sex_Embarked'] = train[['Sex_Embarked']].apply(sexemb_encoder.transform)\n    encoder['Sex_Embarked']=sexemb_encoder\n    \n    sex_encoder = LabelEncoder()\n    sex_encoder.fit(train[['Sex']])\n    train['Sex'] = train[['Sex']].apply(sex_encoder.transform)\n    encoder['Sex']=sex_encoder\n    #Target encoding Ticket_prefix\n    train= process_ticket(train)\n    ticket_enc = LabelEncoder()\n    ticket_enc.fit(train[['Ticket_prefix']])\n    train['Ticket_prefix'] = ticket_enc.transform(train[['Ticket_prefix']])\n    encoder['Ticket_prefix']=ticket_enc\n    #Catboost encoding Cabin_alpha\n    train=process_cabin(train)\n    cabin_aplha_encoder = LabelEncoder()\n    cabin_aplha_encoder.fit(train[['Cabin_alpha']])\n    train['Cabin_alpha'] = cabin_aplha_encoder.transform(train[['Cabin_alpha']])\n    encoder['Cabin_alpha']=cabin_aplha_encoder\n    #Catboost encoding Embarked\n    embarked_encoder = LabelEncoder()\n    embarked_encoder.fit(train[['Embarked']])\n    train['Embarked'] = embarked_encoder.transform(train[['Embarked']])\n    encoder['Embarked']=embarked_encoder\n    #Fetching Titles\n    train=creat_titles(train)\n    #Filling Nan values in age based on title\n    age_mean, train = titles_to_fill_age(train)\n    #TargetEncoding Title \n    enc_title = LabelEncoder()\n    enc_title.fit(train[['Title']])\n    train['Title'] = enc_title.transform(train[['Title']])\n    encoder['Title']=enc_title\n    \n    enc_surname = LabelEncoder()\n    enc_surname.fit(train[['Surname']])\n    train['Surname'] = enc_surname.transform(train[['Surname']])\n    encoder['Surname']=enc_surname\n    #Encoding catagorical age and Catagorical Fare\n    train= catagorical_age_fare(train)\n    enc_FareCat = LabelEncoder()\n    enc_FareCat.fit(train[['CategoricalFare']])\n    train['CategoricalFare'] = enc_FareCat.transform(train[['CategoricalFare']])\n    train['CategoricalFare'].astype(str)\n    encoder['CategoricalFare']=enc_FareCat\n    enc_AgeCat = LabelEncoder()\n    enc_AgeCat.fit(train[['CategoricalAge']])\n    train['CategoricalAge'] = enc_AgeCat.transform(train[['CategoricalAge']])\n    encoder['CategoricalAge']=enc_AgeCat\n    #Creating new feature called family size and is Alone\n    train= process_family(train)\n    #Drop columns that are not needed \n    train = train.drop(['Name','Cabin'], axis=1) # will eb used for plotting graphs\n    x_train= train.drop(['Survived'],axis=1) # for training\n    return train, x_train, encoder, age_mean\n\n\n\nencoder={}\ntrain, x_train,encoder, age_mean= preprocessing_train(train)\n\n#print(\"TEST\",x_test.head(2))","60f59bf6":"x_train.head()","2a768d23":"def titles_to_fill_age_test(age_mean,x_test):\n    if('Mrs' in x_test['Title'].unique()):\n        x_test.loc[x_test['Title']=='Mrs']=x_test.loc[x_test['Title']=='Mrs'].replace(np.nan,age_mean['Mrs_age'])\n   \n    if('Mr' in x_test['Title'].unique()):\n        x_test.loc[x_test['Title']=='Mr']=x_test.loc[x_test['Title']=='Mr'].replace(np.nan,age_mean['Mr_age'])\n    \n    if('Miss' in x_test['Title'].unique()):\n        x_test.loc[x_test['Title']=='Miss']=x_test.loc[x_test['Title']=='Miss'].replace(np.nan,age_mean['Miss_age'])\n    \n    if('Master' in x_test['Title'].unique()):\n        x_test.loc[x_test['Title']=='Master']=x_test.loc[x_test['Title']=='Master'].replace(np.nan,age_mean['Master_age'])\n    \n    if('Rare' in x_test['Title'].unique()):\n        x_test.loc[x_test['Title']=='Rare']=x_test.loc[x_test['Title']=='Rare'].replace(np.nan,age_mean['Rare_age'])\n    return x_test","6b85eae4":"def Preprocess_val_or_test(x_test,encoder,age_mean):\n    \n    x_test = nan_cabin(x_test)\n    x_test= nan_embarked(x_test)\n    x_test= embarked_sex(x_test)\n    x_test['Sex_Embarked'] = x_test[['Sex_Embarked']].apply(encoder['Sex_Embarked'].transform)\n    #encoding sex\n    x_test['Sex'] = x_test[['Sex']].apply(encoder['Sex'].transform)\n    #encoding ticket prefix\n    x_test= process_ticket(x_test)\n    x_test.loc[~x_test['Ticket_prefix'].isin(encoder['Ticket_prefix'].classes_), 'Ticket_prefix']= 'no-prefix'\n    x_test['Ticket_prefix'] = encoder['Ticket_prefix'].transform(x_test[['Ticket_prefix']])\n    #encoding cabin alpha\n    x_test=process_cabin(x_test)\n    x_test['Cabin_alpha'] = encoder['Cabin_alpha'].transform(x_test[['Cabin_alpha']])\n    #encoding enbarked\n    x_test['Embarked'] = encoder['Embarked'].transform(x_test[['Embarked']])\n    #creating titles and filling age using mean value of titles\n    x_test=creat_titles(x_test)\n    x_test= titles_to_fill_age_test(age_mean,x_test)\n    #encoding titles\n    x_test['Title'] = encoder['Title'].transform(x_test[['Title']])\n    #Creating catagorical fares and age\n    x_test= catagorical_age_fare(x_test)\n    x_test['CategoricalFare'] = encoder['CategoricalFare'].transform(x_test[['CategoricalFare']])\n    x_test['CategoricalAge'] = encoder['CategoricalAge'].transform(x_test[['CategoricalAge']])\n    #Creating family and is alone features \n    x_test= process_family(x_test)\n    x_test = x_test.drop(['Name','Cabin'], axis=1)\n    return x_test\n\nx_test= Preprocess_val_or_test(x_test,encoder,age_mean)\nx_test.head()\n    ","5952d970":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(train['Survived'], train['Sex'])\ncm_df= pd.DataFrame(cm, index=['Not Survived','Survived'])\ncm_df.columns=['female','male']\nprint(cm_df)\nsns.set_style(\"white\")\nsns.heatmap(data=cm_df, annot=True,label=['Survived','Sex'],fmt='d')\n\n\n\nplt.figure(figsize=(9,6))\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"CategoricalAge\", kind=\"bar\", data=train)\nplt.title(\"Survival Vs Sex, for different age groups\")\n\nunique_cat=np.sort(train['CategoricalAge'].unique())\nprint(unique_cat, ' are mapped to ',encoder['CategoricalAge'].inverse_transform(unique_cat),'respectively ')\n\nplt.legend() \nplt.show()\n\n","999ad7f2":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(train['Survived'], train['Pclass'])\ncm_df= pd.DataFrame(cm).iloc[0:2,:]\ncm_df=  cm_df.drop([0],axis=1)\ncm_df.columns=['1','2','3']\ncm_df.index=['Not Survived', 'Survived']\nprint(cm_df)\nsns.set_style(\"white\")\nsns.heatmap(data=cm_df, annot=True,label=['Survived','Sex'],fmt='d')\nplt.title(\"Survival vs class\")\n\ntrain_= train.sort_values('CategoricalFare')\nplt.figure(figsize=(9,6))\nsns.catplot(x=\"CategoricalFare\", y=\"Survived\", kind=\"bar\", data=train)\nplt.title(\"Survival Vs CatagoricalFare\")\n\nunique_cat=np.sort(train['CategoricalFare'].unique())\nprint(unique_cat, ' are mapped to ',encoder['CategoricalFare'].inverse_transform(unique_cat),'respectively ')\n\nplt.legend() \nplt.show()\n\n","487ad3e2":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(train['Survived'], train['Ticket_prefix'])\ncm_df= pd.DataFrame(cm).iloc[0:2,:]\n#cm_df=  cm_df.drop([0],axis=1)\ncm_df.columns=encoder['Ticket_prefix'].inverse_transform( cm_df.columns)\ncm_df.index=['Not Survived', 'Survived']\nprint(cm_df)\nsns.set_style(\"white\")\nplt.figure(figsize=(12,4))\nsns.heatmap(data=cm_df, annot=True,label=['Survived','Sex'],fmt='d')\nplt.title(\"Survival vs class\")","b2c7bea9":"\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,6))\n\ncm = confusion_matrix(train['Survived'], train['SibSp'])\ncm_df= pd.DataFrame(cm).iloc[0:2,:]\ncm_df.index=['Not Survived', 'Survived']\ncm_df.columns=train['SibSp'].unique()\nsns.heatmap(data=cm_df, annot=True, fmt='d', ax=ax1)\nax1.set_title(\"Survived Vs Number of Siblings\")\nprint(cm_df, '\\n')\n\n\ncm_p = confusion_matrix(train['Survived'], train['Parch'])\ncmp_df= pd.DataFrame(cm_p).iloc[0:2,:]\ncmp_df.index=['Not Survived', 'Survived']\n\nprint(cmp_df)\n\nsns.heatmap(data=cmp_df, annot=True, fmt='d',ax= ax2)\nax2.set_title(\"Survived Vs Parents + Children\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.catplot(x=\"FamilySize\", y=\"Survived\", kind=\"bar\", data=train)\nplt.title(\"Survival Vs Famiy size\")\n\nplt.figure(figsize=(6,4))\nsns.catplot(x=\"Alone\", y=\"Survived\", kind=\"bar\", data=train)\nplt.title(\"Survival Vs Famiy size\")\n","2898a453":"\nimport seaborn as sns\ncm = confusion_matrix(train['Survived'], train['Embarked'])\ncm_df= pd.DataFrame(cm).iloc[0:2,:]\ncm_df.index=['Not Survived', 'Survived']\ncm_df.columns=encoder['Embarked'].inverse_transform( cm_df.columns)\nsns.heatmap(data=cm_df, annot=True, fmt='d')\n#fig.set_title(\"Survived Vs Number of Siblings\")\nprint(cm_df, '\\n')","ec7f7564":"import seaborn as sns\ncm = confusion_matrix(train['Survived'], train['Cabin_alpha'])\ncm_df= pd.DataFrame(cm).iloc[0:2,:]\ncm_df.index=['Not Survived', 'Survived']\ncm_df.columns=encoder['Cabin_alpha'].classes_\nsns.heatmap(data=cm_df, annot=True, fmt='d')\n#fig.set_title(\"Survived Vs Number of Siblings\")\nprint(cm_df, '\\n')\nplt.figure(figsize=(9,6))\nsns.catplot(x=\"Cabin_alpha\", y=\"Survived\", kind=\"bar\", data=train)\nplt.title(\"Survival Vs Famiy size\")","2ebcc5b4":"from sklearn.feature_selection import SelectKBest, chi2\n\ndef feature_selection(x_train,y_train,x_test):\n    #86= 18,17,16\n    #85= 15, 14, 13 12, 11, 10,\n    #84=9\n    selector = SelectKBest(chi2, k=15)\n\n    X_new = selector.fit_transform(x_train, y_train)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=train.index, \n                                     columns=x_train.columns)\n    \n    x_train_i = selected_features.columns[selected_features.var() != 0]\n    x_test_i = x_test.columns[selected_features.var() != 0]\n\n    \n    \n    return x_train_i, x_test_i\n\n    ","20864d9d":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense\nfrom xgboost import XGBClassifier\nfrom category_encoders import TargetEncoder\n\n\n\n\n\ndef xgb_tunned(x_train, y_train, x_test, y_test, m_depth,gamma_,reg_alpha_,min_child_weight_):\n    #print(\"--- SVC ---\")\n    model = XGBClassifier(n_estimators= 2000,\n                          max_depth= m_depth,\n                          min_child_weight= min_child_weight_,\n                          gamma=gamma_,  \n                          reg_alpha = reg_alpha_,\n                          subsample=0.8,\n                          colsample_bytree=0.8,\n                          objective= 'binary:logistic',\n                          nthread= -1,\n                          scale_pos_weight=1)\n    model.fit(x_train, y_train)\n    predictions= model.predict(x_test)\n    return predictions\n\ndef xgb_(x_train, y_train, x_test, y_test):\n    model = XGBClassifier()\n    model.fit(x_train, y_train)\n    predictions= model.predict(x_test)\n    test_acc = accuracy_score(y_test, predictions)\n    train_predictions = model.predict(x_train)\n    train_acc = accuracy_score(y_train,train_predictions)\n    return train_acc,test_acc, predictions\n\ndef xgb(x_train, y_train, x_test):\n    model = XGBClassifier()\n    model.fit(x_train, y_train)\n    predictions= model.predict(x_test)\n    return predictions\n\n\ndef svm(x_train, y_train, x_test, y_test,ker,c,g):\n    model= SVC(kernel=ker,C=c, gamma=g)\n    model.fit(x_train,y_train)\n    predictions= model.predict(x_test)\n    test_acc = accuracy_score(y_test,predictions)\n    train_predictions = model.predict(x_train)\n    train_acc = accuracy_score(y_train,train_predictions)\n    return  train_acc, test_acc, predictions\n    \n    \ndef knn(x_train, y_train, x_test, y_test):\n    #print(\"--- KNN ---\")\n    model= KNeighborsClassifier()\n    model.fit(x_train,y_train)\n    predictions = model.predict(x_test)\n    test_acc = accuracy_score(y_test,predictions)\n    #print(\"val\\t:\",accuracy_score(y_test,predictions))\n    train_predictions = model.predict(x_train)\n    train_acc = accuracy_score(y_train,train_predictions)\n    #print(\"train\\t:\",accuracy_score(y_train,train_predictions))\n    return train_acc, test_acc, predictions\n\ndef rf(x_train, y_train, x_test, y_test):\n    #print(\"--- Random Forest ---\")\n    model= RandomForestClassifier()\n    model.fit(x_train,y_train)\n    predictions = model.predict(x_test)\n    test_acc = accuracy_score(y_test,predictions)\n    train_predictions = model.predict(x_train)\n    train_acc = accuracy_score(y_train,train_predictions)\n    return  train_acc, test_acc, predictions\n\ndef lr(x_train, y_train, x_test, y_test):\n    #print(\"--- Logistic Regression ---\")\n    model= LogisticRegression(max_iter=700)\n    model.fit(x_train,y_train)\n    predictions = model.predict(x_test)\n    test_acc = accuracy_score(y_test,predictions)\n    #print(\"val\\t:\",accuracy_score(y_test,predictions))\n    train_predictions = model.predict(x_train)\n    train_acc = accuracy_score(y_train,train_predictions)\n    #print(\"train\\t:\",accuracy_score(y_train,train_predictions))\n    \n    coeff_df = pd.DataFrame(x_train.columns.delete(0))\n    coeff_df.columns = ['Feature']\n    coeff_df[\"Correlation\"] = pd.Series(model.coef_[0])\n\n    coeff_df=coeff_df.sort_values(by='Correlation', ascending=False)\n    print(coeff_df)\n    return train_acc,test_acc,  predictions\n\n\nx_train, x_test, y_train, y_test, train= load()\nencoder={}\ntrain, x_train,encoder, mean_age= preprocessing_train(train)\nx_test= Preprocess_val_or_test(x_test,encoder,mean_age)\nx_train=x_train.drop(['PassengerId','Surname'],axis=1)\nx_test=x_test.drop(['PassengerId','Surname'],axis=1)\ntr_acc,tes_acc,preds=xgb_(x_train, y_train, x_test, y_test)\nprint('Train accuracy: ',tr_acc,'Val Acuracy: ',tes_acc)","28c1f69b":"result= x_test\nresult['Ground_Truth']=y_test\nresult['Predicted']= preds\n(result.loc[result['Ground_Truth']!=result['Predicted']]).head()","abb45b4e":"test = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')\nx_train, x_test, y_train, y_test, train= load()\ntrain, x_train,encoder, mean_age= preprocessing_train(train)\ntest= Preprocess_val_or_test(test,encoder,age_mean)\nprint(test.head())\npred= pd.DataFrame()\npred['PassengerId']=test['PassengerId']\n\nx_train=x_train.drop(['PassengerId','Surname'],axis=1)\ntest=test.drop(['PassengerId','Surname'],axis=1)\npred['Survived'] = pd.DataFrame(xgb(x_train, y_train, test))\nprint(pred.head())\npred.to_csv('mycsvfile.csv',index=False)\n# ","a1fa3351":"**Impact of Class and Fare on survival**\n\n* Class 1 had the highest number of survivers\n* Class 3 had the highest number of non survivor\n* No particular relationship between class 2 and survival\n* It is evident as the price of the tickets increase, the liklihood of survival also increased.\n\n\n","142c7130":"**Selecting k best features**\nTo select features, I tried to use SelectKBest from skelearn, since my model gave better results (on test) using all the features, I decided to not use it.\n","78bf60b7":"Only 'Age', 'Cabin' and 'Enbarked' have null values in them.\n* Age\n    - Age definitely is an important feature. \n    - Why? We know women and children were removed from the ship first. There could be an important relationship between the age and survival\n    - How? Since about 18% of the vaue are null, other values can be used to fill in the null values. \n    - https:\/\/www.kaggle.com\/imoore\/titanic-the-only-notebook-you-need-to-see , contains a very interesting correlation between survived and the title. \n    - Fetched the title from the name ['Mrs', 'Mr', 'Master', 'Miss', 'Rare'], are the categories that I used. Took an average of the ages according to the title and filled in null values. \n\n\n","470ede00":"**Impact of having Siblings  and parents+ childeren around**\n* there is definitely a positive impact of family size on survival\n* If a person was alone he was more likely to die\n","b55caacd":"Creating Bins for Age and Fare.","81100f82":"# Null Values Analysis","25116a98":"There seems to exist a correaltion between sex and embark point acc to https:\/\/www.kaggle.com\/imoore\/multiple-classification-models-work-in-progress","13af157c":"# Loading Data","032ec372":"* Embarked \n     - Contains very few null values, we can easily handle them by giving them the most common value.","887fca86":"There may not be a very clear realtionship between the cabin alphabets and survival, but keeping this feature improved the model score.","79281ea1":"As mentioned above the titles have been used to get a mean of passengers age. For example, passengers with tittle Mrs, had an average age of 35. So the missed values have been filed accordingly.","33a1025c":"* Ticket\n    - Ticket contains a prefix followed by a number. \n    - Assuming that the prefix is categorising passengers, it looks like it can be useful.\n    - Fetched out the prefix in a new feature called the 'Ticket_Prefix'.\n    - Now that the 'Ticket' only has numbers, created a new feature containing only three digits of the ticket, hoping to capture some relationship between the ticket the persons placement on the ship. \n    - Lastly the entire ticket only as number is kept for now, and while coercing as numbers if it is nan, it has been filled with zeros. \n    - Have also kept the last three digits of the ticket, jus to check if it contains any useful information about the passengers position on the ship.","0639b8fe":"* Cabin\n    - Cabin if of the form 'D50'.\n    - The letter from the cabin number has been used to create a new feature called 'Cabin_alpha'.\n    - The remaing number has been used to create a new feature called the 'Cabin_num'.\n    ","22c0c450":"Adding a new feature to capture the information about family size, and to see whether he was travelling alone. \n","9a082209":"Impact of Embarkation point on Survival\n* Person embarning his journey from point S was more likley to die\n* Person embarking his journey from point C was slighlty more likely to survive\n","5d0d1330":"For Manual analysis","7e48f568":"* Cabin \n    - Cabin contains around 70% null values. \n    - What we can fetch out of the available information we yet have to see. \n    - Why? There could be a relationship between the available cabins and survival.\n    - Have assigned a random value 'H777' to those entries where cabin information is not present.\n    - Also created a new feature called 'Has_Cabin'. ","fd711a2f":"Impact of prefix on survival\n* As visible in the graph, we cannot get a lot of information from prefix, however we can investigate if having a prefix or not having a prefix will help us or not.","4d376ff8":"# Fitting the Model\nTrying Mutiple Models\n- SVM\n- KNN\n- Random Forest\n- Logistic Regression\n- xgb with parameter Tuning\n- xgb with default parameters (since it gave the best accuracy, it was choosen)\n","fea0533d":"# Feature Analysis","2a9191db":"After preprocessing, this is what train looks like. ","5792454e":"**Hypothesis:**\n - Gender Vs Survival: Who survived more men or women \n - Class Vs survival : Any impact of class on survival\n - Fare  Vs Survival : any impact of fare on survival\n - Parch and Psi     :Impact of having famiy with them\n - Age Vs Survival   : Which age group survived which did not\n - Ticket            : is ticket unique, how can it be used?\n - Relationship between embarkation point and survival\n - Cabin alphabet vs survival\n \n ","838bf4a7":"# Data Preprocessing\nOriginal Features \n* Id : OK\n* Pclass : OK\n* Name: dropped, Processed to form new fearures \n* Sex : label encoded\n* Age : OK, processed to form new features \n* Sibsp : OK, processed to form new features\n* Parch : OK, processed to form new features\n* Ticket : Processed to form more features\n* Fare : OK, processed to from more features \n* Cabin : dropped for now\n* Embarked : Target Encoded\n\nNotes :\n- One hot encoding or label encodig : One hot encoding should be used when there is no realtionship between te categories, for example Male or Female, Countries etc. Label Encoding must be used when a relationship between them exists. For example good or bad or some kind of scoring mechanism. If we use Label encoding in an example between conunties, then Say Ind = 2, Japan = 1 and our model will end up learning a relationship that 2 > 1, so it can draw correlations like if the country number increases the population increases. \n\n\n","2e86b889":"**Impact of Gender and Age on Survival**\n\n* There is a corrleation between female and survived.  most Women Survived and  most Men Did not survive\n* The categorical ages are sorted into follwoing bins =  ['Children' 'Teens' 'adults' 'elderly' 'youngadults']\n* The catplot as well we can see that women survived more. \n* In case of women as the age grows the liklihood of survival also grows.\n* Whereas in case of men, as the age inceased the liklihood of seuvival descreased.\n\n"}}