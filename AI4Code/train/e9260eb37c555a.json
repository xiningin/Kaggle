{"cell_type":{"9ec736af":"code","c3dfd96a":"code","eae6f41f":"code","49b453d4":"code","95d758b9":"code","c5db9924":"code","7bea8282":"code","26428444":"code","6650b3f1":"code","c3cb8b82":"code","0f493118":"code","9ca1c177":"code","0dab4d55":"code","e41194da":"code","b191e7e6":"code","609dec9d":"code","0c40a8cb":"code","9fd76d9b":"code","78f41083":"code","c5d5f9f5":"code","a4a2c086":"code","b35dfea8":"code","6ccce174":"code","f59fc133":"code","a9d1527b":"code","93ff79ee":"code","cda1c598":"code","09366ce7":"code","37d560bb":"code","cc6eac1a":"code","9d18b2a1":"code","19452a4e":"code","88415048":"code","e8f21a09":"code","1814400a":"code","d2cba890":"code","b64110c5":"code","3e4b1fca":"code","cab82398":"code","2a25d2f1":"code","36c1e468":"code","914c292f":"code","80692c2d":"code","fe6eb422":"code","446e4639":"code","bce1d1f9":"code","06537b1e":"code","52755f65":"markdown","b90c0b08":"markdown","d299e25f":"markdown","ed213f23":"markdown","9ff1af2d":"markdown","290d8948":"markdown","8c646d9c":"markdown","e11db207":"markdown","4abb24f9":"markdown","d317343b":"markdown","8f0f755a":"markdown","9e3b9cfc":"markdown","b6af9ffe":"markdown","677fca15":"markdown"},"source":{"9ec736af":"import sys\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\n\nfrom torchvision.models import vgg16_bn","c3dfd96a":"prefix = '98'","eae6f41f":"path = Path('.')\npath_hr = path\/'hr' # low res images\npath_lr = path\/'lr' # high res images\n\n\npath_test = Path('.\/test') # images for prediction\npath_tmp = Path('.\/tmp')\nexport_path = Path('\/export')","49b453d4":"img_f = get_image_files(path_lr);\nfor i in range(20):\n  img = open_image(img_f[i])\n  print(img.shape)","95d758b9":"img","c5db9924":"src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)","7bea8282":"tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0)","26428444":"def get_data(bs,size):\n    data = (src.label_from_func(lambda x: path_hr\/x.name)\n           .transform(tfms,size=size, tfm_y=True)\n           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n\n    data.c = 3\n    return data","6650b3f1":"bs,size=32,128\narch = models.resnet34","c3cb8b82":"data = get_data(bs,size)","0f493118":"data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))","9ca1c177":"t = data.valid_ds[0][1].data\nt = torch.stack([t,t])","0dab4d55":"def gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))\/(c*h*w)","e41194da":"gram_matrix(t)","b191e7e6":"base_loss = F.l1_loss","609dec9d":"vgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)","0c40a8cb":"blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\nblocks, [vgg_m[i] for i in blocks]","9fd76d9b":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        out_feat = self.make_features(target, clone=True)\n        in_feat = self.make_features(input)\n        self.feat_losses = [base_loss(input,target)]\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","78f41083":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","c5d5f9f5":"wd = 1e-3\nlearn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,\n                     blur=True, norm_type=NormType.Weight)\ngc.collect();","a4a2c086":"learn.lr_find()\nlearn.recorder.plot()","b35dfea8":"lr = 1e-3","6ccce174":"def do_fit(save_name, lrs=slice(lr), pct_start=0.9):\n    save_name = f'{prefix}_{save_name}'\n    learn.fit_one_cycle(1, lrs, pct_start=pct_start)\n    learn.save(export_path\/save_name)\n    learn.show_results(rows=4)","f59fc133":"do_fit('1a', slice(lr*10))","a9d1527b":"learn.show_results(rows=5, imgsize=5)","93ff79ee":"learn.unfreeze()","cda1c598":"do_fit('1b', slice(1e-5,lr))","09366ce7":"data = get_data(12,size*2)","37d560bb":"learn.data = data\nlearn.freeze()\ngc.collect()","cc6eac1a":"learn.show_results()","9d18b2a1":"learn.load(export_path\/f'{prefix}_1b');","19452a4e":"do_fit('2a')","88415048":"learn.unfreeze()","e8f21a09":"do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)","1814400a":"learn.show_results(rows=15)","d2cba890":"learn = None\ngc.collect();","b64110c5":"size = 500\narch = models.resnet34","3e4b1fca":"#Good transfroms for orto\ntfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0)","cab82398":"test_data = (ImageImageList.from_folder(path_test).split_none().label_from_func(lambda x: x)\n      .transform(tfms, size=size, tfm_y=True)\n      .databunch(bs=1, no_check=True).normalize(imagenet_stats, do_y=True))","2a25d2f1":"learn = unet_learner(test_data, arch, loss_func=F.l1_loss, blur=True, norm_type=NormType.Weight)","36c1e468":"learn = learn.load(export_path\/f'{prefix}_2b');","914c292f":"img_path = test_data.train_ds.x.items[0]\nimg = open_image(img_path); img.shape","80692c2d":"show_image(img, figsize=(9,9)); img.shape","fe6eb422":"preds = learn.predict(img)","446e4639":"pred_img = preds[0]\npred_img; pred_img.shape","bce1d1f9":"def export_images(fnames, size):\n  import shutil\n  import zipfile\n  from datetime import datetime\n  now = datetime.now()\n  timestamp = now.strftime('%Y%m%d_%H%M%z')\n\n  with zipfile.ZipFile(export_path\/f'predicted_{timestamp}.zip', 'w') as f:\n    for img_f in fnames:\n      img = open_image(img_f)\n      pred = learn.predict(img)\n      export_img = pred[0].resize(size)\n      filename = f'{img_f.stem}.png'\n      print(f'exporting {filename}...')\n      export_img.save(path_tmp\/filename)\n      shutil.copy(path_test\/f'{img_f.stem}.wld', path_tmp \/ f'{img_f.stem}.wld')\n      f.write(path_tmp\/filename)\n      f.write(path_tmp\/f'{img_f.stem}.wld')","06537b1e":"test_img_f = get_image_files(path_test)\nexport_images(test_img_f, 500)","52755f65":"# Feature loss function","b90c0b08":"# Intro","d299e25f":"# Data","ed213f23":"## Batch export function","9ff1af2d":"# Create the databunch","290d8948":"This notebook was used to make a super resolution version of old aerial images. Instead of a GAN it make use of a Perceptual\/Feature loss function as described in [this paper by Johnson et al](https:\/\/arxiv.org\/abs\/1603.08155). This notebook is based on the one used in the fast.ai course-v3, lesson 7 about super resolutions: https:\/\/course.fast.ai\/\n\n\nInput:\n\"lr\" -> tiles extracted from the old aerial imagery (1998), 500x500px, .png-files\n\"hr\" -> tiles extracted from a much newer aerial imagery (2017), 500x500px, .png-files\n\nThe input and target images cover the exact same area on the ground.","8c646d9c":"Check the data","e11db207":"Databunch for testdata","4abb24f9":"Augmentations suitable for ortophotos","d317343b":"Load the latest saved model","8f0f755a":"# Train","9e3b9cfc":"# Inference","b6af9ffe":"# Setup","677fca15":"Paths setup"}}