{"cell_type":{"40be06e4":"code","488bb096":"code","1844dc9c":"code","63ac4e4a":"code","d54d0e6f":"code","3554a37d":"code","def22f09":"code","6c69519a":"code","2f3fbede":"code","db752d31":"code","65ff9439":"code","1428be69":"code","ff3f34b6":"code","a11e9f4b":"code","9e2ade1c":"code","0bf4b1b0":"code","680b3f12":"code","574b787a":"code","228ecc85":"code","10eff1ca":"code","29652749":"code","2619287f":"code","5e2472e1":"code","d14b60cc":"code","3dc76854":"code","d57d1b2a":"code","95d5c12a":"code","a5d94800":"code","3d1f5447":"code","d66533a5":"code","ed9624dd":"code","201e79ec":"markdown","626e4fee":"markdown","9e9c5fb3":"markdown","869909b0":"markdown","e11dd966":"markdown","556dcbc7":"markdown","9db3e68f":"markdown","a2070e14":"markdown","f80cda54":"markdown","810accd4":"markdown","c939bba3":"markdown","0cfb0c63":"markdown","60f49b82":"markdown","233e2de5":"markdown","9da0a3d8":"markdown","7bbfc0e6":"markdown","ef0451d1":"markdown","8253dfe8":"markdown"},"source":{"40be06e4":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom tensorflow.keras.layers import Embedding,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM,Bidirectional,GRU\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB","488bb096":"# Reading data from csv\ntrain = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/fake-news\/test.csv\")\ntrain.head()","1844dc9c":"test.head()","63ac4e4a":"# Displaying rows and columns in dataset\nprint(\"There are {} number of rows and {} number of columns for training.\".format(train.shape[0],train.shape[1]))\nprint(\"There are {} number of rows and {} number of columns for testing.\".format(test.shape[0],test.shape[1]))","d54d0e6f":"# Checking the null values in training data.\ntrain.isnull().sum()","3554a37d":"# Checking the null values in testing data.\ntest.isnull().sum()","def22f09":"# Handling nan values in dataset using empty spaces\ndef handle_nan(train_data,test_data):\n    '''Input: Data to the function containing Nan values.\n       Output : Cleaned data containing no Nan values.\n       Function: Cleaning Nan values.\n     '''\n    train = train_data.fillna(\" \")\n    test  = test_data.fillna(\" \")\n    return train,test\n\ntrain,test = handle_nan(train,test)\n","6c69519a":"# Creating a variable \"merged\" by merging columns \"title\" and \"author\"\ntrain[\"merged\"] = train[\"title\"]+\" \"+train[\"author\"]\ntest[\"merged\"]  = test[\"title\"]+\" \"+test[\"author\"]","2f3fbede":"# Seperating Independent and dependent features\nX = train.drop(columns=['label'],axis=1)\ny = train['label']","db752d31":"# Creating One-Hot Representations\nmessages = X.copy()\nmessages.reset_index(inplace=True)\nmessages_test = test.copy()\nmessages_test.reset_index(inplace=True)","65ff9439":"# Performing data preprocessing on column 'title'\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ndef perform_preprocess(data):\n    '''Input: Data to be processed\n       Output: Preprocessed data\n    '''\n    corpus = []\n    for i in range(0,len(data)):\n        review = re.sub('[^a-zA-Z]',' ',data['merged'][i])\n        review = review.lower()\n        review = review.split()\n        review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n        review = ' '.join(review)\n        corpus.append(review)\n    return corpus\n    \ntrain_corpus = perform_preprocess(messages)\ntest_corpus  = perform_preprocess(messages_test)\ntrain_corpus[1]","1428be69":"test_corpus[1]","ff3f34b6":"# Converting to one-hot repr.\nvocab_size = 5000\none_hot_train = [one_hot(word,vocab_size) for word in train_corpus]\none_hot_test  = [one_hot(word,vocab_size) for word in test_corpus]","a11e9f4b":"one_hot_test[1]","9e2ade1c":"# Embedding Representation \nsent_length = 20\nembedd_docs_train = pad_sequences(one_hot_train,padding='pre',maxlen=sent_length)\nembedd_docs_test  = pad_sequences(one_hot_test,padding='pre',maxlen=sent_length)\nprint(embedd_docs_train)","0bf4b1b0":"print(embedd_docs_test)","680b3f12":"# Converting Embedding repr. to array\nx_final = np.array(embedd_docs_train)\ny_final = np.array(y)\nx_test_final = np.array(embedd_docs_test)","574b787a":"# Dimensions of prev. array repr.\nx_final.shape,y_final.shape,x_test_final.shape","228ecc85":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42, stratify = y_final)\nX_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\nx_test_final = x_test_final","10eff1ca":"model_1 = LogisticRegression(max_iter=900)\nmodel_1.fit(X_train,Y_train)\npred_1 = model_1.predict(x_test)\ncr1    = classification_report(y_test,pred_1)\nprint(cr1)","29652749":"model_2 = MultinomialNB()\nmodel_2.fit(X_train,Y_train)\npred_2 = model_2.predict(x_test)\ncr2    = classification_report(y_test,pred_2)\nprint(cr2)","2619287f":"model_3 = DecisionTreeClassifier()\nmodel_3.fit(X_train,Y_train)\npred_3 = model_3.predict(x_test)\ncr3    = classification_report(y_test,pred_3)\nprint(cr3)","5e2472e1":"model_4 = RandomForestClassifier()\nmodel_4.fit(X_train,Y_train)\npred_4 = model_4.predict(x_test)\ncr4    = classification_report(y_test,pred_4)\nprint(cr4)","d14b60cc":"model_5 = XGBClassifier()\nmodel_5.fit(X_train,Y_train)\npred_5 = model_5.predict(x_test)\ncr5    = classification_report(y_test,pred_5)\nprint(cr5)","3dc76854":"model_6 = CatBoostClassifier(iterations=200)\nmodel_6.fit(X_train,Y_train)\npred_6 = model_5.predict(x_test)\ncr6    = classification_report(y_test,pred_5)\nprint(cr6)","d57d1b2a":"# Creating the LSTM Model for prediction\nembedding_feature_vector = 40\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,embedding_feature_vector,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","95d5c12a":"# Training the model\nmodel.fit(X_train,Y_train,validation_data=(x_valid,y_valid),epochs=10,batch_size=64)","a5d94800":"predictions = model.predict_classes(x_test)\ncr = classification_report(y_test,predictions)\nprint(cr)","3d1f5447":"score_1 = accuracy_score(y_test,pred_1)\nscore_2 = accuracy_score(y_test,pred_2)\nscore_3 = accuracy_score(y_test,pred_3)\nscore_4 = accuracy_score(y_test,pred_4)\nscore_5 = accuracy_score(y_test,pred_5)\nscore_6 = accuracy_score(y_test,pred_6)\nscore_7 = accuracy_score(y_test,predictions)\nresults = pd.DataFrame([[\"Logistic Regression\",score_1],[\"Naive Bayes\",score_2],[\"Decision Tree\",score_3],\n                       [\"Random Forest\",score_4],[\"XGBOOST\",score_5],[\"CatBoost\",score_6],[\"LSTM\",score_7]],columns=[\"Model\",\"Accuracy\"])\nresults","d66533a5":"# Making Predictions on test data\npredictions_test = pd.DataFrame(model.predict_classes(x_test_final))\ntest_id = pd.DataFrame(test[\"id\"])\nsubmission = pd.concat([test_id,predictions_test],axis=1)\nsubmission.columns = [\"id\",\"label\"]\nsubmission.to_csv(\"Submission.csv\",index=False)","ed9624dd":"submission.head()","201e79ec":"**Predictions on Testing Data**","626e4fee":"**Below code converts the pre-processed words to one-hot vectors in the range of vocabulary size=5000. This is done to obtain numerical feature matrix**","9e9c5fb3":"**2. Naive Bayes**","869909b0":"# Creating Models\n**In this phase, several models are created and evaluated against various metrics shown using classification report.**","e11dd966":"# Data Pre-processing\n**In Data Pre-processing following steps are followed:** \n**1. Firstly, all the sequences except english characters are removed from the string.**\n**2. Next, to avoid false predictions or ambiguity with upper and lowercase, all the characters in strings are converted    to lowercase.**\n**3. Next, all the sentences are tokenized into words.**\n**4. To facilitate fast processing, stemming is applied to the tokenized words.**\n**5. Next, words are joined together and stored in the corpus.**\n\n**Note: In this tutorial, we have used \"merged\" column for classification task. Also, the loop inside the function runs over all the examples in the merged column.**","556dcbc7":"**Discussion: From the above results, it appears that LSTM Model gives the highest accuracy amongst various models. Therefore, it is selected as the final model for making predictions on final testing data.**","9db3e68f":"**5. XGBOOST**","a2070e14":"**3. Decision Trees**","f80cda54":"**Checking Null Values**","810accd4":"**Below code creates an embedding layer which applies \"pre\" padding to the one-hot encoded features with sentence length = 20. Padding is applied so that the length of every sequence in the dataset should be same.**","c939bba3":"**Tabulating the results of various implemented models.**","0cfb0c63":"**6.Catboost**","60f49b82":"**This is a NLP problem where the task is to classify Fake News in an article. This notebook consists of various stages needed for identifying fake news such as data preprocessing, model experimentation, and evaluation of results.**","233e2de5":"**Dividing the dataset into training,validation and testing data (ratio: 80\/10\/10) using train_test_split technique.**","9da0a3d8":"# Evaluation of Models","7bbfc0e6":"**4. Random Forest**","ef0451d1":"**1. Logistic Regresssion**","8253dfe8":"**7. LSTM**\n\n**In this model, 1.) The value for embedding feature vectors = 40 which are target feature vectors for the embedding layer. 2.) Single LSTM Layer with 100 nodes are used. 3.)Dense Layer with 1 neuron and sigmoid activation function is used since, this is a binary classification problem. 4) Dropout technique is used to avoid overfiiting and adam optimizer is used for optimizing the loss function.**"}}