{"cell_type":{"acde0ce9":"code","dd755d00":"code","b38e117d":"code","11a4bee9":"code","b9aaf3ef":"code","354d5793":"code","6894a21e":"code","21386d05":"code","46555b11":"code","53d4d575":"code","6d2109d5":"code","86df6e75":"code","11809458":"code","afd7df6b":"code","d2da7340":"code","fb5846b2":"code","8763e655":"code","8d061edc":"code","269054c0":"code","a3c73641":"code","c019bf6e":"markdown","a7b6020f":"markdown","ec399105":"markdown","8aad441a":"markdown","493b01df":"markdown","130169ae":"markdown","eb11f10b":"markdown","67be1e07":"markdown","5c814dca":"markdown","78d50e47":"markdown","12ab55e1":"markdown","15d589b6":"markdown","8899150d":"markdown","6a87a1a3":"markdown","b1668481":"markdown","2462b8a8":"markdown","bbb9ee51":"markdown","97619d53":"markdown","ef01820a":"markdown","3699b8f6":"markdown","11bfa93d":"markdown","dac611a6":"markdown","e46bfd14":"markdown","f3d331ea":"markdown","2eea4f64":"markdown","6e5a538f":"markdown"},"source":{"acde0ce9":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random","dd755d00":"admissions = pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\")\nadmissions = admissions.drop('Serial No.',axis = 1)","b38e117d":"admissions.head()","11a4bee9":"admissions.describe()","b9aaf3ef":"# Basic correlogram\nsns.pairplot(admissions)","354d5793":"corr = admissions.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","6894a21e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.metrics import mean_absolute_error","21386d05":"X = admissions.drop('Chance of Admit ',axis = 1)\ny = admissions['Chance of Admit ']\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size = .25,random_state = 123)","46555b11":"lin_model = LinearRegression()","53d4d575":"lin_model.fit(X_train,y_train)","6d2109d5":"print('Mean absolute error for linear model: %0.4f' %mean_absolute_error(y_val,lin_model.predict(X_val)))","86df6e75":"rf_model = RandomForestRegressor(n_estimators = 1000,random_state = 123)\nrf_model.fit(X_train,y_train)","11809458":"print('Mean absolute error for linear model: %0.4f' %mean_absolute_error(y_val,rf_model.predict(X_val)))","afd7df6b":"feature_importance = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()","d2da7340":"rf_model = RandomForestRegressor(n_jobs = -1,random_state = 123)\nparam_grid = {'n_estimators': [500, 700, 1000, 1200], \n                    'max_depth': [4, 5, 6, 7, 8], \n                    'min_samples_split': [2, 3, 4, 5, 6],\n                    'max_features': [1,2,3,4,5,6,7]}\nrf_grid = GridSearchCV(estimator = rf_model,param_grid = param_grid,\n                       cv = 3,n_jobs = -1)\nrf_grid.fit(X_train,y_train)","fb5846b2":"rf_grid.best_params_","8763e655":"rf_model.set_params(**rf_grid.best_params_)","8d061edc":"rf_model.fit(X_train,y_train)","269054c0":"print('Mean absolute error for linear model: %0.4f' %mean_absolute_error(y_val,rf_model.predict(X_val)))","a3c73641":"feature_importance = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()","c019bf6e":"The problem with this feature importance is the model is still quite naive so the feature importance may be inaccurate. Lets fine tune the model and then see if it changes.","a7b6020f":"Look at an overview of the data","ec399105":"#### Creating data sets","8aad441a":"1. Expand models tested to see which one is best for this case\n2. Begin feature engineering\n3. Expand on tuning of hyper-parameters","493b01df":"### Future Works","130169ae":"#### Feature Importance","eb11f10b":"Read in data from csv and see interesting characteristics","67be1e07":"Quick look at how the data is formatted","5c814dca":"Thank you if you've read over this notebook and feel free to ask any questions or point out any errors. I always enjoy both teaching and learning.","78d50e47":"In conclusion, we see that test scores are the most important with getting into grad school with supplemental material behind and research experience as least important. ","12ab55e1":"Encouraging to see that test scores (GRE, TOEFL, CGPA) have the best relations with chance of being accepted. I guess studying does pay off.","15d589b6":"### Modeling\n\n","8899150d":"# Admissions Prediction\n## Marco Gancitano","6a87a1a3":"### Data Import","b1668481":"### Conclusion","2462b8a8":"#### Linear Regression","bbb9ee51":"Again we see that test scores have the highest correlation with supplemental works being next (SOP, LOR) and research experience being last","97619d53":"We see that this combination creates the best cross validation accuracy.\n- Max depth: 7\n- Max features: 2\n- Min sample split: 5\n- Trees : 500","ef01820a":"With random forest regression being more accurate we'll work with that","3699b8f6":"#### Model Fine Tuning","11bfa93d":"Model is slightly better and we see that it isn't mostly dependent on one variable which helps with robustness","dac611a6":"These are the given variables:\n1. GRE Scores ( out of 340 ) \n    - Graduate Record Examinations\n2. TOEFL Scores ( out of 120 )\n    - Test of English as a Foreign Language \n3. University Rating ( out of 5 ) \n    - Quality Rating of Undergraduate\n4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) \n5. Undergraduate GPA ( out of 10 ) \n6. Research Experience ( either 0 or 1 ) \n7. Chance of Admit ( ranging from 0 to 1 )","e46bfd14":"Next, we look at numeric correlation between variables","f3d331ea":"Import needed packages for the notebook","2eea4f64":"Looking at the importance of features when estimating the chance of admissions. This is another way to see the explanatory power of the given features just like the pairplot and correlation matrix.","6e5a538f":"#### Random Forest Regression"}}