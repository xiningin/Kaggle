{"cell_type":{"bfe80181":"code","096a6766":"code","f4506cc8":"code","60ebfe85":"code","eb8aeb37":"code","47aa6002":"code","9f461938":"code","248ebdcc":"code","2361c58c":"code","da75f001":"code","74f94f19":"code","f63ac0f8":"code","b0c9c41b":"code","459214fd":"code","418b23e4":"code","50c79890":"code","15ce09f1":"code","43b202ae":"code","72b1e9ab":"code","9c30c63d":"code","ce2767de":"code","fd54ed47":"code","b7def9d4":"code","252ae384":"code","cf080a28":"code","6cbc24ae":"code","c15f425c":"code","6ea11411":"code","fef53634":"markdown","7278f8e3":"markdown","61d8d019":"markdown","81de5632":"markdown","e45b3d8b":"markdown","958e3549":"markdown","b7ea097d":"markdown","6be26d4a":"markdown"},"source":{"bfe80181":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","096a6766":"import warnings\nwarnings.filterwarnings(action='ignore') \n\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","f4506cc8":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","60ebfe85":"# train dataframe is shaped (891,12) with \"Survived\" <- label\ntrain.head()","eb8aeb37":"train.shape","47aa6002":"# train dataframe is shaped (418,11) without \"Survived\" <-label\ntest.head()","9f461938":"test.shape","248ebdcc":"train['Name_bin'] = train['Name'].str.extract('( [A-Z]+\\w*)', expand=False).str.strip()","2361c58c":"train['Name_bin'] = np.where(train['Name_bin'].isin(['Mr','Miss','Mrs','Master']), train['Name_bin'], 'somebody')\ntrain['Name_bin'].value_counts()","da75f001":"def plot_bar(dataframe,colname):\n    survived = dataframe[dataframe['Survived']==1][colname].value_counts()\n    dead = dataframe[dataframe['Survived']==0][colname].value_counts()\n    result = pd.DataFrame([survived,dead], index=['Survived','dead'])\n    return result\n\nplt.figure(figsize=(7,6))\nplot_bar(train,'Name_bin').plot(kind='bar')\nplt.show()","74f94f19":"test['Name_bin'] = test['Name'].str.extract('( [A-Z]+\\w*)', expand=False).str.strip()\ntest['Name_bin'] = np.where(test['Name_bin'].isin(['Mr','Miss','Mrs','Master']), test['Name_bin'], 'somebody')","f63ac0f8":"for ls_df in [train, test]:\n    ls_df['Age'] = ls_df['Age'].astype(float)\n    ls_df['Age_median'] = ls_df.groupby(['Name_bin'])['Age'].transform('median')\n    ls_df['Age'] = np.where(ls_df['Age'].isnull(), ls_df['Age_median'], ls_df['Age'])","b0c9c41b":"train['Embarked'].fillna(\"S\", inplace=True)","459214fd":"test['Fare'] = test['Fare'].astype(float)","418b23e4":"test['Fare_median'] = test.groupby(['Pclass'])['Fare'].transform('median')\ntest['Fare'] = np.where(test['Fare'].isnull(), test['Fare_median'], test['Fare'])","50c79890":"bins = [0, 15, 25, 35, 60, 81]  # binning\nbin_names = ['child','young','adult','middle','senior'] # label tagging\ntrain['Age_bin'] = pd.cut(train['Age'],\n                          bins = bins,\n                          labels=bin_names,\n                          include_lowest = True)\ntrain['Age_bin'].value_counts()","15ce09f1":"test['Age_bin'] = pd.cut(test['Age'],\n                          bins = bins,\n                          labels=bin_names,\n                          include_lowest = True)","43b202ae":"train['Family_cnt'] = train['SibSp'].astype(int) + train['Parch'].astype(int)\ntest['Family_cnt'] = test['SibSp'].astype(int) + test['Parch'].astype(int)","72b1e9ab":"feature_list = ['Survived','Pclass','Sex','Age_bin','Family_cnt','Fare','Embarked','Name_bin']","9c30c63d":"sex = {'male':1, 'female':2}\nage_bin = {'child':1, 'young':2,'adult':3, 'middle':4, 'senior':5}\nembarked = {'C':1,'Q':2,'S':3}\nname_bin = {'Mr':1,'Mrs':2,'Miss':3,'Master':4,'somebody':5}","ce2767de":"for item in [train, test]:\n    item.replace({\"Sex\": sex}, inplace=True)\n    item.replace({\"Age_bin\": age_bin}, inplace=True)\n    item.replace({\"Embarked\": embarked}, inplace=True)\n    item.replace({\"Name_bin\": name_bin}, inplace=True)","fd54ed47":"train_data = train[feature_list[1:]].copy()\ntrain_target = train[['Survived']].copy()","b7def9d4":"X_train, X_test, y_train, y_test = train_test_split(train_data, train_target, test_size = 0.2, random_state=40)","252ae384":"clf = SVC()\nfor model in [SVC(),SGDClassifier(),RandomForestClassifier(),DecisionTreeClassifier(),KNeighborsClassifier()]:\n    clf = model\n    clf.fit(X_train, y_train)\n    accuracy = round(clf.score(X_test, y_test) * 100, 2)\n    print(type(model).__name__ + \" : \" + str(accuracy) + \"%\")","cf080a28":"skfolds = StratifiedKFold(n_splits=5, random_state=42) \n\nfor model in [SVC(),SGDClassifier(),RandomForestClassifier(),DecisionTreeClassifier(),KNeighborsClassifier()]:\n    clf = model\n    score = cross_val_score(clf, train_data, train_target, cv=skfolds, scoring='accuracy')\n    avg_score = round(np.mean(score) * 100)\n    print(type(model).__name__ + \" : \" + str(avg_score) + \"%\")","6cbc24ae":"svc_clf = SVC()\nsvc_clf.fit(train_data, train_target)\npredict = svc_clf.predict(test[feature_list[1:]])\nsubmission_data = pd.DataFrame({'PassengerId':test['PassengerId'],\n                            'Survived':predict})\nsubmission_data.to_csv('titanic_submission_SVC.csv', index=False)","c15f425c":"rf_clf = RandomForestClassifier()\nrf_clf.fit(train_data, train_target)\npredict = rf_clf.predict(test[feature_list[1:]])\nsubmission_data = pd.DataFrame({'PassengerId':test['PassengerId'],\n                            'Survived':predict})\nsubmission_data.to_csv('titanic_submission_RandomForestClassifier.csv', index=False)","6ea11411":"# xgboost Classification\n\n\nparams = {\n    'min_child_weight':[1,2,3,4,5,6,7,8,9,10],\n    'gamma':[0.2,0.3,0.5,0.7,1.0,1.5,2,3,4,5],\n    'subsample':[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'colsample_bytree':[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'max_depth':[3,4,5,6,7,8]\n}\n\nxgb_model = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', silent=True, nthread=1)\nfolds = 10\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1100)\nrandom_search = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=param_comb, \n                                   scoring='roc_auc', n_jobs=4, cv=skf.split(X_train, y_train), verbose=3, random_state=1100)\n\nxgb_fit = random_search.fit(X_train, y_train)\n\npredict = xgb_fit.predict(test[feature_list[1:]])\nsubmission_data = pd.DataFrame({'PassengerId':test['PassengerId'],\n                            'Survived':predict})\nsubmission_data.to_csv('titanic_submission_XGBClassifier.csv', index=False)","fef53634":"2. Age column preprocessing ","7278f8e3":"1. Name column preprocessing : categorizing ","61d8d019":"# Feature Engineering","81de5632":"test data is also categorizing...","e45b3d8b":"Predicting with Support Vector Classification","958e3549":"# Data Preparation","b7ea097d":"Predicting with XGBoost Classification","6be26d4a":"Predicting with Random Forest Classification"}}