{"cell_type":{"23d47109":"code","0bd34d52":"code","a6fe4b4f":"code","08f7f545":"code","4bbcf192":"code","c9df7661":"code","ac9f637c":"code","8b523cbe":"code","efda8f7a":"markdown","1cebe5bd":"markdown","5594950e":"markdown","5cf95104":"markdown","e97ba5cf":"markdown","2820efd9":"markdown"},"source":{"23d47109":"# config.py\n\nclass config:\n\n    TRAIN = \"..\/input\/machine-learning-classification\/train.csv\"\n\n    TEST = \"..\/input\/machine-learning-classification\/test.csv\"\n\n    TRAIN_CLEANED = \"train_cleaned.csv\"\n\n    TEST_CLEANED = \"test_cleaned.csv\"\n\n    TRAINING_FILE = \"train_folds.csv\"\n\n    # SAMPLE_SUBMISSION = \"input\/sample.csv\"\n\n    NUM_FOLDS = 10\n\n    MODEL_PATH = \".\/models\/\"","0bd34d52":"# making a folder for models\n\nimport os\n\ntry:\n    os.mkdir(config.MODEL_PATH)\nexcept:\n    pass","a6fe4b4f":"# clean.py\n\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\nif __name__ == \"__main__\":\n\n    train = pd.read_csv(config.TRAIN)\n    test = pd.read_csv(config.TEST)\n\n    # feature engineering\n    train['Variance of the integrated profile'] = train['Standard deviation of the integrated profile'] ** 2\n    test['Variance of the integrated profile'] = test['Standard deviation of the integrated profile'] ** 2\n\n    train['Variance of the DM-SNR curve'] = train['Standard deviation of the DM-SNR curve'] ** 2\n    test['Variance of the DM-SNR curve'] = test['Standard deviation of the DM-SNR curve'] ** 2\n\n    train['mean_diff'] = train['Mean of the integrated profile'] - train['Mean of the DM-SNR curve']\n    train['sd_diff'] = train['Standard deviation of the integrated profile'] - train['Standard deviation of the DM-SNR curve']\n    train['kurtosis_diff'] = train['Excess kurtosis of the integrated profile'] - train['Excess kurtosis of the DM-SNR curve']\n    test['mean_diff'] = test['Mean of the integrated profile'] - test['Mean of the DM-SNR curve']\n    test['sd_diff'] = test['Standard deviation of the integrated profile'] - test['Standard deviation of the DM-SNR curve']\n    test['kurtosis_diff'] = test['Excess kurtosis of the integrated profile'] - test['Excess kurtosis of the DM-SNR curve']\n\n    train['Mean of the integrated profile squared'] = train['Mean of the integrated profile'] ** 2\n    test['Mean of the integrated profile squared'] = train['Mean of the integrated profile'] ** 2\n\n\n    # scaling\n    scaler = preprocessing.StandardScaler()\n    features_to_scale = [col for col in train.columns if col not in (\"id\", \"target_class\")]\n    train_scaled = pd.DataFrame(scaler.fit_transform(train[features_to_scale]), columns=features_to_scale)\n    test_scaled = pd.DataFrame(scaler.transform(test[features_to_scale]), columns=features_to_scale)\n\n    train_scaled['id'] = train['id']\n    test_scaled['id'] = test['id']\n\n    train_scaled['target_class'] = train['target_class']\n\n    train_scaled.to_csv(config.TRAIN_CLEANED, index=False)\n    test_scaled.to_csv(config.TEST_CLEANED, index=False)","08f7f545":"# create_folds.py\n\nimport pandas as pd \nimport numpy as np \n\nfrom sklearn import model_selection\n\ndef create_folds(df, task=\"classification\", num_folds=config.NUM_FOLDS):\n    df['kfold'] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    y = df.target_class.values\n\n    kf = model_selection.StratifiedKFold(n_splits=num_folds)\n\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, 'kfold'] = f\n\n    return df\n\nif __name__ == \"__main__\":\n\n    df = pd.read_csv(config.TRAIN_CLEANED)\n\n    folds_df = create_folds(df)\n\n    folds_df.to_csv(config.TRAINING_FILE, index=False)","4bbcf192":"# model_dispatcher.py\n\nfrom sklearn import linear_model\nfrom sklearn import tree\nfrom sklearn import ensemble\nimport xgboost as xgb \nimport lightgbm as lgbm \nimport catboost as cb \n\nclass model_dispatcher:\n    models = {\n        \"dt_gini\": tree.DecisionTreeClassifier(max_depth=4, criterion='gini', random_state=42),\n        \"lr\": linear_model.LogisticRegression(C=10, max_iter=500),\n        \"dt_entropy\": tree.DecisionTreeClassifier(max_depth=4, criterion='entropy', random_state=42)\n    }","c9df7661":"import pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nimport joblib\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef run(df, fold, model_name='lr'):\n\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n\n    model = model_dispatcher.models[model_name]\n\n    X_train = train_df.drop(['id', 'target_class', 'kfold'], axis=1)\n    X_valid = valid_df.drop(['id', 'target_class', 'kfold'], axis=1)\n\n    print(\"Fitting model...\")\n    model.fit(X_train, train_df.target_class.values)\n\n    print(\"Generating preds...\")\n    preds = model.predict_proba(X_valid)[:, 1]\n\n    score = metrics.roc_auc_score(valid_df.target_class.values, preds)\n\n    print(f\"Fold: {fold}, Score: {score}\")\n\n    print(\"Saving model...\")\n    model_path = config.MODEL_PATH + model_name + \"\/\" + f\"{model_name}_feat_eng_2_{fold}.bin\"\n    joblib.dump(model, model_path)\n\n    return score","ac9f637c":"models_to_use = [\"dt_gini\", \"dt_entropy\"]\ndf = pd.read_csv(config.TRAINING_FILE)\n\nfor model_name in models_to_use:\n    \n    try:\n        os.mkdir(config.MODEL_PATH + model_name)\n    except:\n        pass\n\n    scores = []\n\n    for fold_ in range(config.NUM_FOLDS):\n        score = run(df, fold_, model_name=model_name)\n        scores.append(score)\n\n    print(\"-\"*50)\n    print(f\"CV for {model_name}: {np.mean(scores)}\")\n    print(\"-\"*50)","8b523cbe":"# ensemble.py\n\nimport pandas as pd \nimport numpy as np\nimport joblib\n\nif __name__ == \"__main__\":\n\n    test = pd.read_csv(config.TEST_CLEANED)\n    sample = test[['id']]\n\n    model_folders = [\"dt_entropy\", \"dt_gini\"]\n    model_names = ['dt_entropy_feat_eng_2', \"dt_gini_feat_eng_2\"]\n\n    test_df = test.drop(['id'], axis=1)\n\n    model_paths = []\n\n    for model_folder, model_name in zip(model_folders, model_names):\n        for i in range(config.NUM_FOLDS):\n            model_pth = config.MODEL_PATH + model_folder + \"\/\" + f\"{model_name}_{i}.bin\"\n            model_paths.append(model_pth)\n\n    for model_path in model_paths:\n        print(f\"Using {model_path}\")\n        model = joblib.load(model_path)\n        try:\n            preds += model.predict_proba(test_df)[:, 1]\n        except:\n            preds = model.predict_proba(test_df)[:, 1]\n    preds \/= len(model_paths)\n\n    sample.loc[:, 'target_class'] = preds\n\n    sample.to_csv(\"dt_entropy_dt_gini_max_depth_4_feat_eng_2.csv\", index=False)","efda8f7a":"# Ensemble\n\nHere, a simple average of both model's predictions are taken, 10 folds => 10 models. Therefore, if we use *dt_gini*, the predictions which we get, should be divided by 10.","1cebe5bd":"# Model dispatcher\n\nThis class is used to keep all the models in one place as it becomes easy for experimenting with different models","5594950e":"# Cleaning the data\n\n## Feature Engineering\n\nAll these features were added based on experiment basis by comparing the score in cross-validation. These features are : -\n- Variance of the integrated profile\n- Variance of the DM-SNR curve\n- mean_diff\n- sd_diff\n- kurtosis_diff\n- Mean of the integrated profile squared\n\n## Scaling\n\nThis step is not necessary, it was just done to test Logistic Regression together. Tree based models do not require scaling.","5cf95104":"# Create folds\n\nThis code cell creates 10 folds in a **stratified** manner for our cross-validation setup. *StratifiedKFold* is very important when there is an imbalance in the classes as it maintains the ratio for each fold.","e97ba5cf":"# Engine\n\nThis function is used to run all experiments for a particular model and fold. It also saves the model at the end, making it easier to reproduce the results.","2820efd9":"# Defining config\n\nThis class is used to keep all file paths in one place."}}