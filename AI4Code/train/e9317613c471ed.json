{"cell_type":{"b72753a5":"code","bdfc5db1":"code","11fdd582":"code","455013d0":"code","9b1b4e1a":"code","6fba67e7":"code","8d4cce5b":"code","f54976de":"code","d39d858c":"code","c10a2eca":"code","5d73b862":"code","80ae194a":"code","12bdcd5c":"code","705864fa":"code","68db791c":"markdown","800fd8a0":"markdown","65dae970":"markdown","42125e05":"markdown","97546c3a":"markdown","18979972":"markdown","6b3a569e":"markdown","bed20baf":"markdown","44aa71da":"markdown","c391b973":"markdown","76639ef8":"markdown"},"source":{"b72753a5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport surprise  #Scikit-Learn library for recommender systems. \n","bdfc5db1":"import pandas as pd\nimport surprise","11fdd582":"# Abre arquivo via pandas.\nraw = pd.read_csv('..\/input\/ratings.csv')\nraw.describe()","455013d0":"# Remove duplicados.\nraw.drop_duplicates(inplace=True)\nraw.describe()","9b1b4e1a":"print(f'Existem {raw.shape[0]} avalia\u00e7\u00f5es.')\nprint('Usu\u00e1rios \u00fanicos:', len(raw.user_id.unique()))\nprint('Livros \u00fanicos:', len(raw.book_id.unique()))\nprint(f\"O usu\u00e1rio m\u00e9dio avalia {raw.user_id.value_counts().median()} livros.\")\nprint(f\"Avalia\u00e7\u00e3o mais alta: {raw.rating.max()}. Mais baixa: {raw.rating.min()}\")\nraw.head()","6fba67e7":"# Corrigindo ordem...\nraw=raw[['user_id','book_id','rating']] \n\n# Definindo limites...\nreader = surprise.Reader(rating_scale=(1,5)) \n\ndata = surprise.Dataset.load_from_df(raw,reader)","8d4cce5b":"# C\u00f3digo vindo do notebook original.\nclass ProbabilisticMatrixFactorization(surprise.AlgoBase):\n# Randomly initializes two Matrices, Stochastic Gradient Descent to be able to optimize the best factorization for ratings.\n    def __init__(self,learning_rate,num_epochs,num_factors):\n       # super(surprise.AlgoBase)\n        self.alpha = learning_rate #learning rate for Stochastic Gradient Descent\n        self.num_epochs = num_epochs\n        self.num_factors = num_factors\n    def fit(self,train):\n        #randomly initialize user\/item factors from a Gaussian\n        P = np.random.normal(0,.1,(train.n_users,self.num_factors))\n        Q = np.random.normal(0,.1,(train.n_items,self.num_factors))\n        #print('fit')\n\n        for epoch in range(self.num_epochs):\n            for u,i,r_ui in train.all_ratings():\n                residual = r_ui - np.dot(P[u],Q[i])\n                temp = P[u,:] # we want to update them at the same time, so we make a temporary variable. \n                P[u,:] +=  self.alpha * residual * Q[i]\n                Q[i,:] +=  self.alpha * residual * temp \n\n                \n        self.P = P\n        self.Q = Q\n\n        self.trainset = train\n    \n    \n    def estimate(self,u,i):\n        #returns estimated rating for user u and item i. Prerequisite: Algorithm must be fit to training set.\n        #check to see if u and i are in the train set:\n        #print('gahh')\n\n        if self.trainset.knows_user(u) and self.trainset.knows_item(i):\n            #print(u,i, '\\n','yep:', self.P[u],self.Q[i])\n            #return scalar product of P[u] and Q[i]\n            nanCheck = np.dot(self.P[u],self.Q[i])\n            \n            if np.isnan(nanCheck):\n                return self.trainset.global_mean\n            else:\n                return np.dot(self.P[u,:],self.Q[i,:])\n        else:# if its not known we'll return the general average. \n           # print('global mean')\n            return self.trainset.global_mean\n                \n        ","f54976de":"raw.user_id.iloc","d39d858c":"Alg1 = ProbabilisticMatrixFactorization(\n    learning_rate=0.05,\n    num_epochs=4,\n    num_factors=10\n)\ndata1 = data.build_full_trainset()\nAlg1.fit(data1)\nprint(raw.user_id.iloc[4],raw.book_id.iloc[4])\nAlg1.estimate(raw.user_id.iloc[4],raw.book_id.iloc[4])","c10a2eca":"gs = surprise.model_selection.GridSearchCV(\n    ProbabilisticMatrixFactorization, \n    param_grid={'learning_rate':[0.005,0.01], 'num_epochs':[5,10], 'num_factors':[10,20]},\n    measures=['rmse', 'mae'],\n    cv=2\n)\ngs.fit(data)","5d73b862":"print('RMSE: ',gs.best_score['rmse'],'MAE: ',gs.best_score['mae'])\nprint('RMSE: ',gs.best_params['rmse'],'MAE: ',gs.best_params['mae'])\nbest_params = gs.best_params['rmse']","80ae194a":"bestVersion = ProbabilisticMatrixFactorization(\n    learning_rate=best_params['learning_rate'],\n    num_epochs=best_params['num_epochs'],\n    num_factors=best_params['num_factors']\n)","12bdcd5c":"# k-fold CV para avaliar o melhor modelo. \nkSplit = surprise.model_selection.KFold(n_splits=10,shuffle=True)\nfor train,test in kSplit.split(data):\n    bestVersion.fit(train)\n    prediction = bestVersion.test(test)\n    surprise.accuracy.rmse(prediction,verbose=True)","705864fa":"data.df.isna().sum() #.head()\nraw.isna().sum() #.head()\nbestVersion.estimate(123,42)","68db791c":"Surprise: biblioteca de sistemas de recomenda\u00e7\u00e3o, ajuda na limpeza de dados\n\nPandas: estrutura de dados","800fd8a0":"Ver arquivo CSV \u2192\u2192\u2192\u2192","65dae970":"\u00c9 poss\u00edvel imp\u013aementar nosso pr\u00f3prio algoritmo no Surprise e fazer uso da infraestrutura de avalia\u00e7\u00e3o e sele\u00e7\u00e3o de modelo.\n\u00c9 necess\u00e1rio estender a classe AlgoBase.\nSegue padr\u00e3o fit\/estimate.\n\nVamos adotar Stochastic Gradient Descent para estimar qual seriam os melhores fatores latentes.","42125e05":"Root Mean Square Error \n\nMean Absolut Error","97546c3a":"Inicia com duas matrizes aleat\u00f3rias que conter\u00e3o os fatores latentes. Se n\u00e3o houvesse valores ausentes, poderia ser feito por PCA ou SVD. Uma matriz para usu\u00e1rios e outra para itens.\n\nVamos criar uma matriz de 100.000x40 (40 fatores para cada usu\u00e1rio) e outra matriz de 40x10.000 (40 fatores para cada item).","18979972":"Treino usando todo o conjunto - apenas ilustrativo.","6b3a569e":"Usando GridSearch do Surprise. Similar ao GridSearchCV do sk-learn. Escolhemos os hiper-par\u00e2metros e deixamos ele escolher seus valores.\n\nobs. busca aleat\u00f3ria pode ser mais efetiva que em *grid*.\n\n**desenho no quadro**","bed20baf":"A valida\u00e7\u00e3o cruzada d\u00e1 uma ideia melhor, menos enviesada do erro do que a avalia\u00e7\u00e3o de um \u00fanico modelo.\n\n**Valida\u00e7\u00e3o cruzada estratificada \u00e9 infal\u00edvel?**\n\n**Normaliza\u00e7\u00e3o dos atributos \u00e9 infal\u00edvel?**\n\n**Leave-One-Out \u00e9 sempre mais custoso, p. ex. que 10-fold CV?   (n\u00e3o..., mas sim... e n\u00e3o)**\n","44aa71da":"## Conjunto de dados\n![image.png](attachment:2785d5a4-a709-473f-ac2a-ff0ed041a21c.png)\n\nRefer\u00eancia: https:\/\/papers.nips.cc\/paper\/3208-probabilistic-matrix-factorization.pdf ","c391b973":"# Modelagem e teste de sistema de recomenda\u00e7\u00e3o baseado em fatora\u00e7\u00e3o de matrizes\nNotebook derivado de:\nhttps:\/\/www.kaggle.com\/robottums\/probabalistic-matrix-factorization-with-suprise","76639ef8":"Para carregar dados no Surprise \u00e9 preciso seguir a ordem: user, item, rating. "}}