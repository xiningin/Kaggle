{"cell_type":{"9ca79417":"code","479faccb":"code","97f0e259":"code","0ec6c2a0":"code","4e9ae58c":"code","a20f4e3e":"code","0b5d371a":"code","79383773":"code","a805ef12":"code","937a4d4f":"code","87633ba8":"code","0d37cb1f":"code","31ac8cb6":"code","aec4a4f7":"code","f7065c84":"code","6d92f9b2":"code","daaaa5c3":"code","5cb1b93b":"code","484be764":"code","8ae5a3c9":"code","2c886798":"code","97d3656f":"code","6e8ec691":"code","c1caf6f4":"code","b469bb2d":"code","61ceff2e":"code","d2a321d9":"code","14454fa0":"code","a11db9ff":"code","bc840082":"code","b3b6b415":"code","8d76d46f":"code","a1dd137a":"code","ee969691":"code","852c9fa9":"code","6030368b":"code","9163ce91":"code","4661d6a9":"code","846dde91":"code","72b93d9c":"code","7c7f829b":"code","891a0c4b":"code","9db3e369":"code","5510f183":"code","82440abb":"code","80220190":"code","3a4526b4":"code","58e0ef05":"code","e0812f3f":"code","8cb82cdc":"code","0fa339ac":"code","774d2120":"code","4338057d":"code","d93423ed":"code","77591a17":"markdown","1d95010f":"markdown","64c5bd4a":"markdown","fd452936":"markdown","2c1c6266":"markdown","b50d697a":"markdown","ef5b4cc1":"markdown","e523ab5e":"markdown","e44b6de4":"markdown","1754970f":"markdown","ba672579":"markdown","0864eb10":"markdown","e05c76e4":"markdown","56d5e4a7":"markdown","c11d9b7a":"markdown","e74bb018":"markdown","fb03c1cc":"markdown","38ee3fc2":"markdown","ba1e74dd":"markdown","86d0d39d":"markdown","071b94ec":"markdown","0ce68ac8":"markdown","4df102ed":"markdown","05016a79":"markdown","d088dc3e":"markdown","8bbc4855":"markdown","5495e62d":"markdown"},"source":{"9ca79417":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","479faccb":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, GridSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import classification_report, accuracy_score","97f0e259":"train_path = '..\/input\/learn-together\/train.csv'\ntest_path = '..\/input\/learn-together\/test.csv'\ntrain_df = pd.read_csv(train_path, index_col='Id')\ntest_df = pd.read_csv(test_path, index_col='Id')\ntrain_df.head()","0ec6c2a0":"test_df.head()","4e9ae58c":"# Find if there is any categorical data containing strings\ns = (train_df.dtypes =='object')\nobjects_cols = list(s[s].index)\n\nprint('Categorical Variables \\n', objects_cols)","a20f4e3e":"# Print columns name\ntrain_df.columns","0b5d371a":"# Count the number of unique values in each category\nnum_val = train_df.nunique()\nprint(num_val)","79383773":"numerical_cols= ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points']\ncategorical_cols = ['Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n       'Soil_Type39', 'Soil_Type40']\n\nObj_type ='Cover_Type'\n\nprint('Features number = ', len(numerical_cols)  + len(categorical_cols))","a805ef12":"f,ax = plt.subplots(figsize=(8,6))\nCateg = numerical_cols.copy()\nCateg.append(Obj_type)\n\nsns.heatmap(train_df[Categ].corr(),annot=True, linewidths=.5, fmt='.1f', ax=ax)\nplt.show()","937a4d4f":"train_df.plot(kind='scatter', x='Elevation', y='Horizontal_Distance_To_Roadways', alpha=0.5, color='blue', figsize = (12,9))\nplt.title('Elevation And Horizontal Distance To Roadways')\nplt.xlabel(\"Vertical Distance\")\nplt.ylabel(\"Horizontal Distance\")\nplt.show()","87633ba8":"train_df['Elevation'].plot(kind='hist', alpha=0.5)","0d37cb1f":"train_df['Horizontal_Distance_To_Roadways'].plot(kind='hist', alpha=0.5)","31ac8cb6":"train_df.plot(kind='scatter', x='Vertical_Distance_To_Hydrology', y='Horizontal_Distance_To_Hydrology', alpha=0.5, color='green', figsize = (12,9))\nplt.title('Vertical And Horizontal Distance To Hydrology')\nplt.xlabel(\"Vertical Distance\")\nplt.ylabel(\"Horizontal Distance\")\nplt.show()","aec4a4f7":"train_df['Vertical_Distance_To_Hydrology'].plot(kind='hist', alpha=0.5)","f7065c84":"train_df['Horizontal_Distance_To_Hydrology'].plot(kind='hist', alpha=0.5)","6d92f9b2":"train_df.plot(kind='scatter', x='Hillshade_9am', y='Hillshade_3pm', alpha=0.5, color='black', figsize = (12,9))\nplt.title('Vertical And Horizontal Distance To Hydrology')\nplt.xlabel(\"Vertical Distance\")\nplt.ylabel(\"Horizontal Distance\")\nplt.show()","daaaa5c3":"train_df['Hillshade_9am'].plot(kind='hist', alpha=0.5)","5cb1b93b":"train_df['Hillshade_3pm'].plot(kind='hist', alpha=0.5)\n","484be764":"s = train_df.copy()\ns = s.groupby('Cover_Type')\ns['Elevation'].size()","8ae5a3c9":"s['Elevation'].plot(kind='hist', alpha=0.5)\nplt.legend()","2c886798":"s['Aspect'].plot(kind='hist', alpha=0.5)\nplt.legend()","97d3656f":"s['Slope'].plot(kind='hist', alpha=0.5)\nplt.legend()","6e8ec691":"s['Horizontal_Distance_To_Hydrology'].plot(kind='hist', alpha=0.5)\nplt.legend()\n\nplt.title(\"Histogram for Horizontal_Distance_To_Hydrology in different Cover Type\")","c1caf6f4":"# Select areas\nArea1 = train_df[train_df['Wilderness_Area1']==1].copy()\nArea2 = train_df[train_df['Wilderness_Area2']==1].copy()\nArea3 = train_df[train_df['Wilderness_Area3']==1].copy()\nArea4 = train_df[train_df['Wilderness_Area4']==1].copy()","b469bb2d":"def histogram_plot(feature):\n    plt.hist(Area1[feature], bins='auto', alpha = 0.5)\n    plt.hist(Area2[feature], bins='auto', alpha = 0.5)\n    plt.hist(Area3[feature], bins='auto', alpha = 0.5)\n    plt.hist(Area4[feature], bins='auto', alpha = 0.5)","61ceff2e":"feature1 = 'Cover_Type'\nhistogram_plot(feature1)\n\n\nplt.title(\"Histogram for Cover Type in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","d2a321d9":"feature1 = 'Elevation'\nhistogram_plot(feature1)\n\n\nplt.title(\"Histogram for Elevation in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","14454fa0":"feature2 = 'Aspect'\nhistogram_plot(feature2)\n\nplt.title(\"Histogram for Aspect in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","a11db9ff":"feature3 = 'Slope'\nhistogram_plot(feature3)\nplt.title(\"Histogram for Aspect in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","bc840082":"feature4 = 'Horizontal_Distance_To_Hydrology'\nhistogram_plot(feature4)\nplt.title(\"Histogram for Horizontal Distance To Hydrology in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","b3b6b415":"feature5 = 'Vertical_Distance_To_Hydrology'\nhistogram_plot(feature5)\nplt.title(\"Histogram for Vertical_Distance_To_Hydrology in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","8d76d46f":"feature6 = 'Horizontal_Distance_To_Roadways'\nhistogram_plot(feature6)\nplt.title(\"Histogram for Horizontal_Distance_To_Roadways  in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","a1dd137a":"feature7 = 'Hillshade_9am'\nhistogram_plot(feature7)\nplt.title(\"Histogram for Hillshade_9am  in different Wilderness Area\")\nplt.legend(['Area 1', 'Area 2', 'Area 3', 'Area 4'])","ee969691":"Select_num = list(range(2,8))\nSelect_num.insert(0,0)\n\nnum_cols = numerical_cols.copy()\n\n# Here I am removing the columns that seem to be irrelevant after the data visualization\nnum_cols.remove('Aspect')\nnum_cols.remove('Hillshade_Noon')\n\nfeatures2 = num_cols.copy() \nfeatures2.extend(categorical_cols)\n\nprint(\"The number of features that are considered in this study are:\", len(features2))","852c9fa9":"X_df = train_df.drop(['Cover_Type'], axis=1)\ny_df = train_df['Cover_Type']\nX_train, X_val, y_train, y_val = train_test_split(X_df[features2], y_df, test_size=0.2, random_state = 0)\n\nprint(X_train.shape)","6030368b":"\"\"\"\ndef scoreModel(n_estim, X_train, y_train):\n    for n_est in n_estim:\n        model = RandomForestClassifier(n_estimators=n_est, random_state = 0)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        mae_3 = mean_absolute_error(y_val, y_pred)\n        print(\"\\nFor n_estimators = {} \\n Mean Absolute Error: {}\".format(n_est, mae_3))\n        #print(classification_report(y_val, y_pred))\n        print('Accuracy = ', accuracy_score(y_val, y_pred))\n\nn_estim = [20, 50, 100, 150, 200, 250, 300] \nscoreModel(n_estim, X_train[features2], y_train)\n\"\"\"","9163ce91":"\"\"\"\nmodel = RandomForestClassifier(n_estimators=100, random_state = 0)\nmodel.fit(X_train, y_train)\n\"\"\"","4661d6a9":"\"\"\"\ny_pred = model.predict(X_val)\ny_pred.shape\n\"\"\"","846dde91":"# print(classification_report(y_val, y_pred))","72b93d9c":"# print('Accuracy = ', accuracy_score(y_val, y_pred))\n        ","7c7f829b":"\"\"\"\n# Calculate MAE\nmae_3 = mean_absolute_error(y_val, y_pred)\nprint(\"Mean Absolute Error:\" , mae_3)\n\"\"\"","891a0c4b":"\"\"\"\n# Preprocessing of validation data, get predictions\npreds = model.predict(test_df[features2])\nprint(preds)\n\"\"\"","9db3e369":"from xgboost import XGBClassifier","5510f183":"\"\"\"\ndef scoreModel_xgb(n_estim, X_train, y_train):\n    for n_est in n_estim:\n        xgb = XGBClassifier( n_estimators = n_est,  #todo : search for good parameters\n                    learning_rate= 0.5,  #todo : search for good parameters\n                    objective= 'binary:logistic', #this outputs probability,not one\/zero. should we use binary:hinge? is it better for the learning phase?\n                    random_state= 1,\n                    n_jobs=-1)\n        xgb.fit(X_train, y_train)\n        preds_xgb = xgb.predict(X_val)\n        print('Accuracy for ', n_est , ' = ', accuracy_score(y_val, preds_xgb))\n        \nn_estim = [100, 500, 1000, 2000, 5000] \nscoreModel_xgb(n_estim, X_train[features2], y_train)\n\"\"\"","82440abb":"\"\"\"\nn_estimators_best = 2000\nxgb = XGBClassifier( n_estimators=n_estimators_best,  \n                    learning_rate= 0.5, \n                    objective= 'binary:logistic', \n                    random_state= 1,\n                    n_jobs=-1)\n\"\"\"","80220190":"# xgb.fit(X_train, y_train)","3a4526b4":"\"\"\"\ny_pred2 = xgb.predict(X_val)\ny_pred2.shape\nprint('Accuracy = ', accuracy_score(y_val, y_pred2))\nmae_3 = mean_absolute_error(y_val, y_pred2)\nprint(\"Mean Absolute Error:\" , mae_3)\n\"\"\"","58e0ef05":"\"\"\"\n# Preprocessing of validation data, get predictions\npreds2 = xgb.predict(test_df[features2])\nprint(preds2)\n\"\"\"","e0812f3f":"\"\"\"\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nmodel_XGB = XGBClassifier(#silent=False, \n                      #scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 1,\n                      subsample = 0.8,\n                      objective='multi:softmax', \n                      n_estimators=2000, \n                      #reg_alpha = 0.3,\n                      max_depth=3, \n                      earlystoppingrounds=5,\n                      gamma=1)\n\n# X_train, X_val, y_train, y_val\n#eval_set = [(X_train, y_train), (X_val, y_val)]\neval_set = [ (X_val, y_val)]\neval_metric = [\"rmse\"]\nmodel_XGB.fit(X_train, y_train, eval_metric=eval_metric, verbose=True)\ny_pred_XGB = model_XGB.predict(X_val)\n\nprint('Accuracy = ', accuracy_score(y_val, y_pred_XGB))\nmae_XGB = mean_absolute_error(y_val, y_pred_XGB)\nprint(\"Mean Absolute Error:\" , mae_XGB)\n\n\"\"\"","8cb82cdc":"\"\"\"\n# Preprocessing of validation data, get predictions\npreds_XGB = model_XGB.predict(test_df[features2])\n\"\"\"","0fa339ac":"\"\"\"\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Train model\nAdaCl = AdaBoostClassifier(n_estimators=1000)\nAdaCl.fit(X_train, y_train)\n\n# PRedictions\ny_pred3 = AdaCl.predict(X_val)\ny_pred3.shape\n\n# Statistics\nprint('Accuracy = ', accuracy_score(y_val, y_pred3))\nmae_3 = mean_absolute_error(y_val, y_pred3)\nprint(\"Mean Absolute Error:\" , mae_3)\n\n# Preprocessing of validation data, get predictions\npreds3 = AdaCl.predict(test_df[features2])\nprint(preds3)\n\"\"\"","774d2120":"\nfrom lightgbm import LGBMClassifier\n\n# Elevation and Horizontal_Distance_To_Roads\n# * Horizontal_Distance_to_Hydrology and Vertical_Distance_To_Roadways\n# * Hillshade_9am and Hillshade_3pm\n\nlgbm = LGBMClassifier(objective='multiclass',learning_rate=0.01, n_estimators=2000 , random_state=5, num_leaves = 500)\nlgbm.fit(X_train, y_train)\n\n# PRedictions\ny_pred4 = lgbm.predict(X_val)\n\n# Statistics\nprint('Accuracy = ', accuracy_score(y_val, y_pred4))\nmae_4 = mean_absolute_error(y_val, y_pred4)\nprint(\"Mean Absolute Error:\" , mae_4)\n","4338057d":"\n# Preprocessing of validation data, get predictions\npreds4 = lgbm.predict(test_df[features2])\n#print(preds4)\n","d93423ed":"test_ids = test_df.index\n\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': preds4})\noutput.to_csv('submission.csv', index=False)\n\noutput.head()\n","77591a17":"### Plot of Hillshade_9am and Hillshade_3pm","1d95010f":"## 4.2. XGBooost","64c5bd4a":"### Effect of Slope\n","fd452936":"# 4. Modelling\n## Objective:\n### I am interested predict an integer classification for the forest cover type.\n\n## 4.1. Ramdom Forest Classifier","2c1c6266":"### Plot of Horizontal_Distance_to_Hydrology and Vertical_Distance_To_Roadways","b50d697a":"### Get the list of categorical variables","ef5b4cc1":"### Effect of Aspect","e523ab5e":"### The categorical variables are:\n\n* Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n* Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n* Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation","e44b6de4":"### This means that for the cover type:\n* The cover type depends on Elevation. However, it seems not to depend on aspect, slope  or Horizontal_Distance_To_Hydrology","1754970f":"## 3.1. Plot of most important correlations","ba672579":"# Beginner-Friendly competence \n## Roosevelt National Forest Classification\nClassify forest types based on information about the area","0864eb10":"## 3.2. Plot for the Features of the different Cover Types.\n\n1. Spruce\/Fir\n2. Lodgepole Pine\n3. Ponderosa Pine\n4. Cottonwood\/Willow\n5. Aspen\n6. Douglas-fir\n7. Krummholz","e05c76e4":"### Effect of Horizontal Distance To Hydrology\n","56d5e4a7":"# 5. Output","c11d9b7a":"## 3.3. Here I plot the different features for Wilderness areas.\n\nThe wilderness areas are:\n\n1. Rawah Wilderness Area\n2. Neota Wilderness Area\n3. Comanche Peak Wilderness Area\n4. Cache la Poudre Wilderness Area","e74bb018":"This means that all the categories are numeric:","fb03c1cc":"## Models summary\n\n### Random Forest classifier:\n- model = RandomForestClassifier(n_estimators=100, random_state = 0)\n- Accuracy =  0.8525132275132276\n- Mean Absolute Error: 0.3568121693121693\n\n### XGBoost Classifier\n- n_estimators_best = 2000\n- xgb = XGBClassifier( n_estimators=n_estimators_best, learning_rate= 0.5, objective= 'binary:logistic', random_state= 1, n_jobs=-1)\n- Accuracy =  0.8392857142857143\n- Mean Absolute Error: 0.3872354497354497\n\n### AdaBoost Classifier\n- AdaCl = AdaBoostClassifier(n_estimators=100)\n- Accuracy =  0.4176587301587302\n- Mean Absolute Error: 1.874669312169312\n\n### Light GBM Classifier\n- lgbm = LGBMClassifier(objective='multiclass', random_state=5)\n- Accuracy =  0.8353174603174603\n- Mean Absolute Error: 0.3968253968253968\n---------\n- lgbm = LGBMClassifier(objective='multiclass', n_estimators=400, random_state=5)\n- Accuracy =  0.8558201058201058\n- Mean Absolute Error: 0.3482142857142857\n--------\n- lgbm = LGBMClassifier(objective='multiclass', n_estimators=700, random_state=5)\n- Accuracy =  0.8578042328042328\n- Mean Absolute Error: 0.34755291005291006\n------\n- lgbm = LGBMClassifier(objective='multiclass', n_estimators=1000, random_state=5)\n- Accuracy =  0.8611111111111112\n- Mean Absolute Error: 0.3402777777777778\n-----\n- lgbm = LGBMClassifier(objective='multiclass', n_estimators=1500, random_state=5)\n- Accuracy =  0.8637566137566137\n- Mean Absolute Error: 0.32771164021164023","38ee3fc2":"This means that the most important correlations are between:\n* Elevation and Horizontal_Distance_To_Roads\n* Horizontal_Distance_to_Hydrology and Vertical_Distance_To_Roadways\n* Hillshade_9am and Hillshade_3pm","ba1e74dd":"## 2. Exploratory Data Analysis (EDA)","86d0d39d":"## 3. Data visualization","071b94ec":"### Plot of Elevation and Horizontal_Distance_To_Roads","0ce68ac8":"## 4.4. Light GBM Classifier","4df102ed":"## Imports","05016a79":"## 4.3. AdaBoost Classifier","d088dc3e":"### The numeric variables are:\n* Elevation - Elevation in meters\n* Aspect - Aspect in degrees azimuth\n* Slope - Slope in degrees\n* Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n* Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n* Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n* Horizontal_Distance_To_Fire_Points    5826\n* Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n* Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n* Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice","8bbc4855":"## 1. Load data","5495e62d":"### This means that for the Areas:\n* Elevation is an important parameter: Low elevations will be in Area 4\n* Aspect, area, horizontal and vertical distance to hydrology seems to be not a very relevant parameter\n* Cover type 4 is only present in area 4\n* Area 2 present cover types 1, 2 and 7"}}