{"cell_type":{"1faa58a3":"code","a4daa87f":"code","441a5285":"code","ac438d62":"code","ece45403":"code","47f28190":"code","d704d54d":"code","8add56ea":"code","b72e54ac":"code","ee089502":"code","a3d528b5":"markdown","571f1561":"markdown","bf90c1bf":"markdown","c769e604":"markdown","a4c65d83":"markdown","830837a8":"markdown","61302eef":"markdown","05074d12":"markdown","0f735aa6":"markdown"},"source":{"1faa58a3":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\n\nfrom early_stopping import * ####\n!pip install GPUtil ####\nfrom GPUtil import showUtilization ####\nimport gc ####\n\nwarnings.filterwarnings('ignore')","a4daa87f":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","441a5285":"def print_gpu_cache(detailed=False):\n    print(\"GPU Usage\")\n    showUtilization()\n    if detailed:\n        print(\"GPU memory summary\")\n        print(torch.cuda.memory_summary(device=None, abbreviated=False))","ac438d62":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='..\/input\/roberta-base\/vocab.json', \n            merges_file='..\/input\/roberta-base\/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx\n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict","ece45403":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        \n        config = RobertaConfig.from_pretrained(\n            '..\/input\/roberta-base\/config.json', output_hidden_states=True)    \n        self.roberta = RobertaModel.from_pretrained(\n            '..\/input\/roberta-base\/pytorch_model.bin', config=config)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n\n    def forward(self, input_ids, attention_mask):\n        _, _, hs = self.roberta(input_ids, attention_mask)\n         \n        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n        x = torch.mean(x, 0)\n        x = self.dropout(x)\n        x = self.fc(x)\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits","47f28190":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","d704d54d":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)","8add56ea":"#def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename): ####\ndef train_model(model, dataloaders_dict, criterion, optimizer, scheduler, num_epochs, filename): ####\n    model.cuda()\n\n    es = EarlyStopping(patience=2, mode=\"max\") ####\n    \n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            #for data in (dataloaders_dict[phase]): ####\n            for batch_idx, data in enumerate(dataloaders_dict[phase]): ####\n                ids = data['ids'].cuda()\n                masks = data['masks'].cuda()\n                tweet = data['tweet']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_idx'].cuda()\n                end_idx = data['end_idx'].cuda()\n                \n                #optimizer.zero_grad() ####\n                model.zero_grad() ####\n\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    start_logits, end_logits = model(ids, masks)\n\n                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        if batch_idx % 50 == 0: ####\n                            print( ####\n                                \"batch_idx \" + str(batch_idx), ####\n                                \", opt lr \" + str(round(optimizer.param_groups[0]['lr'], 6)), ####\n                                \", scheduler lr \" + str(round(scheduler.get_last_lr()[0], 6)) ####\n                            ) ####\n                        optimizer.step()\n                        scheduler.step() ####\n\n                    epoch_loss += loss.item() * len(ids)\n                    \n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    for i in range(len(ids)):                        \n                        jaccard_score = compute_jaccard_score(\n                            tweet[i],\n                            start_idx[i],\n                            end_idx[i],\n                            start_logits[i], \n                            end_logits[i], \n                            offsets[i])\n                        epoch_jaccard += jaccard_score\n                    \n            epoch_loss = epoch_loss \/ len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard \/ len(dataloaders_dict[phase].dataset)\n            \n            print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n            \n            if phase == 'val': ####\n                es(epoch_jaccard, model, model_path=filename) ####\n                \n        if es.early_stop: ####\n            print(\"Early stopping\") ####\n            break ####\n    \n    #torch.save(model.state_dict(), filename) ####","b72e54ac":"#num_epochs = 3 ####\nnum_epochs = 5 ####\nbatch_size = 32\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)","ee089502":"%%time\n\ntrain_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n    print(f'Fold: {fold}')\n\n    model = TweetModel()\n    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n    criterion = loss_fn    \n    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n    scheduler = optim.lr_scheduler.OneCycleLR( ####\n        optimizer, max_lr=3e-5, steps_per_epoch=len(dataloaders_dict[\"train\"]), epochs=num_epochs ####\n    ) ####\n\n    train_model(\n        model, \n        dataloaders_dict,\n        criterion, \n        optimizer,\n        scheduler, ####\n        num_epochs,\n        f'roberta_fold{fold}.pth')\n    \n    print_gpu_cache() ####\n    torch.cuda.empty_cache() ####\n    del model, optimizer, dataloaders_dict, scheduler ####\n    gc.collect() ####\n    print_gpu_cache() ####","a3d528b5":"# Loss Function","571f1561":"# Training Function","bf90c1bf":"# Model","c769e604":"# Data Loader","a4c65d83":"# Evaluation Function","830837a8":"### Notice in this notebook\n\n- Important handling for Question-Answering models\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n\n<br>\n\n- Text is lowercased. Notice:\n    - The use of `lower()`\n    - ByteLevelBPETokenizer is `lowercase`\n\n<br>\n\n- Although Huggingface has a ready-to-use Question-Answering model (class RobertaForQuestionAnswering), we are not using it; we DIY a Question-Answering model by taking the raw output of a Roberta model and specifying the Question-Answering logic in `forward()`. One of the purposes of doing so is to get the opportunity to custimize (e.g. averaging the last four output layers). \n  \n  The `forward()` of Roberta or Bert question answering contains the following steps:\n  - The Roberta model outputs hidden states\n  - Get the last one or the last a few layers of the hidden states\n  - Use a fully connected layer to map `hidden_size` to 2\n  - Split to start_logits and end_logits\n  \n  Then loss is calculated.\n\n<br>\n\n- The output of the model would go through `softmax`, before going through `argmax`. The `softmax` makes sure that output values lie in the range [0,1] and sum to 1, so that it makes sense to element-wise add 10 lists of output values from 10 different models (10-fold split) and then take the average, before going through `argmax`.\n\n<br>\n\n- Code structure of this notebook:\n\n        class TweetDataset\n            def __init__\n            def __getitem__\n                process one tweet\n                return a dict of information about this tweet\n        class TweetModel\n            def __init__\n            def forward\n                return start_logits, end_logits\n        def loss_fn\n        def train\n            for each epoch\n                for [train, val]\n                    if train\n                        model.train()\n                    else\n                        model.eval() # disable dropout\n                    epoch_loss = 0\n                    epoch_jaccard = 0\n                    for each batch from train or val DataLoader\n                        with torch.no_grad() if eval\n                            optimizer.zero_grad()\n                            model predict\n                            calculate and accumulate loss\n                            if train\n                                loss.backward()\n                                optimizer.step()\n                                scheduler.step()\n                            calculate jaccard_score for each tweet and accumulate them\n                    calculate the average loss as epoch_loss\n                    calculate the average jaccard_score as epoch_jaccard\n                    if val\n                        update early stopping, and save model if improved\n                do early stopping if out of patience\n\n        for each train-val split\n            train\n\n<br>\n\n### Changes compared to the [original notebook](https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch):\n- Split into two notebooks (train and inference)\n- Added early stopping, and used one of the following manipulations to the Roberta output in `forward()`\n    - Averaged the last four layers (Private 0.71557, Public 0.71521)\n        - Further added learning rate scheduler and fine tuned, and added memory cleaning (THIS RUN) (Private 0.71620, Public 0.71286)\n    - Concatenated last hidden layer and the third last layer, instead of averaging the last four layers (Private 0.71647, Public 0.71240)\n    - Reduced to use only the last one layer of hidden layer instead of averaging the last four layers (Private 0.71439, Public 0.71240)\n    \n### View the [inference notebook here](https:\/\/www.kaggle.com\/kanruwang\/tweet-sentiment-roberta-pytorch-inference)\n\n<br>\n\n# Libraries","61302eef":"# Seed","05074d12":"# GPU Memory Releasing Function","0f735aa6":"# Training"}}