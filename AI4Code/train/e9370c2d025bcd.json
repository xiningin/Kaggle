{"cell_type":{"3742ccc3":"code","ab6f4205":"code","c514e3a7":"code","74ae299d":"code","223442cd":"code","fbbf795a":"code","836efe8e":"code","c4acbb02":"code","206fdbfa":"code","2c2b77e6":"markdown","8ea5f63b":"markdown","36245f69":"markdown"},"source":{"3742ccc3":"# Import environment from Environment.ipynb by copying final cell\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\nfrom IPython.display import clear_output\nfrom IPython import display\n\nimport time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set(rc={'figure.figsize':(15, 10)})\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 5})\n\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\n\n#------------------------------------------------------------------------------------------\n\nstates = ['A','B','C','D','E','F','G','T','M','recycling','trash']\nx_list = [4,3,2,1,1,1,2,3,3,4,4]\ny_list = [1,1,1,1,2,3,3,3,2,3,2]\n\n# The low-level actions the agent can make in the environment\nactions = ['left','right','up','down']\nrewards = [-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,-0.04,1,-1]\n\n#initial_policy = ['left','up','right','up','up','right','right','right','up','up','up']\n\n#------------------------------------------------------------------------------------------\n\ndef action_outcome(state_x,state_y,action):\n    if action == 'left':\n        u = -1\n        v = 0\n    elif action == 'right':\n        u = 1\n        v = 0\n    elif action == 'up':\n        u = 0\n        v = 1\n    elif action == 'down':\n        u = 0\n        v = -1\n    else:\n        print(\"Error: Invalid action given\")\n        \n    # Override if action hits wall to not move\n    if (state_x == 1) & (u == -1):\n        u = 0\n        v = v\n    elif (state_x == 4) & (u == 1):\n        u = 0\n        v = v\n    elif (state_y == 1) & (v == -1):\n        u = u\n        v = 0\n    elif (state_y == 3) & (v == 1):\n        u = u\n        v = 0\n    elif (state_x == 2)&(state_y == 1) & (v == 1):\n        u = u\n        v = 0\n    elif (state_x == 1)&(state_y == 2) & (u == 1):\n        u = 0\n        v = v  \n    elif (state_x == 2)&(state_y == 3) & (v == -1):\n        u = u\n        v = 0         \n    elif (state_x == 3)&(state_y == 2) & (u == -1):\n        u = 0\n        v = v \n    # Make so it cannot get out of bin\n    elif (state_x == 4)&(state_y == 3):\n        u = 0\n        v = 0\n    elif (state_x == 4)&(state_y == 2):\n        u = 0\n        v = 0\n    return(u,v)\n\ndef environment(state, action):\n    # Outcome probabilities\n    if (state=='recycling')|(state=='trash'):\n        prob = 0\n    elif (state=='T'):\n        prob = 1\n\n    elif (state=='M'):\n        prob = 0.7\n   \n    elif (state=='B'):\n        prob = 0.7\n\n    elif (state=='A'):\n        prob = 0.7\n\n    elif (state=='C'):\n        prob = 0.7\n\n    elif (state=='D'):\n        prob = 0.7\n\n    elif (state=='E'):\n        prob = 0.7\n\n    elif (state=='F'):\n        prob = 0.7\n\n    elif (state=='G'):\n        prob = 0.7\n\n    else:\n        prob = \"Error\"\n        print(\"Error state\", state)\n\n    action_rng = np.random.rand()\n    if action_rng<=prob:\n        action = action\n    else:\n        action_sub_list = actions.copy()\n        action_sub_list.remove(action)\n        action = random.choice(action_sub_list)\n        \n        \n    state_x = x_list[states.index(state)]\n    state_y = y_list[states.index(state)]\n    u = action_outcome(state_x,state_y,action)[0]\n    v = action_outcome(state_x,state_y,action)[1]\n    next_state_x = state_x + u\n    next_state_y = state_y + v\n    # Returns index of x + y position to then find the state name\n    next_state = states[' '.join(str(x_list[i])+ \"_\" + str(y_list[i]) for i in range(0,len(x_list))).split().index(str(next_state_x) + \"_\" + str(next_state_y))]\n    reward = rewards[states.index(next_state)]\n    return(state, action, state_x, state_y, u, v, next_state, next_state_x, next_state_y, reward)\n\n#------------------------------------------------------------------------------------------\n","ab6f4205":"def Q_learning_v1(Q_table, start_state, num_episodes, epsilon, num_random_episodes, alpha, gamma):\n    \n    Q_value_tracker = pd.DataFrame()\n    for episode in range(0,num_episodes):\n        clear_output(wait=True)\n        display.clear_output(wait=True)\n        print(\"Episodes Completed: \", np.round( (episode\/num_episodes)*100,2),\"%\")\n        \n        # Add this so start state doesn't need to be given by user input\n        if (start_state is None):\n            state = random.choice(states)\n        else:\n            state = start_state\n            \n        # Initialise action loop    \n        a = 1\n        while True:\n            # End loop at terminal states\n            if (state == 'recycling')|(state == 'trash'):\n                break\n            else:\n                # Apply epsilon-greedy\n                #------\n                # We set first few episodes to follow purely randomly selection for exploration\n                greedy_rng = np.random.rand()\n                if (episode<num_random_episodes):\n                    action = random.choice(actions)\n                # Randomly select with P=epsilon\n                elif (greedy_rng <= epsilon):\n                    action = random.choice(actions)\n                # Greedy (max value currently) otherwise \n                else:\n                    # Pick action in state with highest Q value, randomly select if multiple match same max value\n                    Q_table_max = Q_table[Q_table['Q'] == max(Q_table['Q'])]\n                    if len(Q_table_max)>1:\n                        action = Q_table_max.sample().iloc[0]['action']\n                    else:\n                        action = Q_table_max.iloc[0]['action']\n                #------\n                \n                \n                # Environment probabilistric outcome\n                #---\n                # environment fn output: return(state, action, state_x, state_y, u, v, next_state, next_state_x, next_state_y, reward)\n                outcome = environment(state, action)\n                \n                new_state = outcome[6]\n                new_x = outcome[7]\n                new_y = outcome[8]\n                r = outcome[9]\n                #------\n                \n                \n                # Update Values\n                #------\n                # Update Q based on episode outcome\n                # Q learning update: Q(s_{t},a_{t}) <-- (1-alpha)*Q(s_{t},a_{t}) + alpha[r + gamma*max(Q(s_{t+1},a))]\n                Q_table_new_state = Q_table[Q_table['state']==new_state]\n                max_Q_new_state = Q_table_new_state[Q_table_new_state['Q'] == max(Q_table_new_state['Q'])].iloc[0]['Q']\n                \n                Q_table['Q'] = np.where( (Q_table['state']==state)&(Q_table['action']==action), \n                                        ((1-alpha)*Q_table['Q']) + (alpha*(r + (gamma*max_Q_new_state))),\n                                        Q_table['Q'] )\n                #------\n                \n                # Move to next action, make the new state the current state\n                #------\n                a=a+1\n                state=new_state\n                #------\n        \"\"\"     \n        # Optimal Policy Plot - REMOVE THIS IF YOU WANT IT TO RUN FASTER FOR FINAL OUTPUT\n        #------\n        # Plot best actions in each state\n\n        Q_table['Q_norm'] = (Q_table['Q']-min(Q_table['Q']))\/( max(Q_table['Q']) - min(Q_table['Q']) )\n        for n,state in enumerate(states):\n            if state == 'recycling':\n                plt.scatter(x_list[n],y_list[n], s=150, color='g', marker='+')\n            elif state == 'trash':\n                plt.scatter(x_list[n],y_list[n], s=150, color='r', marker='x')\n            else:\n                plt.scatter(x_list[n],y_list[n], s=150, color='b')\n            plt.text(x_list[n]+0.05,y_list[n]+0.05,states[n])\n            Q_table_output_state = Q_table[Q_table['state']==state].reset_index(drop=True)\n            for action in range(0,len(Q_table_output_state)):\n                if (Q_table_output_state['Q_norm'].iloc[action] == Q_table_output_state['Q_norm'].max()):\n                    plt.quiver(x_list[n],y_list[n],Q_table_output_state['u'][action],Q_table_output_state['v'][action], alpha = 0.5,\n                              width = 0.01*Q_table_output_state['Q_norm'][action])\n\n        plt.title(\"Grid World Diagram for Classroom Paper Throwing Environment: \\n Optimal Action for each State\")\n        plt.xticks([])\n        plt.yticks([])\n        plt.ylim(0,4)\n        plt.xlim(0,5)\n        plt.show()\n        time.sleep(2.0)\n        #------\n        \"\"\"\n        Q_value_tracker = Q_value_tracker.append(pd.DataFrame({'episode':episode, 'mean_Q': Q_table['Q'].mean(),'total_Q':Q_table['Q'].sum()}, index=[episode]))\n        \n    return(Q_table, Q_value_tracker)","c514e3a7":"# Initialise RL flat agent\n\n## Fix episode start if running on small scale for understanding\nstart_state = None\n\n## Parameters\nnum_episodes = 500\nepsilon = 0.2\nnum_random_episodes = 50\nalpha = 0.2\ngamma = 0.8","74ae299d":"# Initialise Q Values equal to 0\nQ_table = pd.DataFrame()\nfor n1,state in enumerate(states):\n    action_list = pd.DataFrame()\n    for n2,action in enumerate(actions):\n        state_x = x_list[n1]\n        state_y = y_list[n1]\n        u = action_outcome(state_x,state_y,action)[0]\n        v = action_outcome(state_x,state_y,action)[1]\n        action_list  = action_list.append(pd.DataFrame({'state':state,'x':x_list[n1],'y':y_list[n1],'u':u,'v':v, 'action':action}, index=[(n1*len(actions)) + n2]))\n\n    Q_table = Q_table.append(action_list)\nQ_table['Q']=0","223442cd":"Q_table.head()","fbbf795a":"Mdl = Q_learning_v1(Q_table, start_state, num_episodes, epsilon, num_random_episodes, alpha, gamma)\nQ_table_output = Mdl[0]\nQ_table_tracker_output = Mdl[1]","836efe8e":"plt.plot(Q_table_tracker_output['total_Q'])\nplt.ylabel(\"Total Q\")\nplt.xlabel(\"Episode\")\nplt.title(\"Total Q by Episode\")\nplt.show()","c4acbb02":"# Normalise so we can still plot negative values\nQ_table_output['Q_norm'] = (Q_table_output['Q']-min(Q_table_output['Q']))\/( max(Q_table_output['Q']) - min(Q_table_output['Q']) )\nQ_table_output.head(50)","206fdbfa":"# Plot best actions in each state\nfor n,state in enumerate(states):\n    if state == 'recycling':\n        plt.scatter(x_list[n],y_list[n], s=150, color='g', marker='+')\n    elif state == 'trash':\n        plt.scatter(x_list[n],y_list[n], s=150, color='r', marker='x')\n    else:\n        plt.scatter(x_list[n],y_list[n], s=150, color='b')\n    plt.text(x_list[n]+0.05,y_list[n]+0.05,states[n])\n    Q_table_output_state = Q_table_output[Q_table_output['state']==state].reset_index(drop=True)\n    for action in range(0,len(Q_table_output_state)):\n        if (Q_table_output_state['Q_norm'].iloc[action] == Q_table_output_state['Q_norm'].max()):\n            plt.quiver(x_list[n],y_list[n],Q_table_output_state['u'][action],Q_table_output_state['v'][action], alpha = 0.5,\n                      width = 0.01*Q_table_output_state['Q_norm'][action])\n    \nplt.title(\"Grid World Diagram for Classroom Paper Throwing Environment: \\n Optimal Action for each State\")\nplt.xticks([])\nplt.yticks([])\nplt.ylim(0,4)\nplt.xlim(0,5)\nplt.show()\n","2c2b77e6":"### Q-Learning Update\n\n$Q(s,a) \\leftarrow (1-\\alpha)*Q(s_{t},a_{t}) + \\alpha[r + \\gamma max_a(Q(s_{t+1},a))]$\n\nWhich can be expanded to:\n\n$Q(s,a) \\leftarrow  Q(s_{t},a_{t}) + \\alpha[ r + \\gamma max_a(Q(s_{t+1},a)) - Q(s_{t},a_{t})]$\n\nwhere $\\alpha$ is the learning rate and $\\gamma$ is the discount parameter.\n","8ea5f63b":"# A Q-Learning Agent for the Simple Classroom Environment Example\n\nThe following notebook uses a simple Q-Learning agent with episodic experience to find an optimal policy for the Classroom previously defined.\n\nI have set some extra challenges at the end I suggest you try to complete in your own time.\n\nPlease let me know if you have any questions.\n\n","36245f69":"# Next Steps\n\n## Part 1: Understand Algorithms\n\n1. Apply TD(0) update approach\n2. Apply Monte Carlo update approach\n3. Apply SARSA update approach\n4. Compare learning patterns between each method\n\n## Part 2: Understand Paramters\n\n1. Vary the Learning Rate $\\alpha$\n2. Vary the Discount Rate $\\gamma$\n3. Vary the Action Selection parameter $\\epsilon$\n\n## Part 3: Environmental Changes\n\n1. Change the probabilities of students following the command (e.g. Make \"M\" less likely to follow command)\n2. Vary the Reward Signal to see how this effects learning, what happens if we dont use r=-0.04 for all actions and instead use r=0\n"}}