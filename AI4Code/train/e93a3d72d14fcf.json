{"cell_type":{"1553e9cd":"code","5076f0d2":"code","020c6794":"code","232d73d0":"code","6ba656c1":"code","89b873ff":"code","b7c1c4c6":"code","8f5ad67f":"code","1db364dc":"code","3a170139":"code","e47e740b":"code","2c2ab769":"code","3c780ab0":"code","0ff2d787":"code","1d57566d":"code","d3a6e024":"code","755c465a":"code","e66cba73":"code","830f1ff2":"code","f43e74b0":"code","52bd3ea5":"code","a91ae55b":"code","d61e4f02":"code","1e0976ce":"code","4a79d342":"code","34e86e0a":"code","834ab4b4":"code","105b23cb":"code","49028209":"code","3aaa8d0a":"code","a3031e4d":"code","4d53024a":"code","bcfae8b0":"code","97bb6666":"code","e6b49c41":"code","6283197c":"code","89e0262d":"markdown","15634a57":"markdown","a8176dd2":"markdown","23474d81":"markdown","554d280f":"markdown","e34ba7af":"markdown","d2c765b3":"markdown","69a52db7":"markdown","0be967d9":"markdown","30c7a9c4":"markdown","05264f12":"markdown","b6b75d03":"markdown","b4f74817":"markdown","d3fd97d1":"markdown","989f44df":"markdown","58667f3b":"markdown","7bfa011f":"markdown","4d9798d0":"markdown","b82ecacc":"markdown","7c2e3f4d":"markdown","886862a7":"markdown"},"source":{"1553e9cd":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","5076f0d2":"class_ = pd.read_csv(\"..\/input\/class.csv\")\nzoo = pd.read_csv(\"..\/input\/zoo.csv\")","020c6794":"zoo.head()","232d73d0":"zoo.info()","6ba656c1":"zoo.describe()","89b873ff":"zoo.drop(\"animal_name\",axis=1,inplace=True)","b7c1c4c6":"color_list = [(\"red\" if i ==1 else \"blue\" if i ==0 else \"yellow\" ) for i in zoo.hair]","8f5ad67f":"unique_list = list(set(color_list))\nunique_list","1db364dc":"pd.plotting.scatter_matrix(zoo.iloc[:,:7],\n                                       c=color_list,\n                                       figsize= [20,20],\n                                       diagonal='hist',\n                                       alpha=1,\n                                       s = 300,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","3a170139":"sns.countplot(x=\"hair\", data=zoo)\nplt.xlabel(\"Hair\")\nplt.ylabel(\"Count\")\nplt.show()\nzoo.loc[:,'hair'].value_counts()","e47e740b":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1)\nx,y = zoo.loc[:,zoo.columns != 'hair'], zoo.loc[:,'hair']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint(\"Prediction = \",prediction)","2c2ab769":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 1)\nx,y = zoo.loc[:,zoo.columns != 'hair'], zoo.loc[:,'hair']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint('With KNN (K=1) accuracy is: ',knn.score(x_test,y_test)) # accuracy","3c780ab0":"k_values = np.arange(1,25)\ntrain_accuracy = []\ntest_accuracy = []\n\nfor i, k in enumerate(k_values):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n    # Plot\nplt.figure(figsize=[13,8])\nplt.plot(k_values, test_accuracy, label = 'Testing Accuracy')\nplt.plot(k_values, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(k_values)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))\n","0ff2d787":"x = np.array(zoo.loc[:,\"eggs\"]).reshape(-1,1)\ny = np.array(zoo.loc[:,'hair']).reshape(-1,1)\n\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('Egg')\nplt.ylabel('Hair')\nplt.show()","1d57566d":"from sklearn.linear_model import LinearRegression\nregression = LinearRegression()\n\npredict_space = np.linspace(min(x),max(x)).reshape(-1,1)\nregression.fit(x,y)\npredicted = regression.predict(predict_space)\n\nprint(\"R^2 Score: \",regression.score(x,y))\n\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('Egg')\nplt.ylabel('Milk')\nplt.show()\n","d3a6e024":"from sklearn.model_selection import cross_val_score\nregression = LinearRegression()\nk=5\ncv_result = cross_val_score(regression,x,y,cv=k)\nprint(\"CV Scores: \",cv_result)\nprint(\"CV Average: \",np.sum(cv_result)\/k)","755c465a":"from sklearn.linear_model import Ridge\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\nridge = Ridge(alpha= 0.001,normalize = True)\nridge.fit(x_train,y_train)\nridge_predict = ridge.predict(x_test)\nprint(\"Ridge Score: \",ridge.score(x_test,y_test))","e66cba73":"from sklearn.linear_model import Lasso\nx = np.array(zoo.loc[:,['eggs','airborne','fins','legs',\"hair\",\"class_type\"]])\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 3, test_size = 0.3)\nlasso = Lasso(alpha = 0.0001, normalize = True)\nlasso.fit(x_train,y_train)\nridge_predict = lasso.predict(x_test)\nprint('Lasso score: ',lasso.score(x_test,y_test))\nprint('Lasso coefficients: ',lasso.coef_)","830f1ff2":"from sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = zoo.loc[:,zoo.columns != \"hair\"], zoo.loc[:,\"hair\"]\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1 )\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(\"Confisuon Matrix: \\n\",cm)\nprint(\"Classification Report: \\n\",classification_report(y_test,y_pred))","f43e74b0":"sns.heatmap(cm,annot=True,fmt=\"d\")\nplt.show()","52bd3ea5":"from sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n#hair = 1 no = 0 \nx,y = zoo.loc[:,(zoo.columns != 'hair')], zoo.loc[:,'hair']\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()\n","a91ae55b":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","d61e4f02":"# grid search cross validation with 2 hyperparameter\n# 1. hyperparameter is C:logistic regression regularization parameter\n# 2. penalty l1 or l2\n# Hyperparameter grid\nparam_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 12)\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\nprint(\"Best Accuracy: {}\".format(logreg_cv.best_score_))","1e0976ce":"# get_dummies\ndf = pd.get_dummies(zoo)\ndf.head(10)","4a79d342":"# SVM, pre-process and pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nsteps = [('scalar', StandardScaler()),\n         ('SVM', SVC())]\npipeline = Pipeline(steps)\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 1)\ncv = GridSearchCV(pipeline,param_grid=parameters,cv=3)\ncv.fit(x_train,y_train)\n\ny_pred = cv.predict(x_test)\n\nprint(\"Accuracy: {}\".format(cv.score(x_test, y_test)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))","34e86e0a":"plt.scatter(zoo['hair'],zoo['tail'])\nplt.xlabel('Hair')\nplt.ylabel('Tail')\nplt.show()","834ab4b4":"data2 = zoo.loc[:,['tail','hair']]\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 2)\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nplt.scatter(zoo['hair'],zoo['tail'],c = labels)\nplt.xlabel('Hair')\nplt.xlabel('Tail')\nplt.show()","105b23cb":"# cross tabulation table\ndf = pd.DataFrame({'labels':labels,\"hair\":zoo['hair']})\nct = pd.crosstab(df['labels'],df['hair'])\nprint(ct)","49028209":"inertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(zoo)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()\n# we choose the elbow < 1","3aaa8d0a":"data2 = zoo.drop(\"hair\",axis=1)","a3031e4d":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar,kmeans)\npipe.fit(data2)\nlabels = pipe.predict(data2)\ndf = pd.DataFrame({'labels':labels,\"hair\":zoo['hair']})\nct = pd.crosstab(df['labels'],df['hair'])\nprint(ct)","4d53024a":"from scipy.cluster.hierarchy import linkage,dendrogram\n\nmerg = linkage(data2.iloc[:20,0:5],method = 'single')\ndendrogram(merg, leaf_rotation = 90, leaf_font_size = 5)\nplt.show()","bcfae8b0":"from sklearn.manifold import TSNE\nmodel = TSNE(learning_rate=100,random_state=42)\ntransformed = model.fit_transform(data2)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list )\nplt.xlabel('Values')\nplt.ylabel('Count')\nplt.show()","97bb6666":"from sklearn.decomposition import PCA\nmodel = PCA()\nmodel.fit(data2[0:4])\ntransformed = model.transform(data2[0:4])\nprint('Principle components: ',model.components_)","e6b49c41":"# PCA variance\nscaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler,pca)\npipeline.fit(data2)\n\nplt.bar(range(pca.n_components_), pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.show()","6283197c":"# apply PCA\npca = PCA(n_components = 2)\npca.fit(data2)\ntransformed = pca.transform(data2)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list)\nplt.show()","89e0262d":"<p id='10'><h2><b>Logistic Regression<\/b><\/h2><\/p>","15634a57":"Train Test Split","a8176dd2":"Visualizing Eggs and Hair on Scatter","23474d81":"<p id='1'><h2><b>Data Overview<\/b><\/h2><\/p>\nThis dataset consists of 101 animals from a zoo. There are 16 variables with various traits to describe the animals. The 7 Class Types are: Mammal, Bird, Reptile, Fish, Amphibian, Bug and Invertebrate\n\nThe purpose for this dataset is to be able to predict the classification of the animals, based upon the variables. It is the perfect dataset for those who are new to learning Machine Learning.\n\n<b>zoo.csv<\/b>  \nAttribute Information: (name of attribute and type of value domain)\n\nanimal_name: Unique for each instance  \nhair Boolean  \nfeathers Boolean  \neggs Boolean  \nmilk Boolean  \nairborne Boolean  \naquatic Boolean  \npredator Boolean  \ntoothed Boolean  \nbackbone Boolean  \nbreathes Boolean  \nvenomous Boolean  \nfins Boolean  \nlegs Numeric (set of values: {0,2,4,5,6,8})  \ntail Boolean  \ndomestic Boolean  \ncatsize Boolean  \nclass_type Numeric  \n\n<b>class.csv<\/b>  \nThis csv describes the dataset\n\nClass_Number Numeric  \nNumber_Of_Animal_Species_In_Class Numeric  \nClass_Type character -- The actual word description of the class  \nAnimal_Names character -- The list of the animals that fall in the category of the class  ","554d280f":"<p id='9'><h2><b>Lasso<\/b><\/h2><\/p>","e34ba7af":"<p id='11'><h2><b>Support Vector Machine<\/b><\/h2><\/p>","d2c765b3":"> <h1><b>TABLE OF CONTENTS<\/b><\/h1>\n<ul>\n    <a href='#1'><li>Data Overview<\/li><\/a>\n    <a href='#3'><li>Plotting scatter matrix<\/li><\/a>\n    <a href='#4'><li>Visualizing has hair or not ?<\/li><\/a>\n    <a href='#5'><li>KNN<\/li><\/a>\n    <a href='#6'><li>Linear Regression<\/li><\/a>\n    <a href='#7'><li>Cross Validation<\/li><\/a>\n    <a href='#8'><li>Ridge<\/li><\/a>\n    <a href='#9'><li>Lasso<\/li><\/a>\n    <a href='#10'><li>Logistic Regression<\/li><\/a>\n    <a href='#11'><li>Support Vector Machine<\/li><\/a>\n    <a href='#12'><li>K-Means Clustering<\/li><\/a>\n    <a href='#13'><li>Inertia<\/li><\/a>\n    <a href='#14'><li>Dendogram<\/li><\/a>\n    <a href='#15'><li>t-distributed Stochastic Neighbor Embedding<\/li><\/a>\n    <a href='#16'><li>PCA<\/li><\/a>\n    <a href='#17'><li>References<\/li><\/a>\n<\/ul>","69a52db7":"<p id='15'><h2><b>t-distributed Stochastic Neighbor Embedding<\/b><\/h2><\/p>","0be967d9":"<p id='14'><h2><b>Dendogram<\/b><\/h2><\/p>","30c7a9c4":"<p id='3'><h2><b>Plotting scatter matrix<\/b><\/h2><\/p>","05264f12":"<p id='13'><h2><b>Inertia<\/b><\/h2><\/p>","b6b75d03":"<p id='4'><h2><b>Visualizing has hair or not ?<\/b><\/h2><\/p>","b4f74817":"<p id='8'><h2><b>Ridge<\/b><\/h2><\/p>","d3fd97d1":"<p id='16'><h2><b>PCA<\/b><\/h2><\/p>","989f44df":"<p id='17'><h2><b>References<\/b><\/h2><\/p>\n\nhttps:\/\/www.kaggle.com\/kanncaa1\/machine-learning-tutorial-for-beginners\n\nhttps:\/\/www.kaggle.com\/kanncaa1\/logistic-regression-implementation\n\nhttps:\/\/www.kaggle.com\/kanncaa1\/recommendation-systems-tutorial\n","58667f3b":"<p id='12'><h2><b>K-Means Clustering<\/b><\/h2><\/p>","7bfa011f":"<p id='5'><h2><b>KNN<\/b><\/h2><\/p>","4d9798d0":"<p id='7'><h2><b>Cross Validation<\/b><\/h2><\/p>","b82ecacc":"With this set function we find unique values in a list...","7c2e3f4d":"Plotting regression line and scatter","886862a7":"<p id='6'><h2><b>Linear Regression<\/b><\/h2><\/p>"}}