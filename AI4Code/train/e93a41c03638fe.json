{"cell_type":{"55ddd543":"code","e1321f45":"code","2cdbec9a":"code","01d90577":"code","7341c801":"code","5baf4f33":"code","4cd595f3":"code","ad318610":"code","06ce258d":"code","83403e58":"code","54f1325f":"code","87d5495e":"code","91042469":"code","838e13ae":"code","eaff0fac":"code","32fb5314":"code","272d9517":"code","5e1f48f1":"code","c3ca713b":"code","4bd3b977":"code","d87117ea":"code","1de27cbf":"code","34f80f10":"code","ae63dcc1":"code","8838763c":"code","9210a882":"code","6af200e2":"code","f3daa2af":"code","a08f9a70":"code","b4902b05":"code","5e81896b":"code","839bb271":"code","0149fb57":"code","9e76df4f":"code","f0491f53":"code","27ddf052":"code","9636e688":"code","fde49a19":"code","be15d5a4":"code","c9f738ae":"code","2b9e6e29":"code","6c9ca643":"code","9f9798dd":"code","e54c7288":"code","6372d29b":"code","9acf850d":"code","cd76d7d6":"code","d39ccc08":"code","ef27a976":"code","0e39620e":"code","7142486e":"code","ceff510c":"code","c3b45823":"code","e17f2b22":"code","33a20977":"code","4847f310":"code","da52e9fc":"code","397b1602":"code","3c5582d9":"code","b4df7221":"code","e0279480":"code","38799747":"code","ae92a44e":"code","05e28ca9":"code","29c94a92":"code","242e4c58":"code","42c52424":"code","6515647c":"code","4c877633":"code","1eb50627":"code","a530ea06":"code","625f4c6c":"code","d0870148":"markdown","59fef9f7":"markdown","f5d51ac3":"markdown","ed16790f":"markdown","15eb1ac7":"markdown","33706057":"markdown","748f6ed3":"markdown","f84ea20e":"markdown","c148324c":"markdown","f91aabbc":"markdown","5e564066":"markdown","de6bd3fe":"markdown","36b21540":"markdown"},"source":{"55ddd543":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1321f45":"train_ds = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain_ds","2cdbec9a":"test_ds = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_ds","01d90577":"train_ds['target'].value_counts()","7341c801":"train_ds.isnull().sum()","5baf4f33":"train_ds = train_ds.drop(['id','keyword','location'],axis=1)\ntrain_ds","4cd595f3":"test_ds = test_ds.drop(['id','keyword','location'],axis=1)\ntest_ds","ad318610":"import re                                  \nimport string  \nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer  ","06ce258d":"sent = []\nfor sentence in train_ds['text']:\n    sent_formatted = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', sentence) #Removes hyperlinks\n    sent_formatted = re.sub(r'#', '', sent_formatted) #Removes hastags\n    sent_formatted = re.sub(r'[0-9]', '', sent_formatted) #Removes numbers\n    sent_formatted = re.sub(r'@[A-Za-z]*', '', sent_formatted) #Removes @ tags\n    sent.append(sent_formatted) ","83403e58":"sentence = sent[100]\nprint(sentence)","54f1325f":"tokenized_sent = []\n\ntokenizer = TweetTokenizer(preserve_case=False, \n                           strip_handles=True,\n                           reduce_len=True)\n\nfor sentence in sent:\n    tokenized_sentence = tokenizer.tokenize(sentence)\n    tokenized_sent.append(tokenized_sentence)","87d5495e":"sentence = tokenized_sent[100]\nprint(sentence)","91042469":"stopwords_english = stopwords.words('english') \n\nprint('Stop words in english : \\n')\nprint(stopwords_english)\n\nprint('\\nPunctuations : \\n')\nprint(string.punctuation)","838e13ae":"formatted_sent = []\nfor sentence in tokenized_sent:\n    formatted_words = []\n    for word in sentence:\n        if word not in stopwords_english and word not in string.punctuation and len(word)>2:  #Removes word with less than 2 characters, present in english stop words or is a punctuation\n            formatted_words.append(word)\n    formatted_sent.append(formatted_words)","eaff0fac":"sentence = formatted_sent[100]\nprint(sentence)","32fb5314":"lemma_sent = []\n\nlemma = WordNetLemmatizer()\n\nfor sentence in formatted_sent:\n    lemma_words = []\n    for word in sentence:\n        lemma_word = lemma.lemmatize(word)\n        lemma_words.append(lemma_word)\n    lemma_sent.append(lemma_words)","272d9517":"sentence = lemma_sent[100]\nprint(sentence)","5e1f48f1":"final_sentence_list = []\nfor sentence in lemma_sent:\n    sent = ' '.join([str(word) for word in sentence])\n    final_sentence_list.append(sent)","c3ca713b":"sentence = final_sentence_list[100]\nprint(sentence)","4bd3b977":"train_ds['FormattedText'] = final_sentence_list","d87117ea":"train_ds","1de27cbf":"train_ds = train_ds.drop(['text'],axis = 1)\ntrain_ds.rename(columns = {'FormattedText':'text'},inplace = True)\ntrain_ds","34f80f10":"X_train = train_ds['text']\ny_train = train_ds['target']\n","ae63dcc1":"X_train_array = X_train.to_numpy()\ny_train_array = y_train.to_numpy()\n","8838763c":"X_train_array","9210a882":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\ntf.config.run_functions_eagerly(True)","6af200e2":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train_array, y_train_array))\n\nfor text,label in train_dataset.take(1):\n    print('Text: ', text.numpy())\n    print('Label: ', label.numpy())","f3daa2af":"# test_dataset = tf.data.Dataset.from_tensor_slices((X_test_array))\n\n# for test_text in test_dataset.take(1):\n#     print('Text: ', test_text.numpy())","a08f9a70":"BUFFER_SIZE = 4000\nBATCH_SIZE = 64","b4902b05":"train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n# test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","5e81896b":"VOCAB_SIZE = 12000\n\n\n#This layer will only be used in LSTM and GRU architectures for obtaining numerical vector representation of words. \n#For BERT we will use bert spcific vectorization technique.\n\nencoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder.adapt(train_dataset.map(lambda text, target: text))","839bb271":"vocabulary = np.array(encoder.get_vocabulary())\nvocabulary[10:20]","0149fb57":"print(\"Original Text :\" +str(text))\nencoded_text = encoder(text).numpy()\nprint(\"Numeric Represenation :\" +str(encoded_text))\n","9e76df4f":"model = tf.keras.Sequential([\n    encoder,\n    \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=16,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","f0491f53":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=2, min_lr=0.001)","27ddf052":"model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=['accuracy'])","9636e688":"epochs = 5\nhistory = model.fit(train_dataset,epochs=epochs,callbacks = [reduce_lr])","fde49a19":"stacked_model = tf.keras.Sequential([\n    encoder,\n    \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=16,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,return_sequences=True)),\n    \n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1)\n])\n\nstacked_model.summary()","be15d5a4":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=2, min_lr=0.001)","c9f738ae":"stacked_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=['accuracy'])","2b9e6e29":"epochs = 5\nstacked_history = stacked_model.fit(train_dataset,epochs=epochs,callbacks = [reduce_lr])","6c9ca643":"gru_model = tf.keras.Sequential([\n    encoder,\n    \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=16,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16,return_sequences=True)),\n    \n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1)\n])\n\ngru_model.summary()","9f9798dd":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.25, patience=2, min_lr=0.001)","e54c7288":"gru_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=['accuracy'])","6372d29b":"epochs = 5\ngru_history = gru_model.fit(train_dataset,epochs=epochs,callbacks = [reduce_lr])","9acf850d":"#pip install tensorflow-text","cd76d7d6":"#pip install tf-models-official","d39ccc08":"import tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer","ef27a976":"bert_model_name = 'small_bert\/bert_en_uncased_L-4_H-512_A-8'\n\n#Note: You can get these bert model and tfhub details on the tensorflow classify text with BERT page\n\ntfhub_handle_encoder = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1'\ntfhub_handle_preprocess = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3'","0e39620e":"def classifier_model():\n    \n    #Pretrained BERT \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='vectorizing')\n    encoder_inputs = preprocessing_layer(text_input)\n    bert = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT')\n    outputs = bert(encoder_inputs)\n    \n    #Our own custom classification network\n    custom = outputs['pooled_output']\n    custom = tf.keras.layers.Dropout(0.1)(custom)\n    classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')(custom)\n    \n    return tf.keras.Model(text_input, classifier)","7142486e":"bert_model = classifier_model()","ceff510c":"bert_model.summary()","c3b45823":"epochs = 5\nsteps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\n#You can also try to use Adam optimizer. But when it comes to transformer based models, it is best to fine-tune them using the same parameters as their pretraining.","e17f2b22":"bert_model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer=optimizer,\n              metrics=['accuracy'])","33a20977":"bert_history = bert_model.fit(train_dataset,epochs=epochs)","4847f310":"submission_ds = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission_ds","da52e9fc":"sent = []\nfor sentence in test_ds['text']:\n    sent_formatted = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', sentence)\n    sent_formatted = re.sub(r'#', '', sent_formatted)\n    sent_formatted = re.sub(r'[0-9]', '', sent_formatted)\n    sent_formatted = re.sub(r'@[A-Za-z]*', '', sent_formatted)\n    sent.append(sent_formatted) ","397b1602":"tokenized_sent = []\n\ntokenizer = TweetTokenizer(preserve_case=False, \n                           strip_handles=True,\n                           reduce_len=True)\n\nfor sentence in sent:\n    tokenized_sentence = tokenizer.tokenize(sentence)\n    tokenized_sent.append(tokenized_sentence)","3c5582d9":"formatted_sent = []\nfor sentence in tokenized_sent:\n    formatted_words = []\n    for word in sentence:\n        if word not in stopwords_english and word not in string.punctuation and len(word)>2:\n            formatted_words.append(word)\n    formatted_sent.append(formatted_words)","b4df7221":"lemma_sent = []\n\nlemma = WordNetLemmatizer()\n\nfor sentence in formatted_sent:\n    lemma_words = []\n    for word in sentence:\n        lemma_word = lemma.lemmatize(word)\n        lemma_words.append(lemma_word)\n    lemma_sent.append(lemma_words)","e0279480":"final_sentence_list = []\nfor sentence in lemma_sent:\n    sent = ' '.join([str(word) for word in sentence])\n    final_sentence_list.append(sent)","38799747":"test_ds['text'] = final_sentence_list\ntest_ds","ae92a44e":"X_test = test_ds['text']\nX_test_array = X_test.to_numpy()","05e28ca9":"X_test_array","29c94a92":"test_dataset = tf.data.Dataset.from_tensor_slices((X_test_array))\n\nfor test_text in test_dataset.take(2):\n    print('Text: ', test_text.numpy())","242e4c58":"test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","42c52424":"# encoded_text = []\n\n# for test_text in test_dataset:\n#     encoded_text.append(encoder(test_text).numpy())\n\n# # encoded_text\n","6515647c":"y_pred = bert_model.predict(test_dataset)","4c877633":"result = []\nfor i in y_pred:\n    if i >= 0:\n        result.append(1)\n    else:\n        result.append(0)","1eb50627":"submission_ds['target'] = result\nsubmission_ds","a530ea06":"submission_ds['target'].value_counts()","625f4c6c":"submission_ds.to_csv('submission.csv', index=False)","d0870148":"# 3. Model Building:","59fef9f7":"# 3.3 Text classification with GRUs :","f5d51ac3":"# 3.2 Text classification with stacked LSTMs :","ed16790f":"# **Extracting data and preprocessing :**","15eb1ac7":"# 2. Converting the text to a numerical vector format using tensorflow TextVectorizer:","33706057":"# 4. Preparing test data for submission:","748f6ed3":"Here, we observe keyword and location variables contain null values. Since, these are not important columns we will be dropping them along with id:","f84ea20e":"Generally, we employ the following steps while preprocessing texts:\n<ol>\n    <li>Tokenising the string<\/li>\n    <li>Converting characters to lowercase<\/li>\n    <li>Removing stop words and punctuations<\/li>\n    <li>Stemming or lemmatization<\/li>\n<\/ol>","c148324c":"The distribution of classes is optimum. No class imbalance found. If found, handle accordingly.","f91aabbc":"# 3.4 Text Classification with BERT (Transformer Model):","5e564066":"# 5. Please upvote this notebook if you find it helpful.","de6bd3fe":"# 3.1 Text classification with LSTM:","36b21540":"References: \nhttps:\/\/www.tensorflow.org\/text\/tutorials\/classify_text_with_bert"}}