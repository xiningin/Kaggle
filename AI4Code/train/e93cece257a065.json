{"cell_type":{"469aa867":"code","5e25a247":"code","2d07d032":"code","7e04414a":"code","fd7f13f9":"code","4c45d49e":"code","120a938d":"code","b32776f0":"code","1bd4c621":"code","e4be8c1f":"code","3891bfeb":"code","df1b63e3":"code","c8bf0d57":"code","2eb17c7d":"code","290e1bc1":"code","89665c79":"code","200bbb7b":"code","1dbe7b34":"code","0837a2e9":"code","2c527e32":"code","7a64626e":"code","fceb8310":"code","416383ef":"code","6640a78b":"code","09cc5b76":"code","8a9e7a94":"code","6ab01dae":"code","e70515ed":"code","c67a8057":"code","f2321b5b":"code","64418b07":"code","a0131e70":"code","9ce66eff":"code","8548747c":"code","50ed478e":"markdown","efab7a74":"markdown","a6a1e0c5":"markdown","67563ef1":"markdown","c09be48b":"markdown","c3e09c7c":"markdown","701c6d2d":"markdown","e34ffc5e":"markdown","7deb4423":"markdown","c3f8f453":"markdown","e4adbf45":"markdown","012181e4":"markdown","1d508bb6":"markdown","d533faee":"markdown","3a2da156":"markdown","03e5ccb5":"markdown","517761d6":"markdown","3fff128e":"markdown","f0febe6d":"markdown","f1008152":"markdown","f88e3439":"markdown","753e1643":"markdown","9badc542":"markdown","3bb3c938":"markdown","8ab957c0":"markdown","a2633a50":"markdown","58a75b0b":"markdown","2609a90d":"markdown","2040942e":"markdown","9f4f7d1d":"markdown","099ca763":"markdown","81bafdd6":"markdown","da19c194":"markdown","8b3039ce":"markdown","773ab86d":"markdown","e9666499":"markdown","5ad15eec":"markdown","58ef2456":"markdown","f2434009":"markdown","86f8f4bf":"markdown","0d288b3a":"markdown","07498fc9":"markdown"},"source":{"469aa867":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e25a247":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","2d07d032":"train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')","7e04414a":"train.head()\n","fd7f13f9":"test.head()","4c45d49e":"train.isnull().any(),test.isnull().any()\n","120a938d":"#string.punctuation\n","b32776f0":"#def remove_punct(text):\n#    text  = \"\".join([char for char in text if char not in string.punctuation])\n #   text = re.sub('[0-9]+', '', text)\n  #  text = re.sub('\\n', '', text)\n   # text = re.sub('\\t', '', text)\n    #return text\n\n\n#train['comment_text'] = train['comment_text'].apply(lambda x: remove_punct(x))\n#test['comment_text'] = test['comment_text'].apply(lambda x: remove_punct(x))\n\n\n","1bd4c621":"def lower(text):\n    lowercase_text = [word.lower() for word in text.split()]\n    text=\" \".join(lowercase_text)\n    return text","e4be8c1f":"train['comment_text'] = train['comment_text'].apply(lambda x: lower(x))\ntest['comment_text'] = test['comment_text'].apply(lambda x: lower(x))\n","3891bfeb":"train.head()","df1b63e3":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]","c8bf0d57":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","2eb17c7d":"#for occurence of words\n#tokenizer.word_counts\n#for index of words\n#tokenizer.word_index","290e1bc1":"list_tokenized_train[:1]","89665c79":"totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]","200bbb7b":"plt.hist(totalNumWords,bins = np.arange(0,410,10))\nplt.show()","1dbe7b34":"maxlen = 200\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","0837a2e9":"inp = Input(shape=(maxlen, ))","2c527e32":"embed_size = 128\nx = Embedding(max_features, embed_size)(inp)","7a64626e":"x = LSTM(60, return_sequences=True,name='lstm_layer')(x)","fceb8310":"x = GlobalMaxPool1D()(x)","416383ef":"x = Dropout(0.1)(x)","6640a78b":"x = Dense(50, activation=\"relu\")(x)\n","09cc5b76":"x = Dropout(0.1)(x)\n","8a9e7a94":"x = Dense(6, activation=\"sigmoid\")(x)\n","6ab01dae":"model = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","e70515ed":"batch_size = 32\nepochs = 2\nmodel.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","c67a8057":"model.summary()","f2321b5b":"sub=pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')\n","64418b07":"sub.head()","a0131e70":"from sklearn.metrics import roc_curve, roc_auc_score\npreds = model.predict(X_te, batch_size=1024)\nsub[list_classes] = preds","9ce66eff":"sub.head()","8548747c":"sub.to_csv('sample_submission.csv',index=False)","50ed478e":"the occurrence and the index of each words in the dictionary:\nit takes alot of time to execute and alot of output is shown uncomment to check","efab7a74":"Before we could pass the output to a normal layer, we need to reshape the 3D tensor into a 2D one. We reshape carefully to avoid throwing away data that is important to us, and ideally we want the resulting data to be a good representative of the original data.\n\nTherefore, we use a Global Max Pooling layer which is traditionally used in CNN problems to reduce the dimensionality of image data. In simple terms, we go through each patch of data, and we take the maximum values of each patch. These collection of maximum values will be a new set of down-sized data we can use.","a6a1e0c5":"By indicating an empty space after comma, we are telling Keras to infer the number automatically.","67563ef1":"**Max Pooling**","c09be48b":"It's finally time to put our model to the test. We'll feed in a list of 32 padded, indexed sentence for each batch and split 10% of the data as a validation set. This validation set will be used to assess whether the model has overfitted, for each batch. The model will also run for 2 epochs ( in other word it will run through the whole training set 2 times ).","c3e09c7c":"![Screenshot 2021-12-21 004643.png](attachment:e4e9f38c-2d95-4540-a763-cc7a584a9fd9.png)","701c6d2d":"Predicting with our model","e34ffc5e":"the dependent variables are in the training set itself so we need to split them up, into X and Y sets.","7deb4423":"One of the ways to go about it is to see the distribution of the number of words in sentences.","c3f8f453":"**Input Layer**\n* the inputs into our networks are our list of encoded sentences. We begin our defining an Input layer that accepts a list of sentences that has a dimension of 200.","e4adbf45":"* We start with the import of the the functions that we are going to use \n# We import the standard Keras library\n\n\n","012181e4":"Finally, we feed the output into a Sigmoid layer. The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n\n","1d508bb6":"checking \"list_tokenized_train\"  you will see that Keras has turned our words into index representation for us\n\n","d533faee":"Removing punctuations and lower cases will lower the score after checking the data more i found out that the punctuations are used to express such bad words for example s#$t\n","3a2da156":"having a 2D tensor, we pass it to a Dropout layer which indiscriminately \"disable\" some nodes so that the nodes in the next layer is forced to handle the representation of the missing data and the whole network could result in better generalization.\n\n","03e5ccb5":"![Screenshot 2021-12-21 004701.png](attachment:691eb767-a552-4f37-87a6-c3588e7f6918.png)","517761d6":"A common preprocessing step is to check for nulls, and fill the null values with something before proceeding to the next steps. If you leave the null values intact, it will affect the model results","3fff128e":"As we can see, most of the sentence length is about 30+. We could set the \"maxlen\" to about 50.\nand to avoid any kind of information loss we can extend it to 200","f0febe6d":"our comments are now econded and ready to fed to the model\n","f1008152":"* Next, we feed this Tensor into the LSTM layer. We set the LSTM to produce an output that has a dimension of 60 and want it to return the whole unrolled sequence of results\n* LSTM or RNN works by recursively feeding the output of a previous network into the input of the current network, and you would take the final output after X number of recursion. But depending on use cases, you might want to take the unrolled, or the outputs of each recursion as the result to pass to the next layer. And this is the case.","f88e3439":"Next, we pass it to our Embedding layer, where we project the words to a defined vector space depending on the distance of the surrounding words in a sentence. Embedding allows us to reduce model size and most importantly the huge dimensions we have to deal with instead of  using one-hot encoding to represent the words in our sentence.","753e1643":"We set the dropout layer to drop out 10% of the nodes.","9badc542":"General architecture of our model ","3bb3c938":"Let's clean the text ","8ab957c0":"We feed the output into a Dropout layer again.\n\n","a2633a50":"*  the embedding size (vector size) is a parameter that you can tune and experiment\n*  embedding function will return us a  3-D tensor\n*  Tensors are simply mathematical objects that can be used to describe physical properties, just like   scalars and vectors. In fact tensors are merely a generalisation of scalars and vectors; a scalar  is a zero rank tensor, and a vector is a first rank tensor.","58a75b0b":"# Loading the train and test files.\n\n","2609a90d":" lists out all your layer outputs","2040942e":"Now we move to regularization our neurol network (to reduce overfitting by using dropout and dense layers)","9f4f7d1d":"we going to  feed the comments into the LSTM as part of the neural network but we can't just feed the words as it is.\n\nSo we are going to do a natural language processing to the comments which are simply explained as :\n\n1. Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n2. Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n3. Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]","099ca763":"Looks like we don't need to deal with the null values.\n\n\n","81bafdd6":"* NOTE: each function will be explained when used.\n","da19c194":"Note that we have to define the number of unique words in our dictionary when tokenizing the sentences\n* we picked 20000 ","8b3039ce":"* removing uppercases","773ab86d":"What it does is going through the samples, recursively run the LSTM model for many time (depending on the values of the tensor passed from the last cell stocked in x) , passing in the coordinates of the words each time. And because we want the unrolled version, we will receive a Tensor shape of (None, 1, 1), where 60 is the output dimension we have defined.","e9666499":"After a drop out layer, we connect the output of drop out layer to a densely connected layer and the output passes through a RELU function. In short, this is what it does:\nActivation( (Input X Weights) + Bias)\n\n\nThe rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.","5ad15eec":"**Embedding layer**","58ef2456":"And we have to feed a stream of data that has a consistent length(fixed number of features) \nand here we will face a problem since the comments aren't all equal so\nexample:\n\nComment #1: [8,9,3,7,3,6,3,6,3,6,2,3,4,9]\n\nComment #2: [1,2]\n\nas a solution we will use padding that will make all comments having the same length by filling with missing values with zeros to get :\n\nComment #1: [8,9,3,7,3,6,3,6,3,6,2,3,4,9]\n\nComment #2: [1,2,0,0,0,0,0,0,0,0,0,0,0,0]\n\nand now another problem how can we choose the fixed number of features since  if you put it too short, you might lose some useful feature that could cost you some accuracy points down the path.If you put it too long, your LSTM cell will have to be larger to store the possible values or states.\n\n\n","f2434009":"All is left is to define the inputs, outputs and configure the learning process. We have set our model to optimize our loss function using Adam optimizer, define the loss function to be \"binary_crossentropy\" since we are tackling a binary classification.And learning rateby his default value  set at 0.001.","86f8f4bf":"LSTM takes in a tensor of [Batch Size, Time Steps, Number of Inputs]. Batch size is the number of samples in a batch, time steps is the number of recursion it runs for each input, or it could be pictured as the number of \"A\"s in the above picture. Lastly, number of inputs is the number of variables(number of words in each sentence in our case) you pass into LSTM as pictured in \"x\" above.","0d288b3a":"# Building model","07498fc9":"* Punctuation remove"}}