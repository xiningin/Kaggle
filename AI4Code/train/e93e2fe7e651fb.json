{"cell_type":{"020c929c":"code","2696d925":"code","b799ab0c":"code","2ac95404":"code","c0d20184":"code","ec862ed0":"code","ec515722":"code","b66120cf":"code","fbf4616d":"code","b5ec40fd":"code","37cdc045":"code","2832ee0c":"code","87a2332b":"code","f34d5373":"code","0a40a118":"code","0704a6b3":"code","79c7e0f1":"code","e77fdb3a":"code","04ca4f0a":"code","8f3a00e9":"code","22587c6c":"code","267fb626":"code","d8299277":"code","e3329265":"code","aa948ee9":"code","6798325b":"code","a8c04735":"code","47f08370":"code","7d0b81be":"code","ac263886":"code","4f57a4f0":"code","1f5af9f7":"code","5a2a5ff1":"code","9a95953c":"code","8d92862b":"code","a66a4499":"code","b2599541":"code","9c43313b":"code","54ca95ce":"code","8facdaef":"code","8ae59192":"code","640f3c6d":"code","bc7ba627":"code","50762afa":"code","be48caf6":"code","eab63182":"code","bb17e2f4":"code","86c139c9":"code","db34dad3":"code","e3b49788":"code","0b2c233d":"code","9e1c5e84":"code","10b173c6":"code","9a45b884":"code","9ecb75b5":"code","58c98fb6":"code","55f93232":"code","d75f191b":"code","01631921":"code","2bf6303c":"code","7c3344cc":"code","92d8e0ae":"code","3e4b5ca8":"code","a394b4d3":"code","061a1626":"code","3d309129":"code","9d9dc277":"code","e23b19b3":"code","763fa56e":"code","9932702b":"code","476cb97d":"code","dead0ef8":"code","7b1c6e41":"code","439bcd64":"code","768beee6":"code","42208199":"code","88d0956d":"code","32e7ecd8":"code","0d4f8a9e":"code","799214ac":"code","69aee2f8":"markdown","abf0b66e":"markdown","8848ef46":"markdown","e3a885de":"markdown","548bb328":"markdown","1111b75d":"markdown","d5a8f40b":"markdown","86a821fa":"markdown","63b8afe7":"markdown","68efa13f":"markdown","94b31405":"markdown","7f0d1c68":"markdown","d780e724":"markdown","29f8988c":"markdown","fd930321":"markdown","fe14d954":"markdown","1a959777":"markdown","e0a9452f":"markdown","46e939b9":"markdown","e1452bda":"markdown","2720e974":"markdown","b9bc228b":"markdown","4e217115":"markdown","089b5d84":"markdown","e34e97e9":"markdown","77f39c37":"markdown","841f7c36":"markdown"},"source":{"020c929c":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom catboost import CatBoostClassifier, Pool\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.pipeline import Pipeline \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\nimport time\nimport datetime\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GroupKFold\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport numpy as np, pandas as pd, os, gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2696d925":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')","b799ab0c":"def detect_num_cols_to_shrink(list_of_num_cols, dataframe):\n \n    convert_to_int8 = []\n    convert_to_int16 = []\n    convert_to_int32 = []\n    \n    #  sadly the datatype float8 does not exist\n    convert_to_float16 = []\n    convert_to_float32 = []\n    \n    for col in list_of_num_cols:\n        \n        if dataframe[col].dtype in ['int', 'int8', 'int32', 'int64']:\n            \n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 255:\n                convert_to_int8.append(col)\n                \n            elif diff < 65535:\n                convert_to_int16.append(col)\n                \n            elif diff < 4294967295:\n                convert_to_int32.append(col)   \n                \n        elif dataframe[col].dtype in ['float', 'float16', 'float32', 'float64']:\n            \n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 65535:\n                convert_to_float16.append(col)\n                \n            elif diff < 4294967295:\n                convert_to_float32.append(col) \n        \n    list_of_lists = []\n    list_of_lists.append(convert_to_int8)\n    list_of_lists.append(convert_to_int16)\n    list_of_lists.append(convert_to_int32)\n    list_of_lists.append(convert_to_float16)\n    list_of_lists.append(convert_to_float32)\n    \n    return list_of_lists","2ac95404":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","c0d20184":"train_transaction = reduce_mem_usage(train_transaction, verbose = True)\ntrain_identity = reduce_mem_usage(train_identity,verbose = True)\ntest_transaction = reduce_mem_usage(test_transaction, verbose=True)\ntest_identity = reduce_mem_usage(test_identity,verbose = True)","ec862ed0":"pd.set_option('display.max_columns', None)","ec515722":"train_transaction.head()","b66120cf":"train_identity.head()","fbf4616d":"test_transaction.head()","b5ec40fd":"test_identity.head()","37cdc045":"#gom 2 dataset l\u1ea1i th\u00e0nh 1 theo transactionID\nfinal_train = pd.merge(train_transaction, train_identity, how = 'left', on = 'TransactionID')\n\nfinal_test = pd.merge(test_transaction, test_identity, how = 'left', on = 'TransactionID')\n#final_test = test_transaction.merge(test_identity, on='TransactionID', how = 'inner')","2832ee0c":"del train_transaction\ndel train_identity\ndel test_transaction\ndel test_identity","87a2332b":"#in ra\nprint(final_train.shape)\nprint(final_test.shape)","f34d5373":"def different_columns(traincols, testcols):\n    \n    for i in traincols:\n        \n        if i not in testcols:\n            \n            print(i)\n            \ndifferent_columns(final_train.columns, final_test.columns)","0a40a118":"final_test = final_test.rename(columns = {\"id-01\": \"id_01\", \"id-02\": \"id_02\", \"id-03\": \"id_03\", \n                            \"id-06\": \"id_06\", \"id-05\": \"id_05\", \"id-04\": \"id_04\", \n                            \"id-07\": \"id_07\", \"id-08\": \"id_08\", \"id-09\": \"id_09\", \n                            \"id-10\": \"id_10\", \"id-11\": \"id_11\", \"id-12\": \"id_12\", \n                            \"id-15\": \"id_15\", \"id-14\": \"id_14\", \"id-13\": \"id_13\", \n                            \"id-16\": \"id_16\", \"id-17\": \"id_17\", \"id-18\": \"id_18\", \n                            \"id-21\": \"id_21\", \"id-20\": \"id_20\", \"id-19\": \"id_19\", \n                            \"id-22\": \"id_22\", \"id-23\": \"id_23\", \"id-24\": \"id_24\", \n                            \"id-27\": \"id_27\", \"id-26\": \"id_26\", \"id-25\": \"id_25\", \n                            \"id-28\": \"id_28\", \"id-29\": \"id_29\", \"id-30\": \"id_30\", \n                            \"id-31\": \"id_31\", \"id-32\": \"id_32\", \"id-33\": \"id_33\", \n                            \"id-34\": \"id_34\", \"id-35\": \"id_35\", \"id-36\": \"id_36\", \n                            \"id-37\": \"id_37\", \"id-38\": \"id_38\"})\nfinal_test.head()","0704a6b3":"different_columns(final_train.columns, final_test.columns)","79c7e0f1":"y = final_train.isFraud\ny.describe\n#1 l\u00e0 fraud, c\u00f2n 0 l\u00e0 k ph\u1ea3i fraud\nsns.displot(\n    data=y\n)\n#d\u1eef li\u1ec7u kh\u00e1 ph\u00e2n b\u1ed1 sai, x\u1eed l\u00fd ntn","e77fdb3a":"y.dtypes","04ca4f0a":"def getNulls(data):\n    \n    total = data.isnull().sum()\n    percent = data.isnull().sum() \/ data.isnull().count()\n    missing_data = pd.concat([total, percent], axis = 1, keys = ['total', 'precent'])\n    \n    return missing_data","8f3a00e9":"missing_data_train = getNulls(final_train)\nmissing_data_train.head(434).T","22587c6c":"del missing_data_train","267fb626":"numerical = final_train.select_dtypes(include='number')\nnumerical.head()","d8299277":"def make_corr(Vs,Vtitle=''):\n    cols = ['isFraud'] + Vs\n    plt.figure(figsize=(15,15))\n    sns.heatmap(numerical[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n    if Vtitle!='': plt.title(Vtitle,fontsize=14)\n    else: plt.title(Vs[0]+' - '+Vs[-1],fontsize=14)\n    plt.show()\nVs = ['V'+str(i) for i in range(5,16)]\nVtitle = 'V5 - V16'\nmake_corr(Vs,Vtitle)","e3329265":"sns.distplot(a = numerical['card1'])","aa948ee9":"numerical['TransactionAmt'].describe()","6798325b":"sns.distplot(a = numerical['TransactionAmt'])","a8c04735":"sns.distplot(a= np.log1p(numerical['TransactionAmt']))","47f08370":"#xem th\u1eed categorical\ncategorical = final_train.select_dtypes(exclude='number')\ncategorical","7d0b81be":"ntrain = final_train.shape[0]\nntest = final_test.shape[0]\nall_data = pd.concat([final_train, final_test], axis = 0, sort = False)","ac263886":"all_data = all_data.drop(columns=['isFraud'])","4f57a4f0":"del numerical\ndel categorical\ndel final_train\ndel final_test","1f5af9f7":"all_data.shape","5a2a5ff1":"all_data_cols = all_data.columns","9a95953c":"n = (all_data.dtypes != 'object')\n\nnum_all_cols = list(n[n].index) \n\nprint(num_all_cols)","8d92862b":"num_cols_to_shrink_all = detect_num_cols_to_shrink(num_all_cols, all_data)\n\nconvert_to_int8 = num_cols_to_shrink_all[0]\nconvert_to_int16 = num_cols_to_shrink_all[1]\nconvert_to_int32 = num_cols_to_shrink_all[2]\n\nconvert_to_float16 = num_cols_to_shrink_all[3]\nconvert_to_float32 = num_cols_to_shrink_all[4]","a66a4499":"print(\"starting with converting process....\")\n\nfor col in convert_to_int16:\n    \n    all_data[col] = all_data[col].astype('int16') \n    \nfor col in convert_to_int32:\n    all_data[col] = all_data[col].astype('int32') \n\nfor col in convert_to_float16:\n    all_data[col] = all_data[col].astype('float16')\n    \nfor col in convert_to_float32:\n    all_data[col] = all_data[col].astype('float32')\n    \nprint(\"successfully converted!\")","b2599541":"all_data","9c43313b":"v =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\nv += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\nv += [332, 325, 335, 338] # b4 lots NAN","54ca95ce":"cols = ['V'+str(x) for x in v]\nfor i in all_data_cols:\n    if (i.startswith(\"V\")) and i not in cols:\n        all_data = all_data.drop(columns=[i])\n\nall_data ","8facdaef":"numerical = all_data.select_dtypes(include='number')\ncategorical = all_data.select_dtypes(exclude = 'number')\n\nnumeric_transformer = Pipeline(steps=[('mean',SimpleImputer(strategy='constant',fill_value=-1))])\ncategorical_transformer = Pipeline(steps=[('constant', SimpleImputer(strategy='constant',fill_value=-1))])","8ae59192":"#final_all_data[numerical.columns]= numeric_transformer.fit_transform(final_all_data[numerical.columns])\n#final_all_data[categorical.columns] = categorical_transformer.fit_transform(final_all_data[categorical.columns])","640f3c6d":"final_train = all_data[ : ntrain]\nfinal_test = all_data[ntrain : ]","bc7ba627":"del all_data","50762afa":"for i,f in enumerate(final_train.columns):\n    # FACTORIZE CATEGORICAL VARIABLES\n    if (np.str(final_train[f].dtype)=='category')|(final_train[f].dtype=='object'): \n        df_comb = pd.concat([final_train[f],final_test[f]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        if df_comb.max()>32000: print(f,'needs int32')\n        final_train[f] = df_comb[:len(final_train)].astype('int16')\n        final_test[f] = df_comb[len(final_train):].astype('int16')\n    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n    elif f not in ['TransactionAmt','TransactionDT']:\n        mn = np.min((final_train[f].min(),final_test[f].min()))\n        final_train[f] -= np.float32(mn)\n        final_test[f] -= np.float32(mn)\n        final_train[f].fillna(-999,inplace=True)\n        final_test[f].fillna(-999,inplace=True)","be48caf6":"y.value_counts()","eab63182":"final_train.head()","bb17e2f4":"final_test.head()","86c139c9":"#s\u1eed d\u1ee5ng CatBoostClassifier regressor\n#model = LogisticRegression(solver='lbfgs',max_iter=10000)\n\n#model = CatBoostClassifier()\nmodel = xgb.XGBClassifier(n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        #nthread=4,\n        tree_method='hist' \n        #tree_method='gpu_hist' \n                           )","db34dad3":"X_train = final_train\ny_train = y\nX_test = final_test","e3b49788":"del final_train\ndel y\ndel final_test","0b2c233d":"final_col = X_train['TransactionID']\nX_train = X_train.drop(columns=['TransactionID'])","9e1c5e84":"final_col","10b173c6":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","9a45b884":"cols = list(X_train.columns )\ncols.remove('TransactionDT')","9ecb75b5":"idxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","58c98fb6":"X_train","55f93232":"#h = model.fit(X_train.loc[idxT,cols], y_train[idxT], \n        #eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n       # verbose=50, early_stopping_rounds=100)","d75f191b":"cols","01631921":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    clf = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            tree_method='hist'\n            # USE GPU\n            #tree_method='gpu_hist' \n        )        \n    h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n    oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n    preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        \nprint('#'*20)\nprint ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","2bf6303c":"def encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","7c3344cc":"encode_CB('card1','addr1')\n#encode_CB('card1_addr1','P_emaildomain')\n\nX_train['day'] =X_train.TransactionDT \/ (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT \/ (24*60*60)\nX_test['uid'] =X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","92d8e0ae":"%%time\n# FREQUENCY ENCODE UID\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\nencode_FE(X_train,X_test,['uid'])\n# AGGREGATE \nencode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREGATE\nencode_AG(['C14'],['uid'],['std'],X_test,X_test,fillna=True,usena=True)\n# AGGREGATE \nencode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREATE \nencode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n# NEW FEATURE\nX_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\nX_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\nprint('outsider15')","3e4b5ca8":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT\/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT\/np.float32(24*60*60) ","a394b4d3":"cols = list(X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['DT_M','day','uid']:\n    cols.remove(c)","061a1626":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","3d309129":"X_train","9d9dc277":"#X = X.drop(columns = ['id_07','id_21','id_22','id_24' ,'id_25' ,'id_26'])\n# CHRIS - TRAIN 75% PREDICT 25%\nidxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","e23b19b3":"X_train = X_train.drop(columns=['uid'])","763fa56e":"cols = X_train.columns","9932702b":"X_train","476cb97d":"idxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","dead0ef8":"cols","7b1c6e41":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    clfz = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            tree_method='hist'\n            # USE GPU\n            #tree_method='gpu_hist' \n        )        \n    hz = clfz.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n    oof[idxV] += clfz.predict_proba(X_train[cols].iloc[idxV])[:,1]\n    preds += clfz.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        \nprint('#'*20)\nprint ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","439bcd64":"feature_imp = pd.DataFrame(sorted(zip(clfz.featureimportances,cols)), columns=['Value','Feature']) plt.figure(figsize=(20, 10)) sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50]) plt.title('XGB95 Most Important Features') plt.tight_layout() plt.show()","768beee6":"#start_time = time.time()\n#predicted_X_test = hz.predict_proba(final_test[X.columns])\n#print(\"Time to execute SKLearn: %s\" % (time.time() - start_time))","42208199":"#predicted_X_test.shape\n","88d0956d":"#predicted_X_test[:, 1]","32e7ecd8":"test_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\nfinal_col = test_transaction['TransactionID']","0d4f8a9e":"final_col","799214ac":"my_submission = pd.DataFrame({'TransactionID':final_col, 'isFraud': preds})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('.\/submission.csv', index=False)\nmy_submission.head(5)","69aee2f8":"# **2.2 D\u1eef li\u1ec7u Categorical**","abf0b66e":"* H\u1ecd v\u00e0 t\u00ean: Ph\u1ea1m C\u00f4ng Ch\u00ednh\n* MSSV: 18020235\n\n**T\u1ed4NG QU\u00c1T:**\n\n1. GI\u1edaI THI\u1ec6U DATA:\n* 1.1 Th\u00f4ng tin t\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u\n* 1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t v\u1ec1 Target\n* 1.3 Metric \u0111\u00e1nh gi\u00e1\n2. KHAI PH\u00c1 D\u1eee LI\u1ec6U\n* 2.1 D\u1eef li\u1ec7u Numerical\n* 2.2 D\u1eef li\u1ec7u Categorical\n* 2.3 T\u1ed5ng h\u1ee3p c\u00e1c d\u1eef li\u1ec7u l\u1ea5y l\u00e0m Feature\n3. FEATURE ENGINEERING\n* 3.1 X\u1eed l\u00fd d\u1eef li\u1ec7u numerical\n* 3.2 X\u1eed l\u00fd d\u1eef li\u1ec7u Categorical\n4. X\u00c2Y D\u1ef0NG M\u00d4 H\u00ccNH","8848ef46":"**Vesta features:**\nC\u00e1c c\u1ed9t V chia s\u1ebb \"r\u1ea5t nhi\u1ec1u\" m\u1ed1i t\u01b0\u01a1ng quan v\u00e0 m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng l\u1edbn Nan\u2019s. M\u1ee5c ti\u00eau c\u1ee7a b\u01b0\u1edbc n\u00e0y l\u00e0 t\u00ecm c\u00e1c c\u1ed9t \"t\u01b0\u01a1ng t\u1ef1\" d\u1ef1a tr\u00ean s\u1ed1 l\u01b0\u1ee3ng \"NaN\" v\u00e0 T\u01b0\u01a1ng quan> 0,75. Qu\u00e1 tr\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c t\u1ef1 \u0111\u1ed9ng h\u00f3a cho t\u1ea5t c\u1ea3 c\u00e1c nh\u00f3m NaN kh\u00e1c nhau, s\u1eed d\u1ee5ng m\u1ed9t t\u1eadp l\u1ec7nh. Ph\u01b0\u01a1ng ph\u00e1p \u0111\u01b0\u1ee3c gi\u1ea3i th\u00edch nh\u01b0 d\u01b0\u1edbi \u0111\u00e2y:","e3a885de":"**T\u1ed5ng k\u1ebft v\u1ec1 Nan:**\n* Nhi\u1ec1u c\u1ed9t c\u00f3 Nan h\u01a1n 25% => N\u1ebfu b\u1ecf th\u00ec s\u1ebd m\u1ea5t kh\u00e1 nhi\u1ec1u data\n* \u00cdt d\u1eef li\u1ec7u n\u00ean fill Nan b\u1eb1ng s\u1ed1 \u1ea3o (-999 ho\u1eb7c -1)\n* C\u00f3 nhi\u1ec1u c\u1ed9t mang th\u00f4ng tin \u0111\u1eb7c tr\u01b0ng (card1-6 l\u00e0 s\u1ed1 th\u1ebb, addr1-2 l\u00e0 s\u1ed1 nh\u00e0). Fill b\u1eb1ng mean s\u1ebd kh\u00f4ng c\u00f3 \u00fd ngh\u0129a","548bb328":"**FACTORIZE CATEGORICAL VARIABLES and SHIFT ALL NUMERICS POSITIVE SET NAN to -1**","1111b75d":"**No UID**","d5a8f40b":"**Ch\u1ecdn Features**","86a821fa":"**UID Magic**","63b8afe7":"* TransactionDT: h\u1eb9n gi\u1edd t\u1eeb m\u1ed9t ng\u00e0y gi\u1edd tham chi\u1ebfu nh\u1ea5t \u0111\u1ecbnh (kh\u00f4ng ph\u1ea3i d\u1ea5u th\u1eddi gian th\u1ef1c t\u1ebf)\n* TransactionAMT: s\u1ed1 ti\u1ec1n thanh to\u00e1n giao d\u1ecbch b\u1eb1ng USD\n* card1 - card6: th\u00f4ng tin th\u1ebb thanh to\u00e1n, ch\u1eb3ng h\u1ea1n nh\u01b0 lo\u1ea1i th\u1ebb, lo\u1ea1i th\u1ebb, ng\u00e2n h\u00e0ng ph\u00e1t h\u00e0nh, qu\u1ed1c gia, v.v.\n* addr: \u0111\u1ecba ch\u1ec9\n* dist: kho\u1ea3ng c\u00e1ch\n* C1-C14: \u0111\u1ebfm, ch\u1eb3ng h\u1ea1n nh\u01b0 c\u00f3 bao nhi\u00eau \u0111\u1ecba ch\u1ec9 \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y c\u00f3 li\u00ean quan \u0111\u1ebfn th\u1ebb thanh to\u00e1n, v.v. \u00dd ngh\u0129a th\u1ef1c t\u1ebf \u0111\u01b0\u1ee3c che gi\u1ea5u.\n* D1-D15: th\u1eddi gian giao d\u1ecbch, ch\u1eb3ng h\u1ea1n nh\u01b0 c\u00e1c ng\u00e0y gi\u1eefa giao d\u1ecbch tr\u01b0\u1edbc \u0111\u00f3, v.v.\n* Vxxx: Vesta \u0111\u00e3 thi\u1ebft k\u1ebf c\u00e1c t\u00ednh n\u0103ng phong ph\u00fa, bao g\u1ed3m x\u1ebfp h\u1ea1ng, \u0111\u1ebfm v\u00e0 c\u00e1c quan h\u1ec7 th\u1ef1c th\u1ec3 kh\u00e1c.","68efa13f":"* ProductCD: product code, the product for each transaction\n* card4, card6: payment card information, such as card type, card category, issue bank, country, etc.\n* M1-M9: match, such as names on card and address, etc.\n* idxx, DeviceType, DeviceInfo: Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.","94b31405":"**TransactionAmt:**","7f0d1c68":"**1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t Metric**\n\n* Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n* N\u00ean s\u1eed d\u1ee5ng F1 score, t\u00ecm Precision, Recall, Confusion Matrix\n* N\u00ean d\u00f9ng model n\u00e0o \u0111\u1ec3 d\u1ec5 t\u1ed1i \u01b0u AUC???","d780e724":"# **2. KHAI PH\u00c1 D\u1eee LI\u1ec6U**","29f8988c":"# **1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t Target**","fd930321":"**Pipeline Preprocessing**","fe14d954":"**2.1 D\u1eef li\u1ec7u Numerical:**","1a959777":"**Gi\u1ea3m k\u00edch th\u01b0\u1edbc d\u1eef li\u1ec7u**","e0a9452f":"# Submission","46e939b9":"# **1. GI\u1edaI THI\u1ec6U DATA**\n    \n# 1.1 T\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o\n\n* train_identity:\n    * 23 lo\u1ea1i Numerical\n    * 14 categorical\n    * 4 Boolean\n    * T\u1ed5ng quan: Nhi\u1ec1u bi\u1ebfn thi\u1ebfu d\u1eef li\u1ec7u\n\n* train_transaction:\n    * 356 lo\u1ea1i Numerical @@ (30\/6\/2021 ch\u01b0a s\u1eeda)\n    * 30 categorical\n    * 8 lo\u1ea1i boolean\n    * T\u1ed5ng quan: File h\u1ebft 1.7gb, qu\u00e1 nhi\u1ec1u bi\u1ebfn, c\u1ea7n gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u\n    * Ch\u01b0a r\u00f5 chi\u1ec1u gi\u1ea3m","e1452bda":"**2.3 D\u1eef li\u1ec7u Boolean**","2720e974":"**Distance:**\ndistances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.","b9bc228b":"# 4. X\u00e2y d\u1ef1ng Model","4e217115":"Ki\u1ec3m tra gi\u1eefa 2 file c\u00f3 c\u00e1c c\u1ed9t n\u00e0o kh\u00e1c nhau","089b5d84":"# 3. Feature Engineering","e34e97e9":"**Ki\u1ec3m tra l\u1ea1i train v\u00e0 test c\u00f3 c\u00f2n c\u1ed9t kh\u00e1c kh\u00f4ng**","77f39c37":"**S\u1eed d\u1ee5ng Log1P \u0111\u1ec3 \u0111\u01b0a v\u1ec1 normal distribution**","841f7c36":"**Card1, 2, 3, 5:**"}}