{"cell_type":{"35980016":"code","74115bc8":"code","b01dcb41":"code","7190a9c9":"code","e841b52b":"code","5550f4e1":"code","053a4a4c":"code","05b74708":"code","55aed324":"code","c8acdc97":"code","c43cb827":"code","12bbb03b":"code","e7ba457e":"code","930fa372":"code","d1998ae4":"code","bcb324f1":"code","cd9b6f67":"code","684805e0":"code","4e0c63c9":"code","f497be97":"code","cb2bff68":"code","bff669dd":"code","2ee4aacb":"code","46eccb51":"code","1e153f4b":"code","17c1c8bc":"code","98941185":"code","ab08a123":"code","9fdd27ef":"code","ef9900f7":"code","d08f3b93":"code","3e98df3d":"code","a2eb954d":"code","c16df416":"code","69d238c2":"code","d3cac5da":"code","b390ae38":"code","a3f1f160":"code","a69ff52c":"code","adef3df1":"code","74a8c339":"code","0d5bf8f1":"code","378bb89b":"code","1f9cf2f7":"code","4bd0be86":"code","63218db2":"code","ac572cc7":"code","9ff286a5":"code","c22f4c3a":"code","68e06cbb":"code","0851badc":"code","4b054e69":"code","c1403959":"code","f4aa496f":"code","81a8ba00":"code","20b91de2":"code","77c689e5":"markdown","58dd9e63":"markdown","953bf82d":"markdown","9435de99":"markdown","dcc0bc87":"markdown","e27abd91":"markdown","145c1379":"markdown","c1d6c9ea":"markdown","4660025c":"markdown","0e2cc34a":"markdown","9ddaaa98":"markdown","429cb4fd":"markdown","dd5d0067":"markdown","10360a03":"markdown","304c9a9b":"markdown","83acc7ae":"markdown","dfe2e597":"markdown","be8c93e0":"markdown","e0541326":"markdown","9a1e4664":"markdown","6ec60d66":"markdown","927c54fb":"markdown","8a3d654b":"markdown","3489f5bb":"markdown","3dbf9e04":"markdown","11052297":"markdown","d5940d0a":"markdown","1d4c9436":"markdown","9714d032":"markdown","52717700":"markdown","09506cb9":"markdown","ae566c05":"markdown","7cfc0978":"markdown","fd0ab954":"markdown","7592ce38":"markdown","ef49807f":"markdown","308cc0b9":"markdown","0778dec0":"markdown","a688e22b":"markdown","15e89c3b":"markdown","77904310":"markdown","1b9ad835":"markdown","daddaa06":"markdown","3e574e8d":"markdown","d85c313d":"markdown","b6e1cbe8":"markdown","d9a701ce":"markdown","39873470":"markdown","d7e0d6c4":"markdown","96c24522":"markdown","1cc46c36":"markdown","81eb2796":"markdown","578d5820":"markdown","0ff249c4":"markdown","dbaf0210":"markdown","e5be3372":"markdown","fd8f3f70":"markdown"},"source":{"35980016":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#preprocessing and visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, Normalizer\n\n#model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split, RepeatedKFold\nfrom sklearn.pipeline import make_pipeline \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score , max_error , mean_squared_log_error, make_scorer\nfrom sklearn.compose import TransformedTargetRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74115bc8":"def load_data(file_path):\n    file = pd.read_csv(file_path)\n    return file","b01dcb41":"def all_data_concat(train, test):\n    all_data = pd.concat(objs = [train, test], axis = 0)\n    train_objs_num = len(train)\n    return all_data, train_objs_num","7190a9c9":"def data_information(data, dataname):\n    \"\"\" This function is to print all the details of dataset as the first part of the exploratory data analysis. \"\"\"\n    print((\"-\" * 100 ) + \"\\n\" + dataname + \": DATA FRAME SHAPE:\")\n    print(data.shape) \n    print((\"-\" * 100 ) + \"\\n\" + dataname +\": DATA FRAME HEAD:\")\n    print(data.head(10))\n    print((\"-\" * 100 ) + \"\\n\" + dataname +\": DATA FRAME COLUMNS:\")\n    print(data.columns)\n    print((\"-\" * 100 ) + \"\\n\" + dataname +\": DATA FRAME DESCRIBTION:\")\n    print(data.describe())\n    print((\"-\" * 100 ) + \"\\n\" + dataname +\": DATA FRAME INFO:\")\n    print(data.info())\n    print((\"-\" * 100 ) + \"\\n\" + dataname +\": DATA FRAME NULLS:\")\n    print(data.isna().sum())  \n    print((\"-\" * 100 ) + \"\\n\")\n    get_missing_values(data)\n    print((\"-\" * 100 ) + \"\\n\")\n    numerical_features, categorical_features = data_type(data)\n    print(\"Numerical features are :\", numerical_features )\n    print(\"Categorical features are :\", categorical_features )\n    print((\"-\" * 100 ) + \"\\n\")\n","e841b52b":"def data_type(data):\n    \"\"\"This function is used to seperate the categorical features and the numerical ones.\"\"\"\n    numerical_features = [x for x in data.select_dtypes(include='number').columns if x not in [target]]\n    categorical_features = data.select_dtypes(exclude='number').columns.tolist()\n    return numerical_features, categorical_features","5550f4e1":"#Information about Missing Data\ndef get_missing_values(data, dataname = \"data\"):\n    \"\"\" This function is used to get the missing data from the features and the ratios of each column. \"\"\"\n    print( \"Number of missing values from \" + dataname)\n    #get the sum of all missing values in the dataset\n    missing = data.isnull().sum()\n    print(\"Missing data of each column\")\n    missing = missing[missing > 0]\n    missing.sort_values(inplace = True)\n    #missing.plot.bar()\n    print(\"Missing data ratios from \" + dataname)\n    _nulls = (data.isnull().sum() \/ len(data)) * 100\n    _nulls = _nulls[_nulls > 0]\n    missing_data = pd.DataFrame({'Missing Ratio %' :_nulls}).sort_values('Missing Ratio %', ascending=False)\n    print(missing_data)","053a4a4c":"def examin_outlayers(data, columns):\n    Q1 = data[columns].quantile(0.25)\n    Q3 = data[columns].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers_count = (((data[columns] < (Q1 - 1.5 * IQR)) | (data[columns] > (Q3 + 1.5 * IQR))).sum()).sort_values(ascending=False)\n    print(outliers_count.head(12))\n    ","05b74708":"def numerical_feature_distribution(data, numerical_features):\n    \"\"\" This function is used to plot the numerical features' distripution \"\"\"\n    \n    numerical_features_df = data[numerical_features].copy()\n    skewed_feats = numerical_features_df.apply(lambda x: skew(x)).sort_values(ascending=False)\n    print(\"\\nSkew in numerical features: \\n\")\n    skew_numerical = pd.DataFrame({'Skew' :skewed_feats})\n    print(skew_numerical.head(10))\n    \n    data[numerical_features].hist(bins = 100, figsize=(30,25));\n","55aed324":"def ordina_features_boxplot(data,target):\n    # creating orders list of pandas series that store the median values sorted of each category\n    orders = []\n    for col in ordinal:\n        orders.append(data.groupby(by=[col])[target].median().sort_values())\n    def boxplot(x, y, **kwargs):\n        sns.boxplot(x=x, y=y)\n        x=plt.xticks(rotation=90)\n\n        #create a box plot to show how the output changes with qualitative features\n    f = pd.melt(data, id_vars = target, value_vars = ordinal)\n    g = sns.FacetGrid(f, col = \"variable\",  col_wrap = 4, sharex = False, sharey = False, height = 5)\n    g = g.map(boxplot, \"value\", \"y\")","c8acdc97":"def label_col_plot(train,cols,label='y'):\n    '''Plotting data ordered by mean of label in each category'''\n    num_cols = int(len(cols))\n    figure, axes = plt.subplots(int(np.ceil(num_cols\/3)), 3)\n    figure.suptitle('Plotting data ordered by mean of label in each category')\n    \n    \n    figure.set_size_inches(15, 10)\n    plt.tight_layout()\n    rows = 0\n    for i in range(0, num_cols-1, 3):\n        mean_in_col = train[[cols[i],label]].groupby(cols[i]).mean()\n        mean_in_col.sort_values(by=label,ascending=False)\n        axes[rows, 0].scatter(x=mean_in_col.index,y=mean_in_col[label])\n        title = cols[i] + \" by mean of label in each category\"\n        axes[rows, 0].set_title(title)\n        \n        if (i+1) >= num_cols:\n            break\n        mean_in_col = train[[cols[i+1],label]].groupby(cols[i+1]).mean()\n        mean_in_col.sort_values(by=label,ascending=False)\n        axes[rows, 1].scatter(x=mean_in_col.index,y=mean_in_col[label])\n        title = cols[i+1] + \" by mean of label in each category\"\n        axes[rows, 1].set_title(title)\n        \n        if (i+2) >= num_cols:\n            break\n        mean_in_col = train[[cols[i+2],label]].groupby(cols[i+2]).mean()\n        mean_in_col.sort_values(by=label,ascending=False)\n        axes[rows, 2].scatter(x=mean_in_col.index,y=mean_in_col[label])\n        title = cols[i+2] + \" by mean of label in each category\"\n        axes[rows, 2].set_title(title)\n        rows+=1\n        ","c43cb827":"def categorical_feature_distribution(data, categorical_features):\n    \"\"\" This function is used to plot the categorical features features' distripution \"\"\"\n    for cat in categorical_features:\n        f = plt.figure(figsize=(10,5))\n        sns.countplot(x= cat, data = data)","12bbb03b":"def analyze_target(data, target):\n    sns.set(style = 'whitegrid', palette = \"deep\", font_scale = 1.1, rc = {\"figure.figsize\": [8, 5]})\n    sns.histplot(data = data, x = target, bins = 100)\n    \n    target_df = data[:len(data)].copy()\n    target_df = target_df[target]\n    skewed_target = skew(target_df)\n    print(\"\\nSkew in target feature: \\n\")\n    print(skewed_target)\n    \n    f,  (ax1, ax2)  =  plt.subplots(nrows=1, ncols=2, figsize=(13, 6))\n\n    ax1 = data[['Hour','y']].groupby(['Hour']).sum().reset_index().plot(kind='bar',\n                                           legend = False, title =\"Counts of Bike Rentals per hour\", \n                                           stacked=True, fontsize=12, ax=ax1)\n    ax1.set_xlabel(\"Hour\", fontsize=12)\n    ax1.set_ylabel(\"y\", fontsize=12)\n\n    ax2 = data[['Holiday','y']].groupby(['Holiday']).sum().reset_index().plot(kind='bar',  \n          legend = False, stacked=True, title =\"Counts of Bike Rentals by Day\", fontsize=12, ax=ax2)\n\n    ax2.set_xlabel(\"Holiday\", fontsize=12)\n    ax2.set_ylabel(\"Count\", fontsize=12)\n    ax2.set_xticklabels(['Holiday','No Holiday'])\n\n    f.tight_layout()","e7ba457e":"def heat_map_plot(data):\n    \"\"\" This function is used to plot the Correlation heat map of all features. \"\"\"\n    f, ax = plt.subplots(figsize=(32, 26))\n    corr = data.corr()\n    mp = sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, annot = True)\n    mp.set_title(label='Dataset correlation', fontsize=20)","930fa372":"def plot_correlated_features(corr_features):\n    \"\"\" This function is used to plot the highly correlated featurs and each other to find if we have to use polynomial \n        regression transformation  between them before using the linear regression at the end \"\"\"    \n    sns.pairplot(data[corr_features])","d1998ae4":"def drop_columns(data, cols_drop):\n    \"\"\"This function is used to drop a column or row from the dataset.\"\"\"\n    drop_strategies = [(cols_drop,1)]\n    for columns, ax in drop_strategies:\n        if len(columns) == 1: \n            data = data.drop(labels = columns, axis = ax)\n        else:\n            for column in columns:\n                data = data.drop(labels = column, axis = ax)\n    return data","bcb324f1":"def rename_columns(data, cols_rename):\n    \"\"\"This function is to rename columns. \"\"\"\n    data = data.rename(columns = cols_rename)\n    return data","cd9b6f67":"def map_cat_to_num(data, col, values):\n    \"\"\"This function is to transform categorical value from column to numerical value >> { : , :}. \"\"\"\n    data[col]= data[col].map(values)\n    return data","684805e0":"def string_to_categorical(data, cols):\n    for col in cols:\n        data[col] = pd.Categorical(data[col])\n    return data","4e0c63c9":"def impute_row_values(data, col, val0, val1):     \n    \"\"\"This function is to replace value 0 from column with value1. \"\"\"\n    data[col] = data[col].transform(lambda x:val1 if(x==val0) else x)\n    return data","f497be97":"def extract_DMY_form_date(data, date_col):\n    data[date_col] = pd.to_datetime(data[date_col])\n    data[\"Day\"] = data[date_col].dt.dayofweek\n    data[\"Month\"] = data[date_col].dt.month\n    data[\"Year\"] = data[date_col].dt.year    \n    return data","cb2bff68":"#This part is taken from the notebook found here : https:\/\/www.kaggle.com\/alaasedeeq\/object-oriented-programming-for-data-science\ndef fillna(data, fill_strategies):       \n    \"\"\"This function fills NA\/NaN values in a specific column using a specified method(zero,mean,...)\"\"\"\n    def fill(column, fill_with):\n            if str(fill_with).lower() in ['zero', 0]:\n                data[column].fillna(0, inplace=True)\n            elif str(fill_with).lower()=='mode':\n                data[column].fillna(data[column].mode()[0], inplace=True)\n            elif str(fill_with).lower()=='mean':\n                data[column].fillna(data[column].mean(), inplace=True)\n            elif str(fill_with).lower()=='median':\n                data[column].fillna(data[column].median(), inplace=True)\n            else:\n                data[column].fillna(fill_with, inplace=True)\n            return data    \n    #imputing columns according to its strategy\n    for columns, strategy in fill_strategies:\n        if len(columns)==1:\n            fill(columns[0], strategy)\n        else:\n            for column in columns:\n                fill(column, strategy)\n    return data","bff669dd":"# put here the columns you needed to be grouped together\ndef grouping_columns(cols_group):\n    \"\"\" This function is used to group any needed columns togeether. \"\"\"\n    pass","2ee4aacb":"def label_encoder(data,columns):\n    \"\"\" This function is used to encode the data to categorical values \"\"\"\n    # Convert all categorical collumns to numeric values\n    lbl = LabelEncoder() \n    data[columns] = data[columns].apply(lambda x:lbl.fit_transform(x.astype(str)).astype(int))\n    return data ","46eccb51":"def get_dummies(data,columns):\n    \"\"\"This function is used to convert the data to dummies values. \"\"\"\n    # convert our categorical columns to dummies\n    for col in columns:\n        dumm = pd.get_dummies(data[col], prefix = col, dtype = int)\n        data = pd.concat([data, dumm], axis = 1)\n    data.drop(columns, axis = 1, inplace = True)\n    return data","1e153f4b":"def norm_data(data,columns):\n    \"\"\" This function is used to normalize numeric the data.\"\"\"\n    data[columns] = data[columns].apply(lambda x:np.log1p(x)) #Normalize the data with Logarithms\n    return data","17c1c8bc":"def plot_dist(y, y_pred_test, y_pred_train, y_pred_val):\n    figure, axes = plt.subplots(ncols=4)\n    figure.set_size_inches(20, 4)\n    plt.tight_layout()\n    sns.histplot(y, ax=axes[0], bins=100, kde = True)\n    axes[0].set_title('Original Data (Train+Val) Distribution')\n    sns.histplot(y_pred_test, ax=axes[1], bins=100,  kde = True)\n    axes[1].set_title('Predicted Test Data Distribution')\n    sns.histplot(y_pred_train, ax=axes[2], bins=100, kde = True)\n    axes[2].set_title('Predicted Train Data Distribution')\n    if y_pred_val != None:\n        sns.histplot(y_pred_val, ax=axes[3], bins=100, kde = True)\n        axes[3].set_title('Predicted Val Data Distribution')","98941185":"def rmsle(y_true, y_pred, convertExp=True):\n    # Apply exponential transformation function\n    if convertExp:\n        y_true = np.exp(y_true)\n        y_pred = np.exp(y_pred)\n        \n    # Convert missing value to zero after log transformation\n    log_true = np.nan_to_num(np.array([np.log(y+1) for y in y_true]))\n    log_pred = np.nan_to_num(np.array([np.log(y+1) for y in y_pred]))\n    \n    # Compute RMSLE\n    output = np.sqrt(np.mean((log_true - log_pred)**2))\n    return output\n","ab08a123":"def plot_feature_importances(model):\n    feats = {} # a dict to hold feature_name: feature_importance\n    for feature, importance in zip(X.columns, model.feature_importances_):\n        feats[feature] = importance #add the name\/value pair \n\n    plt.figure(figsize=(30,7))\n    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n    importances.sort_values(by='importance').plot(kind='bar', rot=90)","9fdd27ef":"#load datasets into two datfarmes : train and test \ntrain_data = load_data(\"..\/input\/seoul-bike-rental-ai-pro-iti\/train.csv\")\ntest_data = load_data(\"..\/input\/seoul-bike-rental-ai-pro-iti\/test.csv\")\n\n\n#concatenate all train and test data into one dataset\nall_data, train_objs_num = all_data_concat(train_data, test_data)\n\n\n#Select the target column\ntarget = \"y\"\nseed = 2021","ef9900f7":"###Display general initial information about the dataset\n# shape  # info  # description  # nulls  # oultliers\ndata_information(all_data, \" All Dataset\")","d08f3b93":"# def preprocessing(data, data_name = \"Dataset\"):\n#     cols_rename = {\"Temperature(\ufffdC)\": \"Temperature\", \"Dew point temperature(\ufffdC)\": \"Dew point temperature\"}\n#     data = rename_columns(data, cols_rename)                                      # Rename columns\n#     data = extract_DMY_form_date(data, \"Date\")                                    # Change date into Day, month and year columns\n#     data = map_cat_to_num(data,\"Holiday\",{\"No Holiday\":0,'Holiday':1})            # Map values of specific columns\n#     data = map_cat_to_num(data, \"Functioning Day\" , {\"No\":0,'Yes':1})\n#     data[\"Weekend\"]=[1 if x>=5 else 0 for x in data[\"Day\"]]\n#     data[\"isHoliday\"] = data['Weekend']| data[\"Holiday\"]\n    \n#     #data = drop_columns(data, [\"\"])                                              # Drop columns\n#     #data = get_dummies(data,['Day','Year',\"Seasons\",'Functioning Day',\"isHoliday\",\"Holiday\",\"Weekend\"]) \n#     #data = map_cat_to_num(data, \"Seasons\" , {'Spring': 1, 'Summer': 2,'Autumn':3,'Winter':4})\n#     #data = impute_row_values(data, cat_column_to_change, val0, val1)              # Impute row value\n#     #data = impute_row_values(data, \"Solar Radiation (MJ\/m2)\", 0, data[\"Solar Radiation (MJ\/m2)\"].mean)\n#     #data = norm_data(data,norm_columns)                                           # Normalize columns\n#     #label_encoder(data,[\"Seasons\", \"Hour\", \"Day\", \"Month\",\"Year\",\"Holiday\",\"Functioning Day\"])\n    \n#     print(\"Data preprocessing of \" + data_name + \" ended sucssesfully\")\n#     return data","3e98df3d":"def preprocessing(data, data_name = \"Dataset\"):\n    all_data_processed[\"Date\"] = pd.to_datetime(all_data_processed[\"Date\"],format='%d\/%m\/%Y')\n    all_data_processed[\"Holiday\"]= all_data['Holiday'].map( {\"No Holiday\": 0,'Holiday': 1})\n    all_data_processed[\"Functioning Day\"]= all_data[\"Functioning Day\"].map( {\"No\": 0,'Yes': 1})\n    all_data_processed[\"Month\"] = all_data_processed['Date'].dt.month\n    all_data_processed[\"Year\"] = all_data_processed['Date'].dt.year\n    all_data_processed[\"Day\"] = all_data_processed['Date'].dt.dayofweek\n    all_data_processed[\"Weekend\"]=[1 if x>=5 else 0 for x in all_data_processed[\"Day\"]]\n    all_data_processed[\"isHoliday\"] = all_data_processed['Weekend']| all_data_processed[\"Holiday\"]\n    \n    print(\"Data preprocessing of \" + data_name + \" ended sucssesfully\")\n    return all_data_processed","a2eb954d":"all_data = all_data.rename(columns={\"Temperature(\ufffdC)\": \"Temperature\", \"Dew point temperature(\ufffdC)\": \"Dew point temperature\"})\nall_data_processed = all_data.copy()\nall_data_processed = preprocessing(all_data_processed, \"All dataset\")\ntrain_processed = all_data_processed[:train_objs_num]\ntest_processed = all_data_processed[train_objs_num:]","c16df416":"numerical = ['Temperature', 'Humidity(%)','Wind speed (m\/s)', 'Visibility (10m)', 'Dew point temperature',\n             'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\nordinal = ['Year','Month','Day','Hour', 'Seasons', 'Holiday', 'Functioning Day', 'isHoliday']\ntarget = 'y'\nnumerical_and_target = numerical + list(target)\nall_features = numerical_and_target + ordinal","69d238c2":"analyze_target(train_processed, target)","d3cac5da":"numerical_feature_distribution(train_processed, numerical)","b390ae38":"ordina_features_boxplot(train_processed, target)","a3f1f160":"label_col_plot(train_processed,numerical)","a69ff52c":"sns.pairplot(train_processed[numerical_and_target])","adef3df1":"corr = train_processed[numerical_and_target].corr()\nheat_map_plot(corr)","74a8c339":"examin_outlayers(train_processed,all_features)","0d5bf8f1":"plt.figure(figsize=(15,10))\nsns.lineplot(x=\"Date\", y=\"y\",\n             data=train_processed)","378bb89b":"train_processed[\"Year_month\"] = train_processed[\"Date\"].dt.strftime('%Y-%m')\nplt.figure(figsize=(15,10))\nsns.lineplot(x=\"Year_month\", y=\"y\",\n             data=train_processed)\n","1f9cf2f7":"all_data_processed = pd.get_dummies(all_data_processed, columns=['Day','Year',\"Seasons\",'Functioning Day',\"isHoliday\",\"Holiday\",\"Weekend\"])","4bd0be86":"train = all_data_processed[:train_objs_num]\ntest = all_data_processed[train_objs_num:]","63218db2":"selected_features = ['Hour', 'Temperature', 'Humidity(%)', 'Wind speed (m\/s)',\n       'Visibility (10m)' ,'Solar Radiation (MJ\/m2)','Rainfall(mm)', 'Month', 'Day_0',\n       'Day_5', 'Day_6',  'Seasons_Autumn', 'Seasons_Spring', 'Seasons_Summer',\n       'Seasons_Winter', 'Functioning Day_0', 'Functioning Day_1',\n       'Holiday_0', 'Holiday_1','Weekend_0', 'Weekend_1']\n\n\nX = train[selected_features]\ny_no_log = train[target]\ny = np.log1p(y_no_log)\nX_test = test[selected_features]\n","ac572cc7":"X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.4, random_state=seed, shuffle = True)","9ff286a5":"# gbr = GradientBoostingRegressor(random_state=seed)\n# parameters = {'learning_rate': [0.01, 0.02, 0.03, 0.1],\n#                 'subsample'    : [0.3, 0.5, 1],\n#                 'n_estimators' : [1000, 2000, 3000],\n#                 'max_depth'    : [4, 5, 6, 7],\n#                 'loss'         : [\"huber\",\"ls\"],\n#                 'min_samples_leaf': [1,2,3,4], \n#                 'max_leaf_nodes': [None, 10, 20],\n#                 'max_features': [\"log2\", \"sqrt\", None]\n#                 }\n# rmsle_scorer = make_scorer(rmsle, greater_is_better=False) \n# grid_search_gradient_boosting = GridSearchCV(gbr, parameters, cv = 5, \n#                                             n_jobs=-1, scoring=rmsle_scorer, verbose=True)\n\n\n# pipeline = make_pipeline(StandardScaler(), grid_search_gradient_boosting)\n# #pipeline.fit(X, y)","c22f4c3a":"# grid_search_gradient_boosting.best_estimator_","68e06cbb":"gbr = GradientBoostingRegressor(learning_rate=0.02, max_depth=10, max_features=21,\n                                n_estimators=5000, subsample=0.6, loss='huber', min_samples_leaf=2,\n                               max_leaf_nodes=4, random_state=2)\n\npipeline = make_pipeline(StandardScaler(), gbr)\npipeline.fit(X_train, y_train)\n\ny_train_predicted = pipeline.predict(X_train) \ny_train_predicted = np.expm1(y_train_predicted)\ny_train_predicted[y_train_predicted<0] = 0\n\ny_val_predicted = pipeline.predict(X_val) \ny_val_predicted = np.expm1(y_val_predicted)\ny_val_predicted[y_val_predicted<0] = 0\n\nprint(\"The square root of the mean square log error of the training data is\",\n       mean_squared_log_error(np.expm1(y_train), y_train_predicted)**0.5)\nprint(\"The square root of the mean square log error of the validation data is\",\n       mean_squared_log_error(np.expm1(y_val), y_val_predicted)**0.5)\n\n\n# The square root of the mean square log error of the training data is 0.26309761317444164\n# The square root of the mean square log error of the validation data is 0.3523324928122321","0851badc":"pipeline.fit(X, y)\n\ny_train_predicted = pipeline.predict(X) \ny_train_predicted = np.expm1(y_train_predicted)\ny_train_predicted[y_train_predicted<0] = 0\n\nprint(\"The square root of the mean square log error of all data is\", \n      mean_squared_log_error(np.expm1(y), y_train_predicted)**0.5)\n\n## 0.27693606780155","4b054e69":"y_test_predicted = pipeline.predict(X_test) \ny_test_predicted = np.expm1(y_test_predicted)","c1403959":"y_functioning_0_indx = np.where(X_test[\"Functioning Day_1\"] == 0)[0]\ny_test_predicted[y_functioning_0_indx] = 0","f4aa496f":"plot_dist(np.expm1(y), y_test_predicted, y_train_predicted, None)","81a8ba00":"plot_feature_importances(gbr)","20b91de2":"test_data['y']= np.round(y_test_predicted)\noutput = test_data[['ID','y']]\noutput.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","77c689e5":"### Input Output distributions","58dd9e63":"#### Extract Date column into year, month and day columns ","953bf82d":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\"> Challenge 2 - ITI\ud83d\udcda<\/p>","9435de99":"#### Group columns","dcc0bc87":"#### Examining outlayers ","e27abd91":"### Root Mean Square Log Error","145c1379":"#### Categorical Features distribution ","c1d6c9ea":"<h2><center>Display Information about the Data<\/center><\/h2>","4660025c":"#### Convert data to dummies values","0e2cc34a":"## Notes:\n* Year: Data in 2018 is much larger than data in 2017\n* Hour: There is a trend in hour, greatest demand is at 6 pm\n* Seasons: Number of bike rental counts is greater in summer. It's order is Winter, Spring, Autmn, Summer. This is reflected also by month\n* Holiday: holidays without weekends, and isHoliday: holidays with weekends. Days with no holiday has a slightly higher median than days with holidays","9ddaaa98":" <h2><center>Feature Preprocessing<\/center><\/h2>","429cb4fd":"<h2><center>Load train and test datasets<\/center><\/h2>","dd5d0067":"## Notes:\n* Year: Data in 2018 is much larger than data in 2017\n* Hour: There is a trend in hour, greatest demand is at 6 pm\n* Seasons: Number of bike rental counts is greater in summer. It's order is Winter, Spring, Autmn, Summer. This is reflected also by month\n* Holiday: holidays without weekends, and isHoliday: holidays with weekends. Days with no holiday has a slightly higher median than days with holidays","10360a03":"<h1><center>ML Model<\/center><\/h1>","304c9a9b":" <h3><center>Train Test Split <\/center><\/h3>","83acc7ae":"#### Convert object (String) columns to categorical columns\n","dfe2e597":"<h3><center>Ordinal features box plot<\/center><\/h3>","be8c93e0":"# Introduction \nThis notebook uses the following dataset to predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.: https:\/\/www.kaggle.com\/c\/seoul-bike-rental-ai-pro-iti\/overview. \n\n<br> This project consists of the following main sections:<br>\n<ul>\n    <li> I   : <a><b>Data Loading.<\/b><\/a><\/li>\n    <ul>\n        <li>1) loads the data from the csv files into data frame.\n        <li>2) Concats the data (train and test) in one DataFrame.<\/li>\n    <\/ul><br>\n    <li> II  : <a><b>Exploratory data analysis.<\/b><\/a><\/li>\n    <ul>\n        <li>1) Gets feature dtypes, info, shape\n        <li>3) plot feature distribution.\n        <li>4) Exploring Missing data.\n        <li>5) Analyzing target variable.\n        <li>6) Correlation heatmap.<\/li>\n    <\/ul><br>\n    <li> III : <a><b>Data Pre_processing.<\/b><\/a><\/li>\n    <ul>\n        <li>1) Drop and Rename the needed columns   \n        <li>2) convert date columns to datetime and to DMY \n        <li>3) Fills the null values with (mean, meadian, zero,....etc).\n        <li>4) Encodes the data and Converts it to dummies values.\n        <li>5) Normalizes the data before ML.<\/li>\n    <\/ul><br>\n    <li> IV  : <a><b>Machine Learning model preperation<\/b><\/a><\/li>\n    <br>\n    <li> V   : <a><b> Problem solution <\/b><\/a><\/li>\n    <ul>\n        <li>1) Load the files.\n        <li>2) Show the information.\n        <li>3) determine the preprocessing columns.\n        <li>4) Applys the ML algorithms and shows the results.\n        <li>5) Find the best model and then fits it to the data.\n        <li>6) Save the predictions to a csv file.<\/li>\n    <\/ul>\n<\/ul>\n<br>","e0541326":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\"> 3. Data Preprocessing<\/p>","9a1e4664":"#### Loading Packages","6ec60d66":"* Making non- functioning day counts to 0","927c54fb":"<h3><center>Feature Engineering <\/center><\/h3>","8a3d654b":"<h3><center>Visualize correlation<\/center><\/h3>","3489f5bb":"* All data training and submission","3dbf9e04":"\n### Feature Importances\n","11052297":"#### Drop unnecessary columns or rows","d5940d0a":"#### Exploring Missing data","1d4c9436":"#### Analyzing target variable","9714d032":"<h3><center>Apply the ML Model (Gradient Boosting) <\/center><\/h3>\n","52717700":"#### Impute raw values with other values ","09506cb9":"#### Seperate categorical data from numerical data","ae566c05":"## Notes:\n* There is no linear relationships except tempreture with dew tempreture and dew tempreture with humidity. \n* Linear regression will most likely not work","7cfc0978":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\"> 2. Exploratory Data Analysis<\/p>\n","fd0ab954":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\">5. Problem Solution<\/p>","7592ce38":"#### Correlation heatmap","ef49807f":"* Try grid search for gradient boosting parameters (All Data)","308cc0b9":" <h3><center>Submission <\/center><\/h3>\n","0778dec0":"<h3><center>Time Series Plot <\/center><\/h3>","a688e22b":"<h3><center>Detecting relationships<\/center><\/h3>","15e89c3b":"#### Features box plot","77904310":"<h3><center>Examining Outliers<\/center><\/h3>","1b9ad835":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\"> 1. Data Loading<\/p>\n","daddaa06":"#### Normalize the data","3e574e8d":"* Train the best estimator on the training data\n","d85c313d":"#### Numerical Features distribution ","b6e1cbe8":"#### Encode data to categorical values","d9a701ce":"#### Fill NA\/NAN values with zero,mean,mode,...","39873470":" <h3><center>Feature Selection <\/center><\/h3>","d7e0d6c4":"#### Data Information","96c24522":"#### Rename necessary columns","1cc46c36":"<h3><center>Label column plot<\/center><\/h3>","81eb2796":"* Output Analysis","578d5820":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center; border-radius: 15px 50px;\">5. ML Model Preperation<\/p>","0ff249c4":"#### Map categorical columns to numerical ","dbaf0210":"#### Plotting data ordered by mean of label","e5be3372":"<h3><center>Target Feature Historgram<\/center><\/h3>","fd8f3f70":"<h3><center>Numerical Features Distribution<\/center><\/h3>"}}