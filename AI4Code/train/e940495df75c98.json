{"cell_type":{"c2e57a77":"code","a99cdeea":"code","897d195a":"code","f96d3d38":"code","3e8e03c3":"code","93aa76c1":"code","a2aea6cf":"code","a116feff":"code","355e1437":"code","5bdaeb2b":"code","fe8cc125":"code","cc4570f0":"code","2db48fd6":"code","dfc58c99":"code","d9e1800d":"code","d974e120":"code","8df9e6e9":"code","2e75d715":"code","dc8b7180":"code","5e9aa7f3":"code","f4ac8698":"code","7ab9555a":"code","f0508f6e":"code","2bb08b1b":"code","3bab3dbf":"code","47bb1a35":"code","6cc9f88f":"code","40484250":"code","b5481aea":"markdown","eb02c846":"markdown","2d96e43d":"markdown","1f167c8e":"markdown","500d761b":"markdown","311adc58":"markdown","bc1bcb79":"markdown","010ed279":"markdown","337eacf1":"markdown","3b590f65":"markdown","aad81a01":"markdown","24731e34":"markdown","ff8ede80":"markdown","da165b60":"markdown","bda96a79":"markdown","8a1939be":"markdown","567511ba":"markdown","fc6eb5cf":"markdown","920097ce":"markdown","932d4e2f":"markdown","338f71f7":"markdown","76538959":"markdown","91d4350b":"markdown","f2fa1178":"markdown","c5a17ccf":"markdown","d19871da":"markdown","9c01f962":"markdown","fbb92b32":"markdown","0c91987c":"markdown","1d38ca34":"markdown","f831c50e":"markdown","40648dba":"markdown","02439107":"markdown","26e4ca0c":"markdown","5d45682d":"markdown","b6d9008f":"markdown","a26354d3":"markdown","0af45603":"markdown","41980013":"markdown","23c87843":"markdown","c02572cd":"markdown","33cf3452":"markdown","a5f48a62":"markdown"},"source":{"c2e57a77":"#Used for interactivity\n!jupyter nbextension enable --py widgetsnbextension\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as  widgets ","a99cdeea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom scipy.special import comb\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","897d195a":"df = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\ndf\ndf = df.drop(labels = \"Id\", axis = 1)\ndf","f96d3d38":"sLength = df[['SepalLengthCm','Species']]\nsLength","3e8e03c3":"sLength_bySpecies = {}\nfor i, id in enumerate(sLength['Species'].unique()):\n    sLength_bySpecies[id] = sLength[sLength['Species'] == id]\nfor spec in sLength_bySpecies:\n    print(\"Describe for \" + spec + \":\")\n    print(sLength_bySpecies[spec].describe())","93aa76c1":"sWidth = df[['SepalWidthCm','Species']]\nsWidth_bySpecies = {}\nfor i, id in enumerate(sWidth['Species'].unique()):\n    sWidth_bySpecies[id] = sWidth[sWidth['Species'] == id]\nfor spec in sWidth_bySpecies:\n    print(\"Describe for \" + spec + \":\")\n    print(sWidth_bySpecies[spec].describe())","a2aea6cf":"pLength = df[['PetalLengthCm','Species']]\npLength_bySpecies = {}\nfor i, id in enumerate(pLength['Species'].unique()):\n    pLength_bySpecies[id] = pLength[pLength['Species'] == id]\nfor spec in pLength_bySpecies:\n    print(\"Describe for \" + spec + \":\")\n    print(pLength_bySpecies[spec].describe())","a116feff":"pWidth = df[['PetalWidthCm','Species']]\npWidth_bySpecies = {}\nfor i, id in enumerate(pWidth['Species'].unique()):\n    pWidth_bySpecies[id] = pWidth[pWidth['Species'] == id]\nfor spec in pWidth_bySpecies:\n    print(\"Describe for \" + spec + \":\")\n    print(pWidth_bySpecies[spec].describe())","355e1437":"#Everything must be done by COPY\nX , y = df.iloc[:,[0,1,2,3]].copy() , df['Species'].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.5, random_state = 1)\nprint(y_train.value_counts())\nX_train = X_train.values; X_test = X_test.values; y_train = y_train.values; y_test = y_test.values","5bdaeb2b":"#Returns 2-D Array of combinations in form: [[x1,x2],[x3,x4]...[xi,xj]]\ndef combinations(x):\n    count = 0\n    mesh = np.array(np.meshgrid(x,x))\n    combination = mesh.T.reshape(-1,2)  #Gives me same pairs like [0,0], and converse\/commutative like [0,1] [1,0] i need to get rid of these\n    arr = [tuple(row) for row in np.sort(combination)]; uniq1 = np.unique(arr, axis = 0) #This will get rid of converse\/commutative ones\n    for i,row in enumerate(uniq1): #This will get rid of same pairs like [0,0]\n        if row[0] == row[1]:\n            uniq1 = np.delete(uniq1, count, axis = 0)\n            count = count - 1\n        count = count + 1\n    return uniq1","fe8cc125":"#Will graph grapha a scatterplot coloring the unique classes\ndef scatterClass(axe, feature, xval, yval):\n    for i,idx in enumerate(df[feature].unique()):\n        x = np.array(y_train[:] == idx)\n        axe.scatter(X_train[x,xval], X_train[x,yval], label = idx) #It's somehow automatically coloring using distinct colors per species\n        axe.set_xlabel(X.columns[xval])\n        axe.set_ylabel(X.columns[yval])\n        axe.legend()","cc4570f0":"combs = combinations(np.array([0,1,2,3]))\niterations = np.size(combs, axis=0)\nplt.figure(figsize=(30,10))\nfor j in range(iterations):\n    ax = plt.subplot(2,4,j+1)\n    tup = combs[j,:]#grab tupple of combinations in form (x,y) and I should have num = iterations of combinations\n    scatterClass(ax, 'Species', tup[0], tup[1])\nplt.tight_layout()\nplt.show()","2db48fd6":"plt.figure(figsize=(10,10))\nax = plt.subplot()\nscatterClass(ax, 'Species', 2, 3)\nplt.show()","dfc58c99":"#Using countourf\n#Lets first make a normal line y = x + 0 which means w0 = 0, w1 = 1, w2 = -1\nw = np.array([0,1,-1])\ndelta = 0.025\ndef weightsContourf(w0,w1,w2):\n    return contourfLine(np.array([w0,w1,w2]))\ndef contourfLine(w):\n    x = np.arange(df['PetalLengthCm'].values.min() - 0.25, df['PetalLengthCm'].values.max() + 0.25,delta) #We are using PetalLengthCm\n    y = np.arange(df['PetalWidthCm'].values.min() - 0.25,df['PetalWidthCm'].values.max() + 0.25,delta)\n    x1,x2 = np.meshgrid(x,y)\n    #Now I want to be able to make tuples to span through the entire graph from 0 to 3\n    x_vector = np.array([x1.ravel(),x2.ravel()]).T #We have coordinates [x1,x2] = [0,0],[0,1],[0,2]...[3,3]\n    Z = x_vector.dot(w[1:]) + w[0] #This is z = w0 + w1*x1 ... + wn*xn\n    Z = Z.reshape(x1.shape)\n    y_hat = np.where(Z >= 0, 1, -1) #This is where the prediction is made according to the value of Z\n    fig, ax = plt.subplots()\n    ax.contourf(x1, x2, y_hat, alpha = 0.8)\n    scatterClass(ax,'Species',2,3)\n    plt.title(\"Contourf\")\n    plt.xlabel('PetalLengthCm')\n    plt.ylabel('PetalWidthCm')\n    plt.show()\ndis1 = interactive(weightsContourf, w0=widgets.FloatSlider(min=-3,max=3,step=0.1,value=2), w1=widgets.FloatSlider(min=-3,max=3,step=0.1,value=-0.5), w2=widgets.FloatSlider(min=-3,max=3,step=0.1,value=-1))\ndisplay(dis1)","d9e1800d":"#Using line plot\ndef weightsLine(w0,w1,w2):\n    return decisionLine(np.array([w0,w1,w2]))\ndef decisionLine(w):\n    x = np.arange(df['PetalLengthCm'].values.min() - 0.25, df['PetalLengthCm'].values.max() + 0.25,delta)\n    fig, ax = plt.subplots()\n    if(w[2] == 0 and (w[0] == 0 or w[1] == 0)):\n        ax.plot()\n    else:\n        y = -(w[1]\/w[2])*x - (w[0]\/w[2])\n        ax.plot(x,y)\n    plt.title(\"Line Plot\")\n    scatterClass(ax,'Species',2,3)\n    plt.xlabel('PetalLengthCm')\n    plt.ylabel('PetalWidthCm')\n    plt.xlim(df['PetalLengthCm'].values.min() - 0.25, df['PetalLengthCm'].values.max())\n    plt.ylim(df['PetalWidthCm'].values.min() - 0.25,df['PetalWidthCm'].values.max())\n    plt.show()\n    print()\ndisp2 = interactive(weightsLine, w0=widgets.FloatSlider(min=-3,max=3,step=0.1,value=2), w1=widgets.FloatSlider(min=-3,max=3,step=0.1,value=-0.5), w2=widgets.FloatSlider(min=-3,max=3,step=0.1,value=-1))\ndisplay(disp2)","d974e120":"#This is used to make sure we don't have Iris-virginica in our data to keep it an easy binary classification\n#Note, y_test[2] didn't work because the index is not correct\nX_train_novirg = X_train.copy()\ny_train_novirg = y_train.copy()\ncounter = 0\nfor i in range(len(y_train)):\n    if y_train[i] == 'Iris-virginica':\n        X_train_novirg = np.delete(X_train_novirg,counter, axis=0)\n        y_train_novirg = np.delete(y_train_novirg,counter)\n        counter = counter - 1\n    counter = counter + 1\nX_train_novirg = X_train_novirg[:,[2,3]] #This will give us [PetalLengthCm,PetalWidthCm]\ny_train_novirg = np.where(y_train_novirg == 'Iris-setosa',1,-1) #Turns class labels into 1 and -1\n\nX_test_novirg = X_test.copy()\ny_test_novirg = y_test.copy()\ncounter = 0\nfor i in range(len(y_test)):\n    if y_test[i] == 'Iris-virginica':\n        X_test_novirg = np.delete(X_test_novirg,counter,axis=0)\n        y_test_novirg = np.delete(y_test_novirg,counter,axis=0)\n        counter = counter - 1\n    counter = counter + 1\nX_test_novirg = X_test_novirg[:,[2,3]]\ny_test_novirg = np.where(y_test_novirg == 'Iris-setosa',1,-1)","8df9e6e9":"from sklearn.linear_model import Perceptron\nppt = Perceptron(penalty = None, alpha = 0.01, max_iter = 10, shuffle = True, eta0 = 1, random_state = 1) #tol is the stopping criteria, which is a parameter as well\nppt.fit(X_train_novirg,y_train_novirg)\nweightsContourf(ppt.intercept_,ppt.coef_[0,0],ppt.coef_[0,1])","2e75d715":"print(\"Score for training set: \" + str(ppt.score(X_train_novirg,y_train_novirg)*100) + \"%\")\nprint(\"Score for test set: \" + str(ppt.score(X_test_novirg,y_test_novirg)*100) + \"%\")\nprint(\"Weights:\" ,ppt.coef_, \"Intercept:\", ppt.intercept_)","dc8b7180":"X_train_noset = X_train[:,[2,3]].copy()\ny_train_noset = y_train.copy()\nX_test_noset = X_test[:,[2,3]].copy()\ny_test_noset = y_test.copy()\ncounter = 0\nfor i in range(len(y_train)):\n    if y_train[i] == 'Iris-setosa':\n        X_train_noset = np.delete(X_train_noset,counter,axis=0)\n        y_train_noset = np.delete(y_train_noset,counter)\n        counter = counter - 1\n    counter = counter + 1\ny_train_noset = np.where(y_train_noset == 'Iris-versicolor', 1 ,-1)\ncounter = 0\nfor i in range(len(y_test)):\n    if y_test[i] == 'Iris-setosa':\n        X_test_noset = np.delete(X_test_noset,counter,axis=0)\n        y_test_noset = np.delete(y_test_noset,counter)\n        counter = counter - 1\n    counter = counter + 1\ny_test_noset = np.where(y_test_noset == 'Iris-versicolor', 1 ,-1)","5e9aa7f3":"ppt2 = Perceptron(max_iter = 1000000, shuffle = True, random_state = 1, eta0 = 0.000001, tol = None) #Note, setting tol to None will allow it to go past 1e-3 which is very important\nppt2.fit(X_train_noset,y_train_noset)\nweightsContourf(ppt2.intercept_,ppt2.coef_[0,0],ppt2.coef_[0,1])","f4ac8698":"print(\"Score for training set: \", str(ppt2.score(X_train_noset,y_train_noset)*100)+\"%\")\nprint(\"# of wrong classifications:\" , str((1. - ppt2.score(X_train_noset,y_train_noset))*len(X_train_noset)))\nprint()\nprint(\"Score for testing set: \", str(ppt2.score(X_test_noset,y_test_noset)*100)+\"%\")\nprint(\"# of wrong classifications:\" , str((1. - ppt2.score(X_test_noset,y_test_noset))*len(X_test_noset)))","7ab9555a":"#First we will use SepalWidthCm\nX_3d = df[['SepalWidthCm','PetalLengthCm','PetalWidthCm']].copy().values\ny_3d = df['Species'].values\n#Now delete Iris-setosa\nX_vv = X_3d.copy()\ny_vv = y_3d.copy()\ncounter = 0\nfor i in range(len(y)):\n    if y[i] == 'Iris-setosa':\n        X_vv = np.delete(X_vv,counter, axis = 0)\n        y_vv = np.delete(y_vv,counter)\n        counter = counter - 1\n    counter = counter + 1\n#split the training and testing data set\nX_vv_train, X_vv_test, y_vv_train, y_vv_test = train_test_split(X_vv, y_vv, stratify = y_vv, test_size = 0.50, random_state = 1)","f0508f6e":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nfor i in np.unique(y_vv):\n    index = np.array(y_vv[:] == i)\n    ax.scatter(X_vv[index,0],X_vv[index,1],X_vv[index,2], label = i)\nplt.show()","2bb08b1b":"ppt3 = Perceptron(max_iter = 1000000, shuffle = True, random_state = 1, eta0 = 0.000001, tol = None)\nppt3.fit(X_vv_train, y_vv_train)\nprint(\"Score for training set: \", str(ppt3.score(X_vv_train,y_vv_train)*100)+\"%\")\nprint(\"# of wrong classifications:\" , str((1. - ppt3.score(X_vv_train,y_vv_train))*len(X_vv_train)))\nprint()\nprint(\"Score for testing set: \", str(ppt3.score(X_vv_test,y_vv_test)*100)+\"%\")\nprint(\"# of wrong classifications:\" , str((1. - ppt3.score(X_vv_test,y_vv_test))*len(X_vv_test)))","3bab3dbf":"ppt3 = Perceptron(max_iter = 10000, shuffle = True, random_state = 1, eta0 = 0.00000001, tol = None)\nppt3.fit(X_vv_train,y_vv_train)\nprint(\"Training accuracy: \", str(ppt3.score(X_vv_train,y_vv_train)*100)+\"%\")\nprint(\"Testing accuracy: \", str(ppt3.score(X_vv_test,y_vv_test)*100)+\"%\")","47bb1a35":"X_all = df.iloc[:,[0,1,2,3]].values\ny_all = df.iloc[:,-1].values\ny_all = np.reshape(y_all,(-1,1))\n#Now delete Iris-setosa\nX_all_vv = X_all.copy()\ny_all_vv = y_all.copy()\ncounter = 0\nfor i in range(len(y_all)):\n    if y_all[i] == 'Iris-setosa':\n        X_all_vv = np.delete(X_all_vv,counter, axis = 0)\n        y_all_vv = np.delete(y_all_vv,counter)\n        counter = counter - 1\n    counter = counter + 1\n#split the training and testing data set\nX_all_vv_train, X_all_vv_test, y_all_vv_train, y_all_vv_test = train_test_split(X_all_vv, y_all_vv, stratify = y_vv, test_size = 0.50, random_state = 1)","6cc9f88f":"ppt4 = Perceptron(max_iter = 1000, eta0 = 0.001, shuffle = True, random_state = 1, tol = None)\nppt4.fit(X_all_vv_train, y_all_vv_train)\nprint(\"Training accuracy: \", str(ppt4.score(X_all_vv_train,y_all_vv_train)*100)+\"%\")\nprint(\"Testing accuracy: \", str(ppt4.score(X_all_vv_test,y_all_vv_test)*100)+\"%\")","40484250":"ppt4 = Perceptron(max_iter = 3000000, eta0 = 0.000001, shuffle = True, random_state = 1, tol = None)\nppt4.fit(X_all_vv_train, y_all_vv_train)\nprint(\"Training accuracy: \", str(ppt4.score(X_all_vv_train,y_all_vv_train)*100)+\"%\")\nprint(\"Testing accuracy: \", str(ppt4.score(X_all_vv_test,y_all_vv_test)*100)+\"%\")","b5481aea":"We can see that the best possible line would only give us 2 wrong, which means it gets 48\/50 right, so we have 96% accuracy as the best accuracy","eb02c846":"We can see that with this, our accuracy can finally hit 100% on training data, but testing data is still not as accurate as with only using 2 features.","2d96e43d":"* There is a huge distinction between the Iris-setosa mean and the other two species. In fact, Iris-setosa's max is 1.9, while the others min is 3.0 and 4.5, so there is a large disparity and would be wise using this feature if comparing Iris-setosa to the others. \n* Iris-versicolor and Iris-virginica also has a difference, although it isn't as stark. But is still worth while.","1f167c8e":"* Even trying to fine tune the agent by messing with max_iter and eta0 didn't do anything significant.","500d761b":"### **Petal width analysis**","311adc58":"* Now that we see how everything looks and understand how things are spread out, we can train a basic model to compare its accuracy using a Perceptron which we know will not be 100%.\n* We know it would be perfect for seperating Iris-setsa from the other two with 100% however, so we will start with that since its easy to visualize binary classification.\n* First we will train without standardization, so we need to see how the training is done from a Perceptron:\n    1. Initialize the weights to zero or some small random number, and make sure the class labels are either -1 or 1 since the predictions will be as well\n    2. For each training example, $i$ , we compute the prediction, $\\hat{y}$ using the rule we stated above, then update weights based on:\n    $$w_j = w_j + \\Delta{w_j}$$ \n    $$\\Delta{w_j} = \\eta(y^{(i)}-\\hat{y}^{(i)})x_j^{(i)}$$\n    Where $\\eta$ is the learning rate","bc1bcb79":"# Line Fitting By Hand","010ed279":"### **Using all features**","337eacf1":"### **Petal length analysis**","3b590f65":"### **Creating an Line from interactive plot for contourf() and plot()**","aad81a01":"* Just like petal length, petal width is very effective in distinguishing iris-setosa with the other two.\n* But Petal width is better than petal length in distinguishing iris-versicolor and iris-virginica","24731e34":"We first grab the graph in question:","ff8ede80":"### **3-D Graphing**","da165b60":"* We see that the best that we can do is only get 94% for training data, and 98% for testing data. But this doesn't match up with our own line which gives us 96% accuracy. After testing with parameters, I can't seem to make the perceptron reach 96%.\n* Lets see if we can do better with more features.","bda96a79":"* We see that if $w_0 = 2$ , $w_1 = -0.5$ , $w_2 = -1$, then we find a perfectly linear decision boundary splitting Iris-setosa and the other 2 classes.\n* However there are infinitely many valid weight values for a linear decision boundary, this is just one example.","8a1939be":"* Here ^ we see we have split the classes into even amounts","567511ba":"# Data Exploration\/Understanding","fc6eb5cf":"### **Overall feature analysis**","920097ce":"We can see that only using 2 features, we wouldn't be able to perfectly split versicolor and virginica. So we will up the count to 3 features\/dimensions.\n* We will the reuse the same 2 features, but we don't know what the third feature should be.","932d4e2f":"contourf():","338f71f7":"* We can see that in terms of the mean, std, max, and percentiles, there is a distinction in magnitude, in increasing order:\n    1. Iris-setosa (Being the least)\n    2. Iris-versicolor (Being the middle)\n    3. Iris-virginica (Being the biggest)\n* However we can see that the max sepal length for Iris-setosa is 5.8, while the min for both Iris-versicolor and Iris-virginica is 4.9, meaning that values in between 4.9 and 5.8 can belong to all 3 species\/labels. So although there is a correlation with the length of the sepal, it isn't exact","76538959":"### **Sepal width analysis**\nHere we are just going to do the same thing for sepal width","91d4350b":"* In terms of feature importance just from the human eye and intuition:\n    1. Petal Width (Most important, distinguishes setosa to versicolor and virginica)\n    2. Petal Length\n    3. Sepal Length (But most important when it comes to distinguishing versicolor and virginica)\n    4. Sepal Width","f2fa1178":"# Basic Perceptron Learning","c5a17ccf":"* We can see the same applies if we just use a simple plot to graph the decision boundary","d19871da":"* **We can see that with only 10 epochs, it perfectly separates the two**","9c01f962":"### **Sepal Length analysis**","fbb92b32":"* We will first use the features *PetalLengthCm* and *PetalWidthCm* , meaning our coefficient vector will look like: $w$ = $[w_0,w_1,w_2]$\n* Since we are using\/comparing different linear models, we know that the linear decision boundary, $z$ , will be a linear combination of $w$ and inputs, $x$ , and needs a bias, $w_0$ , so that gives us: \n\n$${z = w^Tx = }\\sum_{n}w_ix_i = w_0 + w_1x_1 + w_2x_2 + ... w_ix_i$$\n* The boundary will be where $z = w_0 + w_1x_1 + ... w_ix_i = 0$.\n* Using algebra, in this scenario with only 2 features, $w_0 + w_1x_1 + w_2x_2 = 0 \\rightarrow x_2 = -(\\frac{w_1}{w_2}x_1 + \\frac{w_0}{w_2})$\n* The prediction rule **(For this example, does not apply to every supervised learning algorithm!!!!)** is simply:\n$$\\hat{y} = \\left\\{ \\begin{array}{rc1}\n    1 & \\mbox{if} & {z} \\geq {0} \\\\\n    0 & \\mbox{else} & {z < 0} \\end{array}\\right.$$\n  Where 1 and 0 are different classes. This is also the **THRESHOLD FUNCTION!**\n* Example:\n  $$x = [1,  3]$$\n  $$w = [2 ,1, -2 ]$$\n  $$z = 2 + 1*1 + -2*3 = -3$$\n  $$z = -3 < 0$$\n  $$\\hat{y} = 0$$\n Therefore, we predict the class is 0 for data point $x = [1, 3]$\n* But we will first focus on fine tuning the weight coefficients, $w_1$ , $w_2$ , and $w_0$ by hand","0c91987c":"* We can see that the first 50 belongs to Iris-setosa, next 50 are Iris-versicolor, last 50 are Iris-virginica","1d38ca34":"* **Now we will try to focus on separating Iris-versicolor and Iris-virginica**.","f831c50e":"* Using these parameters, we see that our training set is more accurate, even more that the percect boundary.\n* But more importantly, we see that the testing set is less accurate than before. This means we need to mess with some parameters.","40648dba":"We must first make a function that will be able to draw a line using these principles from the weight coefficients $w$.\nHowever since we will be eventually coloring in regions, we will want to use contour instead of just making a regular line plot.","02439107":"# What I want to learn and practice with this kernel:\n* Learn how to explore and visualize data more deeply to gain a better understanding of the relationship of data\n* Decision Boundary Drawing\n* Affects of adding more features to Perceptron to see variance vs bias tradeoff.\n* Learn basics of different Gradient Descent algorithms.","26e4ca0c":"Lets train the Perceptron now","5d45682d":"* Looking at every way we can graph the flowers using only 2 features, shows us that there is not perfect linear seperator if we only use 2 features. \n* There are some that are better than others however, the best combination being PetalLengthCm and PetalWidthCm.\n* The worst combination is SepalLengthCm and SepalWidthCm, which we deduced just from looking at the means, min, and max from before. \n* Seeing as there is no way we can have a perfect linear seperable line seperating Iris-versicolor and Iris-virginica, let us see if 3 features will do the trick","b6d9008f":"* Because Iris-setosa's sepal width is 3.41 which is a big enough difference compared to the other two to say generally iris-setosa will have wider sepals than the other two. \n* But since iris-versicolor and iris-virginica's difference in mean is only 0.2, there is no definitive correlation to differentiate between these two using only the sepal width.","a26354d3":"### What these different models tell us, is that we overfitted because of how with only 2 features we still managed to get 98% compared to 96% when using 4 features.","0af45603":"# The Iris Dataset:\nThis is my introduction into the Data Science and Analysis world. What I want to achieve, is to create an understanding for the standard laymen to see the patterns and distinctions between these three Iris species. I also want to create a machine learning model (Perceptron and Gradient Descent Specifically) to classify new flowers with high accuracy while using as little data as I can.","41980013":"Now we need to make a vector $w = [w_0, w_1, w_2]$ with bias $w_0$ that allows us to seperate *Iris-Setosa* from *Iris-versicolor*:","23c87843":"### **Graphing 2 features**","c02572cd":"plot():","33cf3452":"And after quick parameter testing, we can see no difference is being made.","a5f48a62":"Here, we practice our visualization skills by adding interactivity and lines to correspond to linear models."}}