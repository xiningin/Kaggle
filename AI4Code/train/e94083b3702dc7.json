{"cell_type":{"4710b2f9":"code","7262fbe0":"code","e6965788":"code","7c78a3aa":"code","148f4a92":"code","367a05de":"code","38034237":"code","f005a098":"code","ff3c1bba":"code","3a6ed32e":"code","abc514c9":"markdown","c1b512ac":"markdown","d4e8b8a3":"markdown","00ccfa68":"markdown","7d2ba8f9":"markdown","b82af35c":"markdown","871a8907":"markdown","4e423591":"markdown","08929363":"markdown","ee8997ca":"markdown","3a2ef7f4":"markdown"},"source":{"4710b2f9":"%%sh\npip install -q timm\npip install -q einops\npip install -q rich\npip install -q wandb --upgrade;","7262fbe0":"import os\nimport sys\nimport re\nimport gc\nimport platform\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport einops\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nimport timm\nimport glob\nimport cv2\n\nfrom rich import print as _pprint\nfrom rich.progress import track\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport wandb\n\nimport warnings\nwarnings.simplefilter('ignore')","e6965788":"def get_patient_id(patient_id):\n    if patient_id < 10:\n        return '0000'+str(patient_id)\n    elif patient_id >= 10 and patient_id < 100:\n        return '000'+str(patient_id)\n    elif patient_id >= 100 and patient_id < 1000:\n        return '00'+str(patient_id)\n    else:\n        return '0'+str(patient_id)\n\ndef get_path(row):\n    patient_id = get_patient_id(row.BraTS21ID)\n    return f'..\/input\/rsna-miccai-png\/train\/{patient_id}\/FLAIR\/'\n\ndef wandb_log(**kwargs):\n    \"\"\"\n    Logs a key-value pair to W&B\n    \"\"\"\n    for k, v in kwargs.items():\n        wandb.log({k: v})\n        \ndef cprint(string):\n    \"\"\"\n    Utility function for beautiful colored printing.\n    \"\"\"\n    _pprint(f\"[black]{string}[\/black]\")","7c78a3aa":"Config = dict(\n    MAX_FRAMES = 12,\n    EPOCHS = 5,\n    LR = 2e-4,\n    IMG_SIZE = (224, 224),\n    FEATURE_EXTRACTOR = 'resnext50_32x4d',\n    DR_RATE = 0.35,\n    NUM_CLASSES = 1,\n    RNN_HIDDEN_SIZE = 100,\n    RNN_LAYERS = 1,\n    TRAIN_BS = 4,\n    VALID_BS = 4,\n    NUM_WORKERS = 4,\n    infra = \"Kaggle\",\n    competition = 'rsna_miccai',\n    _wandb_kernel = 'tanaym'\n)","148f4a92":"run = wandb.init(\n    project='pytorch',\n    config=Config,\n    group='vision',\n    job_type='train',\n    anonymous='allow'\n)","367a05de":"class Augments:\n    \"\"\"\n    Contains Train, Validation Augments\n    \"\"\"\n    train_augments = A.Compose([\n        ToTensorV2(p=1.0),\n    ],p=1.)\n    \n    valid_augments = A.Compose([\n        ToTensorV2(p=1.0),\n    ], p=1.)","38034237":"class RSNADataset(Dataset):\n    def __init__(self, df, augments=None, is_test=False):\n        self.df = df\n        self.augments = augments\n        self.is_test = is_test\n        \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        paths = self.getPaths(row)\n        frames = []\n        for path in paths:\n            img = cv2.imread(path)\n            img = cv2.resize(img, Config['IMG_SIZE'])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            frames.append(img)\n\n        frames_tr = np.stack(frames, axis=2)\n        \n        if self.augments:\n            for frame in frames:\n                frame = self.augments(image=frame)['image']\n                frames_tr.append(frame)\n            \n        if self.is_test:\n            return frames_tr\n        else:\n            label = torch.tensor(row['MGMT_value']).float()\n            return frames_tr, label\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def getPaths(self, row):\n        paths = glob.glob(row['path'] + '*.png')\n        sortedPaths = self.sort(paths)\n        maxWindowStart = len(sortedPaths) - Config['MAX_FRAMES']\n        start = 0 # np.random.randint(1, maxWindowStart)\n        paths = sortedPaths[start:Config['MAX_FRAMES']]\n        \n        return paths\n        \n    def sort(self, entry):\n        # https:\/\/stackoverflow.com\/a\/2669120\/7636462\n        convert = lambda text: int(text) if text.isdigit() else text \n        alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n    \n        return sorted(entry, key = alphanum_key)","f005a098":"class ResNextModel(nn.Module):\n    def __init__(self):\n        super(ResNextModel, self).__init__()\n        self.backbone = timm.create_model(Config['FEATURE_EXTRACTOR'], pretrained=True, in_chans=1)\n    def forward(self, x):\n        return self.backbone(x)\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x\n\nclass RSNAModel(nn.Module):\n    def __init__(self, pretrained=True):\n        super(RSNAModel, self).__init__()\n        self.backbone = ResNextModel()\n        num_features = self.backbone.backbone.fc.in_features\n        \n        self.backbone.backbone.fc = Identity()\n        self.dropout= nn.Dropout(Config['DR_RATE'])\n        self.rnn = nn.LSTM(num_features, Config['RNN_HIDDEN_SIZE'], Config['RNN_LAYERS'])\n        self.fc1 = nn.Linear(Config['RNN_HIDDEN_SIZE'], Config['NUM_CLASSES'])\n        \n    def forward(self, x):\n        b_z, fr, h, w = x.shape\n        ii = 0\n        in_pass = x[:, ii].unsqueeze_(1)\n        y = self.backbone((in_pass))\n        output, (hn, cn) = self.rnn(y.unsqueeze(1))\n        for ii in range(1, fr):\n            y = self.backbone((x[:, ii].unsqueeze_(1)))\n            out, (hn, cn) = self.rnn(y.unsqueeze(1), (hn, cn))\n        out = self.dropout(out[:, -1])\n        out = self.fc1(out)\n        return out","ff3c1bba":"def train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch, device, log_wandb=True, verbose=False):\n    \"\"\"\n    Trains model for one epoch\n    \"\"\"\n    model.train()\n    running_loss = 0\n    prog_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n    for batch, (frames, targets) in prog_bar:\n        optimizer.zero_grad()\n        \n        frames = frames.to(device, torch.float)\n        targets = targets.to(device, torch.float)\n        \n        # Re arrange the frames in the format our model wants to recieve\n        frames = einops.rearrange(frames, 'b h w f -> b f h w')\n\n        preds = model(frames).view(-1)\n        loss = loss_fn(preds, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_item = loss.item()\n        running_loss += loss_item\n        \n        prog_bar.set_description(f\"loss: {loss_item:.4f}\")\n        \n        if log_wandb == True:\n            wandb_log(\n                batch_train_loss=loss_item\n            )\n        \n        if verbose == True and batch % 20 == 0:\n            print(f\"Batch: {batch}, Loss: {loss_item}\")\n    \n    avg_loss = running_loss \/ len(train_dataloader)\n    \n    return avg_loss\n\n@torch.no_grad()\ndef valid_one_epoch(model, valid_dataloader, loss_fn, epoch, device, log_wandb=True, verbose=False):\n    \"\"\"\n    Validates the model for one epoch\n    \"\"\"\n    model.eval()\n    running_loss = 0\n    prog_bar = tqdm(enumerate(valid_dataloader), total=len(valid_dataloader))\n    for batch, (frames, targets) in prog_bar:\n        frames = frames.to(device, torch.float)\n        targets = targets.to(device, torch.float)\n        \n        # Re arrange the frames in the format our model wants to recieve\n        frames = einops.rearrange(frames, 'b h w f -> b f h w')\n        preds = model(frames).view(-1)\n        loss = loss_fn(preds, targets)\n        \n        loss_item = loss.item()\n        running_loss += loss_item\n        \n        prog_bar.set_description(f\"val_loss: {loss_item:.4f}\")\n        \n        if log_wandb == True:\n            wandb_log(\n                batch_val_loss=loss_item\n            )\n        \n        if verbose == True and batch % 10 == 0:\n            print(f\"Batch: {batch}, Loss: {loss_item}\")\n    \n    avg_val_loss = running_loss \/ len(valid_dataloader)\n    \n    return avg_val_loss","3a6ed32e":"if __name__ == \"__main__\":\n    log_wandb = True\n    if torch.cuda.is_available():\n        print(\"Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n        device = torch.device('cuda')\n    else:\n        print(\"\\nGPU not found. Using CPU: {}\\n\".format(platform.processor()))\n        device = torch.device('cpu')\n    \n    \n    # Load training csv file\n    df = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\n    df['path'] = df.apply(lambda row: get_path(row), axis=1)\n\n    # Removing two patient ids from the dataframe since there are not FLAIR directories for these ids. \n    df = df.loc[df.BraTS21ID!=109]\n    df = df.loc[df.BraTS21ID!=709]\n    df = df.reset_index(drop=True)\n    \n    train_df, valid_df = train_test_split(df, test_size=0.1, stratify=df.MGMT_value.values)\n    train_df = train_df.reset_index(drop=True)\n    valid_df = valid_df.reset_index(drop=True)\n    \n    print(f'Size of Training Set: {len(train_df)}, Validation Set: {len(valid_df)}')\n    \n    model = RSNAModel()\n    model = model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=Config['LR'])\n\n    train_loss_fn = nn.BCEWithLogitsLoss()\n    valid_loss_fn = nn.BCEWithLogitsLoss()\n    \n    print(f\"\\nUsing Backbone: {Config['FEATURE_EXTRACTOR']}\")\n    \n    train_data = RSNADataset(train_df)\n    valid_data = RSNADataset(valid_df)\n    \n    train_loader = DataLoader(\n        train_data,\n        batch_size=Config['TRAIN_BS'], \n        shuffle=True,\n        num_workers=Config['NUM_WORKERS']\n    )\n    \n    valid_loader = DataLoader(\n        valid_data, \n        batch_size=Config['VALID_BS'], \n        shuffle=False,\n        num_workers=Config['NUM_WORKERS']\n    )\n    \n    current_loss = 1000\n    for epoch in range(Config['EPOCHS']):\n        print(f\"\\n{'--'*8} EPOCH: {epoch+1} {'--'*8}\\n\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, train_loss_fn, epoch=epoch, device=device, log_wandb=log_wandb)\n        \n        valid_loss = valid_one_epoch(model, valid_loader, valid_loss_fn, epoch=epoch, device=device, log_wandb=log_wandb)\n        \n        print(f\"val_loss: {valid_loss:.4f}\")\n        \n        if log_wandb == True:\n            wandb_log(\n                train_loss=train_loss,\n                valid_loss=valid_loss\n            )\n        \n        if valid_loss < current_loss:\n            current_loss = valid_loss\n            torch.save(model.state_dict(), f\"model_{Config['FEATURE_EXTRACTOR']}.pt\")","abc514c9":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83c\udff4\u200d\u2620\ufe0f Training and Validation Functions<\/h2>\n<\/div>","c1b512ac":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\udcc8 Model Class with ResNext Backbone<\/h2>\n<\/div>","d4e8b8a3":"<center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\"\/><\/center><br>\n<p style=\"text-align:center\">WandB is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","00ccfa68":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\udcd4 Imports and Installation<\/h2>\n<\/div>","7d2ba8f9":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83c\udfd7 Training and Validating the Model<\/h2>\n<\/div>","b82af35c":"<h3><a href=\"https:\/\/wandb.ai\/anony-mouse-125639\/pytorch\/runs\/23gv9jk5?apiKey=6b04b2e314f0ee65d4e8bdc3aa267c124d7624a9\">View the complete dashboard here \u2728<\/a><\/h3>\n\n![Results](https:\/\/i.imgur.com\/gcn59xp.gif)","871a8907":"<div class=\"alert alert-info\">\n    <h1 align='center'>Brain Tumor Video Classification using PyTorch + W&B Tracking \u2728<\/h1>\n<\/div>\n\n<p style='text-align: center'>\nI'm approaching this problem as a video classification problem, using all images in the FLAIR folder. Thanks to Ayush Thakur's <a href='https:\/\/www.kaggle.com\/ayuraj\/train-brain-tumor-as-video-classification-w-b'>Notebook<\/a> doing the same but in Tensorflow.<br>\nI have also used Weights and Biases tracking to keep track of the training process and the experiments I am conducting.\n<\/p>\n\n<div style='text-align: center'>\n    <strong>You can upvote this kernel, if you found it useful!<\/strong>\n<\/div>","4e423591":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\udcbb Custom Dataset Class<\/h2>\n<\/div>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc In this custom Dataset, I am essentially reading \"MAX_FRAMES\" number of images from a patient's FLAIR folder and making list of those frames and converting it to torch tensor.\n<\/div>","08929363":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\ude80 Config Dictionary and W&B Integration <\/h2>\n<\/div>","ee8997ca":"To login to W&B, you can use below snippet.\n\n```python\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wb_key)\n```\nMake sure you have your W&B key stored as `WANDB_API_KEY` under Add-ons -> Secrets\n\nYou can view [this](https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases) notebook to learn more about W&B tracking.\n\nIf you don't want to login to W&B, the kernel will still work and log everything to W&B in anonymous mode.","3a2ef7f4":"<div class=\"alert alert-success\">\n    <h2 align='center'>\u26fd Utility Functions <\/h2>\n<\/div>"}}