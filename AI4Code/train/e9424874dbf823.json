{"cell_type":{"d6b77e06":"code","a3bf3ff9":"code","5e3c9213":"code","34f568e7":"code","2ef296af":"code","84933b85":"code","78fa345b":"code","96a54b91":"code","8397fcc3":"code","03f56b9a":"code","b4059495":"code","f6a2f15a":"code","0b572bbb":"code","0fcf5b12":"code","caa7bcc1":"code","04d627a9":"code","4df6f542":"markdown","b174e83b":"markdown","aedf024f":"markdown","bf69e5a0":"markdown","0c577aa1":"markdown","5821bae8":"markdown","c88a2dab":"markdown","119d4b69":"markdown","ae9a976a":"markdown","1fe4b981":"markdown","cc866cab":"markdown","12a8784d":"markdown","66321b1c":"markdown","d1471d57":"markdown","40a96f87":"markdown","ec737d4a":"markdown","091bbe66":"markdown","9980905b":"markdown","ee4d2468":"markdown","7a334b31":"markdown","c13e0e1b":"markdown","cbb73416":"markdown","a4d5aa8a":"markdown","016a2935":"markdown","079330c5":"markdown","1e0e0a71":"markdown","db69ca02":"markdown","7974f4bc":"markdown","d696662e":"markdown","ccd62223":"markdown","c0c67185":"markdown","fe467d16":"markdown","f30fb53a":"markdown","194f990a":"markdown","2cb22c88":"markdown","511ba997":"markdown","841f7647":"markdown"},"source":{"d6b77e06":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split                          # Train test split\nfrom sklearn.feature_selection import RFE                                     # RFE\nfrom sklearn.metrics import r2_score,mean_squared_error                       # Metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs         # Forward & backward selection\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs     # Ploting for forward & backward selection ","a3bf3ff9":"boston = load_boston()\n\n#IV\nX = pd.DataFrame(boston.data , columns=boston.feature_names)\n#DV\ny = pd.DataFrame(boston.target , columns=['PRICE'])\n\ndf = pd.concat([X,y],1)\ndf.head()","5e3c9213":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","34f568e7":"plt.figure(figsize=(12,7))\nsns.heatmap(df.corr(),vmax=0.9,annot=True)\nplt.show()","2ef296af":"corr = df.corr()\ncor_target = abs(corr['PRICE'])  \nimp_features_corr = cor_target[cor_target>=0.4]\nimp_features_corr","84933b85":"#Linear Regression model\nmodel = LinearRegression()\n\n#Initialize RFE\nrfe = RFE(model,5)\nrfe.fit(X_train,y_train)","78fa345b":"pd.DataFrame(list(zip(X.columns,rfe.support_,rfe.ranking_)),columns=['Features','Support','Rank']).T","96a54b91":"y_pred_rfe = rfe.predict(X_test)\n\nprint('RMSE : ',np.sqrt(mean_squared_error(y_test,y_pred_rfe)))\nprint('R2_Score : ',r2_score(y_test,y_pred_rfe))","8397fcc3":"#model to train\nmodel_tune = LinearRegression()\n#keep track of metric\nscore_list=[]\n\nfor n in range(13):\n    rfe = RFE(model_tune,n+1)\n    rfe.fit(X_train,y_train)\n    y_pred = rfe.predict(X_test)\n    score = r2_score(y_test,y_pred)\n    score_list.append(score)\n    \n    \nplt.figure(figsize=(10,5))\nplt.plot([1,2,3,4,5,6,7,8,9,10,11,12,13],score_list)\nplt.xticks([1,2,3,4,5,6,7,8,9,10,11,12,13])\nplt.xlabel('Number of Features')\nplt.ylabel('R2_Score')\nplt.grid(True)\nplt.show()","03f56b9a":"model = LinearRegression()\n\n#Initialize RFE\nrfe = RFE(model,9)\nrfe.fit(X_train,y_train)\ny_pred_rfe = rfe.predict(X_test)\n\nprint('RMSE : ',np.sqrt(mean_squared_error(y_test,y_pred_rfe)))\nprint('R2_Score : ',r2_score(y_test,y_pred_rfe))","b4059495":"model_1 = LinearRegression()\n# Forward Selection \nsfs1 = sfs(model_1 , k_features=12 , forward=True , scoring='r2')\nsfs1 = sfs1.fit(X_train,y_train)","f6a2f15a":"fig = plot_sfs(sfs1.get_metric_dict())\nplt.grid(True)\nplt.show()","0b572bbb":"sfs1.k_feature_names_","0fcf5b12":"model_1 = LinearRegression()\n#Backward Elimination\nsfs2 = sfs(model_1 , k_features=8 , forward=False , scoring='r2')\nsfs2 = sfs2.fit(X_train,y_train)","caa7bcc1":"fig = plot_sfs(sfs2.get_metric_dict())\nplt.grid(True)\nplt.show()","04d627a9":"sfs2.k_feature_names_","4df6f542":"- RFE requires a specified number of features to keep, however it is often not known in advance how many features are valid.\n- To find the optimal number of features cross-validation is used with RFE to score different feature subsets and select the best scoring collection of features.\n- The RFECV visualizer plots the number of features in the model along with their cross-validated test score and variability and visualizes the selected number of features.","b174e83b":"- Support : shows which columns are selected (True-selected)\n- Rank : shows what rank is given to the column ","aedf024f":"Model building based on RFE (5 Features)","bf69e5a0":"<h3>- Backward Elimination Method","0c577aa1":" - We can see that after eliminating more than 9 fearutes recursively the performance of the model seems to go down","5821bae8":"The above are the features that are selected using Forward Selection ","c88a2dab":"<h3> - Corelation Method","119d4b69":"Running RFE with n_features_to_select = 9","ae9a976a":"We can see that after adding more than 9 fearutes recursively the performance of the model seems to go down ","1fe4b981":"<h3> - Recursive Feature Elimination (RFE) Method ","cc866cab":"Import BOSTON HOUSING Data","12a8784d":"The same function is used for both Forward & Backward , the attribute forward=True - Forward & forward=False - Bckward","66321b1c":"- Feature selection methods ahelp us create an accurate predictive model.\n- They help you by choosing features that will give you   better accuracy with less less data.\n- Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model\n- Feature selection techniques are used for several other reasons:\n   - Simplification of models to make them easier to interpret by researchers\/users.\n   - Shorter training times,\n   - To avoid the curse of dimensionality,\n   - Enhanced generalization by reducing overfitting","d1471d57":"<h3>- Forward Selection Method","40a96f87":"We can see that there are some features that have a very high corelation with the targert variable and few features having moderate corelation with the target variable ","ec737d4a":"- Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached.\n- Features are ranked by the model\u2019s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.","091bbe66":"### Feature Selection Techniques","9980905b":"Hyperparameter Tuning (n_features_to_select)","ee4d2468":" - The corelation method is a very basic way of feature selection \n - We calculate the corelation between the feature and the target variable and if the corelation is high we can select that feature\n - The problem with this method is , corelation dosent imply causality i.e. simply because there is a high orelation between the feature and target variable it dose not mean that the feature is cause the effect on the target variable ","7a334b31":"Train Test Split","c13e0e1b":"---","cbb73416":"- In backward selection you start with a full model including all your variables and then you drop those you do not need\/ are not significant \n- In this method, you start with fitting a model with all the features.\n- Then you drop the least significant feature (p-value<0.05), so long as it is not significant at our chosen critical level. - - You continue by successively re-fitting reduced models and applying the same rule until all remaining features are statistically significant","a4d5aa8a":"This figure shows an ideal RFECV curve, the curve jumps to a good r2_score when 9 features are captured, then gradually decreases the non informative features are added into the model.","016a2935":"- Feature selection is also called variable selection or attribute selection.<br>\n- It is the selection of attributes in your data (such as columns in tabular data) that are most relevant to the predictive      modeling problem you are working on.\n- The reason why we use feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information.\n- The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate","079330c5":"<h4> Why do we need Feature Selection ?","1e0e0a71":"Let is set a threshold of 0.4 and select all the feature that have a corelation of more than 0.4 with the target variable ","db69ca02":"The above are the features that can be selected for model building ","7974f4bc":"- What is Feature Selection ?\n- Why do we need Feature Selection ?\n- Types of Feature Selection Techniquies\n   - Corelation Method\n   - Recursive Feature Elimination (RFE)\n   - Forward Selection \n   - Backward Selection","d696662e":"The above are the features that are selected using Forward Selection","ccd62223":"Support & Rank - Shows which columns are selected based on the rank","c0c67185":"---","fe467d16":"Import Libraries","f30fb53a":"<h4> What is Feature Selection ?","194f990a":"<h4> Types of Feature Selection Techniquies","2cb22c88":"---","511ba997":"---","841f7647":"- In forward selection you start with your null model and add predictors.\n- In Forward selection procedure, one adds features to the model one at a time.\n- At each step, each feature that is not already in the model is tested for inclusion in the model. The most significant of these feature is added to the model, so long as it's P-value is below some pre-set level i.e., 0.05.\n<br><br>\n- Forward selection has drawbacks, including the fact that each addition of a new feature may render one or more of the already included feature non-significant "}}