{"cell_type":{"65218e8f":"code","551372eb":"code","4587e554":"code","27524cbf":"code","d65ebdbb":"code","26b8dc5a":"code","1fbd85d2":"code","f91a0da6":"code","a5fe3ee4":"code","041749fa":"code","1b7ffa2a":"code","b4052b05":"code","1448b57e":"code","d41ab7eb":"code","bbde731f":"code","b43feb2c":"markdown","3d251b51":"markdown","15a82824":"markdown","a4aafc94":"markdown","0cb6827e":"markdown","6c5a901b":"markdown","ee43bcae":"markdown","9b3b17e2":"markdown","be027947":"markdown","62a14b63":"markdown","8ed5d51f":"markdown"},"source":{"65218e8f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","551372eb":"# colors\nbg_color = '#fbfbfb'\ntxt_color = '#5c5c5c'\n\ncmap = ['#68595b','#7098af','#6f636c','#907c7b']","4587e554":"%%time\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')","27524cbf":"# basic overview\nprint('train_shape:',df_train.shape)\nprint('test_shape:', df_test.shape)","d65ebdbb":"df_train.info()","26b8dc5a":"df_test.info()","1fbd85d2":"# missing values\nprint('missing values train:',sum(df_train.isna().sum()))\nprint('missing values test:',sum(df_test.isna().sum()))","f91a0da6":"# metrics\ntarget_count_0 = df_train.query('target == 0')['target'].count()\ntotal_count = df_train['target'].count()\n\n# plot\nfig, ax = plt.subplots(tight_layout=True, figsize=(12,2.5))\nfig.patch.set_facecolor(bg_color)\n\nax.barh(\n    y=1, width=target_count_0, \n    color=cmap[1], alpha=0.75, lw=1, edgecolor='white'\n)\n\nax.barh(\n    y=1, width=total_count-target_count_0, left=target_count_0,\n    color=cmap[2], alpha=0.25, lw=1, edgecolor='white'\n)\n\nax.axis('off')\n\n# annotations\nax.annotate(\n    s=f\"{np.round(target_count_0\/total_count*100,2)} %\",\n    xy=(2.5e5,1.05),\n    va='center', ha='center',\n    fontsize=36, fontweight='bold', fontfamily='serif',\n    color='#fff'\n)\n\nax.annotate(\n    s='Count Target Class: 0',\n    xy=(2.5e5,0.85),\n    va='center', ha='center',\n    fontsize=16, fontstyle='italic', fontfamily='serif',\n    color='#fff'\n)\n\nax.annotate(\n    s=f\"{np.round((total_count-target_count_0)\/total_count*100,2)} %\",\n    xy=(7.5e5,1.05),\n    va='center', ha='center',\n    fontsize=36, fontweight='bold', fontfamily='serif',\n    color='#fff'\n)\n\nax.annotate(\n    s='Count Target Class: 1',\n    xy=(7.5e5,0.85),\n    va='center', ha='center',\n    fontsize=16, fontstyle='italic', fontfamily='serif',\n    color='#fff'\n)\n\nfig.text(\n    s='::Target Distribution',\n    x=0, y=1.25,\n    fontsize=17, fontweight='bold',\n    color=txt_color, \n    va='top', ha='left'\n)\n\nfig.text(\n    s='''\n    the target variable is nearly\n    equally distributed among both classes.\n    ''',\n    x=0, y=1.2,\n    fontsize=11, fontstyle='italic',\n    color=txt_color,\n    va='top', ha='left'\n)\n\nplt.show()","a5fe3ee4":"# sampling data to speed up EDA\nnp.random.seed(2003)\nsmpl_train = df_train.sample(10000)\nsmpl_test = df_test.sample(10000)","041749fa":"# get continous variables\ncont_feat = [col for col in smpl_train.columns if smpl_train[col].dtype == 'float']\n\n# plot\nfig, ax = plt.subplots(20, 12, tight_layout=True, figsize=(12, 12))\nax = ax.flatten()\n\nfor idx, feat in enumerate(cont_feat):\n    \n    sns.kdeplot(\n        data=smpl_test,\n        x=feat,\n        shade=True,\n        color=cmap[0],\n        edgecolor='black',\n        alpha=0.8,\n        ax=ax[idx]\n    )\n    \n    sns.kdeplot(\n        data=smpl_train,\n        x=feat,\n        shade=True,\n        color=cmap[1],\n        edgecolor='black',\n        alpha=0.8,\n        ax=ax[idx]\n    )\n    \n    ax[idx].set_xticks([])\n    ax[idx].set_yticks([])\n    ax[idx].set_xlabel('')\n    ax[idx].set_ylabel('')\n    ax[idx].set_title(f'{feat}', loc='center', fontsize=10)\n    sns.despine(left=True)\n\nfig.text(\n    s='::Continous Feature Distribution',\n    x=0, y=1.05,\n    fontsize=17, fontweight='bold',\n    color=txt_color, \n    va='top', ha='left'\n)\n\nfig.text(\n    s='Train & Test-Data are nearly equally distributed',\n    x=0, y=1.02,\n    fontsize=11, fontstyle='italic',\n    color=txt_color,\n    va='top', ha='left'\n)\n\nplt.show()","1b7ffa2a":"# get discrete variables\ndisc_feat = [col for col in smpl_train.columns if smpl_train[col].dtype == 'int' and col not in ['id','target']]\n\nfig, ax = plt.subplots(9, 5, tight_layout=True, figsize=(12, 12))\nax = ax.flatten()\n\nfor idx, feat in enumerate(disc_feat):\n    \n    sns.kdeplot(\n        data=smpl_test,\n        x=feat,\n        shade=True,\n        color=cmap[0],\n        edgecolor='black',\n        alpha=0.8,\n        ax=ax[idx]\n    )\n    \n    sns.kdeplot(\n        data=smpl_train,\n        x=feat,\n        shade=True,\n        color=cmap[1],\n        edgecolor='black',\n        alpha=0.8,\n        ax=ax[idx]\n    )\n    \n    ax[idx].set_xticks([])\n    ax[idx].set_yticks([])\n    ax[idx].set_xlabel('')\n    ax[idx].set_ylabel('')\n    ax[idx].set_title(f'{feat}', loc='center', fontsize=10)\n    sns.despine(left=True)\n\nfig.text(\n    s='::Discrete Feature Distribution',\n    x=0, y=1.05,\n    fontsize=17, fontweight='bold',\n    color=txt_color, \n    va='top', ha='left'\n)\n\nfig.text(\n    s='Train & Test-Data are nearly equally distributed',\n    x=0, y=1.02,\n    fontsize=11, fontstyle='italic',\n    color=txt_color,\n    va='top', ha='left'\n)\n\nplt.show()","b4052b05":"# prepare dataframe for modeling\nX = df_train.drop(columns=['id','target']).copy()\ny = df_train['target'].copy()","1448b57e":"# model params\nlgbm_params = {\n    'device_type' : 'gpu'\n}\n\ncatb_params = {\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nxgb_params = {\n    'predictor': 'gpu_predictor',\n    'tree_method': 'gpu_hist',\n    'gpu_id' : 0,\n    'verbosity': 0\n}","d41ab7eb":"%%time\n# spot checking which model to chose\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=2003)\n\nmodels = [\n    ('LGBM', LGBMClassifier\t(**lgbm_params)),\n    ('CATB', CatBoostClassifier(**catb_params)),\n    ('XGB', XGBClassifier(**xgb_params))\n]\n\nscores = dict()\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_hat = model.predict_proba(X_test)[:,1]\n    fpr, tpr, _ = roc_curve(y_test, y_hat)\n    auc_score = auc(fpr, tpr)\n    scores[name] = auc_score","bbde731f":"scores_df = pd.DataFrame([scores]).transpose().rename(columns={0:'AUC'})\n\nfig, ax = plt.subplots(figsize=(12,6))\n\nsns.barplot(\n    data=scores_df,\n    x='AUC',\n    y=scores_df.index,\n    orient='h',\n    color=cmap[1],\n    ax=ax\n)\n\nfor idx in range(0, len(scores_df)):\n    x = scores_df['AUC'][idx]\n    ax.annotate(\n        s=f\"AUC: {np.round(x,3)}\",\n        xy=(x-0.01, idx),\n        va='center', ha='right'\n    )\n\nsns.despine(left=True)\nplt.show()","b43feb2c":"#### **Insights:**\n* quite a big dataset with 1 million rows and 287 colums *(...Hi there GPU quota)*\n* all numerical variables\n* mixture of continous (float, 240 cols) and discrete variables (int, 47 cols)\n* we have no missing values","3d251b51":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Spot-Checking<\/div>","15a82824":"### <div style='background:#5d7c8c;color:white;padding:0.25em;border-radius:0.2em'>Univariate Analysis: Target<\/div>","a4aafc94":"#### **Conclusion:**\n* Catboost performs best, \"straight-out-of-the-box\"\n* the treatment of the skewed features and especially the discrete variables might be interesting for feature-engineering","0cb6827e":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Import Data<\/div>","6c5a901b":"#### **Insights:**\n* our target classes are equally distributed (~50\/50)\n* we can spot heavily skewed features and features with a multi-modal distribution\n* we have some interestingly skewed features among the discrete variables (e.g. f275-f284)","ee43bcae":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Simple EDA<\/div>","9b3b17e2":"**Hi**,<br><br>\nI just wanted to share my quick EDA and a basic spot-check for this month competition.\nSince the dataset is quite big, I just used \"the big three\" (LGBM, XGB & CATB) for a spot-check with GPU enabled.\nAnyway, I hope it's still useful for someone...\n\n**Thanks for taking some time to check out my notebook. Feel free to leave an upvote if you like it or even copy some parts**\n\nBest Regards","be027947":"### <div style='background:#5d7c8c;color:white;padding:0.25em;border-radius:0.2em'>Basic Overview<\/div>","62a14b63":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Introduction<\/div>","8ed5d51f":"### <div style='background:#5d7c8c;color:white;padding:0.25em;border-radius:0.2em'>Univariate Analysis: Features<\/div>"}}