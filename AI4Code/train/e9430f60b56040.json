{"cell_type":{"6e5a41ad":"code","835d6547":"code","cb440541":"code","c3897085":"code","b11fa273":"code","b2fdbb93":"code","318fd4b2":"code","b96b7217":"code","61d390c4":"markdown","ef617be6":"markdown","fc168fb1":"markdown","8f72c037":"markdown","b0f3798e":"markdown"},"source":{"6e5a41ad":"# Load basic packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nimport numpy as np #foundational package for scientific computing\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nimport sklearn #collection of machine learning algorithms\nimport pickle #saving and loading models\nimport os\n\n# Visualization libraries\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n# Common model helpers for preprocessing etc.\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Configure Visualization Defaults\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\ninpdir = '..\/input'\nprint(os.listdir(inpdir))\n\n# Any results you write to the current directory are saved as output.","835d6547":"def read_data(year):\n    for dirs in os.listdir(inpdir):\n        if year in dirs:\n            d1 = inpdir + '\/' + dirs\n            n = os.listdir(d1)\n            name = os.listdir(d1 + '\/' + n[0] + '\/data')\n            return pd.read_csv(d1 + '\/' + n[0] + '\/data\/' + name[0])\n        \ndef trim_locations(df):\n    location_columns = ['latitude', 'longitude']\n    df.columns = [col.lower() for col in df.columns]\n    for col in location_columns:\n        df[col] = df[col].apply(pd.np.round(2))\n    return df\n        \n# Used for basic data exploration, getting info, description,\n# shape, amount of null values and some sample and head data\ndef basic_data_exploration(df):\n    print('-'*20)\n    print('Information about the dataset: ')\n    print(df.info())\n    print('-'*20)\n    print('Data description: ')\n    print(df.describe(include='all'))\n    print('-'*20)\n    print('Null values in our data: ')\n    print(df.isnull().sum().sort_values(ascending=False))\n    print('-'*20)\n    print('Data shape: ')\n    print(df.shape)\n    print('-'*20)\n    print('Data head (5): ')\n    print(df.head(5))\n    print('-'*20)\n    print('Data sample (10): ')\n    print(df.sample(10))\n    print('-'*20)\n    \n# Encode categories, will give them integer values to process\ndef encode_categories(df, columns):\n    \n    for col in columns:\n        df[col] = str(df[col])\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n    return df\n\n        \n# Read the data in, starting with 2010 - 2016 only, might extend\ndf2010 = read_data('2010')\ndf2011 = read_data('2011')\ndf2012 = read_data('2012')\ndf2013 = read_data('2013')\ndf2014 = read_data('2014')\ndf2015 = read_data('2015')\ndf2016 = read_data('2016')\n\ndataframes = [df2010,df2011,df2012,df2013,df2014,df2015,df2016]\n# Basic data exploration in numbers\n#basic_data_exploration(df)","cb440541":"# Amount of crimes per year\nyear = 2010\nx = []\ny = []\nfor years in dataframes:\n    x.append(year)\n    year +=1\n    y.append(years['delegacia'].count())\n\nplt.bar(x, y, 0.8, align='center')\nplt.ylabel('Amount of crimes')","c3897085":"# Amount of crimes per month in 2015\nmonthscrimes = df2015['mes'].value_counts()\nmonths = monthscrimes.plot.bar(x='mes', y='count', rot=0)","b11fa273":"# Amount of crimes per month in 2016\nmonthscrimes = df2016['mes'].value_counts()\nmonths = monthscrimes.plot.bar(x='mes', y='count', rot=0)","b2fdbb93":"# Amount of crimes per police station\npolicestationcrimes = df2016['delegacia'].value_counts().head(20)\npolicestations = policestationcrimes.plot.bar(x='delegacia', y='count', rot=90)\ntotal = df2016['delegacia'].count()\nprint('Total amount of crimes in 2016 (with registered policstation): ' + str(total))\npolicestationsamount = len(df2016['delegacia'].unique())\nprint('Total numer of policestations registered: ' + str(policestationsamount))\nprint('Amount of crimes registered in the 20 biggest police stations: {:.2%} '.format((policestationcrimes.sum()\/total)))","318fd4b2":"# Check amount of unique values for non-numeric features\nnumeric_features = df2016.select_dtypes(include=np.number).columns\nnon_numeric_features = df2016.select_dtypes(exclude=np.number).columns\nfor col in non_numeric_features:\n    print (len(df2016[col].unique()))\n\n# We could encode some of the features if we would want to model anything.\n#\n# df_enc = df2016.copy(deep=True)\n# df_enc = encode_categories(df_enc, [col for col in non_numeric_features if len(df[col].unique())<1000])\n# print(df_enc.info())\n","b96b7217":"def correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n    \n# Plot a simple correlation heatmap\ncorrelation_heatmap(df2016)","61d390c4":"This is one of my first kernels, starting with some basic data exploration and maybe later use the data to make predictions: \n\nWhat has been added already:\n- Function to import data from a certain year (I'm currently only using 2016)\n- Some basic imports functions to see what kind of data we have\n- Basic function to trim the locations thanks to Sohier Dane\n\nWhat I want to add:\n- Exploration and plotting of where the most crimes occured\n- Differences between different years, do locations and amount of crimes change?\n- When do most crimes occur? Are there specific months etc.","ef617be6":"So we found out that in October  and March the most crimes are committed in 2016, and in January the least.\n\nOnly real similarity is that October and March are high-crime months.","fc168fb1":"Most crimes in 2014, significant decrease in 2015\/2016.","8f72c037":"49.07 % of the crimes registered in about 2.5% of the policestations.","b0f3798e":"Most crimes in October, November and March, least in February September August in 2015"}}