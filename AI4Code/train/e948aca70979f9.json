{"cell_type":{"ed8cc47d":"code","fc994f29":"code","53ae69be":"code","54e24da2":"code","46ca0c61":"code","475418d5":"code","3a042b84":"code","3f821282":"code","993ed250":"code","92381752":"code","48a75f02":"code","f8535faf":"code","9070749b":"code","a94c3e19":"code","ace93bd3":"code","f14d53b9":"code","7ad9bff7":"code","b2c3ebbe":"code","dc422fcf":"code","66328482":"code","039a5aae":"code","46dcdbe0":"code","6134a03e":"code","9100e023":"code","4962cdc0":"code","5a4ecfd2":"code","d8d2ceca":"code","595183b3":"code","defa80dd":"code","69b9f310":"code","8803f68d":"code","4243830d":"code","d80984f3":"code","8add8dca":"code","5e6c415d":"code","e55b8f90":"code","02f96b13":"code","a19e9b0c":"code","b3a51f5f":"code","27c0a6cf":"code","f531a3a6":"code","440b24a5":"code","26f61848":"code","6243208a":"code","c5da7bd7":"code","c45a1086":"code","011a5c57":"code","a8c9d8a6":"code","052f6329":"code","0d8efef7":"code","f176b40d":"code","a6ce10d4":"code","d33ec352":"code","247f0f28":"code","ba93fb3f":"code","8509a79e":"markdown","bf6bb101":"markdown","39f91008":"markdown","87c0e83a":"markdown","af300765":"markdown","1545d61e":"markdown","871c713f":"markdown","3530f738":"markdown","4f2b5889":"markdown","314b5c0c":"markdown","1e7cfee3":"markdown","8bf49072":"markdown","861a2824":"markdown","843f93a2":"markdown","db4af71c":"markdown","e2cb9476":"markdown","0d722903":"markdown","87a26076":"markdown","121e9dac":"markdown","9c16c7dc":"markdown","7a6ce1ca":"markdown","e8fe923d":"markdown","99ea8d4f":"markdown","98ea3a27":"markdown","3dbeda32":"markdown","3b365730":"markdown","e92edfda":"markdown","748c8938":"markdown","68c72a8f":"markdown","36660d69":"markdown","ca159a33":"markdown","78030325":"markdown","397ec829":"markdown","2fa66782":"markdown","c48b50a8":"markdown","1fbae4bb":"markdown","87ef89f4":"markdown","a07ccb87":"markdown","985c916d":"markdown","0c172e1e":"markdown","4d186918":"markdown","3eb96f34":"markdown","bf48bcc2":"markdown","1b4d0704":"markdown","f66e5655":"markdown","d6a396da":"markdown","9d171950":"markdown","8b71d425":"markdown","6599ee74":"markdown"},"source":{"ed8cc47d":"# All project packages imported at the start\n\n# Project packages\nimport pandas as pd\nimport numpy as np\n\n#\u00a0Visualisations\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#\u00a0Statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom statistics import mode\nfrom scipy.special import boxcox1p\n\n#\u00a0Machine Learning\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import Lasso, Ridge, RidgeCV, ElasticNet\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom catboost import Pool, CatBoostRegressor, cv\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","fc994f29":"# Reading in the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","53ae69be":"# Inspecting the train dataset\ntrain.info()","54e24da2":"# And now the test data\ntest.info()","46ca0c61":"# Viewing the first 10 observations\ntrain.head(10)","475418d5":"# Let's get confirmation on the dataframe shapes\nprint(\"\\nThe train data size is: {} \".format(train.shape)) \nprint(\"The test data size is: {} \".format(test.shape))","3a042b84":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the 'Id' colum since it's unnecessary for the prediction process\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","3f821282":"# Checking for outliers in GrLivArea as indicated in dataset documentation\nsns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=True)\nplt.show()","993ed250":"# Removing two very extreme outliers in the bottom right hand corner\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#\u00a0Re-check graph\nsns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=True)\nplt.show()","92381752":"(mu, sigma) = norm.fit(train['SalePrice'])\n\n#\u00a01. Plot Sale Price\nsns.distplot(train['SalePrice'] , fit=norm);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n\n# Get the fitted parameters used by the function\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","48a75f02":"#\u00a02. Plot SalePrice as a QQPlot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","f8535faf":"#\u00a0Applying a log(1+x) transformation to SalePrice\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","9070749b":"#\u00a01. Plot Sale Price\nsns.distplot(train['SalePrice'] , fit=norm);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","a94c3e19":"#\u00a02. Plot SalePrice as a QQPlot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","ace93bd3":"# Saving train & test shapes\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# Creating y_train variable\ny_train = train.SalePrice.values\n\n# New all encompassing dataset\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\n# Dropping the target\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\n# Printing all_data shape\nprint(\"all_data size is: {}\".format(all_data.shape))","f14d53b9":"# Getting a missing % count\nall_data_missing = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_missing = all_data_missing.drop(all_data_missing[all_data_missing == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Percentage':all_data_missing})\nmissing_data.head(30)","7ad9bff7":"# Visualising missing data\nf, ax = plt.subplots(figsize=(10, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Missing Percentage'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","b2c3ebbe":"# Initiate correlation matrix\ncorr = train.corr()\n# Set-up mask\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set-up figure\nplt.figure(figsize=(14, 8))\n# Title\nplt.title('Overall Correlation of House Prices', fontsize=18)\n# Correlation matrix\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","dc422fcf":"# All columns where missing values can be replaced with 'None'\nfor col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    all_data[col] = all_data[col].fillna('None')","66328482":"# All columns where missing values can be replaced with 0\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    all_data[col] = all_data[col].fillna(0)","039a5aae":"# All columns where missing values can be replaced with the mode (most frequently occurring value)\nfor col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])","46dcdbe0":"# Imputing LotFrontage with the median (middle) value\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","6134a03e":"# Checking the new missing % count\nall_data_missing = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_missing = all_data_missing.drop(all_data_missing[all_data_missing == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio':all_data_missing})\nmissing_data.head(30)","9100e023":"#\u00a0Converting those variables which should be categorical, rather than numeric\nfor col in ('MSSubClass', 'OverallCond', 'YrSold', 'MoSold'):\n    all_data[col] = all_data[col].astype(str)\n    \nall_data.info()","4962cdc0":"#\u00a0Applying a log(1+x) transformation to all skewed numeric features\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n#\u00a0Compute skewness\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(15)","5a4ecfd2":"# Check on number of skewed features above 75% threshold\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"Total number of features requiring a fix for skewness is: {}\".format(skewness.shape[0]))","d8d2ceca":"# Now let's apply the box-cox transformation to correct for skewness\nskewed_features = skewness.index\nlam = 0.15\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lam)","595183b3":"#\u00a0Creating a new feature: Total Square Footage\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","defa80dd":"#\u00a0Identifying features where a class is over 97% represented\nlow_var_cat = [col for col in all_data.select_dtypes(exclude=['number']) if 1 - sum(all_data[col] == mode(all_data[col]))\/len(all_data) < 0.03]\nlow_var_cat","69b9f310":"# Dropping these columns from both datasets\nall_data = all_data.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)","8803f68d":"# List of columns to Label Encode\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# Process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# Check on data shape        \nprint('Shape all_data: {}'.format(all_data.shape))","4243830d":"# Get dummies\nall_data = pd.get_dummies(all_data)\n\nall_data.shape","d80984f3":"# Now to return to separate train\/test sets for Machine Learning\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","8add8dca":"# Set up variables\nX_train = train\nX_test = test\n\n# Defining two rmse_cv functions\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)","5e6c415d":"# Setting up list of alpha's\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30]\n\n# Iterate over alpha's\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]","e55b8f90":"# Plot findings\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")","02f96b13":"# 5 looks like the optimal alpha level, so let's fit the Ridge model with this value\nmodel_ridge = Ridge(alpha = 5)","a19e9b0c":"# Setting up list of alpha's\nalphas = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n\n# Iterate over alpha's\ncv_lasso = [rmse_cv(Lasso(alpha = alpha)).mean() for alpha in alphas]\n\n# Plot findings\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")","b3a51f5f":"# Initiating Lasso model\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005))","27c0a6cf":"# Setting up list of alpha's\nalphas = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n\n# Iterate over alpha's\ncv_elastic = [rmse_cv(ElasticNet(alpha = alpha)).mean() for alpha in alphas]\n\n# Plot findings\ncv_elastic = pd.Series(cv_elastic, index = alphas)\ncv_elastic.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")","f531a3a6":"# Initiating ElasticNet model\nmodel_elastic = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.0005))","440b24a5":"# Setting up list of alpha's\nalphas = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n# Iterate over alpha's\ncv_krr = [rmse_cv(KernelRidge(alpha = alpha)).mean() for alpha in alphas]\n\n# Plot findings\ncv_krr = pd.Series(cv_krr, index = alphas)\ncv_krr.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")","26f61848":"# Initiatiing KernelRidge model\nmodel_krr = make_pipeline(RobustScaler(), KernelRidge(alpha=6, kernel='polynomial', degree=2.65, coef0=6.9))","6243208a":"# Initiating Gradient Boosting Regressor\nmodel_gbr = GradientBoostingRegressor(n_estimators=1200, \n                                      learning_rate=0.05,\n                                      max_depth=4, \n                                      max_features='sqrt',\n                                      min_samples_leaf=15, \n                                      min_samples_split=10, \n                                      loss='huber',\n                                      random_state=5)","c5da7bd7":"# Initiating XGBRegressor\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.2,\n                             learning_rate=0.06,\n                             max_depth=3,\n                             n_estimators=1150)","c45a1086":"# Initiating LGBMRegressor model\nmodel_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1080,\n                              max_bin=75, \n                              bagging_fraction=0.80,\n                              bagging_freq=5, \n                              feature_fraction=0.232,\n                              feature_fraction_seed=9, \n                              bagging_seed=9,\n                              min_data_in_leaf=6, \n                              min_sum_hessian_in_leaf=11)","011a5c57":"# Initiating CatBoost Regressor model\nmodel_cat = CatBoostRegressor(iterations=2000,\n                              learning_rate=0.10,\n                              depth=3,\n                              l2_leaf_reg=4,\n                              border_count=15,\n                              loss_function='RMSE',\n                              verbose=200)\n\n# Initiating parameters ready for CatBoost's CV function, which I will use below\nparams = {'iterations':2000,\n          'learning_rate':0.10,\n          'depth':3,\n          'l2_leaf_reg':4,\n          'border_count':15,\n          'loss_function':'RMSE',\n          'verbose':200}","a8c9d8a6":"# Fitting all models with rmse_cv function, apart from CatBoost\ncv_ridge = rmse_cv(model_ridge).mean()\ncv_lasso = rmse_cv(model_lasso).mean()\ncv_elastic = rmse_cv(model_elastic).mean()\ncv_krr = rmse_cv(model_krr).mean()\ncv_gbr = rmse_cv(model_gbr).mean()\ncv_xgb = rmse_cv(model_xgb).mean()\ncv_lgb = rmse_cv(model_lgb).mean()","052f6329":"# Define pool\npool = Pool(X_train, y_train)\n\n# CV Catboost algorithm\ncv_cat = cv(pool=pool, params=params, fold_count=10, shuffle=True)","0d8efef7":"# Select best model\ncv_cat = cv_cat.at[1999, 'train-RMSE-mean']","f176b40d":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Ridge',\n              'Lasso',\n              'ElasticNet',\n              'Kernel Ridge',\n              'Gradient Boosting Regressor',\n              'XGBoost Regressor',\n              'Light Gradient Boosting Regressor',\n              'CatBoost'],\n    'Score': [cv_ridge,\n              cv_lasso,\n              cv_elastic,\n              cv_krr,\n              cv_gbr,\n              cv_xgb,\n              cv_lgb,\n              cv_cat]})\n\n# Build dataframe of values\nresult_df = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\nresult_df.head(8)","a6ce10d4":"# Plotting model performance\nf, ax = plt.subplots(figsize=(10, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=result_df['Model'], y=result_df['Score'])\nplt.xlabel('Models', fontsize=15)\nplt.ylabel('Model performance', fontsize=15)\nplt.ylim(0.10, 0.116)\nplt.title('RMSE', fontsize=15)","d33ec352":"# Fit and predict all models\nmodel_lasso.fit(X_train, y_train)\nlasso_pred = np.expm1(model_lasso.predict(X_test))\n\nmodel_elastic.fit(X_train, y_train)\nelastic_pred = np.expm1(model_elastic.predict(X_test))\n\nmodel_ridge.fit(X_train, y_train)\nridge_pred = np.expm1(model_ridge.predict(X_test))\n\nmodel_xgb.fit(X_train, y_train)\nxgb_pred = np.expm1(model_xgb.predict(X_test))\n\nmodel_gbr.fit(X_train, y_train)\ngbr_pred = np.expm1(model_gbr.predict(X_test))\n\nmodel_lgb.fit(X_train, y_train)\nlgb_pred = np.expm1(model_lgb.predict(X_test))\n\nmodel_krr.fit(X_train, y_train)\nkrr_pred = np.expm1(model_krr.predict(X_test))\n\nmodel_cat.fit(X_train, y_train)\ncat_pred = np.expm1(model_cat.predict(X_test))","247f0f28":"# Create stacked model\nstacked = (lasso_pred + elastic_pred + ridge_pred + xgb_pred + lgb_pred + krr_pred + gbr_pred) \/ 7","ba93fb3f":"# Setting up competition submission\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = stacked\nsub.to_csv('house_price_predictions.csv',index=False)","8509a79e":"Again, i'll be using RobustScaler to scale all features before initiating the ElasticNet model.","bf6bb101":"#### 3. ElasticNet Regression\nElastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).","39f91008":"#### 7. LightGBM\nA more recent gradient boosting algorithm which boasts significantly faster runtime than XGBoost, while still offering best-in-class predictive power.","87c0e83a":"# House Prices: Advanced Regression Techniques\n\n## Table of Contents\n- <b>Introduction<\/b>\n- <b>Data Processing<\/b>\n    - Outliers\n    - Target variable\n- <b>Feature engineering<\/b>\n    - Missing data\n        - <i>Exploration<\/i>\n        - <i>Imputation<\/i>\n    - Converting features\n- <b>Machine Learning<\/b>\n    - Set up\n    - Initiating algorithms\n        - <i>Generalized linear models<\/i>\n        - <i>Ensemble methods (Gradient tree boosting)<\/i>\n    - Fitting algorithms\n        - <i>Fit all models<\/i>\n        - <i>Rank model performance<\/i>\n    - Stacking algorithms\n- <b>Final predictions<\/b>","af300765":"With the rmse_cv function in place, I am going to tackle modelling in three phases - hopefully making it easy to follow:\n\n1. Initiating algorithms\n2. Fitting algorithms\n3. Stacking algorithms","1545d61e":"#### 6. XGBoost\nAnother gradient boosting algorithm; one that's well documented as being the key to many winning solutions on Kaggle.","871c713f":"#\u00a0Machine Learning\n## Set-up\nBefore modelling I am going to define a function that returns the cross-validation 'rmse' error, following 10-folds. This will ensure that all rmse scores produced have been smoothed out across the entire dataset and are not a result of any irregularities, which otherwise would provide a misleading representation of model performance. And that, we do not want.","3530f738":"### Fit all models\nI'll now run the custom rmse_cv function on each algorithm to understand each model's performance. This function doesn't work for the CatBoost algorithm, so I will just fit this for now and will return with a solution at a later date.","4f2b5889":"### Imputation\n\nI have bundled features into a few different operations depending on what best fits their structure, whether that is replacing with a string or integer to denote zero, or imputation via a specific value. I have spared a lot of the trial and erroring with the final code used to achieve 0 missing values across both datasets.","314b5c0c":"#### 1. Ridge Regression (<i>L2 Regularisation<\/i>)\nRidge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients <b>close to zero.<\/b>\n\nThe shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients.\n\nFor regularised regression models, the key tuning parameter is <b>alpha<\/b> - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data. Thus I will define multiple alpha's, iterate over them and plot the result so we can easily see the optimal alpha level.","1e7cfee3":"#### 8. CatBoost\nAll the way from Russia, CatBoost is a new gradient boosting algorithm able to work with categorical features <b>without<\/b> any prior processing needed. I am still finding my feet with implementing the CatBoostRegressor - thus this section of the kernel is very much a work in progress. Any guidance on working with this algorithm would be greatly appreciated - especially with regards to performing cross-validation and hyperparameter tuning. The below parameters again came from my own trial & error.","8bf49072":"Lots of strong correlations on show, especially Overall Quality (not surprising)! Features regarding the Garage are also relating strongly. Right, let's impute the missing values ready for modelling.","861a2824":"## 3. Stacking algorithms\nI've ran eight models thus far, and they've all performed pretty well. I'm now quite keen to explore stacking as a means of achieving an even higher score. In a nutshell, stacking uses as a first-level (base) the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions. Stacking can be beneficial as combining models allows the best elements of their predictive power on the given challenged to be pooled, thus smoothing over any gaps left from an individual model and increasing the likelihood of stronger overall model performance.\n\nOk, let's get model predictions and then stack the results!","843f93a2":"### Exploration\nAs was evident when initially inspecting the data, many feature variable are missing values. To get a better sense of this, I will compile a ranked table of missing values by the % of data missing.","db4af71c":"## Converting variables\n### Amending dtypes\nI am going to perform a few further actions before modelling the data. This will be not an exhaustive engineering process, but instead some simple steps that will hopefully support more powerful future models. \n\nFirstly, there are some variables that should in fact be categorical rather than numeric, so i'll complete this step below.","e2cb9476":"## Introduction\n\nHello Kagglers! In this kernel i'll be taking on the Kaggle Competition: 'House Prices: Advanced Regression Techniques'. This competition uses the Ames Housing Dataset, which itself contains 1460 observations in both training and tests sets, and 80 features to boot. The challenge is to predict property Sale Price, hence this is a Regression problem. \n\nThroughout this kernel I will provide explanations about my code so you can understand the logic behind each action. While i'll conduct some feature engineering, my main focus will be to explore the predictive models and hopefully build an effective stacked model for final prediction.\n\nAt the time of posting, this model achieved a score within the top 12% of the Leaderboard, achieved through a simple approach to stacking.\n\nWell that's enough from me - enjoy the read and please feel free to share with me any feedback regarding my code or overall approach! I'm always looking to improve :).","0d722903":"An addition to the Lasso model - I will use a Pipeline to scale features. For the L1 norm to work properly, it's essential this step is taken before fitting the model.","87a26076":"## Final predictions\nNow to create the stacked model! I'm going to keep this very simple by equally weighting every model. This is done by summing together the models and then dividing by the total count. Weighted averages could be a means of gaining a slightly better final predictions, whereby the best performing models take a bigger cut of the stacked model. One of the more important considerations when undertaking any kind of model stacking is model independence. Stacking models that draw similar conclusions from the data is quite unlikely to yield a better score compared to a single model, because there's no additional insight being drawn out. Rather, model's that tackle the dataset in different ways, and that are able to detect unique aspects within it stand a better chance of contributing to a more powerful overall stacked model, since as a whole, more of the nuances within the data have been recognised and accounted for.\n\nPlease note, I am not going to include the CatBoost model as I found the model prediction declined when this was included - looks at the output it appears as though it is overfitting the data (visible through the differing learn\/test scores). I will return to this model later with a view to improve it's application to the current dataset.","121e9dac":"Let's now make this data clearer by plotting it in a graph - enter barplot:","9c16c7dc":"## Missing data","7a6ce1ca":"Another check on the Missing data table reveals exactly the desired outcome - nothing.","e8fe923d":"##\u00a0Outliers\n\nThe Ames dataset documentation reveals two outliers in the feature GrLivArea (Above grade (ground) living area square feet) - let's inspect these with a quick graph:","99ea8d4f":"That gives a better feel for what we are initally working with. As one final step pre-data processing, I'm going to take a copy of the ID column and remove it from both dataframes, since this is only needed when submitting final predictions to the Kaggle leaderboard, as opposed to be helpful within any predictive model.","98ea3a27":"### New feature\nI'm also going to create a new feature to bring together a few similar Features, into an overall 'Total Square Footage'.","3dbeda32":"We can see here the Target Variable is right skewed. A log transformation should help bring it back to normality. The code below will complete this.","3b365730":"The updated graph is looking better now. Praise to the documentation!","e92edfda":"# Feature Engineering\n\nFirstly,  I will compile all data into a single dataset to save code duplication across both train & test sets:","748c8938":"A couple of features look severely depleted, but the rest only suffer a few omissions which means imputing these blank variables certainly becomes an option. To get a better sense for how each feature correlates to the target variable, i'll draw up a correlation matrix, before then tackling the missing data. See below!","68c72a8f":"##\u00a0Target Variable\n\nLet's now learn more about the Target Variable - Sale Price. I'm particularly interested in detecting any skew which would become problematic during the modelling phase.","36660d69":"We can see from the above graph that the LASSO and ElasticNet are the best cross-validated models, scoring very closely to one another. Gradient boosting hasn't fared quite as well, however each algorithm still obtains a very respectable RMSE. The CatBoost model has not been cross-validated so I am not going to consider this algorithm (for the time being). ","ca159a33":"### Rank model performance\nThe moment of truth - let's see how each algorithm has performed, and which one tops the pile.","78030325":"As well as scaling features again for the Kernel ridge regression, I've defined a few more parameters within this algorithm:\n\n- Kernel: Polynomial\n    - <i>This means that the algorithm will not just consider similarity between features, but also similarity           between combinations of features.<\/i>\n- Degree & Coef0: \n    - <i>These are used to define the precise structure of the Polynomial kernel. I arrived at the below numbers          through a bit of trial and error. Implementing a GridSearchCV would probably yield a better overall fit.<\/i>","397ec829":"#### 2. Lasso Regression <i>(L1 regularisation)<\/i> \nLasso stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients.\n\nIn the case of lasso regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be <b>exactly equal to zero<\/b>. This means that, lasso can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model. For this reason, I usually prefer working with the Lasso algorithm over Ridge.\n\nLet's take the same appraoch to alpha selection, before initiating the Lasso model.","2fa66782":"A thing of beauty - the target variable now looks far more amenable for modelling. Let's move on now to some feature engineering.","c48b50a8":"### A. Generalized linear models\nI'm going to specifically focus on 'regularised' regression models within this section. <b>Regularisation<\/b> is a form of regression that shrinks (or 'regularises') the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. This will be particularly helpful for the current dataset where the model needs to account for ~80 features.\n\nThere are different types of regularised regressions - I will now explore each of them.","1fbae4bb":"####\u00a05. Gradient Boosting\nFor the Gradient Boosting algorithm I will use 'huber' as the loss function as this is robust to outliers. The other parameters on display originate from other kernels tackling this challenge, followed by trial and error to refine them to this specific dataset. Again, applying GridSearchCV will help to define a better set of parameters than those currently on display.\n\nFor the Gradient Boosting model I will use 'huber' as the loss function as this is robust to outliers.","87ef89f4":"### B. Ensemble methods (Gradient tree boosting)\nBoosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n\nThis technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time\/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of a boosting algorithm, and these are what i'll be applying to the current data next.","a07ccb87":"There a lot of object dtypes and a lot of missing values within this dataset. We'll need to consider these during data processing. \n\nTO add, a lot of features have been abbreviated. For reference, here are their full names along with a brief explanation:\n\n- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n- MSSubClass: The building class\n- MSZoning: The general zoning classification\n- LotFrontage: Linear feet of street connected to property\n- LotArea: Lot size in square feet\n- Street: Type of road access\n- Alley: Type of alley access\n- LotShape: General shape of property\n- LandContour: Flatness of the property\n- Utilities: Type of utilities available\n- LotConfig: Lot configuration\n- LandSlope: Slope of property\n- Neighborhood: Physical locations within Ames city limits\n- Condition1: Proximity to main road or railroad\n- Condition2: Proximity to main road or railroad (if a second is present)\n- BldgType: Type of dwelling\n- HouseStyle: Style of dwelling\n- OverallQual: Overall material and finish quality\n- OverallCond: Overall condition rating\n- YearBuilt: Original construction date\n- YearRemodAdd: Remodel date\n- RoofStyle: Type of roof\n- RoofMatl: Roof material\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n- MasVnrType: Masonry veneer type\n- MasVnrArea: Masonry veneer area in square feet\n- ExterQual: Exterior material quality\n- ExterCond: Present condition of the material on the exterior\n- Foundation: Type of foundation\n- BsmtQual: Height of the basement\n- BsmtCond: General condition of the basement\n- BsmtExposure: Walkout or garden level basement walls\n- BsmtFinType1: Quality of basement finished area\n- BsmtFinSF1: Type 1 finished square feet\n- BsmtFinType2: Quality of second finished area (if present)\n- BsmtFinSF2: Type 2 finished square feet\n- BsmtUnfSF: Unfinished square feet of basement area\n- TotalBsmtSF: Total square feet of basement area\n- Heating: Type of heating\n- HeatingQC: Heating quality and condition\n- CentralAir: Central air conditioning\n- Electrical: Electrical system\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n- LowQualFinSF: Low quality finished square feet (all floors)\n- GrLivArea: Above grade (ground) living area square feet\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms\n- FullBath: Full bathrooms above grade\n- HalfBath: Half baths above grade\n- Bedroom: Number of bedrooms above basement level\n- Kitchen: Number of kitchens\n- KitchenQual: Kitchen quality\n- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n- Functional: Home functionality rating\n- Fireplaces: Number of fireplaces\n- FireplaceQu: Fireplace quality\n- GarageType: Garage location\n- GarageYrBlt: Year garage was built\n- GarageFinish: Interior finish of the garage\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet\n- GarageQual: Garage quality\n- GarageCond: Garage condition\n- PavedDrive: Paved driveway\n- WoodDeckSF: Wood deck area in square feet\n- OpenPorchSF: Open porch area in square feet\n- EnclosedPorch: Enclosed porch area in square feet\n- 3SsnPorch: Three season porch area in square feet\n- ScreenPorch: Screen porch area in square feet\n- PoolArea: Pool area in square feet\n- PoolQC: Pool quality\n- Fence: Fence quality\n- MiscFeature: Miscellaneous feature not covered in other categories\n- MiscVal: $Value of miscellaneous feature\n- MoSold: Month Sold\n- YrSold: Year Sold\n- SaleType: Type of sale\n- SaleCondition: Condition of sale","985c916d":"<b>Box Cox Transformation of (highly) skewed features<\/b>\n\nSkewed features are a formality when dealing with real-world data. Transformation techniques can help to stabilize variance, make data more normal distribution-like and improve the validity of measures of association.\n\nThe problem with the Box-Cox Transformation is estimating lambda. This value will depend on the existing data, and as such should be considered when performing cross validation on out of sample datasets.","0c172e1e":"## 2. Fitting algorithms","4d186918":"### Class imbalance\nLastly, a test for any significance class imbalance. Any variable that is represented by a single class by greater than 97% will be removed from the datasets. I also explored the same strategy at the 95% level, but found that model performance decreased ever so slightly with the removal of two further features - LandSlope & MiscFeature. Thus, I will stick at the 97% level.","3eb96f34":"### Get dummies\nI will now round up the feature engineering stage of this project by creating dummy variables ready for model building.","bf48bcc2":"## 1. Initiating algorithms\nI'm going to be working with two broad sets of algorithms within this kernel:\n\n1. Generalized linear models\n2. Ensemble methods (specifically Gradient Tree Boosting)","1b4d0704":"### Transforming skewed feature variables\nOk, the dataset is starting to look better. I considered and fixed for skew within the Target variable earlier on, let's now do the same for all remaining numeric Feature variables.","f66e5655":"Yep, two pretty clear outliers in the bottom right hand corner. It's not always appropriate to delete outliers - removing too many can actually detriment the model's quality. These two however look relatively safe, and with backing from the documentation i'm going to go ahead and clear them.","d6a396da":"# Data Processing","9d171950":"### Label encoding\nThis step build on the previous step whereby all text data will become numeric. This is a requirement for Machine Learning, that is, only numerical data can be fed into a predictive model. There are many other encoding techniques available, some of which more powerful than Label Encoding which does incur the risk of falsely ranking variables, e.g. coding three locations into 0, 1 and 2 might imply that 2 is a higher value than 0, which is incorrect as the numbers just represent different categories (locations). This is a simple approach, however, and therefore I'm going to stick with it for the current kernel.\n\nCheck out this link for more on encoding data: \nhttps:\/\/www.kdnuggets.com\/2015\/12\/beyond-one-hot-exploration-categorical-variables.html","8b71d425":"And there you have it! Within this kernel I have performed simple data preparation techniques before applying several models, and then combining their performance into a single stacked model. This achieved a final RMSE that pitched me within the top 12% of the leaderboard.\n\nI hope the approach and techniques on display in this kernel have been helpful in terms of not just solving the current challenges, but other regression and broader machine learning challenges. \n\nIf this kernel has indeed helped you - i'd very much like to hear it :). Please also share with me any suggestions that could improve my final model, i'm always looking to learn more. In terms of future version, I aim to tackle the following:\n\n- Perfecting the CatBoost model\n- Performing a more rigorous GridSearchCV\n- Exploring more complex methods of model stacking for better final prediction.\n\nThank you for reading :). ","6599ee74":"####\u00a04. Kernel ridge regression\nOK, this is not strictly a generalized linear model. Kernel ridge regression (KRR) combines Ridge Regression (linear least squares with l2-norm regularization) with the 'kernel trick'. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space."}}