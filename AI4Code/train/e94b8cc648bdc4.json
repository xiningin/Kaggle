{"cell_type":{"44929319":"code","a40e0f3b":"code","73e67643":"code","f34e8adc":"code","d86a331e":"code","c2c2aecc":"code","aac74f8a":"code","c45ef2c1":"code","e326de61":"code","ab81fcf5":"code","3e9b57e9":"code","c9e13cb1":"code","900635d4":"code","97aa9664":"code","1c8fff1b":"code","f28ca7cf":"code","18ffe655":"code","db2067e6":"code","6749bf6e":"code","f42ba53a":"code","efeca868":"code","d58774ee":"code","a55c350e":"code","cb596bad":"code","46b5d3e9":"code","114a5c15":"code","a6de6c88":"code","3a20eaa2":"code","758ac1d4":"code","d0ca900f":"markdown","9e6a46bc":"markdown","5250348b":"markdown","f2fe64d0":"markdown","c8605a68":"markdown","932634a2":"markdown","b1657df5":"markdown","40d2bbe7":"markdown","75eea67d":"markdown","4d8eeac2":"markdown","3596d33d":"markdown","af6b2375":"markdown","c3566dd8":"markdown","da9fbf55":"markdown","2d5d5421":"markdown","af1e0bb8":"markdown","069d8cc3":"markdown","0db5010e":"markdown","2cc30c42":"markdown","5b87cdea":"markdown","198d33ab":"markdown","f314b6e7":"markdown","ffc74cda":"markdown","ebbf99a4":"markdown","b97f79f3":"markdown","5a65c538":"markdown","13f7dc5b":"markdown"},"source":{"44929319":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom testcases_v2 import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n%matplotlib inline\nnp.random.seed(1) # set a seed so that the results are consistant\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import Image","a40e0f3b":"X, Y = load_planar_dataset()","73e67643":"# Visualize the data\nplt.scatter(X[0, :],X[1, :], c=Y, s=35,cmap=plt.cm.Spectral)","f34e8adc":"shape_X = X.shape\nshape_Y = Y.shape\nm = shape_X[1] # training set size\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('We have m = %d training examples!' % (m))","d86a331e":"# Train the Logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV()\nclf.fit(X.T, Y.T)","c2c2aecc":"# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda j: clf.predict(j), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))\/float(Y.size)*100) + '% ' + \"(percentage of correctly labelled datapoints)\")\n","aac74f8a":"Image(\"https:\/\/raw.githubusercontent.com\/enzymesnaer\/images\/master\/classification.png\", width=500, height=200)","c45ef2c1":"def layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    n_x = X.shape[0] # size of input layer\n    n_h = 4\n    n_y = Y.shape[0] # size of output layer\n    return (n_x, n_h, n_y)\n    ","e326de61":"X_assess, Y_assess = layer_sizes_test_case()\n(n_x, n_h, n_y) =  layer_sizes(X_assess, Y_assess)\n(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))","ab81fcf5":"def initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Returns: \n    params: python dict containing your parameters:\n    W1 -- weight matrix of shape(n_h, n_x)\n    b1 -- bias vector of shape (n_h, 1)\n    W2 -- weight matrix of shape (n_y, n_h)\n    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    np.random.seed(2) # We set up a seed so that our output remains identical.\n    \n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y, 1))\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    return parameters","3e9b57e9":"n_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \\n\" + str(parameters[\"W1\"]))\nprint(\"b1 = \\n\" + str(parameters[\"b1\"]))\nprint(\"W2 = \\n\" + str(parameters[\"W2\"]))\nprint(\"b2 = \\n\" + str(parameters[\"b2\"]))","c9e13cb1":"def forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing our parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n    return A2, cache\n    \n    ","900635d4":"X_assess, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(X_assess, parameters)\n\n# Note: we use the mean here just to make sure that our output matches test cases. \nprint(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))","97aa9664":"def compute_cost(A2, Y, parameters):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n    \n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \"\"\"\n    m = Y.shape[1] # number of examples\n    # Compute the cross entropy cost\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1-Y)\n    cost = -np.sum(logprobs) \/ m\n    cost = np.squeeze(cost) # makes sure the cost is the dimensions we expect.\n    assert(isinstance(cost, float))\n    return cost\n","1c8fff1b":"A2, Y_assess, parameters = compute_cost_test_case()\n\nprint(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))","f28ca7cf":"Image(\"https:\/\/raw.githubusercontent.com\/enzymesnaer\/images\/master\/grad_summary.png\", width=500, height=200)","18ffe655":"def backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    \n    # Retrieve also A1 and A2 from dictionary \"cache\"\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2\n    dZ2 = A2 - Y\n    dW2 = np.dot(dZ2, A1.T) \/ m\n    db2 = np.sum(dZ2, axis = 1, keepdims = True) \/ m\n    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n    dW1 = np.dot(dZ1, X.T) \/ m\n    db1 = np.sum(dZ1, axis = 1, keepdims = True) \/ m\n    \n    grads = {\n        \"dW1\": dW1,\n        \"db1\": db1,\n        \"dW2\": dW2,\n        \"db2\": db2\n    }\n    \n    return grads","db2067e6":"parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\ngrads = backward_propagation(parameters, cache, X_assess, Y_assess)\nprint (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\nprint (\"db1 = \\n\"+ str(grads[\"db1\"]))\nprint (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\nprint (\"db2 = \\n\"+ str(grads[\"db2\"]))","6749bf6e":"Image(\"https:\/\/raw.githubusercontent.com\/enzymesnaer\/images\/master\/sgd.gif\", width=500, height=200)","f42ba53a":"Image(\"https:\/\/raw.githubusercontent.com\/enzymesnaer\/images\/master\/sgd_bad.gif\", width=500, height=200)","efeca868":"def update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    \n    # Update rule for each parameter\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","d58774ee":"parameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \\n\" + str(parameters[\"W1\"]))\nprint(\"b1 = \\n\" + str(parameters[\"b1\"]))\nprint(\"W2 = \\n\" + str(parameters[\"W2\"]))\nprint(\"b2 = \\n\" + str(parameters[\"b2\"]))","a55c350e":"def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        A2, cache = forward_propagation(X, parameters)\n        \n        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n        cost = compute_cost(A2, Y, parameters)\n        \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        grads = backward_propagation(parameters, cache, X, Y)\n        \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        parameters = update_parameters(parameters, grads)\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n    return parameters\n","cb596bad":"X_assess, Y_assess = nn_model_test_case()\nparameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\nprint(\"W1 = \\n\" + str(parameters[\"W1\"]))\nprint(\"b1 = \\n\" + str(parameters[\"b1\"]))\nprint(\"W2 = \\n\" + str(parameters[\"W2\"]))\nprint(\"b2 = \\n\" + str(parameters[\"b2\"]))","46b5d3e9":"def predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0\/1 using 0.5 as the threshold.\n    A2, cache = forward_propagation(X, parameters)\n    predictions = A2 > 0.5\n    return predictions","114a5c15":"parameters, X_assess = predict_test_case()\n\npredictions = predict(parameters, X_assess)\nprint(\"predictions mean = \" + str(np.mean(predictions)))","a6de6c88":"# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))","3a20eaa2":"# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))\/float(Y.size)*100) + '%')","758ac1d4":"plt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))\/float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))","d0ca900f":"## Tuning hidden layer size","9e6a46bc":"The following code will load a \"flower\" 2-class dataset into variables X and Y.","5250348b":"We have:\n- a numpy-array (matrix) X that contains your features (x1, x2)\n- a numpy-array (vector) Y that contains your labels (red:0, blue:1).","f2fe64d0":"**General gradient descent rule:** <br>\n$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n\n**Illustration:** <br>\nThe gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.","c8605a68":"Using the cache computed during forward propagation, we can now implement backward propagation.","932634a2":"## Defining the neural network structure\n### Define 3 variables\n> - n_x: the size of the input layer\n> - n_y: the size of the hidden layer (set this to 4)\n> - n_y: the size of the output layer\n\nWe use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.","b1657df5":"- Tips:\n> -To compute dZ1 we need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So we can compute $g^{[1]'}(Z^{[1]})$ using (1 - np.power(A1, 2)).","40d2bbe7":"We can now plot the decision boundary of these models. Run the code below.","75eea67d":"Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.\n\nNow, let's try out several hidden layer sizes.","4d8eeac2":"Visualize the dataset using matplotlib. The data looks like a \"flower\" with some red (label y=0) and some blue (y=1) points. Our goal is to build a model to fit this data.","3596d33d":"Reminder: The general methodology to build a Neural Network is to:\n\n1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n2. Initialize the model's parameters\n3. Loop:\n    - Implement forward propagation\n    - Compute loss\n    - Implement backward propagation to get the gradients\n    - Update parameters (gradient descent)\n\nWe often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model(). Once we've built nn_model() and learnt the right parameters, we can make predictions on new data.","af6b2375":"(these are not the sizes you will use for your network, they are just used to assess the function you've just coded)","c3566dd8":"### Integrating above parts in right order in neural network model in nn_model():\n","da9fbf55":"Now that we have computed $A^{[2]}$ (in the Python variable \"A2\"), which contains $a^{[2](i)}$ for every example, we can compute the cost function as follows:\n\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$","2d5d5421":"## Predictions\nUse above model to predict by building predict(). Use forward propagation to predict results.<br>\nReminder: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases} 1 & \\text{if}\\ activation > 0.5 \\\\ 0 & \\text{otherwise} \\end{cases}$\n<br>\nAs an example, if we would like to set the entries of a matrix X to 0 and 1 based on a threshold we would do: X_new = (X > threshold)","af1e0bb8":"The dataset is not linearly separable, so logistic regression doesn't perform well. Hopefully a neural network will do better.","069d8cc3":"## Implementing function backward_propagation()\n**Instructions:**\nBackpropagation is usually the hardest (most mathematical) part in deep learning. We'll use the six equations listed in the summary, since we are building a vectorized implementation.","0db5010e":"## Computing Cost: Implementing compute_cost()\n**Instructions:**\n- There are many ways to implement the cross-entropy loss. This is how we would implement $- \\sum\\limits_{i=0}^{m} y^{(i)}\\log(a^{[2](i)})$: \n> logprobs = np.multiply(np.log(A2),Y)<br>\n> cost = - np.sum(logprobs) # no need to use a for loop!\n- (We can use either np.multiply() and then np.sum() or directly np.dot()).","2cc30c42":"# **Planar data classification with one hidden layer**\n> - Implement a 2-class classification neural network with a single hidden layer\n> - Use units with a non-linear activation function, such as tanh\n> - Compute the cross entropy loss\n> - Implement forward and backward propagation","5b87cdea":"For one example $x^{(i)}$:$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$$$y^{(i)}_{prediction} = \\begin{cases} 1 &amp; \\mbox{if } a^{[2](i)} &gt; 0.5 \\\\ 0 &amp; \\mbox{otherwise } \\end{cases}\\tag{5}$$\n\nGiven the predictions on all the examples, you can also compute the cost $J$ as follows: $$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large \\right) \\small \\tag{6}$$","198d33ab":"## The loop: Implementing forward_propagation()\n**Instructions:**\n- Look at the above mathematical representation of our classifier.\n- We can use the function sigmoid(). It is built-in (imported) in the notebook.\n- We can use the function np.tanh(). It is part of the numpy library.\n- The steps we have to implement are:\n> 1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of initialize_parameters()) by using parameters[\"..\"].\n> 2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all our predictions on all the examples in the training set).\n- Values needed in the backpropagation are stored in \"cache\". The cache will be given as an input to the backpropagation function.","f314b6e7":"# Neural Network model\nLogistic regression did not work well on the \"flower dataset\". We are going to train a Neural Network with a single hidden layer.","ffc74cda":"It is time to run the model and see how it performs on a planar dataset. Run the following code to test our model with a single hidden layer of $n_h$ hidden units.","ebbf99a4":"## Simple Logistic Regression\nBefore building a full neural network, lets first see how logistic regression performs on this problem. We can use sklearn's built-in functions to do that.","b97f79f3":" Implementing the update rule (function *update_parameters(args)*) using gradient descent. We have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).","5a65c538":"Interpretation:\n\n- The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data.\n- The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting.\n- You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting.","13f7dc5b":"Initialize the model's parameters with initialize_parameters()\n- Make sure our parameters sizes are right.\n- We will initialize the weights matrices with random values.\n> - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a, b)\n- We will initialize the bias vectors as zeros\n> - Use: np.zeros((a, b))to initialize a matrix of shape (a,b) with zeros."}}