{"cell_type":{"d44bd1ad":"code","79ea597d":"code","b01db0af":"code","4fa1ced5":"code","503ed7eb":"code","109111e5":"code","df2bd3d1":"code","78871abd":"code","eaf55274":"code","0e21d77b":"code","c15812f7":"code","7d954599":"code","1ea104cd":"code","7206f1b3":"code","4b4d6e91":"code","4c002274":"code","86db827a":"code","75e3e915":"code","89a552b5":"code","0c93e773":"code","2a633fb7":"code","a099cac7":"code","04fccdc1":"code","0c4084aa":"code","8dd1cdc4":"code","928205cd":"code","dd74eaa9":"code","d4ccb76d":"code","67fae7fc":"code","eadda25b":"code","4010e13e":"code","0f6573fd":"code","b3967c3c":"code","569559ab":"code","104b05db":"code","d0a06645":"code","19482637":"code","bd88d46d":"code","6005c621":"code","c185001f":"code","84b312a0":"code","c58dac60":"code","dce6687f":"code","9d0bf4ff":"code","9f0987dc":"code","fbd52320":"code","61af790e":"code","25fcc86f":"code","c1a8382f":"code","e90e81d9":"code","6bc6c506":"code","370d7dd1":"code","63766c6b":"markdown","7610b033":"markdown","49beffe2":"markdown","c477f50b":"markdown","2c5469ec":"markdown","348cd632":"markdown","53bd5e63":"markdown","9f4cccce":"markdown","16b13fb6":"markdown","3efcc6b0":"markdown","01cdb908":"markdown","31ea3e91":"markdown","6ab3db47":"markdown","c47c8266":"markdown","36ebe3d6":"markdown","b9c4c725":"markdown","ef15df1e":"markdown","c8788968":"markdown","427c10d5":"markdown","152b7c8f":"markdown","5046bd8a":"markdown","84c5418d":"markdown","f4e2b695":"markdown","892696f6":"markdown","a21939de":"markdown","463ad86f":"markdown","fea5ebc6":"markdown","69a19e0d":"markdown","388e2ae2":"markdown","cde2716b":"markdown","815f2361":"markdown","273168c2":"markdown","06c90fe4":"markdown","5a3d9520":"markdown","66e290df":"markdown","7c244b9c":"markdown","547577ef":"markdown","faec8027":"markdown","97d5a9cf":"markdown","7eeb23a1":"markdown","e5966a3c":"markdown","962f56dd":"markdown","9801945a":"markdown","286ee493":"markdown","e8099a4f":"markdown","c77d3141":"markdown","463c63e4":"markdown","3e101fca":"markdown","1d930c79":"markdown","600536e8":"markdown","a35218de":"markdown","716e7a85":"markdown","c1fcbe51":"markdown","f086ce6a":"markdown","de044b44":"markdown","745d6408":"markdown","7e0f570c":"markdown","b4b7908f":"markdown","1f4fdad3":"markdown","139176c4":"markdown","0f963437":"markdown"},"source":{"d44bd1ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","79ea597d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import resample\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVR\nfrom pprint import pprint\nfrom matplotlib import pyplot\nimport time\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor","b01db0af":"df  = pd.read_csv('\/kaggle\/input\/concrete-compressive-strength\/concrete.csv')\ndf.head(5)","4fa1ced5":"rows_count, columns_count = df.shape\nprint('Total Number of rows :', rows_count)\nprint('Total Number of columns :', columns_count)\n","503ed7eb":"df.info()","109111e5":"sns.heatmap(df.isna(), yticklabels=False, cbar=False, cmap='viridis')","df2bd3d1":"df_transpose = df.describe().T\ndf_transpose","78871abd":"concrete_df = df.copy()","eaf55274":"plt.figure(figsize=(12,6))\nsns.boxplot(data=concrete_df, orient=\"h\", palette=\"Set2\", dodge=False)","0e21d77b":"sns.pairplot(concrete_df,markers=\"h\", diag_kind = 'kde')\nplt.show()","c15812f7":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['cement'],ax=ax1)\nax1.tick_params(labelsize=15)\nax1.set_xlabel('cement', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\n\nsns.boxplot(concrete_df['cement'],ax=ax2)\nax2.set_title(\"Box Plot\")\nax2.set_xlabel('cement', fontsize=15)","7d954599":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['slag'],ax=ax1)\nax1.set_xlabel('Slag', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['slag'],ax=ax2)\nax2.set_xlabel('Slag', fontsize=15)\nax2.set_title(\"Box Plot\")\n","1ea104cd":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['ash'],ax=ax1)\nax1.set_xlabel('Ash', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['ash'],ax=ax2)\nax2.set_xlabel('Ash', fontsize=15)\nax2.set_title(\"Box Plot\")\n","7206f1b3":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['water'],ax=ax1)\nax1.set_xlabel('Water', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['water'],ax=ax2)\nax2.set_xlabel('Water', fontsize=15)\nax2.set_title(\"Box Plot\")\n\n","4b4d6e91":"outlier_columns = []\n\nQ1 =  concrete_df['water'].quantile(0.25) # 1\u00ba Quartile\nQ3 =  concrete_df['water'].quantile(0.75) # 3\u00ba Quartile\nIQR = Q3 - Q1                      # Interquartile range\n\nLTV_water = Q1 - 1.5 * IQR   # lower bound \nUTV_water = Q3 + 1.5 * IQR   # upper bound\n\nprint('Interquartile range = ', IQR)\nprint('water <',LTV_water ,'and >',UTV_water, ' are outliers')\nprint('Numerber of outliers in water column below the lower whisker =', concrete_df[concrete_df['water'] < (Q1-(1.5*IQR))]['water'].count())\nprint('Numerber of outliers in water column above the upper whisker =', concrete_df[concrete_df['water'] > (Q3+(1.5*IQR))]['water'].count())\n\n# storing column name and upper-lower bound value where outliers are presense \noutlier_columns.append('water')\nupperLowerBound_Disct = {'water':UTV_water}","4c002274":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['superplastic'],ax=ax1)\nax1.set_xlabel('superplastic', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['superplastic'],ax=ax2)\nax2.set_xlabel('Superplastic', fontsize=15)\nax2.set_title(\"Box Plot\")\n","86db827a":"Q1 =  concrete_df['superplastic'].quantile(0.25) # 1\u00ba Quartile\nQ3 =  concrete_df['superplastic'].quantile(0.75) # 3\u00ba Quartile\nIQR = Q3 - Q1                      # Interquartile range\n\nLTV_superplastic = Q1 - 1.5 * IQR   # lower bound \nUTV_superplastic = Q3 + 1.5 * IQR   # upper bound\n\nprint('Interquartile range = ', IQR)\nprint('superplastic <',LTV_superplastic ,'and >',UTV_superplastic, ' are outliers')\nprint('Numerber of outliers in superplastic column below the lower whisker =', concrete_df[concrete_df['superplastic'] < (Q1-(1.5*IQR))]['superplastic'].count())\nprint('Numerber of outliers in superplastic column above the upper whisker =', concrete_df[concrete_df['superplastic'] > (Q3+(1.5*IQR))]['superplastic'].count())\n\n# storing column name and upper-lower bound value where outliers are presense\noutlier_columns.append('superplastic')\nupperLowerBound_Disct['superplastic'] = UTV_superplastic","75e3e915":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['coarseagg'],ax=ax1)\nax1.set_xlabel('Coarseagg', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['coarseagg'],ax=ax2)\nax2.set_xlabel('Coarseagg', fontsize=15)\nax2.set_title(\"Box Plot\")","89a552b5":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['fineagg'],ax=ax1)\nax1.set_xlabel('Fineagg', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['fineagg'],ax=ax2)\nax2.set_xlabel('Fineagg', fontsize=15)\nax2.set_title(\"Box Plot\")\n","0c93e773":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['age'],ax=ax1)\nax1.set_xlabel('Age', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['age'],ax=ax2)\nax2.set_xlabel('Age', fontsize=15)\nax2.set_title(\"Box Plot\")","2a633fb7":"Q1 =  concrete_df['age'].quantile(0.25) # 1\u00ba Quartile\nQ3 =  concrete_df['age'].quantile(0.75) # 3\u00ba Quartile\nIQR = Q3 - Q1                      # Interquartile range\n\nLTV_age = Q1 - 1.5 * IQR   # lower bound \nUTV_age = Q3 + 1.5 * IQR   # upper bound\n\nprint('Interquartile range = ', IQR)\nprint('age <',LTV_age ,'and >',UTV_age, ' are outliers')\nprint('Numerber of outliers in age column below the lower whisker =', concrete_df[concrete_df['age'] < (Q1-(1.5*IQR))]['age'].count())\nprint('Numerber of outliers in age column above the upper whisker =', concrete_df[concrete_df['age'] > (Q3+(1.5*IQR))]['age'].count())\n\n# storing column name and upper-lower bound value where outliers are presense\noutlier_columns.append('age')\nupperLowerBound_Disct['age'] = UTV_age","a099cac7":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['strength'],ax=ax1)\nax1.tick_params(labelsize=15)\nax1.set_xlabel('strength', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\n\nsns.boxplot(concrete_df['strength'],ax=ax2)\nax2.set_title(\"Box Plot\")\nax2.set_xlabel('strength', fontsize=15)","04fccdc1":"print('These are the columns which have outliers : \\n\\n',outlier_columns)\nprint('\\n\\n',upperLowerBound_Disct)","0c4084aa":"concrete_df_new = concrete_df.copy()","8dd1cdc4":"for col_name in concrete_df_new.columns[:-1]:\n    q1 = concrete_df_new[col_name].quantile(0.25)\n    q3 = concrete_df_new[col_name].quantile(0.75)\n    iqr = q3 - q1\n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    \n    concrete_df_new.loc[(concrete_df_new[col_name] < low) | (concrete_df_new[col_name] > high), col_name] = concrete_df_new[col_name].median()","928205cd":"plt.figure(figsize=(15,8))\nsns.boxplot(data=concrete_df_new, orient=\"h\", palette=\"Set2\", dodge=False)","dd74eaa9":"concrete_df_new.shape","d4ccb76d":"concrete_df_new.corr()","67fae7fc":"mask = np.zeros_like(concrete_df_new.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(15,7))\nplt.title('Correlation of Attributes', y=1.05, size=19)\nsns.heatmap(concrete_df_new.corr(),vmin=-1, cmap='plasma',annot=True,  mask=mask, fmt='.2f')","eadda25b":"cluster_range = range( 2, 6 )   # expect 3 to four clusters from the pair panel visual inspection hence restricting from 2 to 6\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(concrete_df_new)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","4010e13e":"# Elbow plot\n\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","0f6573fd":"cluster = KMeans( n_clusters = 3, random_state = 2354 )\ncluster.fit(concrete_df_new)\n\nprediction=cluster.predict(concrete_df_new)\nconcrete_df_new[\"GROUP\"] = prediction     # Creating a new column \"GROUP\" which will hold the cluster id of each record\n\nconcrete_df_new_copy = concrete_df_new.copy(deep = True)  # Creating a mirror copy for later re-use instead of building repeatedly","b3967c3c":"centroids = cluster.cluster_centers_\ncentroids","569559ab":"# All variables are on same scale, hence we can omit scaling.\n# But to standardize the process we will do it here\nXScaled = concrete_df_new.apply(zscore)\nXScaled.head()","104b05db":"plt.figure(figsize=(12,6))\nsns.boxplot(data=XScaled, orient=\"h\", palette=\"Set2\", dodge=False)","d0a06645":"y_set = XScaled[['strength']]\nX_set = XScaled.drop(labels= \"strength\" , axis = 1)","19482637":"y_set = XScaled[['strength']]\nX_set = XScaled.drop(labels= \"strength\" , axis = 1)\n\n# data spliting using 80:20 train test data ratio and randon seeding 7\nX_model_train, X_test, y_model_train, y_test = train_test_split(X_set, y_set, test_size=0.20, random_state=7)","bd88d46d":"print('---------------------- Data----------------------------- \\n')\nprint('x train data {}'.format(X_model_train.shape))\nprint('y train data {}'.format(y_model_train.shape))\nprint('x test data  {}'.format(X_test.shape))\nprint('y test data  {}'.format(y_test.shape))\n","6005c621":"# data spliting using 70:30 train test data ratio and randon seeding 7\nX_train, X_validate, y_train, y_validate = train_test_split(X_model_train, y_model_train, test_size=0.30, random_state=7)","c185001f":"print('---------------------- Data----------------------------- \\n')\nprint('x train data {}'.format(X_train.shape))\nprint('y train data {}'.format(y_train.shape))\nprint('x test data  {}'.format(X_validate.shape))\nprint('y test data  {}'.format(y_validate.shape))\n","84b312a0":"# Defining the kFold function for the cross validation\nn_split = 10\nrandon_state = 7\nkfold = KFold(n_split, random_state = randon_state)\nlinear_model = []\nlinear_model_score = []\nlinear_model_RMSE = []\nlinear_model_R_2 = []\nModel = []\nRMSE = []\nR_sq = []","c58dac60":"rfTree = RandomForestRegressor(n_estimators=100)\nrfTree.fit(X_train, y_train.values.ravel())\nprint('Random Forest Regressor')\nrfTree_train_score = rfTree.score(X_train, y_train)\nprint(\"Random Forest Regressor Model Training Set Score:\",rfTree_train_score)\n\n\nrfTree_score = rfTree.score(X_validate, y_validate)\nprint(\"Random Forest Regressor Model Validation Set Score:\", rfTree_score)\n\nrfTree_rmse = np.sqrt((-1) * cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Random Forest Regressor Model RMSE :\", rfTree_rmse)\n\n\nrfTree_r2 = cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Random Forest Regressor Model R-Square Value :\", rfTree_r2)\n\nrfTree_model_df = pd.DataFrame({'Trainng Score': [rfTree_train_score],\n                           'Validation Score': [rfTree_score],\n                           'RMSE': [rfTree_rmse],\n                           'R Squared': [rfTree_r2]})\nrfTree_model_df","dce6687f":"print(\"Random Forest Regressor Model Test Data Set Score:\")\nrfTree_test_score = rfTree.score(X_test, y_test)\nprint(rfTree_test_score)","9d0bf4ff":"rf = RandomForestRegressor(random_state = 7)\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","9f0987dc":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10 , stop = 100, num = 3)]   # returns evenly spaced 10 numbers\n# Number of features to consider at every split\nmax_features = ['auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 10, num = 2)]  # returns evenly spaced numbers can be changed to any\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)","fbd52320":"rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 5, scoring='neg_mean_absolute_error', \n                              cv = kfold, verbose=2, random_state=7, n_jobs=-1,\n                              return_train_score=True)\n# Fit the random search model\nrf_random.fit(X_train, y_train.values.ravel());","61af790e":"# best ensemble model (with optimal combination of hyperparameters)\nrfTree = rf_random.best_estimator_\nrfTree.fit(X_train, y_train.values.ravel())\nprint('Random Forest Regressor')\nrfTree_train_score = rfTree.score(X_train, y_train)\nprint(\"Random Forest Regressor Model Training Set Score:\",rfTree_train_score)\n\nrfTree_score = rfTree.score(X_validate, y_validate)\nprint(\"Random Forest Regressor Model Validation Set Score:\",rfTree_score)\n\nrfTree_rmse = np.sqrt((-1) * cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Random Forest Regressor Model RMSE :\", rfTree_rmse)\n\nrfTree_r2 = cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Random Forest Regressor Model R-Square Value :\", rfTree_r2)\n\nrfTree_random_model_df = pd.DataFrame({'Trainng Score': [rfTree_train_score],\n                           'Validation Score': [rfTree_score],\n                           'RMSE': [rfTree_rmse],\n                           'R Squared': [rfTree_r2]})\nrfTree_random_model_df","25fcc86f":"rfTree_test_score = rfTree.score(X_test, y_test)\nprint(\"Random Forest Regressor Model Test Data Set Score:\", rfTree_test_score)","c1a8382f":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [10],\n    'max_features': ['log2'],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [5,10],\n    'n_estimators': np.arange(50, 71)\n}\nrfg = RandomForestRegressor(random_state = 7)\n\ngrid_search = GridSearchCV(estimator = rfg, param_grid = param_grid, \n                          cv = kfold, n_jobs = 1, verbose = 0, return_train_score=True)\n\ngrid_search.fit(X_train, y_train.values.ravel());\ngrid_search.best_params_","e90e81d9":"# best ensemble model (with optimal combination of hyperparameters)\nrfTree = grid_search.best_estimator_\nrfTree.fit(X_train, y_train.values.ravel())\nprint('Random Forest Regressor')\nrfTree_train_score = rfTree.score(X_train, y_train)\nprint(\"Random Forest Regressor Model Training Set Score:\", rfTree_train_score)\n\nrfTree_score = rfTree.score(X_validate, y_validate)\nprint(\"Random Forest Regressor Model Validation Set Score:\",rfTree_score)\n\nrfTree_rmse = np.sqrt((-1) * cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Random Forest Regressor Model RMSE :\", rfTree_rmse)\n\nrfTree_r2 = cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Random Forest Regressor Model R-Square Value :\", rfTree_r2)\n\nrfTree_random_model_df = pd.DataFrame({'Trainng Score': [rfTree_train_score],\n                           'Validation Score': [rfTree_score],\n                           'RMSE': [rfTree_rmse],\n                           'R Squared': [rfTree_r2]})\nrfTree_random_model_df","6bc6c506":"def input_scores(name, model, x, y):\n    Model.append(name)\n    RMSE.append(np.sqrt((-1) * cross_val_score(model, x, y, cv=kfold, \n                                               scoring='neg_mean_squared_error').mean()))\n    R_sq.append(cross_val_score(model, x, y, cv=kfold, scoring='r2').mean())\n#Comment: Above function uses to append the cross validation scores of the algorithms.","370d7dd1":"rfTree_random_model_df","63766c6b":"## Random Forest Regressor ::-","7610b033":"<b>Comment :<\/b> From above we can see that all three groups are at same lavel, I dont find any difference so we will not be proceding with the clustering.","49beffe2":"## Separating Independent and Dependent :-","c477f50b":"# ::-------------------------------------- Data Visualization ------------------------------------::","2c5469ec":"###  Data type of each attribute :-","348cd632":"### Ash :-","53bd5e63":"<b>Observation : <\/b> From the above boxplot we can see that there are outliers in some columns. From the above ploting I can say slag, water, superplastic, fineagg and age column are having clear outliers. Let see these plots separately. I will be finding the outliers counts in individual attributes analysis and <b>fixing the outliers<\/b> after visualization and analysis of each attribute.","9f4cccce":"### Shape of the data :- ","16b13fb6":"#### GridsearchCV :","3efcc6b0":"<b> Dataset : <\/b> Concrete Compressive Strength\n\n<b> Domain : <\/b>Material manufacturing\n\n<b>Description: <\/b> The actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not\nscaled).The data has 8 quantitative input variables, and 1 quantitative output\nvariable, and 1030 instances (observations).\n\n<b> Objective : <\/b>Modeling of strength of high performance concrete using Machine Learning\n\n\n<b>Steps : <\/b>This project involved feature exploration and selection to predict the strength of high-performance concrete. Used Regression models like Decision tree regressors to find out the most important features and predict the strength. Cross-validation techniques and Grid search were used to tune the parameters for best model performance.\n\n<b>Skills and Tools :<\/b> Regression, Decision trees, feature engineering","01cdb908":"# :::::::::::::::::::::Comparing performances of all the models::::::::::::::::::::::: ","31ea3e91":"<b>Insight : <\/b>From above we can see that there are outliers in Fineagg column and there are two peaks in distribution plot and there is right skewness because long tail is at right side(mean>median).","6ab3db47":"### Checking the presence of missing values :-","c47c8266":"## Age :-","36ebe3d6":"## ----------------------------------------------- Fixing Outliers -----------------------------------------------------\n\n- As we have seen above outlier are presence in the given dataset.\n- There are multiple ways to deal with outliers but I mostly prefer either to drop the outliers or repalce it with median\/mean.\n- Here I am going to replace the outliers with median becase if we drop then ther may be chance to loose some important information.\n- I have also shown the number of outliers presence in each column in below code.","b9c4c725":"#### After fixing outliers shape of dataframe:","ef15df1e":"<b>Correlation Insight :<\/b>- From above correlation matrix we can see that there are many features which are  correlated. if we see carefully then ash and cement are having corelation of -0.40. superplastic and water are having corelation of -0.66 . fineagg and water are having corelation of -0.45. Age and strenght have correlation of 0.50.\nLittle correlation of ~0.6 between Superplasticizer and Water (which is negative as evident from scatter matrix), but lets move forward as it is.\n\n'cement' has the highest correlation with the area of 'concrete_compressive_strength'(which is a positive correlation), followed by 'superplasticizer', which is also a positive correlation, 'ash' has the least correlation.","c8788968":"## Conclusion ::-","427c10d5":"## Checking the presence of outliers :-","152b7c8f":"<b>Observation : <\/b> From above heatmap we can see that there is no missing values are present.","5046bd8a":"# :::::::::::::::::::::::::::::::::::::::: Model Building :::::::::::::::::::::::::::::::::::::::::","84c5418d":"<b>Insight: <\/b>The elbow plot confirms our visual analysis that there are likely 3 good clusters.","f4e2b695":"# ::--------------------------- Exploratory Data Analysis -------------------------------- ::","892696f6":"## KMeans Clustering :-","a21939de":"Observation : <\/b> From the above pair plot we can infer the association among the attributes and target column as follows:\n- No high correlation between any two features\n- Strength have some possitive linear relation with cement and some with superplastic that means if the quantity of cement or superplastic is more then concrete is having more strength.\n- More strength is between 20-150 days aprox.\n- Strength is again decreasing again after 250 days approx.\n- Also It is quite visible multiple gaussian slag,ash,water, superplastic, age.\n- slag, cement and ash also have a tendency to create linear relation but it's not prominant.\n- Rest of the relation between other individual attributes are mostly formed cloud shape or symmetrical shape.","463ad86f":"## We will be  creating create 3 part of our dataset. We'll be working working test and validation data. And one part of data will be kept for Test the final score of our models.","fea5ebc6":"<b>Insight : <\/b>From above we can see that there are no outliers in coarseagg and there is a right skewness because long tail is at right side(mean>median).","69a19e0d":"<b>Comment: <\/b> I found the Random Forest Regressor is having lowest Root Mean Square Error (RMSE) and Higest R Square value. So, I can say it is the best model to execute our model. ","388e2ae2":"<b>Insight: <\/b>From above boxplot we can see that there are outliers in slug values lies between range 100 to 200. 400 is the higest slug value.","cde2716b":"<b>Comment: <\/b> So the model is our Random Forest Regressor model. After executing the model I found. \n - Training Data Score : 0.974034\n - Validation Data Score : 0.893274\n - Test Data Score : 0.8571364194016591\nWe need to tune our model further and need to check if the model score in test data can be improvised or not.","815f2361":"### Water :-","273168c2":"## Creating and view the correlation matrix :-","06c90fe4":"<b>Insight : <\/b>From above we can see that there are outliers in Age column and there are many peaks in distribution plot and there is left skewness because long tail is at left side(mean<median).","5a3d9520":"## Fineagg :-","66e290df":"<b>Insight : <\/b>From above we can see that there are no outliers in cement column and it's looks like normally distributed. Cemennt values lies between range 100 to 500. ","7c244b9c":"<b>Insight :<\/b> From above we can see that there are no outliers in ash column. We can see a tall tower at range of 0 to 20 which indicates if slug value is between 100 and 200.","547577ef":"## Superplastic :-","faec8027":"<b>Comment :<\/b> Here I have used numpy, pandas, matplotlib, seaborn, scipy for EDA and Data Visualization. Also used sklearn for data spliting, model building and for confusion matrix. ","97d5a9cf":"## Strength :-","7eeb23a1":"## ---------------------------------------- Correlation using Heatmap --------------------------------------------","e5966a3c":"### Slag :-","962f56dd":"### Copying Dataframe :-\n- Before doing any manipulation with the dataframe it is better to copy the dataframe into another dataframe and keep the original dataframe as it is.","9801945a":"###  Pair plot that includes all the columns of the data frame :-","286ee493":"## Hyper-tuning Random Forest Regressor ::-\nRandomSearchCV","e8099a4f":"## Coarseagg :-","c77d3141":"****************<b>Observations : <\/b> From above we can see that Mean and the median is nearly same for the Cement, Water, Superplastic, Coarseagg, Fineagg, Strength so we can say it is approximately normally distributed. Slag, Ash, Age are having much values at the maximum portion so we can say it is skewed towards right side.","463c63e4":"<b>Comment:<\/b> Here I have read the Concrete dataset using read_csv() function of pandas. df is a dataframe. I have used head() funtion to display first 10 records of the dataset.\n\n<b> Features(attributes) Understanding from the above dataframe :- <\/b> \n- <b>Cement <\/b> measured in kg in a m3 mixture\n- <b>Blast <\/b> measured in kg in a m3 mixture\n- <b>Fly ash <\/b> measured in kg in a m3 mixture\n- <b>Water <\/b> measured in kg in a m3 mixture\n- <b>Superplasticizer <\/b> measured in kg in a m3 mixture\n- <b>Coarse Aggregate <\/b> measured in kg in a m3 mixture\n- <b>Fine Aggregate <\/b> measured in kg in a m3 mixture\n- <b>Age <\/b> day (1~365)\n\n<b>Concrete compressive strength:-<\/b> measured in MPa\n\n","3e101fca":"<b>Insight : <\/b>From above we can see that there are outliers in water column and there is right skewness because long tail is at the right side.\n####  As ouliers are there in water so we will check how many outliers are there in the water.","1d930c79":"- From the above we have come to the conclusion that RandomForestRegressor is giving the good accuracy score. \n- I have also tested with validation data which is also giving better result with RandomForestRegressor. \n- Hence we can proceed with the RandomForestRegressor to  modeling of strength of high performance concrete.","600536e8":"<b>Comment:<\/b> Shape of the dataframe is (1030, 9).\nThere are 1030 rows and 9 columns in the dataset. ","a35218de":"<b>Insight : <\/b>From above we can see that there are no outliers in strength column and there are two peaks in distribution plot and there is right skewness because long tail is at right side(mean>median)","716e7a85":"<b> Insight : <\/b>From above we can see that there are outliers in superplastic column and there is right skewness because long tail is at right side(mean>median).\n\n<b>As outliers are there in superplastic so  we will check how many outliers are there in the superplastic.<\/b>","c1fcbe51":"## Import the necessary libraries :","f086ce6a":"df.apply(lambda x: sum(x.isnull()))","de044b44":"<b>Comment :<\/b> Here we can see that all the variables are numerical.","745d6408":"###  Descriptive Statistics :-","7e0f570c":"## Analysis of each attributes with the help of plots :-\n\n### Cement  :-","b4b7908f":"## Standardization Independent Varaibles :-","1f4fdad3":"<b>Important : <\/b> Now we can see in boxplot that most of the outliers are replaced with their median in dataframe. We have seen that outliers most of the outliers are removed but because of the gaussian by replacing it with median value, the attributes raised with new outliers which we can ignore. ","139176c4":"Defination of the function to comparing models","0f963437":"<b>Insight: <\/b> From above graph we see that the result of standardization(Z-score) is that the features are rescaled and their properties of a standard normal distribution changed to \u03bc=0 and \u03c3=1."}}