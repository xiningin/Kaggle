{"cell_type":{"b732a3bc":"code","7014dfbb":"code","cfd29b7d":"code","4b6dd6f5":"code","0237ad07":"code","40fa7458":"code","f26955cd":"code","6dbed9c7":"code","a174bc7f":"code","abc0369a":"code","72b27d33":"code","56176961":"code","571e7cb0":"markdown","33104322":"markdown","be011110":"markdown","01ca6d6f":"markdown","cce7352c":"markdown","fbf9d883":"markdown","43ef36a0":"markdown","6812ecca":"markdown","99e393ec":"markdown"},"source":{"b732a3bc":"import matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport pylab as pl\nimport numpy as np\nimport random\nimport glob\n\nfrom IPython import display\n\nfrom keras.layers import Input, Dense\nfrom keras import Model\n\n\nimport os","7014dfbb":"def syn_dataset():\n  syn_shape = (random.randint(2200,3500), 1)\n  return tf.random.normal(mean=random.randint(5,12), shape=syn_shape,\n                          stddev=round(random.uniform(0.15,0.5),2),\n                          dtype=tf.float32)\n  \ncounts, bin, ignored = plt.hist(syn_dataset().numpy(), 100)\n\naxes = plt.gca()\naxes.set_xlim([0, 12])\naxes.set_ylim([0,95])","cfd29b7d":"def build_generator(input_shape):\n  inputs = Input(input_shape)\n  l = Dense(64, activation = keras.activations.elu)(inputs)\n  l = Dense(64, activation = keras.activations.elu)(l)\n  f = Dense(1)(l)\n\n  gen = Model(inputs = inputs, outputs = f)\n\n  return gen","4b6dd6f5":"def build_discriminator(input_shape):\n  input = Input(input_shape)\n  d = Dense(32, activation = keras.activations.elu)(input)\n  d = Dense(1)(d)\n\n  dis = Model(inputs = input, outputs = d)\n  return dis","0237ad07":"input_shape = (1,)\n\nnoise_shape = (random.randint(50,100),)\n\ngene = build_generator(noise_shape)\n\ndisc = build_discriminator(input_shape)\n","40fa7458":"  bce = keras.losses.BinaryCrossentropy(from_logits=True)","f26955cd":"def disc_loss(real_output, generated_output):\n  return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(generated_output), generated_output)","6dbed9c7":"def gen_loss(generated_output):\n  return bce(tf.ones_like(generated_output), generated_output)","a174bc7f":"if not os.path.exists(\".\/gif\/\"):\n    os.makedirs(\".\/gif\/\")\n\ndef train():\n  optimizer = keras.optimizers.Adam(1e-5)\n\n  @tf.function\n  def train_step():\n    with tf.GradientTape(persistent=True) as tape:\n      real_data = syn_dataset()\n      noise_vector = tf.random.normal(mean = 0.0, stddev=1.0, \n                                      shape=(real_data.shape[0], \n                                             noise_shape[0]))\n      fake_data = gene(noise_vector)\n\n      d_fake_data = disc(fake_data)\n      d_real_data = disc(real_data)\n      d_loss_value = disc_loss(generated_output=d_fake_data, real_output=d_real_data)\n\n      g_loss_value = gen_loss(generated_output=d_fake_data)\n\n\n      d_gradients = tape.gradient(d_loss_value, disc.trainable_variables)\n\n      g_gradients = tape.gradient(g_loss_value, gene.trainable_variables)\n\n      del tape\n\n      optimizer.apply_gradients(zip(d_gradients, disc.trainable_variables))\n      optimizer.apply_gradients(zip(g_gradients, gene.trainable_variables))\n\n      return real_data, fake_data, g_loss_value, d_loss_value\n\n  fig, ax = plt.subplots()\n  no=1000\n  for step in range(40000):\n    real_data, fake_data, g_loss_value, d_loss_value= train_step()\n\n    if step % 200 == 0:\n      print(\n          \"Generator Loss: \", g_loss_value.numpy(),\n          \" Discriminator Loss: \", d_loss_value.numpy(),\n          \" steps: \", step\n          )\n      \n      ax.hist(fake_data.numpy(), 100)\n      ax.hist(real_data.numpy(), 100)\n\n      props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5)\n\n      textstr = f\"step={step}\"\n      ax.text(\n          0.05,\n          0.95,\n          textstr,\n          transform=ax.transAxes,\n          fontsize=14,\n          verticalalignment=\"top\",\n          bbox=props\n      )\n\n      axes = plt.gca()\n      axes.set_xlim([0, 15])\n      axes.set_ylim([0, 95])\n      display.display(pl.gcf())\n      display.clear_output(wait=True)\n      plt.savefig(\".\/gif\/{}.png\".format(no))\n      no=no+1\n      plt.gca().clear()\n\ntrain()","abc0369a":"from PIL import Image\n\nframes = []\nimgs = glob.glob(\"gif\/*.png\")\nimgs.sort()\nfor i in imgs:\n    new_frame = Image.open(i)\n    frames.append(new_frame)\n\n \n\nframes[0].save('training.gif', format='GIF',\n               append_images=frames[1:],\n               save_all=True,\n               duration=300, loop=0)","72b27d33":"from IPython.display import Image \n\nImage(open('training.gif','rb').read())\n","56176961":"def synthetic_dataset_uniform():\n  syn_shape = (random.randint(2200,3600), 1)\n  return tf.random.uniform(shape = syn_shape, minval= 0, maxval=500, \n                           dtype=tf.float32)\n  \ncounts, bin, ignored = plt.hist(synthetic_dataset_uniform().numpy(), 100)\n\naxes = plt.gca()\naxes.set_xlim([0,15])\naxes.set_ylim([0,100])","571e7cb0":"# Introduction","33104322":"# Creating a Synthetic Dataset\n\nWith this following function I have made a synthetic data for out netowrk to learn. I have used \"Normal Distribution\", below I have also attached a synthetic data generator for Uniform distribution. If you want to use any other kind of distribution head over to [tf.random](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/random) and chose whatever you want and build a function with it.","be011110":"# Defining the Loss Function\n\nFor a greater idea in GAN loss function I would suggest look into this [article](https:\/\/developers.google.com\/machine-learning\/gan\/loss#:~:text=Send%20feedback-,Loss%20Functions,distribution%20of%20the%20real%20data.&text=minimax%20loss%3A%20The%20loss%20function,the%20paper%20that%20introduced%20GANs.)","01ca6d6f":"# Creating the Model\n\nWe have defined a simple generator and a discriminator. To make deeper models you can look in to different architectures like DC-GAN, AC-GAN, SRGAN, etc.","cce7352c":"**Importing the necessary Libraries and packages**","fbf9d883":"**Unifrom Distribution**\n\nAs mentioned earlier this function creates a random uniform distribution. You can tweak the hyper parameters or even hardcode them to your wish.","43ef36a0":"# Conclusion\n\nWe create a gif animation from all the snaps that were taken during the training process and then save them and display them.","6812ecca":"The following Notebook is made to simplify the approach towards GANS.\n\nI tried to keep the notebook as simple as posible. If you want you can also implement different kinds of GANS using the same principles used here.\n\nPlease do **UPVOTE** this if you think that this particular notebook helped you in anyway.","99e393ec":"# Training out Networks, and displaying in real time what is happening."}}