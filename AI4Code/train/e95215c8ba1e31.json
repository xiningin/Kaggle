{"cell_type":{"a7bbbeb6":"code","843bcd77":"code","8b12fc49":"code","753b8c9f":"code","62836694":"code","ed138a79":"code","e674d261":"code","b89b757f":"code","27f8a4e7":"code","0ffbf0f5":"code","333b2452":"code","84de4b19":"code","9d21909e":"code","867887a2":"code","d63d9726":"markdown","7aef42a9":"markdown","9e3a1c4f":"markdown","6b39f92f":"markdown","c034cb84":"markdown","ff9386e0":"markdown","8d5563c5":"markdown","4d9ef821":"markdown","b358c303":"markdown"},"source":{"a7bbbeb6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn import datasets","843bcd77":"# Creating iris dataset. Best way to understand PCA and it's application :)\niris = pd.DataFrame(datasets.load_iris().data,columns=datasets.load_iris().feature_names)","8b12fc49":"iris.head()","753b8c9f":"target = [datasets.load_iris().target_names[i] for i in datasets.load_iris().target]\ntarget","62836694":"# Joining Explanatory variables with target feature.\niris = iris.join(pd.DataFrame(target,columns=['Target']))","ed138a79":"iris.head()","e674d261":"#Importing required libraries\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix","b89b757f":"# Splitig dataset into train and test\nX_train,X_test,y_train,y_test = train_test_split(iris.drop('Target',axis=1),iris.Target,random_state=42,test_size =0.2)","27f8a4e7":"%%time\n#Will use to see the execution time\ndtc = DecisionTreeClassifier(random_state=42)\ndtc.fit(X_train,y_train)\nprint(\"Accuracy Score\",accuracy_score(y_test,dtc.predict(X_test)))","0ffbf0f5":"pca = PCA(n_components=2) # We will reduce our 4 dimensional data into 2 dimensional\npca.fit(X_train)","333b2452":"train_comp = pca.transform(X_train) #Transforming X_train data\ntest_comp = pca.transform(X_test) #Transforming X_test data","84de4b19":"%%time\ndtc = DecisionTreeClassifier(random_state=42)\ndtc.fit(train_comp,y_train)\nprint(\"Accuracy Score\",accuracy_score(y_test,dtc.predict(test_comp)))","9d21909e":"plt.figure(figsize=(12,7))\nsns.scatterplot(x=train_comp[:,0],y=train_comp[:,1],hue=y_train)\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.title(\"Training PCA Data\")\nplt.show()","867887a2":"plt.figure(figsize=(12,7))\nsns.scatterplot(x=test_comp[:,0],y=test_comp[:,1],hue=y_test)\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.title(\"Testing PCA Data\")\nplt.show()","d63d9726":"<h3 style=\"color:gold;\">We can see that we even don't require to built a model after transforming our data using PCA. We can classify our most of the data by a very simple scatter plot.<\/h3>","7aef42a9":"# Now Let's Do PCA On Our Iris Data ","9e3a1c4f":"# So Let's Do A Simple Practical On PCA","6b39f92f":"<h1 style=\"text-align:center;\">Principal Component Analysis(PCA)<\/h1>\n\n<h3>Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.<\/h3>\n\n# Pros of PCA\n\n1. Removes Correlated Features\n\n2. Remove Noise\n\n3. Reduce Computation Time\n\n4. Improve Visualization\n\n5. Reduce Overfitting\n\n# Cons Of PCA\n\n**Biggest loss of PCA is that we can lost some information.**","c034cb84":"<h1 style=\"color:Blue;\">How PCA Helps In Visualizing Data In More Robust Way?<h1>\n\n<h2>Is it Possible to interpret this four dimensional data that how our data is being classified in different different categories? Yes it may be possible but that may become a very complex. But with the help of PCA we can do it a very easily. We will see that how easily we can interpret our 4 dimensional iris data by reducing it into 2 dimension with a very simple scatter plot. Let's see :<\/h2>","ff9386e0":"<h1 style=\"color:red;\">So we can clearly see that exceution time decreased drastically while accuracy remains constant.<\/h1>\n\n<h3>It is one of the property of PCA that it reduce computation time by reducing dimensions while not losing much information!<\/h3>","8d5563c5":"<h1 style=\"border:black;border-width:5px; border-style:solid;text-align:center;color:blue;\">So this is how PCA can be very useful in many aspects in Machine learning as well as in Data Analytics.<\/h1>","4d9ef821":"# Let create a model by taking all 4 features","b358c303":"<h2>If you liked it then please upvote it.<\/h2>\n\n<h2>If you have any suggestion regarding this notebook then please tell me in comment section so that i can make improvement in it.<\/h2>"}}