{"cell_type":{"19ec38ea":"code","ba26fd85":"code","3dbaaf69":"code","f3da5d3b":"code","a59dcdcc":"code","9c1202cf":"code","9ccb78fd":"code","6d28714d":"code","c6b77af4":"code","fab164f8":"code","02dcc659":"code","4e29d91d":"code","3a3cd168":"code","8bc57233":"code","f87d275d":"code","0bfd316d":"code","df21ce31":"code","aaadd5db":"code","6e8cd173":"code","28c93726":"code","363abb16":"code","57862fb8":"code","bc883cb4":"code","6975aa80":"code","06dd54b5":"code","5921806f":"markdown","2d361ab2":"markdown","f4317430":"markdown","bc82a7c9":"markdown","9b1ab309":"markdown","596f8763":"markdown"},"source":{"19ec38ea":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"..\/input\"))\n['train.csv', 'gender_submission.csv', 'test.csv']\n# Step 1 is to import both data sets\ntraining_data = pd.read_csv(\"..\/input\/train.csv\")\ntesting_data = pd.read_csv(\"..\/input\/test.csv\")\n\n# Step two is to create columns which I will add to the respective datasets, in order to know which row came from which dataset when I combine the datasets\ntraining_column = pd.Series([1] * len(training_data))\ntesting_column = pd.Series([0] * len(testing_data))\n\n# Now we append them by creating new columns in the original data. We use the same column name\ntraining_data['is_training_data'] = training_column\ntesting_data['is_training_data'] = testing_column","ba26fd85":"# Now we can merge the datasets while retaining the key to split them later\ncombined_data = training_data.append(testing_data, ignore_index=True, sort=False)\n\n# Encode gender (if == female, True)\ncombined_data['female'] = combined_data.Sex == 'female'\n\n# Split out Title\ntitle = []\nfor i in combined_data['Name']:\n    period = i.find(\".\")\n    comma = i.find(\",\")\n    title_value = i[comma+2:period]\n    title.append(title_value)\ncombined_data['title'] = title\n\n# Replace the title values with an aliased dictionary\ntitle_arr = pd.Series(title)\ntitle_dict = {\n    \"Capt\" : \"Rare\",\n    \"Col\" : \"Rare\",\n    \"Major\" : \"Rare\",\n    \"Jonkheer\" : \"Rare\",\n    \"Don\" : \"Rare\",\n    \"Sir\" : \"Rare\",\n    \"Dr\" : \"Rare\",\n    \"Rev\" : \"Rare\",\n    \"the Countess\" : \"Rare\",\n    \"Dona\" : \"Rare\",\n    \"Mme\" : \"Mrs\",\n    \"Mlle\" : \"Miss\",\n    \"Ms\" : \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Rare\"\n}\ncleaned_title = title_arr.map(title_dict)\ncombined_data['cleaned_title'] = cleaned_title\n\n# Fill NaN of Age - first create groups to find better medians than just the overall median and fill NaN with the grouped medians\ngrouped = combined_data.groupby(['female','Pclass', 'cleaned_title']) \ncombined_data['Age'] = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n#add an age bin\nage_bin_conditions = [\n    combined_data['Age'] == 0,\n    (combined_data['Age'] > 0) & (combined_data['Age'] <= 16),\n    (combined_data['Age'] > 16) & (combined_data['Age'] <= 34),\n    (combined_data['Age'] > 34) & (combined_data['Age'] <= 49),\n    (combined_data['Age'] > 49) & (combined_data['Age'] <= 64),\n    combined_data['Age'] > 64\n]\nage_bin_outputs = [0, 1, 2, 3, 4, 5]\ncombined_data['age_bin'] = np.select(age_bin_conditions, age_bin_outputs, 'Other').astype(int)\n\n# Fill NaN of Embarked\ncombined_data['Embarked'] = combined_data['Embarked'].fillna(\"S\") \n\n# Fill NaN of Fare, adding flag for boarded free, binning other fares\ncombined_data['Fare'] = combined_data['Fare'].fillna(combined_data['Fare'].mode()[0]) \ncombined_data['boarded_free'] = combined_data['Fare'] == 0 \nfare_bin_conditions = [\n    combined_data['Fare'] == 0,\n    (combined_data['Fare'] > 0) & (combined_data['Fare'] <= 7.9),\n    (combined_data['Fare'] > 7.9) & (combined_data['Fare'] <= 14.4),\n    (combined_data['Fare'] > 14.4) & (combined_data['Fare'] <= 31),\n    combined_data['Fare'] > 31\n]\nfare_bin_outputs = [0, 1, 2, 3, 4]\ncombined_data['fare_bin'] = np.select(fare_bin_conditions, fare_bin_outputs, 'Other').astype(int)\n\n# Fill NaN of Cabin with a U for unknown. Not sure cabin will help.\ncombined_data['Cabin'] = combined_data['Cabin'].fillna(\"U\") \n\n# Counting how many people are riding on a ticket\nfrom collections import Counter\ntickets_count = pd.DataFrame([Counter(combined_data['Ticket']).keys(), Counter(combined_data['Ticket']).values()]).T\ntickets_count.rename(columns={0:'Ticket', 1:'ticket_riders'}, inplace=True)\ntickets_count['ticket_riders'] = tickets_count['ticket_riders'].astype(int)\ncombined_data = combined_data.merge(tickets_count, on='Ticket')\n\n# Finding cabin group\ncabin_group = []\nfor i in combined_data['Cabin']:\n    cabin_group.append(i[0])\ncombined_data['cabin_group'] = cabin_group\n\n# Adding a family_size feature as it may have an inverse relationship to either of its parts\ncombined_data['family_size'] = combined_data.Parch + combined_data.SibSp + 1\n\n# Mapping ports to passenger pickup order\nport = {\n    'S' : 1,\n    'C' : 2,\n    'Q' : 3\n}\ncombined_data['pickup_order'] = combined_data['Embarked'].map(port)\n\n# Encode childhood\ncombined_data['child'] = combined_data.Age < 16\n\n# One-Hot Encoding the titles\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cleaned_title'], prefix=\"C_T\")], axis = 1)\n\n# One-Hot Encoding the Pclass\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Pclass'], prefix=\"PClass\")], axis = 1)\n\n# One-Hot Encoding the  cabin group\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cabin_group'], prefix=\"C_G\")], axis = 1)\n\n# One-Hot Encoding the ports\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Embarked'], prefix=\"Embarked\")], axis = 1)","3dbaaf69":"new_train_data=combined_data.loc[combined_data['is_training_data']==1]\nnew_test_data=combined_data.loc[combined_data['is_training_data']==0]\n# here is the expanded model set and metric tools\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\nk_fold = KFold(n_splits = 10, shuffle=True, random_state=0) \nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n# Here are the features\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'child', 'Embarked_S', 'Embarked_C', \n            'Embarked_Q', 'pickup_order', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n            'C_T_Rare', 'C_G_A', 'C_G_B', 'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', \n            'C_G_T', 'C_G_U', 'family_size', 'PClass_1', 'PClass_2', 'PClass_3', 'ticket_riders']\ntarget = 'Survived'\ncvs_train_data = new_train_data[features]\ncvs_test_data = new_test_data[features]\ncvs_target = new_train_data['Survived']\ncvs_train_data.shape","f3da5d3b":"# Define the classifiers I will use\nclassifiers = [\n    RandomForestClassifier(n_estimators=10, random_state=0),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=9, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    DecisionTreeClassifier(random_state=0),\n    LogisticRegression(solver='liblinear'),\n    KNeighborsClassifier(n_neighbors=15),\n    SVC(gamma='auto'),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    XGBClassifier(),\n    GaussianNB(),\n    LinearSVC()]\n\n# Fit and use cross_val_score and k_fold to score accuracy\nclf_scores = [['Score', 'Classifier']]\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(cvs_train_data, cvs_target)\n    prediction = clf.predict(cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1\n    \nprint(clf_scores)","a59dcdcc":"clf_scores2 = clf_scores.copy()\ndf_scores = pd.DataFrame(clf_scores2, columns=clf_scores2.pop(0))\ndf_scores.sort_values('Score', ascending=False)","9c1202cf":"from sklearn.feature_selection import chi2, SelectKBest","9ccb78fd":"fit_list = [SelectKBest(score_func=chi2, k=i) for i in range (0,32)]\n\nfit_test = SelectKBest(score_func=chi2, k='all')\nfit_test.fit(cvs_train_data, cvs_target)\nnp.set_printoptions(precision=3)\n\nfeatures3 = fit_test.transform(cvs_train_data)\nfit_test_scores = pd.DataFrame(fit_test.scores_)\nfit_test_scores['feature_name'] = pd.DataFrame(cvs_train_data.columns)\nfit_test_scores.columns = ['chi_squared', 'feature_name']\nfit_test_scores.sort_values('chi_squared', ascending=False)","6d28714d":"# So we're just going to take the features with a chi_squared over 1. ","c6b77af4":"new_features = ['Fare','female','C_T_Mr','C_T_Mrs','C_T_Miss','PClass_1','PClass_3','C_G_B','C_G_U','Embarked_C','Age','C_G_D','C_G_E','child','C_G_C','Parch','C_T_Master','PClass_2','ticket_riders','Embarked_S','pickup_order','C_G_F','SibSp']","fab164f8":"len(new_features)","02dcc659":"# rf_ stands for reduced features\nrf_cvs_train_data = new_train_data[new_features]\nrf_cvs_test_data = new_test_data[new_features]\nrf_cvs_target = new_train_data['Survived']\nrf_cvs_train_data.shape","4e29d91d":"# Fit and use cross_val_score and k_fold to score accuracy\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, rf_cvs_train_data, rf_cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(rf_cvs_train_data, rf_cvs_target)\n    prediction = clf.predict(rf_cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}_2.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","3a3cd168":"new_features_2 = ['age_bin', 'SibSp', 'Parch', 'fare_bin', 'female', 'child', 'Embarked_S', 'Embarked_C', \n            'Embarked_Q', 'pickup_order', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n            'C_T_Rare', 'family_size', 'PClass_1', 'PClass_2', 'PClass_3', 'ticket_riders']\n\n\n","8bc57233":"#nf stands for new features aka with bins\nnf_cvs_train_data = new_train_data[new_features_2]\nnf_cvs_test_data = new_test_data[new_features_2]\nnf_cvs_target = new_train_data['Survived']\nnf_cvs_train_data.shape, nf_cvs_test_data.shape","f87d275d":"# Fit and use cross_val_score and k_fold to score accuracy\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, nf_cvs_train_data, nf_cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(nf_cvs_train_data, nf_cvs_target)\n    prediction = clf.predict(nf_cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}_3.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","0bfd316d":"combined_data.dtypes.index","df21ce31":"new_features_3 = ['PassengerId', 'Pclass', 'Age', 'SibSp',\n       'Parch', 'Fare', 'female', 'age_bin', 'boarded_free',\n       'fare_bin', 'ticket_riders', 'family_size',\n       'pickup_order', 'child', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n       'C_T_Rare', 'PClass_1', 'PClass_2', 'PClass_3', 'C_G_A', 'C_G_B',\n       'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', 'C_G_T', 'C_G_U',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S']\n#nf stands for new features aka with bins\nnf3_cvs_train_data = new_train_data[new_features_3]\nnf3_cvs_test_data = new_test_data[new_features_3]\nnf3_cvs_target = new_train_data['Survived'].astype(int)\nnf3_cvs_train_data.shape, nf3_cvs_test_data.shape, nf3_cvs_target.shape","aaadd5db":"fit_list = [SelectKBest(score_func=chi2, k=i) for i in range (0,32)]\n\nfit_test = SelectKBest(score_func=chi2, k='all')\nfit_test.fit(nf3_cvs_train_data, nf3_cvs_target)\nnp.set_printoptions(precision=3)\n\nfeatures3 = fit_test.transform(nf3_cvs_train_data)\nfit_test_scores = pd.DataFrame(fit_test.scores_)\nfit_test_scores['feature_name'] = nf3_cvs_train_data.columns\nfit_test_scores.columns = ['chi_squared', 'feature_name']\nfit_test_scores.sort_values('chi_squared', ascending=False)","6e8cd173":"from sklearn.feature_selection import RFE","28c93726":"classifiers2 = [\n    RandomForestClassifier(n_estimators=10, random_state=0),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=9, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    DecisionTreeClassifier(random_state=0),\n    LogisticRegression(solver='liblinear'),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    XGBClassifier(),\n    LinearSVC()]\n","363abb16":"# d = [['clf_name', 'feature_name', 'n_features', 'support', 'ranking']]\n# for clf in classifiers2:\n#     for i in range (1,3):\n#         name = clf.__class__.__name__\n#         print(name, i)\n#         rfe = RFE(clf, i)\n#         fitted_clf = rfe.fit(nf3_cvs_train_data, nf3_cvs_target)\n#         n_feat = fitted_clf.n_features_\n#         n_supp = fitted_clf.support_\n#         rank = fitted_clf.ranking_\n#         d_new = [name, new_features_3, n_feat, n_supp, rank]\n#         d.append(d_new)\n        \n# d_f = pd.DataFrame(d)\n# d_f","57862fb8":"# from yellowbrick.features import RFECV\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Create RFECV visualizer with linear SVM classifier\n# for clf in classifiers2:\n#     viz = RFECV(clf)\n#     viz.fit(nf3_cvs_train_data, nf3_cvs_target);\n#     viz.poof();","bc883cb4":"for clf in classifiers2:\n    for i in (1, 3, 8, 15, 20, 21, 24, 27, 28, 31, 33):\n        name = clf.__class__.__name__\n        print(name, i)\n        rfe = RFE(clf, i)\n        fitted_clf = rfe.fit(nf3_cvs_train_data, nf3_cvs_target)\n        n_feat = fitted_clf.n_features_\n        n_supp = fitted_clf.support_\n        rank = fitted_clf.ranking_\n        d_new = [name, new_features_3, n_feat, n_supp, rank]\n        d.append(d_new)\n\n","6975aa80":"d_f = pd.DataFrame(d)\nd_f.to_csv('csv')","06dd54b5":"# Fit and use cross_val_score and k_fold to score accuracy\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, nf3_cvs_train_data, nf3_cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(nf3_cvs_train_data, nf3_cvs_target)\n    prediction = clf.predict(nf3_cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}_4.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","5921806f":"# Well I am stuck with feature engineering. Let's see if I can get rid of some noise by checking feature selection","2d361ab2":"# Combine and process","f4317430":"# I need to find a better way to determine which features to use. ","bc82a7c9":"## New chi squared","9b1ab309":"# try again with bins","596f8763":"# Load data"}}