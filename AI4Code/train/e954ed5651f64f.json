{"cell_type":{"814da45c":"code","7e0e6eed":"code","4133589a":"code","b9a0806d":"code","8782b108":"code","797d208d":"code","ddd4845b":"code","6cbf4b2b":"code","d4e1d5f6":"code","c9385770":"code","428b85af":"code","6fd0dbb4":"code","aad1c476":"code","033c458f":"code","2731e9b5":"code","56b58f06":"code","44f62814":"code","9987a34a":"code","5bbcc37b":"code","2c41698f":"code","018db594":"code","15f781d6":"code","b6579978":"code","e1dd8cda":"code","34a03002":"code","3dbe88a1":"code","007f7eb0":"code","a5ab4408":"code","ddb3c7a9":"code","608f7d51":"code","112c4a95":"code","8a2b0f35":"code","81bf0188":"code","980b54b5":"code","34549d5d":"code","b58d27b7":"code","aa1f4951":"code","a93002a0":"code","4da4dfc4":"code","0415c3e0":"code","3dc2aa02":"code","c8058c37":"code","316332a4":"code","2fa59bdd":"code","b60b0529":"code","d8effd6e":"code","fc4a0cb9":"code","b313f2ef":"code","c964da16":"code","7d82af0e":"code","0c6befa9":"code","de9a88e2":"code","40d19bc7":"code","792fadb8":"code","ab0e6e80":"code","41226afd":"code","5f32852f":"code","0d8a3e62":"code","14c27a8f":"code","762c89ae":"code","76bb047b":"code","a3fe5b0f":"code","7bff5232":"code","35d59861":"markdown","92c7085c":"markdown","aaf4aef7":"markdown","0a6e9121":"markdown","46040edb":"markdown","10fbd5c6":"markdown","53ff5444":"markdown","7b427b49":"markdown","8169a1fb":"markdown","05cbe2d7":"markdown","5b34fb4a":"markdown","b144af54":"markdown","c3d27278":"markdown","298e94c8":"markdown","fb115b6a":"markdown","29948047":"markdown","51421cb6":"markdown","8cbf3b89":"markdown","3a3a774c":"markdown","355ba254":"markdown","ed66cfef":"markdown","7b9b5616":"markdown","12f4c8aa":"markdown","71a30296":"markdown","c5a94e21":"markdown","1ade2f92":"markdown","a6803fbd":"markdown","b3d9486a":"markdown","c1493448":"markdown","1f87eb44":"markdown","1ce8aeee":"markdown","dc56eba6":"markdown","8842caa0":"markdown"},"source":{"814da45c":"import numpy as np \nimport pandas as pd \nimport os\nimport json\nfrom pandas.io.json import json_normalize\nimport ast\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\n%matplotlib notebook\nfrom scipy.stats import skew, boxcox\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom mpl_toolkits.mplot3d import Axes3D\nimport ast\nimport re\nimport yaml\nimport json\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nimport eli5\nimport time\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings  \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n","7e0e6eed":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\nsam_sub = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nprint( \"train dataset:\", train.shape,\"\\n\",\"test dataset: \",test.shape,\"\\n\",\"sample_submission dataset:\", sam_sub .shape)","4133589a":"# All time winner in the dataset\ntrain.loc[train['revenue'].idxmax(),['title','revenue','release_date']]","b9a0806d":"# Top 20 revenue movie in the dataset\n\ntrain.sort_values(by='revenue', ascending=False).head(20)[['title','revenue','release_date']]","8782b108":"#Let's take a look at the dataset\ntrain.info()","797d208d":"test.info()","ddd4845b":"#Counting NA in dataset\n\nfig = plt.figure(figsize=(15, 10))\ntrain.isna().sum().sort_values(ascending=True).plot(kind='barh',colors='Blue', fontsize=20)\n","6cbf4b2b":"fig = plt.figure(figsize=(15, 10))\ntest.isna().sum().sort_values(ascending=True).plot(kind='barh',colors='Orange', fontsize=20)","d4e1d5f6":"# Revising some wrong information\n\ntrain.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\ntest.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee","c9385770":"## From this function, you can convert release_date column from the character data type to the datetime data type\n\ndef date_features(df):\n    df['release_date'] = pd.to_datetime(df['release_date'])\n    df['release_year'] = df['release_date'].dt.year\n    df['release_month'] = df['release_date'].dt.month\n    df['release_day'] = df['release_date'].dt.day\n    df['release_quarter'] = df['release_date'].dt.quarter\n    df.drop(columns=['release_date'], inplace=True)\n    return df\n\ntrain=date_features(train)\ntest=date_features(test)\n\ntrain['release_year'].head(10)","428b85af":"train['release_year'].iloc[np.where(train['release_year']> 2019)][:10]","6fd0dbb4":"train['release_year']=np.where(train['release_year']> 2019, train['release_year']-100, train['release_year'])\ntest['release_year']=np.where(test['release_year']> 2019, test['release_year']-100, test['release_year'])","aad1c476":"## Filling NA values with mode of each column\n\nfillna_column = {'release_year':'mode','release_month':'mode',\n                'release_day':'mode'}\n\nfor k,v in fillna_column.items():\n    if v == 'mode':\n        fill = train[k].mode()[0]\n    else:\n        fill = v\n    print(k, ': ', fill)\n    train[k].fillna(value = fill, inplace = True)\n    test[k].fillna(value = fill, inplace = True)","033c458f":"# Putting revised year, month, and day together \n\ndef year_month_together(df):\n    year = df[\"release_year\"].astype(int).copy().astype(str)\n    month=df['release_month'].astype(int).copy().astype(str)\n    day=df['release_day'].astype(int).copy().astype(str) \n    df[\"release_date\"]=  month.str.cat(day.str.cat(year,sep=\"\/\"), sep =\"\/\") \n    df['release_date']=pd.to_datetime(df['release_date'],format=\"%m\/%d\/%Y\")\n    df['release_dow'] = df['release_date'].dt.dayofweek\n    return df \n\ntrain=year_month_together(train)\ntest=year_month_together(test)\n\ntrain['release_date'].head(10)","2731e9b5":"# Counting movie number by release date\n\nd1 = train['release_date'].value_counts().sort_index()\nd2 = test['release_date'].value_counts().sort_index()\ndata = [go.Histogram(x=d1.index, y=d1.values, name='train'), go.Histogram(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Counts of release_date\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","56b58f06":"# Average revenue by month\n\nfig = plt.figure(figsize=(10,10))\n\ntrain.groupby('release_month').agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\nplt.ylabel('Revenue (100 million dollars)')","44f62814":"release_year_mean_data=train.groupby(['release_year'])['budget','popularity','revenue'].mean()\nrelease_year_mean_data.head()\n\nfig = plt.figure(figsize=(10, 10))\nrelease_year_mean_data['popularity'].plot(kind='line')\nplt.ylabel('Mean Popularity value')\nplt.title('Mean Popularity Over Years')","9987a34a":"release_year_mean_data=train.groupby(['release_year'])['budget','popularity','revenue'].mean()\nrelease_year_mean_data.head()\n\nfig = plt.figure(figsize=(13,13))\nax = plt.subplot(111,projection = '3d')\n\n# Data for three-dimensional scattered points\nzdata =train.popularity\nxdata =train.budget\nydata = train.revenue\nax.scatter3D(xdata, ydata, zdata, c=zdata, s = 200)\nax.set_xlabel('Budget of the Movie',fontsize=17)\nax.set_ylabel('Revenue of the Movie',fontsize=17)\nax.set_zlabel('Popularity of the Movie',fontsize=17)\n","5bbcc37b":"# Creating correlation matrix \n\ncorr = train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(11, 9))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","2c41698f":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)","018db594":"# Counting NAs as 0\ntrain['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()","15f781d6":"train['collection_name'] = train['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntrain['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntest['collection_name'] = test['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntest['has_collection'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain = train.drop(['belongs_to_collection'], axis=1)\ntest = test.drop(['belongs_to_collection'], axis=1)","b6579978":"# Most common collection \ntrain['collection_name'].value_counts()[1:10]","e1dd8cda":"train['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()\nlist_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nmost_common_genres=Counter([i for j in list_of_genres for i in j]).most_common()\nfig = plt.figure(figsize=(10, 6))\ndata=dict(most_common_genres)\nnames = list(data.keys())\nvalues = list(data.values())\n\nplt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='teal')\nplt.xlabel('Count')\nplt.title('Movie Genre Count')\nplt.show()\n","34a03002":"train['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\nfor g in top_genres:\n    train['genre_' + g] = train['all_genres'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_genres:\n    test['genre_' + g] = test['all_genres'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['genres'], axis=1)\ntest = test.drop(['genres'], axis=1)","3dbe88a1":"# Movie title text analysis \n\ntext = \" \".join(review for review in train.title)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))\n\nstopwords = set(stopwords.words('english'))\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\nfig = plt.figure(figsize=(11, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('movie title in train data')\nplt.axis(\"off\")\nplt.show()","007f7eb0":"# Text analysis on top 4 movie genres title \n\ndrama=train.loc[train['genre_Drama']==1,]\ncomedy=train.loc[train['genre_Comedy']==1,]\naction=train.loc[train['genre_Action']==1,]\nthriller=train.loc[train['genre_Thriller']==1,]\n\n\n\ntext_drama = \" \".join(review for review in drama.title)\ntext_comedy = \" \".join(review for review in comedy.title)\ntext_action = \" \".join(review for review in action.title)\ntext_thriller = \" \".join(review for review in thriller.title)\n\n\nwordcloud1 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Reds\").generate(text_drama)\nwordcloud2 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Blues\").generate(text_comedy)\nwordcloud3 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Greens\").generate(text_action)\nwordcloud4 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Greys\").generate(text_thriller)\n\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.subplot(221)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.title('drama movie title')\nplt.axis(\"off\")\n\nplt.subplot(222)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.title('comedy movie title')\nplt.axis(\"off\")\nplt.show()\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.subplot(223)\nplt.imshow(wordcloud3, interpolation='bilinear')\nplt.title('action movie title')\nplt.axis(\"off\")\n\nplt.subplot(224)\nplt.imshow(wordcloud4, interpolation='bilinear')\nplt.title('thriller movie title')\nplt.axis(\"off\")\nplt.show()\n","a5ab4408":"drama_revenue=drama.groupby(['release_year']).mean()['revenue']\ncomedy_revenue=comedy.groupby(['release_year']).mean()['revenue']\naction_revenue=action_revenue=action.groupby(['release_year']).mean()['revenue']\nthriller_revenue=thriller.groupby(['release_year']).mean()['revenue']\n\nrevenue_concat = pd.concat([drama_revenue,comedy_revenue,action_revenue,thriller_revenue], axis=1)\nrevenue_concat.columns=['drama','comedy','action','thriller']\nrevenue_concat.index=train.groupby(['release_year']).mean().index","ddb3c7a9":"# Mean revenue over years by top 4 genres \n\ndata = [go.Scatter(x=revenue_concat.index, y=revenue_concat.drama, name='drama'), go.Scatter(x=revenue_concat.index, y=revenue_concat.comedy, name='comedy'),\n       go.Scatter(x=revenue_concat.index, y=revenue_concat.action, name='action'),go.Scatter(x=revenue_concat.index, y=revenue_concat.thriller, name='thriller')]\nlayout = go.Layout(dict(title = 'Mean Revenue by Top 4 Movie Genres Over Years',\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Revenue'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","608f7d51":"# Counting the frequency of production company \nlist_of_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nmost_common_companies=Counter([i for j in list_of_companies for i in j]).most_common(20)\n\nfig = plt.figure(figsize=(10, 6))\ndata=dict(most_common_companies)\nnames = list(data.keys())\nvalues = list(data.values())\n\nplt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='brown')\nplt.xlabel('Count')\nplt.title('Top 20 Production Company Count')\nplt.show()\n","112c4a95":"train['num_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_production_companies'] = train['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\nfor g in top_companies:\n    train['production_company_' + g] = train['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_production_companies'] = test['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_companies:\n    test['production_company_' + g] = test['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_companies', 'all_production_companies'], axis=1)\ntest = test.drop(['production_companies', 'all_production_companies'], axis=1)","8a2b0f35":"# Getting the mean revenue of top 8 most common production companies \n\nWarner_Bros=train.loc[train['production_company_Warner Bros.']==1,]\nUniversal_Pictures=train.loc[train['production_company_Universal Pictures']==1,]\nTwentieth_Century_Fox_Film=train.loc[train['production_company_Twentieth Century Fox Film Corporation']==1,]\nColumbia_Pictures=train.loc[train['production_company_Columbia Pictures']==1,]\nMGM=train.loc[train['production_company_Metro-Goldwyn-Mayer (MGM)']==1,]\nNew_Line_Cinema=train.loc[train['production_company_New Line Cinema']==1,]\nTouchstone_Pictures=train.loc[train['production_company_Touchstone Pictures']==1,]\nWalt_Disney=train.loc[train['production_company_Walt Disney Pictures']==1,]\n\nWarner_Bros_revenue=Warner_Bros.groupby(['release_year']).mean()['revenue']\nUniversal_Pictures_revenue=Universal_Pictures.groupby(['release_year']).mean()['revenue']\nTwentieth_Century_Fox_Film_revenue=Twentieth_Century_Fox_Film.groupby(['release_year']).mean()['revenue']\nColumbia_Pictures_revenue=Columbia_Pictures.groupby(['release_year']).mean()['revenue']\nMGM_revenue=MGM.groupby(['release_year']).mean()['revenue']\nNew_Line_Cinema_revenue=New_Line_Cinema.groupby(['release_year']).mean()['revenue']\nTouchstone_Pictures_revenue=Touchstone_Pictures.groupby(['release_year']).mean()['revenue']\nWalt_Disney_revenue=Walt_Disney.groupby(['release_year']).mean()['revenue']\n\n\nprod_revenue_concat = pd.concat([Warner_Bros_revenue,Universal_Pictures_revenue,Twentieth_Century_Fox_Film_revenue,Columbia_Pictures_revenue,\n                                MGM_revenue,New_Line_Cinema_revenue,Touchstone_Pictures_revenue,Walt_Disney_revenue], axis=1)\nprod_revenue_concat.columns=['Warner_Bros','Universal_Pictures','Twentieth_Century_Fox_Film','Columbia_Pictures','MGM','New_Line_Cinema','Touchstone_Pictures','Walt_Disney']\n\nfig = plt.figure(figsize=(13, 7))\nprod_revenue_concat.agg(\"mean\",axis='rows').sort_values(ascending=True).plot(kind='barh',x='Production Companies',y='Revenue',title='Mean Revenue (100 million dollars) of Top 8 Most Common Production Companies')\nplt.xlabel('Revenue (100 million dollars)')","81bf0188":"data = [go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.Warner_Bros, name='Warner_Bros'), go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.Universal_Pictures, name='Universal_Pictures'),\n       go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.Twentieth_Century_Fox_Film, name='Twentieth_Century_Fox_Film'),go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.Columbia_Pictures, name='Columbia_Pictures'),\n       go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.MGM, name='MGM'), go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.New_Line_Cinema, name='New_Line_Cinema'),\n       go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.Touchstone_Pictures, name='Touchstone_Pictures'),go.Scatter(x=prod_revenue_concat.index, y=prod_revenue_concat.Walt_Disney, name='Walt_Disney')]\n\n\nlayout = go.Layout(dict(title = 'Mean Revenue of Top 8 Movie Production Companies over Years',\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Revenue'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","980b54b5":"list_of_countries = list(train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nmost_common_countries=Counter([i for j in list_of_countries for i in j]).most_common(20)\n\nfig = plt.figure(figsize=(10, 6))\ndata=dict(most_common_countries)\nnames = list(data.keys())\nvalues = list(data.values())\n\nplt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='purple')\nplt.xlabel('Count')\nplt.title('Country Count')\nplt.show()\n","34549d5d":"train['num_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_countries'] = train['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]\nfor g in top_countries:\n    train['production_country_' + g] = train['all_countries'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_countries'] = test['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_countries:\n    test['production_country_' + g] = test['all_countries'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_countries', 'all_countries'], axis=1)\ntest = test.drop(['production_countries', 'all_countries'], axis=1)","b58d27b7":"# English is the majority spoken language \nlist_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nmost_common_languages=Counter([i for j in list_of_languages for i in j]).most_common(20)\n\nfig = plt.figure(figsize=(10, 6))\ndata=dict(most_common_languages)\nnames = list(data.keys())\nvalues = list(data.values())\n\nplt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='gray')\nplt.xlabel('Count')\nplt.title('Language Count')\nplt.show()","aa1f4951":"train['num_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_languages'] = train['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\nfor g in top_languages:\n    train['language_' + g] = train['all_languages'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['iso_639_1'] for i in x])) if x != {} else '')\nfor g in top_languages:\n    test['language_' + g] = test['all_languages'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['spoken_languages', 'all_languages'], axis=1)\ntest = test.drop(['spoken_languages', 'all_languages'], axis=1)","a93002a0":"list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n\nmost_common_keywords=Counter([i for j in list_of_keywords for i in j]).most_common(20)\n\nfig = plt.figure(figsize=(10, 6))\ndata=dict(most_common_keywords)\nnames = list(data.keys())\nvalues = list(data.values())\n\nplt.barh(sorted(range(len(data)),reverse=True),values,tick_label=names,color='purple')\nplt.xlabel('Count')\nplt.title('Top 20 Most Common Keyword Count')\nplt.show()\n","4da4dfc4":"# Text analysis on keywords by top 4 genres\n\n\ntext_drama = \" \".join(review for review in drama['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))\ntext_comedy = \" \".join(review for review in comedy['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))\ntext_action = \" \".join(review for review in action['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))\ntext_thriller = \" \".join(review for review in thriller['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else ''))\n\n\nwordcloud1 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Reds\").generate(text_drama)\nwordcloud2 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Blues\").generate(text_comedy)\nwordcloud3 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Greens\").generate(text_action)\nwordcloud4 = WordCloud(stopwords=stopwords, background_color=\"white\",colormap=\"Greys\").generate(text_thriller)\n\n\nfig = plt.figure(figsize=(25, 20))\n\nplt.subplot(221)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.title('Drama Keywords')\nplt.axis(\"off\")\n\nplt.subplot(222)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.title('Comedy Keywords')\nplt.axis(\"off\")\nplt.show()\n\nfig = plt.figure(figsize=(25, 20))\n\nplt.subplot(223)\nplt.imshow(wordcloud3, interpolation='bilinear')\nplt.title('Action Keywords')\nplt.axis(\"off\")\n\nplt.subplot(224)\nplt.imshow(wordcloud4, interpolation='bilinear')\nplt.title('Thriller Keywords')\nplt.axis(\"off\")\nplt.show()","0415c3e0":"train['num_Keywords'] = train['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\nfor g in top_keywords:\n    train['keyword_' + g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_Keywords'] = test['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_keywords:\n    test['keyword_' + g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['Keywords', 'all_Keywords'], axis=1)\ntest = test.drop(['Keywords', 'all_Keywords'], axis=1)","3dc2aa02":"list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_cast'] = train['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(30)]\nfor g in top_cast_names:\n    train['cast_name_' + g] = train['all_cast'].apply(lambda x: 1 if g in x else 0)\n\n    \ntest['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_cast'] = test['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_cast_names:\n    test['cast_name_' + g] = test['all_cast'].apply(lambda x: 1 if g in x else 0)","c8058c37":"#Mean revenue comparison of 10 most common actor\/actress \n\ncast_name_Samuel_L_Jackson=train.loc[train['cast_name_Samuel L. Jackson']==1,]\ncast_name_Robert_De_Niro=train.loc[train['cast_name_Robert De Niro']==1,]\ncast_name_Morgan_Freeman=train.loc[train['cast_name_Morgan Freeman']==1,]\ncast_name_J_K_Simmons=train.loc[train['cast_name_J.K. Simmons']==1,]\ncast_name_Bruce_Willis=train.loc[train['cast_name_Bruce Willis']==1,]\ncast_name_Liam_Neeson=train.loc[train['cast_name_Liam Neeson']==1,]\ncast_name_Susan_Sarandon=train.loc[train['cast_name_Susan Sarandon']==1,]\ncast_name_Bruce_McGill=train.loc[train['cast_name_Bruce McGill']==1,]\ncast_name_John_Turturro=train.loc[train['cast_name_John Turturro']==1,]\ncast_name_Forest_Whitaker=train.loc[train['cast_name_Forest Whitaker']==1,]\n\n\ncast_name_Samuel_L_Jackson_revenue=cast_name_Samuel_L_Jackson.mean()['revenue']\ncast_name_Robert_De_Niro_revenue=cast_name_Robert_De_Niro.mean()['revenue']\ncast_name_Morgan_Freeman_revenue=cast_name_Morgan_Freeman.mean()['revenue']\ncast_name_J_K_Simmons_revenue=cast_name_J_K_Simmons.mean()['revenue']\ncast_name_Bruce_Willis_revenue=cast_name_Bruce_Willis.mean()['revenue']\ncast_name_Liam_Neeson_revenue=cast_name_Liam_Neeson.mean()['revenue']\ncast_name_Susan_Sarandon_revenue=cast_name_Susan_Sarandon.mean()['revenue']\ncast_name_Bruce_McGill_revenue=cast_name_Bruce_McGill.mean()['revenue']\ncast_name_John_Turturro_revenue=cast_name_John_Turturro.mean()['revenue']\ncast_name_Forest_Whitaker_revenue=cast_name_Forest_Whitaker.mean()['revenue']\n\n\ncast_revenue_concat = pd.Series([cast_name_Samuel_L_Jackson_revenue,cast_name_Robert_De_Niro_revenue,cast_name_Morgan_Freeman_revenue,cast_name_J_K_Simmons_revenue,\n                                cast_name_Bruce_Willis_revenue,cast_name_Liam_Neeson_revenue,cast_name_Susan_Sarandon_revenue,cast_name_Bruce_McGill_revenue,\n                                cast_name_John_Turturro_revenue,cast_name_Forest_Whitaker_revenue])\ncast_revenue_concat.index=['Samuel L. Jackson','Robert De Niro','Morgan Freeman','J.K. Simmons','Bruce Willis','Liam Neeson','Susan Sarandon','Bruce McGill',\n                            'John Turturro','Forest Whitaker']\n\n\nfig = plt.figure(figsize=(13, 7))\ncast_revenue_concat.sort_values(ascending=True).plot(kind='barh',title='Mean Revenue (100 million dollars) by Top 10 Most Common Cast')\nplt.xlabel('Revenue (100 million dollars)')","316332a4":"# Consider other factors like gender and characters \n\nlist_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nlist_of_cast_characters = list(train['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)\n\n\ntrain['genders_0'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['cast_character_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \n\ntest['genders_0'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['cast_character_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\n\ntrain = train.drop(['cast'], axis=1)\ntest = test.drop(['cast'], axis=1)","2fa59bdd":"list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_crew'] = train['crew'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(30)]\nfor g in top_crew_names:\n    train['crew_name_' + g] = train['all_crew'].apply(lambda x: 1 if g in x else 0)\n\ntest['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_crew'] = test['crew'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_crew_names:\n    test['crew_name_' + g] = test['all_crew'].apply(lambda x: 1 if g in x else 0)","b60b0529":"crew_name_Avy_Kaufman=train.loc[train['crew_name_Avy Kaufman']==1,]\ncrew_name_Robert_Rodriguez=train.loc[train['crew_name_Robert Rodriguez']==1,]\ncrew_name_Deborah_Aquila=train.loc[train['crew_name_Deborah Aquila']==1,]\ncrew_name_James_Newton_Howard=train.loc[train['crew_name_James Newton Howard']==1,]\ncrew_name_Mary_Vernieu=train.loc[train['crew_name_Mary Vernieu']==1,]\ncrew_name_Steven_Spielberg=train.loc[train['crew_name_Steven Spielberg']==1,]\ncrew_name_Luc_Besson=train.loc[train['crew_name_Luc Besson']==1,]\ncrew_name_Jerry_Goldsmith=train.loc[train['crew_name_Jerry Goldsmith']==1,]\ncrew_name_Francine_Maisler=train.loc[train['crew_name_Francine Maisler']==1,]\ncrew_name_Tricia_Wood=train.loc[train['crew_name_Tricia Wood']==1,]\n\n\ncrew_name_Avy_Kaufman_revenue=crew_name_Avy_Kaufman.mean()['revenue']\ncrew_name_Robert_Rodriguez_revenue=crew_name_Robert_Rodriguez.mean()['revenue']\ncrew_name_Deborah_Aquila_revenue=crew_name_Deborah_Aquila.mean()['revenue']\ncrew_name_James_Newton_Howard_revenue=crew_name_James_Newton_Howard.mean()['revenue']\ncrew_name_Mary_Vernieu_revenue=crew_name_Mary_Vernieu.mean()['revenue']\ncrew_name_Steven_Spielberg_revenue=crew_name_Steven_Spielberg.mean()['revenue']\ncrew_name_Luc_Besson_revenue=crew_name_Luc_Besson.mean()['revenue']\ncrew_name_Jerry_Goldsmith_revenue=crew_name_Jerry_Goldsmith.mean()['revenue']\ncrew_name_Francine_Maisler_revenue=crew_name_Francine_Maisler.mean()['revenue']\ncrew_name_Tricia_Wood_revenue=crew_name_Tricia_Wood.mean()['revenue']\n\n\ncrew_revenue_concat = pd.Series([crew_name_Avy_Kaufman_revenue,crew_name_Robert_Rodriguez_revenue,crew_name_Deborah_Aquila_revenue,crew_name_James_Newton_Howard_revenue,\n                                crew_name_Mary_Vernieu_revenue,crew_name_Steven_Spielberg_revenue,crew_name_Luc_Besson_revenue,crew_name_Jerry_Goldsmith_revenue,\n                                crew_name_Francine_Maisler_revenue,crew_name_Tricia_Wood_revenue])\ncrew_revenue_concat.index=['Avy Kaufman','Robert Rodriguez','Deborah Aquila','James Newton Howard','Mary Vernieu','Steven Spielberg','Luc Besson','Jerry Goldsmith',\n                            'Francine Maisler','Tricia Wood']\n\n\nfig = plt.figure(figsize=(13, 7))\ncrew_revenue_concat.sort_values(ascending=True).plot(kind='barh',title='Mean Revenue (100 million dollars) by Top 10 Most Common Crew')\nplt.xlabel('Revenue (100 million dollars)')","d8effd6e":"# Consider other factors like crew jobs, gender, and department \n\nlist_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nlist_of_crew_genders = list(train['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nlist_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\n\n\ntrain['genders_0'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['crew_character_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntop_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    train['jobs_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    train['departments_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n    \n\n    \ntest['genders_0'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['crew_character_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\nfor j in top_crew_jobs:\n    test['jobs_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\nfor j in top_crew_departments:\n    test['departments_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n\ntrain = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)","fc4a0cb9":"# Plot the distribution of the revenue\n\nfig = plt.figure(figsize=(30, 25))\n\nplt.subplot(221)\ntrain['revenue'].plot(kind='hist',bins=100)\nplt.title('Distribution of Revenue')\nplt.xlabel('Revenue')\n\nplt.subplot(222)\nnp.log1p(train['revenue']).plot(kind='hist',bins=100)\nplt.title('Train Log Revenue Distribution')\nplt.xlabel('Log Revenue')\n\n\nprint('Skew of revenue attribute: %0.1f' % skew(train['revenue']))","b313f2ef":"# Adjusting other skewed variables such as popularity and budget\n\nprint('Skew of train budget attribute: %0.1f' % skew(train['budget']))\nprint('Skew of test budget attribute: %0.1f' % skew(test['budget']))\nprint('Skew of train popularity attribute: %0.1f' % skew(train['popularity']))\nprint('Skew of test popularity attribute: %0.1f' % skew(test['popularity']))","c964da16":"# Before log transformation and after log transformation for train budget and train popularity \nfig = plt.figure(figsize=(30, 25))\n\nplt.subplot(221)\ntrain['budget'].plot(kind='hist',bins=100)\nplt.title('Train Budget Distribution')\nplt.xlabel('Budget')\n\nplt.subplot(222)\nnp.log1p(train['budget']).plot(kind='hist',bins=100)\nplt.title('Train Log Budget Distribution')\nplt.xlabel('Log Budget')\n\nplt.show()\n\nfig = plt.figure(figsize=(30, 25))\n\nplt.subplot(223)\ntest['budget'].plot(kind='hist',bins=100)\nplt.title('Train Popularity Distribution')\nplt.xlabel('Popularity')\n\nplt.subplot(224)\nnp.log1p(test['budget']).plot(kind='hist',bins=100)\nplt.title('Train Log Popularity Distribution')\nplt.xlabel('Log Popularity')\nplt.show()\n","7d82af0e":"# Revising budget variable \n\npower_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","0c6befa9":"# Putting log variables for skewed data \ntrain['log_budget']=np.log1p(train['budget'])\ntest['log_budget']=np.log1p(test['budget'])\ntrain['log_popularity']=np.log1p(train['popularity'])\ntest['log_popularity']=np.log1p(test['popularity'])","de9a88e2":"def prepare(df):\n    df['_budget_runtime_ratio'] = df['budget']\/df['runtime'] \n    df['_budget_popularity_ratio'] = df['budget']\/df['popularity']\n    df['_budget_year_ratio'] = df['budget']\/(df['release_year']*df['release_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_year']\/df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity']\/df['release_year']\n    df['_year_to_log_budget'] = df['release_year'] \/ df['log_budget']\n    df['_year_to_log_popularity'] = df['release_year'] \/ df['log_popularity']\n\n    df['has_homepage'] = 0\n    df.loc[pd.isnull(df['homepage']) ,\"has_homepage\"] = 1\n    \n    df['isTaglineNA'] = 0\n    df.loc[df['tagline'] == 0 ,\"isTaglineNA\"] = 1 \n    \n    df['isTitleDifferent'] = 1\n    df.loc[ df['original_title'] == df['title'] ,\"isTitleDifferent\"] = 0 \n\n    df['isMovieReleased'] = 1\n    df.loc[ df['status'] != \"Released\" ,\"isMovieReleased\"] = 0 \n\n    df['original_title_letter_count'] = df['original_title'].str.len() \n    df['original_title_word_count'] = df['original_title'].str.split().str.len() \n    df['title_word_count'] = df['title'].str.split().str.len()\n    df['overview_word_count'] = df['overview'].str.split().str.len()\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n    df['meanruntimeByYear'] = df.groupby(\"release_year\")[\"runtime\"].aggregate('mean')\n    df['meanPopularityByYear'] = df.groupby(\"release_year\")[\"popularity\"].aggregate('mean')\n    df['meanBudgetByYear'] = df.groupby(\"release_year\")[\"budget\"].aggregate('mean')\n\n    return df\n\ntrain_new=prepare(train)\ntest_new=prepare(test)\n","40d19bc7":"train_new.to_csv(\"train_new.csv\", index=False)\ntest_new.to_csv(\"test_new.csv\", index=False)","792fadb8":"drop_columns=['homepage','imdb_id','poster_path','status','title', 'release_date','tagline', 'overview', 'original_title','all_genres','all_cast',\n             'original_language','collection_name','all_crew']\ntrain_new=train_new.drop(drop_columns,axis=1)\ntest_new=test_new.drop(drop_columns,axis=1)","ab0e6e80":"print( \"updated train dataset:\", train_new.shape,\"\\n\",\"updated test dataset: \",test_new.shape)\n\n# Just double checking the difference of variables between train and test \nprint(train_new.columns.difference(test_new.columns)) # good to go! ","41226afd":"# Formating for modeling\n\nX = train_new.drop(['id', 'revenue'], axis=1)\ny = np.log1p(train_new['revenue'])\nX_test = test_new.drop(['id'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","5f32852f":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\n\nlgb_model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\nlgb_model.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)\n\neli5.show_weights(lgb_model, feature_filter=lambda x: x != '<BIAS>')","0d8a3e62":"\"\"\"\nTaking too much time! (I also put too many combinations)\n\n# Print the best parameters found\ngridParams = {\n    \"max_depth\": [5,6,7,8],\n    \"min_data_in_leaf\": [15,20,25,30],\n    'learning_rate': [0.01,0.005],\n    'num_leaves': [15,20,25,30,35,40],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['regression'],\n    'random_state' : [501], # Updated from 'seed'\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4]\n    }\n\ngrid_search = GridSearchCV(lgb_model, n_jobs=-1, param_grid=gridParams, cv = 3, verbose=5)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n\nRandom_Search_Params ={\n    \"max_depth\": [4,5,6],\n    \"min_data_in_leaf\": [15,20,25],\n    'learning_rate': [0.01,0.005],\n    'num_leaves': [25,30,35,40],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['regression'],\n    'random_state' : [501] # Updated from 'seed'\n    }\n\nn_HP_points_to_test = 50\n\nrandom_search = RandomizedSearchCV(\n    estimator=lgb_model, param_distributions= Random_Search_Params, \n    n_iter=n_HP_points_to_test,\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)\n\nrandom_search.fit(X_train, y_train)\nprint('Best score reached: {} with params: {} '.format(random_search.best_score_, random_search.best_params_))\n\n# Using parameters already set above, replace in the best from the random search\n\nparams['learning_rate'] = random_search.best_params_['learning_rate']\nparams['max_depth'] = random_search.best_params_['max_depth']\nparams['num_leaves'] = random_search.best_params_['num_leaves']\nparams['reg_alpha'] = random_search.best_params_['reg_alpha']\nparams['reg_lambda'] = random_search.best_params_['reg_lambda']\n\n\n\"\"\"","14c27a8f":"# Obtain from Random Search \n\nopt_parameters = {'random_state': 501, 'objective': 'regression', 'num_leaves': 40, 'min_data_in_leaf': 15, 'max_depth': 4, 'learning_rate': 0.01, 'boosting_type': 'gbdt'} \n\nparams['learning_rate'] = opt_parameters['learning_rate']\nparams['max_depth'] = opt_parameters['max_depth']\nparams['num_leaves'] = opt_parameters['num_leaves']\nparams['min_data_in_leaf'] = opt_parameters['min_data_in_leaf']","762c89ae":"n_fold = 5\nrandom_seed=2222\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=True, model=None):\n\n    oof = np.zeros(X.shape[0])\n    prediction = np.zeros(X_test.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn':\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=500, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=10000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=10000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","76bb047b":"start = time.time()\noof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb')\nend = time.time()\n\nprint(\"time elapsed:\",end - start, \"second\")","a3fe5b0f":"xgb_params = {'eta': 0.01,\n              'objective': 'reg:linear',\n              'max_depth': 6,\n              'min_child_weight': 3,\n              'subsample': 0.8,\n              'colsample_bytree': 0.8,\n              'eval_metric': 'rmse',\n              'seed': 11,\n              'silent': True}\n\nstart = time.time()\noof_xgb, prediction_xgb = train_model(X, X_test, y, params=xgb_params, model_type='xgb')\nend = time.time()\nprint(\"time elapsed:\",end - start, \"second\")","7bff5232":"sam_sub['revenue'] = np.expm1(prediction_lgb)\nsam_sub.to_csv(\"lgb.csv\", index=False)\nsam_sub['revenue'] = np.expm1(prediction_xgb)\nsam_sub.to_csv(\"xgb.csv\", index=False)\nsam_sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) \/ 2)\nsam_sub.to_csv(\"blend_lgb_xgb.csv\", index=False)","35d59861":"## **Top 5 Revenue Movies in the Dataset**\n\n\n* \n<table><tr>\n<td> <img src=\"https:\/\/is2-ssl.mzstatic.com\/image\/thumb\/Video118\/v4\/04\/6a\/b4\/046ab45e-6099-3e1a-ccef-7b3f8b07f057\/contsched.sumsaanu.lsr\/268x0w.jpg\" alt=\"Drawing\" style=\"width: 300px;\"\/> <\/td>\n<td> <img src=\"https:\/\/images-na.ssl-images-amazon.com\/images\/I\/81ClciXon2L._SY445_.jpg\"  alt=\"Drawing\" style=\"width: 300px;\"\/> <\/td>\n<td> <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/f\/ff\/Avengers_Age_of_Ultron_poster.jpg\"  alt=\"Drawing\" style=\"width: 300px;\"\/> <\/td>\n<td> <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/d\/d6\/Beauty_and_the_Beast_2017_poster.jpg\"  alt=\"Drawing\" style=\"width: 300px;\"\/> <\/td>\n<td> <img src=\"https:\/\/images-na.ssl-images-amazon.com\/images\/I\/51BG6ovCJpL.jpg\"  alt=\"Drawing\" style=\"width: 300px;\"\/> <\/td>\n<\/tr><\/table>","92c7085c":"![](http:\/\/www.squareeyed.tv\/wp-content\/uploads\/2015\/12\/Steven-Spielberg-3.jpg) <br>\n<center> Steven Spieberg is getting roughly 310 million in USD per movie released...\ud83d\ude32 <\/center>","aaf4aef7":"# **Just for fun: top revenue movies**","0a6e9121":"![](https:\/\/cdn3.movieweb.com\/i\/article\/NfXngO3xqCPpg6RXrqozbo6CgJbcRI\/798:50\/Avengers-4-Casting-Call-Nick-Fury-Samuel-L.jpg) <br>\n<center>Nick is getting roughly 290 million in USD per movie released...\ud83d\ude32 <\/center>","46040edb":"<center>While waiting for Random Search to do the job, watching Panda is a very productive thing to do...! Gotta love pandas \u2764\ufe0f\ufe0f\ud83d\udc3c<\/center> <br><br>\n    \n    \n<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/1v6M41Divso?start=21\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>","10fbd5c6":"## **Cast**","53ff5444":"It is interesting to see the overlap of tiltles between Drama and Comedy and between Action and Thriller. ","7b427b49":"From this interactive plot, we can tell that the majority of movies in the dataset released from 00s - 10s.<br>I am wondering the **sample bias issue** in the dataset. Since there are not many movies released from 90s, it is hard to compare movies from 90s and 00s-10s. \n","8169a1fb":"# **Loading the dataset and libraries**","05cbe2d7":"# **Let the Party Begin!**","5b34fb4a":"## **Spoken Language**","b144af54":"<img src=\"https:\/\/media.giphy.com\/media\/26ufm5ivPHbIV81AQ\/giphy.gif \"  height=\"700px\" width=\"700px\">","c3d27278":"# **Data Pre-processing & EDA**","298e94c8":"We can also use 3D plot to invetigate the relationship between budget, revenue, and popularity of movies. <br>\n  From the 3D plot and the corrplot, we can see that they have somewhat strong positive correlations to each other.","fb115b6a":"## **Keywords**","29948047":"![](https:\/\/www.thewrap.com\/wp-content\/uploads\/2014\/12\/walt_disney_company_logo.jpg) <br>\n<center>This Mickey is getting roughly 280 million in USD per movie released...\ud83d\ude32 <strike>(time to get some shirts?)<\/strike> <\/center>","51421cb6":"There are some records that release above 2019 (coming from the future?!).<br> For these records, it probably makes sense to change the century (ex. 2065 --> 1965, we are not ready to accept you, 2065!!) ","8cbf3b89":"The popularity over years plot implies that the mean popularity of movies is increasing over year. Year 2017 has a high mean popularity in the train dataset","3a3a774c":"**ID **- Integer unique id of each movie\n\n**Belongs_to_collection** - Contains the TMDB Id, Name, Movie Poster and Backdrop URL of a movie in JSON format. You can see the Poster and Backdrop Image like this: https:\/\/image.tmdb.org\/t\/p\/original\/. Example: https:\/\/image.tmdb.org\/t\/p\/original\/\/iEhb00TGPucF0b4joM1ieyY026U.jpg\n\n**Budget**:Budget of a movie in dollars. 0 values mean unknown.\n\n**Genres** : Contains all the Genres Name & TMDB Id in JSON Format\n\n**Homepage** - Contains the official homepage URL of a movie. Example: http:\/\/sonyclassics.com\/whiplash\/ , this is the homepage of Whiplash movie.\n\n**Imdb_id** - IMDB id of a movie (string). You can visit the IMDB Page like this: https:\/\/www.imdb.com\/title\/\n\n**Original_language** - Two digit code of the original language, in which the movie was made. Like: en = English, fr = french.\n\n**Original_title** - The original title of a movie. Title & Original title may differ, if the original title is not in English.\n\n**Overview** - Brief description of the movie.\n\n**Popularity** - Popularity of the movie in float.\n\n**Poster_path** - Poster path of a movie. You can see the full image like this: https:\/\/image.tmdb.org\/t\/p\/original\/\n\n**Production_companies** - All production company name and TMDB id in JSON format of a movie.\n\n**Production_countries** - Two digit code and full name of the production company in JSON format.\n\n**Release_date** - Release date of a movie in mm\/dd\/yy format.\n\n**Runtime** - Total runtime of a movie in minutes (Integer).\n\n**Spoken_languages** - Two digit code and full name of the spoken language.\n\n**Status** - Is the movie released or rumored?\n\n**Tagline** - Tagline of a movie\n\n**Title** - English title of a movie\n\n**Keywords** - TMDB Id and name of all the keywords in JSON format.\n\n**Cast** - All cast TMDB id, name, character name, gender (1 = Female, 2 = Male) in JSON format\n\n**Crew** - Name, TMDB id, profile path of various kind of crew members job like Director, Writer, Art, Sound etc.\n\n**Revenue** - Total revenue earned by a movie in dollars.","355ba254":"## **Json Format Columns to Dictionary Format**\n<br>\n\nIn order to clean up the Json format columns, I am going to convert these columns to the dictionary format and analyze each column.","ed66cfef":"## **Attribute Explanation**","7b9b5616":"## **Crew**","12f4c8aa":"## **Random Search vs. Grid Search**\n<br>\nWhile waiting for Random Search doing the job, I will write down the difference between Grid Search and Random Search. <br>Both of them are commonly used hyperparameter optimization tools. However, I prefer to use Random Search, and here is why....<br>First of all, Grid Search takes so much time! I was actually waiting for Grid Search to be done for like 1 hour and found out that I could also use Random Search. <br><br>\n<center><div style=\"width: 300px; font-size:80%; text-align:center;\"><img src=\"https:\/\/media.giphy.com\/media\/RKS1pHGiUUZ2g\/giphy.gif \" ,> Me waiting for Grid Search to be done<\/div><\/center><br>\n**Grid Search** tries every combination of a list of values of the hyperparameters that we set and evaluates the model for each combination, while **Random Search** takes random combinations of the hyperparameters to find the best combination for building a model. The chances of finding the optimal parameter are comparatively higher in random search because Random Search end up being trained on the optimized parameters without any aliasing ([Random Search for Hyper-Parameter Optimization paper](http:\/\/www.jmlr.org\/papers\/v13\/bergstra12a.html)). If you want to know more about their difference, [Deepak's medium article](https:\/\/medium.com\/@senapati.dipak97\/grid-search-vs-random-search-d34c92946318) is very helpful.","71a30296":"The below information is from [B H's Kernel](https:\/\/www.kaggle.com\/zero92\/stacking-xgb-lgbm-cat-new-variables). His kernel has a nice analysis, so take a look at his kernel as well! ","c5a94e21":"# **Feature Engineering** <br><br>\nThe distribution of revenue is quite skewed. I am going to use log transformation on revenue to deal with skewed data.","1ade2f92":"## **Production Companies**","a6803fbd":"## **Genres**\n<br>\nMany genres of movies are drama and comedy in the train data. I am going to conduct text analysis on movie titles by genre and revenue comparison over years by genres.","b3d9486a":"Currently,  Action is doing pretty well compared to other genres (considering top 20 revenue movies, it makes sense). It is also interesting to see the peak revenue of Thriller during 1970s (It would have been terrible years for me..)","c1493448":"## **Belong_to collection**","1f87eb44":"## **Work in Progress...** <br><br>\n\nI will keep working on building more models until I get a better score! <br>Thank you for reading my kernel and don't forget to vote if you enjoyed my kernel \ud83d\ude0a<br><br>\n    <center><strong>Ending the kernel with the best decision-making model in this century....<\/strong> <\/center><br>\n<img src=\"https:\/\/media.giphy.com\/media\/wPygVtYTyXkfC\/giphy.gif \"  height=\"400px\" width=\"400px\">\n\n","1ce8aeee":"## **Production Countries**","dc56eba6":"![](https:\/\/pbs.twimg.com\/profile_images\/789117657714831361\/zGfknUu8_400x400.jpg)","8842caa0":"#  **Welcome :)** <br><br>\n\nBefore starting the analysis, I would like to welcome you coming to my kernel! I recently got into this amazing Kaggle world \u2764\ufe0f\ufe0f <br>I have been enjoying this learning journey and building a great network with Data Enthusiasts like you!<br>  In this Kaggle tutorial, I will go over how to approach and build Gradient Boosting models with the help of exploratory data analysis (EDA) and feature engineering on [The Movie Database](https:\/\/www.themoviedb.org\/). I will try to be thorough as much as possible! Hope that you will enjoy a little festival here and stay warm.....! (especially for eople living in the Midwestern region in the US)\n\n<br>\n<img src=\"https:\/\/media.giphy.com\/media\/piJE8Q4AVfs4vsABAF\/giphy.gif \"  height=\"300px\" width=\"300px\">\n\n<br>\n## **The Goal of the Competition:**<br>\n\nUsing metadata on over 7,000 past films from The Movie Database, I will predict their overall worldwide box office revenue.\n\n## **Simplified Version of How to Start with Gradient Boosting Models:** <br>\n\n1. Perform an Exploratory Data Analysis (EDA) on the dataset <br>\n2. Feature engineering: take the features that you already have and combine them or extract more information <br>\n3. Build a quick and dirty model, or a baseline model, which can serve as a comparison against later models that you will build <br>\n4. Monitor performance and early stopping for avoiding model fitting issues. Also take a look at feature importance of variables and check whether the variables that you are seeing make sense <br>\n5. Tune parameters of models (I usually focus on tuning parameters for Tree Booster, such as eta, max_depth, min_child_weight). <br>\n6. Compare different models by metric (ex. RMSE) <br>\n7. Get a model that performs better! \n\n\n\n## **Special Thanks to:** <br>\n\n\n[Andrew Lukyanenko's kernel](https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-model-interpretation), [Kamal Chhirang's kernel](https:\/\/www.kaggle.com\/kamalchhirang\/eda-simple-feature-engineering-external-data\/notebook), and  [Shubhammank's kernel](https:\/\/www.kaggle.com\/shubhammank\/tmdb-eda). I learned a lot from their kernels, and I highly recommend you to take a look at them! (one of reaons that Kaggle is so awesome)\n\n\n"}}