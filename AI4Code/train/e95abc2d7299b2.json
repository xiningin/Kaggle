{"cell_type":{"f94c8f81":"code","6003b875":"code","dcd2e462":"code","4f317766":"code","9adf9cf9":"code","de0fcd17":"code","e0f4c650":"code","1576fc91":"code","98b9aa82":"code","9d679542":"code","ed935096":"code","7f1930af":"code","249409ed":"code","20a6bf72":"code","021c7ee4":"markdown","e2fb58ed":"markdown","2757b620":"markdown","69121fa9":"markdown","7e7b6bcf":"markdown","1a358441":"markdown","b28a5494":"markdown"},"source":{"f94c8f81":"# useful\nimport random, os, math, sys, gc, datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm.notebook import tqdm\nfrom typing import Union, Optional\nfrom time import time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# neural nets\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow_addons as tfa\nimport tensorflow_probability as tfp\n\n# custom\nimport riiideducation","6003b875":"# PIVOT DATAFRAMES\npiv1 = pd.read_csv(\"..\/input\/riiid-fixed-infos\/content.csv\")\npiv2 = pd.read_csv(\"..\/input\/riiid-fixed-infos\/task.csv\")\npiv3 = pd.read_csv(\"..\/input\/riiid-fixed-infos\/user.csv\")\n\nfor col, df in zip([\"content_sum\", \"task_container_sum\", \"user_sum\"], [piv1, piv2, piv3]):\n    df[col] = (df[col] - df[col].min()) \/ (df[col].max() - df[col].min())\n#\nm1 = piv1[\"content_sum\"].median()\nm2 = piv2[\"task_container_sum\"].median()\nm3 = piv3[\"user_sum\"].median()\n\n\n# OTHER CONSTABTS\nTARGET = \"answered_correctly\"\nTIME_MEAN = 21000.0\nTIME_MIN = 0.0\nTIME_MAX = 300000.0\nmap_prior = {True:1, False:0}","dcd2e462":"def preprocess(df):\n    df = df.merge(piv1, how=\"left\", on=\"content_id\")\n    df[\"content_emb\"] = df[\"content_emb\"].fillna(0.5)\n    df[\"content_sum\"] = df[\"content_sum\"].fillna(m1)\n    df = df.merge(piv2, how=\"left\", on=\"task_container_id\")\n    df[\"task_container_emb\"] = df[\"task_container_emb\"].fillna(0.5)\n    df[\"task_container_sum\"] = df[\"task_container_sum\"].fillna(m2)\n    df = df.merge(piv3, how=\"left\", on=\"user_id\")\n    df[\"user_emb\"] = df[\"user_emb\"].fillna(0.5)\n    df[\"user_sum\"] = df[\"user_sum\"].fillna(m3)\n    df[\"prior_question_elapsed_time\"] = df[\"prior_question_elapsed_time\"].fillna(TIME_MEAN)\n    df[\"duration\"] = (df[\"prior_question_elapsed_time\"] - TIME_MIN) \/ (TIME_MAX - TIME_MIN)\n    df[\"prior_answer\"] = df[\"prior_question_had_explanation\"].map(map_prior)\n    df[\"prior_answer\"] = df[\"prior_answer\"].fillna(0.5)\n    #df = df.fillna(-1)\n    epsilon = 1e-6\n    df[\"score\"] = 2*df[\"content_emb\"]*df[\"user_emb\"] \/ (df[\"content_emb\"]+ df[\"user_emb\"] + epsilon)\n    return df\n#=========","4f317766":"%%time\ntr = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\", \n                 low_memory=False, nrows=10**7)","9adf9cf9":"%%time\ntr = preprocess(tr)","de0fcd17":"FE = [\"content_emb\",\"content_sum\" ,\"task_container_emb\", \"task_container_sum\",\n      \"user_emb\", \"user_sum\",\"duration\", \"prior_answer\",\"score\"]","e0f4c650":"x = tr.loc[tr.answered_correctly!=-1, FE].values\ny = tr.loc[tr.answered_correctly!=-1, TARGET].values","1576fc91":"@tf.function\ndef sparsemoid(inputs: tf.Tensor):\n    return tf.clip_by_value(0.5 * inputs + 0.5, 0., 1.)\n\n@tf.function\ndef identity(x: tf.Tensor):\n    return x\n\nclass ODST(tf.keras.layers.Layer):\n    def __init__(self, n_trees: int = 3, depth: int = 4, units: int = 1, threshold_init_beta: float = 1., **kwargs):\n        super(ODST, self).__init__()\n        self.initialized = False\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n    \n    def build(self, input_shape: tf.TensorShape):\n        feature_selection_logits_init = tf.zeros_initializer()\n        self.feature_selection_logits = tf.Variable(initial_value=feature_selection_logits_init(shape=(input_shape[-1], self.n_trees, self.depth), dtype='float32'),\n                                 trainable=True)        \n        \n        feature_thresholds_init = tf.zeros_initializer()\n        self.feature_thresholds = tf.Variable(initial_value=feature_thresholds_init(shape=(self.n_trees, self.depth), dtype='float32'),\n                                 trainable=True)\n        \n        log_temperatures_init = tf.ones_initializer()\n        self.log_temperatures = tf.Variable(initial_value=log_temperatures_init(shape=(self.n_trees, self.depth), dtype='float32'),\n                                 trainable=True)\n        \n        indices = tf.keras.backend.arange(0, 2 ** self.depth, 1)\n        offsets = 2 ** tf.keras.backend.arange(0, self.depth, 1)\n        bin_codes = (tf.reshape(indices, (1, -1)) \/\/ tf.reshape(offsets, (-1, 1)) % 2)\n        bin_codes_1hot = tf.stack([bin_codes, 1 - bin_codes], axis=-1)\n        self.bin_codes_1hot = tf.Variable(initial_value=tf.cast(bin_codes_1hot, 'float32'), \n                                          trainable=False)\n        \n        response_init = tf.ones_initializer()\n        self.response = tf.Variable(initial_value=response_init(shape=(self.n_trees, self.units, 2**self.depth), dtype='float32'), \n                                    trainable=True)\n                \n    def initialize(self, inputs):        \n        feature_values = self.feature_values(inputs)\n        \n        # intialize feature_thresholds\n        percentiles_q = (100 * tfp.distributions.Beta(self.threshold_init_beta, \n                                                      self.threshold_init_beta)\n                         .sample([self.n_trees * self.depth]))\n        flattened_feature_values = tf.map_fn(tf.keras.backend.flatten, feature_values)\n        init_feature_thresholds = tf.linalg.diag_part(tfp.stats.percentile(flattened_feature_values, percentiles_q, axis=0))\n        \n        self.feature_thresholds.assign(tf.reshape(init_feature_thresholds, self.feature_thresholds.shape))\n        \n        \n        # intialize log_temperatures\n        self.log_temperatures.assign(tfp.stats.percentile(tf.math.abs(feature_values - self.feature_thresholds), 50, axis=0))\n        \n        \n        \n    def feature_values(self, inputs: tf.Tensor, training: bool = None):\n        feature_selectors = tfa.activations.sparsemax(self.feature_selection_logits)\n        # ^--[in_features, n_trees, depth]\n\n        feature_values = tf.einsum('bi,ind->bnd', inputs, feature_selectors)\n        # ^--[batch_size, n_trees, depth]\n        \n        return feature_values\n        \n    def call(self, inputs: tf.Tensor, training: bool = None):\n        if not self.initialized:\n            self.initialize(inputs)\n            self.initialized = True\n            \n        feature_values = self.feature_values(inputs)\n        \n        threshold_logits = (feature_values - self.feature_thresholds) * tf.math.exp(-self.log_temperatures)\n\n        threshold_logits = tf.stack([-threshold_logits, threshold_logits], axis=-1)\n        # ^--[batch_size, n_trees, depth, 2]\n\n        bins = sparsemoid(threshold_logits)\n        # ^--[batch_size, n_trees, depth, 2], approximately binary\n\n        bin_matches = tf.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n        # ^--[batch_size, n_trees, depth, 2 ** depth]\n\n        response_weights = tf.math.reduce_prod(bin_matches, axis=-2)\n        # ^-- [batch_size, n_trees, 2 ** depth]\n\n        response = tf.einsum('bnd,ncd->bnc', response_weights, self.response)\n        # ^-- [batch_size, n_trees, units]\n        \n        return tf.reduce_sum(response, axis=1)\n    \nclass NODE(tf.keras.Model):\n    def __init__(self, units: int = 1, n_layers: int = 1, dropout_rate = 0.1, \n                 link: tf.function = tf.identity, n_trees: int = 3, depth: int = 4, \n                 threshold_init_beta: float = 1., feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n                 **kwargs):\n        super(NODE, self).__init__()\n        self.units = units\n        self.n_layers = n_layers\n        self.n_trees = n_trees\n        self.depth = depth\n        self.units = units\n        self.threshold_init_beta = threshold_init_beta\n        self.feature_column = feature_column\n        self.dropout_rate = dropout_rate\n        \n        if feature_column is None:\n            self.feature = tf.keras.layers.Lambda(identity)\n        else:\n            self.feature = feature_column\n        \n        self.bn = tf.keras.layers.BatchNormalization()\n        self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n        self.ensemble = [ODST(n_trees = n_trees,\n                              depth = depth,\n                              units = units,\n                              threshold_init_beta = threshold_init_beta) \n                         for _ in range(n_layers)]\n        \n        self.link = link\n        \n    def call(self, inputs, training=None):\n        X = self.feature(inputs)\n        X = self.bn(X, training=training)\n        X = self.dropout(X, training=training)\n        \n        for i, tree in enumerate(self.ensemble):\n            H = tree(X)\n            X = tf.concat([X, H], axis=1)\n            \n        return self.link(H)","98b9aa82":"def create_NODE(n_layers, units, dropout_rate, depth, n_trees, link, learning_rate):\n    \n    node = NODE(n_layers = n_layers, units = units, dropout_rate = dropout_rate, \n                depth = depth, n_trees = n_trees, link = link)\n    \n    node.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), \n                 metrics = [tf.keras.metrics.AUC(name = 'auc')], \n                 loss = 'binary_crossentropy')\n    \n    return node","9d679542":"N_STARTS = 1\nN_SPLITS = 10\nEPOCHS = 100\nBATCH_SIZE = 30000\nVERBOSE = 0\n\ntrain = x\ntrain_targets = y\n\nres = np.zeros(train_targets.shape[0])\n\nfor seed in range(N_STARTS):\n    start_time_seed = time()\n    K.clear_session()\n    tf.random.set_seed(seed)\n    mean_score = 0\n    skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n    for n, (tr_idx, te_idx) in enumerate(skf.split(train_targets, train_targets)):\n        \n        start_time_fold = time()\n        x_tr, x_val = train[tr_idx], train[te_idx]\n        y_tr, y_val = train_targets[tr_idx], train_targets[te_idx]\n            \n        model = create_NODE(n_layers = 1, units = 1, dropout_rate = 0.3, depth = 6, \n                            n_trees = 16, link = tf.keras.activations.sigmoid, learning_rate = 1e-3)\n        ckp = ModelCheckpoint(f'NODE_{seed}_{n}', monitor = 'val_auc', verbose = VERBOSE, \n                              save_best_only = True, save_weights_only = True, mode = 'max')\n        rlr = ReduceLROnPlateau(monitor = 'val_auc', factor = 0.1, patience = 3, \n                                verbose = VERBOSE, min_delta = 1e-4, mode = 'max')\n        es = EarlyStopping(monitor = 'val_auc', min_delta = 1e-4, patience = 5, mode = 'max', \n                           baseline = None, restore_best_weights = True, verbose = VERBOSE)\n        history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = EPOCHS, \n                            batch_size = BATCH_SIZE, callbacks = [ckp, rlr, es], verbose = VERBOSE)\n        hist = pd.DataFrame(history.history)\n        fold_score = hist['val_auc'].max()\n        mean_score += fold_score \/ N_SPLITS\n        model.load_weights(f'NODE_{seed}_{n}')\n        val_predict = model.predict(x_val, batch_size = BATCH_SIZE * 4)[:, 0]\n        \n        res[te_idx] += val_predict \/ N_STARTS\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[2:7]}] NODE Seed {seed}, Fold {n}:', fold_score)\n        break\n    break\n        \n#     print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[2:7]}] NODE Seed {seed} Mean Score:', mean_score)","ed935096":"# print(f'NODE OOF Metric: {roc_auc_score(train_targets, res)}')","7f1930af":"env = riiideducation.make_env()\niter_test = env.iter_test()","249409ed":"model = create_NODE(n_layers = 1, units = 1, dropout_rate = 0.3, depth = 6, \n                    n_trees = 16, link = tf.keras.activations.sigmoid, learning_rate = 1e-3)\n\n# it = 0\nfor test_df, sample_prediction_df in iter_test:\n#     it += 1\n#     if it % 100 == 0:\n#         print(it)\n    test_df = preprocess(test_df)\n    x_te = test_df[FE].values\n    model.load_weights(f'NODE_0_0')\n    test_df['answered_correctly'] = model.predict(x_te, batch_size = 50_000, verbose = 0)[:, 0]\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n#=================================================\n# print(it)","20a6bf72":"# it = 0\n# for test_df, sample_prediction_df in iter_test:\n#     it += 1\n#     if it % 100 == 0:\n#         print(it)\n#     test_df = preprocess(test_df)\n#     x_te = test_df[FE].values\n#     test_df['answered_correctly'] = 0\n#     for seed in range(N_STARTS):\n#         for fold in range(N_SPLITS):\n#             model = create_NODE(n_layers = 1, units = 1, dropout_rate = 0.3, depth = 6, \n#                                 n_trees = 16, link = tf.keras.activations.sigmoid, learning_rate = 1e-3)\n#             model.load_weights(f'NODE_{seed}_{fold}')\n#             test_df['answered_correctly'] += model.predict(x_te, batch_size = 50_000, verbose = 0)[:, 0] \/ (N_STARTS * N_SPLITS)\n#     env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n# #=================================================\n# print(it)","021c7ee4":"# TRAIN","e2fb58ed":"# Riiid Neural Oblivious Decision Ensembles\n\nFrom paper [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data][1]. \n\nGithub (Pytorch): https:\/\/github.com\/Qwicen\/node\n\nKaggle (Tensorflow): https:\/\/www.kaggle.com\/marcusgawronsky\/differentiable-catboost-node-in-tensorflow-2-0\n\n![image.png](attachment:image.png)\n\n[1]: https:\/\/arxiv.org\/pdf\/1909.06312.pdf","2757b620":"# Model","69121fa9":"### CONSTANTS","7e7b6bcf":"# PREDICTION","1a358441":"## TRAINING","b28a5494":"# ODST"}}