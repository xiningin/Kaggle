{"cell_type":{"7b63ab9e":"code","f3ece28a":"code","a7954dfc":"code","54a00b38":"code","5a139688":"code","52b06e12":"code","39c5a543":"code","8a264e2c":"code","3c34b766":"code","924e5342":"code","8ceb76c4":"code","67f1c00b":"code","2dc88d45":"code","0d402225":"code","4e74a968":"code","3a2d6f56":"code","97ac10a4":"code","85995afe":"code","7e202118":"code","a3ca9ef6":"code","3e0b6490":"code","f04252e7":"code","ee8ed84d":"code","cb3c6a89":"code","e4f6b30c":"code","461d64fe":"code","0f4458eb":"markdown","30a00ce3":"markdown","1f3255e3":"markdown","356d6d3f":"markdown","2ab9c895":"markdown","5f034c24":"markdown","04f0f119":"markdown","ce8917c3":"markdown","19fbddc8":"markdown","0895af64":"markdown","4c719889":"markdown","f0687c72":"markdown","578c10a2":"markdown","3ebedd94":"markdown","8a57a24c":"markdown","8e7d027f":"markdown","3cc81f0c":"markdown","79a702ca":"markdown","9fed8e3c":"markdown"},"source":{"7b63ab9e":"import numpy as np\nimport pandas as pd\nimport re\n\ndf = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\n\n# helper function to remove twitter handles\ndef remove_pattern(input_text, pattern):\n    r = re.findall(pattern, input_text) # retuns a list with substrings with 'pattern'\n    for i in r:\n        input_text = re.sub(i, \"\", input_text) # remove pattern \n    return input_text\n\n# to lower\ndf['to_lower'] = df['tweet'].apply(lambda x: x.lower())\n# find pattern for twitter handles using regex\n# @user\ndf['handle_removed'] = np.vectorize(remove_pattern)(df['to_lower'], \"@[\\w]*\")\n# 1. convert pandas series to string\n# 2. call replace method on string\n# 3. Use regex to replace everything except [a-z] and [A-Z] with space (\" \")\n# 4. use \"[^a-zA-Z#]\" to retain hash-symbol (not doing it here)\ndf['puncs_removed'] = df['handle_removed'].str.replace(\"[^a-zA-Z]\", \" \")\n\ndf.head(3)","f3ece28a":"# to numpy array\ndf['puncs_removed'].values[:3]","a7954dfc":"len(df['puncs_removed'])","54a00b38":"from sklearn.feature_extraction.text import CountVectorizer\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\nbow = CountVectorizer(\n            stop_words     = 'english',\n            binary         = False, # True -> Binary BoW,\n            ngram_range    = (1,1), # (1, 1) -> only unigrams, (1, 2) -> unigrams and bigrams, and (2, 2) -> only bigrams\n            #vocabulary    = Mapping \/ iterable (custom vocabulary)\n        )","5a139688":"# use `fit_transform` with training data (seeing first time => generates vocabulary)\n# use `transform` with test data (not seeing first time => needs vocabulary - already genearated by `fit_transform`)\nfeatures = bow.fit_transform(df['puncs_removed'].values)\n\nprint(features.shape)\nprint(type(features))","52b06e12":"print(dir(bow))","39c5a543":"print(\"vocabulary size is: \", len(bow.vocabulary_))\n\n# method 1:\n# out of 37255 \nprint(\"method 1:\")\nfor vocab_word, index in bow.vocabulary_.items():\n    if index == 0   : print(\"feature 0 repesents word: \", vocab_word)\n    if index == 100 : print(\"feature 100 repesents word: \", vocab_word)\n        \n# method 2: \nprint(\"method 2:\")\nprint(f\"feature 0 repesents word: {bow.get_feature_names()[0]}\")\nprint(f\"feature 100 repesents word: {bow.get_feature_names()[100]}\")","8a264e2c":"# `transform` instead of `fit_transform` for test-data\nbow.transform(['hi']) # (1x37255)","3c34b766":"from nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.corpus import stopwords\n\n''' \n# Lemmatisation:\n# base word of stem might not be an actual word whereas, lemma is an actual language word\n\n>>> from nltk.stem import WordNetLemmatizer\n>>> wnl = WordNetLemmatizer()\n>>> print(wnl.lemmatize('dogs'))\ndog\n>>> print(wnl.lemmatize('churches'))\nchurch\n>>> print(wnl.lemmatize('aardwolves'))\naardwolf\n\nUSAGE: instead of stemming pd.series below, use\n`.apply(lambda x: [wnl.lemmatize(i) for i in x])`\n'''\n\n\nstopwords = set( stopwords.words('english') )\n\n# SnowballStemmer stemmer better\n#stemmer = PorterStemmer('english')\nstemmer = SnowballStemmer('english')\n\n# Tokenize before stemming. \n# Tokenize: Split into particular words i.e into list\ntokenized_tweet = df['puncs_removed'].apply(lambda x: x.split())\n\n# Stopword removal (in-place)\ntokenized_tweet = tokenized_tweet.apply(lambda tokens: [i if i not in stopwords else '' for i in tokens])\n\n# Stemming (can be replaced w\/ Lemmatisation)\n# Iterate over every word in each list \n# So that `having` and `have` both can be converted into `have`\nstemmed_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n\n# convert list of words into a line\nfor i in range(len(stemmed_tweet)):\n    stemmed_tweet[i] = ' '.join(stemmed_tweet[i])\ndf[\"processed\"] = stemmed_tweet\n\n# display\ndf[['puncs_removed', 'processed']].head(3)","924e5342":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# use exactly same as BoW\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\ntfidf = TfidfVectorizer(\n            stop_words   = 'english',\n            ngram_range  = (1,2) # uni as well as bi\n        )","8ceb76c4":"# use `fit_transform` with training data (seeing first time => generates vocabulary)\n# use `transform` with test data (not seeing first time => needs vocabulary - already genearated by `fit_transform`)\ntfidf_features = tfidf.fit_transform(df['processed'].values)\n\nprint(tfidf_features.shape)\nprint(type(tfidf_features))","67f1c00b":"def get_topn_tfidfs_of_a_sample(tfidf_features, sample_idx, n=25):\n    \"\"\"\n    tfidf_features  : np.ndarray of dims (num_samples, num_tfidf_feats)\n    sample_idx      : row_idx\n    \"\"\"\n    tfidfs_of_a_row = tfidf_features[sample_idx].toarray().flatten() # (1x31194) -> (31194,)\n    desc_idxs = np.argsort(tfidfs_of_a_row)[::-1][:n]\n    \n    top_n_tfidfs = tfidfs_of_a_row[desc_idxs]\n    top_n_tfidfs_featwords = np.array(tfidf.get_feature_names())[desc_idxs]\n    \n    return top_n_tfidfs_featwords, top_n_tfidfs","2dc88d45":"# Analyze top-n TF-IDFs of a >>data-sample 0 and 1<<\n# inspiration: http:\/\/buhrmann.github.io\/tfidf-analysis\nbar_xs_0, bar_ys_0 = get_topn_tfidfs_of_a_sample(tfidf_features, 0, n=25)\nbar_xs_1, bar_ys_1 = get_topn_tfidfs_of_a_sample(tfidf_features, 1, n=25)","0d402225":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axarr = plt.subplots(1, 2)\nfig.set_size_inches(12,5)\n\n# sample at idx 0\nsns.barplot(bar_ys_0, bar_xs_0, ax=axarr[0])\naxarr[0].set_title('For sample at idx 0')\naxarr[0].grid()\n\n# sample at idx 1\nsns.barplot(bar_ys_1, bar_xs_1, ax=axarr[1])\naxarr[1].set_title('For sample at idx 1')\naxarr[1].grid()\n\nfig.tight_layout(pad=3.0)\nplt.show()","4e74a968":"! curl -O \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"","3a2d6f56":"from gensim.models import Word2Vec, KeyedVectors\npretrained_model = KeyedVectors.load_word2vec_format('.\/GoogleNews-vectors-negative300.bin.gz', binary=True)","97ac10a4":"# word -> 300 dim vec (pretrained)\npretrained_model['test'].shape","85995afe":"# similarity using norm of distance vector \n# output is normalized (between 0,1)\npretrained_model.similarity('king', 'queen')","7e202118":"pretrained_model.most_similar('king')","a3ca9ef6":"sentences = df['processed'].values # ndarray of stemmed\n\nlist_of_list_of_list_of_words = []\nfor sentence in sentences:\n    list_of_list_of_list_of_words.append(\n        sentence.split()\n    )\n    \nlist_of_list_of_list_of_words[:2]","3e0b6490":"# train custom model\ncustom_model = Word2Vec(\n                list_of_list_of_list_of_words,\n                min_count     = 1,\n                size          = 300,\n                workers       = 4\n                )","f04252e7":"custom_model.wv['lyft'].shape # custom vocabulary","ee8ed84d":"custom_model.wv.most_similar('lyft')","cb3c6a89":"list_of_sentences = df['processed'].values # ndarray of stemmed sentences\nlist_of_sentences[0]","e4f6b30c":"from tqdm import tqdm\n\navg_w2v_sentences = []\nfor sentence in tqdm(list_of_sentences):\n    w2vs = []\n    for word in sentence.split():\n        w2vs.append(custom_model.wv[word])\n        \n    w2vs = np.array(w2vs)\n    avg_w2v_sentence_vec = np.sum(w2vs, axis=0) \/ len(sentence)\n    avg_w2v_sentences.append(avg_w2v_sentence_vec)","461d64fe":"avg_w2v_sentences = avg_w2v_sentences\n\nprint(avg_w2v_sentences[0].shape)\nprint(len(avg_w2v_sentences))","0f4458eb":"# **04. Sentence to vectors**","30a00ce3":"# **04. WORD2VEC**\n\nWith genism,\n\n- Can train custom model (w\/ custom data)\n- Can use pretrained model","1f3255e3":"# **03. TF-IDF**\n\n> Based on two key ideas - \n> - Normalize BoW counts **in datasample** [by sum](https:\/\/www.kaggle.com\/l0new0lf\/02-08-normalisation-vs-standardisation-vs-probs) i.e to probabilities **(Property of datasample)**\n> - Penalize more frequent and reward less frequently words **(Property of dataset)**\n\n","356d6d3f":"# **02. n-Grams**\n\n> Reatains sequence information. *( n = number of neighbors)*\n\n**BoW** where separate **consecutive-groups-of-words** become different features ($feat_k$). <br>\nSize of group is given by `n` in n-Grams\n\n### **Note (Main Disadvantage)**\n\n- $size_{vocabluary} \\propto n \\propto sparsity_{x_i}\\,\\,\\,$ (As `n` increases, sparsity and vocabulary increases)\n- Extremely sparse vector\n\n### **Advantage over simple BoW**\n\n- Partial sequence info is retained\n    > eg. \"do not\" -> [\"do\", \"not\"] is not informative but bigram,  \"do not\" -> [\"do not\"] is!","2ab9c895":"Make sure for pretrained model, input word **exists in google-news vocabulary**\n\n> Better used lemmatisation(base word exists in vocab) instead of stemming","5f034c24":"To accept words that **do not exist** in google-news vocabulary, train custom model\n\nInput is list of data samples (which is again list of individual words)\n```\n[\n    ['w1', 'w2', ..., 'wm'],\n    ['w1', 'w2', ..., 'wm'],\n    ['w1', 'w2', ..., 'wm'],\n    .\n    .\n    .\n]\n```\n\nWord2Vec parameters (custom model)\n- list of sentences (sentence is list of words) as shown above\n- **min_count:** min number of occurances of word required to create a unique vector for it\n- **size:** dim of output vector (larger the better)\n- **workers:*** cpu cores to use\n\n> Note: \n> - vocabulary size i.e data size must be large (to compensate for almost any random valid input at test time)\n> - Here, used **stemmed** text. **Better use raw punc_removed text**","04f0f119":"**TERM FREQUENCY (TF):** Converts simple counts to probabilities by dividing w\/ sum of all words' counts **in datasample** <br>\n**INVERSE DOC. FREQ. (IDF):** Penalize more frequent and reward less frequently words **in dataset**\n\n<br><br>\n**$$\\text{TF-IDF(}x_i^{feat_k}) = \\text{TF} * \\text{IDF}$$**\n<br><br>\n\n$\\text{TF(}x_i^{feat_k}) = \\frac{\\text{count of k-th word in }x_i}{\\text{Sum of counts of all words in x_i} }$ where $x_i$ is a datasample (string or document)\n<br>\n<br>\n$\\text{IDF(}x_i^{feat_k}) = \\log{\\bigg( \\frac{n}{n_{\\text{with k-th word}}} \\bigg)}$ where **$n$** is total number of samples in dataset and **$n_{\\text{with k-th word}}$** is number of samples w\/ **atleast** one k-th word.\n\n> Can even use method similar to [laplace smoothing]() in IDF equation \n\n### **NOTE**\n\n- **TF** Normalizes [by sum](https:\/\/www.kaggle.com\/l0new0lf\/02-08-normalisation-vs-standardisation-vs-probs) by **ROW**\n- As IDF is inverse, `IDF >>> TF`. Not good as IDF can nullify impact of TF in overall value. Hence, **use log(IDF)** to monotonically decrease it's value.\n- IDF is **small** `~ log(1)` for frequent words and **large** `~ log(num_of_samples)` for rare words\n    \n### **DISADVANTAGE**\n\n- Doesn't take semantic meaning into account\n- Sparse vector\n\n### **Ponder**\n\nWhat if **IDF** Inverse-Normalizes original counts [by sum](https:\/\/www.kaggle.com\/l0new0lf\/02-08-normalisation-vs-standardisation-vs-probs) by **COLUMN** i.e $\\log{ \\bigg( \\frac{N_X^{feat_k}}{N_{x_i}^{feat_k}} \\bigg)}$ Where, $N_X^{feat_k}$ is count of k-th word in whole dataset  $X$ and $N_{x_i}^{feat_k}$ is count of k-th word in sample $x_i$\n","ce8917c3":"# Preprocessing for **String** data\n\n01. BoW \/ Binary BoW\n02. n-Grams\n03. TF-IDF\n04. Word2Vec (SOTA) \/ Avg. Word2Vec \/ Weighted Word2Vec","19fbddc8":"# **02. BoW Improvement**\n\n- Tokenize (space separated list)\n- Stopword removal\n- Stemming \/ Lemmatisation","0895af64":"# **04. WORD2VEC**\n\n> [2013 Paper](https:\/\/arxiv.org\/pdf\/1301.3781.pdf) *Takes semantic meaning into consideration unlike any methods above.*\n>\n> - Converts a given word (text) to any d-sized dense vector.\n> $d \\propto \\text{information retained}$\n> - Takes neighbors of the word into consideration (while training)\n\nCan be understood w\/ help of *Matrix Factorisation \/ Deep Learning*\n\n## **Advantages**\n\n- Dense vector! (unlike any methods above)\n- Incorporate semantic meaning (unlike any methods above)\n    - vector distance preserves similarity of different words\n    - vector direction preserves temporal information (tenses) and relationships b\/w words\n    - if $\\vec{v_1}$ is related to $\\vec{v_2}$ and $\\vec{v_3}$ is related to $\\vec{v_4}$, $\\,\\,\\, (\\vec{v_1} - \\vec{v2} ) \\,\\, \/\/ \\,\\, (\\vec{v_3} - \\vec{v4} )$ Thus, relationship is maintained! \n    \n\n## **Disadvantages**\n\n- Black box\n- Need extremely large training set\n\n## **Sentence to Vector**\n\nEither of three methods are applicable\n\n- Use complex SOTA techniques like *Sentence2Vec* (Need retraining and large corpus of data)\n- Take avg. of element word vectors \n$$\\vec{\\text{sentence vector}} = \\frac{1}{N_{\\text{num of words}}} \\sum_{i=0}^{N_{\\text{num of words}}}{\\text{w2v}(word_i)}$$\n\n- Take TF-IDF weighted avg. element of word vectors\n\n$$\\vec{\\text{sentence vector}} = \\frac{1}{\\sum_{i=0}^{N_{\\text{num of words}}}{\\text{TF-IDF}_i}} \\sum_{i=0}^{N_{\\text{num of words}}}{\\text{TF-IDF}_i * \\text{w2v}(word_i)}$$\n\n> In both above cases, resulant sentence vector will be of **same dims** as of individual word vec","4c719889":"> - For `n = 31962` samples, `37255` dimension vector\n> - Vocabulary size is `37255`\n> - returns sparse matrix (saves memory)\n\nUse `features.todense()` or `features.toarray()` to convert to dense `np.ndarray`","f0687c72":"# **01. Bag of Words Representation**\n\n   > Separate words become different features ($feat_k$)\n   > \n   > $x_i^{feat_k} = \\text{count of k-th word in string }x_i \\,\\,\\,\\, (where\\, 0 < k < size_{vocabluary})$\n\nFor Binary BoW,\n   > $x_i^{feat_k} \\begin{cases}\n      0, & \\text{if count of k-th word in string \ud835\udc65\ud835\udc56 }=0 \\\\\n      1, & \\text{if count of k-th word in string \ud835\udc65\ud835\udc56 }>0\n    \\end{cases}$ \n\n***As we are simply counting number of $k^{th}$ word*** in BoW representation, it can be further **IMPROVED** by following techniques --\n   - **Stopword removal:** Remove useless frequently occuring words (*and, is, or, the* etc.) \n       > Semantic information is lost\n   - **Stemming:** Change similar words to their base words (\"do\" \/ \"done\" \/ \"doing\" $\\rightarrow$ \"do\" (base word))\n       > *snowball* stemmer powerful compared to *porter* stemmer (old technique)\n   - **Lemmatisation:** Same as stemming but,\n       > base word of stem might not be an actual word whereas, lemma is an actual language word.\n   \n### **Note**\n\n- $size_{vocabluary} \\propto sparsity_{x_i}$ (As vocabulary increases, sparsity if $x_i$ increases)\n- To know how similar \/ dissimilar two vectors are, take [Norm](https:\/\/www.kaggle.com\/l0new0lf\/l1-l2-ln-norm) of their **difference** *(Norm gives size of a vector)*\n- [L1 Norm](https:\/\/www.kaggle.com\/l0new0lf\/l1-l2-ln-norm) of Binary BoW is `= number of different words` \n\n### **Disadvantages**\n\n- **Compute:** In case of Logistic regression, `n_dims = n_vocab` $\\Rightarrow$ Large training and inference time\n- **Space:** Sparse vector\n- **Inherent:**\n    - Simple counts \n        - *Doesn't incorporate semantics*\n        - *Information is lost*\n        - *Sequence info ignored*\n    - Problem w\/ *heteronyms* (`Lead` in pencil is not same as `Lead` in leader)\n    - Problem w\/ synonyms (same meaning words w\/ difft spelling will be made into different features instead of making same)","578c10a2":"**B. Using TF-IDF weighted w2v** (see formula above)\n\n> Just like Avg Word2vec above, use TF-IDF data and genism w2v to compute the vector representing sentence sentence","3ebedd94":"# **03. TF-IDF**\n\n> *Same usage as BoW above*","8a57a24c":"> **Num of samples = n = 31962**","8e7d027f":"# DATA","3cc81f0c":"# CODE","79a702ca":"# **01. BoW \/ Binary Bow \/ n-Grams**","9fed8e3c":"**A. Using Avg Word2vec** (see formula above)"}}