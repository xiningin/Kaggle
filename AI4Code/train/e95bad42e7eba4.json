{"cell_type":{"ddc0f3ca":"code","af31ca5a":"code","bfa7e504":"code","af597bd0":"code","ec003d66":"code","929f4098":"code","aadca2ad":"code","8faa22c3":"code","d747002a":"code","8db85129":"code","a719c37e":"code","4b1bb6e0":"code","b363242c":"code","f59720b7":"code","46842a28":"code","c59cffbc":"code","9f1eec8e":"code","bbba4284":"code","6b60a412":"code","720f9200":"code","97898544":"code","fd446f03":"code","df2d681a":"code","213ed50c":"code","fd56fbf5":"code","eb58f2d2":"code","cafdad95":"code","b1c57145":"code","cbf5d924":"code","3b25fdb0":"code","f30f126d":"code","c874e216":"markdown","3bf7a836":"markdown","6fd5a9be":"markdown","fdd5ce9d":"markdown","d6d1dde9":"markdown","41a275c5":"markdown","dc1dd993":"markdown","3ed1a16b":"markdown","c949041e":"markdown","76c034a1":"markdown","d7ac67ec":"markdown","fdf51533":"markdown","e2671775":"markdown","caca9fee":"markdown"},"source":{"ddc0f3ca":"from scipy import misc\nimport matplotlib.pyplot as plt\n\n\nimg = misc.face()\nplt.imshow(img);","af31ca5a":"import cv2\nimg = cv2.resize(img, (224, 224))\nplt.imshow(img);","bfa7e504":"fig, ax = plt.subplots(1, 14)\nfor i in range(14):\n#     for j in range(5):\n    patch = img[0:16, 16 * i:16 * (i + 1)]\n    ax[i].imshow(patch)\n    ax[i].axis('off')\nplt.show()","af597bd0":"16 * 16","ec003d66":"patch.flatten().shape","929f4098":"import sys\n\npackage_path = '..\/input\/vision-transformer-pytorch\/VisionTransformer-Pytorch'\nsys.path.append(package_path)","aadca2ad":"import os\nimport pandas as pd\n\nimport time\nimport datetime\nimport copy\nimport matplotlib.pyplot as plt\nimport json\nimport seaborn as sns\nimport cv2\nimport albumentations as albu\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n\n\n# ALBUMENTATIONS\nimport albumentations as albu\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n    \nfrom albumentations.pytorch import ToTensorV2","8faa22c3":"# BASE_DIR=\"..\/input\/plant-pathology-2021-fgvc8\/\"\nBASE_DIR = '..\/input\/plant-pathology-2021-224x224\/'\nTRAIN_IMAGES_DIR = os.path.join(BASE_DIR, 'train_imgs')","d747002a":"train_df = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')\ntrain_df.head()","8db85129":"print(\"Count of training images {0}\".format(len(os.listdir(TRAIN_IMAGES_DIR))))","a719c37e":"class_name = train_df['labels'].value_counts().index\nclass_count = train_df['labels'].value_counts().values","4b1bb6e0":"train_df.labels.value_counts()","b363242c":"plt.figure(figsize=(8,5))\nsns.countplot(data=train_df, y='labels');","f59720b7":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nle.fit(train_df.labels)\ntrain_df['labels'] = le.transform(train_df.labels)","46842a28":"def visualize_images(image_ids, labels):\n    plt.figure(figsize=(16, 12))\n    \n    for idx, (image_id, label) in enumerate(zip(image_ids, labels)):\n        plt.subplot(3, 3, idx+1)\n        \n        image = cv2.imread(os.path.join(TRAIN_IMAGES_DIR, image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        plt.imshow(image)\n        plt.title(f\"Class: {label}\", fontsize=12)\n        \n        plt.axis(\"off\")\n        \n    plt.show()\n    \n\ndef plot_augmentation(image_id, transform):\n    plt.figure(figsize=(16, 4))\n    \n    img = cv2.imread(os.path.join(TRAIN_IAMGES_DIR, image_id))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    x = transform(image=img)['image']\n    plt.imshow(x)\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    x = transform(image=img)['image']\n    plt.imshow(x)\n    \ndef visualize(images, transform):\n    '''\n    Plot images and their transformations\n    '''\n    fig = plt.figure(figsize=(32, 16))\n    \n    for i, im in enumerate(images):\n        ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plt.imshow(im)\n        \n    for i, im in enumerate(images):\n        ax = fig.add_subplot(2, 5, i+6, xticks=[], yticks=[])\n        plt.imshow(transform(image=im)['image'])","c59cffbc":"# CUSTOM DATASET CLASS\nclass PlantDataset(Dataset):\n    def __init__(\n        self, df:pd.DataFrame, imfolder:str, train:bool=True, transforms=None\n    ):\n        self.df = df\n        self.imfolder = imfolder\n        self.train = train\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        im_path = os.path.join(self.imfolder, self.df.iloc[index]['image'])\n        im = cv2.imread(im_path, cv2.IMREAD_COLOR)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n        \n        if (self.transforms):\n            '''\n            When AlbumentationCompose, a dictionary with key 'image' is created\n            '''\n            im = self.transforms(image=im)['image']\n            \n        if (self.train):\n            label = self.df.iloc[index]['labels']\n            return im, label\n        else:\n            return im","9f1eec8e":"# AUGMENTATIONS\n\ntrain_augs = albu.Compose([\n    albu.RandomResizedCrop(height=384, width=384, p=1.0),\n    albu.HorizontalFlip(p=0.5),\n    albu.VerticalFlip(p=0.5),\n    albu.RandomBrightnessContrast(p=0.5),\n    albu.ShiftScaleRotate(p=0.5),\n    albu.Normalize(    \n        mean=[0.3, 0.3, 0.3],\n        std=[0.1, 0.1, 0.1],),\n    CoarseDropout(p=0.5),\n    Cutout(p=0.5),\n    ToTensorV2(),\n])\n\nvalid_augs = albu.Compose([\n    albu.Resize(height=384, width=384, p=1.0),\n    albu.Normalize(\n        mean=[0.3, 0.3, 0.3],\n        std=[0.1, 0.1, 0.1],),\n    ToTensorV2(),\n])\n","bbba4284":"# DATA SPLIT\ntrain, valid = train_test_split(\n    train_df,\n    test_size=0.1,\n    random_state=42,\n    stratify=train_df.labels.values\n)\n\n# reset index on both dataframes\ntrain = train.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)\n\n# targets in train,valid datasets\ntrain_targets = train.labels.values\nvalid_targets = valid.labels.values","6b60a412":"# DEFINE PYTORCH CUSTOM DATASET\ntrain_dataset = PlantDataset(\n    df=train,\n    imfolder=TRAIN_IMAGES_DIR,\n    train=True,\n    transforms=train_augs\n)\n\nvalid_dataset = PlantDataset(\n    df=valid,\n    imfolder=TRAIN_IMAGES_DIR,\n    train=True,\n    transforms=valid_augs\n)","720f9200":"def plot_image(img_dict):\n    image_tensor = img_dict[0]\n    target = img_dict[1]\n    print(target)\n    image = image_tensor.permute(1, 2, 0)\n    plt.imshow(image)","97898544":"plot_image(train_dataset[7])","fd446f03":"# MAKE PYTORCH DATALOADER\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    num_workers=4,\n    shuffle = True\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    num_workers=4,\n    shuffle = False\n)","df2d681a":"# TRAIN\ndef train_model(datasets, dataloaders, model, criterion, optimizer, scheduler, num_epochs, device):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs-1))\n        print('-' * 10)\n        \n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0.0\n            running_total = 0.0\n            \n            for step, (inputs, labels) in enumerate(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n               \n                # Zero out the grads\n                optimizer.zero_grad()\n                \n                # Forward\n                # Track history in train mode\n                with torch.set_grad_enabled(phase == 'train'):\n                    model = model.to(device)\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1) \n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                running_total += len(labels.data)\n                \n                if (step + 1) % 100 == 0:\n                    print(f'[{step + 1}\/{len(dataloaders[phase])}].')\n                    print(f'Loss {running_loss \/ running_total}. Accuracy {running_corrects \/ running_total}')\n            \n            if phase == 'train':\n                scheduler.step()\n                \n            epoch_loss = running_loss \/ len(datasets[phase])\n            epoch_acc = running_corrects.double() \/ len(datasets[phase])\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n        \n        print()\n    \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:.4f}'.format(best_acc))\n    \n    model.load_state_dict(best_model_wts)\n    \n    return model","213ed50c":"# from vision_transformer_pytorch import VisionTransformer\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(device)\n\n# datasets = {'train': train_dataset,\n#             'valid': valid_dataset}\n\n# dataloaders = {'train': train_loader,\n#                'valid': valid_loader}\n\n# # # LOAD PRETRAINED ViT MODEL\n# # model = VisionTransformer.from_pretrained('ViT-B_16', num_classes=12)        \n\n# model_path = '..\/input\/plantpathologyvitb169epochs\/vit_b-16_9epoch_pretrained.pt'\n# model = torch.load(model_path)\n\n# # OPTIMIZER\n# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n# # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)\n# # optimizer = AdamP(model.parameters(), lr=1e-4, weight_decay=0.001)\n\n# # LEARNING RATE SCHEDULER\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n# criterion = nn.CrossEntropyLoss()\n# num_epochs = 3","fd56fbf5":"# torch.save(model, 'full_plant_pathology_vit.pt')","eb58f2d2":"# len(dataloaders['train'])","cafdad95":"# # MODEL TRAIN\n# trained_model = train_model(datasets, dataloaders,\n#                             model, criterion,\n#                             optimizer, scheduler,\n#                             num_epochs, device)","b1c57145":"# Save the mode after training\n# torch.save(model, 'vit_b-16_12epoch_pretrained.pt')","cbf5d924":"from vision_transformer_pytorch import VisionTransformer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# LOAD PRETRAINED ViT MODEL\nmodel_path = '..\/input\/plantpathologyvitb169epochs\/vit_b-16_12epoch_pretrained.pt'\nmodel = torch.load(model_path)\n# model.load_state_dict(torch.load(PATH))\nmodel.eval()","3b25fdb0":"from pathlib import Path\n\ndf_preds = pd.DataFrame()\nfor path in Path('..\/input\/plant-pathology-2021-fgvc8\/test_images').iterdir():\n    img = cv2.imread(str(path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = valid_augs(image=img)['image'].cuda()\n    pred = model(img[None])\n    \n    df_preds = df_preds.append(\n        {'image': path.parts[-1], 'labels': le.inverse_transform(torch.argmax(pred.cpu(), dim=1))[0]},\n        ignore_index=True)\n    \ndf_preds","f30f126d":"df_preds.to_csv('submission.csv', index=False)","c874e216":"\n3. \u041f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a \u043f\u0430\u0442\u0447\u0443 \u044d\u043c\u0431\u044d\u0434\u0434\u0438\u043d\u0433, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e\n\n\u042d\u0442\u043e \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u0430\u044f \u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u0441 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438.\n\n\n\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u044b \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u044f\u0442 \u043e\u0442 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u044b\u0445 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u044d\u043c\u0431\u044d\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0442\u0447 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0443\u0437\u043d\u0430\u0442\u044c \u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n\n\n\n\u042d\u0442\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043f\u0430\u0442\u0447\u0435\u0439 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u044d\u043d\u043a\u043e\u0434\u0435\u0440\u0430 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430.","3bf7a836":"Self-Attention \u043c\u0435\u0436\u0434\u0443 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u043c\u0438. \u041a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, \u0435\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 640x640, \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0435 \u043d\u0430\u0434\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c self-attention \u0434\u043b\u044f 409\u043a \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0439. \u041d\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u0435\u0439 \u0432\u0441\u0435\u0433\u043e, \u0441\u0430\u043c\u044b\u0439 \u0432\u0435\u0440\u0445\u043d\u0438\u0439 \u043f\u0440\u0430\u0432\u044b\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c \u0432\u0440\u044f\u0434 \u043b\u0438 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043d\u0438\u0436\u043d\u0438\u0439 \u043b\u0435\u0432\u044b\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c. ViT \u0441\u043f\u0440\u0430\u0432\u0438\u043b\u0441\u044f \u0441 \u044d\u0442\u043e\u0439 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u043e\u0439 \u0437\u0430 \u0441\u0447\u0435\u0442 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0430 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043f\u0430\u0442\u0447\u0438 (\u043a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, 16x16).\n\n![image.png](attachment:cded490a-0e7b-4b0e-99c5-cd91584bb995.png)","6fd5a9be":"\u041e\u0431\u044b\u0447\u043d\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u043a\u0430\u043a 3D array (\u0432\u044b\u0441\u043e\u0442\u0430, \u0448\u0438\u0440\u0438\u043d\u0430, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432) \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043a \u043d\u0438\u043c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043b\u043e\u0438. \u041d\u043e \u0442\u0443\u0442 \u0435\u0441\u0442\u044c \u0440\u044f\u0434 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432:\n\n- \u043d\u0435 \u0432\u0441\u0435 \u043f\u0438\u043a\u0441\u0435\u043b\u0438 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e \u043f\u043e\u043b\u0435\u0437\u043d\u044b;\n- \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u043d\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0441 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u043c\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u043c\u0438\u0441\u044f \u0434\u0430\u043b\u0435\u043a\u043e \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430;\n- \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b \u0432 \u043e\u0447\u0435\u043d\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u044f\u0445.\n\n\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0430\u0432\u0442\u043e\u0440\u044b \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e\u0442 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b \u0438 \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440.\n\n- \u0412\u043d\u0430\u0447\u0430\u043b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043e\u0431\u044b\u0447\u043d\u044b\u0439 backbone \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f feature maps\n- \u0414\u0430\u043b\u0435\u0435 feature map \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b\n- \u0422\u043e\u043a\u0435\u043d\u044b \u043f\u043e\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u044b\n- \u0412\u044b\u0445\u043e\u0434 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430 \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\n- \u0410 \u0435\u0441\u043b\u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0442\u044c \u0432\u044b\u0445\u043e\u0434 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430 \u0441 feature map, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438","fdd5ce9d":"### Multi-Head Self-Attention\n\n\u042d\u0442\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0443 \u0441\u043b\u043e\u044f \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0434\u0432\u0443\u043c\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u0430\u043c\u0438:\n\n1. \u042d\u0442\u043e \u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0444\u043e\u043a\u0443\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0445.\n\n2. \u042d\u0442\u043e \u0434\u0430\u0435\u0442 \u0441\u043b\u043e\u044e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u201c\u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u201d. \u0421 Multi-Head self-attention \u0435\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0432\u0435\u0441\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0440\u0438\u0446 query\/key\/value. \u041a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u044d\u0442\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c. \u0417\u0430\u0442\u0435\u043c, \u043f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u0430\u0436\u0434\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0435\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0432 \u0434\u0440\u0443\u0433\u043e\u0435 \u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_qkv.png' width=600>","d6d1dde9":"### Self-Attention\n\u0411\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c:\n\n\u201dThe animal didn't cross the street because it was too tired\u201d\n\n\u0427\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 it \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438? It \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u0443\u043b\u0438\u0446\u0435 \u0438\u043b\u0438 \u043a \u0436\u0438\u0432\u043e\u0442\u043d\u043e\u043c\u0443? \u041a\u043e\u0433\u0434\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u043e it, self-attention \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0435\u0439 \u0430\u0441\u0441\u043e\u0446\u0438\u0438\u0440\u043e\u0432\u0430\u0442\u044c it \u0441 animal, \u043d\u043e it \u0435\u0449\u0435 \u0438 \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 tired.\n\nSelf-attention \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u043a\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0432 \u043f\u043e\u0438\u0441\u043a\u0430\u0445 \u043f\u043e\u0434\u0441\u043a\u0430\u0437\u043e\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u043c\u043e\u0447\u044c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043b\u0443\u0447\u0448\u0435\u043c\u0443 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044e \u044d\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430.\n\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self-attention_visualization.png' width=400>","41a275c5":"\u0421\u0441\u044b\u043b\u043a\u0438:\n1. [Vision Transformers: A New Computer Vision Paradigm](https:\/\/medium.com\/swlh\/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2)\n2. [Visual Transformers: Token-based Image Representation and Processing for Computer Vision](https:\/\/arxiv.org\/pdf\/2006.03677.pdf)\n3. [Vision Transformer (ViT) - An image is worth 16x16 words | Paper Explained](https:\/\/www.youtube.com\/watch?v=j6kuz_NqkG0)\n4. [\u041f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u043e\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 4. Self-Attention. Transformer overview](https:\/\/youtu.be\/UETKUIlYE6g)","dc1dd993":"### Training","3ed1a16b":"This is notebook is based on [CassavaLeaf ViT baseline Notebook](https:\/\/www.kaggle.com\/szuzhangzhi\/vision-transformer-vit-cuda-as-usual) and Adjusted for Plant pathology 2021 dataset.","c949041e":"### Transformer Encoder\n\n![image.png](attachment:6a08e9ff-e266-493e-880d-5ab34014dc73.png)\n\n\u042d\u043d\u043a\u043e\u0434\u0435\u0440 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437\n- Multi-Head Self Attention Layer(MSP) \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u044b\u0445\u043e\u0434\u043e\u0432 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u043c\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c\u0438. \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e Multi-Head \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0438 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438.\n- \u041c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u044b\u0435 \u043f\u0435\u0440\u0441\u0435\u043f\u0442\u0440\u043e\u043d\u044b (MLP) \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0434\u0432\u0430 \u0441\u043b\u043e\u044f \u0441 GELU (Gaussian Error Linear Unit)\n- Layer Norm (LN) \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0435\u0434 \u043a\u0430\u0436\u0434\u044b\u043c \u0431\u043b\u043e\u043a\u043e\u043c, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0430 \u043d\u0435 \u0432\u0432\u043e\u0434\u0438\u0442 \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u043d\u043e\u0432\u044b\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u043c\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438. \u041f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f\n- Residual connections \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u043b\u043e\u043a\u0430\n\n\u0414\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u0430 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c MLP \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0435\u043c.\n\u0412\u0435\u0440\u0445\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 ViT \u0438\u0437\u0443\u0447\u0430\u044e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b, \u0442\u043e\u0433\u0434\u0430 \u043a\u0430\u043a \u043d\u0438\u0436\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 \u0438\u0437\u0443\u0447\u0430\u044e\u0442 \u043a\u0430\u043a \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435, \u0442\u0430\u043a \u0438 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 ViT \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u0449\u0438\u0435 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u044b.\n","76c034a1":"\n\u041f\u0435\u0440\u0432\u044b\u0439 \u0448\u0430\u0433 \u0432 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0438 self-attention - \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0438\u0437 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0432\u0435\u043a\u0442\u043e\u0440 Query, \u0432\u0435\u043a\u0442\u043e\u0440 Key \u0438 \u0432\u0435\u043a\u0442\u043e\u0440 Value. \u042d\u0442\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u044b \u0441\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u043f\u0443\u0442\u0435\u043c \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430 \u043d\u0430 \u0442\u0440\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f.\n- Query - \u0441\u043b\u043e\u0432\u043e, \u0441 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0432\u0441\u0451 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u043e\u0435\n- Key - \u0441\u043b\u043e\u0432\u043e, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u043c\u043e\u0442\u0440\u0438\u043c\n- Value - \u0437\u0434\u0435\u0441\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f \u0441\u043c\u044b\u0441\u043b \u0441\u043b\u043e\u0432\u0430\n\n\u041f\u0440\u0438 \u043f\u0435\u0440\u0435\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0438 Query \u043d\u0430 Key \u043c\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u043e \u0441 Key \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u043e \u0441\u043b\u043e\u0432\u0443 \u0441 Query.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_vectors.png' width=500>\n\n\n\u0414\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 $\\sqrt{d}$ (\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u044b\u0439 \u043a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 Key \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432. \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention-matrix-calculation-2.png' width=400>","d7ac67ec":"\n2. \u0412\u044b\u0442\u044f\u043d\u0443\u0442\u044c 2D \u043f\u0430\u0442\u0447\u0438 \u0432 1D \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\n\n\u041a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0442\u0447 \u0432\u044b\u0442\u044f\u0433\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0432 1D \u043f\u0430\u0442\u0447 \u043f\u0443\u0442\u0435\u043c \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0430\u0446\u0438\u0438 \u0432\u0441\u0435\u0445 \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439 \u0438 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u043f\u0440\u043e\u0435\u043a\u0446\u0438\u044f \u0434\u043e \u0436\u0435\u043b\u0430\u0435\u043c\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 - \u044d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u043f\u0430\u0447\u0442 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430. (\u042d\u0442\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0430\u0442\u0447\u0435\u0439 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u0430\u044f)","fdf51533":"\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438:\n1. [Swin-Transformer](https:\/\/github.com\/microsoft\/Swin-Transformer)\n2. [VisionTransformer-Pytorch](https:\/\/github.com\/tczhangzhi\/VisionTransformer-PyTorch)\n3. [vit-pytorch](https:\/\/github.com\/lucidrains\/vit-pytorch)","e2671775":"## Testing","caca9fee":"1. \u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 \u043f\u0430\u0447\u0442\u0438 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\n\n\u0415\u0441\u0442\u044c 2D \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430  H * W, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043c\u043e\u0436\u0435\u0442 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c\u0441\u044f \u043d\u0430 N \u043f\u0430\u0442\u0447\u0435\u0439, \u0433\u0434\u0435 $N=\\frac{H * W}{P^2}$. \u0415\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 48x48, \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u0430\u0442\u0447\u0430 16x16, \u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f $N=\\frac{48 * 48}{16^2} = 9$."}}