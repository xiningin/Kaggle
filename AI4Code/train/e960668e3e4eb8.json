{"cell_type":{"05f433d4":"code","52b6edb3":"code","aec5e49a":"code","1b8c2c2f":"code","fdf0aa37":"code","434a5204":"code","a7e7e42f":"code","10afbebe":"code","3d009bb1":"code","89ce6665":"code","d2201575":"code","c4c47651":"code","fb30ee60":"code","a06e1739":"code","109ccda1":"code","a613328d":"code","b38e76ec":"code","adf94728":"code","1d7ca070":"code","7bb45f49":"code","2d9f999e":"code","7fffd8e6":"code","e8a5ec7f":"code","a417efac":"code","deefbc90":"markdown","06041a12":"markdown","7fd23509":"markdown","0913db97":"markdown","dfd8d546":"markdown","4271c9bc":"markdown","02de3b14":"markdown","310fbb87":"markdown","1487f731":"markdown","2d11bece":"markdown","ba24cf0b":"markdown","1a33cc54":"markdown"},"source":{"05f433d4":"# To get pi\nimport math\n\n# To do linear algebra\nimport numpy as np\n\n# To store data\nimport pandas as pd\n\n# To create nice plots\nimport seaborn as sns\n\n# To count things\nfrom collections import Counter\n\n# To create interactive plots\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\n# To handle tensors\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\n# To handle datasets\nfrom kaggle_datasets import KaggleDatasets\n\n# To create plots\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom matplotlib.colors import LinearSegmentedColormap\n\n\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","52b6edb3":"try:\n    # Detect the hardware\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    # Configure tensorflow to use TPUs\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # Get a strategy for distributing the model to TPUs\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default strategy to use CPUs or a GPU\n    strategy = tf.distribute.get_strategy()\n\n# Display number of model replicas\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n\n\n\n# Allow optimization\nMIXED_PRECISION = True\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","aec5e49a":"# Set batch size\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\n\n# Set number of epochs\nEPOCHS = 250\n\n# Patience for the learning rate\nLR_PATIENCE = 5\n\n# Patience for early stopping\nSTOPPING_PATIENCE = 30\n\n# Serialized array shape\nSHAPE = (600, 800, 4)\n\nR_SHAPE = (512, 512, 4)\n\n# Number of classes\nNUM_CLASSES = 13\n\n#Set number of images in buffer\nBUFFER_SIZE = 1024 \n\n# Set paths to the data\nPATH_DATA = 'semantic-segmentation-with-carla-and-tpus'\n\n# Allow self optimization\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Label of the classes\nCLASSES = {0:'Unlabeled',\n           1:'Building',\n           2:'Fence',\n           3:'Other',\n           4:'People',\n           5:'Posts',\n           6:'Road Marking',\n           7:'Street',\n           8:'Sidewalk',\n           9:'Vegatation',\n           10:'Vehicle',\n           11:'Wall',\n           12:'Traffic Sign'}\n\n# RGB colors of the classes\nCOLORS = [(80\/255, 168\/255, 250\/255),\n          (242\/255, 130\/255, 30\/255),\n          (50\/255, 50\/255, 50\/255),\n          (27\/255, 44\/255, 129\/255),\n          (163\/255, 68\/255, 222\/255),\n          (115\/255, 0\/255, 0\/255),\n          (255\/255, 255\/255, 255\/255),\n          (191\/255, 191\/255, 191\/255),\n          (150\/255, 150\/255, 150\/255),\n          (22\/255, 146\/255, 0\/255),\n          (245\/255, 239\/255, 46\/255),\n          (181\/255, 103\/255, 10\/255),\n          (235\/255, 0\/255, 0\/255)]","1b8c2c2f":"def loadDataset(filenames):\n    # Disable order to increase speed\n    options = tf.data.Options()\n    options.experimental_deterministic = False\n    \n    # Define a TFRecords dataset with all filenames\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    # Add options for the order to the dataset\n    dataset = dataset.with_options(options)\n    # Return a dataset\n    dataset = dataset.map(readTFRecord, num_parallel_calls=AUTO)\n    return dataset\n\n\n\ndef readTFRecord(example):\n    # Parse the serialized example\n    example = tf.io.parse_single_example(example, exampleStructure)\n    return example['raw_image']\n\n\n\n# Define structure of examples\nexampleStructure = {'raw_image': tf.io.FixedLenFeature([np.prod(SHAPE)], tf.int64)}\n\n\n# Set the path to all data files\ngcs_path = KaggleDatasets().get_gcs_path(PATH_DATA)\n\n# Get a list of all files from the training folder\ntrain_files = tf.io.gfile.glob(gcs_path + '\/data_train\/data_train\/*.tfrecords')\n# Get a list of all files from the testing folder\ntest_files = tf.io.gfile.glob(gcs_path + '\/data_test\/data_test\/*.tfrecords')\n\n\n# Create datasets from file names\ntrain_dataset = loadDataset(train_files)\ntest_dataset = loadDataset(test_files)","fdf0aa37":"def countLabels(dataset):\n    # Counter for the labels\n    label_counter = Counter()\n    \n    # Number of images\n    n_images = 0\n\n    # Iterate over the whole dataset\n    for image_tensor in dataset:\n        \n        # Count images\n        n_images += 1\n\n        # Get the label channel\n        flat_mask = tf.reshape(image_tensor, SHAPE)[:,:,3].numpy()\n\n        # Count the labels\n        x = np.bincount(flat_mask.flatten())\n        y = np.nonzero(x)[0]\n\n        # Update the counter\n        for key, val in zip(y,x[y]):\n            label_counter[key] += val\n            \n    # Return counted labels and number of images\n    return label_counter, n_images\n\n\n# Count labels in train- and test-dataset\ntrain_label_counts, n_train_images = countLabels(train_dataset)\ntest_label_counts, n_test_images = countLabels(test_dataset)\n\n\n# Create dataframe for the counted train labels\ndf_train_label_counts = pd.DataFrame.from_dict(train_label_counts, orient='index').rename({0:'Train'}, axis=1)\ndf_train_label_counts.index = [CLASSES[i] for i in df_train_label_counts.index]\n\n# Create dataframe for the counted test labels\ndf_test_label_counts = pd.DataFrame.from_dict(test_label_counts, orient='index').rename({0:'Test'}, axis=1)\ndf_test_label_counts.index = [CLASSES[i] for i in df_test_label_counts.index]\n\n\n# Combine the label counts dataframes\ndf_label_counts = df_train_label_counts.join(df_test_label_counts)\ndf_label_counts = df_label_counts.stack().to_frame().reset_index().rename({'level_0':'Label', 'level_1':'Dataset', 0:'Count'}, axis=1)\n\n\n##### Create Label Count Plot #####\n\ndf_tmp_train = df_label_counts[df_label_counts['Dataset']=='Train'].sort_values('Count', ascending=False)\ndf_tmp_test = df_label_counts[df_label_counts['Dataset']=='Test'].sort_values('Count', ascending=False)\n\n\n# Set up the data\ntrace1 = go.Bar(y = df_tmp_train['Label'],\n                x = df_tmp_train['Count'],\n                base = 0,\n                name = 'Train',\n                textposition = 'auto',\n                hovertemplate = 'Pixel:  %{x}<br>Label: %{y}',\n                width = 0.4,\n                marker = dict(color = '#cc3600'),\n                orientation='h',\n                opacity = 1.0)\n\ntrace2 = go.Bar(y = df_tmp_test['Label'],\n                x = df_tmp_test['Count'],\n                base = 0,\n                name = 'Test',\n                textposition = 'auto',\n                hovertemplate = 'Pixel:  %{x}<br>Label: %{y}',\n                width = 0.4,\n                marker = dict(color = '#cc9c00'),\n                orientation='h',\n                opacity = 1.0)\n\n# Set up the layout\nlayout = dict(title = 'How Many Pixels Are There Per Label?',\n              xaxis = dict(title = 'Count On Logscale',\n                           type='log'),\n              yaxis = dict(title = 'Label'),\n              font = dict(family = 'sans-serif',\n                          size = 18,\n                          color = '#2f2f2f'),\n              plot_bgcolor = '#dddddd')\n\n# Create the plot\nfig = go.Figure(data=[trace1, trace2], layout=layout)\niplot(fig)","434a5204":"# Create a colormap for the labels\ncm = LinearSegmentedColormap.from_list('semantic_map', \n                                       COLORS, \n                                       N=NUM_CLASSES)\n\n# Normalize the labels\nnorm = Normalize(vmin=0, vmax=12)\n\n# Setup subplots\nrows, cols = 5, 4\n\n# Create iterator for dataset tensors\ntensor_iterator = iter(train_dataset)\n\n# Create the subplots\nfig, arr = plt.subplots(rows, cols, figsize=(14, 14))\n\n# Iterate over the first images\nfor i in range(int(rows*cols\/2)):\n    \n    # Get the next data tensor\n    image_tensor = next(tensor_iterator)\n    \n    # Get the image and mask channels\n    flat_image = tf.reshape(image_tensor, SHAPE)[:,:,:3].numpy()\n    flat_mask = tf.reshape(image_tensor, SHAPE)[:,:,3].numpy()\n    \n    # Reshape the image and mask \n    image = flat_image.reshape((SHAPE[0], SHAPE[1], 3))\n    mask = flat_mask.reshape((SHAPE[0], SHAPE[1]))\n    \n    \n    # Populate the subplots\n    arr[i\/\/2][i*2%cols].imshow(image)\n    arr[i\/\/2][i*2%cols].set_title('Image: {}'.format(i))\n    arr[i\/\/2][i*2%cols].axis('off')\n    arr[i\/\/2][i*2%cols+1].imshow(mask, cmap=cm, norm=norm)\n    arr[i\/\/2][i*2%cols+1].set_title('Segmentation: {}'.format(i))\n    arr[i\/\/2][i*2%cols+1].axis('off')\nplt.show()","a7e7e42f":"def trainingDataset(dataset, augmentation=True):\n    # Reshape image tensor\n    dataset = dataset.map(reshapeImage, num_parallel_calls=AUTO)\n    \n    # Perform augmentation\n    if augmentation:\n        # Random horizontal flip\n        dataset = dataset.map(tf.image.random_flip_left_right, num_parallel_calls=AUTO)\n        # Random rotation, shear, zoom and shift\n        dataset = dataset.map(transformImage, num_parallel_calls=AUTO)\n    \n    # Split image and mask\n    dataset = dataset.map(splitImage, num_parallel_calls=AUTO)\n    \n    # Repeat the dataset \n    dataset = dataset.repeat()\n    # Set a buffersize to randomly choose images from\n    dataset = dataset.shuffle(BUFFER_SIZE)\n    # Set batchsize\n    dataset = dataset.batch(BATCH_SIZE)\n    # Prepare the next batch while training\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\n\ndef reshapeImage(tensor):\n    # Reshape serialized data to tensor\n    tensor = tf.reshape(tensor, SHAPE)\n    return tf.image.resize(tensor, (R_SHAPE[0], R_SHAPE[1]))\n\n\n\ndef splitImage(tensor):\n    # Slice image from tensor\n    image = tensor[:,:,:3]\n    # Slice mask from tensor\n    mask = tensor[:,:,3]\n    \n    # Cast and normalize the ints to floats in [0, 1] range\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image, mask\n\n\n\ndef transformImage(tensor):\n    # Get dimensions of image\n    dim_x, dim_y, dim_z = R_SHAPE\n    \n    # Get random values for the transformation\n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 15. * tf.random.normal([1],dtype='float32')\n    h_zoom = 1. + tf.random.normal([1],dtype='float32')\/5.\n    w_zoom = 1.33 + tf.random.normal([1],dtype='float32')\/5.\n    h_shift = 50. * tf.random.normal([1],dtype='float32')\n    w_shift = 100. * tf.random.normal([1],dtype='float32')\n  \n    # Get transformation matrix from random transformation values\n    m = transformationMatrix(rot, shr, h_zoom, w_zoom, h_shift, w_shift) \n\n    # Get a list of destination pixel indices\n    x = tf.repeat( tf.range(dim_x\/\/2, -dim_x\/\/2, -1), dim_y )\n    y = tf.tile( tf.range(-dim_y\/\/2,dim_y\/\/2), [dim_x] )\n    z = tf.ones([dim_x*dim_y], dtype='int32')\n    idx = tf.stack( [x, y, z] )\n    \n    # Rotate the destination pixels onto the origin pixels\n    idx2 = K.dot(m, tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -dim_x\/\/2+1, dim_x\/\/2)\n    \n    # Find origin pixel values\n    idx3 = tf.stack( [dim_x\/\/2-idx2[0,], dim_x\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(tensor, tf.transpose(idx3))\n    \n    # Return transformed image\n    return tf.reshape(d, R_SHAPE)\n\n\n\ndef transformationMatrix(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    ##### Create a single 3x3 transformation matrix from 4 individual transformations #####\n        \n    # Convert degrees to radians\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # Rotation matrix\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # Shear matrix\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # Zoom matrix\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # Shift matrix\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    # Combine all four transformation matrices into a single transformation matrix\n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\n\n\ndef testingDataset(dataset):\n    # Reshape image tensor\n    dataset = dataset.map(reshapeImage, num_parallel_calls=AUTO)\n    \n    # Split image and mask\n    dataset = dataset.map(splitImage, num_parallel_calls=AUTO)\n    \n    # Set batchsize\n    dataset = dataset.batch(BATCH_SIZE)\n    # Prepare the next batch while training\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\n\n# Get the training dataset\ntrain_dataset = trainingDataset(train_dataset, augmentation=True)\n\n# Create iterator for dataset tensors\ntrain_dataset_iterator = iter(train_dataset.unbatch())\n\n# Create the subplots\nfig, arr = plt.subplots(5, 4, figsize=(14, 14))\n\n# Iterate over the first images\nfor i in range(10):\n    \n    # Get the next image and mask\n    image, mask = next(train_dataset_iterator)\n    \n    \n    # Populate the subplots\n    arr[i\/\/2][i*2%4].imshow(image)\n    arr[i\/\/2][i*2%4].set_title('Image: {}'.format(i))\n    arr[i\/\/2][i*2%4].axis('off')\n    arr[i\/\/2][i*2%4+1].imshow(mask, cmap=cm, norm=norm)\n    arr[i\/\/2][i*2%4+1].set_title('Segmentation: {}'.format(i))\n    arr[i\/\/2][i*2%4+1].axis('off')\nplt.show()","10afbebe":"OUTPUT_CHANNELS = 13\n\n# def downsample(filters, size, apply_instancenorm=True):\n#     initializer = tf.random_normal_initializer(0., 0.02)\n#     gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n#     result = keras.Sequential()\n#     result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n#                              kernel_initializer=initializer, use_bias=False))\n\n#     if apply_instancenorm:\n#         result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n#     result.add(layers.LeakyReLU())\n\n#     return result","3d009bb1":"with strategy.scope():\n    base_model = tf.keras.applications.MobileNetV2(input_shape=[512, 512, 3], include_top=False)\n    \n    for layer in base_model.layers:\n        if isinstance(layer, tf.keras.layers.BatchNormalization):\n            # we do aggressive exponential smoothing of batch norm\n            # parameters to faster adjust to our new dataset\n            layer.momentum = 0.9","89ce6665":"layer_names = [\n    'block_1_expand_relu',   \n    'block_3_expand_relu',   \n    'block_6_expand_relu',   \n    'block_13_expand_relu',  \n    'out_relu',      \n]\nm_layers = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\nwith strategy.scope():\n    down_stack = keras.Model(inputs=base_model.input, outputs=m_layers)\n    down_stack.trainable = True","d2201575":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.3))\n\n    result.add(layers.ReLU())\n\n    return result","c4c47651":"def ResidualBlock(filters, size, pad='same'):\n    initializer1 = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    initializer2 = tf.random_normal_initializer(0., 0.02)\n    \n    model = tf.keras.Sequential()\n#     model.add(ReflectionPadding2D())\n    model.add(tf.keras.layers.Conv2D(filters,size, padding=pad, kernel_initializer=initializer1, use_bias=False))\n    model.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n    model.add(tf.keras.layers.Activation('relu'))\n#     model.add(ReflectionPadding2D())\n    model.add(tf.keras.layers.Conv2D(filters,size, padding=pad, kernel_initializer=initializer2, use_bias=False))\n    model.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))  \n    return model","fb30ee60":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.W3 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, features, x1, x2):\n        \n        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(x1) + self.W3(x2)))\n\n\n        score = self.V(attention_hidden_layer)\n\n        attention_weights = tf.nn.softmax(score, axis=-1)\n\n        context_vector = attention_weights * features\n        \n        return context_vector\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'W1': self.W1,\n            'W2': self.W2,\n            'W3': self.W3,\n            'V': self.V\n        })\n        return config\n","a06e1739":"# def Generator():\n#     inputs = layers.Input(shape=[512,512,3])\n\n#     # bs = batch size\n#     down_stack = [\n#         downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n#         downsample(128, 4), # (bs, 64, 64, 128)\n#         downsample(256, 4), # (bs, 32, 32, 256)\n#         downsample(512, 4), # (bs, 16, 16, 512)\n#         downsample(512, 4), # (bs, 8, 8, 512)\n#         downsample(512, 4), # (bs, 4, 4, 512)\n#         downsample(512, 4), # (bs, 2, 2, 512)\n#         downsample(1024, 4), # (bs, 1, 1, 512)\n#     ]\n    \n#     res_block = [\n#         ResidualBlock(1024,2, 'same'),\n#         ResidualBlock(1024,2, 'same'),\n#         ResidualBlock(1024,2, 'same'),\n#         ResidualBlock(1024,2, 'same'),\n#         ResidualBlock(1024,2, 'same'),\n#         ResidualBlock(1024,2, 'same'),\n#     ]\n    \n#     mid_res = [ [ResidualBlock(64,4, 'same'),ResidualBlock(64,4, 'same')], [ResidualBlock(128,4, 'same'),ResidualBlock(128,4, 'same')], \n#               [ResidualBlock(256,4, 'same'),ResidualBlock(256,4, 'same')], [ResidualBlock(512,4, 'same'),ResidualBlock(512,4, 'same')],\n#               [ResidualBlock(512,4, 'same'),ResidualBlock(512,4, 'same')], [ResidualBlock(512,4, 'same'),ResidualBlock(512,4, 'same')],\n#               [ResidualBlock(512,4, 'same'),ResidualBlock(512,4, 'same')], [0] ]\n\n#     up_stack = [\n#         upsample(1024, 4, apply_dropout=True), \n#         upsample(512, 4, apply_dropout=True), \n#         upsample(512, 4, apply_dropout=True), \n#         upsample(512, 4),  \n#         upsample(512, 4),  \n#         upsample(128, 4), \n#         upsample(64, 4), \n#     ]\n\n#     initializer = tf.random_normal_initializer(0., 0.02)\n\n#     last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n#                                   strides=2,\n#                                   padding='same',\n#                                   kernel_initializer=initializer, activation='softmax')\n\n\n\n#     x = inputs\n    \n#     initializer1 = tf.random_normal_initializer(0., 0.02)\n#     gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    \n#     x = tf.keras.layers.Conv2D(64, 1, kernel_initializer=initializer1)(x)\n    \n# #     x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\n\n#     # Downsampling through the model\n#     skips = []\n#     mid_skips = []\n#     for down, res_down in zip(down_stack, mid_res):\n#         x = down(x)\n#         x1 = x\n#         skips.append(x)\n#         if 0 not in res_down:\n#             for res in res_down:\n#                 x1 = res(x1)\n#             mid_skips.append(x1)\n        \n        \n#     for r in res_block:\n#         x1 = x\n#         x2 = r(x)\n        \n#         x = tf.keras.layers.add([x1, x2])\n\n#     skips = reversed(skips[:-1])\n#     mid_skips = reversed(mid_skips)\n\n#     # Upsampling and establishing the skip connections\n#     for up, skip, mid_skip in zip(up_stack, skips, mid_skips):\n#         x = up(x)\n#         x = layers.Concatenate()([x, skip, mid_skip])\n\n#     x = last(x)\n\n#     return keras.Model(inputs=inputs, outputs=x)","109ccda1":"def unet_model(output_channels=13):\n    \n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    \n    inputs = tf.keras.layers.Input(shape=[512, 512, 3])\n    \n    x = inputs\n    \n    at = BahdanauAttention(128)(x, x, x)\n    \n    x = tf.keras.layers.add([x, at])\n    \n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\n    \n    f_inputs = x\n    \n    mid_skips = []\n    \n    res_block = [\n        ResidualBlock(320,3, 'same'),\n        ResidualBlock(320,3, 'same'),\n        ResidualBlock(320,3, 'same'),\n#         ResidualBlock(2048,3, 'same'),\n#         ResidualBlock(1024,1, 'same'),\n#         ResidualBlock(1024,1, 'same'),\n    ]\n    \n    up_stack = [\n    upsample(512, 3),  # 4x4 -> 8x8\n    upsample(256, 3),  # 8x8 -> 16x16\n    upsample(128, 3),  # 16x16 -> 32x32\n    upsample(64, 3),   # 32x32 -> 64x64\n]\n    \n#     up_stack = [\n#     pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n#     pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n#     pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n#     pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n# ]\n\n\n\n\n    \n    mid_res = [ [ResidualBlock(64,3, 'same'),ResidualBlock(64,3, 'same')], [ResidualBlock(128,3, 'same'),ResidualBlock(128,3, 'same')], \n              [ResidualBlock(256,3, 'same'),ResidualBlock(256,3, 'same')],\n              [ResidualBlock(512,3, 'same'),ResidualBlock(512,3, 'same')], [0] ]    \n    \n    \n  # Downsampling through the model\n    skips = down_stack(x)\n#     x = skips[-1]\n\n    \n    \n    for skip, res_down in zip(skips, mid_res):\n        x = skip\n        x1 = x\n        if 0 not in res_down:\n            for res in res_down:\n                x1 = res(x1)\n                at = BahdanauAttention(128)(x1, x1, x1)\n                x1 = tf.keras.layers.add([x1, at])\n                x1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x1)\n                \n            mid_skips.append(x1)  \n            \n    temp = x\n    \n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    x = layers.Conv2D(320, 3, padding='same', kernel_initializer=initializer)(x)\n    \n    x = layers.LeakyReLU()(x)\n    \n    at = BahdanauAttention(128)(x, x, x)\n    \n    x = tf.keras.layers.add([x, at])\n    \n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\n\n    \n    for r in res_block:\n        x1 = x\n        x2 = r(x)\n        \n        x = tf.keras.layers.add([x1, x2])\n        at = BahdanauAttention(128)(x, x, x)\n        x = tf.keras.layers.add([x, at])\n        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)\n        \n    x = layers.Concatenate()([x, temp])\n    \n    at = BahdanauAttention(128)(x, x, x)\n    x = tf.keras.layers.add([x, at])\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    skips = reversed(skips[:-1])\n    mid_skips = reversed(mid_skips)\n\n\n  # Upsampling and establishing the skip connections\n    for up, skip, mid_skip in zip(up_stack, skips, mid_skips):\n        x = up(x)\n        at = BahdanauAttention(128)(x, mid_skip, skip)\n        x = tf.keras.layers.add([x, at])\n        x = tf.keras.layers.BatchNormalization()(x)\n        concat = tf.keras.layers.Concatenate()\n        x = concat([x, skip, mid_skip])\n        at = BahdanauAttention(128)(x, x, x)\n        x = tf.keras.layers.add([x, at])\n        x = tf.keras.layers.BatchNormalization()(x)\n\n  # This is the last layer of the model\n    last = tf.keras.layers.Conv2DTranspose(\n      32, 3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n    x = last(x)\n    \n    x = layers.LeakyReLU()(x)\n    \n    at = BahdanauAttention(128)(x, x, x)\n    x = tf.keras.layers.add([x, at])\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    conv_inp1 = ResidualBlock(32,3, 'same')(f_inputs)\n    conv_inp2 = ResidualBlock(32,3, 'same')(conv_inp1)\n    \n    at = BahdanauAttention(128)(x, conv_inp1, conv_inp2)\n    x = tf.keras.layers.add([x, at])\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = layers.Concatenate()([x, conv_inp1, conv_inp2])\n    \n    at = BahdanauAttention(128)(x, x, x)\n    x = tf.keras.layers.add([x, at])\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    x = layers.Conv2D(output_channels, 1, padding='same', kernel_initializer=initializer, activation='softmax')(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","a613328d":"# from tensorflow.keras.utils import get_custom_objects\n\n\n\n# # taken from https:\/\/www.kaggle.com\/marcosnovaes\/hubmap-3-unet-models-with-keras-cpu-gpu\/notebook\n# # dice coefficient \n\n# def diceCoefficient(y_true, y_pred, epsilon = 10 ** -7):\n#     y_true=K.flatten(y_true)\n#     y_pred=K.flatten(y_pred)\n#     intersection = K.sum(K.abs(y_true * y_pred))\n#     return (2. * intersection + epsilon) \/ (K.sum(K.square(y_true)) + K.sum(K.square(y_pred)) + epsilon)\n\n# def dice_loss(y_true, y_pred):\n#     loss = 1. - diceCoefficient(y_true, y_pred, epsilon = 1.)\n#     return loss\n\n# def custom_loss(y_true, y_pred):\n#     y_pred_sa = tf.argmax(y_pred, axis=-1)\n#     y_pred_sa = tf.cast(y_pred_sa, dtype=tf.float32)\n#     y_true = tf.cast(y_true, dtype=tf.float32)\n#     return dice_loss(y_true, y_pred_sa) + tf.keras.losses.MSE(y_true, y_pred_sa) + tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n\n# def diceCoefficient_metrics(y_true, y_pred, epsilon = 10 ** -7):\n#     y_pred = tf.argmax(y_pred, axis=-1)\n# #     y_true=K.flatten(y_true)\n# #     y_pred=K.flatten(y_pred)\n#     y_pred=tf.cast(K.flatten(y_pred), tf.float32)\n#     y_true=tf.cast(K.flatten(y_true), tf.float32)\n#     intersection = K.sum(K.abs(y_true * y_pred))\n#     return (2. * intersection + epsilon) \/ (K.sum(K.square(y_true)) + K.sum(K.square(y_pred)) + epsilon)\n\n\n# get_custom_objects().update({\"tf_loss\": custom_loss})","b38e76ec":"with strategy.scope():\n    model = unet_model(13)\n    optimizer = tf.keras.optimizers.Adam()\n    \n    model.compile(optimizer=optimizer, \n                  loss='sparse_categorical_crossentropy',\n                  metrics=['sparse_categorical_accuracy'])\n    ","adf94728":"model.summary()","1d7ca070":"tf.keras.utils.plot_model(model, show_shapes=True)","7bb45f49":"# Get the train- and test-dataset\ntrain_dataset = trainingDataset(loadDataset(train_files), augmentation=True)\ntest_dataset = testingDataset(loadDataset(test_files))\n\n\n# Setup callbacks\nlearning_rate = tf.keras.callbacks.ReduceLROnPlateau(patience=LR_PATIENCE, verbose=1, factor=0.5, min_delta=0.00001)\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=STOPPING_PATIENCE, verbose=1, restore_best_weights=True)\n\n\n# Fit the model to the data\nhistory = model.fit(train_dataset, \n                    steps_per_epoch = int(n_train_images \/ BATCH_SIZE),\n                    epochs = EPOCHS,\n                    callbacks = [early_stopping, learning_rate],\n                    validation_data = test_dataset,\n                    validation_steps = int(n_test_images \/ BATCH_SIZE))","2d9f999e":"# Get the training results\nacc = history.history['sparse_categorical_accuracy']\nval_acc = history.history['val_sparse_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nlr = history.history['lr']\n\n\n\n##### Plot Training And Validation Accuracy ######\n\n# Create template for hovertool\nhovertemplate = 'Sparse Accuracy: %{y:.3f}<br>Epoch: %{x}'\n\n# Set up the data\ntrace1 = go.Scatter(x = list(range(1, len(acc)+1)), \n                    y = acc, \n                    name = 'Training',\n                    marker = dict(color = '#cc3600'),\n                    hovertemplate = hovertemplate)\n\ntrace2 = go.Scatter(x = list(range(1, len(acc)+1)), \n                    y = val_acc, \n                    name = 'Testing',\n                    marker = dict(color = '#0033cc'),\n                    hovertemplate = hovertemplate)\n\n# Set up the layout\nlayout = go.Layout(title = 'Sparse Accuracy During The Training',\n                   font = dict(family = 'sans-serif',\n                               size = 14,\n                               color = '#2f2f2f'),\n                   xaxis = dict(title = 'Epoch'),\n                   yaxis = dict(title = 'Accuracy',\n                                type='log'),\n                   plot_bgcolor = '#ffdacc',\n                   hovermode='x')\n\n# Create the plot\nfig = go.Figure(data=[trace1, trace2], layout=layout)\niplot(fig)\n\n\n\n##### Plot Training And Validation Loss ######\n\n# Create template for hovertool\nhovertemplate = 'Loss:   %{y:.3f}<br>Epoch: %{x}'\n\n# Set up the data\ntrace1 = go.Scatter(x = list(range(1, len(loss)+1)), \n                    y = loss, \n                    name = 'Training',\n                    marker = dict(color = '#cc3600'),\n                    hovertemplate = hovertemplate)\n\ntrace2 = go.Scatter(x = list(range(1, len(loss)+1)), \n                    y = val_loss, \n                    name = 'Testing',\n                    marker = dict(color = '#0033cc'),\n                    hovertemplate = hovertemplate)\n\n# Set up the layout\nlayout = go.Layout(title = 'Loss During The Training',\n                   font = dict(family = 'sans-serif',\n                               size = 14,\n                               color = '#2f2f2f'),\n                   xaxis = dict(title = 'Epoch'),\n                   yaxis = dict(title = 'Loss',\n                                type='log'),\n                   plot_bgcolor = '#ffdacc',\n                   hovermode='x')\n\n# Create the plot\nfig = go.Figure(data=[trace1, trace2], layout=layout)\niplot(fig)\n\n\n\n##### Plot Learning Rate ######\n\n# Create template for hovertool\nhovertemplate = 'Learning Rate:  %{y:.3f}<br>Epoch: %{x}'\n\n# Set up the data\ntrace1 = go.Scatter(x = list(range(1, len(lr)+1)), \n                    y = lr, \n                    name = 'Training',\n                    marker = dict(color = '#cc3600'),\n                    hovertemplate = hovertemplate)\n\n# Set up the layout\nlayout = go.Layout(title = 'Learning Rate During The Training',\n                   font = dict(family = 'sans-serif',\n                               size = 14,\n                               color = '#2f2f2f'),\n                   xaxis = dict(title = 'Epoch'),\n                   yaxis = dict(title = 'Learning Rate',\n                                type='log'),\n                   plot_bgcolor = '#ffdacc',\n                   hovermode='x')\n\n# Create the plot\nfig = go.Figure(data=[trace1], layout=layout)\niplot(fig)","7fffd8e6":"# Create iterator for dataset tensors\ntest_dataset_iterator = iter(test_dataset.unbatch())\n\n# Create the subplots\nfig, arr = plt.subplots(5, 3, figsize=(30, 30))\n\n# Iterate over the first images\nfor i in range(5):\n    \n    # Get the next image and mask\n    image, mask = next(test_dataset_iterator)\n    \n    prediction_r = model.predict(np.expand_dims(image.numpy(), axis=0))[0]\n    prediction = tf.argmax(prediction_r, axis=2)\n    \n    \n    # Populate the subplots\n    arr[i][0].imshow(image)\n    arr[i][0].set_title('Image: {}'.format(i))\n    arr[i][0].axis('off')\n    arr[i][1].imshow(mask, cmap=cm, norm=norm)\n    arr[i][1].set_title('Segmentation: {}'.format(i))\n    arr[i][1].axis('off')\n    arr[i][2].imshow(prediction, cmap=cm, norm=norm)\n    arr[i][2].set_title('Prediction: {}'.format(i))\n    arr[i][2].axis('off')\nplt.show()","e8a5ec7f":"# # Get counter for all pixel-predictions\n# counter = Counter()\n\n# # Iterate over the whole test dataset\n# for images, masks in test_dataset.take(1):\n    \n#     # Get a prediction for the images\n#     predictions_r = model.predict(images)\n#     # Get the pixel labels\n#     predictions = tf.argmax(predictions_r, axis=3)\n    \n#     # Add the preditions to the counter\n#     counter.update(zip(masks.numpy().flatten(), predictions.numpy().flatten()))\n\n\n# # Extract data from counter\n# counter_data = [[i, j, value] for (i, j), value in counter.items()]\n\n# # Create dataframe from counter\n# df_counter = pd.DataFrame(counter_data, columns=['Truth', 'Prediction', 'Count'])\n\n# # Pivot dataframe to get confusion matrix\n# piv = pd.pivot_table(df_counter, index='Truth', columns='Prediction', values='Count').fillna(0)\n\n# # Add missing columns\n# for i in piv.index:\n#     if i not in piv.columns:\n#         piv[i] = 0\n\n# # Sort columns\n# piv = piv[piv.index]\n\n# # Rename the columns and indices\n# piv = piv.rename(CLASSES, axis=0)\n# piv = piv.rename(CLASSES, axis=1)\n\n\n# # Count correct pixel\n# num_correct_pixel = np.diag(piv).sum()\n\n# # Count all pixel\n# num_all_pixel = piv.sum().sum()\n\n# # Compute overall accuracy\n# accuracy = num_correct_pixel \/ num_all_pixel\n\n\n# # Normalize the dataframe\n# piv = piv.divide(piv.sum(axis=1), axis=0).round(3)\n\n\n# # Create a plot for the confusion matrix\n# plt.figure(figsize = (12, 12))\n# plt.title('Confusion Matrix With Overall Accuracy: {:.3f}'.format(accuracy))\n# sns.heatmap(piv, annot=True, cmap='binary')\n# plt.show()","a417efac":"model.save_weights('model.h5')","deefbc90":"## <a id=4>4. Load The Serialized Datasets<\/a>\n\nThe **tf-record files contain the serialized images and semantic masks** in a format optimized for TPUs.\nThe separate files are loaded as a Kaggle-Dataset and are broken down directly into the individual images when iterated.","06041a12":"The number of individual pixel labels is divided into three orders of magnitude.\nThis unbalanced distribution can lead to the strongly represented labels (Unlabeled, Street, Vehicle) suppressing the gradients of the rare categories (People, Traffic Signs).\n\n**Learning the structures of the rare categories can be difficult with this data set.**\n\n## <a id=6>6. What Do The Images And Masks Look Like?<\/a>\n\nA colored semantic mask is assigned to each RGB image.\nSince the following images have not been augmented, they **represent the raw training dataset**.","7fd23509":"It seems as if the model gets a **rough overview of the images very quickly**, but then fails because of too little data for the rare label.\n\n## <a id=11>11. How Does The Model Perform?<\/a>\n\nComparing some test images and creating the confusion matrix can show which labels pose a challenge.","0913db97":"## <a id=2>2. Ensure TPU Support<\/a>\n\nIn order to be able to use the speed of TPUs, they must be controlled explicitly with a strategy.\nFurther speed optimization can also be achieved with `mixed_precision` and` set_jit`.","dfd8d546":"## <a id=10>10. How Did The Training Process Go?<\/a>\n\nAn insight into the training process can be obtained with the **loss values and the accuracy during the training**.","4271c9bc":"## <a id=9>9. Start The Training Process<\/a>\n\nThe training process is monitored by `ReduceLROnPlateau` and as soon as a plateau is reached, the learning rate is reduced.\nIt is also accompanied by `EarlyStopping`, which is supposed to prevent overfitting.\n\nThanks to the TPUs, the training on the big augmented images can be accelerated enormously compared to the use of CPUs and GPUs.","02de3b14":"# How Can You Label Pixels In Images With TPUs And Semantic Segmentation?\n\n**In this notebook I will try to label pixels in images from the [CARLA simulator](https:\/\/carla.org\/) in order to find coherent, meaningful areas.**<br>\nFeel free to suggest new ideas for improvement and exploration.\n\nThe images in the dataset were taken during car trips in \"Town 1\" of the simulator in different weather conditions.\n32 such trips are included in the dataset.\nEach of the trips is made up of either 100 or 250 consecutive images.\n\n\"Leaking\" can occur because the images of individual trips can be very similar to one another and some sections of the map are rendered during several trips.\nFor this reason, three trips without overlapping with the training dataset were separated as test dataset.\n\n\n[1. Import Libraries](#1)    \n[2. Ensure TPU Support](#2)    \n[3. Set Up Training Variables](#3)       \n[4. Load The Serialized Datasets](#4)    \n[5. What Does The Label Distribution Look Like?](#5)    \n[6. What Do The Images And Masks Look Like?](#6)    \n[7. How Does The Augmentation Transform The Images And Masks?](#7)    \n[8. Define The Model](#8)    \n[9. Start The Training Process](#9)    \n[10. How Did The Training Process Go?](#10)  \n[11. How Does The Model Perform?](#11)    \n[12. How Can It All Be Summed Up?](#12)    \n\n\n\n## <a id=1>1. Import Libraries<\/a>","310fbb87":"## <a id=5>5. What Does The Label Distribution Look Like?<\/a>\n\nThe serialized tensors have a (600, 800, 4) shape.\n**The first three channels are the RGB channels** of the simulated image and **the fourth channel represents the label of the respective pixel.**\n\nIn order to get an overview of the number of the respective labels in the data records, all images are run through and the labels of the label channel are counted.","1487f731":"As was to be expected, the model can **roughly distinguish between the most common labels and the rare labels cannot prevail**.\n\n## <a id=12>12. How Can It All Be Summed Up?<\/a>\n\nThe **images are basically suitable for teaching the model a semantic segmentation**, but in order to achieve a high level of accuracy with rare labels or in general in unforeseen situations, there are by far **not enough images available**.\n\nThe trips of the datasets were always spatially separated in the simulator, which probably means that the simplest leakage could be prevented. However, for this reason the model cannot be evaluated on the entire map.\nA possible better approach would be to **create the training and testing datasets in different cities** (e.g. \"Town 1\" and \"Town 3\").\n\nAnother approach for higher accuracy is to **give the model several consecutive images at the same time for classification.** The temporal connection of the images and the 3-dimensional movement of the objects could then be used by the model.\n\n\n### Thanks for reading this notebook! Feel free to experiment with it and take it to the next level.\nHave a nice day!","2d11bece":"## <a id=7>7. How Does The Augmentation Transform The Images And Masks?<\/a>\n\nBecause only 4550 images are contained in the training dataset, **additional images are simulated with augmentation**.\nUsing a transformation matrix, random rotations, shifts, shearings, reflections and zooms are **carried out directly on the TPUs** for each image and its mask.\n\nThese transformed images are used to train the network.","ba24cf0b":"## <a id=8>8. Define The Model<\/a>\n\nThe model has a **basic [U-Net](https:\/\/en.wikipedia.org\/wiki\/U-Net) structure** that calculates features from the images with the help of convolutions and then calculates them back to the semantic mask.","1a33cc54":"## <a id=3>3. Set Up Training Variables<\/a>"}}