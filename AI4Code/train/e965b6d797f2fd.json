{"cell_type":{"7fd936e4":"code","4c2f6150":"code","8c5467c9":"code","0ce5d65b":"code","db2d5498":"code","6a790bf9":"code","b98ada5f":"code","824664d8":"code","ed47fed4":"code","48d0525b":"code","abe105e3":"code","c2987d76":"code","fbfbe5ea":"code","5e44c6f9":"code","09c596c0":"code","bcde4cff":"code","bbb8be1f":"code","21682b79":"markdown","074a709a":"markdown"},"source":{"7fd936e4":"!pip install pytorch-tabnet","4c2f6150":"import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nfrom tqdm import tqdm\nimport gc\n\nSEED = 0","8c5467c9":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\", index_col='id')","0ce5d65b":"pointy = [0,2,4,9,12,16,19,20,21,23,24,27,28,30,31,32,33,35,39,42,44,46,48,49,51,52,53,56,58,59,60,61,62,63,64,68,69,72,73,75,76,78,79,81,83,84,87,88,89,90,92,93,94,95,98,99]\nbimodal = [x for x in range(0,100) if x not in pointy]\n\npointy = list(map(lambda x: 'f'+str(x), pointy))\nbimodal = list(map(lambda x: 'f'+str(x), bimodal))\n\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]","db2d5498":"def create_features(df, cols, prefix='new_'):\n    df[prefix+'sum'] = df[cols].sum(axis=1)\n    df[prefix+'std'] = df[cols].std(axis=1)\n    df[prefix+'avg'] = df[cols].mean(axis=1)\n    df[prefix+'max'] = df[cols].max(axis=1)\n    df[prefix+'min'] = df[cols].min(axis=1)\n    \n    return df","6a790bf9":"train = create_features(train, pointy, 'point_')\ntrain = create_features(train, bimodal, 'bimodal_')\ntest = create_features(test, pointy, 'point_')\ntest = create_features(test, bimodal, 'bimodal_')","b98ada5f":"def check_peak(df, test_df, cols, suffix='_peak'):\n    for col in cols:\n        peak = threshold_otsu(df[col])\n        df[str(col)+suffix] = df[col] > peak\n        test_df[str(col)+suffix] = test_df[col] > peak","824664d8":"#check_peak(train, test, bimodal)","ed47fed4":"test.head()","48d0525b":"X = train.drop([\"target\"], axis=1)\nX_test = test\ny = train[\"target\"]","abe105e3":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","c2987d76":"import pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\nimport shutil\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score","fbfbe5ea":"x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","5e44c6f9":"shutil.make_archive('tabnet_model', 'zip', '..\/input\/nov-tabnet-pretrain\/test_pretrain2')\n\nloaded_pretrain = TabNetPretrainer()\nloaded_pretrain.load_model('.\/tabnet_model.zip')","09c596c0":"# define the model\nclf1_nopreproc = TabNetClassifier(n_steps=4,\n                                  optimizer_fn=torch.optim.Adam,\n                                  optimizer_params=dict(lr=2e-2),\n                                  scheduler_params={\"step_size\":2, # how to use learning rate scheduler\n                                                    \"gamma\":0.85},\n                                  scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                                  mask_type='entmax' # \"sparsemax\"\n                                  )\n\n# fit the model \nclf1_nopreproc.fit(\n    x_train,y_train,\n    eval_set=[(x_val, y_val)],\n    eval_name=['valid'],\n    eval_metric=['auc'],\n    max_epochs=1000, patience=30,\n    batch_size=8192, virtual_batch_size=4096,\n    num_workers=0,\n    weights=1,\n    drop_last=False,\n    from_unsupervised=loaded_pretrain\n)            ","bcde4cff":"preds = clf1_nopreproc.predict_proba(X_test)[:,1]","bbb8be1f":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv', index_col='id')\nsubmission['target'] = preds\nsubmission.to_csv('submission.csv')","21682b79":"## contribution 1\nI pretty much looked at whether the distribution was a unimodal point, or a bimodal distribution\nThen split the features according to which distribution it was under and then apply feature engineering to each","074a709a":"## contribution 2\nThe bimodal distributions clearly influence the target. We can create a boolean comparison as to which peak it sits under\n\nSee: www.kaggle.com\/realtimshady\/eda-feature-exploration"}}