{"cell_type":{"5e1b66ff":"code","76afe18f":"code","556269ea":"code","d66ea8da":"code","cc759214":"code","e2dcf65e":"code","561ad92f":"code","245dc21e":"code","74ed7176":"code","9e8dbee1":"code","8a0c623e":"code","25726354":"code","95bebe1a":"code","673f6262":"code","37403e02":"code","d5ea57cc":"code","6b66ddac":"code","a729f91b":"code","ff7b82c8":"code","26b48309":"code","52b0a615":"code","4d13108e":"code","d062a83f":"code","b210c2d2":"code","78040a25":"code","3fda4277":"code","89dd733a":"code","8cb92728":"code","654764ad":"markdown","4d101141":"markdown","e9a34189":"markdown","538af408":"markdown","af545128":"markdown","77f2e901":"markdown","99592d0b":"markdown","71f62f24":"markdown","2c01113e":"markdown","7942ac77":"markdown","93b9b0a2":"markdown","0165c4cb":"markdown","82f7152a":"markdown","d4b8fee0":"markdown","720e365c":"markdown","6bbbaf83":"markdown","5cc4d1c0":"markdown","66bf2eab":"markdown","df395be1":"markdown","7c04bcec":"markdown","2cd0104e":"markdown","c9382e54":"markdown","a9ecf64e":"markdown","23a4a2fa":"markdown","72ac9d6d":"markdown","12bb923f":"markdown","797b227b":"markdown","4757f5d3":"markdown","f2f3a686":"markdown"},"source":{"5e1b66ff":"import numpy as np\nimport pandas as pd\nimport itertools\nimport tqdm\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nimport PIL\nimport urllib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n                                  AnnotationBbox)\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nimport os\n\ndef crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. \n    Parameters\n    ----------\n    lag : int, default 0\n    datax, datay : pandas.Series objects of equal length\n\n    Returns\n    ----------\n    crosscorr : float\n    \"\"\"\n    return datax.corr(datay.shift(lag))\n\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nroot_path = '\/kaggle\/input\/tabular-playground-series-jul-2021\/'","76afe18f":"train = pd.read_csv(root_path + 'train.csv')\ntest = pd.read_csv(root_path + 'test.csv')","556269ea":"train.info(), test.info()","d66ea8da":"MIN_DATETIME_TRAIN = train.date_time.min()\nMAX_DATETIME_TRAIN = train.date_time.max()\n\nassert len(train.date_time.unique()) == len(pd.date_range(MIN_DATETIME_TRAIN, MAX_DATETIME_TRAIN, freq='1H')), \"There are gaps in train dates\"\n\nMIN_DATETIME_TEST = test.date_time.min()\nMAX_DATETIME_TEST = test.date_time.max()\n\nassert len(test.date_time.unique()) == len(pd.date_range(MIN_DATETIME_TEST, MAX_DATETIME_TEST, freq = '1H')), \"There are gaps in test dates\"","cc759214":"fig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n# Generate the training\/testing visualizations for each CV split\n\n# Fill in indices with the training\/test groups\ndates = pd.concat([train[['date_time']].assign(data='train'), test[['date_time']].assign(data = 'test')], axis = 0)\n\nindices = np.array([1] * len(dates))\nindices[dates.data == 'train'] = 1\nindices[dates.data == 'test'] = 0\n\n# Visualize the results\nax.scatter(range(len(train)), [.5] * len(train),\n           c=indices[indices==1], marker='_', lw=15, cmap=cmap_cv,\n           vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train), len(train)+len(test)), [1.] * len(test),\n           c=indices[indices==0], marker='_', lw=15, cmap=cmap_cv,\n           vmin=-.2, vmax=1.2)\n\ndate_col = dates['date_time']\n\nif date_col is not None:\n    tick_locations  = ax.get_xticks()\n    for i in (tick_locations)[1:-1]:\n        ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n    \n    tick_locations_str = [str(int(i)) for i in tick_locations]\n    ax.set_xticks(tick_locations)\n    ax.set_xticklabels(tick_dates, rotation = 35)\n    ax.grid()\n    #ax.set_yticklabels([])\n    ax.set(yticks=np.arange(2) + .5, yticklabels=[],\n           xlabel='date_time', ylabel=\"set\",\n           ylim=[0., 1.5])\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              ['Training set', 'Testing set'], loc=(1.02, .8))\nplt.suptitle(\"train.csv and test.csv date_time division\", fontsize = 20, fontweight = 'bold')\nplt.title(\"train: {}-{}\\t test: {} - {}\".format(MIN_DATETIME_TRAIN, MAX_DATETIME_TRAIN, MIN_DATETIME_TEST, MAX_DATETIME_TEST), fontsize = 10)","e2dcf65e":"feature_cols = [i for i in train.columns if all(x not in i for x in ['target', 'date_time'])]\ntarget_cols = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","561ad92f":"train[feature_cols + ['date_time']].merge(test[feature_cols + ['date_time']], on = 'date_time', suffixes = ('_train', '_test'))","245dc21e":"train.isna().sum(axis = 0).rename('Number_of_Nans').to_frame().transpose()","74ed7176":"index_col = 'date_time'\nN_BINS = 20\nplt.style.use('fivethirtyeight')\nfor feature in feature_cols[:1]:\n    \n    mean = round(train[feature].mean(), 2)\n    median = round(train[feature].median(),2)\n    st_dev = round(train[feature].std(), 2)\n    \n    fig,ax = plt.subplots(1, 1, figsize = (20, 6))\n    (train[[index_col, feature]].sample(500).sort_values(index_col, ignore_index = True).set_index(index_col)\n     .plot(lw = 2, linestyle = \"-.\", ax=ax, color = (0.31883238319215684, 0.4266050511215686, 0.8598574482039216)))\n    fig.suptitle('{} through time'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n\n\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n    y_max = Ys.max()\n    new_maxy = y_max*1.07\n\n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    if x_min < 0:\n        new_minx = x_min*0.8\n    else:\n        new_minx = x_min*1.2\n    x_max = Xs.max()\n    new_maxx = x_max*1.01\n\n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_minx, new_maxx)\n\n    ax.hlines(y = mean, xmin = x_min, xmax = x_max, colors='crimson',\n              linestyles='dashdot', label='mean', alpha = 0.3, linewidth = 3)\n    ax.text(x = x_max*0.9, y = train[feature].mean(), s = 'mean: {}'.format(mean))\n\n    fig.show()\n\n    fig,ax = plt.subplots(1, 1, figsize = (18, 6))\n    \n    if feature == 'deg_C':\n        hot = 'https:\/\/gilmour.com\/gilmour_map\/images\/256\/hot.png'\n        cold = 'https:\/\/cdn1.iconfinder.com\/data\/icons\/winter-37\/32\/thermometer_cold_snow_winter_weather_forecast_temperature-128.png'\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(hot, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.25)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [45, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(cold, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.35)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [0, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n    \n    fig.suptitle('{} distribution'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    percentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n\n    ax.grid()\n    \n    sns.histplot(data = train, x = feature, ax = ax, kde=False, bins = N_BINS, stat = 'density', \n                 alpha = 0.5, fill = True, linewidth = 3, edgecolor='black', color = 'red')\n    sns.kdeplot(data = train, x = feature, ax = ax, alpha = 0.01, fill = True, \n                linewidth = 3, color = 'blue')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n        \n    y_max = Ys.max()\n    new_maxy = y_max*1.1\n    \n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    \n    if x_min < 0:\n        new_xmin = x_min*0.7\n    else:\n        new_xmin = x_min*1.2\n        \n    x_max = Xs.max()\n    new_maxx = x_max*1.07\n    \n    ax.grid()\n    \n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_xmin-0.05, new_maxx)\n    ax.text(new_xmin, new_maxy*0.2, \"mean: {}\".format(mean), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.35, \"median: {}\".format(median), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.5, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n    \n    \n    percentiles_asked = [0.25, 0.5, 0.75]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n    for m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = '#FFC30B')\n        ax.text(percentile-0.16, new_maxy, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n    \n    fig.show()","9e8dbee1":"index_col = 'date_time'\nN_BINS = 20\nplt.style.use('fivethirtyeight')\nfor feature in feature_cols[1:]:\n    \n    mean = round(train[feature].mean(), 2)\n    median = round(train[feature].median(),2)\n    st_dev = round(train[feature].std(), 2)\n    \n    fig,ax = plt.subplots(1, 1, figsize = (20, 6))\n    (train[[index_col, feature]].sample(500).sort_values(index_col, ignore_index = True).set_index(index_col)\n     .plot(lw = 2, linestyle = \"-.\", ax=ax, color = (0.31883238319215684, 0.4266050511215686, 0.8598574482039216)))\n    fig.suptitle('{} through time'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n\n\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n    y_max = Ys.max()\n    new_maxy = y_max*1.07\n\n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    if x_min < 0:\n        new_minx = x_min*0.8\n    else:\n        new_minx = x_min*1.2\n    x_max = Xs.max()\n    new_maxx = x_max*1.01\n\n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_minx, new_maxx)\n\n    ax.hlines(y = mean, xmin = x_min, xmax = x_max, colors='crimson',\n              linestyles='dashdot', label='mean', alpha = 0.3, linewidth = 3)\n    ax.text(x = x_max*0.9, y = train[feature].mean(), s = 'mean: {}'.format(mean))\n\n    fig.show()\n\n    fig,ax = plt.subplots(1, 1, figsize = (18, 6))\n    \n    if feature == 'deg_C':\n        hot = 'https:\/\/gilmour.com\/gilmour_map\/images\/256\/hot.png'\n        cold = 'https:\/\/cdn1.iconfinder.com\/data\/icons\/winter-37\/32\/thermometer_cold_snow_winter_weather_forecast_temperature-128.png'\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(hot, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.25)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [45, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n        \n        arr_img = PIL.Image.open(urllib.request.urlopen(hot))\n        arr_img = plt.imread(cold, format='png')\n\n        imagebox = OffsetImage(arr_img, zoom=0.35)\n        imagebox.image.axes = ax\n\n        ab = AnnotationBbox(imagebox, xy = [0, 0.05],\n                        xybox=(30, 5),\n                        frameon = False,\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        pad=0.5,\n                        arrowprops=dict(\n                            arrowstyle=\"->\",\n                            connectionstyle=\"angle,angleA=0,angleB=90,rad=3\")\n                        )\n\n        ax.add_artist(ab)\n    \n    fig.suptitle('{} distribution'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    percentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n\n    ax.grid()\n    \n    sns.histplot(data = train, x = feature, ax = ax, kde=False, bins = N_BINS, stat = 'density', \n                 alpha = 0.5, fill = True, linewidth = 3, edgecolor='black', color = 'red')\n    sns.kdeplot(data = train, x = feature, ax = ax, alpha = 0.01, fill = True, \n                linewidth = 3, color = 'blue')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n        \n    y_max = Ys.max()\n    new_maxy = y_max*1.1\n    \n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    \n    if x_min < 0:\n        new_xmin = x_min*0.7\n    else:\n        new_xmin = x_min*1.2\n        \n    x_max = Xs.max()\n    new_maxx = x_max*1.07\n    \n    ax.grid()\n    \n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_xmin-0.05, new_maxx)\n    ax.text(new_xmin, new_maxy*0.2, \"mean: {}\".format(mean), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.35, \"median: {}\".format(median), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.5, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n    \n    \n    percentiles_asked = [0.25, 0.5, 0.75]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n    for m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = '#FFC30B')\n        ax.text(percentile-0.16, new_maxy, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n    \n    fig.show()","8a0c623e":"index_col = 'date_time'\nN_BINS = 20\nplt.style.use('fivethirtyeight')\nfor feature in target_cols:\n    \n    mean = round(train[feature].mean(), 2)\n    median = round(train[feature].median(),2)\n    st_dev = round(train[feature].std(), 2)\n    \n    fig,ax = plt.subplots(1, 1, figsize = (18, 6))\n    (train[[index_col, feature]].sample(500).sort_values(index_col, ignore_index = True).set_index(index_col)\n     .plot(lw = 2, linestyle = \"-.\", ax=ax, color = (0.31883238319215684, 0.4266050511215686, 0.8598574482039216)))\n    fig.suptitle('{} through time'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n\n\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n    y_max = Ys.max()\n    new_maxy = y_max*1.07\n\n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    if x_min < 0:\n        new_minx = x_min*0.8\n    else:\n        new_minx = x_min*1.2\n    x_max = Xs.max()\n    new_maxx = x_max*1.01\n\n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_minx, new_maxx)\n\n    ax.hlines(y = mean, xmin = x_min, xmax = x_max, colors='crimson',\n              linestyles='dashdot', label='mean', alpha = 0.3, linewidth = 3)\n    ax.text(x = x_max*0.9, y = train[feature].mean(), s = 'mean: {}'.format(mean))\n\n    fig.show()\n\n    fig,ax = plt.subplots(1, 1, figsize = (16, 6))\n    \n    fig.suptitle('{} distribution'.format(feature), fontsize = 20, color ='black', fontweight = 'bold')\n\n    percentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n\n    ax.grid()\n    \n    sns.histplot(data = train, x = feature, ax = ax, kde=False, bins = N_BINS, stat = 'density', \n                 alpha = 0.5, fill = True, linewidth = 3, edgecolor='black', color = 'red')\n    sns.kdeplot(data = train, x = feature, ax = ax, alpha = 0.01, fill = True, \n                linewidth = 3, color = 'blue')\n\n    Ys = ax.get_yticks()\n    y_min = Ys.min()\n    if y_min < 0:\n        new_miny = y_min*0.93\n    else:\n        new_miny = y_min*1.07\n        \n    y_max = Ys.max()\n    new_maxy = y_max*1.1\n    \n    Xs = ax.get_xticks()\n    x_min = Xs.min()\n    \n    if x_min < 0:\n        new_xmin = x_min*0.7\n    else:\n        new_xmin = x_min*1.2\n        \n    x_max = Xs.max()\n    new_maxx = x_max*1.07\n    \n    ax.grid()\n    \n    ax.set_ylim(new_miny, new_maxy)\n    ax.set_xlim(new_xmin-0.05, new_maxx)\n    ax.text(new_xmin, new_maxy*0.2, \"mean: {}\".format(mean), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.35, \"median: {}\".format(median), size = 12, alpha = 1)\n    ax.text(new_xmin, new_maxy*0.5, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n    \n    \n    percentiles_asked = [0.25, 0.5, 0.75]\n    percentiles = train[feature].quantile(percentiles_asked).tolist()\n    for m, percentile in enumerate(percentiles):\n        ax.axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = '#FFC30B')\n        ax.text(percentile-0.16, new_maxy, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n    \n    fig.show()","25726354":"for idx, feature in enumerate(feature_cols):\n    \n    if idx%4 == 0:\n        fig, axes = plt.subplots(2, 2, figsize = (20, 14))\n        ax = axes.ravel()\n\n    sns.kdeplot(x = train[feature], \n            ax = ax[idx%4], alpha = 0.25, fill = True, label = 'train', \n            linewidth = 3, color = 'blue')\n\n    sns.kdeplot(x = test[feature], \n            ax = ax[idx%4], alpha = 0.25, fill = True, label = 'test', \n            linewidth = 3, color = 'red')\n    \n    if idx%4 ==0:\n        ax[idx%4].legend(fontsize = 20, loc = 'upper right')\n\n    ax[idx%4].set_ylabel('Density', fontsize = 15)\n    ax[idx%4].set_title('')\n    fig.suptitle('Train vs Test Distribution comparison {}'.format(feature), \n             fontsize = 20, fontweight = 'bold')\n\n    plt.subplots_adjust(hspace = 0.6)","95bebe1a":"corr_df = train.drop('date_time', axis = 1).copy()\ncorr_matrix = round(corr_df.corr(), 2)\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ncolors = sns.color_palette('coolwarm', 16)\nlevels = np.linspace(-1, 1, 16)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n\nfig, axes = plt.subplots(1, 2, figsize = (18, 10), gridspec_kw={'width_ratios': [2, 1]})\nax = axes.ravel()\nmask_feature = np.triu(np.ones_like(corr_matrix[feature_cols].loc[feature_cols], dtype=bool))\nsns.heatmap(corr_matrix[feature_cols].loc[feature_cols], \n            mask = mask_feature,\n            annot=True, ax = ax[0], cbar=False,\n            cmap = cmap_plot, \n            norm = norm, annot_kws={\"size\": 15, \"color\": 'black', 'fontweight' : 'bold'})\nax[0].hlines(range(len(feature_cols)), *ax[0].get_xlim(), color = 'black')\nax[0].vlines(range(len(feature_cols)), *ax[0].get_ylim(), color = 'black')\n\nmask_target = np.triu(np.ones_like(corr_matrix[target_cols].loc[target_cols], dtype=bool))\nsns.heatmap(corr_matrix[target_cols].loc[target_cols], \n            mask = mask_target,\n            annot=True, ax = ax[1], \n            cmap = cmap_plot, \n            norm = norm, annot_kws={\"size\": 15, \"color\": 'black', 'fontweight' : 'bold'})\nax[1].hlines(range(len(target_cols)), *ax[0].get_xlim(), color = 'black')\nax[1].vlines(range(len(target_cols)), *ax[0].get_ylim(), color = 'black')\nax[0].set_title('Features', fontsize = 20)\nax[1].set_title('Targets', fontsize = 20)\n\nfig.suptitle('Correlation Matrix', \n             fontsize = 20, color = 'black', fontweight = 'bold')\nfig, ax = plt.subplots(1, 1, figsize = (20, 20))\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n            cmap = cmap_plot, \n            norm = norm, annot_kws={\"size\": 15, \"color\": 'black', 'fontweight' : 'bold'})\nax.hlines(range(len(corr_matrix.columns)), *ax.get_xlim(), color = 'black')\nax.vlines(range(len(corr_matrix.columns)), *ax.get_ylim(), color = 'black')\nax.xaxis.set_ticks_position('bottom')\nax.set_title('Distinct values for each variable', fontsize = 20)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 'vertical', fontsize = 15, color = 'black')\nax.set_yticklabels(ax.get_yticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax.xaxis.label.set_size(14)\n\ncircle_rad = 25  # This is the radius, in points\nax.plot(4.5, 9.5, 'o',\n        ms=circle_rad * 2, mec='red', mfc='none', mew=4)\n\ncircle_rad = 25  # This is the radius, in points\nax.plot(4.5, 5.5, 'o',\n        ms=circle_rad * 2, mec='blue', mfc='none', mew=4)\n\nfig.suptitle('Correlation Matrix for {}'.format('train.csv'), \n             fontsize = 20, color = 'black', fontweight = 'bold')\nplt.title(\"Circled highest and lowest correlation values\", fontsize = 12)\nfig.show()\n    ","673f6262":"colors = [(0.31883238319215684, 0.4266050511215686, 0.8598574482039216), \n          (0.810615674827451, 0.26879706171764706, 0.23542761153333333)]\n\nfor enum, feature in enumerate(feature_cols):\n    \n    if enum % 2 == 0:\n        fig, axes = plt.subplots(2, 3, figsize = (20, 12))\n        ax = axes.ravel()\n        \n    if enum == 0:\n        plt.suptitle(\"Autocorrelation Analysis for feature columns\", size = 20, fontweight='bold')\n    \n    series = pd.concat([train[['date_time', feature]], test[['date_time', feature]]], axis = 0).drop_duplicates(ignore_index=True)\n    \n    series = series.set_index('date_time')\n    \n    series.sample(1000).sort_index().plot(lw = 3, ax = ax[enum%2*3], title = feature, color = colors[enum%2], legend = False)\n    ax[0].set_xlabel('date_time')\n    ax[0].set_xticks([])\n    ax[3].set_xlabel('date_time')\n    ax[3].set_xticks([])\n    \n    #myFmt = matplotlib.dates.DateFormatter(\"%Y-%m-%d\")\n    #ax[enum*3].xaxis.set_major_formatter(myFmt)\n\n    acf_stat = acf(series.fillna(method = 'ffill'), nlags = 24)\n    pd.Series(acf_stat[1:]).plot(ax = ax[enum%2*3+1], color = colors[enum%2],\n                                 title = feature +' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1))\n\n    pacf_stat = pacf(series.fillna(method = 'ffill'), nlags = 24, method = 'ols')\n    pd.Series(pacf_stat[1:]).plot(ax = ax[enum%2*3+2], title = feature +' pacf', alpha = None, lw = 2, \n                                  linestyle = '-.', ylim=(-1,1), color = colors[enum%2],)","37403e02":"colors = [(0.31883238319215684, 0.4266050511215686, 0.8598574482039216), \n          (0.810615674827451, 0.26879706171764706, 0.23542761153333333),\n          '#FFCD00']\n\nfor enum, feature in enumerate(target_cols):\n    \n    if enum % 3 == 0:\n        fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n        ax = axes.ravel()\n    \n    plt.suptitle(\"Autocorrelation Analysis for target columns\", size = 20, fontweight='bold')\n    \n    series = train.set_index('date_time')[feature].copy()\n    \n    series.sample(1000).sort_index().plot(lw = 3, ax = ax[enum%3*3], title = feature, color = colors[enum%3], legend = False)\n    ax[0].set_xlabel('date_time')\n    ax[0].set_xticks([])\n    ax[3].set_xlabel('date_time')\n    ax[6].set_xlabel('date_time')\n    ax[3].set_xticks([])\n    ax[6].set_xticks([])\n\n    acf_stat = acf(series.fillna(method = 'ffill'), nlags = 24)\n    pd.Series(acf_stat[1:]).plot(ax = ax[enum%3*3+1], color = colors[enum%3],\n                                 title = feature +' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1))\n\n    pacf_stat = pacf(series.fillna(method = 'ffill'), nlags = 24, method = 'ols')\n    pd.Series(pacf_stat[1:]).plot(ax = ax[enum%3*3+2], title = feature +' pacf', alpha = None, lw = 2, \n                                  linestyle = '-.', ylim=(-1,1), color = colors[enum%3],)","d5ea57cc":"feature_df = (pd.concat([train[['date_time']+ feature_cols], test[['date_time']+ feature_cols]], axis = 0)\n              .drop_duplicates(ignore_index = True))\n\nautocorr_df_features = (pd.DataFrame(feature_df.drop('date_time', axis = 1).apply(lambda x: x.autocorr(), 0))\n                    .reset_index().rename(columns = {'index': 'column', 0: 'autocorrelation'})\n                    .sort_values('autocorrelation', ascending = False))\n\nautocorr_df_target = (pd.DataFrame(train[target_cols].apply(lambda x: x.autocorr(), 0))\n                    .reset_index().rename(columns = {'index': 'column', 0: 'autocorrelation'})\n                    .sort_values('autocorrelation', ascending = False))\n\nautocorr_df = (pd.concat([autocorr_df_features, autocorr_df_target], axis = 0)\n                    .sort_values('autocorrelation', ignore_index = True, ascending = False))\nautocorr_df['autocorrelation'] = autocorr_df['autocorrelation'].round(4)\n\ndel autocorr_df_features, autocorr_df_target\n\nfig, ax = plt.subplots(1, 2, figsize = (16, 8), gridspec_kw={'width_ratios': [2, 1]})\nfig.suptitle('Autocorrelation lag 1 for each feature and target')\nsns.barplot(x='autocorrelation', y='column', data=(autocorr_df), ax = ax[0], palette = 'coolwarm')\ny_labels = autocorr_df.column.tolist()\nax[0].set_yticklabels([])\nax[0].set_xticklabels([])\nt=0\nfor p in ax[0].patches:\n    width = p.get_width() \n    if width < 0.01:\n        ax[0].text(width,\n        p.get_y() + p.get_height() \/ 2, \n        '{:1.4f}'.format(width),\n        ha = 'left', \n        va = 'center')\n    else:\n        ax[0].text(width\/4, \n\n        p.get_y() + p.get_height() \/ 2, \n        '{} {:1.4f}'.format(y_labels[t], width),\n        ha = 'left',  \n        va = 'center',\n        color = 'black',\n        fontsize = 12)\n    t+=1\n    \nbbox=[-0.2, 0, 1.2, 0.9]\nax[1].axis('off')\nax[1].title.set_text('')\nccolors = plt.cm.BuPu(np.full(len(autocorr_df.columns), 0.1))\n\nmpl_table = ax[1].table(cellText = autocorr_df.values, bbox=bbox, colLabels=autocorr_df.columns, colColours=ccolors)\nmpl_table.auto_set_font_size(False)\nmpl_table.auto_set_column_width(col=list(range(len(autocorr_df.columns))))\nmpl_table.set_fontsize(14)\nplt.subplots_adjust(hspace = 0.6)\n","6b66ddac":"TOTAL_LAGS = 25\ntotal_lags = range(1, TOTAL_LAGS)\nfeatures = list(set(train.columns) - set(['date_time']))\ncombinations = list(itertools.product(features, features))\nCROSS_THRESHOLD = 0.5\n\ncross_corr = {}\n\nfor j in total_lags:\n    cross_corr[j] = []\n    for k in tqdm.tqdm(combinations):\n        cross_corr[j].append(crosscorr(train[k[0]], train[k[1]], lag = j))\n\ncross_corr = pd.DataFrame(cross_corr)\ncross_corr.columns = ['cross_correlation_lag_{}'.format(i) for i in range(1, TOTAL_LAGS)]\n\ncross_correlations = (pd.concat([pd.DataFrame(combinations).rename(columns = {0: 'first_feature', 1: 'second_feature'}),\n                                 pd.DataFrame(cross_corr)], 1))\n\ncross_correlations_melt = (pd.melt(cross_correlations, id_vars=['first_feature', 'second_feature'], \n                           value_vars=['cross_correlation_lag_{}'.format(i) for i in range(1, TOTAL_LAGS)],\n                           var_name = 'lag',\n                           value_name = 'cross_correlation')\n                          .assign(lag=lambda x: x.lag.str.replace('cross_correlation_lag_', \"\")))\n\n\ndef sort_features(x, y):\n\n    return tuple(sorted([x,y]))\n\ncross_correlations_melt[['pair_of_features']] = (cross_correlations_melt.apply(lambda x:sort_features(x.first_feature,\n                                                                                                          x.second_feature), 1))\n\ncross_correlations_melt['first_feature'] = cross_correlations_melt['pair_of_features'].apply(lambda x: x[0])\ncross_correlations_melt['second_feature'] = cross_correlations_melt['pair_of_features'].apply(lambda x: x[1])\n\ncross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                          \"__\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"__lag\" +\n                                               cross_correlations_melt['lag']\n                                              ).astype(str)\n\ncross_correlations_melt = cross_correlations_melt.drop_duplicates(['pair_of_features'], ignore_index=True)\n\ncross_correlations_melt = (cross_correlations_melt.loc[(abs(cross_correlations_melt.cross_correlation) > CROSS_THRESHOLD) & \n                                  (cross_correlations_melt.first_feature!= cross_correlations_melt.second_feature)]\n                              .reset_index(drop = True))","a729f91b":"display(cross_correlations_melt.sort_values('cross_correlation', ascending = False, ignore_index = True).head(5))","ff7b82c8":"display(cross_correlations_melt.sort_values('cross_correlation', ascending = True, ignore_index = True).head(5))","26b48309":"cols = ['sensor_2', 'target_benzene']\ndf_cross = train.copy()\ndf_cross['target_benzene__lag1'] = df_cross['target_benzene'].shift(1)\ndf_cross['target_benzene__lag1'] = (df_cross['target_benzene__lag1'] - df_cross['target_benzene__lag1'].mean())\/df_cross['target_benzene__lag1'].std()\n\ndf_cross['sensor_2'] = (df_cross['sensor_2'] - df_cross['sensor_2'].mean())\/df_cross['sensor_2'].std()\ndf_cross['sensor_3__lag1'] = df_cross['sensor_3'].shift(1)\ndf_cross['sensor_3__lag1'] = (df_cross['sensor_3__lag1'] - df_cross['sensor_3__lag1'].mean())\/df_cross['sensor_3__lag1'].std()\n\ndf_cross = df_cross.set_index('date_time')\n\nfig, axes = plt.subplots(2, 1, figsize = (14, 10))\nax = axes.ravel()\n\ndf_cross[['sensor_2', 'target_benzene__lag1']].plot(ax = ax[0], lw = 2, alpha = 0.5, linestyle = \"-.\")\n\n\n(df_cross[['sensor_2', 'sensor_3__lag1']].plot(ax = ax[1], lw = 2, \n                                                 linestyle = \"-.\",  alpha = 0.5, sharex=True))\n\nfig.suptitle('Positive (Above) vs Negative (Below) crosscorrelation')\n\nmyFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\nax[1].xaxis.set_ticks([])\nax[0].set_title('sensor_2 vs target_benzene__lag1: 0.824438 Correlation')\nax[1].set_title('sensor_2 vs sensor_3__lag1: -0.72352 Correlation')\nax[1].legend(loc=\"upper right\", bbox_to_anchor=(1.1,1.1))","52b0a615":"def plot_cv_indices(cv, n_splits, X, y, date_col = None):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize = (11, 7))\n    \n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii+1] * len(indices),\n                   c=indices, marker='_', lw=10, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n\n    # Formatting\n    yticklabels = list(range(n_splits))\n    \n    if date_col is not None:\n        tick_locations  = ax.get_xticks()\n        tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n\n        tick_locations_str = [str(int(i)) for i in tick_locations]\n        new_labels = ['\\n\\n'.join(x) for x in zip(list(tick_locations_str), tick_dates) ]\n        ax.set_xticks(tick_locations)\n        ax.set_xticklabels(new_labels)\n    \n    ax.set(yticks=np.arange(n_splits+2), #yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[5.5, 0]\n          )\n    #ax.set_yticklabels([\"\"] +range(1, n_splits+1)+[])\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              ['Testing set', 'Training set'], loc=(1.02, .8))\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    \nfrom sklearn.model_selection import KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, TimeSeriesSplit\ncvs = [KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit, TimeSeriesSplit]\nn_points = 100\nn_splits = 5\nX = np.random.randn(100, 10)\npercentiles_classes = [.1, .3, .6]\ny = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n\nfor i, cv in enumerate(cvs):\n    this_cv = cv(n_splits=n_splits)\n    plot_cv_indices(this_cv, n_splits, X, y, date_col=None)","4d13108e":"sample_submission = pd.read_csv(root_path +\"\/sample_submission.csv\")\nassert len(sample_submission) == len(test), 'Different number of values between sample_submission.csv and test.csv'","d062a83f":"train_length = len(train)-1 #I drop the last row, since already present in test\n\nfull_df = pd.concat([train[['date_time']+feature_cols].iloc[:-1], test[['date_time']+feature_cols]], axis = 0, ignore_index = True)\nassert len(full_df) == train_length + len(test)\n\nnew_feature_cols=[]\n\nfor feature_col in feature_cols:\n    new_feature = feature_col+\"_lag\"\n    full_df[new_feature] = full_df[feature_col].shift()\n    new_feature_cols.append(new_feature)","b210c2d2":"ts_fold = TimeSeriesSplit(n_splits = 5)","78040a25":"predictions = []\n\nfor train_idx, val_idx in tqdm.tqdm(ts_fold.split(X=full_df.iloc[:train_length], y=train.iloc[:train_length][target_cols])):\n    \n    preds = []\n    for target in target_cols:\n        lgbm_model = LGBMRegressor(max_depth = 10)\n        lgbm_model.fit(full_df.loc[train_idx, feature_cols+new_feature_cols], train.loc[train_idx, target].values)\n        preds.append(lgbm_model.predict(full_df.iloc[train_length:][feature_cols+new_feature_cols]))\n    \n    predictions.append(preds)","3fda4277":"preds = np.mean(predictions, axis = 0)","89dd733a":"sample_submission[target_cols] = preds.transpose()","8cb92728":"sample_submission.to_csv('sample_submission.csv', index = False)","654764ad":"Let's check whether there are gaps in our dates: ","4d101141":"no gaps! \n\nLet's see it graphically ","e9a34189":"Small datasets memory wise and no null present. All columns except `date_time` are `float64`","538af408":"**There is one timestamp of overlap between train and test: 2011-01-01 00:00:00**\n\nLet's check whether values correspond for that timestamp between train and test.","af545128":"<h6> Train Feature distributions: <\/h6>","77f2e901":"Train target distributions","99592d0b":"<h6> Most negatively correlated features <\/h6>","71f62f24":"<h5> AutoCorrelation Analysis <\/h5>","2c01113e":"<h5> CrossCorrelation Analysis <\/h5>","7942ac77":"<h6> Show some info about `train` and `test` <\/h6>","93b9b0a2":"<a id = \"files\"><\/a>\n<h4>Data Description<\/h4>\n\nIn this competition you are predicting the values of air pollution measurements over time, based on basic weather information (temperature and humidity) and the input values of 5 sensors.\n\nThe three target values to you to predict are: *target_carbon_monoxide, target_benzene, and target_nitrogen_oxides*\n\n<h4>Files<\/h4>\n\n`train.csv` - the training data, including the weather data, sensor data, and values for the 3 targets\n\n`test.csv` - the same format as train.csv, but without the target value; your task is to predict the value for each of these targets.\n\n`sample_submission.csv` - a sample submission file in the correct format.","0165c4cb":"Unhide to see all:","82f7152a":"<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>Exploratory Data Analysis<\/i><\/h4>","d4b8fee0":"<h6> An example of positive and negatively cross-correlated features <\/h6>","720e365c":"<h6> Train vs Test Feature distributions <\/h6>","6bbbaf83":"<a id = \"first_eda\"><\/a>","5cc4d1c0":"<a id = 'sub'><\/a>\n<h5> Example usage of Time Series Split for Sample Submission <\/h5>","66bf2eab":"**You can see they are exactly the same. So for the first test point we may also have the target values.** ","df395be1":"As you can see Time Series Split is the most appropriate one when dealing with time series data, like in this case. ","7c04bcec":"<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>First Exploration<\/i><\/h4>","2cd0104e":"<h6> Read Data <\/h6>","c9382e54":"Let's check whether there are nan values in our columns (we know from `.info()` there are none):","a9ecf64e":"**Personal take**: It seems that each feature\/target is correlated with himself at the previous timestamp (1 hour before), as we can see directly from the partial autocorrelation plot. Train and test were concatenated for feature columns (while target columns are populated just in train, of course). ","23a4a2fa":"<h2 style=\"background-color:#e6f7ff;\" align = 'center' > Tabular Playground Series July 2021 <\/h2>\n\nIn this notebook I'll perform some exploratory data analysis, trying to update it often. \n\n<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>Table of Contents<\/i><\/h4>\n\n- [Data Description](#files)\n- [First Exploration](#first_eda):\n    - general info about data\n    - train vs test date_time\n    \n- [Exploratory Data Analysis](#eda)\n\n    - features distributions (train, train vs test)\n    - correlation analysis\n    - auto-cross correlation analysis\n    - cross validation strategies\n\n**Under Construction**\n\n- [TimeSeriesSplit and Sample_Submission](#sub)\n\n\n*Versioning:*\n\nCheck Version 16 for just outputs.","72ac9d6d":"<h6> Most positively correlated features <\/h6>","12bb923f":"<h5> Types of CrossValidation Techniques <\/h5>\n\nLet's see different `sklearn.model_selection` Split generators and choose the one most suitable for Time Series data.  ","797b227b":"<h5> Correlation Matrix <\/h5> ","4757f5d3":"<a id = 'train'><\/a>","f2f3a686":"Let's build a training set, creating some lag 1 columns for our features (since autocorrelation is most at lag 1). "}}