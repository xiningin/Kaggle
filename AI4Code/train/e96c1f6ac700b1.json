{"cell_type":{"09a5a620":"code","4f97220c":"code","917ed063":"code","e261bc36":"code","08a37da0":"code","21843b6c":"code","fcd5910f":"code","5e99e165":"code","0c5848a9":"code","2c93e0c6":"code","5b682ffb":"code","b679a15f":"code","6d990b42":"code","8b2c3bac":"code","e4966de0":"code","19cc6f07":"code","30628432":"code","a9cb3f24":"code","bfc04876":"code","077cc97b":"code","b70c749f":"code","a9775471":"code","510db49d":"code","3f37b836":"code","e51087ad":"code","1b8cc6f7":"code","3e24e070":"code","5734c635":"code","d6e866b8":"code","425644e0":"code","a8c15016":"code","25e127e8":"code","9ebc62a9":"code","23e667ee":"code","c549fd41":"code","34496e4d":"code","beb263a4":"code","acf62270":"code","59a669af":"code","36df0147":"code","67ce4d6a":"code","fbe9f9e4":"code","e8d77bfe":"code","2ff75aa2":"code","cf27e0a1":"code","a8b705e6":"code","749be350":"code","1374ae40":"code","4271ec25":"code","e86523be":"code","36809c18":"code","44f975db":"code","6fde9e9a":"code","c8b5712e":"code","ca54f295":"code","5284da21":"code","fc6b6254":"code","816f1995":"code","ae20372f":"code","4ff7efc3":"code","8100e2f0":"code","fb31df89":"code","2665f268":"code","673828ec":"code","220e168e":"code","81d92564":"code","113c9f51":"code","218b0409":"code","8a819c1d":"code","b96b04ef":"code","802a743e":"code","66aa0036":"code","371f7d17":"code","6a626871":"code","a21e5bd6":"code","e5a5c791":"code","a9ae964d":"code","7e6cb34b":"code","e2ed780d":"code","a94f4381":"code","ffdf6ad3":"code","67c85957":"code","30f615c4":"code","5aba4be6":"code","4056d0ba":"code","97e2236c":"code","f009d1ac":"code","28f72144":"markdown","c4665b61":"markdown","b88627d2":"markdown","5ad1f829":"markdown","902c8f0d":"markdown","882e6b99":"markdown","e34f8a55":"markdown","4c716e88":"markdown","896617b3":"markdown","df38531c":"markdown","61565571":"markdown","e12f5ab9":"markdown","e7d22834":"markdown","b5ebde5d":"markdown"},"source":{"09a5a620":"import riiideducation\n# import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score, train_test_split, validation_curve, ShuffleSplit, RandomizedSearchCV\nfrom sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt","4f97220c":"# load all data\ntrain = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n                   nrows=5000000,\n                   dtype={'timestamp': 'int64',\n                          'user_id': 'int32',\n                          'content_id': 'int16',\n                          'content_type_id': 'int8',\n                          'task_container_id': 'int16',\n                          'answered_correctly':'int8',\n                          'prior_question_elapsed_time': 'float32',\n                          'prior_question_had_explanation': 'boolean'}\n                   )","917ed063":"n = len(train)\nn","e261bc36":"train.head()","08a37da0":"#reading in question df\nquestions_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv')","21843b6c":"# add num_tags as an additional feature","fcd5910f":"questions_df['num_tags'] = questions_df['tags'].apply(lambda x:len(x.split()) if pd.notna(x) else 0)\n","5e99e165":"questions_df.head()","0c5848a9":"questions_df = questions_df[['question_id','part','num_tags']]","2c93e0c6":"#reading in lecture df\nlectures_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/lectures.csv')","5b682ffb":"lectures_df.head()","b679a15f":"lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])","6d990b42":"lectures_df.head()","8b2c3bac":"part_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\n\ntypes_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]","e4966de0":"part_lectures_columns","19cc6f07":"# merge lecture features to train dataset\ntrain_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')","30628432":"train_lectures.head()","a9cb3f24":"print(len(train_lectures), len(train_lectures)\/n)","bfc04876":"# collect per user stats\nuser_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()","077cc97b":"# add boolean features\nfor column in user_lecture_stats_part.columns:\n    bool_column = column + '_boolean'\n    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)","b70c749f":"user_lecture_stats_part","a9775471":"#clearing memory\ndel(train_lectures)","510db49d":"#removing True or 1 for content_type_id which are not questions\n\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)","3f37b836":"train[(train.task_container_id == 9999)].tail()","e51087ad":"train[(train.content_type_id == False)].task_container_id.nunique()","1b8cc6f7":"#saving value to fillna\nelapsed_mean = train.prior_question_elapsed_time.mean()\n","3e24e070":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ngroup3 = group1 \/ group2","5734c635":"group3['avg_questions_seen'] = group3.avg_questions.cumsum()","d6e866b8":"group3.iloc[10].avg_questions_seen","425644e0":"results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_final.columns = ['answered_correctly_user']\n\nresults_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_final.columns = ['explanation_mean_user']","a8c15016":"results_u2_final.explanation_mean_user.describe()","25e127e8":"train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')","9ebc62a9":"results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\nresults_q_final.columns = ['quest_pct']","23e667ee":"results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\nresults_q2_final.columns = ['count']","c549fd41":"question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')\n","34496e4d":"question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","beb263a4":"question2.quest_pct = round(question2.quest_pct,5)","acf62270":"display(question2.head(), question2.tail())","59a669af":"train.head()","36df0147":"len(train)","67ce4d6a":"train.answered_correctly.mean()","fbe9f9e4":"prior_mean_user = results_u2_final.explanation_mean_user.mean()\nprior_mean_user","e8d77bfe":"train.loc[(train.timestamp == 0)].answered_correctly.mean()","2ff75aa2":"train.loc[(train.timestamp != 0)].answered_correctly.mean()","cf27e0a1":"train.drop(['timestamp', 'content_type_id', 'question_id', 'part', 'num_tags'], axis=1, inplace=True)","a8b705e6":"len(train)","749be350":"train.head()","1374ae40":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]\nlen(train) + len(validation)","4271ec25":"len(validation)","e86523be":"validation.answered_correctly.mean()","36809c18":"train.answered_correctly.mean()","44f975db":"results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_val.columns = ['answered_correctly_user']\n\nresults_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_val.columns = ['explanation_mean_user']","6fde9e9a":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","c8b5712e":"X.answered_correctly.mean()","ca54f295":"train.answered_correctly.mean()","5284da21":"results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_X.columns = ['answered_correctly_user']\n\nresults_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_X.columns = ['explanation_mean_user']","fc6b6254":"#clearing memory\ndel(train)","816f1995":"X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nX = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")\n\nX = pd.merge(X, user_lecture_stats_part, on=['user_id'], how=\"left\")","ae20372f":"validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nvalidation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")\n\nvalidation = pd.merge(validation, user_lecture_stats_part, on=['user_id'], how=\"left\")","4ff7efc3":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\nX.prior_question_had_explanation.fillna(False, inplace = True)\nvalidation.prior_question_had_explanation.fillna(False, inplace = True)\n\nvalidation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])","8100e2f0":"X.columns","fb31df89":"#reading in question df\n#question2 = pd.read_csv('\/kaggle\/input\/question2\/question2.csv)","2665f268":"content_mean = question2.quest_pct.mean()\n\nquestion2.quest_pct.mean()\n#there are a lot of high percentage questions, should use median instead?","673828ec":"#filling questions with no info with a new value\nquestion2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n\n\n#filling very hard new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n\n#filling very easy new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)","220e168e":"X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nvalidation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nX.part = X.part - 1\nvalidation.part = validation.part - 1","81d92564":"X.head()","113c9f51":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\nX.head()\n\ny_val = validation['answered_correctly']\nX_valdf = validation.drop(['answered_correctly'], axis=1)","218b0409":"# select columns for modeling","8a819c1d":"X_train = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n       'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n       'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n       'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean', 'num_tags',]]\nX_val = X_valdf[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n               'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n               'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n               'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n               'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean', 'num_tags',]]\n","b96b04ef":"\n# Filling with 0.5 for simplicity; there could likely be a better value\nX_train['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_train['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_train['quest_pct'].fillna(content_mean, inplace=True)\nX_train['num_tags'].fillna(0, inplace=True)\nX_train['part'].fillna(4, inplace = True)\nX_train['avg_questions_seen'].fillna(1, inplace = True)\nX_train['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_train['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n\nX_train['part_1'].fillna(0, inplace = True)\nX_train['part_2'].fillna(0, inplace = True)\nX_train['part_3'].fillna(0, inplace = True)\nX_train['part_4'].fillna(0, inplace = True)\nX_train['part_5'].fillna(0, inplace = True)\nX_train['part_6'].fillna(0, inplace = True)\nX_train['part_7'].fillna(0, inplace = True)\nX_train['type_of_concept'].fillna(0, inplace = True)\nX_train['type_of_intention'].fillna(0, inplace = True)\nX_train['type_of_solving_question'].fillna(0, inplace = True)\nX_train['type_of_starter'].fillna(0, inplace = True)\n#X_train['part_1_boolean'].fillna(0, inplace = True)\n#X_train['part_2_boolean'].fillna(0, inplace = True)\n#X_train['part_3_boolean'].fillna(0, inplace = True)\n#X_train['part_4_boolean'].fillna(0, inplace = True)\n#X_train['part_5_boolean'].fillna(0, inplace = True)\n#X_train['part_6_boolean'].fillna(0, inplace = True)\n#X_train['part_7_boolean'].fillna(0, inplace = True)\nX_train['type_of_concept_boolean'].fillna(0, inplace = True)\nX_train['type_of_intention_boolean'].fillna(0, inplace = True)\nX_train['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX_train['type_of_starter_boolean'].fillna(0, inplace = True)","802a743e":"X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_val['quest_pct'].fillna(content_mean,  inplace=True)\n\nX_val['part'].fillna(4, inplace = True)\nX_val['avg_questions_seen'].fillna(1, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)\nX_val['num_tags'].fillna(0, inplace=True)\nX_val['part_1'].fillna(0, inplace = True)\nX_val['part_2'].fillna(0, inplace = True)\nX_val['part_3'].fillna(0, inplace = True)\nX_val['part_4'].fillna(0, inplace = True)\nX_val['part_5'].fillna(0, inplace = True)\nX_val['part_6'].fillna(0, inplace = True)\nX_val['part_7'].fillna(0, inplace = True)\nX_val['type_of_concept'].fillna(0, inplace = True)\nX_val['type_of_intention'].fillna(0, inplace = True)\nX_val['type_of_solving_question'].fillna(0, inplace = True)\nX_val['type_of_starter'].fillna(0, inplace = True)\n#X_val['part_1_boolean'].fillna(0, inplace = True)\n#X_val['part_2_boolean'].fillna(0, inplace = True)\n#X_val['part_3_boolean'].fillna(0, inplace = True)\n#X_val['part_4_boolean'].fillna(0, inplace = True)\n#X_val['part_5_boolean'].fillna(0, inplace = True)\n#X_val['part_6_boolean'].fillna(0, inplace = True)\n#X_val['part_7_boolean'].fillna(0, inplace = True)\nX_val['type_of_concept_boolean'].fillna(0, inplace = True)\nX_val['type_of_intention_boolean'].fillna(0, inplace = True)\nX_val['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX_val['type_of_starter_boolean'].fillna(0, inplace = True)","66aa0036":"X_train_values = X_train.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_train_scaled = min_max_scaler.fit_transform(X_train_values)\nx_train_scaled","371f7d17":"X_val_values = X_val.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_val_scaled = min_max_scaler.fit_transform(X_val_values)\nx_val_scaled","6a626871":"y_train = y.values\ny_val = y_val.values","a21e5bd6":"x_train_scaled.shape, len(y_train)","e5a5c791":"from tensorflow.keras import models, Sequential\nfrom tensorflow.keras.layers import Embedding","a9ae964d":"dim = x_train_scaled.shape[1]\n","7e6cb34b":"dim","e2ed780d":"# Define simple fully_connected model\ndim = x_train_scaled.shape[1]\ninputs = tf.keras.Input(shape=(dim,))\nx = tf.keras.layers.Dense(16, activation='relu')(inputs)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(16, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nout = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_DNN = tf.keras.Model(inputs=inputs, outputs = out)\nmodel_DNN.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0002), loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.Precision()])\nmodel_DNN.summary()","a94f4381":"# Train\nepoch_num = 20\nhistory = model.fit(x_train_scaled, np.array(y_train), validation_data=(x_val_scaled, np.array(y_val)), batch_size=64, epochs=epoch_num)\n","ffdf6ad3":"# smaller learning rate showed better curve, but larger one e.g. 0.001 has better result","67c85957":"_ = plt.plot(history.epoch, history.history['loss'], c='r')\n_ = plt.plot(history.epoch, history.history['val_loss'], c='g')\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Loss\")\n#_ = plt.xlim(0, 1)  # limit axes for better visualization\n#_ = plt.ylim(0, 0.1)","30f615c4":"y_pred = model_DNN.predict(x_val_scaled)\ny_true = np.array(y_val)\nroc_auc_score(y_true, y_pred)","5aba4be6":"env = riiideducation.make_env()\n","4056d0ba":"iter_test = env.iter_test()","97e2236c":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n    \n    test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\")\n    test_df['part_1'].fillna(0, inplace = True)\n    test_df['part_2'].fillna(0, inplace = True)\n    test_df['part_3'].fillna(0, inplace = True)\n    test_df['part_4'].fillna(0, inplace = True)\n    test_df['part_5'].fillna(0, inplace = True)\n    test_df['part_6'].fillna(0, inplace = True)\n    test_df['part_7'].fillna(0, inplace = True)\n    test_df['type_of_concept'].fillna(0, inplace = True)\n    test_df['type_of_intention'].fillna(0, inplace = True)\n    test_df['type_of_solving_question'].fillna(0, inplace = True)\n    test_df['type_of_starter'].fillna(0, inplace = True)\n    #test_df['part_1_boolean'].fillna(0, inplace = True)\n    #test_df['part_2_boolean'].fillna(0, inplace = True)\n    #test_df['part_3_boolean'].fillna(0, inplace = True)\n    #test_df['part_4_boolean'].fillna(0, inplace = True)\n    #test_df['part_5_boolean'].fillna(0, inplace = True)\n    #test_df['part_6_boolean'].fillna(0, inplace = True)\n    #test_df['part_7_boolean'].fillna(0, inplace = True)\n    test_df['type_of_concept_boolean'].fillna(0, inplace = True)\n    test_df['type_of_intention_boolean'].fillna(0, inplace = True)\n    test_df['type_of_solving_question_boolean'].fillna(0, inplace = True)\n    test_df['type_of_starter_boolean'].fillna(0, inplace = True)\n    test_df['num_tags'].fillna(0, inplace = True)\n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n    test_df['part'] = test_df.part - 1\n\n    test_df['part'].fillna(4, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    test_df_train = test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n            'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n            'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n            #'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n            'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean','num_tags',]]\n    \n    test_df_values = test_df_train.values #returns a numpy array\n    min_max_scaler = preprocessing.MinMaxScaler()\n    test_df_values = min_max_scaler.fit_transform(test_df_values)\n\n    test_df['answered_correctly'] =  model_DNN.predict(test_df_values)\n    #print(test_df)\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","f009d1ac":"#students don't appear in every task container ID what can I do about this, can't always follow sequentially?","28f72144":"Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\nI am convinced there is a better way to match the test data.","c4665b61":"# use lecture","b88627d2":"Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions.","5ad1f829":"# Modeling with DNN","902c8f0d":"## Reading Data and Importing Libraries ##","882e6b99":"This notebook is mostly based on https:\/\/www.kaggle.com\/takamotoki\/lgbm-iii-part3-adding-lecture-features and modified with machine learning method: Deep Learning\n","e34f8a55":"## Extracting Training Data ##","4c716e88":"## Making Predictions for New Data ##","896617b3":"# Data Normalization","df38531c":"## Merging Data ##","61565571":"# Training Dataset","e12f5ab9":"## Data Exploration ##","e7d22834":"2% is lecture","b5ebde5d":"## Creating Validation Set (Most Recent Answers by User) ##"}}