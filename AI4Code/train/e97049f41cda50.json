{"cell_type":{"cff6e03f":"code","875c0cd7":"code","a2db3ac3":"code","7cdf9238":"code","0e209854":"code","b6c74947":"code","93af214f":"code","80fdd314":"code","cf8733fe":"code","bd54befb":"code","7c20a05f":"code","97985b77":"code","7f88073a":"code","130387aa":"code","2cd1974b":"code","2801b93c":"code","cd7da2e0":"code","60509f7c":"code","dfafe0c1":"code","52a97d91":"code","77f506c0":"code","6ef17a28":"code","ef4eef73":"code","9f76c62c":"code","df95daa5":"code","5d8ee679":"code","dcfae674":"code","1ddc6810":"code","9b834b08":"markdown","9ee9c80d":"markdown","2afdfb25":"markdown","36c0ab27":"markdown","b360df8b":"markdown","abdf1fb1":"markdown","36f07fa9":"markdown","b47b7a1b":"markdown","9a88ba5f":"markdown","2d5b360e":"markdown","5be6780a":"markdown","e7bf2d63":"markdown","e016ac85":"markdown","ac0ccea2":"markdown","60ab8c4a":"markdown","186ef2cf":"markdown"},"source":{"cff6e03f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport copy\nfrom collections import Counter\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport nltk, re, string, collections\nfrom nltk.util import ngrams # function for making ngrams\n\nimport random\nimport token\n# import bs4 as bs\n# import urllib.request\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom wordcloud import WordCloud, STOPWORDS","875c0cd7":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndisplay(train.head())\ndisplay(test.head())","a2db3ac3":"print('train.text NA', train.text.isna().sum())\nprint('test.text NA', test.text.isna().sum())","7cdf9238":"train_text = train[['text']]\ntest_text = test[['text']]","0e209854":"combined_dataset = pd.concat(objs = [train_text, test_text], axis = 0)\nprint('combined', combined_dataset.shape)","b6c74947":"combined_dataset['text'] = combined_dataset['text'].str.replace(\"[^a-zA-Z#]\", \" \")\ncombined_dataset['text'] = combined_dataset['text'].apply(lambda x : \" \".join(x.lower() for x in x.split()) )\ncombined_dataset['text'] = combined_dataset['text'].apply(lambda x: x.replace('#',''))","93af214f":"# Removing URLs, html tags, and emojis\nexample =\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started <div> <h1>Real or Fake<\/h1> <p>Kaggle <\/p> <\/div> I am a #king Omg another Earthquake \ud83d\ude14\ud83d\ude14\"\nre_list = ['https?:\/\/\\S+|www\\.\\S+', '<.*?>', \n                           \"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                              u\"\\U00002702-\\U000027B0\"\n                              u\"\\U000024C2-\\U0001F251\"\n                           \"]+\"]\nconcat_re = re.compile( '|'.join( re_list) ).sub(f'', example)\nconcat_re","80fdd314":"# Sample test on dataframe\ndf = {'Text':['https:\/\/www.kaggle.com\/c\/nlp-getting-started co', '<div> <h1>Sharingan or Byakugan<\/h1>', 'Summoning Jutsu \ud83d\ude14\ud83d\ude14']}\ndf = pd.DataFrame(df)\nprint('Before')\ndisplay(df)\ndf['Text'] = df['Text'].apply(lambda x: re.compile( '|'.join( re_list)).sub(f'', x))\nprint('After')\ndisplay(df)","cf8733fe":"# Applying on the combined dataset\ncombined_dataset['text'] = combined_dataset['text'].apply(lambda x: re.compile( '|'.join( re_list)).sub(f'', x))","bd54befb":"# Removing punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I ***will become a H^&o*#k--,a...#g#e ##\"\nprint(remove_punct(example))","7c20a05f":"# Applying on the combined dataset\ncombined_dataset['text'] = combined_dataset['text'].apply(lambda x : remove_punct(x))","97985b77":"train_text_len = len(train_text)\ntrain_text = copy.copy(combined_dataset[:train_text_len])\ntest_text = copy.copy(combined_dataset[train_text_len:])\nprint('combined dataset ', combined_dataset.shape)\nprint('train_text        ', train_text.shape)\nprint('test_text         ', test_text.shape)","7f88073a":"stopwords = stopwords.words('english')\nstopwords = set(STOPWORDS)","130387aa":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='orange',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=40, \n        scale=3,\n        #random_state=1\n                         ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(20, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.4)\n\n    plt.imshow(wordcloud,interpolation=\"bilinear\")\n    plt.show()","2cd1974b":"show_wordcloud(train_text, \"Commonly occuring words - train_text\")\nshow_wordcloud(train_text, \"Commonly occuring words - test\")","2801b93c":"# Train: Converting the entries to a list\ntrain_text_list = train_text['text'].tolist()\ntrain_text_list","cd7da2e0":"# Train: Creating a counter for the types of entries under text and printing the occurrences\nCounter = Counter(train_text_list) \nmost_occurences = Counter.most_common()  \nprint(most_occurences) ","60509f7c":"# Train: Converting the occurrences into a dataframe\npd.set_option('display.max_colwidth', 150)\ntrain_text_summary = pd.DataFrame(most_occurences, columns = ['Content' , 'Count']) \ntrain_text_summary.head(10)","dfafe0c1":"with open('train_text.txt', 'w') as f:\n    for item in train_text_list:\n        f.write(\"%s\\n\" % item)","52a97d91":"# Train: reading input file; the encoding is specified here \nfile = open('train_text.txt', encoding=\"utf8\")\na= file.read()\n\n# Stopwords\n# stopwords = set(line.strip() for line in open('stopwords.txt')) - if you have custom defined txt file of stop words\nstopwords = stopwords\nstopwords = stopwords.union(set(['mr','mrs','one','two','said','http','https','a','b','c','d','e','f','g','h','i','j','k',\n                                 'l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','co','via','re']))\n# Instantiate a dictionary, and for every word in the file, \n# Add to the dictionary if it doesn't exist. If it does, increase the count.\nwordcount = {}\n# To eliminate duplicates, remember to split by punctuation, and use case demiliters.\nfor word in a.lower().split():\n    word = word.replace(\".\",\"\")\n    word = word.replace(\",\",\"\")\n    word = word.replace(\":\",\"\")\n    word = word.replace(\"\\\"\",\"\")\n    word = word.replace(\"!\",\"\")\n    word = word.replace(\"\u00e2\u20ac\u0153\",\"\")\n    word = word.replace(\"\u00e2\u20ac\u02dc\",\"\")\n    word = word.replace(\"*\",\"\")\n    if word not in stopwords:\n        if word not in wordcount:\n            wordcount[word] = 1\n        else:\n            wordcount[word] += 1\n# Print most common word\n# n_print = int(input(\"How many most common words to print: \")) - Use this to get the option to enter the number of most common words you want to be populated\n# Disabled for kernel commit purpose\n\nprint(\"\\nOK. The {} most common words are as follows\\n\".format(10))\nword_counter = collections.Counter(wordcount)\nfor word, count in word_counter.most_common(10):\n    print(word, \": \", count)\n# Close the file\nfile.close()","77f506c0":"# Train: Visual of above selection\nlst = word_counter.most_common(10)\ndf = pd.DataFrame(lst, columns = ['Word', 'Count'])\ncolors = list('rgbkymc')\ndf.plot.barh(x='Word',y='Count', color = colors, figsize = (20,8))","6ef17a28":"def listToString(l):  \n    # initialize an empty string \n    s = \" \" \n    # return string   \n    return (s.join(l))\n\nstring = listToString(train_text_list)","ef4eef73":"# Train: getting individual words\ntokenized = string.split()\ntokenized","9f76c62c":"train_text_filtered = [w for w in tokenized if not w in stopwords] \n  \ntrain_text_filtered = [] \n  \nfor w in tokenized: \n    if w not in stopwords: \n        train_text_filtered.append(w) \n  \nprint(train_text_filtered) ","df95daa5":"# Train\n# Getting list of all the bi-grams\ntrain_bigrams = ngrams(train_text_filtered, 2)\ntrain_bigrams\n\n# Getting the frequency of each bigram\ntrain_bigrams_freq = collections.Counter(train_bigrams)\n\n# The ten most popular bigrams\ntrain_bigrams_freq.most_common(10)","5d8ee679":"train_trigrams = ngrams(train_text_filtered, 3)\ntrain_trigrams\n\ntrain_trigrams_freq = collections.Counter(train_trigrams)\n\ntrain_trigrams_freq.most_common(10)","dcfae674":"train_quadgrams = ngrams(train_text_filtered, 4)\ntrain_quadgrams\n\ntrain_quadgrams_freq = collections.Counter(train_quadgrams)\n\ntrain_quadgrams_freq.most_common(10)","1ddc6810":"train_pentagrams = ngrams(train_text_filtered, 5)\ntrain_pentagrams\n\ntrain_pentagrams_freq = collections.Counter(train_pentagrams)\n\ntrain_pentagrams_freq.most_common(10)","9b834b08":"![babysteps.jpg](attachment:babysteps.jpg)","9ee9c80d":"### b. Trigrams","2afdfb25":"## 5. Most Occurences - you get to choose <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","36c0ab27":"# Acknowledgements\n\nkernels: \n* https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert (Including structuring of the kernel and sub-references. Thank you very much @vbmokin )\n\nOther resources:\n* https:\/\/towardsdatascience.com\/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n* https:\/\/gist.github.com\/balachandrapai\/aaef66de224e358ed5b03184062ad67c","b360df8b":"### c. Quadgrams (nom. chk please)","abdf1fb1":"## 4. Word Cloud <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","36f07fa9":"### As Counter is immutable not able to add something similar for test data set. Exploration continues....","b47b7a1b":"## 3. Data Cleaning <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","9a88ba5f":"## 6. Weight? - nGrams <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","2d5b360e":"# Basic of the Basic....my baby steps towards NLP:\n* Basic cleaning and tokenizing  \n* Option to enter how making frequently occuring words you want to see\n* nGrams","5be6780a":"![feedme.png](attachment:feedme.png)","e7bf2d63":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. Import libraries\n1. [Reading files and checking NAs](#2)\n1. [Data Cleaning](#3)\n1. [Word Cloud](#4)\n1. [Most Occurences - you get to choose](#5)\n1. [Weight? - nGrams](#6)","e016ac85":"## 2. Reading files and checking NAs <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","ac0ccea2":"### a. Bigrams","60ab8c4a":"### d. Pentagrams","186ef2cf":"## 1. Import libraries"}}