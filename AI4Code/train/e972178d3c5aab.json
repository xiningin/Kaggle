{"cell_type":{"8a64797a":"code","fb299a62":"code","818ce5b0":"code","44669012":"code","97500ba5":"code","a9c2df17":"code","a53ca9f2":"code","16c6a4a2":"code","bcfbdde7":"code","287eb4fc":"code","c5b19837":"code","2f856873":"code","08683e6f":"code","e7bf69cb":"code","06f26b3a":"code","3e45268b":"code","fba9ba62":"code","e4b7337e":"code","88f9b4d9":"markdown","39a970ca":"markdown","9b6c1cd0":"markdown","d4ccf63f":"markdown","4ef727bd":"markdown","280270d4":"markdown","16f70fcc":"markdown","7e84815c":"markdown","3d1823f1":"markdown","6aaf577b":"markdown","740ad4b1":"markdown","4da8cd36":"markdown","8d8b30cb":"markdown","e9f410d8":"markdown","a4b80406":"markdown","7655c0e2":"markdown"},"source":{"8a64797a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nRAW_DATA = \"..\/input\"\nhist = pd.read_csv(f'{RAW_DATA}\/historical_transactions.csv', low_memory=False, \n                     parse_dates=[\"purchase_date\"])\nnew =  pd.read_csv(f'{RAW_DATA}\/new_merchant_transactions.csv', low_memory=False, \n                     parse_dates=[\"purchase_date\"])\n\ntrain = pd.read_csv(f'{RAW_DATA}\/train.csv', low_memory=False, \n                     parse_dates=[\"first_active_month\"])\n\ntest = pd.read_csv(f'{RAW_DATA}\/test.csv', low_memory=False, \n                     parse_dates=[\"first_active_month\"])","fb299a62":"hist.purchase_date.min(), hist.purchase_date.max(),new.purchase_date.min(), new.purchase_date.max()","818ce5b0":"from dateutil import rrule\nlen(list(rrule.rrule(rrule.MONTHLY, dtstart=hist.purchase_date.min(), until=hist.purchase_date.max())))","44669012":"len(list(rrule.rrule(rrule.MONTHLY, dtstart=new.purchase_date.min(), until=new.purchase_date.max())))","97500ba5":"len(list(rrule.rrule(rrule.MONTHLY, dtstart=new.purchase_date.min(), until=hist.purchase_date.max())))","a9c2df17":"hist['month']= [x.strftime(\"%y-%m\") for x in hist.purchase_date]\ncounts = hist.month.value_counts().sort_index()\ncounts.plot(kind='bar')","a53ca9f2":"new['month']= [x.strftime(\"%y-%m\") for x in new.purchase_date]\ncounts = new.month.value_counts().sort_index()\ncounts.plot(kind='bar')","16c6a4a2":"hist.month_lag.min(), hist.month_lag.max() , new.month_lag.min(), new.month_lag.max()","bcfbdde7":"tmp = new.loc[new.month_lag == 1, 'purchase_date'].value_counts() < 100\nnew.loc[tmp.values, 'purchase_date'].min(), new.loc[tmp.values, 'purchase_date'].max()","287eb4fc":"pd.DataFrame(new.loc[new.month_lag == 1, 'month'].value_counts()).sort_index().plot(\n    title=\"Number of Transactions with month_lag == 1\")","c5b19837":"new.groupby(['month', 'month_lag']).agg({'card_id': pd.Series.nunique }).T","2f856873":"cnts = new.groupby([ 'month_lag', 'month']).agg({'card_id': pd.Series.nunique, 'purchase_date' : 'count'})\ncnts.columns=['number of cards', 'transaction count']","08683e6f":"cnts.loc[1,:]","e7bf69cb":"cnts.loc[1,:].plot(title = \"monthly counts for month_lag=1\")","06f26b3a":"new_card_ids = new.groupby([ 'month_lag', 'month']).agg({'card_id': list })\nhist_cards_monthly_purchase_count = hist.groupby(['card_id', 'month']).agg({'purchase_date': 'count'})","3e45268b":"hist_cards_monthly_purchase_count.head(15).T","fba9ba62":"prom_cards = list(set(new_card_ids.loc[1].loc['17-03'].values[0]))\nhist_card_monthly_purchase_count.loc[prom_cards].T.head()","e4b7337e":"prom_cards = list(set(new_card_ids.loc[1].loc['17-07'].values[0]))\nhist_card_monthly_purchase_count.loc[prom_cards].T.head()","88f9b4d9":"### Checking time duration of the datasets in months","39a970ca":"* Historic transaction count reaches maximun in December of 2017 and then starts to decline for two months.\n* New transactions count spikes in March of 2018, the month when historic tranactions are no longer collected.\nDoes it mean that the reference date used in the computation of the month lag is somewhere between December 1st 2017 to January 1st 2018?\n","9b6c1cd0":"### **Number of new_merchant transactoins per day and per month **","d4ccf63f":"\n**Let's check the month lag values in the new and historic tranascations**","4ef727bd":"###  **Number of card_id counts and  new_merchants transactions counts per month**","280270d4":"## Purchase dates overlap","16f70fcc":"*Please upvote if you find this kernel mildly intersting or entertaining*","7e84815c":"These numbers contradict the observation. It seems that the spike in sales in december of 2017 is due to seasonal demand.\nThe reference date seems to be February 1st 2018. Sales in historic transactions that happened during February of 2018 suppose to have month_lag == 0 \nTne month_lag for new transactions was not computed prior to  March 1st of 2018, hance the month lag is positive 1 and 2 for the new transactions \nSpike in sales reflected in the new_merchant transactions in March of 2018 seems to be due to promotion of new merchants that started on the reference date. \nThe spike in sales to new merchants perhaps scared ELO and that prompted this competition, perhaps because that the core merchants in the historic transactions took a serious hit.\n\nThis would be a nice conclusion, but it does not seem to hold given what you are going to see next.","3d1823f1":"- Histoiric transactions start 2017-01-01 00:00:08 and goes on for 14 months\n- New transactions start on 2017-03-01 03:24:51 and goes on for 14 months\n\nIt looks like there is a lag of 2 months between the datasets.","6aaf577b":"### Duration of the overlap between the datasets ","740ad4b1":"## **Card_id monthly salles in hist and new transactions**","4da8cd36":"The historic transactions file contain sales leading up to the month when promotion started, but omits data about historic transactions during the time period covered in new_transactions.  This explains the two months lag between historic_treansactions and new_transacitions.\n\nIt seems that two month lag between the datasets might be explained be a two month data collection window prior to promotion. The historic_transactions dataset was used for creating the initial model that led to creation of the first promotion. Once promotion was launched to a (selected) group of customers, the new_transactions dataset was started. The new_transactions dataset (perhaps) contains  data about all card_ids that were exposed to promotion and transactions outside of the core historic group that were a result of promotion were recorded. Hence the new_transaction dataset (perhaps) contains record of all card_ids that completed transactions as a result of promotion. \n\n","8d8b30cb":"## **Transactions per month**","e9f410d8":"New merchant transactions file contains transactions for cards that were selected for promotions that were suggesting new merchants to existing customers. ELO ran  promotions for new merchants at slow pace since March 1st of 2017 gradually increasing exposure. In March of 2017 number of cards with month_lag == 1 was 213. By January 1st of 2018 there were 18696 cards and by February of 2018  the promotion went out to 173574 card_ids\n\nOn March 1st 2018 the promotion was released to a broader population . Sales to new merchants spiked and historic merchants were on declining trend, but we do not have data in historic transactoins to substantiate that . I guess that this spike led  ELO to a decision that the loyalty score that they have created does not explain true customer loyalty behavior. There was a need for new way of computing loyalty score differently. ELO decided to do this competition. ","a4b80406":"Histoiric transactions start 2017-01-01 00:00:08 and end 2018-02-28 23:59:51   ","7655c0e2":"## Two month difference between the historic and new transactions datasets"}}