{"cell_type":{"417e8610":"code","0034c977":"code","5e0f2395":"code","d5e3dd8d":"code","27932da6":"code","c7ad6167":"code","8287c221":"code","dedc9b2a":"code","37323289":"code","64548fcf":"code","d9f6bc3d":"code","d7cd18eb":"code","f0059f12":"code","b2461089":"code","63ad2b76":"code","9c51b73d":"code","10a91864":"code","de54606d":"code","3045ad92":"code","4b948eff":"code","f4599942":"code","d1188e47":"code","5ced08d8":"code","bc8e1ed1":"code","001e5030":"code","a2ff7b41":"code","c92aab72":"code","b7796327":"code","6906ea08":"code","5cb0ac61":"code","11cc3918":"code","48617139":"code","067244b6":"code","d261f434":"code","562c72b3":"code","133c652a":"code","c096ad27":"code","96203948":"code","85e59140":"code","fd16de11":"code","d1a0e755":"code","dad9a504":"code","1ff7990a":"code","073d9fdd":"markdown","6cca5dca":"markdown","91981c65":"markdown","2c47f88e":"markdown","5558fe4f":"markdown","e73b53e5":"markdown","b7aa61cc":"markdown","e1d7d05b":"markdown","2c42e4ca":"markdown","e01b267f":"markdown","7b5a401d":"markdown","94e1ee42":"markdown","ef7b0dbd":"markdown","0cd76d9f":"markdown","61390b57":"markdown","08d52cd7":"markdown","d52c3be1":"markdown","a59de282":"markdown","91e327d7":"markdown"},"source":{"417e8610":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0034c977":"data = pd.read_csv(\"\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")","5e0f2395":"data.head()","d5e3dd8d":"data.describe()","27932da6":"count = data['Category'].value_counts()\nprint(count)","c7ad6167":"print(\"Spam % is \",(count[1]\/float(count[0]+count[1]))*100)","8287c221":"data.drop_duplicates(inplace = True)","dedc9b2a":"spam_list = data[data[\"Category\"] == \"spam\"][\"Message\"].unique().tolist()\nspam = \" \".join(spam_list)\nspam_wordcloud = WordCloud().generate(spam)\nplt.figure(figsize=(12,8))\nplt.imshow(spam_wordcloud)\nplt.show()","37323289":"ham_list = data[data[\"Category\"] == \"ham\"][\"Message\"].unique().tolist()\nham = \" \".join(ham_list)\nham_wordcloud = WordCloud().generate(ham)\nplt.figure(figsize=(12,8))\nplt.imshow(ham_wordcloud)\nplt.show()","64548fcf":"# mapping labels to 1 and 0\ndata['Category'] = data.Category.map({'ham':0, 'spam':1})","d9f6bc3d":"X=data['Message']\ny=data['Category']","d7cd18eb":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=201)","f0059f12":"# vectorising the text\nvect = CountVectorizer()\nvect.fit(X_train)","b2461089":"vect.vocabulary_","63ad2b76":"# transform\nX_train_transformed = vect.transform(X_train)\nX_test_tranformed =vect.transform(X_test)","9c51b73d":"from sklearn.naive_bayes import BernoulliNB\n\n# instantiate bernoulli NB object\nbnb = BernoulliNB()\n\n# fit \nbnb.fit(X_train_transformed,y_train)\n\n# predict class\ny_pred_class = bnb.predict(X_test_tranformed)\n\n# predict probability\ny_pred_proba =bnb.predict_proba(X_test_tranformed)\n\n# accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","10a91864":"metrics.confusion_matrix(y_test, y_pred_class)","de54606d":"#stop words are useless words\nnltk.download('stopwords')","3045ad92":"#Tokenization (a list of tokens), will be used as the analyzer\n#1.Punctuations are [!\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~]\n#2.Stop words in natural language processing, are useless words (data).\ndef process_text(text):\n    \n    #1 Remove Punctuationa\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    \n    #2 Remove Stop Words\n    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    #3 Return a list of clean words\n    return clean_words","4b948eff":"messages_bow = CountVectorizer(analyzer=process_text).fit_transform(data['Message'])","f4599942":"X_train, X_test, y_train, y_test = train_test_split(messages_bow, data['Category'], random_state = 201)","d1188e47":"# instantiate bernoulli NB object\nbnb = BernoulliNB()\n\n# fit \nbnb.fit(X_train,y_train)\n\n# predict class\ny_pred_class = bnb.predict(X_test)\n\n# predict probability\ny_pred_proba =bnb.predict_proba(X_test)\n\n# accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","5ced08d8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nSpam_model = LogisticRegression(solver='liblinear', penalty='l1')\nSpam_model.fit(X_train, y_train)\npred = Spam_model.predict(X_test)\naccuracy_score(y_test,pred)","bc8e1ed1":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters_KNN = {'n_neighbors': (5,10,15,20), }\n\ngrid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\n\ngrid_KNN.fit(X_train, y_train)","001e5030":"print(grid_KNN.best_params_)\nprint(grid_KNN.best_score_)","a2ff7b41":"#trying diffferent values for n_neighbours\nparameters_KNN = {'n_neighbors': (4,5,6,7), }\n\ngrid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\n\ngrid_KNN.fit(X_train, y_train)","c92aab72":"print(grid_KNN.best_params_)\nprint(grid_KNN.best_score_)","b7796327":"model = KNeighborsClassifier(n_neighbors=5)\n\n# Train the model using the training sets\nmodel.fit(X_train,y_train)\n\n#Predict Output\npred= model.predict(X_test)\naccuracy_score(y_test,pred)","6906ea08":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","5cb0ac61":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","11cc3918":"data","48617139":"X_train, X_test, y_train, y_test = train_test_split(data.Message.values, data.Category.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","067244b6":"data['Message'].apply(lambda x:len(str(x).split())).max()","d261f434":"# auc value returned \ndef roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","562c72b3":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 200\n\ntoken.fit_on_texts(list(X_train) + list(X_test))\nxtrain_seq = token.texts_to_sequences(X_train)\nxvalid_seq = token.texts_to_sequences(X_test)\n\n#zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","133c652a":"%%time\nwith strategy.scope():\n    # A simpleRNN without any pretrained embeddings and one dense layer\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    model.add(SimpleRNN(100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel.summary()","c096ad27":"model.fit(xtrain_pad, y_train,epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multipl","96203948":"scores = model.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,y_test)))","85e59140":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","fd16de11":"embedding_matrix = np.zeros((len(word_index) + 1, 200))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","d1a0e755":"with strategy.scope():\n    # A simple bidirectional LSTM with glove embeddings and one dense layer\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                     200,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\n    model.add(Bidirectional(LSTM(200, dropout=0.3, recurrent_dropout=0.3)))\n\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \n    \nmodel.summary()","dad9a504":"model.fit(xtrain_pad, y_train,epochs=5, batch_size=64*strategy.num_replicas_in_sync)","1ff7990a":"scores = model.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,y_test)))","073d9fdd":"accuracy decreased. \nlet us try something else","6cca5dca":"## Logistic Regression","91981c65":"configuring TPU [reference](https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert)","2c47f88e":"poor than others till now.","5558fe4f":"## RNN","e73b53e5":"Let us now divide the data into train and test","b7aa61cc":"a better one!","e1d7d05b":"## KNN","2c42e4ca":"## Naive Bayes Trial 2","e01b267f":"so the best value is for 5. let us train using that","7b5a401d":"## Deep Learning","94e1ee42":"![](https:\/\/www.online-tech-tips.com\/wp-content\/uploads\/2019\/05\/stop-spam-1.jpg.optimal.jpg)","ef7b0dbd":"We can see that we achieved a high accuracy(99.93%) by simply using RNN. Power of Deep Learning!!\nBut it can be overfitted. Let us try some other algorithms","0cd76d9f":"again test and train","61390b57":"let us use Count vectorizer. \nIn order to use textual data for predictive modeling, the text must be parsed to remove certain words \u2013 this process is called tokenization. These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization).\n\nScikit-learn\u2019s CountVectorizer is used to convert a collection of text documents to a vector of term\/token counts. It also enables the \u200bpre-processing of text data prior to generating the vector representation. This functionality makes it a highly flexible feature representation module for text.\n[Ref](https:\/\/www.educative.io\/edpresso\/countvectorizer-in-python)","08d52cd7":"## Bidirectional RNN","d52c3be1":"Let us check if this is imbalanced","a59de282":"image from [here](https:\/\/www.online-tech-tips.com\/computer-tips\/how-to-stop-spam-emails\/)","91e327d7":"## Naive Bayes"}}