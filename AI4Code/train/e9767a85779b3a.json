{"cell_type":{"3fc264e3":"code","04911b3c":"code","a11b2ccc":"code","2281f9c9":"code","f83b3a82":"code","d1b21b7f":"code","df31d752":"code","bf4cabb5":"code","1ab58722":"code","93c11cc3":"code","59f93048":"code","1782309b":"code","bcaa0efc":"code","2e2a5d47":"code","7ecd0afe":"code","a1940893":"code","32ddde99":"code","7dd05e0a":"code","0e090156":"code","5a819901":"code","486bbbfb":"code","c568238e":"code","d97c5f18":"code","256106ba":"code","049deb09":"code","74ba4f8f":"code","cdd8ae81":"code","53372b00":"code","161e13be":"code","c623ea78":"code","bd0e4e03":"code","d49bf89e":"code","da033d49":"markdown","d65236d8":"markdown","7a8f269c":"markdown","19450f33":"markdown","7cfdf3d3":"markdown","0d3f204e":"markdown","8e72571a":"markdown","a9ecd0e4":"markdown","2a42d971":"markdown","fa8467a4":"markdown","1f61e883":"markdown","02d1f4f2":"markdown"},"source":{"3fc264e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04911b3c":"os.listdir('..\/input\/v2-plant-seedlings-dataset')","a11b2ccc":"from numpy.random import seed\nseed(101)\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\nimport cv2\n\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","2281f9c9":"folder_list = os.listdir('..\/input\/v2-plant-seedlings-dataset')\ntotal_images = 0\n# loop through each folder\nfor folder in folder_list:\n    # set the path to a folder\n    path = '..\/input\/v2-plant-seedlings-dataset\/' + str(folder)\n    # get a list of images in that folder\n    images_list = os.listdir(path)\n    # get the length of the list\n    num_images = len(images_list)\n    \n    total_images = total_images + num_images\n    # print the result\n    print(str(folder) + ':' + ' ' + str(num_images))\n\n# print the total number of images available\nprint('Total Images: ', total_images)","f83b3a82":"try:\n    all_images_dir = 'images_all'\n    os.mkdir(all_images_dir)\nexcept:\n    print(\"Already there\")","d1b21b7f":"!ls","df31d752":"folder_list = os.listdir('..\/input\/v2-plant-seedlings-dataset')\nfor folder in folder_list:\n    #path to the folder\n    if(folder=='nonsegmentedv2'):\n        continue\n    path = '..\/input\/v2-plant-seedlings-dataset\/' + str(folder)\n    #list of all files in the folder\n    file_list = os.listdir(path)\n    # move the 0 images to images_all\n    for fname in file_list:\n        # source path to image\n        src = os.path.join(path, fname)\n        # Change the file name because many images have the same file name.\n        # Add the folder name to the existing file name.\n        new_fname = str(folder) + '_' + fname\n        # destination path to image\n        dst = os.path.join(all_images_dir, new_fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)","bf4cabb5":"len(os.listdir('images_all'))","1ab58722":"image_list = os.listdir('images_all')\n#dataframe of all images\ndf_data = pd.DataFrame(image_list, columns=['image_id'])\ndf_data.head()","93c11cc3":"#each filename has the format species_name_565.png\n#here we extract the class names for each image\ndef get_class(x):\n    # split into a list\n    a = x.split('_')\n    # the target is the first index in the list\n    cname = a[0]\n    return cname\ndf_data['target'] = df_data['image_id'].apply(get_class)\ndf_data.head()","59f93048":"df_data.shape","1782309b":"SAMPLE_SIZE=250\nIMAGE_SIZE = 128\ntarget_list = os.listdir('..\/input\/v2-plant-seedlings-dataset')\ni=target_list.index(\"nonsegmentedv2\")\ntarget_list.pop(i)\n#print(target_list)\nfor target in target_list:\n    # Filter out a target and take a random sample\n    df = df_data[df_data['target'] == target].sample(SAMPLE_SIZE, random_state=101)\n    # if it's the first item in the list\n    if target == target_list[0]:\n        df_sample = df\n    else:\n        # Concat the dataframes\n        df_sample = pd.concat([df_sample, df], axis=0).reset_index(drop=True)","bcaa0efc":"df_sample['target'].value_counts()","2e2a5d47":"y = df_sample['target']\ndf_train, df_val = train_test_split(df_sample, test_size=0.10, random_state=101, stratify=y)\nprint(df_train.shape)\nprint(df_val.shape)","7ecd0afe":"df_train['target'].value_counts()","a1940893":"df_val['target'].value_counts()","32ddde99":"try:\n    base_dir = 'base_dir'\n    os.mkdir(base_dir)\n    train_dir = os.path.join(base_dir, 'train_dir')\n    os.mkdir(train_dir)\n    val_dir = os.path.join(base_dir, 'val_dir')\n    os.mkdir(val_dir)\n\n\n\n    for folder in folder_list:\n        folder = os.path.join(train_dir, str(folder))\n        os.mkdir(folder)\n    for folder in folder_list:\n        folder = os.path.join(val_dir, str(folder))\n        os.mkdir(folder)\nexcept:\n    print(\"already created\")","7dd05e0a":"os.listdir('base_dir\/train_dir')","0e090156":"df_data.set_index('image_id', inplace=True)","5a819901":"train_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\nfor image in train_list:\n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image\n    # get the label for a certain image\n    folder = df_data.loc[image,'target']\n    # source path to image\n    src = os.path.join(all_images_dir, fname)\n    # destination path to image\n    dst = os.path.join(train_dir, folder, fname)\n    \n    # resize the image and save it at the new location\n    image = cv2.imread(src)\n    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    # save the image at the destination\n    cv2.imwrite(dst, image)\n\nfor image in val_list:\n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image\n    # get the label for a certain image\n    folder = df_data.loc[image,'target']\n    # source path to image\n    src = os.path.join(all_images_dir, fname)\n    # destination path to image\n    dst = os.path.join(val_dir, folder, fname)\n    # resize the image and save it at the new location\n    image = cv2.imread(src)\n    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    # save the image at the destination\n    cv2.imwrite(dst, image)","486bbbfb":"folder_list = os.listdir('base_dir\/train_dir')\ntotal_images = 0\n# loop through each folder\nfor folder in folder_list:\n    # set the path to a folder\n    path = 'base_dir\/train_dir\/' + str(folder)\n    # get a list of images in that folder\n    images_list = os.listdir(path)\n    # get the length of the list\n    num_images = len(images_list)\n    total_images = total_images + num_images\n    # print the result\n    print(str(folder) + ':' + ' ' + str(num_images))\n# print the total number of images available\nprint('Total Images: ', total_images)","c568238e":"os.rmdir('base_dir\/train_dir\/nonsegmentedv2')","d97c5f18":"folder_list = os.listdir('base_dir\/val_dir')\ntotal_images = 0\n# loop through each folder\nfor folder in folder_list:\n    # set the path to a folder\n    path = 'base_dir\/val_dir\/' + str(folder)\n    # get a list of images in that folder\n    images_list = os.listdir(path)\n    # get the length of the list\n    num_images = len(images_list)\n    total_images = total_images + num_images\n    # print the result\n    print(str(folder) + ':' + ' ' + str(num_images))\n# print the total number of images available\nprint('Total Images: ', total_images)","256106ba":"os.rmdir('base_dir\/val_dir\/nonsegmentedv2')","049deb09":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","74ba4f8f":"datagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","cdd8ae81":"kernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(12, activation = \"softmax\"))\n\nmodel.summary()","53372b00":"model.compile(Adam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])","161e13be":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=10, verbose=1,\n                    callbacks=callbacks_list)","c623ea78":"model.metrics_names","bd0e4e03":"# Print the validation loss and accuracy.\n\n# Here the best epoch will be used.\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = model.evaluate_generator(test_gen, steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","d49bf89e":"import matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","da033d49":"### Split into training and validation sets","d65236d8":"### Training of the model","7a8f269c":"### Extract the class names from the file names of the images","19450f33":"### Get total images in each folder","7cfdf3d3":"### Train and validation folders created","0d3f204e":"### We need to balance the dataset for better classification","8e72571a":"### Compiling all folders into one","a9ecd0e4":"### Next step is to create the CNN model","2a42d971":"### Cross check the validation data count for each class","fa8467a4":"### Cross check the train data count for each class","1f61e883":"### Resize and save images to respective directories","02d1f4f2":"### Setting up the model"}}