{"cell_type":{"a30cafd3":"code","1544e9cb":"code","2de8e59e":"code","5d90438c":"code","80c9565d":"code","934b504e":"code","3affa599":"code","9c0d37eb":"code","307018cc":"code","10a6ded8":"code","37def050":"code","aab7db0a":"code","1e7a47b4":"code","578a2fa9":"code","1e8ee724":"code","516ec496":"code","95f9e774":"code","ce21770e":"code","20251580":"code","37d596ad":"code","21147890":"code","86748945":"code","ad9c37aa":"code","959bd0dc":"code","008a4b51":"code","ad8de52e":"code","ce3a2420":"code","6830ff5c":"code","2970ff03":"code","584c1fd2":"code","c8624ff3":"code","82bd43ef":"code","aa043573":"code","46d398c4":"code","bd47b775":"code","07376482":"code","c511bf37":"code","424d7122":"code","9e48a498":"code","9fe33a5d":"code","c0d415de":"code","6443ccba":"code","d1d650c2":"code","62c5254d":"code","09ec1cad":"code","b90a64ca":"code","e94a86ec":"code","bc60d37e":"code","6063130b":"code","c136aae6":"code","e50a9d5c":"code","fa08f325":"code","dce4234d":"code","cb52bc3e":"code","854c5b50":"code","688c9f1f":"code","10da09e8":"code","1f8b5a97":"code","5dd26190":"markdown","499834be":"markdown","d3e6652c":"markdown","893be981":"markdown"},"source":{"a30cafd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1544e9cb":"import os\nimport zipfile\nfrom subprocess import check_output\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/instacart-market-basket-analysis\/'):\n    for filename in filenames:        \n        archive = zipfile.ZipFile(os.path.join(dirname, filename), mode='r')\n        archive.extractall(path=\"\/kaggle\/working\")\n        archive.close()\n\nprint(check_output([\"ls\", \"..\/working\"]).decode(\"utf8\"))","2de8e59e":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        col_type2 = df[col].dtype.name\n        \n        if ((col_type != object) and (col_type2 != 'category')):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","5d90438c":"orders = reduce_mem_usage(pd.read_csv('..\/working\/orders.csv'))\norder_products_prior = reduce_mem_usage(pd.read_csv('..\/working\/order_products__prior.csv'))\norder_products_train = reduce_mem_usage(pd.read_csv('..\/working\/order_products__train.csv'))\nproducts = reduce_mem_usage(pd.read_csv('..\/working\/products.csv'))\ndepartments = reduce_mem_usage(pd.read_csv('..\/working\/departments.csv'))\naisles = reduce_mem_usage(pd.read_csv('..\/working\/aisles.csv'))","80c9565d":"print(orders.shape)\nprint(order_products_prior.shape)\nprint(order_products_train.shape)\nprint(products.shape)\nprint(departments.shape)\nprint(aisles.shape)","934b504e":"departments.head(2)","3affa599":"#printing the list of unique departments\nprint(list(departments.department.unique()))","9c0d37eb":"aisles.head(2)","307018cc":"#printing the list of unique aisle\nprint(list(aisles.aisle.unique()))","10a6ded8":"products.head()","37def050":"products.department_id.value_counts()","aab7db0a":"products.groupby('department_id').head()","1e7a47b4":"orders.eval_set.dtypes","578a2fa9":"orders.user_id.nunique()","1e8ee724":"prior_df = orders[orders.eval_set == 'prior']","516ec496":"temp = reduce_mem_usage(pd.merge(left=prior_df, right=order_products_prior, on='order_id'))\ntemp.head()","95f9e774":"import seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\n\nsns.histplot(x='order_hour_of_day',data=temp)","ce21770e":"temp2 = reduce_mem_usage(pd.merge(left=products, right=aisles, on='aisle_id'))\ntemp3 = reduce_mem_usage(pd.merge(left=temp2, right=departments, on='department_id'))\ntemp3","20251580":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,6))\n\ntemp3['department'].value_counts().sort_values(ascending=True).plot(kind='barh',color='red')","37d596ad":"from wordcloud import WordCloud,ImageColorGenerator\n\nimport matplotlib.pyplot as plt\n\n#making of word cloud from Product name column\ntext = \" \".join(topic for topic in temp3.product_name.astype(str))\nprint (\"There are {} words in the combination of all Product Name.\".format(len(text)))\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\", width=800, height=400).generate(text)\n\nplt.axis(\"off\")\nplt.rcParams[\"figure.figsize\"] = (15,6)\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.show()","21147890":"final_prior_df = reduce_mem_usage(pd.merge(left=temp, right=temp3, on='product_id'))\nfinal_prior_df.head()","86748945":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,5))\nsns.countplot(x='reordered',data=final_prior_df)","ad9c37aa":"plt.figure(figsize=(10,7))\nfinal_prior_df['user_id'].value_counts().sort_values(ascending= False).head(20).plot(kind='bar',color='steelblue')","959bd0dc":"final_prior_df['days_since_prior_order'].dropna(inplace= True)","008a4b51":"final_prior_df['days_since_prior_order'].value_counts().sort_values(ascending= False).plot(kind='bar',color='teal')","ad8de52e":"ord_count_per_prod = reduce_mem_usage(final_prior_df[['order_id','user_id','product_name']].groupby('product_name').nunique().reset_index())\nord_count_per_prod.head()","ce3a2420":"ord_count_per_prod['product_name'].nunique()","6830ff5c":"final_prior_df.isnull().sum()\/len(final_prior_df)","2970ff03":"final_prior_df[final_prior_df.days_since_prior_order.isnull()]['order_number'].nunique()","584c1fd2":"variables = ['variables','orders_df','order_products__prior_df','order_products__train_df','products_df','department_df','aisles_df']","c8624ff3":"variables.append('final_prior_df')","82bd43ef":"def my_reset(varnames):\n    \"\"\"\n    varnames are what you want to keep\n    \"\"\"\n    globals_ = globals()\n    to_save = {v: globals_[v] for v in varnames}\n    to_save['my_reset'] = my_reset  # lets keep this function by default\n    del globals_\n    get_ipython().magic(\"reset\")\n    globals().update(to_save)","aa043573":"final_prior_df.head()","46d398c4":"final_prior_df.groupby(['product_id','user_id','product_name'])['order_id'].value_counts().sort_values(ascending=False)","bd47b775":"aisles.aisle","07376482":"from wordcloud import WordCloud,ImageColorGenerator\n\nimport matplotlib.pyplot as plt\n\n#making of word cloud from aisle column\ntext = \" \".join(topic for topic in aisles.aisle.astype(str))\nprint (\"There are {} words in the combination of all Aisles.\".format(len(text)))\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\", width=800, height=400).generate(text)\n\nplt.axis(\"off\")\nplt.rcParams[\"figure.figsize\"] = (15,6)\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.show()","c511bf37":"final_prior_df.columns","424d7122":"products_=final_prior_df['product_name']","9e48a498":"final_prior_df['order_dow'].value_counts().sort_values(ascending = False).plot(kind='bar',color='darkcyan')","9fe33a5d":"dummies_df = pd.get_dummies(data=final_prior_df, prefix=['Day','Hour'], columns=['order_dow','order_hour_of_day'], drop_first=True)\ndummies_df.head(10)","c0d415de":"dummies_df.columns","6443ccba":"user_prod_df = dummies_df.groupby(['user_id','product_id']).agg({'order_id':'nunique',\n                                                                 'days_since_prior_order':'mean',\n                                                                 'reordered':'max',\n                                                                 'Day_1':'sum',\n                                                                 'Day_2':'sum',\n                                                                 'Day_3':'sum',\n                                                                 'Day_4':'sum',\n                                                                 'Day_5':'sum',\n                                                                 'Day_6':'sum',\n                                                                 'Hour_1':'sum',\n                                                                 'Hour_2':'sum',\n                                                                 'Hour_3':'sum',\n                                                                 'Hour_4':'sum',\n                                                                 'Hour_5':'sum',\n                                                                 'Hour_6':'sum',\n                                                                 'Hour_7':'sum',\n                                                                 'Hour_8':'sum',\n                                                                 'Hour_9':'sum',\n                                                                 'Hour_10':'sum',\n                                                                 'Hour_11':'sum',\n                                                                 'Hour_12':'sum',\n                                                                 'Hour_13':'sum',\n                                                                 'Hour_14':'sum',\n                                                                 'Hour_15':'sum',\n                                                                 'Hour_16':'sum',\n                                                                 'Hour_17':'sum',\n                                                                 'Hour_18':'sum',\n                                                                 'Hour_19':'sum',\n                                                                 'Hour_20':'sum',\n                                                                 'Hour_21':'sum',\n                                                                 'Hour_22':'sum',\n                                                                 'Hour_23':'sum'\n                                                                }).reset_index()\nuser_prod_df.head(10)","d1d650c2":"user_purchase_df = dummies_df.groupby(['user_id']).agg({         'order_id':'nunique',\n                                                                 'product_id': 'nunique',\n                                                                 'days_since_prior_order':'mean',\n                                                                 'reordered':'sum',\n                                                                 'Day_1':'sum',\n                                                                 'Day_2':'sum',\n                                                                 'Day_3':'sum',\n                                                                 'Day_4':'sum',\n                                                                 'Day_5':'sum',\n                                                                 'Day_6':'sum',\n                                                                 'Hour_1':'sum',\n                                                                 'Hour_2':'sum',\n                                                                 'Hour_3':'sum',\n                                                                 'Hour_4':'sum',\n                                                                 'Hour_5':'sum',\n                                                                 'Hour_6':'sum',\n                                                                 'Hour_7':'sum',\n                                                                 'Hour_8':'sum',\n                                                                 'Hour_9':'sum',\n                                                                 'Hour_10':'sum',\n                                                                 'Hour_11':'sum',\n                                                                 'Hour_12':'sum',\n                                                                 'Hour_13':'sum',\n                                                                 'Hour_14':'sum',\n                                                                 'Hour_15':'sum',\n                                                                 'Hour_16':'sum',\n                                                                 'Hour_17':'sum',\n                                                                 'Hour_18':'sum',\n                                                                 'Hour_19':'sum',\n                                                                 'Hour_20':'sum',\n                                                                 'Hour_21':'sum',\n                                                                 'Hour_22':'sum',\n                                                                 'Hour_23':'sum'\n                                                                }).reset_index()\nuser_purchase_df.head(10)","62c5254d":"product_purchase_df = dummies_df.groupby(['product_id']).agg({   'order_id':'nunique',\n                                                                 'user_id': 'nunique',\n                                                                 'days_since_prior_order':'mean',\n                                                                 'reordered':'sum',\n                                                                 'Day_1':'sum',\n                                                                 'Day_2':'sum',\n                                                                 'Day_3':'sum',\n                                                                 'Day_4':'sum',\n                                                                 'Day_5':'sum',\n                                                                 'Day_6':'sum',\n                                                                 'Hour_1':'sum',\n                                                                 'Hour_2':'sum',\n                                                                 'Hour_3':'sum',\n                                                                 'Hour_4':'sum',\n                                                                 'Hour_5':'sum',\n                                                                 'Hour_6':'sum',\n                                                                 'Hour_7':'sum',\n                                                                 'Hour_8':'sum',\n                                                                 'Hour_9':'sum',\n                                                                 'Hour_10':'sum',\n                                                                 'Hour_11':'sum',\n                                                                 'Hour_12':'sum',\n                                                                 'Hour_13':'sum',\n                                                                 'Hour_14':'sum',\n                                                                 'Hour_15':'sum',\n                                                                 'Hour_16':'sum',\n                                                                 'Hour_17':'sum',\n                                                                 'Hour_18':'sum',\n                                                                 'Hour_19':'sum',\n                                                                 'Hour_20':'sum',\n                                                                 'Hour_21':'sum',\n                                                                 'Hour_22':'sum',\n                                                                 'Hour_23':'sum'\n                                                                }).reset_index()\nproduct_purchase_df.head(10)","09ec1cad":"temp = pd.merge(left=user_prod_df,  right=user_purchase_df, on='user_id', suffixes=('','_user'))\ntemp.head(10)","b90a64ca":"features_df = pd.merge(left=temp,  right=product_purchase_df, on='product_id', suffixes=('','_prod'))\nfeatures_df.head(10)","e94a86ec":"features_df.shape","bc60d37e":"features_df.info()","6063130b":"variables = ['features_df']\nmy_reset(variables)","c136aae6":"reduce_features_df = reduce_mem_usage(features_df)","e50a9d5c":"variables = ['reduce_features_df']\nmy_reset(variables)","fa08f325":"reduce_features_df.isnull().sum()","dce4234d":"reduce_features_df.drop(columns=['days_since_prior_order'],inplace= True)","cb52bc3e":"reduced_feature= reduce_features_df[:1000]","854c5b50":"reduced_feature.head(1)","688c9f1f":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(reduced_feature, test_size=0.3, random_state=100)\nprint(X_train.shape)\nprint(X_test.shape)","10da09e8":"from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import NearestNeighbors\n\nmodel_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\nmodel_knn.fit(X_train)","1f8b5a97":"query_index = np.random.choice(X_train.shape[0])\ndistances, indices = model_knn.kneighbors(X_train.iloc[query_index, :].values.reshape((1, -1)), n_neighbors = 6)\n\nfor i in range(0, len(distances.flatten())):\n    if i == 0:\n        print('Recommendations for {0}:\\n'.format(i, X_train.index[indices.flatten()[i]]))\n    else:\n        print('{0}: {1}'.format(i, X_train.index[indices.flatten()[i]]))","5dd26190":"### Let's check each dataset one by one","499834be":"### Let's print the shape of each dataset","d3e6652c":"### Now we have following DataFrames:\n\n* orders: This table includes all orders, namely prior, train, and test. It has single primary key (order_id).\n\n* order_products_train: This table includes training orders. It has a composite primary key (order_id and product_id) and indicates whether a product in an order is a reorder or not (through the reordered variable).\n\n* order_products_prior : This table includes prior orders. It has a composite primary key (order_id and product_id) and indicates whether a product in an order is a reorder or not (through the reordered variable).\n\n* products: This table includes all products. It has a single primary key (product_id)\n\n* aisles: This table includes all aisles. It has a single primary key (aisle_id)\n* departments: This table includes all departments. It has a single primary key (department_id)","893be981":"The goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users.\n\nFor each user, 4 and 100 of their orders are given, with the sequence of products purchased in each order."}}