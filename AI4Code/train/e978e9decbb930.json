{"cell_type":{"20d2579d":"code","90c5348a":"code","8563e940":"code","87c51e37":"code","b0480793":"code","4fe22aab":"code","e79e0518":"code","8e42d26a":"code","8de55bcc":"code","558ca211":"code","0edb2b57":"code","53acb2fa":"code","6ca33b59":"code","57133e4c":"code","ce07552e":"code","477ffb2c":"code","9f463fc5":"code","94c7fa53":"code","c33e185f":"code","8bc53639":"code","aba97b52":"code","d4e1dc14":"code","b38e9275":"code","05a60a28":"code","eff9397b":"code","989f5909":"markdown","cc4b9420":"markdown","396a84bf":"markdown","d3df2a43":"markdown","5d85e5fd":"markdown","5445665d":"markdown","ca28d638":"markdown","6236eff1":"markdown","f2329ed8":"markdown","50c7c153":"markdown","320b91d8":"markdown","28af2f87":"markdown","a7214ae9":"markdown","3625714d":"markdown","d889e858":"markdown","ddb1bb0d":"markdown","efabfea3":"markdown","bd351923":"markdown","08c35685":"markdown"},"source":{"20d2579d":"#Importing important libraries\n\nimport numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport pandas_profiling as pp","90c5348a":"dataset = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')","8563e940":"dataset.describe()","87c51e37":"pp.ProfileReport(dataset)","b0480793":"#Dropping customerID as its useless for our clustering analysis\ndataset.drop('CustomerID',axis=1,inplace =True)","4fe22aab":"g=sns.pairplot(dataset)\ng.fig.set_size_inches(15,15)","e79e0518":"#Changing the gender column from categorical to numerical\ndataset.loc[dataset.Gender == 'Male' ,'Gender'] = 1\ndataset.loc[dataset.Gender == 'Female' ,'Gender'] = 0\n\ndataset.Gender= dataset.Gender.astype(int)","8e42d26a":"\nfrom sklearn.preprocessing import StandardScaler\ncolumns = dataset.columns.values.tolist()\nfor i in columns:\n    if i != 'Gender':\n        ss = StandardScaler()\n        scaled = ss.fit_transform(dataset[[i]])\n        dataset[i] = scaled","8de55bcc":"#Selecting the best number of cluster based on intertia scoring\nkm_list = list()\n\nfor clust in range(1,15):\n    km = KMeans(n_clusters=clust,init='k-means++', random_state=42)\n    km = km.fit(dataset)\n    \n    km_list.append(pd.Series({'clusters': clust, \n                              'inertia': km.inertia_,\n                              'model': km}))","558ca211":"#Plotting the effect of increased cluser numbers on inertia\nplot_data = (pd.concat(km_list, axis=1)\n             .T\n             [['clusters','inertia']]\n             .set_index('clusters'))\n\nax = plot_data.plot(marker='o',ls='-')\nax.set_xticks(range(0,15,1))\nax.set_xlim(0,16)\nax.axhline(y=190, c='black',ls='--')\nax.set(xlabel='Cluster', ylabel='Inertia');\n","0edb2b57":"from sklearn.cluster import AgglomerativeClustering\n#Note the base n_cluster value of 2 is taken to show the whole tree\nag = AgglomerativeClustering(n_clusters=2, linkage='ward', compute_full_tree=True)\nag = ag.fit(dataset)","53acb2fa":"from scipy.cluster import hierarchy\nZ = hierarchy.linkage(ag.children_, method='ward')\n\nfig, ax = plt.subplots(figsize=(15,5))\n\n# Some color setup\nred = 'red'\nblue = 'blue'\nthreshold = 480\n\nhierarchy.set_link_color_palette([red, 'green'])\n\nden = hierarchy.dendrogram(Z, orientation='top', \n                           p=30, truncate_mode='lastp',\n                           show_leaf_counts=True, ax=ax,\n                           above_threshold_color=blue)\nax.axhline(y=threshold, c='black',ls='--')\nax.set_ylabel('Distance')\nax.set_xlabel('datapoint_Count')\nplt.show()","6ca33b59":"#Kmeans with 5 clusters\nkm = KMeans(n_clusters=5,init='k-means++', random_state=36)\nkm = km.fit(dataset)\ndataset['km'] = km.fit_predict(dataset)","57133e4c":"#AgglomerativeClustering with ward linkage\nag = AgglomerativeClustering(n_clusters=5, linkage='ward', compute_full_tree=True)\nag = ag.fit(dataset.iloc[:,:-1])\ndataset['agglom'] = ag.fit_predict(dataset)","ce07552e":"#Plotting the clusters predicted.\ncolor = 'brgym'\nalpha = 0.5\nlabels=['Cluster1','Cluster2','Cluster3','Cluster4','Cluster5']\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][km.labels_==i],dataset['Spending Score (1-100)'][km.labels_==i],c = color[i],alpha = alpha,s=20)\n    plt.scatter(km.cluster_centers_[i,2],km.cluster_centers_[i,3],c = color[i], marker = 'X', s = 200,label=labels[i])\nplt.ylabel('Annual Income')\nplt.xlabel('Spending Score')\nplt.legend()","477ffb2c":"#Using agglomerative hierarchichal clustering labels\ncolor = 'brgcm'\nalpha = 0.5\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][ag.labels_==i],dataset['Spending Score (1-100)'][ag.labels_==i],c = color[i],alpha = alpha,s=20)\nplt.ylabel('Annual Income')\nplt.xlabel('Spending Score')","9f463fc5":"color = 'brgym'\nalpha = 0.5\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][ag.labels_==i],dataset['Age'][ag.labels_==i],c = color[i],alpha = alpha,s=20)\nplt.ylabel('Annual Income')\nplt.xlabel('Age')\nplt.legend()","94c7fa53":"#Taking just 'Annual Income (k$)', 'Spending Score (1-100)' for clustering\ndataset_small = dataset[['Annual Income (k$)', 'Spending Score (1-100)']]\nkm = KMeans(n_clusters=5,init='k-means++', random_state=42)\nkm = km.fit(dataset_small)\ndataset_small['km'] = km.fit_predict(dataset_small)\n","c33e185f":"#Plotting\ncolor = 'brgcm'\nalpha = 0.5\nfor i in range(5):\n    plt.scatter(dataset_small['Annual Income (k$)'][km.labels_==i],dataset_small['Spending Score (1-100)'][km.labels_==i],c = color[i],alpha = alpha,s=20)\n    plt.scatter(km.cluster_centers_[i,0],km.cluster_centers_[i,1],c = color[i], marker = 'X', s = 200,label=labels[i])\nplt.ylabel('Annual Income')\nplt.xlabel('Spending Score')\nplt.legend()","8bc53639":"#Checking the new model in the old dataset with different features\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][km.labels_==i],dataset['Age'][km.labels_==i],c = color[i],alpha = alpha,s=20,label=labels[i])\nplt.ylabel('Annual Income')\nplt.xlabel('Age')\nplt.legend()","aba97b52":"#So ,in this specifi dataset,selecting just the two variables and clustering is a more apt solution than takin the whole dataset\n#We will replce the predicted labels column of the old dataset with the new one\ndataset['km']=dataset_small['km']","d4e1dc14":"#Checking if we can reduce the number of features by using PCA\ndataset_pca = dataset.iloc[:,:-2]","b38e9275":"from sklearn.decomposition import PCA\n\nfeature_weight = []\nvariance_explained = []\n\n# We can select upto 4 features as it is tha max\n\nfor n in range(1, 4):\n    \n    PCAmod = PCA(n_components=n)\n    PCAmod.fit(dataset_pca)\n    \n    # Store the model and variance\n    variance_explained.append(PCAmod.explained_variance_ratio_.sum())\n    \n    # Calculate and store feature importances\n    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n    feature_weight.append(pd.DataFrame({'n':n, \n                                             'features': dataset_pca.columns,\n                                             'values':abs_feature_values\/abs_feature_values.sum()}))\nvar=pd.DataFrame(variance_explained)\nvar   ","05a60a28":"features_df = (pd.concat(feature_weight)\n               .pivot(index='n', columns='features', values='values'))\n\nfeatures_df","eff9397b":"#Plotting the variance explanation\nax = var.plot(kind='bar')\n\nax.set(xlabel='Number of dimensions',\n       ylabel='Percent explained variance',\n       title='Explained Variance vs Dimensions')","989f5909":"### *It is evident that the model could not predict these two features entirely.However this might be also due to the fact that it might have given other features(other than Annual Income or Spending Score) more importance.\n### *Cluster 5 and 3 are wrongly labelled or interchanged.","cc4b9420":"### *From the dendrogram ,it is eveident from the dotted line that the distance doesnt decrease drastically if we increase the cluster number even further. Both these analysis points to taking the number of clusters as 5. ","396a84bf":"## Selecting the cluster number based on Hierarchical Clustering Distance Threshold ","d3df2a43":"## PCA and Variance Explanation Analysis","5d85e5fd":"### *Almost exact result using agglomerative clustering too.\n### *We can either use this or give the selected two variables more importance by clustering just on the basis of these two variables.","5445665d":"### *From the above dataframe,it is clear that Gender as a feature have the least importance in all out PCA lists.Age and Gender forms the maximum importaent duo followed closely by Annual Income","ca28d638":"### *Annual Income higher than 100k can be seen only for people in the age range 30-50\n### *Another intersing pattern of cluster formation can be seen between datapoints in the AnnualIncome-SpendingScore chart-Almost 5 clusters.","6236eff1":"### *As expected the model correclty formed 5 clusters without large number of outliers.","f2329ed8":"### *So just with 2 dimesions,we can capture almost 72% of the dataset variance.If it is increased to 3 ,more than 92 percent can be captured ,so that we can reduce with a single or two features from this dataset without losing much information.","50c7c153":"## Detailed EDA of the dataset using pandas profiling","320b91d8":"### *The dotted line shows that after 5 clusters are formed,there is no large decrease in inertia value.We can choose either 4n or 5 according to our threshold inertia value.I took 5 as it is evident from the earlier graph of Scoring and Income.Note:This if we want to cluster based on that specific features.","28af2f87":"## **Importing some useful libraries**","a7214ae9":"### The above graph cements our earlier insight that other features might have had more importane in pur clustering as the model almost succesfully clustered the datapoints in the above chart.","3625714d":"## Importing dataset ","d889e858":"## Scaling as in a general sense,differnt scale might affect the cluster formation due to distance difference between features .In this specific dataset,even an unscaled feature set works(I have tried.)","ddb1bb0d":"## 2.Clustering with just the two features taken","efabfea3":"## Cluster Number Selection Based on Inertia Method","bd351923":"## 1.Clustering with the entire dataset taken.","08c35685":"### *As seen from the correlation chart,there is no true corelation between any variables"}}