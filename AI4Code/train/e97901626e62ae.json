{"cell_type":{"793dd414":"code","78677e70":"code","a5a712ef":"code","2b07ab4f":"code","818a76ae":"code","b5d4513c":"code","c3a5e2a4":"code","c1cebd4b":"code","f36944db":"code","4a4fe1ad":"code","015e8786":"code","a2fb5737":"code","309ad14f":"code","f07f3018":"code","b2992494":"code","f7557e72":"code","9918aa2b":"code","7ecd4e6d":"code","517b9fd2":"code","e993df21":"code","0acb7733":"code","02bf1f55":"code","ffab4313":"code","bd403269":"markdown","8196d760":"markdown","af51be81":"markdown","f386c748":"markdown","596a7f50":"markdown","f4e8ce57":"markdown","892ae4bf":"markdown","337f395f":"markdown","5101d1c5":"markdown","d908480f":"markdown","67c6c5d0":"markdown","16775985":"markdown","154ced0e":"markdown","351be3f2":"markdown","65bf8286":"markdown","9ef0e83f":"markdown"},"source":{"793dd414":"import time\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input, BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import model_from_json","78677e70":"# random seed\nseed = 33\nnp.random.RandomState(seed)\n\n# validation to training split ration\nvalid_size = 0.1\n\n# use data augmentation i nthe first part of training\nto_augment = False","a5a712ef":"data_path = '..\/input\/Kannada-MNIST\/'\n\ntrain_path = data_path + 'train.csv'\ntest_path = data_path + 'test.csv'\ndig_path = data_path + 'Dig-MNIST.csv'\nsample_path = data_path + 'sample_submission.csv'\n\nsave_path = ''\nload_path = '..\/input\/kennada-mnist-pretrained-model\/'","2b07ab4f":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\ndig_df = pd.read_csv(dig_path)\nsample_df = pd.read_csv(sample_path)","818a76ae":"# convert dataframes to numpy matricies\nX = train_df.drop('label', axis=1).to_numpy()\ny = train_df['label'].to_numpy()\nX_dig = dig_df.drop('label', axis=1).to_numpy()\ny_dig = dig_df['label'].to_numpy()\nX_test = test_df.drop('id', axis=1).to_numpy()\n\n# reshape X's for keras and encode y using one-hot-vector-encoding\nX = X.reshape(-1, 28, 28, 1)\ny = to_categorical(y)\nX_dig = X_dig.reshape(-1, 28, 28, 1)\nX_test = X_test.reshape(-1, 28, 28, 1)\n\n# normalize the data to range(0, 1)\nX = X \/ 255\nX_dig = X_dig \/ 255\nX_test = X_test \/ 255\n\nprint('X shape is {}'.format(X.shape))\nprint('y shape is {}'.format(y.shape))\nprint('X_dig shape is {}'.format(X_dig.shape))\nprint('y_dig shape is {}'.format(y_dig.shape))\nprint('X_test shape is {}'.format(X_test.shape))","b5d4513c":"# split to train and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=valid_size, random_state=seed) \n\nprint('X_train shape = {}'.format(X_train.shape))\nprint('Y_train shape = {}'.format(y_train.shape))\nprint('X_valid shape = {}'.format(X_valid.shape))\nprint('Y_valid shape = {}'.format(y_valid.shape))","c3a5e2a4":"# model builder\ndef build_model(optimizer):\n    model = Sequential()\n    \n    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\n    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu'))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\n    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    \n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","c1cebd4b":"# save model\ndef save_trained_model(model, save_path, optimizer):\n    # serialize model to JSON\n    model_json = model.to_json()\n    with open('{}Kennada MNIST with {}.json'.format(save_path, optimizer), \"w\") as json_file:\n        json_file.write(model_json)\n\n    # serialize weights to HDF5\n    model.save_weights('{}Kennada MNIST with {}.h5'.format(save_path, optimizer))\n\n    \n# load pretrained model\ndef load_trained_model(optimizers, optimizer, load_path):\n    # load json and create model\n    json_file = open('{}Kennada MNIST with {}.json'.format(load_path, optimizers[optimizer]), 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    model = model_from_json(loaded_model_json)\n\n    # load weights into new model\n    model.load_weights('{}Kennada MNIST with {}.h5'.format(load_path, optimizers[optimizer]))\n    \n    # compile the model\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","f36944db":"def load_history(load_path, optimizer):\n    history = pd.read_csv('{}Kennada MNIST with {}.csv'.format(load_path, optimizer))\n    \n    return history.to_dict('list')\n\ndef save_history(history, save_path, optimizer):\n    hist_df = pd.DataFrame(history)\n    hist_df.to_csv('{}Kennada MNIST with {}.csv'.format(save_path, optimizer), index=False)","4a4fe1ad":"# integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32\nbatch_size = 1024\n# integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\nverbose = 0\n# integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\nepochs = 30","015e8786":"# every optimizer has a name\noptimizers = {\n    'sgd':        'SGD',\n    'rmsprop':    'RMSprop',\n    'adagrad':    'Adagrad',\n    'adadelta':   'Adadelta',\n    'adam':       'Adam',\n    'adamax':     'Adamax',\n    'nadam':      'Nadam',\n}\n\n# and default learning rate\nlearning_rates = {\n    'sgd':        1e-2,\n    'rmsprop':    1e-3,\n    'adagrad':    1e-2,\n    'adadelta':   1.0,\n    'adam':       1e-3,\n    'adamax':     2e-3,\n    'nadam':      2e-3,\n}","a2fb5737":"# create learning rate decay callback borrowed from here: https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=0, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n# artificially increase training set\ntrain_datagen = ImageDataGenerator(rescale=1.0,\n                                   rotation_range=10,\n                                   width_shift_range=0.25,\n                                   height_shift_range=0.25,\n                                   shear_range=0.1,\n                                   zoom_range=0.25,\n                                   horizontal_flip=False)\n\n# artificially increase validation set\nvalid_datagen = ImageDataGenerator(rescale=1.0)","309ad14f":"# prepare empty dictionaries\nhistory = {}\nmodel = {}\n\nfor n, optimizer in enumerate(optimizers):\n    # build model for every optimizer\n    model[optimizer] = build_model(optimizer)\n\n    # measure training time\n    start = time.time()\n\n    # train model\n    if to_augment:\n        h = model[optimizer].fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n                                           steps_per_epoch=100,\n                                           epochs=epochs,\n                                           validation_data=valid_datagen.flow(X_valid, y_valid),\n                                           callbacks=[learning_rate_reduction],\n                                           verbose=verbose)\n    else:\n        h = model[optimizer].fit(X_train,\n                                 y_train,\n                                 batch_size=batch_size,\n                                 epochs=epochs,\n                                 validation_data=(X_valid,y_valid),\n                                 callbacks=[learning_rate_reduction],\n                                 verbose=verbose)\n\n    history[optimizer] = h.history\n\n    # print results\n    print(\"{0} Optimizer: \".format(optimizers[optimizer]))\n    print(\"Epochs={0:d}, Train accuracy={1:.5f}, Validation accuracy={2:.5f}, Training time={3:.2f} minutes\"\n              .format(epochs, \n                      max(history[optimizer]['accuracy']), \n                      max(history[optimizer]['val_accuracy']), \n                      (time.time()-start)\/60))","f07f3018":"# apply smoothing filter\ndef smoothing_filter(data, filter_n=3):\n    # filter_n should be odd number\n    # extend the end for better accuracy at the end\n    data = np.concatenate((data, [data[-1]]*filter_n))\n    \n    # apply filter\n    data = np.convolve(data, [1\/filter_n]*filter_n)\n    \n    # remove filter delay and padding\n    return data[int(np.ceil(filter_n\/2)) : -filter_n]\n\n\n# plot training accuracy helper function\ndef plot_training_accuracy(history, names, epochs, to_smooth=False, filter_n=3, styles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']):\n    # filter_n should be odd number\n    plt.figure(figsize=(15, 5))\n    \n    for n, h in enumerate(history.values()):\n        # get validation accuracy history\n        val_acc = h['val_accuracy']\n        \n        # smooth on request\n        if to_smooth:\n            val_acc = smoothing_filter(val_acc, filter_n)\n        \n        # plot history\n        plt.plot(val_acc, linestyle=styles[n])\n    \n    plt.title('Model validation accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(names, loc='upper left')\n    axes = plt.gca()\n    axes.set_ylim([0.99, 0.997])\n    axes.set_xlim([0, epochs-1])","b2992494":"# plot learning hystory for all optimizers\nplot_training_accuracy(history, optimizers.values(), epochs)","f7557e72":"# filter_n should be odd number\nfilter_n = 7\n\n# plot learning hystory for all optimizers\nplot_training_accuracy(history, optimizers.values(), epochs, to_smooth=True, filter_n=filter_n)","9918aa2b":"# find optimizer with best score\ndef get_best_optimizer(history, optimizers, to_smooth=False, filter_n=3):\n    # allocate memory\n    best_val_scores = np.zeros((len(optimizers),))\n    \n    # find best score for each optimizer\n    for n, h in enumerate(history.values()):\n        # get validation accuracy history\n        val_acc = h['val_accuracy']\n        \n        # smooth on request\n        if to_smooth:\n            val_acc = smoothing_filter(val_acc, filter_n)\n        \n        # find best val score\n        best_val_scores[n] = np.max(val_acc)\n    \n    # returns best optimizers key as string\n    return list(optimizers.keys())[np.argmax(best_val_scores)]","7ecd4e6d":"best_optimizer = get_best_optimizer(history, optimizers, to_smooth=True, filter_n=7)\n\nprint(\"Optimizer with best validation score is '{}'.\".format(optimizers[best_optimizer]))","517b9fd2":"additional_epochs = 50","e993df21":"# save best model\nfilepath = save_path + 'best_model_with_'+ best_optimizer + '_on_' + str(additional_epochs) + '.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n\n# create new model\nnew_model = build_model(best_optimizer)\n\n# measure training time\nstart = time.time()\n\n# train model\nnew_history = new_model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n                                      steps_per_epoch=100,\n                                      epochs=additional_epochs,\n                                      validation_data=valid_datagen.flow(X_valid, y_valid),\n                                      callbacks=[learning_rate_reduction, checkpoint],\n                                      verbose=verbose)\nnew_history = new_history.history\n\n# print results\nprint(\"{0} Optimizer: \".format(optimizers[best_optimizer]))\nprint(\"Epochs={0:d}, Train accuracy={1:.5f}, Validation accuracy={2:.5f}, Training time={3:.2f} minutes\"\n          .format(additional_epochs, \n                  max(new_history['accuracy']), \n                  max(new_history['val_accuracy']), \n                  (time.time()-start)\/60))","0acb7733":"# make predictions helper function\ndef make_prediction(model, x):\n    y_pred = model.predict(x)\n    return np.argmax(y_pred, axis=1)","02bf1f55":"# predict on the Dig-MNIST set\ny_pred = make_prediction(new_model, X_dig)\n\n# build confusion matrix\nconf = confusion_matrix(y_dig, y_pred)\nconf = pd.DataFrame(conf, index=range(0,10), columns=range(0,10))\n\n# plot the confusion matrix\nplt.figure(figsize=(12,10))\nsns.heatmap(conf, annot=True);","ffab4313":"# predict on the test set\ny_result = make_prediction(new_model, X_test)\n\n# save predictions\nsample_df['label'] = y_result\nsample_df.to_csv('submission.csv',index=False)","bd403269":"Finally, lets train the model with different optimizers and see the difference.","8196d760":"Define dictionaries of all Keras optimizers [9] and default learning rates.","af51be81":"### Split to training and validation data","f386c748":"The graph is noisy and it is hard to pick optimizer like this. Let\u2019s smooth the lines and plot again.","596a7f50":"## Check performance on Dig-MNIST data","f4e8ce57":"#### Load datasets","892ae4bf":"## Train network","337f395f":"Notice that we don\u2019t need to encode y_dig because it is more convenient this way to compare results. Unlike y, that will be part of the model training and has to be encoded for Keras.","5101d1c5":"## Load and prepare data\n\nThe competition contains four files:\n\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n* Dig-MNIST.csv - an additional labeled set of characters that can be used to validate or test model results before submitting to the leaderboard\n\nWe will use the train.csv for model training and validation, Dig-MNIST.csv to evaluate model performances, and we will make predictions and submit results from test.csv data. sample_submission.csv will be used as a template for our submission.\n\n#### Set paths","d908480f":"## Make submission file\n\nFinally lets submit our predictions on the test dataset and see results.","67c6c5d0":"Now, let\u2019s train a new model on more epochs for better performance.","16775985":"#### Preprocess data","154ced0e":"## Import libraries","351be3f2":"## Build model\n\nLet\u2019s build a CNN model. But how do we choose the right one? Thankfully, the MNIST dataset and Kannada MNIST dataset are fairly similar so we can use the best model from [8] and try all the optimizers on it.","65bf8286":"Not all optimizers are born equal. They come in many equations and complexities but one thing they have in common, they are all there to help you train your network. It does not matter if you are working on prediction, classification, or even segmentation task, they will always try to do their best!\n\nEvery optimizer has its own strong and weaker sides and every developer should be familiar with them. All the optimizers goal is the same, reduce the loss as much as possible in their own way by manipulating model parameters.\n\nWhile there are many optimizers in the wild, in this notebook we will go over those which are implemented in Keras libraries.\n\n### Optimizers background\n\nAll the optimizers have the same baseline where the gradients of the cost function are calculated through the entire model chain and then subtract those gradients from the model parameters. We subtract because we are trying to find the lowest point in the cost function plane and the gradients \"point\" up the slope. What is defining the optimizers is how they regularize the update process of the parameters as we will see later.\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT-zueD_MQZh2N6DU0NVJxuyMEVOcnOChPEryJdmnoPBVf09utd&s\">\n\n\n### SGD (Stochastic Gradient Descent)\n\nFirst candidate for today is the GD (Gradient Descent) optimizer. There are actually three types of GDs and Keras implements all of them in one function. Let\u2019s look on them.\n\n\n#### BGD (Batch Gradient Descent))\n\nThe first type of GD is called BGD and it is the simplest optimizer to understand. But don\u2019t let that mislead you as at large datasets he is the most computationally complex. That is because in BGD the entire detaset needs to be fed into the network to make only one \"step\". On top of that, in very large datasets there might not be enough RAM to hold the entire dataset. On the other hand, BGD theoretically will always aim to the lowest point in the loss function plane.\n\nThe equation for BGD takes the gradient of the cost function and subtracts it from the parameter. Usually a regularization hyper parameter is added called the \"learning rate\" to regulate the convergence. The \"learning rate\" is usually in the range of (1e-3, 1e-2). Below is the equation for BGD:\n\n\\begin{align}\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)\n\\end{align}\n\n* \u03b8: parameter to update\n* \u03b7: learning rate\n* \u2207<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\">\u03b8<\/span>J(\u03b8): derivative of cost function \n\n\n#### SGD (Stochastic Gradient Descent)\n\nThe second type is SGD which is the exact opposite from BGD. Here to take one \"step\" we compute only one example from the dataset so eventually in one epoch we will make steps as the number of examples in the dataset. This technique much faster than BGD and more practical as you don\u2019t need to store the entire dataset in the RAM. Instead, the relevant data can be loaded at need. Those advantages come with a price as SGD suffers from high variance and the \"steps\" will not always be toward convergence. There is a way around this by carefully reducing the learning rate at each epoch. Doing so can improve SGD performance as much as being the same as BGD. Practically SGD is preferred over BGD for applications where the dataset is not small.\n\nBelow is the equation for SGD. the only difference from BGD is the derivative of the cost function uses only one example.\n\n\\begin{align}\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})\n\\end{align}\n\n\n* \u03b8: parameter to update\n* \u03b7: learning rate\n* \u2207<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\">\u03b8<\/span>J(\u03b8; X<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i)<\/span>; Y<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i)<\/span>): derivative of cost function respect to one example\n* X<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i)<\/span>: features of example i\n* Y<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i)<\/span>: ground truth of example i\n\n\n#### MBGD (Mini-Batch Gradient Descent))\n\nThe third type is MBGD and it is a compromise between SGD and BGD where we learn on a batch (several) of examples at every \"step\". Being a compromise, its variance is lower than SGD which makes it more stable. Practically, try to use power of 2 batch sizes as the ML frameworks are usually more optimized for them.\n\nBelow is the equation of MBGD and it is the same as SGD except the derivative depends on a batch of examples.\n\n\\begin{align}\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})\n\\end{align}\n\n* \u03b8: parameter to update\n* \u03b7: learning rate\n* \u2207<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;\">\u03b8<\/span>J(\u03b8; X<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i)<\/span>; Y<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i)<\/span>): derivative of cost function respect to several examples\n* X<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i:i+n)<\/span>: features of example i to n\n* Y<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">(i:i+n)<\/span>: ground truth of example i to n\n\n\n#### GD Conclusion\n\nIn Keras you can control the batch size which gives you the option to turn the optimizer to BGD by setting batch size to the length of the data. Or make it SGD by setting batch size to 1. Or you can set batch size to any other number between 1 and dataset size to get MBGD. I would argue that this optimizer should be called MBGD with options to BGD and SGD to be less confusing. Nether less, Keras calls it SGD.\n\n\n### Adagrad (Adaptive Gradient )\n\nAdagrad differs from SGD by computing different learning rate for each parameter that changes every step. Let\u2019s define g as the partial derivative with respect to \u03b8:\n\n\\begin{align}\ng_{t, i} = \\nabla_\\theta J( \\theta_{t, i} )\n\\end{align}\n\nNow looking on the Adagrads equation, it has two new parameters:\n\n\\begin{align}\n\\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i}\n\\end{align}\n\n* G<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span>: sum of the squares of previous gradients\n* \u03f5: small number to eliminate division by zero (usually around 1e-8)\n\nWhile Adagrad automatically reduces the learning rate differently for every parameter, it has a major drawback. By summing squares of gradients (which always be positive numbers), eventually will yield a large number that will make the gradient to \"disappear\" (be close to zero).\n\nA more detailed explanation can be found in [1] and [2].\n\n\n### Adadelta (ADAPTIVE LEARNING RATE METHOD)\n\nAdadelta tries to solve Adagrad's drawbacks and it is a direct extension. Instead of summing all the past gradients, Adadelta restricts the number of past gradients on which it depends by calculating RMS on a running average. Let\u2019s write all the Adadelta equations and then explain them:\n\n\\begin{align}\nE[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g^2_t\n\\end{align}\n\n\\begin{align}\nRMS[g]_{t} = \\sqrt{E[g^2]_t + \\epsilon}\n\\end{align}\n\n\\begin{align}\nRMS[\\Delta \\theta]_{t} = \\sqrt{E[\\Delta \\theta^2]_t + \\epsilon}\n\\end{align}\n\n\\begin{align} \n\\begin{split}\n\\Delta \\theta_t &= - \\dfrac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}} g_{t} \\\\ \n\\theta_{t+1} &= \\theta_t + \\Delta \\theta_t \n\\end{split} \n\\end{align}\n\n* E[g<span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\">2<\/span>]<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span>: running average\n* \u03b3: decay constant. usually around 0.9\n\nThere are two main things that changed in the equations. First the denominator changed to the RMS over running mean that we mentioned before. Second is the numerator that changed from a learning rate constant to RMS over the previous parameter update vector. The numerator had to change because as the author of the paper said \"acts as an acceleration term, accumulating previous gradients over a window of time\".\n\nOne additional advantage of Adadelta is that there is no need to choose a learning rate.\n\nA more detailed explenetion can be found in [1] and [3].\n\n\n### RMSprop\n\nThe RMSprop is actually an unpublished algorithm. It was proposed in Coursera course [4] lecture [6]. The algorithm is an extension of Adagrad and very similar to Adadelta. RMSpror changes only the denominator to the same equation as Adadelta and setting \u03b3=0.9:\n\n\\begin{align} \n\\begin{split} \nE[g^2]_t &= 0.9 E[g^2]_{t-1} + 0.1 g^2_t \\\\ \n\\theta_{t+1} &= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_{t} \n\\end{split} \n\\end{align}\n\n\n### Adam (Adaptive Moment Estimation)\n\nAdam optimizer is an extension of two optimizers, RMSpror and Momentum [7]. The Adam uses the Momentum to correct the first momentum and RMSprop to correct the second momentum.\n\n\\begin{align} \n\\begin{split} \nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ \nv_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \n\\end{split} \n\\end{align}\n\n* m<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span>: exponentially decaying average (first momentum)\n* v<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span>: exponentially decaying average of past squared gradients (second momentum)\n\nAt initialization stage m<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span> and v<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span> should be initialized to zero vectors. This initialization creates a problem of biasing the gradient to zero. To overcome this problem, the author adds bias correction equations:\n\n\\begin{align} \n\\begin{split} \n\\hat{m}_t &= \\dfrac{m_t}{1 - \\beta^t_1} \\\\ \n\\hat{v}_t &= \\dfrac{v_t}{1 - \\beta^t_2} \n\\end{split} \n\\end{align}\n\nFinally, to update the parameters we use the following equation:\n\n\\begin{align} \n\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n\\end{align}\n\nThe author suggests to use the following hyper parameter values:\n* \u03b2<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">1<\/span> = 0.9\n* \u03b2<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">2<\/span> = 0.999\n* \u03b3 = 1e-8\n\nAdam is one of the most used optimizers in ML as it is stable and usually produces the best results.\n\nA more detailed explanation can be found in [1], [5], and [7]\n\n\n### AdaMax\n\nWhile Adam uses l<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">2<\/span> norm to calculate v<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span>, Adamax uses l<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">\u221e<\/span>. This has two inpacts on the equetion. First one beeing that we dont need the bias correction terms. The second is that we v<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">t<\/span> equation changes from l<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">2<\/span> to l<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">\u221e<\/span> norm (to not get confused, we will call the new parameter u instead of v):\n\n\\begin{align} \n\\begin{split} \nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n\\end{split} \n\\end{align}\n\n\\begin{align} \n\\begin{split} \nu_t &= \\beta_2^\\infty v_{t-1} + (1 - \\beta_2^\\infty) |g_t|^\\infty & = \\max(\\beta_2 \\cdot v_{t-1}, |g_t|) \\\\\n\\end{split} \n\\end{align}\n\n\\begin{align} \n\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{u_t} \\hat{m}_t\n\\end{align}\n\nLike previously, the author suggests to use the following hyper parameter values:\n* \u03b2<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">1<\/span> = 0.9\n* \u03b2<span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\">2<\/span> = 0.999\n* \u03b7 = 2e-3\n\n### Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n\nLast but not least, Nadam is Adam with NAG [1]. This makes the Nadam optimizer do two steps every time, one in the direction of the previous gradient and then in the direction in the new gradient. This adds a short memory to the system and reduces oscillation that might accur in the training process.\n\nAfter equation manipulation we get the Nadam equations:\n\n\\begin{align} \n\\begin{split} \n\\hat{m}_t &= \\dfrac{m_t}{1 - \\beta^t_1} \\\\ \n\\hat{v}_t &= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split} \n\\end{align}\n\n\\begin{align} \n\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} (\\beta_1 \\hat{m}_t + \\dfrac{(1 - \\beta_1) g_t}{1 - \\beta^t_1})\n\\end{align}\n","9ef0e83f":"## Reference links\n\n[1] <a href=https:\/\/ruder.io\/optimizing-gradient-descent\/> An overview of gradient descent optimization algorithms <\/a>\n\n[2] <a href=https:\/\/medium.com\/konvergen\/an-introduction-to-adagrad-f130ae871827> An Introduction to AdaGrad <\/a>\n\n[3] <a href=https:\/\/datascience.stackexchange.com\/questions\/27676\/understanding-the-mathematics-of-adagrad-and-adadelta> Understanding the mathematics of AdaGrad and AdaDelta <\/a>\n\n[4] <a href=https:\/\/www.coursera.org\/learn\/neural-networks-deep-learning> Neural Networks and Deep Learning <\/a>\n\n[5] <a href=https:\/\/www.coursera.org\/lecture\/deep-neural-network\/adam-optimization-algorithm-w9VCZ> Adam optimization algorithm <\/a>\n\n[6] <a href=http:\/\/www.cs.toronto.edu\/~tijmen\/csc321\/slides\/lecture_slides_lec6.pdf> Overview of mini-batch gradient descent <\/a> page 29\n\n[7] <a href=https:\/\/towardsdatascience.com\/stochastic-gradient-descent-with-momentum-a84097641a5d> Stochastic Gradient Descent with momentum <\/a>\n\n[8] <a href=https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6> Introduction to CNN Keras - Acc 0.997 (top 8%) <\/a>\n\n[9] <a href=https:\/\/keras.io\/optimizers\/> Keras optimizers <\/a>"}}