{"cell_type":{"5788f7d2":"code","6017a66b":"code","5cc4cf83":"code","47852461":"code","6cd4802c":"code","bbd93f16":"code","14f0d7b5":"code","0d6712d1":"code","3f6e2f00":"code","be6e241d":"code","d03e5b74":"code","c6410982":"code","0edcf475":"code","33e59323":"code","9e36d02f":"code","b9f5b4dc":"code","6df38918":"markdown","e3612281":"markdown","06d860c5":"markdown","4d4601d5":"markdown","28ba1073":"markdown","2947ca76":"markdown","dbf7ea24":"markdown","e1d4cc43":"markdown","7337dc28":"markdown","a01d9656":"markdown","200b8ea6":"markdown","ec090dfc":"markdown"},"source":{"5788f7d2":"import random\nimport math\nfrom numpy.random import seed\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\nfrom keras.models import Model, Sequential\nfrom keras.layers import Flatten, Dense, Dropout, Permute, Lambda, Conv2D, multiply, Concatenate, concatenate, \\\n    BatchNormalization, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom collections import OrderedDict\nfrom tqdm import tqdm\npd.options.plotting.backend = \"plotly\"","6017a66b":"data_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\"\nBATCH_SIZE = 12  # try reducing batch size or freeze more layers if your GPU runs out of memory\nNUM_EPOCHS = 40\nLEARNING_RATE = 0.0001\nIMAGE_SIZE = (150, 150)\nNUM_CLASSES=2\ndata_list = os.listdir(data_dir+'\/train')","5cc4cf83":"# Train datagen here is a preprocessor\ntrain_datagen = ImageDataGenerator(rescale=1. \/ 255,\n                                   rotation_range=50,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.25,\n                                   zoom_range=0.1,\n                                   channel_shift_range=20,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                   validation_split=0.2,\n                                   fill_mode='constant')\n\n# For multiclass use categorical n for binary us\ntrain_batches = train_datagen.flow_from_directory(data_dir+'\/train',\n                                                  target_size=IMAGE_SIZE,\n                                                  shuffle=True,\n                                                  batch_size=BATCH_SIZE,\n                                                  seed=42,\n                                                  class_mode=\"binary\"\n                                                  # For multiclass use categorical n for binary use binary\n                                                  )\n\nvalid_batches = train_datagen.flow_from_directory(data_dir+'\/val',\n                                                  target_size=IMAGE_SIZE,\n                                                  shuffle=True,\n                                                  batch_size=BATCH_SIZE,\n                                                  seed=42,\n                                                  class_mode=\"binary\"\n                                                 )\n","47852461":"train_batches","6cd4802c":"# Helper functions\nfrom keras.callbacks import LearningRateScheduler\n# learning decay rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.0001\n    drop = 0.4  # in 0.5 it provided an accuracy of 80%+\n    epochs_drop = 4.0  # 5.0 gives an optimal epochs_drop\n    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) \/ epochs_drop))\n    return lrate\n\n# learning schedule callback\nlrate = LearningRateScheduler(step_decay)\ncallbacks_list = [lrate]","bbd93f16":"from keras.applications.vgg16 import VGG16\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(150, 150, 3))\n\n# layer_base=conv_base.output\nconv_base = Model(inputs=conv_base.inputs, outputs=conv_base.get_layer('block4_pool').output)","14f0d7b5":"outputs=conv_base.get_layer('block4_pool').output\noutputs.shape[-1]","0d6712d1":"def spatial_attention(input_feature):\n    kernel_size = 7\n    if K.image_data_format() == \"channels_first\":\n        channel = input_feature.shape[1]\n        cbam_feature = Permute((2, 3, 1))(input_feature)\n    else:\n        channel = input_feature.shape[-1]\n        cbam_feature = input_feature\n\n    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n    assert avg_pool.shape[-1] == 1\n    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n    assert max_pool.shape[-1] == 1\n    concat = Concatenate(axis=3)([avg_pool, max_pool])\n    assert concat.shape[-1] == 2\n    cbam_feature = Conv2D(filters=1,\n                          kernel_size=kernel_size,\n                          strides=1,\n                          padding='same',\n                          activation='sigmoid',\n                          kernel_initializer='he_normal',\n                          use_bias=False)(concat)\n    assert cbam_feature.shape[-1] == 1\n\n    if K.image_data_format() == \"channels_first\":\n        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n    x1 = concatenate([input_feature, cbam_feature])\n\n    return x1","3f6e2f00":"def Trunc_VGG(conv_base):\n    conv_base.trainable = True\n    layer_4th = conv_base.get_layer('block4_pool').output\n    attention_module = spatial_attention(layer_4th)\n    print(attention_module.shape)\n    flatten = Flatten()(attention_module)\n    # flatten=GlobalAveragePooling2D()(attention_module)\n    dropout = Dropout(0.5)(flatten)\n    dense = Dense(256, activation='relu')(dropout)\n    # dropout = Dropout(0.5)(dense)\n    pred = (Dense(NUM_CLASSES, activation='softmax'))(dense)\n    model = Model(inputs=conv_base.inputs, outputs=pred)\n    return model","be6e241d":"import time\ntrain_s_time = time.clock()\nmodel = Trunc_VGG(conv_base)","d03e5b74":"# FIT MODEL\nprint(len(train_batches))\nprint(len(valid_batches))\n\nSTEP_SIZE_TRAIN = train_batches.n \/\/ train_batches.batch_size\nSTEP_SIZE_VALID = valid_batches.n \/\/ valid_batches.batch_size","c6410982":"from keras import optimizers\nmodel.compile(loss='binary_crossentropy',  # for multiclass use categorical_crossentropy\n              # optimizer=optimizers.SGD(lr=LEARNING_RATE,momentum=0.9),\n              optimizer=optimizers.Adam(lr=LEARNING_RATE),\n              #  optimizer=optimizers.Adam(lr_schedule),\n              metrics=['acc'])\n\nprint(model.summary())","0edcf475":"result = model.fit_generator(train_batches,\n                             steps_per_epoch=STEP_SIZE_TRAIN,\n                             verbose=1,\n                             validation_data=valid_batches,\n                             validation_steps=STEP_SIZE_VALID,\n                             epochs=NUM_EPOCHS,\n                             callbacks=callbacks_list\n                             )\n\nprint('Training time:' + str(time.clock() - train_s_time) + 'secs.')","33e59323":"import matplotlib\nimport matplotlib.pyplot as plt\n\nprint(result.history.keys())\nfig1 = plt.figure(1)\n\n# summarize history for accuracy\nplt.plot(result.history['acc'])\nplt.plot(result.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper left')\n#plt.savefig(root_path + '\/' + 'acc.png')\nplt.show()\n\n# summarize history for loss\nfig2 = plt.figure(2)\nplt.plot(result.history['loss'])\nplt.plot(result.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper left')\n#plt.savefig(root_path + '\/' + 'loss.png')\nplt.show()","9e36d02f":"test_datagen = ImageDataGenerator(rescale=1. \/ 255)\neval_generator = test_datagen.flow_from_directory(data_dir+'\/test', target_size=IMAGE_SIZE, batch_size=1,\n                                                  shuffle=False, seed=42, class_mode=\"binary\")\neval_generator.reset()\neval_generator.reset()\ntest_s_time = time.clock()\nx = model.evaluate_generator(eval_generator,\n                             steps=np.ceil(len(eval_generator)),\n                             use_multiprocessing=False,\n                             verbose=1,\n                             workers=1,\n                             )\nprint('Testing time:' + str(time.clock() - test_s_time) + 'secs.')\nprint('Test loss:', x[0])\nprint('Test accuracy:', x[1])","b9f5b4dc":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfilenames = eval_generator.filenames\nnb_samples = len(filenames)\neval_generator.reset()\nprdict = model.predict_generator(eval_generator, steps=np.ceil(len(eval_generator)))\npp = prdict\nprdict = np.argmax(prdict, axis=-1)\nclasses = eval_generator.classes[eval_generator.index_array]\nacc = sum(prdict == classes) \/ len(prdict)\n# names = [\"Covid\", \"Normal\", \"Pneumonia Bacteria\", \"Pneumonia Viral\"]\n# names = [\"No_findings\", \"Covid\", \"Normal\",\"Pneumonia Bacterial\", \"Pneumonia Viral\"]\nnames = data_list\n# names = [\"Covid-19\", \"No_findings\", \"Pneumonia\"]\n# names=[\"covid\",\"non\"]\n# names=[\"COVID-19\",\"MERS\",\"Normal\",\"Pneumocystis\",\"SARS\",\"Streptococcus\",\"Varicella\"]\nprint(classification_report(classes, prdict))\nreport = classification_report(classes, prdict, output_dict=True)","6df38918":"# Compile a model","e3612281":"Tha aim of this notebook is to build a deep learning model (VGG-16) to classify the patient either he has Pneumonia or not using Chest X-ray.\n\nFollwing activities will be done to do so :\n- Load dataset and apply transforms(for Data Augmentation)\n- Load Dataset into batches \n- Plot Images \n- Fine-Tuning VGG-16 Model- Spatail Attentions\n- Train Model\n- Testset Results\n- Actual vs Predicted Plotting","06d860c5":"# plot the training loss plot","4d4601d5":"# Actual vs Predicted Plotting ","28ba1073":"# Buil model and train","2947ca76":"# Chest X-Ray Images (Pneumonia)\n","dbf7ea24":"* > ## Hi, Everyone, this is a modified version of notebook in python for Pneumonia classification","e1d4cc43":"# train a model","7337dc28":"## Fine-Tuning VGG-16","a01d9656":"### Loading Dataset and applying transform","200b8ea6":"# Testset Results","ec090dfc":" [](http:\/\/) # Finally, give thumbs up this kernel if you found it useful."}}