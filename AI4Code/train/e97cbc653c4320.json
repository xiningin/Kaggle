{"cell_type":{"c2d7f0d8":"code","9c8b5ec3":"code","d7ea6c58":"code","8dd88d88":"code","708bfe7b":"code","4396b563":"code","3e1dfe76":"code","262bd275":"code","ad0e6821":"code","4ffc5313":"code","7ac04e66":"code","57bc74e0":"code","4bff3fed":"code","81931633":"code","0d625ab0":"code","bf08de56":"code","6c14f4c2":"code","5d09642b":"markdown","911a18c0":"markdown","b8a417d1":"markdown","e88b3eab":"markdown","bedcf61b":"markdown","dfb74f27":"markdown","c2d357cf":"markdown","d1dd3def":"markdown","bfb119b6":"markdown","a15c529e":"markdown","76852493":"markdown","48d9fa11":"markdown","70ea3f2b":"markdown","ab7e8295":"markdown"},"source":{"c2d7f0d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c8b5ec3":"#Creating the DataFrame\ndataset = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","d7ea6c58":"#Let's take a look at the data we have available\ndataset.head()","8dd88d88":"#Firstly, we will drop the \"Time\" column\ndataset.drop(columns=\"Time\", inplace=True)","708bfe7b":"#Let's analyze how many frauds and no-frauds there are in our data\ndataset[\"Class\"].value_counts()","4396b563":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nXy = scaler.fit_transform(dataset.values)\nX = Xy[:, :-1]\ny = Xy[:, -1]","3e1dfe76":"# Creation of the train and test datasets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)","262bd275":"from imblearn.over_sampling import RandomOverSampler\ncc = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = cc.fit_resample(X_train, y_train)","ad0e6821":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\nenc.fit(y_train.reshape(-1,1))\ny_resampled = enc.transform(y_resampled.reshape(-1,1)).toarray()\ny_test = enc.transform(y_test.reshape(-1,1)).toarray()\n","4ffc5313":"from kerastuner.tuners import RandomSearch\nfrom tensorflow.keras import layers, Sequential, optimizers\n\ndef build_model(hp):\n    model = Sequential()\n    for i in range(hp.Int('num_layers', 1, 2)):\n        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=10, #m\u00ednimo n\u00famero de neuronas en la capa\n                                            max_value=50, #m\u00e1ximo n\u00famero de neuronas en la capa\n                                            step=10),\n                               activation='relu')),\n    model.add(layers.Dense(2, activation='softmax'))\n    opt = optimizers.Adam(learning_rate=0.005)\n    model.compile(optimizer=opt,\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n    return model\n","7ac04e66":"#max_trials is the number of combinations that will be tested. It is important to remark \n#that if it's not the number of total possibilities it won't be guaranteed that the result \n#obtained is the best one for our NN, it will be the best of the combinations tested.\n#This is used because testing all the possibilities will take a lot of time.\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5, \n    executions_per_trial=3,\n    directory='my_dir',\n    project_name='helloworld',\n    overwrite=True)","57bc74e0":"history = tuner.search(X_resampled, y_resampled,\n             epochs=5,\n             validation_data=(X_test, y_test))","4bff3fed":"models = tuner.get_best_models(num_models=3)\ntuner.get_best_hyperparameters(num_trials=1)[0].get_config()","81931633":"pred = np.argmax(models[0].predict(X_test), axis=1)\nreal = np.argmax(y_test, axis=1)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(real, pred)","0d625ab0":"model = keras.Sequential(\n    [\n        keras.layers.Dense(40, activation=\"relu\", name=\"initial\", input_shape=(29,)),\n        keras.layers.Dense(30, activation=\"relu\", name=\"hidden\"),\n        keras.layers.Dense(2, activation=\"softmax\", name=\"output\")  \n    ]\n)\nopt = keras.optimizers.Adam(learning_rate=0.005)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=[tf.keras.metrics.FalseNegatives()])","bf08de56":"history = model.fit(x=X_resampled,\n                  y=y_resampled,\n                  epochs = 10,\n                  validation_data=(X_test, y_test))","6c14f4c2":"pred = np.argmax(model.predict(X_test), axis=1)\nreal = np.argmax(y_test, axis=1)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(real, pred)","5d09642b":"# Data pre-processing","911a18c0":"## Creating the NN to be used","b8a417d1":"#### By modifiying some parameters we can reduce the False Positives but the False Negatives will increase, so we should understand what we want to achieve.","e88b3eab":"#### We are going to use kerastuner.tuners in order to find the best parameters for our NN. We will look for the number of layers (one or two) and the number of units per layer (10, 20, 30, 40 or 50).  \n#### Learning rate and activation functions are provided.\n#### Since the OneHotEncoder was performed, we need two units in the output layer. If it's not performed, just one should be added.","bedcf61b":"#### The results we obtain for this NN are:","dfb74f27":"#### Once that the best parameters have been found after several checks we can configure the NN that provides the best results.\n\n#### We will create a NN with an input layer of 40 units, a hidden layer of 30 units and an output layer of 2 units. Learning rate will be 0.005","c2d357cf":"#### Once the train and the test datasets have been created, we are going to carry out the oversampling just to the train dataset. \nWe have to be aware that the test dataset must be the real one, we shoulnd't do the oversampling to it.","d1dd3def":"#### To finish with the pre-processing part we are going to perform a OneHotEncoder to the Class column in the train and the test dataset. This step would be needed depending on the NN that is used. In our case, we need it.","bfb119b6":"#### Once the model has been trained and the predictions have been performed, we can analyze the results. We will use the confusion matrix to do so.","a15c529e":"#### Let's get the best 3 models and the parameters of the best one. As explained, this will change based on the combinations that will be tested. We won't obtained always the same result.","76852493":"# Neural Network modeling ","48d9fa11":"#### We have 492 frauds and 284315 no-frauds, which means just 0,17% of frauds. We are dealing with an imbalance dataset","70ea3f2b":"#### In this particular case the number of False Negative are more important than the number of False Positive, since the impact of the False Negative (a fraud not detected) will be more serious. That's what we have intented to reduce.","ab7e8295":"#### Although most of the columns are already normalized (a SVD has already being performed on the data in order to reduce its dimensionality), we are going to perform a normalization again to have all our columns normalized."}}