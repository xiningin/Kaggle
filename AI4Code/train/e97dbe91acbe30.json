{"cell_type":{"17854729":"code","be5e9535":"code","6c431a44":"code","47d6181f":"code","9b62b5bf":"code","a42f1754":"code","19843e6b":"code","f82e1115":"code","a4491887":"code","e36faf55":"code","c3eac7dc":"code","37788ab0":"code","585060a0":"code","f9853b9c":"code","09cd1355":"code","7d211664":"code","4a70b064":"code","2bd632fa":"code","75faa84c":"code","b959d56f":"code","30fd9600":"code","a300d54a":"code","8b8e33ae":"code","23c25578":"code","4f47609b":"code","5259615f":"markdown","a0fad05f":"markdown","f0b2b6ec":"markdown","0300ed6a":"markdown","052f97b9":"markdown","5b80184b":"markdown","abd17eb6":"markdown","ce0eb3ee":"markdown","0ab4b928":"markdown","b21786de":"markdown","fdde43a8":"markdown","547f3ad3":"markdown","5b4aefdf":"markdown","7db204ad":"markdown","c89fa41d":"markdown","9a6951a7":"markdown","7a7ce2a4":"markdown","65a0734f":"markdown","cecbecd6":"markdown","2a3582f7":"markdown","b25f4778":"markdown","a7bbb79f":"markdown","1945735d":"markdown","81767e23":"markdown","617eddee":"markdown","eb03d40f":"markdown","654b26a8":"markdown","e7ba3343":"markdown","d80dc695":"markdown","8f136957":"markdown","ab07e15d":"markdown","b69dcc6b":"markdown","b67c4263":"markdown","e02a0e9a":"markdown"},"source":{"17854729":"import numpy as np\nimport pandas as pd\nimport os, re\nfrom collections import OrderedDict\n\nbase_dir = (\"..\/input\")\ndf = pd.read_csv(os.path.join(base_dir,\"multipleChoiceResponses.csv\"))\ndf_text = pd.read_csv(os.path.join(base_dir,\"freeFormResponses.csv\"))\ndf_survey = pd.read_csv(os.path.join(base_dir, \"SurveySchema.csv\"))","be5e9535":"display(df.head())","6c431a44":"n=20\nprint('First {} columns: {}'.format(n, ', '.join(df.columns.values[:n])))\ndef count_column_name_formats(df):\n    print('Unique column name format:\\n{}'.format(df.columns.str.replace('[0-9]+','?').value_counts()))\ncount_column_name_formats(df)","47d6181f":"# Back up in case we want to go back to the raw data.\ndf_raw = df.copy()","9b62b5bf":"# Rename columns\n# Q?_OTHER --> Q?_OTHER_TEXT\ndf = df.rename(columns = lambda s: re.sub('^(Q[1-9]+[0-9]*_OTHER)$',r'\\1_TEXT',s))\n\n# Q?_MULTIPLE_CHOICE --> Q?\ndf = df.rename(columns = lambda s: re.sub('^(Q[1-9]+[0-9]*)_MULTIPLE_CHOICE$',r'\\1',s))","a42f1754":"# Split out metadata and times\ncolumn_headers, df = df.iloc[0], df.iloc[1:]\ntimes, df = df.iloc[:,0], df.iloc[:,1:]\n\ntext_cols = df.filter(regex='_TEXT$').columns\n# The text columns contain information about if there is text in freeFromResponses.csv or not.\n# but since the data has been randomized I don't see how it would every be useful.\ndf_text_before_randomizing =  df.filter(regex='_TEXT$')\ndf.drop(columns=df_text_before_randomizing.columns, inplace = True)","19843e6b":"%%time\n# Times are numeric\ntimes = pd.to_numeric(times)\n\n# Text columns are integer (essentially boolean)\ndf_text_before_randomizing = df_text_before_randomizing.apply(pd.to_numeric)\n\n# Convert remaining colums to categorical\ndf = df.astype('category')","f82e1115":"assert (times >=0).all(), \"Times should be non-negative.\"\nassert ((df_text_before_randomizing == -1) | (df_text_before_randomizing >= 0)).all().all(),\\\n\"Values in text columns should equal -1 or be non-negative.\"\n\n# Q? columns are for \"select one\"-type questions\nselect_one_cols = df.filter(regex='^Q[1-9]+[0-9]*$').columns\n\n# Q?_Part_? columns are for select-any questions \nselect_any_cols =  df.filter(regex='^Q[1-9]+[0-9]*_Part_[1-9]+[0-9]*$').columns\n\nassert (df[select_one_cols].apply(lambda c: len(c.cat.categories), axis=0) > 1).all(),\\\n\"select-one column `Q?_Part_?` should have more than 1 (non-NaN) category.\"\nassert (df[select_any_cols].apply(lambda c: len(c.cat.categories), axis=0) == 1).all(),\\\n\"select-any column `Q?_Part_?` should have exactly 1 (non-NaN) category.\"","a4491887":"ncat = df[select_any_cols].apply(lambda c: len(c.cat.categories), axis=0)\nproblem_ncat = ncat[ncat != 1]\nprint(\"Number of categories\/column:\\n{}\".format(problem_ncat.sort_values(ascending=False)))\n\n# \"Hack\" to compactly extract the question label from the columns without changing their order,\n# there is probably a better way... suggestions?\nproblem_questions =  list(OrderedDict({idx[:idx.find('_')]: '' for idx in problem_ncat.index}))\n\nprint(\"Questions with != 1 category: {}\".format(', '.join(problem_questions)))","e36faf55":"first_cols = OrderedDict()\nfor col in problem_ncat.index:\n    q = re.sub('^(Q[1-9]+[0-9]*).*$',r'\\1', col)\n    if q in first_cols:\n        continue\n    else:\n        first_cols[q] = col\n\nfor question, col in first_cols.items():\n    print(' - '.join([question, column_headers[col]]))","c3eac7dc":"# Recognized question types\nQTYPES = ['select one', 'select any', 'multiple select one', 'percentages']\n\ndef determine_question_type(q, headers):\n    assert 'QTYPES' in globals(), \"Question types QTYPES needs to be defined\"\n\n    # Only checks columns 'Q?' or 'Q?_Part_?'\n    q_headers = headers.filter(regex='{}_Part_[1-9]+[0-9]*$|^{}$'.format(q,q))\n\n    ncols = len(q_headers)\n    if ncols == 0:\n        raise ValueError('no columns found for {}'.format(q))\n        \n    # The conditions for determining the question type are not fully determinant.\n    # Therefore the types are listed in order of the strictness of the condition.\n    header_0 = q_headers.iloc[0].lower()\n    \n    has_one_col = (ncols == 1)\n    consistent_qtype = OrderedDict({'select one': has_one_col, \n                                    'select any': not has_one_col and 'select all that apply' in header_0,\n                                    'percentages': not has_one_col and '100%' in header_0,\n                                    'multiple select one': not has_one_col and (ncols > 1)})\n\n    assert set(consistent_qtype).issubset(set(QTYPES)), 'question type should be globally known'\n\n    # Return the first consistent type\n    for qtype, is_consistent in consistent_qtype.items():\n        if is_consistent:\n            return qtype;\n\n    raise ValueError('Could not determine question type')","37788ab0":"# Question labels Q?. Not so elegant, but extracts the unique question label and maintaining the order.\nqs = list(OrderedDict({re.sub('^(Q[1-9]+[0-9]*).*$',r'\\1', col): None for col in df.columns}))\n\n# Keep two convenience dicts\n# One mapping the Q? to question type\nquestion_type = dict()\nfor q in qs:\n    question_type[q] = determine_question_type(q, column_headers)\nprint(' | '.join( ': '.join ([str(k),str(v)]) for k, v in question_type.items()))\n\n# and one mapping to the question string.\nquestions = dict()\nfor q in qs:\n    # Extract the question string from the first header starting with q\n    for col, header in zip(column_headers.index, column_headers):\n        if col.startswith(q):\n            questions[q] = header.partition(' - ')[0]\n            break;\n    assert q in questions, \"question for {} not found\".format(q)","585060a0":"# Helper function. Return columns for question q in dataFrame df.\ndef cols_for_question(q, df):\n    return df.filter(regex='^{}$|^{}_.*$'.format(q, q)).columns\n\n# Set dtype to numeric for all \"percentages\" questions.\nfor q, qtype in question_type.items():\n    if (qtype == 'percentages'):\n        for col in cols_for_question(q, df):\n            df[col] = pd.to_numeric(df[col])                        ","f9853b9c":"def check_data(q, df, headers):\n    # Note format for output\n    note = '{q} at {loc} type \\\"{qtype}\\\" {msg}'\n        \n    qtype = determine_question_type(q, headers)\n\n    # Check only on non-text columns\n    cols = df.filter(regex='^{}$|^{}_Part_[1-9]+[0-9]*$'.format(q, q)).columns\n    ncats =  df[cols].apply(lambda col: len(col.astype('category').cat.categories), axis=0)\n    \n    if qtype == 'select one':\n        # <= 1 categories in this case is suspicious, but could be due to\n        # skewed or not enough data so just give a note.\n        for col, ncat in zip(cols, ncats):\n            if ncat <= 1:\n                loc = \"column {}\".format(col)\n                msg = \"has {ncat} categories\".format(ncat=ncat)\n                print(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n    elif qtype == 'select any':\n        # Typically 1 category, but could be 0 if there is limited data\n        for col, ncat in zip(cols, ncats):\n            loc = \"column {}\".format(col)\n            msg = \"has {ncat} categories\".format(ncat=ncat)\n            if ncat < 1:\n                print(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n            if ncat > 1:\n                raise ValueError(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n    elif qtype == 'multiple select one':\n        # Theoretically, each column should have the same categories,\n        # but this may not be true for the actual data, so just give a note.\n        df_cats = df[cols].apply(lambda col: col.astype('category').cat.categories, axis=0)\n        \n        # Check if categories in all columns are equal by comparing to the union of all categories.\n        cats_union = set(df_cats.values.flatten())\n        for col in cols:\n            cats = set(df_cats[col])\n            if cats != cats_union:\n                loc = \"column {}\".format(col)\n                msg = \"has categories:\\n{cats}\\n but union with other columns is\\n{union}\"\\\n                .format(cats=cats, union=cats_union)\n                print(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n    elif qtype == 'percentages':\n        # Values should be numeric\n        try:\n            df_num = df[cols].apply(pd.to_numeric)\n        except ValueError as err:\n            loc = \"column {}\".format(col)\n            msg = \"failed to convert to numerical type\"\n            print(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n            raise\n        \n        # Percentage values should be >=0 and <=100\n        in_range = df_num.isna() | ((df_num <= 100) & (df_num >= 0))\n        if not in_range.all().all():\n            # find the data (rows) which are out of range\n            col_not_in_range = ~(in_range.all(axis=1))\n            row_not_in_range = ~(in_range.all(axis=0))\n        \n            rows = col_not_in_range.index[col_not_in_range].values\n            cols = row_not_in_range.index[row_not_in_range].values\n            loc = \"columns {cols}, rows {rows}\".format(cols=cols, rows=rows)\n            msg = \"has values out of allowed range [0,100]\"\n            raise ValueError(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n        # Values should add to 100.\n        sums = df_num.dropna().sum(axis=1)\n        if not (sums == 100).all():\n            bad_sums = sums[sums != 100]\n            bad_indices = bad_sums.index\n\n            note_alt_2 = 'Note: at {loc} type \\\"{qtype}\\\" {msg}'\n            loc = \"rows {}\".format(bad_indices.values)\n            msg = \"has incorrect sums {} (not equal to 100)\".format(bad_sums.values)\n            raise ValueError(note.format(q=q, qtype=qtype, loc=loc, msg=msg))\n    else:\n        raise ValueError(\"Unknown question type {}\".format(qtype))","09cd1355":"# Print notes and errors found by check_data()\nfor q in qs:\n    try:\n        check_data(q, df, column_headers)\n    except ValueError as err:\n        print(' '.join([str(err),'(error)']))","7d211664":"# Print basic info about question Q34\nq = 'Q34'\ndf_q = df.filter(cols_for_question(q=q, df=df))\ni_suspect = 3\n\n# Header for question q\nheaders_q = column_headers[df_q.columns]\n\n# Print question\nprint(': '.join([q, questions[q]]))\n\n# Print column info.\n# Each column maps to a component associated with a percentage value.\nprint(' | '.join(['column', 'component', 'value (%)']))\nperc_sum = 0\nfor col, header, val in zip(headers_q.index, headers_q, df_q.loc[i_suspect]):\n    component = header.split(' - ')[-1]\n    print(' | '.join([col, component, str(val)]))\n    perc_sum += val\nprint(\"Total sum = {}\".format(str(perc_sum)))","4a70b064":"cols = cols_for_question(q=q, df=df_text_before_randomizing)\nprint(' | '.join(['column', 'component']))\nfor col in cols:\n    component  =  column_headers[col].split( ' - ')[-1]\n    print(' | '.join([col, component]))\n\ndisplay(df_text_before_randomizing[cols].describe())","2bd632fa":"print(\"Some statistics:\")\ncols = cols_for_question(df=df_text, q=q)\nprint(df_text[cols].iloc[1:].apply(pd.to_numeric).describe())","75faa84c":"# Add the missing column\nold_cols = cols_for_question(df=df, q=q)\nnew_col = '{q}_Part_7'.format(q=q)\n\n# Add what's missing from 100%\ndf[new_col] = 100 - df[old_cols].sum(1)\n\n# Rows with NaN summed to zero above, but should be NaN also in the new column.\nhas_nan_on_row = df.loc[:,old_cols].isna().any(1)\ndf.loc[has_nan_on_row, new_col] = np.nan","b959d56f":"q, i ='Q35', 2823\ndisplay(df.loc[i, cols_for_question(df=df, q=q)])","30fd9600":"df.drop(i, inplace = True) # too easy!","a300d54a":"q = 'Q47'\nprint(questions[q])","8b8e33ae":"cols = cols_for_question(df=df, q=q)\ndisplay(df[cols[-1]].dropna().head(10))","23c25578":"# Fix the typo by renaming the column\nold_name, new_name = 'Q47_Part_16', 'Q47_OTHER_TEXT'\ndf.rename(columns={old_name: new_name}, inplace=True)\n\n# Copy data to text dataset\ndf_text[new_name] = df[new_name].astype('object')\n\n# Just for consistency, keep a column in the text dataFrame that was split off,\n# changing NaN to -1 and strings to 0.\ndf_text_before_randomizing[new_name] = df[new_name].replace(to_replace = r'.*', regex=True, value='0').fillna(-1).astype('int64')\n\n# Finally remove it from main data\ndf = df.drop(columns=[new_name])","4f47609b":"# Print notes and errors found by check_data()\nfor q in qs:\n    try:\n        check_data(q, df, column_headers)\n    except ValueError as err:\n        print(' '.join([str(err),'(error)']))","5259615f":"The reshuffling however means that we can't make use of that data anymore. Nonetheless, because there is only one column missing, it's not neeeded since we know what value makes the sum correct. So the tasks now are:\n* add column `Q34_Part_7` to `df`\n* fill it with missing percentage  by enforcing the column sum to equal 100.\n* delete columns `Q34_OTHER_TEXT`","a0fad05f":"We can now understand the origin of the problem. The percentage values of the component \"Other\" should have been put in a column `Q34_Part_7` which was however incorrectly called`Q34_OTHER_TEXT`. Because the naming implies text data, its contents have ended up being split out, reshuffled and put into `freeFormResponses.csv`. Indeed, in `df_text['Q34_OTHER_TEXT']` we find such fitting numerical data, not text!","f0b2b6ec":"# First observations<a name=\"First-observations\"><\/a>\nWith this type of simple analysis and visual inspection I was able to postulate the following about the contents and format of the dataset and compile it into the following pretty boring but useful list. Note my use of `?` as a placeholder for an integer.\n* The first row  stores strings describing the column data, typically a question string and the info about the answer options.\n* The first column stores the time the respondent spent on taking the survey.\n* Questions are labelled by `Q?`.\n* Two main types of questions:\n    * \"select one\", e.g. `Q1`:\n    > \"What is your gender?\"\n    * \"select any\" (\"select all that apply\"), e.g. `Q11`:\n    > Select any activities that make up an important part of your role at work: (Select all that apply)*\n* Columns named `Q?` belong to \"select one\"-type questions.\n    * Possible values are `NaN` or either of the answer strings for the question (categorical)\n    * A`NaN`value means the question was not answered.\n* Columns named `Q?_Part_?` belong \"select any\"-type questions.\n    * Each column has info about if a certain answer option was selected.\n    * The answer that the column corresponds to can be found at the end of the string on the first row. E.g. in `Q11_Part_1`:\n    > *Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyze and understand data to influence product or business decisions*\n    * There are two possible values: `NaN` or the answer string of the column (binary categorical)\n    * A value of `NaN` means either that the option was not selected or the question was not answered. \n* Columns with names suffixed by `_TEXT`  has data on whether additional text was provided or not.\n    * `OTHER_TEXT` generally refers to text for the selection \"Other\" .\n    *  `Part_?_TEXT`  refers to text for one out of multiple selections having text data.\n    * A value of `-1` means no text data was provided.\n    * A non-negative integer value  means that text data was provided (stored in the free text file).\n* There are two column types which only occur once, `Q?_MULTIPLE_CHOICE` and `Q?_OTHER`. Is there something special about them?\n\nPhew, that's quite a list!","0300ed6a":"I checked the time here because before I split out the text columns from `df` this `dtype` conversions to `category` took > 10 s. I haven't investigated why it's so much faster doing `astype` without filtering on columns, maybe someone knows?\n\nFinally, make value checks based on the column and question type:","052f97b9":"# A function that checks the data<a name=\"A-function-that-checks-the-data\"><\/a>\nAfter determining the question type, the data can be checked to make sure it's consistent with assumptions and constraints.","5b80184b":"Change types:","abd17eb6":"Extract headers from data and split datasets:","ce0eb3ee":"I'm just going to remove you, sorry!","0ab4b928":"# First fixes<a name=\"First-fixes\"><\/a>\nBased on these first observations, there is already some stuff to do:\n* The single column with name format `Q?_OTHER` is a typo and should be `Q?_OTHER_TEXT` to be consistent.\n* Similarly for `Q?_MULTIPLE_CHOICE`; it belongs to a \"select one\" question and should be renamed to `Q?`.\n* Split `_TEXT` columns and store separately.\n* Split out the first row with info about the column (metadata) and store separately.\n* Split the survey times from the actual questions and store separately. Make sure times are numeric and `>=0`.\n* The`_TEXT`columns mix string and numeric types. Change to numeric and set `dtype`.  Note that even though this data won't be used  here, it's worth ensuring that they contain what we think they do.\n* Make sure the only negative value of `_TEXT` columns is  `-1` (if there are others I wouldn't know how to interpret them).\n* Set `dtype` of `Q?` and `Q?_Part_?` columns to `category`  to save some space.\n* Make sure `Q1` has `> 1` categories and `Q?_Part_?` has exactly 1 category (not counting `NaN`-values).","b21786de":"# And it's a wrap!<a name=\"And-it's-a-wrap!\"><\/a>\nA final check on the values shows that the data is now clean.","fdde43a8":"# New findings, new problems<a name=\"New-findings,-new-problems\"><\/a>\nFrom the output above we learn:\n* `Q34`, `Q35` are of a different type than I knew about, where percentages are to be specified by the respondent. For this, columns should have `dtype` `float64` rather than `category`\n* `Q38` had zero categories in two columns, corresponding to \"Towards Data Science Blog\" and \"Analytics Vidhya\", meaning not a single person selected them. This is surprising to me at least, since they pop up in my scope every now and then. And is it just a coincidence that it's the two last columns that are zero...? In any case, this shows that my check was too strict, assuming there is \"enough\" data everywhere.\n*  `Q39` and `Q41` also correspond to a new question type: multiple similar \"select one\" questions are bunched together under the same question label. Each column corresponds to a different \"select one\" question.\n* `Q47` is more fishy!  One column, `Q47_Part_16` out of at least 16 has >100 categories while the rest has 1. What's going on?\n\nI'll now go through these issues in more detail and and fix them one-by-one.","547f3ad3":"# Data gone missing in Q34<a name=\"Data-gone-missing-in-Q34\"><\/a>\nWe can look at the suggested fishy data point with index 3 to verify that the percentages (in `Part_1` to `Part_6`) do not add to 100.","5b4aefdf":"# Introduction\nWelcome to my first public kernel where I'll do some thorough **data cleaning** of the Kaggler 2018 survey dataset. Why would I focus on this aspect? My initial idea for a project was the role of gender for data scientists. I wanted to automatically pinpoint interesting, significant differences between men and women and let that initial wide scan be a guide for subsequent deeper analysis.  I realized however that implementing this without too much hardcoding was a bit of a project in itself, so this is it!  In addition, a main goal for me was to get familiar with the Python library pandas and data cleaning is the perfect task for that.","7db204ad":"# Cleaning checklist<a name=\"Cleaning-checklist\"><\/a>\nTypical things to look out for:\n* typos or inconsistent naming in strings\n* inconsistent types (e.g. string vs numerical values).\n* broken constraints on numerical values, e.g. a negative time interval.\n\nIn my experience, adding checks (e.g. `assert`) is a great way to detect problems. This might also catch errors in your own initial assumptions or understanding about the data.\n\nI find it can also be convenient to:\n* change naming conventions\n* sort out data from metadata\n* split data into multiple datasets if they'll be analyzed independently, or only a subset is of interest.","c89fa41d":"Some of this we already knew, we still need to figure out `Q47`,  but the value errors of `Q34` and `Q35` are new. Good to find out about! ","9a6951a7":"# Lessons learned<a name=\"Lessons-learned\"><\/a>\nWhen you're handed a dataset without being able talk to directly the people who compiled the dataset, it can really be a detective work to even understand what the data *is*. In the case of the Kaggle survey, there were several different question types and they were encoded in different ways, sometimes with typos in either the data or the format. \n\nThe simple workflow I used here to explore and clean the data has been:\n* First, visually inspect the data\n* Describe the data format  (e.g. as a list of the different column types and constraints on their content)\n* Decide how you want to analyze the data and extract the relevant data.\n* Perform automated tests based on the description of the data you made. If the tests fail either the test is wrong or the data is wrong.\n* Iterate!\n\nCleaning is certainly not the most fun part of data analysis, but I think it's good practice carry out this step systematically at the get-go with so you don't get surprises and mess to sort out later on!","7a7ce2a4":"and change the value of `dtype` for all \"percentages\" questions to numeric.","65a0734f":"and we expect each column to be binary categorical variable (`NaN` or the answer string), but the last \"part\" had 127 categories. Let's look at the values:","cecbecd6":" The assertion for the number of categories being equal to 1 in `Q?_Part_?` failed! This means something is wrong in the data or with my assumptions. A closer look at the reason:","2a3582f7":"Rename columns:","b25f4778":"# The data<a name=\"The-data\"><\/a>\nIn October 2018, thousands of Kagglers participated in a survey, answering questions about themselves in their role as data scientists. See the full description of the survey [here](https:\/\/www.kaggle.com\/kaggle\/kaggle-survey-2018\/home).  The data consists of the following files:\n\n* `multipleChoiceResponses.csv`: all survey response data except free form text responses\n* `freeFormResponses.csv`: the free form text responses, order reshuffled.\n*  `SurveySchema.csv`: conditions for who gets to answer which questions\n\n** I will focus on the main dataset** `multipleChoiceResponses.csv` and will leave `freeFormResponses.csv` for the NLP lovers out there. The contents of  `SurveySchema.csv` was not obvious to me initially, as is clear from [these questions](https:\/\/www.kaggle.com\/kaggle\/kaggle-survey-2018\/discussion\/71315) I had. In any case, it would mainly be useful for figuring out why some people haven't responded to certain questions. As is clear from the discussion thread linked above, this can happen for either of the following reasons: the respondent\n1. isn't eligible to answer (e.g. a question about the current job for someone who is unemployed)\n2. aborts the survey before answering all questions\n3. skips a question (made possible by mistake for certain questions).\n\n\n# Objectives \nMy goal is to automatically generate basic statistics and plots from the dataset to get an overview. To achieve this I need to:\n* understand what different types of question there are\n* understand how data is encoded for each question\n* decide what basic plots and statistics to generate for each question type\n* write code for extracting data for all questions and presenting it compactly as plots and statistics\n\nFor the last step to work smoothly, **the data needs to be clean and tidy**.","a7bbb79f":"#  Trolling attempt in Q35?<a name=\"Trolling-attempt-in-Q35\"><\/a>\nNow to `Q35`. Respondent no. 2823 managed to sneak in some illegitimate percentage values; apparently it was possible!","1945735d":"# Exploration of data contents and format<a name=\"Exploration-of-data-contents-and-format\"><\/a>\nThe main dataset is stored in `df`.  A first visual inspection gives an idea of what's in it:","81767e23":"We can now map each question to its type.","617eddee":"Now check all questions.","eb03d40f":"What are these questions?","654b26a8":"* [Introduction](#Introduction)\n* [The data](#The-data)\n* [Objectives](#Objectives)\n* [Imports and file loading](#Imports-and-file-loading)\n* [Exploration of data contents and format](#Exploration-of-data-contents-and-format)\n* [First observations](#First-observations)\n* [Cleaning checklist](#Cleaning-checklist)\n* [First fixes](#First-fixes)\n* [New findings, new problems](#New-findings,-new-problems)\n* [More question types](#More-question-types)\n* [A function that determines the question type](#A-function-that-determines-the-question-type)\n* [A function that checks the data](#A-function-that-checks-the-data)\n* [Data gone missing in Q34](#Data-gone-missing-in-Q34)\n* [Trolling attempt in Q35?](#Trolling-attempt-in-Q35?)\n* [Text data hiding out in Q47](#Text-data-hiding-out-in-Q47)\n* [And it's a wrap!](#And-it's-a-wrap!)\n* [Lessons learned](#Lessons-learned)","e7ba3343":"# Text data hiding out in Q47<a name=\"Text-data-hiding-out-in-Q47?\"><\/a>\nFinally, we get to sorting out the fishy column of`Q47`. This is a \"select any\"-type question:","d80dc695":"Looking at the columns names and frequencies provides a more quantitative overview.","8f136957":"# A function that determines the question type<a name=\"A-function-that-determines-the-question-type\"><\/a>\nRemembering my goal of automatically (without hardcoding) determining the question type from the data, one realizes that the column names are not enough since different question types all have `Q?_Part_?` columns. But, as we saw with `Q38` looking at the data itself is  also a fragile approach since it assumes a certain amount of data for it to be efficient. Therefore, I'll use the column formats together with the question string itself to determine the question type, and then use the data for consistency checks.","ab07e15d":"# More question types<a name=\"More-question-types\"><\/a>\nFor `Q34`, `Q35`, `Q39`, `Q41` the problem were my initial assumptions. We see that the data has two more question types with a different format:\n* \"percentages\", `Q34` and `Q35`.\n    * the respondent is asked to fill in the relative contribution of different components to some task.\n    * each `Q?_Part_?` contains a percentage value\n    * Values are or a number or `NaN` if the question was not answered.\n    * numerical constraints: 1) each value >=0 and <= 100;  2) sum of values = 100.\n    * `dtype`: `float64`\n* \"multiple select one\", `Q39` and `Q41`\n    * multiple similar select-one questions are listed under the same question label.\n    * each `Q?_Part_?` is of the same format as  a select-one `Q?` column.\n    * possible answers identical for different questions so they can be compared (e.g. `Q39`: `'Much better', 'Much worse', 'Neither better nor worse','No opinion; I do not know', 'Slightly better', or 'Slightly worse'`).\n    * `dtype`: `category`","b69dcc6b":"Oops! (And hello Siraj!) This is text data that should've been reshuffled and put into `freeFormResponses.csv`, but somehow this column got mislabeled; it should have been `Q47_OTHER_TEXT`.  Because it now didn't get the `_TEXT` suffix, it didn't either get treated as text in the post-processing.  To fix this I'll rename the column and move the text data to the text dataset (and change to correct `dtype`).","b67c4263":"Ok... What could be missing? Our only chance is that it was by mistake stored in a `_TEXT` columns, which we prevously split off into a separate table.  Indeed, we find a missing column for the option \"Other\" there. However, we see that these values are not the missing percentages (since they are not in [0,100]).","e02a0e9a":"# Imports and file loading<a name=\"Imports-and-file-loading\"><\/a>\nWe start off with some basic imports and file loading."}}