{"cell_type":{"1508697b":"code","32506acd":"code","557af9a2":"code","43f97c95":"code","1872e751":"code","a9db718a":"code","7e925bc8":"code","63f2265f":"code","8aca87b1":"code","9e4a28f5":"code","fedc1455":"code","20da3600":"code","2f3d62a7":"code","a72cbe09":"code","f80371b8":"code","ef78e44c":"code","1df56df4":"code","d0c5fdd4":"code","77d37e91":"code","1145eea1":"code","75e3f1b5":"code","7621e29e":"code","d29e71d9":"code","e5e8805b":"markdown","fd726424":"markdown","01a74a3c":"markdown","dd63ce4e":"markdown","ca6f2183":"markdown","9e020795":"markdown","c29f4601":"markdown","bd7da9c5":"markdown","8e21fbd4":"markdown","97ea8887":"markdown","99035b1a":"markdown","b344a8b7":"markdown","bc848ec6":"markdown","f7485278":"markdown","7b965691":"markdown","f0d6f8f5":"markdown","1d2c1ef3":"markdown","50c4b53f":"markdown"},"source":{"1508697b":"import os\nimport cv2\nimport glob\nimport torch\nimport datetime\nimport numpy as np\nfrom PIL import Image\nimport torch.nn as nn\nimport albumentations as A\nfrom torch import from_numpy\nimport torch.nn.functional as F\nfrom torch.nn import DataParallel\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot as plt\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import SGD, Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader","32506acd":"# Data path\nINPUT_PATH = '..\/input\/cityscapes-image-pairs\/cityscapes_data'","557af9a2":"INPUT_IMG_SIZE = 256\n\nOUTPUT_CLASSES = 12\n\nLEARINING_RATE = 0.01\n\nOPIMIZER_NAME = 'Adam'\n    \nSCHEDULER_NAME = 'stepLR'\n\nBATCH_SIZE = 8\n\nMAX_EPOCHS = 10\n\nAUGMENTATION = False \n\n# MEAN = (0.485, 0.456, 0.406)\n\n# STD = (0.229, 0.224, 0.225)\n\n# MEAN = (0.5,0.5,0.5) \n\n# STD = (0.5,0.5,0.5) \n\nMEAN = None\n\nSTD = None \n\nTHRESH_ACC_CKP = 0.9\n\nSCHEDULER_STEP_SIZE = 4\n\nUSE_CUDA = torch.cuda.is_available()","43f97c95":"train_files = glob.glob(os.path.join(INPUT_PATH + '\/train', '*jpg'))\ntest_files = glob.glob(os.path.join(INPUT_PATH + '\/val', '*jpg'))\n\nprint('Total train images:', len(train_files))\nprint('Total test images:', len(test_files))\n\n","1872e751":"fig, axes = plt.subplots(1,2, figsize = (20,5))\nfor i, ax in enumerate(axes.ravel()):\n    img = cv2.imread(train_files[i])\n    ax.imshow(img)\n    #ax.axis('off')\nplt.show()","a9db718a":"for i in train_files[:4]:\n    org_img = cv2.imread(i)\n    img = org_img[:, 0:256,:]\n    msk = org_img[:, 256:,:]\n    img = img\/255\n    \n    print('-'*40)\n    print('Original image shape:',img.shape)\n    print('Mask image shape:',msk.shape)\n    print('-'*40)\n    \n    plt.figure(figsize=(12,8))\n    plt.subplot(1,2,1)\n    plt.imshow(img)\n    plt.title('Original image')\n    plt.subplot(1,2,2)\n    plt.imshow(msk)\n    plt.title('Mask (RGB)')\n    plt.show()\n    ","7e925bc8":"kmeans_data = []\nfor i in train_files[:50]:\n    org_img = cv2.imread(i)\n    #img = org_img[:, 0:INPUT_IMG_SIZE,:]\n    msk = org_img[:, INPUT_IMG_SIZE:,:]\n    kmeans_data.append(msk)\nkmeans_data = np.array(kmeans_data)\nkmeans_data = kmeans_data.reshape(-1,3)\nprint(kmeans_data.shape)","63f2265f":"# train kmeans with 10 clusters\nencoder = KMeans(n_clusters=OUTPUT_CLASSES)\nencoder.fit(kmeans_data)","8aca87b1":"colors = {0: [255,0,0],\n          1: [0,255,0],\n          2: [0,0,255],\n          3: [255,69,0],\n          4: [255,0,255],\n          5: [210,105,30],\n          6: [192,255,62],\n          7: [127,255,0],\n          8: [0,238,238],\n          9: [72,118,255],\n          10: [72,255,255],\n          11: [255,118,255]\n          }","9e4a28f5":"# encoding with trained kmeans\nfor i in train_files[:3]:\n    org_img = cv2.imread(i)\n    #img = org_img[:, 0:256,:]\n    msk = org_img[:, 256:,:]      #shape (256,256,3)\n    test = msk.reshape(-1,3)      #shape (65536,3)\n    pred = encoder.predict(test)  #shape (65536)\n    \n    enc_pred = pred.reshape(INPUT_IMG_SIZE,INPUT_IMG_SIZE)  #shape (256,256)\n    \n    pred = np.array([colors[p] for p in pred]).reshape(256,256,3)\n    \n    print('No of classes in encoded mask:',np.unique(enc_pred))\n    \n    plt.figure(figsize=(15,10))\n    plt.subplot(1,2,1)\n    plt.imshow(msk)\n    plt.title('Original mask (RGB)')\n    plt.subplot(1,2,2)\n    plt.imshow(enc_pred)\n    plt.title('Encoded mask')\n    plt.show()","fedc1455":"class Dataset(Dataset):\n    def __init__(self, images_list, size, augmentation = False, mean = None, std = None):\n        self.images_list = images_list\n        self.size = size\n        self.augmentation = augmentation\n        if mean is None or std is None:\n            self.mean = [0., 0., 0.]\n            self.std = [1., 1., 1.]\n        else:\n            self.mean = mean\n            self.std = std\n        \n    def __len__(self):\n        return len(self.images_list)\n\n    def __getitem__(self, index):\n        pil_image = Image.open(self.images_list[index]).convert('RGB')\n        org_img = np.array(pil_image)\n        \n        np_image = org_img[:, 0:256,:]\n        np_target = org_img[:, 256:,:] \n        \n        test = (np_target.reshape(-1,3))      \n        pred = encoder.predict(test)\n        seg_msk = pred.reshape(256,256)\n        \n        if self.augmentation:\n            trans_obj = A.Compose([A.Resize(self.size, self.size),\n                           #A.OneOf([A.ToSepia()], p=0), \n                           A.Rotate(10, border_mode=4, p=0.5),\n                           A.HorizontalFlip(p=0.3),\n                           A.VerticalFlip(p=0.3),\n                           #GridDistortion(num_steps=100,distort_limit=0.3,p=1),\n                           #InvertImg(p=0.4),\n                           #A.CLAHE(clip_limit=4, p=0),\n                           A.OneOf([A.OpticalDistortion(),A.ElasticTransform()],  p=0.4),  \n                           #A.OneOf([A.GaussNoise(), A.GaussianBlur()], p=0),\n                           A.CoarseDropout(max_holes=30, max_height=int(self.size*0.015),\n                                         max_width=int(self.size*0.015), fill_value=255, p=0.4),\n                           A.RandomBrightnessContrast(brightness_limit=0.3,p = 0.4),\n                           A.RandomSnow(snow_point_lower=0.4,snow_point_upper=0.5,brightness_coeff=1.5,p = 0.4),\n                           #RandomGamma(gamma_limit=(200,200),p = 1),\n                           #A.RGBShift(p = 0),\n                           A.Normalize(self.mean, self.std)\n                          ])\n        else:\n            trans_obj = A.Compose([A.Resize(self.size, self.size),\n                                   A.Normalize(self.mean, self.std)])\n            \n        transformed = trans_obj(image = np_image, mask = seg_msk)\n        img_tensor = from_numpy(transformed['image']).permute(2, 0, 1)\n        mask_tensor = from_numpy(transformed['mask'])\n        return img_tensor, mask_tensor\n    ","20da3600":"class UNet(nn.Module):\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n        return output_out","2f3d62a7":"class UNetTunable(nn.Module):\n    def __init__(\n        self,\n        in_channels=1,\n        n_classes=2,\n        depth=5,\n        wf=6,\n        padding=False,\n        batch_norm=False,\n        up_mode='upconv',\n        conv_mode='standard'\n    ):\n        \"\"\"\n        Implementation of\n        U-Net: Convolutional Networks for Biomedical Image Segmentation\n        (Ronneberger et al., 2015)\n        https:\/\/arxiv.org\/abs\/1505.04597\n        Using the default arguments will yield the exact version used\n        in the original paper\n        Args:\n            in_channels (int): number of input channels\n            n_classes (int): number of output channels\n            depth (int): depth of the network\n            wf (int): number of filters in the first layer is 2**wf\n            padding (bool): if True, apply padding such that the input shape\n                            is the same as the output.\n                            This may introduce artifacts\n            batch_norm (bool): Use BatchNorm after layers with an\n                               activation function\n            up_mode (str): one of 'upconv' or 'upsample'.\n                           'upconv' will use transposed convolutions for\n                           learned upsampling.\n                           'upsample' will use bilinear upsampling.\n        \"\"\"\n        \n        super(UNetTunable, self).__init__()\n        assert conv_mode in ('standard', 'dilated')\n        assert up_mode in ('upconv', 'upsample', 'dilated')\n        \n        self.padding = padding\n        self.depth = depth\n        prev_channels = in_channels\n        self.down_path = nn.ModuleList()\n        for i in range(depth):\n            self.down_path.append(\n                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm,conv_mode)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.up_path = nn.ModuleList()\n        for i in reversed(range(depth - 1)):\n            self.up_path.append(\n                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm,conv_mode)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        blocks = []\n        for i, down in enumerate(self.down_path):\n            x = down(x)\n            if i != len(self.down_path) - 1:\n                blocks.append(x)\n                x = F.max_pool2d(x, 2)\n\n        for i, up in enumerate(self.up_path):\n            x = up(x, blocks[-i - 1])\n\n        return self.last(x)\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_size, out_size, padding, batch_norm,conv_mode):\n        super(UNetConvBlock, self).__init__()\n        \n        if conv_mode == 'standard':\n            block = []\n            block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n            block.append(nn.ReLU())\n            if batch_norm:\n                block.append(nn.BatchNorm2d(out_size))\n            block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n            block.append(nn.ReLU())\n            if batch_norm:\n                block.append(nn.BatchNorm2d(out_size))\n            self.block = nn.Sequential(*block)\n            \n        elif conv_mode == 'dilated':\n            block = []\n            block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding), dilation = 1 ))\n            block.append(nn.ReLU())\n            if batch_norm:\n                block.append(nn.BatchNorm2d(out_size))\n            block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding), dilation = 1 ))\n            block.append(nn.ReLU())\n            if batch_norm:\n                block.append(nn.BatchNorm2d(out_size))\n            self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        out = self.block(x)\n        return out\n\n\nclass UNetUpBlock(nn.Module):\n    def __init__(self, in_size, out_size, up_mode, padding, batch_norm, conv_mode):\n        super(UNetUpBlock, self).__init__()\n        if up_mode == 'upconv':\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n        elif up_mode == 'upsample':\n            self.up = nn.Sequential(\n                nn.Upsample(mode='bilinear', scale_factor=2),\n                nn.Conv2d(in_size, out_size, kernel_size=1),\n            )\n        elif up_mode == 'dilated':\n            self.up = nn.Sequential(\n                nn.Upsample(mode='bilinear', scale_factor=2),\n                nn.Conv2d(in_size, out_size, kernel_size=1),\n            )\n\n        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm, conv_mode)\n\n    def center_crop(self, layer, target_size):\n        _, _, layer_height, layer_width = layer.size()\n        diff_y = (layer_height - target_size[0]) \/\/ 2\n        diff_x = (layer_width - target_size[1]) \/\/ 2\n        return layer[\n            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n        ]\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        crop1 = self.center_crop(bridge, up.shape[2:])\n        out = torch.cat([up, crop1], 1)\n        out = self.conv_block(out)\n\n        return out","a72cbe09":"class Trainer:\n    def __init__(self,               \n                 model,                          # Model to be trained.\n                 crit,                           # Loss function\n                 train_data = None,              # Training data set\n                 val_data = None,                # Validation (or test) data set\n                 initialize_from_ckp = None,     # If you want to continue training from a specific checkpoint\n                 opti_name = None,\n                 scheduler_name = None,\n                 input_img_size = None,\n                 batch_size = None,\n                 out_classes = None,\n                 use_cuda = False,\n                 max_epochs = None,\n                 learning_rate = None,\n                 thresh_acc = None,\n                 experiment_name = None,\n                 step_size = None\n                 ):\n        self.model = model\n        self.crit = crit\n        self.train_data = train_data\n        self.val_data = val_data\n        self.opti_name = opti_name\n        self.scheduler_name = scheduler_name\n        self.input_img_size = input_img_size\n        self.batch_size = batch_size\n        self.out_classes = out_classes\n        self.cuda = use_cuda\n        self.max_epochs = max_epochs\n        self.learning_rate = learning_rate\n        self.thresh_acc_ckp = thresh_acc\n        self.step_size = step_size\n        \n        if use_cuda:\n            self.model = model.cuda()\n            self.crit = crit.cuda()\n            self.model = DataParallel(model)\n            \n        self.optim = self.select_optimizer()\n        self.scheduler = self.select_scheduler()\n        self.train_dl = self.data_loader(self.train_data)\n        self.val_test_dl = self.data_loader(self.val_data)\n        self.nowtime = datetime.datetime.now().strftime('%d-%m-%Y [%H.%M.%S]')\n        \n        if initialize_from_ckp is not None:\n            self.restore_checkpoint(initialize_from_ckp)\n        \n    def data_loader(self, data_object):\n        return DataLoader(data_object, batch_size = self.batch_size, shuffle=True)\n    \n    def select_optimizer(self):\n        if self.opti_name == 'Adam':\n            return Adam(self.model.parameters(), lr=self.learning_rate)\n        if self.opti_name == 'SGD':\n            return SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)\n        \n    def select_scheduler(self):\n        if self.scheduler_name == 'stepLR':\n            return lr_scheduler.StepLR(optimizer=self.optim, step_size = self.step_size, gamma=0.1)\n        if self.scheduler_name == 'ReduceLROnPlateau':\n            return lr_scheduler.ReduceLROnPlateau(optimizer = self.optim, mode='max', factor=0.2)\n            \n    def restore_checkpoint(self, checkpoint):\n        cuda_device = torch.device('cuda' if self.cuda else 'cpu')\n        ckp = torch.load(checkpoint, map_location = cuda_device)\n        self.model.load_state_dict(ckp)\n        for param in self.model.features.parameters():\n            param.requires_grad = False\n        # self._optim.load_state_dict(ckp)\n    \n    def save_checkpoint(self, folder_name):\n        torch.save({'state_dict': self.model.state_dict(),\n                'optimizer_state_dict': self.optim.state_dict(),\n                }, folder_name + '\/checkpoint.ckp')\n    \n    def IoU(self, label, pred, num_classes=12):\n        label = label.detach().cpu()\n        pred = pred.detach().cpu()\n        \n        pred = F.softmax(pred, dim=1)              \n        pred = torch.argmax(pred, dim=1).squeeze(1)\n        iou_list = list()\n        present_iou_list = list()\n        \n        pred = pred.view(-1)\n        label = label.view(-1)\n        # Note: Following for loop goes from 0 to (num_classes-1)\n        # and ignore_index is num_classes, thus ignore_index is\n        # not considered in computation of IoU.\n        for sem_class in range(num_classes):\n            pred_inds = (pred == sem_class)\n            target_inds = (label == sem_class)\n            if target_inds.long().sum().item() == 0:\n                iou_now = float('nan')\n            else: \n                intersection_now = (pred_inds[target_inds]).long().sum().item()\n                union_now = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection_now\n                iou_now = float(intersection_now) \/ float(union_now)\n                present_iou_list.append(iou_now)\n            iou_list.append(iou_now)\n        return np.mean(present_iou_list)\n        \n    def train_epoch(self):\n        device = torch.device('cuda' if self.cuda else 'cpu')\n        self.model.train()\n        loss_list = []\n        acc_list = []\n        for i, (images, labels) in enumerate(self.train_dl):\n            batchsize = images.shape[0]\n            images = images.to(device, dtype=torch.float32)\n            labels = labels.to(device, dtype=torch.float32)\n            self.optim.zero_grad()\n            preds = self.model(images)\n            loss = self.crit(preds, labels.long())\n            accuracy = self.IoU(labels, preds) \n            #print(accuracy)\n            loss.backward()\n            self.optim.step()\n            lossitem = loss.item() \n            del loss  # this may be the fix for my OOM error\n            loss_list.append(lossitem)\n            acc_list.append(accuracy)\n        \n        loss_avg = np.mean(loss_list) \n        acc_avg = np.mean(acc_list) \n        return loss_avg, acc_avg\n    \n    def val_epoch(self, current_epoch):\n        device = torch.device('cuda' if self.cuda else 'cpu')\n        self.model.eval()\n        loss_list = []\n        acc_list = []\n        with torch.no_grad():\n            for i, (images, labels) in enumerate(self.val_test_dl):\n                im, lb = images, labels\n                batchsize = images.shape[0]\n                images = images.to(device, dtype=torch.float32)\n                labels = labels.to(device, dtype=torch.float32)\n                preds = self.model(images)\n                loss = self.crit(preds, labels.long()) \n                lossitem = loss.item() \n                del loss\n                \n                accuracy = self.IoU(labels, preds) \n                loss_list.append(lossitem)\n                acc_list.append(accuracy)\n                if i == 2:\n                    print('-'*20)\n                    print('Validation results after {} epochs:'.format(current_epoch))\n                    plt.figure(figsize=(20,15))\n                    plt.subplot(1,3,1)\n                    plt.imshow(im[0].permute(1,2,0))\n                    plt.title('image')\n                    plt.subplot(1,3,2)\n                    plt.imshow(lb[0])\n                    plt.title('mask')\n                    plt.subplot(1,3,3)\n                    p = preds[0].permute(1,2,0)\n                    p = torch.argmax(p, dim=2)\n                    plt.imshow(p.cpu())\n                    plt.title('pediction')\n                    plt.show()\n            loss_avg = np.mean(loss_list) \n            acc_avg = np.mean(acc_list) \n        return loss_avg, acc_avg\n    \n    def print_train_logs(self, results):\n        epoch_counter, train_loss, train_accuracy, val_loss, val_accuracy = results\n        print('[Epochs-{}\/{}]:'. format(epoch_counter, self.max_epochs))\n        print('[Train_loss:{:0.4f} | Train_acc:{:0.4f} | Val_loss:{:0.4f} | Val_acc:{:0.4f}]'\n                  .format(train_loss, train_accuracy, val_loss, val_accuracy))\n        \n    def plot_logs(self, results):\n        epoch_counter, train_loss, train_accuracy, val_loss, val_accuracy = results\n        plt.figure(figsize=(15,5))\n        plt.subplot(1,2,1)\n        plt.plot(epoch_counter, train_accuracy)\n        plt.plot(epoch_counter, val_accuracy)\n        plt.title('acc')\n        plt.subplot(1,2,2)\n        plt.plot(epoch_counter, train_loss)\n        plt.plot(epoch_counter, val_loss)\n        plt.title('loss')\n        plt.show()\n    \n    def train(self):\n        \n        model_path = 'results\/%s' % (self.nowtime)\n        epoch_counter_list = []\n        train_loss_list = []\n        train_accuracy_list = []\n        val_loss_list = []\n        val_accuracy_list = []\n        \n        for epoch_counter in range(1, self.max_epochs+1):\n            train_loss, train_accuracy = self.train_epoch()\n            val_loss, val_accuracy= self.val_epoch(epoch_counter)\n            epoch_counter_list.append(epoch_counter)\n            train_loss_list.append(train_loss)\n            train_accuracy_list.append(train_accuracy)\n            val_loss_list.append(val_loss)\n            val_accuracy_list.append(val_accuracy)\n            epoch_results = [epoch_counter, train_loss, train_accuracy, val_loss, val_accuracy]\n            self.print_train_logs(epoch_results)\n\n            if val_accuracy > self.thresh_acc_ckp and val_accuracy == max(val_accuracy_list):\n                new_ckp = '\/epoch_{}_val_acc [{:0.3f}]' . format(epoch_counter, val_accuracy)\n                temp_path = model_path + new_ckp\n                os.makedirs(temp_path, exist_ok=True)\n                if os.path.exists(temp_path): \n                    self.save_checkpoint(temp_path)\n\n            if self.scheduler:\n                self.scheduler.step()\n        train_results = [epoch_counter_list, train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list]\n        #self.plot_logs(train_results)","f80371b8":"train_data_obj = Dataset(train_files, INPUT_IMG_SIZE, AUGMENTATION, mean=MEAN, std=STD)\nval_data_obj = Dataset(test_files, INPUT_IMG_SIZE, AUGMENTATION, mean=MEAN, std=STD)","ef78e44c":"print(train_data_obj.__len__())\nprint(val_data_obj.__len__())","1df56df4":"for i in range(5):\n    img,  msk = train_data_obj.__getitem__(i)\n    print(img.shape, msk.shape)\n    print(img.max(), img.min())\n    print(msk.max(), msk.min())\n    plt.figure(figsize=(15,10))\n    plt.subplot(1,2,1)\n    plt.imshow(img.permute(1,2,0))\n    plt.subplot(1,2,2)\n    plt.imshow(msk)\n    plt.show()","d0c5fdd4":"unet = UNet(num_classes=OUTPUT_CLASSES)","77d37e91":"unet_tune = UNetTunable(in_channels=3,\n                        n_classes=OUTPUT_CLASSES,\n                        depth=5,\n                        wf=6,\n                        padding=True,\n                        batch_norm=True,\n                        up_mode='upconv',\n                        conv_mode='dilated'\n                       )","1145eea1":"print(sum(p.numel() for p in unet.parameters()))\nprint(sum(p.numel() for p in unet_tune.parameters()))","75e3f1b5":"model = unet\nloss_function = nn.CrossEntropyLoss()","7621e29e":"trainer =  Trainer(model = model, \n                  crit = loss_function, \n                  train_data = train_data_obj, \n                  val_data = val_data_obj,\n                  initialize_from_ckp = None,\n                  opti_name = OPIMIZER_NAME,\n                  scheduler_name = SCHEDULER_NAME,\n                  input_img_size = INPUT_IMG_SIZE,\n                  batch_size = BATCH_SIZE,\n                  out_classes = OUTPUT_CLASSES,\n                  use_cuda = USE_CUDA,\n                  max_epochs = MAX_EPOCHS,\n                  learning_rate = LEARINING_RATE,\n                  thresh_acc = THRESH_ACC_CKP,\n                  step_size = SCHEDULER_STEP_SIZE \n                 )","d29e71d9":"trainer.train()","e5e8805b":"## Define trainer object","fd726424":"### As you can observe: \n\n*  Masks are in the RGB format \n*  The Number of segmented classes is changing in every mask (non uniform class number in the label data).\n*  Masks are not encoded\n\n### Preparation:\n\n*  Using Kmeans algorithm --> clustering the unique colors present in a mask\n![image.png](attachment:7227db45-a365-4e36-add7-86951e5ea8a3.png)\n\n*  Use trained Kmeans to encode RGB mask as color encoded mask\n\n![segmentation_example.png](attachment:c212f368-8acf-406a-a565-501b3921f32f.png)\n\n*  As illustated in the above example, every object in that image has been encoded with a specific number\n*  I am also doing the same thing by using kmeans","01a74a3c":"## Define models","dd63ce4e":"## Trainer","ca6f2183":"## Start training","9e020795":"## Required libraries","c29f4601":"## Define loss function","bd7da9c5":"* I am going to use 50 masks as training data for clustering\n* (50, 256, 256, 3) --> (50 x 256 x 256, 3) --> (3276800, 3) \n* I have 3276800 data points (3 --> RGB values) for clustering\n* Using 12 kmeans clutseres","8e21fbd4":"## Mask encoding","97ea8887":"## Create dataset objects","99035b1a":"## Data visualization","b344a8b7":"* #### As previously stated, the original image and its corresponding mask image are combined into a single image.\n* #### We can cut these images into two halves (original image and mask)","bc848ec6":"## Load dataset","f7485278":"## Model-2: Tunable Unet","7b965691":"## No of trainable parameters","f0d6f8f5":"## Model-1: Unet","1d2c1ef3":"## Data preprocessing","50c4b53f":"## Parameters"}}