{"cell_type":{"0b5d1910":"code","42808054":"code","b916f8a7":"code","90620bb7":"code","8d725180":"code","f5636771":"code","ef0d09d0":"code","20bfb3c9":"code","180aae14":"code","8b6672c9":"code","49ab8f9c":"code","3650ae65":"code","ef83f4a2":"code","a782c4be":"code","4b34f198":"code","b0c1a51c":"code","e358b185":"code","68675474":"code","3aa79ebb":"code","f0475e63":"code","9e8f936d":"code","189e6304":"code","c74682ae":"code","cdd69a9c":"code","26294dca":"code","5863b826":"code","b3636607":"code","aa40d3ee":"code","765e96b5":"code","5fcb68d0":"code","1d2bb51e":"code","0b26184b":"code","34b712ba":"code","a0a95e08":"code","56e32990":"code","deec71b7":"code","aa08c4b2":"code","29efd09f":"code","daadc879":"code","4b724724":"code","0afdb5bf":"code","4181260d":"code","cdabe3b7":"code","814ef2a4":"code","f145ecad":"code","809511ed":"code","325a926e":"code","289f25c8":"code","c9e99dae":"markdown","1c146939":"markdown","96014bc2":"markdown","ba5b26c3":"markdown","7bfa15c2":"markdown","4a3ec19c":"markdown","9e61b316":"markdown","9614d0a0":"markdown","e21f29e5":"markdown","b8df91ad":"markdown","363dc773":"markdown","40e550ce":"markdown","c2e8094f":"markdown","be60ad96":"markdown","1538e3a7":"markdown","18b27fbc":"markdown","80f70b1b":"markdown","f323114a":"markdown","ceb3af44":"markdown","de7a7879":"markdown","59c47b6c":"markdown","74704e21":"markdown","8a76cd26":"markdown","8776c9f0":"markdown","f01e6f8b":"markdown","6920bcac":"markdown","1d614479":"markdown","512037b3":"markdown","59fb1063":"markdown","4c3a5a38":"markdown","66033f93":"markdown","f450948b":"markdown","50a4479c":"markdown","57ca8b9b":"markdown","2388409b":"markdown","4a57c49c":"markdown","3999f363":"markdown","f442b22f":"markdown","fbf7d958":"markdown","870f8a58":"markdown","249a19b7":"markdown","4a05df16":"markdown","130f15c0":"markdown","af5c4e1d":"markdown"},"source":{"0b5d1910":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow.compat.v2.feature_column as fc\nimport statsmodels.api as sm\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind\nfrom sklearn import linear_model\nfrom IPython.display import clear_output\nfrom six.moves import urllib\n\n# to import from Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","42808054":"# load training and testing data \ndf_train = pd.read_csv(\"\/kaggle\/input\/league-of-legends-challenger-ranked-games2020\/Challenger_Ranked_Games.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/league-of-legends-challenger-ranked-games2020\/Master_Ranked_Games.csv\")\ndf_train.head()","b916f8a7":"# clean training data\ndf_train_clean = df_train.filter(like=\"blue\")\ngameDuration = df_train.pop(\"gameDuraton\")\ndf_train_clean.insert(0, \"gameDuration\", gameDuration, True)\ny_train = df_train_clean.pop(\"blueWins\")\ndf_train_clean.head()","90620bb7":"# clean testing data\ndf_test_clean = df_train.filter(like=\"blue\")\ndf_test_clean.drop([\"blueFirstBlood\", \"blueTotalHeal\"], axis = 1)\ny_test = df_test_clean.pop(\"blueWins\")\ndf_test_clean.head()","8d725180":"# Separate winning and losing stats\nwin_stats = df_train_clean.loc[df_train[\"blueWins\"] == 1]\nloss_stats = df_train_clean.loc[df_train[\"blueWins\"] == 0]","f5636771":"# summary stats of team that won\nwin_stats.describe()","ef0d09d0":"# summary stats of team that lost\nloss_stats.describe()","20bfb3c9":"def draw_histograms(df, variables, n_rows, n_cols):\n    fig=plt.figure(figsize=(15,15))\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=20,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n        plt.tight_layout()\n    \n    plt.show()","180aae14":"numerical_col_names = [c for c in df_test_clean.columns if c.lower()[4:9] != \"first\"]\n# numerical_col_names = [c for c in numerical_col_names if c not in [\"blueDragonKills\", \"blueBaronKills\", \"blueTowerKills\", \"blueInhibitorKills\"]]\nnumerical_stats = df_test_clean[numerical_col_names]\nhistogram_all = df_test_clean[numerical_col_names]\nhistogram_data_win = win_stats[numerical_col_names]\nhistogram_data_lose = loss_stats[numerical_col_names]\nhistogram_all.head()","8b6672c9":"from matplotlib import pyplot\ndef compare_histograms(df1, df2, variables, n_rows, n_cols):\n        fig=plt.figure(figsize=(15,15))\n        for i, var_name in enumerate(variables):\n            ax=fig.add_subplot(n_rows,n_cols,i+1)\n            df1[var_name].hist(bins=20, ax=ax, label=\"Won\") # for histogram\n            df2[var_name].hist(bins=20, ax=ax, label=\"Lost\") # for histogram\n            ax.set_title(var_name+\" Distribution\")\n            pyplot.legend(loc=\"upper right\")\n            plt.tight_layout()\n        \n        plt.show()","49ab8f9c":"compare_histograms(histogram_data_win, histogram_data_lose, histogram_data_win, 9, 2)","3650ae65":"draw_histograms(histogram_all, histogram_all.columns, 9, 2)","ef83f4a2":"import pylab \ndef draw_qqplots(df, variables, n_rows, n_cols):\n    fig=plt.figure(figsize=(15,15))\n    for i, var_name in enumerate(variables):\n        \n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        stats.probplot(df[var_name], dist=\"norm\", plot=pylab)\n        # df[var_name].hist(bins=20,ax=ax)\n        ax.set_title(var_name+\" qq plot\")\n        plt.tight_layout()\n    \n    plt.show()","a782c4be":"draw_qqplots(histogram_all, histogram_all.columns, 9, 2)","4b34f198":"piechart_col_names = [c for c in df_test_clean.columns if c.lower()[4:9] == \"first\"]\n# piechart_col_names = piechart_col_names + [\"blueDragonKills\", \"blueBaronKills\", \"blueTowerKills\", \"blueInhibitorKills\"]\npiechart_all = df_train_clean[piechart_col_names]\npiechart_all.head()","b0c1a51c":"piechart_win = win_stats[piechart_col_names]\npiechart_loss = loss_stats[piechart_col_names]","e358b185":"def draw_piecharts(df, variables, n_rows, n_cols):\n    fig=plt.figure(figsize=(15,15))\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        percentages = list(df[var_name].value_counts(normalize=True, sort=False) * 100)\n        labels = sorted(df[var_name].unique())\n        explode = ([0.02] * len(percentages))\n        ax.pie(percentages,shadow=True, explode=explode, labels=labels, autopct='%1.1f%%')\n        ax.set_title(var_name)\n\n        ax.legend(\n          title=\"# of occurances\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1),\n          labels=['%s, %1.1f %%' % (l, s) for l, s in zip(labels, percentages)]\n          )\n        plt.tight_layout()\n    \n    plt.show()","68675474":"draw_piecharts(piechart_all, piechart_all.columns, 3, 3)","3aa79ebb":"draw_piecharts(piechart_win, piechart_win.columns, 3 ,3)","f0475e63":"draw_piecharts(piechart_loss, piechart_loss.columns, 3 ,3)","9e8f936d":"histogram_all.skew() # from scipy.stats library","189e6304":"from sklearn import preprocessing\ndef normalize_data(df):\n  df = df+1\n  scaler = preprocessing.PowerTransformer(method=\"box-cox\", standardize=True).fit(df)\n  norm_df = pd.DataFrame(scaler.transform(df),columns=df.columns)\n  return norm_df","c74682ae":"norm = normalize_data(histogram_all)\nnorm.head()","cdd69a9c":"norm.skew()","26294dca":"draw_histograms(norm, norm.columns, 6, 3)","5863b826":"draw_qqplots(norm, norm.columns, 6, 3)","b3636607":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","aa40d3ee":"vif_init = calc_vif(norm)\nvif_init","765e96b5":"norm2 = norm.drop([\"blueTotalLevel\", \"blueAvgLevel\", \"blueTotalGold\", \"blueKillingSpree\", \"blueAssist\"], axis=1)\nvif_norm2 = calc_vif(norm2)\nvif_norm2","5fcb68d0":"import seaborn as sn\ncorr_matrix_norm = norm2.corr()\nmask = np.triu(corr_matrix_norm)\nf, ax = plt.subplots(figsize=(15,15))\nax = sn.heatmap(corr_matrix_norm, annot=True, mask=mask, cmap=\"coolwarm\", square=True)\nplt.show()","1d2bb51e":"norm_X_train = piechart_all.join(norm2) # join categorical with numerical variables\nX = norm_X_train.to_numpy()\nY = y_train\nmodel = sm.Logit(Y,X).fit()\npredictions = model.predict(X)\nprint_model = model.summary(xname=list(norm_X_train.columns))\nprint(print_model)","0b26184b":"logit_insig = [\"blueFirstTower\", \"blueFirstBaron\", \"blueWardkills\", \"blueChampionDamageDealt\", \"blueTotalMinionKills\", \"blueTotalHeal\", \"blueObjectDamageDealt\"]\nX = norm_X_train.drop(list(logit_insig), axis = 1)\nY = y_train\nmodel = sm.Logit(Y,X).fit()\npredictions = model.predict(X)\nprint_model = model.summary()\nprint(print_model)","34b712ba":"# odd ratios of each predictor variable, with raw data\n\n# getting the names of significant variables\nsig_vars = []\nfor key, val in model.params.iteritems():\n    sig_vars.append(key)\n\nraw_num_model_df = piechart_all.join(histogram_all) # join categorical with numerical variables\nX_raw = raw_num_model_df[sig_vars]\nY = y_train\nraw_num_model = sm.Logit(Y,X_raw).fit()\nnp.exp(raw_num_model.params)","a0a95e08":"blue_sig_stats = norm_X_train\n\nblue_sig_stats[\"blueWins\"] = y_train\nblue_sig_stats.head()","56e32990":"blue_wins = blue_sig_stats.where(df_train[\"blueWins\"] == 1)\nblue_wins = blue_wins.dropna()\nblue_wins.head()","deec71b7":"blue_loses = blue_sig_stats.where(df_train[\"blueWins\"] == 0)\nblue_loses = blue_loses.dropna()\nblue_loses.head()","aa08c4b2":"SIG_VARS = sig_vars\nSIG_BIN_CAT_VARS = [v for v in list(piechart_all.columns) if v in SIG_VARS]\nSIG_NUM_VARS = [v for v in list(histogram_all.columns) if v in SIG_VARS]\nALL_SIG_VARS = SIG_BIN_CAT_VARS + SIG_NUM_VARS\nprint(SIG_BIN_CAT_VARS)\nprint(SIG_NUM_VARS)","29efd09f":"# INSIG_VARS = logit_insig\n# SIG_BIN_CAT_VARS = [v for v in list(piechart_all.columns) if v not in INSIG_VARS]\n# SIG_NUM_VARS = [v for v in list(histogram_all.columns) if v not in INSIG_VARS]\n# ALL_SIG_VARS = SIG_BIN_CAT_VARS + SIG_NUM_VARS","daadc879":"def format_test_data(df):\n    df = df.head(26904)\n    bin_data = df[SIG_BIN_CAT_VARS]\n    # normalize numerical variables\n    num_data = df[SIG_NUM_VARS]\n    norm_num_data = normalize_data(num_data)\n    win_loss_col = df[\"blueWins\"]\n    test_data = pd.concat([bin_data, norm_num_data, win_loss_col], axis=1)\n    return test_data","4b724724":"def format_train_data(df):\n    df = df.head(26904)\n    bin_data = df[SIG_BIN_CAT_VARS]\n    # normalize numerical variables\n    num_data = df[SIG_NUM_VARS]\n    norm_num_data = normalize_data(num_data)\n    test_data = pd.concat([bin_data, norm_num_data], axis=1)\n    return test_data","0afdb5bf":"clean_train_data = format_train_data(df_train)\nclean_test_data = format_test_data(df_test)\nclean_test_data.head()","4181260d":"feature_columns = []\ny_test_clean = clean_test_data.pop(\"blueWins\")\n\nfor feature_name in SIG_BIN_CAT_VARS:\n  vocabulary = clean_train_data[feature_name].unique()\n  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\nfor feature_name in SIG_NUM_VARS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))","cdabe3b7":"def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=100):\n  def input_function():  # inner function, this will be returned\n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n    if shuffle:\n      ds = ds.shuffle(1000)  # randomize order of data\n    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n    return (ds)  # return a batch of the dataset\n  return input_function  # return a function object for use\n\ntrain_input_fn = make_input_fn(clean_train_data, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\neval_input_fn = make_input_fn(clean_test_data, y_test_clean, num_epochs=1, shuffle=False)\n","814ef2a4":"ds = make_input_fn(clean_train_data, y_train, batch_size=10)()\nfor feature_batch, label_batch in ds.take(1):\n  print('Some feature keys:', list(feature_batch.keys()))\n  print()\n  print('A batch of blueDragonKills:', feature_batch['blueDragonKills'].numpy())\n  print()\n  print('A batch of Labels:', label_batch.numpy())","f145ecad":"linear_est = tf.estimator.LinearClassifier(feature_columns= feature_columns)\n# We create a linear estimator by passing the feature columns we created earlier","809511ed":"linear_est.train(train_input_fn)  # train\nresult = linear_est.evaluate(eval_input_fn)  # get model metrics\/stats by testing on testing data\n\nclear_output()  # clears console output\nprint(result)  # the result variable is simply a dict of stats about our model","325a926e":"pred_dicts = list(linear_est.predict(eval_input_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n\nprobs.plot(kind='hist', bins=20, title='predicted probabilities')","289f25c8":"from sklearn.metrics import roc_curve\nfrom matplotlib import pyplot as plt\n\nfpr, tpr, _ = roc_curve(y_test_clean, probs)\nplt.plot(fpr, tpr)\nplt.title('ROC curve')\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.xlim(0,)\nplt.ylim(0,)","c9e99dae":"#### Displaying Significant Stats \n\n  * Note: feel free to look into the significant stats in won and lost games\n    * there were NaN values because the win and lost values in \"blueWins\" was joined with the win and lost groups.\n    * those said NaN rows were removed for a cleaner view","1c146939":"## Formatting Data for Tensorflow","96014bc2":"### Pie Charts\n\n---\n  * Note: Pie charts were chosen to better visualize variables that were either binary, or had a narrow range of values\n---\n\n\n  * Including both win and lost games (i.e. all games):\n    * Equal chance for both teams to get first blood\n    * First dragon is prioritized greater than first baron\n    * It is rare to observe 4+ dragons slain\n    * There is an uniform distribution of tower kills\n\n\n  * Notable Comparisons:\n    * For binary variables, won games secure the objective more often than lost games\n    * It is more likely to capture a total of 1 dragon in lost games than won games\n    * The frequency of capturing 2+ dragons differs greatly between won and lost games\n    * Tower kills in won games have a uniform distribution, whereas lost games have majority of its data split between: 0, 2, and 6\n    * In both won and lost games, 0 barons captured most frequent\n","ba5b26c3":"# Modelling the Data\n\n### Key points of Modelling the Data: \n  * **logistic modelling** is best to represent the data\n  * the following variables **are statistically significant**, and are included in modelling the data:\n    * Numerical Variables: \n      * blueDragonKills         \n      * blueTowerKills          \n      * blueWardPlaced           \n      * blueWardkills            \n      * blueKills                \n      * blueDeath                \n      * blueTotalMinionKills     \n      * blueJungleMinionKills   \n      * blueObjectDamageDealt\n\n    * Binary Varaibles: \n      * blueFirstBlood         \n      * blueFirstTower          \n      * blueFirstBaron           \n      * blueFirstInhibitor\n    \n\n  * these variables will be observed when predicting the win probability of a game","7bfa15c2":"# Visualizing\/Describing the Data\n\n## Key Characteristics of the Dataset\n\n\n  * Both won and lost games are macro focused - both seek to secure objectives\n  * Overall, the differences in macro management is greater than the differences in micro management\n    * i.e) differences in dragon,baron,tower, and inhibitors are greater than the differences in gold\/creep score (CS)\/wards placed\/wards destroyed\n  * Majority of the measured variables are moderately\/heavily right skewed\n    * Dataset will benefit from a transformation to move closer to a normal distribution\n","4a3ec19c":"The linear model is created using the feature columns specified previously","9e61b316":"### QQ Plots\n\n\n  * Right skewed variables, shown to have light left tails, and heavy right tails\n\n  \n  * Notable Variables:\n    * Total minion kills\n      * looks to be the most normally distributed variable out of all\n    * Champion damage dealt\/assists\/ total heal \n      * will need\/benefit most with a transformation to a normal distribution\n    * Total level\n      * Both left and right tails are heavy\n      * Majority of variables distributed near median\n","9614d0a0":"Here is the pipeline that supplies the TensorFlow model with data. It includes:\n\n  * values of the features\n  * corresponding labels\n  * feeds 100 points at a time","e21f29e5":"The odds ratio compares the odds of winning to the odds of losing for an increase of 1 unit.\n","b8df91ad":"# Load and Clean Datasets\n\n\nThe datasets are from games played in the highest level of competitive play. More specifically, Master, GrandMaster, and Challenger. \n\nIn perspective, these players make up **~0.083% of a player base of 115+ million**.\n\n* Dataset Source: https:\/\/www.kaggle.com\/gyejr95\/league-of-legends-challenger-ranked-games2020\/data#","363dc773":"## Further Skew Calculation\n\n\n  * Skew calculation found here: **https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.skew.html**\n\n\n  * **Majority of the variables have moderate skewness** based on calculation\n\n  \n  * Notable Variables:\n    * Assist has most of its weight in the left tail\n      * Also true for total heal\n    * Total level has\tmost of its weight in the right tail","40e550ce":"### Variance Inflation Factor\n  \n  >  ","c2e8094f":"### Refining the First Model\n  \n  * for the second model, all the variables are statistically significant ((i.e.) p-values < 0.05). \n  #### R<sup>2<\/sup> Value: \n    * R<sup>2<\/sup> value did not decrease drastically, supporting the notion that removing insignificant variables had no effect on the models fit of the data.","be60ad96":"# Initial Setup of Environment\n  * importing required packages\/libraries for:\n      * graphical models\n      * perform statistical tests","1538e3a7":"### Using a Linear Classification Model to Predict Win Probability\n  > The probability of winning can take on values from 0 to 1 and NOT of values of 0 and 1. In other words, the probability is a continuous outcome, and not a binary outcome.\n\t\t\n  > In the previous section, a logistic model was chosen to model the data because the outcome was binary and more importantly, it was given i.e.) win or loss. Therefore the p-values that identify the significant variables, or in other words the variables that influence winning, are credible. \n  \n  > These significant variables will be used to create a linear classification model to predict the win probability of a game.","18b27fbc":"This is an example of what a single input from a batch looks like","80f70b1b":"## Testing the Model and Results","f323114a":" Instead of hard coding the significant variables, I looped over the groups of variables and only included the ones that are significant","ceb3af44":"* Training data will be based on Challenger games - the highest level of competitve play\n\n* Testing data will be contain Master games - the second highest level of competitive play","de7a7879":"# Preparing the Data for Analysis\n\n## Key Points of Data Preparation: \n\n\n  * By normalizing the data via Box-Cox transformation, severely skewed variables appear closer to a normal distribution\n   * Confirmed by skewness calculation, histograms, and QQ plots\n\n\n  * Total level, average level, and total gold had enormous interaction with other variables and have been removed\n\n\n  * Killing spree and assists are indirect measures of total kills and were removed due to redundancy \n\n\n  * Variables for further analysis: \n    * Numerical Varaibles: \n        * blueWardPlaced\n        * blueWardkills\n        * blueKills\n        * blueDeath\n        * blueChampionDamageDealt\n        * blueTotalMinionKills\n        * blueJungleMinionKills\n        * blueTotalHeal\n        * blueObjectDamageDealt\n    \n    * Binary Variables:\n        * blueFirstBlood\n        * blueFirstTower\n        * blueFirstBaron\n        * blueFirstDragon\n        * blueFirstInhibitor\n","59c47b6c":"## Normalizing and Standardizing Data\n\nNormalizing and standardizing data is couple of the methods in preprocessing data.\n\n> Standardizing solves the differences of scales between variables. This is a problem because **larger scales will have a larger influence** on for example, a multivariate analysis, when that factor may not be an important predictor. When all variables are measured on the same scale, each variable is equally important when under analysis. \n\n> Normalzing solves skewed data\/heteroskedasticity. With skewed data, there is **larger variability in the skewed region**. This is a problem in linear regression because for the **effect of the variable on the result will be enhanced** for values in the skewed region\n\n> By normalizing, the variability becomes constant and the effect of the variable is true for all values.\n\n***\n  > More information about:\n\n* [Benfits of normalizing data](https:\/\/towardsai.net\/p\/data-science\/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff)\n\n\n* [Benefits of normalizing data 2](https:\/\/blog.minitab.com\/blog\/applying-statistics-in-quality-projects\/how-could-you-benefit-from-a-box-cox-transformation)\n\n\n* [Skewness](https:\/\/help.gooddata.com\/doc\/en\/reporting-and-dashboards\/maql-analytical-query-language\/maql-expression-reference\/aggregation-functions\/statistical-functions\/predictive-statistical-use-cases\/normality-testing-skewness-and-kurtosis#:~:text=As%20a%20general%20rule%20of,the%20distribution%20is%20approximately%20symmetric.)\n\n  \n\n\n","74704e21":"## Feature Engineering for TensorFlow Model","8a76cd26":"Surprisingly, the model has an **accuracy of ~96%**","8776c9f0":"### Important Terminology: \n#### P-Values:\n  > P-values indicate if the variable has an effect on the outcome variable. In other words, we set that variable to a value of 0 - if it makes a difference in the outcome, then it has an effect on the model.\n\t\n  > When the variables P-value > 0.05, it is removed because it does not make a difference in predicting the outcome. Variables with a P-value < 0.05, they are retained in the model because removing it changes the predicted outcome. \n\n#### Maximum Likelihood Estimation (MLE):\n  > MLE determines the parameters for each variable in the model, where it maximizes the likelihood of fitting predicted value to the observed value. These values can be determined by iterating through values and picking the model that has the least amount of error.\n\t\n  > This is similar to fitting data using least squares in linear regression, except the data is projecting on a log(odds) line to find the least squares, then it is translated back into probability values and log(likelihood). \n\n#### Pseudo R<sup>2<\/sup> Value:\n  > R<sup>2<\/sup> measures how well the model fits with the current data. It ranges from 0 to 1, the closer it is to 1, the better the model represents the data. \n---\nMore information about:\n  * [P-values](https:\/\/blog.minitab.com\/blog\/adventures-in-statistics-2\/how-to-interpret-regression-analysis-results-p-values-and-coefficients#:~:text=The%20p%2Dvalue%20for%20each,can%20reject%20the%20null%20hypothesis.&text=Typically%2C%20you%20use%20the%20coefficient,keep%20in%20the%20regression%20model.)\n  * [Maximum Likelihood Estimation (MLE)](https:\/\/towardsdatascience.com\/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)\n  * [Pseudo R<sup>2<\/sup> values](https:\/\/thestatsgeek.com\/2014\/02\/08\/r-squared-in-logistic-regression\/)\n  * [Video explaination of P-values and R<sup>2<\/sup> values](https:\/\/www.youtube.com\/watch?v=xxFYro8QuXA)\n  * [Video explaination of MLE](https:\/\/www.youtube.com\/watch?v=BfKanl1aSG0)","f01e6f8b":"# Predicting Win Probability\n\n### Key points of Predicting Win Probability: \n  * after testing the model with Master tier games, the model achieved an accuracy of 96%!\n  * columns have been labelled as numerical or categorical for the tensorflow model\n","6920bcac":"## Creating A Logistic Model","1d614479":"  ### First Model\n  \n  * for the first model, all the variables from the clean dataset were included\n  #### P-Values:\n    * the following had p-values > 0.05 and will be removed: \n        * blueFirstTower\n        * blueFirstBaron\n        * blueWardkills\n        * blueTotalHeal\n        * blueFirstDragon\n        * blueBaronKills\n        * blueChampionDamageDealt\n        * blueTotalMinionKills\n        * blueTotalHeal\n        * blueObjectDamageDealt\n    * all other variables are significant to the model\n\n  ### R<sup>2<\/sup> Value:\n    * surprisingly, the first model has a high R<sup>2<\/sup> value of 0.8462\n\n","512037b3":"  > Total level, average level, and total gold have an enormous VIF, this means these variables are highly associated with other variables.\n\n\n  > These will be removed to reduce the VIF of other variables and we can get closer to the true VIF scores.\n\n  > Additionally, killing spree and assist are **indirect measures of total kills and is redundent**. Therefore, **killing spree and assists were removed** from further analysis.","59fb1063":"  > Here the data was normalzied with a Box-Cox transformation - a power transformation. \n\n  > As a result, the both problems were resovled:\n  1. all variables fit within a similar scale\n   * larger scaled variables will not longer have a larger influence in a model\n\n  2. variables are closer to a normal distribution\n  \n  \n   More information about:\n  * [Box-Cox transformations](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html)\n\n  * [Box-Cox transforamtions using sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.power_transform.html)","4c3a5a38":"### Creating the TensorFlow Pipeline\n  > A pipeline feeds data to the model in parts called \u2018batches\u2019. Everytime a batch is fed into the model, the parameters update to better fit the data. In other words, the more data exposed to the model, the more optimization made to the model. \n","66033f93":"## Key Observations in Graphs\n\n  * Won games have prioritized objectives by at least 3.0x more than lost games\n  * Majority of the variables are moderately skewed to the right\n    * Calculated skews are within a range of (-0.1 - 1.5)\n  * First dragon is prioritized more than first baron \n  * The percentage of securing objectives is greater in won games than in lost games\n\n\n\n","f450948b":"## Cleaning the Data\nThe initial dataset contains data for both teams (i.e. Blue and Red teams). I am only interested in the stats of one team - in this case, I chosen the blue team.\n\n* Notes: \n    * It does not matter which team is analyzed because the same stats are recoreded both teams\n    * Both teams have winning and losing games recorded","50a4479c":"## Testing for Multicollinearity\n\n### What is Multicollinearity?\n\n  > Multicollinearity is the interaction between independent variables. It can be **measured by calculating the variance inflation factor (VIF)** and **visualized with a correlation matrix**. \n  \n  > Additionally, VIF scores are only applicable to **ordinary least square** models. Whereas, correlation matrices are applicable to a wider range of models.\n\n\n  > The greater a VIF is, the greater correlation it has with other variables. As a guideline, VIF >= 10 is most problematic because the less reliable that variable is in predicting an outcome. \n  * More info about: \n    * [Variance inflation factors (VIFs)](https:\/\/www.displayr.com\/variance-inflation-factors-vifs\/#:~:text=The%20higher%20the%20value%2C%20the,being%20regarded%20as%20very%20high.&text=If%20one%20variable%20has%20a,must%20also%20have%20high%20VIFs.)\n    * [VIF Wiki](https:\/\/en.wikipedia.org\/wiki\/Variance_inflation_factor)\n    * [Corrrelation Wiki](https:\/\/en.wikipedia.org\/wiki\/Correlation_and_dependence#Correlation_matrices)\n\n","57ca8b9b":"## Background Info About TensorFlow Modelling\n","2388409b":"### How the data needs to be formatted for TensorFlow\n  > Depending on how the variable is measured and encoded, the numbers can have different meanings. For example a categorical variable such as color, can encode 1 for red, 2 for blue, etc., but 1 < 2 does not make sense. In comparison to a numerical variable such as heart rate, 160BPM > 150BPM does make sense\n\n* Note: BPM = beats per minute \n\t\n\n  > Tensorflow has estimators that use a \u201cfeature column\u201d system.Each variable must be specified as categorical, numerical, etc., such that the model will know how to interpret the data for each feature. \n","4a57c49c":"  > The **lower the number in the correlation matrix, the lower interaction** between the two variables. \n\n\n  > Some notable interactions: \n  * Total minion kills - ward kills\n  * Total minion kills - wards placed\n\n\n  > It is not as clear to why these variables are associated with each other, usually in game, minion kills and ward kills\/placed are seen as independent. It raises the question: **Why do minion kills associate with ward kills\/placed?** \n","3999f363":"## Key Observations in Summary Statistics:\n\n\n---\n  * Note: the following observations were based on the averages\n---\n\n\n  * Towers:\n    * **First tower destroyed** was priortized by a factor of **~2.0x** in won games than lost games\n    * **Total towers destroyed** was priortized by a factor of **~3.3x** in won games than lost games\n\n\n  * Inhibitors:\n    * **First inhibtors destroyed** was priortized **~11.3x** in won games than lost games\n    * **Total inhibtors destroyed** was priortized **~9.3x** in won games than lost games\n\n\n  * Dragons\/Barons:\n    * **First dragon** was priortized by a factor of **~2.0x** in won games than lost games\n    * Won games have slain **~7.2x** more dragons than lost games (**Total dragons slain**)\n    * **First baron** was priortized by a factor of **~4.6x** in won games than lost games\n    * **Total barons slain** was priortized by a factor of **~4.3x** in won games than lost games\n\n  * Kills\/Deaths:\n    * **First blood** was priortized by a factor of **~1.5x** in won games than lost games\n    * In won games, there is **~1.6x** more **team kills** than lost games\n    * In won games, there is **~0.6x** less **team deaths** than lost games\n\n\n  * Micro\/Miscellaneous: \n    * In won games, there were **~1.04x** more **wards placed** than in lost games \n    * In won games, there were **~1.1x** more **wards destroyed** than in lost games\n    * There was a difference of **~20%** in **gold** between won and lost games\n    * There was a difference of **~5.5%** in **creep score (CS)** between won and lost games\n    * There was a difference of **~1.2%** in **total damage** between won and lost games\n    ","f442b22f":"### Histograms:\n\n\n  * Note: the following observations are **true for both won and lost games**\n\n  * **Normal** distributed variables:\n    * Total gold\n    * Total minion kills\n    * Total level\n    * Average level \n\n\n  * **Right skewed** distributed variables:\n    * Ward placed\n    * Ward kills\n    * Kills\n    * Death\n    * Assists\n    * Champion damage dealt\n    * Jungle minion kills\n    * Total level\n    * Killing spree\n    * Total heal\n    * Object damage dealt\n\n\n","fbf7d958":"## Identifying Significant Variables\n","870f8a58":"### Which Model is Best? \n  > Given that the outcome is binary (i.e.) 0 = lose, 1 = win), a **logistic regression model** is best. \n\n### Why not Linear Regression? \n  > Linear regression **assumes that the response variable is continuous**. For example, if we were to graph a curve where total gold is on the X-axis, and win or lose on the Y-axis, there would be a sigmoidal curve, an S-shaped curve. This curve certainly does not show a linear relationship, therefore, linear regression wouldn\u2019t be a good model for the data. \n\n### What is Logistic Regression?\n\n  > Logistic regression models the probability of a true or false outcome, in this case the outcome is win or lose. This model is **best for classification**, and can be given a threshold to differentiate a true from or false prediction. \n  \n  > The model is built similar to a linear regression model, where the predictor variables can be nominal, continuous, and categorical,\n and with a log(odds) transformation, predicts which group the data point belongs to.\n\n\n\n![Logistic Regression Curve](https:\/\/miro.medium.com\/max\/725\/1*QY3CSyA4BzAU6sEPFwp9ZQ.png)\n\nMore information about:\n\n  * [What is Logistic Regression?](https:\/\/www.youtube.com\/watch?v=yIYKR4sgzI8)\n  * [How Logistic Regression is used](https:\/\/www.statisticssolutions.com\/what-is-logistic-regression\/#:~:text=Like%20all%20regression%20analyses%2C%20the,or%20ratio%2Dlevel%20independent%20variables.)","249a19b7":"This is where the significant variables are labelled as categorical, or numerical\n  * more info can be found in the background information in this section","4a05df16":"Here I formatted the datasets by: \n  1. getting the significant variable columns\n  2. normalizing only the numerical variables\n  3. attaching the binary and categorical variables\n    * these were not transformed\n  4. attaching the results\/labels, indicating if blue team won or lost\n    * this was only done for the test data","130f15c0":"# TensorFlow Model Learning Perfomance\n\n### Key points of Predicting Win Probability: \n  * given the accuracy of the model, predicted probabilties have a bimodal distrbution, with peaks at 0 and 1. \n\n  * the model has a low false positive rate, and a high true positive rate\n    * as shown with the logarithmic ROC curve","af5c4e1d":"## Choosing A Model to Represent the Data"}}