{"cell_type":{"16c9fe8a":"code","f11d797f":"code","11fc9ea7":"code","e7bd5da8":"code","fb95b470":"code","30da1883":"code","63f7fe06":"code","00716be4":"code","e8d731f7":"code","47966d1b":"code","8db235a9":"code","bf4cb121":"code","434e3599":"code","882ca30a":"code","00d75e52":"code","57b62f9a":"code","fa10f8fd":"code","f7406b5c":"code","6a501a20":"code","780f300e":"code","6b969dfa":"code","4c20dc91":"code","c6c9b2b7":"markdown","a3e595fa":"markdown","f5b4f64a":"markdown","76732c28":"markdown","970f27b7":"markdown","62626d6d":"markdown","5209e2a8":"markdown","5c3b581e":"markdown","a5477972":"markdown","6d2badf7":"markdown","7d325df6":"markdown","ccd34751":"markdown","fe0b1997":"markdown","495f2bcd":"markdown","e0eb9d0b":"markdown","3dfc28ce":"markdown","d3a5f9ee":"markdown","20f4181c":"markdown","53ba97ab":"markdown","fde76038":"markdown","f88fb42b":"markdown"},"source":{"16c9fe8a":"from keras.preprocessing.image import load_img\nimport matplotlib.pyplot as plt\n#Load an image and determine image shape for analysis.\nIMAGE = load_img(\"\/kaggle\/input\/hummingbirds-at-my-feeders\/video_test\/06090116image.jpg\")\nplt.imshow(IMAGE)\nplt.axis(\"off\")\nplt.show()","f11d797f":"#Load an image.\nIMAGE = load_img(\"\/kaggle\/input\/hummingbirds-at-my-feeders\/video_test\/Broadtailed_male\/010.jpg\")\nplt.imshow(IMAGE)\nplt.axis(\"off\")\nplt.show()","11fc9ea7":"#Load an image.\nIMAGE = load_img(\"\/kaggle\/input\/hummingbirds-at-my-feeders\/video_test\/Broadtailed_male\/012.jpg\")\nplt.imshow(IMAGE)\nplt.axis(\"off\")\nplt.show()","e7bd5da8":"#Load an image.\nIMAGE = load_img(\"\/kaggle\/input\/hummingbirds-at-my-feeders\/video_test\/No_bird\/203.jpg\")\nplt.imshow(IMAGE)\nplt.axis(\"off\")\nplt.show()","fb95b470":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\nimport random\n        \n       \nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:95% !important; }<\/style>\"))\n","30da1883":"#This will setup my directories for all of the data files in the 100-bird-species dataset. \nbasedir = '\/kaggle\/input\/hummingbirds-at-my-feeders\/hummingbirds'\nprint('Base directory contains ', os.listdir(basedir))\ntraindir = os.path.join(basedir, 'train')\nvaliddir = os.path.join(basedir, 'valid')\ntestdir = os.path.join(basedir, 'test')\n","63f7fe06":"# Count images for each species\ndef cntSamples(directory):\n    specs = []\n    for root, dirs, files in os.walk(directory, topdown=True):\n        dirs.sort()\n        for name in dirs:\n            if name not in specs:\n                specs.append(name)\n\n    # file counts for each species\/gender category \n    nums = []\n    for b in specs:\n        path = os.path.join(directory,b)\n        num_files = len(os.listdir(path))\n        nums.append(num_files)\n         \n    # Create Dictionary\n    adict = {specs[i]:nums[i] for i in range(len(specs))}\n    return adict\n\n#Provide an index number for each species for future reference\ndef indexID(Dict):\n    i=0\n    for key, value in Dict.items():\n        Dict[key] = i\n        i+=1\n    return(Dict)\n\n#create seperate labels for images \ndef label_images(DIR, dataset):\n    label = []\n    image = []\n    j=0\n    for i in range (0,30):\n        j = random.randint(0, len(dataset.filenames))\n        label.append(dataset.filenames[j].split('\/')[0])\n        image.append(DIR + '\/' + dataset.filenames[j])\n    return [label,image]\n\n#create seperate labels for images \ndef label_images2(DIR, dataset):\n    label = []\n    image = []\n    qty = len(dataset.filenames)\n    for i in range(0,qty):\n        label.append(dataset.filenames[i].split('\/')[0])\n        image.append(DIR + '\/' + dataset.filenames[i])\n    return [label,image]\n\n\n#Return names for the predicted species listing.\ndef getKeysbyValues(LabelDict, testvaluelist):\n    testLabellist = []\n    listItems = LabelDict.items()\n    for testvalue in testvaluelist:\n        for value in listItems:\n            if value[1] == testvalue:\n            #if np.allclose(value[1],testvalue):\n                testLabellist.append( {value[0]} )     \n    return testLabellist\n\n#Code obtained from: https:\/\/scikit-learn.org\/0.18\/auto_examples\/model_selection\/plot_confusion_matrix.html\n#Don't forget that data generator needs Shuffle=False\nfrom sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm=np.round(cm,2)\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n        ","00716be4":"testDict =  cntSamples(testdir)\ntrainDict = cntSamples(traindir)\nvalidDict = cntSamples(validdir)\nindex = indexID(cntSamples(traindir))\nDict = {'Index':index,'Train Images':trainDict, 'Test Images':testDict, 'Valid Images':validDict}\nspeciescnt = pd.DataFrame.from_dict(Dict)\ndisplay(HTML(speciescnt.to_html()))","e8d731f7":"from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\nimport matplotlib.pyplot as plt","47966d1b":"###################\ngeneral_datagen = ImageDataGenerator(rescale=1.\/255, )\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n        #width_shift_range=0.2, ###added\n        #height_shift_range=0.2, ###added\n        #rotation_range=5, ###added\n        brightness_range = [0.5,1.5],\n        shear_range=0.2,\n        zoom_range=0.2, ###from .2 to .4\n        horizontal_flip=True#,\n        #fill_mode='nearest'\n                                  ) ###added","8db235a9":"batch_size = 32\ntrain_data = train_datagen.flow_from_directory(\n    traindir, classes = index, batch_size=batch_size,\n    target_size=(224,224))\ntest_data = general_datagen.flow_from_directory(\n    testdir, classes = index, batch_size=batch_size,\n    target_size=(224,224), shuffle = False)\nvalid_data = general_datagen.flow_from_directory(\n    validdir, classes = index, batch_size=batch_size,\n    target_size=(224,224), shuffle = False)","bf4cb121":"#plot the random images.\ny,x = label_images(traindir, train_data)\n\nfor i in range(0,6):\n    X = load_img(x[i])\n    plt.subplot(2,3,+1 + i)\n    plt.axis(False)\n    plt.title(y[i], fontsize=8)\n    plt.imshow(X)\nplt.show()","434e3599":"import tensorflow as tf\nfrom tensorflow.keras import backend, models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Activation\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","882ca30a":"#Modeling parameters\ntraingroups = len(train_data)\ntestgroups = len(test_data)\nvalidgroups = len(valid_data)\nimagedata=img_to_array(load_img(x[1])) \nSHAPE=imagedata.shape\n#This will establish the prediction groups for the model.\nclasses = os.listdir(traindir)\nclass_count = len(classes)","00d75e52":"#Let's try the mobilenet with ReduceLROnPlateau with augmentation\nbackend.clear_session()\n\n#Bring in the imagenet dataset training weights for the Mobilenet CNN model.\n#Remove the classification top.\nbase_mobilenet = MobileNet(weights = 'imagenet', include_top = False, \n                           input_shape = SHAPE)\nbase_mobilenet.trainable = False # Freeze the mobilenet weights.\n\nmodel = Sequential()\nmodel.add(base_mobilenet)\n\nmodel.add(Flatten()) \nmodel.add(Activation('relu'))\nmodel.add(Dense(class_count)) \nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n#Compile\nmodel.compile(optimizer = tf.keras.optimizers.SGD(lr=0.01, \n                                                  momentum=.9, nesterov=True), #lr=.001\/mo=.9:91%,lr=.01\/momentum=.8:94%,lr=.01mo=.9:97.5%, lr=.01\/mo=.8 92%, lr=.01\/mo=.85:95% accuracy\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])\n#fit modelz\nhistory = model.fit_generator( \n    train_data, \n    steps_per_epoch = traingroups, \n    epochs = 50,\n    validation_data = valid_data,\n    validation_steps = validgroups,\n    verbose = 1,\n    callbacks=[EarlyStopping(monitor = 'val_accuracy', patience = 10, \n                             restore_best_weights = True),\n               ReduceLROnPlateau(monitor = 'val_loss', factor = 0.9, #0.2 to 0.5 dropped to fast 0.7\n                                 patience = 3, verbose = 1)]) \n                # left verbose 1 so I could see the learning rate decay","57b62f9a":"#plot accuracy vs epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Evaluate against test data.\nscores = model.evaluate(test_data, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","fa10f8fd":"\ntrue_label, true_image = label_images2(testdir, test_data)\npred = model.predict_generator(test_data)\npred_classes=np.argmax(pred,axis=1)\nlabel = [index[k] for k in true_label]\n\nconf_mat=confusion_matrix(label,pred_classes)\nplt.figure()\nplot_confusion_matrix(conf_mat, classes = index,normalize=True)","f7406b5c":"     \ntestDict =  cntSamples(testdir)\n\nvideofile = '\/kaggle\/input\/hummingbirds-at-my-feeders\/video_test'\nvideodata = general_datagen.flow_from_directory(\n    directory = videofile,\n    classes = index,\n    target_size=(224,224),\n    #batch_size=batch_size,\n    class_mode = None,\n    shuffle = False)\nvideodata.reset()\npred=model.predict_generator(videodata,verbose=1,steps=267\/batch_size)\npredicted_class_indices=np.argmax(pred,axis=1)\nprint(predicted_class_indices)\npredictions = getKeysbyValues(index, predicted_class_indices)\nfilenames=videodata.filenames\nresults=pd.DataFrame({\"Filename\":filenames,\n                      \"Predictions\":predictions})\ndisplay(HTML(results.to_html()))\n","6a501a20":"true_label, true_image = label_images2(videofile, videodata)\npred = model.predict_generator(videodata)\npred_classes=np.argmax(pred,axis=1)\nfrom sklearn.preprocessing import LabelEncoder\ncode = LabelEncoder()\nlabel = code.fit_transform(true_label)\n#confusion matrix\nconf_mat=confusion_matrix(label,pred_classes)\nnewindex = indexID(cntSamples(videofile))\nplt.figure()\nplot_confusion_matrix(conf_mat, classes = newindex)","780f300e":"model.save('bird_classifier.h5')","6b969dfa":"\n\n#plot the random images.\n\n\n","4c20dc91":"All_images='..\/input\/hummingbirds-at-my-feeders\/All_images'\nalldata = general_datagen.flow_from_directory(\n    All_images, batch_size=batch_size,\n    target_size=(224,224), shuffle = False)\n\ntrue_label, true_image = label_images2(All_images, alldata)\npred = model.predict_generator(alldata)\npred_classes=np.argmax(pred,axis=1)\nfrom sklearn.preprocessing import LabelEncoder\ncode = LabelEncoder()\nlabel = code.fit_transform(true_label)\n#confusion matrix\nconf_mat=confusion_matrix(label,pred_classes)\nnewindex = indexID(cntSamples(All_images))\nplt.figure()\nplot_confusion_matrix(conf_mat, classes = newindex, normalize =True)","c6c9b2b7":"I will define the model using the classes 'class_count' from the datasets.Compile and fit the model.","a3e595fa":"The model I created here with this code was able to find and classify birds with a 97.5% accuracy. The video feed test was a little more difficult due to bird size.  All of the photos (including those in test) are sized to the majority of the window. The video feed test is 'as-is' from the motion detection of the camera. At this time, I am leaving my CNN model as is, because it is likely that my video stream capture \/ motion detection was focused on the bird feeder motion and not the bird, so it is likely that I would have 2 indicators in the video stream if I were to put a box around the object identified. I would rather capture the motion of the bird.  Two examples that are likely from the same frame-set in the video are presented below. ","f5b4f64a":"* Starting point\n* * 25 correct male class, 14 incorrect no bird out of 64 - 95% on test\n* Increased zoom range from .2 to .4 \n* * 20 correct male class, 15 incorrect no bird out of 64. - 94% on test\n* Added augmentation (shift hieght\/width 20%) arguments:\n* * 9 correctly classified, 13 incorrect no bird - 94% on test.\n* Added brightness (.5-1.5)\n* * 34 correctly male class, 13 incorrect no bird, 93% on test \n\n\n\n\n\n\n* Added augmentation (shift hieght\/width 20% and zoom range increased from .2 to .4) arguments: \n* * 28 correct class, 14 incorrect no bird out of 64 - 95% on test\n* Added 40 degree rotation augmentation.\n* * 13 correct 14 incorrect no bird out of 64 91% on test.\n* changed rotation from 40 to 5\n* * 15 correct class 14 incorrect nobird out of 64 and 94% on test\n* Removed rotation and added brightness level (.2,1)\n* * 25 correct male class, 14 incorrect no bird out of 64 - 93% on test\n* Changed from (.2,1) brightness level (.5,1.5)\n","76732c28":"Plot some random samples from our training dataset.","970f27b7":" This task for my 8 week project will use the photos collected from my feeders' cameras with a binary bird-finder classification model to create a CNN multi-species and gender classification model. The primary birds at the feeders this year are broad-tailed (*Selasphorous platycerus*) and a few rufous (*Selasphorous rufus*) hummingbirds. So these are the species that are currently in the dataset being analyzed.  ","62626d6d":"Load the files using the data generators.","5209e2a8":"Load the image processing and displaying packages:","5c3b581e":"Here is the confusion matrix for this run on test data.","a5477972":"The purpose of this project is to create an image classification for hummingbird species and genders that visit feeders in Colorado. (It is applicable to anywhere that hummingbirds migrate or breed given additional datasets for those species.) The ultimate goal is to have a classification system that can be deployed at any feeder. This is important to the continued monitoring of hummingbird species and migration patterns. Humming bird migration is otherwise reliant on individual bird watchers to see and report their observations. If avid bird lovers setup a similar system, then conservation organizations would have better information on migratory and breeding patterns. This knowledge can be used to determine if specific changes in the environment or ecology has positively or negatively impacted bird life.","6d2badf7":"# **CNN Hummingbird Species and Gender Image Classification**","7d325df6":"Although it is able to classify this as a hummingbird.","ccd34751":"Setup some parameters for the model(s).","fe0b1997":"I initially install my needed file management libraries:","495f2bcd":"Other errors in my classification vs the CNN model are presented here. I classified this as No_bird and the CNN model classified it as a broadtailed_female. I thought that I had deleted all of the wingtips from the video-set.","e0eb9d0b":"Create data generators for training set and test\/valid sets.","3dfc28ce":"Define some functions for evaluating the data: \n* cntSamples\n* * Counts the samples in each category\/directory.\n* indexID\n* * Indexes the categories so I can reverse classify the image sets.\n* label_images\n* * Matches the directory name to the image. Used for presenting\/plotting images in the dataset.\n* getKeysbyValues\n* * Matches all the videos to the index from indexID.\n    \n","d3a5f9ee":"My model classified as No_bird:","20f4181c":"Create the model - in this case using transfer learning with MobileNet - \"small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices\" [1] or small computers for offline video feed\/detection. This is a good choice for this project. \n\nReference:\n[1] https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/slim\/nets\/mobilenet_v1.md","53ba97ab":"Setup the directory structure to the training, test, and valid folders: ","fde76038":"Let's look at the hummingbird training dataset.","f88fb42b":"Load the libraries need for Modeling here:"}}