{"cell_type":{"8936aef4":"code","a457f8d0":"code","d857adfd":"code","953f96bc":"code","f63c12f0":"code","0bb089c7":"code","f5cd98aa":"code","5f859d39":"code","d06fafd0":"code","87bb0dc4":"code","aa916116":"code","eadd9938":"code","2004c073":"code","538d2edf":"code","f6ab0c8a":"code","22d8a03e":"code","0d887e98":"code","f234218f":"code","c543f79e":"code","1bc434e0":"code","09057dd5":"code","74978feb":"code","e04754b5":"code","ec1016cb":"code","2b9be448":"code","4ac7ccf7":"code","89d15a26":"code","e4df8412":"code","0c4c1637":"code","aa3f26a3":"code","10103a1e":"code","e338506d":"code","07d4881b":"code","9d6bb563":"code","6f957e92":"code","26828011":"code","91137a36":"code","57a83182":"code","adbc81be":"code","2f340e0c":"code","b72d5ad9":"markdown","13bac882":"markdown"},"source":{"8936aef4":"##PLEASE UPVOTE IF YOU FIND IT USEFUL\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.filterwarnings(action='ignore')","a457f8d0":"train= pd.read_csv('..\/input\/hackerearths-reduce-marketing-waste\/train.csv')\ntest = pd.read_csv('..\/input\/hackerearths-reduce-marketing-waste\/test.csv')\ntest_sub=pd.read_csv('..\/input\/hackerearths-reduce-marketing-waste\/test.csv')","d857adfd":"train.head()","953f96bc":"train.info()","f63c12f0":"train.describe(include='all')","0bb089c7":"train.columns","f5cd98aa":"#replacing $ sign in deal_value and Weighted_amount column and changing datatype\ntrain['Deal_value']=train['Deal_value'].apply(str).str.replace('$','')\ntrain['Weighted_amount']=train['Weighted_amount'].apply(str).str.replace('$','')\ntrain['Deal_value']=train['Deal_value'].astype('float')\ntrain['Weighted_amount']=train['Weighted_amount'].astype('float')","5f859d39":"#testdata\n#replacing $ sign in deal_value and Weighted_amount column and changing datatype\ntest['Deal_value']=test['Deal_value'].apply(str).str.replace('$','')\ntest['Weighted_amount']=test['Weighted_amount'].apply(str).str.replace('$','')\ntest['Deal_value']=test['Deal_value'].astype('float')\ntest['Weighted_amount']=test['Weighted_amount'].astype('float')","d06fafd0":"test.head()","87bb0dc4":"train.head()","aa916116":"#CREATE HTML REPORT OF DATA Using the pandas Profiler\nprofile = ProfileReport(train)\nprofile","eadd9938":"train.isnull().sum()","2004c073":"test.isnull().sum()","538d2edf":"#null value treatment\ntrain['Deal_value']=train['Deal_value'].fillna((train['Deal_value'].mean()))\ntrain['Weighted_amount']=train['Weighted_amount'].fillna((train['Weighted_amount'].mean()))\ntrain['Industry']=train['Industry'].fillna((train['Industry'].mode()[0]))\ntrain['Last_lead_update']=train['Last_lead_update'].fillna((train['Last_lead_update'].mode()[0]))\ntrain['Resource']=train['Resource'].fillna((train['Resource'].mode()[0]))\n","f6ab0c8a":"# As here we split the Location into place and state as Geography feature having\n# USA has place and state both in Location feature while in case of India it is missing\n\ntrain[['place','state']]=train['Location'].str.split(' ',1, expand=True)\ntrain['state'].fillna('0',inplace=True)\ntrain['Geography'].fillna('USA',inplace=True)\ntrain.loc[train['state']=='0', 'Geography'] = 'India'\ntrain.drop(columns=['place','state'], inplace=True)","22d8a03e":"#null value treatment\ntest['Deal_value']=test['Deal_value'].fillna((test['Deal_value'].mean()))\ntest['Weighted_amount']=test['Weighted_amount'].fillna((test['Weighted_amount'].mean()))\ntest['Industry']=test['Industry'].fillna((test['Industry'].mode()[0]))\ntest['Last_lead_update']=test['Last_lead_update'].fillna((test['Last_lead_update'].mode()[0]))\ntest['Resource']=test['Resource'].fillna((test['Resource'].mode()[0]))\n","0d887e98":"test[['place','state']]=test['Location'].str.split(' ',1, expand=True)\ntest['state'].fillna('0',inplace=True)\ntest['Geography'].fillna('USA',inplace=True)\ntest.loc[test['state']=='0', 'Geography'] = 'India'\ntest.drop(columns=['place','state'], inplace=True)","f234218f":"train.isna().sum()","c543f79e":"train.drop(train.loc[train['Success_probability']>100].index, inplace = True)","1bc434e0":"#Filling all the null value with the most occuring one\ntrain_most_common_imputed = train.apply(lambda x: x.fillna(x.value_counts().index[0]))","09057dd5":"#label Encoding\nle= preprocessing.LabelEncoder()\nfor column in train_most_common_imputed.columns:\n    if train_most_common_imputed[column].dtype==object:\n        train_most_common_imputed[column]=le.fit_transform(train_most_common_imputed[column])\n    else:\n        pass","74978feb":"test = test.apply(lambda x: x.fillna(x.value_counts().index[0]))","e04754b5":"#label Encoding Test Data\nle= preprocessing.LabelEncoder()\nfor column in test.columns:\n    if test[column].dtype==object:\n        test[column]=le.fit_transform(test[column])\n    else:\n        pass","ec1016cb":" #Selecting best featues according to importance\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\ntrain_most_common_imputed[train_most_common_imputed < 0] = 0\nX = train_most_common_imputed.iloc[:,0:]  \ny = train_most_common_imputed.iloc[:,-1] \n\ny=y.astype('int')\nbestfeatures = SelectKBest(score_func=chi2, k=16)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(23,'Score')) ","2b9be448":"X.drop(columns=['Deal_title','Lead_name','Date_of_creation','Contact_no','POC_name',\n       'Lead_POC_email','Success_probability'],inplace=True)","4ac7ccf7":"test.drop(columns=['Deal_title','Lead_name','Date_of_creation','Contact_no','POC_name',\n       'Lead_POC_email'],inplace=True)","89d15a26":"#BOXPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\na=X['Weighted_amount']\nb=X['Fund_category']\nc=X['Geography']\nd=X['Lead_source']\ne=X['Level_of_meeting']\nf=X['Internal_rating']\nto_plot=[a,b,c,d,e,f]\nfig=plt.figure(1,figsize=(16,9))\nax=fig.add_subplot(111)\nbp=ax.boxplot(to_plot)","e4df8412":"#removing Outliars from training data\nfrom scipy import stats\nz_scores = stats.zscore(X)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nX = X[filtered_entries]\nX","0c4c1637":"X_train,X_test,Y_train,Y_test= train_test_split(X,y,test_size=.2,random_state=121)","aa3f26a3":"from catboost import CatBoostRegressor \nfrom sklearn.metrics import mean_absolute_error\nmodel_cat= CatBoostRegressor(iterations=2000,random_state=121,learning_rate=.1,max_depth=3)\nmodel_cat.fit( X_train, Y_train, use_best_model=True, eval_set=(X_test, Y_test), plot=True )\n","10103a1e":"Y_pred_test=model_cat.predict(X_test)\nY_pred=model_cat.predict(test)\n","e338506d":"from sklearn import metrics\nscore=max(0,100-np.sqrt(metrics.mean_squared_error(Y_test,Y_pred_test)))\nprint(score)","07d4881b":"from sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint\n\nmod= CatBoostRegressor()\n\npar={'max_depth':[3,5,10],\n    'n_estimators':[100,200,300,400]\n     ,'learning_rate':[0.1,0.01,0.001]}\ndef hyperparameter_tuning(mod,param_d,p,q):\n    rdmsearch=  GridSearchCV(mod, param_grid=param_d,n_jobs=-1,cv=9,scoring='neg_mean_squared_error')\n    rdmsearch.fit(p,q)\n    ht_params = rdmsearch.best_params_\n    ht_score = rdmsearch.best_score_\n    return ht_params, ht_score\n\nrf_parameters, rf_ht_score = hyperparameter_tuning(mod, par,  X_train, Y_train)\n","9d6bb563":"print(rf_parameters, rf_ht_score)","6f957e92":"test.head()","26828011":"y_test_title=test_sub.Deal_title.values","91137a36":"sub=pd.DataFrame(y_test_title,columns=['Deal_title'])","57a83182":"sub['Success_probability']=Y_pred","adbc81be":"sub.head()","2f340e0c":"sub.to_csv('sub1.csv',index=False)","b72d5ad9":"## Null value Treatment","13bac882":"## HyperParameter Tuning"}}