{"cell_type":{"b385dd7c":"code","d0fca64d":"code","db50b6d3":"code","15c2acad":"code","b900b622":"code","47a9a435":"code","6ad94f8d":"code","a08edcf3":"code","8ded7091":"code","08ac943a":"code","741cdfaa":"code","44a74dbf":"code","2c7ac17b":"code","8a87c372":"code","69e46d05":"code","74f651d4":"code","02961f99":"code","7a9f82cc":"code","c69f202b":"code","1456e015":"markdown","d6dd121f":"markdown","520cc8a0":"markdown","1581fd4e":"markdown","6092add8":"markdown","12ab859a":"markdown","4b251f40":"markdown","ff79a3d1":"markdown","3ad17461":"markdown","17cc158c":"markdown","3f615548":"markdown","e8428d17":"markdown","ab2365c3":"markdown","58ba6e1d":"markdown","9a4f2ef5":"markdown","25452535":"markdown","45ac79d0":"markdown","0e696278":"markdown","ef37da21":"markdown","db60dd61":"markdown","1999ed9c":"markdown","5a737911":"markdown","747b2e79":"markdown","b89bc681":"markdown","a7fe2c99":"markdown","10ccd0bb":"markdown"},"source":{"b385dd7c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\nimport matplotlib.cm as cm\nfrom sklearn.preprocessing import StandardScaler\n\n\nX = np.random.randn(100).reshape(100,1)\ny = 3*X + 2*np.random.randn(100).reshape(100,1)+1\n\nplt.scatter(X,y)\nplt.title(\"Dataset\")\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Label\")\nplt.show()","d0fca64d":"linspace = np.arange(-1,1,0.01)\nplt.plot(linspace,np.square(linspace))\nplt.xlabel(\"Error\")\nplt.ylabel(\"MSE\")\nplt.show()","db50b6d3":"n = len(X)\nd = 1\n\n# Add column of ones to X\nX_t = np.reshape(np.dstack((np.ones((n,1)),X)),(n,d+1))\n\n# One Shot Solution\ntheta_star = np.dot(np.linalg.inv(np.dot(X_t.T,X_t)),np.dot(X_t.T,y))\n\n# Plot Data\nplt.scatter(X,y)\n\n# Plot Regressor Line\nlinspace = np.arange(-4,3,0.1).reshape(-1,1)\ny_line = np.dot(np.dstack((np.ones((70,1)),linspace)).reshape(70,2),theta_star)\nplt.plot(linspace,y_line, c='r')\n\nplt.title(\"Dataset\")\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Label\")\nplt.show()","15c2acad":"\ndef f(X, Y):\n    return X*X + (Y*Y)*1.5\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(121, projection='3d')\n\n# Make the X, Y meshgrid.\nxs = np.linspace(-1, 1, 100)\nys = np.linspace(-1, 1, 100)\nX_3d, Y_3d = np.meshgrid(xs, ys)\n\n# Set the z axis limits so they aren't recalculated each frame.\nax.set_zlim(-0.5, 2)\n\n\nZ = f(X_3d, Y_3d)\nwframe = ax.plot_surface(X_3d, Y_3d, Z, rstride=2, cstride=2,zorder=10, cmap=cm.coolwarm)\nwframe = ax.contour(X_3d, Y_3d, Z, zdir='z', offset=-0.5, cmap=cm.coolwarm)\nplt.title(\"f(x\u20d7)\",fontsize=20)\nplt.xlabel(\"X1\",fontsize=15)\nplt.ylabel(\"X2\",fontsize=15)\nax1 = fig.add_subplot(122)\nplt.title(\"f(x\u20d7)\",fontsize=20)\nplt.xlabel(\"X1\",fontsize=15)\nplt.ylabel(\"X2\",fontsize=15)\nwframe = ax1.contour(X_3d, Y_3d, Z, cmap=cm.coolwarm)\nplt.show()","b900b622":"alpha = 0.1\n\n# f(x\u20d7 )\ndef f(x):\n    return x[0]*x[0] + x[1]*x[1]*1.5\n\n# \u2207f(x\u20d7 )\ndef grad_f(x):\n    return np.array([2*x[0],3*x[1]])\n\niter_max = 12\n\nx = -0.75*np.ones((iter_max,2))\n\n# \u2207f(x\u20d7 ) on many points at once\ndef fvec(x_array):\n    output = []\n    for x in x_array:\n        output.append(f(x))\n    return np.array(output)\nfig = plt.figure(figsize=(30,int(iter_max\/3) * 10))\nfor i in range(0,iter_max):\n    \n    ax1 = fig.add_subplot(int(iter_max\/3),3,i+1)\n    plt.title(\"f(x\u20d7)\",fontsize=20)\n    plt.xlabel(\"X1\",fontsize=15)\n    plt.ylabel(\"X2\",fontsize=15)\n    \n    wframe = ax1.contour(X_3d, Y_3d, Z, cmap=cm.coolwarm, zorder = -2)\n    wframe = ax1.plot(x[:i+1,0],x[:i+1,1],c='k', zorder = -1)\n    wframe = ax1.scatter(x[:i+1,0],x[:i+1,1], zorder = 1)\n    if i < iter_max-1:\n        x[i+1] = x[i] - alpha * grad_f(x[i])\n\nplt.show()","47a9a435":"# Scale original data\nXn = StandardScaler().fit_transform(X)\nprint(\"Original Data Mean\", X.mean(axis = 0), \"\\nOriginal Data Variance\", X.var(axis = 0)\n      , \"\\nNormalised Data Mean\", Xn.mean(axis = 0), \"\\nNormalised Data Variance\", Xn.var(axis = 0))\n# Add column of ones\nXn = np.reshape(np.dstack((np.ones((n,1)),Xn)),(n,d+1))\nyn = StandardScaler().fit_transform(y)\n# Plot Normalised data\nplt.scatter(Xn[:,1],yn)\nplt.show()\n","6ad94f8d":"# Learning rate\nalpha = 0.1\n\nmax_iterations = 100\n\n# Vector of errors at each iteration of the desccent\nerr_iter = np.zeros(max_iterations)\n\ndef J(X, y, theta):\n    e = y - np.dot(X,theta)\n    return (1\/n) * np.dot(e.T,e)\n\ndef grad_J(X, y, theta):\n    return (2\/n) * (np.dot(np.dot(X.T,X),theta) - np.dot(X.T,y))\n\n# Initialise theta at random\ntheta = np.random.randn(d+1).reshape(d+1,1)\n\nfor i in range(0, max_iterations):\n    # Compute J(theta_k-1)\n    err_iter[i] = J(Xn, yn, theta)\n    # Update theta_k\n    theta -= alpha * grad_J(Xn, yn, theta)\n    \nfig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(121)\nplt.plot(np.arange(1,101,1),err_iter)\nplt.title(\"Gradient Descent Optimisation\",fontsize=15)\nplt.xlabel(\"Iterations\",fontsize=10)\nplt.ylabel(\"Error\",fontsize=10)\n\n\nax1 = fig.add_subplot(122)\n\n# Plot Data\nplt.scatter(Xn[:,1],yn)\n\n# Plot Regressor Line with theta from the gradient descent\nlinspace = np.arange(-4,3,0.1).reshape(-1,1)\ny_line = np.dot(np.dstack((np.ones((70,1)),linspace)).reshape(70,2),theta)\nplt.plot(linspace,y_line, c='r')\n\nplt.title(\"Dataset\")\nplt.xlabel(\"Scaled Feature\")\nplt.ylabel(\"Scaled Label\")\nplt.show()\n\nprint(\"Scaled data Coefficients:\\n\", theta)","a08edcf3":"import scipy.stats as stat\n\nbeta = theta[1][0]\nprint (\"beta:\",beta)\n\n# compute residuals y - y_hat\n\ny_hat = np.dot(Xn,theta)\ne = yn - y_hat\n\n# compute the standard error\n\nse = np.sqrt((np.dot(e.T,e)\/(n-2))\/(Xn[:,1].var()*(n-1)))[0][0]\n\n# compute t-score\n\ntscore = beta\/(se)\n\nprint(\"t-score(beta):\", tscore)\n\n# compute probability through the t-student probability density function\n\npvalue = 2*stat.t.pdf(tscore,n-2)\n\nprint(\"p-value:\", pvalue)\n","8ded7091":"# yn is normalised and as such there's no need to subtract the mean when computing the sum of squares\n\nSStot = np.dot(yn.T,yn)[0][0]\n\nSSres = np.dot(e.T,e)[0][0]\n\nR2 = 1 - (SSres\/SStot)\n\nprint(\"R2 coefficient is:\",R2)","08ac943a":"mse = np.dot(e.T,e)[0][0]\n\nprint(\"MSE:\", mse)","741cdfaa":"fig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(121)\nplt.plot(np.arange(0,len(X),1),yn, label='Real Value')\nplt.plot(np.arange(0,len(X),1),y_hat,label='Prediction')\nax1.legend()\nplt.title(\"Labels VS Prediction point by point\")\n\nax2 = fig.add_subplot(122)\n\nplt.plot(np.arange(0,len(X),1),np.sort(yn,axis=0), label='Real Value')\nplt.plot(np.arange(0,len(X),1),np.sort(y_hat,axis=0),label='Prediction')\nax2.legend()\nplt.title(\"Labels VS Prediction point by point sorted\")\nplt.show()","44a74dbf":"y_poly = np.exp(-2*Xn[:,1]+1).reshape(n,1) + 10*np.random.randn(100).reshape(100,1)+1\ny_poly = StandardScaler().fit_transform(y_poly)\nslope, intercept, r_value, p_value, std_err = stat.linregress(Xn[:,1],y_poly.reshape(n,))\n\nlin_poly_hat = slope * Xn[:,1] + intercept\n\nplt.scatter(Xn[:,1],y_poly)\nplt.plot(Xn[:,1],lin_poly_hat, 'r')\nplt.title(\"Dataset 2\")\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Label\")\nplt.show()\n\nprint(\"p-value: \",p_value, \"\\nR2: \", r_value*r_value)","2c7ac17b":"alpha = 0.01\nmax_iterations = 200\n\ndef J_poly(X, y, theta):\n    e = y - np.dot(X,theta)\n    return (1\/n) * np.dot(e.T,e)\n\ndef grad_J_poly(X, y, theta):\n    e = y - np.dot(X,theta)\n    grad = -(2\/n) * np.array([np.sum(e), np.dot(X[:,1],e), np.dot(X[:,2],e), np.dot(X[:,3],e)])\n    return grad.reshape(4,1)\n\n# Initialise theta at random\ntheta_poly = np.random.randn(4).reshape(4,1)\n\nchi = np.array(list(map(lambda x: [1,x,x*x,x*x*x], list(Xn[:,1])))).reshape(n,4)\n\n\nerr_iter = np.zeros(max_iterations)\n\nfor i in range(0, max_iterations):\n    # Compute J(theta_k-1)\n    err_iter[i] = J_poly(chi, y_poly, theta_poly)\n    # Update theta_k\n    theta_poly -= alpha * grad_J_poly(chi, y_poly, theta_poly)\n    \nfig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(121)\nplt.plot(np.arange(1,max_iterations+1,1),err_iter)\nplt.title(\"Gradient Descent Optimisation\",fontsize=15)\nplt.xlabel(\"Iterations\",fontsize=10)\nplt.ylabel(\"Error\",fontsize=10)\n    \nax1 = fig.add_subplot(122)\n\n# Plot Data\nplt.scatter(Xn[:,1],y_poly)\n\n# Plot Regressor Line with theta from the gradient descent\nlinspace = np.arange(-4,3,0.1).reshape(-1,1)\nlinspace_poly = np.array(list(map(lambda x: [1,x,x*x,x*x*x], list(linspace)))).reshape(-1,4)\ny_line = np.dot(linspace_poly,theta_poly).reshape(-1)\nplt.plot(linspace,y_line, c='r')\n\nplt.title(\"Dataset\")\nplt.xlabel(\"Scaled Feature\")\nplt.ylabel(\"Scaled Label\")\nplt.show()\n","8a87c372":"alpha = 0.01\nmax_iterations = 200\n\n# Initialise theta at random\ntheta_poly = np.random.randn(4).reshape(4,1)\n\nerr_iter = np.zeros(max_iterations)\n\nfor i in range(0, max_iterations):\n    # Compute J(theta_k-1)\n    err_iter[i] = J_poly(chi, yn, theta_poly)\n    # Update theta_k\n    theta_poly -= alpha * grad_J_poly(chi, yn, theta_poly)\n    \nfig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(121)\nplt.plot(np.arange(1,max_iterations+1,1),err_iter)\nplt.title(\"Gradient Descent Optimisation\",fontsize=15)\nplt.xlabel(\"Iterations\",fontsize=10)\nplt.ylabel(\"Error\",fontsize=10)\n    \nax1 = fig.add_subplot(122)\n\n# Plot Data\nplt.scatter(Xn[:,1],yn)\n\n# Plot Regressor Line with theta from the gradient descent\nlinspace = np.arange(-4,3,0.1).reshape(-1,1)\nlinspace_poly = np.array(list(map(lambda x: [1,x,x*x,x*x*x], list(linspace)))).reshape(-1,4)\ny_line = np.dot(linspace_poly,theta_poly).reshape(-1)\nplt.plot(linspace,y_line, c='r')\n\nplt.title(\"Dataset\")\nplt.xlabel(\"Scaled Feature\")\nplt.ylabel(\"Scaled Label\")\nplt.show()\n","69e46d05":"e = yn - np.dot(chi,theta_poly)\n\nprint(\"Linear MSE:\", mse,\"\\nPolynomial MSE:\", np.dot(e.T,e)[0,0])","74f651d4":"alpha = 0.001\nmax_iterations = 20000\nLambda = 5\n\ndef J_poly(X, y, theta):\n    e = y - np.dot(X,theta)\n    return (1\/n) * np.dot(e.T,e) + Lambda * np.dot(theta[1:].T,theta[1:])\n\ndef grad_J_poly(X, y, theta):\n    e = y - np.dot(X,theta)\n    grad = -(2\/n) * np.array([np.sum(e), np.dot(X[:,1],e)-Lambda*theta[1], np.dot(X[:,2],e)-Lambda*theta[2], np.dot(X[:,3],e)-Lambda*theta[3]])\n    return grad.reshape(4,1)\n\n# Initialise theta at random\ntheta_poly = np.random.randn(4).reshape(4,1)\n\nerr_iter = np.zeros(max_iterations)\n\nfor i in range(0, max_iterations):\n    # Compute J(theta_k-1)\n    err_iter[i] = J_poly(chi, yn, theta_poly)\n    # Update theta_k\n    theta_poly -= alpha * grad_J_poly(chi, yn, theta_poly)\n    \nfig = plt.figure(figsize=(30,10))\nax1 = fig.add_subplot(121)\nplt.plot(np.arange(1,max_iterations+1,1),err_iter)\nplt.title(\"Gradient Descent Optimisation\",fontsize=15)\nplt.xlabel(\"Iterations\",fontsize=10)\nplt.ylabel(\"Error\",fontsize=10)\n    \nax1 = fig.add_subplot(122)\n\n# Plot Data\nplt.scatter(Xn[:,1],yn)\n\n# Plot Regressor Line with theta from the gradient descent\nlinspace = np.arange(-4,3,0.1).reshape(-1,1)\nlinspace_poly = np.array(list(map(lambda x: [1,x,x*x,x*x*x], list(linspace)))).reshape(-1,4)\ny_line = np.dot(linspace_poly,theta_poly).reshape(-1)\nplt.plot(linspace,y_line, c='r')\n\nplt.title(\"Dataset\")\nplt.xlabel(\"Scaled Feature\")\nplt.ylabel(\"Scaled Label\")\nplt.show()\n\nprint(theta_poly)\ne = yn - np.dot(chi,theta_poly)\n\nprint(\"Linear MSE:\", mse,\"\\nPolynomial MSE:\", np.dot(e.T,e)[0,0])","02961f99":"n, p = 10, 0.5  # number of trials, probability of each trial\nX = np.random.binomial(n, p, 100000)\nax = sns.countplot(X, color='teal')\nax.set_title('B(10,0.5) Distribution over 100000 samples')\nax.set_ylabel('Occurrences')\nax.set_xlabel('Number of Heads')\n\nplt.show()","7a9f82cc":"ax = sns.countplot(X, palette = sns.color_palette(['red']*2+['teal']*7+['red']*2))\nax.set_title('B(10,0.5) Distribution over 100000 samples')\nax.set_ylabel('Occurrences')\nax.set_xlabel('Number of Heads')\n\nplt.show()\n","c69f202b":"sigma = 2\nmu = 1\nexperiments = 1000\n\nX = sigma*np.random.randn(10000)+mu\n\nexp_gauss_0 = sigma*np.random.randn(10000,1000)+mu\n\nZ = (exp_gauss_0.mean(axis=0)-mu)\/(sigma\/np.sqrt(experiments))\n\nx = sigma * np.random.randn(1000,10) + mu\nt = (x.mean(axis=1)-mu)\/np.sqrt((x.var(axis=1)\/10))\n\nx_2 = sigma * np.random.randn(1000,100) + mu\nt_2 = (x_2.mean(axis=1)-mu)\/np.sqrt((x_2.var(axis=1)\/100))\n\nfig = plt.figure(figsize=(20,20))\nplt.subplot(221)\nax = sns.distplot(X, color='teal', hist = False, kde=True)\nax.set_title('N(1,4) Distribution over 10000 samples')\nax.set_xlabel('X')\n\nplt.subplot(222)\nax1 = sns.distplot(Z, color='teal', hist = False, norm_hist = True )\nax1.set_title('N(0,1) Distribution over 1000 samples')\nax1.set_xlabel('X')\n\nplt.subplot(223)\nax2 = sns.distplot(t, color='teal', hist = False, kde=True, norm_hist = True)\nax2.set_title('t-distribution with 9 degrees of freedom over 1000 samples')\nax2.set_xlabel('X')\n\nplt.subplot(224)\nax3 = sns.distplot(t_2, color='teal', hist = False, kde=True, norm_hist = True)\nax3.set_title('t-distribution with 99 degrees of freedom over 1000 samples')\nax3.set_xlabel('X')\n\nplt.show()","1456e015":"The p-value can be computed as $P(|X - 5| \\geq 4|H_0)$ having considered all the outcomes of assuming $H_0$ true, we have\n\n$P(|X - 5| \\geq 4|H_0) =2 P(X\\geq 9) = 2\\sum_{i=9}^{10}{{10}\\choose{i}} 0.5^{10} = 0.021484375$.\n\nThis means that doing this experiment, assuming $H_0$ true, our observation is pretty remarkable and rare.\n\nThe p-value is $< 0.05$, therefore we can't accept $H_0$ and we have to refuse it in favor of $H_a$.\n\n**THE COIN IS FISHY!**","d6dd121f":"## Performance Evaluation\n\nSo how did our regression fare?\n\nLet us see if our regression is significant.\n\nAssuming we know what a [p-value](#p-value) is, and how to use it in conjunction with a [t-student distribution](#t-student-distribution)...\n\nwe're going to assume as the Null Hypothesis that there is no correlation between $X$ and $y$, therefore $E[Cov(X_i^{(n)T},y^{(n)})] = 0, i = 1,2,...,d$, and thus,\n\nfor all the linear coefficients, $E[\\theta^*_i]=0$.\n\nAs we have stated $\\theta^*_i$ comes from a Normal Distribution, its t-score would follow a t-student distribution of $n-1$ degrees of freedom; but since our $\\hat{\\theta^*}$ is in fact computed upon the sample estimators of $\\Sigma$ and $\\gamma$, the t-score computed on $\\hat{\\theta^*}$  is the result of two stages of sampling upon a Normally distributed random variable and therefore it follows a t-student distribution of $n-2$ degrees of freedom.\n\n$H_0: E[\\theta_1^*] = 0, \\space H_a: E[\\theta_1^*] \\neq 0$\n\nIf $\\beta$ is the result of your regression, we can compute the p-value as such: $P(|t|\\geq$ t-score$(\\beta)) = 2P(t\\geq$ t-score$(\\beta))$.\n\nt-score$(\\beta)$ can be computed as $\\frac{\\beta-0}{se(\\beta)}$, with $se$ being the standard error of the regressor.\n\nWe set our threshold at $0.05$ and start computing.","520cc8a0":"The last metric would be the one we wanted to minimise from the start, the Mean Squared Error.","1581fd4e":"### p-value\n\nLet's say we have an hypothesis we want to test let us call it $H_a$, to accept an hypothesis through an experiment we must take into account the aleatory nature of the observations.\n\nWe call $H_0$ the Null Hypothesis, incosistent with $H_a$, and $H_a$ the alternative hypothesis.\n\nWhat we want to decide is if our observation can lead us to refuse $H_0$ in favor of $H_a$.\n\nTo do so we set a threshold we call significance level, a typical value is $0.05$\n\nWe refuse $H_0$ in favor $H_a$ if $P(X \\space equal \\space or \\space more \\space extreme \\space than \\space x | H_0)< 0.05$ \n\nWhere the probability is known as p-value, $X$ is a random variable that describes the observation as a statistic and $x$ is the actual statistic observed from the experiment.\n\n\"More extreme\" might be greater, lower or both depending on the $H_0$, in general it designates the observations that would be even rarer under the assumption of $H_0$ \n\n**Example**\n\n$X\\tilde{}B(10,0.5)$\n\n$X =\\sum_{i=1}^{10}X_i, X_i\\tilde{}B(1,0.5)$\n\nThe first being the Binomial distribution and the latter the Bernoulli distribution.\n\nThe Bernoulli distribution with $p=0.5$ models the flip of a fair coin with 1 being a head and 0 a tail.\n\nA Binomial distribution of $n$ elements and $p=0.5$ models the number of heads when you flip $n$ fair coins.\n\nFirst things first, let's visualise and recall some facts about the Binomial distribution.","6092add8":"Our goal is to find the best line to represent the relation (or trend) between $X$ and $y$.\nTo do so, we first define what it means to be the best, or rather what it means to be bad.\n\nLet us call $\\hat{y} = X^{(t)}\\theta$ our regressor line, where $\\theta$ is a vector of $d+1$ dimensions and $X^{(t)}$ is the $n * (d+1)$ matrix $1_n|X$. \n\nThis is a more compact version of the  $\\hat{y} = X^T \\theta_{1,...,d}+ \\theta_0$, with $\\theta_{1,...,d}$ as the vector of the coefficients of each dimension and $\\theta_0$ as the intercept.\n\nWe define the error $J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2$, where $e_i=y_i-\\hat{y_i}$ and $\\hat{y_i} = X^{(t)}_i \\theta$ for $i=1,2,...,n$\n\n$J(\\theta)$ thus defined  is called Mean Squared Error (or MSE for short), this function has several interesting features:\n1. $MSE(e) \\propto e^2$, thus it punishes errors the harshlier, the more they diverge from the correct value.\n2. $MSE(e) =  MSE(-e)$,  thus it considers error on one side equal to their counterparts on the other side.\n3. $MSE(\\theta)$ is a convex function, thus a single global minimum exists.","12ab859a":"### t-student distribution\n\nThe t-student distribution with $n$ degrees of freedom describes the random variable $X = \\frac{Z}{\\sqrt{V\/n}}$, where $Z$ and $V$ are two indipendent random variables, and $Z \\tilde{} N(0,1), V \\tilde{} \\chi^2_n $\n\nBeing $\\chi^2_n = \\sum_{i=1}^n X_i^2, \\space X_i \\tilde{} N(0,1)$.\n\nHaving $X \\tilde{}  N(\\mu,\\sigma^2)$, we know that $Z = \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2\/n}}, Z \\tilde{} N(0,1)$.\n\nLet's say we do not know the variance, we then define the t-score of the population of a realisation of $X$ over $n$ samples, \n\n$t_{n} = \\frac{\\bar{X}-\\mu}{\\sqrt{S^2\/n}}$.\n\nLet's state that by **Cochran's Theorem** $V = \\frac{(n-1) S^2}{\\sigma^2}$ follows $\\chi^2_{n-1}$\n\nWe than have $t_{n} = \\frac{\\bar{X}-\\mu}{\\sqrt{S^2\/n}} = \\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{\\sigma^2}{n}\\frac{(n-1)S^2}{(n-1)\\sigma^2}}}$ by dividing and multiplying the argument of the square root by $(n-1)\\sigma^2$.\n\nRecalling that $Z = \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2\/n}}$ and $V = \\frac{(n-1) S^2}{\\sigma^2}$,\n\n$t_{n} = \\frac{Z}{\\sqrt{V\/(n-1)}}$, which means it follows the t-student distribution with $n-1$ degrees of freedom.\n\nLet us visualise it.","4b251f40":"Lastly we may visualise how well our regression predicts the real values with some graphs.\n","ff79a3d1":"And there we are, having finished our first lesson in Machine Learning, see you later\n\nFrogm Out!\n\n![Frogmouth](https:\/\/img00.deviantart.net\/09d0\/i\/2012\/340\/1\/c\/tawny_frogmouth_by_divinghusky-d5n9ljq.jpg)","3ad17461":"Now we're talking! This polynomial regression fits the data with more precision, so why not apply the same regressor to our original data?","17cc158c":"The p-value is $<< 0.05$, we are very confident that our regression is statistically significant.","3f615548":"## Polynomial Regression\n\nLet's say we have a different dataset, this time already scaled and with its linear regression.","e8428d17":"## The Solution\n\nWe then want to find the $\\theta^*$ which minimises  $J(\\theta)$.\n\nWe may do that by finding the $\\theta$ which satisfies $\\nabla J(\\theta) = 0_{d+1}$\n\nSuch solution is called the One Shot Solution.\n\n\n\nBeing $y = \\begin{bmatrix}\n           y_{1} \\\\\n           y_{2} \\\\\n           \\vdots \\\\\n           y_{n}\n         \\end{bmatrix}$\n         \nWe can rewrite $MSE(\\theta)$ as $\\frac{1}{n}(y- X^{(t)}\\theta)^T(y- X^{(t)}\\theta)$.\n\nThen we can [prove](#Derivation-of-the-One-Shot-Solution) that the optimum is $\\theta^*=(X^{(t)T}X^{(t)})^{-1}X^{(t)T}y$ ","ab2365c3":"With regularisation, the curve tends towards a straight line again, the highest coefficient is the one relative to $ X^{(n)}$ and the $MSE$ returns to the value seen in the linear regression.","58ba6e1d":"Doesn't look half bad in this case either! But which regression was the best one for this dataset?\n\nWe can't compare $R^2$ as previously stated, so let's review our $MSE$","9a4f2ef5":"We observe that with the scaled dataset, the formula $\\theta^*=(X^{(n)T}X^{(n)})^{-1}X^{(n)T}y^{(n)}$ , can be rewritten as \n\n$\\theta^*=\\Sigma^{-1} \\gamma$, where $\\Sigma$ is the variance-covariance matrix of $X^{(n)}$ and $\\gamma = \\begin{bmatrix}\n            Cov(1_n, y^{(n)}) \\\\\n           Cov(X^{(n)T}_1, y^{(n)}) \\\\\n           Cov(X^{(n)T}_2, y^{(n)}) \\\\\n           \\vdots \\\\\n            Cov(X^{(n)T}_{d}, y^{(n)})\n         \\end{bmatrix}$\n  \n  Since $X^{(n)}_i$ are random variables from an unknown distribution of mean zero and variance 1,\n  \n  and the same reasoning can be applied to $y^{(n)}$,\n  \n   we can the state for the [Central Limit Theorem](#Central-Limit-Theorem) that $\\theta^*$ is a vector of random variables from distributions that tend to the Normal Distribution \n   \n   with a certain mean and variance, for $n \\to \\infty$. \n   \n   This will be useful later.","25452535":"## Gradient Descent\n\nWe've seen the solution to our problem, and we've noted that it comprises a pretty onerous set of operations, especially the inverse of the matrix $X^{(t)T}X^{(t)}$.\n\nTherefore we may want to find another way to find the optimal $\\theta$, one such way is the iterative optimisation: a set of algorithms that use an iterative improvement of the solution until a certain condition is met.\n\nAmong these algorithms the simplest is called **Gradient Descent**.\n\nProvided a convex scalar function $f(\\vec{x})$, such as the following","45ac79d0":"$P(X=k) = {{n}\\choose{k}} p^k (1-p^{n-k})$ or in our case $P(X=k) = {{10}\\choose{k}} 0.5^{10}$\n\n$P(X\\geq k) = \\sum_{i=k}^{n}{{n}\\choose{i}} p^i (1-p^{n-i})$ or in our case $P(X\\geq k) = \\sum_{i=k}^{10}{{10}\\choose{i}} 0.5^{10}$\n\n$\\mu = np$ or in our case $\\mu = 10 \\cdot 0.5 = 5$\n\nWe want to inspect a suspicious coin, we want to understand its fairness through an experiment.\n\nWe flip ten coins, compute the number of heads as a statistic $x$, take as the Null Hypothesis that the coin is fair $H_0: p=0.5$, as the Alternative Hypothesis that the coin is unfair $H_a: p \\neq 0.5$.\n\nWe set the threshold for the significance level at $0.05$\n\nWhat we're looking for is to compute the p-value and compare it to the significance level.\n\nThus we have $H_0: p=0.5, H_1: p \\neq 0.5$, we observe $x=9$\n\nGiven that we assume $H_0$ true, we derive several conclusions:\n1. The statistic \"number of heads\" $X \\tilde{} B(10,0.5)$\n2. An observation with $n$ tails ($10-n$ heads) is as extreme as one with $n$ heads\n3. An observation more extreme than a given observation is one that's to the right of the rightmost threshold on the distribution or to the left of the leftmost. Or, in coins, more than 9 heads and less than one head. \n\n$x = 9$ gives us the rightmost threshold $9$, we then have $1$ as the leftmost one and the points that are as extreme as our observation or more extreme than it are the points:\n\n$|X-\\mu|\\geq 9-\\mu$, which means $|X-5|\\geq 4$\n","0e696278":"As we have already stated, $J(\\theta)$ is a convex function and we can apply the Gradient Descent to minimise it.\n\nIt's worth noting that the Gradient Descent converges faster when the features are appropiately scaled to one another.\n\nWe scale $X$  with so that each feature has mean zero and its standard deviation is 1, then we add the aforementioned $1_n$, then we do the same for $y$\n\n$x^{(n)}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$","ef37da21":"## The Problem\n\nWe have a dataset, defined as an $X$ and a $y$, where $X$ is a matrix of $n$ samples and $d$ features, \nwhile $y$ is a vector of $n$ labels or values, one for each line of $X$.\n\nAs an example throughout the lesson we'll use an $X$ with just one feature.","db60dd61":"> The less is more\n\nThe polynomial regression tries \"too hard\" to fit the points to the detriment of the whole regressor, and the higher is the degree of the polynomial, the worse is this tendency. \n\nThis phenomenon is known as overfitting and it is the Machine Learning equivalent of learning the lesson by heart, instead of understanding it. (Ed. That's bad)\n\nTo avoid overfitting we introduce...\n\n## Regularisation\n\nBy adding a term to the cost function, we can punish an excessive complexity of the model.\n\n$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2 + \\lambda ||\\space\\theta\\space||^2$, with $||\\space\\theta\\space||^2$ being the squared norm of the coefficients but $\\theta_0$ and $\\lambda$ a constant used to regulate the trade-off between complexity of the model and its accuracy.\n\nIn fact large values of $\\lambda$ will induce the optimisation to care more about a simple model its performance notwithstanding; while a value leaning towards zero will bring the cost back to the unregularised version where only the error of the model matters.\n\nWe use gradient descent again, updating the cost and its gradient for the last time.","1999ed9c":"## Appendix\n\n### Derivation of the One Shot Solution\n$MSE(\\theta) = \\frac{1}{n}(y- X^{(t)}\\theta)^T(y- X^{(t)}\\theta)$\n$=\\frac{1}{n}(y^Ty- 2\\theta^TX^{(t)T} y +\\theta^T X^{(t)T} X^{(t)} \\theta)$\n\n$\\nabla MSE(\\theta) = \\frac{1}{n} (-2 X^{(t)T} y +2X^{(t)T}X^{(t)}\\theta)$\n\nWe want $\\nabla MSE(\\theta) = 0_{d+1}$, which implies, assuming $X^{(t)T}X^{(t)}$ is invertible, \n\n$\\theta^* = (X^{(t)T}X^{(t)})^{-1} X^{(t)T}y$\n","5a737911":"We immediately see tht a linear regression does not yield good results, even though the p-value is good, with a dataset with such a nonlinear correlation.\n\nIt's important to note that $R^2$ is a coefficient tailored for the linear regression and thus gives us a worse result than the p-value, which is just telling us that there is indeed a correlation between the feature and the label.\n\nWe might then try to use a higher degree polynomial to fit our data, so that instead of $\\hat{y_i^{(n)}} = X^{(n)}_i\\theta$ we have $\\hat{y_i^{(n)}} = X^{(n)}_i\\theta +  X^{(n)2}_i\\psi + \\dots + X^{(n)k}_i \\omega$.\n\nWe can rewrite the latter in a more concise way as such:\n\nFollowing our example we'll limit $k$ to 3 so that $\\hat{y_i^{(n)}} = \\theta_0 + X^{(n)}_i\\theta_1 +  X^{(n)2}_i\\theta_2 + X^{(n)3}_i \\theta_3$, where this time $\\theta_j$ represents the coefficient of the feature to the $j^{th}$ degree, rather than the coefficient of the $j^{th}$ feature.\n\nWe may rewrite in a more concise way the previous formula as such:\n\n$y^{(n)}_i = \\chi_i \\theta$ with $\\chi_i = \\begin{bmatrix}\n           1  \\space\n           X^{(n)}_i  \\space\n           X^{(n)2}_i  \\space\n           X^{(n)3}_i \\space\n         \\end{bmatrix}$\n\nAs the complexity arises, we'lll only see the solution with the gradient descent: the idea behind it stays the same, we just have to update the cost and its gradient.","747b2e79":"### Central Limit Theorem\n\n$ $\n\n**Characteristic function**\n\nLet us call \n$\\varphi_X: \\mathbb{R} \\to \\mathbb{C}$, the characteristic function of the distribution of the random variable $X$\n\nWe define it as:\n\n$\\varphi_X(t)=E[e^{itX}] = \\int_{\\mathbb{R}} e^{itx} dF_X(x)$\n\nwhere $E[\\bullet]$ is the Expected operator and $F_X(x)$ is the Cumulative Distribution Function of $X$.\n\nIf the latter is differentiable, and we call $f_X(x)$ the Probability Density Function, we can rewrite $\\varphi_X(t)$\n\n$\\varphi_X(t) = \\int_{\\mathbb{R}} e^{itx} f_X(x)dx$\n\nIt's trivial to prove that $\\varphi_X(t) = \\varphi_Y(t) \\iff X \\space and \\space Y \\space are \\space identically \\space distributed$\n\n$ $\n\n\n**Characteristic function of Standard Normal distribution**\n\n\nLet's compute $\\varphi_X(t)$ for a random variable $X \\tilde{}N(0,1)$.\n\n$\\varphi_X(t) = \\int_{\\mathbb{R}} e^{itx} f_X(x)dx = \\int_{-\\infty}^{\\infty} e^{itx} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx =\n\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2 - 2itx}{2}}dx$\n\nWe subtract and add $t^2$ in the numerator of the exponent.\n\n$\\varphi_X(t) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2 - 2itx - t^2 + t^2}{2}}dx = \n\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2 - 2itx - t^2}{2}}e^{-\\frac{t^2}{2}}dx =\ne^{-\\frac{t^2}{2}}\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x- it)^2}{2}}dx$\n\nWe call $y = x -it$, we derive that $\\frac{dy}{dx} = 1$\n\nThen, $\\varphi_X(t) = e^{-\\frac{t^2}{2}}\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{y^2}{2}}dy$\n\nThe second term is the integral on $\\mathbb{R}$ of a Standard Normal Distribution and therefore equal to $1$\n\n$\\varphi_X(t) = e^{-\\frac{t^2}{2}}$ for $X \\tilde{} N(0,1)$\n\n\n$ $\n\n**Derivation of the Central Limit Theorem**\n\nLet's assume we have $n$ independent, identically distributed random variables \n${\\{X_1,X_2,\\dots,X_n\\}}$ and $X_i \\tilde{} D(\\mu,\\sigma^2)$ with $D$ being an unknown distribution.\n\n\n$\\sum_{i=1}^{n}X_i$ will have mean $n\\mu$ and variance $n\\sigma^2$\n\nWe consider the random variable\n\n$Z_n =\\frac{\\sum_{i=1}^{n}X_i - n\\mu}{\\sqrt{n\\sigma^2}} = \\frac{\\sum_{i=1}^{n}(X_i - \\mu)}{\\sqrt{n\\sigma^2}} =  \\frac{\\sum_{i=1}^{n}Y_i}{\\sqrt{n}}$\n\nwith the $n$ random variables $Y_i = \\frac{X_i - \\mu}{\\sigma}$ with $0$ mean and variance $1$\n\nThe characteristic function of $Z_n$, $\\varphi_{Z_n}$ is given by $\\varphi_{Z_n}(t) = E\\big[e^{iZ_nt}\\big] = E\\Bigg[e^{\\frac{\\sum_{j=1}^{n}Y_j}{\\sqrt{n}}it}\\Bigg] =\nE\\Bigg[\\prod_{j=1}^{n} e^{\\frac{iY_jt}{\\sqrt{n}}}\\Bigg]$\n\nSince $X_i$ are independent, so are $Y_i$ and therefore are $e^{\\frac{iY_jt}{\\sqrt{n}}}$, from this follows\n\n$\\varphi_{Z_n}(t) = \\prod_{j=1}^{n}E\\Bigg[e^{\\frac{iY_jt}{\\sqrt{n}}}\\Bigg] = \\prod_{j=1}^{n}\\varphi_{Y_j}\\big(\\frac{t}{\\sqrt{n}}\\big)$\n\nSince $X_i$ are identically distributed, so are $Y_i$ and therefore their characteristic functions are equal.\n\n$\\varphi_{Z_n}(t) = \\bigg[\\varphi_{Y_1}\\bigg(\\frac{t}{\\sqrt{n}}\\bigg)\\bigg]^n$\n\nBy Taylor's Theorem $\\varphi_{Y_1}\\big(\\frac{t}{\\sqrt{n}}\\big) = 1 - \\frac{t^2}{2n} + o\\big(\\frac{t^2}{n}\\big), \\space \\frac{t}{\\sqrt{n}} \\to 0$\n\n\nSince $e^x= \\lim_{n\\to \\infty}(1 + \\frac{x}{n})^n$ and $\\frac{t}{\\sqrt{n}} \\to 0$ for $n \\to \\infty$\n\nWe have that $\\varphi_{Z_n}(t) = \\bigg[1 - \\frac{t^2}{2n} + o\\big(\\frac{t^2}{n}\\big)\\bigg]^n$, and for $n \\to \\infty, \\space \\varphi_{Z_n}(t) = e^{-\\frac{t^2}{2}}$\n\nTherefore for $n \\to \\infty, \\space Z_n \\tilde{} N(0,1)$\n\n$(\\sum_{i=1}^{n}X_i) \\tilde{} N(n\\mu,n\\sigma^2)$\n\nAnd the sample mean $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_i$ will approach $N(\\mu,\\frac{\\sigma^2}{n})$\n","b89bc681":"# Lesson 1 - Linear Regression\n\n## Table of Contents\n### 1. [The Problem](#The-Problem)\n### 2. [The Solution](#The-Solution)\n### 3. [Gradient Descent](#Gradient-Descent)\n### 4. [Performance Evaluation](#Performance-Evaluation)\n### 5. [Polynomial Regression](#Polynomial-Regression)\n### 6. [Regularisation](#Regularisation)\n\n### Appendix\n   1. [Derivation of the One Shot Solution](#Derivation-of-the-One-Shot-Solution)\n   2. [Central Limit Theorem](#Central-Limit-Theorem) \n   3. [p-value](#p-value)\n   4. [t-student distribution](#t-student-distribution)\n","a7fe2c99":"Now that we know that the regression is significant, we ask ourselves how much information it conveys.\n\nTo do that we compute the $R^2$ or coefficient of determination, which represent the portion of the total variance that is explained by the linear regression and tells us how well our model fits the data in a scale from $0$ (doesn't fit at all) to $1$ (the regressor can predict every point)\n\nWe define it as $R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1-\\frac{SS_{res}}{SS_{tot}} $, where $SS_{tot} = \\sum_{i=1}^{n}(y_i-\\bar{y})^2 \\propto Var(y)$ is the total sum of squares,\n\n$SS_{reg} = \\sum_{i=1}^{n}(\\hat{y_i}-\\bar{y})^2$ is the sum of squares explained by our model,\n\n$SS_{res} = \\sum_{i=1}^{n}e_i^2$ is the residual sum of squares.","10ccd0bb":"We may find $\\vec{x}^*$ that minimises $f(\\vec{x})$ by following these steps:\n\n1. Initialise $\\vec{x}$ with random values\n2. Update $\\vec{x}: \\space  \\space \\vec{x_k} := \\vec{x_{k-1}}-\\alpha\\nabla f(\\vec{x_{k-1}})$\n3. Check stop condition, if it is met, $\\vec{x_k}$ is the solution, otherwise go back to stage 2\n\nThe stop condition might be a threshold on the value of $f(\\vec{x})$, on the difference between two subsequent iterations $f(\\vec{x}_k) - f(\\vec{x_{k-1}})$, or a limit on the maximum number of iterations.\n\n$\\alpha$ is a constant with the following constraint $ \\alpha > 0 $, known as the Learning Rate, it controls how much $\\vec{x}$ is changed at each iteration, with a value too low it may take a long time to converge to the solution, with a value to high the algorithm may become erratic when near the optimal solution and \"jump\" over it.\n\nOther algorithms change $\\alpha$ at each step, through various means such as computing or approximating higher order derivatives.\n\n"}}