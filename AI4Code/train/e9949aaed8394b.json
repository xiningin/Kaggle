{"cell_type":{"ace348a0":"code","aad82dfe":"code","b4c15845":"code","22d99b8f":"code","426cfc7d":"code","6f0a98e8":"code","bcbb36cd":"code","8a8f9aff":"code","6bc19bbf":"code","c810d536":"code","83036cb0":"code","8fc12ba6":"code","65700bd3":"code","77b50e55":"code","e9ab9de8":"code","baaeb405":"code","9bd86fda":"code","d42f6b7c":"code","7af94c43":"code","06ecf88a":"code","cec5a3ae":"code","3156c90c":"code","196b08d0":"code","b4a7d56e":"code","e074c23c":"code","abb7c0a8":"code","6e620fa8":"code","e2e029cd":"code","96932300":"code","1079b450":"code","cde0a2dc":"code","3435052e":"code","c8a88527":"code","4951fb21":"code","0525dece":"code","f91c79ee":"code","538c5982":"code","1581602c":"code","2b86a705":"code","f7791c41":"code","6b9a9b44":"code","aa9d7740":"code","2864e3a6":"code","aa79a0ac":"code","be8cefe7":"code","21a74bf2":"code","b4f832c2":"code","f8902b12":"code","dc60f5f3":"code","d2484cb3":"code","723a149f":"code","eeda24b8":"code","5e04d2b9":"code","786912d6":"code","e5c24939":"markdown","1c1384ae":"markdown","670e336b":"markdown","ae765cd1":"markdown","8559187e":"markdown","741d86f4":"markdown","a35fb694":"markdown","f5a548a5":"markdown","b2e49976":"markdown","8d1d3ade":"markdown","a6ecac7a":"markdown","3d1e01cc":"markdown","58764569":"markdown","53150d32":"markdown","c57f9a16":"markdown","2f8344ef":"markdown","090ff74e":"markdown","e0254a6a":"markdown","343d0d18":"markdown","b8331db7":"markdown","92283514":"markdown","aa555b0f":"markdown","cb218f7a":"markdown","df7de0d9":"markdown","027a1187":"markdown","c633c46c":"markdown","3a9bf5be":"markdown","4ee023bd":"markdown","78f2269d":"markdown","d9f73221":"markdown","98167a27":"markdown","6988c6f5":"markdown","874cf8d6":"markdown","06582a64":"markdown","8c3a9dd4":"markdown","48b21a22":"markdown","eee39858":"markdown","95baa24e":"markdown","41a94013":"markdown"},"source":{"ace348a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno #visualize the distribution of NaN values. \nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt #visualization","aad82dfe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b4c15845":"train_df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain_df.head()","22d99b8f":"test_df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_df.head()","426cfc7d":"plt.figure(figsize=(20,5))\nsns.distplot(train_df.SalePrice, color=\"tomato\")\nplt.title(\"Target distribution in train\")\nplt.ylabel(\"Density\");","6f0a98e8":"train_df.shape","bcbb36cd":"isna_train = train_df.isnull().sum().sort_values(ascending=False)\nisna_test = test_df.isnull().sum().sort_values(ascending=False)","8a8f9aff":"plt.subplot(2,1,1)\nplt_1=isna_train[:20].plot(kind='bar')\nplt.ylabel('Train Data')\nplt.subplot(2,1,2)\nisna_test[:20].plot(kind='bar')\nplt.ylabel('Test Data')\nplt.xlabel('Number of features which are NaNs')","6bc19bbf":"(train_df.isnull().sum()\/len(train_df)).sort_values(ascending=False)[:20]","c810d536":"missing_percentage=(train_df.isnull().sum()\/len(train_df)).sort_values(ascending=False)[:20]\nprint(missing_percentage.index[:5])","83036cb0":"missing_percentage","8fc12ba6":"train_df=train_df.drop(missing_percentage.index[:5],1)\ntest_df=test_df.drop(missing_percentage.index[:5],1)","65700bd3":"missing_percentage.index[5:]","77b50e55":"#Finding the columns whether they are categorical or numerical\ncols = train_df[missing_percentage.index[5:]].columns\nnum_cols = train_df[missing_percentage.index[5:]]._get_numeric_data().columns\nprint(\"Numerical Columns\",num_cols)\ncat_cols=list(set(cols) - set(num_cols))\nprint(\"Categorical Columns:\",cat_cols)","e9ab9de8":"import matplotlib.pyplot as py\nplt.figure(figsize=[12,10])\nplt.subplot(331)\nsns.distplot(train_df['LotFrontage'].dropna().values)\nplt.subplot(332)\nsns.distplot(train_df['GarageYrBlt'].dropna().values)\nplt.subplot(333)\nsns.distplot(train_df['MasVnrArea'].dropna().values)\npy.suptitle(\"Distribution of data before Filling NA'S\")","baaeb405":"train_df['LotFrontage']=train_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntrain_df['GarageYrBlt']=train_df.groupby('Neighborhood')['GarageYrBlt'].transform(lambda x: x.fillna(x.median()))\ntrain_df['MasVnrArea']=train_df.groupby('Neighborhood')['MasVnrArea'].transform(lambda x: x.fillna(x.median()))","9bd86fda":"import matplotlib.pyplot as py\nplt.figure(figsize=[12,10])\nplt.subplot(331)\nsns.distplot(train_df['LotFrontage'].values)\nplt.subplot(332)\nsns.distplot(train_df['GarageYrBlt'].values)\nplt.subplot(333)\nsns.distplot(train_df['MasVnrArea'].values)\npy.suptitle(\"Distribution of data after Filling NA'S\")","d42f6b7c":"test_df['LotFrontage']=test_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntest_df['GarageYrBlt']=test_df.groupby('Neighborhood')['GarageYrBlt'].transform(lambda x: x.fillna(x.median()))\ntest_df['MasVnrArea']=test_df.groupby('Neighborhood')['MasVnrArea'].transform(lambda x: x.fillna(x.median()))","7af94c43":"for column in cat_cols:\n    train_df[column]=train_df.groupby('Neighborhood')[column].transform(lambda x: x.fillna(x.mode()))\n    test_df[column]=test_df.groupby('Neighborhood')[column].transform(lambda x: x.fillna(x.mode()))","06ecf88a":"num_cols = train_df._get_numeric_data().columns\nprint(\"Numerical Columns\",num_cols)\ncat_cols=list(set(cols) - set(num_cols))\nprint(\"Categorical Columns:\",cat_cols)","cec5a3ae":"Neighbour=train_df.groupby(['Neighborhood','YearBuilt'])['SalePrice']\nNeighbour=Neighbour.describe()['mean'].to_frame()\nNeighbour=Neighbour.reset_index(level=[0,1])\nNeighbour=Neighbour.groupby('Neighborhood')","3156c90c":"Neighbour_index=train_df['Neighborhood'].unique()\nfig = plt.figure(figsize=(50,12))\nfig.suptitle('Yearwise Trend of each Neighborhood')\nfor num in range(1,25):\n    temp=Neighbour.get_group(Neighbour_index[num])\n    ax = fig.add_subplot(5,5,num)\n    ax.plot(temp['YearBuilt'], temp['mean'])\n    ax.set_title(temp['Neighborhood'].unique())\n    ","196b08d0":"#Finding the columns whether they are categorical or numerical\ncols = train_df.columns\nnum_cols = train_df._get_numeric_data().columns\nprint(\"Numerical Columns\",num_cols)\ncat_cols=list(set(cols) - set(num_cols))\nprint(\"Categorical Columns:\",cat_cols)","b4a7d56e":"from sklearn.preprocessing import LabelEncoder\nfor i in cat_cols:\n    train_df[i]=LabelEncoder().fit_transform(train_df[i].astype(str)) \n    test_df[i]=LabelEncoder().fit_transform(test_df[i].astype(str)) ","e074c23c":"fig,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(train_df.corr(),ax=ax,annot= False,linewidth= 0.02,linecolor='black',fmt='.2f',cmap = 'Blues_r')\nplt.show()","abb7c0a8":"#price range correlation\ncorr=train_df.corr()\ncorr=corr.sort_values(by=[\"SalePrice\"],ascending=False).iloc[0].sort_values(ascending=False)\nplt.figure(figsize=(15,20))\nsns.barplot(x=corr.values, y=corr.index.values);\nplt.title(\"Correlation Plot\")\n","6e620fa8":"#Forming a new dataset that has columns having more than 0.15 correlation\nindex=[]\nTrain=pd.DataFrame()\nY=train_df['SalePrice']\nfor i in range(0,len(corr)):\n    if corr[i] > 0.15 and corr.index[i]!='SalePrice':\n        index.append(corr.index[i])\nX=train_df[index]","e2e029cd":"X['cond*qual'] = (train_df['OverallCond'] * train_df['OverallQual']) \/ 100.0\nX['home_age_when_sold'] = train_df['YrSold'] - train_df['YearBuilt']\nX['garage_age_when_sold'] = train_df['YrSold'] - train_df['GarageYrBlt']\nX['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF'] \nX['total_porch_area'] = train_df['WoodDeckSF'] + train_df['OpenPorchSF'] + train_df['EnclosedPorch'] + train_df['3SsnPorch'] + train_df['ScreenPorch'] \nX['Totalsqrfootage'] = (train_df['BsmtFinSF1'] + train_df['BsmtFinSF2'] +train_df['1stFlrSF'] + train_df['2ndFlrSF'])\nX['Total_Bathrooms'] = (train_df['FullBath'] + (0.5 * train_df['HalfBath']) +train_df['BsmtFullBath'] + (0.5 * train_df['BsmtHalfBath']))","96932300":"test_df['cond*qual'] = (test_df['OverallCond'] * test_df['OverallQual']) \/ 100.0\ntest_df['home_age_when_sold'] = test_df['YrSold'] - test_df['YearBuilt']\ntest_df['garage_age_when_sold'] =test_df['YrSold'] - test_df['GarageYrBlt']\ntest_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF'] \ntest_df['total_porch_area'] = test_df['WoodDeckSF'] +test_df['OpenPorchSF'] + test_df['EnclosedPorch'] + test_df['3SsnPorch'] + test_df['ScreenPorch'] \ntest_df['Totalsqrfootage'] = (test_df['BsmtFinSF1'] + test_df['BsmtFinSF2'] +test_df['1stFlrSF'] + test_df['2ndFlrSF'])\ntest_df['Total_Bathrooms'] = (test_df['FullBath'] + (0.5 * test_df['HalfBath']) +test_df['BsmtFullBath'] + (0.5 * test_df['BsmtHalfBath']))","1079b450":"Old_Cols=['OverallCond','OverallQual','YrSold','YearBuilt','YrSold','GarageYrBlt','TotalBsmtSF','1stFlrSF','2ndFlrSF','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','BsmtFinSF1','BsmtFinSF2','1stFlrSF','2ndFlrSF','FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']","cde0a2dc":"Final_cols=[]\nfor i in X.columns:\n    if i not in Old_Cols and i!='SalePrice':\n        Final_cols.append(i)\nX=X[Final_cols]","3435052e":"X.columns","c8a88527":"fig = plt.figure(figsize=(20,16))\n\nplt.subplot(2, 2, 1)\nplt.scatter(X['home_age_when_sold'],Y)\nplt.title(\"Home Age Vs SalePrice \")\nplt.ylabel(\"SalePrice\")\nplt.xlabel(\"Home Age\")\n\nplt.subplot(2, 2, 2)\nplt.scatter(X['Total_Bathrooms'],Y)\nplt.title(\"Total_Bathrooms Vs SalePrice \")\nplt.ylabel(\"SalePrice\")\nplt.xlabel(\"Total_Bathrooms\")\n\nplt.subplot(2, 2, 3)\nplt.scatter(X['TotalSF'],Y)\nplt.title(\"TotalSF Vs SalePrice \")\nplt.ylabel(\"SalePrice\")\nplt.xlabel('TotalSF')\n\nplt.subplot(2, 2, 4)\nplt.scatter(X[ 'cond*qual'],Y)\nplt.title(\"House Condition Vs SalePrice \")\nplt.ylabel(\"SalePrice\")\nplt.xlabel('cond*qual')\n\nplt.show()","4951fb21":"X.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","0525dece":"temp=pd.DataFrame()\ntemp=X\ntemp['SalePrice']=Y","f91c79ee":"for i in range(0, len(temp.columns), 5):\n    sns.pairplot(data=temp,\n                x_vars=temp.columns[i:i+5],\n                y_vars=['SalePrice'])","538c5982":"#Deleting outliers\ntemp = temp.drop(temp[(temp['LotArea']>100000) ].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(temp['LotArea'], temp['SalePrice'])\nplt.ylabel('LotArea', fontsize=13)\nplt.xlabel('LotArea', fontsize=13)\nplt.show()","1581602c":"X=temp.loc[:, temp.columns != 'SalePrice']\nY=temp['SalePrice']","2b86a705":"test_df=test_df[Final_cols]","f7791c41":"X.isnull().sum()","6b9a9b44":"temp=X\ntemp[\"SalePrice\"]=Y\n#price range correlation\nfig,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(temp.corr(),ax=ax,annot= False,linewidth= 0.02,linecolor='black',fmt='.2f')\nplt.show()","aa9d7740":"Final_cols=[]\nfor i in X.columns:\n    if i not in Old_Cols and i!='SalePrice':\n        Final_cols.append(i)\nX=X[Final_cols]","2864e3a6":"X.columns","aa79a0ac":"test=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","be8cefe7":"test_df.fillna(test_df.mean(), inplace=True)","21a74bf2":"import xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(X,Y)","b4f832c2":"prediction = model_xgb.predict(test_df)\npred_xgb = pd.DataFrame()\npred_xgb['Id']=test['Id']\npred_xgb['SalePrice'] = prediction\npred_xgb.to_csv(\"..\/working\/submission_xgb.csv\", index = False)","f8902b12":"from sklearn.ensemble import GradientBoostingRegressor\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nGBoost.fit(X,Y)","dc60f5f3":"prediction = GBoost.predict(test_df)\npred_GB = pd.DataFrame()\npred_GB['Id']=test['Id']\npred_GB['SalePrice'] = prediction\npred_GB.to_csv(\"..\/working\/submission_GB.csv\", index = False)","d2484cb3":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X,Y)","723a149f":"prediction = model_lgb.predict(test_df)\npred_LGB = pd.DataFrame()\npred_LGB['Id']=test['Id']\npred_LGB['SalePrice'] = prediction\npred_LGB.to_csv(\"..\/working\/submission_LGB.csv\", index = False)","eeda24b8":"from catboost import CatBoostRegressor\ncb_model = CatBoostRegressor(iterations=700,\n                             learning_rate=0.02,\n                             depth=12,\n                             eval_metric='RMSE',\n                             random_seed = 23,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 75,\n                             od_wait=100)\ncb_model.fit(X, Y)","5e04d2b9":"prediction = cb_model.predict(test_df)\npred_CB = pd.DataFrame()\npred_CB['Id']=test['Id']\npred_CB['SalePrice'] = prediction\npred_CB.to_csv(\"..\/working\/submission_CB.csv\", index = False)","786912d6":"pred_ensemble = pd.DataFrame()\npred_ensemble['Id']=test['Id']\npred_ensemble['SalePrice'] =( 0.6* pred_xgb['SalePrice'] +0.1* pred_CB['SalePrice']+0.2*pred_GB['SalePrice'] +0.1*pred_LGB['SalePrice'])\npred_ensemble.to_csv(\"..\/working\/submission_ensemble.csv\", index = False)","e5c24939":"**Understanding the distribution of Missing Data**","1c1384ae":"**Observation**\n\n1)The newly formed Data has more meaningful columns compared to the original dataset\n\n2)**Home age when sold** and **Garage age when sold** having negative correlation with SalePrice make sense as we know, Price of building decreases as it ages.","670e336b":"**Forming New Features**\n\nSome of the features represent the same so instead of having them individually we can combine them and get new features\n\nThanks for the discussion https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/discussion\/106618#latest-616788","ae765cd1":"**Observation**\n>  From this plot, It is clear that missing data is distributed in equally in both train and test dataset. So we have to figure out a general method to handle the missing values from both the dataset\n","8559187e":"## About the problem\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n","741d86f4":"**Key Take aways**\n\n1)Creative feature engineering \n\n2)Advanced regression techniques like random forest and gradient boosting**(Which I am currently working)**","a35fb694":"**CatBoost Regressor**","f5a548a5":"**Neighbourhood wise salesprice distribution**\n","b2e49976":"Finding the categorical and numerical feature","8d1d3ade":"## Exploratory Data Analysis","a6ecac7a":"Usually Categorical Variables are imputed with mode but it won't make sense(Houses in Newyork has different features compared to San Francisco) in all cases so in order to make them loaclized based on Neighborhood and we can impute the data","3d1e01cc":"## Modelling","58764569":"**Observations:**\n\nFrom this graph we can see outlier's are in every column's but outlier removal is not always good. So, **LotArea**\nhas a very big outlier I am removing that alone","53150d32":"**Gradient Boosting**","c57f9a16":"Finding whether the columns with missing has any pattern or wether they are normally distributed. ","2f8344ef":"#### Please upvote if you like this kernel","090ff74e":"**Finding the outliers**","e0254a6a":"**XG Regressor**","343d0d18":"**Correlation plot for new dataset formed**","b8331db7":"**Imputing the missing values**","92283514":"**Just ensuring the distribution of data before and after filling the missing values remain's the same** It's always good to have distrivution same before and after imputing the missing values","aa555b0f":"**Interesting insights**","cb218f7a":"## Feature selection","df7de0d9":"**Light GB**","027a1187":"Dropping the columns that have more than 30% of missing values","c633c46c":"## Ensembling Weighted average\n","3a9bf5be":"Any suggestions please let me know through your comments","4ee023bd":"**Numerical values distribution**","78f2269d":"**Reading the Files**","d9f73221":"**Categorical Missing value imputation**\n\n","98167a27":"## Handling the Missing Values","6988c6f5":"## Data Exploration and Analysis","874cf8d6":"**Observation**\n\nAbove Figure shows that the target variable is normally distributed","06582a64":"**Description of new features**\n\n1)**cond*qual** - representative of Overall condition and quality.\n\n2)**home_age_when_sold** - Age of Home When sold\n\n3)**garage_age_when_sold** -Age of garage when sold\n\n4)**TotalSF** - Total Square Foot\n\n5)**total_porch_area** - Total Porch Area\n\n6)**Totalsqrfootage** - Total Square Foot\n\n7)**Total_Bathrooms** - Total Bathrooms\n\nNow, It makes sense right? \nThese are certain things we would usually consider for when we are planning to buy a house\n\n","8c3a9dd4":"**Loading the necessary Packages**","48b21a22":"**Label Encoding All the Categorical variables**","eee39858":"**I will be working extensively on the coming days on the modelling part to get good results**","95baa24e":"**Distribution of Data**","41a94013":"**Observations**\n\n1)LotFrontage is normally distributed hence we can impute it with mean\n\n2)GarageYrBlt is skewed so we can either fill it with median\n\n3)MasVnrArea is skewed so we can either fill it with median\n\nTo get better results we can localize this imputations generally houses structure will be common for a particular location so I am localizing this data based on Neihbourhood"}}