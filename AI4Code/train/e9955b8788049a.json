{"cell_type":{"340a5d30":"code","c5f7178b":"code","b762a35a":"code","4e56c477":"code","676d7ae3":"code","274dce4c":"code","0d7a1041":"code","2864bcc6":"code","b74ce46a":"code","13e805ae":"code","db34354b":"code","5645769f":"code","07a3e0a0":"code","4687b284":"code","001ffdfd":"code","2bbc7897":"code","59d3587e":"code","4f9a3682":"code","e0ceb4d6":"code","ae364578":"code","f6c27296":"code","d13c487b":"code","6f9871a6":"code","f5c220d3":"code","caead364":"code","74b2168b":"code","6297c7aa":"code","adb4460d":"code","60b7bc31":"code","700c1498":"code","3540ddf0":"code","413ae8b6":"code","9cb46099":"code","1318a744":"code","2878c2c1":"code","1ef1b936":"code","8c295c0b":"code","447137b8":"code","4a9bd596":"code","953a72ba":"code","bb2f0f72":"code","e8c0f61b":"code","48c6d0d1":"code","86d27989":"code","512a1704":"code","3003687b":"code","61e52784":"code","51e530a3":"code","c1dc2aa2":"code","14f2a4e3":"code","b1da52c9":"code","f985a643":"code","25c8ffc6":"code","881a8489":"code","842ff14c":"code","47d00eef":"code","8ace0c2a":"code","a4d5de47":"code","0d9899ff":"code","698b4d7a":"code","5b8eaa4a":"code","18e3749c":"code","0fb3e6d3":"code","88e75f19":"code","2a89ee92":"code","8022f3cd":"code","0242e752":"code","f0e36b45":"code","4c8b7ba9":"code","952f6856":"markdown","cc9be2ca":"markdown","18d2b93b":"markdown","4eed9b90":"markdown","d41e34ca":"markdown","2d988225":"markdown","67737bd6":"markdown","dc03a7ae":"markdown","baeaf932":"markdown","dfca40a2":"markdown","99c5055b":"markdown","a2dea3ef":"markdown","5d322eab":"markdown","9e583ac4":"markdown","5e66fa73":"markdown","b6eb498f":"markdown","66b6f33f":"markdown","fceb1e32":"markdown","c39a2662":"markdown","e4ce166a":"markdown","e11b80de":"markdown","50765fac":"markdown","60742476":"markdown","8930752b":"markdown","8fd0b7d3":"markdown","d2f9a4d2":"markdown","d798df64":"markdown","afe9f228":"markdown","f05d4ddb":"markdown","854aa795":"markdown","86124650":"markdown","66830792":"markdown","ffb41350":"markdown","15ee4732":"markdown","d81c9625":"markdown","2ce0b46c":"markdown","7a41f29d":"markdown","40a1311e":"markdown","48b220a9":"markdown","7732196c":"markdown","9eabeb25":"markdown","e1a9cdaf":"markdown"},"source":{"340a5d30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport math\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\nfrom scipy.stats import skew, boxcox_normmax\nfrom scipy.special import boxcox1p\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500","c5f7178b":"raw_train_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nraw_test_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\nprint(f\"Train data: {raw_train_df.shape}\")\nprint(f\"Test data: {raw_test_df.shape}\")","b762a35a":"raw_train_df.sample(10)","4e56c477":"raw_train_df.describe()","676d7ae3":"def custom_heatmap(df, include=[], exclude=[]):\n    if include:\n        df = df[include]\n    elif exclude:\n        df = df.drop(exclude, axis=1)\n    fig, ax = plt.subplots(figsize=(30,20)) \n    correlation = df.corr()\n    correlation[abs(correlation) < 0.1] = 0\n    mask = np.zeros_like(correlation, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    sns.heatmap(correlation, ax=ax, cmap=sns.color_palette(\"RdBu_r\", 100), mask=mask, annot=True)\n    plt.show()\n    \n    \ndef custom_scatterplot(df, features=[], target=None, numcols=3):\n    style.use('seaborn')\n    for i, feature in enumerate(features):\n        fig = plt.figure(i\/\/numcols,figsize=(25, 5))\n        ax = plt.subplot(1, numcols, (i%numcols)+1)\n        ax.scatter(df[feature], target, alpha=0.2)\n        ax.set_title(feature)\n        ax.set_ylabel(target.name)  \n    plt.tight_layout()\n    plt.show()\n    \n    \ndef custom_distributionplot(df, columns, numcols=3):\n    style.use('seaborn')\n    for i, col in enumerate(columns):\n        fig = plt.figure(i\/\/numcols,figsize=(25, 5))\n        ax = plt.subplot(1, numcols, (i%numcols)+1)\n        sns.distplot(df[col], ax=ax)\n        ax.set_title(col)\n    plt.tight_layout()\n    plt.show()  \n    \n    \ndef custom_boxplot(df, features=[], target=None, numcols=4):\n    style.use('seaborn')\n    for i, feature in enumerate(features):\n        fig = plt.figure(i\/\/numcols,figsize=(25, 5))\n        ax = plt.subplot(1, numcols, (i%numcols)+1)\n        data = pd.concat([df[feature], target], axis=1)\n        sns.boxplot(x=df[feature], y=target, data=data, ax=ax)\n        ax.set_title(feature)\n        plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \ndef custom_countplot(df, features=[], numcols=4):\n    style.use('seaborn')\n    for i, feature in enumerate(features):\n        fig = plt.figure(i\/\/numcols,figsize=(25, 5))\n        ax = plt.subplot(1, numcols, (i%numcols)+1)\n        sns.countplot(df[feature], ax=ax)\n        ax.set_title(feature)\n        plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    \ndef df_nan_percentage(df):\n    total = df.isnull().sum()[ df.isnull().sum() > 0 ].sort_values(ascending=False)\n    percent = (total\/df.shape[0]) * 100\n    new_df = pd.DataFrame({\n        \"Total\": total,\n        \"Percentage\": percent\n    }, index=total.index)\n    return new_df\n\n\ndef BooleanEncoding(df, features=[], threshold=0):\n    names = []\n    new_features = []\n    \n    for feature in features:\n        names.append(f\"has{feature}\")\n        new_features.append(df[feature].map(lambda x: 1 if x > threshold else 0))\n        \n    new_df = pd.concat(new_features, axis=1)\n    new_df.columns = names\n    return new_df\n\n\ndef check_outliers(df, method=\"z-score\"):\n    if method == \"z-score\":\n        z = np.abs(stats.zscore(np.log1p(df)))\n        outliers = np.where(z>3)[0]\n        \n    elif method == \"iqr-score\":\n        Q1 = df.quantile(0.25)\n        Q3 = df.quantile(0.75)\n        IQR = Q3 - Q1\n        outliers = df[((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)].index\n    \n    return outliers\n\n\ndef check_overfitting(df, threshold=0.05):\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) > (1 - threshold):\n            overfit.append(i)\n    return overfit\n\n    \ndef correct_skewness(df, features):\n    for feat in features:\n        try:\n            df.loc[:, feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n        except Exception as e:\n            print(e)\n    return df\n\n\ndef adjusted_r2(r2,n,k):\n    return r2-(k-1)\/(n-k)*(1-r2)\n","274dce4c":"raw_train_df","0d7a1041":"target_features = [\"Id\", \"SalePrice\"]\n\narea_features = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\",\n                 \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\"]\n\nnominal_features = [\"MSSubClass\",  \"MSZoning\", \"Street\", \"Alley\", \"LotShape\", \"LandContour\", \"LotConfig\", \"LandSlope\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\",\n                    \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"Electrical\", \"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\",\n                    \"Functional\", \"GarageType\", \"GarageFinish\", \"PavedDrive\", \"MiscFeature\", \"MoSold\", \"YrSold\", \"SaleType\", \"SaleCondition\"]\n\nordinal_features = [\"Utilities\", \"OverallQual\", \"OverallCond\", \"YearBuilt\", \"YearRemodAdd\", \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n                    \"HeatingQC\", \"CentralAir\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"KitchenQual\", \"TotRmsAbvGrd\", \"Fireplaces\", \"FireplaceQu\", \"GarageYrBlt\", \"GarageCars\", \"GarageQual\", \"GarageCond\", \n                    \"PoolQC\", \"Fence\", \"MiscVal\"]\n\nprint(f\"Total features: {len(area_features)+ len(nominal_features)+len(ordinal_features)}\")","2864bcc6":"custom_scatterplot(raw_train_df, features=area_features, target=raw_train_df[\"SalePrice\"], numcols=4)","b74ce46a":"raw_train_df[area_features].describe()","13e805ae":"custom_boxplot(raw_train_df[nominal_features], features=nominal_features, target=raw_train_df[\"SalePrice\"])","db34354b":"custom_countplot(raw_train_df[nominal_features], features=nominal_features)","5645769f":"custom_boxplot(raw_train_df[ordinal_features], features=ordinal_features, target=raw_train_df[\"SalePrice\"])","07a3e0a0":"custom_countplot(raw_train_df[ordinal_features], features=ordinal_features)","4687b284":"train_df = raw_train_df.copy()","001ffdfd":"custom_distributionplot(train_df, columns=[\"SalePrice\"])","2bbc7897":"train_df[\"SalePrice\"] = np.log1p(raw_train_df[\"SalePrice\"])","59d3587e":"custom_distributionplot(train_df, columns=[\"SalePrice\"])","4f9a3682":"outliers = [30, 88, 462, 631, 1322]","e0ceb4d6":"train_df = train_df[\n    (train_df[\"SalePrice\"] >= train_df[\"SalePrice\"].quantile(0.01))&\n    (train_df[\"SalePrice\"] <= train_df[\"SalePrice\"].quantile(0.99))&\n    ~(train_df[\"Id\"].isin(outliers))\n]","ae364578":"custom_distributionplot(train_df, columns=[\"SalePrice\"])","f6c27296":"df_nan_percentage(train_df)","d13c487b":"train_df[[\"GarageYrBlt\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]] = train_df[[\"GarageYrBlt\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]].fillna(\"None\")\ntrain_df[[\"BsmtFinType2\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtCond\", \"BsmtQual\"]] = train_df[[\"BsmtFinType2\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtCond\", \"BsmtQual\"]].fillna(\"None\")\ntrain_df[[\"PoolQC\", \"Fence\", \"FireplaceQu\", \"Alley\", \"MiscFeature\"]] = train_df[[\"PoolQC\", \"Fence\", \"FireplaceQu\", \"Alley\", \"MiscFeature\"]].fillna(\"None\")\n\ntrain_df[\"LotFrontage\"] = train_df[\"LotFrontage\"].fillna(train_df[\"LotFrontage\"].median())\ntrain_df[\"MasVnrArea\"] = train_df[\"MasVnrArea\"].fillna(train_df[\"MasVnrArea\"].median())\n\ntrain_df[\"MasVnrType\"] = train_df[\"MasVnrType\"].fillna(train_df[\"MasVnrType\"].mode()[0])\ntrain_df[\"Electrical\"] = train_df[\"Electrical\"].fillna(train_df[\"Electrical\"].mode()[0])","6f9871a6":"restore_train_df = train_df.copy()\ndf_nan_percentage(train_df)","f5c220d3":"# create checkpoint\ntrain_df = restore_train_df.copy()\ntarget_df = train_df[[\"Id\", \"SalePrice\"]]","caead364":"# These features should be categorical and not numerical\ntrain_df['MSSubClass'] = train_df['MSSubClass'].apply(str)\ntrain_df['YrSold'] = train_df['YrSold'].astype(str)\ntrain_df['MoSold'] = train_df['MoSold'].astype(str)","74b2168b":"def create_new_features(df):\n    mapping = {\n        \"Ex\": 10,\n        \"Gd\": 8,\n        \"TA\": 6,\n        \"Fa\": 4,\n        \"Po\": 2,\n        \"None\": 0\n    }\n\n    TotalPorchArea = df[[\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"WoodDeckSF\"]].sum(axis=1)\n    TotalPorchArea.name = \"TotalPorchArea\"\n\n    TerrainArea = df[\"LotArea\"] - ( TotalPorchArea + df[[\"LotFrontage\", \"GrLivArea\", \"TotalBsmtSF\", \"GarageArea\", \"PoolArea\"]].sum(axis=1))\n    TerrainArea.name = \"TerrainArea\"\n    \n    TotalBathrooms = df[[\"FullBath\", \"BsmtFullBath\"]].sum(axis=1) + 0.5*df[[\"HalfBath\", \"BsmtHalfBath\"]].sum(axis=1)\n    TotalBathrooms.name = \"TotalBathrooms\"\n    \n    CustomQual = df[[\"ExterQual\", \"BsmtQual\", \"GarageQual\", \"PoolQC\", \"KitchenQual\", \"FireplaceQu\", \"HeatingQC\"]].applymap(lambda x: mapping[x] if x in mapping else x)\n    CustomQual = CustomQual.sum(axis=1) \/ CustomQual[CustomQual > 0].count(axis=1)\n    CustomQual = pd.concat([df[\"OverallQual\"], CustomQual], axis=1)\n    CustomQual = CustomQual.mean(axis=1)\n    CustomQual.name = \"CustomQual\"\n    \n    \n    CustomCond = df[[\"ExterCond\", \"BsmtCond\", \"GarageCond\"]].applymap(lambda x: mapping[x] if x in mapping else x)\n    CustomCond = CustomCond.sum(axis=1) \/ CustomCond[CustomCond > 0].count(axis=1)\n    CustomCond = pd.concat([df[\"OverallCond\"], CustomCond], axis=1)\n    CustomCond = CustomCond.mean(axis=1)\n    CustomCond.name = \"CustomCond\"\n\n    new_features = pd.concat([TotalPorchArea, TerrainArea, TotalBathrooms, CustomQual, CustomCond], axis=1)\n    return new_features\n\nnew_features = create_new_features(train_df.copy())\nnew_features","6297c7aa":"def create_new_boolean_features(df):\n    mapping = {\n        \"Ex\": 10,\n        \"Gd\": 8,\n        \"TA\": 6,\n        \"Fa\": 4,\n        \"Po\": 2,\n        \"None\": 0\n    }\n\n    MaterialsUsed = pd.get_dummies(df[[\"Foundation\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\"]])\n    #MaterialsUsed = MaterialsUsed.T.groupby([s.split('_')[1] for s in MaterialsUsed.T.index.values]).sum().T\n    \n    Style = pd.get_dummies(df[[\"HouseStyle\", \"RoofStyle\", \"BldgType\", \"LotShape\", \"LotConfig\", \"GarageType\"]])\n    #Style = Style.T.groupby([s.split('_')[1] for s in Style.T.index.values]).sum().T\n\n    Surroundings = pd.get_dummies(df[[\"Alley\", \"Street\", \"LandContour\", \"LandSlope\", \"Condition1\", \"Condition2\", \"MSZoning\", \"Neighborhood\"]])\n    #Surroundings = Surroundings.T.groupby([s.split('_')[1] for s in Surroundings.T.index.values]).sum().T\n\n    Utilities = pd.get_dummies(df[[\"Electrical\", \"Heating\"]])\n    #Utilities = Utilities.T.groupby([s.split('_')[1] for s in Utilities.T.index.values]).sum().T\n\n    others = pd.get_dummies(df[[\"MSSubClass\", \"YrSold\", \"MoSold\"]])\n    \n    new_boolean_features = BooleanEncoding(df, features=[\"MasVnrArea\", \"LowQualFinSF\", \"BsmtFinSF2\", \"2ndFlrSF\", \"WoodDeckSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"Fireplaces\"], threshold=0)\n    \n    hasCentralAir = df[\"CentralAir\"].map({\"N\":0, \"Y\": 1})\n    hasCentralAir.name = \"hasCentralAir\"\n    \n    hasExFireplace = df[\"FireplaceQu\"].map(lambda x: 1 if x == \"Ex\" else 0)\n    hasExFireplace.name = \"hasExFireplace\"\n    hasExPool = df[\"PoolQC\"].map(lambda x: 1 if x == \"Ex\" else 0)\n    hasExPool.name = \"HasExPool\"\n    hasExKitchenQual = df[\"KitchenQual\"].map(lambda x: 1 if x == \"Ex\" else 0)\n    hasExKitchenQual.name = \"hasExKitchenQualQC\"\n    hasExHeatingQC = df[\"HeatingQC\"].map(lambda x: 1 if x == \"Ex\" else 0)\n    hasExHeatingQC.name = \"hasHeatingQC\"\n    hasBsmtExposure = df[\"BsmtExposure\"].map(lambda x: 0 if x == \"No\" else 1)\n    hasBsmtExposure.name = \"hasBsmtExposure\"\n    \n    new_features = pd.concat([MaterialsUsed, Style, Utilities, others, new_boolean_features, hasCentralAir, hasExFireplace, hasExPool, hasExKitchenQual, hasExHeatingQC, hasBsmtExposure], axis=1)\n    return new_features\n\nnew_boolean_features = create_new_boolean_features(train_df.copy())\nnew_boolean_features","adb4460d":"boolean_features_dist = (new_boolean_features.sum() \/ new_boolean_features.shape[0])\nboolean_features_dist","60b7bc31":"# Chossing boolean features that are balanced\nuseful_boolean_features = boolean_features_dist[\n    (boolean_features_dist > 0.3) & \n    (boolean_features_dist < 0.7)\n].index.values","700c1498":"train_df[new_features.columns] = new_features\ntrain_df","3540ddf0":"train_df.describe()","413ae8b6":"train_df.skew()","9cb46099":"skew_features = train_df.skew()[abs(train_df.skew()) > 0.5]\nskew_features","1318a744":"new_skew_df = correct_skewness(train_df.copy(), skew_features.index)\nnew_skew_df","2878c2c1":"drop_skewed_features = new_skew_df.skew()[abs(new_skew_df.skew()) > 0.5]\ntrain_df[skew_features.index] = new_skew_df[skew_features.index]\ntrain_df = train_df.drop(drop_skewed_features.index, axis=1)\ntrain_df.skew()","1ef1b936":"train_df[useful_boolean_features] = new_boolean_features[useful_boolean_features]\ntrain_df","8c295c0b":"# Remove features that have more than 90% zeros\noverfitting_features = check_overfitting(train_df, threshold=0.1)\noverfitting_features","447137b8":"# Drop Columns not needed\ntrain_df.drop(overfitting_features, axis=1, inplace=True)\ntrain_df.drop(train_df.select_dtypes(\"object\").columns, axis=1, inplace=True)\ntrain_df.drop(target_df.columns, axis=1, inplace=True)\ntrain_df.drop([\"YearRemodAdd\"], axis=1, inplace=True)","4a9bd596":"df_nan_percentage(train_df)","953a72ba":"train_df","bb2f0f72":"train_df.columns","e8c0f61b":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator, TransformerMixin):\n    def __init__(self, columns, copy=True, with_mean=True, with_std=True):\n        self.scaler = StandardScaler(copy=copy, with_mean=with_mean, with_std=with_std)\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n    def transform(self, X, y=None, copy=None):\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)","48c6d0d1":"unscaled_train_df = train_df.copy()\n\nscale_features = [\n    'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n    'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', \n    'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars','GarageArea', \n    'TotalPorchArea', 'TotalBathrooms', 'CustomQual', 'CustomCond'\n]\n\nscaler = CustomScaler(scale_features)\nscaler.fit(unscaled_train_df)","86d27989":"target_df.reset_index(drop=True, inplace=True)\nscaled_features = scaler.transform(unscaled_train_df.reset_index(drop=True))\nscaled_features","512a1704":"scaled_train_df = scaled_features.copy()\ncustom_scatterplot(scaled_train_df, features=train_df.columns, target=target_df[\"SalePrice\"])","3003687b":"custom_heatmap(pd.concat([scaled_train_df, pd.DataFrame(target_df[\"SalePrice\"])], axis=1))","61e52784":"correlation_df = pd.concat([scaled_train_df, target_df[\"SalePrice\"]], axis=1).corr()","51e530a3":"final_features = correlation_df.loc[:, \"SalePrice\"][\n    (correlation_df.loc[:, \"SalePrice\"] > 0.1) |\n    (correlation_df.loc[:, \"SalePrice\"] < -0.1)\n].index.values\n\nfinal_features = final_features[np.where(final_features != \"SalePrice\")]\nfinal_features","c1dc2aa2":"from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, StratifiedKFold, train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, LarsCV, LassoLarsCV\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline","14f2a4e3":"# model scoring and validation function\ndef cv_rmse(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef inverse_target(target, scale):\n    #expm1 is the inverse of logp1\n    return pd.DataFrame(np.expm1(target\/scale), columns=[\"SalePrice\"]).apply(np.ceil)\n\ndef plot_predictions(y_train, models, predictions, scores):\n    nrows = len(models)\n    sns.set_style(\"white\")\n    fig, axs = plt.subplots(ncols=0, nrows=nrows, figsize=(8, 7))\n    plt.subplots_adjust(top=3.5, right=2)\n    for i, model in enumerate(models, 1):\n        plt.subplot(nrows, 1, i)\n        plt.scatter(predictions[model], np.expm1(y_train))\n        plt.plot([0, 800000], [0, 800000], '--r')\n\n        plt.xlabel('{} Predictions (y_pred)'.format(model), size=15)\n        plt.ylabel('Real Values (y_train)', size=13)\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n\n        plt.title('{} Predictions vs Real Values'.format(model), size=15)\n        plt.text(0, 700000, 'Mean RMSE: {:.6f} \/ Std: {:.6f}'.format(scores[model][0], scores[model][1]), fontsize=15)\n        #ax.xaxis.grid(False)\n        sns.despine(trim=True, left=True)\n    plt.show()\n\n    \ndef predict(models, X, y):\n    predictions = {}\n    scores = {}\n    for name, model in models.items():\n    \n        model.fit(X, y.values.ravel())\n        predictions[name] = np.expm1(model.predict(X))\n    \n        score = cv_rmse(model, X=X, y=y.values.ravel())\n        scores[name] = score\n        \n    return predictions,scores\n","b1da52c9":"ids = target_df[\"Id\"]\nX_train = scaled_train_df.copy()[final_features]\ny_train = target_df[\"SalePrice\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=0.8)\nprint(f\"X train: {X_train.shape}\")\nprint(f\"y train: {y_train.shape}\")\nprint(f\"X test: {X_test.shape}\")\nprint(f\"y test: {y_test.shape}\")","f985a643":"kfolds = KFold(n_splits=8, shuffle=True)\n\n## Ridge Model\nridge_alphas = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nridge = RidgeCV(alphas=ridge_alphas, cv=kfolds)\n\n## Lasso Model\nlasso_alphas = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\nlasso = LassoCV(max_iter=1e7, alphas=lasso_alphas, cv=kfolds)\n\n## Lars Model\nlars = LarsCV(max_iter=500, max_n_alphas=500, cv=kfolds)\n\n## ElasticNet Model\nelasticnet_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nelasticnet_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet = ElasticNetCV(max_iter=1e7, alphas=elasticnet_alphas, cv=kfolds, l1_ratio=elasticnet_l1ratio)\n\n\nlinear_models = {\n    'Ridge': ridge,\n    'Lasso': lasso, \n    'ElasticNet': elasticnet,\n    'Lars': lars\n}","25c8ffc6":"predictions,scores = predict(linear_models, X_train, y_train)\nfor name,score in scores.items():\n    print(\"{} score: {:.4f} ({:.4f})\\n\".format(name, score.mean(), score.std()))","881a8489":"plot_predictions(y_train, linear_models, predictions, scores)","842ff14c":"predictions, scores = predict(linear_models, X_test, y_test)\nfor name,score in scores.items():\n    print(\"{} score: {:.4f} ({:.4f})\\n\".format(name, score.mean(), score.std()))","47d00eef":"params = {\n    'random_state': [1, 2, 3, 4, 5],\n    'bootstrap': [True, False],\n    'max_features': [\"auto\", \"sqrt\", \"log2\"], \n    'n_estimators': [120 , 140, 160, 180, 200, 220, 240, 260, 280, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400]\n}","8ace0c2a":"rf_model = RandomForestRegressor()\ngs_rf = RandomizedSearchCV(rf_model, params, cv=kfolds, scoring='neg_mean_squared_error')\ngs_rf.fit(X_train, y_train)\nprint(\"Scores:\\n\", gs_rf.best_score_)\nprint(\"Best Params:\\n\", gs_rf.best_params_)","a4d5de47":"xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\ngs_xgb = RandomizedSearchCV(xgb_model, params, cv=kfolds, scoring='neg_mean_squared_error')\ngs_xgb.fit(X_train, y_train)\nprint(\"Scores:\\n\", gs_xgb.best_score_)\nprint(\"Best Params:\\n\", gs_xgb.best_params_)","0d9899ff":"rf_model = RandomForestRegressor(**gs_rf.best_params_)\nxgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", **gs_xgb.best_params_)","698b4d7a":"ensemble_models = {\n    \"rf\": rf_model,\n    \"xgb\": xgb_model\n}","5b8eaa4a":"predictions, scores = predict(ensemble_models, X_test, y_test)\nfor name,score in scores.items():\n    print(\"{} score: {:.4f} ({:.4f})\\n\".format(name, score.mean(), score.std()))","18e3749c":"X = raw_test_df.copy()\nX","0fb3e6d3":"class HousePricingModel():\n    \n    def __init__(self):\n        self.used_features = final_features\n        self.scaler = CustomScaler([\n            \"LotFrontage\", \"LotArea\", \"OverallQual\", \"BsmtFinSF1\", \n            'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', \n            'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', \n            'GarageArea', 'TotalPorchArea', 'TotalBathrooms',\n            'CustomQual', 'YearBuilt'\n        ])\n        \n        self.data = None\n    \n    def __area_features_engineering(self, df):\n        mapping = {\n            \"Ex\": 10,\n            \"Gd\": 8,\n            \"TA\": 6,\n            \"Fa\": 4,\n            \"Po\": 2,\n            \"None\": 0\n        }\n        \n        TotalPorchArea = df[[\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"WoodDeckSF\"]].sum(axis=1)\n        TotalPorchArea.name = \"TotalPorchArea\"\n\n        TerrainArea = df[\"LotArea\"] - ( TotalPorchArea + df[[\"LotFrontage\", \"GrLivArea\", \"TotalBsmtSF\", \"GarageArea\", \"PoolArea\"]].sum(axis=1))\n        TerrainArea.name = \"TerrainArea\"\n\n        TotalBathrooms = df[[\"FullBath\", \"BsmtFullBath\"]].sum(axis=1) + 0.5*df[[\"HalfBath\", \"BsmtHalfBath\"]].sum(axis=1)\n        TotalBathrooms.name = \"TotalBathrooms\"\n\n        CustomQual = df[[\"ExterQual\", \"BsmtQual\", \"GarageQual\", \"PoolQC\", \"KitchenQual\", \"FireplaceQu\", \"HeatingQC\"]].applymap(lambda x: mapping[x] if x in mapping else x)\n        CustomQual = CustomQual.sum(axis=1) \/ CustomQual[CustomQual > 0].count(axis=1)\n        CustomQual = pd.concat([df[\"OverallQual\"], CustomQual], axis=1)\n        CustomQual = CustomQual.mean(axis=1)\n        CustomQual.name = \"CustomQual\"\n\n\n        CustomCond = df[[\"ExterCond\", \"BsmtCond\", \"GarageCond\"]].applymap(lambda x: mapping[x] if x in mapping else x)\n        CustomCond = CustomCond.sum(axis=1) \/ CustomCond[CustomCond > 0].count(axis=1)\n        CustomCond = pd.concat([df[\"OverallCond\"], CustomCond], axis=1)\n        CustomCond = CustomCond.mean(axis=1)\n        CustomCond.name = \"CustomCond\"\n\n        new_features = pd.concat([TotalPorchArea, TerrainArea, TotalBathrooms, CustomQual, CustomCond], axis=1)\n        return new_features\n    \n    def __boolean_features_engineering(self, df):\n        mapping = {\n            \"Ex\": 10,\n            \"Gd\": 8,\n            \"TA\": 6,\n            \"Fa\": 4,\n            \"Po\": 2,\n            \"None\": 0\n        }\n\n        MaterialsUsed = pd.get_dummies(df[[\"Foundation\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\"]])\n        Style = pd.get_dummies(df[[\"HouseStyle\", \"RoofStyle\", \"BldgType\", \"LotShape\", \"LotConfig\", \"GarageType\"]])\n        Surroundings = pd.get_dummies(df[[\"Alley\", \"Street\", \"LandContour\", \"LandSlope\", \"Condition1\", \"Condition2\", \"MSZoning\", \"Neighborhood\"]])\n        Utilities = pd.get_dummies(df[[\"Electrical\", \"Heating\"]])\n        others = pd.get_dummies(df[[\"MSSubClass\", \"YrSold\", \"MoSold\"]])\n\n        new_boolean_features = BooleanEncoding(\n            df, \n            features=[\"MasVnrArea\", \"LowQualFinSF\", \"BsmtFinSF2\", \"2ndFlrSF\", \"WoodDeckSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"Fireplaces\"], \n            threshold=0\n        )\n        \n        # Boolean Encoding for categorical variables\n        hasCentralAir = df[\"CentralAir\"].map({\"N\":0, \"Y\": 1})\n        hasCentralAir.name = \"hasCentralAir\"\n        hasExFireplace = df[\"FireplaceQu\"].map(lambda x: 1 if x == \"Ex\" else 0)\n        hasExFireplace.name = \"hasExFireplace\"\n        hasExPool = df[\"PoolQC\"].map(lambda x: 1 if x == \"Ex\" else 0)\n        hasExPool.name = \"HasExPool\"\n        hasExKitchenQual = df[\"KitchenQual\"].map(lambda x: 1 if x == \"Ex\" else 0)\n        hasExKitchenQual.name = \"hasExKitchenQualQC\"\n        hasExHeatingQC = df[\"HeatingQC\"].map(lambda x: 1 if x == \"Ex\" else 0)\n        hasExHeatingQC.name = \"hasHeatingQC\"\n        hasBsmtExposure = df[\"BsmtExposure\"].map(lambda x: 0 if x == \"No\" else 1)\n        hasBsmtExposure.name = \"hasBsmtExposure\"\n\n        new_features = pd.concat([MaterialsUsed, Style, Utilities, others, new_boolean_features, hasCentralAir, hasExFireplace, hasExPool, hasExKitchenQual, hasExHeatingQC, hasBsmtExposure], axis=1)\n        return new_features\n    \n    \n    def __correct_skewness(self, df):\n        new_df = df.copy()\n        features = new_df.columns\n        for feat in features:\n            try:\n                new_df.loc[:, feat] = boxcox1p(new_df[feat], boxcox_normmax(new_df[feat] + 1))\n            except Exception as e:\n                print(f\"{feat} {e}\")\n        \n        return new_df\n        \n\n    def predict(self, model):\n        if self.data is not None:\n            pred_outputs = model.predict(self.data)\n            return np.expm1(pred_outputs)\n        \n        \n    def predicted_outputs(self, model):\n        if self.data is not None:\n            self.preprocessed_data[\"Probability\"] = model.predict_proba(self.data)[:, 1]\n            self.preprocessed_data[\"Prediction\"] = model.predict(self.data)\n            return self.preprocessed_data\n        \n        \n    def load_and_clean_data(self, df):\n        # import the data\n        #df = pd.read_csv(data_file, delimiter=\",\")\n\n        fill_features_with_none = [\n            \"GarageYrBlt\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \n            \"BsmtFinType2\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtCond\", \"BsmtQual\", \n            \"PoolQC\", \"Fence\", \"FireplaceQu\", \"Alley\", \"MiscFeature\", \"KitchenQual\"\n        ]\n        df[fill_features_with_none] = df[fill_features_with_none].fillna(\"None\")\n\n        fill_features_with_mode = [\n            \"MasVnrType\", \"Electrical\", \"SaleType\", \"MSZoning\", \"Utilities\", \n            \"Exterior1st\", \"Exterior2nd\", \"GarageCars\", \"Functional\"\n        ]\n        for feature in fill_features_with_mode:\n            df[feature] = df[feature].fillna(df[feature].mode()[0])\n        \n        fill_features_with_median = [\n            \"BsmtHalfBath\", \"BsmtFullBath\", \"TotalBsmtSF\", \"BsmtUnfSF\", \"BsmtFinSF2\", \n            \"BsmtFinSF1\", \"LotFrontage\", \"MasVnrArea\", \"GarageArea\"\n        ]\n        for feature in fill_features_with_median:\n            df[feature] = df[feature].fillna(df[feature].mode()[0])\n        \n        # convert numerical data to categorical data\n        df['MSSubClass'] = df['MSSubClass'].apply(str)\n        df['YrSold'] = df['YrSold'].astype(str)\n        df['MoSold'] = df['MoSold'].astype(str)\n        \n        # feature engineering\n        new_area_features = self.__area_features_engineering(df.copy()) \n        new_boolean_features = self.__boolean_features_engineering(df.copy())        \n        final_features = list(set(list(df.columns.values) + list(new_area_features.columns.values) + list(new_boolean_features.columns.values)))\n        final_df = pd.concat([df, new_area_features, new_boolean_features], axis=1)\n        self.ids = final_df[\"Id\"]\n        final_df.drop([\"Id\"], axis=1, inplace=True)\n        final_df.drop(final_df.select_dtypes(\"object\").columns, axis=1, inplace=True)\n        final_df = final_df[self.used_features]\n        \n        #skweness\n        skewless_df = self.__correct_skewness(final_df.drop(new_boolean_features.columns, axis=1, errors='ignore')) \n        final_df[skewless_df.columns] = skewless_df \n      \n        #scaling\n        self.scaler.fit(final_df)\n        scaled_data = self.scaler.transform(final_df.copy().reset_index(drop=True))\n\n        self.preprocessed_data = scaled_data.copy()\n        self.data = self.preprocessed_data.copy()[self.used_features]","88e75f19":"models = {**linear_models, **ensemble_models}","2a89ee92":"data_model = HousePricingModel()\ndata_model.load_and_clean_data(X) #Data must be positive\n\npredicted_values = {}\nfor key,value in models.items():\n    y_hat = data_model.predict(value)\n    predicted_values[key] = y_hat","8022f3cd":"predicted_values","0242e752":"#avering values of each model\nmean_predicted_values = np.mean(list(predicted_values.values()), axis=0)\nmean_predicted_values","f0e36b45":"submission = pd.DataFrame()\nsubmission = submission.assign(id=X[\"Id\"], SalePrice=mean_predicted_values)\nsubmission","4c8b7ba9":"submission.to_csv(f\"\/kaggle\/working\/submission_{str(round(time.time()))}.csv\", index=False)","952f6856":"## <span id=\"house_pricing_model_definition\"><\/span> House Pricing Model Definition","cc9be2ca":"# <span id=\"references\"><\/span> References","18d2b93b":"In the cell above we listed all features that have nan values. Features like PoolQC and MiscFeature have almost 100% of nan Values.","4eed9b90":"Then we can exclude values that have a z-score greater than |3|\n\n<img src=\"https:\/\/www.vedantu.com\/seo\/topicPages\/610c7966-844f-4b3d-b581-1642b15bd4fb2679850861881410179.png\"\n     alt=\"Markdown Monster icon\"\n     style=\"width: 600px; float: left; margin-right: 10px;\" \/>","d41e34ca":"Now it is time to test with the real test data, provided.","2d988225":"* Doing some calculations we can observe that:\n    * GrLivArea = 1stFlrSF + 2ndFlrSF\n    * TotalBsmtSF = BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF\n    \n    \n* We can create custom features based on the features above:\n    * TotalPorchArea = OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch + WoodDeckSF\n    * TerrainArea = LotArea - (LotFrontage + GrLivArea + TotalBsmtSF + GarageArea + PoolArea + TotalPorchArea)\n  \n  \n* For features that have more than 50% of zeros we can create boolean variables:\n    * MasVnrArea -> hasMasVnr\n    * LowQualFinSF \t> hasLowQualFinSF\n    * BsmtFinSF2 -> hasBsmtFinSF2\n    * 2ndFlrSF -> has2ndFlrSF\n    * WoodenDeckSF -> hasWoodenDeckSF\n    * EnclosedPorch -> hasEnclosedPorch\n    * 3SsnPorch -> has3SsnPorch\n    * ScreenPorch -> hasScreenPorch\n    * PoolArea -> hasPool\n    \nThe variables that have more than 75% of zeros, we will decide later if we use them or we drop them.","67737bd6":"## <span id=\"results\"><\/span> Results","dc03a7ae":"## <span id=\"new_features\"><\/span> New Features","baeaf932":"In order to get better results, some machine learning algorithms require all features to be in the same range, and scaling them is a way of doing data.\nWe will use a custom scaler to scale only the non binary features.","dfca40a2":"# <span id=\"initialization\"><\/span> Initialization\n<hr>","99c5055b":"* https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\n* https:\/\/www.kaggle.com\/shaygu\/house-prices-begginer-top-7\n* https:\/\/www.kaggle.com\/burhanykiyakoglu\/predicting-house-prices","a2dea3ef":"## <span id=\"missing_values\"><\/span> Missing values\nIn this step we will fill the missing values for each feature.\nWe have three types os features, areas features, nominal features, and ordinal features.\nWe will replace area features that are nan by 0. The thinking behind this decision is because, with high probability these values are nan because that part of the house does not exist, for example, if a house does not have a pool, the PoolQC feature will probabily be nan.\nOther categorical variables we simply use the mode to fill the nan values","5d322eab":"## <span id=\"target_variable\"><\/span> Target Variable","9e583ac4":"## <span id=\"types_of_features\"><\/span> Types of Features\n\nHere we split all the features into three categories: \n    * Areas -> square feet unit features (which are continuous numerical variables)\n    * Nominal -> Categorical Variables but there is no intrinsic ordering to its categories\n    * Ordinal -> Categorical Variables but there is an intrinsic ordering to its categories\n        ex: OverallQual - A quality of 4 is less than 5","5e66fa73":"* For these nominal features we can think of some customized features that can be useful:\n    * **YearsBeforeRem** = YearRemodAdd - YearBuilt -> This corresponds to the number of years an house was built before being remodelated\n    * **NumberBathrooms** = FullBath + BsmtFullBath + 0.5*HalfBath + 0.5*BsmtHalfBath\n    * **Materials:** like Foundation, RoofMatl, Exterior1st, Exterior2nd, MasVnrType could be dummy variables\n    * **Style:** like HouseStyle, RoofStyle, BldgType, LotShape, LotConfig, GarageType could be dummy variables\n    * **Surroundings:** like Alley, Street, LandContour, LandSlope, Condition1, Condition2, MSZoning and Neighborhood could be dummy variables\n    * **Utilities:** like Electrical, Heating, could be dummy variables\n    \n> **NOTE**:  \nWe must be careful wih low frequency categorical variables because they can cause overfitting and we have a lot of those variables in our dataset","b6eb498f":"### <span id=\"nominal_features\"><\/span> Nominal Features","66b6f33f":"* For these Ordinal features we can think of some customized features that can be useful:\n    * **CustomQual:** should be derived from ExterQual, BsmtQual, GarageQual, PoolQC, KitchenQual, FireplaceQu, HeatingQC\n    * **BsmtQual:** must be derived from BsmtExposure, BsmtFinType1, BsmtFinType2\n    * **TotalCond:** like OverallCond, ExterCond, BsmtCond, GarageCond\n    * **hasCentralAir:**\n    * **hasFireplaces:**\n    * **hasExcelentFireplace:** -> Has fireplaces and they ar in excelent condition\n    * **hasExcelentPool:** -> Has pool and is in excelent condition\n    * **hasExcelentKitchenQual:** -> Kitchen with excelent quality\n    * **hasExcelentHeatingQC:** -> Heating with Excelent quality condition\n    * **hasBsmtExposure:**","fceb1e32":"### <span id=\"validate_linear_models\"><\/span> Validate Linear Models","c39a2662":"## <span id=\"scaling\"><\/span> Scaling Features","e4ce166a":"Drop Columns with more than |0.5| of skewness","e11b80de":"# <span id=\"models\"><\/span> Models","50765fac":"# <span id=\"data_exploration\"><\/span> Data Exploration\n<hr>","60742476":"from https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing we have the following outliers:","8930752b":"# <span id=\"predictions\"><\/span> Predictions","8fd0b7d3":"### <span id=\"ordinal_features\"><\/span> Ordinal Features","d2f9a4d2":"# **Predicting House Prices**\n<hr>\n\n# Indice\n1. [Introduction](#introduction)\n1. [Initialization](#initialization)\n1. [Tools](#tools)\n1. [Data Exploration](#data_exploration)\n    1. [Types of Features](#types_of_features)\n    1. [Visualization](#visualization)\n        * [Area Features](#area_features)\n        * [Nominal Features](#nominal_features)\n        * [Ordinal Features](#ordinal_features)\n1. [Data Preprocessing](#data_preprocessing)\n    * [Target Variable](#target_variable)\n    * [Remove Outliers](#remove_outliers)\n    * [Missing Values](#missing_values)\n1. [Feature Engineering](#feature_engineering)\n    * [New Features](#new_features)\n    * [Boolean Features Distribution](#boolean_features_dist)\n    * [Overfitting](#overfitting)\n    * [Skewness](#skewness)\n    * [Scaling](#scaling)\n1. [Models](#models)\n    * [Linear Models](#linear_models)\n        * [Validate Linear Models](#validate_linear_models)\n    * [Ensemble Models](#ensemble_models)\n        * [Validate Ensemble Models](#validate_ensemble_models)\n1. [Predictions](#predictions)\n    * [House Pricing Model Definition](#house_price_model_definition)\n    * [Results](#results)\n1. [References](#References)","d798df64":"# <span id=\"tools\"><\/span> Tools\n<hr>","afe9f228":"competition url: ","f05d4ddb":"With the table above we can assume that, there are probably 72 houses without a garage and 35 houses without Basement.","854aa795":"We can observe that the target variable is right skewed. We have to transform it so that it can be a better approximation to a normal distribution. One widely used approach to deal with skewed data is a Log-transformation. This Log Transformations is basically a transformation of a variable in its log value. The Log Transfomation uses the following function:\n\n$$y=log(1+x)$$","86124650":"## <span id=\"remove_outliers\"><\/span> Remove outliers\n\nWe will use quantiles 0.01 and 0.99 as boundaries for the data, because the values below 0.01 or above 0.99 are very rare and our model should be ok if we ignore these values.","66830792":"### <span id=\"validate_ensemble_models\"><\/span> Validate Ensemble Models","ffb41350":"## <span id=\"skeweness\"><\/span> Skewness","15ee4732":"# <span id=\"overview\"><\/span> Introduction\n<hr>\nThis kernel is a simple kernel with the goal of showing the basic workflow of a Data Science problem.\n\nWe begin by importing all the necessary libraries we need. Than, we create a set of custom tools that will help us with visualizations and some calculations.<br>\nThe Data Science part:\n* <b>Data Exploration:<\/b> Separate the features by types and visualize them in order to understand how the features are distributed, \nif there is any missing values or outliers and begin to think how to solve these problems;\n* <b>Data Preprocessing<\/b>: Remove the outliers and the missing values of our dataset. In addition we transform the target variable, in this case the \"Sale Price\", in order for this variable approximate a normal distribution.\n* <b>Feature Engineering<\/b> Creation of new features, based on the features that we have. Check and Correct feature skewness. Detect overfitting features and drop them. Scaling the features to train the model.\n\nAfter we have a clean and understandable dataset we move the the machine learning part.<br>\nThe ML part:\n* <b>Model<\/b> Create several models, both Linear and Ensemple models, for demonstration purposes. The tricky part here is tuning the hyper-parameters for the model to achieve the best accuracy possible. We use RandomSearchCV to find the best hyper-parameters combination\n* <b>Predictions<\/b> Submit test dataset to all the transformations we used on the train dataset and finally predict its results.\n","d81c9625":"## <span id=\"ensemble_models\"><\/span> Ensemble Models","2ce0b46c":"## <span id=\"boolean_features_dist\"><\/span> Boolean Features Distribution","7a41f29d":"## <span id=\"linear_models\"><\/span> Linear Models","40a1311e":"## <span id=\"visualize\"><\/span> Visualization","48b220a9":"# <span id=\"feature_engineering\"><\/span> Feature Engineering","7732196c":"## <span id=\"overfitting\"><\/span> Overfitting","9eabeb25":"### <span id=\"area_features\"><\/span> Area Features","e1a9cdaf":"# <span id=\"data_preprocessing\"><\/span> Data Preprocessing\n<hr>"}}