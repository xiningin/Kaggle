{"cell_type":{"c263a983":"code","cdec2876":"code","d7ab3a3c":"code","73ccf931":"code","3166c33d":"code","26304d63":"code","9d711ab2":"code","844e1fa0":"code","ea7dfefd":"code","1633193f":"code","84472a25":"code","60e3bbf5":"code","37de3adc":"code","f9c953f0":"code","5beac68d":"code","6191b9dc":"code","bf5b85a3":"code","66e6fe18":"code","bc45e9e4":"code","bdd4a391":"code","048fa3f5":"code","edd9db3c":"code","49316f12":"code","7e7a395c":"code","f4dbe62b":"code","1343d264":"code","1f5bef12":"code","0f573dd6":"code","17eb1666":"code","e09e828a":"code","c738faed":"code","c82c4709":"code","1913151e":"code","95805955":"code","3b03ef3d":"code","81505f76":"code","168e3857":"code","e66b2150":"code","598e81dc":"markdown","21094d1d":"markdown","c6e2bd20":"markdown","89ad152d":"markdown","13c5d0b3":"markdown","89b4cb2f":"markdown","b450b321":"markdown","2cac06bc":"markdown","8415d1c1":"markdown","5943bbce":"markdown","e0e63444":"markdown","a2408115":"markdown","1cf2e42a":"markdown","f9ba78c3":"markdown"},"source":{"c263a983":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom statistics import mode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier","cdec2876":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","d7ab3a3c":"df_train","73ccf931":"def missing_data(df):\n    missing_values = round(df.isnull().sum()\/len(df) * 100,2)\n    df = missing_values.to_frame(name='MissingValuesPercentage')\n    count = df[df['MissingValuesPercentage']> 0]['MissingValuesPercentage'].count()\n    return df[['MissingValuesPercentage']].sort_values(by ='MissingValuesPercentage',ascending=False).head(count)","3166c33d":"missing_data(df_train)","26304d63":"missing_data(df_test)","9d711ab2":"grouper= df_test.groupby(['data-location','type']).mean()","844e1fa0":"df_test['garage'] = df_test['garage'].transform(lambda x: x.fillna(0))\ndf_train['garage'] = df_train['garage'].transform(lambda x: x.fillna(0))","ea7dfefd":"cat_cols = []\nnum_cols = []\nfor col_name in df_test.columns:\n        if(df_test[col_name].dtype == 'object'):\n            a = col_name\n            cat_cols.append(a)\n            \n        else:\n            a = col_name\n            num_cols.append(a)","1633193f":"cat_cols","84472a25":"def conditional_impute(input_df, columns):\n    df = input_df\n    grouper= df.groupby(['data-location','type'])\n    \n    for i in columns:\n        \n        df[i] = grouper[i].transform(lambda x: x.fillna(round(x.median(),1)))     \n    \n    return df","60e3bbf5":"df_train = conditional_impute(df_train, num_cols)\ndf_test = conditional_impute(df_test, num_cols)","37de3adc":"def zero_imputer(df):\n    \n    for col_name in df.columns:\n        if(df[col_name].dtype != 'object'):\n            df[col_name] = df[col_name].transform(lambda x: x.fillna(round(x.mean(),1)))\n    return df","f9c953f0":"zero_imputer(df_train)\nzero_imputer(df_test)","5beac68d":"cat_cols","6191b9dc":"\ndf_test = df_test.drop(['data-url', 'data-isonshow'], 1)\ndf_train = df_train.drop(['data-url', 'data-isonshow'], 1)","bf5b85a3":"df_train['data-date'] = df_train['data-date'].astype('datetime64[ns]')\ndf_test['data-date'] = df_test['data-date'].astype('datetime64[ns]')","66e6fe18":"df_train['Year_listed'] = df_train['data-date'].map(lambda x: x.year)\ndf_train['Month_listed'] = df_train['data-date'].map(lambda x: x.month)\ndf_test['Year_listed'] = df_test['data-date'].map(lambda x: x.year)\ndf_test['Month_listed'] = df_test['data-date'].map(lambda x: x.month)","bc45e9e4":"df_train = df_train.drop('data-date', 1)\ndf_test = df_test.drop('data-date', 1)","bdd4a391":"df_train.head()","048fa3f5":"def type_maker(df, df1, col):\n    dict1 = {'house':2,'apartment':1}\n    for i in col:\n        \n        df = df.replace({i:dict1})\n        df1 = df1.replace({i:dict1})\n    \n    return df, df1","edd9db3c":"df_train, df_test = type_maker(df_train, df_test,col= ['type'])","49316f12":"X_train = df_train.drop(['house-id', 'data-price'], axis = 1)\nX_test = df_test.drop(['house-id'], axis = 1)\ny_train = df_train[['data-price']]","7e7a395c":"df_train.head()","f4dbe62b":"df_train_corr = df_train.corr()\ntop_feature = df_train_corr.index[(df_train_corr['data-price'] > 0.01)]\nplt.subplots(figsize=(12, 8))\ntop_corr = df_train[top_feature].corr()\nsns.heatmap(top_corr, annot=True, cmap = sns.color_palette(\"coolwarm\", 10))\nplt.show()","1343d264":"def encode_dummies(df):\n    df_dummies = pd.get_dummies(df,  columns = ['data-location', 'area'] , drop_first = True)\n    return df_dummies\n\n","1f5bef12":"X_ = encode_dummies(pd.concat([X_train,X_test], axis=0))\nX_train = X_[:len(X_train)]\nX_test = X_[len(X_train):]","0f573dd6":"(X_train.shape, X_test.shape)","17eb1666":"y_train.shape","e09e828a":"X_train.head()","c738faed":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)","c82c4709":"(mu, sigma) = norm.fit(y_train)\n\n# 1. Plot Sale Price\nsns.distplot(y_train , fit=norm);\nplt.ylabel('Frequency')\nplt.title('Price distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n\n# Get the fitted parameters used by the function\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % y_train.skew())\nprint(\"Kurtosis: %f\" % y_train.kurt())","1913151e":"model = LinearRegression()\nmodel.fit(X_train, y_train)","95805955":"lasso = Lasso(alpha=0.02)\n\nlasso.fit(X_train, y_train)","3b03ef3d":"GBoost = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.02,max_depth=4, max_features='sqrt',min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nGBoost.fit(X_train,y_train)","81505f76":"train_OLS = model.predict(X_train)\ntrain_lasso = lasso.predict(X_train)\ntrain_GBoost = GBoost.predict(X_train)\n","168e3857":"test_lm = model.predict(X_test).reshape(-1)\ntest_lasso = lasso.predict(X_test).reshape(-1)\ntest_GBoost = GBoost.predict(X_test).reshape(-1)\n","e66b2150":"# Ordinary Least Squares\nOLS_SalePricePredict =pd.DataFrame({'house-id': df_test['house-id'], 'price': test_lm})\nOLS_SalePricePredict.to_csv('OLS_PricePredict.csv', index=False)\n\n# Lasso Regression\nLasso_SalePricePredict =pd.DataFrame({'house-id': df_test['house-id'], 'price': test_lasso})\nLasso_SalePricePredict.to_csv('Lasso_PricePredict.csv', index=False)\n\n# Gradient Boosting\nGB_SalePricePredict =pd.DataFrame({'house-id': df_test['house-id'], 'price': test_GBoost})\nGB_SalePricePredict.to_csv('GBoost_PricePredict.csv', index=False)\n\n#Weighted\nWeighted_SalePricePredict =pd.DataFrame({'house-id': df_test['house-id'], 'price': 0.5*(test_GBoost+test_lasso)})\nWeighted_SalePricePredict.to_csv('Weighted_PricePredict.csv', index=False)","598e81dc":"Removing the data-url column for the dataset","21094d1d":"Filling in rest of missing data with mean.","c6e2bd20":"Linear Regression","89ad152d":"Gradient Boosting Regression","13c5d0b3":"Filling in missing data","89b4cb2f":"Print to rsults CSV","b450b321":"Separating categorical and numerical columns","2cac06bc":"Getting Dummie Variables","8415d1c1":"Lasso Regression","5943bbce":"Changing the type column to integer","e0e63444":"**Missing Data**","a2408115":"Reformatting the date columns","1cf2e42a":"Initialising X_train, Xtest and y_test","f9ba78c3":"Imputing missing value for numerical columns."}}