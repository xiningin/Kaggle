{"cell_type":{"66a7b93c":"code","edc79ef5":"code","e8381400":"code","b2ce7ed9":"code","a9d47db4":"code","a55a3d1b":"code","f8082568":"code","48758b76":"code","9e93256f":"code","0c726802":"code","09957f2f":"code","f7145495":"code","a1ee52dd":"code","56ed2eb0":"code","9c8508d1":"code","0a23b3a5":"code","3683da5d":"code","32080d36":"code","5aac15c3":"code","5c0078a4":"code","2d2a86a1":"code","51318fa9":"code","e80e5d51":"code","98437c18":"code","5b064971":"code","7d4f92bc":"code","d56c8221":"code","bb0b379d":"code","442d3811":"code","d9531a09":"code","33a853de":"code","a728718e":"code","fbeb95c2":"code","c71a6d48":"code","bf020683":"code","40c79fed":"code","11f1e1f5":"code","544c4af5":"code","b35289c0":"code","176e5629":"code","83938833":"code","2f540e7a":"code","e8bdb6ec":"code","e3b426a8":"code","f45a7441":"code","a517d124":"code","d67e3b95":"code","f5eebc69":"code","33128678":"code","c2645ed7":"code","04de9258":"code","0378963e":"code","42b6be6c":"code","b56bdb48":"code","4c30c04e":"code","6588d819":"code","94fc39a6":"code","59190ace":"code","b23add62":"code","cde33943":"code","de0627d8":"code","ee5a13a1":"code","3bcf928f":"code","50aaf6e2":"code","3f0134a1":"code","68af7fbd":"code","d52ac547":"code","d21a2ae5":"code","1ba18cb3":"code","e7cee57c":"code","1b6bd8d7":"code","85460410":"code","23e5662c":"code","47db37ea":"code","8de906f7":"code","87a2015a":"code","3a9c5c8d":"code","9020375c":"code","50c648d5":"code","4e49169d":"code","10120b66":"code","f814c976":"code","fdc22444":"code","d41fa44d":"code","90d66998":"code","ed5e61a5":"code","418ba4ed":"code","0400b4f4":"code","816c35b7":"code","94dab791":"code","c9fbbba1":"code","d4af69d8":"code","ae7eecdc":"code","b593a488":"code","f57e1844":"code","3a2175d5":"code","67b0fdbb":"code","66cfdaa2":"code","af9a21e7":"code","e671202d":"code","fdba88c3":"code","c0ac2b9d":"code","11c00d03":"code","6f6e8f37":"code","de9706a1":"code","d535eb62":"code","706ed760":"code","0791019b":"code","4f34392e":"code","a3963d16":"code","7d2cd647":"code","eaa63316":"code","08013a0a":"code","b7ec4660":"code","afca8422":"code","831fb13a":"code","1d55e9bd":"code","2cdd2e4b":"code","419236b2":"code","284c0faa":"code","18081190":"code","41f47598":"code","acada339":"code","61877a60":"code","110b4a1d":"code","044dc365":"code","30ab2215":"code","ae301188":"code","6fd401c4":"code","d343c833":"code","eb7a5289":"code","a993a7c5":"code","0e809ee8":"code","12cbd019":"code","05dbdac6":"code","72f730d9":"code","21723ee0":"code","363650fb":"code","4e6af997":"code","1dbaa641":"code","a1734279":"code","8ad289c7":"code","b37a9fb5":"code","731b9f06":"code","8172cfe4":"code","110f459d":"code","1d454f66":"code","a205761c":"code","0bc9be99":"code","f464fd88":"code","359f5502":"code","a4e6bbb9":"code","1561fbd4":"code","e55087ba":"code","05ec8dec":"markdown","23df2d5b":"markdown","d5c2fb31":"markdown","e6567b84":"markdown","89419123":"markdown","7ab4c45a":"markdown","83005bdf":"markdown","14f2e9a3":"markdown","863b4cc5":"markdown","4453c45b":"markdown","107d76f5":"markdown","2689f751":"markdown","2d32f013":"markdown","0264a9e2":"markdown","54db2244":"markdown","2574cb83":"markdown","ee74249d":"markdown","91e2e914":"markdown","ce1da541":"markdown","ad507921":"markdown","1e6bc78b":"markdown","a0828961":"markdown","204c0f35":"markdown","7e81561f":"markdown","b1248d6c":"markdown","f1a1590f":"markdown","1c35392f":"markdown","d1e2a400":"markdown","80752508":"markdown","4b6489bf":"markdown","289bf0f2":"markdown","6b39a877":"markdown","e14e56a7":"markdown","d26eec3e":"markdown","b8c53077":"markdown","377ef1a2":"markdown","ec910b67":"markdown","1239e312":"markdown","17e52e8c":"markdown","21b99556":"markdown","71282a68":"markdown","8c919b6e":"markdown","7376260d":"markdown","7bcf23dc":"markdown","a6307cae":"markdown","91182eba":"markdown","d6dd63b5":"markdown","e7894d75":"markdown","74f3dbc8":"markdown","ac26361c":"markdown","1c81e15a":"markdown","48fde7d7":"markdown","baa08b07":"markdown","f386c7dd":"markdown","eacb750f":"markdown","26949501":"markdown","d4a7779f":"markdown","a4d61c27":"markdown","1e1b7a32":"markdown","8881c273":"markdown","e58667da":"markdown","c6efefdf":"markdown","68a0411f":"markdown","3de2ec6d":"markdown","1b69a241":"markdown","095ab503":"markdown","96627967":"markdown","d3be0e29":"markdown"},"source":{"66a7b93c":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl pytorch-tabnet -q\n!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/ -q","edc79ef5":"#!pip install -q tensorflow==2.3","e8381400":"#\u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n!pip install albumentations -q\n!pip install pymorphy2 -q\n!pip install pymorphy2-dicts -q","b2ce7ed9":"# ====================================================\n# Configurations\n# ====================================================\nimport os\nclass CFG:\n    DEBUG = False\n    \n    #Model Params\n    device = 'CPU' #['CPU','GPU','TPU']\n    N_FOLDS = 5\n    MODEL_NAME = 'tf_efficientnet_b4_ns' # Recommended : ['deit_base_patch16_384','vit_large_patch16_384','tf_efficientnet_b4_ns','resnext50_32x4d']\n    pretrained = True   \n    N_CLASSES = 5\n    #TRAIN_FOLDS = [0,1,2,3,4]\n    TRAIN_FOLDS = [1] #Folds to be Trained\n    \n    scheduler_name = 'CosineAnnealingWarmRestarts'\n    # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'GradualWarmupSchedulerV2']\n    scheduler_update = 'epoch' #['batch','epoch']\n    criterion_name = 'TaylorSmoothedLoss'\n    # ['CrossEntropyLoss', LabelSmoothing', 'FocalLoss','FocalCosineLoss', 'SymmetricCrossEntropyLoss', 'BiTemperedLoss', 'TaylorCrossEntropyLoss', 'TaylorSmoothedLoss']\n    optimizer_name = 'AdamW' #['Adam','AdamW','AdamP','Ranger'] -> AdamP doesn't work on TPUs\n    LR_RAMPUP_EPOCHS = 1\n    LR_SUSTAIN_EPOCHS = 0\n    \n    FREEZE = False #If you fine tune after START_FREEZE epochs\n    START_FREEZE = 8\n    \n    #Image Size\n    HEIGHT = 512 #If VIT or deit is chosen as model: need 384 x 384\n    WIDTH = 512\n    CHANNELS = 3\n    TRAIN_AUG_TYPE = 'train' #['train','lightaug','heavyaug','autoaugment']\n    VALID_AUG_TYPE = 'valid' #['valid']\n    \n    #Training Params\n    BATCH_SIZE = 64 # PER REPLICA FOR TPUS\n    #RECOMMENDED : effnet = 16 ; resnext = 8 ; vit = 4 ; deit = 4\n    EPOCHS = 7# more is definitely plausible and recommended around 10\n    LR = 1e-4\n    LR_START =1e-4\n    LR_MIN = 8e-7\n    weight_decay = 0\n    eps = 1e-8\n    PATIENCE = 4\n    \n    #BiTemperedLoss\n    T1 = 0.2\n    T2 = 1.1\n    LABEL_SMOOTH = 0.20\n    \n    #CosineAnnealingWarmRestarts\n    T_0 = 7\n    \n    #CosineAnnealingLR\n    T_max = EPOCHS\n    \n    NUM_WORKERS = 4\n    \n    model_print = False #If the model architecture is printed\n    tqdm = True #If training bar is shown\n    \n    IMG_MEAN = [0.485, 0.456, 0.406] #Mean for normalization Transform cassava = [0.4303, 0.4967, 0.3134] imgnet = [0.485, 0.456, 0.406]\n    IMG_STD = [0.229, 0.224, 0.225] #STD for normalization Transform cassava = [0.2142, 0.2191, 0.1954] imgnet = [0.229, 0.224, 0.225]\n    \n    USE_2019 = True #Use 2019 images?\n    \n    #n_procs = number of replicas -> TPU\n    n_procs = 1 #You can set it to 1 and run a TPU as a GPU if you want\n    BGR = False #Alternate method for loading images -> set to true is a bit slower \n    SEED = 42","a9d47db4":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-paper')\nplt.rcParams['figure.figsize'] = (18, 4)\npd.set_option('display.max_columns', 550) # \u0431\u043e\u043b\u044c\u0448\u0435 \u043a\u043e\u043b\u043e\u043d\u043e\u043a\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nimport pymorphy2","a55a3d1b":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(CFG.SEED)","f8082568":"# ====================================================\n# Required Installations\n# ====================================================\n!pip install --quiet timm==0.3.2\n\nif CFG.device == 'TPU':\n    import os\n    !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n    os.system('export XLA_USE_BF16=1')\n    os.system('export XLA_TENSOR_ALLOCATOR_MAXSIZE=100000000')\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    import ignite.distributed as idist\n    #CFG.LR = CFG.LR * CFG.n_procs\n    #CFG.BATCH_SIZE = CFG.BATCH_SIZE * CFG.n_procs\n    \nif CFG.optimizer_name == 'Ranger':\n    !pip install -q '..\/input\/pytorch-ranger'\nelif CFG.optimizer_name == 'AdamP':\n    !pip install adamp -q\n    \nif CFG.scheduler_name == 'GradualWarmupSchedulerV2':\n    !pip install git+https:\/\/github.com\/ildoonet\/pytorch-gradual-warmup-lr.git -q","48758b76":"# ====================================================\n# Library\n# ====================================================\nimport random\nimport math\nimport time\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imread\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom tensorflow.keras import regularizers\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models as tvmodels\n#from torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport timm\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport albumentations as A\nfrom albumentations import Compose\nfrom albumentations.pytorch import ToTensorV2\n\n\n\nfrom PIL import Image, ImageOps, ImageEnhance, ImageChops\n\n\nif CFG.optimizer_name == 'Ranger':\n    from pytorch_ranger import Ranger\nelif CFG.optimizer_name == 'AdamP':\n    from adamp import AdamP\n    \nif CFG.scheduler_name == 'GradualWarmupSchedulerV2':\n    from warmup_scheduler import GradualWarmupScheduler\n\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9e93256f":"def log_t(u, t):\n    \"\"\"Compute log_t for `u'.\"\"\"\n    if t==1.0:\n        return u.log()\n    else:\n        return (u.pow(1.0 - t) - 1.0) \/ (1.0 - t)\n\ndef exp_t(u, t):\n    \"\"\"Compute exp_t for `u'.\"\"\"\n    if t==1:\n        return u.exp()\n    else:\n        return (1.0 + (1.0-t)*u).relu().pow(1.0 \/ (1.0 - t))\n\ndef compute_normalization_fixed_point(activations, t, num_iters):\n\n    \"\"\"Returns the normalization value for each example (t > 1.0).\n    Args:\n      activations: A multi-dimensional tensor with last dimension `num_classes`.\n      t: Temperature 2 (> 1.0 for tail heaviness).\n      num_iters: Number of iterations to run the method.\n    Return: A tensor of same shape as activation with the last dimension being 1.\n    \"\"\"\n    mu, _ = torch.max(activations, -1, keepdim=True)\n    normalized_activations_step_0 = activations - mu\n\n    normalized_activations = normalized_activations_step_0\n\n    for _ in range(num_iters):\n        logt_partition = torch.sum(\n                exp_t(normalized_activations, t), -1, keepdim=True)\n        normalized_activations = normalized_activations_step_0 * \\\n                logt_partition.pow(1.0-t)\n\n    logt_partition = torch.sum(\n            exp_t(normalized_activations, t), -1, keepdim=True)\n    normalization_constants = - log_t(1.0 \/ logt_partition, t) + mu\n\n    return normalization_constants\n\ndef compute_normalization_binary_search(activations, t, num_iters):\n\n    \"\"\"Returns the normalization value for each example (t < 1.0).\n    Args:\n      activations: A multi-dimensional tensor with last dimension `num_classes`.\n      t: Temperature 2 (< 1.0 for finite support).\n      num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n\n    mu, _ = torch.max(activations, -1, keepdim=True)\n    normalized_activations = activations - mu\n\n    effective_dim = \\\n        torch.sum(\n                (normalized_activations > -1.0 \/ (1.0-t)).to(torch.int32),\n            dim=-1, keepdim=True).to(activations.dtype)\n\n    shape_partition = activations.shape[:-1] + (1,)\n    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n    upper = -log_t(1.0\/effective_dim, t) * torch.ones_like(lower)\n\n    for _ in range(num_iters):\n        logt_partition = (upper + lower)\/2.0\n        sum_probs = torch.sum(\n                exp_t(normalized_activations - logt_partition, t),\n                dim=-1, keepdim=True)\n        update = (sum_probs < 1.0).to(activations.dtype)\n        lower = torch.reshape(\n                lower * update + (1.0-update) * logt_partition,\n                shape_partition)\n        upper = torch.reshape(\n                upper * (1.0 - update) + update * logt_partition,\n                shape_partition)\n\n    logt_partition = (upper + lower)\/2.0\n    return logt_partition + mu\n\nclass ComputeNormalization(torch.autograd.Function):\n    \"\"\"\n    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n    \"\"\"\n    @staticmethod\n    def forward(ctx, activations, t, num_iters):\n        if t < 1.0:\n            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n        else:\n            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n\n        ctx.save_for_backward(activations, normalization_constants)\n        ctx.t=t\n        return normalization_constants\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        activations, normalization_constants = ctx.saved_tensors\n        t = ctx.t\n        normalized_activations = activations - normalization_constants \n        probabilities = exp_t(normalized_activations, t)\n        escorts = probabilities.pow(t)\n        escorts = escorts \/ escorts.sum(dim=-1, keepdim=True)\n        grad_input = escorts * grad_output\n        \n        return grad_input, None, None\n\ndef compute_normalization(activations, t, num_iters=5):\n    \"\"\"Returns the normalization value for each example. \n    Backward pass is implemented.\n    Args:\n      activations: A multi-dimensional tensor with last dimension `num_classes`.\n      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n      num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n    return ComputeNormalization.apply(activations, t, num_iters)\n\ndef tempered_sigmoid(activations, t, num_iters = 5):\n    \"\"\"Tempered sigmoid function.\n    Args:\n      activations: Activations for the positive class for binary classification.\n      t: Temperature tensor > 0.0.\n      num_iters: Number of iterations to run the method.\n    Returns:\n      A probabilities tensor.\n    \"\"\"\n    internal_activations = torch.stack([activations,\n        torch.zeros_like(activations)],\n        dim=-1)\n    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n    return internal_probabilities[..., 0]\n\n\ndef tempered_softmax(activations, t, num_iters=5):\n    \"\"\"Tempered softmax function.\n    Args:\n      activations: A multi-dimensional tensor with last dimension `num_classes`.\n      t: Temperature > 1.0.\n      num_iters: Number of iterations to run the method.\n    Returns:\n      A probabilities tensor.\n    \"\"\"\n    if t == 1.0:\n        return activations.softmax(dim=-1)\n\n    normalization_constants = compute_normalization(activations, t, num_iters)\n    return exp_t(activations - normalization_constants, t)\n\ndef bi_tempered_binary_logistic_loss(activations,\n        labels,\n        t1,\n        t2,\n        label_smoothing = 0.0,\n        num_iters=5,\n        reduction='mean'):\n\n    \"\"\"Bi-Tempered binary logistic loss.\n    Args:\n      activations: A tensor containing activations for class 1.\n      labels: A tensor with shape as activations, containing probabilities for class 1\n      t1: Temperature 1 (< 1.0 for boundedness).\n      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n      label_smoothing: Label smoothing\n      num_iters: Number of iterations to run the method.\n    Returns:\n      A loss tensor.\n    \"\"\"\n    internal_activations = torch.stack([activations,\n        torch.zeros_like(activations)],\n        dim=-1)\n    internal_labels = torch.stack([labels.to(activations.dtype),\n        1.0 - labels.to(activations.dtype)],\n        dim=-1)\n    return bi_tempered_logistic_loss(internal_activations, \n            internal_labels,\n            t1,\n            t2,\n            label_smoothing = label_smoothing,\n            num_iters = num_iters,\n            reduction = reduction)\n\ndef bi_tempered_logistic_loss(activations,\n        labels,\n        t1 = CFG.T1,\n        t2 = CFG.T2,\n        label_smoothing=CFG.LABEL_SMOOTH,\n        num_iters=5,\n        reduction = 'mean'):\n\n    \"\"\"Bi-Tempered Logistic Loss.\n    Args:\n      activations: A multi-dimensional tensor with last dimension `num_classes`.\n      labels: A tensor with shape and dtype as activations (onehot), \n        or a long tensor of one dimension less than activations (pytorch standard)\n      t1: Temperature 1 (< 1.0 for boundedness).\n      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n      num_iters: Number of iterations to run the method. Default 5.\n      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n        ``'none'``: No reduction is applied, return shape is shape of\n        activations without the last dimension.\n        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n    Returns:\n      A loss tensor.\n    \"\"\"\n\n    if len(labels.shape)<len(activations.shape): #not one-hot\n        labels_onehot = torch.zeros_like(activations)\n        labels_onehot.scatter_(1, labels[..., None], 1)\n    else:\n        labels_onehot = labels\n\n    if label_smoothing > 0:\n        num_classes = labels_onehot.shape[-1]\n        labels_onehot = ( 1 - label_smoothing * num_classes \/ (num_classes - 1) ) \\\n                * labels_onehot + \\\n                label_smoothing \/ (num_classes - 1)\n\n    probabilities = tempered_softmax(activations, t2, num_iters)\n\n    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n            - labels_onehot * log_t(probabilities, t1) \\\n            - labels_onehot.pow(2.0 - t1) \/ (2.0 - t1) \\\n            + probabilities.pow(2.0 - t1) \/ (2.0 - t1)\n    loss_values = loss_values.sum(dim = -1) #sum over classes\n\n    if reduction == 'none':\n        return loss_values\n    if reduction == 'sum':\n        return loss_values.sum()\n    if reduction == 'mean':\n        return loss_values.mean()\n\nclass BiTemperedLogistic(nn.Module):\n    def __init__(self, T1 = CFG.T1, T2 = CFG.T2, LABEL_SMOOTH = CFG.LABEL_SMOOTH):\n        super().__init__()\n        self.T1 = T1\n        self.T2 = T2\n        self.LABEL_SMOOTH = LABEL_SMOOTH\n\n    def forward(self, logits,labels):\n        return bi_tempered_logistic_loss(logits, labels,t1 = self.T1,t2 = self.T2, label_smoothing = self.LABEL_SMOOTH)\n    \nclass SymmetricCrossEntropy(nn.Module):\n\n    def __init__(self, alpha=0.1, beta=1.0, num_classes= 5):\n        super(SymmetricCrossEntropy, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.num_classes = num_classes\n\n    def forward(self, logits, targets, reduction='mean'):\n        onehot_targets = torch.eye(self.num_classes)[targets].cuda()\n        ce_loss = F.cross_entropy(logits, targets, reduction=reduction)\n        rce_loss = (-onehot_targets*logits.softmax(1).clamp(1e-7, 1.0).log()).sum(1)\n        if reduction == 'mean':\n            rce_loss = rce_loss.mean()\n        elif reduction == 'sum':\n            rce_loss = rce_loss.sum()\n        return self.alpha * ce_loss + self.beta * rce_loss\n    \nclass LabelSmoothingLoss(nn.Module): \n    def __init__(self, classes=5, smoothing=0.0, dim=-1): \n        super(LabelSmoothingLoss, self).__init__() \n        self.confidence = 1.0 - smoothing \n        self.smoothing = smoothing \n        self.cls = classes \n        self.dim = dim \n    def forward(self, pred, target): \n        if CFG.criterion_name == 'LabelSmoothingLoss':\n            pred = pred.log_softmax(dim=self.dim) \n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred) \n            true_dist.fill_(self.smoothing \/ (self.cls - 1)) \n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n    \nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        BCE_loss = nn.CrossEntropyLoss()(inputs, targets)\n\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n        \nclass FocalCosineLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, xent=.1):\n        super(FocalCosineLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n        self.xent = xent\n\n        self.y = torch.Tensor([1]).cuda()\n\n    def forward(self, input, target, reduction=\"mean\"):\n        cosine_loss = F.cosine_embedding_loss(input, F.one_hot(target, num_classes=input.size(-1)), self.y, reduction=reduction)\n\n        cent_loss = F.cross_entropy(F.normalize(input), target, reduce=False)\n        pt = torch.exp(-cent_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * cent_loss\n\n        if reduction == \"mean\":\n            focal_loss = torch.mean(focal_loss)\n\n        return cosine_loss + self.xent * focal_loss\n    \nclass TaylorSoftmax(nn.Module):\n    '''\n    This is the autograd version\n    '''\n    def __init__(self, dim=1, n=2):\n        super(TaylorSoftmax, self).__init__()\n        assert n % 2 == 0\n        self.dim = dim\n        self.n = n\n\n    def forward(self, x):\n        '''\n        usage similar to nn.Softmax:\n            >>> mod = TaylorSoftmax(dim=1, n=4)\n            >>> inten = torch.randn(1, 32, 64, 64)\n            >>> out = mod(inten)\n        '''\n        fn = torch.ones_like(x)\n        denor = 1.\n        for i in range(1, self.n+1):\n            denor *= i\n            fn = fn + x.pow(i) \/ denor\n        out = fn \/ fn.sum(dim=self.dim, keepdims=True)\n        return out\n    \nclass TaylorCrossEntropyLoss(nn.Module):\n    '''\n    This is the autograd version\n    '''\n    def __init__(self, n=2, ignore_index=-1, reduction='mean'):\n        super(TaylorCrossEntropyLoss, self).__init__()\n        assert n % 2 == 0\n        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n        self.reduction = reduction\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, labels):\n        '''\n        usage similar to nn.CrossEntropyLoss:\n            >>> crit = TaylorCrossEntropyLoss(n=4)\n            >>> inten = torch.randn(1, 10, 64, 64)\n            >>> label = torch.randint(0, 10, (1, 64, 64))\n            >>> out = crit(inten, label)\n        '''\n        log_probs = self.taylor_softmax(logits).log()\n        loss = F.nll_loss(log_probs, labels, reduction=self.reduction,\n                ignore_index=self.ignore_index)\n        return loss\n    \nclass TaylorSmoothedLoss(nn.Module):\n\n    def __init__(self, n=2, ignore_index=-1, reduction='mean', smoothing=0.2):\n        super(TaylorSmoothedLoss, self).__init__()\n        assert n % 2 == 0\n        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n        self.reduction = reduction\n        self.ignore_index = ignore_index\n        self.lab_smooth = LabelSmoothingLoss(CFG.N_CLASSES, smoothing=CFG.LABEL_SMOOTH)\n\n    def forward(self, logits, labels):\n\n        log_probs = self.taylor_softmax(logits).log()\n        #loss = F.nll_loss(log_probs, labels, reduction=self.reduction,\n        #        ignore_index=self.ignore_index)\n        loss = self.lab_smooth(log_probs, labels)\n        return loss","0c726802":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport random\nimport numpy as np # linear algebra\nfrom numpy import median\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL\nimport cv2\nimport re\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom keras.callbacks import Callback\n# # keras\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.callbacks import LearningRateScheduler,EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nimport albumentations\nimport tensorflow_addons as tfa\n# plt\nimport matplotlib.pyplot as plt\n#\u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#\u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09957f2f":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0430\u0441\u0447\u0435\u0442\u0430 \u043e\u0448\u0438\u0431\u043a\u0438\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred-y_true)\/y_true))\n\n\ndef find_duplicates(data, keys=['sell_id']):\n    \"\"\"\n    \u0420\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u043a\u043e\u043b-\u0432\u043e \u0434\u0443\u0431\u043b\u0435\u0439 \u0432\n    \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435, \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c, \u043f\u0435\u0440\u0435\u0434\u0430\u043d\u043d\u044b\u043c \u0432\n    \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u0430 keys \u0444\u0443\u043d\u043a\u0446\u0438\u0438. \n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043b-\u0432\u043e \u0434\u0443\u0431\u043b\u0435\u0439.\n    :param data: a DataFrame object\n    : param keys: a list of string obj values, a names of feature\n    :return: an int object, a number of duplicates.\n    \n    \"\"\"\n    \n    shape_before = data.shape[0]\n    shape_after = data.drop_duplicates(subset=keys).shape[0]\n    \n    duplicates_number = shape_before - shape_after\n    \n    return duplicates_number\n\n\ndef look_at_data(data, type_data):\n    \"\"\"\n    \u0412\u044b\u0432\u043e\u0434\u0438\u0442 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d \u043f\u0435\u0440\u0432\u0438\u0447\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\n    :param type_data: string\n    :param data: a DataFrame object\n    :return: None\n    \n    \"\"\"\n    # \u0432\u044b\u044f\u0432\u043b\u044f\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438\n    null_df = data.isnull().sum().reset_index()\n    null_df.columns = 'col_name null_count'.split(' ')\n    null_df = null_df.loc[null_df.null_count > 0]\n    \n    # report\n    print(f\"{'\u0414\u0430\u0442\u0430\u0441\u0435\u0442'.ljust(20)} | {type_data.capitalize()}\")\n    print('-'*34)\n    print(f\"{'\u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c'.ljust(20)} | {data.shape}\")\n    print('-'*34)\n          \n\n    # find duplicates\n    duplicates_number = find_duplicates(data)\n    if duplicates_number == 0:\n        print(f\"{'\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b'.ljust(20)} | \u043d\u0435\u0442.\")\n    else:\n        print(f\"{'\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b'.ljust(20)} | \u0435\u0441\u0442\u044c.\")\n        print('-'*34)\n        print(f\"{'\u041a\u043e\u043b-\u0432\u043e \u0434\u0443\u0431\u043b\u0435\u0439'.ljust(20)} | {duplicates_number} \u0448\u0442.\")\n    \n    print('-'*34)  \n    print(f\"\u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 \u0441\u043b\u0435\u0434. \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445:\")\n    \n    print('-'*34)\n    print(f\"{'\u041f\u0440\u0438\u0437\u043d\u0430\u043a'.ljust(20)} | \u043a\u043e\u043b-\u0432\u043e, \u0448\u0442.\")\n    print('-'*34)\n    \n    for index, row in null_df.iterrows():\n        print(f\"{row['col_name'].ljust(21)}| {row['null_count']}\")\n    print('-'*34)\n    \n    \n    display(data.head(3))\n    \ndef find_data_type_discrepancy(train, test):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0438\u0449\u0435\u0442 \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u0432 \u0442\u0438\u043f\u0430\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \n    \u043c\u0435\u0436\u0434\u0443 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438 \u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043d\u0430 \u044d\u043a\u0440\u0430\u043d.\n    \u041d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\n    :param train: DataFrame object with training data\n    :param test: DataFrame object with testing data\n    \"\"\"\n    \n    train_dtypes_df = train.dtypes.reset_index()\n    train_dtypes_df.columns = 'col_name type_train'.split(' ')\n\n\n    test_dtypes_df = test.dtypes.reset_index()\n    test_dtypes_df.columns = 'col_name type_test'.split(' ')\n\n\n    merged_df = train_dtypes_df.merge(test_dtypes_df, how='outer', left_on='col_name', right_on='col_name')\n    merged_df\n\n    print(f\"{'\u0420\u0430\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445:'}\")\n    print(f\"{'\u041f\u0440\u0438\u0437\u043d\u0430\u043a'.ljust(21)} | {'train'.ljust(15)} | {'test'.ljust(15)}\")\n    print('-'*48)\n\n    for index, row in merged_df.iterrows():\n        if str(row['type_train']) == 'nan' or str(row['type_test']) == 'nan':\n            print(f\"{str(row['col_name']).ljust(21)} | {str(row['type_train']).ljust(15)} | {str(row['type_test']).ljust(15)}\")\n\n        elif row['type_train'] != row['type_test']:\n            print(f\"{str(row['col_name']).ljust(21)} | {str(row['type_train']).ljust(15)} | {str(row['type_test']).ljust(15)}\")\n    \n    \n# \u0442\u0435\u0441\u0442 -  \u0443\u0434\u0430\u043b\u0438\u043c LTR \u0438 \u0442\u0430\u043a \u0436\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u0432 1600, 1800, etc.\ndef get_correct_engineDisplacement_test(x):\n    if x == ' LTR':  # \u0443 \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0430\u0432\u0442\u043e \u043e\u0431\u044a\u0435\u043c \u0440\u0430\u0432\u0435\u043d 0 \n        return 0\n    \n    x = float(x.split(' LTR')[0])\n    \n    return int(x * 1000)\n\n\ndef show_main_feature_metrics(data, feature_name, sampling_type=5, is_plot=True):\n    \"\"\"\n    \n    \"\"\"\n    if sampling_type == 5:\n        samples = \"\u0432\u0441\u0435\u0439\"\n    elif sampling_type == 1:\n        samples = \"\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439\"\n        data = data[df.train == 1]\n    else:\n        samples = \"\u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439\"\n        data = data[df.train == 0]\n    \n    if is_plot:\n        ax = sns.countplot(x=feature_name, data=data) # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n        ax.tick_params(axis = 'both', labelsize = 12, labelcolor = 'b', labelrotation = 60)\n        plt.title(f'\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 {feature_name} \u043f\u043e {samples.upper()} \u0432\u044b\u0431\u043e\u0440\u043a\u0435.')\n    unique_number_str = f\"Number of unique values: {data[feature_name].nunique()}\"\n    print(unique_number_str)\n    print('-' * len(unique_number_str))\n    print(f\"Top 5 most popular values:\")\n    if data[feature_name].nunique() >= 5:\n        print(data[feature_name].value_counts(normalize=True)[:5])\n    else:\n        print(data[feature_name].value_counts(normalize=True))\n    print('-' * len(unique_number_str))\n    print(f\"Empty values count: {data[feature_name].isnull().sum()}\")\n    print(f\"Percentage of empty values: {np.round(data[feature_name].isnull().mean(), 2)}\")\n    \n    \ndef show_barplot(data, feature_name, size=(16,10)):\n    \"\"\"\n    :param data: a DataFrame obj\n    :param feature_name: a string\n    :param size: a tuple of characteristics of a barplot size\n    \n    \"\"\"\n    \n    # plot\n    tmp_df = pd.DataFrame(data[feature_name].value_counts()).reset_index().rename(columns={'index':f\"{feature_name}_unique_values\",\n                                                                                feature_name: 'count'})\n    \n    plt.figure(figsize=size)\n    plot = sns.barplot(y='count',x=f\"{feature_name}_unique_values\", data=tmp_df)\n    plot = plt.setp(plot.get_xticklabels(), rotation=45)\n    \n    \ndef show_median_barplot(data, col, target, size=(18,4)):\n    \"\"\"\n    \u0412\u044b\u0432\u043e\u0434\u0438\u0442 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d \u043c\u0435\u0434\u0438\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 target\n    \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 col.\n    :param data: a DataFrame object\n    :param col: a string, a name of a categorical feature\n    :param target: a string, a name of numerical feature\n    \"\"\"\n    plt.figure(figsize=size)\n    \n    plot = sns.barplot(x=col, y=target, data=data, estimator=median)\n    plot.set_title(f\"\u041c\u0435\u0434\u0438\u0430\u043d\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 {target.upper()} \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 {col.upper()}\")\n    plot.set_xticklabels(plot.get_xticklabels(), rotation=45)\n    \n    \n\ndef get_correct_body_type(name):\n    \"\"\"\n    \u041f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0442\u0438\u043f \u043a\u0443\u0437\u043e\u0432\u0430 \u043a\n    \u0443\u043d\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u043c\u0443 \u0432\u0438\u0434\u0443.\n    \u0415\u0441\u043b\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \"\u0442\u0430\u0440\u0433\u0430\" - \u0432\u0435\u0440\u043d\u0443\u0442\u044c \"\u0440\u043e\u0434\u0441\u0442\u0435\u0440\"\n    \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u0438\u0435 \u0442\u0443\u0442: https:\/\/ru.wikipedia.org\/wiki\/%D0%A2%D0%B0%D1%80%D0%B3%D0%B0\n    \n    \"\"\"\n    body_types_lst = '\u0441\u0435\u0434\u0430\u043d|\u0445\u044d\u0442\u0447\u0431\u0435\u043a|\u0445\u044d\u0442\u0447\u0431\u0435\u043a|\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a|\u043a\u0443\u043f\u0435|\u0440\u043e\u0434\u0441\u0442\u0435\u0440|\u043b\u0438\u0444\u0442\u0431\u0435\u043a|\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b|' +\\\n                     '\u043a\u0430\u0431\u0440\u0438\u043e\u043b\u0435\u0442|\u043a\u043e\u043c\u043f\u0430\u043a\u0442\u0432\u044d\u043d|\u043f\u0438\u043a\u0430\u043f|\u043c\u0438\u043d\u0438\u0432\u044d\u043d|\u043c\u0438\u043a\u0440\u043e\u0432\u044d\u043d|\u0444\u0443\u0440\u0433\u043e\u043d|\u043b\u0438\u043c\u0443\u0437\u0438\u043d|\u0444\u0430\u0441\u0442\u0431\u0435\u043a'\n    \n    \n    if '\u0442\u0430\u0440\u0433\u0430' in name:\n        return '\u0440\u043e\u0434\u0441\u0442\u0435\u0440' # \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0435 \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435, \u0442\u0430\u043a \u043a\u0430\u043a \u043a\u0443\u0437\u043e\u0432 \u0442\u0438\u043f\u0430 \u0442\u0430\u0440\u0433\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u0440\u043e\u0434\u0441\u0442\u0435\u0440.\n    \n    for item in body_types_lst.split('|'): \n        if item in name:\n            return item\n    \n    return name        \n        \n# \u0434\u043b\u044f \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 auto_class\ndef compose_key(row):\n    return f\"{row['brand']}_{row['model_name']}_{row['bodyType']}\"\n\ndef fill_class_dict(row):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u0441\u0442\u0440\u043e\u043a\u0443 \u0442\u0438\u043f\u0430 series,\n    \u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u044c class_dict.\n    \u041d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442.\n    \n    \"\"\"\n    \n    key = compose_key(row)\n    value = row['auto_class']\n    \n    if key in class_dict:\n        return\n    class_dict[key] = value\n    return\n\ndef fill_na_class(row):\n    \"\"\"\n    \n    \"\"\"\n    key = compose_key(row)\n    if key in class_dict:\n        row['auto_class'] = class_dict[key]\n    return row\n\n\n# \u0434\u043b\u044f \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 price_segment\ndef fill_price_dict(row):\n    key = compose_key(row)\n    \n    if key not in price_segment_dict:\n        price_segment_dict[key] = row['price_segment']\n    \n    return\n\ndef fill_na_segment(row):\n    \"\"\"\n    \n    \"\"\"\n    key = compose_key(row)\n    if key in price_segment_dict:\n        row['price_segment'] = price_segment_dict[key]\n    return row\n\n\n# \u0434\u043b\u044f \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 seller_type\ndef fill_na_seller_from_train(row):\n    \"\"\"\n    \n    \"\"\"\n    curr_id = row['sell_id']\n    \n    try:\n        value_from_train = train_df[train_df.sell_id == curr_id]\n        if not value_from_train.empty:\n            row['seller_type'] = value_from_train['seller_type'].iloc[0]\n        return row\n    except:\n        return row      \n\n    \ndef fill_na_seller_type_from_description(row):\n    for item in seller_stop_words:\n        if row['description'].find(item) != -1:\n            row['seller_type'] = 'COMMERCIAL'\n            return row\n    \n    row['seller_type'] = 'PRIVATE'\n    \n    return row\n\ndef in_eng_duspl(x):\n    if x in spis_eng:\n        return None\n    else:\n        return x\ndef engineDisplacement(row):\n    row = str(row)\n    engine = re.findall('\\d\\.\\d', row)\n    if engine == []:\n        return None\n    return float(engine[0])\n\ndef owner_age(value):\n    age = 0\n    if type(value) == str:\n        values = value.split(' ')\n        #print(values)\n        if len(values) > 3:\n            if \"\u043b\u0435\u0442\" in values[1]  or \"\u0433\u043e\u0434\" in values[1]:\n                age = age + (int)(values[0])\n            else:\n                age = age + (int)(values[0]) \/ 10\n            if \"\u043c\u0435\u0441\u044f\u0446\u0435\u0432\" in values[4] or \"\u043c\u0435\u0441\u044f\u0446\" in values[4]:\n                age = age + (int)(values[3]) \/ 10\n        else:\n            if \"\u043b\u0435\u0442\" in values[1]  or \"\u0433\u043e\u0434\" in values[1]:\n                age = age + (int)(values[0])\n            if \"\u043c\u0435\u0441\u044f\u0446\u0435\u0432\" in values[1] or \"\u043c\u0435\u0441\u044f\u0446\" in values[1]:\n                age = age + (int)(values[0]) \/ 10\n    else:\n        #print(value)\n        return -1\n    \n    return age\ndef owners_count(value):\n    if type(value) == str:\n        return (int)(value[0])\n    else:\n        return -1\ndef engine_displacement(value):\n    \n    value = value.split(' ')[0]\n    if value== 'undefined':\n        return 0\n    return (float)(value)\n# \u0434\u043b\u044f \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 section\ndef fill_na_section_from_mileage(row):\n    \"\"\"\n    \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442 \u0442\u0438\u043f\u0430 series,\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442 \u0442\u0438\u043f\u0430 series\n    \"\"\"\n    mileage = row['mileage']\n    prod_date = row['productionDate']\n    if (mileage <= 30 and prod_date >= 2018):\n        row['section'] = 'new'\n    else:\n        row['section'] = 'used'    \n    \n    return row\n\n\ndef get_stat_metrics(s):\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442 \u0442\u0438\u043f\u0430 pandas.Series\n    \u0420\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438:\n    median - \u043c\u0435\u0434\u0438\u0430\u043d\u0430\n    perc25, perc75 - \u043d\u0438\u0436\u043d\u0438\u0439 \u0438 \u0432\u0435\u0440\u0445\u043d\u0438\u0439 \u043a\u0432\u0430\u0440\u0442\u0438\u043b\u0438\n    IQR - \u043c\u0435\u0436\u043a\u0432\u0430\u0440\u0442\u0438\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0430\u0445\n    low_border - \u043d\u0438\u0436\u043d\u044f\u044f \u0433\u0440\u0430\u043d\u0438\u0446\u0430\n    upper_border - \u0432\u0435\u0440\u0445\u043d\u044f\u044f \u0433\u0440\u0430\u043d\u0438\u0446\u0430\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.\n    \"\"\"\n    q1 = s.quantile(0.25)\n    q2 = s.median()\n    q3 = s.quantile(0.75)\n    iqr = q3 - q1\n    low_border = q1 - 1.5 * iqr\n    upper_border = q3 + 1.5 * iqr\n    minimum = s.min()\n    maximum = s.max()\n    \n    names = 'q1 q2 q3 iqr low_border upper_border minimum maximum'.split(' ')\n    \n    stats = dict(zip(names,\n                     [q1, q2, q3, iqr, low_border, \\\n                      upper_border, minimum, maximum]))\n    \n    return stats\n\n\n# \u0410\u043d\u0430\u043b\u0438\u0437 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n# \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u0432\u044b\u0432\u043e\u0434\u0430 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d \u0441\u0442\u0430\u0442 \u043c\u0435\u0442\u0440\u0438\u043a \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443\ndef show_stat_data(s):\n    \"\"\"\n    \u0412\u044b\u0432\u043e\u0434\u0438\u0442 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d \u0432 \u0443\u0434\u043e\u0431\u043e\u0447\u0438\u0442\u0430\u0435\u043c\u043e\u043c \u0432\u0438\u0434\u0435\n    \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435.\n    \u0423\u0432\u0435\u0434\u043e\u043c\u043b\u044f\u0435\u0442 \u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432.\n    \u0420\u0438\u0441\u0443\u0435\u0442 \u0433\u0438\u0441\u0442\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.\n    \u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432, \u0440\u0438\u0441\u0443\u0435\u0442 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e\n    \u0433\u0438\u0441\u0442\u0440\u0430\u043c\u043c\u0443 \u0431\u0435\u0437 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432.\n    \"\"\"\n    \n    sd = get_stat_metrics(s) # sd - statistics data\n    print(f'[ \u041f\u0440\u0438\u0437\u043d\u0430\u043a {s.name} ]')\n    print(f\"25-\u0439 \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u044c: {np.round(sd['q1'], 3)}, \u043c\u0435\u0434\u0438\u0430\u043d\u0430: {np.round(sd['q2'], 3)},\", \\\n          f\"75-\u0439 \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u044c: {np.round(sd['q3'], 3)},\", f\"IQR: {np.round(sd['iqr'], 3)}.\")\n    print(f\"\u0413\u0440\u0430\u043d\u0438\u0446\u044b \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432: [{np.round(sd['low_border'], 3)}, {np.round(sd['upper_border'], 3)}]\")\n    print(f\"\u0414\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439: [{sd['minimum']}, {sd['maximum']}]\")\n    \n    if (sd['minimum']<sd['low_border']) or (sd['maximum']>sd['upper_border']):\n        print(f\"\u0412 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u043c\u0435\u044e\u0442\u0441\u044f \u0432\u044b\u0431\u0440\u043e\u0441\u044b!\")\n        count_under_low_border = (s < sd['low_border']).sum()\n        count_upper_upper_border = (s > sd['upper_border']).sum()\n        print(f\"\u041a\u043e\u043b-\u0432\u043e \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432:\")\n        print(f\"\u0417\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u043c\u0438 \u043d\u0438\u0436\u043d\u0435\u0439 \u0433\u0440\u0430\u043d\u0438\u0446\u044b: {count_under_low_border}\")\n        print(f\"\u0417\u0430 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u043c\u0438 \u0432\u0435\u0440\u0445\u043d\u0435\u0439 \u0433\u0440\u0430\u043d\u0438\u0446\u044b: {count_upper_upper_border}\\n\")\n    else:\n        print(f\"\u0412\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u043d\u0435\u0442\\n\")","f7145495":"DATA_DIR = '..\/input\/sf-dst-car-price-prediction-part2\/'\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\ntest = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')\nsubmission = pd.read_csv(DATA_DIR+'sample_submission.csv')\n\nX_test = test.values","a1ee52dd":"train.info()","56ed2eb0":"train.sample(20)","9c8508d1":"categorical_features = ['bodyType','brand','fuelType','model_info','vehicleTransmission','vendor','passport','gearType','class']","0a23b3a5":"train.nunique()","3683da5d":"#\u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\ndef visualize_distributions(titles_values_dict):\n    columns = min(3, len(titles_values_dict))\n    rows = (len(titles_values_dict) - 1) \/\/ columns + 1\n    fig = plt.figure(figsize = (columns * 6, rows * 4))\n    for i, (title, values) in enumerate(titles_values_dict.items()):\n        hist, bins = np.histogram(values, bins = 20)\n        ax = fig.add_subplot(rows, columns, i + 1)\n        ax.bar(bins[:-1], hist, width = (bins[1] - bins[0]) * 0.7)\n        ax.set_title(title)\n    plt.show()\n\nvisualize_distributions({\n    'mileage': train['mileage'].dropna(),\n    'modelDate': train['modelDate'].dropna(),\n    'productionDate': train['productionDate'].dropna()\n})","32080d36":"import seaborn as sns\n# Defining boxplot function.\ndef get_boxplot(df,column):\n    fig, ax = plt.subplots(figsize = (12, 6))\n    sns.boxplot(x=column, \n                y='price',\n                data=df.loc[\n                    df.loc[:, column].isin(df.loc[:, column].value_counts().index)\n                ],\n                ax=ax)\n    plt.xticks(rotation=90)\n    ax.set_title('Boxplot for ' + column)\n    plt.show()","5aac15c3":"train['sample'] = 1 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ntest['sample'] = 0 # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\ntest['price'] = 0 # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f price, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\n\ndata = test.append(train, sort=False).reset_index(drop=True) # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c\nprint(train.shape, test.shape, data.shape)","5c0078a4":"data.info()","2d2a86a1":"data.bodyType.value_counts()","51318fa9":"data.bodyType = data.bodyType.apply(get_correct_body_type)","e80e5d51":"show_main_feature_metrics(data, \"bodyType\")","98437c18":"show_main_feature_metrics(data, \"brand\")","5b064971":"data.fuelType.value_counts()","7d4f92bc":"show_main_feature_metrics(data, \"fuelType\")","d56c8221":"show_median_barplot(data[(data.fuelType == '\u0431\u0435\u043d\u0437\u0438\u043d') ], 'brand', 'price')","bb0b379d":"show_main_feature_metrics(data, 'color')","442d3811":"data['description'][0]","d9531a09":"show_main_feature_metrics(data, 'engineDisplacement')","33a853de":"data[data['engineDisplacement'] == 'undefined LTR']","a728718e":"get_boxplot(data,'engineDisplacement')","fbeb95c2":"data[data['engineDisplacement'] == 'undefined LTR']","c71a6d48":"data[(data['engineDisplacement']=='undefined LTR') & (data['sample'] == 0)]","bf020683":"data.loc[data['engineDisplacement'] == 'undefined LTR', 'engineDisplacement'] = '1.3 LTR'","40c79fed":"data['engineDisplacement'] = data['engineDisplacement'].str.split().apply(\n    lambda s: s[0]\n)\n\ndata['engineDisplacement'] = data['engineDisplacement'].apply(\n    lambda s: float(s)\n)\n\ndata['engineDisplacement'] = data['engineDisplacement']*1000","11f1e1f5":"data['engineDisplacement'].hist()","544c4af5":"np.log(data['engineDisplacement']).hist()","b35289c0":"data['enginePower'].value_counts()","176e5629":"data['enginePower'] = data['enginePower'].apply(lambda x : (int)(x.split(' ')[0]))","83938833":"data['enginePower'].hist(bins=50)","2f540e7a":"np.log(data['enginePower']).hist(bins=50)","e8bdb6ec":"data['modelDate'].hist()","e3b426a8":"data['modelDate'] = 2021 - data['modelDate']","f45a7441":"np.log(data['modelDate']).hist()","a517d124":"data['model_info'].value_counts()","d67e3b95":"data[data['model_info'] == '100']","f5eebc69":"show_main_feature_metrics(data, 'model_info')","33128678":"first_25_model_names = list(data.model_info.value_counts(ascending=False)[:25].index)\nfirst_25_models_df = data[data.model_info.isin(first_25_model_names)]\nlast_25_models_df = list(data.model_info.value_counts(ascending=True)[:25].index)\nlast_25_models_df = data[data.model_info.isin(last_25_models_df)]\nshow_barplot(first_25_models_df, 'model_info')\nshow_barplot(last_25_models_df, 'model_info')","c2645ed7":"for t in data[data['model_info'] == 'CITAN'].description.str.split():\n    print(t)","04de9258":"for t in data[data['model_info'] == '100'].description.str.split():\n    print(t)","0378963e":"for t in data[data['model_info'] == 'COUPE'].description.str.split():\n    print(t)","42b6be6c":"data.loc[data['model_info'] == '100', 'model_info'] = 'S4'\ndata.loc[data['model_info'] == 'COUPE', 'model_info'] = 'S2'\ndata.loc[data['model_info'] == 'None', 'model_info'] = 'C_KLASSE_AMG'","b56bdb48":"data['numberOfDoors'].hist()","4c30c04e":"show_main_feature_metrics(data, 'numberOfDoors')","6588d819":"data['productionDate'].hist()","94fc39a6":"data['productionDate'] = 2021 - data['productionDate']","59190ace":"np.log(data['productionDate']).hist()","b23add62":"data['vehicleConfiguration'].value_counts()","cde33943":"data['vehicleTransmission'].hist()","de0627d8":"data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].hist()","ee5a13a1":"data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'] = data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].apply(lambda x : owners_count(x))","3bcf928f":"data['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'].value_counts(dropna=False)","50aaf6e2":"data['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'] = data['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'].apply(lambda x : owner_age(x))","3f0134a1":"data['\u041f\u0422\u0421'].hist()","68af7fbd":"passport_dict = {\n    '\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442': 0,\n    '\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b': 1,\n}\n\ndata['\u041f\u0422\u0421'] = data['\u041f\u0422\u0421'].map(passport_dict)","d52ac547":"data['\u041f\u0422\u0421'].value_counts()","d21a2ae5":"data['\u041f\u0440\u0438\u0432\u043e\u0434'].hist()","1ba18cb3":"data['\u0420\u0443\u043b\u044c'].value_counts()","e7cee57c":"data[(data['\u0420\u0443\u043b\u044c']=='\u041f\u0440\u0430\u0432\u044b\u0439')&(data['sample'] == 0)]","1b6bd8d7":"#data.drop(index=data[data['\u0420\u0443\u043b\u044c']=='\u041f\u0440\u0430\u0432\u044b\u0439'].index, axis=0,inplace=True)","85460410":"train['price'].hist(bins=100)","23e5662c":"# Checking the correlation matrix.\nplt.subplots(figsize=(8,6))\nsns.heatmap(data.corr().abs(), vmin=0, vmax=1)","47db37ea":"#\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u0430\u043a \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438\ncategorical_features = ['bodyType', 'brand', 'color',  'fuelType','model_info', '\u041f\u0422\u0421', '\u041f\u0440\u0438\u0432\u043e\u0434','vehicleTransmission']\n\n#\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\nnumerical_features = ['mileage', 'modelDate', 'productionDate','engineDisplacement','enginePower','\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b','numberOfDoors'] ","8de906f7":"data.drop(columns=['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435','name','vehicleConfiguration','\u0420\u0443\u043b\u044c'], inplace=True)","87a2015a":"# Checking the frequency distribution.\nfor col in categorical_features:\n    get_boxplot(data,col)","3a9c5c8d":"from itertools import combinations\nfrom scipy.stats import ttest_ind\n# Defining Student's test function.\ndef get_stat_dif(df,column):\n    cols = df.loc[:, column].value_counts().index[:10]\n    combinations_all = list(combinations(cols, 2))\n    for comb in combinations_all:\n        if ttest_ind(df.loc[df.loc[:, column] == comb[0], 'price'], \n                        df.loc[train.loc[:, column] == comb[1], 'price']).pvalue \\\n            <= 0.05\/len(combinations_all):\n            print('Statistically significant differences were found for the column', column)\n            break","9020375c":"# Using the Student's test.\nfor col in categorical_features:\n    get_stat_dif(train,col)","50c648d5":"# Checking the correlation matrix.\ndata.corr().abs().sort_values(by='price', ascending=False)","4e49169d":"#for column in numerical_features:\n#    print(column,data[column].value_counts())","10120b66":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df = df_input.copy()\n    \n    df['description'] = df['description'].fillna('[]')\n    df['description_len'] = df['description'].apply(lambda x: len(x.split()))\n    df['description_word'] = df['description'].apply(lambda x: [str(i).lower() for i in x.split()])\n    #bins = [10, 35, 100, 125, 150, 175, 200, 225, 250, 801]\n    #numbers = ['5000','12','25','35', '45', '50', '65', '75', '150']\n    \n    #df['nalog']=pd.cut(df['enginePower'].apply(lambda x : x.split(' ')[0]),bins,labels=numbers)\n    #df.loc[pd.isna(df.nalog),'nalog'] = '12'\n    df['leather']= df['description_word'].apply(lambda x: \n                                                1 if ('\u0442\u0435\u043c\u043d\u044b\u0439' and '\u0441\u0430\u043b\u043e\u043d') in x else 0)\n    df['carter']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0437\u0430\u0449\u0438\u0442\u0430' and '\u043a\u0430\u0440\u0442\u0435\u0440\u0430') in x else 0)\n    df['ABS']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0430\u043d\u0442\u0438\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0430\u044f' and '\u0441\u0438\u0441\u0442\u0435\u043c\u0430') in x else 0)\n    df['airbags']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043f\u043e\u0434\u0443\u0448\u043a\u0438' and '\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438') in x else 0)\n    df['immob']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0438\u043c\u043c\u043e\u0431\u0438\u043b\u0430\u0439\u0437\u0435\u0440') in x else 0)\n    df['central_locking']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439' and '\u0437\u0430\u043c\u043e\u043a') in x else 0)\n    df['on_board_computer']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0431\u043e\u0440\u0442\u043e\u0432\u043e\u0439' and '\u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440') in x else 0)\n    df['cruise_control']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043a\u0440\u0443\u0438\u0437-\u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c') in x else 0)\n    df['climat_control']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043a\u043b\u0438\u043c\u0430\u0442-\u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c') in x else 0)\n    df['multi_rudder']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043c\u0443\u043b\u044c\u0442\u0438\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439' and '\u0440\u0443\u043b\u044c') in x else 0)\n    df['power_steering']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0433\u0438\u0434\u0440\u043e\u0443\u0441\u0438\u043b\u0438\u0442\u0435\u043b\u044c' or '\u0433\u0438\u0434\u0440\u043e' or '\u0443\u0441\u0438\u043b\u0438\u0442\u0435\u043b\u044c' and '\u0440\u0443\u043b\u044f') in x else 0)\n    df['light_and_rain_sensors']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0434\u0430\u0442\u0447\u0438\u043a\u0438' and '\u0441\u0432\u0435\u0442\u0430' and '\u0434\u043e\u0436\u0434\u044f') in x else 0)\n    df['\u0441arbon_body_kits']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043a\u0430\u0440\u0431\u043e\u043d\u043e\u0432\u044b\u0435' and '\u043e\u0431\u0432\u0435\u0441\u044b') in x else 0)\n    df['rear_diffuser_rkp']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0437\u0430\u0434\u043d\u0438\u0439' and '\u0434\u0438\u0444\u0444\u0443\u0437\u043e\u0440') in x else 0)\n    df['door_closers']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0434\u043e\u0432\u043e\u0434\u0447\u0438\u043a\u0438' and '\u0434\u0432\u0435\u0440\u0435\u0439') in x else 0)\n    df['rear_view_camera']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043a\u0430\u043c\u0435\u0440\u0430' or '\u0432\u0438\u0434\u0435\u043e\u043a\u0430\u043c\u0435\u0440\u0430' and '\u0437\u0430\u0434\u043d\u0435\u0433\u043e' and '\u0432\u0438\u0434\u0430') in x else 0)\n    df['amg']= df['description_word'].apply(lambda x: \n                                                    1 if ('amg') in x else 0)\n    df['bi_xenon_headlights']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0431\u0438\u043a\u0441\u0435\u043d\u043e\u043d\u043e\u0432\u044b\u0435' and '\u0444\u0430\u0440\u044b') in x else 0)\n    df['from_salon']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0440\u043e\u043b\u044c\u0444' or '\u043f\u0430\u043d\u0430\u0432\u0442\u043e' or '\u0434\u0438\u043b\u0435\u0440' or '\u043a\u0440\u0435\u0434\u0438\u0442' or '\u043b\u0438\u043a\u0432\u0438\u0434\u0430\u0446\u0438\u044f') in x else 0)\n    df['alloy_wheels']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043b\u0435\u0433\u043a\u043e\u0441\u043f\u043b\u0430\u0432\u043d\u044b\u0435' or '\u043a\u043e\u043b\u0435\u0441\u043d\u044b\u0435' or '\u0434\u0438\u0441\u043a\u0438') in x else 0)\n    df['parking_sensors']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043f\u0430\u0440\u043a\u0442\u0440\u043e\u043d\u0438\u043a' or '\u043f\u0430\u0440\u043a\u0442\u0440\u043e\u043d\u043d\u0438\u043a') in x else 0)\n    df['dents']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u0432\u043c\u044f\u0442\u0438\u043d\u044b' or '\u0432\u043c\u044f\u0442\u0438\u043d\u0430' or '\u0446\u0430\u0440\u0430\u043f\u0438\u043d\u0430' or '\u0446\u0430\u0440\u0430\u043f\u0438\u043d\u044b' or '\u0442\u0440\u0435\u0449\u0438\u043d\u0430') in x else 0)\n    df['roof_with_panoramic_view']= df['description_word'].apply(lambda x: \n                                                    1 if ('\u043f\u0430\u043d\u043e\u0440\u0430\u043c\u043d\u0430\u044f' and '\u043a\u0440\u044b\u0448\u0430') in x else 0)\n\n    # \u041e\u0431\u0438\u0432\u043a\u0430 \u0441\u0430\u043b\u043e\u043d\u0430\n    df['upholstery'] = df['description_word'].apply(lambda x: 1 if ('\u043e\u0431\u0438\u0432\u043a\u0430' and '\u0441\u0430\u043b\u043e\u043d') in x else 0)\n\n    # \u041f\u043e\u0434\u043e\u0433\u0440\u0435\u0432 \u0441\u0438\u0434\u0435\u043d\u0438\u044f\n    df['heated_seat'] = df['description_word'].apply(lambda x: 1 if ('\u043f\u043e\u0434\u043e\u0433\u0440\u0435\u0432' and '\u0441\u0438\u0434\u0435\u043d\u0438\u0435') in x else 0)\n\n    # \u0414\u0430\u0442\u0447\u0438\u043a \u0434\u043e\u0436\u0434\u044f\n    df['rain_sensor'] = df['description_word'].apply(lambda x: 1 if ('\u0434\u0430\u0442\u0447\u0438\u043a' and '\u0434\u043e\u0436\u0434\u044c') in x else 0)\n\n    # \u041e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u043b\u043b\u0435\u0440\n    df['official_dealer'] = df['description_word'].apply(lambda x: 1 if ('\u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439' and '\u0434\u0438\u043b\u043b\u0435\u0440') in x else 0)\n\n    # \u0425\u043e\u0440\u043e\u0448\u0435\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n    df['good_condition'] = df['description_word'].apply(lambda x: 1 if ('\u0445\u043e\u0440\u043e\u0448\u0438\u0439' and '\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435') in x else 0)\n\n    # \u041e\u0442\u043b\u0438\u0447\u043d\u043e\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n    df['excellent_condition'] = df['description_word'].apply(lambda x: 1 if ('\u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0439' and '\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435') in x else 0)\n    # ################### 1. \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 ############################################################## \n    \n    \n    #df['enginePower'] = np.log(df['enginePower'])\n    #df['mileage'] = np.log(df['mileage'])\n    #df['engineDisplacement'] = np.log(df['enginePower'])\n    #df['modelDate'] = np.log(df['mileage'])\n    #df['productionDate'] = np.log(df['mileage'])\n    \n    \n    df.drop(['description','description_len','description_word','sell_id'], axis = 1, inplace=True) \n    # ################### Numerical Features ############################################################## \n    # \u0414\u0430\u043b\u0435\u0435 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438\n    for column in numerical_features:\n        df[column].fillna(df[column].median(), inplace=True)\n\n    \n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\n    scaler = MinMaxScaler()\n    for column in numerical_features:\n        df[column] = scaler.fit_transform(df[[column]])[:,0]\n    \n    \n    \n    # ################### Categorical Features ############################################################## \n    # Label Encoding\n    for column in categorical_features:\n        df[column] = df[column].astype('category').cat.codes\n        \n    # One-Hot Encoding: \u0432 pandas \u0435\u0441\u0442\u044c \u0433\u043e\u0442\u043e\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f - get_dummies.\n    df = pd.get_dummies(df, columns=categorical_features, dummy_na=False)\n    \n \n    return df\n\n","f814c976":"# \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c, \u0447\u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c\ndf_preproc = preproc_data(data)\ndf_preproc.sample(10)\n#for column in df_preproc.columns:\n#    print(df_preproc[column].value_counts())","fdc22444":"df_preproc.info()","d41fa44d":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['price'], axis=1).values\nX_sub = test_data.drop(['price'], axis=1)\nprint(len(X_sub))\nprint(len(sample_submission))","90d66998":"test_data.info()","ed5e61a5":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=CFG.SEED\n)","418ba4ed":"model = CatBoostRegressor(iterations = 1200,\n                          depth=10,\n                          learning_rate = 0.05,\n                          random_seed = CFG.SEED,\n                          eval_metric='MAPE',\n                          custom_metric=['RMSE', 'MAE'],\n                          od_wait=500,\n                          od_type='IncToDec'\n                         )\nmskf = KFold(n_splits=CFG.N_FOLDS, random_state=CFG.SEED, shuffle=True)\n\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(X, y)):\n    print(\"FOLDS : \", fold_nb)\n    ## model\n    X_train, y_train = X[train_idx, :], y[train_idx]\n    X_val, y_val = X[val_idx, :], y[val_idx]\n    model.fit(X_train, y_train,\n             eval_set=(X_val, y_val),\n             verbose_eval=100,\n             use_best_model=True)\n","0400b4f4":"test_predict_catboost = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_catboost))*100:0.2f}%\")","816c35b7":"sub_predict_catboost = model.predict(X_sub)\nsample_submission['price'] = sub_predict_catboost\nsample_submission.to_csv('catboost_submission.csv', index=False)","94dab791":"y = train_data.price.values   #\u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\ny = y.reshape(-1,1)\nX = train_data.drop(['price'], axis=1).values\n#X_sub = test_data.values\nX_sub = test_data.drop(['price'], axis=1)","c9fbbba1":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n#optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n\n#scheduler_params=dict(mode=\"min\",mask_type='entmax',patience=5,min_lr=1e-4, factor=0.9,),\n#  gamma=1.3,lambda_sparse=0,n_d=24, n_a=24,\n\nMAX_EPOCH=200\ntabnet_params = dict( n_steps=10,optimizer_fn=torch.optim.Adam,optimizer_params=dict(lr=0.0001 ),                     \n                     scheduler_params=dict(mode=\"min\",patience=5,min_lr=1e-4),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     device_name = DEVICE\n                     )","d4af69d8":"def MAPELoss(output, target):\n    return torch.mean(torch.abs((target - output) \/ target))    ","ae7eecdc":"from pytorch_tabnet.metrics import Metric\n\nclass MAPE(Metric):\n    def __init__(self):\n        self._name = \"mape\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n\n        return self.mean_absolute_percentage_error(y_true, y_score)\n    \n    def percentage_error(self, actual, predicted):\n        res = np.empty(actual.shape)\n        for j in range(actual.shape[0]):\n            if actual[j] != 0:\n                res[j] = (actual[j] - predicted[j]) \/ actual[j]\n            else:\n                res[j] = predicted[j] \/ np.mean(actual)\n        return res\n\n    def mean_absolute_percentage_error(self, y_true, y_pred): \n        return np.mean(np.abs(self.percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n","b593a488":"'''\nscores_mape_all= []\ntest_cv_preds = []\n\nmskf = StratifiedKFold(n_splits=CFG.N_FOLDS, random_state=CFG.SEED, shuffle=True)\noof_preds = []\noof_targets = []\nscores = []\nscores_mape = []\nmodel = TabNetRegressor(**tabnet_params)\n\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(X, y)):\n    print(\"FOLDS : \", fold_nb)\n\n    ## model\n    X_train, y_train = X[train_idx, :], y[train_idx]\n    X_val, y_val = X[val_idx, :], y[val_idx]\n    \n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [MAPE],\n              max_epochs=MAX_EPOCH,\n              patience=20, \n              batch_size=1024, \n              virtual_batch_size=128,\n              num_workers=8, \n              drop_last=False)#,\n              # use binary cross entropy as this is not a regression problem\n              #torch.nn.functional.binary_cross_entropy_with_logits\n              #loss_fn=MAPELoss)\n\n    preds_val = model.predict(X_val)\n\n    score = np.min(model.history[\"val_mape\"])\n\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(preds_test)\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)\n'''","f57e1844":"#test_predict_tabnet = model.predict(X_test)\n#print(f\"TEST mape: {(mape(y_test, test_predict_tabnet))*100:0.2f}%\")","3a2175d5":"#sub_predict_tabnet = model.predict(X_sub)\n#sample_submission['price'] = sub_predict_tabnet\n#sample_submission.to_csv('tabnet_submission.csv', index=False)","67b0fdbb":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['price'], axis=1).values\nX_sub = test_data.drop(['price'], axis=1)","66cfdaa2":"model = Sequential()\nmodel.add(L.Dense(512, input_dim=X_train.shape[1],\n                 kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, \n        l2=0.000000001\n    ),activation=\"relu\" ))\nmodel.add(L.Dropout(0.5))\nmodel.add(L.Dense(256,\n          kernel_regularizer=regularizers.l1_l2(\n        l1=0.000000001, \n        l2=0.000000001\n    ),activation=\"relu\"))\nmodel.add(L.Dropout(0.5))\nmodel.add(L.Dense(1, activation=\"linear\"))","af9a21e7":"model.summary()","e671202d":"# Compile model\noptimizer = tf.keras.optimizers.Adam(0.0005)\noptimizer = tfa.optimizers.SWA(optimizer)\n# \u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b, \u0442\u0430\u043a \u043b\u0443\u0447\u0448\u0435 \u043d\u0435 \u0434\u0435\u043b\u0430\u0442\u044c\n#optimizer = tf.keras.optimizers.SGD(0.0001)\n\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","fdba88c3":"def make_callbacks():\n    \n    callback_early_stopping = EarlyStopping(monitor='val_MAPE',patience=10, verbose=1,restore_best_weights=True)\n    callback_reduce_lr = ReduceLROnPlateau(monitor='val_MAPE',factor=0.5,min_lr=1e-10,patience=0,verbose=1)\n    callback_learing_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x),\n    path_checkpoint = 'checkpoint.keras'\n    callback_checkpoint = ModelCheckpoint('..\/working\/best_model.hdf5' , monitor=['val_MAPE'], verbose=1, mode='min') #ModelCheckpoint(filepath=path_checkpoint,monitor='val_loss',verbose=1,save_weights_only=True, save_best_only=True)\n\n    return [callback_checkpoint, callback_learing_rate, callback_reduce_lr]\n\ncallbacks_list = make_callbacks()","c0ac2b9d":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['price'], axis=1).values\nX_sub = test_data.drop(['price'], axis=1)","11c00d03":"for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(X, y)):\n    print(\"FOLDS : \", fold_nb)\n\n    X_train, y_train = X[train_idx, :], y[train_idx]\n    X_val, y_val = X[val_idx, :], y[val_idx]\n    history = model.fit(X_train, y_train,\n                        batch_size=512,\n                        epochs=1000, # \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043a\u0430 EarlyStopping \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n                        validation_data=(X_test,  y_test),\n                        callbacks=callbacks_list,\n                        verbose=1,\n                       )","6f6e8f37":"plt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show()","de9706a1":"model.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_1.hdf5')","d535eb62":"test_predict_nn1 = model.predict(X_test)\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn1[:,0]))*100:0.2f}%\")","706ed760":"sub_predict_nn1 = model.predict(X_sub)\nsample_submission['price'] = sub_predict_nn1[:,0]\nsample_submission.to_csv('nn1_submission.csv', index=False)","0791019b":"# Preparing data for training.\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.price.values\nX = train_data.drop(['price'], axis=1)\nX_sub = test_data.drop(['price'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=CFG.SEED\n)\n","4f34392e":"# Defining the lemmatization function.\nmorph = pymorphy2.MorphAnalyzer()\ndf_NLP = data.copy()\n\npatterns = \"[A-Za-z0-9!#$%&'()*+,.\/:;<=>?@[\\]^_`{|}~\u2014\\\"\\-]+\"\n\ndef lemmatize(doc):\n    doc = re.sub(patterns, ' ', doc)\n    tokens = []\n    for token in doc.split():\n        token = token.strip()\n        token = morph.normal_forms(token)[0]\n        tokens.append(token)\n    return ' '.join(tokens)","a3963d16":"# Applying lemmatization.\ndf_NLP['description'] = df_NLP.apply(\n    lambda df_NLP: lemmatize(df_NLP.description), axis=1)","7d2cd647":"data.description","eaa63316":"# TOKENIZER\n# The maximum number of words to be used. (most frequent)\nMAX_WORDS = 100000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 256\n\n\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(data.description)","08013a0a":"# split \u0434\u0430\u043d\u043d\u044b\u0445\ntext_train = data.description.iloc[X_train.index]\ntext_test = data.description.iloc[X_test.index]\ntext_sub = data.description.iloc[X_sub.index]","b7ec4660":"tokenize.word_index","afca8422":"text_train_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_train), maxlen=MAX_SEQUENCE_LENGTH)\ntext_test_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_test), maxlen=MAX_SEQUENCE_LENGTH)\ntext_sub_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_sub), maxlen=MAX_SEQUENCE_LENGTH)\n\nprint(text_train_sequences.shape, text_test_sequences.shape, text_sub_sequences.shape, )","831fb13a":"# \u0432\u043e\u0442 \u0442\u0430\u043a \u0442\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043d\u0430\u0448 \u0442\u0435\u043a\u0441\u0442\nprint(text_train.iloc[6])\nprint(text_train_sequences[6])","1d55e9bd":"model_nlp = Sequential()\nmodel_nlp.add(L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"))\nmodel_nlp.add(L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,))\nmodel_nlp.add(L.LSTM(256, return_sequences=True))\nmodel_nlp.add(L.Dropout(0.5))\nmodel_nlp.add(L.LSTM(128,))\nmodel_nlp.add(L.Dropout(0.25))\nmodel_nlp.add(L.Dense(64, activation=\"relu\"))\nmodel_nlp.add(L.Dropout(0.25))","2cdd2e4b":"model_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","419236b2":"combinedInput = L.concatenate([model_nlp.output, model_mlp.output])\n# being our regression head\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[model_nlp.input, model_mlp.input], outputs=head)","284c0faa":"model.summary()","18081190":"optimizer = tf.keras.optimizers.Adam(0.0001)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","41f47598":"callbacks_list = make_callbacks()","acada339":"history = model.fit([text_train_sequences, X_train], y_train,\n                    batch_size=512,\n                    epochs=500, # \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043a\u0430 EarlyStopping \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n                    validation_data=([text_test_sequences, X_test], y_test),\n                    callbacks=callbacks_list\n                   )","61877a60":"plt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show()","110b4a1d":"model.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_mlp_nlp.hdf5')","044dc365":"test_predict_nn2 = model.predict([text_test_sequences, X_test])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn2[:,0]))*100:0.2f}%\")","30ab2215":"sub_predict_nn2 = model.predict([text_sub_sequences, X_sub])\nsample_submission['price'] = sub_predict_nn2[:,0]\nsample_submission.to_csv('nn2_submission.csv', index=False)","ae301188":"# \u0443\u0431\u0435\u0434\u0438\u043c\u0441\u044f, \u0447\u0442\u043e \u0446\u0435\u043d\u044b \u0438 \u0444\u043e\u0442\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043b\u0438\u0441\u044c \u0432\u0435\u0440\u043d\u043e\nplt.figure(figsize = (12,8))\n\nrandom_image = train.sample(n = 9)\nrandom_image_paths = random_image['sell_id'].values\nrandom_image_cat = random_image['price'].values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(DATA_DIR+'img\/img\/' + str(path) + '.jpg')\n    plt.subplot(3, 3, index + 1)\n    plt.imshow(im)\n    plt.title('price: ' + str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","6fd401c4":"size = (320, 240)\n\ndef get_image_array(index):\n    images_train = []\n    for index, sell_id in enumerate(data['sell_id'].iloc[index].values):\n        image = cv2.imread(DATA_DIR + 'img\/img\/' + str(sell_id) + '.jpg')\n        assert(image is not None)\n        image = cv2.resize(image, size)\n        images_train.append(image)\n    images_train = np.array(images_train)\n    print('images shape', images_train.shape, 'dtype', images_train.dtype)\n    return(images_train)\n\nimages_train = get_image_array(X_train.index)\nimages_test = get_image_array(X_test.index)\nimages_sub = get_image_array(X_sub.index)","d343c833":"from albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n)\n\n\n#\u043f\u0440\u0438\u043c\u0435\u0440 \u0432\u0437\u044f\u0442 \u0438\u0437 \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438: https:\/\/albumentations.readthedocs.io\/en\/latest\/examples.html\naugmentation = Compose([\n    HorizontalFlip(),\n    OneOf([\n        IAAAdditiveGaussianNoise(),\n        GaussNoise(),\n    ], p=0.2),\n    OneOf([\n        MotionBlur(p=0.2),\n        MedianBlur(blur_limit=3, p=0.1),\n        Blur(blur_limit=3, p=0.1),\n    ], p=0.2),\n    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=1),\n    OneOf([\n        OpticalDistortion(p=0.3),\n        GridDistortion(p=0.1),\n        IAAPiecewiseAffine(p=0.3),\n    ], p=0.2),\n    OneOf([\n        CLAHE(clip_limit=2),\n        IAASharpen(),\n        IAAEmboss(),\n        RandomBrightnessContrast(),\n    ], p=0.3),\n    HueSaturationValue(p=0.3),\n], p=1)\n\n#\u043f\u0440\u0438\u043c\u0435\u0440\nplt.figure(figsize = (12,8))\nfor i in range(9):\n    img = augmentation(image = images_train[0])['image']\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","eb7a5289":"def make_augmentations(images):\n    print('\u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439', end = '')\n    augmented_images = np.empty(images.shape)\n    for i in range(images.shape[0]):\n        if i % 200 == 0:\n            print('.', end = '')\n        augment_dict = augmentation(image = images[i])\n        augmented_image = augment_dict['image']\n        augmented_images[i] = augmented_image\n        print('')\n    return augmented_images","a993a7c5":"# NLP part\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(data.description)","0e809ee8":"def process_image(image):\n    return augmentation(image = image.numpy())['image']\n\ndef tokenize_(descriptions):\n    return sequence.pad_sequences(tokenize.texts_to_sequences(descriptions), maxlen = MAX_SEQUENCE_LENGTH)\n\ndef tokenize_text(text):\n    return tokenize_([text.numpy().decode('utf-8')])[0]\n\ndef tf_process_train_dataset_element(image, table_data, text, price):\n    im_shape = image.shape\n    [image,] = tf.py_function(process_image, [image], [tf.uint8])\n    image.set_shape(im_shape)\n    [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\ndef tf_process_val_dataset_element(image, table_data, text, price):\n    [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text), price\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    images_train, X_train, data.description.iloc[X_train.index], y_train\n    )).map(tf_process_train_dataset_element)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    images_test, X_test, data.description.iloc[X_test.index], y_test\n    )).map(tf_process_val_dataset_element)\n\ny_sub = np.zeros(len(X_sub))\nsub_dataset = tf.data.Dataset.from_tensor_slices((\n    images_sub, X_sub, data.description.iloc[X_sub.index], y_sub\n    )).map(tf_process_val_dataset_element)\n\n#\u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c, \u0447\u0442\u043e \u043d\u0435\u0442 \u043e\u0448\u0438\u0431\u043e\u043a (\u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0431\u0440\u043e\u0448\u0435\u043d\u043e \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435):\ntrain_dataset.__iter__().__next__();\ntest_dataset.__iter__().__next__();\nsub_dataset.__iter__().__next__();","12cbd019":"#\u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0430 \u0432 \u0441\u043e\u0441\u0442\u0430\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 EfficientNetB3, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430 \u0432\u0445\u043e\u0434 \u043e\u043d\u0430 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0442\u0438\u043f\u0430 uint8\nefficientnet_model = tf.keras.applications.efficientnet.EfficientNetB3(weights = 'imagenet', include_top = False, input_shape = (size[1], size[0], 3))\nefficientnet_output = L.GlobalAveragePooling2D()(efficientnet_model.output)","05dbdac6":"#\u0441\u0442\u0440\u043e\u0438\u043c \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntabular_model = Sequential([\n    L.Input(shape = X.shape[1]),\n    L.Dense(512, activation = 'relu'),\n    L.Dropout(0.5),\n    L.Dense(256, activation = 'relu'),\n    L.Dropout(0.5),\n    ])","72f730d9":"# NLP\nnlp_model = Sequential([\n    L.Input(shape=MAX_SEQUENCE_LENGTH, name=\"seq_description\"),\n    L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,),\n    L.LSTM(256, return_sequences=True),\n    L.Dropout(0.5),\n    L.LSTM(128),\n    L.Dropout(0.25),\n    L.Dense(64),\n    ])","21723ee0":"#\u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432\u044b\u0445\u043e\u0434\u044b \u0442\u0440\u0435\u0445 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0439\ncombinedInput = L.concatenate([efficientnet_output, tabular_model.output, nlp_model.output])\n\n# being our regression head\nhead = L.Dense(256, activation=\"relu\")(combinedInput)\nhead = L.Dense(1,)(head)\n\nmodel = Model(inputs=[efficientnet_model.input, tabular_model.input, nlp_model.input], outputs=head)\nmodel.summary()","363650fb":"optimizer = tf.keras.optimizers.Adam(0.005)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","4e6af997":"checkpoint = ModelCheckpoint('..\/working\/best_model.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')\nearlystop = EarlyStopping(monitor='val_MAPE', patience=10, restore_best_weights=True,)\ncallbacks_list = make_callbacks()","1dbaa641":"history = model.fit(train_dataset.batch(30),\n                    epochs=100,\n                    validation_data = test_dataset.batch(30),\n                    callbacks=callbacks_list\n                   )","a1734279":"plt.title('Loss')\nplt.plot(history.history['MAPE'], label='train')\nplt.plot(history.history['val_MAPE'], label='test')\nplt.show();","8ad289c7":"model.load_weights('..\/working\/best_model.hdf5')\nmodel.save('..\/working\/nn_final.hdf5')","b37a9fb5":"test_predict_nn3 = model.predict(test_dataset.batch(30))\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn3[:,0]))*100:0.2f}%\")","731b9f06":"sub_predict_nn3 = model.predict(sub_dataset.batch(30))\nsample_submission['price'] = sub_predict_nn3[:,0]\nsample_submission.to_csv('nn3_submission.csv', index=False)","8172cfe4":"blend_predict = (test_predict_catboost + test_predict_nn3[:,0]) \/ 2\nprint(f\"TEST mape: {(mape(y_test, blend_predict))*100:0.2f}%\")","110f459d":"blend_sub_predict = (sub_predict_catboost + sub_predict_nn3[:,0]) \/ 2\nsample_submission['price'] = blend_sub_predict\nsample_submission.to_csv('blend_submission.csv', index=False)","1d454f66":"# MLP\nmodel_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","a205761c":"# FEATURE Input\n# Iput\nproductiondate = L.Input(shape=[1], name=\"productiondate\")\n# Embeddings layers\nemb_productiondate = L.Embedding(len(X.productionDate.unique().tolist())+1, 20)(productiondate)\nf_productiondate = L.Flatten()(emb_productiondate)","0bc9be99":"combinedInput = L.concatenate([model_mlp.output, f_productiondate,])\n# being our regression head\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[model_mlp.input, productiondate], outputs=head)","f464fd88":"model.summary()","359f5502":"optimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","a4e6bbb9":"history = model.fit([X_train, X_train.productionDate.values], y_train,\n                    batch_size=512,\n                    epochs=500, # \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043a\u0430 EarlyStopping \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n                    validation_data=([X_test, X_test.productionDate.values], y_test),\n                    callbacks=callbacks_list\n                   )","1561fbd4":"model.load_weights('..\/working\/best_model.hdf5')\ntest_predict_nn_bonus = model.predict([X_test, X_test.productionDate.values])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn_bonus[:,0]))*100:0.2f}%\")","e55087ba":"# ","05ec8dec":"> Something went wrong, MAPE = 100%","23df2d5b":"# DATA","d5c2fb31":"## NumberOfDoors","e6567b84":"\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f Model 3:    \n* \u0412 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c, \u0431\u043b\u0438\u0437\u043a\u0438\u043c \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u043c\u0443, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438\u043c\u0435\u0435\u0442 \u0441\u043c\u044b\u0441\u043b \u0432\u0437\u044f\u0442\u044c \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c \u043f\u0435\u0440\u0435\u0434 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439. \u041f\u0440\u0438\u043c\u0435\u0440:\n`modelDateNorm = np.log(2020 - data['modelDate'])`\n\u0421\u0442\u0430\u0442\u044c\u044f \u043f\u043e \u0442\u0435\u043c\u0435: https:\/\/habr.com\/ru\/company\/ods\/blog\/325422\n\n* \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430:\n\u041f\u0430\u0440\u0441\u0438\u043d\u0433 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 'engineDisplacement', 'enginePower', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435' \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.\n\n* C\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\u041f\u0440\u0438\u0437\u043d\u0430\u043a name 'name' \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0434\u0430\u043d\u043d\u044b\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0436\u0435 \u0435\u0441\u0442\u044c \u0432 \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445 ('enginePower', 'engineDisplacement', 'vehicleTransmission'), \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u0443\u0434\u0430\u043b\u0438\u0442\u044c. \u0417\u0430\u0442\u0435\u043c \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0435\u0449\u0435 \u0441\u0438\u043b\u044c\u043d\u0435\u0435 \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0432\u044b\u0434\u0435\u043b\u0438\u0432 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 xDrive \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.","89419123":"## Description","7ab4c45a":"## Split data","83005bdf":"## Engine Displacement","14f2e9a3":"# Utils EDA","863b4cc5":"### albumentations","4453c45b":"### Fit","107d76f5":"## \u041f\u0422\u0421","2689f751":"## Price","2d32f013":"# Model 4: NLP + Multiple Inputs","0264a9e2":"> \u0412\u0441\u0435 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438 \u0441 'undefined LTR' - \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043a\u0430\u0440\u044b, \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0446\u0435\u043d\u044b ","54db2244":"### Tokenizer","2574cb83":"> \u0414\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u0442\u0443\u043c \u0438\u043d\u0444\u043e\u043c\u0430\u0440\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0443\u0436\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432","ee74249d":"# EDA","91e2e914":"## Engine Power","ce1da541":"### Fit","ad507921":"## \u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b","1e6bc78b":"> \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c log-\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c","a0828961":"# Model 2: CatBoostRegressor","204c0f35":"> \u0425\u043e\u0442\u0435\u043b \u0431\u044b \u0443\u0434\u0430\u043b\u0438\u0442\u044c, \u043d\u043e \u0440\u0430\u0437 \u0435\u0441\u0442\u044c \u0432 \u0442\u0435\u0441\u0442\u0435, \u0437\u043d\u0430\u0447\u0438\u0442 \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0437\u0430\u043c\u0435\u043d\u044f\u044f \u043d\u0430 \u043f\u043e\u0445\u043e\u0436\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 (\u0441\u0443\u0434\u044f \u043f\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0443 \u0432\u044b\u0448\u043a\u0443)","7e81561f":"## \u0420\u0443\u043b\u044c","b1248d6c":"## tf.data.Dataset\n\u0415\u0441\u043b\u0438 \u0432\u0441\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0432 \u043f\u0430\u043c\u044f\u0442\u0438, \u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0432\u043e\u0437\u043d\u0438\u043a\u043d\u0443\u0442\u044c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0435\u0435 \u043d\u0435\u0445\u0432\u0430\u0442\u043a\u0438. \u041d\u0435 \u0445\u0440\u0430\u043d\u0438\u0442\u0435 \u0432\u0441\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u043f\u0430\u043c\u044f\u0442\u0438 \u0446\u0435\u043b\u0438\u043a\u043e\u043c!\n\n\u041c\u0435\u0442\u043e\u0434 .fit() \u043c\u043e\u0434\u0435\u043b\u0438 keras \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043b\u0438\u0431\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0432\u0438\u0434\u0435 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u0432 \u0438\u043b\u0438 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432, \u043b\u0438\u0431\u043e \u0440\u0430\u0437\u043d\u043e\u0433\u043e \u0440\u043e\u0434\u0430 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440\u044b, \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c \u0438 \u0433\u0438\u0431\u043a\u0438\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f [tf.data.Dataset](https:\/\/www.tensorflow.org\/guide\/data). \u041e\u043d \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440, \u0442\u043e \u0435\u0441\u0442\u044c \u043c\u044b \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c, \u043e\u0442\u043a\u0443\u0434\u0430 \u0431\u0435\u0440\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043a\u0430\u043a\u0443\u044e \u0446\u0435\u043f\u043e\u0447\u043a\u0443 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0439 \u0441 \u043d\u0438\u043c\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c. \u0414\u0430\u043b\u0435\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 tf.data.Dataset.\n\nDataset \u0445\u0440\u0430\u043d\u0438\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c \u0438\u043b\u0438 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439 (tuple) \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u043c\u043e\u0436\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u044d\u0442\u0438 \u043d\u0430\u0431\u043e\u0440\u044b \u043f\u043e \u043e\u0447\u0435\u0440\u0435\u0434\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u0430\u0440\u044b (input, target) \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438. \u0421 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0442\u044c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e \u043c\u0435\u0440\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 ([lazy evaluation](https:\/\/ru.wikipedia.org\/wiki\/%D0%9B%D0%B5%D0%BD%D0%B8%D0%B2%D1%8B%D0%B5_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F)).\n\n`tf.data.Dataset.from_tensor_slices(data)` - \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u043b\u0438\u0431\u043e \u043c\u0430\u0441\u0441\u0438\u0432, \u043b\u0438\u0431\u043e \u043a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u0432. \u0414\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043f\u043e \u043f\u0435\u0440\u0432\u043e\u043c\u0443 \u0438\u043d\u0434\u0435\u043a\u0441\u0443 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043c\u0430\u0441\u0441\u0438\u0432\u0430. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0435\u0441\u043b\u0438 `data = (np.zeros((128, 256, 256)), np.zeros(128))`, \u0442\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0431\u0443\u0434\u0435\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c 128 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043e\u0434\u0438\u043d \u043c\u0430\u0441\u0441\u0438\u0432 256x256 \u0438 \u043e\u0434\u043d\u043e \u0447\u0438\u0441\u043b\u043e.\n\n`dataset2 = dataset1.map(func)` - \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043a \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443; \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043e\u043b\u0436\u043d\u0430 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432, \u043a\u0430\u043a\u043e\u0432 \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u043e\u0440\u0442\u0435\u0436\u0430 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 1 \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u0441\u0442\u043e\u043b\u044c\u043a\u043e, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0443\u0436\u043d\u043e \u0438\u043c\u0435\u0442\u044c \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 2. \u041f\u0443\u0441\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u043c\u0435\u0442\u043a\u0438, \u0430 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u0442\u043e\u0433\u0434\u0430 \u043c\u044b \u043d\u0430\u043f\u0438\u0448\u0435\u043c \u0442\u0430\u043a: `dataset2 = dataset.map(lambda img, label: img)`.\n\n`dataset2 = dataset1.batch(8)` - \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u0431\u0430\u0442\u0447\u0430\u043c; \u0435\u0441\u043b\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 2 \u0434\u043e\u043b\u0436\u0435\u043d \u0432\u0435\u0440\u043d\u0443\u0442\u044c \u043e\u0434\u0438\u043d \u044d\u043b\u0435\u043c\u0435\u043d\u0442, \u0442\u043e \u043e\u043d \u0431\u0435\u0440\u0435\u0442 \u0438\u0437 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 1 \u0432\u043e\u0441\u0435\u043c\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u0441\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u0442 \u0438\u0445 (\u043d\u0443\u043b\u0435\u0432\u043e\u0439 \u0438\u043d\u0434\u0435\u043a\u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 - \u043d\u043e\u043c\u0435\u0440 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430) \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442.\n\n`dataset.__iter__()` - \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0432 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440, \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043c\u0435\u0442\u043e\u0434\u043e\u043c `.__next__()`. \u0418\u0442\u0435\u0440\u0430\u0442\u043e\u0440, \u0432 \u043e\u0442\u043b\u0438\u0447\u0438\u0435 \u043e\u0442 \u0441\u0430\u043c\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0445\u0440\u0430\u043d\u0438\u0442 \u043f\u043e\u0437\u0438\u0446\u0438\u044e \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430. \u041c\u043e\u0436\u043d\u043e \u0442\u0430\u043a\u0436\u0435 \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0446\u0438\u043a\u043b\u043e\u043c for.\n\n`dataset2 = dataset1.repeat(X)` - \u0434\u0430\u0442\u0430\u0441\u0435\u0442 2 \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 1 X \u0440\u0430\u0437.\n\n\u0415\u0441\u043b\u0438 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0432\u0437\u044f\u0442\u044c \u0438\u0437 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 1000 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0438\u0445 \u043a\u0430\u043a \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435, \u0430 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u0430\u043a \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435, \u0442\u043e \u043c\u044b \u043d\u0430\u043f\u0438\u0448\u0435\u043c \u0442\u0430\u043a:\n\n`test_dataset = dataset.take(1000)\ntrain_dataset = dataset.skip(1000)`\n\n\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u043f\u043e \u0441\u0443\u0442\u0438 \u043d\u0435\u0438\u0437\u043c\u0435\u043d\u0435\u043d: \u0442\u0430\u043a\u0438\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438, \u043a\u0430\u043a map, batch, repeat, take, skip \u043d\u0438\u043a\u0430\u043a \u043d\u0435 \u0437\u0430\u0442\u0440\u0430\u0433\u0438\u0432\u0430\u044e\u0442 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442. \u0415\u0441\u043b\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0445\u0440\u0430\u043d\u0438\u0442 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b [1, 2, 3], \u0442\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0432 3 \u0440\u0430\u0437\u0430 \u043f\u043e\u0434\u0440\u044f\u0434 \u0444\u0443\u043d\u043a\u0446\u0438\u044e dataset.take(1) \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043c 3 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u0435\u0440\u043d\u0435\u0442 \u0447\u0438\u0441\u043b\u043e 1. \u0415\u0441\u043b\u0438 \u0436\u0435 \u043c\u044b \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e dataset.skip(1), \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u044e\u0449\u0438\u0439 \u0447\u0438\u0441\u043b\u0430 [2, 3], \u043d\u043e \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c [1, 2, 3] \u043a\u0430\u0436\u0434\u044b\u0439 \u0440\u0430\u0437, \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u0435\u0433\u043e \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0435\u043c.\n\ntf.Dataset \u0432\u0441\u0435\u0433\u0434\u0430 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u0432 graph-\u0440\u0435\u0436\u0438\u043c\u0435 (\u0432 \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c eager-\u0440\u0435\u0436\u0438\u043c\u0443), \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043b\u0438\u0431\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f (`.map()`) \u0434\u043e\u043b\u0436\u043d\u044b \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e tensorflow-\u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043b\u0438\u0431\u043e \u043c\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c tf.py_function \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0431\u0435\u0440\u0442\u043a\u0438 \u0434\u043b\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0439, \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0445 \u0432 `.map()`. \u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0442\u044c [\u0437\u0434\u0435\u0441\u044c](https:\/\/www.tensorflow.org\/guide\/data#applying_arbitrary_python_logic).","f1a1590f":"### RNN NLP","1c35392f":"## Sell_id \n> \u041f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u043c \u043f\u0440\u043e\u0441\u0442\u043e \u0442\u0430\u043c \u0430\u0439\u0434\u0438\u0448\u043d\u0438\u043a\u0438 \u043b\u0435\u0436\u0430\u0442","d1e2a400":"### Data","80752508":"## FuelType","4b6489bf":"## Model_info","289bf0f2":"# Model [TabNet](https:\/\/github.com\/dreamquark-ai\/tabnet)","6b39a877":"## Production Data","e14e56a7":"> \u0412\u0441\u0435 \u043c\u0430\u0448\u0438\u043d\u044b \u0438\u0437 \u0433\u0435\u0440\u043c\u0430\u043d\u0438\u0438, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043d\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0441\u0442\u0440\u0430\u043d\u0443 \u043d\u0435 \u0438\u043c\u0435\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u0430","d26eec3e":"# Model Bonus: \u043f\u0440\u043e\u0431\u0440\u043e\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430","b8c53077":"## Simple Dense NN","377ef1a2":"\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0431\u044b\u0441\u0442\u0440\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u0441\u043c\u043e\u0436\u0435\u0442 \u043b\u0438 \u0441 \u044d\u0442\u0438\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0448 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c.","ec910b67":"## Vehicle Transmission","1239e312":"\n#### \u041e\u0431\u0449\u0438\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438:\n* \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b\n* \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0431\u043e\u043b\u0435\u0435 \u0434\u0435\u0442\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n* \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u0432 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 LR \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u044b\n* \u041f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u043c\n* \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c Fine-tuning\n\n#### Tabular\n* \u0412 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c, \u0431\u043b\u0438\u0437\u043a\u0438\u043c \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u043c\u0443, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438\u043c\u0435\u0435\u0442 \u0441\u043c\u044b\u0441\u043b \u0432\u0437\u044f\u0442\u044c \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c \u043f\u0435\u0440\u0435\u0434 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439. \u041f\u0440\u0438\u043c\u0435\u0440:\n`modelDateNorm = np.log(2020 - data['modelDate'])`\n\u0421\u0442\u0430\u0442\u044c\u044f \u043f\u043e \u0442\u0435\u043c\u0435: https:\/\/habr.com\/ru\/company\/ods\/blog\/325422\n\n* \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430:\n\u041f\u0430\u0440\u0441\u0438\u043d\u0433 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 'engineDisplacement', 'enginePower', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435' \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.\n\n* C\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\u041f\u0440\u0438\u0437\u043d\u0430\u043a name 'name' \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0434\u0430\u043d\u043d\u044b\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0436\u0435 \u0435\u0441\u0442\u044c \u0432 \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445 ('enginePower', 'engineDisplacement', 'vehicleTransmission'). \u041c\u043e\u0436\u043d\u043e \u0443\u0434\u0430\u043b\u0438\u0442\u044c \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435. \u0417\u0430\u0442\u0435\u043c \u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u0441\u0438\u043b\u044c\u043d\u0435\u0435 \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0432\u044b\u0434\u0435\u043b\u0438\u0432 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 xDrive \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.\n\n* \u041f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0434 Feature engineering\n\n\n\n#### NLP\n* \u0412\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u0438\u0437 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0439 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0431\u043b\u043e\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u0430, \u0437\u0430\u043c\u0435\u043d\u0438\u0432 \u0438\u0445 \u043d\u0430 \u043a\u043e\u0434\u043e\u0432\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0438\u0432\n* \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0442\u0435\u043a\u0441\u0442\u0430, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e - \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0441\u0442\u0430\u0432\u044f\u0449\u0438\u0439 \u0432\u0441\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0444\u043e\u0440\u043c\u0443 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e (\u0433\u043b\u0430\u0433\u043e\u043b\u044b \u0432 \u0438\u043d\u0444\u0438\u043d\u0438\u0442\u0438\u0432 \u0438 \u0442. \u0434.), \u0447\u0442\u043e\u0431\u044b \u0442\u043e\u043a\u0435\u043d\u0430\u0439\u0437\u0435\u0440 \u043d\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u043b \u0440\u0430\u0437\u043d\u044b\u0435 \u0444\u043e\u0440\u043c\u044b \u0441\u043b\u043e\u0432\u0430 \u0432 \u0440\u0430\u0437\u043d\u044b\u0435 \u0447\u0438\u0441\u043b\u0430\n\u0421\u0442\u0430\u0442\u044c\u044f \u043f\u043e \u0442\u0435\u043c\u0435: https:\/\/habr.com\/ru\/company\/Voximplant\/blog\/446738\/\n* \u041f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0434 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u043c\u0438 \u043e\u0447\u0438\u0441\u0442\u043a\u0438 \u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u0430\n\n\n\n#### CV\n* \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438\n* Fine-tuning","17e52e8c":"> \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u043e\u0431\u0435\u0449\u0430\u043d\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438","21b99556":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u0438\u043f\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n\n* bodyType - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* brand - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* color - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* description - \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0439\n* engineDisplacement - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u043a \u0442\u0435\u043a\u0441\u0442\n* enginePower - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u043a \u0442\u0435\u043a\u0441\u0442\n* fuelType - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* mileage - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439\n* modelDate - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439\n* model_info - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* name - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439, \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\n* numberOfDoors - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* price - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u0446\u0435\u043b\u0435\u0432\u043e\u0439\n* productionDate - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439\n* sell_id - \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 (\u0444\u0430\u0439\u043b \u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d \u043f\u043e \u0430\u0434\u0440\u0435\u0441\u0443, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u043e\u043c\u0443 \u043d\u0430 sell_id)\n* vehicleConfiguration - \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f (\u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432)\n* vehicleTransmission - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435 - \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u043a \u0442\u0435\u043a\u0441\u0442\n* \u041f\u0422\u0421 - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u041f\u0440\u0438\u0432\u043e\u0434 - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439\n* \u0420\u0443\u043b\u044c - \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439","71282a68":"> \u0412\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u0435\u0440\u0432\u043e\u0435 \u0447\u0438\u0441\u043b\u043e","8c919b6e":"# Model 3: Tabular NN","7376260d":"## BodyType","7bcf23dc":"### Submission","a6307cae":"## Library","91182eba":"## \u0412 baseline \u043c\u044b \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435:\n* \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \"\u043d\u0430\u0438\u0432\u043d\u0443\u044e\"\/baseline \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0443\u044e \u0446\u0435\u043d\u0443 \u043f\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0433\u043e\u0434\u0443 \u0432\u044b\u043f\u0443\u0441\u043a\u0430 (\u0441 \u043d\u0435\u0439 \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438)\n* \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0438 \u043e\u0442\u043d\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c\u00a0\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n* \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0435\u0440\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e CatBoost\n* \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0432\u0442\u043e\u0440\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n* \u0421\u0434\u0435\u043b\u0430\u0435\u043c multi-input \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0442\u0435\u043a\u0441\u0442\u0430 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\n* \u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0432 multi-input \u0441\u0435\u0442\u044c \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n* \u041e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u043c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 (\u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u0435 \u0438\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439)","d6dd63b5":"# Car Price prediction\n\n<img src=\"https:\/\/whatcar.vn\/media\/2018\/09\/car-lot-940x470.jpg\"\/>\n\n## \u041f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f \u043f\u043e \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0430\u043c\n*\u042d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0448\u0430\u0431\u043b\u043e\u043d\u043e\u043c (Baseline) \u043a \u0442\u0435\u043a\u0443\u0449\u0435\u043c\u0443 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044e \u0438 \u043d\u0435 \u0441\u043b\u0443\u0436\u0438\u0442 \u0433\u043e\u0442\u043e\u0432\u044b\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c!*   \n\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u043a\u0430\u043a \u043e\u0441\u043d\u043e\u0432\u0443 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0441\u0432\u043e\u0435\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f.\n\n\n> **Baseline** \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435 \u043a\u0430\u043a \u0448\u0430\u0431\u043b\u043e\u043d, \u0433\u0434\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u043a\u0430\u043a \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0441 \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c ML \u043d\u0430\u0447\u0438\u043d\u043a\u0430 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043f\u0440\u043e\u0441\u0442\u043e\u0439. \u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u043a \u0441\u0430\u043c\u043e\u043c\u0443 ML, \u0430 \u043d\u0435 \u0442\u0440\u0430\u0442\u0438\u0442\u044c \u0446\u0435\u043d\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u043d\u0430 \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438. \n\u0422\u0430\u043a\u0436\u0435 baseline \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0445\u043e\u0440\u043e\u0448\u0435\u0439 \u043e\u043f\u043e\u0440\u043d\u043e\u0439 \u0442\u043e\u0447\u043a\u043e\u0439 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435. \u0415\u0441\u043b\u0438 \u043d\u0430\u0448\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u0445\u0443\u0436\u0435 baseline -  \u043c\u044b \u044f\u0432\u043d\u043e \u0434\u0435\u043b\u0430\u0435\u043c \u0447\u0442\u043e-\u0442\u043e \u043d\u0435 \u0442\u0430\u043a \u0438 \u0441\u0442\u043e\u0438\u0442 \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0443\u0442\u044c) ","e7894d75":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:","74f3dbc8":"## Brand","ac26361c":"## ModelDate","1c81e15a":"> \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u0430\u043c\u044b\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0438 \u0441\u0430\u043c\u044b\u0435 \u0440\u0435\u0434\u043a\u0438\u0435","48fde7d7":"## \u041f\u0440\u0438\u0432\u043e\u0434","baa08b07":"> \u041d\u043e \u0432\u0441\u0435 \u0442\u0430\u043a\u0438\u0435 \u0438\u0437-\u0437\u0430 \u0442\u0430\u043a\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432, \u043b\u0443\u0447\u0448\u0435 \u0443\u0434\u0430\u043b\u0438\u0442\u044c","f386c7dd":"## Vehicle Configuration","eacb750f":"> \u041f\u0435\u0440\u0435\u0432\u0435\u0434\u0435\u043c \u0432 \u043c\u0438\u043b\u043b\u0438\u043b\u0438\u0442\u0440\u044b","26949501":"## \u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435","d4a7779f":"## Color","a4d61c27":"> \u041b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u0443\u0435\u043c","1e1b7a32":"> \u0423 \u043d\u0430\u0441 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432\u0441\u0435\u0433\u043e 6 \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043a\u0430\u0440\u043e\u0432 \u0438 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0435\u0442, \u043f\u043e\u044d\u0442\u043e\u043f\u0443 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0443\u0431\u0440\u0430\u0442\u044c \u0441\u043e\u0432\u0441\u0435\u043c \u044d\u0442\u0438 \u043c\u0430\u0448\u0438\u043d\u044b","8881c273":"> \u0412 \u0442\u0435\u0441\u0442\u0435 \u043d\u0435\u0442 \u043f\u0440\u0430\u0432\u043e\u0440\u0443\u043b\u044c\u043d\u044b\u0445, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0441\u043c\u0435\u043b\u043e \u0443\u0434\u0430\u043b\u044f\u0435\u043c","e58667da":"> \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0435 \u0430\u0432\u0442\u043e \u0441 100, None, COUPE,CITAN","c6efefdf":"# Loss functions","68a0411f":"# Blend","3de2ec6d":"### MLP","1b69a241":"# Model 5: \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438","095ab503":"\u0418\u0434\u0435\u0438 \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f NLP \u0447\u0430\u0441\u0442\u0438:\n* \u0412\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u0438\u0437 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0439 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0431\u043b\u043e\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u0430, \u0437\u0430\u043c\u0435\u043d\u0438\u0432 \u0438\u0445 \u043d\u0430 \u043a\u043e\u0434\u043e\u0432\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0438\u0432\n* \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0442\u0435\u043a\u0441\u0442\u0430, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e - \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0441\u0442\u0430\u0432\u044f\u0449\u0438\u0439 \u0432\u0441\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0444\u043e\u0440\u043c\u0443 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e (\u0433\u043b\u0430\u0433\u043e\u043b\u044b \u0432 \u0438\u043d\u0444\u0438\u043d\u0438\u0442\u0438\u0432 \u0438 \u0442. \u0434.), \u0447\u0442\u043e\u0431\u044b \u0442\u043e\u043a\u0435\u043d\u0430\u0439\u0437\u0435\u0440 \u043d\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u043b \u0440\u0430\u0437\u043d\u044b\u0435 \u0444\u043e\u0440\u043c\u044b \u0441\u043b\u043e\u0432\u0430 \u0432 \u0440\u0430\u0437\u043d\u044b\u0435 \u0447\u0438\u0441\u043b\u0430\n\u0421\u0442\u0430\u0442\u044c\u044f \u043f\u043e \u0442\u0435\u043c\u0435: https:\/\/habr.com\/ru\/company\/Voximplant\/blog\/446738\/\n* \u041f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0434 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u043c\u0438 \u043e\u0447\u0438\u0441\u0442\u043a\u0438 \u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u0430","96627967":"### Multiple Inputs NN","d3be0e29":"### \u0421\u0442\u0440\u043e\u0438\u043c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0431\u0435\u0437 \"\u0433\u043e\u043b\u043e\u0432\u044b\""}}