{"cell_type":{"10cf972a":"code","28fab10c":"code","f947b296":"code","9cfd0d30":"code","5935b3d1":"code","5ed67eea":"code","232c7ac7":"code","45f94711":"code","4cdd6f5b":"code","228c75c2":"code","e2fcd5b2":"code","2d28ae89":"code","610ed8f0":"code","d05ca541":"code","ef6d8a96":"code","c257480a":"code","5ea45c89":"code","ed23ded2":"code","2478ec2f":"code","bd057e07":"code","bfa09cb6":"code","05448794":"code","6bec32fc":"code","cfea3cc1":"code","d7ff2dc3":"code","68ea8e04":"code","e844cce6":"code","4b4cf873":"code","bba538c5":"code","d50c2d97":"code","5298a9c5":"code","b82f6050":"code","c9dcba68":"code","9a05ac72":"code","d794fda2":"code","1aad4fe9":"code","9cb70c2b":"code","8e1f7c9c":"code","caa09de2":"code","154f36b6":"code","367b355f":"code","6db8add1":"code","9e4ec5d1":"code","bd27b969":"code","80e988eb":"code","689ace26":"code","0dbb0013":"code","ec3c05b8":"code","6f5210f9":"code","0bf36e61":"code","bec67e69":"code","f1dbb772":"code","7823f90e":"code","fe015be5":"code","73240494":"code","3fd99615":"code","3b5180a0":"code","5cb07a4f":"code","a7942216":"code","171fc5bc":"markdown","434f6b73":"markdown","46345d6d":"markdown","68e85f26":"markdown","9d077e1e":"markdown","3809a95a":"markdown","77e69991":"markdown","308f8405":"markdown","174a2008":"markdown","25121506":"markdown","eaf6913a":"markdown","f237a366":"markdown","2b046cac":"markdown","cc5f613c":"markdown","6d066ee3":"markdown","cbdaaa43":"markdown","c1edae63":"markdown","bffa2c41":"markdown","fdce6377":"markdown","fb58a90a":"markdown","d5c2e697":"markdown","73d01691":"markdown","fdf689e0":"markdown","29d6728e":"markdown","3a45ca18":"markdown","8bcfcd1a":"markdown","6c9cd2b1":"markdown","8afb1457":"markdown","18d5912d":"markdown","fce0bc1f":"markdown","d4f1d257":"markdown","19578c99":"markdown","6ababea0":"markdown","d29148e9":"markdown","a017b2f3":"markdown","8b5f51e0":"markdown","d7392941":"markdown","6635f120":"markdown","c45b9b98":"markdown","62611753":"markdown","b49809be":"markdown","d3039671":"markdown","b1f0ceea":"markdown","9c178ed3":"markdown","de933630":"markdown","77892534":"markdown","dad9b757":"markdown","b8147d2d":"markdown","b7fe02e0":"markdown","0e6a4aa2":"markdown","09610ffb":"markdown","bfff8f62":"markdown","3aea3a96":"markdown","b13ea088":"markdown","f22cee69":"markdown","a9470f61":"markdown","2a34056d":"markdown","353cda47":"markdown","15fa48c5":"markdown","444a848f":"markdown","4e6ed6c0":"markdown"},"source":{"10cf972a":"housing = pd.read_csv('\/kaggle\/input\/housing\/housing.csv')\nhousing.head()","28fab10c":"housing.info()","f947b296":"housing['ocean_proximity'].value_counts()","9cfd0d30":"housing.describe()","5935b3d1":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","5ed67eea":"import numpy as np\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","232c7ac7":"housing['income_cat'] = np.ceil(housing['median_income']\/1.5)\nhousing['income_cat'].where(housing['income_cat']<5, 5.0, inplace=True)\nhousing['income_cat'].hist()","45f94711":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","4cdd6f5b":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\ncompare_props = pd.DataFrame({\n    \"All_data\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Error - random (%)\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"All_data\"] - 100\ncompare_props[\"Error - stratified (%)\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"All_data\"] - 100\ncompare_props","228c75c2":"for set_ in (strat_test_set, strat_train_set):\n    set_.drop('income_cat', axis=1, inplace=True)","e2fcd5b2":"housing = strat_train_set.copy()","2d28ae89":"housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)","610ed8f0":"housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']\/100, label='Population',\n            c=housing['median_house_value'], figsize=(10,7), cmap=plt.get_cmap('jet'), colorbar=True)","d05ca541":"corr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","ef6d8a96":"from pandas.plotting import scatter_matrix\nattributes = ['median_house_value','median_income','total_rooms','housing_median_age']\nscatter_matrix(housing[attributes], figsize=(12,8))","c257480a":"housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)","5ea45c89":"housing['Rooms_per_family'] = housing['total_rooms']\/housing['households']\nhousing['Bedrooms_per_room'] = housing['total_bedrooms']\/housing['total_rooms']\nhousing['Population_per_family'] = housing['population']\/housing['households']\n\ncorr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","ed23ded2":"housing = strat_train_set.drop('median_house_value', axis=1)\nhousing_labels = strat_train_set['median_house_value'].copy()","2478ec2f":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')\nhousing_num = housing.drop('ocean_proximity', axis=1)\nimputer.fit(housing_num)","bd057e07":"X = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)","bfa09cb6":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nhousing_cat = housing['ocean_proximity']\nhousing_cat_encoded = encoder.fit_transform(housing_cat)\nprint(encoder.classes_)","05448794":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(np.array(housing_cat).reshape(-1,1)) #Direct OneHotEncoder on categorical data\nhousing_cat_1hot","6bec32fc":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, houshold_ix = 3,4,5,6\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self #Dummy function\n    def transform(self, X, y=None):\n        Rooms_per_family = X[:, rooms_ix] \/ X[:, houshold_ix]\n        Population_per_family = X[:, population_ix] \/ X[:, houshold_ix]\n        if self.add_bedrooms_per_room:\n            Bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, Rooms_per_family, Population_per_family, Bedrooms_per_room]\n        else:\n            return np.c_[X, Rooms_per_family, Population_per_family]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","cfea3cc1":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","d7ff2dc3":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","68ea8e04":"num_attribs = list(housing_num)\ncat_attribs = ['ocean_proximity']\n\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_attribs)),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attribs)),\n    ('cat_encoder', OneHotEncoder(sparse=False)),\n])","e844cce6":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion([\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared","4b4cf873":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","bba538c5":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions: ', lin_reg.predict(some_data_prepared))\nprint('Labels: ', list(some_labels))","d50c2d97":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","5298a9c5":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","b82f6050":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","c9dcba68":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, \n                         scoring='neg_mean_squared_error', cv=10)\ntree_rmse_scores = np.sqrt(-scores)","9a05ac72":"def display_scores(scores):\n    print('Results: ', scores)\n    print('Mean: ', scores.mean())\n    print('Std: ', scores.std())\n\ndisplay_scores(tree_rmse_scores)","d794fda2":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=10)\nforest_reg.fit(housing_prepared, housing_labels)\nforest_pred = forest_reg.predict(housing_prepared)\nforest_rmse = np.sqrt(mean_squared_error(housing_labels,forest_pred))\nforest_rmse","1aad4fe9":"scores = cross_val_score(forest_reg, housing_prepared, housing_labels, \n                                  scoring='neg_mean_squared_error', cv=10)\nforest_rmse_scores = np.sqrt(-scores)\ndisplay_scores(forest_rmse_scores)","9cb70c2b":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3,10,30], 'max_features': [2,4,6,8]},\n    {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(housing_prepared, housing_labels)","8e1f7c9c":"print('Best parameters: ', grid_search.best_params_)\nprint('Best model: ',grid_search.best_estimator_)\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score),params)","caa09de2":"feature_importance = grid_search.best_estimator_.feature_importances_\nfeature_importance","154f36b6":"extra_attribs = ['Rooms_per_family', 'Population_per_family', 'Bedroom_per_rooms']\ncat_one_hot_attribs = list(encoder.categories_)\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importance, attributes), reverse=True)","367b355f":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set['median_house_value'].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test,final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","6db8add1":"from sklearn.svm import SVR\n\nmodel = SVR(gamma='auto') #this value will be the default from future versions\nsvr_scores = cross_val_score(model, housing_prepared, housing_labels, \n                scoring='neg_mean_squared_error', cv=10)\nsvr_rmse_scores = np.sqrt(-svr_scores)\ndisplay_scores(svr_rmse_scores)","9e4ec5d1":"display_scores(forest_rmse_scores)","bd27b969":"param_grid_svr = [\n    {'kernel': ['linear'], 'gamma': ['scale', 'auto'], 'C': [0.1, 0.5, 1.0, 2.0, 5.0]},\n    {'kernel': ['rbf'], 'gamma': ['scale', 'auto'], 'C': [0.1, 0.5, 1.0, 2.0, 5.0]}\n]\ngrid_search_svr = GridSearchCV(model, param_grid_svr, cv=5, scoring='neg_mean_squared_error')\ngrid_search_svr.fit(housing_prepared, housing_labels)","80e988eb":"print('Best parameters: ', grid_search_svr.best_params_)\nprint('Best model: ',grid_search_svr.best_estimator_)\ncvres = grid_search_svr.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score),params)","689ace26":"param_grid_C = [\n    {'C': [5.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0, 1000000.0]}\n]\ngrid_search_C = GridSearchCV(SVR(kernel='linear', gamma='scale'), param_grid_C, cv=5, scoring='neg_mean_squared_error')\ngrid_search_C.fit(housing_prepared, housing_labels)","0dbb0013":"print('Best parameters: ', grid_search_C.best_params_)\nprint('Best model: ',grid_search_C.best_estimator_)\ncvres = grid_search_C.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score),params)","ec3c05b8":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal\n\nparams = {'kernel': ['linear', 'rbf'], 'C': reciprocal(20,200000)}\nrandom_grid_search = RandomizedSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', verbose=2, random_state=42)\nrandom_grid_search.fit(housing_prepared, housing_labels)","6f5210f9":"print('Best parameters: ', random_grid_search.best_params_)\nprint('Best model: ',random_grid_search.best_estimator_)\ncvres = random_grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score),params)","0bf36e61":"reciprocal_distrib = reciprocal(20, 200000)\nsamples = reciprocal_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Reverse distribution\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of reverse distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()","bec67e69":"feature_importance = feature_importance[:-1]\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","f1dbb772":"k = 5\ntop_k_feature_indices = indices_of_top_k(feature_importance, k)\nnp.array(attributes)[:5]","7823f90e":"sorted(zip(feature_importance, attributes), reverse=True)[:k]","fe015be5":"full_pipeline_feature_selection = Pipeline([\n    ('full_pipeline', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importance, k))\n])\nhousing_prepared_top_k_features = full_pipeline_feature_selection.fit_transform(housing)","73240494":"np.all(housing_prepared_top_k_features[0:3] == housing_prepared[0:3, top_k_feature_indices])","3fd99615":"prepare_select_predict_pipeline = Pipeline([\n    ('data_preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importance, k)),\n    ('prediction', SVR(**random_grid_search.best_params_))\n])\n\nprepare_select_predict_pipeline.fit(housing, housing_labels)","3b5180a0":"some_data = housing[1:4]\nsome_labels = housing_labels[1:4]\nprint(\"Predictions:\\t\", prepare_select_predict_pipeline.predict(some_data))\nprint(\"Lables:\\t\\t\", list(some_labels))","5cb07a4f":"grid_param = [\n     {'feature_selection__k': list(range(1, len(feature_importance) + 1)),\n     'data_preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n     'prediction__gamma': ['auto', 'scale']}\n]\ngrid_search_prep = GridSearchCV(prepare_select_predict_pipeline, grid_param, cv=5,\n                               scoring='neg_mean_squared_error', error_score=np.nan)\ngrid_search_prep.fit(housing, housing_labels)","a7942216":"print(grid_search_prep.best_params_)\nprint(np.sqrt(-grid_search_prep.best_score_))","171fc5bc":"It is now enough to extend the previously prepared pipeline with a new functionality.","434f6b73":"Let us visualize the data","46345d6d":"We have the result, and although it is not satisfactory, building more models is already much easier with proper data preparation process. So let's try the Decision Tree model type","68e85f26":"We have ten attributes, one of which is categorical and the other numerical.\\\nLet's see a summary of all the features","9d077e1e":"This notebook contains basic methodology for constructing a pipeline for a machine learning project based on a simple example of California house prices data. It was inspired by the content of an excellent book by Aurelien Geron: *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems*","3809a95a":"Recall the results for the Random Forest model","77e69991":"It seems that in this case the linear kernel is doing better. Note that the value of the hyperparameter `C` is the highest value possible - let's test the higher value, leaving the other two hyperparameters unchanged","308f8405":"Let us see that the selected features are in fact the most important ones","174a2008":"Its visualization for the most correlated attributes is shown by the scattering graphs below","25121506":"Let's check feature importance","eaf6913a":"Perform stratified sampling using `StratifiedShuffleSplit` class","f237a366":"This graph can be accompanied by information on population density and the median of prices of flats in a given area can be marked with a colour","2b046cac":"It is advisable to make a copy of the original test set for experimental purposes - especially for feature engineering.","cc5f613c":"Well, we have a model with selected hyperparameters (although the selection process is far from optimal). Let's check if the model is a good general knowledge, i.e. if we will get results on a test set close to that of the corresponding training set. ","6d066ee3":"We can conclude that both geographical location and population have a significant impact on the price of a home.\\\nThe Pearson correlation coefficient additionally indicates how strong is the linear relationship between numerical features","cbdaaa43":"Check performence on training set","c1edae63":"It remains to be verified the correctness of the operation of such a pipeline","bffa2c41":"The `TopFeatureSelector` class selects the `k` best features (from the list of `feature_importances`)","fdce6377":"Let's read the best settings and scores","fb58a90a":"A very simple yet effective approach to dealing with missing observations is to insert the mean value (or, more often in practice, the median due to its insensitivity to outlier observations).","d5c2e697":"Print results","73d01691":"The result seems promising. However, the experience taught, for the sake of certainty, let's carry out a cross-validation","fdf689e0":"Create pipeline for imputing, feature engineering and scaling for numerical attributes","29d6728e":"Of course, we fill in the data gaps in both the training and the test set, but based only on the former (otherwise the model would 'cheat' in the learning process using the knowledge of the set, which it should not 'see' at this stage)","3a45ca18":"The current results seem more realistic...","8bcfcd1a":"The error with respect to the default values has been reduced by almost half! This is a really great result, but the Random Forest model performs even better when the hyperparameter values are adjusted","6c9cd2b1":"We may as well define a class that supports a selection of features entering the model as follows","8afb1457":"## Support Vector Machine for regression\nLet's try to use an algorithm from different class now - Support Vector Machines. \\\nFirst, let's try to use the default hyperparameter values","18d5912d":"The zero value of error in theory should be welcomed, but in practice it only means trouble.  After a moment of reflection we come to the conclusion that this model for each set of predecessors from the training set is exactly the value of the response we expect from it, which, with the limitation of its form (we assume that the model is in the form of a decision tree, or in fact the so-called simple function) heralds problems. So let's limit the training set to a certain subset of it and test the so trained model on the remaining subset. To make the result as independent as possible from such a subset, we will carry out a cross-validation","fce0bc1f":"Link values to coresponding attributes names","d4f1d257":"## First glance at the data","19578c99":"Thanks to Randomized Search we got significantly closer to the results of the Random Forest model. It should be noted that this time the Gauss kernel (`rbf`) turned out to be a better choice, which was not noticeable in the case of searching the grid of parameters.\\\nThe probability distribution (reversed) used for the `C` parameter is illustrated below","6ababea0":"The graph shows clear horizontal lines. In addition, there is a clear upper limit for the median value of house prices. It can be deduced from this that this value has an upper limit and each higher median is 'cut' to it. This may cause some obstacles in the learning phase of the model \n\n##  Feature engineering\n\nThis stage is one of the most important in the process of creating a learning system. A good selection of features in relation to a limited amount of data often determines the success of a given project.\\\nIn order to limit the number of attributes they can be combined - for example, we can create a `Rooms_per_family` feature representing the number of rooms per family in a given district. The same applies to the population `Population_per_family`. Additionally, one could think of the number of bedrooms per room (`Bedrooms_per_room`)","d29148e9":"Let us now delete the attribute created only for the purpose of selecting a test set","a017b2f3":"Then the quality variables should be encoded - in the simplest approach, different classes can be given different numbers (most simply natural)","8b5f51e0":"Their graphic representation is well illustrated by histograms","d7392941":"This particular distribution was chosen because it is suitable for estimating hyperparameters for which we do not know the scale (see right figure).\\\n## Automation of feature selection process\n\nOnce we have determined that the Random Forest model is the most appropriate, it would be good to work on the features - we will try to create a function that selects the attributes with the greatest influence (i.e. the best predictors)","6635f120":"Let's try to improve the results by to try out different combinations of hyperparameters: `kernel`, `C` and `gamma`","c45b9b98":"The comments made earlier on stratified sampling are well illustrated in the following summary","62611753":"Note that we are conducting a simple random data separation. This is usually an acceptable approach, but it is good that the test set is as representative as possible for all data (the whole sample). Of course we cannot fully guarantee this, but with some expertise we can force the distribution of the most important features to be similar - it is called stratified sampling.\\\nWe must therefore consider which feature will play a key role in predictive housing prices - without much doubt we can assume that the average median income is a good candidate. Let us therefore create a new categorical feature corresponding to the aforementioned one","b49809be":"For automation purposes, it's convenient to define a parameterized class, which we can use later in the pipeline - for this purpose it's enough to implement an appropriate interface (thanks to a well thought-out design of the Scikit-Learn package, we only need to implement two methods - fit() and transform()). An example of such a transformer is presented below","d3039671":"The results are similar. We can try to further improve the model by optimizing its hyperparameters.","b1f0ceea":"Comparing numbers is not convenient - it is better to measure performance through one metric: RMS","9c178ed3":"# California housing prices prediction","de933630":"We've searched quite a wide range of parameters. Let's see which model is the best and what results are obtained thanks to it","77892534":"We can (finally ! :) ) define the first model thanks to such prepared stream. We will start with the simplest option - linear regression model","dad9b757":"Fortunately, there is only one column with missing items (`total_bedrooms`). We will deal with this problem in some time.\\\nFor categorical attributes, the cardinal number (i.e. how many different classes are possible) is extremely important","b8147d2d":"And that's it - we've got another model taught on prepared data. Let's check the performance","b7fe02e0":"## The final\n\nWe have a full stream that prepares data and selects the most important features. We have also tested several models and tuned the corresponding hyperparameters. It's time to put it all together in one pipeline","0e6a4aa2":"This approach suggests both the 'order' of occurrence of individual classes and their mutual 'distances'. Another, relatively simple solution is to define for each class a dedicated binary vector (indicating whether the current observation contains this class). This is a good way as long as there are relatively few available classes (otherwise we get a lot of new predictors)","09610ffb":"The pipeline works, the forecasts are good, and remember that using the model of the Random Forest would be even better!\\\nWe can now test several strategies for dealing with missing observations and experiment with an optimal number of important features selected to teach the model","bfff8f62":"## Tuning via randomized search\nUntil now, hyperparameter tuning has consisted in searching the grid for their discreet values - this approach has the advantage that every node of the grid will be examined. If the number of values\/attributes increases, the grid size increases powerfully and this approach becomes impractical. Instead, we can use a random search of the grid of parameters","3aea3a96":"The results obtained in this way should be even higher - see for yourself","b13ea088":"Further exploration of data will be limited to a training set, so as not to suggest too much detail - after all, our model should well generalize the acquired knowledge.\n\n## Data preperation\nCreate test set for further model validation","f22cee69":"As expected, there is a very strong link between the median income and the median house value","a9470f61":"Create pipelines supporting numerical and categorical features","2a34056d":"Let's try another model - it will be a combination of Decision Tree Regressors called a Random Forest Regressor","353cda47":"Five classes is a relatively small number - in the case of one-hot encoding we get only five new features.\\\nFor numeric data there is a number of statistics that provide some information. The most frequently used are presented below","15fa48c5":"We combine these two streams with the FeatureUnion class","444a848f":"This process took almost 15 minutes, but it was worth it","4e6ed6c0":"We have reduced the number of features from 4 to 3, and additionally the newly created two attributes are strongly correlated with the median house value (`Rooms_per_family` , `Bedrooms_per_room`)\\\n## Data preparation"}}