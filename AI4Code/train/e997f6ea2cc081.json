{"cell_type":{"66f75922":"code","ad4fb068":"code","56b56234":"code","9dd58cf4":"code","9295f36b":"code","d0ad5d02":"code","8c46affd":"code","942ca28d":"code","bbf1a7d4":"code","0d379e67":"code","a1e75dbf":"code","512d8f26":"code","4425e372":"code","70a3b3b2":"code","1f6d5085":"code","cdb2878f":"code","60d2ef14":"code","1a1fd9c7":"code","dab27dc2":"code","8f9b5ccf":"code","41979748":"code","892d9445":"code","d3b60d04":"code","a5a8ac2f":"code","b9315d8d":"code","da67ebbd":"code","71c0f97c":"code","9ff96dba":"code","5f838c80":"code","06deb761":"code","aff15b7e":"code","f3a24f9a":"code","0fc768f3":"code","012fdc4b":"code","7d0540d6":"code","025f3b10":"code","038facdb":"code","f9f1415c":"code","6e3c6def":"code","524e401d":"code","3dea9379":"code","b2b017e7":"code","3ee8de54":"code","fb026348":"code","924103e9":"code","b454a22c":"code","0e71db01":"code","065e5662":"code","b94bd2be":"code","2de6353d":"code","de2de9c8":"code","68f192e6":"code","892d4a15":"code","191138f6":"code","6583f5bd":"code","f4065302":"code","90a5fca7":"code","b26fbe9a":"code","54062d78":"code","e6056a14":"code","69398398":"code","b4b37252":"code","637fc7e7":"code","5edeb86a":"code","178921ca":"code","81b75cb3":"code","dc1271e9":"code","3039da9b":"code","96dcc83b":"code","ca008329":"code","03e74566":"code","1356b6d9":"code","3684fe08":"code","8dc31507":"code","b3ed553e":"code","452557a8":"code","fbf37bc2":"code","4c9757fb":"code","ca0785f8":"code","7d0aed20":"code","a64c36ea":"code","fbdd8583":"code","5cd64406":"code","f83df43f":"code","6f3991c0":"code","de5e5ce8":"code","00054842":"code","7582b6b3":"code","9fdefffa":"code","46aed979":"code","875a35ac":"code","541a6ff7":"code","188b91ae":"code","9f8ffd72":"code","c80b8c5d":"code","c91c60da":"code","10e695d2":"code","7875134c":"code","71fd3c02":"code","84ab4158":"code","a71a6893":"code","3983539c":"code","e671210d":"code","f9cc5acb":"code","1e4277db":"code","b8b4c625":"code","4f6d62de":"code","b70e9da6":"code","d8d7664d":"code","ae9621ac":"code","e5849b89":"code","499b528e":"code","31a5e32e":"code","94cacc55":"code","1aa7a274":"code","9d3925e3":"code","64a9aef1":"code","85ed09b8":"code","7c65f74b":"code","91b84235":"code","1747dfb6":"code","840c3947":"code","15b23a2d":"code","b4f02129":"code","1ee089cf":"code","ad3dff84":"code","c1efc5a3":"code","c5543a24":"code","25832aff":"code","a49f1e29":"code","32f9fa14":"code","116b7da7":"code","c59a6f18":"code","0923a4dc":"code","50988820":"code","6f8f59ad":"code","18f50f55":"code","42b8d5db":"code","a3f1d9af":"markdown","09e2c756":"markdown","6e90c737":"markdown","4d5a6873":"markdown","f1a6ccdd":"markdown","76192903":"markdown","01882157":"markdown","d844df48":"markdown","efe5532d":"markdown","60a16949":"markdown","ca2af07d":"markdown","1b32f851":"markdown","af5deb8c":"markdown","10a81836":"markdown","ffc3fbc6":"markdown","736d08dc":"markdown","e870b7c8":"markdown","a5df7bbb":"markdown","cbaab712":"markdown","db0473bc":"markdown","13a95ba8":"markdown","978eb3ac":"markdown","d356cf24":"markdown","17d559e5":"markdown","ba90ef97":"markdown","9e9a64e7":"markdown","0ecc49d9":"markdown","e6c9a040":"markdown","0196a3d8":"markdown","52dce00a":"markdown","c9165a47":"markdown","51c385ae":"markdown","10590274":"markdown","3177f6df":"markdown","3eb5e9c6":"markdown","3f167f6d":"markdown","6cd502bb":"markdown","bf3ccef7":"markdown","29d5820b":"markdown","e61388a3":"markdown","c090e595":"markdown","e1c5c538":"markdown","c9e52ba1":"markdown","06a684f3":"markdown","7a0a1a0d":"markdown","1e7ddde2":"markdown","d9bf0d42":"markdown","b1cdaa07":"markdown","645ffd94":"markdown","87f3a32a":"markdown","70ef9388":"markdown","8e0279fa":"markdown","253e7500":"markdown","fc8e46c1":"markdown","3a1d3229":"markdown","397e4c13":"markdown","bab0fcc4":"markdown","4ec83b77":"markdown","49fde677":"markdown","7d99d915":"markdown","4a1e8e4d":"markdown","18142d51":"markdown","52c2c731":"markdown","028b5294":"markdown","b196fcd0":"markdown","35047bca":"markdown","4e8120b6":"markdown","41e15e9a":"markdown","a312b772":"markdown","ce71d15f":"markdown","6e98dea5":"markdown","43b798bb":"markdown","df6855e2":"markdown","3e8b57fd":"markdown","485b6ea8":"markdown","1856e47e":"markdown","6e566e1a":"markdown","e75c488e":"markdown","a027756b":"markdown","c7046260":"markdown","f4021f93":"markdown","2b471745":"markdown","369ed439":"markdown"},"source":{"66f75922":"import pandas as pd # data processing\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt #visualization\n\n\nfrom collections import Counter    #unordered collection where elements are stored as Dict keys and their count as dict valuenote => used to count the number of key words.  \nimport plotly.graph_objects as go  #visualization & mapping\nfrom sklearn.utils import shuffle  #Shuffle arrays or sparse matrices in a consistent way.\n\n\n# Allowing modules autoreload # It will reload all modified modules each time before running a new line. \n%load_ext autoreload\n%autoreload 2","ad4fb068":"#we want to have % with only 1 digit after comma\ndef format_percentage(value):  \n    return \"{0:.1f}%\".format(value * 100)","56b56234":"#Let's upload our csv datasets \n\n\ncountry = pd.read_csv(\"\/kaggle\/input\/p2country\/EdStatsCountry.csv\")\ncountry.name = \"Country\"\n\ncountry_series = pd.read_csv(\"\/kaggle\/input\/p2countryseries\/EdStatsCountry-Series.csv\")\ncountry_series.name = \"Country-Series\"\n\ndata = pd.read_csv(\"\/kaggle\/input\/p2data\/EdStatsData.csv\")\ndata.name = \"Data\"\n\nnote = pd.read_csv(\"\/kaggle\/input\/p2footnote\/EdStatsFootNote.csv\")\nnote.name = \"FootNote\"\n\nseries = pd.read_csv(\"\/kaggle\/input\/p2series\/EdStatsSeries.csv\")\nseries.name = \"Series\"\n\n\n\ndata_list = [country, country_series, data, note, series]\n\ndatasets = []\nfor df in data_list:\n    datasets.append(df.name)","9dd58cf4":"#We have 5 files to discover. Let's summarize key informations : \n\n\n\n# 1 -\n#Let's set the table layout + content we will include in it : \nfiles_description = pd.DataFrame(columns = [\"Nb rows\", \"Nb columns\", \"Average filling rate\", \"Duplicates\", \"Description\"],\n                                 index = [\"country = EdStatsCountry.csv\", \n                                          \"country_series = EdStatsSeries.csv\", \n                                          \"data = EdStatsData.csv\", \n                                          \"note = EdStatsFootNote.csv\",\n                                          \"series = EdStatsCountry-Series.csv\"\n                                        ])\n\n# 2 -\n#Let's automate the total number rows filling of each file : \nfiles_description[\"Nb rows\"] = [\n    len(country.index),\n    len(country_series.index),\n    len(data.index),\n    len(note.index),\n    len(series.index)\n]\n\n# 3 - \n#Let's automate the total number columns filling of each file : \nfiles_description[\"Nb columns\"] = [\n    len(country.columns),\n    len(country_series.columns),\n    len(data.columns),\n    len(note.columns),\n    len(series.columns)\n]\n\n# 4 -\n#Let's automate the fill-percentile of each file\n# We use the mean() function twice to calculate the mean for each columns, and then the mean for the whole file\nfiles_description[\"Average filling rate\"] = [\n    format_percentage(country.notna().mean().mean()),\n    format_percentage(country_series.notna().mean().mean()),\n    format_percentage(data.notna().mean().mean()),\n    format_percentage(note.notna().mean().mean()),\n    format_percentage(series.notna().mean().mean())\n    \n]\n\n# 5 -\n#Let's automate the number of duplicate keys for each file\nfiles_description[\"Duplicates\"] = [\n    country.duplicated(subset=[\"Country Code\"]).sum(),\n    country_series.duplicated(subset=[\"SeriesCode\"]).sum(),\n    data.duplicated(subset=[\"Indicator Code\"]).sum(),\n    note.duplicated(subset=[\"SeriesCode\"]).sum(),\n    series.duplicated(subset=[\"Series Code\"]).sum()\n]\n\n# 6 -\n#Let's describe the content of each file\nfiles_description[\"Description\"] = [\n    \"Countries'list with their main indicators\",\n    \"Indicators'list with details\",\n    \"Indicators per country and year\",\n    \"Comments about content per country\",\n    \"Origin of the main indicators)\"\n]\n\n\n\nfiles_description","9295f36b":"#Let's list the columns titles for each dataset\n\nfor df in data_list:\n    print(df.name)\n    print(df.columns)\n    print(\"\\n\")","d0ad5d02":"# Let's start with the analysis of the columns called \"unnamed\"\nprint(country['Unnamed: 31'].unique())\nprint(country_series['Unnamed: 3'].unique())\nprint(data['Unnamed: 69'].unique())\nprint(note['Unnamed: 4'].unique())\nprint(series['Unnamed: 20'].unique())","8c46affd":"#let's visualize the size of each dataset\n\ndef calc_sizes(df_list):\n    \n    datasets = []\n    dataset_sizes = []\n\n    # Calculating number of rows, columns and elements\n    for df in df_list:\n        count_rows = df.shape[0]\n        count_columns = df.shape[1]\n        count_elements = df.size\n\n        datasets.append(df.name)\n        dataset_sizes.append(df.size)\n\n        print(\"The dataset {} contains {} elements, in {} rows and {} columns.\"\n              .format(df.name, count_elements, count_rows, count_columns))\n              \n    # Visualization of dataset sizes\n    hist_size = plt.bar(datasets,dataset_sizes)\n    plt.yscale('log')\n    plt.title(\"Size of the datasets\")\n    plt.ylabel(\"Number of elements\")\n    plt.gca().yaxis.grid(True)\n    plt.show()\n\n\n\ncalc_sizes(data_list)","942ca28d":"# The previous output \"Nan\" confirm that we can delete these columns.\ncountry.drop(columns=['Unnamed: 31'], inplace = True)\ncountry_series.drop(columns=['Unnamed: 3'], inplace = True)\ndata.drop(columns=['Unnamed: 69'], inplace = True)\nnote.drop(columns=['Unnamed: 4'], inplace = True)\nseries.drop(columns=['Unnamed: 20'], inplace = True)","bbf1a7d4":"# After this drop, let's see if we still have duplicates. \nfor df in data_list:\n    rows_initial = df.shape[0]\n    rows_cleared = df.drop_duplicates().shape[0]\n    count_duplicates = rows_initial - rows_cleared\n    \n    print(\"The dataset {} contains {} duplicates.\".format(df.name, count_duplicates))","0d379e67":"#Let's visualize the missing values\n#(We offer different visualisation of the same information)\n\ndef calc_missing2(df_list):\n    \n    # Initialization\n    missing_data = []\n    available_data = []\n    dataset_percent_missing = []\n    datasets = []\n    dataset_sizes = []\n\n    \n    \n    # Calculating available and missing data\n    for df in df_list:\n        count_NaN = df.isnull().sum().sum()\n        percent_NaN = 100 * round(count_NaN \/ df.size,3)\n        missing_data.append(count_NaN)\n        available_data.append(df.size - count_NaN)\n        dataset_percent_missing.append(percent_NaN)\n        datasets.append(df.name)\n        dataset_sizes.append(df.size)\n        \n        print(\"{} cells in the dataset {} are filled with NaN, representing {}% of the dataset.\"\n          .format(count_NaN, df.name, percent_NaN))\n                \n\n\n    # Visual representation\n    pivot = pd.DataFrame({'Dataset sizes':dataset_sizes,\n                          'Available data':available_data},\n                        index = datasets)\n    pivot.plot.bar(figsize = (10,7), rot = 0, color=['black', 'green'])\n    plt.yscale('log')\n    plt.title(\"Elements in datasets\")\n    plt.ylabel(\"Number of elements\")\n    plt.gca().yaxis.grid(True)\n    plt.show()\n    \n    # Second plot gives the percentage of NaN elements in the datasets\n    plt.subplot()\n    plt.bar(datasets, dataset_percent_missing, color=['orange'])\n    plt.title(\"Percentage of NaN in the datasets\")\n    plt.show()\n    print(\"\")   \n    \n    \n    # Representing available and empty cells in a pie chart for each dataset\n    for index in [0,1,2,3,4]:\n        plt.pie([missing_data[index], available_data[index]], \n            labels = ['NaN', 'Available data'], \n            colors = ['orange','green'], labeldistance = None,\n            autopct = lambda x: str(round(x, 2)) + '%', pctdistance = 0.5,\n            startangle = 90)\n        pie_title = \"Ratio Available data VS NaNs in the data set \" + datasets[index]\n        plt.title(pie_title)\n        plt.legend(loc = 'best')\n        plt.show()\n    \ncalc_missing2(data_list)\n","a1e75dbf":"#Let's have an overview of the content\ncountry_series.head(3)","512d8f26":"# Showing some entries with common keywords\ndf_shuffled = shuffle(country_series)","4425e372":"# Number of unique values in the column \"description\"\nlen(df_shuffled[\"DESCRIPTION\"].unique())","70a3b3b2":"# Let's list the occurence of unique values\ndf_shuffled[\"DESCRIPTION\"].value_counts().head(20)","1f6d5085":"# Looking for main keywords in description\n\ndef most_common_words(labels, nb = 20):     # Returns the 20 most common words of the dataset\n    \n    words = []\n    for lab in labels:\n        words += lab.split(\" \")\n    counter = Counter(words)\n    for word in counter.most_common(nb):\n        print(word)\n\n\n\n\nmost_common_words(country_series['DESCRIPTION'].values)","cdb2878f":"# Let's have a look at the dataset\ndf_shuffled = shuffle(note.copy())\ndf_shuffled.head(20)","60d2ef14":"# Number of unique values in the column \"description\"\nlen(df_shuffled['DESCRIPTION'].value_counts())","1a1fd9c7":"# Let's list the occurence of unique values\ndf_shuffled['DESCRIPTION'].value_counts()","dab27dc2":"# Looking for main keywords in description\nmost_common_words(df_shuffled['DESCRIPTION'])","8f9b5ccf":"#In the previous outputs and visualization, we saw that this dataset had a majority of NaNs.\n#We can follow different strategies with the NaNs, but let's start with the first step : \n#How many entries without data do we have in the columns called \"YEAR\" : \ndata['nb_measure'] = data.count(axis = 'columns')-4\n\n\n\n\n# Number of rows with 100% NaNs in the column \"Year\"\nempty_rows = data[data['nb_measure']==0].shape[0]\nnon_empty_rows = data.shape[0] - empty_rows\n\n# Visualization\nplt.pie([empty_rows, non_empty_rows], \n        labels = ['100% NaNs', 'Not 100% empty'], colors = ['orange','green'], labeldistance = None,\n        autopct = lambda x: str(round(x, 2)) + '%', pctdistance = 0.5,\n        startangle = 90)\nplt.title(\"Representation of the split between 100% NaNs columns in the dataset called Data\")\nplt.legend(loc = 'best')\nplt.show()","41979748":"# Let's evaluate if each indicator is given for every country\nprint(\"Total number of indicators = \", series.shape[0])\nprint(\"Mean number of indicator per country = \", data['Country Name'].value_counts().mean())\nprint(\"Standard deviation of indicators per country = \", data['Country Name'].value_counts().std())\nprint(\"\\n\")\n\n# and let's evaluate the opposite : do we have every countries for every indicator\nprint(\"Total number of countries = \", country.shape[0])\nprint(\"Mean number of countries per indicator = \", data['Indicator Code'].value_counts().mean())\nprint(\"Standard deviation of countries per indicator = \", data['Indicator Code'].value_counts().std())","892d9445":"#Let's delete the empty rows of the dataset\ndata_cleaned = data[data['nb_measure']>0]\ndata_cleaned.head(20)","d3b60d04":"#The previous output mention the impossible number of countries. \n#Over 197. \n#It means that we have groups of countries and world region included as well. \n#We need to reorganize these informations. Let's start with the missing country code in Country dataset (= the difference between 241 & 242 in the previous output)\n\nmissing_codes = []\nfor code in data[\"Country Code\"].unique():\n    if code not in country['Country Code'].unique():\n        missing_codes.append(code)\nprint(missing_codes)","a5a8ac2f":"#Now, let's combine the countries per world region, \n#and replace the missing values #fillna\ncountry[['Short Name','Region']].fillna(\"NA\").groupby(['Region']).count()","b9315d8d":"#Now, let's go deeper in details and let's analyze the region through their \"short name\"\ncountry[country['Region'].isna()][['Short Name', \"Country Code\"]]","da67ebbd":"#let's continue our understanding of the countries list per region :\n# Countries in the \"Latin America & Caribbean\" region\ncountry[country['Region'] == \"Latin America & Caribbean\"][['Short Name', \"Country Code\"]]","71c0f97c":"#Let's visualize our geographical region defined by our dataset \"country\"\n\ndef region_mapping(df):\n    \n    # Construction of a dedicated dataset\n    df_wip = df[['Country Code', 'Region']].copy()\n    df_wip['Region_index'] = df_wip['Region'].str.len()\n    \n    # Visualization\n    fig = go.Figure(data=go.Choropleth(\n        locations = df_wip[df_wip['Region'].isna() == False]['Country Code'],\n        z = df_wip[df_wip['Region'].isna() == False]['Region_index'],\n        colorscale = 'ice',\n        reversescale = False,\n        marker_line_color='white',\n        marker_line_width=0.5,\n    ))\n    fig.update_layout(\n        title_text = 'World Regions'\n    )\n    fig.show()\n    \n\nregion_mapping(country)\n","9ff96dba":"#The data have been collected through the continent, but also through the years. \n#We can imagine that some data collection were not done correctly every years as it take times and that the informations are harder to collect in some countries. \n#Moreover, internet was not available to everybody at the same moment. Some countries had to wait more before having access to internet. \n\n\n#In order to answer to the first question, we can see if we have more or less cleaned data through the years. \nsummarise_cleaning = pd.DataFrame(data.iloc[0:,4:-1].count()\/data.iloc[0:,4:-1].shape[0], columns = ['raw dataset'])\nsummarise_cleaning['cleaned dataset'] = data_cleaned.iloc[0:,4:-1].count()\/data_cleaned.iloc[0:,4:-1].shape[0]\n\nplot_cleaning = summarise_cleaning.plot.bar(figsize = (20,5), title = \"Fillrate of dataset called Data (in %)\", color=['black', 'green'])\nplt.ylabel(\"%\")\nplt.xlabel(\"YEARS\")\n\nplot_cleaning.yaxis.grid(True)","5f838c80":"#We have determine that the data before 2000 are not going to be usefull. So let's delete the data from the previous years => Dropping years before 2000\n\n\ndef drop_years(df, start_year, end_year):\n    \n    df_wip = df.copy()\n    year = start_year\n    \n    while year <= end_year:\n        del df_wip[str(year)]\n        year += 1\n    \n    df_wip['nb_measure'] = df_wip.count(axis = 'columns')-5\n    \n    return df_wip[df_wip['nb_measure']>0]\n\n\n\ndata_cleaned = drop_years(data_cleaned, 1970, 1999)","06deb761":"# Now let's visualize the focus on the \"current\" data. \n\ndef count_measures(df, year):\n    \n    counting = df.loc[:,str(year):'2017'].count(axis = 'columns')\n    counting.value_counts(sort = False, normalize = True).plot(kind='bar', color = ['green']).yaxis.grid(True)\n    \n    plot_title = \"Distribution of number of measures from now (2017) and since \" + str(year)\n    plt.title(plot_title)\n    plt.xlabel(\"x = Number of measures on per row\")\n    plt.ylabel(\"Percentage of rows with x measures\")\n    \n    \ncount_measures(data_cleaned, 2000)","aff15b7e":"#Through the analysis, we understood that the data were maybe not collected every years in every countries. \n#Therefore, it is important to know when was the last year when the data were recorded. \n#We will create a new columns in order to show the value and year of the last measure. \n\n\ndef last_measure(df):\n    \n    df_wip = df.copy()\n    year = int(df_wip.columns[4])\n\n    df_wip['last_year'] = df_wip.loc[:,'2000':'2017'].apply(pd.Series.last_valid_index, axis = 1)\n\n    def last_value(x):\n        index= x['last_year']\n        if index is not None:\n            result = x[index]\n        else:\n            result = None\n        return result\n\n    df_wip['last_value'] = df_wip.apply(last_value, axis = 1)\n    df_wip['last_year'] = df_wip['last_year'].apply(lambda x : int(x))\n\n    return df_wip\n\n\n\n\ndata_cleaned = last_measure(data_cleaned)\ndata_cleaned","f3a24f9a":"#Let's list the available indicators. \n\nseries['Topic'].value_counts().plot.bar(figsize = (15,5), grid = True, rot = 90, color =['green'])\nplt.show()","0fc768f3":"#Now let's join the datasets in order to import the main indicators in the dataframe\n\nseries['Series Code'] = series['Series Code'].apply(str.upper)\ndata_cleaned['Indicator Code'] = data_cleaned['Indicator Code'].apply(str.upper)\n\ndata_cleaned = data_cleaned.merge(series[['Series Code',\"Topic\"]], how='left', \n                                        left_on= 'Indicator Code',right_on = 'Series Code')\n\n","012fdc4b":"# Let's make sure that the merging was done successfully :\ndata_cleaned['Topic'].value_counts(dropna = False)","7d0540d6":"# Unfound indicator codes\ndata_cleaned[data_cleaned['Topic'].isnull()]['Indicator Code'].unique()","025f3b10":"# Now let's delete the indicator that we are not going to use. \nirrelevant_topics = ['Primary', 'Expenditures', 'Engaging the Private Sector (SABER)','School Finance (SABER)', \n                     'School Autonomy and Accountability (SABER)', 'Early Childhood Education', 'Pre-Primary',\n                     'Health: Population: Structure', 'Early Child Development (SABER)','Workforce Development (SABER)',\n                     'Social Protection & Labor: Labor force structure', 'School Health and School Feeding (SABER)',\n                     'Social Protection & Labor: Unemployment',\n                     'Economic Policy & Debt: National accounts: US$ at current prices: Aggregate indicators', 'EMIS',\n                     'Economic Policy & Debt: National accounts: US$ at constant 2010 prices: Aggregate indicators',\n                     'Health: Risk factors', 'Health: Population: Dynamics', 'Health: Mortality', 'Background',\n                     'Economic Policy & Debt: National accounts: Atlas GNI & GNI per capita'\n                    ]\n\n\ndef drop_topics(df, topic_list):\n    \n    df_wip = df.copy()\n    initial_size = df_wip.shape[0]\n    \n    for topic in topic_list:\n        df_wip = df_wip[df_wip['Topic'] != topic]\n    \n    final_size = df_wip.shape[0]\n    \n    sizes = [initial_size, final_size]\n    labels = ['Initial','After drops']\n    \n    plt.bar(labels, sizes)\n    plt.title('Comparing number of rows in the dataset after dropping irrelevant topics')\n    plt.ylabel('Number of rows')\n    \n    plt.rcParams['axes.axisbelow'] = True\n    plt.gca().yaxis.grid(True)\n    plt.show()\n    \n    return df_wip\n\n\n\n\ntest = drop_topics(data_cleaned, irrelevant_topics)","038facdb":"#The next slot in the forecast in 2020, then every 5 years until 2100.\n# Let's analyze the indicators with available data after 2017\ndata_cleaned['nb_projected'] = data_cleaned.loc[:,'2020':'2100'].count(axis = 1)\ndata_cleaned[data_cleaned['nb_projected'] > 0]","f9f1415c":"# Now let's analyze the number of indicators and countries \nproj_countries = len(data_cleaned[data_cleaned['nb_projected'] > 0]['Country Code'].unique())\nproj_indic = len(data_cleaned[data_cleaned['nb_projected'] > 0]['Indicator Code'].unique())\n\nprint(\"Projected data is available for {} countries \/ world zones on {} different indicators.\"\n      .format(proj_countries, proj_indic))","6e3c6def":"#And let's calculate the number of countries per projected indicator. \nproj_mean = data_cleaned[data_cleaned['nb_projected'] > 0][['Indicator Code', 'nb_projected']].groupby('Indicator Code').count().mean()[0]\nproj_std = data_cleaned[data_cleaned['nb_projected'] > 0][['Indicator Code', 'nb_projected']].groupby('Indicator Code').count().std()[0]\nprint(\"Mean number of countries per projected indicator : \" + str(proj_mean) + \" (standard deviation = \" + str(proj_std) + \").\")\n\nproj_mean = data_cleaned[data_cleaned['nb_projected'] > 0][['Country Code', 'nb_projected']].groupby('Country Code').count().mean()[0]\nproj_std = data_cleaned[data_cleaned['nb_projected'] > 0][['Country Code', 'nb_projected']].groupby('Country Code').count().std()[0]\nprint(\"Mean number of projected indicators per country : \" + str(proj_mean) + \" (standard deviation = \" + str(proj_std) + \").\")","524e401d":"#Let's list the subjects of these forecasted data\nprojected_data = data_cleaned[data_cleaned['nb_projected'] > 0].copy()\nmost_common_words(projected_data['Indicator Name'].unique(), 50)","3dea9379":"# What are the names of the projected series\nprojected_data['Indicator Name'] = projected_data['Indicator Name'].apply(lambda x : str.replace(x, \"Wittgenstein Projection: \", \"\"))\nprojected_data['Indicator Name']","b2b017e7":"# Let's calculate the number of indicators listed in the forecasted data, which are related to the mean years at school\nlen(projected_data[projected_data['Indicator Name'].str.contains('Mean years of schooling')]['Indicator Name'].unique())","3ee8de54":"# Let's calculate the number of indicators listed in the forecasted data, which are related to the highest level of education\nlen(projected_data[projected_data['Indicator Name'].str.contains(' by highest level of educational attainment')]['Indicator Name'].unique())","fb026348":"# In order to built our conclusion with the most detailled notebook, let's list the countries which have no forecasted data. \n#For these countries, if we want to include them in our analysis, we will have to build a Machine Learning Model (which is not the purpose of the exercice)\ncountries_with_proj = projected_data['Country Name'].unique()\nall_countries = data_cleaned['Country Name'].unique()\n\n[country for country in all_countries if country not in countries_with_proj]","924103e9":"#Let's have a look at the most common words we can find in the indicators names\nmost_common_words(data_cleaned['Indicator Name'], 50)","b454a22c":"# Let's list the indicators related to computer \/ internet access. \ncomputer = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"computer\")][['Indicator Code', 'Indicator Name']]\ncomputer.groupby(['Indicator Name']).count()","0e71db01":"#FOCUS ON THE INDICATORS : Personal computers (per 100 people) (it's code is : IT.CMP.PCMP.P2)\n#According to the previous output, this indicator is strongly represented. \n\n#Therefore, let's see if we have a reliable representation of this indicator through the years + \n#do we have this indicator in enough countries.\n#If yes, we would be able to use this indicator in our final recommandation.\n\n\ndef plot_indicator_distrib(df, indicator):\n    \n    indic_code = df[df['Indicator Name'] == indicator]['Indicator Code'].unique()[0]\n    mean_value = str(round(df[df['Indicator Name'] == indicator]['last_year'].mean()))\n    with_indicator = len(df[df['Indicator Name'] == indicator]['Country Code'].unique())\n    no_indicator = len(df['Country Code'].unique()) - with_indicator\n    mean_measures = str(round(df[df['Indicator Name'] == indicator]['nb_measure'].mean(),1))\n    \n    print('Indicator : ' + indicator)\n    print(\"Indicator code : \" + indic_code)\n    print(\"\")\n    \n    \n    # Visual representation\n    plt_wip = df[df['Indicator Name'] == indicator]['last_year'].value_counts(sort=False).plot.bar()\n    plot_title = \"Distribution of last_year for the \" + indic_code + \" indicator\"\n    plt.title(plot_title)\n    plt.text(0,70, \"mean last_year = \\n\" + mean_value )\n    plt.gca().yaxis.grid(True)\n    plt.show()\n    print(\"\")\n    \n    # Part of represented countries\n    plt.pie([no_indicator, with_indicator], \n            labels = [indic_code + ' is not available', indic_code + ' is available'], \n            colors = ['orange','green'], labeldistance = None,\n            autopct = lambda x: str(round(x, 2)) + '%', pctdistance = 0.5,\n            startangle = 90)\n    \n    pie_title = \"Percentage of countries with available \" + indic_code + \" indicator\"\n    plt.title(pie_title)\n    plt.legend(loc = 'best')\n    plt.show()\n    print(\"\")\n    \n    \n    # Distribution of number of measures in the 2000 - 2017 period\n    plt_measure = df[df['Indicator Name'] == indicator][['Country Code','nb_measure']].groupby(['nb_measure']).count().plot.bar()\n    plt.title(\"Distribution of the number of measures between 2000 and 2017 for the \\n\" +  indic_code + \" indicator\")\n    plt.text(0,40, \"mean number of measures = \\n\" + mean_measures )\n    plt.gca().yaxis.grid(True)\n    plt.xlabel(\"x = Number of measures since 2000\")\n    plt.ylabel(\"Number of countries with x measures\")\n    plt.show()\n    print(\"\")\n\nplot_indicator_distrib(data_cleaned, \"Personal computers (per 100 people)\")\n","065e5662":"#Now, let's evaluate the other indicator related to the number of people using internet : Internet users (per 100 people) (it's code is : IT.NET.USER.P2)\ninternet = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"Internet\")][['Indicator Code', 'Indicator Name']]\ninternet.groupby(['Indicator Name']).count()","b94bd2be":"# Average last mesure and country availability\nplot_indicator_distrib(data_cleaned, \"Internet users (per 100 people)\")","2de6353d":"# Let's list the indicators that could be related to this level of education \nsecondary = data_cleaned[\n    data_cleaned['Indicator Name'].str.contains(\"upper secondary\")\n    & data_cleaned['Indicator Name'].str.contains(\"both sexes\")\n    ][['Indicator Code', 'Indicator Name']]\n\nsecondary.groupby(['Indicator Name']).count().sort_values(by=['Indicator Code'], ascending = False)","de2de9c8":"# We want to know if the data are reliable, so let's check the average last mesure and country availability for the indicator \"Enrolment in upper secondary education, both sexes (number)\"\n#which is mostly represented in this output\nplot_indicator_distrib(data_cleaned, \"Enrolment in upper secondary education, both sexes (number)\")","68f192e6":"# We want to know if the data are reliable, and ratio are usefull for this evaluation. \n# so let's check the \"Gross enrolment ratio, upper secondary, both sexes (%)\" indicator.\n\nplot_indicator_distrib(data_cleaned, \"Gross enrolment ratio, upper secondary, both sexes (%)\")","892d4a15":"# Now that we have done the analysis for the secondary education level, let's do the same think for the tertiary, \n# which is still in the targeted age of our customer. \ntertiary = data_cleaned[\n    data_cleaned['Indicator Name'].str.contains(\"tertiary\")\n    & data_cleaned['Indicator Name'].str.contains(\"both sexes\")\n    ][['Indicator Code', 'Indicator Name']]\n\ntertiary.groupby(['Indicator Name']).count().sort_values(by=['Indicator Code'], ascending = False)","191138f6":"# How many potential students (prospect) could we count in the tertiary education ?\nplot_indicator_distrib(data_cleaned, \"Population of the official age for tertiary education, both sexes (number)\")","6583f5bd":"# Same question about the indicator \"Gross enrolment ratio, tertiary, both sexes (%)\" \nplot_indicator_distrib(data_cleaned, \"Gross enrolment ratio, tertiary, both sexes (%)\")","f4065302":"# Same question about the indicator \"Enrolment in tertiary education, all programmes, both sexes (number)\"\nplot_indicator_distrib(data_cleaned, \"Enrolment in tertiary education, all programmes, both sexes (number)\")","90a5fca7":"#FOCUS ON UPPER SECONDARY\n# For the problematic of our customer, we need to analyze the current number of potential student \/ prospect, but most importantly, \n# we need to evaluate the potential market evolution within the next years ! \n#(reminder : the data are from 2017 and we need to evaluate the data of 2020 as forecast data, even if we are now in 2021)\nprojection = data_cleaned[\n    data_cleaned['Indicator Name'].str.contains(\"Projection\") \n    & data_cleaned['Indicator Name'].str.contains(\"Upper Secondary. Total\")][['Indicator Code', 'Indicator Name']]\n\nprojection.groupby(['Indicator Name']).count()","b26fbe9a":"#Let's visualize these information for Upper Secondary according to the Wittgenstein Projection\n\ndef plot_projections(df, indicator):\n    \n    indic_code = df[df['Indicator Name'] == indicator]['Indicator Code'].unique()[0]\n    with_indicator = len(df[df['Indicator Name'] == indicator]['Country Code'].unique())\n    no_indicator = len(df['Country Code'].unique()) - with_indicator\n    mean_measures = str(round(df[df['Indicator Name'] == indicator]['nb_projected'].mean(),1))\n    \n    # Part of represented countries\n    \n    plt.pie([no_indicator, with_indicator], \n            labels = [indic_code + ' is not available', indic_code + ' is available'], \n            colors = ['red','green'], labeldistance = None,\n            autopct = lambda x: str(round(x, 2)) + '%', pctdistance = 0.5,\n            startangle = 90)\n    \n    pie_title = \"Percentage of countries with available \" + indic_code + \" indicators\"\n    plt.title(pie_title)\n    plt.legend(loc = 'best')\n    \n    plt.show()\n    \n    # Distribution of number of measures from 2020 to 2100\n    \n    plt_measure = df[df['Indicator Name'] == indicator][['Country Code','nb_projected']].groupby(['nb_projected']).count().plot.bar()\n    plt.title(\"Distribution of the number of projections from 2020 to 2100 for the \\n\" +  indic_code + \" indicator\")\n    plt.text(0,40, \"mean number of projections = \\n\" + mean_measures )\n    plt.gca().yaxis.grid(True)\n    plt.xlabel(\"x = Number of measures from 2100\")\n    plt.ylabel(\"Number of countries with x measures\")\n    \n    plt.show()\n    \n\nplot_projections(data_cleaned, \n                 \"Wittgenstein Projection: Population in thousands by highest level of educational attainment. Upper Secondary. Total\")","54062d78":"#FOCUS ON POST SECONDARY\n# For the problematic of our customer, we need to analyze the current number of potential student \/ prospect, but most importantly, \n# we need to evaluate the potential market evolution within the next years ! \n#(reminder : the data are from 2017 and we need to evaluate the data of 2020 as forecast data, even if we are now in 2021)\n\n\n\nprojection = data_cleaned[\n    data_cleaned['Indicator Name'].str.contains(\"Projection\") \n    & data_cleaned['Indicator Name'].str.contains(\"Post Secondary. Total\")][['Indicator Code', 'Indicator Name']]\n\nprojection.groupby(['Indicator Name']).count()","e6056a14":"#Let's visualize these information for Post Secondary according to the Wittgenstein Projection\n\nplot_projections(data_cleaned, \n                 \"Wittgenstein Projection: Population in thousands by highest level of educational attainment. Post Secondary. Total\")","69398398":"# In order to answer this question, let's analyze the unattendance indicator\noos_upper = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"out-of-school\")\n                           & data_cleaned['Indicator Name'].str.contains(\"upper secondary\")\n                           ][['Indicator Code', 'Indicator Name']]\noos_upper.groupby(['Indicator Name']).count().sort_values(by = ['Indicator Code'], ascending = False)","b4b37252":"# Let's focus on the indicator \"Rate of out-of-school youth of upper secondary school age, both sexes (%)\"\nplot_indicator_distrib(data_cleaned, \"Rate of out-of-school youth of upper secondary school age, both sexes (%)\")","637fc7e7":"# Let's list the different indicators and representative ratio \ngraduation = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"graduation ratio\")\n                            & data_cleaned['Indicator Name'].str.contains(\"both\")\n                           ][['Indicator Code', 'Indicator Name']]\ngraduation.groupby(['Indicator Name']).count().sort_values(by = ['Indicator Code'], ascending = False)","5edeb86a":"# Reminder : what is a gross graduation ratio ?\nseries[series['Series Code'] == \"UIS.GGR.2\"]['Long definition'].iloc[0]","178921ca":"# Let's understand the countries ' population level of education thanks to the indicator \"completed upper\". \ngraduation = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"completed upper\")\n                            & data_cleaned['Indicator Name'].str.contains(\"Total\")\n                           ][['Indicator Code', 'Indicator Name']]\ngraduation.groupby(['Indicator Name']).count().sort_values(by = ['Indicator Code'], ascending = False)","81b75cb3":"# now let's visualize these informations \nplot_indicator_distrib(data_cleaned, \n                       \"UIS: Percentage of population age 25+ with completed upper secondary education. Total\")","dc1271e9":"# Same questions but instead of secondary, we analyze the completed education, in global\ngraduation = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"completed\")\n                            & data_cleaned['Indicator Name'].str.contains(\"Total\")\n                           ][['Indicator Code', 'Indicator Name']]\ngraduation.groupby(['Indicator Name']).count().sort_values(by = ['Indicator Code'], ascending = False)","3039da9b":"#Let's visualize the Percentage of population age 25+ with at least completed post-secondary education (ISCED 4 or higher)\nplot_indicator_distrib(data_cleaned, \n                       \"UIS: Percentage of population age 25+ with at least completed post-secondary education (ISCED 4 or higher). Total\")","96dcc83b":"# Let's list the languages related indicators (focus on english)\nenglish = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"English\", \"english\")][['Indicator Code', 'Indicator Name']]\nenglish.groupby(['Indicator Name']).count()","ca008329":"# Let's list the languages related indicators (focus on french)\nfrench = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"French\", \"french\")][['Indicator Code', 'Indicator Name']]\nfrench.groupby(['Indicator Name']).count()","03e74566":"# Let's list the languages related indicators (foreign languages in general) => ability for the population to learn new languages\nlanguage = data_cleaned[data_cleaned['Indicator Name'].str.contains(\"language\", \"Language\")][['Indicator Code', 'Indicator Name']]\nlanguage.groupby(['Indicator Name']).count()","1356b6d9":"# Let's list the indicators related to the number of students per teacher\ndf_pupil = data_cleaned[data_cleaned['Indicator Name'].str.contains('Pupil-teacher ratio')][['Indicator Code', 'Indicator Name']]\ndf_pupil.groupby(['Indicator Name']).count()","3684fe08":"# Let's visualize the ratio pupil-teacher ratio for the age of our target : upper secondary \nplot_indicator_distrib(data_cleaned,\"Pupil-teacher ratio in upper secondary education (headcount basis)\")","8dc31507":"# Let's visualize the ratio pupil-teacher ratio for the age of our target : tertiary\nplot_indicator_distrib(data_cleaned,\"Pupil-teacher ratio in tertiary education (headcount basis)\")","b3ed553e":"# Let's list the income related indicators \ndf_GDP = data_cleaned[data_cleaned['Indicator Name'].str.contains('GDP')][['Indicator Code', 'Indicator Name']]\ndf_GDP.groupby(['Indicator Name']).count()","452557a8":"#We want to evaluate the financial potential of the countries => Let's work with the GDP indicator\n# Per capita gross domestic product (GDP) is a financial metric that breaks down a country's economic output per person and is calculated by dividing the GDP of a nation by its population.\n\nplot_indicator_distrib(data_cleaned,\"GDP per capita (constant 2005 US$)\")","fbf37bc2":"#We want to evaluate the financial potential of the countries => Let's work with the PPP indicator\n# GDP per capita (PPP based) is gross domestic product converted to international dollars using purchasing power parity rates and divided by total population. An international dollar has the same purchasing power over GDP as a U.S. dollar has in the United States.\nplot_indicator_distrib(data_cleaned,\"GDP per capita, PPP (constant 2011 international $)\")","4c9757fb":"# According to the previous indicators discovery, here is the list of the selected indicators we will be working with \nindicators_selection = [\n    'IT.NET.USER.P2', #  Internet access\n    'UIS.E.3', # Number of students in upper secondary\n    'SE.SEC.ENRR.UP', # Enrolment rate in upper secondary\n    'SP.TER.TOTL.IN', # Population in age for tertiary\n    'SE.TER.ENRR', # Enrolment rate in tertiary\n    'SE.TER.ENRL', # Number of students in tertiary\n    'PRJ.POP.ALL.3.MF', # Projected number of students in secondary\n    'PRJ.POP.ALL.4.MF', # Projected number of students in tertiary\n    'UIS.EA.3.AG25T99', # Completion rate in upper secondary\n    'UIS.EA.4T6.AG25T99', # Completion rate in post secondary\n    'UIS.PTRHC.3', # Pupil-teacher ratio in upper secondary\n    'UIS.PTRHC.56', # Pupil - teacher ratio in tertiary\n    'NY.GDP.PCAP.PP.KD' # GDP per capita (purchasing power parity)\n]","ca0785f8":"# Checking indicators\nall_indicators = data_cleaned['Indicator Code'].unique()\n[indic for indic in indicators_selection if indic not in all_indicators]","7d0aed20":"df_selection = pd.DataFrame()\n\nfor indic in indicators_selection:\n    df_selection = df_selection.append(data_cleaned[data_cleaned['Indicator Code'] == indic])\n    \n[indic for indic in indicators_selection if indic not in df_selection['Indicator Code'].unique()]","a64c36ea":"# Let's include geographical & income information to our dataframe with indicators. \ndf_selection = df_selection.merge(country[['Country Code','Region', 'Income Group']], how='left',\n                                  left_on= 'Country Code',right_on = 'Country Code')\n\n\n# Let's include the definition and unit of measure to our dataframe\ndf_selection = df_selection.merge(series[['Series Code','Short definition','Long definition', 'Unit of measure']], \n                                  how='left',left_on= 'Indicator Code',right_on = 'Series Code')\n\n\n# After these merging, what is the filling rate of our columns ? \ndf_selection.count() \/ df_selection.shape[0]","fbdd8583":"# List the entries with no regions => need to be filled\ndf_selection[df_selection['Region'].isna()]['Country Name'].unique()","5cd64406":"# According to the previous output, let's fill the missing regions\n\ndf_gibraltar = df_selection[df_selection['Country Name'] == 'Gibraltar'].copy()\ndf_gibraltar['Region'] = 'Europe & Central Asia'\n\ndf_nauru = df_selection[df_selection['Country Name'] == 'Nauru'].copy()\ndf_nauru['Region'] = 'East Asia & Pacific'\n\ndf_virgin = df_selection[df_selection['Country Name'] == 'British Virgin Islands'].copy()\ndf_virgin['Region'] = 'Latin America & Caribbean'\n\ndf_selection = df_selection[(df_selection['Country Name'] != 'Gibraltar') \n             & (df_selection['Country Name'] != 'Nauru')\n             & (df_selection['Country Name'] != 'British Virgin Islands')\n            ].append([df_gibraltar, df_nauru, df_virgin])","f83df43f":"# Let's clean the dataset and drop the useless columns for our problematic \ndf_selection = df_selection.drop(['2017', 'Series Code_x', 'Series Code_y', 'Short definition', 'Unit of measure'], axis=1)","6f3991c0":"#Let's preview this new dataset\ndf_selection.head()","de5e5ce8":"# Let's focus on the indicator about internet access (code : IT.NET.USER.P2 )\ninternet = df_selection[df_selection['Indicator Code'] == \"IT.NET.USER.P2\"]","00054842":"# In order to answer to the problematic, we have to set our statistics according to geographical zones\n\ndef world_stats(df, col = 'last_value'):    \n    descr_world = df[df['Region'].isna() == False][[col]].describe().round(2).rename(columns = {col : 'World'})\n    descr_region = df[df['Region'].isna() == False].pivot(index = 'Country Name', columns='Region',values=col).describe().round(2)\n\n    return descr_world.join(descr_region)\n#or return descr_world.join(descr_region).T #revert the columns and rows\n\n\nworld_stats(internet)","7582b6b3":"# Before any conclusion, let's evaluate the impact of the outliers on our data\n\ndef region_boxplot(df, col = 'last_value'):\n    \n    # Creation of a dedicated pivot table\n    pivot = df[df['Region'].isna() == False].pivot(index = 'Country Name', columns='Region',values= col)\n    \n    # Plotting \n    region_bp = pivot.boxplot(\n        figsize = (15,5), \n        rot = 0\n    )\n    \n    plt.tight_layout()\n    plt.title(df['Indicator Name'].unique()[0])\n    plt.show()\n\n\nregion_boxplot(internet)\n","9fdefffa":"# We can see several outliers in the \"Europa & Central Asia\" zone. \n# Let's list them in order to evaluate if we should keep them or delete them. \ninternet[internet['Region'] == 'Europe & Central Asia'][\n    ['Country Name', 'last_value']].sort_values(by=['last_value']).head(5)","46aed979":"# We can see several outliers in the \"Sub-Saharan Africa\" zone. \n# Let's list them in order to evaluate if we should keep them or delete them. \ninternet[internet['Region'] == 'Sub-Saharan Africa'][\n    ['Country Name', 'last_value']].sort_values(by=['last_value']).tail(5)","875a35ac":"#According to the previous outputs, let's built our countries ranking, related to the internet access indicator. \n#INTERNATIONAL RANKING (top and worse)\n\n\ndef world_topN(df, col = 'last_value', world_n = 10, last = True):\n    \n    # Creating the top 10 countries table\n    top_list = df[df['Region'].isna() == False][[\n        'Country Name', col]].sort_values(by = col, ascending = False).head(world_n)['Country Name'].values\n    top_score = df[df['Region'].isna() == False][['Country Name', col\n                                                 ]].sort_values(by = col, ascending = False).head(world_n)[col].values\n    # Addind last 10 (worse) countries\n    if last == True:\n        last_list = df[df['Region'].isna() == False][[\n            'Country Name', col]].sort_values(by = col, ascending = True).head(world_n)['Country Name'].values\n        last_score = df[df['Region'].isna() == False][[\n            'Country Name', col]].sort_values(by = col, ascending = True).head(world_n)[col].values\n        \n        world_top = pd.DataFrame({'Top countries' : top_list, 'Top scores' : top_score,\n                                  'Worse countries' : last_list , 'worse scores' : last_score},\n                                 index = np.arange(1,world_n + 1,1)) \n    \n    else:\n        world_top = pd.DataFrame({ 'Top countries' : top_list, 'Top scores' : top_score} , index = np.arange(1,world_n + 1,1))\n    \n    return world_top\n\n\nworld_topN(internet, world_n = 10, last = True)\n","541a6ff7":"#According to the previous outputs, let's built our countries ranking, related to the internet access indicator. \n#Europe & Central Asia RANKING (top and worse)\n\nworld_topN(internet[internet['Region'] == 'Europe & Central Asia'], world_n = 5, last = True)","188b91ae":"#Let's visualize the previous output on a map\n\ndef map_indic(df, indic_column, color_scale, reverse_scale = True, map_title = \"\", scale_title = \"\"):\n    \n    fig = go.Figure(data=go.Choropleth(\n        \n        # Informations\n        locations = df[df['Region'].isna() == False]['Country Code'],\n        z = df[df['Region'].isna() == False][indic_column],\n        text = df[df['Region'].isna() == False]['Country Name'],\n        \n        # Map design\n        colorscale = color_scale,\n        reversescale = reverse_scale,\n        marker_line_color='darkgray',\n        marker_line_width=0.5,\n        \n        colorbar_title = scale_title,\n    ))\n    \n    fig.update_layout(\n        title_text = map_title\n    )\n    \n    fig.show()\n    \n\nmap_indic(internet, indic_column = 'last_value', color_scale = 'ice', reverse_scale = True,\n          map_title = \"Recommandation ranking for your international development - based on the population access to the Internet\", scale_title = \"Internet users per 100 people\")","9f8ffd72":"# Let's focus on the indicators about upper secondary and tertiary students. \n#We had several related indicators. \n#We will combine these indicators in order to provide a combined evaluation and recommandation.\n#Let's create a dedicated dataset.\n\n\n# Let's also merging total student population in a single dataset\ndf_nb_upper_sec = df_selection[df_selection['Indicator Code'] == \"UIS.E.3\"][[\n    'Country Name', 'Country Code','Region','Indicator Name', 'last_value']].rename(columns = {'last_value' : 'Upper secondary'})\n\ndf_nb_tertiary = df_selection[df_selection['Indicator Code'] == \"SE.TER.ENRL\"][[\n    'Country Name', 'Country Code','Region','Indicator Name', 'last_value']].rename(columns = {'last_value' : 'Tertiary'})\n\ndf_students = df_nb_upper_sec.merge(df_nb_tertiary, on = ['Country Name', 'Country Code', 'Region'], how = 'outer')\ndf_students['Total students'] = df_students['Upper secondary'].replace(np.NaN, 0) + df_students['Tertiary'].replace(np.NaN, 0)\n\n# Adding intenet access for ponderation\ndf_internet2 = internet[['Country Name', 'Country Code','Region', 'Indicator Name', 'last_value']].rename(\n    columns = {'last_value' : 'Internet Access'})\n\ndf_students = df_students.merge(df_internet2, on = ['Country Name', 'Country Code', 'Region'], how = 'outer')\ndf_students['Accessible students'] = (df_students['Total students'] * df_students['Internet Access'] \/ 100).round(0)","c80b8c5d":"# In order to answer to the problematic, we have to set our statistics according to geographical zones\nworld_stats(df_students, 'Accessible students')","c91c60da":"# Let's visualize the total amount of prospect, per geographical zone \nnb_student = df_students[['Region','Upper secondary','Tertiary', 'Accessible students']]\n\n(nb_student.groupby(by = 'Region').sum()\/1000000).plot.bar(figsize = (18,5), color = ['darkblue','slateblue','darkviolet'])\n\nplt.title(\"Potential students per geographical zone\")\nplt.xlabel(\"\")\nplt.xticks(rotation = 0)\nplt.ylabel(\"Number of students (in millions)\")\nplt.gca().yaxis.grid(True)\nplt.show()","10e695d2":"# Before any conclusion, let's evaluate the impact of the outliers on our data\nregion_boxplot(df_students, col = 'Accessible students')","7875134c":"#for a ranking focused on 'East Asia & Pacific' :\n\nworld_topN(df_students[df_students['Region'] == 'East Asia & Pacific'], col = 'Accessible students', world_n = 5, last = True)","71fd3c02":"#for a ranking focused on 'South Asia' :\nworld_topN(df_students[df_students['Region'] == 'South Asia'], col = 'Accessible students', world_n = 3, last = True)","84ab4158":"# Let's evaluate the ranking if we focus on the international accessible students. \nworld_topN(df_students, col = 'Accessible students' ,world_n = 10, last = True)","a71a6893":"#Let's visualize the previous output on a map  \nmap_indic(df_students, indic_column = 'Accessible students', color_scale = 'ice', reverse_scale = True,\n          map_title = \"Recommandation ranking for your international development - based on the accessibility of the students\", scale_title = \"Number of students\")","3983539c":"# In order to include all the necessary data from the forecast already established, we create a new dataframe \ndf_future_students = df_selection[df_selection['Indicator Code'] == 'PRJ.POP.ALL.4.MF'].drop(\n    columns = ['2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016',\n              'nb_measure','last_year','last_value','nb_projected'])","e671210d":"# Let's visualize the forecast provided by the dataset (from 2020 until 2100) for the evolution per Region\n(df_future_students.groupby(by = 'Region').sum()\/1000).T.plot(figsize = (10,15))\nplt.title(\"Expected population with a max of tertiary education\", fontsize = 15, fontweight = 'bold')\nplt.xlabel(\"Years\", fontsize = 13, fontstyle = 'italic', backgroundcolor = 'lightgrey')\nplt.ylabel(\"Population (millions)\", fontsize = 13, fontstyle = 'italic',backgroundcolor = 'lightgrey')\nplt.gca().yaxis.grid(True)\nplt.show()","f9cc5acb":"# Adding columns to calculate the projected student population growth\ndf_future_students['5y_growth'] = (df_future_students['2025'] - df_future_students['2020']) \/ df_future_students['2020']\ndf_future_students['10y_growth'] = (df_future_students['2030'] - df_future_students['2020']) \/ df_future_students['2020']\ndf_future_students['15y_growth'] = (df_future_students['2035'] - df_future_students['2020']) \/ df_future_students['2020']\ndf_future_students['20y_growth'] = (df_future_students['2040'] - df_future_students['2020']) \/ df_future_students['2020']","1e4277db":"# Let's use the previous output in order to rank the top and worse countries \n#FOCUS ; in the next 5 years => 2020-2025\nworld_topN(df_future_students, col = '5y_growth', world_n = 10, last = True)","b8b4c625":"# Let's use the previous output in order to rank the top and worse countries \n#FOCUS ; in the next 10 years => 2020-2030\nworld_topN(df_future_students, col = '10y_growth', world_n = 10, last = True)","4f6d62de":"# Let's use the previous output in order to rank the top and worse countries \n#FOCUS ; in the next 15 years => 2020-2035\nworld_topN(df_future_students, col = '15y_growth', world_n = 10, last = True)","b70e9da6":"# Let's use the previous output in order to rank the top and worse countries \n#FOCUS ; in the next 20 years => 2020-2040\nworld_topN(df_future_students, col = '20y_growth', world_n = 10, last = True)","d8d7664d":"# Let's visualize the previous output\nmap_indic(df_future_students, indic_column = '20y_growth', color_scale = 'ice', reverse_scale = True,\n          map_title = \"Recommandation ranking for your international development - based on student growth within the next 20 years\", scale_title = \"Growth factor\")","ae9621ac":"# Let's list the different indicators related to the student enrollment ...\ndf_oos_prospect = df_selection[['Country Name', 'Country Code','Region']].drop_duplicates()\n\ndf_potential_tertiary = df_selection[df_selection['Indicator Code'] == 'SP.TER.TOTL.IN'][\n    ['Country Name', 'Country Code','Region','last_value']].rename(\n    columns = {'last_value' : 'Tertiary potential'}).replace(np.NaN, 0)\n\ndf_enrolment_tertiary = df_selection[df_selection['Indicator Code'] == 'SE.TER.ENRR'][\n    ['Country Name', 'Country Code','Region','last_value']].rename(\n    columns = {'last_value' : 'Tertiary enrolment rate'})\n\ndf_enrolment_upper = df_selection[df_selection['Indicator Code'] == 'SE.SEC.ENRR.UP'][\n    ['Country Name', 'Country Code','Region','last_value']].rename(\n    columns = {'last_value' : 'Upper secondary enrolment rate'})\n\n","e5849b89":"# ... and now let's merge them. \ndf_oos_prospect = df_oos_prospect.merge(\n    df_potential_tertiary, on = ['Country Name', 'Country Code', 'Region'], how = 'left').merge(\n    df_enrolment_tertiary, on = ['Country Name', 'Country Code', 'Region'], how = 'left').merge(\n    df_nb_upper_sec, on = ['Country Name', 'Country Code', 'Region'], how = 'left').merge(\n    df_enrolment_upper, on = ['Country Name', 'Country Code', 'Region'], how = 'left').merge(\n    df_internet2, on = ['Country Name', 'Country Code', 'Region'], how = 'left').drop(\n    columns = ['Indicator Name_x','Indicator Name_y'])\n\ndf_oos_prospect.describe(include=\"all\")","499b528e":"# We can see that some enrollment rates are above the limit of 100%. \n#We could delete the data. \n#We decide to adjust them and reduce them to 100%. \ndf_oos_prospect['Tertiary enrolment rate'] = df_oos_prospect['Tertiary enrolment rate'].apply(lambda x : 100 if x>100 else x )\ndf_oos_prospect['Upper secondary enrolment rate'] = df_oos_prospect['Upper secondary enrolment rate'].apply(lambda x : 100 if x>100 else x )","31a5e32e":"#Let's check if it is now in reliable values.\ndf_oos_prospect","94cacc55":"# With these new informations, let's create a new indicator : \"out-of-school prospects indicator\"\ndf_oos_prospect['OOS upper sec'] = df_oos_prospect['Upper secondary'] * (1-df_oos_prospect['Upper secondary enrolment rate']\/100) \/ (df_oos_prospect['Upper secondary enrolment rate']\/100)\n    \ndf_oos_prospect['OOS tertiary'] = df_oos_prospect['Tertiary potential'] * (1-df_oos_prospect['Tertiary enrolment rate']\/100)\n    \ndf_oos_prospect['Accessible prospect'] = (df_oos_prospect['OOS upper sec'] + df_oos_prospect['OOS tertiary']) * df_oos_prospect['Internet Access']","1aa7a274":"#Before taking any decision, let's have an overview of the traditional statistics of this new indicator.\nworld_stats(df_oos_prospect, 'Accessible prospect')","9d3925e3":"#According to the previous output, let's evaluate now the best and worse countries for our customer international development. \nworld_topN(df_oos_prospect, col = 'Accessible prospect' ,world_n = 10, last = True)\n","64a9aef1":"# Let's visualize the previous outputs.\nmap_indic(df_oos_prospect, indic_column = 'Accessible prospect', color_scale = 'ice', reverse_scale = True,\n          map_title = \"Recommandation ranking for your international development - based on prospect accessibility\", scale_title = \"Number of potential students\")","85ed09b8":"# We have several indicators and values linked to the country income. Let's create a new dataframe for this combo.\ndf_income = df_selection[df_selection['Income Group'].isna() == False][[\n    'Country Name', 'Country Code','Region', 'Income Group']].drop_duplicates()","7c65f74b":"# Let's harmonize the different values linked to the incomes\ndf_income['Income index'] = df_income['Income Group']\n\ndf_income['Income index'] = df_income['Income index'].replace('Low income', 1).replace('Lower middle income', 2).replace(\n    'Upper middle income',3).replace('High income: nonOECD',4).replace('High income: OECD',5)","91b84235":"#Let's visualize the previous outputs.\n#In order to bring diversity in our code (but not in our result) we will use a different mapping color setting.\n\nincome_scale = [[0, 'rgb(255,0,0)'],\n                [0.25, 'rgb(255,102,0)'],\n                [0.50, 'rgb(255,255,0)'],\n                [0.75, 'rgb(0,255,0)'],\n                [1, 'rgb(0,128,0)']]\n\nmap_indic(df_income, indic_column = 'Income index', color_scale = income_scale, reverse_scale = False,\n          map_title = \"Recommandation ranking for your international development - based on Income group\", scale_title = \"Income group\")","1747dfb6":"# If we want to know if the geographical zone have a high income because it includes a high number of countries, we can use this pivot table. \ndf_income[['Income Group','Region','Country Name']].groupby(by = ['Income Group', 'Region'], as_index = False).count().pivot(\n    index = 'Income Group', columns = 'Region', values = 'Country Name')","840c3947":"# As for the previous indicators, let's create a dedicated dataframe\ndf_PTratio_sec = df_selection[df_selection['Indicator Code'] == 'UIS.PTRHC.3']\ndf_PTratio_ter = df_selection[df_selection['Indicator Code'] == 'UIS.PTRHC.56']\n\n#Reminder : \n#'UIS.PTRHC.3', # student-teacher ratio in upper secondary\n#'UIS.PTRHC.56', # student - teacher ratio in tertiary","15b23a2d":"# As for the previous indicators, let's evaluate the global statistics of this new dataframe\n#FOCUS : student-teacher ratio in upper secondary\nworld_stats(df_PTratio_sec, col = 'last_value')","b4f02129":"# As for the previous indicators, let's evaluate the global statistics of this new dataframe\n#FOCUS : student - teacher ratio in tertiary\nworld_stats(df_PTratio_ter, col = 'last_value')","1ee089cf":"#Let's evaluate the outliers\n#FOCUS : student-teacher ratio in upper secondary\n\nregion_boxplot(df_PTratio_sec, col = 'last_value')","ad3dff84":"#Let's evaluate the outliers\n#FOCUS : student - teacher ratio in tertiary\n\nregion_boxplot(df_PTratio_ter, col = 'last_value')","c1efc5a3":"#According to the previous outputs, let's see the ranking for student-teacher ratio in upper secondary\nworld_topN(df_PTratio_sec, col = 'last_value' ,world_n = 10, last = True)","c5543a24":"#According to the previous outputs, let's see the ranking for student - teacher ratio in tertiary\nworld_topN(df_PTratio_ter, col = 'last_value' ,world_n = 10, last = True)","25832aff":"#Let's visualize the previous outputs\n#FOCUS : student-teacher ratio in upper secondary\n\nmap_indic(df_PTratio_sec, indic_column = 'last_value', color_scale = 'ice', reverse_scale = True,\n          map_title = \"Recommandation ranking for your international development - based on Pupil - Teacher ratio in upper secondary\", scale_title = \"Pupils per teacher\")","a49f1e29":"#Let's visualize the previous outputs\n#FOCUS : student - teacher ratio in tertiary\n\ndf_PTratio_ter2 = df_PTratio_ter.copy()\ndf_PTratio_ter2[\"last_value\"] = np.clip(df_PTratio_ter[\"last_value\"], 0, 70)\n\nmap_indic(df_PTratio_ter2, indic_column = 'last_value', color_scale = 'ice', reverse_scale = True,\n          map_title = \"Recommandation ranking for your international development - based on Pupil - Teacher ratio in tertiary\", scale_title = \"Pupils per teacher\")","32f9fa14":"# Let's create the final dataset with the combinaison of our previous indicators datasets. \ndf_ranks = df_selection[df_selection['Region'].isna() == False][\n    ['Country Name','Country Code','Region','Income Group']].drop_duplicates()","116b7da7":"# => normalized internet access\ndef normalize_serie(serie):\n    \n    s_min = serie.min()\n    s_max = serie.max()\n    \n    norm_serie = serie.apply(lambda x : (x-s_min) \/ (s_max - s_min))\n    \n    return norm_serie\n\n\ndf_ranks = df_ranks.merge(internet[['Country Code','last_value']], on = 'Country Code', how = 'left')\ndf_ranks['Internet Access'] = normalize_serie(df_ranks['last_value'])\ndf_ranks = df_ranks.rename(columns = {'last_value' : 'Internet access'})","c59a6f18":"# => normalized number of students\ndf_ranks = df_ranks.merge(df_students[['Country Code','Accessible students']], on = 'Country Code', how = 'left')\ndf_ranks['Accessible Students'] = normalize_serie(np.clip(df_ranks[\"Accessible students\"], 0, 10000000))","0923a4dc":"# => normalized growth rate on 20 years\ndf_ranks = df_ranks.merge(df_future_students[['Country Code','20y_growth']], on = 'Country Code', how = 'left')\ndf_ranks['20 years growth'] = normalize_serie(df_ranks['20y_growth'])","50988820":"# => normalized pupils per teacher ratio\ndf_ranks = df_ranks.merge(df_PTratio_sec[['Country Code','last_value']], on = 'Country Code', how = 'left')\ndf_ranks['Pupils per Teacher(sec)'] = normalize_serie(df_ranks['last_value'])\ndf_ranks = df_ranks.rename(columns = {'last_value' : 'PTR_sec'})\n\ndf_ranks = df_ranks.merge(df_PTratio_ter[['Country Code','last_value']], on = 'Country Code', how = 'left')\ndf_ranks['Pupils per Teacher(ter)'] = normalize_serie(df_ranks['last_value'])\ndf_ranks = df_ranks.rename(columns = {'last_value' : 'PTR_ter'})","6f8f59ad":"df_ranks","18f50f55":"import seaborn as sns\ndf_heatmap = df_ranks[[\n    'Internet Access','Accessible Students','20 years growth',\n    'Pupils per Teacher(sec)','Pupils per Teacher(ter)'\n    ]].copy().set_index(df_ranks['Country Name'])\n\ndf_heatmap[\"sum\"] = df_heatmap.sum(axis = 1)\ndf_heatmap = df_heatmap.sort_values(by = ['sum'], ascending = False)","42b8d5db":"# Let's visualize our top 15 countries. \n#According to the indicators, the ranking can be different. \n#It will be up to the company and decision maker to take the strategical decision which makes more sens according to the current company strategy and SWOT. \n\nprint(\"\")\nsns.heatmap(df_heatmap[0:15].drop(columns = ['sum']), cmap = 'coolwarm', square = False, linewidth = .5, annot = True)\nplt.title('Top 15 countries recommanded, according to related education indicators')\nplt.show()","a3f1d9af":"**Objectives of this project : international development?**\n\n- Which countries have a strong customer potential for our services?\n- For each of these countries, how will this customer potential evolve?\n- In which countries should the company operate first?\n\n","09e2c756":"<a name=\"3.2.6\"><\/a>\n    3.2.6 - Part of student leaving school in secondary and tertiary \/ with no degree","6e90c737":"> \ud83d\udc4c We have 7 different regions in our dataset","4d5a6873":"\ud83d\udca1**In which countries should you launch your international development ?**\ud83d\udca1\n\n- The decision maker will have the possibility to choose between the best 15 countries according to the priorities of the company.\n- The answer will depends from the following informations : \n    - values of the company\n    - SWOT of the company\n    - current strategy of the company\n- For example : \n    - example 1 : If the company is looking for accessible students, India, US, Brazil and China will be the best countries to focus. If the second most important indicator of the company is the ratio Pupils per teacher, then the most reliable choice would be to focus on India. \n    - example 2 : if the company decide that the most important indicator is the access to internet, we will focus on UK, Japan, Rep Korea and Germany. And if the second most important indicator is the accessibility to the students, then the best choice would be to focus on Japan. \n\n\n","f1a6ccdd":"<a name=\"3.2.2\"><\/a>\n    3.2.2 - Student in secondary education","76192903":"<a name=\"3.1\"><\/a>\n    3.1 - Introduction","01882157":"<a name=\"2.10\"><\/a>\n    2.10 - Merging of the datasets","d844df48":"<a name=\"3.2.7\"><\/a>\n    3.2.7 - Part of student with a completed upper education","efe5532d":"> \u26a0\ufe0f Even after deleting rows with no data, columns fillrate rarely exceed 40%, confirming that each indicator is not availiable for every year in the dataset => we cannot reasonably rely on a single year to build further analysis.","60a16949":"# Plan","ca2af07d":"<a name=\"5.6\"><\/a>\n5.6 Conclusions of the recommandation","1b32f851":"Thanks to this heatmap, we will be able to see the best correlation between the geographical area and the different indicators. ","af5deb8c":"\ud83c\udf0f **Which country should you select for your international development in EdTech ?** \ud83d\uddfa\n\n*Project for CentraleSupElec *\n\n--------------","10a81836":"> \u2714\ufe0f no more duplicates","ffc3fbc6":"> some countries do not appears (where is the French Guinea?) and some \"countries\" should not be included in this country list as they are geographical zones (US Virgun Islands...)","736d08dc":"<a name=\"1.2\"><\/a>\n    1.2 Parameters","e870b7c8":"<a name=\"1.4\"><\/a>\n    1.4 - Description of the datasets","a5df7bbb":"Students with no degree is an interesting indicator because it could means :\n- either the students with no degree could be interesting prospect as they could use a diploma\n- either these students are not interested at all by any diploma, online or not","cbaab712":"<a name=\"3.2.5\"><\/a>\n    3.2.5 - Students not attending school in secondary or tertiary level","db0473bc":"<a name=\"1.1\"><\/a>\n    1.1 Import of the packages","13a95ba8":"<a name=\"5.2\"><\/a>\n5.2 Heatmap","978eb3ac":"> The forecast of the dataset include the following key information : each one of the 308 indicators is given for the same 167 countries.","d356cf24":"<a name=\"3.2.10\"><\/a>\n    3.2.10 - GPD","17d559e5":"<a name=\"4.2.5\"><\/a>\n        4.2.5 Focus : Income group (stats + mapping + outliers + top countries recommandation)","ba90ef97":"<a name=\"4.2.2\"><\/a>\n        4.2.2 Focus : Upper Secondary and tertiary students (stats + mapping + outliers + top countries recommandation)","9e9a64e7":"> \ud83d\udc40 We are now in 2021, but the dataset was collected in 2017. So every data after 2017 were forecast. So even if we took the data until 2020, it will not be correct because we will mix real data collected on the fiel and forecast. For this reason, this dataset include the following period : 2000-2017","0ecc49d9":"<a name=\"4.2.1\"><\/a>\n        4.2.1 Focus : Internet access (stats + mapping + outliers + top countries recommandation)","e6c9a040":"After 2017, the datasets also included forecast based on previous data. Even if now we are in 2021, we are going to analyze these information for our recommandation.","0196a3d8":"<a name=\"2.4\"><\/a>\n    2.4 - Content of \"note\" dataset","52dce00a":"> The output confirms that we could select the indicator IT.CMP.PCMP.P2 (= personal computer) in order to measure the international computer access for the students. (indicator is available in the majority of the countries and is providing a reliable number of measures since 2000). ","c9165a47":"\ud83d\udca1 **feedbacks**\n- \u2714\ufe0f We don't have duplicates anymore\n- Some dataset have enough informations, some dataset have too many NaN for any reliable conclusion.\n- We could start thinking that each country could use different indicators for the same purpose. Some countries might have more difficulties to gather these informations as well. Moreover, internet was not available to every cities for ages. So some informations might be missing, or differently organized, or not organized every years according to the countries. ","51c385ae":"**Mission**\n\n\nPerform a pre-exploratory analysis of the dataset : \n\n\n- Validate the quality of this dataset (does it contain a lot of missing, duplicate data?)\n\n- Describe the information in the dataset (number of columns? number of rows?)\n\n- Select the information that seems relevant to answer the problem (which columns contain information that may be useful to answer the problem of the company?)\n\n- Determine orders of magnitude of classical statistical indicators for different geographical areas and countries of the world (mean\/median\/standard deviation by country and continent or geographical block)\n\n- Information about the company : start-up based in France providing online courses for \"lyc\u00e9e\" and \"university\" level. ","10590274":"<a name=\"2.9\"><\/a>\n    2.9 - Bivariate analysis - drop unecessary informations","3177f6df":"It could be interesting to start the market analysis with this information : the current courses are in french and in english. Translation of the courses are possible but are expensive. So the first steps of the international development should be to focus on countries whith a high ratio of english\/french speaking people in the population. ","3eb5e9c6":"------------------------","3f167f6d":"<a name=\"2.6\"><\/a>\n    2.6 - Bivariate analysis - List of countries","6cd502bb":"Reminder : GPD = Gross Domestic Product: the total value of goods and services produced by a country in a year ([source](https:\/\/dictionary.cambridge.org\/dictionary\/english\/gdp))","bf3ccef7":"<a name=\"2.5\"><\/a>\n    2.5 - Content of \"data\" dataset","29d5820b":"<a name=\"2.7\"><\/a>\n    2.7 - Bivariate analysis - Years","e61388a3":"* [1 - Upload the environment](#1)\n    * [1.1 - Import of the packages](#1.1)\n    * [1.2 - Parameters](#1.2)\n    * [1.3 - Loading the datasets](#1.3)\n    * [1.4 - Description of the datasets](#1.4)\n    * [1.5 - First conclusions](#ccl1)\n* [2 - Data exploratory & Cleaning](#2)\n    * [2.1 - Visualisation](#2.1)\n    * [2.2 - Delete duplicates](#2.2)\n    * [2.3 - Content of \"country_series\" dataset](#2.3)\n    * [2.4 - Content of \"note\" dataset](#2.4)\n    * [2.5 - Content of \"data\" dataset](#2.5)\n    * [2.6 - Bivariate analysis- List of countries](#2.6)\n    * [2.7 - Bivariate analysis- Years](#2.7)\n    * [2.8 - Bivariate analysis- Last year of record](#2.8)\n    * [2.9 - Bivariate analysis- drop unecessary informations](#2.9)\n    * [2.10 - Merging of the datasets](#2.10)\n* [3 - Indicators](#3)\n    * [3.1 - introduction](#3.1)\n    * [3.2 - Selection of the indicators](#3.2)\n        * [3.2.1 - Access to computer & internet](#3.2.1)\n        * [3.2.2 - Student in secondary education](#3.2.2)\n        * [3.2.3 - Number of student in tertiary (number \/ percentage)](#3.2.3)\n        * [3.2.4 - Projection of student population growth](#3.2.4)\n        * [3.2.5 - Students not attending school in secondary or tertiary level](#3.2.5)\n        * [3.2.6 - Part of student leaving school in secondary and tertiary \/ with no degree](#3.2.6)\n        * [3.2.7 - Part of student with a completed upper education](#3.2.7)\n        * [3.2.8 - Level capacity in English \/ French](#3.2.8)\n        * [3.2.9 - Pupil - teacher ratio](#3.2.9)\n        * [3.2.10 - GPD](#3.2.10)\n* [4 - Features engineering & final cleaning](#4)\n    * [4.1 - Selection of the indicators](#4.1)\n    * [4.2 - Creation of the final dataframe](#4.2)\n        * [4.2.1 - Focus : Internet access (stats + mapping + outliers + top countries recommandation)](#4.2.1)\n        * [4.2.2 - Focus : Upper Secondary and tertiary students (stats + mapping + outliers + top countries recommandation)](#4.2.2)\n        * [4.2.3 - Focus : Predicted student population growth (stats + mapping + outliers + top countries recommandation)](#4.2.3)\n        * [4.2.4 - Focus : Prospect students not yet enrolled in universities (stats + mapping + outliers + top countries recommandation)](#4.2.4)\n        * [4.2.5 - Focus : Income group (stats + mapping + outliers + top countries recommandation)](#4.2.5)\n        * [4.2.6 - Focus : Student - Teacher ratio (stats + mapping + outliers + top countries recommandation)](#4.2.6)\n* [5 - Countries recommandation](#5)\n    * [5.1 - Normalized](#5.1)\n    * [5.2 - Heatmap](#5.2)\n    * [5.3 - Conclusions of the recommandation](#5.3)\n","c090e595":"\ud83c\udfaf **Competencies assessed:**\n\n- Master the basic operations of the Python language for Data Science\n\n- Manipulating data with specialized Python libraries\n\n- Make a graphical representation using an adapted Python library\n\n- Setting up a Python environment\n\n- Use a Jupyter notebook to facilitate code writing and collaboration","e1c5c538":"Next steps \/ How to improve this notebook : I could also work on a scoring ratio, in order to provide an automatic grade for each country \/ geographical zone.\n\n**Thank you for your interest in this project** \n\nLeave a comment or upvote this notebook ! \n\nI have used some external ressources ([1](https:\/\/www.kaggle.com\/petegore\/open-classroom-ds-p1),[2](https:\/\/github.com\/AHollierDS\/ParcoursDS_Projet2)) and built the strategy with my Mentor.\n\nMy portefolio is accessible if you want to have a look, [click here](https:\/\/www.kaggle.com\/vanessatribet\/portfolio).  ","c9e52ba1":"<a name=\"2.2\"><\/a>\n    2.2 - Delete duplicates ","06a684f3":"> as we have forecasted data until 2100, we could go on with the evaluation, but we can assume that the market might change a lot, the needs will change, the competitors will be different, .... So a 20 years long term vision is already a good \"start\".","7a0a1a0d":"<a name=\"3.2\"><\/a>\n    3.2 - Selection of the indicators","1e7ddde2":"Students who are not attending school is an interesting indicator. \n- Either it means that these students could be interested users as they need to catch up some basics\n- Either it means that they will not attend online classes neither and that the countries with high unattendance rate should not be a priority in the business development. ","d9bf0d42":"> We confirm that the indicator IT.NET.USER.P2 (= internet user) is also relevant. For our problematic, this indicator is key as internet is the mandatory service required in order to study. ","b1cdaa07":"<a name=\"3.2.3\"><\/a>\n    3.2.3 - Number of student in tertiary (number \/ percentage)","645ffd94":"<a name=\"3\"><\/a>\n    3 - Indicators","87f3a32a":"<a name=\"4.1\"><\/a>\n    4.1 Selection of the indicators","70ef9388":"> \u26a0\ufe0f 2 feedbacks : We have a strange high number of countries (officially only 197). We have a difference between 241 and 242 countries.","8e0279fa":"<a name=\"4.2.3\"><\/a>\n        4.2.3 Focus : Predicted student population growth (stats + mapping + outliers + top countries recommandation)","253e7500":"**Dataset:**\n\n- World Bank data are available at: https:\/\/datacatalog.worldbank.org\/dataset\/education-statistics\n\n- The World Bank\u2019s \u201cEdStats All Indicator Query\u201d lists 4000 international indicators describing access to education, graduation and information on teachers, education-related expenditures... We can find additional info on this site: http:\/\/datatopics.worldbank.org\/education\/\n","fc8e46c1":"<a name=\"5.1\"><\/a>\n5.1 normalization","3a1d3229":"<a name=\"1.3\"><\/a>\n    1.3 - Loading the datasets","397e4c13":"<a name=\"3.2.4\"><\/a>\n    3.2.4 - Projection of student population growth","bab0fcc4":"------------","4ec83b77":"<a name=\"4.2\"><\/a>\n    4.2 Creation of the final dataframe","49fde677":"-------------------------","7d99d915":"\u2753 Why do we have so many missing values ? \n\nMaybe it is because each indicator is not used by every country. ","4a1e8e4d":"<a name=\"3.2.8\"><\/a>\n    3.2.8 - Level capacity in English \/ French","18142d51":"For any business development, we need to understand the reason why the prospect could need the provided services. In our case, one of the reason could be that their are too many students per teacher \/ not enough teachers in some region. ","52c2c731":"<a name=\"ccl1\"><\/a>\n\n\ud83d\udca1 **Feedbacks**\n- According to these informations, the datasets which could provide answers to our problematic are : \"data\" & \"series\". Indeed, \"data\" gives the indicator value for a given country and a given year. It could also include forecast for the next decades. \"series\" will be usefull for the understanding of our indicators.\n- As country names are already available in the dataset \"Data\". Therefore, we don't need to merge with the dataset \"Country\".\n- Dataset country-series & note : the titles of the columns are not providing enough informations to us. We don't know yet if we are going to use them or not.\n- \u274c Each dataset contain columns called \"unnamed\". Thir content is empty. \n- \u274c We have duplicates\n- \u274c We have NaNs\n- We have 3665 indicators => the difficulty would be to select the ones that make sens and that have enough measures to be reliable","028b5294":"<a name=\"3.2.9\"><\/a>\n    3.2.9 - Pupil - teacher ratio","b196fcd0":"> 27 countries are not registered under any region","35047bca":"> \u26a0\ufe0f As the indicators have different scale and values, we need to normalize them !","4e8120b6":"- We have more than 30 indicators available\n\n\n- Some key indicators might be more usefull than others : \n    - \"Attainement\"\n    - \"Population\"\n    - \"Secondary\"\n    - \"Tertiary\"\n    - ... \n    \n- We also need to include indicators about internet access because the services of our client are provided online. ","41e15e9a":"<a name=\"3.2.1\"><\/a>\n    3.2.1 - Access to computer & internet","a312b772":"> This dataset is very similar to the Country_series dataset (the column \"year\" is missing). ","ce71d15f":"<a name=\"2.3\"><\/a>\n    2.3 - Content of \"country_series\" dataset","6e98dea5":"<a name=\"5\"><\/a>\n5. Countries recommandation","43b798bb":"<a name=\"4\"><\/a>\n4. Features engineering & final cleaning","df6855e2":"<a name=\"4.2.4\"><\/a>\n        4.2.4 Focus : Prospect students not yet enrolled in universities (stats + mapping + outliers + top countries recommandation)","3e8b57fd":"> We can see some irrelevant data (income and geographical should not be in the same columns)","485b6ea8":"> Our problematic is to provide recommandation for a futur international development. So we need to base our analysis on reliable and not too old data => not need to keep data from before 2000.","1856e47e":"<a name=\"1\"><\/a>\n## 1 - Upload the environment\n","6e566e1a":"<a name=\"4.2.6\"><\/a>\n        4.2.6 Focus : Student - Teacher ratio (stats + mapping + outliers + top countries recommandation)","e75c488e":"<a name=\"2\"><\/a>\n# 2 - Data exploratory & Cleaning\n<a name=\"2.1\"><\/a>\n    2.1 - Visualisation","a027756b":"The customer brief include the target of their services : they offer online training content for a high school and university audience.\nSo we should focus on this level of education, for males & females. ","c7046260":"------------------------------","f4021f93":"<a name=\"2.8\"><\/a>\n    2.8 - Bivariate analysis - Last year of record","2b471745":"----------------------------","369ed439":"Now that we have described, analysed, selected, merged the different indicators, according to the customer problematic, now, we can provide countries recommandations."}}