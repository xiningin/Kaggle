{"cell_type":{"1c0997db":"code","fcdd20c7":"code","54221ef3":"code","97aed23f":"code","7aaa80ab":"code","8b52f3a0":"code","222d8cf2":"code","a3ae4d6c":"code","b302a48e":"code","8679f05a":"code","63edc03b":"code","0809c730":"code","a18bca36":"code","a1ead935":"code","fa8d1fdd":"code","790fde99":"code","5413a827":"code","bc5a1655":"code","d7ed3641":"code","91e62f9d":"code","0977b078":"code","4eb34ecb":"code","d636c3c3":"code","5b357524":"code","a9e7fd95":"code","e0afcafe":"code","0f8cab2e":"code","00acf444":"code","7b06560e":"code","f2510f8a":"code","3d4f4c6c":"code","bea161d9":"code","7e51c1fb":"code","c67d1d4b":"code","a29794b1":"code","88e42c60":"code","1fcd3727":"code","aa0667db":"code","016a8d50":"code","badd0605":"code","76c61fc8":"code","e123c9e7":"code","02c715e1":"code","d11ef31b":"code","b4e8320b":"code","59564c67":"code","3eb30836":"code","a2b67f0f":"code","0adc214a":"code","e7c3c3c3":"code","3b7b79e8":"code","715da0ca":"code","3df86f62":"code","c480a142":"code","699f82c9":"code","47c1f8bb":"code","ff19cf98":"code","5473cc01":"code","51533767":"code","c9826c86":"code","31d1cd5c":"code","85ac1820":"code","160c2a78":"code","782fa6c4":"code","156ac5d8":"code","b44720c2":"code","d542ab0f":"code","6ee33fef":"code","35daa1cf":"code","ce22ddaf":"code","dff27963":"code","0ddfc99a":"code","75f934fd":"code","6ce46cc0":"code","6fc08d41":"code","46d9fb80":"code","1d790776":"code","f5c4b506":"code","31c243a3":"markdown","6b4ff72d":"markdown","4748b6b5":"markdown","fa5316ad":"markdown","81238661":"markdown","b0620b1f":"markdown","fa73f0a5":"markdown","3ff29918":"markdown","50084c80":"markdown","8b45bc73":"markdown","10944b8e":"markdown","ea0ebabd":"markdown","cb981a85":"markdown"},"source":{"1c0997db":"# Import libraries to store data\nimport pandas as pd\nimport numpy as np\n\n# Import libraries to visualize data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Feature selection\nfrom boruta import BorutaPy\n\n# Import libraries to process data\nfrom sklearn.preprocessing import StandardScaler\n\n# Import libraries to classify data and score results\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Import libraries used in functions and for feedback\nimport os\nimport gc\nimport logging\nimport warnings\nimport pandas_profiling","fcdd20c7":"# Settings\npath = os.getcwd()\ngc.enable()\n%matplotlib inline\npd.options.display.max_seq_items = 150\npd.options.display.max_rows = 150\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings(\"ignore\")","54221ef3":"# Kaggle kernel: IS_LOCAL = False\nIS_LOCAL = False\nif(IS_LOCAL):\n    PATH='..\/input\/'\nelse:\n    PATH='..\/input\/competicao-dsa-machine-learning-dec-2019\/'","97aed23f":"print(os.listdir(PATH))","7aaa80ab":"# Logger\ndef get_logger():\n    FORMAT = '[%(levelname)s] %(asctime)s: %(name)s: %(message)s'\n    logging.basicConfig(format=FORMAT)\n    logger = logging.getLogger('Main')\n    logger.setLevel(logging.DEBUG)\n    return logger\n\nlogger = get_logger()","8b52f3a0":"logger.info('Start load data')","222d8cf2":"# Read in data into a dataframe\ntrain = pd.read_csv(os.path.join(PATH, 'dataset_treino.csv'))\ntest = pd.read_csv(os.path.join(PATH, 'dataset_teste.csv'))","a3ae4d6c":"logger.info('Start exploratory data analysis')","b302a48e":"# Show dataframe columns\nprint(train.columns)","8679f05a":"# Display top of dataframe\ntrain.head()","63edc03b":"# Display the shape of dataframe\ntrain.shape","0809c730":"# See the column data types and non-missing values\ntrain.info()","a18bca36":"# See the column data types\ntrain.dtypes","a1ead935":"# Unique values by features\ntrain.nunique(dropna=False, axis=0)","fa8d1fdd":"# Missing values by features\ntrain.isnull().sum(axis=0)","790fde99":"# Statistics of numerical features\ntrain.describe().T","5413a827":"# Correlation map for train dataset\ncorr = train.corr()\n_ , ax = plt.subplots( figsize =( 18 , 18 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = False, annot_kws = {'fontsize' : 12 })","bc5a1655":"'''\n# Using pandas_profiling\n# Display the report in a Jupyter notebook\nprofile_train = train.profile_report(style={'full_width':True}, title='Pandas Profiling Report')\nprofile_train\n'''","d7ed3641":"'''\n# Generate a HTML report file\n#profile_train.to_file(output_file=\"train.html\")\n'''","91e62f9d":"# Show dataframe columns\nprint(test.columns)","0977b078":"# Display top of dataframe\ntest.head()","4eb34ecb":"# Display the shape of dataframe\ntest.shape","d636c3c3":"# See the column data types and non-missing values\ntest.info()","5b357524":"# See the column data types\ntest.dtypes","a9e7fd95":"# Unique values by features\ntest.nunique(dropna=False, axis=0)","e0afcafe":"# Missing values by features\ntest.isnull().sum(axis=0)","0f8cab2e":"# Statistics of numerical features\ntest.describe().T","00acf444":"# Correlation map for train dataset\ncorr = test.corr()\n_ , ax = plt.subplots( figsize =( 18 , 18 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = False, annot_kws = {'fontsize' : 12 })","7b06560e":"'''\n# Using pandas_profiling\n# Display the report in a Jupyter notebook\nprofile_test = test.profile_report(style={'full_width':True}, title='Pandas Profiling Report')\nprofile_test\n'''","f2510f8a":"'''\n# Generate a HTML report file\n#profile_test.to_file(output_file=\"test.html\")\n'''","3d4f4c6c":"logger.info('Start feature engineering')","bea161d9":"# Preprocessing before data engineering\ntrain_dim = train.shape[0]\n\n# Predict feature\npredict = train['target']\n\n# New dataframe\ndata = pd.concat([train, test]).reset_index(drop=True)\n\n# Drop the columns\ndata = data.drop(['ID', 'target'], axis=1)","7e51c1fb":"# See the column data types and non-missing values\ndata.info()","c67d1d4b":"logger.info('Start processing missing values')","a29794b1":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","88e42c60":"missing_values_table(data)","1fcd3727":" # Fill missing data with 'None'\nfor col in data.columns[data.dtypes == 'object']:\n    data[col] = data[col].fillna('None')","aa0667db":"# Fill missing data with media\ndata.fillna(data.mean(), inplace=True)","016a8d50":"missing_values_table(data)","badd0605":"# Label encoding\nfor col in data.columns[data.dtypes == 'object']:\n    data[col] = data[col].factorize()[0]","76c61fc8":"# Kaggle: divide dataset of train and test\ntrain = data[:train_dim]\ntest = data[train_dim:]\n\n# Append the predict feature\ntrain['target'] = predict","e123c9e7":"# See the column data types and non-missing values\ntrain.info()","02c715e1":"# See the column data types and non-missing values\ntest.info()","d11ef31b":"# Separating predictors and target features\nX = train.drop(['target'], axis=1)\ny = train['target']\n\nX = X.values\ny = y.values\ny = y.ravel()\n\n# Random Forest Classifier\nrf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\nrf.fit(X, y)\n\n# Define the feature selection method\nfeat_selector = BorutaPy(rf, n_estimators=100, verbose=2, random_state=1)\n\n# Search for all relevant features\nfeat_selector.fit(X, y)\n\n# Check selected features\nfeat_selector.support_\n\n# Check feature ranking\nfeat_selector.ranking_\n\n# Call call transform () on training data to filter features\nX_train_filtered = feat_selector.transform(X)\n\n# Show the dataset shape\nX_train_filtered.shape","b4e8320b":"# Test dataset\nX_test = test\nX_test = X_test.values\nX_test_filtered = feat_selector.transform(X_test)\nX_test_filtered.shape","59564c67":"logger.info('Scaling features')","3eb30836":"# Separate out the features and targets\nX_train = X_train_filtered.copy()\ny_train = train['target']\nX_test = X_test_filtered.copy()","a2b67f0f":"'''\n# Separate out the features and targets\nX_train = train.drop(['target'], axis=1)\ny_train = train['target']\nX_test = test\n'''","0adc214a":"X_train.shape","e7c3c3c3":"X_test.shape","3b7b79e8":"y_train.shape","715da0ca":"# Create the scaler object\nscaler = StandardScaler()\n# Fit on the training data\nscaler.fit(X_train)\n# Transform both the training and testing data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","3df86f62":"gc.collect()","c480a142":"logger.info('Prepare model')","699f82c9":"!nvidia-smi","47c1f8bb":"# XGBoost\ndef run_model(model, X_tr, y_tr, useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n    \n    # Cross-Validation\n    if useTrainCV:\n        xgb_param = model.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_tr, label=y_tr)\n        \n        print ('Start cross validation')\n        cvresult = xgb.cv(xgb_param, \n                          xgtrain, \n                          num_boost_round=model.get_params()['n_estimators'], \n                          nfold=cv_folds,\n                          metrics=['logloss'],\n                          stratified=True,\n                          verbose_eval=True,\n                          early_stopping_rounds=early_stopping_rounds)\n\n        model.set_params(n_estimators=cvresult.shape[0])\n        best_tree = cvresult.shape[0]\n        print('Best number of trees = {}'.format(best_tree))\n        \n    # Fit\n    model.fit(X_tr, y_tr, eval_metric='logloss')\n        \n    # Prediction\n    train_pred = model.predict(X_tr)\n    train_pred_prob = model.predict_proba(X_tr)[:,1]\n    \n    # Log and Chart\n    print(\"Log Loss (Train): %f\" % log_loss(y_tr, train_pred_prob))\n    print(\"Log Loss (Test): %f\" % cvresult['test-logloss-mean'][best_tree-1])\n    \n    feature_imp = pd.Series(model.feature_importances_.astype(float)).sort_values(ascending=False)\n    \n    plt.figure(figsize=(18,8))\n    feature_imp[:25].plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')\n    plt.tight_layout()","ff19cf98":"# Model params\nmodelXGB = XGBClassifier(learning_rate = 0.1,\n                         n_estimators = 300,\n                         max_depth = 5,\n                         min_child_weight = 1,\n                         gamma = 0,\n                         subsample = 0.8,\n                         colsample_bytree = 0.8,\n                         objective = 'binary:logistic',\n                         n_jobs = -1,\n                         tree_method = 'gpu_hist',\n                         scale_pos_weight = 1)","5473cc01":"print(modelXGB)","51533767":"# Run model\nrun_model(modelXGB, X_train, y_train)","c9826c86":"# Make predictions in testing dataset\ntest_pred_prob = modelXGB.predict_proba(X_test)[:,1]","31d1cd5c":"gc.collect()","85ac1820":"# Defining the parameters that will be tested in GridSearch\n# max_depth e min_child_weight\nparam_v1 = {\n    'max_depth':range(2,10),\n    'min_child_weight':range(1,6)\n}\n\ngrid_1 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 300, \n                                                max_depth = 5,\n                                                min_child_weight = 1, \n                                                gamma = 0, \n                                                subsample = 0.8, \n                                                colsample_bytree = 0.8,\n                                                objective = 'binary:logistic',\n                                                tree_method = 'gpu_hist',\n                                                scale_pos_weight = 1),\n                                                param_grid = param_v1, \n                                                scoring = 'neg_log_loss',\n                                                n_jobs = -1,\n                                                verbose = 1,\n                                                iid = False, \n                                                cv = 5)\n\n# Fit and get the best grid parameters\ngrid_1.fit(X_train, y_train)\ngrid_1.best_params_, grid_1.best_score_","160c2a78":"gc.collect()","782fa6c4":"# Defining the parameters that will be tested in GridSearch\n# gamma\nparam_v2 = {\n    'gamma':[i\/10.0 for i in range(0,10)]\n}\n\ngrid_2 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 300, \n                                                max_depth = grid_1.best_params_['max_depth'],\n                                                min_child_weight = grid_1.best_params_['min_child_weight'], \n                                                gamma = 0, \n                                                subsample = 0.8, \n                                                colsample_bytree = 0.8,\n                                                objective = 'binary:logistic',\n                                                tree_method = 'gpu_hist',\n                                                scale_pos_weight = 1),\n                                                param_grid = param_v2, \n                                                scoring = 'neg_log_loss',\n                                                n_jobs = -1,\n                                                verbose = 1,\n                                                iid = False, \n                                                cv = 5)\n\n# Fit and get the best grid parameters\ngrid_2.fit(X_train, y_train)\ngrid_2.best_params_, grid_2.best_score_","156ac5d8":"gc.collect()","b44720c2":"# Defining the parameters that will be tested in GridSearch\n# subsample e colsample_bytree\nparam_v3 = {\n    'subsample':[i\/10.0 for i in range(4,10)],\n    'colsample_bytree':[i\/10.0 for i in range(4,10)]\n}\n\ngrid_3 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 300, \n                                                max_depth = grid_1.best_params_['max_depth'],\n                                                min_child_weight = grid_1.best_params_['min_child_weight'], \n                                                gamma = grid_2.best_params_['gamma'], \n                                                subsample = 0.8, \n                                                colsample_bytree = 0.8,\n                                                objective = 'binary:logistic',\n                                                tree_method = 'gpu_hist',\n                                                scale_pos_weight = 1),\n                                                param_grid = param_v3, \n                                                scoring = 'neg_log_loss',\n                                                n_jobs = -1,\n                                                verbose = 1,\n                                                iid = False, \n                                                cv = 5)\n\n# Fit and get the best grid parameters\ngrid_3.fit(X_train, y_train)\ngrid_3.best_params_, grid_3.best_score_","d542ab0f":"gc.collect()","6ee33fef":"# Defining the parameters that will be tested in GridSearch\n# reg_alpha\nparam_v4 = {\n    'reg_alpha':[0, 0.001, 0.005]\n}\n\ngrid_4 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 300, \n                                                max_depth = grid_1.best_params_['max_depth'],\n                                                min_child_weight = grid_1.best_params_['min_child_weight'], \n                                                gamma = grid_2.best_params_['gamma'], \n                                                subsample = grid_3.best_params_['subsample'], \n                                                colsample_bytree = grid_3.best_params_['colsample_bytree'],\n                                                objective = 'binary:logistic',\n                                                tree_method = 'gpu_hist',\n                                                scale_pos_weight = 1),\n                                                param_grid = param_v4, \n                                                scoring = 'neg_log_loss',\n                                                n_jobs = -1,\n                                                verbose = 1,\n                                                iid = False, \n                                                cv = 5)\n\n# Fit and get the best grid parameters\ngrid_4.fit(X_train, y_train)\ngrid_4.best_params_, grid_4.best_score_","35daa1cf":"gc.collect()","ce22ddaf":"# Creating the XGB model with all optimizations\n# reducing Learning Rate and increasing the number of estimators\nmodelXGB_v2 = XGBClassifier(learning_rate = 0.001, \n                            n_estimators = 40000, \n                            max_depth = grid_1.best_params_['max_depth'],\n                            min_child_weight = grid_1.best_params_['min_child_weight'],\n                            gamma = grid_2.best_params_['gamma'], \n                            subsample = grid_3.best_params_['subsample'],\n                            colsample_bytree = grid_3.best_params_['colsample_bytree'],\n                            reg_alpha = grid_4.best_params_['reg_alpha'], \n                            objective = 'binary:logistic', \n                            n_jobs = -1,\n                            tree_method = 'gpu_hist',\n                            scale_pos_weight = 1)","dff27963":"print(modelXGB_v2)","0ddfc99a":"# Run model\nrun_model(modelXGB_v2, X_train, y_train)","75f934fd":"# Make predictions in testing dataset\ntest_pred_prob = modelXGB_v2.predict_proba(X_test)[:,1]","6ce46cc0":"logger.info(\"Prepare submission\")","6fc08d41":"# Read in data into a dataframe\nsubmission = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))","46d9fb80":"submission['PredictedProb'] = test_pred_prob.reshape(test_pred_prob.shape[0])","1d790776":"submission.to_csv('submission.csv', index=False)","f5c4b506":"submission.head(20)","31c243a3":"### Scaling Features","6b4ff72d":"### Model Optimization ","4748b6b5":"# Competi\u00e7\u00e3o DSA de Machine Learning - Set\/2019\n# Maicon Moda","fa5316ad":"## Exploratory Data Analysis","81238661":"## Load Data","b0620b1f":"### Feature Selection","fa73f0a5":"## End","3ff29918":"## Feature Engineering","50084c80":"## Import Packages and Initial Settings","8b45bc73":"### Label Encoding","10944b8e":"## Submission","ea0ebabd":"## Machine Learning","cb981a85":"### Processing Missing Values"}}