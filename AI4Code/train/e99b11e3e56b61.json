{"cell_type":{"b37dd719":"code","1a0add10":"code","10d1b524":"code","614dbac8":"code","790e9fde":"code","32011b59":"code","9eb547c7":"code","e048742e":"code","92355ab8":"code","8a230e68":"code","16096995":"code","8d7cd7e2":"code","8e99b1ee":"code","b9362589":"code","043fd5a0":"code","90e6f931":"code","d06255f4":"code","9ad1e310":"code","101fe640":"code","e77a8539":"code","9f258afb":"code","4b4b71e1":"code","0dee1657":"code","a6a86489":"code","ff6f4050":"code","bc5efca2":"code","d5091725":"code","088b3fdd":"code","76eea960":"code","97f93ad4":"code","7de2f6bb":"code","160e2280":"code","eaa249cc":"code","f2657eaf":"code","356d69a7":"code","23629b26":"code","3fcc8261":"code","e3ff9bcd":"code","85fca822":"code","178b7204":"code","aa9c4cb9":"code","5ab9e905":"code","9ba57f42":"code","253c8460":"code","cba13018":"code","0ed2ac62":"code","a197fc0d":"code","e8256588":"code","d94175b7":"code","f2a1e702":"code","254a6dd4":"code","b66bcaa2":"code","24c262ad":"code","cba097bb":"code","8b9c105d":"code","24a3122c":"code","2c7f1dbd":"code","be08673c":"code","2a9a10f7":"code","5547dc1b":"code","ad85501f":"code","299ed6f1":"code","21ff8c7c":"code","534c78ef":"code","da5ad58b":"code","40d5147e":"code","9c0dfe0c":"code","4c721362":"code","0b77005a":"code","29c168a0":"code","4368f686":"code","e2faa9c7":"code","cb11b32b":"code","91ad80c0":"code","0088e96d":"code","75c2a788":"code","d6808a64":"code","0cdfe440":"code","e453f84e":"code","68239f8d":"markdown","b15f6449":"markdown","af8d487d":"markdown","86cf5ccb":"markdown","5ed20b43":"markdown","af233426":"markdown","e353a158":"markdown","13fcd43e":"markdown","0c85faa0":"markdown","818c74e3":"markdown","d59dcb9c":"markdown","3c9ea8cc":"markdown","533ce449":"markdown"},"source":{"b37dd719":"# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom patsy import dmatrices\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom pandas import DataFrame","1a0add10":"data = pd.read_csv('..\/input\/loan-default-prediction\/train_v2.csv.zip',low_memory=False)","10d1b524":"data.shape","614dbac8":"data.head()","790e9fde":"#Identification of variables and data types\ndata.info()","32011b59":"data.describe()\n","9eb547c7":"data.isnull().sum().sum()","e048742e":"mis_val=data.isnull().sum()","92355ab8":"  mis_val_percent = 100 * data.isnull().sum() \/ len(data)","8a230e68":"def tableformissingvalues(df):\n \n        missingvalues = df.isnull().sum()\n        percent = 100 * df.isnull().sum() \/ len(df)\n        table = pd.concat([missingvalues,percent], axis=1)\n        tablerenamed = table.rename(\n            columns = {0 : 'Missing Values', 1 : 'Percentage'})\n        \n#         # Sort the table by percentage of missing descending\n#         tablerenamed= tablerenamed[\n#             mtablerenamed.iloc[:,1] != 0].sort_values(\n#         'Percentage', ascending=False).round(1)\n        \n        # Print some summary information\n#         print (\"Your selected dataframe has \" + str(data.shape[1]) + \" columns.\\n\"      \n#             \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n#               \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return tablerenamed","16096995":"tableformissingvalues(data).head(50)","8d7cd7e2":"#filling missing values with mean\ndata.fillna(data.mean(), inplace=True)","8e99b1ee":"data.shape","b9362589":"#Adding new binary clasifier column\ndata['Classifier'] = [0 if x ==0 else 1 for x in data['loss']] ","043fd5a0":"data.shape","90e6f931":"data.head()","d06255f4":"#dropping all the columns which still have missing values\ndata.dropna(inplace=True)\n","9ad1e310":"data.shape","101fe640":"data.describe()","e77a8539":"plt.figure(figsize=(5,5))\ndata['loss'].plot(kind='density')","9f258afb":"data.info()\n","4b4b71e1":"#data.to_csv (r'C:\\Users\\abeer\\Desktop\\trainingdata.csv', index = False, header=True)","0dee1657":"trainingdata = pd.read_csv('..\/input\/training-data',low_memory=False)","a6a86489":"trainingdata.shape\n","ff6f4050":"trainingdata.isnull().sum().sum()","bc5efca2":"plt.hist(trainingdata['Classifier'],color = 'yellow', edgecolor = 'black',  bins = int(100\/5))","d5091725":"sns.countplot(x ='Classifier', data = trainingdata) ","088b3fdd":"correlations_data = trainingdata.corr()['Classifier'].sort_values()","76eea960":"plt.figure(figsize=(50,35))\n\nsns.heatmap(correlations_data,fmt='.1g',vmin=-1, vmax=1, center= 0)","97f93ad4":"# # Correlations between Features and Target\n\n# Find all correlations and sort \n\n# Print the most negative correlations\nprint(correlations_data.head(15), '\\n')\n\n# Print the most positive correlations\nprint(correlations_data.tail(15))","7de2f6bb":"#removing the columns which have  smame value in all the rows\nfor i in trainingdata.columns:\n    if len(set(trainingdata[i]))==1:\n        trainingdata.drop(labels=[i], axis=1, inplace=True)","160e2280":"trainingdata.shape","eaa249cc":" #let\u2019s now find out the number of columns that are of the object data type and figure out how we can make those values numeric.\nprint(\"Data types and their frequency\\n{}\".format(trainingdata.dtypes.value_counts()))","f2657eaf":"#The categprical input data in the data frame\ntrainingdata.select_dtypes(include=['object'])\n#after execution of the above line the data seems to be redundant as it ranges from 0-96424400336838002265208913920, and if it is truly categorical, we wuld dummify it. but observing the range of values, we see that dummification would not be very useful for our features, so we will drop them","356d69a7":"for i in trainingdata.select_dtypes(include=['object']).columns:\n    trainingdata.drop(labels=i, axis=1, inplace=True)","23629b26":"trainingdata.shape","3fcc8261":"zeroes = trainingdata[trainingdata['Classifier'] == 0] \nzeroes.shape","e3ff9bcd":"#top 500 zeroes\nzeroes=zeroes[:500]\nzeroes.shape","85fca822":"ones = trainingdata[trainingdata['Classifier'] == 1] \nones.shape","178b7204":"#top 500 ones\nones=ones[:500]\nones.shape","aa9c4cb9":"frames=[zeroes,ones]","5ab9e905":"#this data will be used for training model, and it is also balanced\ntrain=pd.concat(frames)\ntrain.shape","9ba57f42":"Y1 = train['Classifier'] # dependent variable\nX1 = train.drop('Classifier', axis=1)\n\nvif  = [variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])]\n","253c8460":"vif","cba13018":"VIF = DataFrame (vif,columns=['VIF Score'])","0ed2ac62":"VIF[\"features\"] = X1.columns\n","a197fc0d":"VIF.shape","e8256588":"VIF1 = VIF[VIF['VIF Score'] > 5.0] ","d94175b7":"VIF1.shape","f2a1e702":"list1=list(VIF1['features']) \nlist1","254a6dd4":"train.shape","b66bcaa2":"train=train.drop(list1, axis = 1)","24c262ad":"train.shape","cba097bb":"# # # Split Into Training and Testing Sets\n# Splitting data into training and testing\nfrom sklearn.model_selection import train_test_split\n# Separate out the features and targets\nfeatures = train.drop(columns=['Classifier','loss'])\n\ntargets = pd.DataFrame(train['Classifier'])\n\n# Split into 80% training and 20% testing set\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size = 0.2, random_state = 42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","8b9c105d":"# # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","24a3122c":"# Convert y to one-dimensional array (vector)\ny_train = np.array(y_train).reshape((-1, ))\ny_test = np.array(y_test).reshape((-1, ))","2c7f1dbd":"X_train","be08673c":"X_test\n","2a9a10f7":"testdata=pd.read_csv('..\/input\/loan-default-prediction\/test_v2.csv.zip',low_memory=False)","5547dc1b":"testdata.fillna(testdata.mean(), inplace=True) ","ad85501f":"#dropping all the columns which still have missing values\ntestdata.dropna(inplace=True)","299ed6f1":"testdata.isnull().sum().sum()","21ff8c7c":"#removing the columns which have  smame value in all the rows\nfor i in testdata.columns:\n    if len(set(testdata[i]))==1:\n        testdata.drop(labels=[i], axis=1, inplace=True)","534c78ef":" #let\u2019s now find out the number of columns that are of the object data type and figure out how we can make those values numeric.\nprint(\"Data types and their frequency\\n{}\".format(testdata.dtypes.value_counts()))","da5ad58b":"for i in testdata.select_dtypes(include=['object']).columns:\n    testdata.drop(labels=i, axis=1, inplace=True)","40d5147e":"list2=list1","9c0dfe0c":"testdata=testdata.drop(list2, axis = 1)","4c721362":"testdata.head()\n#Since testdata does not have loss and classifier column, we will eliminate loss column from train also\n","0b77005a":"train.head()","29c168a0":"# Function to calculate mean absolute error\ndef crossvalidation(X_train, y_train, model):\n    # Applying k-Fold Cross Validation\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n    return accuracies.mean()\n\n# Takes in a model, trains the model, and evaluates the model on the test set\ndef trainmodel(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(testdata)\n    model_cross = crossvalidation(X_train, y_train, model)\n    \n    # Return the performance metric\n    return model_cross","4368f686":"# # Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnaive = GaussianNB()\nnaive_cross = trainmodel(naive)\n\nprint('Naive Bayes Performance on the test set: Cross Validation Score = %0.4f' % naive_cross)","e2faa9c7":"NBprediction=naive.predict(testdata)","cb11b32b":"# # Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrandom = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nrandom_cross = trainmodel(random)\n\nprint('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)","91ad80c0":"RFprediction=random.predict(testdata)\nRFprediction","0088e96d":"from sklearn.linear_model import LogisticRegression \nclassifier = LogisticRegression(random_state = 0) \nlogistic_cross=trainmodel(random)\n\nprint('Logistic regression Performance on the test set: Cross Validation Score = %0.4f' % logistic_cross)\n","75c2a788":"LRprediction=random.predict(testdata)\nLRprediction","d6808a64":"from sklearn import svm\n#create a classifier\nSVM = svm.SVC(kernel=\"linear\")\nsvm_cross=trainmodel(random)\nprint('SVM Performance on the test set: Cross Validation Score = %0.4f' % svm_cross)","0cdfe440":"SVMprediction=random.predict(testdata)\nSVMprediction","e453f84e":"#Since the testdata does not contain target value,I have calculated cross validation scores manually.","68239f8d":"    Column wise Missing Values","b15f6449":"    Row wise Missing Values","af8d487d":"The models accuracy of cross validation are as follows:","86cf5ccb":"Readig the new data from which we have removed columns which consisted missing values even after imputing. this new data would also have a new column \"Classifier'","5ed20b43":"SVM:85.62%","af233426":"Testing on testdata","e353a158":"      Total Missing Values in this data\n","13fcd43e":"Random Forest:86.50%","0c85faa0":"Logistic Regression:87%","818c74e3":"Creating table for missing value and their percentage in various columns","d59dcb9c":"for i in range(len(data.index)) :\n    print(\"Nan in row \", i , \" : \" ,  data.iloc[i].isnull().sum())\n","3c9ea8cc":"Naive Bayes: 78.12%","533ce449":"Loan Default Prediction\n"}}