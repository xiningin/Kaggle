{"cell_type":{"1d41fd31":"code","0b413832":"code","5dcc0e6c":"code","c0df3a3d":"code","a7562738":"code","2a1165db":"code","d49d8706":"code","ef04ca35":"code","58ca5984":"code","f2e02a82":"code","85e3eba4":"code","38c4f800":"code","75c2d92b":"code","d24a22b7":"code","a6346a81":"code","d5c72cd2":"code","04adcdcd":"code","4ec65952":"code","51357a70":"code","d7eb9f25":"code","ec06abb3":"code","6e8014e9":"code","67c4f82b":"code","8a6605c3":"code","a17f21f1":"code","2ff4e866":"code","37c5a2a6":"code","78b70d17":"code","d38d8afc":"code","309b7313":"markdown","a9a4cee4":"markdown","4dbb1a01":"markdown","6727e0a3":"markdown","5ae4c4dd":"markdown","cd9bed10":"markdown","90934192":"markdown"},"source":{"1d41fd31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b413832":"import numpy as np \nimport pandas as pd \nimport os \nimport matplotlib.pyplot as plt \nimport seaborn as sns \ncolor = sns.color_palette() \nimport matplotlib as mpl \n%matplotlib inline \nfrom sklearn import preprocessing as \u0440\u0440 \nfrom scipy.stats import pearsonr \nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, log_loss \nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import (RandomForestClassifier,\n                              AdaBoostClassifier,\n                              GradientBoostingClassifier,\n                              HistGradientBoostingClassifier)\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb \nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer \nimport keras\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, GlobalMaxPool1D\nfrom keras.layers.core import Dense, Dropout\nfrom keras.layers.recurrent import LSTM","5dcc0e6c":"data=pd.read_csv(\"..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\") \ndata","c0df3a3d":"print(data.shape)\nprint(data[\"fraudulent\"].value_counts())","a7562738":"data = data.replace(np.nan, '', regex=True)\ndef standardize_text(df, text_field):\n    df2 = df.copy(deep=True)\n    df2[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n    df2[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n    df2[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n    df2[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \", regex=True)\n    df2[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n    df2[text_field] = df[text_field].str.lower()\n    return df2\ndf_clean = standardize_text(data, \"description\")","2a1165db":"df_clean['text'] = df_clean[['title', 'department','company_profile','description','requirements',\n                     'benefits']].apply(lambda x: ' '.join(x), axis = 1)","d49d8706":"tokenizer = RegexpTokenizer(r'\\w+')\ndf_clean[\"tokens\"] = df_clean[\"text\"].apply(tokenizer.tokenize)\nnltk.download('wordnet')\nnltk.download('stopwords')","ef04ca35":"lemmatizer = WordNetLemmatizer()\nstudentDfObj = df_clean[\"tokens\"]\nlistOfDFRows = studentDfObj.to_numpy().tolist()\nlen(listOfDFRows)","58ca5984":"from nltk.corpus import stopwords\nlemm_tokens = []\nlemms = []\nfor i in range(len(listOfDFRows)):\n    for words in listOfDFRows[i]:\n        if words not in stopwords.words('english'):\n            lemmas = lemmatizer.lemmatize(words.lower(), pos='v')\n            lemms.append(lemmas)\n    lemm_tokens.append(lemms)\n    lemms = []\na = lemm_tokens\n%store a","f2e02a82":"%store -r a\nlen(a)","85e3eba4":"df_clean = df_clean.assign(lemm_tokens=a)","38c4f800":"y = df_clean['fraudulent']\nX = df_clean['lemm_tokens']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","75c2d92b":"def dummy_fun(doc):\n    return doc\ntfidf = TfidfVectorizer(\n    analyzer='word',\n    tokenizer=dummy_fun,\n    preprocessor=dummy_fun,\n    token_pattern=None)  \ntfidf.fit(X_train)\nX_train_tfidf = tfidf.transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\nprint(X_train_tfidf.shape)\nprint(X_test_tfidf.shape)","d24a22b7":"print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\nfrom imblearn.under_sampling import NearMiss\nnr = NearMiss()\n\nX_train_miss, y_train_miss = nr.fit_resample(X_train_tfidf, y_train.ravel())\nX_test_miss, y_test_miss = nr.fit_resample(X_test_tfidf, y_test.ravel())\n\nprint('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape))\nprint('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape))\nprint(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1)))\nprint(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))","a6346a81":"sgd_clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", random_state=100, max_iter=1000)\nsgd_clf.fit(X_train_miss, y_train_miss)\ny_predicted_sgd_clf = sgd_clf.predict(X_test_miss)\nprint(classification_report(y_test_miss, y_predicted_sgd_clf))","d5c72cd2":"print(\"ROC-AUC:\", roc_auc_score(y_predicted_sgd_clf,y_test_miss))\nprint(\"Accuracy:\", accuracy_score(y_predicted_sgd_clf,y_test_miss))","04adcdcd":"clf_log = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                         multi_class='multinomial', random_state=40)\nclf_log.fit(X_train_miss, y_train_miss)\ny_predicted_log = clf_log.predict(X_test_miss)\n\nprint(classification_report(y_test_miss, y_predicted_log))","4ec65952":"print(\"ROC-AUC:\", roc_auc_score(y_predicted_log,y_test_miss))\nprint(\"Accuracy:\", accuracy_score(y_predicted_log,y_test_miss))","51357a70":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=14727)\ntokenizer.fit_on_texts(X)\ntrain_wordemb = tokenizer.texts_to_sequences(X_train)\ntest_wordemb = tokenizer.texts_to_sequences(X_test)\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index","d7eb9f25":"from keras.preprocessing.sequence import pad_sequences\nmaxlen = 100\n\ntrain_wordemb = pad_sequences(train_wordemb, padding='post', maxlen=maxlen)\ntest_wordemb = pad_sequences(test_wordemb, padding='post', maxlen=maxlen)","ec06abb3":"X_train_miss2, y_train_miss2 = nr.fit_resample(train_wordemb, y_train.ravel())\nX_test_miss2, y_test_miss2 = nr.fit_resample(test_wordemb, y_test.ravel())","6e8014e9":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim = vocab_size,\n                           output_dim=embedding_dim, \n                           input_length=100))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","67c4f82b":"def plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    \nhistory = model.fit(X_train_miss2, y_train_miss2, epochs=50, verbose=False,\n                    validation_data=(X_test_miss2, y_test_miss2), batch_size=50)\nloss, accuracy = model.evaluate(X_train_miss2, y_train_miss2, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test_miss2, y_test_miss2, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","8a6605c3":"model1 = Sequential()\nmodel1.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel1.add(layers.GlobalMaxPool1D())\nmodel1.add(layers.Dense(10, activation='relu'))\nmodel1.add(layers.Dense(1, activation='sigmoid'))\nmodel1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel1.summary()","a17f21f1":"history1 = model1.fit(X_train_miss2, y_train_miss2, epochs=5, verbose=False,\n                    validation_data=(X_test_miss2, y_test_miss2), batch_size=10)\nloss, accuracy = model1.evaluate(X_train_miss2, y_train_miss2, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model1.evaluate(X_test_miss2, y_test_miss2, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history1)","2ff4e866":"sgd_clf2 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", random_state=100, max_iter=1000)\nsgd_clf2.fit(X_train_tfidf, y_train)\ny_predicted_sgd_clf2 = sgd_clf2.predict(X_test_tfidf)\nprint(classification_report(y_test, y_predicted_sgd_clf2))","37c5a2a6":"print(\"ROC-AUC:\", roc_auc_score(y_predicted_sgd_clf2,y_test))\nprint(\"Accuracy:\", accuracy_score(y_predicted_sgd_clf2,y_test))","78b70d17":"clf_log2 = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                         multi_class='multinomial', random_state=40)\nclf_log2.fit(X_train_tfidf, y_train)\ny_predicted_log2 = clf_log2.predict(X_test_tfidf)\n\nprint(classification_report(y_test, y_predicted_log2))","d38d8afc":"print(\"ROC-AUC:\", roc_auc_score(y_predicted_log2,y_test))\nprint(\"Accuracy:\", accuracy_score(y_predicted_log2,y_test))","309b7313":"NearMiss for Balancing Data","a9a4cee4":"Found, that the method TFIdfVectorizer gives better results than Tokenizer","4dbb1a01":"Use Word Embeddings on dataframe","6727e0a3":"## Balanced vs. Unbalanced data","5ae4c4dd":"The greatest accuracy and ROC-AUC metrics was in the model SGDClassifier (98% ROC-AUC, 98% accuracy)","cd9bed10":"Use TFIdfVectorizer on dataframe","90934192":"# use a TFIdfVectorizer method with unbalanced data"}}