{"cell_type":{"c51719ad":"code","d5da8640":"code","ce991e92":"code","9adc641a":"code","be9c0c3f":"code","db403461":"code","ba67f024":"code","c419aaa2":"code","69921279":"code","b70aa594":"code","8f84ddad":"code","14128186":"code","51ee338a":"code","7d916b35":"code","9a7feab8":"code","4a3d7bf1":"code","e3603aa7":"code","9929b391":"code","2e4049e8":"code","1d32208a":"code","623f559f":"code","15c0fddb":"code","54ca041a":"code","a9a9ebb3":"code","4726d7a2":"code","974cb9a6":"code","7648e708":"code","ae005e2c":"code","9addd9bc":"code","36176f75":"code","7ea67987":"markdown","6bf10e4e":"markdown","198fa93a":"markdown","a8c13d19":"markdown","ee34f503":"markdown","a64c3f6a":"markdown","802dc5c9":"markdown","63a30e4c":"markdown","1f3a6deb":"markdown","393dd702":"markdown","88fd706d":"markdown","62bad7af":"markdown","1a3e2d48":"markdown","932b62a3":"markdown","83514512":"markdown","f8741cc8":"markdown","4d17aee1":"markdown","16fc52f5":"markdown","c452e093":"markdown","a5902b14":"markdown","62e01997":"markdown","52ddbfb5":"markdown"},"source":{"c51719ad":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport numpy as np\nimport pandas as pd\n\nfrom fastai.vision import *","d5da8640":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 0\nseed_everything(SEED)","ce991e92":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head(10)","9adc641a":"def generate_df(train_df,sample_num=1):\n    train_df['path'] = train_df['experiment'].str.cat(train_df['plate'].astype(str).str.cat(train_df['well'],sep='\/'),sep='\/Plate') + '_s'+str(sample_num) + '_w'\n    train_df = train_df.drop(columns=['id_code','experiment','plate','well']).reindex(columns=['path','sirna'])\n    return train_df\nproc_train_df = generate_df(train_df)  ","be9c0c3f":"proc_train_df.head(10)","db403461":"import cv2\nimg = cv2.imread(\"..\/input\/train\/HEPG2-01\/Plate1\/B03_s1_w2.png\")\nplt.imshow(img)\ngray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\nplt.imshow(gray_img)\ngray_img.shape","ba67f024":"def open_rcic_image(fn):\n    images = []\n    for i in range(6):\n        file_name = fn+str(i+1)+'.png'\n        im = cv2.imread(file_name)\n        im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n        images.append(im)\n    image = np.dstack(images)\n    #print(pil2tensor(image, np.float32).shape)#.div_(255).shape)\n    return Image(pil2tensor(image, np.float32).div_(255))\n  \nclass MultiChannelImageList(ImageList):\n    def open(self, fn):\n        return open_rcic_image(fn)","c419aaa2":"il = MultiChannelImageList.from_df(df=proc_train_df,path='..\/input\/train\/')","69921279":"def image2np(image:Tensor)->np.ndarray:\n    \"Convert from torch style `image` to numpy\/matplotlib style.\"\n    res = image.cpu().permute(1,2,0).numpy()\n    if res.shape[2]==1:\n        return res[...,0]  \n    elif res.shape[2]>3:\n        #print(res.shape)\n        #print(res[...,:3].shape)\n        return res[...,:3]\n    else:\n        return res\n\nvision.image.image2np = image2np","b70aa594":"il[0]","8f84ddad":"from sklearn.model_selection import StratifiedKFold\n#train_idx, val_idx = next(iter(StratifiedKFold(n_splits=int(1\/0.035),random_state=42).split(proc_train_df, proc_train_df.sirna)))\nfrom sklearn.model_selection import train_test_split\ntrain_df,val_df = train_test_split(proc_train_df,test_size=0.035, stratify = proc_train_df.sirna, random_state=42)\n_proc_train_df = pd.concat([train_df,val_df])","14128186":"data = (MultiChannelImageList.from_df(df=_proc_train_df,path='..\/input\/train\/')\n        .split_by_idx(list(range(len(train_df),len(_proc_train_df))))\n        .label_from_df()\n        .transform(get_transforms(),size=256)\n        .databunch(bs=128,num_workers=4)\n        .normalize()\n       )","51ee338a":"data.show_batch()","7d916b35":"!pip install efficientnet_pytorch","9a7feab8":"from efficientnet_pytorch import *","4a3d7bf1":"\"\"\"Inspired by https:\/\/github.com\/wdhorton\/protein-atlas-fastai\/blob\/master\/resnet.py\"\"\"\n\nimport torchvision\nRESNET_MODELS = {\n    18: torchvision.models.resnet18,\n    34: torchvision.models.resnet34,\n    50: torchvision.models.resnet50,\n    101: torchvision.models.resnet101,\n    152: torchvision.models.resnet152,\n}\n\ndef resnet_multichannel(depth=50,pretrained=True,num_classes=1108,num_channels=6):\n        model = RESNET_MODELS[depth](pretrained=pretrained)\n        w = model.conv1.weight\n        model.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        model.conv1.weight = nn.Parameter(torch.stack([torch.mean(w, 1)]*num_channels, dim=1))\n        return model\n\n    \nDENSENET_MODELS = {\n    121: torchvision.models.densenet121,\n    161: torchvision.models.densenet161,\n    169: torchvision.models.densenet169,\n    201: torchvision.models.densenet201,\n}\n\ndef densenet_multichannel(depth=121,pretrained=True,num_classes=1108,num_channels=6):\n        model = DENSENET_MODELS[depth](pretrained=pretrained)\n        w = model.features.conv0.weight\n        model.features.conv0 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        model.features.conv0.weight = nn.Parameter(torch.stack([torch.mean(w, 1)]*num_channels, dim=1))\n        return model\n        \n        \n#EFFICIENTNET_MODELS = {\n#    'b0': '..\/input\/efficientnet-pytorch\/efficientnet-b0-08094119.pth',\n#    'b1': '..\/input\/efficientnet-pytorch\/efficientnet-b1-dbc7070a.pth',\n#    'b2': '..\/input\/efficientnet-pytorch\/efficientnet-b2-27687264.pth',\n#    'b3': '..\/input\/efficientnet-pytorch\/efficientnet-b3-c8376fa2.pth',\n#    'b4': '..\/input\/efficientnet-pytorch\/efficientnet-b4-e116e8b3.pth',\n#    'b5': '..\/input\/efficientnet-pytorch\/efficientnet-b5-586e6cc6.pth'\n#}\n\n\ndef efficientnet_multichannel(pretrained=True,name='b0',num_classes=1108,num_channels=6,image_size=256):\n    model = EfficientNet.from_pretrained('efficientnet-'+name,num_classes=num_classes)\n    #model.load_state_dict(torch.load(EFFICIENTNET_MODELS[name]))\n    w = model._conv_stem.weight\n    #s = model._conv_stem.static_padding\n    model._conv_stem = utils.Conv2dStaticSamePadding(num_channels,32,kernel_size=(3, 3), stride=(2, 2), bias=False, image_size = image_size)\n    model._conv_stem.weight = nn.Parameter(torch.stack([torch.mean(w, 1)]*num_channels, dim=1))\n    return model","e3603aa7":"def resnet18(pretrained,num_channels=6):\n    return resnet_multichannel(depth=18,pretrained=pretrained,num_channels=num_channels)\n\ndef _resnet_split(m): return (m[0][6],m[1])\n\ndef densenet161(pretrained,num_channels=6):\n    return densenet_multichannel(depth=161,pretrained=pretrained,num_channels=num_channels)\n  \ndef _densenet_split(m:nn.Module): return (m[0][0][7],m[1])\n\ndef efficientnetb0(pretrained=True,num_channels=6):\n    return efficientnet_multichannel(pretrained=pretrained,name='b0',num_channels=num_channels)\n","9929b391":"from fastai.metrics import *\nlearn = Learner(data, efficientnetb0(),metrics=[accuracy]).to_fp16()\nlearn.path = Path('..\/')","2e4049e8":"learn.unfreeze()\n#learn.lr_find() #<-- uncomment to determine the learning rate (commented to reduce time)\n#learn.recorder.plot(suggestion=True) ","1d32208a":"learn.fit_one_cycle(18,1e-3)","623f559f":"learn.recorder.plot_losses()\nlearn.recorder.plot_metrics()","15c0fddb":"learn.save('stage-2')\nlearn.export()","54ca041a":"test_df = pd.read_csv('..\/input\/test.csv')\nproc_test_df = generate_df(test_df.copy())","a9a9ebb3":"data_test = MultiChannelImageList.from_df(df=proc_test_df,path='..\/input\/test\/')\nlearn.data.add_test(data_test)","4726d7a2":"preds, _ = learn.get_preds(DatasetType.Test)","974cb9a6":"preds_ = preds.argmax(dim=-1)","7648e708":"test_df.head(10)","ae005e2c":"submission_df = pd.read_csv('..\/input\/sample_submission.csv')","9addd9bc":"submission_df.sirna = preds_.numpy().astype(int)\nsubmission_df.head(10)","36176f75":"submission_df.to_csv('submission.csv',index=False)","7ea67987":"With the multi-channel `ImageList` defined, we can now create a DataBunch of the train images. Let's first create a stratified split of dataset and get the indices. ","6bf10e4e":"Now we can get out predictions on the test set.","198fa93a":"## Loading and formatting data\n\nHere I will load the csv into the DataFrame, and create a column in the DataFrame with the path to the corresponding image (`generate_df`)","a8c13d19":"I will use a pretrained EfficientNet. There is code for other models thatt you can try but the EfficientNet seems to do the best. I have to now adjust the CNN arch to take in 6 channels as opposed to the usual 3 channels:","ee34f503":"We have to redefine the following function to be able to view the image in the notebook. I view just the first 3 channels.","a64c3f6a":"We add the data to our DataBunch:","802dc5c9":"Now let's view an example image:","63a30e4c":"## Creating and Training a Model","1f3a6deb":"As I subclassed the ImageList function I can load images with the `ImageList` function `.from_df`. ","393dd702":"Let's create our Learner:","88fd706d":"Let's look at an example image. These images are 6-channel images, but the each of the six channels are saved as separate files. Here, I open just one channel of the image.","62bad7af":"Let's now load our test csv and process the DataFrame like we did for the training data.","1a3e2d48":"# Recursion Cellular Image Classification - fastai starter\n\nWelcome to the Recursion Cellular Image Classification Kaggle competition! Here, I provide a basic fastai starter code.","932b62a3":"## Load modules","83514512":"In fastai, there is a modular data API that allows you to easily load images, add labels, split into train\/valid, and add transforms. The base class for loading the images is an `ItemList`. For image classification tasks, the base class is `ImageList` which in turn subclasses the `ItemList` class. Since `ImageList` can only open 3-channel images, we will define a new `ImageList` class where we redefine the loading function:","f8741cc8":"## Future work:\n\nThis is only a simple baseline. There are many different things we can change:\n* Use both sites (right now I only use site 1)\n* Model architecture\n* Train multiple classifiers for different cell types\n* **Metric learning** - This will be the key to successful submissions","4d17aee1":"** BEFORE YOU FORK, PLEASE SUPPORT AND UPVOTE **","16fc52f5":"We will now unfreeze and train the entire model.","c452e093":"That's it!","a5902b14":"Let's open the sample submission file and load it with our predictions to create a submission.","62e01997":"## Inference and Submission Generation","52ddbfb5":"Now we create the `DataBunch`"}}