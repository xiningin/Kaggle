{"cell_type":{"67a76d8e":"code","08cfcd5d":"code","cc0500ce":"code","9020684e":"code","6f07832e":"code","b283aaac":"code","2fb25b41":"code","fe4c83b7":"code","e4bf4779":"code","e1059fb5":"code","51d25419":"code","cddd22d7":"code","1dff564b":"code","9a624a8e":"code","cf03ff10":"code","5ec9f6fb":"code","2ac96bf1":"code","297f7ce8":"code","5e050a54":"code","c259006d":"code","29dc2671":"code","9347609a":"code","270e2ce7":"code","644e6648":"code","cff967de":"code","e1ad8380":"code","ac6c0729":"code","8c45c813":"code","6161f620":"code","eb064c79":"code","8080bb70":"code","1448380c":"code","f542070c":"code","0e4ef6e4":"code","ac91bb6f":"code","cd296008":"code","5abc6b97":"code","123af608":"code","bf82d27f":"code","cc835f3e":"code","cce947a5":"code","7c48846c":"markdown","6b553aa7":"markdown","0e1afa88":"markdown","ba3ff4b5":"markdown","0f52d7df":"markdown","56dc7ab0":"markdown","919712d1":"markdown","99e8e980":"markdown","c2d0c20e":"markdown","1177d53b":"markdown","1f47612a":"markdown","3012be7a":"markdown","1a8bd3d4":"markdown","de9b668a":"markdown","d81456a3":"markdown","0a2f37ae":"markdown","5fa9ea20":"markdown"},"source":{"67a76d8e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\npath = \"\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\"\ndf = pd.read_csv(path)\nprint(df.dtypes)\ndf.head(5)","08cfcd5d":"#checking on missing data\nprint(df.shape)\nprint(df.isnull().sum())","cc0500ce":"#since of of our misssing data are in 2 columns: last_review and reviews_per_month\n#we're checking the data to see if there's anything special.\ndf[df[\"last_review\"].isnull()].head(5)","9020684e":"#does null values in columns: last_review and reviews_per_month indicate that there has never been any review ? \n#Indeed. These places simply has never gotten a review as shown below.\ndf[df[\"last_review\"].isnull() & df[\"reviews_per_month\"].isnull()][\"number_of_reviews\"].sum()","6f07832e":"#since these places have never gotten a review, we should then replace null values in the\n#last_review, reviews_per_month as simply, 0.\ndf.fillna({'reviews_per_month':0,\"last_review\":0}, inplace=True)\ndf.isnull().sum()","b283aaac":"#now we can just simply drop the missing data from name, host_name (37 rows total). \n#Due to the size of our dataset, these won't pose a problem and\/or provide extra insights\ndf.dropna(inplace=True)\nprint(df.shape)\n\n#checking on prices column (where the prices are unreasonable, ie: $0 a night)\ndf[df[\"price\"] <= 0]","2fb25b41":"#remove those invalid prices, serves no purpose in prediction and\/or analysis.\ndf = df[df[\"price\"] > 0]\n\n#using a new df for visualization purposes\nvis_df = df.copy()","fe4c83b7":"#config of the 6 subplots\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(20,20))\n\n#Based on the first 2 plots, it seems that the price distributions of our listing are not really normal distributions.\nsns.barplot(x=\"neighbourhood_group\",y=\"price\", hue=\"room_type\", estimator=np.median,\n            data=vis_df,ax=ax1).set_title(\"Median Price Per Night by neighbourhood_group\")\nsns.countplot(x=\"neighbourhood_group\",hue=\"room_type\",data=vis_df,ax=ax2).set_title(\"Total number of listing for each neighbourhood_group\")\nsns.boxplot(x=\"neighbourhood_group\",y=\"availability_365\",data=vis_df,ax=ax3).set_title(\"Availability based on neighbourhood_groups\")\nsns.violinplot(x=\"neighbourhood_group\",y=\"price\",data=vis_df,ax=ax4).set_title(\"Violin Plot of distribution of prices for each neighbourhood_group\")\nax4.set_ylim(0,500)\n\n#does number of reviews and reviews per month correlates?\nsns.scatterplot(data=vis_df,x=\"number_of_reviews\",y=\"reviews_per_month\",hue=\"room_type\",ax=ax5).set_title(\"number of reviews vs. reviews per month\")\n#does more reviews = more expensive?\nsns.scatterplot(data=vis_df,x=\"number_of_reviews\",y=\"price\",hue=\"room_type\",ax=ax6).set_title(\"Minimum nights vs. price\")\n#ax6.set_xlim(0,30)","e4bf4779":"#checking correlation on general features.\ncorr = vis_df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr,linewidths=0.5)\n\n#most features don't seem to correlate really well with one another, except for number of reviews vs. reviews per month as discussed on previous plot (light orange).","e1059fb5":"#config of the 4 subplots\nsub_price_df = vis_df[vis_df[\"price\"] < 500] #for better visualization as we know most of the prices are lower than $500\nfig,((ax1,ax2),(ax3,ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(15,15))\nsns.scatterplot(data=vis_df, x=\"latitude\",y=\"longitude\",hue=\"neighbourhood_group\",ax=ax1)\nsns.scatterplot(data=vis_df, x=\"latitude\",y=\"longitude\",hue=\"room_type\",ax=ax2)\nsns.scatterplot(data=sub_price_df, x=\"latitude\",y=\"longitude\",hue=\"price\",ax=ax3)\nsns.scatterplot(data=vis_df, x=\"latitude\",y=\"longitude\",hue=\"availability_365\",ax=ax4)","51d25419":"#to explore the keywords in the name and hostnames. We'll plot the first 30 most listing hosts.\nplt.figure(figsize=(15,10))\nname = vis_df.groupby(\"host_name\")\nname.size().sort_values(ascending=False)[:30].plot.bar()\nplt.xlabel(\"Names\")\nplt.ylabel(\"Count\")\nplt.title(\"Frequency of posting by each host\")\nplt.show()","cddd22d7":"#\" \".join(vis_df[\"name\"])\n#notice that we just join all the words with the generate() method. Will concat all the text in that column\nplt.subplots(figsize=(20,20))\nwordcloud = WordCloud(max_font_size=150,max_words=200, background_color=\"white\",height=1080, width=1920).generate(\" \".join(vis_df[\"name\"]))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","1dff564b":"#df (not vis_df) is already cleaned with null values as above.\n#these are columns we'll be using for prediction.\ndf_reg = df[[\"neighbourhood_group\",\"room_type\",\"minimum_nights\",\"calculated_host_listings_count\",\n             \"availability_365\",\"latitude\",\"longitude\",\"price\"]]\ndf_reg.head()","9a624a8e":"#looking at average listing price for each of the neighbourhood_group in order to ordinal rank them.\n#seems that Bronx is the worst, Staten Island and Queens are roughly similar. This was also indicated on one of the barplots above.\nprint(df_reg[df_reg[\"neighbourhood_group\"] == \"Bronx\"][\"price\"].median())\nprint(df_reg[df_reg[\"neighbourhood_group\"] == \"Staten Island\"][\"price\"].median())\nprint(df_reg[df_reg[\"neighbourhood_group\"] == \"Queens\"][\"price\"].median())\nprint(df_reg[df_reg[\"neighbourhood_group\"] == \"Brooklyn\"][\"price\"].median())\nprint(df_reg[df_reg[\"neighbourhood_group\"] == \"Manhattan\"][\"price\"].median())","cf03ff10":"print(df_reg[\"room_type\"].value_counts())\nprint(df_reg[\"neighbourhood_group\"].value_counts())","5ec9f6fb":"#Generic labelencoder helper function\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n#either onehot encoding or ordinal encoding. Labelencoding doesn't help rank the neighbourhood_group and room_type for the price prediction.\n#cover for both columns: \"neighbourhood_group\" and \"room_type\".\ndef ordinal_encoding(row):\n    if row == \"Bronx\":\n        return 1\n    elif row == \"Queens\" or row == \"Staten Island\":\n        return 2\n    elif row == \"Brooklyn\":\n        return 3\n    elif row == \"Manhattan\":\n        return 4\n    \n    elif row == \"Shared room\":\n        return 1\n    elif row == \"Private room\":\n        return 2\n    elif row == \"Entire home\/apt\":\n        return 3\n    \n\n#ordinal value ordered by usage of cost of rent in each neighbourhood group and the general \"space\" of the rental.\ndf_reg[\"neighbourhood_group\"] = df_reg[\"neighbourhood_group\"].apply(ordinal_encoding)\ndf_reg[\"room_type\"] = df_reg[\"room_type\"].apply(ordinal_encoding)","2ac96bf1":"#double checking our transformation\/encoding\nprint(df_reg[\"room_type\"].value_counts())\nprint(df_reg[\"neighbourhood_group\"].value_counts())","297f7ce8":"#treatment of outliers\n#config of the 6 subplots\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(20,20))\n\nsns.kdeplot(data=df_reg[\"minimum_nights\"],ax=ax1)\nax1.set_xlim(0,200)\nax1.set_title(\"Dist. of minimum_night\")\n\nsns.kdeplot(data=df_reg[\"minimum_nights\"],cumulative=True,ax=ax2)\nax2.set_xlim(0,40)\nax2.set_title(\"Cummulative Dist. of minimum_nights\")\n\nsns.kdeplot(data=df_reg[\"price\"],ax=ax3).set(xlim=(0,500))\nax3.set_xlim(0,500)\nax3.set_title(\"Dist. of price\")\n\nsns.kdeplot(data=df_reg[\"price\"],cumulative=True,ax=ax4).set(xlim=(0,500))\nax4.set_xlim(0,500)\nax4.set_title(\"Cummulative Dist. of price\")\n\nsns.kdeplot(data=df_reg[\"availability_365\"],ax=ax5).set(xlim=(0,365))\nax5.set_xlim(0,365)\nax5.set_title(\"Dist. of availability_365\")\n\nsns.kdeplot(data=df_reg[\"availability_365\"],cumulative=True,ax=ax6).set(xlim=(0,500))\nax6.set_xlim(0,365)\nax6.set_title(\"Cummulative Dist. of availability_365\")","5e050a54":"df_reg.describe()","c259006d":"#dropping outliers.\ndf_reg = df_reg[(df_reg[\"minimum_nights\"] < 40) & (df_reg[\"price\"] < 500) &(df_reg[\"availability_365\"] < 366)]","29dc2671":"#about 2% of data lost\ndf_reg.describe()","9347609a":"X = df_reg.drop(columns=[\"price\"])\ny = df_reg[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","270e2ce7":"from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\n\nfrom keras import layers, Input, Model\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras import optimizers\nfrom keras import callbacks\n\n#new\nfrom keras.utils import plot_model","644e6648":"class modeling:\n    \n    def __init__(self,df):\n        self.df = df #input df\n        self.X_train = None \n        self.X_test = None\n        self.y_train = None \n        self.y_test = None\n        \n        #scaler for standard scaling and minmax scaling.\n        self.X_scaler = None\n        self.X_train_ss = None\n        self.X_test_ss = None\n        \n        #mainly for neural network for convergence.\n        self.y_scaler = 500.0\n        self.y_train_ss = None #for neural network.\n        self.y_test_ss = None #for neural network.\n        \n        #df to display the training\/testing results based off model list and index list\n        self.result_table = None\n        \n        self.model_list = []\n        self.index_list = []\n        \n        #store predicted results\n        self.nn_model = None\n        self.history = None\n    \n    \n    def train_test_split(self,ratio=0.25,seed=42):\n        '''\n        function: to split train\/test\n        Parameters:\n            ratio of split, seed number.\n        Return:\n            set the training\/testing attributes.\n        '''\n        assert(self.df is not None), \"no dataframe to split train\/test\"\n        try:    \n            X = self.df.drop(columns=[\"price\"])\n            y = self.df[\"price\"]\n            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,y,test_size=ratio,random_state=seed)\n        \n        except:\n            print(\"check feature columns and label 'price' column to make sure df is functional\")\n    \n    \n    def standard_scaling(self):\n        '''\n        Function: This function will use Standard Scaling on training set, and using the fitting on the whole training dataset, setting them into the scaled variables\n            of the attribute list.\n        Parameters:\n            None\n        Return:\n            None.\n        '''\n        features = X_train.columns.tolist()\n        assert((self.X_train is not None) and (self.X_test is not None) and (self.y_train is not None) and (self.y_test is not None)),\"call method self.train_test_split() prior\"\n        assert(features),\"cannot feature scaling with no feature columns. Check main df\"\n        \n        standard_scaler = preprocessing.StandardScaler(with_mean=True,with_std=True)\n        self.X_scaler = standard_scaler.fit(self.X_train)\n        self.X_train_ss = standard_scaler.transform(self.X_train)\n        self.X_test_ss = standard_scaler.transform(self.X_test)\n        \n        \n        #convert the newly scaled data into dataframes\n        self.X_train_ss = pd.DataFrame(self.X_train_ss,columns=features)\n        self.X_test_ss = pd.DataFrame(self.X_test_ss,columns=features)\n        \n        #since we truncated at $500, for neural network usage\n        self.y_train_ss = self.y_train\/500.0\n        self.y_test_ss = self.y_test\/500.0\n    \n    def minmax_scaling(self):\n        '''\n        Function: This function will use MinMax Scaling on training set, and using the fitting on the whole training dataset, setting them into the scaled variables\n            of the attribute list.\n        Parameters:\n            None\n        Return:\n            None.\n        '''        \n        features = self.X_train.columns.tolist()\n        assert((self.X_train is not None) and (self.X_test is not None) and (self.y_train is not None) and (self.y_test is not None)),\"call method self.train_test_split() prior\"\n        assert(features),\"cannot feature scaling with no feature columns. Check main df\"\n        \n        minmax_scaler = preprocessing.MinMaxScaler()\n        self.X_scaler = minmax_scaler.fit(self.X_train)\n        self.X_train_ss = minmax_scaler.transform(self.X_train)\n        self.X_test_ss = minmax_scaler.transform(self.X_test)\n        \n        #convert the newly scaled data into dataframes\n        self.X_train_ss = pd.DataFrame(self.X_train_ss,columns=features)\n        self.X_test_ss = pd.DataFrame(self.X_test_ss,columns=features)\n        \n        #since we truncated at $500, for neural network usage\n        self.y_train_ss = self.y_train\/500.0\n        self.y_test_ss = self.y_test\/500.0\n    \n    def summary(self):\n        '''\n        fucntion: to establish a df for viewing the result\n        Parameters: \n            model_lst: list of dictionary of results\n            index: list of the names of the models in that order\n\n        Return:\n            A df with model training and testing results\n        '''\n        assert (len(self.model_list) > 0) and (len(self.index_list)==len(self.model_list)),\"empty model_list attribute or mismatched sizes of model_list\/index_list attributes\"\n        \n        df = pd.DataFrame(columns=[\"train_mse\",\"train_mae\",\"test_mse\",\"test_mae\"])\n        for i in range(len(self.model_list)):\n            temp = pd.DataFrame(self.model_list[i],[self.index_list[i]]) #values must be in a 2d list for df[[]]\n            df = df.append(temp)\n        self.result_table = df\n        print(self.result_table)\n        \n        \n    def add_model(self,model,name,scaled=0):\n        '''\n        function: to add a model for evaluation.\n        Parameters:\n            model: sklearn model\n            name: key value of the added model.\n            scale: using the scaled data (need to call self.standard_scaling() or self.minmax_scaling() methods first)\n        Return:\n            None\n        '''\n        assert(name) not in self.index_list,\"name parameter must be changed since it's already existed in the result table\"\n        if not scaled:\n            m = model\n            m.fit(self.X_train,self.y_train)\n            m_pred = m.predict(self.X_test)\n            \n            train_mse = mean_squared_error(m.predict(self.X_train),self.y_train)\n            train_mae = mean_absolute_error(m.predict(self.X_train),self.y_train)\n\n            test_mse = mean_squared_error(m_pred,self.y_test)\n            test_mae = mean_absolute_error(m_pred,self.y_test)\n\n            #return in dict format for later usage\n            self.model_list.append({\"train_mse\":[train_mse], \"train_mae\":[train_mae], \"test_mse\":[test_mse], \"test_mae\":[test_mae]})\n            self.index_list.append(name)\n            \n        elif scaled:\n            #unless using NN, scaling the target is not needed\n            assert(self.X_scaler), \"need to scale the data prior to modeling\"\n            m = model\n            m.fit(self.X_train_ss,self.y_train)\n            m_pred = m.predict(self.X_test_ss)\n\n            train_mse = mean_squared_error(m.predict(self.X_train_ss),self.y_train)\n            train_mae = mean_absolute_error(m.predict(self.X_train_ss),self.y_train)\n            test_mse = mean_squared_error(m_pred,self.y_test)\n            test_mae = mean_absolute_error(m_pred,self.y_test)\n\n            #return in dict format for later usage\n            self.model_list.append({\"train_mse\":[train_mse], \"train_mae\":[train_mae], \"test_mse\":[test_mse], \"test_mae\":[test_mae]})\n            self.index_list.append(name)\n        \n        \n    def build_nn(self,depth_matrix):\n        '''\n        function: to add a model for evaluation. Set self.nn_model after the build.\n        Parameters:\n            depth_matrix: list of nodes for each hidden layer. Ie: [10,10,10] for 3 hidden layers of 10 nodes each.\n        Return:\n            None\n        '''\n        input_tensor = Input(shape=(self.X_train.shape[1],))\n                             \n        x = Dense(depth_matrix[0])(input_tensor)\n        for i in range(1,len(depth_matrix)):\n            x = Dense(depth_matrix[i])(x)\n            x = Activation(\"relu\")(x)\n            x = Dropout(rate=0.5)(x)\n            x = BatchNormalization()(x)\n            \n        output_tensor = Dense(1)(x)\n        model = Model(inputs=input_tensor,outputs=output_tensor)\n        model.compile(optimizer=optimizers.Adam(lr=0.00005),loss=\"mse\",metrics=[\"mae\",\"mse\"])\n        \n        print(model.summary())\n        plot_model(model,show_shapes=True)\n        self.nn_model = model\n    \n    def plot_history(self):\n        '''\n        function: helper function that will output training\/validation plots for neural network after training.\n        Parameter:\n            None\n        Return:\n            None\n        '''\n        fig,(ax1,ax2) = plt.subplots(1, 2,figsize=(30,10))\n        \n        #we multiply the mae\/mse by $500 because that's what we rescaled our target down to from the original 0-500 dollars range target.\n        ax1.plot([i for i in range(1,len(self.history.history[\"mae\"])+1)],\n                 self.history.history[\"mae\"])\n        ax1.plot([i for i in range(1,len(self.history.history[\"val_mae\"])+1)],\n                 self.history.history[\"val_mae\"])\n        ax1.legend([\"train_mae\",\"val_mae\"])\n        ax1.set_title(\"MAE\")\n        ax1.set_ylim(0,0.2)\n        \n        ax2.plot([i for i in range(1,len(self.history.history[\"mse\"])+1)],\n                 self.history.history[\"mse\"])\n        ax2.plot([i for i in range(1,len(self.history.history[\"val_mse\"])+1)],\n                 self.history.history[\"val_mse\"])\n        ax2.legend([\"train_mse\",\"val_mse\"])\n        ax2.set_title(\"MSE\")    \n        ax2.set_ylim(0,0.2)\n    \n    def nn_train(self,ep,b_size,scaling=\"standard\",plotting=True):\n        '''\n        function: to train neutral network after building it. Need to call self.build_nn() prior.\n        Parameter:\n            ep: number of epochs\n            b_size: batch size\n            scaling: either \"standard\" or \"minmax\" for scaling the data prior to training. Default is \"standard\".\n            plotting: to call self.plot_history() function to plot the training\/validation data. Default is True.\n        '''\n        assert(self.nn_model is not None),\"Need to build model first before training\"\n        \n        \n        if scaling ==\"standard\":\n            self.standard_scaling()\n        elif scaling == \"minmax\":\n            self.minmax_scaling()\n        else:\n            raise(\"scaling parameter needs to be either 'standard' or 'minmax' \")\n        self.history = self.nn_model.fit(self.X_train_ss,self.y_train_ss,epochs=ep,batch_size=b_size,\n                                     validation_data=(self.X_test_ss,self.y_test_ss),verbose=True)\n        if plotting:\n            self.plot_history()\n        \n        ","cff967de":"#evaluating baseline models for screening.\nmodel = modeling(df_reg)\nmodel.train_test_split()\nmodel.standard_scaling()\nmodel.add_model(LinearRegression(),\"lr_bl\",scaled=0)\nmodel.add_model(SGDRegressor(loss=\"huber\"),\"SGD_bl\",scaled=1) #huber loss is mse but less sensitive with outliers - which are many of our high-end rent data\nmodel.add_model(Ridge(),\"ridge_bl\",scaled=1)\nmodel.add_model(Lasso(),\"lasso_bl\",scaled=1)\nmodel.add_model(SVR(kernel='linear'),\"SVRlinear_bl\",scaled=1)\nmodel.add_model(SVR(kernel='rbf'),\"SVRrbf_bl\",scaled=1)\nmodel.add_model(tree.DecisionTreeRegressor(),\"dt_bl\",scaled=0)\nmodel.add_model(tree.ExtraTreeRegressor(),\"extratree_dl\",scaled=0)\nmodel.add_model(XGBRegressor(),\"xgb_bl\",scaled=0)\nmodel.add_model(RandomForestRegressor(criterion=\"mae\",n_jobs=-1),\"rf_bl\",scaled=0) #previous runs see that mae loss outperform mse due to many outliers that we have for target\nmodel.summary()","e1ad8380":"df_reg.describe()","ac6c0729":"#inheriting modeling class\nclass gridsearch(modeling):\n    def __init__(self,df):\n        super().__init__(df)\n        self.model_list = dict() #holding all gridsearched models and its results\n        self.best_models = dict()\n        \n    \n    def gridsearchcv(self,model,name,hyperparameters_grid,scale_type):\n        '''\n        function: to do gridsearch and store the results of our gridsearch in self.model_list dictionary.\n        Parameters:\n            model: a baseline model from sklearn.\n            name: key name for the model being gridsearch to store in the dictionary.\n            hyperparameters_grid: a dictionary of our DOE\/gridsearch.\n            scale_type: either \"minmax\" or \"standard\" for scaling the features\n        Return:\n            None\n        '''\n        \n        #record 2 metrics, but we optimize with refit=\"MAE\" metric\n        grid = GridSearchCV(model,hyperparameters_grid,scoring=[\"neg_mean_absolute_error\",\"neg_mean_squared_error\"],refit=\"neg_mean_absolute_error\")\n        if not scale_type:\n            grid.fit(self.X_train,self.y_train)\n            \n        elif scale_type == \"minmax\":\n            self.train_test_split()\n            self.minmax_scaling()\n            grid.fit(self.X_train_ss,self.y_train_ss)\n        \n        elif scale_type == \"standard\":\n            self.train_test_split()\n            self.standard_scaling()\n            grid.fit(self.X_train_ss,self.y_train_ss)\n            \n        self.model_list[name] = grid.cv_results_\n        \n        \n    def extract_best_params(self):\n        '''\n        function: to extract out the best model of gridsearch models and store them in the dictionary of self.best_models.\n        Parameters:\n            None\n        Return:\n            None\n        '''\n        assert(bool(self.model_list)),\"no grid search was done to extract out best parameters. Need to do gridsearch first\"\n        for model in self.model_list:\n            rank_list = self.model_list[str(model)][\"rank_test_neg_mean_absolute_error\"]\n            index = rank_list.tolist().index(1) #return the #1 best set of hyperparameter ranked of the list.\n            best_params = self.model_list[str(model)][\"params\"][index] #extract out the best hyperparameters of our gridsearch\n            self.best_models[model] = best_params\n    \n    \n            \n        \n        ","8c45c813":"grid = gridsearch(df_reg)\ngrid.train_test_split()\ngrid.standard_scaling()","6161f620":"'''\ngrid.gridsearchcv(model=RandomForestRegressor(),name=\"rf\",\n                  hyperparameters_grid={'n_jobs':[-1],'n_estimators':[300,400,500,600],'max_features':[\"sqrt\",\"log2\"],'max_depth':[5,10,15,20]},\n                  scale_type=None)\ngrid.extract_best_params()\ngrid.best_models\n'''","eb064c79":"'''\ngrid.gridsearchcv(model=LinearRegression(),name=\"lr\",\n                 hyperparameters_grid={\"fit_intercept\":[False,True],\"normalize\":[True,False]},scale_type=None)\ngrid.extract_best_params()\ngrid.best_models\n'''","8080bb70":"#\u2018huber\u2019 modifies \u2018squared_loss\u2019 to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon\n#l1_ratio is used for elasticnet only (l1 to l2 ratio). l1 ratio being 0 corresponse to only l2 regularization.\n'''\ngrid.gridsearchcv(model=SGDRegressor(),name=\"SGD\",\n                 hyperparameters_grid={\"loss\":[\"squared_loss\",\"huber\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"],\n                                      \"penalty\":[\"elasticnet\",\"l2\",\"l1\"],\n                                      \"l1_ratio\":[0.1,0.15,0.2,0.3]},scale_type=\"standard\")\ngrid.extract_best_params()\ngrid.best_models\n'''","1448380c":"#epsilon street margin, C is ~1\/lambda for regularization.\n#due to processing time, we will not optimizing each of the parameters C\/epsilon\/degree interaction amongst each other.\n'''\ngrid.gridsearchcv(model=SVR(),name=\"SVRLinear1\",\n                 hyperparameters_grid={\"kernel\":[\"poly\"],\"degree\":[2,3,4]},scale_type=\"standard\")\ngrid.extract_best_params()\ngrid.best_models\n'''\n#final solution = 3rd degree","f542070c":"#due to processing time, we will not be optimizing the whole factorial of hyperparameters. We will optimize them individually\n#not enough processing resources\n'''\ngrid.gridsearchcv(model=SVR(),name=\"SVRLinear2\",\n                 hyperparameters_grid={\"kernel\":[\"poly\"],\"degree\":[3],\"C\":[0.01,1,10,100]},scale_type=\"standard\")\ngrid.extract_best_params()\ngrid.best_models\n'''","0e4ef6e4":"#not enough resources to run gridsearch\n'''grid.gridsearchcv(model=SVR(),name=\"SVRLinear3\",\n                 hyperparameters_grid={\"kernel\":[\"poly\"],\"degree\":[3],\"epsilon\":[0.05,0.2,0.3,0.4]},scale_type=\"standard\")                     \ngrid.extract_best_params()\ngrid.best_models\n'''","ac91bb6f":"#not enough resources to run gridsearch\n'''\ngrid.gridsearchcv(model=SVR(),name=\"SVRrbf1\",\n                 hyperparameters_grid={\"kernel\":[\"rbf\"],\"C\":[0.1,10,100,1000],scale_type=\"standard\")\ngrid.extract_best_params()\ngrid.best_models\n'''","cd296008":"#not enough resources to run gridsearch\n'''\ngrid.gridsearchcv(model=SVR(),name=\"SVRrbf2\",\n                 hyperparameters_grid={\"kernel\":[\"rbf\"],\"epsilon\":[0.1,0.2,0.4]},scale_type=\"standard\")\ngrid.extract_best_params()\ngrid.best_models\n'''","5abc6b97":"#decision tree trying to combat overfitting\n'''\ngrid.gridsearchcv(model=tree.DecisionTreeRegressor(),name=\"dt\",\n                 hyperparameters_grid={\"criterion\":[\"mse\",\"mae\"],\"splitter\":[\"random\",\"best\"],\"max_depth\":[None,5,10,15,20],\"max_features\":[\"sqrt\",\"log2\",\"auto\"]},scale_type=None)\ngrid.extract_best_params()\ngrid.best_models\n'''","123af608":"'''\ngrid.gridsearchcv(model=tree.ExtraTreeRegressor(),name=\"extratree\",\n                 hyperparameters_grid={\"criterion\":[\"mse\",\"mae\"],\"max_depth\":[None,5,10,15,20],\"max_features\":[\"sqrt\",\"log2\",\"auto\"]},scale_type=None)\ngrid.extract_best_params()\ngrid.best_models\n'''","bf82d27f":"'''\ngrid.gridsearchcv(model=XGBRegressor(),name=\"xgb\",\n                 hyperparameters_grid={\"eta\":[0.1,0.2,0.3],\"max_depth\":[2,4,6,8],\"subsample\":[0.3,0.5,0.7,1]},scale_type=None)\ngrid.extract_best_params()\ngrid.best_models\n'''","cc835f3e":"model.add_model(RandomForestRegressor(criterion=\"mae\",n_jobs=-1,max_features='sqrt',n_estimators=500,max_depth=15),\"rf_best\",scaled=0)\nmodel.add_model(LinearRegression(fit_intercept=True,normalize=True,n_jobs=-1),\"lr_best\",scaled=0)\nmodel.add_model(SGDRegressor(l1_ratio=0.15,loss='huber',penalty='elasticnet'),'SGD_best',scaled='standard')\nmodel.add_model(SVR(kernel=\"poly\",degree=3),\"SVRLinear_best\",scaled='standard')\nmodel.add_model(tree.DecisionTreeRegressor(criterion='mae',max_depth=10,max_features='auto',splitter='best'),\"dt_best\",scaled=None)\nmodel.add_model(tree.ExtraTreeRegressor(criterion='mae',max_depth=10,max_features='auto'),\"extratree_best\",scaled=0)\nmodel.add_model(XGBRegressor(eta=0.1,max_depth=8,subsample=1),\"xgb_best\",scaled=0)\n\nmodel.summary()","cce947a5":"model = modeling(df_reg)\nmodel.train_test_split()\nmodel.standard_scaling()\nmodel.build_nn(depth_matrix=[300,300,300,300])\nmodel.nn_train(ep=100,b_size=30,scaling=\"standard\",plotting=True)","7c48846c":"Making use of spatial information using the data given with latitude and longitude information from airbnb. As we can see where exactly the neighbourhood groups are in #1, the spatial distribution of room types for rental in #2, the prices distribution in #3, and the hotspot where booking\/availability occurs in #4.","6b553aa7":"**Background:**\n\nThis data is provided by Airbnb on the year of 2019, in New York City. Since Airbnb is a successful, data driven company within the hospitality sector, we would can expect a lot of interesting finds within this dataset. Below are what we'll be doing throughout this notebook:\n\n\n1. EDA on the given dataset to see any interesting finds.\n2. Analyse various Machine Learning models to predict the prices of the listing.","0e1afa88":"**Best Models**:\n\nRF: max_depth:15, max_features:'sqrt', n_estimators:500\n\nLR: intercept\/normalization did not help the model. The base model was unchanged in term of loss.\n\nSGD: l1_ratio:0.15, loss:huber, penalty:elasticnet\n\nRidge\/Lasso: do not seem to be overfitting, regularized, similar to LR anyhow. We'll skip.\n\nXtraTree: criterion:mae, max_depth:10, max_features:auto\n\nDecisionTree: criterion:mae, max_depth:10, max_features:auto, splitter:best\n\nxgb: no improvement from baseline, eta:0.1, max_depth:8, subsample:1\n\nSVR: not enough resources to run gridsearch.","ba3ff4b5":"Our goal is to help predict\/suggest the price of the new listing by when a new host create a post. In order to do this, we would have to use regression models as well as feature engineering some of our existing features. \n\nColumns to drop:\n\nGroup1. id, name host_id, host_name, last_review: these conveys no numerical purposes\n\nGroup2. reviews_per_month, number_of_reviews: These are numerical and important, however they are a leakage since we need to predict the price based on a NEW host listing.\nwe'll keep the neighborhood_group and room types because we know that certain neighborhood_groups are more expensive than others (at the map data), and same with room_type.","0f52d7df":"*Plot #1* : shows 2 main points - the relative pricing amongst each neighbourhood group and the relative pricing amongst each room type. This will be useful for ordinal transformation of our data later on.\n\n*Plot #2* : shows the total number of listing per neighbourhood group. It seems that Manhattan, Brooklyn, and Queens are where most of the listings happen\n\n*Plot #3* : shows the days in which the listings are available for booking by neighbourhood group. Again, it does show that Brooklyn, Manhattan, and Queens are very much busily booked as availability throughout the years are much lower than the other neighbourhood groups.\n\n*Plot #4* : shows distribution of price by each neighbourhood group. It seems that Manhattan has the most spread out distribution, where Bronx is in the total opposite. The plot also shows us that most of our data points are within $500 a night (or even lower if we want to be aggressive).\n\n*Plot #5* : shows that the number of reviews has a slight positive correlation to the reviews_per_month. This indicates that not all properties have a regular rate of reviews coming in. There's a huge variation in that part. Therefore, both of the parameters need to be used for prediction the price of our listing.\n\n*Plot #6* : shows no clear correlation between number of reviews vs. price (do popular places tend to be more expensive ?). In fact, it's interesting to see that most of the expensive places have few reviews but NONE of the high review properties are expensive places. This is expected but there seems to be no outlier on this whatsoever.","56dc7ab0":"Finally, the last word cloud tells us the frequency of the listing names. As expected, the locations of the listing are the most important information - which are followed by positive words such as \"Amazing\",\"Bright\",\"Beautiful\",and \"heart\" to attract the customers.","919712d1":"Overall, we see that the optimized ensemble techniques such as xgboost (38 dollars MAE) and random forest (37 dollars MAE) are doing much better than the algorithm (including neural network). This is not a real surprise. Our neural network was able to perform at the level of around 0.085*500 = 42 dollars MAE from the target. This is actually very good as we have seen from dataframe that the price column has a standard deviation of around 85 dollars. This level of error is 1\/2 of the standard deviation and giving the fact that we don't know anything about the rental such as how many rooms in an entire apartment, what year the houses were built, the square feet of the place, or even the zillow prices of the private lots. I'm confident that we could've done better given those details.","99e8e980":"## Background & Objectives","c2d0c20e":"## Exploratory Data Analysis","1177d53b":"We're simply checking for correlations between features, so we can drop them for regression purposes. However, there doesn't seem to be highly correlated sets of features as shown above. Therefore we'll not drop any of these columns **just** because of multicollinity.","1f47612a":"It seems that most of the data are contained within: 40 minimum nights, $500 in price, and 365 availability days. Machine learning algorithms are not good at predicting outliers for regression purposes (not neccessarily linear). Using metrics such as MSE, coupling with many outliers will certainly hurt our performance. Therefore, we'll be looking to drop those outliers as suggested above.","3012be7a":"Columns:\n\n*ID*: Listing ID.\n\n*Name*: Name of the listing.\n\n*host_id*: Host ID.\n\n*host_name*: Name of the host.\n\n*neighborhood_groups* : General location.\n\n*neighborhood* : The area within the neighborhood group.\n\n*latitude* : Latitude.\n\n*longitude* : Longitude.\n\n*room_type*: Type of rooms (ie: Private Room, Entire home\/apt).\n\n*Price* : price per night.\n\n*minimum_nights* : amount of days out of the year when listing is available for booking.\n\n*number_of_reviews* : Total number of reviews.\n\n*last_review* : Date of the last review.\n\n*reviews_per_month* : Frequency of reviews per month.\n\n*calculated_host_listings_count* : How many properties this host has.\n\n*availability_365* : It is an amount of days out of the year when listing is available for booking.","1a8bd3d4":"## Feature Engineering","de9b668a":"The second to last plot tells us the number of posting each host has, ranked. This is especially useful for users who would like to stick with those hosts who are more professional in the industry and\/or working for a company related to airbnb. ","d81456a3":"## Machine Learning Models","0a2f37ae":"Since our df_reg data has a price mean of 130 dollars and the standard deviation of 85 dollars, it seems that some of these baselines are all great as the MAEs are 1\/2 a sigma away from the std. We can finetune these further to find the best model. Particularly - XGB, RF, SRVrbf, SGD are the ones that we should focus our energy on. This is expected as these algorithms are known to be more complex than simply LR. Ensemble and SVM are some of the top performers for shallow learning.\n\nDT seem to be overfitting quite a bit, as expected. However, RF model overfitting is another big question because it is designed to lower the variance. Yet, it's still less overfitting compared to DT as expected.","5fa9ea20":"## Data Wrangling"}}