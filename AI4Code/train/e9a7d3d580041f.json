{"cell_type":{"cf9cfb16":"code","cb2d434b":"code","a8610ca7":"code","777d6230":"code","d21166cf":"code","81bc0d0b":"code","3db89fe7":"code","0948f045":"code","58d4f40b":"code","cb9a7b7a":"code","3d4217d6":"code","152cae5a":"code","3fb6cb83":"code","231917ac":"code","c016d12e":"code","8bef7211":"code","3462c7b3":"code","00c0e168":"code","677d60a4":"code","1c009f78":"code","7f08d0a2":"code","c087a8c8":"code","93c0a27e":"code","8b1b968a":"code","c4719494":"code","5ec33a0e":"code","cf9026b3":"code","bf4677ab":"code","550296a9":"code","3768a397":"code","a06f4878":"code","2cc7eb76":"code","8b35633b":"code","2d665754":"code","4fa66f6c":"code","fa0bd25d":"code","cc8facbe":"code","adcf4fc7":"code","6ca43853":"code","55720da0":"code","bb3ae7c8":"code","01fb4adc":"code","c8c3b03a":"code","4a1ae149":"code","55bd1cc1":"code","3092de8b":"code","e86a164f":"code","0729fa28":"code","dd2441c8":"code","1e1b7ea8":"code","51e3edbf":"code","b237000e":"code","2a1e8a1e":"code","a05c2023":"code","a08dacb8":"code","a0f514fb":"code","ac30c174":"code","e1e34cf7":"code","7ed0ea4c":"code","11020cc8":"code","0724cb86":"code","38d291ba":"code","6d701cd4":"code","dfb3ad70":"code","37b91e66":"code","a51c9914":"code","8637142f":"code","188e47ab":"code","386f48a5":"code","8988c963":"code","be6faa2d":"code","06e175dc":"code","22ec402b":"code","2c3f3d10":"code","081c9220":"code","d266c793":"code","63a46f66":"code","24811030":"code","675475c7":"code","c9b0915c":"code","f0b8ecd0":"markdown","227a40f3":"markdown","30bc5f14":"markdown","20af46fc":"markdown","b6b2b455":"markdown","c033c11b":"markdown","4154e52b":"markdown","6915403f":"markdown","a160d27d":"markdown","ca23c42a":"markdown","01c9f374":"markdown","37b61196":"markdown","2ee86e1a":"markdown","a6e2c140":"markdown","f0a258e4":"markdown","3f66cea2":"markdown","13c29c3f":"markdown","13e43434":"markdown","c1dc0242":"markdown","cfbbd4be":"markdown","e23cefa4":"markdown","edfe4b12":"markdown","4eab482f":"markdown","1b716eda":"markdown","eefd70c3":"markdown","2e7834d3":"markdown","8272393b":"markdown","b66412a1":"markdown","9e73902a":"markdown","2c2e0f79":"markdown","58e69ead":"markdown","0cd5df45":"markdown","74744e83":"markdown","a9c2d8bb":"markdown","10c75d46":"markdown","35b93453":"markdown"},"source":{"cf9cfb16":"#Import Kaggle Data Into Colab\nfrom google.colab import files\n\nfiles.upload()","cb2d434b":"#Install Kaggle API\n!pip install -q kaggle","a8610ca7":"!mkdir -p ~\/.kaggle","777d6230":"#Moving the Kaggle API to appropriate folder\n!cp kaggle.json ~\/.kaggle\/","d21166cf":"#List of Kaggle Datasets\n!kaggle datasets list","81bc0d0b":"#List of datasets related to Credit Card Fraud\n!kaggle datasets list -s creditcardfraud","3db89fe7":"# Download the CreditCard Fraud Detection Dataset locally\n!kaggle datasets download -d mlg-ulb\/creditcardfraud","0948f045":"#List of contents downloaded\n!ls \/content","58d4f40b":"#Unzip the dataset\n!unzip creditcardfraud.zip","cb9a7b7a":"!ls \/content","3d4217d6":"#Importing Necessary Libraries for data loading, exploration\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","152cae5a":"#Loading File using Pandas\nccdata = pd.read_csv('creditcard.csv')","3fb6cb83":"#Checking Head\nccdata.head()","231917ac":"#Get DataFrame Info\nccdata.info()","c016d12e":"#List of Columns in Dataframe\nccdata.columns","8bef7211":"#Checking for NULL Values in given data\nccdata.isnull().sum()","3462c7b3":"#Identify how many Fraud\/Non-Fraud samples are available\nccdata['Class'].value_counts()","00c0e168":"#For easy processing, extract Fraud and Non-Fraud Dataframes\nfraud_df = ccdata[ccdata['Class']== 1]\nnonfraud_df = ccdata[ccdata['Class'] == 0]","677d60a4":"#Ensuring the data extracted correctly, by checking the length - Should match with cdata['Class'].value_counts() in above step\nlen(fraud_df), len(nonfraud_df)","1c009f78":"np.min(fraud_df['Amount']), np.max(fraud_df['Amount']), np.mean(fraud_df['Amount']), np.median(fraud_df['Amount'])","7f08d0a2":"np.min(nonfraud_df['Amount']), np.max(nonfraud_df['Amount']), np.mean(nonfraud_df['Amount']), np.median(nonfraud_df['Amount'])","c087a8c8":"#Transactions with Amount 0.0 in both Fraud and Non-Fraud\nlen(fraud_df[fraud_df['Amount'] == 0.0]), len(nonfraud_df[nonfraud_df['Amount'] == 0.0])","93c0a27e":"zeroA_fraud = len(fraud_df[fraud_df['Amount'] == 0.0])\nzeroA_nonfraud = len(nonfraud_df[nonfraud_df['Amount'] == 0.0])\n\nperZA_fraud = (zeroA_fraud\/len(fraud_df))*100\nperZA_nonfraud = (zeroA_nonfraud\/len(nonfraud_df))*100\n\nprint('Percent of \\'0\\' value transactions in Fraud: ', perZA_fraud)\nprint('Percent of \\'0\\' value transactions in Non-Fraud: ', perZA_nonfraud)","8b1b968a":"sns.distplot(fraud_df['Amount']), sns.distplot(nonfraud_df['Amount'])","c4719494":"sns.distplot(fraud_df['Time']), sns.distplot(nonfraud_df['Time'])","5ec33a0e":"sns.distplot(fraud_df['Amount'], kde=False, rug=True)","cf9026b3":"sns.distplot(nonfraud_df['Amount'], kde=False, rug=True)","bf4677ab":"#Scatter Plot combining both fraud and non-fraud\nplt.scatter(ccdata['Amount'],ccdata['Class'])\nplt.xlabel('Amount')\nplt.ylabel('Class')","550296a9":"#Scatter Plot combining both fraud and non-fraud\nplt.scatter(ccdata['Time'],ccdata['Class'])\nplt.xlabel('Time')\nplt.ylabel('Class')","3768a397":"sns.distplot(fraud_df['Time'], kde=False, rug=True)","a06f4878":"sns.distplot(nonfraud_df['Time'], kde=False, rug=True)","2cc7eb76":"plt.scatter(ccdata['V1'],ccdata['Class'])\nplt.xlabel('V1')\nplt.ylabel('Class')","8b35633b":"plt.scatter(ccdata['V2'],ccdata['Class'])\nplt.xlabel('V2')\nplt.ylabel('Class')","2d665754":"#Scatter Plot combining both fraud and non-fraud\nplt.scatter(ccdata['Time'],ccdata['Amount'])\nplt.xlabel('Time')\nplt.ylabel('Amount')","4fa66f6c":"plt.scatter(fraud_df['Time'], fraud_df['Amount'])\nplt.xlabel('Time')\nplt.ylabel('Amount')","fa0bd25d":"plt.scatter(nonfraud_df['Time'], nonfraud_df['Amount'])\nplt.xlabel('Time')\nplt.ylabel('Amount')","cc8facbe":"sns.pairplot(fraud_df)","adcf4fc7":"fraud_df.columns","6ca43853":"column = 'Amount'\nccdata[column]=np.log(ccdata[column])\nsns.boxplot(y = column, x = 'Class', data = ccdata)\nplt.ylabel(column)\nplt.title(column)\nplt.yticks()\nplt.show()","55720da0":"column = 'Time'\nccdata[column]=np.log(ccdata[column])\nsns.boxplot(y = column, x = 'Class', data = ccdata)\nplt.ylabel(column)\nplt.title(column)\nplt.yticks()\nplt.show()","bb3ae7c8":"for i, column in enumerate(fraud_df.columns):\n  plt.figure(figsize = (3, 3))\n  data = fraud_df.copy()\n  if 0 in data[column].unique():\n    pass\n  else:\n    data[column]=np.log(data[column])\n    sns.boxplot(y = column, x = 'Class', data = data)\n    plt.ylabel(column)\n    plt.title(column)\n    plt.yticks()\n    plt.show()","01fb4adc":"plt.figure(figsize = (30, 30))\n\ncorr_mat = ccdata.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True, vmin=-1, cmap='coolwarm')","c8c3b03a":"for i, column in enumerate(ccdata.columns):\n  plt.figure(figsize = (3, 3))\n  data = ccdata.copy()\n  if 0 in data[column].unique():\n    pass\n  else:\n    data[column]=np.log(data[column])\n    sns.boxplot(y = column, x = 'Class', data = data)\n    plt.ylabel(column)\n    plt.title(column)\n    plt.yticks()\n    plt.show()","4a1ae149":"#importing necessary libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold","55bd1cc1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score","3092de8b":"#Split the input data based on given input\n#Default: 70-30%\ndef getTrainTestSplit(X, y, split='70', seed = 0):\n  if(split == '90'):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10, random_state=seed)\n  elif(split == '80'):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=seed)\n  elif(split == '70'):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.30, random_state=seed)\n  elif(split == '65'):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.35, random_state=seed)\n\n  print('X_train.shape: ',X_train.shape, 'X_test.shape: ', X_test.shape, 'y_train.shape: ', y_train.shape, 'y_test.shape: ', y_test.shape)\n  return X_train, X_test, y_train, y_test","e86a164f":"#Extract the features based on input type\n'''\n  ALL_FEATURES: Use all the features, as-is, nothing dropped\n  CORREL_FEATURES: The features where the data distribution is in similar range for both FRAUD and GENUINE samples\n  NON_CORREL_FEATURES: The features where the data distribution is in different range for both FRAUD and GENUINE samples\n  RF_FEATURES: The featuers that were found importance in RandomForest Evaluation  \n'''\ndef getFeatures(data, featType = 'ALL_FEATURES'):\n  data_use = data.copy()\n  #Dropping Time feature as it is not contributing much differentiation\n  #Norm_Amount is used in place of Amount\n  X = data_use.drop(['Class', 'Time', 'Amount'], axis = 1)\n  y = data_use['Class']\n\n  if(featType == 'CORREL_FEATURES'):\n    X = data_use.drop(['V3', 'V5', 'V6', 'V7', 'V9', 'V10', 'V12' ,'V13', 'V14', 'V15', 'V18', 'V22', 'V24', 'V25', 'V26'], axis = 1)\n  elif(featType == 'NON_CORREL_FEATURES'):\n    X = data_use.drop(['V1', 'V2', 'V4', 'V8', 'V11', 'V16', 'V17' ,'V19', 'V20', 'V21', 'V23', 'V27', 'V28'], axis = 1)    \n  elif(featType == 'RF_FEATURES'):\n    X = data_use[['V9', 'V10', 'V11', 'V13', 'V15', 'V16', 'V17']]\n\n  print('X.Shape: ',X.shape, ' Y.Shape: ', y.shape)\n  return X, y","0729fa28":"def getClassifier(classifier_type='RandomForest'):\n  if(classifier_type == 'LogisticRegression'):\n    classifier = LogisticRegression()\n  elif(classifier_type == 'DecisionTree'):\n    classifier = DecisionTreeClassifier()\n  elif(classifier_type == 'SVM'):\n    classifier = svm.SVC(kernel='poly', C=1)\n  elif(classifier_type == 'NaiveBayes'):\n    classifier = GaussianNB()    \n  else:\n    classifier = RandomForestClassifier(random_state=0)\n\n  return classifier","dd2441c8":"#Get the dataset\ndef getData(data, datatype = 'ORIGINAL'):\n  data_use = data.copy()\n\n  if(datatype == 'ORIGINAL'):\n    #Return the original dataset as-is: Highly imbalanced: Fraud is of 0.017% of Total Samples\n    print('Data Shape: ',data_use.shape)\n    return data_use\n  elif(datatype == 'UNDER_SAMPLE'):\n    ''' \n      Return the sample dataset from the original dataset\n      Since Fraud Samples are 492, Extracting same number of samples [choosen randomly] from the Genuine Samples as well\n      Returned dataset is a fraction of original dataset: 984 Samples [out of 284,807], but the classes are balanced\n    '''\n    df_fraud = data_use[data_use['Class']==1]\n    df_genuine = data_use[data_use['Class']==0]\n    df_genuine = df_genuine.sample(n = len(df_fraud), replace = False)\n    data_use = pd.concat([df_fraud, df_genuine])\n    print('Data Shape: ',data_use.shape)    \n    return data_use\n  elif(datatype == 'OVER_SAMPLE'):\n    ''' \n      Return the over sampled dataset from the original dataset\n      Since Genuine Samples are 284315, Generating same number of synthetic samples from the Fraud Samples\n      Returned dataset is a almost double of original dataset: 568630 Samples [compared to 284,807], but the classes are balanced\n    '''    \n    df_fraud = data_use[data_use['Class']==1]\n    df_genuine = data_use[data_use['Class']==0]\n    df_fraud = df_fraud.sample(n = len(df_genuine), replace = True)\n    data_use = pd.concat([df_fraud, df_genuine])\n    print('Data Shape: ',data_use.shape)    \n    return data_use \n  elif(datatype == 'SMOTE'):\n    df_fraud = data_use[data_use['Class']==1]\n    df_genuine = data_use[data_use['Class']==0]\n    X = data_use.drop(['Class'], axis=1)\n    y = data_use['Class']\n    sm = SMOTE(ratio='auto',kind='regular')\n    X_sampled,y_sampled = sm.fit_sample(X,y.values.ravel())\n    column_names = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n                    'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n    X_sampled_df = pd.DataFrame(X_sampled, columns=column_names)\n    y_sampled_df = pd.DataFrame(y_sampled, columns=['Class'])\n    data_use = pd.concat([X_sampled_df, y_sampled_df])\n    print('Data Shape: ',data_use.shape)    \n    print('Before SMOTE: NumFraud = ',len(df_fraud), ' Genuine: ', len(df_genuine))\n    print('After SMOTE: NumFraud = ',len(data_use[data_use['Class']==1]), ' Genuine: ', len(data_use[data_use['Class']==0]))\n    print('Data Shape: ',data_use.shape)    \n    return data_use    ","1e1b7ea8":"#Train the model\ndef train_model(X_train, X_test, y_train, y_test, classifier):\n  #print('X_train.shape: ',X_train.shape, 'X_test.shape: ', X_test.shape, 'y_train.shape: ', y_train.shape, 'y_test.shape: ', y_test.shape)\n\n  #Fit the model\n  classifier.fit(X_train, y_train)\n  #Get Predictions\n  y_pred = classifier.predict(X_test)\n  #Calculate accuracy\n  acc = accuracy_score(y_test, y_pred)\n  #Get the confusion matrix\n  labels = [0, 1]\n  target_names=['Genuine', 'Fraud']\n  conf_matrix = confusion_matrix(y_test, y_pred, labels)\n  #class_report = classification_report(y_test, y_pred, target_names=target_names)\n  class_report = classification_report(y_test, y_pred, target_names = target_names, output_dict=True)\n  fp_rate = conf_matrix[1][0]\/(conf_matrix[1][0] + conf_matrix[0][0])\n  tp_rate = conf_matrix[1][1]\/(conf_matrix[1][1] + conf_matrix[0][1])  \n  return acc, conf_matrix, class_report, y_pred, fp_rate, tp_rate","51e3edbf":"}def show_confusion_matrix(cf_matrix):\n  group_names = ['Genuine: TP','Fraud: FP','Genuine: FP','Fraud: TP']\n  group_counts = ['{0:0.0f}'.format(value) for value in\n                cf_matrix.flatten()]\n  group_percentages = ['{0:.2%}'.format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n  labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n  labels = np.asarray(labels).reshape(2,2)\n\n  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Greens')","b237000e":"test_classifiers = 'LogisticRegression'\ntest_features = 'ALL_FEATURES'\ntest_datatypes = 'ORIGINAL'\ntest_split = '70'\ntest_seed = 42\n\n#Data Sample to use\ndata_use = getData(ccdata, test_datatypes)\n#Features to use in the input data\nX, y = getFeatures(data_use, test_features)\n#Get the classifier for modelling\nclassifier = getClassifier(test_classifiers)\n#Split the data based on input condition and set the relevant seed\nX_train, X_test, y_train, y_test = getTrainTestSplit(X, y, test_split, test_seed)\nprint(np.unique(y_test, return_counts=True))\nprint('*****Testing ',test_classifiers, ', Data Split: ',test_split,'(%), feature:',test_features,' Dataset: ',test_datatypes,'*****')\naccuracy, conf_matrix, class_report, y_pred, fp_rate, tp_rate = train_model(X_train, X_test, y_train, y_test, classifier)\nprint(show_confusion_matrix(conf_matrix))\nprint(class_report)","2a1e8a1e":"test_classifiers = 'RandomForest'\ntest_features = 'ALL_FEATURES'\ntest_datatypes = 'ORIGINAL'\ntest_split = '70'\ntest_seed = 42\n\n#Data Sample to use\ndata_use = getData(ccdata, test_datatypes)\n#Features to use in the input data\nX, y = getFeatures(data_use, test_features)\n#Get the classifier for modelling\nclassifier = getClassifier(test_classifiers)\n#Split the data based on input condition and set the relevant seed\nX_train, X_test, y_train, y_test = getTrainTestSplit(X, y, test_split, test_seed)\nprint(np.unique(y_test, return_counts=True))\nprint('*****Testing ',test_classifiers, ', Data Split: ',test_split,'(%), feature:',test_features,' Dataset: ',test_datatypes,'*****')\naccuracy, conf_matrix, class_report, y_pred, fp_rate, tp_rate = train_model(X_train, X_test, y_train, y_test, classifier)\nprint(show_confusion_matrix(conf_matrix))\nprint(class_report)","a05c2023":"from matplotlib.pyplot import figure\n\nimportances = classifier.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in classifier.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","a08dacb8":"#Testing with multiple classifiers with different combination of featuresets\ntest_classifiers = ['LogisticRegression', 'DecisionTree', 'RandomForest', 'NaiveBayes']\ntest_features = ['ALL_FEATURES', 'CORREL_FEATURES', 'NON_CORREL_FEATURES', 'RF_FEATURES']\ntest_datatypes = ['ORIGINAL', 'UNDER_SAMPLE', 'OVER_SAMPLE']#, 'SMOTE']\ntest_split = ['70'] #['90', '80', '65']\ntest_seed = [42]\n\nconf_matrix_list = []\nclass_report_list = []\n\n#Data Sample to use\nfor data in test_datatypes:\n  data_use = getData(ccdata, data)\n  #Features to use in the input data\n  for feat in test_features:\n    X, y = getFeatures(data_use, feat)\n    #Get the classifier for modelling\n    for test in test_classifiers:\n      classifier = getClassifier(test)\n      #Split the data based on input condition and set the relevant seed\n      for split in test_split:\n        for seed in test_seed:\n          X_train, X_test, y_train, y_test = getTrainTestSplit(X, y, split, seed)\n          print(np.unique(y_test, return_counts=True))\n          print('*****Testing ',test, ', Data Split: ',split,'(%), feature:',feat,' Dataset: ',data,'*****')\n          accuracy, conf_matrix, class_report,y_pred, fp_rate, tp_rate = train_model(X_train, X_test, y_train, y_test, classifier)\n          conf_matrix_list.append(conf_matrix)\n          class_report_list.append(class_report)\n          print(class_report)","a0f514fb":"def testcrossfold(data):\n  data_use = data.copy()\n  X = data_use.drop(['Class'], axis=1)\n  y = data_use['Class']#pd.DataFrame(data_use['Class'], columns=['Class'])\n\n\n  recall_scores = []\n  report_list= []\n  #Choosing 10 splits as per convention\n  kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n  # enumerate the splits and summarize the distributions\n  for train_ix, test_ix in kfold.split(X):\n    # select rows\n    #print(len(train_ix), len(test_ix))\n    train_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n    train_y, test_y = y[train_ix], y[test_ix]\n    print(np.unique(train_y, return_counts=True))\n    print(np.unique(test_y, return_counts=True))\n    classifier = getClassifier('LogisticRegression')\n    classifier.fit(train_X, train_y)\n    y_pred = classifier.predict(test_X)\n    targets = ['Genuine', 'Fraud']\n    report = classification_report(test_y, y_pred, target_names = targets, output_dict=True)\n    report_list.append(report)\n    recall_scores.append(report['Fraud']['recall'])\n  return recall_scores, report_list","ac30c174":"recall_scores, report = testcrossfold(ccdata)","e1e34cf7":"recall_fraud = pd.DataFrame(report,columns=['ReCall_Fraud'])\n\nsns.set(style=\"white\", rc={\"lines.linewidth\": 3})\nsns.barplot(x=['Iter1','Iter2','Iter3','Iter4','Iter5', 'Iter6', 'Iter7', 'Iter8', 'Iter9', 'Iter10'],y=\"ReCall_Fraud\",data=recall_fraud)\nplt.show()\nsns.set()","7ed0ea4c":"#Calculating FPRate, TP Rate\n#Testing with multiple classifiers with different combination of featuresets\ntest_classifiers = ['LogisticRegression', 'DecisionTree', 'RandomForest', 'NaiveBayes']\ntest_features = ['ALL_FEATURES']\ntest_datatypes = ['ORIGINAL']\ntest_split = ['70']\ntest_seed = [42]\n\nfp_rate_list = []\ntp_rate_list = []\n\n#Data Sample to use\nfor data in test_datatypes:\n  data_use = getData(ccdata, data)\n  #Features to use in the input data\n  for feat in test_features:\n    X, y = getFeatures(data_use, feat)\n    #Get the classifier for modelling\n    for test in test_classifiers:\n      classifier = getClassifier(test)\n      #Split the data based on input condition and set the relevant seed\n      for split in test_split:\n        for seed in test_seed:\n          X_train, X_test, y_train, y_test = getTrainTestSplit(X, y, split, seed)\n          print(np.unique(y_test, return_counts=True))\n          print('*****Testing ',test, ', Data Split: ',split,'(%), feature:',feat,' Dataset: ',data,'*****')\n          _,_,_,_,fp_rate, tp_rate = train_model(X_train, X_test, y_train, y_test, classifier)\n          fp_rate_list.append(fp_rate)\n          tp_rate_list.append(tp_rate)\n          print('fp_rate: ',fp_rate, ', tp_rate: ',tp_rate)\n          print('\\n')","11020cc8":"#Original Data ROC Curve\nfor i, txt in enumerate(test_classifiers):\n  plt.annotate(txt, (fp_rate_list[i], tp_rate_list[i]), fontsize = 5)\n\nplt.plot(fp_rate_list, tp_rate_list)\nplt.show()","0724cb86":"#UNDER_SAMPLE ROC Curve\nfor i, txt in enumerate(test_classifiers):\n  plt.annotate(txt, (fp_rate_list[i], tp_rate_list[i]), fontsize = 5)\n\nplt.plot(fp_rate_list, tp_rate_list)\nplt.show()","38d291ba":"#Testing with multiple classifiers with different combination of featuresets\ntest_classifiers = ['LogisticRegression', 'DecisionTree', 'RandomForest', 'NaiveBayes']\ntest_features = ['CORREL_FEATURES']\ntest_datatypes = ['UNDER_SAMPLE']#, 'SMOTE']\ntest_split = ['70'] #['90', '80', '65']\ntest_seed = [42]\n\nconf_matrix_list = []\nclass_report_list = []\n\n#Data Sample to use\nfor data in test_datatypes:\n  data_use = getData(ccdata, data)\n  #Features to use in the input data\n  for feat in test_features:\n    X, y = getFeatures(data_use, feat)\n    #Get the classifier for modelling\n    for test in test_classifiers:\n      classifier = getClassifier(test)\n      #Split the data based on input condition and set the relevant seed\n      for split in test_split:\n        for seed in test_seed:\n          X_train, X_test, y_train, y_test = getTrainTestSplit(X, y, split, seed)\n          print(np.unique(y_test, return_counts=True))\n          print('*****Testing ',test, ', Data Split: ',split,'(%), feature:',feat,' Dataset: ',data,'*****')\n          accuracy, conf_matrix, class_report,y_pred, fp_rate, tp_rate = train_model(X_train, X_test, y_train, y_test, classifier)\n          conf_matrix_list.append(conf_matrix)\n          class_report_list.append(class_report)\n          print(class_report)","6d701cd4":"conf_matrix_list[0]","dfb3ad70":"class_report_list[0]","37b91e66":"fraud_prec_list = []\ngen_prec_list = []\nfraud_rec_list = []\ngen_rec_list = []\nfraud_f1_list = []\ngen_f1_list = []\n\nlabel_list = ['LR', 'DT', 'RF', 'NB']\n\n\nfor i in range(len(class_report_list)):\n    #print(class_report_list[i]['Fraud']['precision'], class_report_list[i]['Fraud']['recall'], class_report_list[i]['Fraud']['f1-score'])\n    fraud_prec_list.append(class_report_list[i]['Fraud']['precision'])\n    gen_prec_list.append(class_report_list[i]['Genuine']['precision'])\n\n    fraud_rec_list.append(class_report_list[i]['Fraud']['recall'])\n    gen_rec_list.append(class_report_list[i]['Genuine']['recall'])\n\n    fraud_f1_list.append(class_report_list[i]['Fraud']['f1-score'])\n    gen_f1_list.append(class_report_list[i]['Genuine']['f1-score'])     \n    ","a51c9914":"fraud_prec_list","8637142f":"plt.boxplot(fraud_prec_list, labels=label_list)\n\npalette = ['r', 'g', 'b', 'y']\n\nfor x, val, c in zip(fraud_prec_list, label_list, palette):\n    plt.scatter(x, val, alpha=0.4, color=c)","188e47ab":"class_report_list[0]['Fraud']['recall'], class_report_list[1]['Fraud']['recall'], class_report_list[2]['Fraud']['recall'], class_report_list[3]['Fraud']['recall'],","386f48a5":"    fraud_prec_list.append(report['Fraud']['precision'])\n    gen_prec_list.append(report['Genuine']['precision'])\n\n    fraud_rec_list.append(report['Fraud']['recall'])\n    gen_rec_list.append(report['Genuine']['recall'])\n\n    fraud_rec_list.append(report['Fraud']['f1-score'])\n    gen_rec_list.append(report['Genuine']['f1-score'])   ","8988c963":"fraud_rec_list","be6faa2d":"plt.scatter(label_list, fraud_prec_list)","06e175dc":"plt.scatter(label_list, fraud_prec_list, c='blue', marker = '*', s= 85)\nplt.xlabel('Experimented Algos')\nplt.ylabel('Fraud: Precision')","22ec402b":"plt.scatter(label_list, fraud_rec_list, c='blue', marker = '*', s= 85)\nplt.xlabel('Experimented Algos')\nplt.ylabel('Fraud: Recall')","2c3f3d10":"plt.scatter(label_list, gen_rec_list, c='blue', marker = '*', s= 85)\nplt.xlabel('Experimented Algos')\nplt.ylabel('Genuine: Recall')","081c9220":"plt.scatter(label_list, fraud_prec_list, c='blue', marker = '*', s= 85)\nplt.scatter(label_list, fraud_prec_list, c='blue', marker = '*', s= 85)\nplt.scatter(label_list, fraud_rec_list, c='red', marker = 'v', s= 85)\nplt.scatter(label_list, fraud_f1_list, c='orange', marker = 'H', s= 85)","d266c793":"len(fraud_rec_list), fraud_rec_list","63a46f66":"plt.scatter(fraud_rec_list, label_list, c='red', marker = 'v')","24811030":"# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(fraud_prec_list)\nax.set_xticklabels(label_list)\nplt.show()","675475c7":"def plot_classification_report(cr, title='Classification report ', with_avg_total=False, cmap=plt.cm.Blues):\n\n    lines = cr.split('\\n')\n\n    classes = []\n    plotMat = []\n    for line in lines[2 : (len(lines) - 3)]:\n        print(line)\n        t = line.split()\n        print(t)\n        classes.append(t[0])\n        v = [float(x) for x in t[1: len(t) - 1]]\n        print(v)\n        plotMat.append(v)\n\n    if with_avg_total:\n        aveTotal = lines[len(lines) - 1].split()\n        classes.append('avg\/total')\n        vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n        plotMat.append(vAveTotal)\n\n\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    x_tick_marks = np.arange(3)\n    y_tick_marks = np.arange(len(classes))\n    plt.xticks(x_tick_marks, ['precision', 'recall', 'f1-score'], rotation=45)\n    plt.yticks(y_tick_marks, classes)\n    plt.tight_layout()\n    plt.ylabel('Classes')\n    plt.xlabel('Measures')","c9b0915c":"plot_classification_report(class_report_list[0], title='Classification report ', with_avg_total=False, cmap=plt.cm.Blues)","f0b8ecd0":"Utility function to get the classifier [As multiple classifiers are used for testing, wrote a utility function for ease of use]","227a40f3":"CORREL_FEATURES: ['V3', 'V5', 'V6', 'V7', 'V9', 'V10', 'V12' ,'V13', 'V14', 'V15', 'V18', 'V22', 'V24', 'V25', 'V26']\n\nNON_CORREL_FEATURES: ['V1', 'V2', 'V4', 'V8', 'V11', 'V16', 'V17' ,'V19', 'V20', 'V21', 'V23', 'V27', 'V28']","30bc5f14":"From the above plot, most of the transactions are < 5000 [Median Value ~ 88]","20af46fc":"From all the analysis:\n1. No patterns for 'Amount', 'Time' observed w.r.t Fraudulent\/Non-Fraudulent transactions\n2. No correlations among features","b6b2b455":"Continuing with Uni-Variate Analysis\nIdentifying outliers for the columns from V1 to V28 using Boxplots","c033c11b":"Not sure, if outliers of these columns will have any impact in modelling since these features are PCA normalized and no information on what each column representing","4154e52b":"Utility function for train-test splitting","6915403f":"1) From the box-plots, tried using the features which are in similar data d","a160d27d":"Uni-Variate Analysis: No Specific patterns observed in 'Time' feature\n\nChecking plots for other columns: V1 to V28","ca23c42a":"Above Info Indicates: 30 Columns [Time, V1 to V28, Amount, Class]\n\nAs per the description given in Kaggle, V1 to V28 are transformed using PCA [Confidentiality issues]\n\nTime, Amount are not transformed\/normalized\n\nFraud\/Normal is indicated by Class Column: Fraud: 1, Normal: 0","01c9f374":"Testing Cross-Fold Validation with Logistic Regression Classifier","37b61196":"From the above, the prominent features are: V16, V11, V13, V10, V9, V15, and V17\nChoosing these features and evaluating accuracy with other algos as well.\nThese features are defined as 'RF_FEATURES'","2ee86e1a":"No Correlation observed among the variables from the above heatmap [Already the features were PCA normalized]","a6e2c140":"Utility function to handle model training","f0a258e4":"Recall for 'Fraud' category is 0.62\nInorder to develop a fool proof system, need better recall values for Fraud Category. Will check other classification methods and also data balancing techniques","3f66cea2":"Feature Engineering:\n\n1) Choose better features: Using Random Forest and identify the prominent features","13c29c3f":"From the above information:\n  1. Fraud Transaction: Max Amount: 2125.87, Median Amount: 122.211\n  2. Non-Fraud Transaction: Max Amount: 25691.16, Median Amount: 88.29\n  3. There are rows with amount = 0.0 in Fraud[5%], Non-Fraud[0.6%] --> Probably need Imputation??\n\nLets get the distribution of 'Amount' to know majority of transaction amount","13e43434":"In-addition to above approach, also using RandomForest algorithm to identify the prominent features","c1dc0242":"Uni-Variate Analysis: No Specific patterns observed in 'Amount' feature [Cannot decide less amount means Fraud, more amount means Non-Fraud]\n\nChecking if 'Time' has any patterns in fraud transactions","cfbbd4be":"From the above results, it is understood that the selective features [CORREL_FEATURES\/NON_CORREL_FEATURES giving the best recall for Fraud and Genuine with Decision Tree and Random Forest Classifiers for all types of datasets [ORIGINAL, UNDER_SAMPLE, OVER_SAMPLE]","e23cefa4":"Most of the amount in the transaction given are in low ranges, not very clear on how much is the amount from these charts.Checking other distribution plot","edfe4b12":"Feature-Engineering","4eab482f":"From the above plot, most of the transactions are about 100 or so [Median Value~122]","1b716eda":"Fraud and Non-Fraud Transactions are happening across all times of day, no specific patterns emerged from above plot.\n\nChecking individual plots for 'Time' if any other patterns can be identified","eefd70c3":"Based on the above box plots, choosing the features which have similar data distribution for Fraud and Genuine classes. These features are defined as CORREL_FEATURES and is used in validating with various algos. \n\nThe features that are not CORREL_FEATURES are defined as NON_CORREL_FEATURES","2e7834d3":"Tried using Pairplot to understand the distribution - However too much data, not really understanding anything from the above. Probably this will make more sense in later steps","8272393b":"Utility function for feature extraction from the given data","b66412a1":"Let's begin Un-variate Analysis\n1. 'Amount' -> Any pattern in amount for fraud\/non-fraud transactions\n    \n    Since this is a numerical field, check the min, max, median and also distribution of the feature","9e73902a":"Methods to improve recall\n\nFeature Engineering:\n\n1) Choose better features: Using Random Forest and identify the prominent features\n\nData Balancing:\n\n1) UNDER_SAMPLE\n\n2) OVER_SAMPLE\n\n3) Cross-Fold Validation\n\nModelling:\n\n1) Hyperparameter Tuning","2c2e0f79":"Inorder to understand the behaviour, testing the data with\n\na) Multiple Classifiers [Logistic Regression, Decision Tree, RandomForest, NaiveBayes]\n\nb) Multiple Features [CORREL_FEATURES, NON_CORREL_FEATURES, RF_FEATURES]\n\nc) Multiple Datasets [Original (As-is), UNDER_SAMPLE, OVER_SAMPLE]","58e69ead":"No NULL Values from the above result -> Not using any imputation method at this stage","0cd5df45":"Utility function to get the dataset: Multiple methods are used [UNDER_SAMPLING, OVER_SAMPLING, SMOTE]","74744e83":"Applying Logistic Regression classifier for the given data [as-is] with ALL_FEATURES and generating the confusion matrix, classification report","a9c2d8bb":"Moving onto Correlations among variables","10c75d46":"The data looks like highly biased as the fraudulent samples are only 0.17% of whole dataset\n\nIn next steps of modeling, this has to be keep in mind for choosing train, test datasets","35b93453":"Lets Begin the Data Exploration\n1. Check if any null values are present, if so : need to do relevant imputation\n2. Check possible\/unique values for each column. \n    Ensure only 1\/0 are given for all Class column\n3.Do Uni-variate and Bi-Variate Analysis and observe any patterns emerge"}}