{"cell_type":{"3a2f0076":"code","b4b4f079":"code","0c6c0257":"code","50980325":"code","69c99636":"code","34593d78":"code","aba59eb1":"code","059841f3":"code","b6cb0dc3":"code","e5ca1699":"code","729a7794":"code","0bca8860":"code","6af85549":"code","caef0107":"code","bdc64327":"code","ca6a436a":"code","5e3a310f":"code","f85acdfe":"code","4fc9c5fa":"code","49e7f640":"code","3e02cfd9":"code","c022c8d7":"code","f3e0193c":"code","8a0806e8":"code","17308234":"code","538fa018":"code","871797bf":"code","1ca44c57":"code","517032dd":"code","d08c7ea7":"code","0111667f":"code","63dd4b64":"code","fd466e7e":"code","d82de19f":"code","43dd5aca":"markdown","e1907779":"markdown","dc1620d4":"markdown","031f0f76":"markdown","deeeb7b8":"markdown","1273cd7d":"markdown","e83c7064":"markdown","d31b50ae":"markdown","c770d179":"markdown","3764ffcf":"markdown","3b850582":"markdown","6576f931":"markdown","6f46a5a9":"markdown","2bc3d338":"markdown","76521cab":"markdown","fc21f688":"markdown","def484dc":"markdown","0ce0f66e":"markdown","2e03b26a":"markdown","324bb0d2":"markdown","6a4cf8d1":"markdown","29d87b06":"markdown","c56ef564":"markdown","1ba30f34":"markdown","8af0e415":"markdown","94f5ac83":"markdown","bf3da861":"markdown","02d9db4a":"markdown","897889b4":"markdown","b8cab99d":"markdown","24dff2b2":"markdown","d4553e10":"markdown","4e3e51e2":"markdown","d70105ab":"markdown","3f3be7d7":"markdown","ec94837a":"markdown","aa3af509":"markdown","375cc623":"markdown","5627e34d":"markdown","eae006c5":"markdown","477face7":"markdown","93024d52":"markdown","d9459ba5":"markdown"},"source":{"3a2f0076":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import mutual_info_classif \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, confusion_matrix, classification_report, f1_score, accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, roc_auc_score\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4b4f079":"df_heart = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf_heart.head()","0c6c0257":"df_heart.info()","50980325":"X = df_heart.drop('DEATH_EVENT', axis = 1) #define features\ny = df_heart['DEATH_EVENT'] #define target\n\n#split datasets in train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)","69c99636":"model = [] #define a list of model\n#append each model we want to try to the list\nmodels = [] #create a list\n#append each model to the list\nmodels.append(('LR', LogisticRegression(C = 1, solver = 'liblinear')))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisionTree', DecisionTreeClassifier(random_state = 1)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state = 1)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 1)))\nmodels.append(('GradiantBoost', GradientBoostingClassifier(random_state = 1)))\n#setting the parameters eval_metric and use_label_encoder removes all warning before showing the results \n#(they don't impact the cross validation score)\nmodels.append(('XGBoost', XGBClassifier(n_estimators = 500, eval_metric='mlogloss', use_label_encoder =False)))","34593d78":"results = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, shuffle = True, random_state=1)\n    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = \"accuracy\")\n    cv_f1 = cross_val_score(model, X_train, y_train, cv = kfold, scoring = \"f1\")\n    results.append(cv_results)\n    results.append(cv_f1)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_f1.mean())\n    print(msg)","aba59eb1":"#get the shape of the dataset\ndf_heart.shape","059841f3":"#statistical description is the most straightforward method to have a clear picture of the data set\n#advantage here --> all numerical values\ndf_heart.describe()","b6cb0dc3":"men = len(df_heart[df_heart['sex'] == 1])\nwomen = len(df_heart[df_heart['sex'] == 0])\nprint('There are', men, 'men')\nprint('There are', women, 'women')","e5ca1699":"#check the ratio death \/ gender\nmen_death = len(df_heart[(df_heart['sex'] == 1) & (df_heart['DEATH_EVENT'] == 1)])\nwomen_death = len(df_heart[(df_heart['sex'] == 0) & (df_heart['DEATH_EVENT'] == 1)])\nprint('The ratio death \/ sex for men is', \"{:.2f}\".format((men_death \/ men) * 100), '%')\nprint('The ratio death \/ sex for women is', \"{:.2f}\".format((women_death \/ women) * 100), '%')","729a7794":"plt.figure(figsize = (10,5))\nsns.countplot(data = df_heart, x = 'DEATH_EVENT', hue = 'sex')\nplt.show()","0bca8860":"plt.figure(figsize = (10,5))\nsns.histplot(data = df_heart, x = 'age', hue = 'sex') #distribution age per gender (0 = female, 1 = male)\nplt.show()","6af85549":"plt.figure(figsize = (20,8))\nsns.countplot(data = df_heart, x = 'age', hue = 'DEATH_EVENT')\nplt.show()","caef0107":"plt.figure()\nsns.pairplot(data = df_heart, hue = 'DEATH_EVENT')\nplt.show()","bdc64327":"#feature with binary values won't be checked for skewness\nbinary = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT']\n#check the skew score\nskew = (df_heart.drop(binary, axis = 1)).skew()\nprint(skew.sort_values(ascending = False))","ca6a436a":"plt.figure(figsize = (15, 12))\nplt.subplot(3,3,1)\nsns.histplot(df_heart['age'], kde = True)\nplt.subplot(3,3,2)\nsns.histplot(df_heart['creatinine_phosphokinase'], kde = True)\nplt.subplot(3,3,3)\nsns.histplot(df_heart['ejection_fraction'], kde = True)\nplt.subplot(3,3,4)\nsns.histplot(df_heart['platelets'], kde = True)\nplt.subplot(3,3,5)\nsns.histplot(df_heart['serum_creatinine'], kde = True)\nplt.subplot(3,3,6)\nsns.histplot(df_heart['serum_sodium'], kde = True)\nplt.subplot(3,3,7)\nsns.histplot(df_heart['time'], kde = True)\nplt.show()","5e3a310f":"#define column not impacted by the log\nnot_log = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT', 'age', 'time']\n\n#apply the log on the rest\nlog = df_heart.drop(not_log, axis = 1) \nlog.columns\ndf_heart[['creatinine_phosphokinase', 'ejection_fraction', 'platelets',\n       'serum_creatinine', 'serum_sodium']] = np.log(df_heart[['creatinine_phosphokinase', 'ejection_fraction', 'platelets',\n       'serum_creatinine', 'serum_sodium']])","f85acdfe":"#verify\n#better but not perfect\nskew = (df_heart.drop(not_log, axis = 1)).skew()\nprint(skew.sort_values(ascending = False))","4fc9c5fa":"#AGE\nbin_ranges = [30, 45, 60, 75, 100] #create the groups\nbin_names = [1, 2, 3, 4] #create the name given to the group\n\ndf_heart['Age_group'] = pd.cut(np.array(df_heart['age']), bins = bin_ranges)\ndf_heart['Age_label'] = pd.cut(np.array(df_heart['age']),bins = bin_ranges, labels = bin_names)\n\n#check the groups and count\ndf_heart['Age_label'].value_counts().sort_index()","49e7f640":"#one hot encoding on the age_label\ndf_heart = pd.get_dummies(df_heart, columns=['Age_label'],prefix='age')","3e02cfd9":"X = df_heart.drop(['DEATH_EVENT','Age_group'], axis =  1)\ny = df_heart['DEATH_EVENT']","c022c8d7":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","f3e0193c":"def make_mi_scores(X_scaled, y):\n    mi_scores = mutual_info_classif(X_scaled, y, discrete_features = True)\n    mi_scores = pd.Series(mi_scores, name = \"MI Scores\", index = X_scaled.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y)\nmi_scores  # show a few features with their MI scores","8a0806e8":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","17308234":"X = df_heart[['time', 'creatinine_phosphokinase', 'platelets', 'serum_creatinine', 'age', 'ejection_fraction', 'serum_sodium']]\ny = df_heart['DEATH_EVENT']\n\nX_scaled = StandardScaler().fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state = 1)","538fa018":"model = [] #define a list of model\n#append each model we want to try to the list\nmodels = [] #create a list\n#append each model to the list\nmodels.append(('LR', LogisticRegression(C = 1, solver = 'liblinear')))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisionTree', DecisionTreeClassifier(random_state = 1)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state = 1)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 1)))\nmodels.append(('GradiantBoost', GradientBoostingClassifier(random_state = 1)))\n#setting the parameters eval_metric and use_label_encoder removes all warning before showing the results \n#(they don't impact the cross validation score)\nmodels.append(('XGBoost', XGBClassifier(n_estimators = 500, eval_metric='mlogloss', use_label_encoder =False)))","871797bf":"results = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, shuffle = True, random_state=1)\n    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = \"accuracy\")\n    cv_f1 = cross_val_score(model, X_train, y_train, cv = kfold, scoring = \"f1\")\n    results.append(cv_results)\n    results.append(cv_f1)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_f1.mean())\n    print(msg)","1ca44c57":"model_rfc = RandomForestClassifier(criterion = 'gini', max_depth = 4, max_features = 'auto', n_estimators = 500, random_state = 1)\nmodel_rfc.fit(X_train, y_train) #fit the model\npredictions_train = model_rfc.predict(X_train) #compute it to calculate the mse later and compare to generalization error (overfit\/underfit)\npredictions_test = model_rfc.predict(X_test) #run prediction on the test set\nmodel_rfc ","517032dd":"print('--------------------Random Forest Classifier--------------------')\nprint()\n#ACCURACY\nprint('Training accuracy is', model_rfc.score(X_train, y_train))\nprint('Test accuracy is', model_rfc.score(X_test, y_test))\nprint('-' * 40)\n\n#CROSS VALIDATION Mean Squared Error\nmse = - cross_val_score(model_rfc, X_train, y_train, cv = 10,scoring = 'neg_mean_squared_error',n_jobs = -1)\nprint('The generalization error is equal to', mse.mean())\nprint('The train error is equal to', MSE(y_train, predictions_train))\nprint('-' * 40)\n\n#CROSS VALIDATION ACCURACY\nscore_accuracy = cross_val_score(model_rfc, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint('Average accuracy score is', score_accuracy.mean())\nprint('-' * 40)\n\n#CONFUSIN MATRIX\nconfusion = confusion_matrix(y_test, predictions_test)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('-' * 40)\n\n#CLASSIFICATION REPORT\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, predictions_test)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, predictions_test)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, predictions_test)))\nprint('F1: {:.2f}'.format(f1_score(y_test, predictions_test)))\nprint('-' * 40)\nprint('The classification report is')\nprint(classification_report(y_test, predictions_test))","d08c7ea7":"#[:,1] is necessary since we need a 1d array, without it it is a 2d array\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, model_rfc.fit(X_train, y_train).predict_proba(X_test)[:,1])\n\n# find threshold closest to zero\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], 'o', markersize = 10,\n label=\"threshold rf\", fillstyle = \"none\", c = 'k', mew = 2)\nplt.plot(precision_rf, recall_rf, label = \"precision recall curve\")\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.legend(loc = \"best\")\nplt.show()","0111667f":"#[:,1] is necessary since we need a 1d array, without it it is a 2d array\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, model_rfc.fit(X_train, y_train).predict_proba(X_test)[:,1])\n#above, the ROC curve. below, the AUC curve\nroc_auc_rf = auc(fpr_rf, tpr_rf)\n\nplt.plot(fpr_rf, tpr_rf, label=\"ROC Curve\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Postive Rate (recall)\")\n# find threshold closest to zero\nclose_zero = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(fpr_rf[close_zero], tpr_rf[close_zero], 'o', markersize = 10,\n label = \"threshold zero\", fillstyle = \"none\", c = 'k', mew = 2)\nplt.legend(loc = 'best')\nplt.show()","63dd4b64":"rf_auc = roc_auc_score(y_test, model_rfc.fit(X_train, y_train).predict_proba(X_test)[:,1])\nprint('The AUC for the Random Forest is :', rf_auc)","fd466e7e":"#get all the parameters for the random forest classifier\nmodel_rfc.get_params()","d82de19f":"#grid search\n\n#define the hyperparameters first\nparam_grid = {\n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n    \n\n#define the grid search\ngrid_search = GridSearchCV(estimator = model_rfc, param_grid = param_grid, cv = 10, scoring = 'accuracy', error_score = 0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n#print the optimal hyperparameters and accuracy\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","43dd5aca":"<b>3. Visualize all data at once<\/b>\n<br>\nA more straightforward to visualize all data at once is to use the pairplot. This give a good picture on how the data is. Helpful for feature engineering.","e1907779":"As we can see, the death ratio for men and women is almost the same. Which means, men and women have the same chance to die by heart failure","dc1620d4":"With the feature selection performed previously, I'm going to only keep the top 7 features.","031f0f76":"<b>2. Feature selection<\/b>","deeeb7b8":"Visualize skew features","1273cd7d":"# Feature engineering\n\n\u201cFeature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\u201d\n<br>\n\u2014 Dr. Jason Brownlee","e83c7064":"# Model Evaluation","d31b50ae":"<b>1. Visualize the distribution of the target by sex<\/b>","c770d179":"After running the classification report we can see that the model is better at predicting 0s (no death) than 1s (death). One reason that can explain this is that our data set has more 0s than 1s so it will naturaly tend to better perform on predicting 0s.\n<br>\n<br>\nA way to understand and fix our recall score, is to visualize the ROC and AUC curves, which works well on binary classification problems.","3764ffcf":"Apply the log on the previous features except age and time","3b850582":"Binning age","6576f931":"The problem with continuous features is that it will often be skewed. With the logarithmic is a good solution but not always the best. For the age and time features, a binning seems more appropriate. BUT, time is not that skewed (value close to 0), I won't touch it.","6f46a5a9":"When tuning the model using the Grid Search method, we have to be careful with the necessary CPU to run this. Grid Search will try all possible combinations of parameters for a given model. In our case, Random Forest algorithm has 19 possible parameters. And for each parameter, let's say we want to try 5 possible values. With just that we reach a huge 5^19 possible combinations for our Grid Search. Good luck with that. Of course, here I'll choose few parameters.","2bc3d338":"From the curves above, different threshold can be tested to find the best compromise to improve our model's accuracy. And of course, we can compar to different learning algorithm.","76521cab":"# Conclusion","fc21f688":"# Modeling","def484dc":"<b>AUC<\/b>\n<br>\nAs for the precision-recall curve, we often want to summarize the ROC curve using a single number, the area under the curve (this is commonly just referred to as the AUC, and it is understood that the curve in question is the ROC curve).","0ce0f66e":"# Learning Model dummy test \nTest different classification learning model before cleaning of feature engineering the data.\n<br>\nThis step is out of curiosity (just for fun) to see what model performs better if no there is no cleaning process.\n<br>\n<br>\nI would say this step is mostly useless in the real world because the data is rarely ready for analysis. But in Kaggle, a good majority of datasets are ready for analysis and require little cleaning.\n<br>\nFor this dataset, there are only numerical numerical values for the features. ML models like numerical values. So, this step can be done just by curiosity.\n<br>\n<br>\nI won't bother with tunning the models. I'll put the basic parameters or none. This step is basically like a dummy check\/discovery.","2e03b26a":"# Model tuning","324bb0d2":"KNN and SVM models perform poorly, while the others perform well. But remember, no data cleaning was perform. Weird enough, the cross validation results using the f1 score is equal to 0 for the SVM method.","6a4cf8d1":"I'll perform a comparison of different learning model based on the feature engineering I just performed. I will choose the best accuracy and f1 score for the model evaluation and model tuning. Based on the findings, I will improve the feature engineering to get better scores (depends on the evaluation metrics).","29d87b06":"<b>1. Deal skewed features<\/b>","c56ef564":"By using this parameters found with the Grid Search method, out training set decreased from 1 to 0.915 while our cross validation mean accuracy rose from 0.81 to 0.83. Which is good, our model is not overfitted anymore and have a better prediction. Additionally, our precision score is better but our recall score went worse from 0.85 to 0.7 -> meaning that our model has difficulties to find all true positives instance, which can be verified with the confusion matrix.","1ba30f34":"# Am I gonna die by heart failure or not?\nIn this notebook, I use the Heart Failure data set to determine if a person would die of heart failure or not given a set of features. The notebook is separated in 7 parts:\n<ol>\n    <li>Learning model dummy test<\/li>\n    <li>Data Exploratory Analysis<\/li>\n    <li>Feature Engineering<\/li>\n    <li>Feature scaling and selection<\/li>\n    <li>Modeling<\/li>\n    <li>Model Evaluation<\/li>\n    <li>Model Tuning<\/li>\n<\/ol>\nThe goal of this notebook is not necessarly to obtain the best accuracy score for prediction. But rather apply and understand each and every step that would lead to a good machine learning model.\n<br>\n<br>\nFor this, I use the of 4 ressources:\n<ol>\n    <li>Introduction to Machine Learning with Python - O'Reilly<\/li>\n    <li>Hands On Machine Learning with Scikit Learn Keras and Tensorflow - O'Reilly<\/li>\n    <li>Generalization Error - Datacamp - https:\/\/s3.amazonaws.com\/assets.datacamp.com\/production\/course_6280\/slides\/chapter2.pdf<\/li>\n    <li>Machine Learning Basics - Lecture slides for Chapter 5 of Deep Learning - www.deeplearningbook.org - Ian Goodfellow<\/li>\n<\/ol>","8af0e415":"# Feature scaling and selection","94f5ac83":"<ul>\n    <li>Sex - Gender of patient Male = 1, Female =0<\/li>\n    <li>Age - Age of patient<\/li>\n    <li>Diabetes - 0 = No, 1 = Yes<\/li>\n    <li>Anaemia - 0 = No, 1 = Yes<\/li>\n    <li>High_blood_pressure - 0 = No, 1 = Yes<\/li>\n    <li>Smoking - 0 = No, 1 = Yes<\/li>\n    <li>DEATH_EVENT - 0 = No, 1 = Yes<\/li>\n    <li>Time - Follow-up period (days)<\/li>\n    <li>Serum_sodium - Level of serum sodium in the blood (mEq\/L)<\/li>\n    <li>Sereum_creatine - Level of serum creatinine in the blood (mg\/dL)<\/li>\n    <li>Platelets - Platelets in the blood (kiloplatelets\/mL)<\/li>\n    <li>Ejection_fraction - Percentage of blood leaving the heart at each contraction (percentage)<\/li>\n    <li>Creatine_phosophokinase - Level of the CPK enzyme in the blood (mcg\/L)<\/li>\n<\/ul>","bf3da861":"Notes:\n<br>\nFor the criterion:\n<ul>\n    <li>The gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.<\/li>\n    <li>Entropy is a measure of information that indicates the disorder of the features with the target.<\/li>\n<\/ul>","02d9db4a":"<b>Precision vs Recall curve<\/b>\n<br>\nChanging the threshold that is used to make a classification decision in a model is a way to adjust the trade-off of precision and recall for a given classifier. The hard part is to develop a model that still has reasonable precision with this threshold.\n<br>\n","897889b4":"One of the ways to select the right feature right at the beginning of the observation is by using the Mutual Information method. It measures associations between a feature and the target. It is an advantge to use it when there are dozens of features, which can be quite difficult to perform an EDA and cleaning on them.\n<br>\n<br>\nIn this case, using the Random Forest algorithm, we bump into an overfitting model. One of the solution to overcome this problem is to choose only the necessary features (with a big impact on the target) to eliminate any noise.","b8cab99d":"With the heart disease data set, we were able to go through the ML pipeline from the recovery of the data to the model evaluation.\n<br>\n<br>\nOf course we can improve the model created in this notebook, but the main idea is there. An extensive work can be performed for the feature engineering part of the model.\n<br>\n<br>\nMoreover, as I already stated in this notebook, the little amount of entries for this data set makes results not satisfying (overfitting, ROC and Precision-Recall curves). We would need more data to have a satisfying model.","24dff2b2":"Let's visualize the results. Always better to see a visualization to interpret anything.","d4553e10":"<b>ROC<\/b>\n<br>\nSimilar to the precision-recall curve, the ROC curve considers all possible thresholds for a given classifier, but instead of reporting precision and recall, it shows the false positive rate (FPR) against the true positive rate (TPR).","4e3e51e2":"Death per age","d70105ab":"Note to myself:\n<br>\nOnly 299 entries --> risk of overfitting. Careful with the model complexity","3f3be7d7":"# Data Exploratory Analysis","ec94837a":"<b>2. Visualize the age distribution<\/b> ","aa3af509":"In this dataset, Men are more likely to die of heart failure. BUT, they are more represented than female. Which means that we cannot say for sure than men really die more than women. Ideally we should have 50-50 men and women.\n<br>\n<br>\nA way to unbias this assumption, is to check the ratio of death \/ total sex for men and women.","375cc623":"Based on the previous results, I decided to go with the random forest classifier algorithm. This algorithm is easier to work with for tuning (CPU issues). Additionaly, the random forest algorithm is perfect to avoid overfitting the data. And since we have a small number of entries, we might be overfitted on other algorithm like decision tree.","5627e34d":"<b>1. Standardization<\/b>","eae006c5":"Overfitting occurs when the gap between the training error and test error is too large. In our case, we have an overfitting problem. Even with the most basics random forest, we couldn't avoid this problem. It exist couples of solutions to get this problem out of the way:\n<ul>\n    <li>cross-validation<\/li>\n    <li>optimal hyperparameter tuning for your model<\/li>\n    <li>removing noisy (less then important) features to simplify your model<\/li>\n    <li>adding more data to your training dataset<\/li>\n    <li>appling regularization<\/li>\n    <li>building the ensemble of several models to achieve more balanced predictions<\/li>\n    <li>implementing some algorithm-specific model tweaks (for example, in deep learning, you may want to think about reducing your neural network\u2019s capacity or introducing drop-out layers etc.)<\/li>\n<\/ul>\nAfter doing a model tuning, the model wasn't overfitted anymore. The generalized error is superior to the training error and the gap is not big.","477face7":"For feature scaling and selection, I'm gonna keep the age features. Who knows, the age feature might be better than a one hot encoding. Let's see -> yes there is a better associations between the target and this feature.","93024d52":"I hope you enjoyed this notebook. Don't hesitate to advice me on some part and upvote it if you liked it.\n<br>\n<br>\n<i>Be curious.<\/i>","d9459ba5":"To tune our model, we are going to use the Grid Search method, which is basically trying all possible combinations of the parameters of interest. (\/!\\ Since the CPU can be limited for some laptop, I will only use a couple of parameters to avoid overheating the laptop's CPU)."}}