{"cell_type":{"f8f7002d":"code","5977b1cb":"code","808bfc24":"code","3455233b":"code","707290e4":"code","1c33c786":"code","ca585479":"code","109fd14f":"code","1416eb3b":"code","b4ff905b":"code","11aabb5e":"code","7ac9a39b":"code","21d3873b":"code","cea81810":"code","96ca2114":"code","cd6afb0e":"code","8104be1e":"code","0f4295fe":"code","d49b6ffe":"code","e61cc9e4":"code","89743668":"code","2865b749":"code","9c33c97b":"code","b0c2b63b":"code","aafe4314":"code","65076f39":"code","a9a0f227":"code","846fc016":"code","ab0f21c2":"code","a233fb7c":"code","7b87510b":"code","bdc8e5b2":"code","c731c955":"code","e9d3b8a0":"code","5d37d471":"code","67c0089e":"code","da765ceb":"code","95e103b3":"markdown","580b2ec0":"markdown","46ba3105":"markdown","2f0177ce":"markdown","dc917343":"markdown","1e160e83":"markdown","5132c205":"markdown","3526fb85":"markdown","c0a5c197":"markdown","632a169a":"markdown","cb172a46":"markdown","8ecb47b7":"markdown","b582626b":"markdown","86882073":"markdown","4ed3d888":"markdown","b3e3b861":"markdown","f9230149":"markdown","9c89b720":"markdown","4864e2ef":"markdown","5a5ffc9f":"markdown","4dc2941b":"markdown","2f8a7ef6":"markdown","7098cd92":"markdown","8e605412":"markdown","c7437daf":"markdown","311e7f88":"markdown","109a2d9c":"markdown","d3c09459":"markdown","181111f8":"markdown","88492200":"markdown","aa304a52":"markdown"},"source":{"f8f7002d":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import svm\n\n\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n#TRAIN\/VALIDATION\/TEST SPLIT\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\nTEST_SIZE = 0.20 # test size using_train_test_split","5977b1cb":"data_df=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","808bfc24":"print(\"Credit Card Fraud Detection- rows:\",data_df.shape[0],\"columns:\",data_df.shape[1])","3455233b":"data_df.head()","707290e4":"data_df.describe()","1c33c786":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","ca585479":"#checking for data unbalance wrt the target value i.e., class\ntemp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=data, layout=layout)\niplot(fig,filename='class')","109fd14f":"class_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\nclass_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\n\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","1416eb3b":"data_df['Hour'] = data_df['Time'].apply(lambda x: np.floor(x \/ 3600))\n\ntmp = data_df.groupby(['Hour', 'Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()\ndf = pd.DataFrame(tmp)\ndf.columns = ['Hour', 'Class', 'Min', 'Max', 'Transactions', 'Sum', 'Mean', 'Median', 'Var']\ndf.head()","b4ff905b":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\ns = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Sum\", data=df.loc[df.Class==0])\ns = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Sum\", data=df.loc[df.Class==1], color=\"red\")\nplt.suptitle(\"Total Amount\")\nplt.show()","11aabb5e":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\ns = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Transactions\", data=df.loc[df.Class==0])\ns = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Transactions\", data=df.loc[df.Class==1], color=\"red\")\nplt.suptitle(\"Total Number of Transactions\")\nplt.show()","7ac9a39b":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\ns = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==0])\ns = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==1], color=\"red\")\nplt.suptitle(\"Average Amount of Transactions\")\nplt.show()","21d3873b":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\ns = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Max\", data=df.loc[df.Class==0])\ns = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Max\", data=df.loc[df.Class==1], color=\"red\")\nplt.suptitle(\"Maximum Amount of Transactions\")\nplt.show()","cea81810":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\ns = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Min\", data=df.loc[df.Class==0])\ns = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Min\", data=df.loc[df.Class==1], color=\"red\")\nplt.suptitle(\"Minimum Amount of Transactions\")\nplt.show()","96ca2114":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\nplt.show()","cd6afb0e":"tmp = data_df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","8104be1e":"class_1.describe()","0f4295fe":"#plotting fraudulent transactions against time\nfraud = data_df.loc[data_df['Class'] == 1]\n\ntrace = go.Scatter(\n    x = fraud['Time'],y = fraud['Amount'],name=\"Amount\",marker=dict(color='rgb(238,23,11)',line=dict(color='red',width=1), opacity=0.5, ),text= fraud['Amount'],mode = \"markers\")\ndata = [trace]\nlayout = dict(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest'\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='fraud-amount')","d49b6ffe":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","e61cc9e4":"#plotting linearly correlated values i.e., V7 and V20\ns = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","89743668":"#plotting inversely correlated values i.e., V1 and V5\ns = sns.lmplot(x='V1', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","2865b749":"var = data_df.columns.values\n\ni = 0\nt0 = data_df.loc[data_df['Class'] == 0]\nt1 = data_df.loc[data_df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show()","9c33c97b":"#defining predictor and target values. In this Case, there are no categorical features\ntarget = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","b0c2b63b":"#splitting the data in train, test and validation set\nRANDOM_STATE = 2018\ntrain_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","aafe4314":"#initializing the randomforestclassifier\nclf = RandomForestClassifier(n_jobs=NO_JOBS,random_state=RANDOM_STATE,criterion=RFC_METRIC,n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","65076f39":"#train the randomforestclassifier using the train_df dataframe and fit function\nclf.fit(train_df[predictors], train_df[target].values)","a9a0f227":"#predicting the target values for the valid_df data using the predict function\npreds = clf.predict(valid_df[predictors])","846fc016":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","ab0f21c2":"#confusion matrix to show the results we obtained\ncm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, xticklabels=['Not Fraud', 'Fraud'],yticklabels=['Not Fraud', 'Fraud'],annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","a233fb7c":"roc_auc_score(valid_df[target].values, preds)","7b87510b":"import xgboost as xgb","bdc8e5b2":"# Prepare the train and validitaion datasets\ndtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(valid_df[predictors], valid_df[target].values)\ndtest = xgb.DMatrix(test_df[predictors], test_df[target].values)\n\n#What to monitor \nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","c731c955":"MAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result","e9d3b8a0":"#train the model\nmodel = xgb.train(params, dtrain,  MAX_ROUNDS,  watchlist,  early_stopping_rounds=EARLY_STOP,  maximize=True, \n                verbose_eval=VERBOSE_EVAL)","5d37d471":"fig, (ax) = plt.subplots(ncols=1, figsize=(12,8))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","67c0089e":"#predicting the target value for the test set using the trained model\npreds = model.predict(dtest)","da765ceb":"roc_auc_score(test_df[target].values, preds)","95e103b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra   \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","580b2ec0":"**Area Under The Curve**","46ba3105":"**Density Plot**","2f0177ce":"*Fraudulent transactions have a more even distribution that non-fraudulent transaction*","dc917343":"*looking at the time column we can confirm that the data contains 284807 transactions within a span of 172792 seconds i.e., 2 consecutive days*","1e160e83":"**Transactions Amount**","5132c205":"**Correlation**","3526fb85":"*The ROC-AUC score obtained with RandomForrestClassifier is 0.85.*","c0a5c197":"# XGBOOST MODEL","632a169a":"*We will run a model using the train_df for training and valid_df for validation.\nWe will use the validation criteria GINI, with the formula GINI = 2 * (AUC) - 1, where AUC is the Receiver Operating Characteristic - Area Under Curve (ROC-AUC). Number of estimators is set to 100 and number of parallel jobs is set to 4.*","cb172a46":"*We initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning.*\n","8ecb47b7":"*The best validation score obtained is 0.984*","b582626b":"*XGBOOST model has a higher accuracy rate than RANDOMFORESTCLASSIFIER for this dataset with a maximum score of 97%*","86882073":"# Read the data","4ed3d888":"**Confusion Matrix**","b3e3b861":"*Some features have good selectivity in terms of distribution of class 0 and class 1 values. V4,V11 have clearly separated distributions for the values of class 0 and class 1, V12,V14,V16,V18 have partially separated distributions. V25,V26,V28 have similar distributions for both the classes. \nIn general, with a few exceptions, the distributions for class 0 (non-fraud transactions) is centered around 0 whereas class 1(fraudulent transactions) have an asymmetric distribution*","f9230149":"*from the above output we can confirm that there are NO MISSING DATA*","9c89b720":"# Data Exploration","4864e2ef":"# RANDOMFORESTCLASSIFIER","5a5ffc9f":"# looking for missing data","4dc2941b":"# Check the data","2f8a7ef6":"*The real transaction have smaller value of mean and outliers whereas the fraudulent transactions have larger mean and outlier values*","7098cd92":"**Area Under the curve**","8e605412":"*There is no notable correlation between V1 and V28 but there are certain correlations between some of the features and time(inverse correlation with V3) and amount(linear correlation with V7 and V20, inverse correlation with V1 and V5*","c7437daf":"**Feature Importance**","311e7f88":"**Feature Importance**","109a2d9c":"# Predictive Model","d3c09459":"*only 492 transactions are fraudulent(0.172%)*","181111f8":"*we can confirm that the two pairs of features are correlated (the regression lines for Class 0 have a positive slope whereas the regression lines for Class 1 have a smaller positive slope)*","88492200":"# Checking for data unbalance","aa304a52":"*we can confirm that the two pairs of features are inversely correlated as the regression lines for Class 0 have a negative slope whereas the regression lines for Class 1 have a very small negative slope*"}}