{"cell_type":{"90859700":"code","bd47c66a":"code","b0bcc60d":"code","c50a2d45":"code","437e1be1":"code","3a76013e":"code","5dbc9af1":"code","e4ff7e02":"code","f28207b1":"code","a1b48698":"code","8eab50ca":"code","8ba585e1":"code","c9b34e01":"code","ae9bdc21":"code","9b68e692":"code","35696fc4":"code","cc938953":"code","e0a122a0":"code","54e56bd9":"code","f23a4bf8":"code","5143ddea":"code","8baf4af1":"code","938d6708":"code","bc83718a":"code","5797bd68":"code","195a3c7d":"code","9a31b8de":"code","cfdf92d8":"code","108af268":"code","2268773f":"code","9b2e8eee":"code","1a9896c2":"code","9aee7d30":"markdown","67e730bc":"markdown","5b5c141d":"markdown","511c6d04":"markdown","58dc6818":"markdown","32811300":"markdown","fbed36df":"markdown","c913dfb9":"markdown","f43e7dcd":"markdown","11615184":"markdown","3833bca1":"markdown","a876dbfb":"markdown","a347c41b":"markdown"},"source":{"90859700":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nimport plotly.figure_factory as ff\nfrom sklearn import preprocessing\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\nfrom sklearn import metrics\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\nfrom sklearn.decomposition import PCA as sklearnPCA\n#import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bd47c66a":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_train","b0bcc60d":"df_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\ndf_test","c50a2d45":"df_train.head()","437e1be1":"df_train.tail()","3a76013e":"df_train.shape","5dbc9af1":"df_train.size","e4ff7e02":"df_train.dtypes","f28207b1":"df_train.columns.values","a1b48698":"df_train.info()","8eab50ca":"df_train.describe()","8ba585e1":"df_train.skew()","c9b34e01":"df_train.corr()","ae9bdc21":"fig = make_subplots(rows=1,cols=2,\n                    subplot_titles=('Countplot',\n                                    'Percentages'),\n                    specs=[[{\"type\": \"xy\"},\n                            {'type':'domain'}]])\nfig.add_trace(go.Bar(y = df_train['target'].value_counts().values.tolist(), \n                      x = df_train['target'].value_counts().index, \n                      text=df_train['target'].value_counts().values.tolist(),\n              textfont=dict(size=15),\n                      name = 'Target',\n                      textposition = 'outside',\n                      showlegend=False,\n              marker = dict(color = 'cornflowerblue',\n                            line_color = 'black',\n                            line_width=3)),row = 1,col = 1)\nfig.add_trace((go.Pie(labels=df_train['target'].value_counts().keys(),\n                             values=df_train['target'].value_counts().values,textfont = dict(size = 16),\n                     textposition='auto',\n                     showlegend = True,\n                     name = 'Target')), row = 1, col = 2)\nfig.update_layout(title={'text': 'Type',\n                         'y':0.9,\n                         'x':0.5,\n                         'xanchor': 'center',\n                         'yanchor': 'top'},\n                  template='plotly_white')\nfig.update_yaxes(range=[0,325000])\niplot(fig)","9b68e692":"! pip install autoviz","35696fc4":"! pip install xlrd","cc938953":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\n\ntrain = df_train\n\nfilename = \"\"\nsep = \",\"\ndft = AV.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=\"target\",\n    dfte=train,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=150000,\n    max_cols_analyzed=30,\n)","e0a122a0":"scalar = StandardScaler()\n\nX=df_train.drop('target',axis=1)\ny=df_train['target']\n\nX_train_v, X_test, y_train_v, y_test = train_test_split(X, y, \n                                                    test_size=0.3, random_state=42)\nX_train, X_validate, y_train, y_validate = train_test_split(X_train_v, y_train_v, \n                                                            test_size=0.2, random_state=42)\n\nX_train = scalar.fit_transform(X_train)\nX_validate = scalar.transform(X_validate)\nX_test = scalar.transform(X_test)\n\nw_p = y_train.value_counts()[0] \/ len(y_train)\nw_n = y_train.value_counts()[1] \/ len(y_train)\n\nprint(f\"Machine Failure weight: {w_n}\")\nprint(f\"Non-Machine Failure weight: {w_p}\")","54e56bd9":"print(f\"TRAINING: X_train: {X_train.shape}, y_train: {y_train.shape}\\n{'_'*55}\")\nprint(f\"VALIDATION: X_validate: {X_validate.shape}, y_validate: {y_validate.shape}\\n{'_'*50}\")\nprint(f\"TESTING: X_test: {X_test.shape}, y_test: {y_test.shape}\")","f23a4bf8":"def print_score(label, prediction, train=True):\n    if train:\n        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n    elif train==False:\n        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\") ","5143ddea":"model = keras.Sequential([\n    keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[-1],)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(1, activation='sigmoid'),\n])\n\nmodel.summary()","8baf4af1":"METRICS = [\n#     keras.metrics.Accuracy(name='accuracy'),\n    keras.metrics.FalseNegatives(name='fn'),\n    keras.metrics.FalsePositives(name='fp'),\n    keras.metrics.TrueNegatives(name='tn'),\n    keras.metrics.TruePositives(name='tp'),\n    keras.metrics.Precision(name='precision'),\n    keras.metrics.Recall(name='recall')\n]\n\nmodel.compile(optimizer=keras.optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=METRICS)\n\ncallbacks = [keras.callbacks.ModelCheckpoint('fraud_model_at_epoch_{epoch}.h5')]\nclass_weight = {0:w_p, 1:w_n}\n\nr = model.fit(\n    X_train, y_train, \n    validation_data=(X_validate, y_validate),\n    batch_size=4048, \n    epochs=100, \n#     class_weight=class_weight,\n    callbacks=callbacks,\n)","938d6708":"y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nprint_score(y_train, y_train_pred.round(), train=True)\nprint_score(y_test, y_test_pred.round(), train=False)\n\nscores_dict = {\n    'ANNs': {\n        'Train': f1_score(y_train, y_train_pred.round()),\n        'Test': f1_score(y_test, y_test_pred.round()),\n    },\n}","bc83718a":"score = model.evaluate(X_test, y_test)\nprint(score)","5797bd68":"plt.figure(figsize=(12, 16))\n\nplt.subplot(4, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='val_Loss')\nplt.title('Loss Function evolution during training')\nplt.legend()\n\nplt.subplot(4, 2, 2)\nplt.plot(r.history['fn'], label='fn')\nplt.plot(r.history['val_fn'], label='val_fn')\nplt.title('Accuracy evolution during training')\nplt.legend()\n\nplt.subplot(4, 2, 3)\nplt.plot(r.history['precision'], label='precision')\nplt.plot(r.history['val_precision'], label='val_precision')\nplt.title('Precision evolution during training')\nplt.legend()\n\nplt.subplot(4, 2, 4)\nplt.plot(r.history['recall'], label='recall')\nplt.plot(r.history['val_recall'], label='val_recall')\nplt.title('Recall evolution during training')\nplt.legend()","195a3c7d":"Y_new_test = scalar.fit_transform(df_test)","9a31b8de":"y_test = model.predict(Y_new_test)","cfdf92d8":"y_test","108af268":"a = []\nfor i in y_test:\n    for j in i:\n        a.append(j)","2268773f":"b = df_test['id'].tolist()","9b2e8eee":"data = {'id':b, 'target':a}\nsubmission_df = pd.DataFrame(data)","1a9896c2":"submission_df","9aee7d30":"> # Feature Selection and Feature Scaling","67e730bc":"## Anatomy of a neural network:\nAs you saw in the previous chapters, training a neural network revolves around the following objects:\n\nLayers, which are combined into a network (or model) The input data and corresponding targets The loss function, which defines the feedback signal used for learning The optimizer, which determines how learning proceeds\n\n## What are activation functions, and why are they necessary?\nWithout an activation function like relu (also called a non-linearity), the Dense layer would consist of two linear operations\u2014a dot product and an addition:\noutput = dot(W, input) + b\n\nSo the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space.\n\n## Loss functions and optimizers:\nkeys to configuring the learning process Once the network architecture is defined, you still have to choose two more things: Loss function (objective function) - The quantity that will be minimized during training. It represents a measure of success for the task at hand. Optimizer - Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).","5b5c141d":"Optimizer is chosen as adam for gradient descent.\n\nBinary_crossentropy is the loss function used.\n\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.","511c6d04":"> # Exploratory Data Analysis of train data","58dc6818":"> # Data Visualization","32811300":"> # Loading Test and Train Datset","fbed36df":"#### Target column is balanced and 0 represents non spam emails 1 represent spam emails","c913dfb9":"The ReLU function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like. One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations. Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is has its own problem, called \"dead neurons,\" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU etc.) can minimize this.","f43e7dcd":"> # Prediction on test data","11615184":"# Autoviz","3833bca1":"> # Importing Libraries","a876dbfb":"## What are artificial neural networks?\nAn artificial neuron network (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network changes - or learns, in a sense - based on that input and output. ANNs are considered nonlinear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. ANN is also known as a neural network.\n\nA single neuron is known as a perceptron. It consists of a layer of inputs(corresponds to columns of a dataframe). Each input has a weight which controls the magnitude of an input. The summation of the products of these input values and weights is fed to the activation function. Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.\n\nThey introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack. Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n\nConcept of backpropagation - Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.\n\nGradient Descent - To explain Gradient Descent I\u2019ll use the classic mountaineering example. Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake? The best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake.","a347c41b":"# Prediction using ANN"}}