{"cell_type":{"f969e44c":"code","f27ac26b":"code","e9cde2af":"code","00e7fc3a":"code","8ea08749":"code","8ec3d482":"code","6770c10a":"code","6f2370a6":"code","46ecf3bb":"code","3bdb5314":"code","eade5adc":"code","84d54b5e":"code","4a7f0835":"code","336e4c20":"code","99a849d3":"code","ccb0e0e7":"code","36a62ff4":"code","645cf1b4":"code","0c11025e":"code","60e32389":"code","880eb50b":"code","0c920dcb":"code","025b4bb3":"code","d7ae1655":"code","0bc86b08":"code","942100cb":"code","ebe3baaf":"code","32b298d8":"code","98b043fd":"code","31a59523":"code","9b5b11e4":"code","2dd1261d":"markdown","78e9332b":"markdown","0ec0c33b":"markdown","0e3ffbdf":"markdown","cd182142":"markdown","983bc2f7":"markdown","20c8b5bb":"markdown","b95187a9":"markdown","29d00316":"markdown","678a6f60":"markdown","ee2283d8":"markdown","21c6d1ea":"markdown","f279cd8f":"markdown","91d3bca6":"markdown","ec03ae8f":"markdown","9df63372":"markdown","62e3d3c4":"markdown","8e63c1fd":"markdown"},"source":{"f969e44c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","f27ac26b":"df = pd.read_csv(\"..\/input\/banknote-authenticationcsv\/BankNote_Authentication.csv\")\ndf.head()","e9cde2af":"df.shape","00e7fc3a":"df['class'].value_counts()","8ea08749":"df.describe()","8ec3d482":"df.info()","6770c10a":"df.isnull().sum()","6f2370a6":"sns.pairplot(data=df)","46ecf3bb":"sns.distplot(df['variance'])","3bdb5314":"sns.distplot(df['skewness'])","eade5adc":"sns.distplot(df['curtosis'])","84d54b5e":"sns.distplot(df['entropy'])","4a7f0835":"sns.boxplot(y=df['variance'])","336e4c20":"sns.boxplot(y=df['skewness'])","99a849d3":"sns.boxplot(y=df['curtosis'])","ccb0e0e7":"sns.boxplot(y=df['entropy'])","36a62ff4":"X = df.drop('class',axis=1)\ny = df['class']","645cf1b4":"X.shape[1]","0c11025e":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()","60e32389":"X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)","880eb50b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33)","0c920dcb":"y_test","025b4bb3":"import tensorflow as tf","d7ae1655":"n_features = X.shape[1]\nmodel = tf.keras.Sequential([\ntf.keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])","0bc86b08":"model.summary()","942100cb":"model.compile(optimizer='adam', loss='binary_crossentropy')","ebe3baaf":"history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_test,y_test))","32b298d8":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n# predict test set\nyhat = model.predict_classes(X_test)\n# evaluate predictions\nscore = accuracy_score(y_test, yhat)\nprint('Accuracy: %.3f' % score)","98b043fd":"# plot learning curves\nplt.title('Learning Curves')\nplt.xlabel('Epoch')\nplt.ylabel('Cross Entropy')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='val')\nplt.legend()\nplt.show()","31a59523":"print(confusion_matrix(y_test,yhat))","9b5b11e4":"print(classification_report(y_test,yhat))","2dd1261d":"### Neural Network Learning Dynamics\n### I have used a Multilayer Perceptron (MLP) model for the dataset using TensorFlow.\n\n### Given that the dataset is small, a small batch size is probably a good idea, e.g. 16 or 32 rows. Using the Adam version of stochastic gradient descent is a good idea when getting started as it will automatically adapt the learning rate and works well on most datasets.\n\n### Before evaluate models in earnest, it is a good idea to review the learning dynamics and tune the model architecture and learning configuration until we have stable learning dynamics, then look at getting the most out of the model.\n\n### We can do this by using a simple train\/test split of the data and review plots of the learning curves. This will help us see if we are over-learning or under-learning; then we can adapt the configuration accordingly.\n\n### First, we must ensure all input variables are floating-point values and encode the target label as integer values 0 and 1.","78e9332b":"**Running the dataset first loads the data before and then prints summary statistics for each variable.\nThe values vary with different means and standard deviations, perhaps some normalization or standardization would be required prior to modeling.**","0ec0c33b":"#### At the end of training, evaluated the model\u2019s performance on the test dataset and report performance as the classification accuracy.","0e3ffbdf":"## loadind the Data","cd182142":"## Checking For Data is Imbalanced Or Not","983bc2f7":"## importing Libraries","20c8b5bb":"# Looking for Outliers","b95187a9":"####  fit the model for 50 training epochs (chosen arbitrarily) with a batch size of 32 because it is a small dataset.\n\n#### As fitting the model on raw data, which might be a good idea, but it is an important starting point.","29d00316":"#### Used a minimal MLP model. In this case,  used one hidden layer with 10 nodes and one output layer (chosen arbitrarily) and also the ReLU activation function in the hidden layer and the \u201che_normal\u201d weight initialization, as together, they are a good practice.\n\n#### The output of the model is a sigmoid activation for binary classification and we will minimize binary cross-entropy loss","678a6f60":"## Model Train Using Neural Network","ee2283d8":"## Exploratoratory Data Analysis","21c6d1ea":"## Finally, plotted learning curves of the cross-entropy loss on the train and test sets during training.","f279cd8f":"## Looking Datatypes of the columns","91d3bca6":"## Looking For Null Values","ec03ae8f":"#### As I can see that while pairplot shows all the histogram in which the first two variables have a Gaussian-like distribution and the next two input variables may have a skewed Gaussian distribution or an exponential distribution. \n#### That have some benefit in using a power transform on each variable in order to make the probability distribution less skewed which will likely improve model performance","9df63372":"## Splitting the Data into X,y","62e3d3c4":"## Statistical Information","8e63c1fd":"## Splitting the data into train ,test split"}}