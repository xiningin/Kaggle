{"cell_type":{"d497f9ca":"code","c4909f18":"code","263b8892":"code","fc8e242c":"code","89f7b101":"code","8c143f03":"code","89f52c0e":"code","0d58ec7b":"code","8e80ac8f":"code","2b8d73f8":"code","896c7f7e":"code","e894b71d":"code","254125d3":"code","f834b3bc":"code","053cf306":"code","f62f10d1":"code","7a770f1d":"code","12ba9a8e":"markdown","3397b4fb":"markdown","ab1cae97":"markdown","a37b7617":"markdown","21b8222f":"markdown","9f0a6c1c":"markdown","80c242bc":"markdown"},"source":{"d497f9ca":"!pip install ktrain","c4909f18":"!git clone https:\/\/github.com\/laxmimerit\/IMDB-Movie-Reviews-Large-Dataset-50k.git","263b8892":"# \/content\/IMDB-Movie-Reviews-Large-Dataset-50k","fc8e242c":"import pandas as pd\nimport numpy as np\nimport ktrain\nfrom ktrain import text\nimport tensorflow as tf","89f7b101":"data_test = pd.read_excel('.\/IMDB-Movie-Reviews-Large-Dataset-50k\/train.xlsx', dtype= str)\ndata_train = pd.read_excel('.\/IMDB-Movie-Reviews-Large-Dataset-50k\/test.xlsx', dtype = str)","8c143f03":"data_train.sample(5)","89f52c0e":"text.print_text_classifiers()","0d58ec7b":"(train, val, preproc) = text.texts_from_df(train_df=data_train, text_column='Reviews', label_columns='Sentiment',\n                   val_df = data_test,\n                   maxlen = 400,\n                   preprocess_mode = 'distilbert')","8e80ac8f":"model = text.text_classifier(name = 'distilbert', train_data = train, preproc=preproc)","2b8d73f8":"learner = ktrain.get_learner(model = model,\n                             train_data = train,\n                             val_data = val,\n                             batch_size = 6)","896c7f7e":"learner.fit_onecycle(lr = 2e-5, epochs=2)","e894b71d":"predictor = ktrain.get_predictor(learner.model, preproc)","254125d3":"predictor.save('.\/')","f834b3bc":"data = ['this movie was really bad. acting was also bad. I will not watch again',\n        'the movie was really great. I will see it again', 'another great movie. must watch to everyone']","053cf306":"predictor.predict(data)","f62f10d1":"predictor.get_classes()","7a770f1d":"predictor.predict(data, return_proba=True)","12ba9a8e":"# Notebook Setup","3397b4fb":"DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bert\u2019s performances as measured on the GLUE language understanding benchmark.","ab1cae97":"BERT is designed to pretrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial taskspecific architecture modifications.","a37b7617":"# What is `DistilBERT`","21b8222f":"# Sentiment Classification Using DistilBERT ","9f0a6c1c":"# What is `ktrain`","80c242bc":"ktrain is a library to help build, train, debug, and deploy neural networks in the deep learning software framework, Keras."}}