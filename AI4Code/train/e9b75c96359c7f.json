{"cell_type":{"7a49a9ab":"code","7b9ff164":"code","46867957":"code","ab41de83":"code","2b4d7a5c":"code","02b1c367":"code","54680242":"code","0e4a0d16":"code","66236b9f":"code","573b8028":"code","0c1eda45":"code","ddd44e6d":"code","becefd45":"code","5aae85e7":"code","0bac2ec0":"code","f99001ec":"code","3d9df08c":"code","3ee5ba4e":"code","723969c6":"code","2874a530":"code","5f1cecca":"code","8a62820e":"code","e099a045":"code","36bf35e7":"code","752e6e43":"code","cb0a5cc2":"code","2aeacd95":"code","42153b68":"code","1fd959a7":"code","78333625":"markdown","9e5567f1":"markdown","1c6a080f":"markdown","00c6eca3":"markdown","6ecb639e":"markdown","eb4f6fab":"markdown","16ba3d75":"markdown","ca9cc952":"markdown","7a71823d":"markdown","e0da1f3a":"markdown","841dbe74":"markdown","4807ac11":"markdown","28f35ba8":"markdown","d9f4014a":"markdown","6a5e1a55":"markdown","16e67a58":"markdown","9582984d":"markdown","c633cf6f":"markdown","00b8b640":"markdown","4d07a7ac":"markdown","fe5c1f79":"markdown","3d151938":"markdown","34f2b0ee":"markdown","cb9f82ed":"markdown","5790ec3e":"markdown","d23d8abf":"markdown","63df4894":"markdown","76551d2c":"markdown"},"source":{"7a49a9ab":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf \nfrom tensorflow import feature_column","7b9ff164":"data = pd.read_csv(\"\/kaggle\/input\/housesalesprediction\/kc_house_data.csv\")","46867957":"data.head()","ab41de83":"data.describe().transpose()","2b4d7a5c":"data.corr()","02b1c367":"data.corr()[\"price\"].sort_values(ascending=False)","54680242":"data.info()","0e4a0d16":"data[\"year\"] = data[\"date\"].apply(lambda date: int(date[0:4]))","66236b9f":"data[\"years_since_built\"] = data[\"year\"] - data[\"yr_built\"]\ndata[\"years_since_renovated\"] = data[\"year\"] - data[\"yr_renovated\"]","573b8028":"unnecessary_column_names = [\"id\", \"date\", \"zipcode\", \"yr_built\", \"yr_renovated\", \"year\"]\nfor column_name in unnecessary_column_names:\n    data.pop(column_name)","0c1eda45":"data.describe().transpose()","ddd44e6d":"data.corr()[\"price\"].sort_values(ascending=False)","becefd45":"numerical_colunmn_names = [\n    'bedrooms',\n    'bathrooms',\n    'sqft_living',\n    'sqft_lot',\n    'floors',\n    'sqft_above',\n    \"sqft_basement\",\n    \"sqft_living15\",\n    \"sqft_lot15\",\n    \"years_since_built\",\n    \"years_since_renovated\",\n    \"long\",\n    \"lat\"\n]\nnumerical_colunmns = [feature_column.numeric_column(name, dtype=float) for name in numerical_colunmn_names]","5aae85e7":"for column in numerical_colunmn_names:\n    data[column] = data[column].astype(float)","0bac2ec0":"categorical_column_names = [\"waterfront\", \"condition\", \"grade\", \"view\"]\ncategorical_column_lists = [sorted(data[item].unique()) for item in categorical_column_names]\ncategorical_columns = [feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(name, category)) for (name,category) in zip(categorical_column_names, categorical_column_lists)]","f99001ec":"min_lat, max_lat = data[\"lat\"].min(), data[\"lat\"].max()\nmin_long, max_long = data[\"long\"].min(), data[\"long\"].max()\nprint(min_lat, max_lat, min_long, min_long)\nnum_buckets = 8\nlatbuckets = np.linspace(start=min_lat, stop=max_lat, num=num_buckets).tolist()\nlonbuckets = np.linspace(start=min_long, stop=max_long, num=num_buckets).tolist()\nprint(latbuckets, lonbuckets)\nlat_column = feature_column.bucketized_column(\n    source_column=feature_column.numeric_column(\"lat\"), boundaries=latbuckets)\nlong_column = feature_column.bucketized_column(\n    source_column=feature_column.numeric_column(\"long\"), boundaries=lonbuckets)\nlocation_column = feature_column.crossed_column(\n    [lat_column, long_column], \n    hash_bucket_size=num_buckets * num_buckets\n)\nlocation_embedding_column = feature_column.embedding_column(categorical_column=location_column, dimension=3)","3d9df08c":"wide_columns = [\n    feature_column.indicator_column(location_column)\n] + categorical_columns\n\ndeep_columns = [location_embedding_column] + numerical_colunmns","3ee5ba4e":"inputs = dict()\nfor item in numerical_colunmns:\n    inputs[item.key] = tf.keras.layers.Input(name=item.key, shape=(), dtype=\"float32\")\nfor item in categorical_columns:\n    inputs[item.categorical_column.key] = tf.keras.layers.Input(name=item.categorical_column.key, shape=(), dtype=\"int32\")","723969c6":"inputs","2874a530":"from sklearn.model_selection import train_test_split\ndata_train, data_test = train_test_split(data, test_size=0.2, random_state=997)\ndata_train.to_csv(\"data_train.csv\",index=False)\ndata_test.to_csv(\"data_test.csv\",index=False)","5f1cecca":"def features_and_labels(row_data):\n    label = row_data.pop(\"price\")\n    features = row_data\n    return features, label\n\ndef create_dataset(pattern, epochs=1, batch_size=32, mode='eval'):\n    dataset = tf.data.experimental.make_csv_dataset(\n        pattern, batch_size\n    )\n    dataset = dataset.map(features_and_labels)\n    if mode == 'train':\n        dataset = dataset.shuffle(buffer_size=1000).repeat(epochs)\n    dataset = dataset.prefetch(1)\n    return dataset","8a62820e":"batch_size = 100\ntrain_data = create_dataset(\"data_train.csv\", batch_size=batch_size, mode='train')\ntest_data = create_dataset(\"data_test.csv\", batch_size=batch_size, mode='eval').take(data_test.shape[0] \/\/ batch_size)","e099a045":"def build_model():\n    deep = tf.keras.layers.DenseFeatures(deep_columns, name='deep_inputs')(inputs)\n    deep = tf.keras.layers.Dense(32, activation='relu')(deep)\n    deep = tf.keras.layers.Dense(32, activation='relu')(deep)\n    deep = tf.keras.layers.Dense(32, activation='relu')(deep)\n    wide = tf.keras.layers.DenseFeatures(wide_columns, name='wide_inputs')(inputs)\n    wide = tf.keras.layers.Dense(64, activation='relu')(wide)\n    combined = tf.keras.layers.concatenate(inputs=[deep, wide], name='combined')\n    output = tf.keras.layers.Dense(1)(combined)\n    model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n    model.compile(optimizer=\"adam\", loss=\"mape\", metrics=[\"mse\", \"mae\", \"mape\"])\n    return model","36bf35e7":"model = build_model()","752e6e43":"tf.keras.utils.plot_model(model, show_shapes=False, rankdir='LR')","cb0a5cc2":"epochs = 400\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10)\nsteps_per_epoch = data_train.shape[0] \/\/ batch_size\nhistory = model.fit(\n    train_data, \n    steps_per_epoch=steps_per_epoch,\n    validation_data=test_data,\n    epochs=epochs,\n    callbacks=[early_stop],\n    verbose=2\n)","2aeacd95":"pd.DataFrame(history.history, columns=[\"loss\", \"val_loss\"]).plot()","42153b68":"pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()","1fd959a7":"pd.DataFrame(history.history, columns=[\"mape\", \"val_mape\"]).plot()","78333625":"**Plot the Model**","9e5567f1":"**Compute Correlation score**\n\nLet's Compute pairwise correlation of columns and see what's the most correlated features of price feature.","1c6a080f":"## Conclusion \n- The MAPE score of this Model is about 17%, it means that mean error this Model predicts are 17% of the acutal house prices.\n- The MAE score of this Model is about 106086.4609, which means that mean error this model predicts are 106086 dollars, which is still a sinificatn amount.\n- MAP \/ MAPE \/ MSE curves of this Model are very similar, so this Model does not overfit.\n- The most important features that can impact a house's prices are: Square footage of the home, overall grade given to the housing unit, Square footage of house apart from basement, Living room area in 2015, Number of bathrooms, whether it has been viewed, Square footage of the basement, Number of bedrooms, Latitude coordinate, whether house has a view to a waterfront.","00c6eca3":"### Create TensorFlow Feature Columns","6ecb639e":"Create categorical columns:","eb4f6fab":"Create a crossed column about location combined with latitude and longitude:","16ba3d75":"**Train test split**","ca9cc952":"Let's train the Model for 400 epochs. Add an EarlyStopping layer so that it will stop after the Model stop imporving.","7a71823d":"## Exploratory Data Analysis","e0da1f3a":"## Model Evaluation","841dbe74":"Let's see type of different features. As we can see date column is object type, so Id and zipcode doesn't have relation to price of house, so we will remove these fields. \nMost of the feature are numeriacal features. However, we need to be noticed of following:\n- Date can indicate year, month, day information. We should cacluate how old the house is combined with yr_built and how many years since renovated combined with yr_renovated.\n- Id and zipcode is not corelated with price so we won't use them to predict house prices. \n- View, waterfront, condition, grade seems like a quantity but it's better to be treated as a category.\n- lat and long column is quantity, at the same time combining them can get a location information.\n","4807ac11":"## Model Development\nCreate a wide and deep Model using 2 DenseFeatures layers. One is deep layer to fit numerical data, another is wide layer to fit sparse and categorical data.","28f35ba8":"**Caculate how long has it been since houses were built and renovated**","d9f4014a":"Now show first 5 rows and statistics infomation:","6a5e1a55":"Create numerical columns:","16e67a58":"Show statistics info:","9582984d":"**Mean Average Percentage Error over time**","c633cf6f":"**Extract year information from date column**","00b8b640":"## Table of Contents\n- Overview\n- Import Packages\n- Import Datasets\n- Exploratory Data Analysis\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Conclusion","4d07a7ac":"**Mean Average Error over time**\n\nIt means that Mean Average Error of house prices this Model predict is about 100000 dollars.","fe5c1f79":"Let's calcuate correlation scores with price again:","3d151938":"**Create TensorFlow Dataset**","34f2b0ee":"## Data Preprocessing\nWe need to preprocess datasets in following ways:\n- Extract year information from date column\n- Caculate how long has it been since houses were built and renovated \n- Remove unnecessary columns\n- Create TensorFlow Feature Columns for Modeling\n- Train test split","cb9f82ed":"**Loss (Mean Squared Error) over time**","5790ec3e":"## Import Datasets","d23d8abf":"## Overview\nIn this notebook I will use dataset House Sales in King County, USA to build a House Price Predictor. First I will import packages and import datasets, then I will do Exploratory Data Analysis and Data Preprocessing base on it, later I will build a deep and wide Model using TensorFlow Feature Columns and DenseFeatures, then I will train this Model, finally I will evaluate this Model.","63df4894":"## Import Packages","76551d2c":"**Remove unnecessary columns**"}}