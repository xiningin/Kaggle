{"cell_type":{"c289a709":"code","8975ad52":"code","eb0e51f0":"code","9609915a":"code","0ca55185":"code","6f16a51b":"code","ffe919d2":"markdown","bfe0e135":"markdown"},"source":{"c289a709":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\nimport time","8975ad52":"df = pd.read_csv('..\/input\/30daysmltrain-folds-10\/train_folds_10.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"..\/input\/30daysml10foldblend\/train_preds_1.csv\")\ndf2 = pd.read_csv(\"..\/input\/30daysml10foldblend\/train_preds_2.csv\")\ndf3 = pd.read_csv(\"..\/input\/30daysml10foldblend\/train_preds_3.csv\")\n\ndf_test1 = pd.read_csv(\"..\/input\/30daysml10foldblend\/test_preds_1.csv\")\ndf_test2 = pd.read_csv(\"..\/input\/30daysml10foldblend\/test_preds_2.csv\")\ndf_test3 = pd.read_csv(\"..\/input\/30daysml10foldblend\/test_preds_3.csv\")\n\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how = \"left\")\n\ndf.head()","eb0e51f0":"df_test.rename(columns={'pred_2':'preds_2'},inplace=True)\ndf_test.head()","9609915a":"parameters={'max_depth': 4, \n             'learning_rate': 0.0417953843318061,\n             'min_child_weight': 298, \n             'gamma': 0.0008903842250630355, \n             'alpha': 0.0008139864374244503, \n             'lambda': 0.00020632373116164646, \n             'colsample_bytree': 0.6866726034515919,\n             'subsample': 0.216039278556118,\n             'n_jobs':4,\n             'random_state':42\n            }","0ca55185":"final_predictions = []\nscores = []\nuseful_features = [\"preds_1\",\"preds_2\",\"preds_3\"]\ndf_test = df_test[useful_features]\nfor fold in range(10):\n    start = time.time()\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model =  XGBRegressor(**parameters,n_estimators=10000)\n    model.fit(xtrain, ytrain, eval_set = [(xvalid,yvalid)], early_stopping_rounds = 222, verbose = False)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    \n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse, time.time()-start)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\n","6f16a51b":"sample_submission.target = np.mean(np.column_stack(final_predictions),axis=1)\nsample_submission.to_csv('submission_blend.csv',index=False)","ffe919d2":"useful_features = [\"preds_1\",\"preds_2\",\"preds_3\"]\ndf_test = df_test[useful_features]\ndef run(trial):\n    fold = 0\n    param ={\n        'max_depth': trial.suggest_categorical('max_depth', [1,2,3,4,5,6,7]),\n        'learning_rate':trial.suggest_float(\"learning_rate\", 1e-3, 0.25, log=True),\n        #'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'booster': 'gbtree',\n        'tree_method': 'gpu_hist',\n        'random_state': 42,\n    }\n    \n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n\n    xgb_model = XGBRegressor(**param,n_estimators=10000)\n    xgb_model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = xgb_model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse","bfe0e135":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=500)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)"}}