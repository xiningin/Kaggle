{"cell_type":{"9adf39e2":"code","d93953ad":"code","6182b865":"code","d9b9200d":"code","399f0718":"code","e4e6e8c1":"code","a0523195":"code","a9c12929":"code","0e071850":"code","4c529939":"code","9bb4892a":"code","ed4ac8d2":"code","b6914344":"code","48722367":"code","49fc5738":"code","4cc1adbd":"code","d9687327":"code","236430c4":"code","d5160aab":"code","609bd09e":"code","0c6a01b8":"code","a0670edc":"code","daa5759c":"code","c82e02d1":"code","cb6163c2":"code","319a04cc":"code","cd758014":"code","287d4d3a":"code","2aadeada":"code","e8c6f2e9":"markdown","d9aa7ed9":"markdown","819cb014":"markdown","5ebf6212":"markdown","b45c3dea":"markdown","00eaf445":"markdown"},"source":{"9adf39e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d93953ad":"import pandas as pd\nimport numpy as np","6182b865":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndf.head()","d9b9200d":"df.info()","399f0718":"df = df.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'] , axis=1)","e4e6e8c1":"df.info()","a0523195":"import matplotlib.pyplot as plt\ndf.Attrition_Flag.value_counts().plot(kind='bar')\nplt.title('Attrition Flag', '')\nplt.show()","a9c12929":"for var in ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']: \n    df[var].value_counts().plot(kind='bar')\n    plt.title(var)\n    plt.show()","0e071850":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nlabels = ['Attrition_Flag', 'Gender', 'Income_Category']\ncategories = ['Education_Level', 'Marital_Status', 'Card_Category']\n\nle = LabelEncoder()\nohe = OneHotEncoder(sparse=False)","4c529939":"df[labels]=df[labels].apply(lambda x: le.fit_transform(x))","9bb4892a":"for cat in categories:\n    cat_features= pd.DataFrame(ohe.fit_transform(df[[cat]]), columns=list([df[cat].unique()]))\n    df = pd.concat([df, cat_features], axis=1)","ed4ac8d2":"df.head()","b6914344":"df.info()","48722367":"df1=df.iloc[:, :24]\ndf2=df.iloc[:, 25:30]\ndf3=df.iloc[:, 31:]\ndf_final = pd.concat([df1, df2, df3], axis=1)","49fc5738":"df_final.columns","4cc1adbd":"X = df_final.drop(columns=categories).drop(columns=['Attrition_Flag']).drop(columns=['CLIENTNUM'])\ny = df_final['Attrition_Flag']\nX.info()\ny.shape","d9687327":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=31)","236430c4":"#Define scoring metrics: Precision on class 0\nfrom sklearn.metrics import recall_score, accuracy_score, make_scorer\ncustom_recall = make_scorer(recall_score, pos_label=0)","d5160aab":"#Build XGB model\nfrom xgboost import XGBClassifier\n","609bd09e":"Test_score = []\nTrain_score = []\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\nfor n_est in n_estimators:\n    xgb = XGBClassifier(objective='binary:logistic', learning_rate=0.1, n_estimators=n_est)\n    xgb.fit(X_train, y_train)\n    y_train_pred = xgb.predict(X_train)\n    Train_score.append(recall_score(y_train, y_train_pred, pos_label=0))\n    y_test_pred = xgb.predict(X_test)\n    Test_score.append(recall_score(y_test, y_test_pred, pos_label=0))","0c6a01b8":"fig, ax = plt.subplots(figsize=(12,8))\nax.plot(Train_score)\nax.plot(Test_score, c = 'red')\nax.set_xticklabels(n_estimators)\nax.set_ylim(0.8, 1.1)\nplt.show()","a0670edc":"Test_score = []\nTrain_score = []\nlearning_rate = list(np.linspace(0.05, 0.25, 10))\n\nfor lr in learning_rate:\n    xgb = XGBClassifier(objective='binary:logistic', learning_rate=lr, n_estimators=400)\n    xgb.fit(X_train, y_train)\n    y_train_pred = xgb.predict(X_train)\n    Train_score.append(recall_score(y_train, y_train_pred, pos_label=0))\n    y_test_pred = xgb.predict(X_test)\n    Test_score.append(recall_score(y_test, y_test_pred, pos_label=0))\n    \nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(Train_score)\nax.plot(Test_score, c = 'red')\nax.set_xticklabels(learning_rate)\nax.set_ylim(0.8, 1.1)\nplt.show()","daa5759c":"xgb = XGBClassifier(objective='binary:logistic', learning_rate=0.1225, n_estimators=400)","c82e02d1":"#Build ParamsGrid\nparams = {\n    'max_depth': [2, 3, 4],\n    'subsample': [0.5, 0.65, 0.8],\n    'colsample_bytree': [0.1, 0.4, 0.7],\n    'lambda' : [0.3, 0.5, 0.7, 0.9]\n}","cb6163c2":"#RandomSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomizedSearchCV(estimator=xgb, param_distributions=params, cv=5, scoring=custom_recall, return_train_score = True)\nclf.fit(X_train, y_train)","319a04cc":"clf.best_params_","cd758014":"clf.cv_results_","287d4d3a":"y_test_pred = clf.best_estimator_.predict(X_test)\nrecall_score(y_test, y_test_pred, pos_label=0)","2aadeada":"accuracy_score(y_test, y_test_pred)","e8c6f2e9":"# Evaluation","d9aa7ed9":"# Split data","819cb014":"# Parameters tuning","5ebf6212":"# Preprocessing categorical data","b45c3dea":"### Build model","00eaf445":"Transform other categorical variables"}}