{"cell_type":{"66b600e7":"code","5812fe58":"code","8e09b47d":"code","a4f5a847":"code","e64770a5":"code","5903c74c":"code","885f36a9":"code","ddd3d893":"code","d7966333":"code","ab6eb699":"code","d67c2f41":"code","43819aca":"code","075ae431":"code","bc87d39a":"code","e54cae4e":"code","5d279655":"code","4039d113":"code","dce614b1":"code","2ca8e31c":"code","bf4099da":"code","b618153d":"code","841921b6":"code","2cabf5b8":"code","a230c76e":"code","6d9dd359":"code","4f92c2d2":"code","ad76704b":"code","065fb402":"code","5a4c0c54":"code","50016e0d":"code","fb41e14b":"code","610746f4":"code","42d04f14":"code","37de07e3":"code","305861d1":"code","dd28152a":"code","1c746a8c":"code","becffb45":"code","ce482ee7":"code","d4b69ebb":"code","fefe665f":"code","1092d53f":"code","fc3bb05c":"code","ae2b772e":"code","3b15a7ec":"markdown","0961594c":"markdown","32bd9f0c":"markdown","dc0bbad2":"markdown","37b02836":"markdown","e5381a7d":"markdown","0fca7207":"markdown","2a7fd4ca":"markdown","bb66e3c3":"markdown","770bd7ea":"markdown","ba67220e":"markdown","2e4988ee":"markdown","c70e0806":"markdown","ffb4a774":"markdown","3314aa08":"markdown","cde646bb":"markdown","a64fa26a":"markdown","b8473ecf":"markdown","94c8b51d":"markdown","e4011d33":"markdown","d683832a":"markdown","2178c9e1":"markdown","9886ee16":"markdown","06664cbb":"markdown","0cb38ff9":"markdown","91cf0866":"markdown","0de55f58":"markdown","f4e3bf0a":"markdown","ece5af9b":"markdown","ac82f444":"markdown","cacff7a3":"markdown","ba1fcc71":"markdown","6bc71225":"markdown","f51a4b6b":"markdown","8811e45d":"markdown","3c5d672c":"markdown","4d6a25ae":"markdown","41f9bc54":"markdown","8e4ae94a":"markdown","6a9080bc":"markdown","177acf93":"markdown","5a9d00c6":"markdown"},"source":{"66b600e7":"import torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\nimport time\nfrom tqdm import tqdm\nimport torchvision","5812fe58":"img_size=64\nbatch_size=16\nworkers=2\nngpu=1\nnum_epochs=40\nNUM_CLASSES=3\nsave_epochs=2\n\n#Define the devide to be used\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","8e09b47d":"#Create the dataset\nroot_path=\"..\/input\/panneauxv3\/00_train\"\n\ntrain_dataset = dset.ImageFolder(root=root_path,\n                           transform=transforms.Compose([\n                               transforms.Resize(img_size),\n                               transforms.RandomCrop(img_size),\n                               transforms.RandomHorizontalFlip(),\n                               transforms.RandomPerspective(),\n                               transforms.ToTensor(),\n                               transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n                           ]))\n\n\n# Create the dataloader\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers,drop_last=True)\n\nlabel_names=[\"other\",\"s_interdit\",\"vit_lim\"]","a4f5a847":"#Create the validation dataset\nroot_path=\"..\/input\/panneauxv3\/00_validation\"\n\nval_dataset = dset.ImageFolder(root=root_path,\n                           transform=transforms.Compose([\n                               transforms.Resize(img_size),\n                               transforms.RandomCrop(img_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n                           ]))\n\n\n# Create the validation dataloader\nvalloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers,drop_last=True)","e64770a5":"print(\"vit_lim has %d train images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_train\/vit_lim\"))))\nprint(\"vit_lim has %d test images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_test\/vit_lim\"))))\nprint(\"vit_lim has %d val images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_validation\/vit_lim\"))))\n\nprint(\"s_interdit has %d train images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_train\/s_interdit\"))))\nprint(\"s_interdit has %d test images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_test\/s_interdit\"))))\nprint(\"s_interdit has %d val images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_validation\/s_interdit\"))))\n\nprint(\"other has %d train images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_train\/other\"))))\nprint(\"other has %d test images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_test\/other\"))))\nprint(\"other has %d val images\"%(len(os.listdir(\"..\/input\/panneauxv3\/00_validation\/other\"))))","5903c74c":"#Weights for the cross entropy loss\n\ntotal_train=len(os.listdir(\"..\/input\/panneauxv2\/00_train\/vit_lim\"))+len(os.listdir(\"..\/input\/panneauxv2\/00_train\/s_interdit\"))+len(os.listdir(\"..\/input\/panneauxv2\/00_train\/other\"))\ntrainloader_weights=torch.tensor(([len(os.listdir(\"..\/input\/panneauxv2\/00_train\/other\"))\/total_train,len(os.listdir(\"..\/input\/panneauxv2\/00_train\/s_interdit\"))\/total_train,len(os.listdir(\"..\/input\/panneauxv2\/00_train\/vit_lim\"))\/total_train])).to(device)\nprint(trainloader_weights)","885f36a9":"real_batch = next(iter(trainloader))\nimages=real_batch[0]\nlabels=real_batch[1]\nMEAN = torch.tensor([0.485, 0.456, 0.406])\nSTD = torch.tensor([0.229, 0.224, 0.225])\nimages = images * STD[:, None, None] + MEAN[:, None, None]\nplt.figure(figsize=(20,12))\ni=0\nfor image,label in zip(images,labels):\n    plt.subplot(4,4,i+1)\n    plt.axis(\"off\")\n    plt.imshow(np.transpose(image,(1,2,0)))\n    plt.title(\"Label : %s\"%(label_names[label]))\n    i+=1","ddd3d893":"def Trainer(model,optimizer, criterion, model_name=\"unamed_model\",num_epochs=num_epochs, dataloader=trainloader,scheduler=None,valloader=valloader):\n    print(\"Start trainining %s\"%model_name)\n    training_start_time=time.time()\n    losses=[]\n    best_val=0\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, data in enumerate(dataloader, 0):\n            start=time.time()        \n            inputs, labels = data\n            inputs=inputs.to(device)\n            labels=labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n\n            maxvalue,argmax=torch.max(outputs,1)        \n\n            loss.backward()\n            losses.append(loss.item())\n            optimizer.step()\n\n            iter_time=time.time()\n\n            if i%10==0:\n                print('[Epoch %d\/%d, batch %d\/%d] loss: %.3f \/ Accuracy %s \/ Iteration in %.3f s' % (epoch + 1,num_epochs, i + 1,len(trainloader),loss.item(),((argmax==labels).sum().item()\/batch_size),iter_time-start))\n\n        if ((epoch+1)%save_epochs==0) or (epoch==num_epochs-1):\n            \n            accuracies=[]\n            for images,labels in valloader:\n                images=images.to(device)\n                labels=labels.to(device)\n                with torch.no_grad():\n                    outputs = model(images)\n                    maxvalue,argmax=torch.max(outputs,1)  \n                    local_accuracy=(argmax==labels).sum().item()\/batch_size\n                    accuracies.append(local_accuracy) \n                    \n            val_score=np.mean(accuracies)\n            print(\"Validation score (accuracy) : %s\"%val_score)\n            if val_score>best_val:\n                print(\"Best validation score so far\")\n                save_path = '.\/models\/'+str(model_name)+'\/bestnet.pth'\n                torch.save(model.state_dict(), save_path)\n                best_val=val_score\n                    \n            save_path = '.\/models\/'+str(model_name)+'\/net_'+str(epoch+1)+'.pth'\n            torch.save(model.state_dict(), save_path)\n        \n        if scheduler!=None:\n            scheduler.step()\n\n    training_end_time=time.time()\n    print(\"Trained %s with %d epochs in %d sec\"%(model_name,num_epochs,training_end_time-training_start_time))\n    return(losses)","d7966333":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(2704, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 3)\n        \n        #self.activation=nn.Softmax()\n        \n        self.dropout=nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.fc3(x)\n        #x=self.activation(x) not needed since in CrossEntropyLoss\n        \n        return x\n\n\nnet = Net().to(device)","ab6eb699":"print(net)\npytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\nprint(\"Number of trainable parameters %s\"%pytorch_total_params)","d67c2f41":"criterion = nn.CrossEntropyLoss(weight=trainloader_weights)\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n#scheduler= optim.lr_scheduler.StepLR(optimizer, step_size=int(num_epochs\/2), gamma=0.1, last_epoch=-1, verbose=False)\n\ntry:\n    os.makedirs(\"models\/net\")\nexcept:\n    pass","43819aca":"losses=Trainer(net,optimizer,criterion,model_name=\"net\")","075ae431":"plt.figure(figsize=(10,10))\nplt.plot(losses)\nplt.title(\"Loss (Cross Entropy Loss) along training\")","bc87d39a":"resnet34=torchvision.models.resnet34(pretrained=True)\nresnet34.to(device)","e54cae4e":"print(\"Number of trainable parameters %s\"%sum(p.numel() for p in resnet34.parameters() if p.requires_grad))","5d279655":"resnet34(next(iter(trainloader))[0].to(device)).shape","4039d113":"resnet34.fc=nn.Identity()","dce614b1":"resnet34(next(iter(trainloader))[0].to(device)).shape","2ca8e31c":"for param in resnet34.parameters():\n    param.requires_grad=False","bf4099da":"class ResNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet34 = resnet34\n        self.fc3 = nn.Linear(512, 128)\n        self.fc4 = nn.Linear(128, NUM_CLASSES)\n        self.dropout=nn.Dropout(p=0.2)\n\n    \n    def forward(self, x):\n        x = self.resnet34(x) #512\n        x = F.relu(self.dropout(self.fc3(x))) #128\n        x = self.dropout(self.fc4(x)) #3\n        \n        return x\n\n\nresnet = ResNet().to(device)","b618153d":"resnet(next(iter(trainloader))[0].to(device))","841921b6":"print(\"Number of trainable parameters %s\"%sum(p.numel() for p in resnet.parameters() if p.requires_grad))","2cabf5b8":"criterion = nn.CrossEntropyLoss(weight=trainloader_weights)\noptimizer = optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9)\nscheduler= optim.lr_scheduler.StepLR(optimizer, step_size=int(num_epochs\/2), gamma=0.1, last_epoch=-1, verbose=False)\n\ntry:\n    os.makedirs(\"models\/resnet\")\nexcept:\n    pass","a230c76e":"losses=Trainer(resnet, optimizer, criterion,model_name=\"resnet\",scheduler=scheduler)","6d9dd359":"plt.figure(figsize=(10,10))\nplt.plot(losses)\nplt.title(\"Loss (Cross Entropy Loss) along training\")","4f92c2d2":"for param in resnet.parameters():\n    param.requires_grad=True","ad76704b":"print(\"Number of trainable parameters %s\"%sum(p.numel() for p in resnet.parameters() if p.requires_grad))","065fb402":"criterion = nn.CrossEntropyLoss(weight=trainloader_weights)\noptimizer = optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9)\nscheduler= optim.lr_scheduler.StepLR(optimizer, step_size=int(num_epochs\/2), gamma=0.1, last_epoch=-1, verbose=False)\n\ntry:\n    os.makedirs(\"models\/resnet_fine\")\nexcept:\n    pass","5a4c0c54":"losses=Trainer(resnet, optimizer, criterion,model_name=\"resnet_fine\",scheduler=scheduler)","50016e0d":"plt.figure(figsize=(10,10))\nplt.plot(losses)\nplt.title(\"Loss (Cross Entropy Loss) along training\")","fb41e14b":"root_path=\"..\/input\/panneauxv2\/00_test\"\n#Create the dataset\ntest_dataset = dset.ImageFolder(root=root_path,\n                               transform=transforms.Compose([\n                               transforms.Resize(img_size),\n                               transforms.RandomCrop(img_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n                           ]))\n\n#withdraw the random flip and random perspective to keep the originals resized and cropped\n\n# Create the dataloader\ntestloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers,drop_last=True)","610746f4":"net=Net()\n\nnum_epochs_test=num_epochs\nsave_path = '.\/models\/net\/net_'+str(num_epochs_test)+'.pth'\nnet.load_state_dict(torch.load(save_path))\nnet.to(device)","42d04f14":"resnet=ResNet()\nsave_path = '.\/models\/resnet\/net_'+str(num_epochs_test)+'.pth'\nresnet.load_state_dict(torch.load(save_path))\nresnet.to(device)","37de07e3":"resnet_fine=ResNet()\nsave_path = '.\/models\/resnet_fine\/net_'+str(num_epochs_test)+'.pth'\nresnet_fine.load_state_dict(torch.load(save_path))\nresnet_fine.to(device)","305861d1":"testbatch=next(iter(testloader))\n\nwith torch.no_grad():\n    predictions_net = net(testbatch[0].to(device))\n    predictions_resnet = resnet(testbatch[0].to(device))\n    predictions_resnet_fine = resnet_fine(testbatch[0].to(device))\n\nvalues_net,argmax_net=torch.max(predictions_net,1)\nvalues_resnet,argmax_resnet=torch.max(predictions_resnet,1)\nvalues_resnet_fine,argmax_resnet_fine=torch.max(predictions_resnet_fine,1)","dd28152a":"images=testbatch[0]\nlabels=testbatch[1]\nimages = images * STD[:, None, None] + MEAN[:, None, None]\n\nplt.figure(figsize=(20,25))\ni=0\nfor image,label in zip(images,labels):\n    plt.subplot(4,4,i+1)\n    plt.axis(\"off\")\n    plt.imshow(np.transpose(image,(1,2,0)))\n    plt.title(\"Label : %s \\n Net : %s \\n ResNet : %s \\n ResNet finetuned : %s\"%(label_names[label],label_names[argmax_net[i]],label_names[argmax_resnet[i]],label_names[argmax_resnet_fine[i]]))\n    i+=1","1c746a8c":"def compute_accuracy(model,loader=testloader,num_epoch_save=10,model_name=\"net\"):\n    model=model.to(device)\n    save_path = '.\/models\/'+str(model_name)+'\/net_'+str(num_epoch_save)+'.pth'\n    #print(save_path)\n    model.load_state_dict(torch.load(save_path))\n    accuracies=[]\n    for images,labels in loader:\n        images=images.to(device)\n        with torch.no_grad():\n            outputs = model(images)\n            maxvalue,argmax=torch.max(outputs,1)  \n            local_accuracy=(argmax==labels.to(device)).sum().item()\/batch_size\n            accuracies.append(local_accuracy)\n    return(np.mean(accuracies))\n","becffb45":"net_train_accuracies=[]\nnet_test_accuracies=[]\nresnet_train_accuracies=[]\nresnet_test_accuracies=[]\nresnetfine_train_accuracies=[]\nresnetfine_test_accuracies=[]\nindex=[]\nfor num in tqdm(range(1,num_epochs+1)):\n    if num%save_epochs==0:\n        index.append(num)\n        net_train_accuracies.append(compute_accuracy(net,loader=trainloader,num_epoch_save=num,model_name=\"net\"))\n        net_test_accuracies.append(compute_accuracy(net,loader=testloader,num_epoch_save=num,model_name=\"net\"))\n        resnet_train_accuracies.append(compute_accuracy(resnet,loader=trainloader,num_epoch_save=num,model_name=\"resnet\"))\n        resnet_test_accuracies.append(compute_accuracy(resnet,loader=testloader,num_epoch_save=num,model_name=\"resnet\"))\n        resnetfine_train_accuracies.append(compute_accuracy(resnet,loader=trainloader,num_epoch_save=num,model_name=\"resnet_fine\"))\n        resnetfine_test_accuracies.append(compute_accuracy(resnet,loader=testloader,num_epoch_save=num,model_name=\"resnet_fine\"))","ce482ee7":"plt.figure(figsize=(10,10))\nplt.plot(net_train_accuracies,label=\"Train net\")\nplt.plot(net_test_accuracies,label=\"Test net\")\nplt.plot(resnet_train_accuracies,label=\"Train resnet\")\nplt.plot(resnet_test_accuracies,label=\"Test resnet\")\nplt.plot(resnetfine_train_accuracies,label=\"Train resnet finetuned\")\nplt.plot(resnetfine_test_accuracies,label=\"Test resnet finetuned\")\nplt.xticks(range(0,int((num_epochs\/save_epochs))),index)\nplt.legend()\nplt.title(\"Mean accuracy per epoch\")","d4b69ebb":"net=Net()\nsave_path = '.\/models\/net\/bestnet.pth'\nnet.load_state_dict(torch.load(save_path))\nnet.to(device)\n\nresnet=ResNet()\nsave_path = '.\/models\/resnet\/bestnet.pth'\nresnet.load_state_dict(torch.load(save_path))\nresnet.to(device)\n\nresnet_fine=ResNet()\nsave_path = '.\/models\/resnet_fine\/bestnet.pth'\nresnet_fine.load_state_dict(torch.load(save_path))\nresnet_fine.to(device)","fefe665f":"def compute_best_accuracy(model,loader=testloader,model_name=\"net\"):\n    model=model.to(device)\n    save_path = '.\/models\/'+str(model_name)+'\/bestnet.pth'\n    #print(save_path)\n    model.load_state_dict(torch.load(save_path))\n    accuracies=[]\n    for images,labels in loader:\n        images=images.to(device)\n        with torch.no_grad():\n            outputs = model(images)\n            maxvalue,argmax=torch.max(outputs,1)\n            local_accuracy=(argmax==labels.to(device)).sum().item()\/batch_size\n            accuracies.append(local_accuracy)\n    return(np.mean(accuracies))","1092d53f":"compute_best_accuracy(net,model_name=\"net\")","fc3bb05c":"compute_best_accuracy(resnet,model_name=\"resnet\")","ae2b772e":"compute_best_accuracy(resnet_fine,model_name=\"resnet_fine\")","3b15a7ec":"Although the model uses a single fully connected layer (that are expensive in terms of number of parameters) and mostly convolutions, it still has more than 21M parameters, that would require a long training to reach an ideal level.","0961594c":"# Remarks","32bd9f0c":"# Evaluate results","dc0bbad2":"We only train a small number of parameters corresponding to the last two layers (but we still save the whole model with its 21M weights) every 2 epochs.","37b02836":"The test data loader does not include transformations (we want to classify \"original\" images) but still needs to have resizing to be able to use the networks above, and normalization to be consistent with their training.","e5381a7d":"Our first method with the pretrained ResNet-34 consists in extracting the final 512 features (as explained above) and training two new fully connected layers. All original parameters are frozen to their value and their gradients are not computing while training.","0fca7207":"# Visualization","2a7fd4ca":"We used the following to see that our dataset has uneven classes. We defined `trainloader_weights` that reflects the distribution of images in each of the classes for the training set. It will be used in the Cross Entropy loss to account for this inequal distribution of classes when computing the loss.","bb66e3c3":"We then uses ResNet-34 pretrained network with a different approach than feature extraction. We unfreeze the parameters and allow all parameters (original ResNet-34 parameters and the parameters of the last layers) to be updated. The difference between them is that the parameters of ResNet-34 are initialized to their pretrained value, while ours are randomly initialized. \n\nA third approach would be to reset all parameters and randomly initialized them then train the whole model, this would be great to have a specific network for our task but it would require a very high number of epochs and thus a long training.","770bd7ea":"The output is of size 1000 (the model was pretrained using ImageNet) thus we ablate the last layer (by setting it to Identity) and will include the almost full ResNet-34 network as a block of a custom network. We uses 2 fully connected layers with a 0.2 dropout (randomly ignoring 20% of the parameters) to avoid overfitting and take advantage of the limited use of fully connected layers in ResNet.","ba67220e":"Unsurprisingly the loss is dropping along training, with a quite high variance, that could be explained by the limited batch size thus the loss can be highly depending of having the model well fitted to the current batch or not.","2e4988ee":"We set various parameters : \n* The image size is set to 64 pixels (it's a sufficient size not to loose information for traffic signs, and using square image is consistent with our task and the use of squared convolutions)\n* The batch size is set to 16\n* We use (if possible) the single GPU provided by Kaggle\n* We proceed to 40 epochs for each model, either for full training or fine tuning\n* The parameters of the trained models are saved every 2 epoch","c70e0806":"We implemented our dataset in the `pytorch` Framework using `dataset.ImageFolder` and we proceed to both resizing and data augmentation with `torch.transforms`. Using `RandomCrop`, `RandomHorizontalFlip` and `RandomPerspective` the images will take different appearances for each iterations, thus at each epoch we use slightly different images. This kind of transformation is useful to have an adaptable model that is not overfitted even though our dataset is limited (see below for the number of images per classes). In the `DataLoader` creation we used shuffling to use different images in each batch at each epoch.","ffb4a774":"We show an example output, here the last two layers (fully connected) are not trained and are thus randomly initialized.","3314aa08":"We visualize the images after transformation with their ground truth label. Most traffic sign are fully represented in the images but non square ones (almost only in the `other` class) can't be fully visualized and are randomly cropped along their larger dimension to match the $64\\times 64$ image size used in the networks.","cde646bb":"# Classification with a trained CNN","a64fa26a":"# Trainer module","b8473ecf":"To train every model we use the same framework define in the `Trainer` function. It consists in the following main steps that are standard proceeding when training a model with `pytorch`.\n\nFor each epoch, iterate over each batch of the dataloader :\n* Send the inputs images and labels to the device used for training (GPU if available)\n* Reset the gradients to 0\n* Compute the ouput of the model with a forward pass\n* Compute the loss (provided as argument)\n* Use the output values and their $\\mathrm{argmax}$ to get the predicted labels\n* Backpropagate the loss and compute the gradients of each (trainable) parameter\n* Update the parameters with optimizer using the desired optimizer (provided in `Trainer` options)\n\nAt the end of each epoch:\n* Save the model and save it as best model if its validation accuracy is the best one (every 2 epoch)\n* Update the learning rate value with the scheduler if provided","94c8b51d":"We created our image dataset from images recovered on Google Images with web-scraping and we also took pictures in the street with our cell phone.\n\nScraping generally defines a technique for extracting content (information) from one or more websites completely automatically. We used the selenium library to scrap images with the Firefox driver, the process executes the search on Google and iterate over the images then save them. \n\nWe selected 100 images among the links of Google Images containing the following keywords:\n- Panneaux sens interdit\n- Rue sens interdit\n- Panneaux Vitesse limite\n- Panneaux Vitesse 30\n- Panneaux Vitesse 20\n- Panneaux Routiers\n\nThen we used a custom tool made with the library `opencv` to quickly crop images.\n\nBoth tools as well as our dataset can be found on github : https:\/\/github.com\/sambent15\/PanneauxClassifier","e4011d33":"Our second approach is to use transfer learning with a much deeper model : ResNet-34 (Kaiming He et al., https:\/\/arxiv.org\/abs\/1512.03385). This model features blocks containing subblocks of $3\\times 3$ convolution with various number of channels (64, 128, 256 and 512), totaling 34 convolutions layers. In addition the ResNet architecture uses batch-normalization inbetween convolutions to regularize the output. Finaly ResNet standardly uses adaptative average pooling to obtain a 512 features output then only one fully connected layer to get the desired number of logits.","d683832a":"# Introduction and data collection","2178c9e1":"We first proceed to the import of needed packages. Our simple custom model will be implemented using `pytorch` pretrained models like the one we will be using (ResNet-34) can be in a straight forward way using `torchvision`.\nOther packages (`os`, `tqdm`, `numpy` are used for other treatments and manipulation out of the main networks).","9886ee16":"We now obtain a much more consequent network to train, although most of the parameters are already initialized at a \"decent\" value and will soon be well adjusted without requiring a lot of epochs.","06664cbb":"Empirically it seems that ResNet model without fine tuning yields more errors, it could be causes by the unadaptated feature creation, that might be corrected with overtrained fully connected layers for the training set but does not work for the test set.","0cb38ff9":"# Method 1 : Freezing parameters and extracting features","91cf0866":"# Method 2 : Fine Tuning of ResNet-34","0de55f58":"This comparision of CNN for classification purposes illustrates the interest of this kind of architecture for that kind of task. It is shown that it can achieve great performances (even though we remain quite far from state of the art accuracies on comparable tasks), even with a very limited network (6 layers) and a few parameters.\n\nOne of the main limitation of our work is the number of images. Collecting datas is a time-expensive task but having too limited datasets leads to a major problem, even though data augmentation may improve training : the distribution of images in each dataset is too different thus the validation set does not capture the diversity of images in the test set when selecting the best model (thus test performances might be lower than validation one).","f4e3bf0a":"We now want to evaluate the models on the test set. We will be comparing models at every 2 epochs regarding their overall accuracy.","ece5af9b":"Along training for a same model it appears that most of the time the training accuracy is above the test accuracy, it denote a kind of overfitting. Even though training techniques can reduce it, it is to be expected when test and training sets are very different, and they might be because the samples are relatively limited.\n\nIt seems that ResNet Finetuned model suffers less from this issue, probably because of the numerous convolutions that may better extract global features whereas fully connected layers would tend to adapt themselves to very particular features.","ac82f444":"We here present an example of predictions using our 3 approaches.","cacff7a3":"We keep the same criterion and optimizer but we use a scheduler. Thus the learning rate is set at a higher level at the beggining (0.1) to obtain a rough adjustment of the weights of the fully connected layers to the task, then (after half of the epochs) to make a more precise adjustment (with a learning rate of $0.01 \\times \\gamma = 0.001$. We did not choose to add more steps (and get a lower learning rate) to avoid overfitting.","ba1fcc71":"# Best models accuracies","6bc71225":"It is interesting to note that the detection can be easily alterated (we guessed the features are deeply modified even though human vision still perceive the original traffic sign on the below image).\n\n![image.png](attachment:3138b300-9d5b-4d68-88bf-471672b077b0.png)","f51a4b6b":"We then define the loss Criterion and Optimizer used in the training.\n\n1. We used the Cross Entropy Loss (precisely a weighted Cross Entropy)\nThis is a standard measure for multi-class classification problems. The loss regarding the class $C$ of weight $W_C$ for an input $x_i$ with logits outputs $x_{i}^{j}$ for $j\\in \\{1,...,K\\}$ is given by : $l(x_i,C)=-W_C \\log\\left(\\frac{\\exp(x_i^C)}{\\sum_j{\\exp{x_i^j}}}\\right)$.\n\nThen the loss is computed for $C=C_i$ the label of $x_i$ and are averaged on the batch with the corresponding weights $\\frac{\\sum_{i=1}^N{l(x_i,C_i)}}{\\sum_{i=1}^N{W_{C_i}}}$\n\n2. We used the Stochastic Gradient Descent Optimizer\nWe did not used a scheduler, because experiments did not show an improvement and overfitting at the end when the learning rate reaches low level) we kept the original 0.001 learning rate all along the training.","8811e45d":"In the past decade CNN have shown great performances for classification tasks on images. Famous benchmark datasets includes ImageNet but also for tasks near ours : classifying images of traffic signs, such as the Germany Traffic Sign Benchmarks (https:\/\/benchmark.ini.rub.de\/). We focused our task into classifiying into 3 classes with french traffic sign : blocked road, speed limit and a general \"other\" class with every other kind of traffic signs. ","3c5d672c":"# Transfer learning : ResNet 34","4d6a25ae":"It appears that the fine tuned ResNet model is the best one in terms of accuracy, but test accuracies are lower than the train accuracies (it could be expected because of overfitting) but also validation (it could be explained by a distribution issue because the validation set is very limited)","41f9bc54":"# Imports","8e4ae94a":"Our first approach is to implement a very simple convolutionnal network. It contains :\n* First convolutional block : maps the 3 channel input to a 6 channel output with a reduced size using a kernel of size ($5\\times 5$), regularize the output with ReLU and and apply a max pooling on of size $(2\\times 2$)\n* Second convolutionnal block : maps the 6 channel input to a 6 channel input with the same kernel size and regularization\n* 3 fully connected layers to map the extracted 2704 features (flattening of the output of the second convolutionnal block) to a prediction of size 3 (logits of the 3 classes). \n\nThis model yields 337891 parameters that requires grad (all of the paramaeters are trained) that are updated at each batch of each epoch.","6a9080bc":"# Dataloader","177acf93":"# Global parameters","5a9d00c6":"Note that images are normalized to match the distribution of the training set used for the pretraining of the ResNet-34 model we are using below."}}