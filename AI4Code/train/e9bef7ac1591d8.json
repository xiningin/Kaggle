{"cell_type":{"682f7a33":"code","51e271be":"code","8a9a7f84":"code","924b9a86":"code","75dc7a28":"code","44a5f52f":"code","bb6a224e":"code","b067e00c":"code","9af213f8":"code","b74b5f07":"code","71cefce9":"code","71910d23":"code","3045143f":"code","a20be9d5":"code","f485dae2":"code","227d22de":"code","7ae4f581":"code","b3c0f391":"code","aa1eddad":"code","cfb7ccf0":"code","a8786e37":"code","e536c62b":"code","2f79d129":"code","a3f7ad13":"code","f960f977":"code","748d14d0":"code","bc9c0dbf":"code","05f4baaf":"markdown","6c7434cd":"markdown","be934d2d":"markdown","40ef4bf6":"markdown","6db611e7":"markdown","954286e6":"markdown","e0e81d23":"markdown","b1e16691":"markdown","117f2912":"markdown","f3344440":"markdown","48afc774":"markdown","273a45f9":"markdown","84d98cf4":"markdown","bb664aac":"markdown","e15bb357":"markdown","e6aeaae4":"markdown","49f35e00":"markdown","e679c8cb":"markdown","2fc8b517":"markdown","87cfb81a":"markdown","5f27ffc3":"markdown","3ffb5318":"markdown"},"source":{"682f7a33":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt","51e271be":"# reading the data\nbikes_hour_df_raws = pd.read_csv('..\/input\/bike-sharing-dataset\/hour.csv')\nbike_day_df_raws = pd.read_csv('..\/input\/bike-sharing-dataset\/day.csv')","8a9a7f84":"# First Rows of bike rental hour dataset\nbikes_hour_df_raws.head()","924b9a86":"# First Rows of bike rental day dataset\nbike_day_df_raws.head()","75dc7a28":"# removing useless feature\nbikes_hour_df = bikes_hour_df_raws.drop(['casual' , 'registered'], axis=1)","44a5f52f":"#lets get information about features\nbikes_hour_df.info()","bb6a224e":"bikes_hour_df['cnt'].describe()","b067e00c":"fig, ax = plt.subplots(1)\nax.plot(sorted(bikes_hour_df['cnt']), color = 'blue', marker = '*', label='cnt')\nax.legend(loc= 'upper left')\nax.set_ylabel('Sorted Rental Counts', fontsize = 10)\nfig.suptitle('Recorded Bike Rental Counts', fontsize = 10)\n","9af213f8":"plt.scatter(bikes_hour_df['temp'], bikes_hour_df['cnt'])\nplt.suptitle('Numerical Feature: Cnt v\/s temp')\nplt.xlabel('temp')\nplt.ylabel('Count of all Biks Rented')","b74b5f07":"plt.scatter(bikes_hour_df['atemp'], bikes_hour_df['cnt'])\nplt.suptitle('Numerical Feature: Cnt v\/s atemp')\nplt.xlabel('atemp')\nplt.ylabel('Count of all Biks Rented')","71cefce9":"plt.scatter(bikes_hour_df['hum'], bikes_hour_df['cnt'])\nplt.suptitle('Numerical Feature: Cnt v\/s hum')\nplt.xlabel('hum')\nplt.ylabel('Count of all Biks Rented')","71910d23":"plt.scatter(bikes_hour_df['windspeed'], bikes_hour_df['cnt'])\nplt.suptitle('Numerical Feature: Cnt v\/s windspeed')\nplt.xlabel('windspeed')\nplt.ylabel('Count of all Biks Rented')","3045143f":"f,  (ax1, ax2)  =  plt.subplots(nrows=1, ncols=2, figsize=(13, 6))\n\nax1 = bikes_hour_df[['season','cnt']].groupby(['season']).sum().reset_index().plot(kind='bar',\n                                       legend = False, title =\"Counts of Bike Rentals by season\", \n                                         stacked=True, fontsize=12, ax=ax1)\nax1.set_xlabel(\"season\", fontsize=12)\nax1.set_ylabel(\"Count\", fontsize=12)\nax1.set_xticklabels(['spring','sumer','fall','winter'])\n\n \nax2 = bikes_hour_df[['weathersit','cnt']].groupby(['weathersit']).sum().reset_index().plot(kind='bar',  \n      legend = False, stacked=True, title =\"Counts of Bike Rentals by weathersit\", fontsize=12, ax=ax2)\n\nax2.set_xlabel(\"weathersit\", fontsize=12)\nax2.set_ylabel(\"Count\", fontsize=12)\nax2.set_xticklabels(['1: Clear','2: Mist','3: Light Snow','4: Heavy Rain'])\n\nf.tight_layout()\n","a20be9d5":"# alternative way of plotting using groupby\nax = bikes_hour_df[['hr','cnt']].groupby(['hr']).sum().reset_index().plot(kind='bar', figsize=(8, 6),\n                                       legend = False, title =\"Total Bike Rentals by Hour\", \n                                       color='orange', fontsize=12)\nax.set_xlabel(\"Hour\", fontsize=12)\nax.set_ylabel(\"Count\", fontsize=12)\nplt.show()","f485dae2":"# lets copy for editing without effecting original\nbikes_df_model_data = bikes_hour_df.copy()\n\noutcome = 'cnt'\n\n#making feature list for each modeling - experiment by adding feature to the exclusion list\nfeature = [feat for feat in list(bikes_df_model_data) if feat not in [outcome, 'instant', 'dteday']]\n\n#spliting data into train and test portion\nX_trian, X_test, y_train, y_test = train_test_split(bikes_df_model_data[feature],\n                                                   bikes_df_model_data[outcome],\n                                                   test_size=0.3, random_state=42)\n\nfrom sklearn import linear_model\nlr_model = linear_model.LinearRegression()\n\n#training model in training set\nlr_model.fit(X_trian, y_train)\n\n# making predection using the test set\ny_pred = lr_model.predict(X_test)\n\n#root mean squared error\nprint('RMSE: %.2f' % sqrt(mean_squared_error(y_test, y_pred)))\n","227d22de":"# lets copy for editing without effecting original\nbikes_df_model_data = bikes_hour_df.copy()\n\noutcome = 'cnt'\n\n#making feature list for each modeling - experiment by adding feature to the exclusion list\nfeature = [feat for feat in list(bikes_df_model_data) if feat not in [outcome, 'instant', 'dteday']]\n\nX_trian, X_test, y_train, y_test = train_test_split(bikes_df_model_data[feature],\n                                                   bikes_df_model_data[outcome],\n                                                   test_size=0.3, random_state=42)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_feat = PolynomialFeatures(2)\nX_train = poly_feat.fit_transform(X_trian)\nX_test = poly_feat.fit_transform(X_test)\n\nfrom sklearn import linear_model\nlr_model= linear_model.LinearRegression()\n\n# training the model on traning set\nlr_model.fit(X_train, y_train)\n\n# make the prediction\ny_pred = lr_model.predict(X_test)\n\n#root mean squared error\nprint(\"Root Mean squared error with PolynomialFeatures set to 2 degrees: %.2f\" \n      % sqrt(mean_squared_error(y_test, y_pred)))","7ae4f581":"# lets copy for editing without effecting original\nbikes_df_model_data = bikes_hour_df.copy()\n\noutcome = 'cnt'\n\n#making feature list for each modeling - experiment by adding feature to the exclusion list\nfeature = [feat for feat in list(bikes_df_model_data) if feat not in [outcome, 'instant', 'dteday']]\n\nX_trian, X_test, y_train, y_test = train_test_split(bikes_df_model_data[feature],\n                                                   bikes_df_model_data[outcome],\n                                                   test_size=0.3, random_state=42)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_feat = PolynomialFeatures(4)\nX_train = poly_feat.fit_transform(X_trian)\nX_test = poly_feat.fit_transform(X_test)\n\nfrom sklearn import linear_model\nlr_model= linear_model.LinearRegression()\n\n# training the model on traning set\nlr_model.fit(X_train, y_train)\n\n# make the prediction\ny_pred = lr_model.predict(X_test)\n\n#root mean squared error\nprint(\"Root Mean squared error with PolynomialFeatures set to 4 degrees: %.2f\" \n      % sqrt(mean_squared_error(y_test, y_pred)))","b3c0f391":"def prepare_data_for_model(raw_dataframe, \n                           target_columns, \n                           drop_first = False, \n                           make_na_col = True):\n    \n    # dummy all categorical fields \n    dataframe_dummy = pd.get_dummies(raw_dataframe, columns=target_columns, \n                                     drop_first=drop_first, \n                                     dummy_na=make_na_col)\n    return (dataframe_dummy)\n\n# make a copy for editing without affecting original\nbike_df_model_ready = bikes_hour_df.copy()\nbike_df_model_ready = bike_df_model_ready.sort_values('instant')\n\n# dummify categorical columns\nbike_df_model_ready = prepare_data_for_model(bike_df_model_ready, \n                                            target_columns = ['season', \n                                                              'weekday', \n                                                              'weathersit'],\n                                            drop_first = True)\n\n# remove the nan colums in dataframe as most are outcome variable and we can't use them\nbike_df_model_ready = bike_df_model_ready.dropna() \n\n\noutcome = 'cnt'\nfeatures = [feat for feat in list(bike_df_model_ready) if feat not in [outcome, 'instant',  'dteday']]  \n\n \nX_train, X_test, y_train, y_test = train_test_split(bike_df_model_ready[features], \n                                                 bike_df_model_ready[['cnt']], \n                                                 test_size=0.5, \n                                                 random_state=42)\nfrom sklearn import linear_model\nmodel_lr = linear_model.LinearRegression()\n \n# train the model on training set\nmodel_lr.fit(X_train, y_train)\n\n# make predictions using the testing set\npredictions = model_lr.predict(X_test)\n \n# print coefficients as this is what our web application will use in the end\nprint('Coefficients: \\n', model_lr.coef_)\n\n# root mean squared error\nprint(\"Root Mean squared error: %.2f\" % sqrt(mean_squared_error(y_test, predictions)))\n ","aa1eddad":"bike_df_model_ready[['weathersit_2.0', 'weathersit_3.0', 'weathersit_4.0']].head()","cfb7ccf0":"# simple approach - make a copy for editing without affecting original\nbike_df_model_ready = bikes_hour_df.copy()\nbike_df_model_ready = bike_df_model_ready.sort_values('instant')\n\n# dummify categorical columns\nbike_df_model_ready = prepare_data_for_model(bike_df_model_ready, \n                                             target_columns = ['season', 'weekday', 'weathersit'])\nlist(bike_df_model_ready.head(1).values)\n\n# remove the nan colums in dataframe as most are outcome variable and we can't use them\nbike_df_model_ready = bike_df_model_ready.dropna() \n\n\noutcome = 'cnt'\nfeatures = [feat for feat in list(bike_df_model_ready) if feat not in [outcome, 'instant', 'dteday']]  \n\n \nX_train, X_test, y_train, y_test = train_test_split(bike_df_model_ready[features], \n                                                 bike_df_model_ready[['cnt']], \n                                                 test_size=0.5, \n                                                 random_state=42)\n \nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel_gbr = GradientBoostingRegressor()\nmodel_gbr.fit(X_train, np.ravel(y_train))\npredictions = model_gbr.predict(X_test)\n\n# root mean squared error\nprint(\"Root Mean squared error: %.2f\" % sqrt(mean_squared_error(y_test, predictions)))\n ","a8786e37":"\n# prior hours\nbikes_hour_df_shift = bikes_hour_df[['dteday','hr','cnt']].groupby(['dteday','hr']).sum().reset_index()\nbikes_hour_df_shift.sort_values(['dteday','hr'])\n# shift the count of the last two hours forward so the new count can take in consideratio how the last two hours went \nbikes_hour_df_shift['sum_hr_shift_1'] = bikes_hour_df_shift.cnt.shift(+1)\nbikes_hour_df_shift['sum_hr_shift_2'] = bikes_hour_df_shift.cnt.shift(+2)\n\nbike_df_model_ready =  pd.merge(bikes_hour_df, bikes_hour_df_shift[['dteday', 'hr', 'sum_hr_shift_1', 'sum_hr_shift_2']], how='inner', on = ['dteday', 'hr'])\n\n# drop NAs caused by our shifting fields around\nbike_df_model_ready = bike_df_model_ready.dropna()\n\noutcome = 'cnt'\n# create a feature list for each modeling - experiment by adding features to the exclusion list\nfeatures = [feat for feat in list(bike_df_model_ready) if feat not in [outcome, 'instant', 'dteday','casual', 'registered']]  \n\n# split data into train and test portions and model\nX_train, X_test, y_train, y_test = train_test_split(bike_df_model_ready[features], \n                                                 bike_df_model_ready[['cnt']], \n                                                 test_size=0.3, random_state=42)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel_gbr = GradientBoostingRegressor()\nmodel_gbr.fit(X_train, np.ravel(y_train))\npredictions = model_gbr.predict(X_test)\n\n# root mean squared error\nprint(\"Root Mean squared error: %.2f\" % sqrt(mean_squared_error(y_test, predictions)))","e536c62b":"# prior hours\nnp.mean(bikes_hour_df_shift['sum_hr_shift_1'])","2f79d129":" \n\n# loop through each feature and calculate the R^2 score\nfeatures = ['hr', 'season', 'holiday', 'temp']\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\n\n# split data into train and test portions and model\nX_train, X_test, y_train, y_test = train_test_split(bike_df_model_ready[features], \n                                                 bike_df_model_ready[['cnt']], \n                                                 test_size=0.3, random_state=42)\n    \nfor feat in features:\n    model_lr = linear_model.LinearRegression()\n    model_lr.fit(X_train[[feat]], y_train)\n    predictions = model_lr.predict(X_test[[feat]])\n    print('R^2 for %s is %f' % (feat, r2_score(y_test, predictions)))\n    ","a3f7ad13":"# train the model on training set\nmodel_lr.fit(X_train, y_train)\n\n# make predictions using the testing set\npredictions = model_lr.predict(X_test)\n \n# root mean squared error\nprint(\"Root Mean squared error: %.2f\" % sqrt(mean_squared_error(y_test, predictions)))\nprint('\\n')\nprint('Intercept: %f' % model_lr.intercept_)\n\n# features with coefficients \nfeature_coefficients  = pd.DataFrame({'coefficients':model_lr.coef_[0], \n                                    'features':X_train.columns.values})\n\nfeature_coefficients.sort_values('coefficients')","f960f977":"# set up constants for our coefficients \nINTERCEPT = -121.029547\nCOEF_HOLIDAY = -23.426176   # day is holiday or not\nCOEF_HOUR = 8.631624        # hour (0 to 23)\nCOEF_SEASON_1 = 3.861149    # 1:springer\nCOEF_SEASON_2 = -1.624812   # 2:summer\nCOEF_SEASON_3 = -41.245562  # 3:fall\nCOEF_SEASON_4 = 39.009224   # 4:winter\nCOEF_TEMP = 426.900259      # norm temp in Celsius -8 to +39","748d14d0":"np.mean(X_train['temp'])","bc9c0dbf":"# mean values\nMEAN_HOLIDAY = 0.0275   # day is holiday or not\nMEAN_HOUR = 11.6        # hour (0 to 23)\nMEAN_SEASON_1 = 1       # 1:spring\nMEAN_SEASON_2 = 0       # 2:summer\nMEAN_SEASON_3 = 0       # 3:fall\nMEAN_SEASON_4 = 0       # 4:winter\nMEAN_TEMP = 0.4967      # norm temp in Celsius -8 to +39\n\n\n# try predicting something - 9AM with all other features held constant\nrental_counts = INTERCEPT + (MEAN_HOLIDAY * COEF_HOLIDAY) \\\n    + (9 * COEF_HOUR) \\\n    + (MEAN_SEASON_1 * COEF_SEASON_1)  + (MEAN_SEASON_2 * COEF_SEASON_2) \\\n    + (MEAN_SEASON_3 * COEF_SEASON_3)  + (MEAN_SEASON_4 * COEF_SEASON_4) \\\n    + (MEAN_TEMP * COEF_TEMP)\n\nprint('Estimated bike rental count for selected parameters: %i' % int(rental_counts))    ","05f4baaf":"We can see their is no null values values and majority of the models in existence requires numerical data ans this is what we have, so  far we are goo to go\n\nlets get into a closer look at our outcome variable 'cnt' count of total rental bike.","6c7434cd":"Let's calculate the R^2 score of each feature to see how each helps us model bike demand (remember that an R^2 of 1 is perfect and an negative R^2 is bad):\n    ","be934d2d":"# Introduction\nIn this artical we\u2019re going to model the Bike Sharing Dataset from the Capital Bikeshare System using regression modeling and learn how variables such as temperature, wind, and time affect bicycle rentals in the mid-Atlantic region of the United States\n\nThe data is graciously made available through the UCI Machine Learning Repository of the University of California, Irvine (https:\/\/archive.ics.uci.edu\/ml\/datasets\/bike+sharing+dataset).\n\nThis project is done while reading and learning the book\nQuickly Turn Python ML Ideas into Web Applications on the Serverless Cloud\nAuthors: Amunategui, Manuel, Roopaei, Mehdi","40ef4bf6":"## Exploring the Data","6db611e7":"# Exploring bike sharing dataset\n\nBike sharing is a very demaded and popular but still a new and experimental process.Using a mobile phone, a rider can sign up online, download a phone application, locate bicycles, and rent one. their are many application in appstore and play store like\nWheelstreet\nWICKEDRIDE\nZoomcar for pedel\nand many more\n\nThis model creates an entire ecosystem where nobody needs to talk or meet in person to start enjoying this service\n\nAccording to Hadi Fanaee-T of the Laboratory of Artificial Intelligence and Decision Support (from the liner notes on the UCI Machine Learning Repository\u2019s Dataset Information):\n\non italics\n\nOpposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns [a] bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of [the] important events in the city could be detected via monitoring these data.1\n\nThe download contains two datasets: \u201chour.csv\u201d and \u201cday.csv.\u201d \nAttribute Information:\n\nBoth hour.csv and day.csv have the following fields, except hr which is not available in day.csv\n\n- instant: record index\n- dteday : date\n- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n- yr : year (0: 2011, 1:2012)\n- mnth : month ( 1 to 12)\n- hr : hour (0 to 23)\n- holiday : weather day is holiday or not (extracted from [Web Link])\n- weekday : day of the week\n- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n+ weathersit : \n- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)\/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)\/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n- hum: Normalized humidity. The values are divided to 100 (max)\n- windspeed: Normalized wind speed. The values are divided to 67 (max)\n- casual: count of casual users\n- registered: count of registered users\n- cnt: count of total rental bikes including both casual and registered","954286e6":"the feature 'hr' or we can say rental hours, clear shows peak office commute hours and afternoon rides are very popular bike time.\n\n## Preparing the Data for modeling\nIn most case to make a machine leaarning model we need to clean the data and here we have done the cleaning in order to be 'model ready',\nwe have alredy drop some unused feature\nwe have no null values.\n\n# Model with linear regression\n\nsimple model with all numerical variables","e0e81d23":"Sorted counts of the bike rentals reveal that the majority of the rentals happen in \nthe 0 - 400 ranges,\nvalues higher than those are rare or outliers\n\n## Quantitative Features vs. Rental Counts\n\nlets create scatter plots of all our float data types and compare them against rental counts to visualize potential relationship","b1e16691":"Set up constants for our coefficients","117f2912":"so by seeing the result we can conclude\ncnt - ranges between min of 1 and max of 977 which means that each hour has seen a minimum of 1 bike rental to max of 977 bike rentals that good right and\n\navg rental count  - 189.5\nas we are dealing with continuous numerical variables where linear regression is the right choice to train and predict bicycle rental count. \nbut you are free to apply any regression model to predict feel free to explore\n\n## Number Summary of the Bike Rental Count 'cnt' Feature","f3344440":"The score does improve using the 2nd and 4rd degree but then degrades beyond that point.\n\n### Model with simple feature enginneering","48afc774":"Let's look at the coefficients from our model and what they mean:","273a45f9":"as we can see both the feature 'temp' and 'atemp' have similar distribution and may present redundancy and even multicollinearity. to keep things clean we can drop the 'atemp' feature","84d98cf4":"so we get rmse of 143.08 and we will use that as our benchmark score.but let see if we can improv on this.\n\n#### Experimenting with Feature \nLet\u2019s see if we can get a better score by experimenting with a few different techniques, including polynomials, nonlinear modeling, and leveraging time-series.\n\n## Model using Polynomials - 2 & 4 degrees ","bb664aac":"As per the scatter plot we can see their is a liner relation between number of bike rented and temperature \nthe warmmer the temp the more bike get rented","e15bb357":"For 'hum' or humidity looks like a big blob throught the edges so show some sparseness.\nand for feature 'windspeed' shows inverse relationship with rentals\n\nfor conculsion we can say too much wind and bike rentals dont seem to mix","e6aeaae4":"## Analysing and cleaning of unused features\n\nBy using the head function we can learn that there are\ndates\nintegers\nfloats\n\ntheir are also some redundant datas like dteday which have alredy been categorized throught 'session', 'yr', 'mnth', 'hr'. so dteday feature can be drop out but we can use it little longer for exploration needs.\ntheir are others also like\ntemp - atemp\n\nwe can also drop 'casual' and 'registered' feature as they will not help us model demand from a single user behaviour","49f35e00":"# Predicting with coefficients - a lightweight way of getting estimates\n\nNow we can use the above coefficients and the regression equation:\n<B>y^ = b0 + bx1 + bxn<\/b>\n\nIt is important to remember that an x unit change in one feature works to estimate y when all other values are held constant. This means we need to get the mean value of all our features to plug into our regresssion equation.\n\n","e679c8cb":"## Let\u2019s Look at Categorical Features","2fc8b517":"# Understanding Bike Rental Demand with Regression Coefficients\n\nhere we will be building a simple and intutive way of interacting with environment factors such as temperature, wind, time and weather and anylyse how they efect the bike rentals ","87cfb81a":"#### Trying a Nonlinear Model\nAs a final modeling experiment, let\u2019s run our dummied data into a \u201cGradient Boosting Regressor\u201d model from sklearn. Switching from one model to another in the sklearn package is trivial, and we only need to load the appropriate model in memory and change two lines","5f27ffc3":"the above graph of 'weathersit' shows that people tends to rent more bike in clear weather ad 'season' show the fall is the top season to rent bike.","3ffb5318":"## Even More Complex Feature Engineering\u2014Leveraging Time-Series\n\nInfo - Here is one last feature engineering experiment; this idea comes from data scientists over at Microsoft.4 The data is a ledger of bike rentals over time, so it is a time-series dataset. Whenever your dataset records events over time, you want to take that into account as an added feature. For example, an event that happened an hour ago is probably more important than one that happened a year ago. Time can also capture trends, changing needs and perceptions, etc. We want to create features that capture all those time-evolving elements!\n\nFor each row of data, we\u2019ll add two new features: the sum of bicycle rentals for the previous hour, and the sum of bicycle rentals from two hours ago. The intuition here is that if we want to understand the current bicycling mood, we can start by looking at what happened an hour ago. If the rentals were great one hour ago, they\u2019re probably going\nto be good now. This time element can be seen as a proxy to prosperous or calamitous times, good or bad weather, etc.\nTo create a sum of bicycles per date and hour, we use Pandas extremely powerful \u201cgroupby()\u201d function. We extract three fields, \u201cdteday,\u201d \u201chr,\u201d and \u201ccnt\u201d and group the count by date and hour "}}