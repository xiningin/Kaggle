{"cell_type":{"58a944ba":"code","375f42a9":"code","8569fc06":"code","cb9d7fa2":"code","a0aa7658":"code","ff284cd5":"code","22bc77a2":"code","f133158f":"code","5234670d":"code","26f53c63":"code","539cfb19":"code","d07882c7":"code","13ee92f1":"code","4648bf6f":"markdown","67c58a9a":"markdown","6e813f12":"markdown","e1c5fe52":"markdown"},"source":{"58a944ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","375f42a9":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error","8569fc06":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","cb9d7fa2":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","a0aa7658":"cols_to_delete = []\nfor name, values in X_train.iteritems():\n    if X_train[name].isnull().sum() > 500000:\n        cols_to_delete.append(name)\n\nX_train.drop(cols_to_delete,axis=1,inplace=True)\nX_test.drop(cols_to_delete,axis=1,inplace=True)","ff284cd5":"X_train.isnull().sum()","22bc77a2":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train_pre, X_test_pre, y_train_pre, y_test_pre = train_test_split(X_train, y_train, test_size = .10, random_state = 0,stratify=y_train)\nlgb_train = lgb.Dataset(X_train_pre, y_train_pre)\nlgb_eval = lgb.Dataset(X_test_pre, y_test_pre, reference=lgb_train)\n","f133158f":"params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'num_leaves': 512,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.80,\n    'bagging_freq': 200,\n    'learning_rate': 0.05,\n    'min_data' : 500,\n    'max_bin': 1024,\n    'max_depth' : -1,\n    'verbose': -1\n}","5234670d":"print('Starting training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=2000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=100)","26f53c63":"sample_submission.head()","539cfb19":"print('Starting predicting...')\n# predict\ny_preds = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n# eval\/\n#print('The rmse of prediction is:', mean_squared_error(y_test_pre, y_pred) ** 0.5)","d07882c7":"sample_submission['isFraud'] = y_preds","13ee92f1":"sample_submission.to_csv('simple_lgbm.csv')","4648bf6f":"**Delete columns having more than 500K nan values**","67c58a9a":"Thanks to other kernels for below load and merge ideas","6e813f12":"**Set up parameters for LGBM**","e1c5fe52":"**Split the data 90-10 for training**"}}