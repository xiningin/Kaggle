{"cell_type":{"afd2f1ad":"code","935ef78f":"code","7f83ffd0":"code","daf09101":"code","679541ab":"code","aba33279":"code","9de6e505":"code","cbbae78d":"code","eb08ac67":"code","f458e2e6":"code","f3cf1b33":"code","bc58b729":"code","3550319b":"code","66f7a0b5":"code","51ec3b4c":"code","75c15fde":"code","e797c29a":"code","b6c7c3c5":"code","35497e02":"code","25c8d24b":"code","97978131":"code","fe7dc797":"code","1be0d5f3":"code","103a1eb5":"code","e1f210e5":"code","f5a5a3c1":"code","da1a6e35":"code","8145c383":"code","929c5dde":"code","996820c9":"code","dea33642":"code","e9be8ba3":"code","0e39a7db":"code","af871474":"code","ee44e029":"code","49094eb4":"code","312e1e99":"code","7bba8e9c":"code","fcbfa034":"code","0382d65f":"code","53c3f8b3":"code","d7c97c24":"code","e64b7014":"code","7087aec6":"code","f589cb2f":"code","32d15f09":"code","0ec2ea18":"code","0ffe6796":"code","50fa1200":"code","5a4784c0":"code","46adbedc":"code","ca164d82":"code","944e1fe5":"code","919c1c56":"code","f3a98cec":"code","9d5a0962":"code","254ecab2":"code","70e7c4b4":"code","905d6af5":"code","fde1ef6a":"code","0affa2f4":"code","300646c3":"code","aabac204":"code","98b34538":"code","3127a258":"code","51fd3cf3":"code","9aefbdff":"code","c260c868":"code","85079ac3":"code","d8bea09b":"code","e823fbcd":"code","f81ccd46":"code","83b077d3":"code","0f490798":"code","ecdcb9fa":"code","645e7729":"code","b23f4d31":"code","b00630e9":"code","4ed4a1d6":"code","df56e407":"code","8561e0fa":"code","f356e786":"code","ee348cc2":"code","ea1a76e6":"code","b303aec1":"markdown","04407f13":"markdown","ec0de446":"markdown","7fb62f41":"markdown","7d5f74e6":"markdown","fb834af5":"markdown","b69f80f1":"markdown","ce738faf":"markdown","abb07287":"markdown","ba21b1f2":"markdown","7018754e":"markdown","9d55f4ac":"markdown","a6c7f427":"markdown","b2cb587d":"markdown","fee407e5":"markdown","69c6d6f9":"markdown","98d370e8":"markdown","65a85378":"markdown","474f1cf8":"markdown","c1d5d180":"markdown","6d2fc81c":"markdown","10c423a5":"markdown","fffcd925":"markdown","89360a13":"markdown","e88ec9da":"markdown","44452e0c":"markdown","40d53f7b":"markdown","acc029d8":"markdown","bd54cf95":"markdown","edbd513e":"markdown","aa5800b0":"markdown","67486539":"markdown","e743eaa4":"markdown","6a531c26":"markdown","dc67e8b7":"markdown","c2cbe910":"markdown","68b14436":"markdown","496d4a57":"markdown","21203e00":"markdown","175cc827":"markdown","eaee4835":"markdown","d86579be":"markdown","9d9aa1a8":"markdown","9d094a3b":"markdown","7b518e4a":"markdown","0e73735d":"markdown","bf329c53":"markdown","512a9b04":"markdown","2a2ebf97":"markdown","af7ecf8b":"markdown","ea58caf6":"markdown","5056b92d":"markdown","925d27f6":"markdown","ca5b1757":"markdown","f7c32d8a":"markdown","86d771e9":"markdown","f9d84606":"markdown","2a0a55f4":"markdown","60e565ed":"markdown","f0606232":"markdown","80260d32":"markdown","8306c74f":"markdown","8370f05b":"markdown","1e443b53":"markdown","563dc44d":"markdown","ee0f0e33":"markdown","2cd76a64":"markdown","ffbf173c":"markdown","03b59d98":"markdown","d0af4c70":"markdown","18cee8d3":"markdown","91cd9f91":"markdown","d106215e":"markdown","52b8a6af":"markdown"},"source":{"afd2f1ad":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#%matplotlib inline #Jupyter's own backend\nimport os\nimport seaborn as sns\nimport pickle\nimport joblib\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/california-housing-prices'):\n    for filename in filenames:\n        csv_path = os.path.join(dirname, filename)","935ef78f":"housing = pd.read_csv(csv_path)\nhousing.head()","7f83ffd0":"housing.info()","daf09101":"housing.describe()","679541ab":"housing.hist(bins = 50, figsize = (20,15))\nplt.show()","aba33279":"housing[\"ocean_proximity\"].value_counts()","9de6e505":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 0)","cbbae78d":"housing[\"median_income\"].hist(bins = 50)","eb08ac67":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n      bins = [0., 1.5, 3.0, 4.5, 6, np.inf],\n      labels = [1, 2, 3, 4, 5])","f458e2e6":"housing[\"income_cat\"].hist(bins = 50)","f3cf1b33":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 0)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","bc58b729":"housing[\"income_cat\"].value_counts()\/len(housing)","3550319b":"strat_test_set[\"income_cat\"].value_counts()\/len(strat_test_set)","66f7a0b5":"for dataset in (strat_train_set, strat_test_set, housing):\n    dataset.drop(\"income_cat\", axis = 1, inplace = True)","51ec3b4c":"housing = strat_train_set.copy()","75c15fde":"housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4,\n            s = housing[\"population\"]\/100, label = \"population\",\n            c = housing[\"median_house_value\"], cmap = \"jet\", colorbar = True, figsize = (10,7))\nplt.legend()","e797c29a":"sns.heatmap(housing.corr())","b6c7c3c5":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"longitude\", \"latitude\", \"population\", \"median_income\"]\n\nscatter_matrix(housing[attributes], figsize = (15,7))","35497e02":"housing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", figsize = (15,7))","25c8d24b":"housing[\"median_house_value\"].value_counts(sort = \"desc\")","97978131":"capped_val_remove = [500001.0, 137500.0, 162500.0, 112500.0, 225000.0, 187500.0, 350000.0, 87500.0, 100000.0, 275000.0,\n                    150000.0, 175000.0]\n\nfor value in capped_val_remove:\n    housing = housing[housing.median_house_value != value]","fe7dc797":"housing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", figsize = (15,7))","1be0d5f3":"housing[\"bedrooms_per_household\"] = housing[\"total_bedrooms\"]\/housing[\"households\"]\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]\/housing[\"households\"]","103a1eb5":"sns.heatmap(housing.corr())","e1f210e5":"housing = strat_train_set.drop(\"median_house_value\", axis = 1)\nhousing_labels = strat_train_set[\"median_house_value\"]","f5a5a3c1":"housing.isna().any()","da1a6e35":"housing[housing.isna().any(axis = 1)]","8145c383":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = \"median\")\n\nhousing_numerical_attributes = housing.drop(\"ocean_proximity\", axis = 1)\nimputer.fit(housing_numerical_attributes)","929c5dde":"imputer.statistics_","996820c9":"X = imputer.transform(housing_numerical_attributes)\nX","dea33642":"housing_tr = pd.DataFrame(X, columns = housing_numerical_attributes.columns, index = housing_numerical_attributes.index)","e9be8ba3":"housing_tr","0e39a7db":"housing_categorical = housing[[\"ocean_proximity\"]]","af871474":"housing_categorical.value_counts()\n\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nhousing_categorical_encoded = ordinal_encoder.fit_transform(housing_categorical)\nhousing_categorical_encoded","ee44e029":"housing_categorical_encoded[:10]","49094eb4":"ordinal_encoder.categories_","312e1e99":"from sklearn.preprocessing import OneHotEncoder\none_hot_encoder = OneHotEncoder()\nhousing_categorical_1hot =  one_hot_encoder.fit_transform(housing_categorical)\nhousing_categorical_1hot","7bba8e9c":"from sklearn.base import BaseEstimator, TransformerMixin\nrooms, bedrooms, population, households = 3,4,5,6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        rooms_per_household = X[:, rooms] \/ X[:, households]\n        population_per_household = X[:, population] \/ X[:, households]\n        \n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms] \/ X[:, rooms]\n            \n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        \n        else:\n            return np.c_[X, rooms_per_household, population_per_household]","fcbfa034":"attribute_add = CombinedAttributesAdder(add_bedrooms_per_room = False)\nhousing_added_attributes = attribute_add.transform(housing.values)","0382d65f":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_att_pipeline = Pipeline(\n[\n    ('imputer', SimpleImputer(strategy = \"median\")),\n    ('attributes_adder', CombinedAttributesAdder()),\n    ('scaler', StandardScaler())\n]\n)","53c3f8b3":"housing_numerical_transformator = num_att_pipeline.fit_transform(housing_numerical_attributes)","d7c97c24":"from sklearn.compose import ColumnTransformer\n\nnumerical_attributes = list(housing_numerical_attributes)\ncategorical_attributes = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer(\n[\n    (\"numerical\", num_att_pipeline, numerical_attributes),\n    (\"categorical\", OneHotEncoder(), categorical_attributes)\n]\n)\n\nhousing_data_prepared = full_pipeline.fit_transform(housing)","e64b7014":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\n\nlin_reg.fit(housing_data_prepared, housing_labels)","7087aec6":"housing_predictions_lr = lin_reg.predict(housing_data_prepared)","f589cb2f":"housing_predictions_lr","32d15f09":"from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(housing_labels, housing_predictions_lr)\nrmse = np.sqrt(mse)","0ec2ea18":"rmse","0ffe6796":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_data_prepared, housing_labels)","50fa1200":"housing_predictions_tree = tree_reg.predict(housing_data_prepared)","5a4784c0":"mse_tree = mean_squared_error(housing_labels, housing_predictions_tree)\nrmse_tree = np.sqrt(mse_tree)","46adbedc":"rmse_tree","ca164d82":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_data_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)","944e1fe5":"tree_rmse_scores = np.sqrt(-scores)","919c1c56":"def display_scores(scores):\n    print(\"Score: \", scores)\n    print(\"Score Mean: \", scores.mean())\n    print(\"Score Standard Deviation: \", scores.std())","f3a98cec":"display_scores(tree_rmse_scores)","9d5a0962":"scores_lr = cross_val_score(lin_reg, housing_data_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)","254ecab2":"lr_rmse_scores = np.sqrt(-scores_lr)","70e7c4b4":"display_scores(lr_rmse_scores)","905d6af5":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor()\n\nrf_reg.fit(housing_data_prepared, housing_labels)","fde1ef6a":"rf_predictions = rf_reg.predict(housing_data_prepared)","0affa2f4":"scores_rf = cross_val_score(rf_reg, housing_data_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)\nrf_rmse_scores = np.sqrt(-scores_rf)\ndisplay_scores(rf_rmse_scores)","300646c3":"import pickle, joblib\n\njoblib.dump(lin_reg ,\"linear_regression_model.pkl\")\njoblib.dump(tree_reg ,\"decision_tree_model.pkl\")\njoblib.dump(rf_reg ,\"random_forest_model.pkl\")\n","aabac204":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30, 60], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [8, 10, 12]}\n]\n\nparam_grid_svr = [\n    {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree' : [4, 5, 6], 'C' : [1, 1.5, 2]}\n]","98b34538":"from sklearn.svm import SVR\nsvr = SVR()","3127a258":"#grid_search = GridSearchCV(rf_reg, param_grid, cv = 5, scoring = 'neg_mean_squared_error', return_train_score = True)\n#grid_search.fit(housing_data_prepared, housing_labels)\n\ngrid_search_svr = GridSearchCV(svr, param_grid_svr, cv = 5, scoring = 'neg_mean_squared_error', return_train_score = True)\ngrid_search_svr.fit(housing_data_prepared, housing_labels)","51fd3cf3":"#grid_search.best_params_\ngrid_search_svr.best_params_","9aefbdff":"rf_reg = RandomForestRegressor(max_features = 6, n_estimators = 60)\n\nrf_reg.fit(housing_data_prepared, housing_labels)\nrf_predictions = rf_reg.predict(housing_data_prepared)","c260c868":"scores_rf = cross_val_score(rf_reg, housing_data_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 5)\nrf_rmse_scores = np.sqrt(-scores_rf)\ndisplay_scores(rf_rmse_scores)","85079ac3":"svr = SVR(kernel = , degree = , C = )\nsvr.fit(housing_data_prepared, housing_labels)\nsvr_predictions = svr.predict(housing_data_prepared)","d8bea09b":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 60, stop = 100, num = 5)]\nmax_features = [int(x) for x in np.linspace(start = 8, stop = 24, num = 5)]\nbootstrap = [True, False]\nmin_samples_split = [2, 5]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features' : max_features,\n               'bootstrap' : bootstrap,\n               'min_samples_split' : min_samples_split\n              }\n\nrf = RandomForestRegressor()\n\n#random_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, scoring = 'neg_mean_squared_error', \n#                                   n_iter = 5, cv = 5, verbose = 2, random_state = 0,\n#                                   n_jobs = -1, return_train_score = True)\n\n#random_search.fit(housing_data_prepared, housing_labels)","e823fbcd":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","f81ccd46":"categorical_attributes = list(full_pipeline.named_transformers_[\"categorical\"].categories_[0])\ncategorical_attributes","83b077d3":"numerical_attributes","0f490798":"added_attributes = [\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\"]","ecdcb9fa":"attributes = numerical_attributes + added_attributes + categorical_attributes","645e7729":"sorted(zip(feature_importances, attributes), reverse = True)","b23f4d31":"data_hot_encoded = pd.DataFrame(housing_data_prepared, index = housing.index)","b00630e9":"data_hot_encoded.drop(data_hot_encoded.columns[13], axis = 1, inplace = True)\ndata_hot_encoded.drop(data_hot_encoded.columns[14], axis = 1, inplace = True)\ndata_hot_encoded.drop(data_hot_encoded.columns[15], axis = 1, inplace = True)","4ed4a1d6":"data_hot_encoded","df56e407":"strat_test_set","8561e0fa":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis = 1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)","f356e786":"final_predictions = final_model.predict(X_test_prepared)","ee348cc2":"final_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)","ea1a76e6":"from scipy import stats\nconfidence = 0.95\n\nsquared_errors = (final_predictions - y_test)**2\n\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc = squared_errors.mean(),\n                         scale = stats.sem(squared_errors)))","b303aec1":"# Preparing the data for Machine Learning algorithms\n****","04407f13":"# Importing the data\n****\nEach row represents one district.","ec0de446":"**Decision Tree**\n****","7fb62f41":"Transformation pipeline for numerical attributes:\n****","7d5f74e6":"Fitting the Random Forest regressor to the training set for the best parameters found by grid search:","fb834af5":"Context\n\nThis is the dataset used in the second chapter of Aur\u00e9lien G\u00e9ron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n\nThe data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.","b69f80f1":"# About the dataset","ce738faf":"Housing prediction (also marked as y_pred in other notebooks) is a predictions vector:","abb07287":"We want to have an idea how precise this estimate is - computing **95% confidence interval** for the generalization error","ba21b1f2":"# Importing the libraries\n****","7018754e":"New data frame with replaced Na values:","9d55f4ac":"# Evaluating the model on the test set\n****","a6c7f427":"Decision Tree RMSE Mean is higher than Linear Regression RMSE Mean, which shows the Decision Tree model performs worse than Linear Regression model.","b2cb587d":"Since the Decision Tree shows the RMSE of 0.0, the validation is needed. We will use K-Fold cross validation:","fee407e5":"# California Housing Prices\n****","69c6d6f9":"Added features:\n****","98d370e8":"From this image, we can see that housing prices are related to the location and the population density. However, this isn't the rule always, as there is housing in the north close to the ocean but with lower price.","65a85378":"Next, scatter plots of the few attributes most correlated to the median house value will be created (pandas' scatter_matrix).","474f1cf8":"Now let's see the correlation matrix and if there are any bigger correlation factors:","c1d5d180":"Creating the clean training_set and separating the predictors and labels:","6d2fc81c":"# Creating a test set","10c423a5":"**Creating new variables - attvalues combinations**\n****","fffcd925":"Categorical features:\n****","89360a13":"Numerical features:\n****","e88ec9da":"**Transformation pipeline**\n****\n\nTransformation pipeline is used to get all the sequences of transformations on columns. This way, we will replace steps such as imputing the Na values, combining attributes and scaling into one pipeline.","44452e0c":"**Random Forest Regression** hyperparameters:\n\nn_estimators = number of trees in the foreset\n\nmax_features = max number of features considered for splitting a node\n\nmax_depth = max number of levels in each decision tree\n\nmin_samples_split = min number of data points placed in a node before the node is split\n\nmin_samples_leaf = min number of data points allowed in a leaf node\n\nbootstrap = method for sampling data points (with or without replacement)\n****\nclass sklearn.ensemble.RandomForestRegressor(\n**n_estimators**=100,\n**criterion**='mse',\n**max_depth**=None,\n**min_samples_split**=2,\n**min_samples_leaf**=1,\n**min_weight_fraction_leaf**=0.0,\n**max_features**='auto',\n**max_leaf_nodes**=None,\n**min_impurity_decrease**=0.0,\n**min_impurity_split**=None,\n**bootstrap**=True,\n**oob_score**=False,\n**n_jobs**=None,\n**random_state**=None,\n**verbose**=0,\n**warm_start**=False,\n**ccp_alpha**=0.0,\n**max_samples**=None)\n****","40d53f7b":"s - the **radius** of circles represents the population size\n\nc - the **color** of the circles represents the price","acc029d8":"**Randomized search**\n****","bd54cf95":"Income category proportions in the overall dataset:","edbd513e":"Income category proportions in the test set:","aa5800b0":"**Linear Regression**\n****","67486539":"The dataset isn't too large -> we can compute standard correlation coefficient (Pearson's) between every pair of attributes:","e743eaa4":"**Visualize the geographical data**\n****","6a531c26":"**Custom transformer** that adds the combined attributes (rooms_per_household, bedrooms_per_room, population_per_household)\n* BaseEstimator - base class\n* Transformermixin - base class\n* CombinedAttributesAdder - custom transformer with add_bedrooms_per_room hyperparameter used to see if the algorithm works better with or without it","dc67e8b7":"Predicting the test set labels:","c2cbe910":"**Data cleaning - missing values**\n****","68b14436":"**Support Vector Regression** hyperparameters:\n\nclass sklearn.svm.SVR(*, **kernel**='rbf', **degree**=3, **gamma**='scale', **coef0**=0.0, **tol**=0.001, **C**=1.0, **epsilon**=0.1, **shrinking**=True, **cache_size**=200, **verbose**=False, **max_iter**=- 1)\n\nkernel {\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019 - Specifies the kernel type to be used in the algorithm. It must be one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a callable. If none is given, \u2018rbf\u2019 will be used. If a callable is given it is used to precompute the kernel matrix.\n\ndegree, default=3 - Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n\nC, default=1.0 - Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n\n****","496d4a57":"**Grid Search**\n****","21203e00":"# Data discovery and visualization\n****","175cc827":"Scores of cross validation mean and std (standard deviation shows how precise the estimate of the performance of the model is):","eaee4835":"The imputer stored median values of every attribute in statistics_ instance variable","d86579be":"Mean of RMSE score of Ranfom Forest is 50,204, which means the Random Forest model performs better than Linear Regression or Decision Tree.\nSaving the models into pickle and joblib:","9d9aa1a8":"This way we see that the proportions in the test set generated using stratified sampling and the overall dataset are almost identical.\nNow we can remove the income_cat attribute:","9d094a3b":"Fitting the Support Vector Regressor to the best hyperparameters found by grid search:","7b518e4a":"\"About the dataset\" section is used for getting the insights about the housing dataset and patterns in data.\nThe data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self explanitory:\n\nlongitude\n\nlatitude\n\nhousingmedianage\n\ntotal_rooms\n\ntotal_bedrooms\n\npopulation\n\nhouseholds\n\nmedian_income\n\nmedianhousevalue\n\nocean_proximity","0e73735d":"One transformer for all the columns would be even more useful (transformations pipeline with multiple transformations used on numerical attributes and OneHotEncoder used on categorical attributes), so here ColumnTransformer comes into play:","bf329c53":"**Random Forest**\n****","512a9b04":"To Fine-tune the model, we can use Grid Search,Randomized Search or Ensemble methods.","2a2ebf97":"With this information, we will drop least important features:","af7ecf8b":"From the heatmap of a correlation matrix, it is visible that median_house_value:\n* has higher **negative** correlation to bedrooms_per_household than with total_bedrooms in a district. We can say that the houses with more bedrooms cost less\n\n* has high **negative** correlation to bedrooms_per_room, so the houses with higher bedrooms\/room ratio are cheaper\n\n* has higher **positive** correlation to rooms_per_household than to total_rooms in a district. Houses with more rooms (bigger houses) cost more.","ea58caf6":"housing_labels - Linear Regression parameter of target attribute median_house_value","5056b92d":"**Handling text and categorical variables**\n****","925d27f6":"Let's take a look into the only non-numerical (ocean_proximity) attribute:","ca5b1757":"housing_categorical_encoded variable has encoded categories of ocean_proximity, however, those categories aren't more similar if closer to one another, so one-hot encoding will be used **instead** of ordinal encoding:","f7c32d8a":"**Model evaluation**\n****","86d771e9":"**Income category proportions in the overall dataset vs test set**\n****","f9d84606":"The scatterplot after removing the capped values:","2a0a55f4":"K-Fold Cross Validation for Random Forest model:","60e565ed":"Test set should be representative of overall population.\nSince median_income attribute is important predictor of median prices, we will see if this part of the test set is representative of overall population.\nSo we will create new income category attribute (income_cat) that will hold median income categories:","f0606232":"# Feature importance\n****","80260d32":"There are 206440 entries, ocean_proximity is object data type (it can hold any value)","8306c74f":"**Stratified sampling based on the income category**\n****\n\nCreating the classes","8370f05b":"# Select and train a model\n****","1e443b53":"In comparison, k-fold cross validation for linear regression model:","563dc44d":"Next, we will sort the attributes with corresponding importances:\n\n**To get the importance for each feature name, just iterate through the columns names and feature_importances together (they map to each other)*","ee0f0e33":"The horizontal lines are result of data capping and they will be removed so that the algorithm doesn't reproduce these data quirks.","2cd76a64":"Typical prediction error of $68,284 is not very satisfying - model is underfitting the training data.\nLet's try with a more complex machine learning model - Decision Tree:","ffbf173c":"**K-Fold cross validation**\n****","03b59d98":"Since the data doesn't have many outliers and the task is regression task, we will use RMSE (Root Mean Squared Error):","d0af4c70":"# Fine-tuning the model\n****","18cee8d3":"It is visible from the correlation matrix that the median house value (target variable) is negatively correlated to latitude and population: the norther the house, the smaller the value. Also, median house value is positively correlated to median income, meaning the higher the median income in the district, the higher the median house value.","91cd9f91":"There are a couple of new attributes we can create from existing ones **for every district**:\n* Number of bedrooms per household\n* Number of rooms per household\n* Number of bedrooms per room\n* Number of people (population) per household","d106215e":"**Median house value and median income**\n****","52b8a6af":"**Looking for correlations**\n****"}}