{"cell_type":{"126174e6":"code","8bc7d4e8":"code","f0e05fc5":"code","55645846":"code","ae1cddf4":"code","d65502ae":"code","afc812e7":"code","21622c01":"code","1f248e21":"code","5d5253f6":"code","e1c617f3":"code","c0cc9795":"code","bbc0d1df":"code","c5e857bc":"markdown","51e1baa5":"markdown","358805e9":"markdown","7e7eb374":"markdown","f60c2c14":"markdown","8f1d2655":"markdown"},"source":{"126174e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8bc7d4e8":"import matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, AveragePooling2D, BatchNormalization, LeakyReLU, Concatenate\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","f0e05fc5":"train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","55645846":"img_rows = 28\nimg_cols = 28\nnum_classes = 10\n\ndef data_prep(raw):\n    out_y = to_categorical(raw.label, num_classes)\n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:]\n    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n    out_x = x_shaped_array \/ 255\n    return out_x, out_y","ae1cddf4":"X_train, y_train = data_prep(train)\nX_test, y_test = data_prep(test)","d65502ae":"print(\"X_train.shape:\", X_train.shape)\nprint(\"y_train.shape\", y_train.shape)","afc812e7":"es = EarlyStopping(monitor='val_accuracy', mode='max', verbose = 1, patience=20)\nmc = ModelCheckpoint('best_cnn_model.h5', monitor='val_accuracy', mode='max', verbose = 1, save_best_only=True)","21622c01":"# build model\ni = Input(shape=(28,28,1))\n\nx = Conv2D(32, kernel_size=(3, 3), strides=1, padding =\"same\")(i)\nx = LeakyReLU(alpha=0.1)(x)\nx = BatchNormalization()(x)\nx = Conv2D(32, (3, 3), padding =\"same\")(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2,2))(x)\nx = Dropout(0.5)(x)\n\nx = Conv2D(64, kernel_size=(3, 3), strides=1, padding =\"same\")(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = BatchNormalization()(x)\nx = Conv2D(64, (3, 3), padding =\"same\")(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2,2))(x)\nx = Dropout(0.5)(x)\n\nx = Flatten()(x)\n\ny = Conv2D(32, kernel_size=(3, 3), strides=1, padding =\"same\")(i)\ny = LeakyReLU(alpha=0.1)(y)\ny = BatchNormalization()(y)\ny = Conv2D(32, (3, 3), padding =\"same\")(y)\ny = LeakyReLU(alpha=0.1)(y)\ny = BatchNormalization()(y)\ny = AveragePooling2D(pool_size=(2,2))(y)\ny = Dropout(0.5)(y)\n\ny = Conv2D(64, kernel_size=(3, 3), strides=1, padding =\"same\")(y)\ny = LeakyReLU(alpha=0.1)(y)\ny = BatchNormalization()(y)\ny = Conv2D(64, (3, 3), padding =\"same\")(y)\ny = LeakyReLU(alpha=0.1)(y)\ny = BatchNormalization()(y)\ny = AveragePooling2D(pool_size=(2,2))(y)\ny = Dropout(0.5)(y)\n\ny = Flatten()(y)\n\nxy = Concatenate()([x, y])\nxy = Dropout(0.5)(xy)\nxy = Dense(256, activation='relu')(xy)\nxy = Dropout(0.5)(xy)\nxy = Dense(10, activation='softmax')(xy)\n\nmodel = Model(inputs = i, outputs = xy, name = 'max_avg')","1f248e21":"model.summary()","5d5253f6":"# run model\nmodel.compile(optimizer='Adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, \n          validation_data=(X_test, y_test), \n          batch_size = 128, epochs = 200, \n          callbacks = [es, mc])","e1c617f3":"def plot_model(history): \n    fig, axs = plt.subplots(1,2,figsize=(16,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy'], 'c') \n    axs[0].plot(history.history['val_accuracy'], 'm') \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss'], 'c') \n    axs[1].plot(history.history['val_loss'], 'm') \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper right')\n    plt.show()","c0cc9795":"plot_model(model.history)","bbc0d1df":"saved_model = load_model('best_cnn_model.h5')\ntest_loss, test_acc = saved_model.evaluate(X_test,  y_test, verbose=0)\nprint('Test Accuracy:', round(test_acc, 2))","c5e857bc":"# Run model","51e1baa5":"# Evaluate model","358805e9":"# Read data","7e7eb374":"# Import packages","f60c2c14":"# Data processing","8f1d2655":"# Build model"}}