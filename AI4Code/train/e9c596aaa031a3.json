{"cell_type":{"8b30eba8":"code","d66c8b39":"code","d937264e":"code","cff9d327":"code","4455bc37":"code","5e488504":"code","a2e06785":"markdown","e9aea191":"markdown","0e8df7b5":"markdown","04b4ae48":"markdown","74423675":"markdown","767fa625":"markdown","975ff39d":"markdown","cc2f4711":"markdown","90cbaa33":"markdown","0dd3d2b1":"markdown","9ecacd56":"markdown","f6729c28":"markdown","b988c4f3":"markdown","aab5eb09":"markdown","f3999431":"markdown","0af8d3bc":"markdown"},"source":{"8b30eba8":"\n# matplotlib library\n%matplotlib inline\n\n# remove unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# increase number of columns shown via pandas\npd.options.display.max_columns = 200\n\n# increase number of rows shown via pandas\npd.options.display.max_rows = 50\n\n# matplotlib default figure size & dpi\nplt.rcParams.update({'figure.figsize': (12, 5), 'figure.dpi': 100})","d66c8b39":"\n# CORRELATION MATRIX\ncorrmat = train.corr()\ncorrmat = np.tril(corrmat)\ncorrmat[corrmat==0] = None\ncorrmat = corrmat.round(1)\nlabels = train.select_dtypes(include='number').columns.values\nf, ax = plt.subplots(figsize=(15, 8))\nsns.heatmap(corrmat, annot=True, vmax=0.8,vmin=-0.8, cmap='seismic_r', xticklabels=labels,yticklabels=labels)\n\nplt.show()\n\n\n# HYPERTUNING\ngs = GridSearchCV(Ridge(random_state=123), param_grid={'alpha':np.linspace(1,100,100)}).fit(X_train, y_train)\ngs.best_params_['alpha']\n\n\n\n# PEARSON CORRELATION\nfeat_list = []\nfor feature in top_corr_features:\n    cor, p = scipy.stats.pearsonr(train[feature], train['SalePrice'])\n    feat_list.append([feature, cor, p])\nfeat_list.sort(key=lambda x: x[2],reverse=True)\n\nfeat_list = np.array(feat_list)\n\ndf = pd.DataFrame(feat_list, index=None,columns=['feature','correlation','p-value'])\ndf['feature'] = df['feature'].astype(str)\ndf['correlation'] = pd.to_numeric(df['correlation'])\ndf['p-value'] = pd.to_numeric(df['p-value'])\n\ndf.head()\n\n\n# CORRELATION CHART SET\nsns.set()\ncols = ['TargetCol', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(kind='reg', data=train[cols], size=3)\nplt.show()\n\n\n# SKEW, CURTOIS & DISTRIBUTION\nfrom scipy.stats import norm, skew\nsns.distplot(train['TargetCol'],fit=scipy.stats.norm)\n(mu, sigma) = norm.fit(train['TargetCol'])\nprint('mu = {}; sigma={}'.format(mu, sigma))\n\n\n# PROBABILITY PLOT\nfig = plt.figure()\nres2 = scipy.stats.probplot(train['TargetCol'].apply(np.log1p), plot=plt)\nplt.show()\n\n\n# SPLIT FEATURES\ncategorical_features = train.select_dtypes(include=['object']).columns\nnumerical_features = train.select_dtypes(exclude=['object']).columns\nnumerical_features = numerical_features[numerical_features != 'TargetCol']\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]\n\n\n# GET SKEWED FEATURES\nfrom scipy.stats import skew\nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)\n\n\n# APPLY LOG ON SKEWED FEATURES\nskewed_cols = skewness[abs(skewness)>0.5].index\ntrain_num[skewed_cols] = train_num[skewed_cols].apply(np.log1p)\n\n\n# ONE HOT ENCODING ON CATEGORICAL FEATURES\ntrain_cat = pd.get_dummies(train_cat)\n\n\n# SPLIT DATA FOR MODELING\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.3)\n\n\n# ROOT MEAN SQUARE DEVIATION (error)\n# example model: RandomForestRegressor(n_estimators=5)\n# scoring options >>\n# classification: precission (max=1), accuracy (max=1 -best), recall (max=1 -best), f1 (min=0, max=1 - best), roc_auc (max=1)\n# Recall = Given a class, will the classifier be able to detect it?\n# Precision = Given a class prediction from a classifier, how likely is it to be correct?\n# F1 Score = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n# Roc_Auc = What is ability of model to differentiate classes?\n\n# regression: neg_mean_squared_error (best: 0), r2 (best: 1), neg_mean_absolute_error (best: 0)\n\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\nkf = KFold(5, shuffle=True)\nrmse = np.sqrt(-cross_val_score(model, x_train, y_train, scoring='neg_mean_squared_error', cv=kf))\nprint(\"RMSE is {}\".format(rmse.mean()))\n\n\ndef rmse_cv(model,X,y):\n    kf = KFold(5, shuffle=True).get_n_splits(X.values)\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse\n\n\n\n# LINEAR REGRESSION\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train, y_train)\n\ny_train_pred = lr.predict(x_train)   # compare with y_train to get residuals, errors\ny_test_pred = lr.predict(x_test)     # compare with y_test to get residuals, errors\n\n\n# RIDGE (regularization)\nridge_pre = RidgeCV(alphas=list(np.lin(0.001, 20, 50)))\nridge_pre.fit(x_train, y_train)\nbest_alpha_pre = ridge_pre.alpha_   # found best alpha, now do the same with linspace around best alpha\n\nridge = RidgeCV(alphas=list(np.linspace(best_alpha_pre-1.5, best_alpha_pre+1.5, 50)), cv=5)   # with folds\nridge.fit(x_train, y_train)\nbest_alpha = ridge.alpha_           # itterative best alpha\n\ncoef = pd.Series(ridge.coef_, index = X_train.columns)\ncoef[coef != 0]                     # get coeficient not dropped (zeroed) by ridge - important features\n\n\n\n\n# REGRESSION MODELS\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, ARDRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.svm import SVR\n\n\n# CLASSIFICATION MODELS\n# names = [\"Nearest Neighbors\",\"Linear SVM\",\"RBF SVM\",\"Gaussian Process\",\"Decision Tree\",\"Random Forest\",\"Neural Net\",\"AdaBoost\",\"Naive Bayes\", \"QDA\"]\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# CLASSIFICATION EVALUATION\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# SIMPLE CONFUSION MATRIX:\ndf = pd.DataFrame({'true': y_test, 'pred': y_test_hat})\nconfusion_matrix = pd.crosstab(df['true'], df['pred'], rownames=['True'], colnames=['Predicted'])\nprint(confusion_matrix)\n\n\n# CLUSTERING - KMEANS\n# find best number of clusters - elbow method - choose highest jumps\nfrom sklearn.cluster import AgglomerativeClustering, DBSCAN   # hierarchical, density based\nfrom sklearn.cluster import KMeans\n\ninertia = []\nfor i in range(1, 13):\n    km = KMeans(n_clusters = i).fit(df)\n    inertia.append(km.inertia_)\n    \nsns.lineplot(x=list(range(1, 13)), y=inertia)\n\n\n# generate clusters\nkm = KMeans(n_clusters = 5).fit(df)\ndf['Clusters'] = km.labels_\n\ndb = DBSCAN(eps=0.5, min_samples=3).fit(df)\ndf['Clusters'] = db.labels_\n\n\n# evaluation\nfrom sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n\n\n# SCALE VARIABLE TO -1, 1 RANGE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n\nx=df.loc[:, num_features].values\nx = StandardScaler().fit_transform(x)\ndf[num_features] = x\n\n\n# OUTLIER ALGORITHMS\n# define outlier\/anomaly detection methods to be compared\n# contamination - percentage of outliers in dataset\n# use fit, predict to find out outliers\nEllipticEnvelope(contamination=outliers_fraction)\nsvm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)\nIsolationForest(contamination=outliers_fraction)\nLocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction)\n\n\n# PRINCIPAL COMPONENT ANALYSIS (PCA) - DIMENSION REDUCTION\n# require variables to be scaled !!!\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_data = pca.fit_transform(x)\npca_df = pd.DataFrame(pca_data, columns=['pca1', 'pca2'])\n\npca.explained_variance_ratio_\t\t\t\t# sum gives how much variance was explained, max is 1 (100%)\npca = PCA(0.95)\t\t\t\t\t\t\t\t# alternative initiation, will generate as much vectors (columns) to reach 95% of explained variance\n\n\n\n# IMPUTE MISSING VALUES - NOT WITH MEAN, NUMERICAL\nfrom sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n\nimp = IterativeImputer(estimator=RandomForestRegressor(n_estimators=5), max_iter=5)      # it's good to limit as will take a lot of time\n\n# note you include more columns to get better estimation (not just null), not-null values are not touched\ncols = ['col1', 'col2', 'col3']\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \ndf[cols] = pd.DataFrame(imp.fit_transform(df[cols]), columns=cols)\n\n\n\n# CREATE CUSTOM DATASET\nfrom sklearn.datasets import make_moons, make_circles, make_classification, load_digits, load_iris\nX, y = make_classification(n_features=8, n_redundant=3, n_informative=4, n_clusters_per_class=2)\ndigits = datasets.load_digits()\niris = datasets.load_iris()\n\n\n# GENERATE CLASSIFICATION DECISSION TREE\nimport graphviz \ndot_data = tree.export_graphviz(tree_model, out_file=None, feature_names=cols, class_names=True, filled=True, rounded=True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n\nfrom sklearn.ensemble import VotingClassifier, BaggingClassifier\n\n# VOTING ENSAMBLE MODELS\nmodels = [\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier())\n]\n\n# Hard Vote or majority rules\nvote_hard = VotingClassifier(estimators=models, voting='hard', n_jobs=-1)\nvote_hard_cv = model_selection.cross_validate(vote_hard, X_train, y_train, cv=cv_split)\nvote_hard.fit(X_train, y_train)\n\n#Soft Vote or weighted probabilities\nvote_soft = VotingClassifier(estimators=models, voting='soft', n_jobs=-1)\nvote_soft_cv = model_selection.cross_validate(vote_soft, X_train, y_train, cv=cv_split)\nvote_soft.fit(X_train, y_train)\n\n\n# OVERSAMPLING (Dealing with Imbalanced Datasets)\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy='minority', random_state=42)\nX_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n\n\n# OVERSAMPLING - using sklearn\nfrom sklearn.utils import resample\nminority = df[df.target==0]\nmajority = df[df.target==1]\n\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\ndf = pd.concat([minority_upsample, majority], axis=0)\n\n\n# SOME DETAILS OF SEVERAL MODELS\nfrom sklearn.pipeline import make_pipeline\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))                     # LASSO \nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))     # Elastic net \nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)                          # Kernel ridge      \nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,                       # Gradient boosting\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,                             # XGBoost\n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,                             # LightGBM\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n\n# XGBOOST RECOMMENDED PARAMETERS\nlearning_rate: 0.01\nn_estimators: 100 if the size of your data is high, 1000 is if it is medium-low\nmax_depth: 3\nsubsample: 0.8\ncolsample_bytree: 1\ngamma: 1","d937264e":"\n# TENSORFLOW - DEEP NETWORKS\n# CNN - image recognition\/classification, object detection in images, \n# RNN - forecast market price, sentiment analysis, predict next word in sentence, voice recognition\n# RBM - find patterns in data (unsupervised way), extract features, dimensionality reduction, handling missing values\n# DBN - image recognition\n# AUTOENCODERS - unsupervised tasks, compress imput data - dimensionality reduction, feature extraction, image recognition\n\n\n# MNIST LAYERS - Model layers overview\n(Input) -> [batch_size, 28, 28, 1] >> Apply 32 filter of [5x5]\n(Convolutional layer 1) -> [batch_size, 28, 28, 32]\n(ReLU 1) -> [?, 28, 28, 32]\n(Max pooling 1) -> [?, 14, 14, 32]\n(Convolutional layer 2) -> [?, 14, 14, 64]\n(ReLU 2) -> [?, 14, 14, 64]\n(Max pooling 2) -> [?, 7, 7, 64]\n[fully connected layer 3] -> [1x1024]\n[ReLU 3] -> [1x1024]\n[Drop out] -> [1x1024]\n[fully connected layer 4] -> [1x10]\n\n\n\n# Basic tensorflow task\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Convolution2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = tf.keras.Sequential()\nmodel.add(Convolution2D(32, (5,5), activation='relu',padding='same', input_shape=(28, 28,1)))\nmodel.add(Convolution2D(32, (5,5), activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Convolution2D(64, (3,3), activation='relu',padding='same'))\nmodel.add(Convolution2D(64, (3,3), activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n\nhistory = model.fit(x=X_train, y=y_train, batch_size=256, epochs=10, validation_data=(X_val, y_val), callbacks=[tensorboard_callback])\nmodel.summary()\n\n\n\n# SAVE MODEL, LOAD MODEL, CHECKPOINTS\n# when loading weights, model structure must be exactly same!\nmodel.save_weights('model.h5')     # when model has finished... it is probably needed to set restore_best_weights to true in early stopping\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath='model.h5', save_best_only=True, save_weights_only=False, monitor='val_accuracy')\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\n# load stored model\nmodel2.load_weights('model.h5')         # model must be created already and must match to definition of original... file is smaller\n# or\nmodel2 = tf.keras.models.load_model('model.h5')   # will generate whole model, file is larger\n\n# evaluate model\n\nloss, acc = model2.evaluate(X_test, y_test, batch_size=16)\n\n\n\n# CORRECT IMBALANCED DATA\ninitial_bias = np.log([count_class_1\/count_class_0])\nweight_for_0 = (1 \/ count_class_0)*(count_all)\/2.0 \nweight_for_1 = (1 \/ count_class_1)*(count_all)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\n\n# IMAGE AUGMENTATION\ndatagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False)\n\n# fit generator on our train features\ndatagen.fit(X_train)\n\n\n# LOAD DATA FROM DIRECTORY - SLOW, BUT WORKS FOR LARGE DATASETS\n# require images to be in proper folders ... train\/classXY\/, test\/classXY\/, ...\ndatagen = ImageDataGenerator( ... some parameters ...)\ntrain_flow = datagen.flow_from_directory('train\/', target_size=(256,256), class_mode='binary', color_mode='grayscale', batch_size=32, shuffle=True)\n\n\n# ANOTHER WAY TO DEFINE MODEL\ndef conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ])\n    \n    return block\n\ndef dense_block(units, dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    \n    return block\n\ndef build_model():\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model\n\n\nmodel = build_model()\n\n\n\n\n# USE KAGGLE TPU TENSORFLOW\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = tf.keras.Sequential( \u2026 ) # define your model normally\n    model.compile( \u2026 )\n\n# train model normally\nmodel.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=\u2026)\n\n\n\n# USE PRETRAINED MODEL\n# Specify location of pre-trained model weights ('imagenet')\nmy_model_2 = ResNet50(weights='imagenet')\n","cff9d327":"# ARIMA TIME SERIES\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nimport itertools\n\nprm = []\nprm_seasonal = []\naic = []\n\n# arima\n# p \u2013 The lag value where the PACF chart crosses the upper confidence interval for the first time\n# q \u2013 The lag value where the ACF chart crosses the upper confidence interval for the first time\n\np = d = q = range(0,2)\ns = [12]\npdq = list(itertools.product(p,d,q))\nPDQ = list(itertools.product(p,d,q,s))\n\nfor param in pdq:\n    for param_seasonal in PDQ:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(train, order=param, seasonal_order = param_seasonal)\n            results = mod.fit()\n            prm.append(param)\n            prm_seasonal.append(param_seasonal)\n            aic.append(results.aic)\n            print('ARIMA {} x {} - AIC: {}'.format(param, param_seasonal, results.aic))\n\n        except:\n            print('ARIMA {} x {} - Failed!'.format(param, param_seasonal))\n\nprint('Best configuration: {} x {} with AIC: {}'.format(prm[np.argmin(aic)], prm_seasonal[np.argmin(aic)], aic[np.argmin(aic)]))\n\n\nmod = sm.tsa.statespace.SARIMAX(train, order=prm[np.argmin(aic)], seasonal_order = prm_seasonal[np.argmin(aic)])\nresults = mod.fit()\n\nresults.summary()\n\nresults.plot_diagnostics(figsize=(15, 12))\nplt.show()\n\n# get predictions\n# option 1\ntest_hat = results.predict(start=test.index.min(), end=test.index.max())\n\n# option 2\ntest_hat = results.get_prediction(start=test.index.min(), end=test.index.max(), dynamic=False)\ntest_hat_ci = pred.conf_int()\ntest_hat.predicted_mean\n\n# option 3\ntest_hat = results.get_forecast(steps=48)\ntest_hat.predicted_mean\n\n\n# STATIONARITY\n# test statistics should be lower than critical value 1% and p-value lower than 0.05\n# methods to get data stationar:\n# - log, exponential, box-cox\n# - data\/log - rolling mean (pandas function)\n# - data\/log - shift (pandas function)\n# - data\/log - ewm (exponential weighted mean, pandas function)\n# - data\/log \ndef check_data_stationary(data, window=7):\n    rolling_mean = data.rolling(window=window).mean()\n    rolling_std = data.rolling(window=window).std()\n    \n    plt.plot(data, color='blue', label='Original')\n    plt.plot(rolling_mean, color='red', label='Rolling mean')\n    plt.plot(rolling_std, color='green', label='Rolling std')\n    plt.legend(loc='best')\n    plt.title('Original data, rolling mean and std')\n    \n    print('Results of Dickey-Fuller Test:')\n    df = adfuller(data)\n    dfout = pd.Series(df[0:4], index=['Test statistics', 'p-value', '#Lags used', 'Number of observations used'])\n    print(dfout, '\\n')\n    for key, value in df[4].items():\n        print('Critical value ({}): {}'.format(key, value))\n\n\n# decompose data\ndecomposition = seasonal_decompose(ts_log_tr)\ndecomposition.plot();    # semicolon avoid printing charts twice\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\n\n# DEEP NETS - LSTM FOR TIME SERIES\n# scaling is crucial in tensorflow, keep scaler instance for prediction reversal\nscaler = MinMaxScaler()\ndf = scaler.fit_transform(df)\n\ntrain_size = int(df.shape[0]*2\/3)\ntrain, test = df[:train_size], df[train_size:]\n\n# create function that will do X and Y datasets. Y is target label, X is vector containing time steps before\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\n# use 14 days window\nlook_back = 14\nX_train, y_train = create_dataset(train, look_back)\nX_test, y_test = create_dataset(test, look_back)\n\n# reshaping to correct format for tensorflow\n# Y - [val1, val2, val3, ...]\n# X - [[[val1, val2, val3, ...]], [[val4, val5, val6, ...]], [[...]], ...]\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n\n# tensorflow\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\nmodel = Sequential()\n# model.add(LSTM(128, input_shape=(1,look_back), return_sequences=True, activation='linear'))   # return sequences must be on True if it is followed by next LSTM\n# model.add(Dropout(0.2))\n# model.add(LSTM(64, return_sequences=False, activation='linear'))\nmodel.add(LSTM(4, activation='tanh', input_shape=(1,look_back)))      # notice that input shape is size of one vector from X\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=1, callbacks=[callback])\n\n# plot val loss for each epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\n# get predictions\ny_test_hat = model.predict(X_test)\ny_test_hat = scaler.inverse_transform(y_test_hat.reshape(-1,1))","4455bc37":"\n# NATURE LANGUAGE PROCESSING - NLTK\n# get best stopwords here: https:\/\/github.com\/6\/stopwords-json\n\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nsent_tokenize(text)   \t\t\t\t\t\t# split text into sentences\nword_tokenize(text)\t\t\t\t\t\t\t# split text into words\nWordNetLemmatizer().lemmatize(word)\t\t\t# trying to get word to it's base form\nPorterStemmer().stem(word)\t\t\t\t\t# trying to shorten word\nstopwords = stopwords.words('english')\t\t# stop words, better to use one from github\n\n# stopwords_en = pd.read_json('https:\/\/raw.githubusercontent.com\/6\/stopwords-json\/master\/dist\/en.json')\n# stopwords_en[0].tolist()\n\n\n# TF-IDF (term frequency-inverse document frequency)\n# Rare terms are more informative than frequent terms\n# Good to use: SGDClassifier, LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(text)\t\t\t\nvectorizer.get_feature_names()\t\t\t\t# words found in text\nvectorizer.vocabulary_\t\t\t\t\t\t# list of words with their counts\nX.toarray()\t\t\t\t\t\t\t\t\t# terms weighted frequency of size [sentence_count x word_counts \/ rows x cols]\nX = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(text)\t\t\t\nvectorizer.get_feature_names()\t\t\t\t# words found in text\nvectorizer.vocabulary_\t\t\t\t\t\t# list of words with their counts\nX.toarray()\t\t\t\t\t\t\t\t\t# terms count [sentence_count x word_counts \/ rows x cols]\nX = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names()))\n\n\n# CLEANUP BLOCK\nwordnet = WordNetLemmatizer()\n\nX = []\nfor sentence in df:\n    # Remove all the special characters\n    sentence = re.sub(r'\\W', ' ', str(sentence))\n \n    # remove all single characters\n    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n \n    # Remove single characters from the start\n    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence) \n \n    # Substituting multiple spaces with single space\n    sentence= re.sub(r'\\s+', ' ', sentence, flags=re.I)\n \n    # Removing prefixed 'b'\n    sentence = re.sub(r'^b\\s+', '', sentence)\n \n    # Remove https\n    sentence = re.sub(r'http', ' ', str(sentence))\n\n    # Converting to Lowercase\n    sentence = sentence.lower()\n \n    # Lemmantize\n    words = [wordnet.lemmatize(w) for w in word_tokenize(sentence) if w not in sw]\n    sentence = ' '.join(words)\n        \n    X.append(sentence)\n\n\n# DEEP NET TEXT CLASSIFICATION\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Initialize tokenizer, count number of rows\ntokenizer = Tokenizer()\nnum_rows = df_train.shape[0]\nX = np.hstack((X_train, X_test))    \t\t\t\t\t# may not be needed, simply we need all words from train & test to fit\n\ntokenizer.fit_on_texts(X)\n\n# max length of 1 row (number of words)\nrow_max_length = max([len(x.split()) for x in X])\n\n# count number of unique words\nvocabulary_size = len(tokenizer.word_index) + 1\n\n# convert words into integers\nX_train_tokens = tokenizer.texts_to_sequences(X_train)\nX_test_tokens = tokenizer.texts_to_sequences(X_test)\n\n# ensure every row has same size - pad missing with zeros\nX_train_pad = pad_sequences(X_train_tokens, maxlen=row_max_length, padding='post')\nX_test_pad = pad_sequences(X_test_tokens, maxlen=row_max_length, padding='post')\n\n# prepare target variables. For tensorflow, if we deal with multinomial target, we must convert vector to matrix having \n# as many columns as there are target options. I.e. having target 0-4, vector must be converted to matrix of 5 cols\ny_train_cat = to_categorical(y_train)\ny_test_cat = to_categorical(y_test)\n\ntarget_length = y_train_cat.shape[1]\nprint('Original vector size: {}'.format(y_train.shape))\nprint('Converted vector size: {}'.format(y_train_cat.shape))\n\n# like dimensionality reduction. Can be 100,500 or even 1000\n# basically if EMBEDDING_DIM == vocabulary_size, then it's identical to bag of words\n# but you cannot handle it if you have 300 000 of words, would be sparse matrix with 99.99% of values to be zero\n# instead, embedding layer will be dense and have smaller dimension\nEMBEDDING_DIM = 128\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=row_max_length))\nmodel.add(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Bidirectional(GRU(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(target_length, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\nhistory = model.fit(X_train_pad, y_train_cat, epochs=100, validation_data=(X_test_pad, y_test_cat), batch_size=128, callbacks=[callback])\n\n\n# \"FUNCTIONAL\" WAY OF MODELING DEEP NETS\n# like dimensionality reduction. Can be 100,500 or even 1000\n# basically if EMBEDDING_DIM == vocabulary_size, then it's identical to bag of words\n# but you cannot handle it if you have 300 000 of words, would be sparse matrix with 99.99% of values to be zero\n# instead, embedding layer will be dense and have smaller dimension\nEMBEDDING_DIM = 512\n\ninp = Input(shape = (row_max_length,))\nx = Embedding(vocabulary_size, EMBEDDING_DIM)(inp)\nx1 = SpatialDropout1D(0.2)(x)\n\nx_gru = Bidirectional(GRU(128, return_sequences = True))(x1)\nx1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\navg_pool1_gru = GlobalAveragePooling1D()(x1)\nmax_pool1_gru = GlobalMaxPooling1D()(x1)\n\nx3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\navg_pool3_gru = GlobalAveragePooling1D()(x3)\nmax_pool3_gru = GlobalMaxPooling1D()(x3)\n\nx_lstm = Bidirectional(LSTM(128, return_sequences = True))(x1)\nx1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\navg_pool1_lstm = GlobalAveragePooling1D()(x1)\nmax_pool1_lstm = GlobalMaxPooling1D()(x1)\n\nx3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\navg_pool3_lstm = GlobalAveragePooling1D()(x3)\nmax_pool3_lstm = GlobalMaxPooling1D()(x3)\n\nx = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru, avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\nx = BatchNormalization()(x)\nx = Dropout(0.2)(Dense(128,activation='relu') (x))\nx = BatchNormalization()(x)\nx = Dropout(0.2)(Dense(100,activation='relu') (x))\nx = Dense(target_length, activation = \"sigmoid\")(x)\nmodel = Model(inputs = inp, outputs = x)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\nhistory = model.fit(X_train_pad, y_train_cat, epochs=100, validation_data=(X_test_pad, y_test_cat), batch_size=128, callbacks=[callback], use_multiprocessing=True)\n\n\n\n# WORD CLOUD\n# text - is list of words. Example: 'hello how are you hello hey good fine hey'\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=800, stopwords=stopwords, max_font_size=100, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15,15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n","5e488504":"# OPEN CV \/ CV2 USAGE\nimport cv2\n\n# read image\nimg = cv2.imread('my_file_path.jpg')\n\n# get image size\nimg.shape   # numpy array\n\n# show image\ncv2.imshow('My test image', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n# resize image, notice switching shape arguments\nimg2 = cv2.resize(img, (int(img.shape[1]\/2), int(img.shape[0]\/2)))\n\n# convert image to grayscale\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# DETECT FACES ON IMAGE AND GET RECTANGLES\n# XML must be downloaded, there are many many more of them on github\nface_cascade = cv2.CascadeClassifier('C:\/Users\/MichalBrezak\/Desktop\/haarcascade_frontalface_default.xml') \ngray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_cascade.detectMultiScale(gray_img, scaleFactor=1.05, minNeighbors=5)   # contains coordinates of rectangles in format (x, y, widht, height)\n\nfor x,y,w,h in faces:\n    img = cv2.rectangle(img, pt1=(x,y), pt2=(x+w,y+h), color=(0,0,255), thickness=3)\n    \ncv2.imshow('Finalized image!', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n# show cv2 image via matplotlib (cv2 stores colors as BGR, matplotlib as RGB - reversed!)\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))","a2e06785":"Contains following functions: word cloud, deep nets using GRU & LSTM, bidirectional LSTM, embedding layer, nltk library, tokenizer, lematizer, tf-idf.","e9aea191":"# Nature Language Processing","0e8df7b5":"# Open CV - face detection","04b4ae48":"Most of blocks require some customization to use in different notebooks, but I think it's not bad to share with community such file.","74423675":"As I started to learn machine learning from scratch, I realized I will never remember all the functions (my memory is really short!)... so how to learn and not forget? Simply: create short notes based on code that was already created. Once you come back to this, you will get point of code and will be able to reuse it... maybe next time you do not need to look into cheatsheet at all ;)","767fa625":"Contains following functions: correlation, pearson correlation, hyperparameter tuning, modeling - regression\/classification, split dataset to train & test, skewness (get & fix), one hot encoding, clustering, rmse on cross-fold, confusion matrix, classification report, clustering, scale features, pca, imputation of missing values, ensamble models (soft\/hard vote). ","975ff39d":"Contains following functions: Arima, Sarimax, stationarity, decomposition, deep nets RNN using LSTM.","cc2f4711":"# Time Series - Arima\/Sarimax & Tensorflow RNN (LSTM)","90cbaa33":"# Standard Machine Learning Functions","0dd3d2b1":"# Default functions and setup","9ecacd56":"# Deep Learning - Tensorflow v2 & Keras","f6729c28":"# Python CheatSheet - ML, DL, NLP","b988c4f3":"Contains following functions: basic usage of openCV library, face detection, face bounding using pretrained model, plot image.","aab5eb09":"Contains following functions: remove warnings, pandas max cols & rows, matplotlib dpi and default figure size.","f3999431":"Contains following functions: basic deep network model (CNN, RNN), save model, load model, checkpoints, evaluate, imbalanced data correction, image augmentation, TPU, pre-trained models.","0af8d3bc":"## If you liked, **please upvote**. Feel free to post any feedback or suggestion you have in your mind.\n## Thanks!"}}