{"cell_type":{"2bb834ed":"code","7516b382":"code","06bd0ea1":"code","8138af15":"code","79db6459":"code","df0aaf47":"code","516b2bfd":"code","92f6c201":"code","93cc7808":"code","7fb81ba1":"code","c58ad566":"code","3a1bafac":"code","b61441e6":"code","7b85e4b0":"code","3a54301e":"code","8d689850":"code","8aade908":"code","cb818c26":"code","470343af":"code","b5aae044":"code","39a5b42b":"markdown","4357112b":"markdown","aae418f0":"markdown"},"source":{"2bb834ed":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7516b382":"train_data = pd.read_csv('..\/input\/train.csv')\nprint(train_data.target.value_counts())","06bd0ea1":"print(train_data.shape)","8138af15":"train_data.isnull().any().any()","79db6459":"train_X = train_data.drop(['ID_code', 'target'], axis = 1)\ntrain_Y = train_data['target']\nprint(train_X.shape)\nprint(train_Y.shape)","df0aaf47":"test_data = pd.read_csv('..\/input\/test.csv')\nprint(test_data.shape)\n","516b2bfd":"test_data.isnull().any().any()","92f6c201":"test_data_X = test_data.drop(['ID_code'], axis = 1)\nprint(test_data_X.shape)","93cc7808":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain_X = sc.fit_transform(train_X)\ntest_data_X = sc.fit_transform(test_data_X)","7fb81ba1":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, Y_train, Y_test = train_test_split(train_X, train_Y, test_size=0.20, random_state=111)","c58ad566":"# import the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout , BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\nfrom keras import regularizers\nfrom keras.constraints import max_norm\n","3a1bafac":"# Initialize the NN \n\ndef CreateNetwork():\n    model = Sequential()\n    model.add(Dense(256, input_dim=X_train.shape[1] , activation='relu', \n                kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dense(1, activation='sigmoid'))\n    #opt = SGD(lr=0.0001, momentum=0.9)\n    #opt = Adam(lr=0.0001, momentum=0.9)\n    #model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    #model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 0.001, decay=0.001\/50), metrics=['accuracy'])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    return model","b61441e6":"# fit model\n\n#from sklearn.model_selection import train_test_split\n#X_train, X_val, Y_train, Y_val = train_test_split(train_X, train_Y, test_size=0.20, random_state=111)\n\n\n#model = CreateNetwork()\ncheckpoint = ModelCheckpoint('rc_model.h5', monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=4, \n                                   verbose=1, mode='max', min_delta=0.0001)\n# early stopping\nearlyStoping = EarlyStopping(monitor='val_loss', \n                                  patience=9, mode='max', verbose=1)\n\ncallbacks_list = [checkpoint,earlyStoping,reduceLROnPlat]\n\n#history = model.fit(X_train, Y_train, batch_size = 1024, epochs=150, validation_split=0.20, \n            #validation_data=(X_val, Y_val), callbacks=callbacks_list)\n\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nmodel = CreateNetwork()   \n\n\nsplits = list(StratifiedKFold(n_splits=5, shuffle=True).split(train_X, train_Y))\n\ny_test_pred_log = np.zeros(len(train_X))\ny_train_pred_log = np.zeros(len(train_X))\nprint(y_test_pred_log.shape)\nprint(y_train_pred_log.shape)\nscore = []\n\nfor i, (train_idx, test_idx) in enumerate(splits):  \n\n    print(\"FOLD %s\" % (i + 1))\n    X_train_fold, X_val_fold  = train_X[train_idx], train_X[test_idx]\n    Y_train_fold, Y_val_fold = train_Y[train_idx], train_Y[test_idx]\n     \n    \n    \n    \n    history = model.fit(X_train_fold, Y_train_fold, batch_size = 32, epochs=100,\n                        validation_data=(X_val_fold, Y_val_fold), callbacks=callbacks_list)\n    \n    model.load_weights('rc_model.h5')\n    \n    prediction = model.predict(X_val_fold,\n                               batch_size=512,\n                               verbose=1)\n    # print(prediction.shape)\n    # prediction = np.sum(prediction, axis=1)\/2\n    score.append(roc_auc_score(Y_val_fold, prediction))\n    \n    prediction = model.predict(test_data_X,\n                               batch_size=512,\n                               verbose=1)\n    # y_test_pred_log += np.sum(prediction, axis=1)\/2\n    y_test_pred_log += prediction.reshape(prediction.shape[0])\n    \n    prediction = model.predict(train_X,\n                               batch_size=512,\n                               verbose=1)\n    # y_train_pred_log += np.sum(prediction, axis=1)\/2\n    y_train_pred_log += prediction.reshape(prediction.shape[0])\n\n    \n    del X_train_fold, Y_train_fold, X_val_fold, Y_val_fold\n\n","7b85e4b0":"print(\"OOF score: \", roc_auc_score(train_Y, y_train_pred_log\/5))\nprint(\"average {} folds score: \".format(5), np.sum(score)\/5)","3a54301e":"#_, train_acc = model.evaluate(X_train, Y_train, verbose=0)\n#print(train_acc)","8d689850":"#_, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n#print(test_acc)","8aade908":"# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['acc'], label='train')\nplt.plot(history.history['val_acc'], label='test')\nplt.legend()\nplt.show()","cb818c26":"# plot loss learning curves\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","470343af":"#preds = model.predict(test_data_X)","b5aae044":"submission = pd.DataFrame({\"ID_code\" : test_data['ID_code'].values,\n                           \"target\" : y_test_pred_log\/5})\nsubmission.to_csv('submission.csv', index = False, header = True)\ndisplay(submission.head(15))\ndisplay(submission.tail(15))\n\n","39a5b42b":"Machine Learning","4357112b":"**Load Data**","aae418f0":"**Preprocess the Data**"}}