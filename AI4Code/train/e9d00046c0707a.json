{"cell_type":{"800ffccf":"code","0448ad1b":"code","bdf86c6d":"code","c16b0870":"code","488c1c9c":"code","0542bb19":"code","2014bd7d":"code","ae85a2e0":"code","0f13152d":"code","ec59452d":"code","ba5cdb1e":"code","5225fd2c":"code","ef72e4fc":"code","3877809a":"code","1d1386c6":"code","0fe211fc":"code","fc5485b9":"code","19d3c127":"code","e33253ad":"code","9e5477e4":"code","2e8acee6":"code","dc66274b":"code","88ba6e8b":"code","919b9c48":"code","4f828fe5":"code","bfde6003":"code","900ffb53":"code","808928a9":"code","3859b94d":"code","d2d1cf7b":"code","88db47fc":"code","d77fbd2a":"code","29318531":"code","49b3d415":"code","81cffb08":"code","910b3ea5":"code","3a637d5f":"code","3f9b257e":"code","4c192fda":"code","7d02e195":"code","938a2541":"code","e100e249":"code","f2a3039e":"code","76532f6d":"markdown","1ba529c9":"markdown","f13f0550":"markdown","5df6a13a":"markdown","0d444993":"markdown","95ef97ec":"markdown","0024d51b":"markdown","54ff51eb":"markdown","fc6f09a9":"markdown","b622fc82":"markdown","95446652":"markdown","d9d0599c":"markdown","86ec4fe8":"markdown","6b610876":"markdown","ebd9d91f":"markdown","80ab526c":"markdown","f17bf22e":"markdown"},"source":{"800ffccf":"import os\nimport shutil\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom os import listdir, makedirs, getcwd, remove\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\n# plotly\nimport plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\n\nimport tensorflow as tf\nfrom plotly.graph_objs import *\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers","0448ad1b":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    #print(cm)\n    \n    plt.figure(figsize=(8,8))\n    plt.grid(False)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","bdf86c6d":"# Check for the directory and if it doesn't exist, make one.\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\n    \n# make the models sub-directory\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)","c16b0870":"# original dataset folder, you can see above\ninput_path = Path('\/kaggle\/input\/flowers-recognition\/flowers')\nflowers_path = input_path \/ 'flowers'","488c1c9c":"# Each species of flower is contained in a separate folder. Get all the sub directories\nflower_types = os.listdir(flowers_path)\nprint(\"Types of flowers found: \", len(flower_types))\nprint(\"Categories of flowers: \", flower_types)","0542bb19":"# A list that is going to contain tuples: (species of the flower, corresponding image path)\nflowers = []\n\nfor species in flower_types:\n    # Get all the file names\n    all_flowers = os.listdir(flowers_path \/ species)\n    # Add them to the list\n    for flower in all_flowers:\n        flowers.append((species, str(flowers_path \/species) + '\/' + flower))\n\n# Build a dataframe        \nflowers = pd.DataFrame(data=flowers, columns=['category', 'image'], index=None)\nflowers.head()","2014bd7d":"# feel free to edit \"0\" (corresponds 0. image)\n# flowers['image'][0]","ae85a2e0":"# Let's check how many samples for each category are present\nprint(\"Total number of flowers in the dataset: \", len(flowers))\nfl_count = flowers['category'].value_counts()\nprint(\"Flowers in each category: \")\nprint(fl_count)","0f13152d":"# Let's do some visualization and see how many samples we have for each category\n\nf, axe = plt.subplots(1,1,figsize=(14,6))\nsns.barplot(x = fl_count.index, y = fl_count.values, ax = axe)\naxe.set_title(\"Flowers count for each category\", fontsize=16)\naxe.set_xlabel('Category', fontsize=14)\naxe.set_ylabel('Count', fontsize=14)\nplt.show()","ec59452d":"# Let's visualize some flowers from each category\n\n# A list for storing names of some random samples from each category\nrandom_samples = []\n\n# Get samples fom each category \nfor category in fl_count.index:\n    samples = flowers['image'][flowers['category'] == category].sample(4).values\n    for sample in samples:\n        random_samples.append(sample)\n\n# Plot the samples\nf, ax = plt.subplots(5,4, figsize=(15,10))\nfor i,sample in enumerate(random_samples):\n    ax[i\/\/4, i%4].imshow(mimg.imread(random_samples[i]))\n    ax[i\/\/4, i%4].axis('off')\nplt.show()    ","ba5cdb1e":"# Make a parent directory `data` and two sub directories `train` and `valid`\n%mkdir -p data\/train\n%mkdir -p data\/valid\n\n# Inside the train and validation sub=directories, make sub-directories for each catgeory\n%cd data\n%mkdir -p train\/daisy\n%mkdir -p train\/tulip\n%mkdir -p train\/sunflower\n%mkdir -p train\/rose\n%mkdir -p train\/dandelion\n\n%mkdir -p valid\/daisy\n%mkdir -p valid\/tulip\n%mkdir -p valid\/sunflower\n%mkdir -p valid\/rose\n%mkdir -p valid\/dandelion\n\n%cd ..\n\n# You can verify that everything went correctly using ls command","5225fd2c":"for category in fl_count.index:\n    samples = flowers['image'][flowers['category'] == category].values\n    #perm = np.random.permutation(samples)\n    # Copy first 100 samples to the validation directory and rest to the train directory\n    for i in range(100):\n        name = samples[i].split('\/')[-1]\n        shutil.copyfile(samples[i],'.\/data\/valid\/' + str(category) + '\/'+ name)\n    for i in range(100,len(samples)):\n        name = samples[i].split('\/')[-1]\n        shutil.copyfile(samples[i],'.\/data\/train\/' + str(category) + '\/' + name)","ef72e4fc":"from keras.applications import VGG16\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(240, 240, 3))","3877809a":"conv_base.summary()","1d1386c6":"base_dir = '\/kaggle\/working\/data'\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'valid')","0fe211fc":"datagen = ImageDataGenerator(rescale=1.\/255)\nbatch_size = 32","fc5485b9":"def extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count, 7, 7, 512))\n    labels = np.zeros(shape=(sample_count, 5))\n\n    generator = ImageDataGenerator(rescale=1.\/255).flow_from_directory(directory,\n        target_size=(240, 240),\n        batch_size = batch_size, \n        class_mode='categorical')\n\n    i = 0\n\n    print('Entering for loop...');\n\n    \n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        features[i * batch_size : (i + 1) * batch_size] = features_batch\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        i += 1\n        if i * batch_size >= sample_count:\n            break\n    return features, labels","19d3c127":"train_features, train_labels = extract_features(train_dir, 3823)\nvalidation_features, validation_labels = extract_features(validation_dir, 500)","e33253ad":"train_features = np.reshape(train_features, (3823, 7 * 7 * 512))\nvalidation_features = np.reshape(validation_features, (500, 7 * 7 * 512))","9e5477e4":"model = models.Sequential()\nmodel.add(layers.Dense(2048, activation='relu', input_dim=7 * 7 * 512))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation='relu', input_dim=7 * 7 * 512))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(128, activation='relu', input_dim=7 * 7 * 512))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(5, activation='softmax'))","2e8acee6":"model.summary()","dc66274b":"model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])","88ba6e8b":"history = model.fit(train_features, train_labels,\n                    epochs=25,\n                    batch_size=16,\n                    validation_data=(validation_features, validation_labels))","919b9c48":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(epochs, acc, 'bo', label='Training acc')\naxes[0].plot(epochs, val_acc, 'b', label='Validation acc')\naxes[0].legend()\n\naxes[1].plot(epochs, loss, 'bo', label='Training loss')\naxes[1].plot(epochs, val_loss, 'b', label='Validation loss')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].legend()\n\nplt.show()","4f828fe5":"model_1_val = val_acc[-1]\nprint(\"Validation Accuracy: \", model_1_val)","bfde6003":"y_pred=model.predict_classes(validation_features)\ncon_mat = tf.math.confusion_matrix(validation_labels.argmax(1), y_pred)\ncon_mat = np.array(con_mat)\nplot_confusion_matrix(cm = con_mat, classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip'], normalize = False)","900ffb53":"model = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(5, activation='softmax'))","808928a9":"model.summary()","3859b94d":"train_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'data\/train',\n        target_size=(240, 240),  # all images will be resized to 240x240\n        batch_size=batch_size,\n        class_mode='categorical')  # more than two classes\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'data\/valid',\n        target_size=(240, 240),\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle = False\n)","d2d1cf7b":"model.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(lr=2e-5),\n              metrics=['acc'])","88db47fc":"history = model.fit_generator(\n          train_generator,\n          epochs=30,\n          validation_data=validation_generator)","d77fbd2a":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(epochs, acc, 'bo', label='Training acc')\naxes[0].plot(epochs, val_acc, 'b', label='Validation acc')\naxes[0].legend()\n\naxes[1].plot(epochs, loss, 'bo', label='Training loss')\naxes[1].plot(epochs, val_loss, 'b', label='Validation loss')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].legend()\n\nplt.show()","29318531":"model_2_val = val_acc[-1]\nprint(\"Validation Accuracy: \", model_2_val)","49b3d415":"validation_generator.reset()\ny_pred = model.predict_generator(validation_generator)\ny_pred = y_pred.argmax(-1)\ncon_mat = tf.math.confusion_matrix(validation_generator.classes, y_pred)\ncon_mat = np.array(con_mat)\nplot_confusion_matrix(cm = con_mat, classes = validation_generator.class_indices.keys(), normalize = False)","81cffb08":"conv_base.summary()","910b3ea5":"conv_base.trainable = True\n\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","3a637d5f":"model.compile(loss='binary_crossentropy',\n              optimizer=optimizers.Adam(lr=2e-5),\n              metrics=['acc'])","3f9b257e":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=6,\n    validation_data=validation_generator,\n    validation_steps=50)","4c192fda":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(epochs, acc, 'bo', label='Training acc')\naxes[0].plot(epochs, val_acc, 'b', label='Validation acc')\naxes[0].legend()\n\naxes[1].plot(epochs, loss, 'bo', label='Training loss')\naxes[1].plot(epochs, val_loss, 'b', label='Validation loss')\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].legend()\n\nplt.show()","7d02e195":"model_3_val = val_acc[-1]\nprint(\"Validation Accuracy: \", model_3_val)","938a2541":"validation_generator.reset()\ny_pred = model.predict_generator(validation_generator)\ny_pred = y_pred.argmax(-1)\ncon_mat = tf.math.confusion_matrix(validation_generator.classes, y_pred)\ncon_mat = np.array(con_mat)\nplot_confusion_matrix(cm = con_mat, classes = validation_generator.class_indices.keys(), normalize = False)","e100e249":"# deleting training and test sets, because kaggle is trying to show all\n# images that we created as output\nshutil.rmtree(\"\/kaggle\/working\/data\")","f2a3039e":"my_color = ['Gold','MediumTurquoise','LightGreen']\ntrace=go.Bar(\n            x=['ConvNet from Scratch', 'Feature Extraction w\/o DA', 'Feature Extraction w\/ DA', 'Fine-Tuning'],\n            y=[0.7937, round(model_1_val,4), round(model_2_val,4), round(model_3_val,4)],\n            text=[0.7937, round(model_1_val,4), round(model_2_val,4), round(model_3_val,4)],\n            textposition='auto',\n            marker=dict(\n                color=px.colors.sequential.deep,\n                line=dict(\n                color=px.colors.sequential.deep,\n                width=0.4),\n            ),\n            opacity=1)\n\ndata = [trace]\nlayout = go.Layout(title = 'Accuracies of Models',\n              xaxis = dict(title = 'Model'),\n              yaxis = dict(title = 'Accuracy'))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","76532f6d":"<hr\/>\n# ** Feature Extraction and Fine Tuning using VGG16**\n<span id=\"0\"><\/span>\n[**Tolgahan \u00c7epel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n<font color=green>\n1. [Overview](#1)\n1. [Importing Libraries and Reading the Dataset](#2)\n1. [Using a Pretrained Convnet](#3)\n1. [Feature Extraction](#4)\n    * [Feature Extraction without Data Augmentation](#5)\n    * [Feature Extraction with Data Augmentation](#6)\n1. [Fine Tuning](#7)    \n1. [Conclusion](#8)","1ba529c9":"## <span id=\"8\"><\/span> ** 6. Conclusion **\n#### [Return Contents](#0)\n","f13f0550":"This cell above, creating a dataframe which contains all images and their path. If we run this command to see first row's image path:\n```\n>> flowers['image'][0]\n```\n\nThe output:\n```\n'\/kaggle\/input\/flowers-recognition\/flowers\/tulip\/122450705_9885fff3c4_n.jpg'\n```\n","5df6a13a":"## <span id=\"1\"><\/span> ** 1. Overview **\n#### [Return Contents](#0)\nI have built a convnet from scratch using this Flower Recognition dataset before. The accuracy was about %79-80. In this kernel, my passion is to get experience about transfer learning and compare results of each models. We know that using transfer learning in small datasets should improve accuracy. Let's see the difference!\n\nYou can view my <a href=\"https:\/\/www.kaggle.com\/tolgahancepel\/flower-recognition-convnet-from-scratch\">Flower Recognition ConvNet from Scratch<\/a> kernel. The first part is completely same.","0d444993":"Original folder list is here as you can see below. But we need a training and test set. Let's get them.\n```\n\/kaggle\/input\/flowers-recognition\/\n    flowers\/\n        tulip\n        daisy\n        sunflower\n        rose\n        dandelion\n```","95ef97ec":"The model is still overfitting, however the accuracy is pretty good!","0024d51b":"## <span id=\"2\"><\/span> ** 2. Importing Libraries and Reading the Dataset **\n#### [Return Contents](#0)\nThe images dataset are not seperated into training and test set. In this section, we are going to copy images and divide into two folder (train and validation). I have added detailed folder list that you can see later. But simply, the folder will bel like this:\n```\ndata\/\n    train\/\n        category1\/(contains all images related to category1)  \n        category2\/(contains all images related to category2)\n        ...\n        ...\n            \n    validation\/\n        category1\/(contains all images related to category1)  \n        category2\/(contains all images related to category2)\n        ...\n        ...\n```","54ff51eb":"### <span id=\"6\"><\/span> ** Feature Extraction with Data Augmentation **\n#### [Return Contents](#0)\nWe can also use pretrained model by adding our Dense layers to end of convolutional base. This will allow us to use data augmentation. But however, it's slower than the first feature extraction method. But we hope that using data augmentation will prevent overfitting and improve the accuracy.","fc6f09a9":"Now, it's time to prepare our training and test sets. This code below copying files into new folders and creating new images stored like below.\n```\ndata\/\n    train\/\n        daisy  \n        dandelion\n        rose\n        sunflower\n        tulip\n            \n    validation\/\n        daisy  \n        dandelion\n        rose\n        sunflower\n        tulip\n```","b622fc82":"## <span id=\"3\"><\/span> ** 3. Using a Pretrained Convnet **\n#### [Return Contents](#0)\nThere are 2 ways to use a pretranied network: **Feature Extraction** and **Fine-Tuning**. In this kernel, we will use both techniques and compare their results. We will use VGG16 architecture which is developed by Karen Simonyan and Andrew Zisserman in 2014. You can read the paper here: <a href=\"https:\/\/arxiv.org\/abs\/1409.1556\">Very Deep Convolutional Networks for Large-Scale Image Recognition<\/a>","95446652":"## <span id=\"4\"><\/span> ** 4. Feature Extraction **\n#### [Return Contents](#0)\nWe can extract features of our images dataset using a pretrained model. This is called Feature Extraction. There are 2 ways to use this method, first one doesn't support data augmentation, but however the second method is usable with data augmentation.","d9d0599c":"If you are familiar with **cd** and **ls** commands, you can see new folders and image files by using them.","86ec4fe8":"### <span id=\"5\"><\/span> ** Feature Extraction without Data Augmentation **\n#### [Return Contents](#0)\nWe will run our images on VGG16 convolutional base and get some 2d arrays. The output of this architecture will be features that extracted. And then, train these arrays with classic neural networks. However, data augmentation is not available for this method.","6b610876":"I have tried to implement my convolutional neural networks knowledge in Keras. In the first kernel, built a model from scratch and get %80 accuracy. This is also good but, because the dataset is \"small\", transfer learning can give better results. You can see validation accuracies of the models. Please give me feedback for improvements. I am still learning.\n\n<b><font color=\"red\">Don't forget to <\/font><\/b> <b><font color=\"green\">UPVOTE <\/font><\/b> if you liked this kernel, thank you. \ud83d\ude42\ud83d\udc4d","ebd9d91f":"## <span id=\"7\"><\/span> ** 5. Fine Tuning **\n#### [Return Contents](#0)","80ab526c":"This function is extracting features on VGG16 convolutional base and returns (features, labels).","f17bf22e":"Even we use Dropout, the model has overfitting problem as you seen. The accuracy is almost same with \"from scratch\" model, about %80. However, using data augmentation is one of the ways to prevent overfitting. Let's try augmentation and see the improvement."}}