{"cell_type":{"577aa5af":"code","5755b42e":"code","2d04dff7":"code","a7f2bb7d":"code","b5a21488":"code","afeec178":"code","80cf1d94":"code","a3cb52a3":"code","3142172d":"code","f1d01028":"code","310dc281":"code","31979603":"code","08634291":"code","88f9b0c1":"code","d4fc0bae":"code","651c1a70":"code","7d7bd7a1":"code","e998d119":"code","f11351a3":"code","4fbfa83b":"code","72ba4180":"code","917de0dd":"code","fe891db3":"code","9718d3b7":"code","0dac67ae":"code","41e296fa":"code","3452eaee":"code","52fc74f2":"code","805870c2":"code","629915ee":"code","41101617":"code","9f41b72c":"code","e7c391c4":"code","15a41eb1":"code","c9dafc11":"code","415614bf":"code","b2b8459b":"code","7ca085a1":"code","131a4b76":"code","5088ffdb":"code","bf74ff97":"code","a7def4c8":"code","08dd8d79":"code","09a5fee4":"code","4a9db0b5":"code","628a2025":"code","1931a425":"code","e89aab25":"code","6039ad16":"code","cc8c4bb6":"code","a31c0ac1":"code","2d74ca72":"code","d8898290":"code","b1f6eaf1":"markdown","c4c2a2cf":"markdown","8a7b2bac":"markdown","e65b6b48":"markdown","6c1f5010":"markdown","286eebbd":"markdown","af1f0ea7":"markdown","c4b90b9e":"markdown","b76c9696":"markdown","0c9722d5":"markdown","fcdd6466":"markdown","90a8a8a7":"markdown","0acfa305":"markdown","3baf3510":"markdown","ae8a59c7":"markdown","dfa1993e":"markdown","985562cc":"markdown","9f5300ab":"markdown","93b365d7":"markdown","21bb8492":"markdown","5c26a0d0":"markdown","af10c124":"markdown","03499538":"markdown","febaf199":"markdown","b0510bf3":"markdown","5bb62d38":"markdown","f8a2d07b":"markdown","a924a51c":"markdown","41c9252c":"markdown","22855251":"markdown","08eb4407":"markdown","e4117dae":"markdown","9cac6a4d":"markdown","4aa5c809":"markdown","bc410f1e":"markdown","d180f8f8":"markdown","c8433726":"markdown","bd2706cd":"markdown","ed9c1d5b":"markdown","1cfb2acc":"markdown","d70466e4":"markdown","65730798":"markdown","8c4bfdb7":"markdown","7b2f1825":"markdown","b5dc6f8e":"markdown","7ef5474f":"markdown"},"source":{"577aa5af":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","5755b42e":"# You have to include the full link to the csv file containing your dataset\ndataset = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndataset.sample(5)","2d04dff7":"dataset.info()","a7f2bb7d":"dataset.describe()","b5a21488":"# Let's replace 'Attrition' , 'overtime' , 'Over18' column with integers before performing any visualizations \ndataset['Attrition'] = dataset['Attrition'].apply(lambda x:1 if x == 'Yes' else 0)\ndataset['OverTime'] = dataset['OverTime'].apply(lambda x:1 if x == 'Yes' else 0)\ndataset['Over18'] = dataset['Over18'].apply(lambda x:1 if x == 'Y' else 0)\ndataset.head()","afeec178":"# Let's see if we have any missing data.\nsns.heatmap(dataset.isnull(),cmap = 'Blues', cbar = False, yticklabels = False)","80cf1d94":"dataset.hist(bins=30,figsize=(20,20),color='g')","a3cb52a3":"# Several features such as 'MonthlyIncome' and 'TotalWorkingYears' are tail heavly\n# It makes sense to drop 'EmployeeCount' and 'Standardhours' since they do not change from one employee to the other","3142172d":"# It makes sense to drop 'EmployeeCount' , 'Standardhours' and 'Over18' since they do not change from one employee to the other\n# Let's drop 'EmployeeNumber' as well\n# use inplace = True to change the values in memory.\n\ndataset.drop(['EmployeeCount','StandardHours','Over18','EmployeeNumber'],axis = 1, inplace = True)","f1d01028":"# Let's see how many employees left the company! \nleft_df = dataset[dataset['Attrition'] == 1]\nstayed_df = dataset[dataset['Attrition'] == 0]","310dc281":"print('1. Total = {} '.format(len(dataset)))\nprint('2. Number of employees left the company = {}'.format(len(left_df)))\nprint('3. Percentage of employees left the company = {}'.format((len(left_df)\/len(dataset))*100))\nprint('4. Number of employees who stayed in the company = {}'.format(len(stayed_df)))\nprint('5. Percentage of employees stayed the company = {}'.format((len(stayed_df)\/len(dataset))*100))","31979603":"left_df.describe()\n\n#  Let's compare the mean and std of the employees who stayed and left \n# 'age': mean age of the employees who stayed is higher compared to who left\n# 'DailyRate': Rate of employees who stayed is higher\n# 'DistanceFromHome': Employees who stayed live closer to home \n# 'EnvironmentSatisfaction' & 'JobSatisfaction': Employees who stayed are generally more satisifed with their jobs\n# 'StockOptionLevel': Employees who stayed tend to have higher stock option level","08634291":"stayed_df.describe()","88f9b0c1":"correlations = dataset.corr()\nf,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(correlations, annot=True)","d4fc0bae":"plt.figure(figsize=(25,12))\nsns.countplot(x = 'Age', hue = 'Attrition', data = dataset)","651c1a70":"plt.figure(figsize=(20,20))\nplt.subplot(511)\nsns.countplot(x = 'JobRole',hue = 'Attrition', data=dataset)\nplt.subplot(512)\nsns.countplot(x = 'MaritalStatus',hue = 'Attrition', data=dataset)\nplt.subplot(513)\nsns.countplot(x = 'JobInvolvement',hue = 'Attrition', data=dataset)\nplt.subplot(514)\nsns.countplot(x = 'JobLevel',hue = 'Attrition', data=dataset)\nplt.subplot(515)\nsns.countplot(x = 'OverTime',hue = 'Attrition', data=dataset)","7d7bd7a1":"plt.figure(figsize = (12,8))\nsns.kdeplot(left_df['DistanceFromHome'], label = 'Employees who left', color = 'r', shade = True)\nsns.kdeplot(stayed_df['DistanceFromHome'],label='Employees who stayed',color = 'b',shade=True)\nplt.xlabel('Distance from home')","e998d119":"plt.figure(figsize=(12,8))\nsns.kdeplot(left_df['YearsWithCurrManager'],shade=True,color='r',label='Employes who left')\nsns.kdeplot(stayed_df['YearsWithCurrManager'],shade=True,color='b',label='Employes who stayed')\n\nplt.xlabel('Number of years with the current manager')\nplt.title('Number of years with the current manager v\/s Atrition')","f11351a3":"plt.figure(figsize=(12,8))\nsns.kdeplot(left_df['TotalWorkingYears'],label='Employees who left',shade = True, color = 'r')\nsns.kdeplot(stayed_df['TotalWorkingYears'],label='Employees who stayed',shade = True, color = 'b')\n\nplt.xlabel('Number of total working years')\nplt.title('Number of total working years v\/s Attrition')","4fbfa83b":"# Let's see the Gender vs. Monthly Income\nsns.boxplot(x='MonthlyIncome',y='Gender',data=dataset)","72ba4180":"# Let's see the Jod role vs. Monthly Income\nplt.figure(figsize=(10,8))\nsns.boxplot(x='MonthlyIncome',y='JobRole',data=dataset)","917de0dd":"cat_var = [key for key in dict(dataset.dtypes)\n             if dict(dataset.dtypes)[key] in ['object'] ] \ncat_var","fe891db3":"X_cat = dataset[['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus']]\nX_cat.head()","9718d3b7":"from sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\nX_cat = onehotencoder.fit_transform(X_cat).toarray()\nX_cat","0dac67ae":"X_cat = pd.DataFrame(X_cat)\nX_cat.head()","41e296fa":"numeric_var = [key for key in dict(dataset.dtypes)\n                   if dict(dataset.dtypes)[key]\n                       in ['float64','float32','int32','int64']]\nnumeric_var","3452eaee":"X_numerical = dataset[['Age','Attrition','DailyRate','DistanceFromHome','Education','EnvironmentSatisfaction','HourlyRate','JobInvolvement','JobLevel','JobSatisfaction','MonthlyIncome','MonthlyRate','NumCompaniesWorked','OverTime','PercentSalaryHike','PerformanceRating','RelationshipSatisfaction','StockOptionLevel','TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']]","52fc74f2":"X_all = pd.concat([X_cat,X_numerical],axis=1)\nX_all.head()","805870c2":"# I will now drop the target variable 'Attrition'\nX_all.drop('Attrition',axis=1,inplace=True)\nX_all.shape","629915ee":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(X_all)\nscaled_data","41101617":"y = dataset['Attrition']\ny.shape","9f41b72c":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression","e7c391c4":"X_train,X_test,y_train,y_test = train_test_split(scaled_data,y,test_size = 0.25, random_state=43)","15a41eb1":"model_LR = LogisticRegression()\nmodel_LR.fit(X_train,y_train)\nLR_pred = model_LR.predict(X_test)\nprint('The accuracy score for Logistic Regression is: {}'.format(100*accuracy_score(LR_pred,y_test)))","c9dafc11":"cm = confusion_matrix(LR_pred,y_test)\nsns.heatmap(cm,annot=True)","415614bf":"print(classification_report(LR_pred,y_test))","b2b8459b":"from sklearn.ensemble import RandomForestClassifier\nmodel_RF = RandomForestClassifier()","7ca085a1":"model_RF.fit(X_train,y_train)\nRF_pred = model_RF.predict(X_test)\nprint('The accuracy score for Random Forest is: {}'.format(100*accuracy_score(RF_pred,y_test)))","131a4b76":"# Testing Set Performance\ncm = confusion_matrix(RF_pred,y_test)\nsns.heatmap(cm,annot=True)","5088ffdb":"print(classification_report(RF_pred,y_test))","bf74ff97":"import tensorflow as tf","a7def4c8":"model_NN = tf.keras.models.Sequential()\nmodel_NN.add(tf.keras.layers.Dense(units=500, activation='relu', input_shape=(50, )))\nmodel_NN.add(tf.keras.layers.Dense(units=500, activation='relu'))\nmodel_NN.add(tf.keras.layers.Dense(units=500, activation='relu'))\nmodel_NN.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","08dd8d79":"model_NN.summary()","09a5fee4":"model_NN.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])","4a9db0b5":"scaled_df = pd.DataFrame(scaled_data,columns=X_all.columns)\nscaled_df.head()","628a2025":"X_train_new,X_test_new,y_train_new,y_test_new = train_test_split(scaled_df,y,test_size = 0.25, random_state=43)","1931a425":"epochs_hist = model_NN.fit(X_train_new, y_train_new, epochs = 30, batch_size = 50)","e89aab25":"y_pred = model_NN.predict(X_test)\ny_pred = (y_pred > 0.5)","6039ad16":"plt.plot(epochs_hist.history['loss'])\nplt.title('Model Loss Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.legend(['Training Loss'])","cc8c4bb6":"plt.plot(epochs_hist.history['accuracy'])\nplt.title('Model Accuracy Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy')\nplt.legend(['Training Accuracy'])","a31c0ac1":"# Testing Set Performance\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True)\n","2d74ca72":"print(classification_report(y_test, y_pred))","d8898290":"#from imblearn.over_sampling import SMOTE\n#oversampler = SMOTE(random_state=0)\n#smote_train, smote_target = oversampler.fit_sample(X_train_new, y_train_new)\n#epochs_hist = model_NN.fit(smote_train, smote_target, epochs = 10, batch_size = 50)","b1f6eaf1":"# TASK #9: TRAIN AND EVALUATE A DEEP LEARNING MODEL ","c4c2a2cf":"### We get really good analyses for some important features such as Age, percentagehike, totalworkingyears, Monthlyincome, attrition. All this by a simple .hist command","8a7b2bac":"# TASK #6: UNDERSTAND HOW TO ASSESS CLASSIFICATION MODELS ","e65b6b48":"## Conclusion\n\n#### In this notebook I did some EDA and visualized the data with the help of plt and sns. I used Logistic Regression, Random Forest & later built an ANN for predictions. Each of them had good accuracy. \n### But This is a mistake I see a lot of people doing.\n#### Hence I want to address here. A good accuracy score is not enough to evaluate the performance of your model. It can actually mislead you sometimes. A real world model should have a good precision & recall score too. Hence its always a good practice to draw a confusion matrix and a classification report to get a better understanding of your model's performance. \n### So how can my models perform better?\n#### Well there are a lot of things that can be done to make the models performe better. One more technique can be to handle the imbalance data. The target 'Attrition' was really imbalanced. The data can be balanced by using suppose SMOTE. \n#### One can use RandomisedSearchCV to find the best params of ML models. The ANN model can also be hyper tuned. There are a number of ways. \n## I would appreciate anyone to copy my notebook can make the predictions better and let me know the results. ","6c1f5010":"### Now I will be scaling down all the values so that we can feed it to our ML\/DL models","286eebbd":"#### Interesting to see the trend that a lot of employees leave between 7 to 10 years of working","af1f0ea7":"![alt text](https:\/\/drive.google.com\/uc?id=1evbDHoW2t0emxkbQd8yevYFZ5woJKRPY)","c4b90b9e":"### Nice to see the gender equality here. Infact female tend to get more salaries here.","b76c9696":"# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE","0c9722d5":"#### We can see that the age group between 28 to 31 have left the most","fcdd6466":"![alt text](https:\/\/drive.google.com\/uc?id=1bX5uGmy5vbYTlp7m4tw_V2kTNzAHpHLp)","90a8a8a7":"### Now lets plot a histogram of all the features together and analyse some important features","0acfa305":"![alt text](https:\/\/drive.google.com\/uc?id=10NJUOTWOBzp2MNkgcPpCF0fLtdoN_jKj)","3baf3510":"![alt text](https:\/\/drive.google.com\/uc?id=19DpnhFkfsNEDPlH1dkfdr1zO36vRcBit)","ae8a59c7":"#### Sales Representitives tend to leave compared to any other job \n#### Single employees tend to leave compared to married and divorced\n#### Less involved employees tend to leave the company \n#### Less experienced (low job level) tend to leave the company ","dfa1993e":"![alt text](https:\/\/drive.google.com\/uc?id=11pNdVw4oWeNOWrkadrrxon7FU4qO5m6U)","985562cc":"# TASK #4: CREATE TESTING AND TRAINING DATASET & PERFORM DATA CLEANING","9f5300ab":"#### Selecting all the numerical values","93b365d7":"![alt text](https:\/\/drive.google.com\/uc?id=1OZLbKm1AJSyvoBgfvlfcLIWZxLOvzOWq)","21bb8492":"# TASK #3: VISUALIZE DATASET","5c26a0d0":"### Now lets see what age group tent to leave the company more.","af10c124":"![alt text](https:\/\/drive.google.com\/uc?id=1WNsznVn7je5r9HGnSLLdABICxrIv2Mrs)","03499538":"### Lets explore more","febaf199":"### Luckily we do not have any missing values. :)","b0510bf3":"### We also now know which features are catogerical.","5bb62d38":"### .describe() gives us a lot of information. For example now I know the average age group of employess in this company which is around 37.","f8a2d07b":"![alt text](https:\/\/drive.google.com\/uc?id=19cXoBqSiqbEGNofnD603bz3xEAsX28hy)","a924a51c":"# TASK #2: IMPORT LIBRARIES AND DATASETS","41c9252c":"![alt text](https:\/\/drive.google.com\/uc?id=1Mk2H7VYfv6ijUS9XqEdBQV6_LaHiyvkJ)","22855251":"![alt text](https:\/\/drive.google.com\/uc?id=1J03xZf6OiYtGV3IgJBUURBWyScpvaAbU)","08eb4407":"#### Handling the catogerical variables.\nFirst we select them, then we transform them.","e4117dae":"#### No doubt why we saw sales representatives leaving the job a lot in my earlier visualizations.","9cac6a4d":"# TASK #5: UNDERSTAND THE INTUITION BEHIND LOGISTIC REGRESSION CLASSIFIERS, ARTIFICIAL NEURAL NETWORKS, AND RANDOM FOREST CLASSIFIER","4aa5c809":"### With Random Forest my precission score for employees who will leave is not so good.","bc410f1e":"# TASK #8: TRAIN AND EVALUATE A RANDOM FOREST CLASSIFIER","d180f8f8":"![alt text](https:\/\/drive.google.com\/uc?id=1ztrMNehNYWMw6NwhOOC9BDBdnoNirpqZ)","c8433726":"### Lets do some more visualizations, but now for continuous values","bd2706cd":"# TASK #7: TRAIN AND EVALUATE A LOGISTIC REGRESSION CLASSIFIER","ed9c1d5b":"### As we can see, as the distance increases the employees tend to leave more as compared to who stayed.","1cfb2acc":"### As seen I got a good recall score with Logistic but my precision score for those who will leave is not good.\n","d70466e4":"#### As seen that in early time with the manager, emloyees tend to leave more than staying but with time passing by the employees tend to stay.","65730798":"#Count the number of employees who stayed and left\n#It seems that we are dealing with an imbalanced dataset \n","8c4bfdb7":"#### Job level is strongly correlated with total working years\n#### Monthly income is strongly correlated with Job level\n#### Monthly income is strongly correlated with total working hours\n#### Age is stongly correlated with monthly income\n#### Also we can see that overtime has a strong affect on Attrition\n","7b2f1825":"![alt text](https:\/\/drive.google.com\/uc?id=1Bk1xFW2tGBdwg-njOhw79MxtYBQnK-6x)","b5dc6f8e":"### Lets create some boxplot for more visualizations","7ef5474f":"\n<table>\n  <tr><td>\n    <img src=\"https:\/\/drive.google.com\/uc?id=1yJKgmHrRFnBk987HJfeDrMcTEXtk0z7W\"\n         alt=\"Fashion MNIST sprite\"  width=\"1000\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <b>Figure 1. Employee Retention Prediction\n  <\/td><\/tr>\n<\/table>\n"}}