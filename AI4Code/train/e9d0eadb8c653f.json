{"cell_type":{"9bac08a5":"code","e608fced":"code","ad947205":"code","71515e85":"code","4acd2663":"code","04f51578":"markdown","0f169c60":"markdown"},"source":{"9bac08a5":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\")) # data files are available in the \"..\/input\/\" directory.","e608fced":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# read data\ntrain = pd.read_csv('..\/input\/train.csv')\n\nx_train = train.iloc[:,1:].values.astype('float32')\ny_train = train.iloc[:,0].values.astype('int32')\n\n# reshape to (train_size, 28, 28, 1)\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\nx_test = x_train[0:8000,:]\ny_test = y_train[0:8000]\ntrainset_size = 10000  # max 36000\nx_train = x_train[-trainset_size:,:]\ny_train = y_train[-trainset_size:]\ny_train = keras.utils.to_categorical(y_train)  # one-hot encode the output, only for the training set\n\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","ad947205":"# build model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(8, 8), activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(64, (8, 8), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # reducing the size of the feature maps\nmodel.add(Dropout(0.25))  # regularization by ignoring randomly selected neurons during training \nmodel.add(Flatten())  # from 2-dimensional back to 1-dimensional\nmodel.add(Dense(64, activation='relu'))  # a \"normal\" layer\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))  # output layer\n# note that dropout and flatten are not really layers with parameters that can be trained\n\nearlystopping=[EarlyStopping(monitor='loss', patience=5, verbose=1, mode='auto')]\n\nplot_model(model, to_file='model.png')","71515e85":"model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),  # optimized gradient descent with automatically updated learning rate\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=128,  # mini batch size\n          epochs=60,  # is the maximum #epochs\n          verbose=1,\n          validation_split=0.2,\n          callbacks=earlystopping)","4acd2663":"# accuracy as a function of #epochs\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# loss as a function of #epochs\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","04f51578":"# Neural Networks\n\n1. Let's go over the code. Look at the architecture of the neural network, #layers and their size, connections between the layers, activation functions, optimizer and cost function. [This](https:\/\/medium.com\/machine-learning-bites\/deeplearning-series-convolutional-neural-networks-a9c2f2ee1524) link contains a picture of a neural network that might clarify things.\n2. A training and test set are created. Is there also a validation set?\n3. Run the model with train_size=10000. Appreciate that it uses computing resources from Google and not from your own computer. With \"GPU\" turned on it is really quite fast compared to your own computer.\n4. In the output you can see that `acc` is higher dan `val_acc`. Why is this the case?\n5. Run the same model again. Early stopping happens at a different epoch. Why is this the case?\n6. Reduce the trainset_size to 1000. Early stopping seems to happen at an earlier epoch. Can this be explained?\n7. Adapt the code to also plot a learning curve (loss as a function of the training set size).\n8. What can conclusions can be drawn from the learning curve?\n9. Change the model trying to get to a high bias situation and to a high variance situation.\n\n\nBased on [this](https:\/\/www.kaggle.com\/tobikaggle\/keras-mnist-cnn-learning-curve) Kaggle notebook. Other examples showing learning curves: [[1]](https:\/\/www.dataquest.io\/blog\/learning-curves-machine-learning\/), [[2]](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/ch04.html)","0f169c60":"![model](.\/model.png)"}}