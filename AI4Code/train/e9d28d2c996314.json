{"cell_type":{"771bd67f":"code","b8a35d18":"code","40b770c6":"code","faf85c5c":"code","75ed5179":"code","a9eaa867":"code","d2d23f27":"code","f7a1e2d0":"code","99344944":"code","2ffff543":"code","6117d471":"code","46c72e12":"code","41a60e48":"code","03576566":"code","b708737e":"code","dcae24a4":"code","f3fc460d":"code","2fbc1500":"code","d673f73d":"code","4ef14276":"code","3496ab07":"code","21647495":"code","bf418488":"code","d4eb8270":"code","7fa7e462":"code","4d2bc55b":"code","c5a9100a":"code","f545e96c":"code","1b8c1434":"code","1ad5259c":"code","14553279":"code","72a6cefb":"code","129093e0":"markdown","a1ee7a6c":"markdown","4a06a89f":"markdown","abd91275":"markdown","abd7703f":"markdown","afbcaece":"markdown","be384712":"markdown","d5b5b9ce":"markdown","897e4ba7":"markdown","34fd2656":"markdown","37061d6a":"markdown","c53bfb66":"markdown","ebf13a02":"markdown","b8a35231":"markdown","ee5debde":"markdown","60552442":"markdown","087f8d7d":"markdown","58a4a4fa":"markdown","2f88ebc7":"markdown","4685ac38":"markdown","6de53676":"markdown","f5bc7cf8":"markdown","c96d1ce6":"markdown","5bf375ce":"markdown","3f97e6ed":"markdown","826ba540":"markdown","8351e36c":"markdown","3b4b5fc3":"markdown","8280fe3f":"markdown","18afb642":"markdown","1751b158":"markdown","e6704948":"markdown","644cd24a":"markdown","b094d1e1":"markdown","3c8a1ba0":"markdown","b02e5997":"markdown","810202cb":"markdown","95cb3f21":"markdown","8fff6149":"markdown","39804298":"markdown","d014a9fc":"markdown","9d9d9827":"markdown"},"source":{"771bd67f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport math\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\n%matplotlib inline\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","b8a35d18":"df = pd.read_csv('..\/input\/weather-dataset\/train.csv').rename(str.lower,axis='columns')","40b770c6":"print(df.info())\nprint('-----------')\nprint(df.shape)\ndf.head(5)","faf85c5c":"sns.set(style=\"darkgrid\")\npalette=[\"#ff000d\",\"#82cafc\"]\nax = sns.countplot(y=\"label\", data=df, palette=palette)\nplt.title('Data balance')\ndf.label.value_counts()\ntotal = len(df['label'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()\/total)\n        x = p.get_x() + p.get_width()\n        y = p.get_y() + p.get_height()\/2\n        ax.annotate(percentage, (x, y))\nplt.show()","75ed5179":"df6 = df.loc[:,('feature_6','label')]\ndf6.loc[:,'feature_6'] = df6.loc[:,'feature_6'].str.strip('a').astype('int64').sort_values()\n\nfig = plt.figure(figsize=(15,5), constrained_layout=True)\ngs = fig.add_gridspec(1, 2)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nsns.countplot(y=df6.loc[df.label == 0,'feature_6'], ax=ax1, data=df6,palette=\"Reds_d\")\nsns.countplot(y=df6.loc[df.label == 1,'feature_6'], ax=ax2,data=df6, palette=\"Blues_d\")\nax2.set_xticklabels(range(0,800,100))\nax1.set_title('Sunny days')\nax2.set_title('Rainy days')\n\ndf_helper = df6.groupby('feature_6')['label'].value_counts(normalize=True)\ndf_helper = df_helper.mul(100).rename('percent').reset_index()\nax3 = sns.catplot(x='feature_6',y='percent',hue='label',kind='bar',\n                data=df_helper,palette=palette,height=5,\n                aspect=3)\nax3.ax.set_ylim(0,100)\nfor p in ax3.ax.patches:\n    txt = str(p.get_height().round(1)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    ax3.ax.text(txt_x,txt_y,txt)\nplt.show()","a9eaa867":"df_helper = df.groupby('feature_9')['label'].value_counts(normalize=True)\ndf_helper = df_helper.mul(100).rename('percent').reset_index()\nax1 = sns.catplot(x='feature_9',y='percent',hue='label',kind='bar',\n                data=df_helper,palette=palette,height=5,\n                aspect=3)\nax1.ax.set_ylim(0,100)\nax1.ax.set_title('Percentage presentation of feature_9')\nfor p in ax1.ax.patches:\n    txt = str(p.get_height().round(1)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    ax1.ax.text(txt_x,txt_y,txt)\ndf_helper = df.groupby('year')['label'].value_counts(normalize=True)\ndf_helper = df_helper.mul(100).rename('percent').reset_index()\nax2 = sns.catplot(x='year',y='percent',hue='label',kind='bar',\n                data=df_helper,palette=palette,height=5, aspect=3)\nax2.ax.set_ylim(0,100)\nax2.ax.set_title('Percentage presentation of year feature')\nfor p in ax2.ax.patches:\n    txt = str(p.get_height().round(1)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    ax2.ax.text(txt_x,txt_y,txt)","d2d23f27":"plt.figure(figsize=(8,5))\nf_13_c = df.feature_13.value_counts()\nax = sns.barplot(f_13_c.index, f_13_c.values, alpha=0.9,palette=palette)\nax.set_title('Frequencies of the categories in feature_13.')\n\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')","f7a1e2d0":"df_cat = df[['feature_5','feature_18','feature_19','label']].astype('category')\n\nfig = plt.figure(figsize=(12,10))\nplt.suptitle(\"Comparison between sunny days and rainy days with 'letters' features\", ha='center',size= 18)\ngs = fig.add_gridspec(3, 2)\n\ncmap = plt.get_cmap('Reds')\ncolor_reds = [cmap(i) for i in np.linspace(0, 1, 16)]\n\ncmap = plt.get_cmap('Blues')\ncolors_blues = [cmap(i) for i in np.linspace(0, 1, 16)]\n\nfor i in range(0,3):\n    labels = df_cat.loc[df_cat.label == 0].iloc[:,i].astype('category').cat.categories.tolist()\n    counts = df_cat.loc[df_cat.label == 0].iloc[:,i].value_counts()\n    sizes = [counts[j] for j in labels]\n    plt.subplot(gs[i, 0])\n    plt.pie(x=sizes, labels=labels, autopct='%1.1f%%',\n        labeldistance=1.1,pctdistance=0.7 ,textprops={'fontsize': 8.5},shadow=True,radius=2, colors=color_reds)\n    plt.title('%s Sunny day' %(df_cat.columns[i]),y=1.5)\n\nfor i in range(0,3):\n    labels = df_cat.loc[df_cat.label == 1].iloc[:,i].astype('category').cat.categories.tolist()\n    counts = df_cat.loc[df_cat.label == 1].iloc[:,i].value_counts()\n    sizes = [counts[j] for j in labels]\n    plt.subplot(gs[i, 1])\n    plt.pie(x=sizes, labels=labels, autopct='%1.1f%%',\n        labeldistance=1.1,pctdistance=0.7 ,textprops={'fontsize': 8.5},shadow=True,radius=2,colors=colors_blues)\n    plt.title('%s Rainy day' %(df_cat.columns[i]),y=1.5)\n\nfig.patch.set_edgecolor('black')  \nfig.patch.set_linewidth('1') \nfig.tight_layout()    \nfig.subplots_adjust(hspace = 1.2,left=0.05)\nplt.show()","99344944":"fig = plt.figure(figsize=(8, 5))\nnull_columns=df.columns[df.isnull().any()]\nax = df[null_columns].isnull().sum().plot(kind='bar',color='black')\nfor p in ax.patches:\n    ax.annotate(p.get_height(), (p.get_x(), p.get_height()+2))\nplt.title('Number of null values in the data')\nplt.xlabel('Features with null values')\nplt.ylabel('Number of null values')\nplt.show()","2ffff543":"continuous_vars = df.drop(['feature_5','feature_9','feature_13','feature_18','feature_19','year','label'],axis=1)\nlabels = df.label\ncontinuous_vars['feature_6'] = continuous_vars['feature_6'].str.strip('a').astype('int64')\ncontinuous_vars['feature_14'] = continuous_vars['feature_14'].str.strip('mm').astype('float64')\ncontinuous_vars = (continuous_vars - continuous_vars.mean()) \/ (continuous_vars.std())\ncontinuous_df = pd.concat([labels,continuous_vars],axis=1)\ncontinuous_df = pd.melt(continuous_df,id_vars='label',var_name='features', value_name='value')\nplt.figure(figsize=(20,10))\nsns.boxplot(x='features', y='value', hue='label', data=continuous_df,palette=\"Set1\")\nplt.xticks(rotation=45)\nplt.title('Box plots to reveal outliers')\nplt.show()","6117d471":"fig3 = plt.figure(figsize=(25, 15))\ngs1 = fig3.add_gridspec(3, 6)\ncv = continuous_vars.drop(['feature_6'],axis=1)\ncv_shape = np.array(cv.columns).reshape(3,6)\nfor i in range(3):\n    for j in range(6):\n        with sns.axes_style(style=\"darkgrid\"):\n            ax = fig3.add_subplot(gs1[i,j])\n            ax.set_title(f\"Distribution of {(cv_shape[i,j])}\",fontsize=15)\n            sns.distplot(cv[cv_shape[i,j]], color=\"darkblue\" ,kde_kws={\"color\": \"black\"},axlabel=False)\nplt.show()","46c72e12":"skew_features = continuous_vars[['feature_0', 'feature_1', 'evaporation','feature_14', 'sunshine','windgustspeed', 'feature_21',\n        'feature_23', 'feature_24']]\n\nfig4 = plt.figure(figsize=(25, 15))\ngs2 = fig4.add_gridspec(3, 3)\nskew_features_shape = np.array(skew_features.columns).reshape(3,3)\nold_settings = np.seterr(all='ignore')\nfor i in range(3):\n    for j in range(3):\n        with sns.axes_style(style=\"darkgrid\"):\n            ax = fig4.add_subplot(gs2[i,j])\n            ax.set_title(f\"Distribution after log-transform {(skew_features_shape[i,j])}\",fontsize=15)\n            sns.distplot(np.sqrt(skew_features[skew_features_shape[i,j]]), axlabel=False,color='firebrick',kde_kws={\"color\": \"black\"})\nold_settings = np.seterr(all='warn')         \nplt.show()","41a60e48":"def train_preprocess(X):\n    \n    X['feature_14'] = X['feature_14'].str.strip('mm').astype('float64')\n    X['feature_6'] = X['feature_6'].str.strip('a').astype('float64')\n    c_df = ['float64','int64','float32','int32']\n    letter_df = ['feature_5', 'feature_18', 'feature_19']\n    remove = ['feature_1','feature_0','feature_24']\n    for i in X.columns:\n        if X[i].dtype == 'object':\n            X[i] = X[i].astype('category')\n        if i in letter_df:\n            newl = []\n            for j in range(ord('A'),ord('Q')):\n                relative = 0\n                relative = math.ceil(X[i].value_counts(normalize=True)[chr(j)] * 100)\n                newl += chr(j) * relative\n            for j in range(len(X[i])):\n                if X[i].isnull()[j]:\n                    X[i][j] = random.choice(newl)\n        if X[i].dtype in c_df:\n            X[i].fillna(X[i].median(), inplace=True)  \n    ind = X[(X['feature_13'] == 'unknown')].index\n    remove_index = []\n    for i in X.columns:\n        if i in remove:\n            q1, q3, iqr = 0, 0, 0\n            q1 = np.quantile(X[i], 0.25)\n            q3 = np.quantile(X[i], 0.75)\n            iqr = q3 - q1\n            for j in range(len(X)):\n                if X[i][j] < (q1 - 1.5 * iqr) or X[i][j] > (q3 + 1.5 * iqr):\n                    remove_index.append(j)\n    for i in range(len(ind)):\n        if int(ind[i]) not in remove_index:\n            remove_index.append(int(ind[i]))\n    l = sorted(list(set(remove_index)))\n    X = X.drop(X.index[l]).reset_index(drop=True)\n    X['feature_13'] = X['feature_13'].astype('float64')\n    return(X)\ndf = train_preprocess(df)","03576566":"def k_fold_cv(x, y, clf, k = 10):\n    data = pd.concat([x,y],axis=1)\n    folds = np.array_split(data, k)\n    new_list = []\n    for i in range(k):\n        train = folds.copy()\n        validation = folds[i]\n        del train[i]\n        train = pd.concat(train, sort=False)\n        x_train = train.values[:,:-1]\n        x_validation = validation.values[:,:-1]\n        y_train = train.values[:, -1]\n        y_validation = validation.values[:, -1]\n        clf.fit(x_train, y_train)\n        y_proba = clf.predict_proba(x_validation)\n        fpr, tpr, thresholds = roc_curve(y_validation, y_proba[:,1])\n        new_list.append(auc(fpr, tpr))\n    return np.mean(new_list)","b708737e":"y = df.label\nX = df.drop(['label'],axis=1)\nX_num = X.select_dtypes(exclude=['category'])\nX_cat = X.select_dtypes(include=['category'])\ncolumnsToEncode = X.select_dtypes(include=['category']).columns","dcae24a4":"mask = np.triu(np.ones_like(continuous_vars.corr(), dtype=np.bool))\nf,ax_h = plt.subplots(figsize=(10, 10))\nsns.heatmap(continuous_vars.corr(), mask=mask, annot=True, linewidths=.5, fmt= '.1f',ax=ax_h, cmap=\"Blues\")\nplt.show()","f3fc460d":"fig = plt.figure(figsize=(24,13),constrained_layout=True)\ngs = fig.add_gridspec(4, 2)\nax1 = fig.add_subplot(gs[0,0])\nax2 = fig.add_subplot(gs[1,0])\nax3 = fig.add_subplot(gs[2,0])\nax4 = fig.add_subplot(gs[3,0])\nax5 = fig.add_subplot(gs[0,1])\nax6 = fig.add_subplot(gs[1,1])\nax7 = fig.add_subplot(gs[2,1])\nax8 = fig.add_subplot(gs[3,1])\n\nwith sns.axes_style(style=\"darkgrid\"):\n    ax1.set_title('LinearReg between feature_0 and evaporation')\n    sns.regplot(x='feature_0',y='evaporation', data=df, ax=ax1,line_kws={'color':'Red','alpha': 0.3})\n    ax2.set_title('LinearReg between feature_1 and evaporation')\n    sns.regplot(x='feature_1',y='evaporation', data=df, ax=ax2,line_kws={'color':'Red','alpha': 0.3})\n    ax3.set_title('LinearReg between feature_12 and feature_11')\n    sns.regplot(x='feature_12',y='feature_11', data=df, ax=ax3,line_kws={'color':'Red','alpha': 0.3})\n    ax4.set_title('LinearReg between feature_17 and maxtemp')\n    sns.regplot(x='feature_17',y='maxtemp', data=df, ax=ax4,line_kws={'color':'Red','alpha': 0.3})\n    ax5.set_title('LinearReg between feature_16 and maxtemp')\n    sns.regplot(x='feature_16',y='maxtemp', data=df, ax=ax5,line_kws={'color':'Red','alpha': 0.3})\n    ax6.set_title('LinearReg between feature_17 and feature_8')\n    sns.regplot(x='feature_17',y='feature_8', data=df, ax=ax6,line_kws={'color':'Red','alpha': 0.3})\n    ax7.set_title('LinearReg between feature_1 and feature_0')\n    sns.regplot(x='feature_1',y='feature_0', data=df, ax=ax7,line_kws={'color':'Red','alpha': 0.3})\n    ax8.set_title('LinearReg between feature_17 and feature_16')\n    sns.regplot(x='feature_17',y='feature_16', data=df, ax=ax8,line_kws={'color':'Red','alpha': 0.3})\nplt.show()","2fbc1500":"v,std,c,n = [],[],[],[]\nfor i in X_num.columns:\n    v.append(np.var(X_num[i]))\n    std.append(np.std(X_num[i]))\n    c.append(abs(y.corr(X_num[i])))\n    n.append(str(i))\ntempdf = pd.DataFrame(list(zip(n,v,std,c)),columns=['Features','Variance','STD','Correlation'])\ntempdf = tempdf.sort_values(by=['STD'],ascending = False)\n\nfig, ax1 = plt.subplots(figsize=(28, 13))\nplt.title(label=\"Standard Deviation and Correlation vs Features\",fontdict={'fontsize':20})\nax1.set_xlabel('Features',fontdict={'fontsize':20})\nax1.set_ylabel('<- Standard Deviation',fontdict={'fontsize':20})\nax1.plot(tempdf.values[:,0],\n         tempdf.values[:,2],label ='Standard Deviation',color='Blue',marker='o')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\nax2.set_ylabel('<- Correlation with labels',fontdict={'fontsize':20})\nax2.plot(tempdf.values[:,0],\n         tempdf.values[:,3],color='Red',marker='o')\nax2.tick_params(axis='y')\nax1.axhline(y=1,color='Orange')\nap = {'arrowstyle':'->', 'color': 'black', \"connectionstyle\":\"arc3,rad=-0.2\"}\nbbx = {'pad':4, 'edgecolor':'orange', 'facecolor': 'orange', 'alpha':0.4}\nax1.annotate(\"Features with low variance\", xy=('year', 1), xytext=('feature_9', 40),arrowprops=ap,bbox={'pad':4, 'edgecolor':'orange', 'facecolor': 'red', 'alpha':0.4})\nax2.annotate(\"Feature 1 correlation\", xy=('feature_1', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_1']))), xytext=('feature_1', 0.2),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Evaporation correlation\", xy=('evaporation', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['evaporation']))), xytext=('feature_17', 0.1),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Feature 12 correlation\", xy=('feature_12', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_12']))), xytext=('feature_12', 0.3),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Feature 17 correlation\", xy=('feature_17', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_17']))), xytext=('feature_14', 0.05),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Maxtemp correlation\", xy=('maxtemp', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['maxtemp']))), xytext=('maxtemp', 0.15),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Feature 0 correlation\", xy=('feature_0', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_0']))), xytext=('year', 0.25),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Feature 16 correlation\", xy=('feature_16', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_16']))), xytext=('feature_17', 0.2),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Feature 8 correlation\", xy=('feature_8', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_8']))), xytext=('feature_8', 0.05),arrowprops=ap,bbox=bbx)\nax2.annotate(\"Feature 11 correlation\", xy=('feature_11', float(\"%.6f\" %(tempdf.set_index('Features', inplace=False).Correlation['feature_11']))), xytext=('feature_11', 0.35),arrowprops=ap,bbox=bbx)\nfig.tight_layout()\nplt.show()","d673f73d":"scaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X_num, y)\nX_scaled = pd.DataFrame(X_scaled,columns=X_num.columns,index=X_num.index)\nselect_feature = SelectKBest(chi2, k=5).fit(X_scaled, y)\nnewpd = pd.DataFrame(list(zip(select_feature.scores_,\n                  X_scaled.columns)),columns=['score','features'])\nnewpd = newpd.sort_values(by=['score'],ascending=False)\nnewpd.reset_index(inplace=True)\nnewpd = newpd.drop(['index'],axis=1)\nprint(newpd)","4ef14276":"reg = LogisticRegression(solver='liblinear')\nX_num_t = X_num.copy()\nscore_list = []\ndiff = []\nbaseline = k_fold_cv(X_num_t,y,reg)\nfor i in newpd.features.iloc[21:0:-1]:\n    auc1 = k_fold_cv(X_num_t,y,reg)\n    score_list.append(auc1)\n    diff.append(baseline - auc1)\n    baseline = auc1\n    X_num_t = X_num_t.drop(i,axis=1)\n\nnewl = list(newpd.features.iloc[21:0:-1].values)\nplt.figure(figsize=(15, 6),constrained_layout=True)\nplt.plot(newpd.features.iloc[21:0:-1], score_list)\nplt.yticks(np.arange(0.8,0.88,step=0.02))\nplt.ylabel('<- AUC')\nplt.xlabel('Feature included before removing each')\nplt.title('AUC as we drop least important feature and difference it makes')\nfor i in range(20):\n    if diff[i] < 0.0009:\n        if i % 2 == 0:\n            plt.annotate('%0.5f' %diff[i], xy=(newl[i],score_list[i]), xytext=(newl[i],score_list[i] - 0.008),arrowprops=ap,bbox=bbx)\n        else:\n            plt.annotate('%0.5f' %diff[i], xy=(newl[i],score_list[i]), xytext=(newl[i],score_list[i] - 0.02),arrowprops=ap,bbox=bbx)\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.show()","3496ab07":"X_lr = pd.concat([X_scaled,X_cat],axis=1)\nX_lr = X_lr.drop(['feature_17','feature_24','feature_8','feature_1','maxtemp','feature_12'],axis=1)\nX_lr = pd.get_dummies(X_lr, columns=columnsToEncode, drop_first=True)\n\nreg_penalty = ['l1', 'l2']\npowers = range(-10,0)\nCs = [10**p for p in powers]\nreg_solver = ['liblinear']\ngrid = dict(penalty=reg_penalty, C=Cs, solver=reg_solver)\nmodel = LogisticRegression()\ngrid = GridSearchCV(estimator=model, param_grid=grid, scoring='roc_auc', cv=10,n_jobs=-1)\ngrid_result = grid.fit(X_lr, y)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_,\n                             grid_result.best_params_))\nfig = plt.figure(figsize=(8,6))\nline1 = plt.plot(powers,grid_result.cv_results_['mean_test_score'][:10],label ='Lasso Regularization',color='blue', linewidth=2, alpha=0.6)\nline2 = plt.plot(powers,grid_result.cv_results_['mean_test_score'][10:],label ='Ridge Regularization',color='green', linewidth=2, alpha=0.6)\nplt.xlabel('Power values')\nplt.ylabel('AUC ->')\nplt.xticks(powers)\nplt.title(label=\"Power & Penalty vs AUC\")\nplt.legend(loc=0)\nplt.show()","21647495":"X_gnb1 = X_num.copy()\nX_gnb2 = X_num.drop(['feature_13','year','feature_9'],axis=1)\nX_gnb3 = X_num.drop(['feature_13','year','feature_9','feature_1','feature_12','feature_17','maxtemp','feature_0'],axis=1)\nX_gnb4 = X_num.drop(['feature_21','evaporation','feature_8','feature_13','year','feature_9','feature_1','feature_12','feature_17','maxtemp','feature_0'],axis=1)\n\ngnb_models = [X_gnb1,X_gnb2,X_gnb3,X_gnb4]\nnames = ['Numerical features','Continuous features','Continuous features\\n after removing\\n correlated features\\n > 0.8 corr','Continuous features\\n after removing\\n correlated features\\n > 0.6 corr']\nresults = []\nmodel = GaussianNB()\nfor i in gnb_models:\n    cv_results = cross_val_score(model,i, y, cv=10, scoring='roc_auc')\n    results.append(cv_results)\nfig = plt.figure(figsize=(10,7))\nsns.axes_style(style=\"darkgrid\") \nax = sns.boxplot(data=results,orient='v').set(xticklabels=names)\nplt.title('Gaussian Naive Bayes models comparison',fontsize=15)\nfig.text(0.02, 0.5, 'AUC ->', va='center', rotation='vertical',fontsize=10)\nplt.show()","bf418488":"X_svm = scaler.fit_transform(X_gnb3, y)\nX_svm = pd.DataFrame(X_svm,columns=X_gnb3.columns,index=X_gnb3.index)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=10,n_jobs=-1)\ngrid_result = grid.fit(X_svm, y)\n\nprint(\"Best AUC: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nfig = plt.figure(figsize=(8,6))\nline1 = plt.plot(c_values,grid_result.cv_results_['mean_test_score'][0:21:2],label ='linear kernel',color='blue', linewidth=2, alpha=0.6)\nline2 = plt.plot(c_values,grid_result.cv_results_['mean_test_score'][1:20:2],label ='poly kernel',color='green', linewidth=2, alpha=0.6)\nplt.xlabel('C values')\nplt.ylabel('AUC ->')\nplt.xticks(c_values)\nplt.title(label=\"C values & kernel vs AUC\")\nplt.legend(loc=0)\nplt.show()","d4eb8270":"X_num = X_num.drop(['feature_13'],axis=1)\nne = list(range(200,1400,200))\nres = []\nfor i in ne:\n    rfc = RandomForestClassifier(n_estimators=i, criterion='entropy', max_depth=100,max_features='sqrt',bootstrap=True)\n    res.append(cross_val_score(rfc, X_num, y, cv=10, scoring='roc_auc',n_jobs=-1).mean())","7fa7e462":"print('Best AUC: %.3f with 1200 numbers of trees' %(max(res)))\nplt.style.use('seaborn-darkgrid')\nplt.plot(ne,res,color='red', linewidth=2, alpha=0.6)\nplt.xticks(ne)\nplt.xlabel('Number of trees in the forest')\nplt.ylabel('AUC')\nplt.title(label=\"Number of trees in the forest vs AUC\")\nplt.show()","4d2bc55b":"def KfoldPlot(X, y, clf, name):\n    \n    X = X.values\n    y = y.values\n    tprs, aucs = [], []\n    mean_fpr = np.linspace(0, 1, 100)\n    fig = plt.figure(figsize=(10, 4),constrained_layout=True)\n    gs = fig.add_gridspec(1, 2)\n    ax1 = fig.add_subplot(gs[0,0])\n    ax2 = fig.add_subplot(gs[0,1])\n    kf = KFold(n_splits=10, random_state=None, shuffle=False)\n    for i, (train, validation) in enumerate(kf.split(X, y)):\n        clf.fit(X[train], y[train])\n        y_proba = clf.predict_proba(X[validation])\n        fpr, tpr, thresholds = roc_curve(y[validation], y_proba[:,1])\n        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(auc(fpr, tpr))\n        ax1.plot(fpr, tpr, color='#D3D3D3', alpha=.8)\n    X_train_cm, X_validation_cm, y_train_cm, y_validation_cm = train_test_split(X, y, test_size=0.2, random_state=42)\n    clf.fit(X_train_cm, y_train_cm)\n    cm = confusion_matrix(y_validation_cm, clf.predict(X_validation_cm))\n    tn, fp, fn, tp = cm.ravel()\n    cm = np.array([[tn,fn],[fp,tp]])\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    ax2 = sns.heatmap(cm,annot=True,fmt='g',cmap='Blues')\n    ax2.set(title=\"Confusion Matrix\")\n    ax2.set_xlabel('Actual')\n    ax2.set_ylabel('Predicted')\n    ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, alpha=.8, color='darkBlue')\n    ax1.plot(mean_fpr, mean_tpr, color='Red', label=r'Mean ROC (AUC = %0.3f)' % (mean_auc), lw=2, alpha=.9)\n    ax1.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver Operating Characteristic\")\n    ax1.legend(loc=\"lower right\")\n    ax1.set_xlabel('False Positive Rate')\n    ax1.set_ylabel('True Positive Rate')\n    ax1.patch.set_edgecolor('black')\n    ax1.patch.set_linewidth('1')  \n    ax1.patch.set_facecolor('white')\n    ax1.grid(False)\n    print(\"Validation AUC = %0.3f for %s model\" %(mean_auc, name))\n    plt.show()\n\nreg = LogisticRegression(C=0.1,solver='liblinear',penalty='l1')\nKfoldPlot(X_lr, y, reg, 'Logistic Regression')","c5a9100a":"gnb = GaussianNB()\nKfoldPlot(X_gnb3, y, gnb, 'Gaussian Naive Bayes')","f545e96c":"svm = SVC(C=0.3, kernel='linear',probability=True)\nKfoldPlot(X_svm, y, svm, 'Support Vector Machine')","1b8c1434":"rfc = RandomForestClassifier(n_estimators=1200, criterion='entropy', max_depth=100,max_features='sqrt',bootstrap=True)\nKfoldPlot(X_num, y, rfc, 'Random Forest Classifier')","1ad5259c":"def test_preprocess(X):\n    X['feature_14'] = X['feature_14'].str.strip('mm').astype('float64')\n    X['feature_6'] = X['feature_6'].str.strip('a').astype('float64')\n    c_df = ['float64','int64','float32','int32']\n    letter_df = ['feature_5', 'feature_18', 'feature_19']\n    for i in X.columns:\n        if X[i].dtype == 'object':\n            X[i] = X[i].astype('category')\n        if i in letter_df:\n            newl = []\n            for j in range(ord('A'),ord('Q')):\n                relative = 0\n                relative = math.ceil(X[i].value_counts(normalize=True)[chr(j)] * 100)\n                newl += chr(j) * relative\n            for j in range(len(X[i])):\n                if X[i].isnull()[j] == True:\n                    X[i][j] = random.choice(newl)\n        if (X[i].dtype in c_df):\n            X[i].fillna(X[i].median(), inplace=True)  \n    return(X)","14553279":"X_test = pd.read_csv('..\/input\/test-without-target\/test_without_target.csv').rename(str.lower,axis='columns')\nX_test = X_test.drop(X_test.columns[0], axis=1)\nX_test = test_preprocess(X_test)\nX_test_num = X_test.select_dtypes(exclude=['category'])","72a6cefb":"rfc.fit(X_num,y)\npred_proba = rfc.predict_proba(X_test_num)[:,1]\n#pd.DataFrame(pred_proba).to_csv(\"Submission.csv\")","129093e0":"There is a strong correlation between a few features, but before we decide if we can drop some of them, we will check their homoschedasticity to see their real association.","a1ee7a6c":"According to this plot, we can see that we have six features that have very low variance, but on the other hand, some features are correlated to the target, so it will be a waste to give up on them. Moreover, we see that some features have high variance but not so correlated with labels.\nWe will check their scores with feature selection with a Chi-squared test. For this purpose, we will scale our feature to be between one to zero.","4a06a89f":"The function that we built, 'train_preprocess', will make the necessary adjustments for data from the same origin. <br> Therefore, we will use a simple logistic regression model that will test our accommodation. We will save the result to decide which and how many features to include for our tests. According to our guidelines, our scoring method is Area Under the Curve, \"AUC.\"\n\nTo validate our models, we use a k-fold cross-validation function. The function divides the data into ten (k=10) partitions. Nine partitions are used as a train set and one as a validation set. We fit the model with the train set and evaluate it with the validation set.","abd91275":"We see that features 0,1 and 24 have outliers, while the other features have extreme values.  ","abd7703f":"For feature selection, we treat our numeric and categorical data separately. \nRegarding the categorical data, we will use pandas get_dummies function with minor modifications for each model. \nConcerning the numeric data, we will calculate correlation, variance, and Chi-squared test to select the best features for model AUC optimization.","afbcaece":"## *3: Build and train models on the data*","be384712":"## Submission details: \n- Course Name: Intro to Machine Learning\n- Faculty: Faculty of Engineering, Tel-Aviv University\n- Name: Dr. Yonatan Jenudi\n- Date: 16.7.2020","d5b5b9ce":"### Dataset description\nThe unprocessed data contains 26 features, divided into 17 float\/continuous features, six categorical features, two int type, and one 'label' (means targets).\nThe dimension of the dataset is 25 (without the labels), and it has 22,161 rows\/observations.\nThe only features known by their names are 'evaporation', 'maxtemp', 'sunshine', 'windgustspeed' and 'year'.\n\nFor every feature that we explore and visualize, we would comment about how we will pre-process it. That way, we will be able to build one function that will pre-process all the data, so we can apply it on the test data and future to come data.","897e4ba7":"To explore our dataset, we want to examine general information about the data and describe it.","34fd2656":"The first model that we want to examine is Logistic Regression. To avoid the multicollinearity problem, we will remove features with high correlation and use the argument drop_first so that it will return P - 1 dummies columns. Also, we use the normalized data between 0 and 1 (X_scaled).\n\nAfterward, we will use GridSearch with Cross-Validation to find the most suitable hyperparameters.","37061d6a":"### Gaussian Naive Bayes","c53bfb66":"This plot demonstrates the importance of the features for the accuracy of the prediction and how much it contributes and could help us drop features.\n\nFirst, we will drop all the features that worsen our AUC: 17, 24, 8, and 1.\n\nSecond, we can use the plot to remove features that we know to have a high correlation with each other to avoid multicollinearity problems in logistic regression: 12 and 'maxtemp.' These features will be removed only for models that we want to avoid multicollinearity.","ebf13a02":"Both plots, representing feature_9 and 'year' features, show that all categories look pretty much the same, without a significant change in the baseline of having a rainy day. Without calculating, we can notice that the variance of both features is low. We will use this fact in feature selecting. Both features will be converted to category type.","b8a35231":"## Intro\nThis project is the final project of the course taken in Tel-Aviv University, Faculty of Engineering, as a part of the BS.c degree 'Sciences for High-Tech.'\nIn this project, we are given a big data set with several features and observations that each describes a day. Our role is to build models that will predict the probability of a rainy day. Our project deals with Binary Classification with two categories classified as 1 - it was a rainy day or 0 - it was not a rainy day according to the features. Some of the features are known, and some aren't.","ee5debde":"Until now, we were exploring and visualized the data set. \nHere are some important insights about how to pre-process the data:\n* Feature 5, 18, and 19 are letters and also contain null values. There is no dominant letter that we can replace all null values with, so we will replace those nulls with the relative appearances in their features. Our categorical features will be encoded with \"get dummies,\" there are some other encoders that we could use. Still, because we have only three categorical features that we want to encode, and know that the dimension of the data would not get much more prominent after using \"get dummies\" (the curse of dimensionality), we use this method. Moreover, after feature selection, our dimension would be shrunken.\n* In our continuous features that include null values, we will convert them to the median of the column. Technically, we could drop all rows with the missing values, and still have a sufficient amount of data. The reason for picking the first method is because we want the data to remain with 'noise' and make it less perfect to avoid overfitting.\n* Feature 13 is a 'boolean' feature (1 or 0), also contains 81 'unknown' values that we will drop from our data.\n* Features 0, 1, and 24 have outliers. Therefore, we will drop the rows that contain outliers from the data.\n* Features 14 and 6, will be stripped off from their built-in string components.\n* Every 'object' data type will be converted into 'category.' Typecast categorical features to a 'category' data type because they make the operations on such columns faster than the 'object' data type.","60552442":"For the final model, we use only our numerical features without scale the data nor droping correlated features, we also drop feature_13. Random forest classifier is less affected by multicollinearity.\nTo find the best hyperparameters, we use an iteration over the number of trees in the forest.","087f8d7d":"After Chi-squared test, we can see all the features by their Chi-squared test score, to decide if we want to remove any of them, we will check their performance by their score. First, we will check AUC when we use all the features, and in every iteration, we will drop the feature with the lowest Chi-squared test score and then recheck AUC.","58a4a4fa":"In our test dataset preprocess, we need to execute the same adjustments we made in our train data, besides deleting rows. Hence, we will change our preprocessing function accordingly.","2f88ebc7":"The next model that we want to examine is Gaussian Naive Bayes. The model assumes that the features used are conditionally independent and are normally distributed. \n\nWe will compare four models: only the numerical features, the continuous features, the continuous features after dropping highly correlated features (features that correlate higher than 0.8), and continuous features after dropping highly correlated features (features that correlate higher than 0.6).","4685ac38":"According to these pie charts, we can see that each feature's relative appearances are close to each other, and there is not a dominant 'letter.' Hence, in the pre-process, we will replace the null\/NaN values in these features by letters by to their relative appearances.\n\nNext, we will examine how many null values our data contains.","6de53676":"For the next model, we use our continuous features after dropping highly correlated features normalized between 0 and 1.\nTo find the best hyperparameters, we use a grid search with two kernel values: 'linear' and 'poly.'","f5bc7cf8":"### Logistic Regression","c96d1ce6":"## *1. Exploration & Visualization*","5bf375ce":"Info about the confusion matrix, the confusion matrix contains four parts:\n* Up left: the model predicted negative, and true values are negative.<br>\n* Up right: the model predicted negative and true values are positive.<br>\n* Down left: the model predicted positive and true values are negative.<br>\n* Down right: the model predicted positive, and true values are positive.<br>\n\n\nWe know that none of our models overfits. We can see this by the differents ROC curves that we printed for each cross-validation. If one of the models was overfitted, the ROC curves could look different for the overfitting model.<br>\nThe best model that we trained in is the Random Forest Classifier, so this is the model that we will use for predictions.","3f97e6ed":"## *4: Evaluate the models that were trained*","826ba540":"In feature_13 count plot, we can observe that there are 81 'unknown' observations. Because our data contains a total of 22161 examples, deleting 81 of them will not change the overall project results. \n\nNext, from the info table, we can notice to have three categorical variables that contain alphabetic letters, Feature5\/18\/19. Before we decide how to convert them into numerical features, we will visualize how they split into the features.","8351e36c":"For our numerical features, we will replace null values by the median of each column. \nFor our categorical features that contain letters, we will replace null values with letters by their relative appearance.\nOur data contains different units of measurements. Therefore, we will normalize our data before we examine for outliers.\n\nOur next step in the visualization stage is to check for outliers.  We will drop our categorical and int type features for this purpose.","3b4b5fc3":"The following features show a strong linear relationship:\n* feature 1 and evaporation<br> \n* feature 12 and feature 11<br> \n* feature 17 and maxtemp<br> \n* feature 16 and maxtemp<br> \n* feature 1 and feature 0<br> \n* feature 17 and feature 16<br> \n* evaporation and feature 0<br> \n* feature 17 and feature 8<br> \n\nTo decide how to handle correlated features, we check for their correlation to the target and also want to see their variance. If a feature contains a lot of the same value (low variance), the model does not have much to learn from it, that is why we will drop features with low variance.","8280fe3f":"In this project, we were illustrated how to visualize, explore, and preprocess a big data set. Then, we built and trained various machine learning models to predict outcomes. After doing so, we evaluated our classification models with ROC curves and confusion matrixes. We demonstrated that our models do not overfit. Finally, we showed that the best model to predict the labels was Random Forest Classifier, so we used it to make probabilities from test data on labels. \nAfter submitting the results to the practitioner, we got our real and final <b>AUC: 0.916<b>","18afb642":"### Summary","1751b158":"### Support Vector Machine","e6704948":"## *5: Make a prediction*","644cd24a":"The final part of the project was to make probabilities with our best model and a test set that we did not use. These results were submitted to the practitioner who has the true labels of the test data.","b094d1e1":"We can see that the best result calculated using the third model.","3c8a1ba0":"## Project Sections\n1. Exploration and visualization.\n2. Preprocessing.\n3. Build and train models on the data.\n4. Evaluate the models that were trained.\n5. Make a prediction.","b02e5997":"By the count plots, we can see that feature_6 has 31 distinct values. We also learn that there is no coherent path between feature_6 categories number and the likelihood of having a sunny\/rainy day.\n\nBy the percentage count plot, we see some \"numbers\" extraordinary regarding our baseline chance of having a rainy day. Numbers that exceeded 23% represent a more prominent probability than usual of a rainy day.\n\nThe next features that we will visualize are the int type features, feature_9, and year.","810202cb":"## *2: Preprocessing*","95cb3f21":"### Random Forest Classifier","8fff6149":"The first thing that we notice is that the data is imbalanced. When we need to decide how to handle imbalanced data, we have three options: leave it imbalanced, upsampled or downsampled. We are exploring data that represents weather, so we will assume that the data were collected from a place where it's more likely to have sunny days than rainy days. Hence, we will not correct the imbalances of the data. But for our future calculation, we still need to keep in mind that that in general, it is three times more likely to have a sunny day rather than a rainy day.\n\nNext, we can see that feature_14 was assigned as a categorical variable, and it was written into the data as a string with the ending 'mm' (presumably means millimeter). This feature should be a float type. So in our pre-process, we will remove the closing 'mm' and convert this predictor into float type.\n\nfeature_6 starts with the letter 'a' and assigned as a categorical feature, let us visualize it.","39804298":"According to these distributions and boxplots, we can see that some features are skewed or non-normally disturbed, and we suspect that they contain outliers. For this purpose, we log-transform and visualize their distributions again.","d014a9fc":"We start with a heatmap to visualize the correlation between every two features.","9d9d9827":"# End-to-end Machine Learning Project - Binary Classification Problem, 7\/2020"}}