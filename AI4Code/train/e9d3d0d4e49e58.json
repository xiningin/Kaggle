{"cell_type":{"96343775":"code","10d7a694":"code","d66aefce":"code","c6bd244a":"code","f65cacaa":"code","69bbb11f":"code","bb0f9835":"code","4ec74c84":"code","0eb89e3c":"code","9a61d1f9":"code","a0da505a":"code","5e7f5036":"code","c723346c":"code","f09eef14":"code","04b1c0e3":"code","92ddbde6":"code","74d0ad6d":"code","76fd69b7":"code","df8fbaa7":"code","25d5b1d3":"code","257ef16b":"code","44dd265a":"markdown","87eaac43":"markdown","6aa785d7":"markdown","2db41c25":"markdown","19708361":"markdown","3c6b573e":"markdown","1ae933b5":"markdown","03bb5c3f":"markdown","7d3bd7ed":"markdown","082e01f5":"markdown","d0c13038":"markdown","1fa1d477":"markdown","7bafe8ef":"markdown"},"source":{"96343775":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","10d7a694":"data = pd.read_csv(\"..\/input\/voice.csv\")","d66aefce":"data.head(10)","c6bd244a":"# Get some information about our data\ndata.info()","f65cacaa":"data.label = [1 if each == \"male\" else 0 for each in data.label]","69bbb11f":"data.info() # now we have label as integer","bb0f9835":"y = data.label.values # main results male or female\nx_data = data.drop([\"label\"], axis = 1) # prediction components","4ec74c84":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values # all data evaluated from 1 to 0","0eb89e3c":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","9a61d1f9":"# Data Shapes\nprint(\"x_train.shape : \", x_train.shape)\nprint(\"x_test.shape : \", x_test.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_test.shape : \", y_test.shape)","a0da505a":"# Transform features to rows (Transpose)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","5e7f5036":"def initializeWeightsAndBias(dimension): # according to our data dimension will be 20\n    w = np.full((dimension, 1), 0.01) \n    b = 0.0\n    return w,b","c723346c":"def sigmoid(z):\n    y_head = (1 \/ (1 + np.exp(-z)))\n    return y_head","f09eef14":"x_train.shape[1]","04b1c0e3":"def forward_backward_propogation(w, b, x_train, y_train):\n    \n    #forward propogation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1] # x_train.shape[1] is for scaling\n    \n    #backward propogation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) \/ x_train.shape[1] # x_train.shape[1] is for scaling\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1] # x_train.shape[1] is for scaling\n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","92ddbde6":"def update(w, b, x_train, y_train, learningRate, numberOfIteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iteration times\n    for i in range(numberOfIteration):\n        # make forward and backward propogation and find costs and gradients\n        cost,gradients = forward_backward_propogation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        #lets update\n        w = w - learningRate * gradients[\"derivative_weight\"]\n        b = b - learningRate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) paramters weights and bias\n    parameters = {\"weight\" : w, \"bias\" : b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation = 'vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","74d0ad6d":"def predict(w,b, x_test):\n    # x_test is an input for forward propogation\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is Male (y_head = 1)\n    # if z is smaller than 0.5, our prediction is Female (y_head = 0)\n    for i in range(z.shape[1]):\n        if z[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n    \n    return Y_prediction","76fd69b7":"def logistic_regression(x_train, y_train, x_test, y_test, learningRate, numberOfIterations):\n    dimension = x_train.shape[0] # that is 20 (feature count of data)\n    w,b = initializeWeightsAndBias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learningRate, numberOfIterations)\n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    print(\"test accuracy : {} %.\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","df8fbaa7":"#Let's try our model and check costs and prediction results.\nlogistic_regression(x_train, y_train, x_test, y_test, learningRate = 1, numberOfIterations = 100)","25d5b1d3":"logistic_regression(x_train, y_train, x_test, y_test, learningRate = 1, numberOfIterations = 1000)","257ef16b":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T, y_test.T)))","44dd265a":"<a id=\"4\"><\/a>\n***Split Operation for Train and Test Data***\n* Data is splitted for training and testing operations. We'll have %20 of data for test and %80 of data for train after split operation.","87eaac43":"<a id=\"5\"><\/a> \n***Matrix creation function for initial weight values***","6aa785d7":"<a id=\"7\"><\/a> \n***Forward and Backward Propogation***\n* Get z values from sigmoid function and calculate loss and cost. ","2db41c25":"As you see above, when the iteration is increased, accuracy increasing too. ","19708361":"<a id=\"1\"><\/a> \n**Read Data and Check Features**","3c6b573e":"# Introduction to Gender Voice Recognation with Logistic Regression\n\n## Index of Contents\n\n* [Read Data and Check Features](#1)\n* [Adjustment of Label values (male = 1, female = 0)](#2)\n* [Data Normalization](#3)\n* [Split Operation for Train and Test Data](#4)\n* [Matrix creation function for initial weight values](#5)\n* [Sigmoid function declaration](#6)\n* [Forward and Backward Propogation](#7)\n* [Updating Parameters](#8)\n* [Prediction with Test Data](#9)\n* [Logistic Regression Implementation](#10)\n* [Logistic Regression with sklearn](#11)","1ae933b5":"<a id=\"3\"><\/a> \n***Data Normalization***","03bb5c3f":"<a id=\"2\"><\/a> \n***Adjustment of Label values (male = 1, female = 0***\n* After getting information about data we'll call male as 1 and female as 0***","7d3bd7ed":"<a id=\"9\"><\/a> \n***Prediction with Test Data***\n* Prediction using test data which is splitted first.","082e01f5":"<a id=\"6\"><\/a> \n***Sigmoid function declaration***","d0c13038":"<a id=\"10\"><\/a> \n***Logistic Regression Implementation***","1fa1d477":"<a id=\"11\"><\/a> \n***Logistic Regression with sklearn***\n* Logistic Regression Classification can be done with sklearn library. All codes which are written above correspond to the codes below.\n","7bafe8ef":"<a id=\"8\"><\/a> \n***Updating parameters***\n* Our purpose is find to optimum weight and bias values using derivative of these values."}}