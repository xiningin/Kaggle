{"cell_type":{"0fb0c77c":"code","c2aaa22e":"code","882ef6de":"code","55bd6a37":"code","cb0e9849":"code","9ceeed59":"code","2e3ab8ee":"code","fe2b1bb6":"code","a9741f2b":"code","a7a45b85":"code","29525ae9":"code","75dbf4dc":"code","f725ab1a":"markdown","51a9b899":"markdown","5472d480":"markdown","d57b529c":"markdown","38c1ec74":"markdown","0c7d27b4":"markdown","8b9aa496":"markdown"},"source":{"0fb0c77c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n#from tqdm import tqdm\n\n#plt.style.use('ggplot')\nplt.style.use('fivethirtyeight')\npd.set_option('display.max_columns',None)\ntf.version.VERSION","c2aaa22e":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(f'There are {len(df_train)} training images and {len(df_test)} test images')\ndf_train.head()","882ef6de":"## Decompressed the raw dataset\ny = df_train['label']\nX = df_train.drop(columns='label',axis=1)\n\nX = (np.expand_dims(X,axis=-1)\n     .reshape((len(X),28,28))\n     .astype(np.float32))\n\ntest_X = (np.expand_dims(df_test,axis=-1)\n          .reshape((len(df_test),28,28))\n          .astype(np.float32))\n\nX = X \/ 255.0\ntest_X = test_X \/ 255.0\n\nX = np.expand_dims(X,axis=-1)\ntest_X = np.expand_dims(test_X,axis=-1)\n\nassert X[0].shape == test_X[0].shape\nprint(f'The new shape of training and test image is {X[0].shape}')","55bd6a37":"##This class store all the hyperparameters\nclass HyperParameters:\n    VALID_SIZE = 0.2\n    BATCH_SIZE = 32 \n    LEARNING_RATE = 1e-3\n    EPOCHS = 20","cb0e9849":"## This function creates tf dataset for training,validation, and test data\ndef create_tf_dataset(X,y=None,val=False,test=False):\n    if test:\n        return (tf.data.Dataset\n                .from_tensor_slices(X)\n                .batch(HyperParameters.BATCH_SIZE)) \n\n        \n    train_X,val_X,train_y,val_y = train_test_split(X,\n                                                   y,\n                                                   test_size=HyperParameters.VALID_SIZE,\n                                                   shuffle=True,\n                                                   random_state=98)\n    \n    if not val:\n        tf_dataset = (tf.data.Dataset\n                      .from_tensor_slices((train_X,train_y))\n                      .shuffle(len(train_X))\n                      .batch(HyperParameters.BATCH_SIZE))\n    \n    elif val:\n        tf_dataset = (tf.data.Dataset\n                      .from_tensor_slices((val_X,val_y))\n                      .shuffle(len(val_X))\n                      .batch(HyperParameters.BATCH_SIZE))\n       \n    \n    return tf_dataset","9ceeed59":"#Build the Model\nclass ModelBuilder(tf.keras.Model):\n    ##Initialize all needed layers\n    def __init__(self):\n        super(ModelBuilder,self).__init__()\n        self._conv1 = tf.keras.layers.Conv2D(32,3,activation=tf.nn.relu)\n        #self.relu = tf.keras.layers.ReLU()\n        self._pooling1 = tf.keras.layers.MaxPool2D((2,2))\n        self._flatten = tf.keras.layers.Flatten()\n        self._dropout = tf.keras.layers.Dropout(0.2)\n        self._dense1 = tf.keras.layers.Dense(256,activation=tf.nn.relu)\n        self._dense2 = tf.keras.layers.Dense(10)\n        #self.softmax = tf.keras.layers.Softmax()\n    \n    ##Define the model structure\n    def __call__(self,inputs,training=False):\n        x = self._conv1(inputs)\n        #x = self.relu(x)\n        x = self._pooling1(x)\n        x = self._flatten(x)\n        ## During training, the flag should be set True.\n        ## During inference, the flag should be set False (Default)\n        if training:\n            x = self._dropout(x,training=training)\n        x = self._dense1(x)\n        x = self._dense2(x)\n        #x = self.softmax(x)\n        return x \n    \n    ##Model summary\n    def summary(self):\n        x = tf.keras.Input(shape=(28, 28, 1))\n        model = tf.keras.Model(inputs=x, outputs=self.__call__(x))\n        return model.summary()        ","2e3ab8ee":"## Compile and Fit the model\nclass ModelCompiler:\n    ## Initialize all needed components of the compiler \n    def __init__(self, model):\n        self.model = model\n        ## If we set from_logits=True, we do not need softmax activation function\n        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=HyperParameters.LEARNING_RATE)\n        self.accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n        self.loss_tracker = tf.keras.metrics.Mean(name='mean_loss')\n    \n    ## Training Step\n    @tf.function  ## Make it fast.\n    def train_step(self, x, y):\n        with tf.GradientTape() as tape: ##Record the operations for automatic differentiation\n            predictions = self.model(x,training = True)\n            loss = self.loss_fn(y, predictions)\n        gradients = tape.gradient(loss, self.model.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n        \n        ##Update the training loss and accuracy after each step\n        self.loss_tracker.update_state(loss)\n        self.accuracy.update_state(y,predictions)\n        \n    ## Validation Step\n    @tf.function  \n    def val_step(self, x, y):\n        predictions = self.model(x, training = False)\n        loss = self.loss_fn(y, predictions)\n        \n        ##Update the validation loss and accuracy after each step\n        self.loss_tracker.update_state(loss)\n        self.accuracy.update_state(y,predictions)\n    \n    ## Prediction Step\n    @tf.function  \n    def test_step(self, x):\n        predictions = self.model(x,training = False)\n        return predictions","fe2b1bb6":"tf_train_dataset = create_tf_dataset(X, y)\ntf_val_dataset = create_tf_dataset(X, y, val=True)\ntf_test_dataset = create_tf_dataset(test_X, test=True)","a9741f2b":"tf.keras.backend.clear_session()","a7a45b85":"MINST_Classifier = ModelBuilder()\nMINST_Classifier.summary()\ntraining_process = ModelCompiler(MINST_Classifier)\nhistory = {'training_loss':[],\n           'training_accuracy':[],\n           'validation_loss':[],\n           'validation_accuracy':[]}\n\n## Customize your training process in this loop\nfor epoch in range(HyperParameters.EPOCHS):\n    print(f'\\nEpoch {epoch + 1}\\n')\n    ## Reset the metrics at the start of the next epoch\n    training_process.loss_tracker.reset_states()\n    training_process.accuracy.reset_states()\n    \n    print('Training....')\n    for images, labels in tf_train_dataset:\n        training_process.train_step(images, labels)\n    \n    print(\n        f'Loss: {training_process.loss_tracker.result()}, '\n        f'Accuracy: {training_process.accuracy.result() * 100}%, '\n    )\n    history['training_loss'].append(training_process.loss_tracker.result())\n    history['training_accuracy'].append(training_process.accuracy.result())\n        \n    ## Reset the metrics\n    training_process.loss_tracker.reset_states()\n    training_process.accuracy.reset_states()\n    \n    print('Validating....')\n    for val_images, val_labels in tf_val_dataset:\n        training_process.val_step(val_images, val_labels)\n        \n    print(\n        f'Validation Loss: {training_process.loss_tracker.result()}, '\n        f'Validation Accuracy: {training_process.accuracy.result() * 100}%\\n'\n    )\n    \n    history['validation_loss'].append(training_process.loss_tracker.result())\n    history['validation_accuracy'].append(training_process.accuracy.result())","29525ae9":"## Visualize the model training history\nfig = plt.figure(figsize=(16,5))\nax = fig.add_subplot(1,2,1)\nax.plot(history['training_loss'],color='blue',label='Training')\nax.plot(history['validation_loss'],color='red',label='Validation')\nax.set_xticks(range(HyperParameters.EPOCHS))\nax.set_title('Loss: Training vs Validation')\nax.legend()\n\nax = fig.add_subplot(1,2,2)\nax.plot(history['training_accuracy'],color='blue',label='Training')\nax.plot(history['validation_accuracy'],color='red',label='Validation')\nax.set_xticks(range(HyperParameters.EPOCHS))\nax.set_title('Accuracy: Training vs Validation')\nax.legend()","75dbf4dc":"## Submission\ntest_labels = np.array([])\nfor test_image in tf_test_dataset:\n    test_pred = training_process.test_step(test_image).numpy()\n    test_label = np.argmax(test_pred,axis=1)\n    test_labels = np.concatenate((test_labels,test_label),axis=None).astype('int64')\n    \ndf_submit = pd.DataFrame({'ImageId':range(1,len(test_X)+1),'Label':test_labels})\ndf_submit.to_csv('submission.csv',index=False)\ndf_submit.head(10)","f725ab1a":"# Preprocessing","51a9b899":"As a high-level API for tensorflow, Keras makes it simple to build, compile and fit a NN with several lines of code. For a long time, I have been curious about what is really inside the 'model.compile()' and 'model.fit()' methods. As a result, I begin writing this notebook and intend to customize the compiling and training process of the NN model. I believe it may be helpful in the future when we want to monitor the details (e.g. magnitude of gradients,change of loss\/accuracy)  during the training process or to add some customized training strategies. Sometimes using the Keras API will sacrifice the flexibility of training the NN model.","5472d480":"I use blogs\/tutorials from Keras and Tensorflow communities\/documents as reference.\n\n1.https:\/\/keras.io\/getting_started\/intro_to_keras_for_researchers\/#the-functional-api-for-modelbuilding\n\n2.https:\/\/keras.io\/guides\/customizing_what_happens_in_fit\/\n\n3.https:\/\/www.tensorflow.org\/tutorials\/quickstart\/advanced","d57b529c":"**If you think my notebook is useful, please consider upvoting it :)**","38c1ec74":"![](https:\/\/miro.medium.com\/max\/4928\/1*-QTg-_71YF0SVshMEaKZ_g.png)","0c7d27b4":"# Wrap up the end-to-end model ","8b9aa496":"# Build the Pipeline"}}