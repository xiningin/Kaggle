{"cell_type":{"ef757560":"code","9502ccf1":"code","c3d64f73":"code","628e1933":"code","1b4df972":"code","cd20a8dd":"code","e27cdf43":"code","3eb721b1":"code","7f45e06b":"code","9a620fd3":"code","c9855509":"code","1fe6c7ac":"code","4fe81d3b":"code","0b025757":"code","f1323614":"code","98a66bc3":"markdown","c43eed64":"markdown","0b9196cc":"markdown","cdc06e72":"markdown","83ee54a5":"markdown","99dce498":"markdown","b38a0acd":"markdown"},"source":{"ef757560":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9502ccf1":"brain_df=pd.read_csv('..\/input\/emotions.csv')\nbrain_df.head()","c3d64f73":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,5))\nsns.countplot(x=brain_df.label,color='mediumseagreen')\nplt.title('Emotional sentiment class,fontsize=20')\nplt.ylabel('class counts',fontsize=18)\nplt.xlabel('class counts',fontsize=18)\nplt.xticks(rotation='vertical')","628e1933":"brain_df.count","1b4df972":"label_df=brain_df['label']\nbrain_df.drop('label',axis=1,inplace=True)\nbrain_df.head()","cd20a8dd":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score,train_test_split\npl_random_forest=Pipeline(steps=[('random_forest',RandomForestClassifier())])\nscores=cross_val_score(pl_random_forest,brain_df,label_df,cv=10,scoring='accuracy')\nprint('Accuracy for RandomForest:',scores.mean())","e27cdf43":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score,train_test_split\npl_log_reg=Pipeline(steps=[('scaler',StandardScaler()),('log_reg',LogisticRegression(multi_class='multinomial',solver='saga',max_iter=200))])\nscores=cross_val_score(pl_log_reg,brain_df,label_df,cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression:',scores.mean())","3eb721b1":"%%time\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscale=scaler.fit_transform(brain_df)\npca=PCA(n_components=20)\npca_vectors=pca.fit_transform(scale)\nfor index,var in enumerate(pca.explained_variance_ratio_):\n    print('variances by PCA',(index+1),\":\",var)\n","7f45e06b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(25,8))\nsns.scatterplot(x=pca_vectors[:,0],y=pca_vectors[:,1],hue=label_df)\nplt.title('PCA distribution ,fontsize=20')\nplt.ylabel('PCA 1',fontsize=18)\nplt.xlabel('PCA 2',fontsize=18)\nplt.xticks(rotation='vertical')","9a620fd3":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score,train_test_split\npl_log_reg_pca=Pipeline(steps=[('scaler',StandardScaler()),('pca',PCA(n_components=2)),('log_reg',LogisticRegression(multi_class='multinomial',solver='saga',max_iter=200))])\nscores=cross_val_score(pl_log_reg_pca,brain_df,label_df,cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression 2 PCA component:',scores.mean())","c9855509":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score,train_test_split\npl_log_reg_pca=Pipeline(steps=[('scaler',StandardScaler()),('pca',PCA(n_components=10)),('log_reg',LogisticRegression(multi_class='multinomial',solver='saga',max_iter=200))])\nscores=cross_val_score(pl_log_reg_pca,brain_df,label_df,cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression 2 PCA component:',scores.mean())","1fe6c7ac":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score,train_test_split\npl_log_reg_pca=Pipeline(steps=[('scaler',StandardScaler()),('pca',PCA(n_components=20)),('log_reg',LogisticRegression(multi_class='multinomial',solver='saga',max_iter=200))])\nscores=cross_val_score(pl_log_reg_pca,brain_df,label_df,cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression 2 PCA component:',scores.mean())","4fe81d3b":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\npl_mlp=Pipeline(steps=[('scaler',StandardScaler()),('mil_ann',MLPClassifier(hidden_layer_sizes=(1275,637)))])\nscores=cross_val_score(pl_mlp,brain_df,label_df,cv=10,scoring='accuracy')\nprint('ANN:',scores.mean())","0b025757":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\npl_svm=Pipeline(steps=[('scaler',StandardScaler()),('svm',LinearSVC())])\nscores=cross_val_score(pl_svm,brain_df,label_df,cv=10,scoring='accuracy')\nprint('SVM:',scores.mean())","f1323614":"%%time\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\npl_xgb=Pipeline(steps=[('svm',xgb.XGBClassifier(objective='multi:softmax'))])\nscores=cross_val_score(pl_xgb,brain_df,label_df,cv=10,scoring='accuracy')\nprint('XGBoost:',scores.mean())","98a66bc3":"Try to increase the pca components","c43eed64":"We will try with xgboost algorithm for the better performance","0b9196cc":"Firstly need to understand the distribution of the data","cdc06e72":"curse of dimentionality affect the accuracy and time ,why dont we try out pca to reduce the dimentionality of the data","83ee54a5":"here am going to use MLClassifier like ANN","99dce498":"Now we will try Logistic Regression ","b38a0acd":"We are going to use random forest approach"}}