{"cell_type":{"0b10e236":"code","c2866d6b":"code","8492a176":"code","72032603":"code","5be6bec7":"code","bef3d1dd":"code","ce39f8af":"code","6b4182bf":"code","e04564fe":"code","4d80af45":"code","587af52f":"code","59670a74":"code","1d606c87":"code","345b9ef7":"code","fc41f385":"code","1eab175a":"code","d8801486":"code","0a1c2331":"code","80079c37":"code","facc31dd":"code","3055b958":"code","c2cbf2e7":"code","7a0d2f4f":"code","28c69471":"code","5de99cb0":"code","93aa40eb":"code","05c8dc69":"code","0982c072":"code","a46e3c38":"code","70b5ca5e":"code","00579af8":"code","acf0a6a8":"code","eec609b5":"markdown","1cc9d68c":"markdown","53ccf52f":"markdown","9fe93a76":"markdown","2d1463d2":"markdown","5888446e":"markdown","8b6a1dd9":"markdown","0e000ade":"markdown","671af08e":"markdown","78767a70":"markdown","1e2e8e1a":"markdown","5cde8c4d":"markdown","6a08fea5":"markdown","dc2863ce":"markdown","38a541c7":"markdown","b5783a87":"markdown","8e86e9f2":"markdown","ad611a8d":"markdown","d8b3ca42":"markdown"},"source":{"0b10e236":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport seaborn as sns\nfrom math import ceil\nimport datetime\n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c2866d6b":"import os\nprint(os.listdir('..\/input\/competitive-data-science-predict-future-sales\/'))","8492a176":"# Import all of them \nsales=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nitem_cat=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitem=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","72032603":"sales.head(2)","5be6bec7":"item.head(2)","bef3d1dd":"item_cat.head(2)","ce39f8af":"shops.head(2)","6b4182bf":"train=sales.copy()\ntest_shops = test.shop_id.unique()\ntrain = train[train.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain = train[train.item_id.isin(test_items)]\n\ngrouped = pd.DataFrame(train.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].sum().reset_index())\nfig, axes = plt.subplots(nrows=5, ncols=2, sharex=True, sharey=True, figsize=(16,20))\nnum_graph = 10\nid_per_graph = ceil(grouped.shop_id.max() \/ num_graph)\ncount = 0\nfor i in range(5):\n    for j in range(2):\n        sns.pointplot(x='date_block_num', y='item_cnt_day', hue='shop_id', data=grouped[np.logical_and(count*id_per_graph <= grouped['shop_id'], grouped['shop_id'] < (count+1)*id_per_graph)], ax=axes[i][j])\n        count += 1","e04564fe":"#formatting the date column correctly\nsales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n# check\nprint(sales.info())","4d80af45":"# Aggregate to monthly level the required metrics\nmonthly_sales=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})","587af52f":"monthly_sales.head()","59670a74":"# number of items per cat \nx=item.groupby(['item_category_id']).count()\nx=x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\nx\n# #plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('# of items', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","1d606c87":"ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(10,5))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","345b9ef7":"plt.figure(figsize=(10,5))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","fc41f385":"import statsmodels.api as sm\n# multiplicative\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"multiplicative\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","1eab175a":"# Additive model\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"additive\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","d8801486":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(ts)","0a1c2331":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","80079c37":"ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(8,8))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","facc31dd":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts)","3055b958":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return ","c2cbf2e7":"# Simulate an AR(1) process with alpha = 0.6\nnp.random.seed(1)\nn_samples = int(1000)\na = 0.6\nx = w = np.random.normal(size=n_samples)\n\nfor t in range(n_samples):\n    x[t] = a*x[t-1] + w[t]\nlimit=12    \n_ = tsplot(x, lags=limit,title=\"AR(1)process\")","7a0d2f4f":"# Simulate an AR(2) process\n\nn = int(1000)\nalphas = np.array([.444, .333])\nbetas = np.array([0.])\n\n# Python requires us to specify the zero-lag value which is 1\n# Also note that the alphas for the AR model must be negated\n# We also set the betas for the MA equal to 0 for an AR(p) model\n# For more information see the examples at statsmodels.org\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nar2 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n_ = tsplot(ar2, lags=12,title=\"AR(2) process\")\n","28c69471":"# Simulate an MA(1) process\nn = int(1000)\n# set the AR(p) alphas equal to 0\nalphas = np.array([0.])\nbetas = np.array([0.8])\n# add zero-lag and negate alphas\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\nma1 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \nlimit=12\n_ = tsplot(ma1, lags=limit,title=\"MA(1) process\")","5de99cb0":"# Simulate MA(2) process with betas 0.6, 0.4\nn = int(1000)\nalphas = np.array([0.])\nbetas = np.array([0.6, 0.4])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nma3 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n)\n_ = tsplot(ma3, lags=12,title=\"MA(2) process\")","93aa40eb":"# Simulate an ARMA(2, 2) model with alphas=[0.5,-0.25] and betas=[0.5,-0.3]\nmax_lag = 12\n\nn = int(5000) # lots of samples to help estimates\nburn = int(n\/10) # number of samples to discard before fit\n\nalphas = np.array([0.8, -0.65])\nbetas = np.array([0.5, -0.7])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\narma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")\n","05c8dc69":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","0982c072":"#Lets use it for the sales time-series.\n#\n# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","a46e3c38":"# Simply use best_mdl.predict() to predict the next values\n# adding the dates to the Time-series as index\nts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nts=ts.reset_index()\nts.head()","70b5ca5e":"total_sales=sales.groupby(['date_block_num'])[\"item_cnt_day\"].sum()\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n\ntotal_sales.index=dates\ntotal_sales.head()","00579af8":"# get the unique combinations of item-store from the sales data at monthly level\nmonthly_sales=sales.groupby([\"shop_id\",\"item_id\",\"date_block_num\"])[\"item_cnt_day\"].sum()\n# arrange it conviniently to perform the hts \nmonthly_sales=monthly_sales.unstack(level=-1).fillna(0)\nmonthly_sales=monthly_sales.T\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nmonthly_sales.index=dates\nmonthly_sales=monthly_sales.reset_index()\nmonthly_sales.head()","acf0a6a8":"#Middle out:\u00b6\n#Let's predict for the store level\n\nmonthly_shop_sales=sales.groupby([\"date_block_num\",\"shop_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nmonthly_shop_sales=monthly_shop_sales.unstack(level=1)\nmonthly_shop_sales=monthly_shop_sales.fillna(0)\nmonthly_shop_sales.index=dates\nmonthly_shop_sales=monthly_shop_sales.reset_index()\nmonthly_shop_sales.head()","eec609b5":"### AR(1) process -- has ACF tailing out and PACF cutting off at lag=1","1cc9d68c":"**2.1: Data Exploration**","53ccf52f":"# Section 2: Exploration","9fe93a76":"- Quick observations: There is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".","2d1463d2":"### MA(1) process -- has ACF cut off at lag=1","5888446e":"### AR(2) process -- has ACF tailing out and PACF cutting off at lag=2\n","8b6a1dd9":"# Section 1:","0e000ade":"- Now after the transformations, our p-value for the DF test is well within 5 %. Hence we can assume Stationarity of the series\n- We can easily get back the original series using the inverse transform function that we have defined above.\n\n- Now let's dive into making the forecasts!","671af08e":"#### 4.2: Number of Item Per Category","78767a70":"### MA(2) process -- has ACF cut off at lag=2","1e2e8e1a":"### AR, MA and ARMA models:\u00b6\nTL: DR version of the models:\n\nMA - Next value in the series is a function of the average of the previous n number of values AR - The errors(difference in mean) of the next value is a function of the errors in the previous n number of values ARMA - a mixture of both.\n\nNow, How do we find out, if our time-series in AR process or MA process?\n\nLet's find out!","5cde8c4d":"# Section 4: Feature Engineering","6a08fea5":"#### 4.4: Stationarity Test","dc2863ce":"**Pedict Future Sale**\n\n**Problem:-**\n\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.","38a541c7":"**1.1: Import Necessary Libraries**","b5783a87":"#### 4.5: Remove Trends","8e86e9f2":"**4.1: Feature Engineering**","ad611a8d":"#### 4.3: Single Series\n- Sales over time of each store-item is a time-series in itself. Before we dive into all the combinations, first let's understand how to forecast for a single series.\n- I've chosen to predict for the total sales per month for the entire company.\n\n- First let's compute the total sales per month and plot that data.","d8b3ca42":"### We've correctly identified the order of the simulated process as ARMA(2,2)"}}