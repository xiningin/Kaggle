{"cell_type":{"3406ccd6":"code","d751ed39":"code","889dbb1c":"code","69c963ed":"code","aa173d64":"code","da373258":"code","a3985e9d":"code","50ddec45":"code","5be38c5d":"code","4ddcf06b":"code","358e0ffe":"code","32d2054e":"code","86478c35":"code","ec7642de":"code","0d18f3ee":"code","d2f2a68e":"code","dec40d7b":"code","087b3277":"code","ff5c1c69":"code","644c16be":"code","a61d75b1":"code","10e6e1d6":"code","30af4ac1":"code","7214e813":"code","7abdc6f8":"code","b6394ee9":"code","4c0a28d6":"code","78b5b822":"code","72be9fb8":"code","5fb1d0f5":"code","52dbecd8":"code","fabaaefb":"code","6b3ab762":"code","4fff0fd7":"code","c26f55af":"markdown","a35761ed":"markdown","dd34b6b6":"markdown","d9a5921b":"markdown","55b36d83":"markdown","e0a46084":"markdown","af6f14c4":"markdown","a3263fca":"markdown","a547598c":"markdown","b78fa5b3":"markdown","e7ec890d":"markdown","6c13cb66":"markdown","ed9bd4c6":"markdown","17fe2f16":"markdown","fb851a26":"markdown","ea78e2a3":"markdown","1db93b24":"markdown","51c6c4d0":"markdown","59a0b3d0":"markdown","acf4ecc4":"markdown","68639167":"markdown","10961add":"markdown","b6ac414b":"markdown","055d88e5":"markdown","cc445727":"markdown"},"source":{"3406ccd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport xlrd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport seaborn as sns\nimport geoplot.crs as gcrs\nimport geoplot as gplt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d751ed39":"train_df = pd.read_csv('\/kaggle\/input\/cee-498-project-4-no2-prediction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/cee-498-project-4-no2-prediction\/test.csv')\nsample_df = pd.read_csv('\/kaggle\/input\/cee-498-project-4-no2-prediction\/sample.csv')","889dbb1c":"print(train_df.shape)\nprint(test_df.shape)\nprint(train_df.info())\nprint(test_df.info())","69c963ed":"train_df.describe()\n","aa173d64":"test_df","da373258":"# First lets rearrange the data using the melt function\ndef features_engineering(df):\n    df_new1 = pd.melt(df, id_vars = ['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km'], value_vars = df.loc[0:254, 'Impervious_100':'Impervious_10000'] , var_name = 'radius_m', value_name = 'impervious percentage')\n    df_new2 = pd.melt(df, id_vars = ['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km'], value_vars = df.loc[0:254, 'Population_100':'Population_10000'], var_name = 'population', value_name = 'pop_number')\n    df_new3 = pd.melt(df, id_vars = ['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km'], value_vars = df.loc[0:254, 'Major_100':'Major_10000'], var_name = 'major roads', value_name = 'maj_road_km')\n    df_new4 = pd.melt(df, id_vars = ['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km'], value_vars = df.loc[0:254, 'Resident_100':'Resident_14000'], var_name = 'resident roads', value_name = 'res_road_km')\n    df_new5 = pd.melt(df, id_vars = ['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km'], value_vars = df.loc[0:254, 'total_100':'total_14000'], var_name = 'total road', value_name = 'tot_road_km')\n\n    df_new1 = df_new1.set_index(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km', df_new1.groupby(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km']).cumcount()])\n    df_new2 = df_new2.set_index(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km', df_new2.groupby(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km']).cumcount()])\n    df_new3 = df_new3.set_index(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km', df_new3.groupby(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km']).cumcount()])\n    df_new4 = df_new4.set_index(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km', df_new4.groupby(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km']).cumcount()])\n    df_new5 = df_new5.set_index(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km', df_new5.groupby(['Monitor_ID','State','Latitude', 'Longitude', 'Observed_NO2_ppb', 'WRF+DOMINO', 'Distance_to_coast_km','Elevation_truncated_km']).cumcount()])\n\n    df3 = pd.concat([df_new1, df_new2, df_new3, df_new4, df_new5],axis=1)\n    df3 = df3.dropna()\n\n    df3['radius_m'] = df3['radius_m'].str.extract('(\\d+)').astype(int)\n    df3['population'] = df3['population'].str.extract('(\\d+)').astype(int)\n    df3['resident roads'] = df3['resident roads'].str.extract('(\\d+)').astype(int)\n    df3['total road'] = df3['total road'].str.extract('(\\d+)').astype(int)\n    df3['major roads'] = df3['major roads'].str.extract('(\\d+)').astype(int)\n    df3\n    df3 = df3.drop(['population','resident roads','total road','major roads'], axis=1)\n    df3 = df3.reset_index()\n    df3 = df3.dropna(subset=['Latitude', 'Longitude'])\n    points = df3.apply(lambda row: Point(row.Longitude, row.Latitude), axis =1)\n    points\n    df3_new = gpd.GeoDataFrame(df3, geometry = points)\n    df3_new.set_crs(epsg=5070, inplace=True)\n    df3_new = df3_new.drop(['Latitude', 'Longitude', 'level_8'], axis=1)\n    df3_new2 = df3_new.drop(['Monitor_ID', 'State', 'radius_m'], axis=1)\n    newdata_2= df3_new[(df3_new != 0).all(1)]\n    df3_new3 = newdata_2.drop(['Monitor_ID', 'State','geometry','Distance_to_coast_km','Elevation_truncated_km'], axis=1)\n    df3_new3 = newdata_2.drop(['Monitor_ID', 'State','geometry','Distance_to_coast_km','Elevation_truncated_km'], axis=1)\n    df3_new2 = df3_new3.loc[df3_new3['radius_m'] == 800]\n    df3_new_3 = df3_new3.loc[df3_new3['radius_m'] == 500]\n    df3_new4 = df3_new3.loc[df3_new3['radius_m'] == 300]\n    df3_new5 = df3_new3.loc[df3_new3['radius_m'] == 1000]\n    df3_new6 = df3_new3.loc[df3_new3['radius_m'] == 1500]\n    df3_new7 = df3_new3.loc[df3_new3['radius_m'] == 2000]\n    df3_new8 = df3_new3.loc[df3_new3['radius_m'] == 3000]\n    df3_new9 = df3_new3.loc[df3_new3['radius_m'] == 4000]\n    df3_new10 = df3_new3.loc[df3_new3['radius_m'] == 5000]\n    df3_new11 = df3_new3.loc[df3_new3['radius_m'] == 6000]\n    df3_new12 = df3_new3.loc[df3_new3['radius_m'] == 7000]\n    df3_new13 = df3_new3.loc[df3_new3['radius_m'] == 8000]\n    df3_new14 = df3_new3.loc[df3_new3['radius_m'] == 9000]\n    df3_new15 = df3_new3.loc[df3_new3['radius_m'] == 10000]\n    #final_df_2 = pd.concat([ df3_new15])\n    final_df_2 = pd.concat([df3_new15])\n    final_df_2 = final_df_2.drop(['radius_m'], axis=1)\n    \n    return final_df_2\n    \ndef features_engineering2(df):\n    df = df[['WRF+DOMINO','Population_10000','Impervious_10000','Resident_10000','Major_10000','total_10000']]\n    \n    return df\n","a3985e9d":"train_df = features_engineering(train_df)\n\ntest_df = features_engineering2(test_df)","50ddec45":"test_df","5be38c5d":"train_df","4ddcf06b":"array = train_df.values\nX_train = array[:,2:7]\nY_train = array[:,1]","358e0ffe":"#from sklearn.model_selection import train_test_split\n#X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.40, random_state=1)\n","32d2054e":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(batch_size=10)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel.add(tf.keras.layers.Dense(80, activation='relu'))\n\nmodel.add(tf.keras.layers.Dense(10, activation='linear'))\nmodel.add(tf.keras.layers.Dense(1, activation='linear'))\n\nmodel.add(tf.keras.layers.Dense(units=1, input_shape=(6,)))\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='mean_squared_error', metrics=[tf.keras.metrics.Accuracy()]\n)\n\nmodel.fit(train_dataset.shuffle(10).batch(1), epochs=200)\n\n","86478c35":"array2 = test_df.values\ntest_df_x= array2[:,1:128]\nsample_df = pd.read_csv('\/kaggle\/input\/cee-498-project-4-no2-prediction\/sample.csv')\nprint(test_df_x.shape)\n","ec7642de":"#from sklearn.ensemble import RandomForestRegressor\n\n#regressor = RandomForestRegressor(n_estimators=40, random_state=0)\n#regressor.fit(X_train, Y_train)\n#y_pred = regressor.predict(X_validation)","0d18f3ee":"#from sklearn import metrics\n\n#print('Mean Absolute Error:', metrics.mean_absolute_error(Y_validation, y_pred))\n#print('Mean Squared Error:', metrics.mean_squared_error(Y_validation, y_pred))\n#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_validation, y_pred)))","d2f2a68e":"#from sklearn import metrics\n\n#print('Mean Absolute Error:', metrics.mean_absolute_error(Y_validation, y_pred))\n#print('Mean Squared Error:', metrics.mean_squared_error(Y_validation, y_pred))\n#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_validation, y_pred)))","dec40d7b":"array2 = test_df.values\ntest_df_x= array2[:,1:6]\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ntest_df_x = sc.fit_transform(test_df_x)\n\nsample_df = pd.read_csv('\/kaggle\/input\/cee-498-project-4-no2-prediction\/sample.csv')\nprint(test_df_x.shape)\ntest_df_y = model.predict(test_df_x)\nsample_df[\"Observed_NO2_ppb\"] = test_df_y\nsample_df.to_csv('sample_SS26.csv',index=False)","087b3277":"sample_df","ff5c1c69":"sample_df.to_csv('sample_SS.csv',index=False)","644c16be":"plt.figure(figsize = (12, 6))\nax = sns.boxplot(x='State', y='Observed_NO2_ppb', data=df3_new)\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\nplt.xticks(rotation=45)","a61d75b1":"df3_new = df3_new.drop(['Latitude', 'Longitude', 'level_8'], axis=1)","10e6e1d6":"df3_new2 = df3_new.drop(['Monitor_ID', 'State', 'radius_m'], axis=1)\nprint(df3_new2.corr()['Observed_NO2_ppb'])","30af4ac1":"newdata_2= df3_new[(df3_new != 0).all(1)]\ndf3_new3 = newdata_2.drop(['Monitor_ID', 'State','geometry'], axis=1)\n\nfor i in range(0, len(df3_new3.columns), 5):\n    sns.pairplot(data=df3_new3,\n                x_vars=df3_new3.columns[i:i+5],\n                y_vars=['Observed_NO2_ppb'])","7214e813":"sns.set_style('ticks')\nfig, ax = plt.subplots()\n\nfig.set_size_inches(11.7, 8.27)\nsns.boxplot(data=df3_new, x=\"radius_m\", y=\"Observed_NO2_ppb\", ax=ax)    \nsns.despine()\n\n","7abdc6f8":"df3_new2 = df3_new3.loc[df3_new3['radius_m'] == 800]\ndf3_new_3 = df3_new3.loc[df3_new3['radius_m'] == 500]\ndf3_new4 = df3_new3.loc[df3_new3['radius_m'] == 300]\ndf3_new5 = df3_new3.loc[df3_new3['radius_m'] == 1000]\ndf3_new6 = df3_new3.loc[df3_new3['radius_m'] == 1500]\ndf3_new7 = df3_new3.loc[df3_new3['radius_m'] == 2000]\ndf3_new8 = df3_new3.loc[df3_new3['radius_m'] == 3000]\ndf3_new9 = df3_new3.loc[df3_new3['radius_m'] == 4000]\ndf3_new10 = df3_new3.loc[df3_new3['radius_m'] == 5000]\ndf3_new11 = df3_new3.loc[df3_new3['radius_m'] == 6000]\ndf3_new12 = df3_new3.loc[df3_new3['radius_m'] == 7000]\ndf3_new13 = df3_new3.loc[df3_new3['radius_m'] == 8000]\ndf3_new14 = df3_new3.loc[df3_new3['radius_m'] == 9000]\ndf3_new15 = df3_new3.loc[df3_new3['radius_m'] == 10000]","b6394ee9":"print(df3_new2.corr()['Observed_NO2_ppb'])\nprint(df3_new_3.corr()['Observed_NO2_ppb'])\nprint(df3_new4.corr()['Observed_NO2_ppb'])\nprint(df3_new5.corr()['Observed_NO2_ppb'])\nprint(df3_new6.corr()['Observed_NO2_ppb'])\nprint(df3_new7.corr()['Observed_NO2_ppb'])\nprint(df3_new8.corr()['Observed_NO2_ppb'])\nprint(df3_new9.corr()['Observed_NO2_ppb'])\nprint(df3_new10.corr()['Observed_NO2_ppb'])\nprint(df3_new11.corr()['Observed_NO2_ppb'])\nprint(df3_new12.corr()['Observed_NO2_ppb'])\nprint(df3_new13.corr()['Observed_NO2_ppb'])\nprint(df3_new14.corr()['Observed_NO2_ppb'])\nprint(df3_new15.corr()['Observed_NO2_ppb'])","4c0a28d6":"final_df_2 = pd.concat([df3_new11, df3_new12, df3_new13, df3_new14, df3_new15])\nfinal_df_2 = final_df_2.drop(['radius_m'], axis=1)\nprint(final_df_2.shape)\nfor i in range(0, len(final_df_2.columns), 5):\n    sns.pairplot(data=final_df_2,\n                x_vars=final_df_2.columns[i:i+5],\n                y_vars=['Observed_NO2_ppb'])\n\n","78b5b822":"final_df_2.isnull().sum()","72be9fb8":"final_df_2.corr()['Observed_NO2_ppb']\nfig = plt.figure(figsize =(15, 7)) \nboxplot = final_df_2.boxplot(column=[ 'Elevation_truncated_km', 'impervious percentage','maj_road_km','res_road_km','tot_road_km'])","5fb1d0f5":"corr = final_df_2.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\n","52dbecd8":"plt.figure(figsize=(9, 8))\nfinal_df_2.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","fabaaefb":"final_df_2[['Distance_to_coast_km', 'Elevation_truncated_km']].corr()  ","6b3ab762":"\nfig = plt.figure(figsize =(15, 7)) \nboxplot = final_df_2.boxplot(column=[ 'pop_number'])\n\n\n","4fff0fd7":"print(final_df_2.corr()['Observed_NO2_ppb'])\nprint(df3_new15.corr()['Observed_NO2_ppb'])","c26f55af":"We see from above, as we move to greater radii, the correlation with all the variables actually gets better\n SO, we will take the top 5 best radii and concat them to form a new dataframe and then check the new correlation with respect to Observed NO2","a35761ed":"There seem to be none. Let's plot a boxplot with some variables to check the outliers","dd34b6b6":"Next, let us check state wise distribution of NO2****","d9a5921b":"There is no appreciable difference between distribution accroding to different radius. It seems there is no particular trend. For every radii the box plot is exactly the same.\nWe found that there is a lot of noise in some of the data and further refinement could help in some way. Let us first check how the observed NO2 varies with radius around the monitor. Let us see if there is a trend for different radii","55b36d83":"Step 1: Let us First Read the Train Data For this Problem","e0a46084":"As we see, except for three variables, all others have a very poor correlation. We can also show a scatter plot with respect to observed NO2 to see how well each variable correlates with Observed NO2\n But, First we will drop all zero values from the dataset\n and then remove all irrelevant columns","af6f14c4":"Although, Distance to coast and Elevation Truncated show a nearly good correlation, we can plot to see how exactly their relationship varies with distance to coast","a3263fca":"We see there are a lot of outliers in the road data.\nLet us plot a heatmap and see the correlations between each parameters","a547598c":"Let us also drop the 'State', 'Monitor ID' and 'Radius_m' columns and then find a correlation of all the columns with respect to the Observed_NO2","b78fa5b3":"Step 3: Next we will try to describe  data to get a feel of the data we have","e7ec890d":"We can see the observed NO2 has a nearly normal distribution. We can say the roads data is heavily left skewed, that is, towards lower road density. Distance_to_coast and Elevation_truncated seem to have similar distribution. From Heatmap we could see that they appeared well correlated to each other. We should further explore their relationship","6c13cb66":" So, let us now find out what happens if we take each radius individually and then find the correlation. May be the correlation would improve.\nWe will now print the correlation of all the above created dataframes with respect to the observed NO2","ed9bd4c6":"As we see they share a good correlation.\n\nWe also saw a peculiarity in population distribution. We should plot a boxplot to check the range of this variable","17fe2f16":"Step 7: The newly created GeoDataframe could then be further refined by dropping the latitude and longitude columns","fb851a26":"Step 6: Since the data has latitude and longitude, we could convert the dataframe\ninto geodataframe ","ea78e2a3":"We see there is a lot noise for each variable. We have to clean the data to reduce the noise and make the data better. But before that we will take a look at a box plot based on radii and Observed NO2 see if there is any appreciable differences ","1db93b24":"The correlation looks much better now and there seems to be some definite relationship between Observed NO2 and certain parameters. we can also check how many missing values exist if any in our new dataframe","51c6c4d0":"Step 4: It seems our data needs some rearrangement. We could use Melt Function to rearrange the data","59a0b3d0":"We could also take a look at the histogram for each variable to understand how the data is distributed","acf4ecc4":"It seems 'Distance_to_coast_km' and 'Elevation_truncated_km\" seem to be least correlated with all other parameters and even with Observed NO2","68639167":"The State wise distribution shows that the mean observed NO2 concentration of overall country is around 15 ppb. Colorado has the highest interquartile range. Some states like Maryland, Wisconsin, Washington do not seem to have adequate data","10961add":"We also check the correlation between individual variables for the two different data set we created.\nThe first- which combines data for the best five radii\nThe second- which includes only the data for radius= 10 km","b6ac414b":"Step 2 : Let us check the Shape and Info of the Data","055d88e5":"We can see their relationship could have been improved if we remove the areas that are too close to the coast. Therefore, as one moves away from the coast, the relationship between elevation gets better with distance","cc445727":"We can see the correlation significantly decreases when we combine the top five . But the datapoints are greater in number and better for our machine learning training."}}