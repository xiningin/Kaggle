{"cell_type":{"3bd25f46":"code","cee7417b":"code","8a518b6b":"code","a4614575":"code","08a70b29":"code","4fff5176":"code","a6f2d89c":"code","3d1820b7":"code","8b964227":"code","441d477d":"code","6aa833bc":"code","24c16491":"code","ce90e22c":"code","fd78d8c8":"code","03d79f92":"code","a8eedaa8":"code","5284892e":"code","e4a25760":"code","6f99f9b7":"code","243f3805":"code","902f75b4":"markdown"},"source":{"3bd25f46":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport matplotlib.pylab as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np \nimport pandas as pd\nfrom tensorflow.keras import models, layers","cee7417b":"# Dataset path\ndata_root= '..\/input\/tomatoleaf\/tomato'","8a518b6b":"# Set the output display precision\npd.set_option(\"display.precision\", 8)","a4614575":"# Select model here from model_handle_map keys\nmodel_name = \"efficientnetv2-s\" # @param ['inception_v2','efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n\nmodel_handle_map = {\n  \"efficientnetv2-s\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_s\/feature_vector\/2\",\n  \"efficientnetv2-m\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_m\/feature_vector\/2\",\n  \"efficientnetv2-l\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_l\/feature_vector\/2\",\n  \"efficientnetv2-s-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_s\/feature_vector\/2\",\n  \"efficientnetv2-m-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_m\/feature_vector\/2\",\n  \"efficientnetv2-l-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_l\/feature_vector\/2\",\n  \"efficientnetv2-xl-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_xl\/feature_vector\/2\",\n  \"efficientnetv2-b0-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_b3\/feature_vector\/2\",\n  \"efficientnetv2-s-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_s\/feature_vector\/2\",\n  \"efficientnetv2-m-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_m\/feature_vector\/2\",\n  \"efficientnetv2-l-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_l\/feature_vector\/2\",\n  \"efficientnetv2-xl-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_xl\/feature_vector\/2\",\n  \"efficientnetv2-b0-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b3\/feature_vector\/2\",\n  \"efficientnetv2-b0\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b3\/feature_vector\/2\",\n  \"efficientnet_b0\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\",\n  \"efficientnet_b1\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b1\/feature-vector\/1\",\n  \"efficientnet_b2\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b2\/feature-vector\/1\",\n  \"efficientnet_b3\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b3\/feature-vector\/1\",\n  \"efficientnet_b4\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b4\/feature-vector\/1\",\n  \"efficientnet_b5\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b5\/feature-vector\/1\",\n  \"efficientnet_b6\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b6\/feature-vector\/1\",\n  \"efficientnet_b7\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/feature-vector\/1\",\n  \"bit_s-r50x1\": \"https:\/\/tfhub.dev\/google\/bit\/s-r50x1\/1\",\n  \"inception_v3\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_v3\/classification\/5\",\n  \"inception_v2\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_v2\/classification\/5\",\n  \"inception_resnet_v2\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_resnet_v2\/feature-vector\/4\",\n  \"resnet_v1_50\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_50\/feature-vector\/4\",\n  \"resnet_v1_101\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_101\/feature-vector\/4\",\n  \"resnet_v1_152\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_152\/feature-vector\/4\",\n  \"resnet_v2_50\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_50\/feature-vector\/4\",\n  \"resnet_v2_101\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_101\/feature-vector\/4\",\n  \"resnet_v2_152\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_152\/feature-vector\/4\",\n  \"nasnet_large\": \"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_large\/feature_vector\/4\",\n  \"nasnet_mobile\": \"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_mobile\/feature_vector\/4\",\n  \"pnasnet_large\": \"https:\/\/tfhub.dev\/google\/imagenet\/pnasnet_large\/feature_vector\/4\",\n  \"mobilenet_v2_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_100_224\/feature_vector\/4\",\n  \"mobilenet_v2_130_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_130_224\/feature_vector\/4\",\n  \"mobilenet_v2_140_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_140_224\/feature_vector\/4\",\n  \"mobilenet_v3_small_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_small_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_small_075_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_small_075_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_large_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_075_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_large_075_224\/feature_vector\/5\",\n}\n\nmodel_image_size_map = {\n  \"efficientnetv2-s\": 384,\n  \"efficientnetv2-m\": 480,\n  \"efficientnetv2-l\": 480,\n  \"efficientnetv2-b0\": 224,\n  \"efficientnetv2-b1\": 240,\n  \"efficientnetv2-b2\": 260,\n  \"efficientnetv2-b3\": 300,\n  \"efficientnetv2-s-21k\": 384,\n  \"efficientnetv2-m-21k\": 480,\n  \"efficientnetv2-l-21k\": 480,\n  \"efficientnetv2-xl-21k\": 512,\n  \"efficientnetv2-b0-21k\": 224,\n  \"efficientnetv2-b1-21k\": 240,\n  \"efficientnetv2-b2-21k\": 260,\n  \"efficientnetv2-b3-21k\": 300,\n  \"efficientnetv2-s-21k-ft1k\": 384,\n  \"efficientnetv2-m-21k-ft1k\": 480,\n  \"efficientnetv2-l-21k-ft1k\": 480,\n  \"efficientnetv2-xl-21k-ft1k\": 512,\n  \"efficientnetv2-b0-21k-ft1k\": 224,\n  \"efficientnetv2-b1-21k-ft1k\": 240,\n  \"efficientnetv2-b2-21k-ft1k\": 260,\n  \"efficientnetv2-b3-21k-ft1k\": 300, \n  \"efficientnet_b0\": 224,\n  \"efficientnet_b1\": 240,\n  \"efficientnet_b2\": 260,\n  \"efficientnet_b3\": 300,\n  \"efficientnet_b4\": 380,\n  \"efficientnet_b5\": 456,\n  \"efficientnet_b6\": 528,\n  \"efficientnet_b7\": 600,\n  \"inception_v3\": 299,\n  \"inception_v2\": 299,\n  \"inception_resnet_v2\": 299,\n  \"nasnet_large\": 331,\n  \"pnasnet_large\": 331,\n}\n\nmodel_handle = model_handle_map.get(model_name)\npixels = model_image_size_map.get(model_name, 224)\n\nIMAGE_SIZE = (pixels, pixels)\n\nprint(f\"Selected model: {model_name} : {model_handle}\")","08a70b29":"# Set Image Shapes to model's preferred size\nIMAGE_SHAPE = IMAGE_SIZE\nTRAINING_DATA_DIR = str(data_root+\"\/train\")\nVALID_DATA_DIR = str(data_root+\"\/val\")\nBATCH_SIZE = 16\n\nprint(TRAINING_DATA_DIR)\n\n# Get validation images from valid directory\nvalid_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255, validation_split=0.99).flow_from_directory(VALID_DATA_DIR, \n                                                                                                            subset=\"validation\", \n                                                                                                            shuffle=True, \n                                                                                                            target_size=IMAGE_SHAPE)\n# Get train images from training directory\ntrain_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255).flow_from_directory(TRAINING_DATA_DIR,\n                                                                                                      subset=\"training\",\n                                                                                                      shuffle=True,\n                                                                                                      target_size=IMAGE_SHAPE)","4fff5176":"# Set dataset labels from directory names\ndataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\ndataset_labels = np.array([key.title() for key, value in dataset_labels])\nprint(dataset_labels)","a6f2d89c":"print(model_name)\n# Create sequential from selected model\nmodel = tf.keras.Sequential([\n    hub.KerasLayer(model_handle,trainable=False),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n])\n\nmodel.build([None, pixels, pixels, 3])\nmodel.summary()\n# Compile model \nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss='categorical_crossentropy',\n              metrics=['acc',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])","3d1820b7":"import datetime\n\nlog_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)","8b964227":"# Set steps per epoch\nsteps_per_epoch = np.ceil(train_generator.samples\/train_generator.batch_size)\n# Set validation steps per epoch\nval_steps_per_epoch = np.ceil(valid_generator.samples\/valid_generator.batch_size)\n# Train the model\nEPOCHS = 30\nhist = model.fit(train_generator,\n                 epochs=EPOCHS,\n                 verbose=1,\n                 steps_per_epoch=steps_per_epoch,\n                 validation_data=valid_generator,\n                 validation_steps=val_steps_per_epoch,\n                 callbacks=[tensorboard_callback]\n                ).history","441d477d":"# Show loss, accuracy, precision and recall values\nloss, acc, precision, recall = model.evaluate(valid_generator)\nprint(\"loss: \",loss,\"acc: \",acc,\"precision: \",precision,\"recall: \",recall)\n# F1 Score\nf1Score = 2 * (precision * recall) \/ (precision + recall)\nprint(\"F1 Score: \",f1Score)","6aa833bc":"acc = hist['acc']\nval_acc = hist['val_acc']\n\nloss = hist['loss']\nval_loss = hist['val_loss']\n\nrecall = hist['recall']\nval_recall = hist['val_recall']\n\nprecision = hist['precision']\nval_precision = hist['val_precision']\n\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 4, 1)\nplt.plot(range(EPOCHS), acc, label='Training Accuracy')\nplt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 4, 2)\nplt.plot(range(EPOCHS), loss, label='Training Loss')\nplt.plot(range(EPOCHS), val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\nplt.subplot(1, 4, 3)\nplt.plot(range(EPOCHS), precision, label='Training Precision')\nplt.plot(range(EPOCHS), val_precision, label='Validation Precision')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Precision')\n\nplt.subplot(1, 4, 4)\nplt.plot(range(EPOCHS), recall, label='Training Recall')\nplt.plot(range(EPOCHS), val_recall, label='Validation Recall')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Recall')\nplt.show()","24c16491":"%load_ext tensorboard","ce90e22c":"%tensorboard --logdir logs\/fit","fd78d8c8":"# Save Tensorflow Model\nSAVED_MODEL = \"saved_models\/\"+model_name+\"\"\nmodel.save(SAVED_MODEL)\n# Load saved Tensorflow Model\nsaved_model = tf.keras.models.load_model(SAVED_MODEL)","03d79f92":"val_image_batch, val_label_batch = next(iter(valid_generator))\ntrue_label_ids = np.argmax(val_label_batch, axis=-1)\nprint(\"Validation batch shape:\", val_image_batch.shape)","a8eedaa8":"tf_model_predictions = saved_model.predict(val_image_batch)\ntf_pred_dataframe = pd.DataFrame(tf_model_predictions)\ntf_pred_dataframe.columns = dataset_labels","5284892e":"predicted_ids = np.argmax(tf_model_predictions, axis=-1)\npredicted_labels = dataset_labels[predicted_ids]\nplt.figure(figsize=(20,20))\nplt.subplots_adjust(hspace=0.5)\nfor n in range(20):\n  plt.subplot(6,5,n+1)\n  plt.imshow(val_image_batch[n])\n  color = \"green\" if predicted_ids[n] == true_label_ids[n] else \"red\"\n  plt.title(predicted_labels[n].title(), color=color)\n  plt.axis('off')\n  _ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")","e4a25760":"!mkdir \"tflite_models\"\nTFLITE_MODEL = \"tflite_models\/\"+model_name+\".tflite\"\nTFLITE_QUANT_MODEL = \"tflite_models\/\"+model_name+\"_quant.tflite\"","6f99f9b7":"ppath = \".\/saved_models\/\" + model_name + \"\"\nprint(ppath)\nconverter = tf.lite.TFLiteConverter.from_saved_model(ppath)\ntflite_model = converter.convert()\nwith open('tflite_models\/'+model_name+'.tflite', 'wb') as f:\n  f.write(tflite_model)","243f3805":"# Zip files for download the models\n!zip -r .\/tflite_models.zip .\/tflite_models\n!zip -r .\/saved_models.zip .\/saved_models","902f75b4":"# **Create Tensorflow and Tflite Image Classification Custom Model**\n\nWith this notepad, you can convert image classification images to **Tensorflow** and **Tensorflow Lite**.\nYou should put your images to correct classes, then *upload* | *select from kaggle* dataset here."}}