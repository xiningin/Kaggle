{"cell_type":{"d524a52b":"code","1bca28a8":"code","f249f528":"code","e92b8fa7":"code","0f1d7037":"code","e37c12ae":"code","802c5dff":"code","3b11cfcf":"code","76f4d237":"code","fa25f51a":"code","ab34f20a":"code","f86bec2a":"code","a115a090":"code","d47a4235":"code","89d073f7":"code","3aa5f99a":"code","667cc2d1":"code","26be02fb":"code","60ac67ab":"code","a471a0e7":"code","db44ea33":"code","be480357":"code","24de0647":"code","e30378a1":"markdown","bffa3427":"markdown","d2b539a0":"markdown","31d354aa":"markdown","b800a1a1":"markdown","73ea49c0":"markdown","99fa77e9":"markdown","89766880":"markdown","7238ed16":"markdown","e7844097":"markdown","33c5d6dd":"markdown","28fc50b1":"markdown","b3bf999f":"markdown","eb0467f5":"markdown","8d7e3a76":"markdown","722727f4":"markdown","d17a87f1":"markdown","e3195efd":"markdown","f1d8e108":"markdown"},"source":{"d524a52b":"import pandas as pd\nimport numpy as np\n\nimport datetime\nimport pickle\nimport os\nfrom tqdm.auto import tqdm\nimport csv\n\nimport shapely\nimport fiona\nfrom shapely.geometry import shape, Point","1bca28a8":"def robust_std(x, fraction=0.9):\n    x = x[~pd.isnull(x)]\n    q = np.quantile(x**2, fraction)\n    x = x[x**2<=q]\n    if len(x) < 2:\n        return 0\n    else:\n        return np.std(x)  \n\ndef robust_mean(x, fraction=0.9):\n    x = x[~pd.isnull(x)]\n    q = np.quantile(x**2, fraction)\n    x = x[x**2<=q]\n    if len(x) < 1:\n        return np.nan\n    else:\n        return np.mean(x)  ","f249f528":"# Parameter definitions\ntrain_start = datetime.datetime(2017, 1, 1)\ntrain_end = datetime.datetime(2021, 8, 14)","e92b8fa7":"week = 191005 # 1 sample week. MTA uses the data on each Saturday, e.g 10\/05\/2019, to name their data file.\n\n# url of data source\nurl = 'http:\/\/web.mta.info\/developers\/data\/nyct\/turnstile\/turnstile_{}.txt'\n\ndf_sample = pd.read_csv(url.format(week)) \ndf_sample.head()","0f1d7037":"starting_date = datetime.datetime(2017,2,11) # 4.5 years of data\nlast_date = datetime.datetime(2021,8,14)\nurl = 'http:\/\/web.mta.info\/developers\/data\/nyct\/turnstile\/turnstile_{}.txt'\n\n# generate list of week identifiers\nweeks = []\ndate = starting_date\nwhile date <= last_date:\n    weeks.append(date.strftime(\"%y%m%d\"))\n    date = date + datetime.timedelta(weeks=1)\n\n    \ndf_initial= pd.DataFrame()\nfor week in tqdm(weeks):\n    df_temp = pd.read_csv(url.format(week))    \n    df_initial = pd.concat([df_initial, df_temp])\ndf_initial.rename(columns={'EXITS                                                               ':'EXITS'}, inplace=True)\ndf_initial= df_initial[df_initial.DESC == 'REGULAR']\ndf_initial= df_initial.drop(['DESC'], axis=1)\n\ndf_initial['datetime'] = pd.to_datetime(df_initial.DATE + ' ' + df_initial.TIME)\ndf_initial= df_initial.sort_values('datetime')\n\ndf_initial.reset_index(drop=True, inplace=True)","e37c12ae":"file_name = 'temp.csv'\n\nif os.path.isfile(file_name):\n    os.remove(file_name)\n    \nfor ca in tqdm(df_initial['C\/A'].unique()):\n    df_temp_ca = df_initial[df_initial['C\/A']==ca].copy()\n    for scp in df_temp_ca['SCP'].unique():\n        df_temp_scp = df_temp_ca[df_temp_ca['SCP']==scp]\n        df_temp_scp.set_index('datetime', inplace=True)\n        \n        # resample to whole hours: \n        try:\n            resampled = df_temp_scp[['ENTRIES', 'EXITS']].resample('H').interpolate('quadratic')\n        except:\n            resampled = df_temp_scp[['ENTRIES', 'EXITS']].resample('H').interpolate('linear')  \n\n        # keep hours 2am, 6am, 10am, 2p, 6pm, 10pm\n        resampled = np.floor(resampled[np.mod(resampled.index.hour,4)==2]) \n        absolute = resampled.diff() \n        \n        df_new = pd.DataFrame(absolute)\n        df_new['C\/A'] = df_temp_scp['C\/A'][0]\n        df_new['UNIT'] = df_temp_scp['UNIT'][0]\n        df_new['SCP'] = df_temp_scp['SCP'][0]\n        df_new['STATION'] = df_temp_scp['STATION'][0]\n        df_new['LINENAME'] = df_temp_scp['LINENAME'][0]\n        df_new['DIVISION'] = df_temp_scp['DIVISION'][0]\n        df_new.reset_index(inplace=True)\n\n        if not os.path.isfile(file_name):\n           df_new.to_csv(file_name, index=False, header=['datetime', 'ENTRIES', 'EXITS',  'C\/A', 'UNIT', 'SCP', 'STATION', 'LINENAME', 'DIVISION'])\n        else: # else it exists so append without writing the header\n           df_new.to_csv(file_name, mode='a', index=False, header=False)\n\ndf_preprocess = pd.read_csv(file_name)\ndf_preprocess.datetime = pd.to_datetime(df_preprocess.datetime)","802c5dff":"# replace negatives with nan\ndf_preprocess.loc[df_preprocess.ENTRIES < 0, 'ENTRIES'] = np.nan  \ndf_preprocess.loc[df_preprocess.EXITS < 0, 'EXITS'] = np.nan  \n# drop all nans\ndf_preprocess = df_preprocess[~pd.isnull(df_preprocess.ENTRIES)]  \ndf_preprocess = df_preprocess[~pd.isnull(df_preprocess.EXITS)]  \ndf_preprocess.reset_index(drop=True, inplace=True)","3b11cfcf":"num_stdevs = 10\ndf_std = df_preprocess.groupby(['C\/A','SCP']).agg({'ENTRIES': [robust_std], 'EXITS': [robust_std]})\ndf_mean = df_preprocess.groupby(['C\/A','SCP']).agg({'ENTRIES': [robust_mean], 'EXITS': [robust_mean]})\n\ndf_preprocess = df_preprocess.join(df_std, on=['C\/A','SCP'])\ndf_preprocess = df_preprocess.join(df_mean, on=['C\/A','SCP'])\n\ndf_preprocess['outlier'] = (df_preprocess['ENTRIES'] >= df_preprocess[('ENTRIES','robust_mean')] + df_preprocess[('ENTRIES', 'robust_std')] * num_stdevs) & (df_preprocess.ENTRIES > 1000)\ndf_preprocess = df_preprocess[~df_preprocess.outlier]\n\ndf_preprocess['outlier'] = (df_preprocess['EXITS'] >= df_preprocess[('EXITS','robust_mean')] + df_preprocess[('EXITS', 'robust_std')] * num_stdevs) & (df_preprocess.EXITS > 1000)\ndf_preprocess = df_preprocess[~df_preprocess.outlier]\n\ndf_preprocess.reset_index(drop=True, inplace=True)","76f4d237":"df_preprocess = df_preprocess.groupby(['C\/A','datetime']).agg({'ENTRIES': 'sum', 'EXITS': 'sum', 'UNIT': 'first', 'STATION': 'first', 'LINENAME': 'first', 'DIVISION': 'first'})\ndf_preprocess = pd.DataFrame(df_preprocess.to_records())\ndf_preprocess = df_preprocess.sort_values('datetime')\ndf_preprocess.reset_index(drop=True, inplace=True)","fa25f51a":"df_preprocess = df_preprocess[~((df_preprocess['C\/A']=='C015') & (df_preprocess['UNIT']=='R246'))]\ndf_preprocess = df_preprocess[~(df_preprocess['STATION']=='NASSAU ST')]\ndf_preprocess.reset_index(drop=True, inplace=True)","ab34f20a":"df_preprocess = df_preprocess[df_preprocess['C\/A']!='N507']\ndf_preprocess = df_preprocess[df_preprocess['C\/A']!='J037']","f86bec2a":"num_days = (df_preprocess.datetime.max()-df_preprocess.datetime.min()).days + 1\nmax_time_points = num_days * 6\nmin_time_points_threshold = int(max_time_points*0.8)\n\ndf_num_ts = pd.DataFrame(df_preprocess.groupby(['C\/A']).ENTRIES.count()).rename(columns={'ENTRIES':'tp count'})\ndf_preprocess = df_preprocess.join(df_num_ts, on=['C\/A'])\ndf_preprocess = df_preprocess[df_preprocess['tp count'] >= min_time_points_threshold]\ndf_preprocess.drop(['tp count'], axis=1, inplace=True)","a115a090":"df_station_data = pd.read_csv('..\/input\/nyc-subway-traffic-data-20172021\/Stations.csv')\ndf_station_lookup = pd.read_csv('..\/input\/nyc-subway-traffic-data-20172021\/station_lookup.csv')","d47a4235":"# Aggregate lines and directions for stations where there are multiple options (because we can't tell from the turnstile data which line people are actually boarding, \n# so we can deal with the multiple rows for each lookup by concatenating all the options)\n\ndef agg_lines(lines):\n    return ', '.join(list(lines.values))\ndef agg_routes(routes):\n    if routes.shape[0]>1:\n        routes = pd.Series(routes.str.cat(sep=' '))\n    return ''.join(list(np.unique(routes.apply(lambda x: str(x).replace(' ','')))))\ndef agg_directions(directions):\n    if directions.shape[0]>1:\n        directions = pd.Series(directions.str.cat(sep=' '))\n    return ', '.join(list(np.unique(directions.apply(lambda x: str(x).replace('&','').replace('  ',' ').replace('The ','').replace('nan','')))))\ndef agg_first(x):\n    return x[0]\n\ndf_station_data = df_station_data.groupby(['Complex ID', 'Division']).agg({'Line':agg_lines, \n                                                                           'Stop Name':agg_first, \n                                                                           'Borough':agg_first,\n                                                                           'Structure':agg_first,\n                                                                           'Daytime Routes': agg_routes, \n                                                                           'GTFS Latitude': 'mean',\n                                                                           'GTFS Longitude': 'mean',\n                                                                           'North Direction Label': agg_directions,\n                                                                           'South Direction Label': agg_directions}).reset_index(drop=False)","89d073f7":"# Commented out because the link is now broken, however the relevant shapefile can still be found if you want to run this yourself\n\n# Add Neighborhood name to df_stations\n\n# neighborhood_dict = {\n#     'Bay Ridge': 'Bay Ridge\/Dyker Heights',\n#     'Bellerose\/Rosedale': 'Queens Village',\n#     'Brooklyn Heights\/Fort Greene': 'Fort Greene\/Brooklyn Heights',\n#     'Brownsville\/Ocean Hill': 'Brownsville',\n#     'Chelsea\/Clinton\/Midtown': 'Clinton\/Chelsea',\n#     'East New York\/Starret City': 'East New York\/Starrett City',\n#     'Flatbush': 'Flatbush\/Midwood',\n#     'Forest Hills\/Rego Park': 'Rego Park\/Forest Hills',\n#     'Greenwich Village\/Financial District': 'Financial District',\n#     'Highbridge\/S. Concourse': 'Highbridge\/Concourse',\n#     'Howard Beach\/S. Ozone Park': 'South Ozone Park\/Howard Beach',\n#     'Jamaica': 'Jamaica\/Hollis',\n#     'Kingsbridge Heights\/Mosholu': 'Kingsbridge Heights\/Bedford',\n#     'Middle Village\/Ridgewood': 'Ridgewood\/Maspeth',\n#     'Morningside Heights\/Hamilton Heights': 'Morningside Heights\/Hamilton',\n#     'Morrisania\/East Tremont': 'Morrisania\/Crotona',\n#     'Mott Haven\/Hunts Point': 'Hunts Point\/Longwood',\n#     'North Crown Heights\/Prospect Heights': 'Crown Heights\/Prospect Heights',\n#     'Pelham Parkway': 'Morris Park\/Bronxdale',\n#     'Riverdale\/Kingsbridge': 'Riverdale\/Fieldston',\n#     'Rockaways': 'Rockaway\/Broad Channel',\n#     'Sheepshead Bay\/Gravesend': 'Sheepshead Bay',\n#     'Soundview\/Parkchester': 'Parkchester\/Soundview',\n#     'South Crown Heights': 'South Crown Heights\/Lefferts Gardens',\n#     'Sunnyside\/Woodside': 'Woodside\/Sunnyside',\n#     'University Heights\/Fordham': 'Fordham\/University Heights',\n#     'Williamsburg\/Greenpoint': 'Greenpoint\/Williamsburg'\n# }\n\n\n# for index, row in df_station_data.iterrows():\n#     latlong = [row['GTFS Longitude'], row['GTFS Latitude']]\n#     point = shapely.geometry.asPoint(latlong)\n#     with fiona.open('Data\/sde-columbia-census_2000_032807211977000-shapefile\/columbia_census_2000_032807211977000.shp') as fiona_collection:\n#         for shapefile_record in fiona_collection: \n#             shape = shapely.geometry.asShape(shapefile_record['geometry'])\n#             if shape.contains(point):\n#                 df_station_data.loc[index, 'Neighborhood'] = shapefile_record['properties']['name']\n\n# df_station_data.Neighborhood = df_station_data.Neighborhood.str.replace(' \/ ','\/')\n# df_station_data.Neighborhood = df_station_data.Neighborhood.apply(lambda name: neighborhood_dict.setdefault(name, name))","3aa5f99a":"ca_list = df_preprocess.groupby(['C\/A']).agg({'ENTRIES': 'count', 'EXITS': 'count', 'UNIT': 'first', 'STATION': 'first', 'LINENAME': 'first', 'DIVISION': 'first'})\nca_list = pd.DataFrame(ca_list.to_records()).drop(['ENTRIES', 'EXITS'], axis=1)\n\n# Join ca_list with df_station_lookup, indexing on 'C\/A' ('booth') and 'UNIT' ('remote')\ndf_stations = ca_list.join(df_station_lookup.set_index(['booth']), on=['C\/A'], how='left').drop(['division'], axis=1)\n\n# take out part that didn't match, and re-join it with best matches (according to 'UNIT' ('remote') and 'Division'\ndf_stations_nonmatch = df_stations[pd.isnull(df_stations.complex_id)][['C\/A', 'UNIT', 'STATION', 'LINENAME', 'DIVISION']]\ndf_stations_nonmatch = df_stations_nonmatch.join(df_station_lookup.groupby(['remote ','division']).first(), on=['UNIT', 'DIVISION'], how='left').drop(['booth'], axis=1)\n\n# replace re-matched rows \ndf_stations = df_stations[~pd.isnull(df_stations.complex_id)]\ndf_stations = pd.concat([df_stations, df_stations_nonmatch])\n\n# join result with df_statiob_data, indexing on 'Complex ID' and 'Division'\ndf_stations = df_stations.join(df_station_data.set_index(['Complex ID', 'Division']), on=['complex_id','DIVISION'], how='left')","667cc2d1":"df_preprocess = df_preprocess.join(df_stations.set_index(['C\/A','UNIT']), on=['C\/A','UNIT'], how='left', rsuffix='x')","26be02fb":"df_preprocess.rename(columns={'C\/A': 'Control Area', 'UNIT': 'Remote Unit', 'DIVISION': 'Division', \n                                'datetime': 'Datetime', 'complex_id': 'Complex ID', 'ENTRIES': 'Entries', 'EXITS': 'Exits', \n                                'line_name': 'Connecting Lines', 'GTFS Latitude': 'Latitude', 'GTFS Longitude': 'Longitude'}, inplace=True)\n\n# re-order columns\ndf_preprocess = df_preprocess[['Control Area', 'Remote Unit', 'Stop Name', 'Line', 'Connecting Lines', 'Daytime Routes', 'North Direction Label', \n                               'South Direction Label', 'Division', 'Structure', 'Borough', 'Neighborhood', 'Latitude', 'Longitude', 'Datetime',\n                               'Entries', 'Exits']]","60ac67ab":"df_preprocess = df_preprocess.groupby(['Datetime', \n                                       'Stop Name', \n                                       'Remote Unit', \n                                       'Line', \n                                       'Connecting Lines']).agg({'Daytime Routes': 'first', 'North Direction Label':'first', \n                                                                             'South Direction Label':'first', 'Division': 'first', 'Structure': 'first', \n                                                                             'Borough': 'first', 'Neighborhood': 'first', 'Latitude': 'first', \n                                                                             'Longitude': 'first', 'Entries': 'sum', 'Exits': 'sum'}).reset_index()\ndf_identifier = df_preprocess.groupby(['Stop Name', 'Remote Unit', 'Line', 'Connecting Lines']).agg({'Daytime Routes': 'first'}).reset_index().reset_index(drop=False)\ndf_identifier = df_preprocess.join(df_identifier.set_index(['Stop Name', 'Remote Unit', 'Line', 'Connecting Lines','Daytime Routes']), on=['Stop Name', 'Remote Unit', 'Line', 'Connecting Lines','Daytime Routes'], how='left')\ndf_preprocess.insert(0, 'Unique ID', df_identifier['index'])","a471a0e7":"df = df_preprocess[~pd.isnull(df_preprocess['Stop Name'])]","db44ea33":"df.Datetime = df.Datetime - datetime.timedelta(hours=2)","be480357":"df.head()","24de0647":"df.to_csv('NYC_subway_traffic_2017-2021.csv.gz', index=False, compression='gzip')","e30378a1":"### Remove Control Areas with bad data\nIt seems these may contain faulty turnrsiles","bffa3427":"### Load and look at single week of turnstile data","d2b539a0":"## Save results","31d354aa":"### Aggregate over SCPs\nSum the entries and exits for all turnstiles that are in the same Control Area - we don't want to distinguish between different turnstiles that are next to each other and all lead to the same place. ","b800a1a1":"### Clean negative, NaNs","73ea49c0":"### Remove C\/As with not enough data\nKeep only control areas that have data for at least 80% of the time points. ","99fa77e9":"### Join turnstile data with df_stations","89766880":"## Preprocess Data\n### Resample to fixed hours, and convert accumulated to absolute passenger counts","7238ed16":"#### The first column - Control Area - is the unique identifier for a single time series (representing passengers going through the same set of turnstiles, though they may split up later to different lines)\n#### The second column - Remote Unit - is a higher-level categorization. It may encompass different nearby stations, or something like two entrances for one station (it's possible to see how entrances increase in one when the second is closed)","e7844097":"## Generate station data","33c5d6dd":"### Remove ouliers\nOutliers are defined as samples with >10 standard deviations of the passengers for that turnstile, assuming at least 1000 passengers (with less than that the std. dev. is too noisy to judge outliers). ","28fc50b1":"### Remove data with bad station identifiers","b3bf999f":"Join **df_stations** with **df_station_data** through **df_station_lookup**","eb0467f5":"## Rename and reorder columns","8d7e3a76":"Change the hour so that it reflects the middle of the 4-hour period during which passengers were counted. ","722727f4":"Add neighborhood name for census data\n\nData from https:\/\/furmancenter.org\/neighborhoods\n\nNeighborhood shapefile from: https:\/\/geodata.lib.berkeley.edu\/catalog\/sde-columbia-census_2000_032807211977000","d17a87f1":"### Load all data (read weekly data from the MTA website and concatenate it into a single DF)","e3195efd":"### Final preprocessing steps\nRemove data from unknown stations (they come from Control Areas like TRAM1, TRAM2, PHT***, etc.)","f1d8e108":"### Aggregate again over adjacent station parts and generate a new unique unique time series key"}}