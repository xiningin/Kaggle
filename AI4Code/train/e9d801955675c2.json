{"cell_type":{"8e298441":"code","fc5da43a":"code","4656e4ef":"code","7e80c1cb":"code","a23b0cf5":"code","3174d23b":"code","e8e30ebb":"code","70ba3e41":"code","dc3626f5":"code","5ff852a9":"code","db653679":"code","960bab4c":"code","1f1e583c":"code","2fe8ddab":"code","829ca676":"code","b08a79cd":"code","9d9df6eb":"code","1a2f0f5d":"code","a8173d95":"code","4dac5efa":"code","4852b66c":"code","cf85adbb":"code","2912b86b":"code","05598738":"code","771de9b8":"code","b9bcf343":"code","5ce70d7c":"code","c260eaee":"code","266fe1e8":"code","f24158a4":"code","b0321bdb":"code","3c079a9a":"code","07732dd3":"code","b7b87389":"code","b96a2675":"code","5c9860c7":"code","c3d21d14":"code","1de31c25":"code","3816a811":"code","5ec5178b":"code","0e1b1dc2":"code","7f67fb47":"code","9c419346":"code","0b24c776":"code","0f9dd074":"code","a14274b6":"code","fa9fc75a":"code","1b45dc9d":"code","a2e3285a":"code","df80bb0d":"code","93cc4c3a":"code","93435f9f":"code","4ea87c7f":"code","3774f0d0":"code","0deb0f44":"code","2c8e2679":"code","d9bde201":"code","253d9535":"code","5d04bf31":"code","5448ca71":"code","da48a131":"code","592fe58b":"code","421ffa5e":"code","dfc95086":"code","faa42040":"code","f45997db":"code","3a1a0407":"code","05ee37dd":"code","aabccfbc":"code","e7ea836a":"code","29c5251f":"code","54e9aa46":"code","f691e516":"code","1d648dbc":"code","38beb4b2":"code","fe809b1f":"code","55d12a7b":"code","cad01c42":"code","a778701b":"code","0828527d":"code","c7f5940b":"code","62da06fa":"code","fbed1dd9":"code","270c0a41":"code","32af6903":"markdown","372680dd":"markdown","bb0a77be":"markdown","06b9f68f":"markdown","a2dc251b":"markdown","56cd5804":"markdown","b988d747":"markdown","18e1d6ac":"markdown","5ae73881":"markdown","9712d6cb":"markdown","16e36664":"markdown","bc8f405e":"markdown"},"source":{"8e298441":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import export_graphviz #plot tree\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC  \nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom numpy.random import seed\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom subprocess import call\nfrom IPython.display import Image\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc5da43a":"\ndef get_accuracy(X_train_NN,y_train_NN):\n    with torch.no_grad():\n        no_samples=0\n        no_preds=0\n        for i in range(0,list(y_train_NN.size())[0]):\n            prediction = net(X_train_NN[i])\n            predicted_class = np.argmax(prediction)\n            no_samples=no_samples+1\n            if(predicted_class==y_train_NN[i]):\n                no_preds=no_preds+1\n        return no_preds\/no_samples\n            \n","4656e4ef":"bank=pd.read_csv('\/kaggle\/input\/bank-marketing-dataset\/bank.csv')","7e80c1cb":"bank.deposit.value_counts()","a23b0cf5":"len(bank.columns)","3174d23b":"bank.describe(include=\"all\")","e8e30ebb":"plt.figure(figsize=(7,4)) \nsns.heatmap(bank.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","70ba3e41":"sns.countplot(y=\"deposit\", data=bank, palette=\"tab10\")","dc3626f5":"fig, ax = plt.subplots(2, 3)\nbank.groupby('job')['deposit'].value_counts(normalize=True).unstack('deposit').plot.bar(stacked=True, ax=ax[1,0],title=\"job\",xlabel=\"\")\nbank.groupby('default')['deposit'].value_counts(normalize=True).unstack('deposit').plot.bar(stacked=True, ax=ax[0,0],title=\"default\",xlabel=\"\")\nbank.groupby('housing')['deposit'].value_counts(normalize=True).unstack('deposit').plot.bar(stacked=True, ax=ax[0,1],title=\"housing\",xlabel=\"\")\nbank.groupby('loan')['deposit'].value_counts(normalize=True).unstack('deposit').plot.bar(stacked=True, ax=ax[0,2],title=\"loan\",xlabel=\"\")\nbank.groupby('poutcome')['deposit'].value_counts(normalize=True).unstack('deposit').plot.bar(stacked=True, ax=ax[1,1],title=\"poutcome\",xlabel=\"\")\nbank.groupby('month')['deposit'].value_counts(normalize=True).unstack('deposit').plot.bar(stacked=True, ax=ax[1,2],title=\"month\",xlabel=\"\")\nfig.set_size_inches(20,12)\nplt.show()","5ff852a9":"bank.describe(include=\"all\")","db653679":"fig, ax = plt.subplots(1, 2)\nfig = bank[bank.deposit=='yes'].plot(kind='scatter',x='age',y='balance',color='#ff8c00', label='Yes', ax=ax[0])\nbank[bank.deposit=='no'].plot(kind='scatter',x='age',y='balance',color='blue', label='No',ax=fig)\nfig.set_xlabel(\"age\")\nfig.set_ylabel(\"balance\")\nfig.set_title(\"balance VS age\")\nfig=plt.gcf()\nfig = bank[bank.deposit=='yes'].plot(kind='scatter',x='age',y='duration',color='#ff8c00', label='Yes', ax=ax[1])\nbank[bank.deposit=='no'].plot(kind='scatter',x='age',y='duration',color='blue', label='No',ax=fig)\nfig.set_xlabel(\"age\")\nfig.set_ylabel(\"duration\")\nfig.set_title(\"duration VS age\")\nfig=plt.gcf()\nfig.set_size_inches(20,10)\nplt.show()","960bab4c":"bank['deposit'][bank['deposit'] == 'yes'] =1\nbank['deposit'][bank['deposit'] == 'no'] = 0","1f1e583c":"bank['deposit']=bank['deposit'].astype(int)","2fe8ddab":"bank2 = pd.get_dummies(bank, drop_first=True)","829ca676":"bank2.columns","b08a79cd":"bank2_norm = (bank2.drop('deposit',1) - np.min(bank2.drop('deposit',1))) \/ (np.max(bank2.drop('deposit',1)) - np.min(bank2.drop('deposit',1))).values\nbank2_norm['deposit']=bank2.deposit","9d9df6eb":"train, test = train_test_split(bank2, test_size = 0.3,random_state=0)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train.shape)\nprint(test.shape)","1a2f0f5d":"train_norm, test_norm = train_test_split(bank2_norm, test_size = 0.3,random_state=0)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train_norm.shape)\nprint(test_norm.shape)","a8173d95":"train_X = train.drop('deposit',1)# taking the training data features\ntrain_y=train.deposit # output of our training data\ntest_X= test.drop('deposit',1) # taking test data features\ntest_y =test.deposit   #output value of test data\n","4dac5efa":"train_X_norm = train_norm.drop('deposit',1)# taking the training data features\ntrain_y_norm=train_norm.deposit # output of our training data\ntest_X_norm= test_norm.drop('deposit',1) # taking test data features\ntest_y_norm =test_norm.deposit   #output value of test data\n","4852b66c":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_y)","cf85adbb":"model.get_n_leaves()","2912b86b":"fig, ax = plt.subplots(2, 2)\ntrain_accuracy=[]\ntest_accuracy=[]\ni_p=[]\nfor i in range(1,30):\n    model=DecisionTreeClassifier(max_depth=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\n    i_p.append(i)\n\nax[0,0].plot(i_p,train_accuracy, label=\"Training Accuracy\", marker='o',)\nax[0,0].plot(i_p,test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nax[0,0].set_xlabel('Max depth', fontsize=18)\nax[0,0].set_ylabel('Accuracy', fontsize=18)\nax[0,0].legend()\n\ntrain_accuracy=[]\ntest_accuracy=[]\ni_p=[]\nfor i in range(1,2000,50):\n    model=DecisionTreeClassifier(min_samples_leaf=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\n    i_p.append(i)\n\nax[0,1].plot(i_p,train_accuracy, label=\"Training Accuracy\", marker='o',)\nax[0,1].plot(i_p,test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nax[0,1].set_xlabel('min_samples_leaf', fontsize=18)\nax[0,1].set_ylabel('Accuracy', fontsize=18)\nax[0,1].legend()\n\ntrain_accuracy=[]\ntest_accuracy=[]\ni_p=[]\nfor i in range(2,2000,50):\n    model=DecisionTreeClassifier(min_samples_split=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\n    i_p.append(i)\n\nax[1,0].plot(i_p,train_accuracy, label=\"Training Accuracy\", marker='o',)\nax[1,0].plot(i_p,test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nax[1,0].set_xlabel('min_samples_split', fontsize=18)\nax[1,0].set_ylabel('Accuracy', fontsize=18)\nax[1,0].legend()\n\n\ntrain_accuracy=[]\ntest_accuracy=[]\ni_p=[]\nfor i in range(2,43):\n    model=DecisionTreeClassifier(max_features=i,max_depth=5)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\n    i_p.append(i)\n\nax[1,1].plot(i_p,train_accuracy, label=\"Training Accuracy\", marker='o',)\nax[1,1].plot(i_p,test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nax[1,1].set_xlabel('max_features', fontsize=18)\nax[1,1].set_ylabel('Accuracy', fontsize=18)\nax[1,1].legend()\nfig.set_size_inches(20,10)\nplt.show()","05598738":"params = {\n    'max_depth': [2, 3, 4,5, 8,10, 20,30,40],\n    'min_samples_split': [5, 50, 100,500,750,1000,1200,1250],\n    'criterion': [\"gini\", \"entropy\"],\n    'splitter': ['best','random']    \n}","771de9b8":"dt = DecisionTreeClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=dt, \n                           param_grid=params, \n                           cv=5, n_jobs=-1, verbose=1, scoring = \"accuracy\")","b9bcf343":"grid_search.fit(train_X,train_y)\n","5ce70d7c":"grid_search.best_estimator_","c260eaee":"np.mean(cross_validate(grid_search.best_estimator_, bank2.drop('deposit',1), bank2.deposit, cv=5)['test_score'])","266fe1e8":"np.mean(metrics.accuracy_score(grid_search.best_estimator_.predict(train_X),train_y))","f24158a4":"grid_search.best_estimator_.get_n_leaves()","b0321bdb":"train_accuracy=[]\ntest_accuracy=[]\nfor i in range(1,30):\n    model=DecisionTreeClassifier(min_samples_leaf=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\nplt.figure(figsize=(10,5))\nplt.plot(train_accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('Max depth')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","3c079a9a":"train_accuracy=[]\ntest_accuracy=[]\nfor i in range(2,30):\n    model=DecisionTreeClassifier(min_samples_split=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\nplt.figure(figsize=(10,5))\nplt.plot(train_accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('Max depth')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","07732dd3":"train_accuracy=[]\ntest_accuracy=[]\nfor i in range(2,43):\n    model=DecisionTreeClassifier(max_features=i,max_depth=4)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    train_accuracy.append(metrics.accuracy_score(prediction,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    test_accuracy.append(np.mean((cv_results['test_score'])))\nplt.figure(figsize=(10,5))\nplt.plot(train_accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(test_accuracy, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('Max depth')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","b7b87389":"y_train_str = train_y.astype('str')\ny_train_str[y_train_str == '0'] = 'no'\ny_train_str[y_train_str == '1'] = 'yes'\ny_train_str = y_train_str.values","b96a2675":"export_graphviz(model, out_file='tree.dot', \n                feature_names = [i for i in train_X.columns],\n                class_names = y_train_str,\n                rounded = True, \n                label='root',\n                precision = 2, filled = True, proportion=False)\n\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\nImage(filename = 'tree.png')","5c9860c7":"accuracy=[]\ncv_score=[]\nfor i in range(1,15):\n    model=KNeighborsClassifier(n_neighbors=i) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X_norm,train_y_norm)\n    prediction=model.predict(test_X_norm)                                               \n#     print('The accuracy of the '+str(i)+' KNN is',metrics.accuracy_score(prediction,test_y))\n    accuracy.append(model.score(train_X_norm,train_y_norm))\n    cv_results = cross_validate(model, bank2_norm.drop('deposit',1), bank2_norm.deposit, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot(accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('K --> Number of nearest neighbors')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","c3d21d14":"accuracy=[]\ncv_score=[]\nfor i in range(1,15):\n    model=KNeighborsClassifier(n_neighbors=i) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)                                               \n#     print('The accuracy of the '+str(i)+' KNN is',metrics.accuracy_score(prediction,test_y))\n    accuracy.append(model.score(train_X,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot(accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('K --> Number of nearest neighbors')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","1de31c25":"# model = svm.SVC(kernel=\"linear\") #select the algorithm\n# model.fit(train_X,train_y)\n# prediction=model.predict(train_X)\n# print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\n# cv_results = cross_validate(model, bank2.drop('target',1), bank2.target, cv=5)\n# print(np.mean(cv_results['test_score']))\n# #we pass the predicted output by the model and the actual output\n\n# model = svm.LinearSVC(C=1.0, max_iter=10000) #select the algorithm\n# model.fit(train_X,train_y)\n# prediction=model.predict(train_X)\n# print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\n# cv_results = cross_validate(model, bank2.drop('target',1), bank2.target, cv=5)\n# print(np.mean(cv_results['test_score']))\n\nmodel = svm.SVC(kernel=\"poly\") #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))\n\nmodel = svm.SVC(kernel=\"rbf\") #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))\n","3816a811":"model = svm.SVC(kernel=\"rbf\",C=0.001) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))\n\nmodel = svm.SVC(kernel=\"rbf\",C=0.01) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))\n\nmodel = svm.SVC(kernel=\"rbf\",C=0.1) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))\n\n\nmodel = svm.SVC(kernel=\"rbf\",C=10) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))","5ec5178b":"model = svm.SVC(kernel=\"rbf\",C=100) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))","0e1b1dc2":"model = svm.SVC(kernel=\"rbf\",C=10000) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))","7f67fb47":"model = svm.SVC(kernel=\"rbf\",C=0.00001) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\nprint(np.mean(cv_results['test_score']))","9c419346":"X_train_NN = torch.tensor(train_X_norm.to_numpy()).float()\nX_test_NN = torch.tensor(test_X_norm.to_numpy()).float()\ny_train_NN = torch.tensor(train_y_norm.values).long()\ny_test_NN = torch.tensor(test_y_norm.values).long()","0b24c776":"class bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(42, 500)\n        self.fc2 = nn.Linear(500, 350)\n        self.fc3 = nn.Linear(350, 200)\n        self.fc5 = nn.Linear(200, 100)\n        self.fc6 = nn.Linear(100, 20)\n        self.fc4 = nn.Linear(20, 2)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        return self.fc4(x)","0f9dd074":"for i in [0.2,0.3,0.1]:\n    net = bankNN()\n    optimizer = optim.SGD(net.parameters(),lr=i)\n    criterion = nn.CrossEntropyLoss()\n    losses = []\n    for epoch in range(1, 500):\n        optimizer.zero_grad()\n        outputs = net(X_train_NN)\n        loss = criterion(outputs, y_train_NN)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    plt.plot(losses,label=\"Learning rate: \"+str(i))\n    plt.legend()\n    fig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)","a14274b6":"for i in [0.001,0.01,0.1]:\n    net = bankNN()\n    optimizer = optim.SGD(net.parameters(),lr=0.3,momentum=i)\n    criterion = nn.CrossEntropyLoss()\n    losses = []\n    for epoch in range(1, 200):\n        optimizer.zero_grad()\n        outputs = net(X_train_NN)\n        loss = criterion(outputs, y_train_NN)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    plt.plot(losses,label=\"Momentum: \"+str(i))\n    plt.legend()\n    fig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)","fa9fc75a":"print(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","1b45dc9d":"for i in range(1,7):\n    print('Depth: '+ str(i))\n    clf = GradientBoostingClassifier(n_estimators=50, max_depth=i,random_state=0)\n    clf = clf.fit(train_X, train_y)\n    prediction=clf.predict(train_X)\n    print('Train Boost',round(metrics.accuracy_score(prediction,train_y),2))\n    cv_results = cross_validate(clf, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    print('Test Boost',round(np.mean(cv_results['test_score']),2))\n    model=DecisionTreeClassifier(max_depth=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    print('Train',round(metrics.accuracy_score(prediction,train_y),2))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5)\n    print('Test',round(np.mean(cv_results['test_score']),2))\n","a2e3285a":"accuracy=[]\ncv_score=[]\nfor i in range(1,500,10):\n    model=GradientBoostingClassifier(n_estimators=i, max_depth=4,random_state=0) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)                                               \n    accuracy.append(model.score(train_X,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot(accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('Number of estimators')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","df80bb0d":"plt.figure(figsize=(10,5))\nplt.plot(range(1,500,10),accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(range(1,500,10),cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('Number of estimators')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","93cc4c3a":"accuracy=[]\ncv_score=[]\nfor i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n    model=GradientBoostingClassifier(n_estimators=10, max_depth=4,random_state=0,learning_rate=i) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)                                               \n    accuracy.append(model.score(train_X,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('learning rate')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","93435f9f":"accuracy=[]\ncv_score=[]\nfor i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n    model=GradientBoostingClassifier(n_estimators=25, max_depth=4,random_state=0,learning_rate=i) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)                                               \n    accuracy.append(model.score(train_X,train_y))\n    cv_results = cross_validate(model, bank2.drop('deposit',1), bank2.deposit, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('learning rate')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","4ea87c7f":"iris = pd.read_csv(\"..\/input\/iris\/Iris.csv\") #load the dataset\niris.drop('Id',axis=1,inplace=True)","3774f0d0":"iris.Species.value_counts()","0deb0f44":"fig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","2c8e2679":"fig = iris[iris.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","d9bde201":"plt.figure(figsize=(7,4)) \nsns.heatmap(iris.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","253d9535":"train, test = train_test_split(iris, test_size = 0.3,random_state=42)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train.shape)\nprint(test.shape)","5d04bf31":"train_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]# taking the training data features\ntrain_y=train.Species# output of our training data\ntest_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] # taking test data features\ntest_y =test.Species   #output value of test data","5448ca71":"iris_norm = (iris.drop('Species',1) - np.min(iris.drop('Species',1))) \/ (np.max(iris.drop('Species',1)) - np.min(iris.drop('Species',1))).values\niris_norm['Species']=iris.Species","da48a131":"iris_norm['Species'][iris_norm['Species'] == 'Iris-versicolor'] =0\niris_norm['Species'][iris_norm['Species'] == 'Iris-virginica'] = 1\niris_norm['Species'][iris_norm['Species'] == 'Iris-setosa'] = 2\niris_norm['Species']=iris_norm['Species'].astype(int)","592fe58b":"train_norm, test_norm = train_test_split(iris_norm, test_size = 0.3,random_state=0)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train_norm.shape)\nprint(test_norm.shape)","421ffa5e":"train_X_norm = train_norm.drop('Species',1)# taking the training data features\ntrain_y_norm=train_norm.Species # output of our training data\ntest_X_norm= test_norm.drop('Species',1) # taking test data features\ntest_y_norm =test_norm.Species   #output value of test data\n","dfc95086":"model=DecisionTreeClassifier(max_depth=3)\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))","faa42040":"cv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5)\nsorted(cv_results.keys())\nprint(cv_results['test_score'])\nprint(np.mean(cv_results['test_score']))\n","f45997db":"y_train_str = train_y.astype('str')\ny_train_str = y_train_str.values","3a1a0407":"\nexport_graphviz(model, out_file='tree.dot', \n                feature_names = [i for i in train_X.columns],\n                class_names = y_train_str,\n                rounded = True, \n                label='root',\n                precision = 2, filled = True, proportion=False)\n\n\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=100'])\n\nImage(filename = 'tree.png')","05ee37dd":"for i in range(1,1):\n    print('Depth: '+ str(i))\n    clf = GradientBoostingClassifier(n_estimators=5, max_depth=i,random_state=0)\n    clf = clf.fit(train_X, train_y)\n    prediction=clf.predict(train_X)\n    print('Train Boost',round(metrics.accuracy_score(prediction,train_y),2))\n    cv_results = cross_validate(clf, iris.drop('Species',1), iris.Species, cv=5)\n    print('Test Boost',round(np.mean(cv_results['test_score']),2))\n    model=DecisionTreeClassifier(max_depth=i)\n    model.fit(train_X,train_y)\n    prediction=model.predict(train_X)\n    print('Train',round(metrics.accuracy_score(prediction,train_y),2))\n    cv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5)\n    print('Test',round(np.mean(cv_results['test_score']),2))\n","aabccfbc":"train_X.columns","e7ea836a":"clf.feature_importances_","29c5251f":"accuracy=[]\ncv_score=[]\nfor i in range(1,50):\n    model=GradientBoostingClassifier(n_estimators=5, max_depth=i,random_state=0) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)                                               \n    accuracy.append(model.score(train_X,train_y))\n    cv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot(accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(cv_score, label=\"3 fold CV Accuracy\", marker='o')\nplt.xlabel('Number of estimators')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","54e9aa46":"# print('Score, Training: ', clf.score(train_X, train_y)) #accuracy score on training data\n# print('Score, Testing: ', clf.score(test_X, test_y))","f691e516":"X_train_NN = torch.tensor(train_X_norm.to_numpy()).float()\nX_test_NN = torch.tensor(test_X_norm.to_numpy()).float()\ny_train_NN = torch.tensor(train_y_norm.values).long()\ny_test_NN = torch.tensor(test_y_norm.values).long()","1d648dbc":"class bankNN(nn.Module):\n    def __init__(self):\n        super(bankNN, self).__init__()\n        self.fc1 = nn.Linear(4, 10)\n        self.fc2 = nn.Linear(10, 10)\n        self.fc3 = nn.Linear(10, 3)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)","38beb4b2":"for i in [0.01,0.05,0.1,0.15]:\n    net = bankNN()\n    optimizer = optim.SGD(net.parameters(),lr=i)\n    criterion = nn.CrossEntropyLoss()\n    losses = []\n    for epoch in range(1, 1000):\n        optimizer.zero_grad()\n        outputs = net(X_train_NN)\n        loss = criterion(outputs, y_train_NN)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    plt.plot(losses,label=\"Learning rate: \"+str(i))\n    plt.legend()\n    fig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)","fe809b1f":"for i in [0.001,0.01,0.1,0.8]:\n    net = bankNN()\n    optimizer = optim.SGD(net.parameters(),lr=0.1,momentum=i)\n    criterion = nn.CrossEntropyLoss()\n    losses = []\n    for epoch in range(1, 500):\n        optimizer.zero_grad()\n        outputs = net(X_train_NN)\n        loss = criterion(outputs, y_train_NN)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    plt.plot(losses,label=\"Momentum: \"+str(i))\n    plt.legend()\n    fig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.xlabel(\"Epoch\", fontsize=18)\nplt.ylabel(\"Loss\", fontsize=18)","55d12a7b":"print(\"Training Accuracy:\",get_accuracy(X_train_NN,y_train_NN))\nprint(\"Testing Accuracy:\",get_accuracy(X_test_NN,y_test_NN))","cad01c42":"def make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n# import some data to play with\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris[['SepalLengthCm', 'SepalWidthCm']]\ny = iris.Species\ny[y== 'Iris-versicolor'] =0\ny[y== 'Iris-virginica'] = 1\ny[y== 'Iris-setosa'] = 2\ny=y.astype(int)\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (svm.SVC(kernel='linear', C=C),\n          svm.LinearSVC(C=C, max_iter=10000),\n          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = ('SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\nX0, X1 = X['SepalLengthCm'], X['SepalWidthCm']\nxx, yy = make_meshgrid(X0, X1)\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=plt.cm.Spectral, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.Spectral, s=35, edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('Sepal length')\n    ax.set_ylabel('Sepal width')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\nfig=plt.gcf()\nfig.set_size_inches(12,10)\nplt.show()","a778701b":"def make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n# import some data to play with\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris[['PetalLengthCm', 'PetalWidthCm']]\ny = iris.Species\ny[y== 'Iris-versicolor'] =0\ny[y== 'Iris-virginica'] = 1\ny[y== 'Iris-setosa'] = 2\ny=y.astype(int)\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (svm.SVC(kernel='linear', C=C),\n          svm.LinearSVC(C=C, max_iter=10000),\n          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = ('SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\nX0, X1 = X['PetalLengthCm'], X['PetalWidthCm']\nxx, yy = make_meshgrid(X0, X1)\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=plt.cm.Spectral, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.Spectral, s=35, edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('Petal length')\n    ax.set_ylabel('Petal width')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\nfig=plt.gcf()\nfig.set_size_inches(12,10)\nplt.show()","0828527d":"model = svm.SVC(kernel=\"linear\") #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5)\nprint(np.mean(cv_results['test_score']))\n#we pass the predicted output by the model and the actual output\n\nmodel = svm.LinearSVC(C=1.0, max_iter=10000) #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5)\nprint(np.mean(cv_results['test_score']))\n\nmodel = svm.SVC(kernel=\"poly\") #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5)\nprint(np.mean(cv_results['test_score']))\n\nmodel = svm.SVC(kernel=\"rbf\") #select the algorithm\nmodel.fit(train_X,train_y)\nprediction=model.predict(train_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,train_y))\ncv_results = cross_validate(model, iris.drop('Species',1), iris.Species, cv=5)\nprint(np.mean(cv_results['test_score']))\n","c7f5940b":"from mlxtend.plotting import plot_decision_regions\n\ndef knn_comparison(cols,data, k):\n x = data[cols].values\n y = data['Species']\n y[y== 'Iris-versicolor'] =0\n y[y== 'Iris-virginica'] = 1\n y[y== 'Iris-setosa'] = 2\n y=y.astype(int).values\n clf = KNeighborsClassifier(n_neighbors=k)\n clf.fit(x, y)\n# Plotting decision region\n plot_decision_regions(x, y, clf=clf, legend=2)\n# Adding axes annotations\n plt.xlabel(cols[0])\n plt.ylabel(cols[1])\n plt.title('KNN with K='+ str(k))\n plt.show()","62da06fa":"for i in [1,5,20]:\n    knn_comparison(['PetalLengthCm','PetalWidthCm'],iris.copy(), i)","fbed1dd9":"for i in [1,5,20]:\n    knn_comparison(['SepalLengthCm','SepalWidthCm'],iris.copy(), i)","270c0a41":"accuracy=[]\ncv_score=[]\nfor i in range(1,100):\n    model=KNeighborsClassifier(n_neighbors=i) #this examines 3 neighbours for putting the new data into a class\n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)                                               \n#     print('The accuracy of the '+str(i)+' KNN is',metrics.accuracy_score(prediction,test_y))\n    accuracy.append(model.score(train_X,train_y))\n    cv_results = cross_validate(model,iris.drop('Species',1), iris.Species, cv=5) \n    cv_score.append(np.mean(cv_results['test_score']))\nplt.figure(figsize=(10,5))\nplt.plot(accuracy, label=\"Training Accuracy\", marker='o')\nplt.plot(cv_score, label=\"5 fold CV Accuracy\", marker='o')\nplt.xlabel('K --> Number of nearest neighbors')\nplt.ylabel('Accuracy')\nplt.legend(loc=\"best\")\nplt.show()","32af6903":"## Decision Tree","372680dd":"## Boosting","bb0a77be":"## KNN","06b9f68f":"## NN","a2dc251b":"# Bank Marketing","56cd5804":"# Iris","b988d747":"## NN","18e1d6ac":"## Decision Tree","5ae73881":"## Boosting","9712d6cb":"## SVC","16e36664":"## KNN","bc8f405e":"## SVM"}}