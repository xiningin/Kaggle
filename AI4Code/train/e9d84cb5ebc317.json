{"cell_type":{"c07f7a26":"code","a9f6fdd1":"code","1cef293d":"code","4b93f715":"code","471ca201":"code","11b873cb":"code","983951ff":"code","4181b64c":"code","3d8bd620":"code","ffcd2607":"code","693a8398":"code","833efcbf":"code","0e802d22":"code","867101f3":"code","c38260ae":"code","f33301c6":"code","874b5943":"code","cf658cc1":"code","c1caff1b":"code","93d8dad7":"code","7fa4f154":"code","dc0a9828":"code","ba790bfe":"code","4ad91092":"code","299cd2cc":"code","3bf85d1a":"code","e5146b7a":"code","0e275f1c":"code","fd5878c3":"markdown","1dd4ed21":"markdown","2bd24056":"markdown","7ed68023":"markdown","c7784ce3":"markdown","c7896311":"markdown","7ccd7798":"markdown"},"source":{"c07f7a26":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings \nwarnings.filterwarnings('ignore')","a9f6fdd1":"df =pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\ndf","1cef293d":"df.info()","4b93f715":"df.describe()","471ca201":"df.isnull().mean()","11b873cb":"### Lets check for distribution of Heart Disease\ndf.HeartDisease.hist();","983951ff":"# Categorical feature\ncat_feat = df.select_dtypes(include='object')\ndisplay(cat_feat.head())\n\n## Numerical Feature\nn_feat = df.select_dtypes(include='int64')\nprint(n_feat.head())","4181b64c":"for i in cat_feat:\n    print(i,':> No of Unique Values : ', df[i].unique(),'\\n')","3d8bd620":"plt.figure(figsize=(20,4))\nj=1\nfor i in cat_feat:\n    plt.subplot(1,5,j)\n    sns.countplot(x= df[i], data= df,hue='HeartDisease' )\n    j=j+1\nplt.show()","ffcd2607":"def with_hue(plot, feature, Number_of_categories, hue_categories):\n    a = [p.get_height() for p in plot.patches]\n    patch = [p for p in plot.patches]\n    for i in range(Number_of_categories):\n        total = feature.value_counts().values[i]\n        for j in range(hue_categories):\n            percentage = '{:.1f}%'.format(100 * a[(j*Number_of_categories + i)]\/total)\n            x = patch[(j*Number_of_categories + i)].get_x() + patch[(j*Number_of_categories + i)].get_width() \/ 2 - 0.15\n            y = patch[(j*Number_of_categories + i)].get_y() + patch[(j*Number_of_categories + i)].get_height() \n            ax.annotate(percentage, (x, y), size = 12)\n    #plt.show()\n\n\nplt.figure(figsize=(22,5))\nj=1\nfor i in cat_feat:\n    plt.subplot(1,5,j)\n    ax= sns.countplot(x= df[i], data= df,hue='HeartDisease', palette='rocket' )\n    n= df[i].nunique()\n    with_hue(ax,df[i],n,2)\n    j=j+1\nplt.show()","693a8398":"sns.pairplot(n_feat, hue='HeartDisease');","833efcbf":"### Checking for outliers and histogram of numerical data\n\nj=1\nfor i in n_feat:\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)})\n    mean=df[i].mean()\n    median=df[i].median()\n    mode=df[i].mode().values[0]\n\n    sns.boxplot(data=df, x=i, ax=ax_box)\n    ax_box.axvline(mean, color='r', linestyle='--')\n    ax_box.axvline(median, color='g', linestyle='-')\n    ax_box.axvline(mode, color='b', linestyle='-')\n\n    sns.histplot(data=df, x=i, ax=ax_hist, kde=True)\n    ax_hist.axvline(mean, color='r', linestyle='--', label=\"Mean\")\n    ax_hist.axvline(median, color='g', linestyle='-', label=\"Median\")\n    ax_hist.axvline(mode, color='b', linestyle='-', label=\"Mode\")\n\n    ax_hist.legend()\n\n    ax_box.set(xlabel='')\n    plt.show()","0e802d22":"plt.figure(figsize=(12,7))\nsns.heatmap(n_feat.corr(), annot=True, cmap='RdYlBu');","867101f3":"# Creating training and test dataset to check for overfitting\ndf_train= df.iloc[1:750]\ndf_test = df.loc[750:]\ndisplay(df_train.head(3))\nprint()\ndisplay(df_test.head(3))\n\ntest_validation = df_test['HeartDisease']\ntest_extra = df_test.drop('HeartDisease', axis=1)\n","c38260ae":"def encoder(df):\n    z=pd.DataFrame()\n    for i in cat_feat:\n        df1=pd.get_dummies(df[i],drop_first=True)\n        z=pd.concat([df1,z],axis=1)\n    for j in n_feat:\n        df1=df[j]\n        z=pd.concat([df1,z],axis=1)\n    return(z)","f33301c6":"train = encoder(df_train)\ntrain","874b5943":"plt.figure(figsize=(12,7))\nsns.heatmap(train.corr(), annot=True, cmap='RdYlBu');","cf658cc1":"y=train['HeartDisease']\nx=train.drop('HeartDisease', axis=1)","c1caff1b":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)","93d8dad7":"from sklearn.preprocessing import StandardScaler\n'''sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)'''","7fa4f154":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb","dc0a9828":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline","ba790bfe":"pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('lr_classifier',LogisticRegression(random_state=7))])\n\npipeline_knn=Pipeline([('scalar2',StandardScaler()),\n                     ('knn_classifier',KNeighborsClassifier())])\n\npipeline_svc=Pipeline([('scalar3',StandardScaler()),\n                     ('svc_classifier',SVC(random_state=7))])\n\npipeline_nb =Pipeline([('scalar4',StandardScaler()),\n                     ('nb_classifier',GaussianNB())])\n\npipeline_dt=Pipeline([('scalar5',StandardScaler()),\n                     ('dt_classifier',DecisionTreeClassifier())])\n\npipeline_randomforest=Pipeline([('scalar6',StandardScaler()),\n                     ('rf_classifier',RandomForestClassifier())])\n\npipeline_xgb = Pipeline([('scalar7', StandardScaler()),\n                 ('xgb_classifier', XGBClassifier(boosting_type='gbdt',max_depth=6,min_child_weight = 1, learning_rate=0.1 ))])\n\npipeline_cbc = Pipeline([('scalar8', StandardScaler()),\n                 ('cbc_classifier', CatBoostClassifier(iterations=520, learning_rate=0.1))])\n\npipeline_lgbm = Pipeline([('scalar9', StandardScaler()),\n                 ('lgbm_classifier', lgb.LGBMClassifier(boosting_type='gbdt',min_child_samples=20,\n                                                        min_child_weight=0.001,iterations=50, learning_rate=0.1))])","4ad91092":"## LEts make the list of pipelines\npipelines = [pipeline_lr, pipeline_knn,pipeline_svc, pipeline_nb, pipeline_dt, pipeline_randomforest, pipeline_xgb,\n            pipeline_cbc,pipeline_lgbm]","299cd2cc":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","3bf85d1a":"# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'KNN', 2: 'SVC', 3: 'NB', 4:'dt', 5:'RF',6:'XGB',7:'CB', 8:'LGBM'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(x_train, y_train)","e5146b7a":"for i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(x_test,y_test)))\n                                \n","0e275f1c":"for i,model in enumerate(pipelines):\n    if model.score(x_test,y_test)>best_accuracy:\n        best_accuracy=model.score(x_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","fd5878c3":"# Model Comparison\n1. Logistic Regression","1dd4ed21":"# Creating Pipeline","2bd24056":"#### Plot graph with percentage\n\ndef without_hue(plot, feature):\n    total = len(feature)\n    for p in plot.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        ax.annotate(percentage, (x, y), size = 12)\n    plt.show()\n  \n\n\n\ndef with_hue(plot, feature, Number_of_categories, hue_categories):\n    a = [p.get_height() for p in plot.patches]\n    patch = [p for p in plot.patches]\n    for i in range(Number_of_categories):\n        total = feature.value_counts().values[i]\n        for j in range(hue_categories):\n            percentage = '{:.1f}%'.format(100 * a[(j*Number_of_categories + i)]\/total)\n            x = patch[(j*Number_of_categories + i)].get_x() + patch[(j*Number_of_categories + i)].get_width() \/ 2 - 0.15\n            y = patch[(j*Number_of_categories + i)].get_y() + patch[(j*Number_of_categories + i)].get_height() \n            ax.annotate(percentage, (x, y), size = 12)\n    plt.show()\n    \nax = sns.countplot(x= 'ChestPainType', data= df,hue='HeartDisease' )\n\nwith_hue(ax,df['ChestPainType'], 4,2)\n\n\n[Source](https:\/\/stackoverflow.com\/questions\/31749448\/how-to-add-percentages-on-top-of-bars-in-seaborn)","7ed68023":"![download.jfif](attachment:3bccf87b-e521-4e91-9dc7-f62021b6e4d0.jfif)","c7784ce3":"# standardize\n### Q. What is data standardization exactly?\nData standardization is about making sure that data is internally consistent; that is, each data type has the same content and format.\nStandardized values are useful for tracking data that isn\u2019t easy to compare otherwise.","c7896311":"# One Hot Encoding","7ccd7798":"# Spliting the training dataset\n"}}