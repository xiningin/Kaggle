{"cell_type":{"7041f7ee":"code","93efa978":"code","e85c1f8e":"code","277fd997":"code","6f75345b":"code","df9a1e98":"code","c7efa23f":"code","84caf240":"code","d61de8da":"code","2860569a":"code","30753286":"code","95d10ef9":"code","f4ea861e":"code","bf9350ef":"code","a05d7d51":"code","95db0188":"code","b0027fd0":"code","542e8215":"code","dcbc8c66":"code","0f22671c":"code","e5541471":"code","054f81f9":"code","8600308a":"code","90264724":"code","b891f98c":"code","70844cbd":"code","0f7b9a0a":"code","9e860cd3":"code","e2dd3043":"code","4ebc96fd":"code","b82d1767":"code","b7176bd7":"code","4872c01d":"code","5d4a1421":"code","5fc4fe92":"code","d68e40d5":"code","8c335be0":"code","46014bda":"markdown","4a4b4d32":"markdown","194756dc":"markdown","c3471d73":"markdown","1d09a1cc":"markdown","aef149af":"markdown","aed686a1":"markdown","fa25e830":"markdown","8f74ec05":"markdown","e008f528":"markdown","c5787a7d":"markdown","aa0ca13d":"markdown","e0cdc8b7":"markdown","d0f40ef5":"markdown"},"source":{"7041f7ee":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","93efa978":"# Get the data and labels\ntrain_data = pd.read_csv(\"..\/input\/ahdd1\/csvTrainImages 60k x 784\/csvTrainImages 60k x 784.csv\")\ntest_data = pd.read_csv(\"..\/input\/ahdd1\/csvTestImages 10k x 784.csv\")\ntrain_label = pd.read_csv(\"..\/input\/ahdd1\/csvTrainLabel 60k x 1.csv\")\ntest_label = pd.read_csv(\"..\/input\/ahdd1\/csvTestLabel 10k x 1.csv\")","e85c1f8e":"# Let's take a look at the shape of the data\ntrain_data.shape","277fd997":"train_label.iloc[:,0]","6f75345b":"# Let's look at the label's distribution\ntrain_label.iloc[:, 0].value_counts().plot(cmap = \"autumn\", kind = \"bar\")","df9a1e98":"# Let's one hot encode the labels:\ny_train = np.zeros(shape=(train_label.shape[0], train_label.values.max() + 1))\nfor index, row in train_label.iterrows():\n    y_train[index][row[\"0\"]] = 1","c7efa23f":"y_test = np.zeros(shape=(test_label.shape[0], test_label.values.max() + 1))\nfor index, row in test_label.iterrows():\n    y_test[index][row[\"0\"]] = 1","84caf240":"y_train","d61de8da":"y_test","2860569a":"# Let's first convert the data to numpy array for better array manipulation:\nx_train = train_data.to_numpy()\nx_test = test_data.to_numpy()","30753286":"# Let's reshape it:\nx_train = x_train.reshape(x_train.shape[0], 28, 28)\nx_test = x_test.reshape(x_test.shape[0], 28, 28)","95d10ef9":"fig, ax = plt.subplots(1,10, figsize = (20,20))\nfor i in range(10):\n    ax[i].imshow(x_train[i])","f4ea861e":"# Let's transpose all the data:\nx_train = np.array([element.transpose() for element in x_train])\nx_test = np.array([element.transpose() for element in x_test])","bf9350ef":"fig, ax = plt.subplots(1,10, figsize = (20,20))\nfor i in range(10):\n    ax[i].imshow(x_train[i])","a05d7d51":"fig, ax = plt.subplots(1,10, figsize = (20,20))\nfor i in range(10):\n    ax[i].imshow(x_train[i*10])","95db0188":"# Let's get a list of shuffled indexes so that we could apply the order to bothe the data and the label\ntrain_shuffle_idx = np.arange(x_train.shape[0])\ntest_shuffle_idx = np.arange(x_test.shape[0])\nnp.random.shuffle(train_shuffle_idx)\nnp.random.shuffle(test_shuffle_idx)","b0027fd0":"# Shuffle:\nx_train = x_train[train_shuffle_idx]\ny_train = y_train[train_shuffle_idx]\nx_test = x_test[test_shuffle_idx]\ny_test = y_test[test_shuffle_idx]","542e8215":"fig, ax = plt.subplots(1,10, figsize = (20,20))\nfor i in range(10):\n    ax[i].imshow(x_train[i*10])","dcbc8c66":"# Let's see if the labels are true:\nfor i in range(10):\n    print(y_train[i*10].argmax())","0f22671c":"# Let's take a look at the maximum values and min values of the data:\nx_train.min()","e5541471":"x_train.max()","054f81f9":"#Let's apply normalization to the data:\nx_train = tf.keras.utils.normalize(x_train)\nx_test = tf.keras.utils.normalize(x_test)","8600308a":"print(f\"x_train's min value: {x_train.min()}\\nx_train's max value: {x_train.max()}\")","90264724":"x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)","b891f98c":"x_train.shape","70844cbd":"model = tf.keras.Sequential()\n\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=5,activation=\"relu\", padding=\"same\", input_shape=x_train.shape[1:]))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\"))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,activation=\"relu\", padding=\"same\"))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(1,1), padding=\"valid\"))\nmodel.add(tf.keras.layers.BatchNormalization())\n          \nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n","0f7b9a0a":"model.summary()","9e860cd3":"# We will use the Adam optimizer which is similar to SGD\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n# We will let the learning rate decrease by a factor of 0.2 once the accuracy does not improve for 3 epochs.\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=0.0, verbose=1)\n# We will stop learning our val_accuracy does not mprove after 7 epochs\nearlyStop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=7, verbose=1)","e2dd3043":"# Compile the model:\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","4ebc96fd":"tf.keras.utils.plot_model(model, show_shapes = True)","b82d1767":"# Training the model:\nhist = model.fit(x=x_train, y=y_train, batch_size=32, epochs=40, validation_split=0.2, callbacks=[reduce_lr, earlyStop])","b7176bd7":"model.evaluate(x_test, y_test)","4872c01d":"hist.history.keys()","5d4a1421":"fig, ax = plt.subplots(2,1, figsize=(10,10))\n\nax[0].set_title(\"Loss\")\nax[0].plot(hist.history[\"loss\"], c=\"b\", label=\"Train_loss\")\nax[0].plot(hist.history[\"val_loss\"], c=\"r\", label=\"Validation_loss\")\nax[0].legend(loc=\"best\")\n\nax[1].set_title(\"Accuracy\")\nax[1].plot(hist.history[\"accuracy\"], c=\"b\", label=\"Train_accuracy\")\nax[1].plot(hist.history[\"val_accuracy\"], c=\"r\", label=\"Validation_accuracy\")\nax[1].legend(loc=\"best\");","5fc4fe92":"sample = x_test[:10]","d68e40d5":"pred = model.predict(sample)","8c335be0":"fig, ax = plt.subplots(1, 10, figsize=(20,20))\nfor idx, x in enumerate(sample):\n    ax[idx].imshow(x)\n    ax[idx].set_xlabel(pred[idx].argmax(), fontsize=50)","46014bda":"* We could see that the labels are distributed equally. This means that we could use accuracy for our evaluation metrics.\n* We could also use Precision, Recall, F1-scores.","4a4b4d32":"* Now, the data is shuffled and all is good","194756dc":"## __The model does a moderate job in predicting.__","c3471d73":"#### It looks like the numbers are flipped and inverted. Let's do a transpose to the numpy array to see if it gets any better","1d09a1cc":"## Data loading:","aef149af":"* This looks much better!\n* It looks like the numbers are in order, let's see if they are not randomly mixed","aed686a1":"#### Since the shape of the data is of the format `[batchsize, height*width*channel]`, let's convert it to the format`[batchsize, height, width]`, channel is 1 (grayscale)","fa25e830":"# __Predicting Arabic Handwritten Digits with Simple Convolutional Layers__\n#### _Any advice is appreciated!_","8f74ec05":"Our model will consist of:\n* * Convolutional 2D layer with 32 or 64 filters (giving us \"n\" feature maps), kernel size of 3(shape = (3X3)), stride = 1, padding = \"same\" so that we get the same shape as input\n* * Maxpooling layer with the shape (2,2), stride = 2\n* * Batch normalization layer so that we don't have to worry about the parameter initializations and we would suffer less from internal covariate shift (Batch normalization also discards the need for regularization methods such as dropout layers). Briefly speaking, batch normalization centers and scales each mini-batch to be centered (around mean = 0 and stddev around 1) and performs linear transformation such that output = slope * ((x - mean(x_minibatch))\/stddev(x_minibatch)) + beta. The reason why we perform linear transformation is so that the output can leverage and exploit the activation functions. \n* We would repeat the above set of layers 3 times, then add:\n* * A flatten layer that flattens the outputs\n* * A dense layer with 10 nodes and activation function \"softmax\" so that we could predict the outputs and optimize them","e008f528":"#### __Our model will take the input of shape `[batchsize, height, width, channel]`, so we need to reshape our numpy arrays to be of the same shape:__","c5787a7d":"## Import libraries:","aa0ca13d":"## __Making the model:__","e0cdc8b7":"* The numbers are in order. This means that we need to randomly arrange them before we start fitting them into model","d0f40ef5":"## Data Exploration:"}}