{"cell_type":{"298eee63":"code","c67e28d3":"code","0ea3e3af":"code","e9b3f31f":"code","2de928f1":"code","3177d852":"code","4018681d":"code","448b245f":"code","688825d0":"code","3967acd1":"code","c7c13469":"code","0612ed13":"code","8c7e4c17":"code","ef144325":"code","07478ca9":"code","194eca82":"code","b8379c53":"code","2b2785c7":"code","ce117470":"code","66e240c2":"code","c518630b":"code","dff52be0":"code","1ddd92bc":"code","7ac9d2d4":"code","5b1b2821":"code","295011ee":"code","f3bf4ae9":"code","ebce698e":"code","85e457e8":"code","59b02d65":"code","7f81d405":"code","ee60cdd9":"code","f4340a80":"code","c7051f7c":"markdown","dca0d964":"markdown","d9deaf29":"markdown","ae79089f":"markdown","9a142ae6":"markdown","f43f8266":"markdown","37846d76":"markdown","7055b378":"markdown","6ca2229f":"markdown","44235677":"markdown","9dc18ec0":"markdown","7580579f":"markdown","7f7e2fbd":"markdown","a52d3891":"markdown","d4d81fef":"markdown","0c543c3c":"markdown","b60fc137":"markdown","8c26032a":"markdown","fe9e1b80":"markdown"},"source":{"298eee63":"from IPython.display import HTML\nHTML('''\n<script>\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=true;\n  });\n<\/script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Hide Code\"><\/form>''')","c67e28d3":"import os\nprint(os.listdir(\"..\/input\"))","0ea3e3af":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.utils import np_utils","e9b3f31f":"import warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"..\/input\/diabetes.csv\")\n#df = df.drop('Unnamed: 0', axis=1)\nprint(df.head())\nprint(df.shape)\nprint(df.columns)","2de928f1":"import seaborn as sns\nimport matplotlib.pyplot as plt","3177d852":"import seaborn as sns\n\ncorr=df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)","4018681d":"# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\n\nh = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\nX = df.drop(['Outcome'], axis = 1).values\npca = PCA(n_components=2,svd_solver='full')\nX = pca.fit_transform(X)\ny = df['Outcome']\n\n\n# X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n#                            random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\n#X += 2 * rng.uniform(size=X.shape)\n#linearly_separable = (X, y)\n\ndatasets = [df]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    #X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.3, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()","448b245f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Outcome'], axis = 1).values\nY = df['Outcome']\n\nX = StandardScaler().fit_transform(X)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)","688825d0":"from sklearn import svm\nimport matplotlib.pyplot as plt\ndef feature_plot(classifier, feature_names, top_features=4):\n coef = classifier.coef_.ravel()\n top_positive_coefficients = np.argsort(coef)[-top_features:]\n top_negative_coefficients = np.argsort(coef)[:top_features]\n top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n plt.figure(figsize=(18, 7))\n colors = ['green' if c < 0 else 'blue' for c in coef[top_coefficients]]\n plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n feature_names = np.array(feature_names)\n plt.xticks(np.arange(1 + 2 * top_features), feature_names[top_coefficients], rotation=45, ha='right')\n plt.show()\n\nprint(df.drop(['Outcome'], axis = 1).columns.values)\n\ntrainedsvm = svm.LinearSVC().fit(X, Y)\nfeature_plot(trainedsvm, df.drop(['Outcome'], axis = 1).columns.values)","3967acd1":"# Preprocessing :\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom itertools import product\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA","c7c13469":"trainedmodel = LogisticRegression().fit(X_Train,Y_Train)\npredictions =trainedmodel.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))","0612ed13":"trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","8c7e4c17":"trainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\npredictionsvm = trainedsvm.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","ef144325":"trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\npredictionstree = trainedtree.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionstree))\nprint(classification_report(Y_Test,predictionstree))","07478ca9":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\ndata = export_graphviz(trainedtree,out_file=None,feature_names=df.drop(['Outcome'], axis = 1).columns,\n                       class_names=['0', '1'],  \n                       filled=True, rounded=True,  \n                       max_depth=2,\n                       special_characters=True)\ngraph = graphviz.Source(data)\ngraph","194eca82":"trainedlda = LinearDiscriminantAnalysis().fit(X_Train, Y_Train)\npredictionlda = trainedlda.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionlda))\nprint(classification_report(Y_Test,predictionlda))","b8379c53":"trainednb = GaussianNB().fit(X_Train, Y_Train)\npredictionnb = trainednb.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionnb))\nprint(classification_report(Y_Test,predictionnb))","2b2785c7":"from xgboost import XGBClassifier\nfrom xgboost import plot_tree\nimport matplotlib.pyplot as plt\nmodel = XGBClassifier()\n\n# Train\nmodel.fit(X_Train, Y_Train)\n\nplot_tree(model)\nplt.figure(figsize = (50,55))\nplt.show()","ce117470":"from itertools import product\nimport itertools\n\npredictions =model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))\n\n# Thanks to: https:\/\/www.kaggle.com\/tejainece\/data-visualization-and-machine-learning-algorithms\ndef plot_confusion_matrix(cm, classes=[\"0\", \"1\"], title=\"\",\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion matrix ' +title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ncm_plot = confusion_matrix(Y_Test,predictions)\n\nplt.figure()\nplot_confusion_matrix(cm_plot, title = 'XGBClassifier')","66e240c2":"pca = PCA(n_components=2,svd_solver='full')\nX_pca = pca.fit_transform(X)\n# print(pca.explained_variance_)\n\nX_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)\n\n# pca = PCA(n_components=2,svd_solver='full')\n# X_reduced = pca.fit_transform(X_Train)\n#X_reduced = TSNE(n_components=2).fit_transform(X_Train, Y_Train)\n\ntrainednb = GaussianNB().fit(X_reduced, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(X_reduced, Y_Train)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_reduced,Y_Train)\ntrainedmodel = LogisticRegression().fit(X_reduced,Y_Train)\n\n# pca = PCA(n_components=2,svd_solver='full')\n# X_test_reduced = pca.fit_transform(X_Test)\n#X_test_reduced = TSNE(n_components=2).fit_transform(X_Test, Y_Test)\n\nprint('Naive Bayes')\npredictionnb = trainednb.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionnb))\nprint(classification_report(Y_Test,predictionnb))\n\nprint('SVM')\npredictionsvm = trainedsvm.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))\n\nprint('Random Forest')\npredictionforest = trainedforest.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\n\nprint('Logistic Regression')\npredictions =trainedmodel.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))","c518630b":"reduced_data = X_reduced\n\ntrainednb = GaussianNB().fit(reduced_data, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(reduced_data, Y_Train)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(reduced_data,Y_Train)\ntrainedmodel = LogisticRegression().fit(reduced_data,Y_Train)\n\n# Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_voting_decision_regions.html\n\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [trainednb, trainedsvm, trainedforest, trainedmodel],\n                        ['Naive Bayes Classifier', 'SVM',\n                         'Random Forest', 'Logistic Regression']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z,cmap=plt.cm.coolwarm, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(reduced_data[:, 0], reduced_data[:, 1], c=Y_Train,\n                                  s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","dff52be0":"# Load libraries\nfrom sklearn import datasets\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Create an LDA that will reduce the data down to 1 feature\nlda = LinearDiscriminantAnalysis(n_components=2)\n\n# run an LDA and use it to transform the features\nX_lda = lda.fit(X, Y).transform(X)\n\n# Print the number of features\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_lda.shape[1])\n\n## View the ratio of explained variance\nprint(lda.explained_variance_ratio_)\n\nX_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_lda, Y, test_size = 0.30, random_state = 101)\n\ntrainednb = GaussianNB().fit(X_reduced, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(X_reduced, Y_Train)\n\nprint('Naive Bayes')\npredictionnb = trainednb.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionnb))\nprint(classification_report(Y_Test,predictionnb))\n\nprint('SVM')\npredictionsvm = trainedsvm.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","1ddd92bc":"from sklearn.manifold import TSNE\nimport time\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(X)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","7ac9d2d4":"plt.figure(figsize=(6,5))\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=Y,\n    palette=sns.color_palette(\"hls\", 2),\n    data=df,\n    legend=\"full\",\n    alpha=0.3\n)","5b1b2821":"pca = PCA(n_components=2,svd_solver='full')\nX_pca = pca.fit_transform(X)\n# print(pca.explained_variance_)\n\n# print('Original number of features:', X.shape[1])\n# print('Reduced number of features:', X_lda.shape[1])\nprint(pca.explained_variance_ratio_)\n\nX_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)","295011ee":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X_reduced)","f3bf4ae9":"kpredictions = kmeans.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,kpredictions))\nprint(classification_report(Y_Test,kpredictions))","ebce698e":"plt.scatter(X_test_reduced[kpredictions ==0,0], X_test_reduced[kpredictions == 0,1], s=100, c='red')\nplt.scatter(X_test_reduced[kpredictions ==1,0], X_test_reduced[kpredictions == 1,1], s=100, c='black')","85e457e8":"import scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering\n\n# create dendrogram\ndendrogram = sch.dendrogram(sch.linkage(X_reduced, method='ward'))\n# create clusters\nhc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'ward')\n# save clusters for chart\nhierarchicalpredictions = hc.fit_predict(X_test_reduced)","59b02d65":"plt.scatter(X_test_reduced[hierarchicalpredictions ==0,0], X_test_reduced[hierarchicalpredictions == 0,1], s=100, c='red')\nplt.scatter(X_test_reduced[hierarchicalpredictions ==1,0], X_test_reduced[hierarchicalpredictions == 1,1], s=100, c='black')","7f81d405":"from keras.utils.np_utils import to_categorical\nY_Train = to_categorical(Y_Train)","ee60cdd9":"from keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\n\n#Y_Test = to_categorical(Y_Test)\n\ninput_dim = X_Train.shape[1]\nnb_classes = Y_Train.shape[1]\n\n# Here's a Deep Dumb MLP (DDMLP)\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=input_dim))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(nb_classes))\nmodel.add(BatchNormalization())\nmodel.add(Activation('sigmoid'))\n\n# we'll use categorical xent for the loss, and RMSprop as the optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nprint(\"Training...\")\nmodel.fit(X_Train, Y_Train, nb_epoch=50, batch_size=16, validation_split=0.1, verbose=80)\n\npreds = model.predict_classes(X_Test, verbose=0)","f4340a80":"print(confusion_matrix(Y_Test,preds))\nprint(classification_report(Y_Test,preds))","c7051f7c":"Naive Bayes","dca0d964":"Decision Tree","d9deaf29":"Linear Discriminant Analysis","ae79089f":"**Data Visualization**","9a142ae6":"Support Vector Machines","f43f8266":"Linear Discriminant Anaylsis","37846d76":"XGBoost","7055b378":"K-Means Clustering","6ca2229f":"**Preprocessing**","44235677":"Random Forest","9dc18ec0":"t-SNE","7580579f":"**Feature Engineering**","7f7e2fbd":"**Clustering**","a52d3891":"Principal Component Analysis","d4d81fef":"**Pima Indians Diabetes Database**","0c543c3c":"Logistic Regression","b60fc137":"**Machine Learning**","8c26032a":"****Deep Learning****","fe9e1b80":"Hierarchical Clustering"}}