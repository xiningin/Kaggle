{"cell_type":{"ef58aed7":"code","7e1acf1d":"code","b24fba4c":"code","6ca474b6":"code","a89fb143":"code","36efdb28":"code","81586f35":"code","aba0216b":"code","01fce0c6":"code","eb5a462d":"code","9d3d47fa":"code","7c0967e2":"code","ee55339d":"code","b7a87c91":"code","ec8e8d75":"code","7634806d":"code","50407805":"code","86b6b704":"code","0f24afd3":"code","e3a1f084":"code","519b9d3e":"code","b85a3eb8":"code","971f5b21":"code","e96bf539":"code","34e88161":"code","d82b5e64":"code","5e3d04ac":"code","108cebd1":"code","7189fc12":"code","474f231d":"code","f65cb0e0":"code","d72b626c":"code","15c27a65":"code","535b2146":"code","f621f3d5":"code","48513ead":"code","31c0e5bf":"code","3aaabbcf":"code","47617d8a":"code","504e1a4a":"code","a440c9fd":"code","528e48a9":"code","5c370541":"code","eacc7a4a":"code","92b23eab":"code","cff4a490":"code","c71512fb":"code","0c9ec911":"code","0d2d7119":"code","36743b24":"code","0018ca43":"code","2b610fbd":"code","b36c089d":"code","75033cf8":"code","ea19ccd5":"code","25f1fa39":"code","58b9c8b2":"markdown","69e3f9e2":"markdown"},"source":{"ef58aed7":"# Load libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport os\nfrom keras.applications.inception_v3 import InceptionV3\n# from keras.applications.resnet50 import ResNet50\n# from tensorflow.keras.applications.vgg16 import VGG16\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model\nfrom keras.utils import np_utils\nimport random\nfrom keras.preprocessing import image, sequence\nimport matplotlib.pyplot as plt\nimport time","7e1acf1d":"# Load data\nimages_dir = os.listdir(\"..\/input\/flickr8k\/flickr_data\/Flickr_Data\/\")\n\nimages_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Images\/'\ncaptions_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\ntrain_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt'\nval_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.devImages.txt'\ntest_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.testImages.txt'\n\ncaptions = open(captions_path, 'r').read().split(\"\\n\")\nx_train = open(train_path, 'r').read().split(\"\\n\")\nx_val = open(val_path, 'r').read().split(\"\\n\")\nx_test = open(test_path, 'r').read().split(\"\\n\")","b24fba4c":"# Loading captions as values and images as key in dictionary\ntokens = {}\n\nfor ix in range(len(captions)-1):\n    temp = captions[ix].split(\"#\")\n    if temp[0] in tokens:\n        tokens[temp[0]].append(temp[1][2:])\n    else:\n        tokens[temp[0]] = [temp[1][2:]]","6ca474b6":"# displaying an image and captions given to it\ntemp = captions[10].split(\"#\")\nfrom IPython.display import Image, display\nz = Image(filename=images_path+temp[0])\ndisplay(z)\n\nfor ix in range(len(tokens[temp[0]])):\n    print(tokens[temp[0]][ix])","a89fb143":"# Creating train, test and validation dataset files with header as 'image_id' and 'captions'\ntrain_dataset = open('flickr_8k_train_dataset.txt','wb')\ntrain_dataset.write(b\"image_id\\tcaptions\\n\")\n\nval_dataset = open('flickr_8k_val_dataset.txt','wb')\nval_dataset.write(b\"image_id\\tcaptions\\n\")\n\ntest_dataset = open('flickr_8k_test_dataset.txt','wb')\ntest_dataset.write(b\"image_id\\tcaptions\\n\")","36efdb28":"# Populating the above created files for train, test and validation dataset with image ids and captions for each of these images\nfor img in x_train:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        train_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        train_dataset.flush()\ntrain_dataset.close()\n\nfor img in x_test:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        test_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        test_dataset.flush()\ntest_dataset.close()\n\nfor img in x_val:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        val_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        val_dataset.flush()\nval_dataset.close()","81586f35":"# # Loading 50 layer Residual Network Model and getting the summary of the model\n# # from IPython.core.display import display, HTML\n# # display(HTML(\"\"\"<a href=\"http:\/\/ethereon.github.io\/netscope\/#\/gist\/db945b393d40bfa26006\">ResNet50 Architecture<\/a>\"\"\"))\n# model = VGG16(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n# # model.summary()\n# # Note: For more details on ResNet50 architecture you can click on hyperlink given below","aba0216b":"# Helper function to process images\ndef preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(299,299,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","01fce0c6":"#Read Img Features\ninfile = open(\"..\/input\/flickr8k-image-extraction\/img_extract.pkl\",'rb')\n# infile = open(\"..\/input\/image-captioning-extracting-features\/img_extract_inc_3d.pkl\",'rb')\nimg_fea = pickle.load(infile)\ninfile.close()","eb5a462d":"train_data = dict((k, img_fea[k]) for k in x_train[:-1])","9d3d47fa":"# train_data = {}\n# ctr=0\n# for ix in x_train:\n#     if ix == \"\":\n#         continue\n#     if ctr >= 3000:\n#         break\n#     ctr+=1\n#     if ctr%1000==0:\n#         print(ctr)\n#     path = images_path + ix\n#     img = preprocessing(path)\n#     pred = model.predict(img).reshape(2048)\n#     train_data[ix] = pred","7c0967e2":"train_data['2513260012_03d33305cf.jpg'].shape","ee55339d":"# # opening train_encoded_images.p file and dumping it's content\n# with open( \"train_encoded_images.p\", \"wb\" ) as pickle_f:\n#     pickle.dump(train_data, pickle_f )  ","b7a87c91":"# Loading image and its corresponding caption into a dataframe and then storing values from dataframe into 'ds'\npd_dataset = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t')\nds = pd_dataset.values\nprint(ds.shape)","ec8e8d75":"pd_dataset.head()","7634806d":"pd_dataset.captions = [item.lower() for item in pd_dataset.captions]\n","50407805":"pd_dataset.captions = pd_dataset.captions.apply(lambda x: x.replace('.', ''))","86b6b704":"# Storing all the captions from ds into a list\nsentences = []\nfor ix in range(ds.shape[0]):\n    sentences.append(ds[ix, 1])\n    \nprint(len(sentences))","0f24afd3":"# First 5 captions stored in sentences\nsentences[:5]","e3a1f084":"# Splitting each captions stored in 'sentences' and storing them in 'words' as list of list\n# words = [i.split() for i in sentences]","519b9d3e":"sentences_low = [item.lower() for item in sentences]","b85a3eb8":"from keras.preprocessing.text import Tokenizer\n\n#Tokenize top 5000 words in Train Captions\ntokenizer = Tokenizer(num_words=5000,\n                      oov_token=\"<unk>\",\n                      filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(sentences_low)\nword_2_indices = tokenizer.word_index\nindices_2_word = tokenizer.index_word","971f5b21":"pickle.dump(word_2_indices ,open(\"word_2_indices.pkl\",\"wb\"))\npickle.dump(indices_2_word ,open(\"indices_2_word.pkl\",\"wb\"))","e96bf539":"vocab_size = len(word_2_indices.keys())\nprint(vocab_size)","34e88161":"# # Creating a list of all unique words\n# unique = []\n# for i in words:\n#     unique.extend(i)\n# unique = list(set(unique))\n\n# print(len(unique))\n\n# vocab_size = len(unique)","d82b5e64":"# # Vectorization\n# word_2_indices = {val:index for index, val in enumerate(unique)}\n# indices_2_word = {index:val for index, val in enumerate(unique)}","5e3d04ac":"# word_2_indices['UNK'] = 0\n# word_2_indices['raining'] = 8253","108cebd1":"# indices_2_word[0] = 'UNK'\n# indices_2_word[8253] = 'raining'","7189fc12":"# print(word_2_indices['<start>'])\n# print(indices_2_word[4011])\n# print(word_2_indices['<end>'])\n# print(indices_2_word[8051])","474f231d":"max_len = 0\n\nfor i in sentences_low:\n    i = i.split()\n    if len(i) > max_len:\n        max_len = len(i)\n\nprint(max_len)","f65cb0e0":"padded_sequences, subsequent_words = [], []\n\nfor ix in range(ds.shape[0]):\n    partial_seqs = []\n    next_words = []\n    text = ds[ix, 1].split()\n    text = [word_2_indices[i] for i in text]\n    for i in range(1, len(text)):\n        partial_seqs.append(text[:i])\n        next_words.append(text[i])\n    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n\n    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n    \n    #Vectorization\n    for i,next_word in enumerate(next_words):\n        next_words_1hot[i, next_word] = 1\n        \n    padded_sequences.append(padded_partial_seqs)\n    subsequent_words.append(next_words_1hot)\n    \npadded_sequences = np.asarray(padded_sequences)\nsubsequent_words = np.asarray(subsequent_words)\n\nprint(padded_sequences.shape)\nprint(subsequent_words.shape)","d72b626c":"# print(padded_sequences[0])","15c27a65":"for ix in range(len(padded_sequences[0])):\n    for iy in range(max_len):\n        print(indices_2_word[padded_sequences[0][ix][iy]],)\n    print(\"\\n\")\n\nprint(len(padded_sequences[0]))","535b2146":"num_of_images = 2000","f621f3d5":"captions = np.zeros([0, max_len])\nnext_words = np.zeros([0, vocab_size])","48513ead":"for ix in range(num_of_images):#img_to_padded_seqs.shape[0]):\n    captions = np.concatenate([captions, padded_sequences[ix]])\n    next_words = np.concatenate([next_words, subsequent_words[ix]])\n\nnp.save(\"captions.npy\", captions)\nnp.save(\"next_words.npy\", next_words)\n\nprint(captions.shape)\nprint(next_words.shape)","31c0e5bf":"# with open('..\/input\/train_encoded_images.p', 'rb') as f:\n#     encoded_images = pickle.load(f, encoding=\"bytes\")","3aaabbcf":"# ds[1, 0].encode()","47617d8a":"imgs = []\n\nfor ix in range(ds.shape[0]):\n    if ds[ix, 0] in train_data.keys():\n#         print(ix, encoded_images[ds[ix, 0].encode()])\n        imgs.append(list(train_data[ds[ix, 0]]))\n\nimgs = np.asarray(imgs)\nprint(imgs.shape)","504e1a4a":"images = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        images.append(imgs[ix])\n        \nimages = np.asarray(images)\n\nnp.save(\"images.npy\", images)\n\nprint(images.shape)","a440c9fd":"image_names = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        image_names.append(ds[ix, 0])\n        \nimage_names = np.asarray(image_names)\n\nnp.save(\"image_names.npy\", image_names)\n\nprint(len(image_names))","528e48a9":"captions = np.load(\"captions.npy\")\nnext_words = np.load(\"next_words.npy\")\n\nprint(captions.shape)\nprint(next_words.shape)","5c370541":"images = np.load(\"images.npy\")\n\nprint(images.shape)","eacc7a4a":"imag = np.load(\"image_names.npy\")\n        \nprint(imag.shape)","92b23eab":"embedding_size = 128\nmax_len = 40","cff4a490":"image_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()","c71512fb":"language_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()","0c9ec911":"conca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"..\/input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","0d2d7119":"hist = model.fit([images, captions], next_words, batch_size=512, epochs=150)","36743b24":"model.save_weights(\"model_weights_inc_v3.h5\")","0018ca43":"def preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","2b610fbd":"def get_encoding(model, img):\n    image = preprocessing(img)\n    pred = model.predict(image).reshape(2048)\n    return pred","b36c089d":"# resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')","75033cf8":"img = \"..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Images\/1453366750_6e8cf601bf.jpg\"\n\n# test_img = get_encoding(resnet, img)\ntest_img = img_fea[\"1453366750_6e8cf601bf.jpg\"]","ea19ccd5":"def predict_captions(image):\n    start_word = [\"<start>\"]\n    while True:\n        par_caps = [word_2_indices[i] for i in start_word]\n        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n        preds = model.predict([np.array([image]), np.array(par_caps)])\n        word_pred = indices_2_word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_len:\n            break\n            \n    return ' '.join(start_word[1:-1])\n\nArgmax_Search = predict_captions(test_img)","25f1fa39":"z = Image(filename=img)\ndisplay(z)\n\nprint(Argmax_Search)","58b9c8b2":"###  **Model**","69e3f9e2":"### Predictions"}}