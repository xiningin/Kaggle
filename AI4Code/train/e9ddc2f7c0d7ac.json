{"cell_type":{"b0cd3d63":"code","ba819924":"code","820a8e06":"code","0afab0cc":"code","773d94c5":"code","ed4a7e04":"code","5f20c770":"code","2f486d7c":"code","756ecf30":"code","e4f2ae79":"code","10c3c514":"code","7537d49c":"code","c16f1b9d":"code","a4b6933e":"code","5d6a5eff":"code","181e95bc":"code","fcbd4b74":"code","37f20aca":"code","9dff87b7":"code","45fdb68f":"code","d17b7bf3":"code","f0989f82":"code","9c7e07ee":"code","6a960150":"code","94499082":"code","e5dba1e1":"code","cb9d5408":"code","4c17c48b":"code","7b40b22d":"code","0ac88cf6":"code","7a07b3f4":"code","a3b771f7":"code","69a87783":"code","ee43f432":"code","31cd2cb6":"code","6f867668":"code","941aa442":"code","e7e226d1":"code","bc970e5e":"code","77be3cb7":"markdown","20e3b4eb":"markdown","8ffcaded":"markdown","cbedfd6d":"markdown","ef9643fb":"markdown","cb27388d":"markdown","5d5364ed":"markdown","23eebba1":"markdown","972bd803":"markdown","65002f96":"markdown","6fc89f35":"markdown","46b2270c":"markdown","c848dcb2":"markdown"},"source":{"b0cd3d63":"# Importing of the librari\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style(style = 'whitegrid' )\nfrom sklearn.impute import SimpleImputer, KNNImputer\n# from impyute.imputation.cs import fast_knn\nfrom fancyimpute import IterativeImputer\nimport sys\nimport matplotlib.gridspec as gridspec\nimport missingno\n\nplt.style.use('seaborn-talk')","ba819924":"# Fetching the data sets and as it can be seen these are the available files in the directrort\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","820a8e06":"data = pd.read_csv(\"..\/input\/google-play-store-apps\/googleplaystore.csv\")\ndata.head()\n\n# As it can be seen below\n# 1. Application Name\n# 2. What is the Application Category\n# 3. Whats the rating out of 5\n# 4. How many Reviews are being provided\n# 5. How many number of installation has been done\n# 6. What kind of Subscription this app Holds\n# 7. If its paid then whats the price paid\n# 8. Whats the Content Rating Age Group\n# 9. What Generes this application belongs to\n# 10. When was the last update happened\n# 11. What's the last updated date of the Application\n# 12. What's the current Version Number of the Application\n# 13. What's the Android version till which the Application is supported","0afab0cc":"print(f\"Number of Data in the Dataset: {data.size}\")\nprint(f\"Dimension of the Dataset: {data.shape}\" )\nprint(f\"Columns of the Dataset are: {data.columns.to_list()}\")","773d94c5":"# To check some of the statistics about the data \ndata.info()","ed4a7e04":"# As it can be see above in Installs there are special charaacters\n# Like + and , and it should ideally be an Integer, hence the data type conversion\n# lets have a look into the data\n\nprint(\"To check the Value counts of different categories\")\ndata.Installs.value_counts()\n# As it can be seen below all the integers has the value with + so we can take an assumption \n# All the Installation numbers are more than whats being maintained ","5f20c770":"# To have better view on the data as it can be seen above that there is a value which which has \n# a type as Free\nprint(data.Type.value_counts())\nprint(\"=\" * 40)\ndata.head()","2f486d7c":"# As it can be seen above there is an app where the Installation is free, which is kindaa wierd \ndata[data[\"Installs\"] == 'Free']\n\n# As it can be see from above for this record we dont have the Category and thats the reason all the records are shifted by left by 1 record\n# Because as it can be seen above the type column can only hold a record with Free type\n# So with the name https:\/\/play.google.com\/store\/apps\/details?id=com.lifemade.internetPhotoframe&hl=en_IN as the per play store the app type is lifestyle \n# Hence the type will be assigned it","756ecf30":"data_index = data[data['App'] == 'Life Made WI-Fi Touchscreen Photo Frame' ].index.tolist()[0]\ntemp_df = data[data['App'] == 'Life Made WI-Fi Touchscreen Photo Frame' ].shift(periods = 1, axis = 1).copy()\n\ntemp_df.at[data_index, 'Category'] = 'LIFESTYLE'\ntemp_df.App = temp_df.App.apply(str)\ntemp_df.at[data_index, 'App'] = 'Life Made WI-Fi Touchscreen Photo Frame'\ntemp_df.Rating = temp_df.Rating.astype('float64')\n\ndata[data['App'] == 'Life Made WI-Fi Touchscreen Photo Frame' ] = temp_df\ndata[data['App'] == 'Life Made WI-Fi Touchscreen Photo Frame' ]\n\n# As it can be seen below now the data looks as expected ","e4f2ae79":"# Hence we can replace + and , in the install feature, as the + and , is a special characters\n# For the training only the numeric features is required\ndata.Installs = data.Installs.apply(lambda x : x.replace('+', '') if '+' in str(x) else x)\ndata.Installs = data.Installs.apply(lambda x : x.replace(',', '') if ',' in str(x) else x)\ndata.Installs = data.Installs.astype('int64')","10c3c514":"print(\"Final Look after the data cleaning\")\nprint(data.Installs.value_counts())","7537d49c":"# Lets analyze the Size \n# here also as it can be seen below the values looks like an Integer \nprint(data.Size.head())\n\n# So lets try to adjust the data \n# 1. Remove the Varies with device with NaN\ndata.Size = data[\"Size\"].apply(lambda x: float(x.replace('Varies with device', 'NaN')) if 'Varies with device' in str(x) else x)\ndata.Size = data[\"Size\"].apply(lambda x: ( float((x.replace('M', '')).replace(',', ''))* 10**6)  if 'M' in str(x) else x)\ndata.Size = data[\"Size\"].apply(lambda x: float(x.replace('k', '')) * 10**3  if 'k' in str(x) else x)\n\ndata.Size = data.Size.apply(np.float64)\n\nprint(\"=\"*20)\nprint(\"After applying the transformation on Size column\")\nprint(data.Size.head())\n\n# Lets also change the data type of the Reviews feature\n# As that also looks like an integer\ndata.Reviews = data.Reviews.astype(np.int64)\n\n# Lets have a look into the price \n# It has a special character $ currency, so all the apps prices are in dollar\n# Assumption 2: The price is in dollar for all the apps\n\ndata.Price = data.Price.apply(lambda x: float(x.replace('$', '')) if '$' in str(x) else float(x))\n\n# And also lets changes the data type of Last Updated to date time\n\ndata['Last Updated'] = pd.to_datetime(data['Last Updated'])","c16f1b9d":"# After apply the transformation, lets check the final Data Types\ndata.info()","a4b6933e":"def print_the_number_missing_data(data):\n    # Lets check the number of duplicated rows\n    print(\"=\"*30)\n    print(f'Number of Duplicate Rows {data[data.duplicated()].shape[0]}')\n\n    #Let's check the percentage of missing values for the application dataset.\n    total_missing = data.isnull().sum()\n    percent = round((100*(total_missing\/data.isnull().count())),2)\n\n    #Making a table for both and get the top 20 Columns\n\n    missing_value_app_data = pd.concat([total_missing, percent], axis =1, keys= ['Total_missing', 'percent'])\n    missing_value_app_data.sort_values(by='Total_missing',ascending=False,inplace=True)\n    print(\"=\"*30)\n    print(\"Number and % of Missing Value\")\n    print(missing_value_app_data)\n\nprint_the_number_missing_data(data)","5d6a5eff":"# So we would drop all the duplicated rows, as duplicated rows is of lowest importance\ndata.drop_duplicates(inplace = True)\nprint(\"After droping the duplicate Record\")\nprint_the_number_missing_data(data)\nmissingno.matrix(data)\nplt.show()","181e95bc":"# Lets see how the categorical data will impact the \nprint(data.Category.value_counts())\nplt.figure(figsize=(15,10))\nsns.categorical.countplot(data.Category, order = data.Category.value_counts().index )\nplt.xticks(rotation = 60)\nmean = np.mean(data.Category.value_counts().values.tolist())\nplt.axhline(mean, color='r', linestyle='--', ls='--',label='p=0.05')\nplt.legend({'Mean':mean})\nplt.title('APPs Category Frequency Distribution')\nplt.show()","fcbd4b74":"# Selecting the top 5 category from the above o\/p\nall_cat_data= data[['Category', 'Installs']].groupby('Category')['Installs'].agg('sum').reset_index(name='Number_of_Installation')\n\ntop5_cat = data.Category.value_counts().index.tolist()[:5]\ntop5_cat_data = all_cat_data[all_cat_data.Category.isin(top5_cat)]\ntop5_cat_data.sort_values(by = ['Number_of_Installation'], ascending = False, inplace = True)\nsns.barplot(y = top5_cat_data['Category'], x = top5_cat_data['Number_of_Installation'], ci = None)\nplt.title('Comparing Top 5 App Category with number of installs')\nplt.show()\n\nprint(\"=\"*20)\n\nall_cat_data.sort_values(by = ['Number_of_Installation'], ascending = False, inplace = True)\nsns.barplot(y = all_cat_data['Category'], x = all_cat_data['Number_of_Installation'], ci = None)\nplt.title('Comparing All App Category with number of installs')\nplt.show()","37f20aca":"# To have a better insight lets try to predict the correlation\nplt.title('Lets see the correlation')\nsns.heatmap(data.corr(), annot=True)\nplt.show()","9dff87b7":"most_reviews = data.groupby('App')[['Reviews']].mean().sort_values('Reviews', ascending=False).head(10).reset_index()\nsns.barplot(y = most_reviews['App'], x = most_reviews['Reviews'], ci = None)\nplt.title('App vs Reviews')\nplt.show()","45fdb68f":"install_type = data.groupby('Type')['Installs'].agg('sum').reset_index(name='Number_of_Installations')\nprint(install_type)\n\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.title('Type of Apps vs Installation')\nsns.barplot(x=install_type['Type'],y=install_type['Number_of_Installations'])\n\nplt.subplot(1,2,2)\nplt.title('Type of Apps vs Installation in Log Scale')\nsns.barplot(x=install_type['Type'],y=install_type['Number_of_Installations'])\nplt.yscale('log')\nplt.show()","d17b7bf3":"paidAppsData = data[data.Type == 'Paid']\npaid_cat_data= paidAppsData[['Category', 'Installs']].groupby('Category')['Installs'].agg('sum').reset_index(name='Number_of_Installation')\npaid_cat_data.sort_values(by = ['Number_of_Installation'], ascending = False, inplace = True)\nsns.barplot(y = paid_cat_data['Category'], x = paid_cat_data['Number_of_Installation'], ci = None)\nplt.title('Comparing All App Category with number of installs for Paid Apps')\nplt.show()","f0989f82":"app_Count_per_android_vrs = data.groupby(\"Android Ver\")['App'].count().reset_index(name = 'App_Count')\napp_Count_per_android_vrs.sort_values(by = ['App_Count'], ascending = False, inplace = True)\nplt.title('Android Version vs Number of Apps')\nsns.barplot(x=app_Count_per_android_vrs['App_Count'],y=app_Count_per_android_vrs['Android Ver'])\nplt.show()","9c7e07ee":"plt.figure(figsize=(10,8))\nsns.barplot(x=data['Content Rating'], y=data['Rating'], palette=\"deep\", ci = False)\nplt.axhline(0, color=\"k\", clip_on=False)\nplt.xlabel('Content Ratings')\nplt.xticks(rotation=90)\nplt.ylabel(\"Ratings \")\nplt.suptitle('Ratings vs Content Ratings', fontsize=20)\n# Adults only 18+ category apps have maximum ratings","6a960150":"sns.barplot(data['Content Rating'].value_counts().index, data['Content Rating'].value_counts())\nplt.xticks(rotation=75);\nplt.title('Age group the app is targeted at')\nplt.show()\n# As it can be seen below most of the Apps are targetted for everyone","94499082":"# Lets check the distribution of rating\nplt.figure(figsize=(10, 5))\nplt.title('User rating Distribution')\nplt.xlabel('Ratings')\nplt.ylabel('Number of users')\n\nplt.hist(data.Rating, bins=np.arange(1,6,0.5))\nplt.show()\n\n# As it can be seen below most of the apps rating are between 3 - 4.5","e5dba1e1":"plt.figure(figsize=(50,15))\nsns.catplot(x='Category',y='Rating',data=data,kind='box',height=10,showmeans=True)\nplt.title(\"Rating of each category\", size=18)\nplt.xticks(rotation=90)\nplt.show()","cb9d5408":"data['Last_Updated_Year'] = data['Last Updated'].dt.year\nmost_downloads_by_year = data.groupby(['Last_Updated_Year','Category'])[['Installs']].sum().sort_values('Installs', ascending=False).reset_index()\nmost_downloads_by_year = most_downloads_by_year.loc[most_downloads_by_year.groupby(\"Last_Updated_Year\")[\"Installs\"].idxmax()]\nmost_downloads_by_year","4c17c48b":"data['Diff_Bet_Second_Last_Update'] =  data['Last Updated'].max() - data['Last Updated']\nprint(data[['Category','Diff_Bet_Second_Last_Update']].sort_values('Diff_Bet_Second_Last_Update', ascending=False).describe())\nprint(\"=\"*20, end = '\\n\\n\\n')\ndata[['App','Category','Diff_Bet_Second_Last_Update']].sort_values('Diff_Bet_Second_Last_Update', ascending=False).head(10)\n# As it can be seen that below are the top 10 Apps and their category and for how long its not been updated\n# And it can be seen most of the apps are frequenty updated, only the top 25% app have not been updated for too long","7b40b22d":"thereshold_val = np.percentile(data['Diff_Bet_Second_Last_Update'].dt.days.values.tolist(), 75)\napps_not_updated_fr_most_Days =  data[data['Diff_Bet_Second_Last_Update'].dt.days > thereshold_val]['Category'].value_counts()\nsns.barplot(y = apps_not_updated_fr_most_Days.index, x = apps_not_updated_fr_most_Days.values, ci = None)\nplt.title('Apps which are not updated for Most number of Days')\nplt.xlabel('Days')\nplt.show()\n\n# As it can be seen below the Family, followed by Game and Tools are the one which is not updated for most number of days","0ac88cf6":"data.describe().T\n# As it can be seen the max value is 19 and the min is 1, so lets try to see the range ","7a07b3f4":"#  This will plot the numeric features that will be boxplot and distribution plot\ndef plot_numeric_features(dataset, column):\n    plt.figure(figsize=(10, 4))\n    plt.suptitle(\"Distribution of \" + column)\n    plt.subplot(1,2,1)\n    sns.boxplot(y = dataset[column])\n    plt.xlabel(column)\n\n    plt.subplot(1,2,2)\n    sns.distplot(data[column])\n    plt.axvline(np.median(data[column]),color='b', linestyle='--')\n    plt.show()\n\ndef plot_numeric_features_with_statistic(df, column):\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)}, figsize=(10,8))\n    mean=df[column].mean()\n    median=df[column].median()\n    mode=df[column].mode().values.tolist()[0]\n    sns.boxplot(df[column], ax=ax_box)\n    ax_box.axvline(mean, color='r', linestyle='--')\n    ax_box.axvline(median, color='g', linestyle='-')\n    ax_box.axvline(mode, color='b', linestyle='-')\n\n    sns.distplot(df[column], ax=ax_hist)\n    ax_hist.axvline(mean, color='r', linestyle='--')\n    ax_hist.axvline(median, color='g', linestyle='-')\n    ax_hist.axvline(mode, color='b', linestyle='-')\n    \n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n\n    ax_box.set(xlabel='')\n    \n# Using the imputation as a strategy\ndef imputation_using_strategy(data, column, strategy):\n    simp_imp = SimpleImputer(strategy= strategy)\n    data_in_consider = data[column].values.reshape(-1,1) \n    simp_imp.fit(data_in_consider)\n    return simp_imp, simp_imp.transform(data_in_consider).reshape(-1)\n\n# This will do the Imputation using the KNN Approach\n# The algorithm uses \u2018feature similarity\u2019 to predict the values of any new data points. \n# This means that the new point is assigned a value based on how closely it resembles \n# the points in the training set.\ndef imputation_using_knn(data, column):\n#     sys.setrecursionlimit(100000) \n#     return fast_knn(data[column].values.reshape(-1,1) , k=30).reshape(-1)\n    imputer = KNNImputer(n_neighbors=30, weights='uniform', metric='nan_euclidean')\n    data_in_consider = data[column].values.reshape(-1,1) \n    imputer.fit(data_in_consider)\n    return imputer, imputer.transform(data_in_consider).reshape(-1)\n\n# Imputation with Multivariate Imputation by Chained Equation (MICE)\n# This type of imputation works by filling the missing data multiple times.\ndef imputation_using_mice(data, column):\n    data_in_consider = data[column].values.reshape(-1,1)\n    mice_imputer = IterativeImputer()\n    mice_imputer.fit(data_in_consider)\n    return mice_imputer, mice_imputer.transform(data_in_consider).reshape(-1)\n\n# Gets the imputed columns and then print the corresponding values as a single image as a subplots\ndef plot_multiple_values(imputed_column, original_col_label, images_per_row = 2):\n    column_list = imputed_column.columns.tolist()\n    images_per_row = min(len(column_list), images_per_row)\n    n_rows = (len(column_list) - 1) \/\/ images_per_row + 1\n    axes = []\n    fig = plt.figure(figsize=(20,12))\n    outer = gridspec.GridSpec(n_rows, images_per_row, wspace =  0.2, hspace=0.1)\n    for i in range(len(column_list)):\n        inner = gridspec.GridSpecFromSubplotSpec(2, 1,subplot_spec=outer[i], wspace=0.1, hspace=0.1, height_ratios = (0.2, 1))\n        ax_box = plt.Subplot(fig, inner[0])\n        column = column_list[i]\n        mean=imputed_column[column].mean()\n        median=imputed_column[column].median()\n        mode=imputed_column[column].mode().values.tolist()[0]\n        sns.boxplot(imputed_column[column], ax=ax_box)\n        ax_box.axvline(mean, color='r', linestyle='--')\n        ax_box.axvline(median, color='g', linestyle='-')\n        ax_box.axvline(mode, color='b', linestyle='-')\n        ax_box.set(xlabel='')\n        fig.add_subplot(ax_box)\n        \n        ax_hist = plt.Subplot(fig, inner[1], sharex = ax_box)\n        sns.distplot(imputed_column[column], ax=ax_hist)\n        ax_hist.axvline(mean, color='r', linestyle='--', label = 'Mean')\n        ax_hist.axvline(median, color='g', linestyle='-', label = 'Median')\n        ax_hist.axvline(mode, color='b', linestyle='-', label = 'Mode')\n        ax_hist.legend()\n        fig.add_subplot(ax_hist)\n    plt.suptitle(f\"To see the Data Distribution of {original_col_label} after apply different Imputation Technique\")\n    for ax in fig.get_axes():\n        ax.label_outer()\n    plt.show()","a3b771f7":"# lets start with the Feature Size\nplot_numeric_features(data, \"Size\")\n# As it can be see most of the apps have a size between 0-0.6 MB","69a87783":"result_df = pd.DataFrame()\nestimator_list = [] # This holds the estimator list that will be used later for the imputation in the test set\n# Imputation using median\nestimator, imputed_data = imputation_using_strategy(data, \"Size\", \"median\")\nestimator_list.append(estimator)\nresult_df['imp_mean'] = pd.Series(imputed_data)\nprint('Imputation with Mean')\n\n\n#Imputation using Most Frequent\nestimator, imputed_data = imputation_using_strategy(data, \"Size\", \"most_frequent\")\nestimator_list.append(estimator)\nresult_df['imp_most_freq'] = pd.Series(imputed_data)\nprint('Imputation with Most Frequent')\n\n# Imputation using the KNN Strategy\nestimator, imputed_data = imputation_using_knn(data, \"Size\")\nestimator_list.append(estimator)\nresult_df['knn'] = pd.Series(imputed_data)\nprint('Imputation with KNN')\n\n# Imputation using the MICE Strategy\nestimator, imputed_data = imputation_using_mice(data, \"Size\")\nestimator_list.append(estimator)\nresult_df['mice'] = pd.Series(imputed_data)\nprint('Imputation with MICE')","ee43f432":"plot_multiple_values(result_df, \"Size\")","31cd2cb6":"final_estimator_list = dict()\nplot_numeric_features_with_statistic(data, \"Size\")\nplt.title(\"Data without Imputation\")\nplt.show()\ndata.Size = result_df['imp_most_freq']\nplot_numeric_features_with_statistic(data, \"Size\")\nplt.title(\"Data After Imputation\")\nfinal_estimator_list['na_Size'] = estimator_list[1]\nplt.show()","6f867668":"print_the_number_missing_data(data)","941aa442":"plot_numeric_features(data, \"Rating\")","e7e226d1":"print(data[data[\"Rating\"] > 5].T)\nprint(\"=\"*40)\nprint(\"Index where the Anomaly rating is present\" , data[data[\"Rating\"] > 5].index.tolist())\nprint(\"=\"*40)\nprint(\"Data will be droppped now\")\ndata.drop(index =data[data[\"Rating\"] > 5].index.tolist(), inplace=True)","bc970e5e":"plot_numeric_features(data, \"Rating\")\n# It can be seen now that the \nplt.figure(figsize=(12, 4))\ng = sns.factorplot(\"Rating\", data=data, aspect=1.5, kind=\"count\")\ng.set_xticklabels(rotation=80)\nplt.show()\n\n# As it can be seen below most of the Rating is highly ","77be3cb7":"As it can be seen above most of the features are categorical except the rating, but thats not correct as per the data snapshot which can be seen above so we need to look deep into the dataset","20e3b4eb":"**Conclusion Driven**\n- There are least amount of correlation except between Installation and Reviews, but correlation is not equivalent to causation\n- With more installation there can be more number of reviews ","8ffcaded":"**Conclusion Driven**\n\n- Now its clearly seen that the Family and Game are the Category that should be targetted for as the best category\n- And it clearly rules the market, as it can be seen wrt to Average line Tools Game and Family related APPs are the dominating Category\n- So if we consider the top 5 Categories which rules the market, the kind of apps will be identified\n    - Family \n    - Game\n    - Tools\n    - Business\n    - Medical\n- So now from a market segment analysis POV, it becomes easier for a APP Developer to target the appropriate APP category to target more customer","cbedfd6d":"**Conclusion**\n- There are about 483 number of duplicates out of total of 10841, so around ~4.5% so we can go ahead and drop the duplicates","ef9643fb":"**Conclusion Driven**\n- As it can also be seen above more number of app installation happened for the Free Kind of apps and very less number of apps are actually paid","cb27388d":"**Conclusion Driven**\n- This basically give a Fair idea about what should be the min Android Version Support based on the number of apps in the market\n- As it can be seen above most of the apps has a minimum support of Android Version 4","5d5364ed":"## Importing Libraries","23eebba1":"**Conclusion Driven**\n- As it can seen from above If there are more number of apps of a particular category, it doesn't necessarily lead to more number of installs.\n    - For ex. out of the 5 top categrories of apps, only Game is the one which has more number of installation","972bd803":"## Data Feteching","65002f96":"**Conclusion Driven**\n- So now it can be seen from above that if the apps needs to be a PAID Kind of Apps then what category it should be targetted for ","6fc89f35":"### Data Cleaning Steps","46b2270c":"**Conclusion**\n- Above it can be seen after using the different imputation techniques, how the data distribution looks like\n- When Imputation using Mean and Most Frequent is being used both the imputation resulted in the similar kind of distribution, which is like centered around mean\n- When MICE Imputation technique was used, it can be seen most of the data was imputed around mean of the data before imputation 0.2 and thats the reason the mode and median shifted to the new location\n- And when the imputation strategy KNN was used, it can be seen it resulted in the same result like for MICE\n\nHence for now I will take a decision to proceed with the Most Frequent Simplt Imputation Strategy \n - As the resulted data dataset is having an nearly identical distribution as the original data\n - And also the imputed data is not directly imputed to the mean and also median and mode is shifted by a little","c848dcb2":"**Conclusion**\n- As it can be seen above after droping the duplicate values now there are no duplicated left\n- But now from the number of null\/NaN values \n"}}