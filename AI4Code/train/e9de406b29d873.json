{"cell_type":{"de8c5aa4":"code","a69223de":"code","55c86bc8":"code","572e12ee":"code","bff2e085":"code","23285c33":"code","94910624":"code","11172905":"code","fd9f2186":"code","58671bbe":"code","fbf72821":"code","3d48b5fe":"code","f70eecfc":"code","6c0ab9ae":"code","9e7e201c":"code","0ff314f1":"code","27c29d3e":"code","27c49ed0":"code","d8eac559":"code","5befc420":"code","57067dc0":"code","17805849":"code","66dc87d4":"code","956d02c5":"code","14f1d559":"code","bca14105":"code","493fbb8a":"code","e35f5044":"code","0747a589":"code","02a7afc3":"code","995d984d":"code","35c7012a":"code","3b1fb674":"code","14f8ded3":"markdown","6d5d7288":"markdown","18fe1e0c":"markdown","85f6dfbc":"markdown","5eac935b":"markdown","a844f8ed":"markdown","0456d8fe":"markdown","a95eaf87":"markdown","c7f5362e":"markdown","dd6a5c3e":"markdown","60e53e6f":"markdown","86836d36":"markdown","9aa2f363":"markdown","15ed7a65":"markdown","277205b0":"markdown","d6b8f986":"markdown","cbdaffb9":"markdown","ec5c73b8":"markdown","c65ccf3d":"markdown","28903e4b":"markdown","b7cb98a1":"markdown","51b9f7ea":"markdown","77d78005":"markdown","d258d46a":"markdown","9f76596f":"markdown","62f2171e":"markdown","67761ec5":"markdown","adf8b95c":"markdown","d4504a0c":"markdown"},"source":{"de8c5aa4":"# Python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport itertools\nfrom itertools import chain\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\nwarnings.filterwarnings('ignore') #ignore warning messages ","a69223de":"# Read data\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","55c86bc8":"null_feat = pd.DataFrame(len(data['id']) - data.isnull().sum(), columns = ['Count'])\n\ntrace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'lightgrey',\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  \"Missing Values\")\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","572e12ee":"# Drop useless variables\ndata = data.drop(['Unnamed: 32','id'],axis = 1)\n\n# Reassign target\ndata.diagnosis.replace(to_replace = dict(M = 1, B = 0), inplace = True)","bff2e085":"# Head\ndata.head()","23285c33":"# describe\ndata.describe()","94910624":"# 2 datasets\nM = data[(data['diagnosis'] != 0)]\nB = data[(data['diagnosis'] == 0)]","11172905":"#------------COUNT-----------------------\ntrace = go.Bar(x = (len(M), len(B)), y = ['malignant', 'benign'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=[ 'gold', 'lightskyblue'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Count of diagnosis variable')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)\n\n#------------PERCENTAGE-------------------\ntrace = go.Pie(labels = ['benign','malignant'], values = data['diagnosis'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightskyblue', 'gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of diagnosis variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","fd9f2186":"def plot_distribution(data_select, size_bin) :  \n    tmp1 = M[data_select]\n    tmp2 = B[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['malignant', 'benign']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig, filename = 'Density plot')","58671bbe":"#plot distribution 'mean'\nplot_distribution('radius_mean', .5)\nplot_distribution('texture_mean', .5)\nplot_distribution('perimeter_mean', 5)\nplot_distribution('area_mean', 10)\n#plot_distribution('smoothness_mean', .5)\n#plot_distribution('compactness_mean' .5)\n#plot_distribution('concavity_mean' .5)\n#plot_distribution('concave points_mean' .5)\n#plot_distribution('symmetry_mean' .5)\n#plot_distribution('fractal_dimension_mean' .5)","fbf72821":"#plot distribution 'se'\nplot_distribution('radius_se', .1)\nplot_distribution('texture_se', .1)\nplot_distribution('perimeter_se', .5)\nplot_distribution('area_se', 5)\n#plot_distribution('smoothness_se', .5)\n#plot_distribution('compactness_se', .5)\n#plot_distribution('concavity_se', .5)\n#plot_distribution('concave points_se', .5)\n#plot_distribution('symmetry_se', .5)\n#plot_distribution('fractal_dimension_se', .5)","3d48b5fe":"#plot distribution 'worst'\nplot_distribution('radius_worst', .5)\nplot_distribution('texture_worst', .5)\nplot_distribution('perimeter_worst', 5)\nplot_distribution('area_worst', 10)\n#plot_distribution('smoothness_worst', .5)\n#plot_distribution('compactness_worst', .5)\n#plot_distribution('concavity_worst', .5)\n#plot_distribution('concave points_worst', .5)\n#plot_distribution('symmetry_worst', .5)\n#plot_distribution('fractal_dimension_worst', .5)","f70eecfc":"#correlation\ncorrelation = data.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)","6c0ab9ae":"#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   xgap = 2,\n                   ygap = 2,\n                   colorscale='Viridis',\n                   colorbar   = dict() ,\n                  )\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9)),\n                       )\n                  )\nfig = go.Figure(data = [trace],layout = layout)\npy.iplot(fig)","9e7e201c":"def plot_feat1_feat2(feat1, feat2) :  \n    trace0 = go.Scatter(\n        x = M[feat1],\n        y = M[feat2],\n        name = 'malignant',\n        mode = 'markers', \n        marker = dict(color = '#FFD700',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = B[feat1],\n        y = B[feat2],\n        name = 'benign',\n        mode = 'markers',\n        marker = dict(color = '#7EC0EE',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = feat1 +\" \"+\"vs\"+\" \"+ feat2,\n                  yaxis = dict(title = feat2,zeroline = False),\n                  xaxis = dict(title = feat1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)","0ff314f1":"plot_feat1_feat2('perimeter_mean','radius_worst')\nplot_feat1_feat2('area_mean','radius_worst')\nplot_feat1_feat2('texture_mean','texture_worst')\nplot_feat1_feat2('area_worst','radius_worst')","27c29d3e":"#seaborn version : \n\npalette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'grey'\n\n# Plot +\nfig = plt.figure(figsize=(12,12))\n\nplt.subplot(221)\nax1 = sns.scatterplot(x = data['perimeter_mean'], y = data['radius_worst'], hue = \"diagnosis\",\n                    data = data, palette = palette, edgecolor=edgecolor)\nplt.title('perimeter mean vs radius worst')\nplt.subplot(222)\nax2 = sns.scatterplot(x = data['area_mean'], y = data['radius_worst'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('area mean vs radius worst')\nplt.subplot(223)\nax3 = sns.scatterplot(x = data['texture_mean'], y = data['texture_worst'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('texture mean vs texture worst')\nplt.subplot(224)\nax4 = sns.scatterplot(x = data['area_worst'], y = data['radius_worst'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('area mean vs radius worst')\n\nfig.suptitle('Positive correlated features', fontsize = 20)\nplt.savefig('1')\nplt.show()","27c49ed0":"plot_feat1_feat2('smoothness_mean','texture_mean')\nplot_feat1_feat2('radius_mean','fractal_dimension_worst')\nplot_feat1_feat2('texture_mean','symmetry_mean')\nplot_feat1_feat2('texture_mean','symmetry_se')","d8eac559":"# seaborn version : \nfig = plt.figure(figsize=(12,12))\n\nplt.subplot(221)\nax1 = sns.scatterplot(x = data['smoothness_mean'], y = data['texture_mean'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('smoothness mean vs texture mean')\nplt.subplot(222)\nax2 = sns.scatterplot(x = data['radius_mean'], y = data['fractal_dimension_worst'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('radius mean vs fractal dimension_worst')\nplt.subplot(223)\nax3 = sns.scatterplot(x = data['texture_mean'], y = data['symmetry_mean'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('texture mean vs symmetry mean')\nplt.subplot(224)\nax4 = sns.scatterplot(x = data['texture_mean'], y = data['symmetry_se'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('texture mean vs symmetry se')\n\nfig.suptitle('Uncorrelated features', fontsize = 20)\nplt.savefig('2')\nplt.show()","5befc420":"plot_feat1_feat2('area_mean','fractal_dimension_mean')\nplot_feat1_feat2('radius_mean','fractal_dimension_mean')\nplot_feat1_feat2('area_mean','smoothness_se')\nplot_feat1_feat2('smoothness_se','perimeter_mean')","57067dc0":"# seaborn version\nfig = plt.figure(figsize=(12,12))\n\nplt.subplot(221)\nax1 = sns.scatterplot(x = data['area_mean'], y = data['fractal_dimension_mean'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('smoothness mean vs fractal dimension mean')\nplt.subplot(222)\nax2 = sns.scatterplot(x = data['radius_mean'], y = data['fractal_dimension_mean'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('radius mean vs fractal dimension mean')\nplt.subplot(223)\nax2 = sns.scatterplot(x = data['area_mean'], y = data['smoothness_se'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('area mean vs fractal smoothness se')\nplt.subplot(224)\nax2 = sns.scatterplot(x = data['smoothness_se'], y = data['perimeter_mean'], hue = \"diagnosis\",\n                    data = data, palette =palette, edgecolor=edgecolor)\nplt.title('smoothness se vs perimeter mean')\n\nfig.suptitle('Negative correlated features', fontsize = 20)\nplt.savefig('3')\nplt.show()","17805849":"target_pca = data['diagnosis']\ndata_pca = data.drop('diagnosis', axis=1)\n\ntarget_pca = pd.DataFrame(target_pca)\n\n#To make a PCA, normalize data is essential\nX_pca = data_pca.values\nX_std = StandardScaler().fit_transform(X_pca)\n\npca = PCA(svd_solver='full')\npca_std = pca.fit(X_std, target_pca).transform(X_std)\n\npca_std = pd.DataFrame(pca_std)\npca_std = pca_std.merge(target_pca, left_index = True, right_index = True, how = 'left')\npca_std['diagnosis'] = pca_std['diagnosis'].replace({1:'malignant',0:'benign'})","66dc87d4":"#explained_variance \nvar_pca = pd.DataFrame(pca.explained_variance_ratio_)\nvar_pca = var_pca.T\n\n#----------SUM AND DROP COMP [7:30]\ncol_list = list(v for v in chain(pca_std.columns[6:30])) \nvar_pca['OTHERS_COMP'] = var_pca[col_list].sum(axis=1)\nvar_pca.drop(var_pca[col_list],axis=1,inplace=True)\nvar_pca = var_pca.T","956d02c5":"labels = ['COMP1','COMP2','COMP3','COMP4','COMP5','COMP6', 'COMP7 - 30']\ncolors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue', 'lightgrey', 'orange', 'white']\n\ntrace = go.Pie(labels = labels, values = var_pca[0].values, opacity = 0.8,\n               textfont=dict(size=15),\n               marker=dict(colors=colors, \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'PCA : components and explained variance (6 comp = 88.8%)')\n \n                   \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","14f1d559":"pca = PCA(n_components = 2)\n\npca_std = pca.fit(X_std, target_pca).transform(X_std)\npca_std = pd.DataFrame(pca_std,columns = ['COMP1','COMP2'])\npca_std = pca_std.merge(target_pca,left_index = True,right_index = True,how = 'left')\npca_std['diagnosis'] = pca_std['diagnosis'].replace({1:'malignant',0:'benign'})","bca14105":"def pca_scatter(target,color) :\n    tracer = go.Scatter(x = pca_std[pca_std['diagnosis'] == target]['COMP1'] ,\n                        y = pca_std[pca_std['diagnosis'] == target]['COMP2'],\n                        name = target, mode = 'markers',\n                        marker = dict(color = color,line = dict(width = 1))\n                       )\n    return tracer\nlayout = go.Layout(dict(title = 'PCA Scatter plot (2 comp = 63.3%)',\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = 'COMP1 = 44.3%',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = 'COMP2 = 19.0%',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        height = 800\n                       ))\ntrace1 = pca_scatter('malignant','#FFD700')\ntrace2 = pca_scatter('benign','#7EC0EE')\nplots = [trace2,trace1]\nfig = go.Figure(data = plots,layout = layout)\npy.iplot(fig)","493fbb8a":"pca = PCA(n_components = 3)\npca_std = pca.fit(X_std, target_pca).transform(X_std)\n\npca_std = pd.DataFrame(pca_std,columns = ['COMP1','COMP2','COMP3'])\npca_std = pca_std.merge(target_pca, left_index = True, right_index = True,how = 'left')\npca_std['diagnosis'] = pca_std['diagnosis'].replace({1:'malignant',0:'benign'})","e35f5044":"M_pca = pca_std[(pca_std['diagnosis'] == 'malignant')]\nB_pca = pca_std[(pca_std['diagnosis'] == 'benign')]","0747a589":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)\/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp\/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/\n                                                 ((tp\/(tp+fp))+(tp\/(tp+fn))))))","02a7afc3":"# Precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","995d984d":"# ROC curve\ndef plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n   # plt.xlim([0.0,0.001])\n   # plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","35c7012a":"# Learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n                        n_jobs = 1, train_sizes = np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training examples')\n    plt.ylabel('Score')\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv = cv, n_jobs = n_jobs, train_sizes = train_sizes)\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis = 1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color = \"r\",\n             label = \"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color = \"g\",\n             label = \"Cross-validation score\")\n    plt.legend(loc = \"best\")\n    return plt","3b1fb674":"# Cross val metric\ndef cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","14f8ded3":"**Load libraries and read the data**","6d5d7288":"## PCA scatter plot with 3 components (72.7%)","18fe1e0c":"The precision-recall curve shows the tradeoff between precision and recall for different threshold","85f6dfbc":"** Features distribution (hue = diagnosis)**","5eac935b":"**Head and describe**","a844f8ed":"The Learning curve determines cross-validated training and test scores.","0456d8fe":"Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. ","a95eaf87":"** Exploratory Data Analysis (EDA)**","c7f5362e":"##  Positive correlated features","dd6a5c3e":"Bellow, you can remove the '#' to show all features distribution (except the first line)","60e53e6f":"**Read the data**","86836d36":"## Correlation matrix","9aa2f363":"** Target distribution (number and %)**","15ed7a65":"##  PCA scatter plot with 2 components (63.3%)","277205b0":"##  Precision \u2013 Recall curve","d6b8f986":"## Negative correlated features","cbdaffb9":"The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.","ec5c73b8":"##  ROC curve","c65ccf3d":"** Reassign target and drop useless features**","28903e4b":"** Load libraries** ","b7cb98a1":"## PCA pie plot with 6 components (88.8%)","51b9f7ea":"Let's check the correlation between few features by pair","77d78005":"# Principal Component Analysis","d258d46a":"All features are complete, only 'Unnamed: 32' is completely null, probably an error in the dataset, we drop it in below","9f76596f":"**Missing values**","62f2171e":"## Compute PCA","67761ec5":" ## Cross validation metrics","adf8b95c":"## Uncorrelated features","d4504a0c":"## Learning curve"}}