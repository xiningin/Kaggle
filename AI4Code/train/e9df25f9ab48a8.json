{"cell_type":{"4f4d772d":"code","0c286288":"code","1172dda4":"code","fbc16094":"code","fcd4477d":"code","a4c5be6c":"code","6132d92f":"code","9c511afb":"code","628a444c":"code","be8db6ba":"code","d5ee1e27":"code","e911ee73":"code","45eaec8b":"code","b44cc0c5":"code","b39c89bb":"code","61e742ca":"code","135b4260":"code","9aecd6af":"code","08c9ab24":"code","0d0d7e22":"code","8ee4fca5":"code","bc113085":"code","f592dab5":"code","691dbe4a":"code","09b06477":"code","5aca483a":"code","556e053f":"code","9b99ee34":"code","6c70c326":"markdown","dae176e0":"markdown","f0a1783e":"markdown","2af61ab4":"markdown","e1548816":"markdown"},"source":{"4f4d772d":"import numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn; sn.set(font_scale=1)\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt           \nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten,Dense,Dropout,BatchNormalization, Input\nfrom tensorflow.keras.models import Sequential, Model\nfrom tqdm import tqdm\nfrom IPython.display import FileLink","0c286288":"path = '\/kaggle\/input\/yoga-pose-image-classification-dataset\/dataset\/'","1172dda4":"print('Total No of yoga poses in the dataset is:', len(os.listdir(path)))","fbc16094":"posses_in_var = os.listdir(path)","fcd4477d":"os.listdir(path)[:10]","a4c5be6c":"classes = os.listdir(path)\nclass_names_label = {class_name:i for i, class_name in enumerate(classes)}","6132d92f":"labels = []\nfor folder in os.listdir(path):\n    label = class_names_label[folder]\n    for file in tqdm(os.listdir(os.path.join(path, folder))):\n        labels.append(label)\n# labels","9c511afb":"no_of_images = []\nfor i in range(107):\n    comp_path = path + posses_in_var[i]\n    no_of_images.append(len(os.listdir(comp_path)))\nno_of_images","628a444c":"print('Total No of images:', sum(no_of_images))","be8db6ba":"plt.figure(figsize = (16,6))\nplt.xticks(rotation = 90)\nplt.bar(posses_in_var, no_of_images)","d5ee1e27":"label1 = path + os.listdir(path)[0]","e911ee73":"img = plt.imread(label1 + '\/' + os.listdir(label1)[3])\nplt.imshow(img)\n# img.shape","45eaec8b":"img_height = []\nimg_width = []\nfor i in range(len(os.listdir(label1))):\n    img = plt.imread(label1 + '\/' + os.listdir(label1)[i])\n    img_height.append(img.shape[0])\n    img_width.append(img.shape[1])","b44cc0c5":"plt.style.use('ggplot')\nplt.figure(figsize = (10,5))\nplt.hist(img_height, bins = 20)","b39c89bb":"plt.style.use('ggplot')\nplt.figure(figsize = (10,5))\nplt.hist(img_width, bins = 20, color = 'blue')","61e742ca":"batch_size = 32\nimg_height = 300\nimg_width = 300","135b4260":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  path,\n  validation_split=0.25,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","9aecd6af":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  path,\n  validation_split=0.25,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","08c9ab24":"# class_names = train_ds.class_names","0d0d7e22":"plt.figure(figsize=(18, 14))\nfor images, labels in train_ds.take(1):\n  for i in range(25):\n    ax = plt.subplot(5, 5, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(classes[labels[i]])\n    plt.axis(\"off\")","8ee4fca5":"normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255)","bc113085":"data_augmentation = tf.keras.Sequential(\n  [\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","f592dab5":"base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\"\n               ,input_shape=(img_height, img_width, 3), pooling='max')\nx = base_model.output\n\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(107, activation=\"softmax\")(x)\nmodel = Model(base_model.input, predictions)","691dbe4a":"model.compile(\n  optimizer='adam',\n  loss = 'sparse_categorical_crossentropy',\n  metrics=['accuracy'])","09b06477":"history = model.fit(train_ds, validation_data=val_ds, epochs=40)","5aca483a":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  plt.figure(figsize = (12,6))\n  plt.plot(epochRange,history.history['accuracy'])\n  plt.plot(epochRange,history.history['val_accuracy'])\n  plt.title('Model Accuracy')\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()\n\n  plt.figure(figsize = (12,6))\n  plt.plot(epochRange,history.history['loss'])\n  plt.plot(epochRange,history.history['val_loss'])\n  plt.title('Model Loss')\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()","556e053f":"plotLearningCurve(history, 40)","9b99ee34":"model.save('yoga_pose_model.h5')\n\nFileLink(r'yoga_pose_model.h5')","6c70c326":"Let's see name of a few yoga posses.","dae176e0":"**Now let's plot the image's height and width distribution of the category 'virabhadrasana i'**","f0a1783e":"Let's check whether our data is balanced or not.","2af61ab4":"Now let's get number of images in each poses.","e1548816":"Total no of Yoga Posses"}}