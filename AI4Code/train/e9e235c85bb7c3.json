{"cell_type":{"529d4fa4":"code","569e4673":"code","27a9ceeb":"code","7f16d560":"code","17ebf36c":"code","10d22e6a":"code","4ee43627":"code","50bb7e3d":"code","5fed3b42":"code","70ed5366":"code","f7e5d660":"code","5b1c1e5b":"code","f6dd2e03":"code","14afbf27":"code","be87a51d":"code","1ef62594":"code","1363e1ae":"code","38c47b3e":"code","077dc944":"code","8241fc97":"code","05203e91":"code","4ac37db0":"code","9d6a06e4":"code","837ac8d2":"code","ccfb3c14":"code","b3ba9232":"code","d13e2043":"code","446077cf":"code","78611ff0":"code","3bf37ece":"code","add57b18":"code","21e8f066":"code","b2e409f0":"code","acebef7a":"code","337e2b18":"code","eedbfe5b":"code","9d78d075":"code","0b64de4c":"code","521946e5":"code","94b49e58":"code","a24ece1a":"code","29b158ab":"markdown","770e82e1":"markdown","cce60a49":"markdown","084dbb52":"markdown","118dc3ed":"markdown"},"source":{"529d4fa4":"!ls -1 .\/target\/valid | wc -l","569e4673":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mplimg\nfrom matplotlib.pyplot import imshow\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nimport tqdm\nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n\nimport warnings\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\n\nfrom collections import OrderedDict\nimport cv2","27a9ceeb":"full_train_df = pd.read_csv(\"..\/input\/humpback-whale-identification\/train.csv\")\nfull_train_df.head()","7f16d560":"print(f\"There are {len(os.listdir('..\/input\/humpback-whale-identification\/train'))} images in train dataset with {full_train_df.Id.nunique()} unique classes.\")\nprint(f\"There are {len(os.listdir('..\/input\/humpback-whale-identification\/test'))} images in test dataset.\")","17ebf36c":"fig = plt.figure(figsize=(25, 4))\ntrain_imgs = os.listdir(\"..\/input\/humpback-whale-identification\/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(2, 20\/\/2, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"..\/input\/humpback-whale-identification\/train\/\" + img)\n    plt.imshow(im)\n    lab = full_train_df.loc[full_train_df.Image == img, 'Id'].values[0]\n    ax.set_title(f'Label: {lab}')","10d22e6a":"full_train_df.Id.value_counts().sort_values(ascending=False).head()","4ee43627":"import copy\nnew_whale_df = full_train_df.query(\"Id == 'new_whale'\")\ntrain_df = full_train_df.query(\"Id != 'new_whale'\")\nprint(new_whale_df.shape)\nprint(train_df.shape)","50bb7e3d":"if not os.path.exists('.\/target'):\n    os.system(\"mkdir .\/target\")","5fed3b42":"if not os.path.exists('.\/target\/train'):\n    os.system(\"mkdir .\/target\/train\")","70ed5366":"for image_name in train_df.Image.values:\n    src_path = os.path.join('..\/input\/humpback-whale-identification\/train', image_name)\n    dist_path = os.path.join('.\/target\/train')\n    os.system(\"cp \" + src_path + \" \" + dist_path)","f7e5d660":"for i in range(1, 4):\n    print(f'There are {train_df.Id.value_counts()[train_df.Id.value_counts().values==i].shape[0]} classes with {i} samples in train data.')","5b1c1e5b":"plt.title('Distribution of classes excluding new_whale');\ntrain_df.Id.value_counts()[1:].plot(kind='hist');","f6dd2e03":"np.array(im).shape","14afbf27":"def prepare_labels(y):\n    # From here: https:\/\/www.kaggle.com\/pestipeti\/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","be87a51d":"class WhaleDataset(Dataset):\n    def __init__(self, datafolder, datatype='train', df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None):\n        self.datafolder = datafolder\n        self.datatype = datatype\n        self.y = y\n        if self.datatype == 'train':\n            self.df = df.values\n        self.image_files_list = [s for s in os.listdir(datafolder)]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files_list)\n    \n    def __getitem__(self, idx):\n        if self.datatype == 'train':\n            img_name = os.path.join(self.datafolder, self.df[idx][0])\n            label = self.y[idx]\n            \n        elif self.datatype == 'test':\n            img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n            label = np.zeros((5004,))\n            \n        image = Image.open(img_name).convert('RGB')\n        image = self.transform(image)\n        if self.datatype == 'train':\n            return image, label\n        elif self.datatype == 'test':\n            # so that the images will be in a correct order\n            return image, label, self.image_files_list[idx]","1ef62594":"y, le_full = prepare_labels(train_df['Id'])","1363e1ae":"data_transforms = transforms.Compose([\n    transforms.Resize((100, 100)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n    ])\ntrain_dataset = WhaleDataset(datafolder='.\/target\/train\/', datatype='train', df=train_df, transform=data_transforms, y=y)\n\nbatch_size = 32\nnum_workers = 2\n\ntrain_sampler = SubsetRandomSampler(list(range(len(os.listdir('.\/target\/train')))))\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)","38c47b3e":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 7, padding=1)\n        self.conv2_bn = nn.BatchNorm2d(32)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)        \n        self.pool2 = nn.AvgPool2d(3, 3)\n        \n        self.fc1 = nn.Linear(64 * 4 * 4 * 16, 1024)\n        self.fc2 = nn.Linear(1024, 5004)\n\n        self.dropout = nn.Dropout(0.5)        \n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv2_bn(self.conv1(x))))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 4 * 4 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","077dc944":"model_conv = Net()\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = optim.Adam(model_conv.parameters(), lr=0.01)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","8241fc97":"model_conv.cuda()\nn_epochs = 7\nloss_list = []\nfor epoch in range(1, n_epochs+1):\n    print(time.ctime(), 'Epoch:', epoch)\n\n    train_loss = []\n    exp_lr_scheduler.step()\n\n    for batch_i, (data, target) in  enumerate(train_loader):\n        #print(batch_i)\n        data, target = data.cuda(), target.cuda()\n        optimizer.zero_grad()\n        output = model_conv(data)\n        loss = criterion(output, target.float())\n        train_loss.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        \n    loss_list.append(np.mean(train_loss))\n    print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}')","05203e91":"plt.plot(range(n_epochs), loss_list, 'r-', label='train_loss')\nplt.legend()","4ac37db0":"torch.save(model_conv, '.\/without_newwhale_model')","9d6a06e4":"len_valid_nw = new_whale_df.shape[0]\nvalid_new_whale_df = new_whale_df.head(int(len_valid_nw * 0.2))\nvalid_df = pd.concat([train_df, valid_new_whale_df]).reset_index(drop=True)\nvalid_df.head()","837ac8d2":"del train_df","ccfb3c14":"if not os.path.exists('.\/target\/valid'):\n    os.system(\"mkdir .\/target\/valid\")","b3ba9232":"!rm -f .\/target\/valid\/*","d13e2043":"!rm -f .\/target\/train\/*","446077cf":"valid_df.shape","78611ff0":"for image_name in valid_df.Image.values:\n    src_path = os.path.join('..\/input\/humpback-whale-identification\/train', image_name)\n    dist_path = os.path.join('.\/target\/valid')\n    os.system(\"cp \" + src_path + \" \" + dist_path)","3bf37ece":"!ls -1 .\/target\/valid | wc -l","add57b18":"y, le_full = prepare_labels(valid_df['Id'])","21e8f066":"data_transforms_valid = transforms.Compose([\n    transforms.Resize((100, 100)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nvalid_set = WhaleDataset(datafolder='.\/target\/valid', datatype='test', transform=data_transforms_valid)\n\nbatch_size = 32\nnum_workers = 2\n\nvalid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, num_workers=num_workers, pin_memory=True)","b2e409f0":"model_conv = torch.load('.\/without_newwhale_model')","acebef7a":"model_conv.eval()\nfor (data, target, name) in valid_loader:\n    data = data.cuda()\n    output = model_conv(data)\n    output = output.cpu().detach().numpy()\n    for i, (e, n) in enumerate(list(zip(output, name))):\n        e_str = [str(s) for s in e]\n        valid_df.loc[valid_df['Image'] == n, 'Predict'] = ' '.join(e_str)\n\nvalid_df.head()","337e2b18":"preds = valid_df['Predict']\nids = valid_df['Id']","eedbfe5b":"def map5(X, y, th):\n    score = 0\n    for i in range(X.shape[0]):\n        str_X = X[i].split(' ')\n        result = [float(s) for s in str_X]\n        result.insert(0, float(th))\n        result = np.array(result)\n        pred = le_full.inverse_transform(result.argsort()[-5:][::-1])\n        for j in range(pred.shape[0]):\n            if pred[j] == y[i]:\n                score += (5 - j)\/5\n                break\n    return float(score\/X.shape[0])","9d78d075":"best_th = 0\nbest_score = 0\nfor th in np.arange(-10, 0, 1):\n    score = map5(preds, ids, th)\n    if score > float(best_score):\n        best_score = score\n        best_th = th\n    print(\"Threshold = {:.3f}, MAP5 = {:.3f}\".format(th,score))","0b64de4c":"print(\"Best Threshold = {:.3f}, Best MAP5 = {:.3f}\".format(best_th,best_score))","521946e5":"data_transforms_test = transforms.Compose([\n    transforms.Resize((100, 100)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntest_set = WhaleDataset(datafolder='..\/input\/humpback-whale-identification\/test\/', datatype='test', transform=data_transforms_test)\n\nbatch_size = 32\nnum_workers = 2\n\ntest_sampler = SubsetRandomSampler(list(range(len(os.listdir('..\/input\/humpback-whale-identification\/test')))))\n# less size for test loader.\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=32, num_workers=num_workers)","94b49e58":"sub = pd.read_csv('..\/input\/humpback-whale-identification\/sample_submission.csv')\n\nmodel_conv.eval()\nfor index, (data, target, name) in enumerate(test_loader):\n    data = data.cuda()\n    output = model_conv(data)\n    output = output.cpu().detach().numpy()\n    for i, (e, n) in enumerate(list(zip(output, name))):\n        e = np.insert(e, 0, float(best_th))\n        sub.loc[sub['Image'] == n, 'Id'] = ' '.join(le_full.inverse_transform(e.argsort()[-5:][::-1]))\n        \nsub.to_csv('submission_without_new_whale.csv', index=False)","a24ece1a":"!rm -rf .\/target","29b158ab":"# Find fixed threshold of new whale\n- https:\/\/www.kaggle.com\/suicaokhoailang\/removing-class-new-whale-is-a-good-idea\n- At first, I couldn't understand what he does in the kernel. I got he found the fixed detection rate of new whale.","770e82e1":"# Predict","cce60a49":"# remove new whale\nOnly new_whale has more pictures than other classes. It causes new_whale detection frequentry. That's why I remove new_whale training set.\n- https:\/\/www.kaggle.com\/suicaokhoailang\/removing-class-new-whale-is-a-good-idea","084dbb52":"# Training","118dc3ed":"# Basic CNN without New Whale"}}