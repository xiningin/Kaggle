{"cell_type":{"ea37bf1a":"code","e2cb54ca":"code","42617c06":"code","e15ed7c7":"code","fe101895":"code","5568183e":"code","68a81b50":"code","367f2054":"code","920673ca":"code","9e5c0081":"code","197b33e0":"code","63b1582b":"code","81295a5f":"code","52009e72":"code","05886572":"code","d2f2a2de":"code","1f5c46e3":"code","e5f02f32":"markdown","66157549":"markdown","e93ed640":"markdown","7951537e":"markdown","06a2955c":"markdown","4c75c8cb":"markdown","bfe7f428":"markdown"},"source":{"ea37bf1a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport string, os \nimport tensorflow as tf\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense, Bidirectional,SimpleRNN\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e2cb54ca":"# csv file\ndf = pd.read_csv('..\/input\/scrapped-lyrics-from-6-genres\/lyrics-data.csv')\n\n\n# As the model takes a lot of time to train using the whole dataset\n# We will instead use only a small portion of it, and in order to increase\n# accuracy, we will train the model one songs written by one author, namely Evlis\n# Presley, whose songs were written back in the 70's, before modern day \"Rap English\"\n# was common in songs.\n\n# Getting songs written by one author\ndf = df[df['ALink']==\"\/elvis-presley\/\"]\n# Training on English songs only\ndf.drop(['ALink','SName','SLink'],axis=1,inplace=True)\ndf = df[df['Idiom']=='ENGLISH']\ndf = df[:500]\ndf.shape","42617c06":"# Tokenization\ntokenizer = Tokenizer()\n# Used to preprocess the text, by removing the comma and\n# other punctuations for example. We also removed numbers\n# It also splits the sentences to words, and turns all words\n# into lower case\n\ntokenizer.fit_on_texts(df['Lyric'].astype(str).str.lower())\n# Applying the tokenizer\n\ntotal_words = len(tokenizer.word_index)+1\n# We add one to the index, as index starts from zero\n\ntokenized_sentences = tokenizer.texts_to_sequences(df['Lyric'].astype(str))\n# Turns the words into integer type by classifying them according to word dictionary.\n#tokenized_sentences[0]\n# Checking the first element, to understand what happened","e15ed7c7":"# Slash sequences into n gram sequence\ninput_sequences = list()\nfor i in tokenized_sentences:\n    for t in range(1, len(i)):\n        n_gram_sequence = i[:t+1]\n        input_sequences.append(n_gram_sequence)\n# Having reached this point, input_sequences contains array of lists of words, starting from\n# a list containg 2 words, then we append a third word to them, then another word, and so on.\n# This causes a problem, as the length of the lists are not equal, so we pad them by adding zeros\n# at the beginnig, till they are all of the same length (the maximum length)\n        \n# Pre padding\nmax_sequence_len = max([len(x) for x in input_sequences]) \n# Gets the length of the longest list in the \n# array of lists\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))","fe101895":"# Dividing the data into X, y -----> the training set, and the labels to be predicted\nX, labels = input_sequences[:,:-1],input_sequences[:,-1]\n# Takes all elemnts in each row, except the last element, and places them in X\n# while labels takes the last element (the element which we should predict)\n\ny = tf.keras.utils.to_categorical(labels, num_classes=total_words) # One hot encoding\n# number of classes is now equal to the number of unique words in the song lyrics","5568183e":"# creating model\nmodel = Sequential()","68a81b50":"#model = Sequential()\nmodel.add(Embedding(total_words, 40, input_length=max_sequence_len-1))\n# dimension of input: total_words, the number of unique words we have\n# 40: the desired dimension of the output\n# input_length: the sequence length is all the words except the last one (the one\n# we will predict)\nmodel.add(Bidirectional(SimpleRNN(250))) # 250 is the average number of words in a song\n# So our cycle is the average length of a song\n# We used LSTM instead of simple RNN as simple RNN faces a vanishing gradient\n# problem, also, we need to remember the previous words, to predict the next words.\n\nmodel.add(Dropout(0.1)) # To overcome overfitting\nmodel.add(Dense(total_words, activation='softmax'))","367f2054":"model.summary()","920673ca":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nearlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\nhistory = model.fit(X, y, batch_size=32, epochs=2,callbacks=[earlystop], validation_split = 0.2)","9e5c0081":"# save model before continuing the code, in case we run out of memory\nfrom tensorflow.keras.models import load_model\nmodel.save('lyrics_generator_Simp.h5')","197b33e0":"df = pd.DataFrame(history.history)\ndf[['loss','val_loss']].plot()","63b1582b":"# el input_text dh el text elly badeehlo, w bykamel 3leh el o8nya\n# next_words dyh 3adad el kalemat el hy3mlha prediction\ndef complete_this_song(input_text, next_words):\n    for _ in range(next_words):\n        # for _ in... this is like a place holder, which upholds the syntax.\n        # We use it when we don't want to use the variable, so we leave it empty.\n        \n        # Doing the same things to the input as we did when training the model\n        token_list = tokenizer.texts_to_sequences([input_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        #predicted = model.predict_classes(token_list, verbose=0)\n        predicted = np.argmax(model.predict(token_list), axis=-1)\n        \n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                # Gets the word corresponding the the value predicted\n                # [Converting from numeric to string again]\n                output_word = word\n                break\n        input_text += \" \" + output_word\n    return input_text\n","81295a5f":"#complete_this_song(\"i must tell you about\", 40)","52009e72":"complete_this_song(\"Never have i ever\", 155)","05886572":"complete_this_song(\"This is the beginning\", 155)","d2f2a2de":"complete_this_song(\"if i could\", 12)","1f5c46e3":"complete_this_song(\"i must tell you about\", 40)","e5f02f32":"# Import Libraries","66157549":"# Plot losses","e93ed640":"# Save The Model","7951537e":"# Model","06a2955c":"# Reading Data","4c75c8cb":"# Data Preprocessing","bfe7f428":"# Generating Song Lyrics"}}