{"cell_type":{"0bbdbf8c":"code","22b73f6f":"code","8d850e91":"code","ca3283bb":"code","aa1ca029":"code","cc77426b":"code","fc1fe559":"code","002838f3":"code","95ffbe87":"code","e3cb218f":"code","20d2499d":"code","910c1233":"code","99a551e4":"code","eccaf868":"code","e8bde520":"code","500e305f":"code","4c3e4868":"code","bf624472":"code","bae07b99":"code","5a28c9c0":"code","909cfe56":"code","b45ff785":"code","f7bfdcdc":"code","7e4c1362":"code","ccc39202":"code","b4614504":"code","854c38ae":"code","36f142d0":"code","64f11942":"code","2cb514d0":"code","eb326406":"code","e429afd5":"code","1aeb7636":"code","1bb3593e":"code","9ff8b0c4":"code","0dcc9160":"code","78d6910e":"code","733a2037":"code","69fa0ebb":"code","97fc6a4f":"code","bbb725c8":"code","81d4e1d6":"code","cddd9ae3":"code","209b36ee":"code","1aad4f4a":"code","1953a59a":"code","a5f3b75d":"code","b3305c30":"code","b5ac0eea":"code","d7cc0b0f":"code","8224cf4d":"code","68a83e03":"code","2260ad0e":"code","55d11eaa":"markdown","f35182bf":"markdown","819e9248":"markdown","969a5341":"markdown","363d2875":"markdown","46770dd1":"markdown","7d008698":"markdown","2937dc4c":"markdown","f094a237":"markdown","8662cfc1":"markdown","ed689d40":"markdown","20afc2d5":"markdown","c859742d":"markdown","c3e492e7":"markdown","c98d0bc5":"markdown","42dfafdc":"markdown","2fca09c0":"markdown","d3e5461e":"markdown","ad1c6388":"markdown","05e7b4df":"markdown","29f7b9f5":"markdown","cb97de19":"markdown","629e62b0":"markdown","e3ffd42c":"markdown","1f908c7e":"markdown","e300ae39":"markdown","e423fe5d":"markdown","14e58267":"markdown","680455ea":"markdown","16234934":"markdown","6a57f235":"markdown","a2b2619c":"markdown","c40bbeac":"markdown","aa5a03df":"markdown","6fd7ed2d":"markdown","68c47c3a":"markdown","22747e27":"markdown","bc2b4591":"markdown","1dc26373":"markdown","576a1dbc":"markdown","7f965308":"markdown","cdad4411":"markdown","70ed0142":"markdown","83a3fc94":"markdown","79c5e170":"markdown","af6d3ab7":"markdown","8f13febe":"markdown","349cfc9f":"markdown","4a868057":"markdown"},"source":{"0bbdbf8c":"# these will be the libraries we will use\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport nltk\n\nimport os\n\nimport re\n","22b73f6f":"pip install openpyxl","8d850e91":"data = pd.read_excel('..\/input\/progressive-rock-pop-songs-lyrics\/selected_songs.xlsx')\n\ndata","ca3283bb":"data[\"genre\"].unique()","aa1ca029":"def rid_of_specials(words):\n    new= ''\n    for i in range(len(words)):\n        a = re.sub('[^A-Za-z]+', ' ', words[i]).lower()\n        new += a\n    return new","cc77426b":"data[\"lyrics\"] = data[\"lyrics\"].apply(rid_of_specials)\ndata[\"lyrics\"].head(10)","fc1fe559":"data.genre","002838f3":"data[\"lyrics\"]","95ffbe87":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nsw_nltk = (stopwords.words('english'))\nstop_words = set(sw_nltk)","e3cb218f":"def remove_sw(x):\n    x = x.split(' ')\n    return  ' '.join(z for z in x if z not in stop_words)\nstopped = data[\"lyrics\"].apply(remove_sw)\nstopped","20d2499d":"from nltk.stem import WordNetLemmatizer\n# Step 1\nlemmatizer = WordNetLemmatizer()\n# Step 2\nlemmatized = [lemmatizer.lemmatize(i) for i in stopped]\n# Step 3\nprepeared_sentence = [''.join(j) for j in lemmatized]\ndata['Lyrics_Processed'] = prepeared_sentence\ndata['Lyrics_Processed']","910c1233":"data","99a551e4":"from sklearn.feature_extraction.text import CountVectorizer\n# Step 1\nvectorizer = CountVectorizer()\n# Step 2\nX = vectorizer.fit_transform(prepeared_sentence)\n# Step 3\nfeature_names = vectorizer.get_feature_names()\n# Step 4\nlyrics_vectorized = pd.DataFrame(X.toarray(), columns = feature_names)\n","eccaf868":"lyrics_vectorized","e8bde520":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nX = tfidfconverter.fit_transform(X).toarray()\nX","500e305f":"X.shape","4c3e4868":"# splitwords = [ nltk.word_tokenize( str(sentence) ) for sentence in  data['Lyrics_Processed'] ]\n# print(splitwords)","bf624472":"# our genre column of 2 unique variables\n\ndata.genre","bae07b99":"# we assign numerical values 1 and 0 to genres \"prog\" and \"pop\", respectively.\n\ngenre_array = data.genre.map({\"prog\":1, \"pop\":0})\ngenre_array","5a28c9c0":"lyrics_vectorized","909cfe56":"# step 1\n\ndata['Lyrics_Processed'].str.len().hist(bins=50, range = (0,3000))","b45ff785":"# step 2\n\ndata['Lyrics_Processed'].str.split().\\\n    map(lambda x: len(x)).\\\n    hist(bins=50, range = (0,500))","f7bfdcdc":"# step 3\n\ncorpus=[]\nnew= data['Lyrics_Processed'].str.split()\nnew=new.values.tolist()\ncorpus=[word for i in new for word in i]\n\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx, y= [], []\nfor word,count in most[:20]:\n    if (word not in stop_words):\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","7e4c1362":"# we see that 412th row is the last progressive rock row.\n# 412 was found with trial and error in minimal amount of time.\n\nprint(data.genre[412])\nprint(data.genre[413])","ccc39202":" # creating a variable for rock lyrics, then assigning the \"Lyrics Processed\" column's rock lyric parts to it.\n\nrock_lyrics = data['Lyrics_Processed'][0:413]\nrock_lyrics","b4614504":"# same process, now for pop lyrics.\n\npop_lyrics = data['Lyrics_Processed'][413:829]\npop_lyrics","854c38ae":"def get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]","36f142d0":"# top 10 bigrams for all lyrics\n\ntop_n_bigrams=get_top_ngram(data['Lyrics_Processed'],2)[:10] \nx,y=map(list,zip(*top_n_bigrams)) \nsns.barplot(x=y,y=x)","64f11942":"# many repetitions for total of both genres, even when n = 3\n\ntop_tri_grams=get_top_ngram(data['Lyrics_Processed'],n=3)\nx,y=map(list,zip(*top_tri_grams))\nsns.barplot(x=y,y=x)","2cb514d0":"# top 10 bigrams for rock lyrics\n\ntop_n_bigrams=get_top_ngram(rock_lyrics,2)[:10] \nx,y=map(list,zip(*top_n_bigrams)) \nsns.barplot(x=y,y=x)","eb326406":"top_n_bigrams=get_top_ngram(pop_lyrics,2)[:10] \nx,y=map(list,zip(*top_n_bigrams)) \nsns.barplot(x=y,y=x)","e429afd5":"# top 10 trigrams for rock lyrics\n\ntop_tri_grams=get_top_ngram(rock_lyrics,n=3)\nx,y=map(list,zip(*top_tri_grams))\nsns.barplot(x=y,y=x)","1aeb7636":"# \"la la la\" seems to be very prominent in both genres\n\ntop_tri_grams=get_top_ngram(pop_lyrics,n=3)\nx,y=map(list,zip(*top_tri_grams))\nsns.barplot(x=y,y=x)","1bb3593e":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\n\ndef show_wordcloud(data):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30,\n        scale=4,\n        random_state=1)\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(corpus)","9ff8b0c4":"corpus_rock=[]\nnew= rock_lyrics.str.split()\nnew=new.values.tolist()\ncorpus_rock=[word for i in new for word in i]\n\ndef show_wordcloud(data):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30,\n        scale=4,\n        random_state=0)\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(corpus_rock)","0dcc9160":"corpus_pop=[]\nnew= pop_lyrics.str.split()\nnew=new.values.tolist()\ncorpus_pop=[word for i in new for word in i]\n\ndef show_wordcloud(data):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30,\n        scale=4,\n        random_state=0)\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(corpus_pop)","78d6910e":"from textblob import TextBlob\ndef polarity(text):\n    return TextBlob(text).sentiment.polarity\n\n# polarity is a floating-point number that lies in the range of [-1,1] \n# where 1 means positive statement and -1 means a negative statement.\n\ndata['polarity_score']=data['Lyrics_Processed'].\\\n   apply(lambda x : polarity(x))\ndata['polarity_score'].hist()","733a2037":"# We have a pretty normally distributed polarity score,\n# which means that these genres in total are neutral in feelings overall","69fa0ebb":"# polarity of pop lyrics\n\ndata['polarity_score']=pop_lyrics.\\\n   apply(lambda x : polarity(x))\ndata['polarity_score'].hist()","97fc6a4f":"# polarity of rock lyrics\n\n\ndata['polarity_score']=rock_lyrics.\\\n   apply(lambda x : polarity(x))\ndata['polarity_score'].hist()","bbb725c8":"data","81d4e1d6":"# the code is very much incomplete after this point.","cddd9ae3":"# we only need these columns\n\ndata[[\"genre\", \"Lyrics_Processed\"]]","209b36ee":"data.columns","1aad4f4a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, genre_array, test_size=0.2, random_state=0)","1953a59a":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train)","a5f3b75d":"print(\"Naive Bayes Score: \", nb.score(X_test,y_test) )","b3305c30":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(X_train, y_train)","b5ac0eea":"print(\"Decision Tree Score: \", dt.score(X_test,y_test) )","d7cc0b0f":" from sklearn.ensemble import RandomForestClassifier\n    \nclassifier = RandomForestClassifier(n_estimators=550, max_depth=300, random_state=0)\nclassifier.fit(X_train, y_train)","8224cf4d":"y_pred = classifier.predict(X_test)\ny_pred","68a83e03":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","2260ad0e":"print(\"Naive Bayes Score: \", nb.score(X_test,y_test) )\nprint(\"Decision Tree Score: \", dt.score(X_test,y_test) )\nprint(\"Random Forest Score: \",accuracy_score(y_test, y_pred))","55d11eaa":"<a id = \"34\"><\/a>\n## Naive Bayes","f35182bf":"<a id = \"38\"><\/a>\n# 9 - Conclusion","819e9248":"<a id = \"23\"><\/a>\n### a) Creating variables for lyrics of each genre","969a5341":"<a id = \"30\"><\/a>\n### a) Justifying how in theory more is better, but in real life it is not","363d2875":"Using Random Forest Classifier, our precision, f1-score and accuracy are all above 0.90. ","46770dd1":"<a id = \"24\"><\/a>\n### b) Visualizing & analyzing ngrams","7d008698":"<a id = \"7\"><\/a>\n### a) Removing special characters","2937dc4c":"<a id = \"32\"><\/a>\n# 6 -  PART IV : Train - Test Splitting","f094a237":"To summarize,\n- We defined the problem,\n- We then imported the data & the libraries to use,\n- We preprocessed the data, removing certain parts, lemmatizing & vectorizing it,\n- We have visualized the data for a better understanding,\n- The lyrics were put through a sentiment analysis,\n- The data was then split into train & test groups,\n- Random Forest model was applied for prediction,\n- Finally, the model was evaluated, and accuracy ratings of 0.91, 0.82 and 0.9 were the outcomes, among others.","8662cfc1":"<a id = \"28\"><\/a>\n## Creating Additional Features (Optional)\nThis option was not utilized.","ed689d40":"<a id = \"26\"><\/a>\n\n## Sentiment analysis","20afc2d5":"This histogram shows us that the amount of **characters** in song lyrics are mostly in the range of ~(300 - 1200)","c859742d":"<a id = \"33\"><\/a>\n# 7 -  PART V : Modelling","c3e492e7":"<a id = \"6\"><\/a>\n## Removing Numbers, Punctuations and Lowercasing the Words","c98d0bc5":"<a id = \"11\"><\/a>\n## Lemmatization","42dfafdc":"<a id = \"18\"><\/a>\n### c) Differences between TF-IDF & Count Vectorizer\nTF-IDF stands for \"Term Frequency \u2013 Inverse Document\u201d.\n\nThe point of TF-IDF is to calculate how frequent words appear in our dataset, with taking into account how certain words can appear disproportionality more, like the stop words which we got rid of already.\n\nAlso worth noting is that TF-IDF returns floats (because it keeps a **frequency score**), while CountVectorizer returns integers","2fca09c0":"<a id = \"4\"><\/a>\n##  Importing the Data","d3e5461e":"<a id = \"29\"><\/a>\n## Select \/ Eliminate redundant features","ad1c6388":"Before we continue on with more EDA, we'd like to assign two matrices of words to each genre, so we can work easier with these new variables.\n","05e7b4df":"<a id = \"27\"><\/a>\n# 5 -  PART III : Feature Creation & Selection","29f7b9f5":"<a id = \"31\"><\/a>\n### b) Performing feature selection\/elimination","cb97de19":"<a id = \"12\"><\/a>\n### a) Explanation of code\n\n- WordNetLemmatizer was imported from nltk.stem\n- A variable \"lemmatizer\" then was created, the imported lemmatizer was assigned.\n- Using list comprehension for a for loop, our series \"stopped\" was lemmatized and assigned to \"lemmatized\"","629e62b0":"<a id = \"36\"><\/a>\n## Random Forest Algorithm","e3ffd42c":"<a id = \"3\"><\/a>\n# 3 -  PART I : Data Preprocessing","1f908c7e":"<a id = \"37\"><\/a>\n# 7 -  PART VI : Model Evaluation","e300ae39":"<a id = \"1\"><\/a>\n# 1 - Problem Definition\n   In platforms like last.fm, users generally add tags to make the song get seen by those who wish listening them. Some users find what they will listen with these tags which includes information about the genres. Although, there are many missing tags which lead users spend more time finding what to listen. For this problem, we want to implement machine learning algorithms to improve data quality of the last.fm, and the user experience. \n\n<b>Our task is here, classifiying  whether given song's genre is progressive rock or pop by its lyrics.<\/b><p>This project is different than other classification-based projects. So, after you complete that assignment; we will be able to comprehend many basic concepts of Natural Language Processing.<\/p>\n","e423fe5d":"<a id = \"3\"><\/a>\n##  Importing the Libraries","14e58267":"<a id = \"15\"><\/a>\n## Vectorization","680455ea":"<a id = \"2\"><\/a>\n# 2 - Importing","16234934":"<a id = \"20\"><\/a>\n# 4 -  PART II : Exploratory Data Analysis","6a57f235":"<a id = \"25\"><\/a>\n\n## Creating Wordclouds","a2b2619c":"<a id = \"21\"><\/a>\n## General Attributes of Songs & Lyrics","c40bbeac":"Ngrams are n amounts of adjacent words. We will utilize them to find patterns & repetitions if possible.\n","aa5a03df":"<a id = \"22\"><\/a>\n## Using Ngrams","6fd7ed2d":"# Predicting the Genre (pop or prog. rock) of a Song by its lyrics","68c47c3a":"This histogram on the other hand, shows us that the amount of **words** in songs are mostly in the range of ~(30 - 200)\n\nNote that we have already removed stop words.","22747e27":"Looking at these three histograms, we can deduce:\n- Overall average polarity score is between (0.0 - 0.20)\n- Pop lyrics' polarity score is between (0.10 - 0.20)\n- Rock lyrics' polarity score is between (-0.05 - 0.15)\n\nWhich means rock lyrics have a slightly more negative score than pop lyrics\n\nBut overall, both genres seem to have a more positive score than neutral.","bc2b4591":"<a id = \"10\"><\/a>\n### Explanation \n\n\n1) We imported nltk to remove (and then later tokenize) words\n\n2) We imported stopwords, and let a variable \"sw_nltk\" be equal to a tuple of stop words in english\n\n3) Then, we let another variable \"stop_words\" be equal to a set containing these stop words\n\n4) A function \"remove_sw\" was defined, which uses the variable defined in step 3\n\n5) Finally, we applied this function to our \"lyrics\" column, using .apply\n","1dc26373":"~ 0.91, 0.82, 0.9 are the accuracy scores for our prediction, using Naive Bayes, Decision Tree and Random Forest respectively.","576a1dbc":"<a id = \"35\"><\/a>\n## Decision Trees","7f965308":"<a id = \"19\"><\/a>\n## Target Preperation","cdad4411":"<a id = \"8\"><\/a>\n### b) Explanation\n- We have defined a function called rid_of_specials in order to remove special characters from our lyrics.\n\n- Then, we applied this to the \"lyrics\" column of our imported \"selected_songs\" data, using .apply()\n","70ed0142":"# Introduction\n\n1.  [Problem Definition](#1)\n1.  [Importing](#2)\n    1. [Importing the Libraries](#3)\n    1. [Importing the Data](#4)\n1.  [PART I : Data Preprocessing](#5)\n    1. [Removing Numbers, Punctuations and Lowercasing the Words](#6)\n        1. [Removing special characters](#7)\n        1. [Explanation](#8)\n    1. [Removing Stopwords](#9)\n        * [Explanation](#10)\n    1.  [Lemmatization](#11)\n        1. [Explanation](#12)\n        1. [Explanation of Lemmatization](#13)\n        1. [Alternatives to Lemmatization](#14)\n    1.  [Vectorization](#15)\n        1. [Explanation of code](#16)\n        1. [Explanation of Vectorization](#17)\n        1. [Differences between TF-IDF & Count Vectorizations](#18)\n    1.  [Target Preperation](#19)\n1.  [PART II - Exploratory Data Analysis](#20)\n    1. [General Attributes of Songs & Lyrics](#21)\n    1. [Using Ngrams](#22)\n        1. [Creating variables for both genres](#23)\n        1. [Visualizing & analyzing ngrams](#24)\n    1. [Creating Wordclouds](#25)\n    1. [Sentiment Analysis](#26)\n1.  [PART III - Feature Creation & Selection](#27)\n\n    1. [Creating Additional Features (Optional)](#28)\n\n    1. [Select \/ Eliminate redundant features](#29)\n\n        1. [Justifying how in theory more is better, but in real life it is not](#30)\n\n        1. [Performing feature selection\/elimination](#31)\n\n1. [PART IV : Train - Test Splitting](#32)\n\n1. [PART V : Modelling](#33)\n\n    1. [Naive Bayes](#34)\n\n    1. [Decision Trees](#35)\n\n    1. [Random Forest](#36)\n\n1. [PART VI : Model Evaluation](#37)\n1. [Conclusion](#38)","83a3fc94":"<a id = \"13\"><\/a>\n### b) Explanation of Lemmatization\n\nLemmatization is, in essence, grouping the different forms of the same word together. This helps us not have noisy data.\n\ne.g: liking, like, likes, liked gets grouped into one single item, e.g. \"like\".\n\nPre-made lemmatizers from libraries such as nltk.stem can help us lemmatize such words, and clean our data.","79c5e170":"<a id = \"9\"><\/a>\n## Removing Stopwords","af6d3ab7":"As seen on numerous machine learning examples, more information, more learning, more of everything is not always what is best for our prediction model, since it can result in skewed estimations such as with overfitting.","8f13febe":" <a id = \"17\"><\/a>\n### b) Explanation of countvectorizer\n\nCountvectorizer helps us tokenize, build a vocabulary & encode new documents.\n\nAs explained above, when we use CountVectorizer together with fit() and transform(), it returns an encoded vector with a length of the entire vocabulary and an integer count in the form of (sparse) vectors for the number of times each word appeared in the document.\n","349cfc9f":"<a id = \"16\"><\/a>\n### a) Explanation of code\n\n- Step 1: We assigned CountVectorizer() to our variable: \"vectorizer\"\n- Step 2: We used .fit_transform() to encode the data in prepared_sentence as a vector.\n- Step 2.1: .fit() also learns the vocabulary of the input.\n- Step 3: As its name suggests, get_feature_names extracts each column's name. And we assign it to feature_names.\n- Step 4: Finally, we use .toarray() on our encoded vector and.\n- Step 4.1: pd.DataFrame to align all data in a tabular form.\n","4a868057":"<a id = \"14\"><\/a>\n### c) Alternatives to Lemmatization\nOne alternative of lemmatization is called \"Stemming\", which is also included in the nltk library.\n\nThe way stemming works is by removing the last few characters it deems unnecessary, trying to reduce the word to its essentials.\n\nSince Lemmatization understands \/ considers context and works with the english language as a whole, stemming can be disadvantageous when used in certain words. For example, one word can have different lemmas depending on how it is used. Stemming does not consider this."}}