{"cell_type":{"ee15a0f2":"code","4e41421c":"code","06dc50c0":"code","da697fcb":"code","d30e6457":"code","6a3169d6":"code","2290eebd":"code","b23ab033":"code","6fa5fa0a":"code","44993de8":"code","1f4e8752":"code","503da76a":"code","0936f8b7":"code","a1f14e15":"code","684a9e35":"code","ff7a5447":"code","2c94d815":"code","6b7a9836":"code","661f1d48":"code","f6bd0439":"code","b6b9b6bc":"code","359614be":"code","ad45d83a":"code","0074747d":"code","4c6adda2":"code","17527583":"code","bb2a0050":"code","6d03e40c":"code","16519f64":"code","710c312f":"code","4de65e1e":"code","12d841ee":"code","d8dbffe7":"code","e790f3b9":"code","4b568463":"code","5d2b6292":"markdown","a4f379fb":"markdown","0d568582":"markdown","5c71eeb2":"markdown","29dd24ef":"markdown","3887b6d8":"markdown","376e5f54":"markdown","98be0039":"markdown","6d81f302":"markdown","c1c24950":"markdown","c3c149b6":"markdown","beebf3e5":"markdown","0dc2abf4":"markdown","1caea8a4":"markdown","4532dd28":"markdown","a326ef08":"markdown","311aba16":"markdown","2602a2d7":"markdown","46973f55":"markdown","c60ac898":"markdown","c56c610c":"markdown","77688220":"markdown","7880f113":"markdown","c68a0478":"markdown","01ec0ea6":"markdown","683341ab":"markdown","74856391":"markdown","47339083":"markdown","6ff885f3":"markdown","cf335d60":"markdown","de79fb88":"markdown"},"source":{"ee15a0f2":"import pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport time\nfrom itertools import product","4e41421c":"INPUTFOLDER = '..\/input\/competitive-data-science-predict-future-sales\/'\n\nitem_categories = pd.read_csv(os.path.join(INPUTFOLDER, 'item_categories.csv'))\nitems           = pd.read_csv(os.path.join(INPUTFOLDER, 'items.csv'))\nsales           = pd.read_csv(os.path.join(INPUTFOLDER, 'sales_train.csv'))\nshops           = pd.read_csv(os.path.join(INPUTFOLDER, 'shops.csv'))\ntest            = pd.read_csv(os.path.join(INPUTFOLDER, 'test.csv'))","06dc50c0":"MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nLINEWIDTH=2\nALPHA=.6\n\ndfp = sales[['date', 'date_block_num','item_cnt_day']].copy()\n\n# Extract the year and the month from the date column into indepedent columns\ndfp['date']  = pd.to_datetime(dfp['date'], format='%d.%m.%Y')\ndfp['year']  = dfp['date'].dt.year\ndfp['month'] = dfp['date'].dt.month\ndfp.drop(['date'], axis=1, inplace=True)\n\n# Sum the number of sold items for each date_block_num (which is the consecutive month number from January 2013 to October 2015)\ndfp = dfp.groupby('date_block_num', as_index=False)\\\n       .agg({'year':'first', 'month':'first', 'item_cnt_day':'sum'})\\\n       .rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)\n\nplt.figure(figsize=(16,6))\n# Plot the sales of the year 2013\nplt.plot(MONTHS, dfp[dfp.year==2013].item_cnt_month, '-o', color='steelblue', linewidth=LINEWIDTH, alpha=ALPHA,label='2013')\n\n# Plot the sales of the year 2014\nplt.plot(MONTHS, dfp[dfp.year==2014].item_cnt_month, '-o', color='seagreen', linewidth=LINEWIDTH, alpha=ALPHA,label='2014')\n\n# Plot the sales of the year 2015 until October\nplt.plot(MONTHS[:10], dfp[dfp.year==2015].item_cnt_month, '-o', color='maroon', linewidth=LINEWIDTH, alpha=ALPHA,label='2015')\n\n# Capturing the trend between October and November (For year 2013 and 2014)\ndelta_2013 = dfp.iloc[10].item_cnt_month - dfp.iloc[9].item_cnt_month\ndelta_2014 = dfp.iloc[22].item_cnt_month - dfp.iloc[21].item_cnt_month\navg_delta = (delta_2013 + delta_2014) \/ 2\n# Add the average to the previous month (October 2015)\nnov_2015 = dfp.iloc[33].item_cnt_month + avg_delta\n\n# MONTHS[9:11] equals ['Oct', 'Nov']\nplt.plot(MONTHS[9:11], [dfp.iloc[33].item_cnt_month, nov_2015], '--o', color='gray', linewidth=LINEWIDTH, alpha=ALPHA, label='Prediction', zorder=-1)\n\n# Axes parameters\nax = plt.gca()\nax.set_title('Sales per month')\nax.set_ylabel('# of items')\nax.grid(axis='y', color='gray', alpha=.2)\n    \n# Remove the frame off the chart\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nplt.legend(loc=2, title='Legend')\nplt.show()\n\ndel dfp","da697fcb":"# Top N \nN=15\n\ndef get_ratio(year, topn, N):\n    # Get total sold items for each year\n    total = dfp.loc[year].item_cnt_year.sum()\n    ratio = topn\/total*100\n    return \"{0}: the total of the top {1} best selling items is {2} over a total of {3} for that year, which represents {4:.2f}%\".format(year, N, topn, total, ratio)\n\ndfp = sales[['date', 'item_id', 'item_cnt_day']].copy()\ncats = item_categories.copy()\n\n# Extract the year from the date column\ndfp['year'] = pd.to_datetime(dfp['date'], format='%d.%m.%Y').dt.year\ndfp.drop('date', axis=1, inplace=True)\ndfp.item_cnt_day = dfp.item_cnt_day.astype(int)\n\n# Remove returns\ndfp = dfp[dfp.item_cnt_day>0]\n\n# Add the category of each item\ndfp = dfp.merge(items[['item_id','item_category_id']], how='left', on='item_id')\n\n# Number of categories sold each year\ndfp = dfp.groupby(['year', 'item_category_id'])\\\n       .agg({'item_cnt_day':'sum'})\\\n       .rename(columns={'item_cnt_day':'item_cnt_year'}, inplace=False)\n\n# Top N categories sold \ntop = dfp['item_cnt_year'].groupby('year', group_keys=False).nlargest(N)\n# Convert top to a dataframe\ntop = pd.DataFrame(top).reset_index()\n# Add category type to be plotted lated\ntop = top.merge(cats[['item_category_id','item_category_name']], how='left', on='item_category_id')\n\n# To print the top selling categories for each year\n#print(top)\n\nyears = [2013, 2014, 2015]\nfig, axes = plt.subplots(1, 3, figsize=(16,6))\n\n#Prepare colors for the top N\ncolors = [[] for i in range(3)]\nfor alpha in np.arange(N, 0, -1)\/N:\n    colors[0].append((.275, .51, .706, alpha))\n    colors[1].append((.18, .55, .34, alpha))\n    colors[2].append((.5, 0, 0, alpha))\n    \nfor ax, year, cs in zip(axes, years, colors):\n    # Get top items for each year\n    year_filter = top[top.year==year]\n    plot_sizes = year_filter.item_cnt_year\n    plot_labels = year_filter.item_category_name.str[:15]#+'('+plot_sizes.astype(str)+')'\n    \n    # Get the ratio\n    print(get_ratio(year, plot_sizes.sum(), N))\n    \n    # Plot the pie\n    ax.pie(plot_sizes, labels=plot_labels, radius=1.5, colors=cs,labeldistance=.5, rotatelabels=True, startangle=90, wedgeprops={\"edgecolor\":\"1\",'linewidth': .5})\n    # Set titles below pies\n    ax.set_title(year, y=-0.2)\n\n# Space pies\nfig.tight_layout()\nfig.suptitle('Top selling categories for each year', fontsize=16)\nplt.show()\n\ndel dfp","d30e6457":"fig, axes = plt.subplots(2, 1)\nplt.subplots_adjust(hspace=0.5)\n\nflierprops = dict(marker='o', markerfacecolor='cornflowerblue', markersize=6, markeredgecolor='navy')\n\n_ = axes[0].boxplot(x=sales.item_cnt_day, flierprops=flierprops, vert=False)\n_ = axes[1].boxplot(x=sales.item_price, flierprops=flierprops, vert=False)\n\n_ = axes[0].set_title('item_cnt_day')\n_ = axes[1].set_title('item_price')","6a3169d6":"sales = sales[(sales.item_price<100000)&(sales.item_price>0)]\nsales = sales[(sales.item_cnt_day>0)&(sales.item_cnt_day<1000)]\n\n# Remove duplicate shops\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales.loc[sales.shop_id==0, 'shop_id'] = 57\ntest.loc[test.shop_id==0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales.loc[sales.shop_id==1, 'shop_id'] = 58\ntest.loc[test.shop_id==1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales.loc[sales.shop_id==10, 'shop_id'] = 11\ntest.loc[test.shop_id==10, 'shop_id'] = 11","2290eebd":"# Correct the name of a shop\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n# The first part of the shop_name is the city e.g. \u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 (Serguiev Possad)\nshops[\"shop_city\"] = shops.shop_name.str.split(' ').map(lambda x: x[0])\n# The second part of the shop_name is the category e.g. \u0422\u0426 (shopping center)\nshops[\"shop_category\"] = shops.shop_name.str.split(\" \").map(lambda x: x[1])\nshops.loc[shops.shop_city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"shop_city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\" #(Iakoutsk)\n# Feature encoding\nshops[\"shop_city\"] = LabelEncoder().fit_transform(shops.shop_city)\nshops[\"shop_category\"] = LabelEncoder().fit_transform(shops.shop_category)\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\nshops.head()","b23ab033":"item_categories[\"category_type\"] = item_categories.item_category_name.apply(lambda x: x.split(\" \")[0]).astype(str)\n# The category_type \"Gamming\" and \"accesoires\" becomes \"Games\"\nitem_categories.loc[(item_categories.category_type==\"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")|(item_categories.category_type==\"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category_type\"] = \"\u0418\u0433\u0440\u044b\"\nitem_categories[\"split\"] = item_categories.item_category_name.apply(lambda x: x.split(\"-\"))\nitem_categories[\"category_subtype\"] = item_categories.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n# Feature encoding\nitem_categories[\"category_type\"] = LabelEncoder().fit_transform(item_categories.category_type)\nitem_categories[\"category_subtype\"] = LabelEncoder().fit_transform(item_categories.category_subtype)\nitem_categories = item_categories[[\"item_category_id\", \"category_type\", \"category_subtype\"]]\nitem_categories.head()","6fa5fa0a":"sales = sales.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\\\n          .agg({'item_cnt_day':'sum'})\\\n          .rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)\n        \ntest['date_block_num'] = 34\ntest['item_cnt_month'] = 0\ndel test['ID']\n\ndf = sales.append(test)\ndf","44993de8":"# check that the shop\/item pairs in the test set are represented in the matrix\n#missing = test[(~test.shop_id.isin(matrix[matrix.date_block_num<34].shop_id))|(~test.item_id.isin(matrix[matrix.date_block_num<34].item_id))]\n#missing[['shop_id', 'item_id']]","1f4e8752":"matrix = []\n# Try creating a matrix of product(sales['date_block_num'].unique(), sales.shop_id.unique(), sales.item_id.unique()) which are about 45m lines\nfor num in df['date_block_num'].unique(): \n    tmp = df[df.date_block_num==num]\n    matrix.append(np.array(list(product([num], tmp.shop_id.unique(), tmp.item_id.unique())), dtype='int16'))\n    #matrix.append(np.array(list(product([num], shops.shop_id, items.item_id)), dtype='int16'))\n\n# Turn the grid into a dataframe\nmatrix = pd.DataFrame(np.vstack(matrix), columns=['date_block_num', 'shop_id', 'item_id'], dtype=np.int16)\n\n# Add the features from sales data to the matrix\nmatrix = matrix.merge(df, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\n\n#Merge features from shops, items and item_categories:\nmatrix = matrix.merge(shops, how='left', on='shop_id')\nmatrix = matrix.merge(items[['item_id','item_category_id']], how='left', on='item_id')\nmatrix = matrix.merge(item_categories, how='left', on='item_category_id')\n\n# Add month\nmatrix['month'] = matrix.date_block_num%12\n# Clip counts\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0, 20)","503da76a":"# Set columns types to control the matrix' size\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix['month'] = matrix['month'].astype(np.int8)\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].astype(np.int32)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['category_type'] = matrix['category_type'].astype(np.int8)\nmatrix['category_subtype'] = matrix['category_subtype'].astype(np.int8)\nmatrix","0936f8b7":"print('{0:.2f}'.format(matrix.memory_usage(index=False, deep=True).sum()\/(2**20)), 'MB')","a1f14e15":"def lag_feature(df, lags, col):\n    print(col)\n    for i in lags:\n        shifted = df[[\"date_block_num\", \"shop_id\", \"item_id\", col]].copy()\n        shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col+\"_lag_\"+str(i)]\n        shifted.date_block_num += i\n        df = df.merge(shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n    return df","684a9e35":"# lag the target item_cnt_month\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'item_cnt_month')","ff7a5447":"# shop\/date_block_num aggregates lags\ngb = matrix.groupby(['shop_id', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_shop'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_shop')\nmatrix.drop('cnt_block_shop', axis=1, inplace=True)","2c94d815":"# item\/date_block_num aggregates lags\ngb = matrix.groupby(['item_id', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_item'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_item')\nmatrix.drop('cnt_block_item', axis=1, inplace=True)","6b7a9836":"# category\/date_block_num aggregates lags\ngb = matrix.groupby(['category_type', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_category'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['category_type', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_category')\nmatrix.drop('cnt_block_category', axis=1, inplace=True)","661f1d48":"# matrix.to_csv('matrix.csv', index=False)\n# matrix = pd.read_csv('matrix.csv')\nmatrix","f6bd0439":"from sklearn.preprocessing import StandardScaler\n\ndef standard_mean_enc(df, col):\n    mean_enc = df.groupby(col).agg({'item_cnt_month': 'mean'})\n    scaler = StandardScaler().fit(mean_enc)\n    return {v: k[0] for v, k in enumerate(scaler.transform(mean_enc))}","b6b9b6bc":"cols_to_mean_encode = ['shop_category', 'shop_city', 'item_category_id', 'category_type', 'category_subtype']\n\nfor col in cols_to_mean_encode:\n    # Train on the train data\n    mean_enc = standard_mean_enc(matrix[matrix.date_block_num < 33].copy(), col) # X_train, y_train\n    # Apply to Train, Validation and Test\n    matrix[col] = matrix[col].map(mean_enc)\nmatrix","359614be":"# Remove the 2013's sales data\nmatrix = matrix[matrix.date_block_num>=12] \nmatrix.reset_index(drop=True, inplace=True)\nmatrix","ad45d83a":"X_train = matrix[matrix.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = matrix[matrix.date_block_num < 33]['item_cnt_month']\nX_val = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val =  matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)","0074747d":"X_train.drop('date_block_num', axis=1, inplace=True)\nX_val.drop('date_block_num', axis=1, inplace=True)\nX_test.drop('date_block_num', axis=1, inplace=True)","4c6adda2":"splits = []\nfor block in [27, 28, 29, 30, 31, 32]:\n    train_idxs = matrix[matrix.date_block_num < block].index.values\n    test_idxs = matrix[matrix.date_block_num == block].index.values\n    splits.append((train_idxs, test_idxs))\nsplits","17527583":"from sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\n\nhyper_params = {'max_depth': [3, 4, 5, 6, 7, 8, 9], \n                'gamma': [0, 0.5, 1, 1.5, 2, 5], \n                'subsample': [0.6, 0.7, 0.8, 0.9, 1], \n                'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1], \n                'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                'max_bin' : [256, 512, 1024]\n               }\n\nxgbr = XGBRegressor(seed = 13, tree_method = \"hist\") #gpu_hist\nclf = RandomizedSearchCV(estimator = xgbr, \n                   param_distributions = hyper_params,\n                   n_iter = 2, #500\n                   scoring = 'neg_root_mean_squared_error',\n                   cv = splits,\n                   verbose=3)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters:\", clf.best_params_)\nprint(\"Lowest RMSE: \", -clf.best_score_)","bb2a0050":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nyhat_val_lr = lr.predict(X_val).clip(0, 20)\nprint('Validation RMSE:', mean_squared_error(y_val, yhat_val_lr, squared=False)) #Validation RMSE: 0.9645168655662343\nyhat_test_lr = lr.predict(X_test).clip(0, 20)","6d03e40c":"from xgboost import XGBRegressor\n\nts = time.time()\n\nxgb = XGBRegressor(seed = 13, \n    tree_method = \"hist\", #gpu_hist\n    subsample = 0.9,\n    max_depth = 9,\n    learning_rate = 0.1,\n    gamma = 2,\n    colsample_bytree = 0.9\n    )\nxgb.fit(\n    X_train,y_train,\n    eval_metric=\"rmse\",\n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    verbose=True,\n    early_stopping_rounds = 10\n    )\nprint('Training took: {0}s'.format(time.time()-ts))\nyhat_val_xgb = xgb.predict(X_val).clip(0, 20)\nprint('Valdation RMSE:', mean_squared_error(y_val, yhat_val_xgb, squared=False)) #Valdation RMSE: 0.9273184120626018\nyhat_test_xgb = xgb.predict(X_test).clip(0, 20)","16519f64":"import pickle\npickle.dump(xgb, open(\"xgboost.pickle.dat\", \"wb\"))\n#loaded_model = pickle.load(open(\"xgboost_base.pickle.dat\", \"rb\"))","710c312f":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xgb, (10,14))","4de65e1e":"y_train_meta = matrix[matrix.date_block_num.isin([27, 28, 29, 30, 31, 32])].item_cnt_month","12d841ee":"X_train_meta = [[],[]]\nfor block in [27, 28, 29, 30, 31, 32]:\n    print('Block:', block)\n    # X and y Train for blocks from 12 to block\n    X_train_block = matrix[matrix.date_block_num < block].drop(['date_block_num', 'item_cnt_month'], axis=1)\n    y_train_block = matrix[matrix.date_block_num < block].item_cnt_month\n    # X and y Test for block\n    X_val_block = matrix[matrix.date_block_num == block].drop(['date_block_num', 'item_cnt_month'], axis=1)\n    #y_test_block = matrix[matrix.date_block_num == block].item_cnt_month\n    \n    # Fit first model \n    print(' LR fitting ...')\n    lr.fit(X_train_block, y_train_block)\n    print(' LR fitting ... done')\n    # Append prediction results on X_val_block to X_train_meta (first column)\n    X_train_meta[0] += list(lr.predict(X_val_block).clip(0, 20))\n    \n    # Fit second model\n    print(' XGB fitting ...')\n    xgb.fit(\n        X_train_block, y_train_block,\n        eval_metric=\"rmse\",\n        eval_set=[(X_train_block, y_train_block)],\n        #eval_set=[(X_train_block, y_train_block), (X_val_block, y_test_block)],\n        verbose=0,\n        early_stopping_rounds = 10\n    )\n    print(' XGB fitting ... done')\n    # Append prediction results on X_val_block to X_train_meta (second column)\n    X_train_meta[1] += list(xgb.predict(X_val_block).clip(0, 20))\n# Turn list into dataframe\nX_train_meta = pd.DataFrame({'yhat_lr': X_train_meta[0], 'yhat_xgb': X_train_meta[1]})","d8dbffe7":"plt.scatter(X_train_meta.yhat_lr, X_train_meta.yhat_xgb)","e790f3b9":"stacking = LinearRegression()\nstacking.fit(X_train_meta, y_train_meta)\n\n#Squared: If True returns MSE value, if False returns RMSE value.\nyhat_train_meta = stacking.predict(X_train_meta).clip(0, 20)\nprint('Meta Training RMSE:', mean_squared_error(y_train_meta, yhat_train_meta, squared=False))\n# Meta Training RMSE: 0.813971713370181\n\nyhat_val_meta = stacking.predict(np.vstack((yhat_val_lr, yhat_val_xgb)).T).clip(0, 20)\nprint('Meta Validation RMSE:', mean_squared_error(y_val, yhat_val_meta, squared=False))\n# Meta Validation RMSE: 0.9184725317670576\n\nyhat_test_meta = stacking.predict(np.vstack((yhat_test_lr, yhat_test_xgb)).T).clip(0, 20)","4b568463":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": yhat_test_meta\n})\nsubmission.to_csv('submission_stacking.csv', index=False)\n# Public score 0.92466","5d2b6292":"## Linear Regression","a4f379fb":"Add 'city' and 'category' to shops:\\\n-The first part of the **shop_name** is the city e.g. \u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 (Serguiev Possad)\\\n-The second part of the **shop_name** is the category e.g. \u0422\u0426 (shopping center)","0d568582":"Compute monthly sales, in the same representation as the test data:","5c71eeb2":"# Model training","29dd24ef":"Plot the number of items sold from Jan to Dec for the years 2013, 2014 and 2015:","3887b6d8":"# Exploratory Data Analysis","376e5f54":"# Ensembling","98be0039":"# Feature engineering","6d81f302":"Mean encoding and scaling : first split the data into Train and Validation, estimate encodings on Train, then apply them to Validation set:","c1c24950":"This is my attempt at solving the problem of predicting future sales.\\\nThis notebook is inspired by the work of other participants. A huge thanks to all, I learned a lot from your code.\\\nI'm completing this task as part of HSE University's course on coursera \"How to Win a Data Science Competition\".","c3c149b6":"Remove a few columns:","beebf3e5":"Removing outliers from **item_price** and **item_cnt_day**, and duplicate **shops**:","0dc2abf4":"# Train\/Validation\/Test split","1caea8a4":"# Hyperparameters tuning","4532dd28":"Best selling categories for each year:","a326ef08":"# Data preparation","311aba16":"# Submission","2602a2d7":"Add **type** and **subtype** to **item_categories**:","46973f55":"# Modules imports","c60ac898":"Plot XGBoost's features importance:","c56c610c":"## XGBoost","77688220":"Serialize and Deserialize the XGBoost model with Pickle:","7880f113":"## Train meta-features","c68a0478":"Create a feature matrix:","01ec0ea6":"## Label mean encodings","683341ab":"## Lagged features","74856391":"# Data loading","47339083":"# Introduction","6ff885f3":"Detecting outliers in **item_price** and **item_cnt_day**","cf335d60":"## Stacking","de79fb88":"# Plan\n* Introduction\n* Modules imports\n* Data loading\n* EDA\n* Data preparation\n* Feature engineering\n* Train\/Validation\/Test split\n* Hyperparameters tuning\n* Model training\n* Ensembling\n* Submission\n\n\n\n"}}