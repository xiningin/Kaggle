{"cell_type":{"f69f4334":"code","35342d46":"code","95632ba6":"code","366863e3":"code","1ee4e22f":"code","31d0df15":"code","9d37c935":"code","6c778eb0":"code","ffb28994":"code","383d1a5f":"code","63719100":"code","b679cdad":"code","c913a7bb":"code","a8e1dc08":"code","e85e059e":"code","9f37b997":"code","30f77954":"markdown","3faede31":"markdown","dfc2b494":"markdown","5304f06d":"markdown","b4015d80":"markdown","28156118":"markdown","c8bfda3a":"markdown","e5a2758d":"markdown","6a4b0713":"markdown","6e1242d8":"markdown","f9b83436":"markdown","c11b0367":"markdown","7f02d555":"markdown"},"source":{"f69f4334":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35342d46":"train_filepath = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_filepath = '..\/input\/home-data-for-ml-course\/test.csv'\n\nX_full = pd.read_csv(train_filepath, index_col='Id', parse_dates=True)\nX_test_full = pd.read_csv(test_filepath, index_col='Id', parse_dates=True)\n\n# Test data has no 'target' column\nf_string = f'''The difference between train_data and test_data is {set(X_full.columns) - \nset(X_test_full.columns)}.\\n'''\nprint(f_string)\n\n# Remove rows from train_data with empty target, and check\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n# print(X_full.SalePrice.isnull().any())\n\n# Separate target (y) from predictors (X)\ny = X_full.SalePrice\nX_full.drop('SalePrice', axis=1, inplace=True)\n\n# At this point, we want to identify our numerical and categorical columns, for later reference.\nprint(f\"Unique datatypes: {set([X_full[col].dtype for col in X_full.columns])}\\n\")\n\n# Identify numerical cols\nnumerical_cols = [col for col in X_full.columns\n                 if X_full[col].dtypes in ['int64', 'float64']]\nprint('Number of numerical columns:', len(numerical_cols))\n\n# Identify categorical cols (upper-bound on cardinality)\ncardinality = 10\ncategorical_cols = [col for col in X_full.columns\n                   if X_full[col].dtypes == 'object'\n                   and X_full[col].nunique() <= cardinality]\n\nremoved_cols = list(set(X_full.columns) - set(categorical_cols) - set(numerical_cols))\n\nprint('Number of categorical columns:', len(categorical_cols), \\\n      f\"(Removed {removed_cols})\\n\")\n\n# Remove columns with high cardinality from X_full and X_test_full\nX = pd.concat([X_full[numerical_cols], X_full[categorical_cols]], axis=1)\nX_test = X_test_full.drop(removed_cols, axis=1)\nprint('Total columns:', len(X.columns))","95632ba6":"print('Column Name\\t  No. unique values\\n-----------------------------------')\nprint(X_full.select_dtypes(include=['object']).nunique().sort_values(ascending=True))","366863e3":"# Lets list the number of missing values to see if it's worth it to drop the whole column!\nprint(f\"We're losing {len(X.isnull().sum()[X.isnull().any()].index)} columns.\\n\")\nprint('Column name\\tNumber of missing values\\n----------------------------------------')\nprint(f'No. of entries:\\t{len(X_full.index)}\\n----------------------------------------')\nprint(X.isnull().sum()[X.isnull().any()].sort_values(ascending=False))","1ee4e22f":"# Drop columns with missing values\nX_drop = X.dropna(axis=1)\nprint(f\"{len(set(X.columns) - set(X_drop.columns))} columns dropped.\")","31d0df15":"# Create instance of SimpleImputer object\nimputer = SimpleImputer(strategy='mean')\n\n# Impute missing values of categorical columns with 'Unknown'\nX_impute_cat = X.select_dtypes(include=['object']).fillna('Unknown')\nX_impute_cat.head()\n\n# Impute missing values of numerical columns with mean value\nX_impute_num = pd.DataFrame(imputer.fit_transform(X[numerical_cols]))\n\n# Add back column names and index for numerical columns\nX_impute_num.columns = numerical_cols\nX_impute_num.set_index(X.index, inplace=True)\nX_impute_num.head()\n\n# Rejoin X_impute\nX_impute = X_impute_num.join(X_impute_cat)\n\n# Check that order of columns is preserved after imputation\nprint(f\"Columns in the right order: {(X_impute.columns == X.columns).all()}\")\n\n# Inspect X_impute\nX_impute[numerical_cols].describe()\nX_impute[categorical_cols].describe()\nX_impute.head()","9d37c935":"# Copy X to avoid adjusting original\nX_impute_plus = X.copy()\n\n# Create list containing\n\n# Add columns to indicate imputation\nfor col in list(X.isnull().any()[X.isnull().any()].index):\n    X_impute_plus[col + '_was_imputed'] = X[col].isnull()\n\nprint(len(set(X_impute_plus.columns) - set(X.columns)), 'columns were added:\\n')\nprint(list(set(X_impute_plus.columns) - set(X.columns)))\n\n# Impute categorical columns with 'Unknown'\nX_impute_plus_cat = X.select_dtypes(include=['object']).fillna('Unknown')\nX_impute_plus_cat.head()\n\n# Create instance of SimpleImputer\nimputer_plus = SimpleImputer(strategy='mean')\n\n# Impute numerical columns with mean value\nX_impute_plus_num = pd.DataFrame(imputer_plus.fit_transform(X[numerical_cols]))\n\n# Add back column and names for numerical columns\nX_impute_plus_num.set_index(X.index, inplace=True)\nX_impute_plus_num.columns = numerical_cols\nX_impute_plus_num.head()\n\n# Rejoin X_impute_plus\nX_impute_plus = pd.concat([X_impute_plus_num, X_impute_plus_cat, \\\n                           X_impute_plus[list(set(X_impute_plus.columns) - set(X.columns))]], axis=1)\n\n# New datatypes in X_impute_plus\nprint('\\nDatatypes in X_impute_plus:', list(X_impute_plus.apply(lambda df: df.dtypes).unique()))\n\n# Inspect X_impute_plus\nX_impute_plus.select_dtypes(include=['object']).describe()\nX_impute_plus.select_dtypes(include=['int64', 'float64']).describe()\nX_impute_plus.head()","6c778eb0":"# Select cols_was_imputed for categorical columns\ncategorical_cols_was_imputed = [col + '_was_imputed' for col in categorical_cols]\ncategorical_cols_was_imputed = set(categorical_cols_was_imputed) & set(X_impute_plus.columns)\ncategorical_cols_was_imputed\n\nprint(f\"Out of 19 _was_imputed cols, {len(categorical_cols_was_imputed)} were categorical.\")\n\n# Drop categorical columns from X_drop\nX_drop_drop = X_drop.select_dtypes(exclude=['object'])\nX_drop_drop.head()\n\n# Drop categorical columns from X_impute\nX_impute_drop = X_impute.select_dtypes(exclude=['object'])\nX_impute_drop.head()\n\n# Drop categorical columns from X_impute_plus\nX_impute_plus_drop = X_impute_plus.drop(categorical_cols_was_imputed, axis=1)\nX_impute_plus_drop = X_impute_plus_drop.select_dtypes(exclude=['object'])\nX_impute_plus_drop.head()","ffb28994":"'''\nResults:\nThere are NaN values in X_test. Help! Do we need to replicate the whole workflow for the\nX_test data as well?\n\nActually, this will be fine, because the difference is at most {Nan}.\n\nOnce we choose the best pre-processing workflow, we can apply the missing data workflow \nto the X_test data. That means we won't have this issue.\n\nIf we did find unique values in the X_test categorical columns (and not in the X_categoric-\nal columns, then we would need to remove the column entirely, or choose a new X_test.\n'''\n\n# Doing a little investigation, we forgot to remove NaN from categorical cols in X_test\nfor col in categorical_cols:\n    if not (set(X_test[col].unique()) <= set(X[col].unique())):\n        print(col)\n        print('Difference:\\t', set(X_test[col].unique()) - set(X[col].unique()))\n        print('Set of X_test\\t', set(X_test[col].unique()))\n        print('Set of X\\t', set(X[col].unique()), '\\n')","383d1a5f":"# Create instance of LabelEncoder object\nlabel_encoder = LabelEncoder()\n\n# Label encode X_impute_plus\nX_impute_plus_LE = X_impute_plus.copy()\n\nfor col in X_impute_plus_LE:\n    if col in categorical_cols:\n        X_impute_plus_LE[col] = label_encoder.fit_transform(X_impute_plus[col])\n\nX_impute_plus_LE.head()\nX_impute_plus_LE.apply(lambda df: df.dtypes).unique()\n\n# Label encode X_impute\nX_impute_LE = X_impute.copy()\n\nfor col in X_impute_LE:\n    if col in categorical_cols:\n        X_impute_LE[col] = label_encoder.fit_transform(X_impute[col])\n\nX_impute_LE.head()\nX_impute_LE.apply(lambda df: df.dtypes).unique()\n\n# Label encode X_drop\nX_drop_LE = X_drop.copy()\n\nfor col in X_drop_LE:\n    if col in categorical_cols:\n        X_drop_LE[col] = label_encoder.fit_transform(X_drop[col])\n\nX_drop_LE.head()\nX_drop_LE.apply(lambda df: df.dtypes).unique()","63719100":"# Create instance of OneHotEncoder object\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# One-hot encode X_impute_plus\nX_impute_plus_OH = X_impute_plus.copy()\nnew_col_number = sum(X_impute_plus_OH[categorical_cols].apply(lambda df: df.nunique()))\nprint(f\"We currently have {len(X_impute_plus_OH.columns)} columns.\")\nprint(f\"We will end up with {len(X_impute_plus_OH.columns) - len(categorical_cols) + new_col_number} columns.\")\n\nX_impute_plus_one_hot = pd.DataFrame(OH_encoder.fit_transform(X_impute_plus_OH[categorical_cols]))\nX_impute_plus_one_hot.index = X_impute_plus_OH.index # Add back the index\nX_impute_plus_OH.drop(categorical_cols, axis=1, inplace=True) # Remove categorical columns\nX_impute_plus_OH = pd.concat([X_impute_plus_OH, X_impute_plus_one_hot], axis=1) # Add one-hot columns\nprint(f\"We ended up with {len(X_impute_plus_OH.columns)} columns.\\n\")\n\n# One-hot encode X_impute\nX_impute_OH = X_impute.copy()\nnew_col_number = sum(X_impute_OH[categorical_cols].apply(lambda df: df.nunique()))\nprint(f\"We currently have {len(X_impute_OH.columns)} columns.\")\nprint(f\"We will end up with {len(X_impute_OH.columns) - len(categorical_cols) + new_col_number} columns.\")\n\nX_impute_one_hot = pd.DataFrame(OH_encoder.fit_transform(X_impute_OH[categorical_cols]))\nX_impute_one_hot.index = X_impute_OH.index # Add back the index\nX_impute_OH.drop(categorical_cols, axis=1, inplace=True) # Remove categorical columns\nX_impute_OH = pd.concat([X_impute_OH, X_impute_one_hot], axis=1) # Add one-hot columns\nprint(f\"We ended up with {len(X_impute_OH.columns)} columns.\\n\")\n\n# One-hot encode X_drop\nX_drop_OH = X_drop.copy()\nnew_col_number = sum(X_drop_OH.select_dtypes(include=['object']).apply(lambda df: df.nunique()))\nprint(f\"We currently have {len(X_drop_OH.columns)} columns.\")\nprint(f\"We will end up with {len(X_drop_OH.columns) - len(X_drop_OH.select_dtypes(include=['object']).columns) + new_col_number} columns.\")\n\nX_drop_one_hot = pd.DataFrame(OH_encoder.fit_transform(X_drop_OH.select_dtypes(include=['object'])))\nX_drop_one_hot.index = X_drop_OH.index # Add back the index\nX_drop_OH.drop(X_drop_OH.select_dtypes(include=['object']).columns, axis=1, inplace=True) # Remove categorical columns\nX_drop_OH = pd.concat([X_drop_OH, X_drop_one_hot], axis=1) # Add one-hot columns\nprint(f\"We ended up with {len(X_drop_OH.columns)} columns.\")","b679cdad":"# Create a quick model:\nmodel = RandomForestRegressor(n_estimators=100, random_state=1)\n\n# List containing all the datasets:\ndatasets = [X_drop_drop, X_impute_drop, X_impute_plus_drop,\n           X_drop_LE, X_impute_LE, X_impute_plus_LE,\n           X_drop_OH, X_impute_OH, X_impute_plus_OH]\n\n# Find the cross validation score:\nscores = list()\n\nfor dataset in datasets:\n    scores.append(-cross_val_score(model, dataset, y, cv=5, verbose=1, scoring='neg_mean_absolute_error'))","c913a7bb":"score_dict = dict(zip(['X_drop_drop', 'X_impute_drop', 'X_impute_plus_drop',\n                       'X_drop_LE', 'X_impute_LE', 'X_impute_plus_LE',\n                       'X_drop_OH', 'X_impute_OH', 'X_impute_plus_OH'], scores))\n\n# sort(score_dict)\n\nprint(f\"{'-'*31}\\n{'Dataset' : <20}{'Score' : ^14}\\n{'-'*31}\")\nfor key in score_dict.keys():\n    print(f\"{key : <20}{score_dict[key].mean() : ^14.6}\")","a8e1dc08":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Handle numerical and categorical predictors separately\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine both into a preprocessor step\npreprocessing = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\n# Define our model\nmodel = RandomForestRegressor(n_estimators=100, random_state=1)\n\n# Create our pipeline\npipeline = Pipeline(steps=[\n    ('preprocessing', preprocessing),\n    ('model', model)\n])","e85e059e":"# Test and fit our model\nresults = -cross_val_score(pipeline, X, y, cv=5, verbose=1, scoring='neg_mean_absolute_error')\n\nprint(f\"{'-'*31}\\n{'Dataset' : <20}{'Score' : ^14}\\n{'-'*31}\")\nprint(f\"{'Pipeline' : <20}{results.mean() : ^14.6}\")","9f37b997":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Make a copy to avoid changing the original\nX_xgb = X.copy()\n\nX_xgb_OH = OneHotEncoder(sparse=False, handle_unknown='ignore')\n\n# XGBRegressor can't handle categoricals easily, one-hot encode them first\nX_xgb_OH = pd.DataFrame(X_xgb_OH.fit_transform(X_xgb[categorical_cols]))\n\n# One-hot encoding removes indexes; add them back\nX_xgb_OH.index = X_xgb.index\n\n# Drop categorical columns from X_xgb, we'll add the one-hot encoded version after\nX_xgb.drop(categorical_cols, axis=1, inplace=True)\n\n# Add X_xgb_OH\nX_xgb = pd.concat([X_xgb, X_xgb_OH], axis=1)\n\n# Split training data and validation data, for the purpose of training our model\nX_train, X_valid, y_train, y_valid = train_test_split(X_xgb, y, train_size=0.8, test_size=0.2,\\\n                                                     random_state=1)\n# Create instance of XGBRegressor object\nxgb = XGBRegressor(n_estimators=200, learning_rate=0.10)\n\n# Fit the data\nxgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=0)\n\n# Cross-validation score\nresults = -cross_val_score(xgb, X_xgb, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(f\"{'-'*31}\\n{'Dataset' : <20}{'Score' : ^14}\\n{'-'*31}\")\nprint(f\"{'XGBRegressor' : <20}{results.mean() : ^14.6}\")","30f77954":"## Step 3: Cross-validation\nInstead of `train_test_split`, we can use `cross_val_score`, which will evaluate the loss function, while iterating over segments of the dataset as the 'validation data'.","3faede31":"## Step 4: Pipelines\nUsing pipelines, we can try to simplify the process futher. However, we cannot do things like impute, and then add columns specifying which rows were imputed.\n\nTo recap, we start with `X` and `X_test` as our uncleaned datasets. The resulting dataset is equivalent to `X_impute_OH`.","dfc2b494":"## Step 3: Dealing with categorical values\nWe have, again, 3 options:\n* Drop categorical values\n* Label encoding\n* One-hot encoding\n\n### Drop columns with categorical values\n","5304f06d":"### One-hot encoding\n\nOne-hot encoding removes the index. We will replace it.","b4015d80":"## What's next: XGBoost ensemble estimators\n\nAn XGBoost Regressor is an ensemble of estimators (i.e. it takes the sum of the predictions of multiple independent estimators). The workflow goes something like:\n\n1. Pick a model, and parametize it at random\n2. Calculate the loss function\n3. Use gradient descent to calculate the change in parameters that would most reduce the loss function\n4. Use new parameters in new estimator and add it to the ensemble\n\nThe parameters are:\n* `n_estimators`: usually between 100 and 1000, also equal to the number of iterations\n* `learning_rate`: default = 0.10; the smaller the value, the more accurate the gradient calculated, but the longer the training time\n\nThe parameters of XGBRegressor().fit() are:\n* `early_stopping_rounds`: number of consecutive deprovements before halt\n* `eval_set = [(X_valid, y_valid)]` to evaluate against to determine whether to halt\n* `eval_metric`: 'mae', etc.","28156118":"## Step 2: Deal with missing values\n\nAt this point, we have the following issues with our data:\n* Missing values (numerical, categorical)\n* Categorical values (categorical only)\n\nLet's start with missing values.\n\nThere are 3 options:\n* Drop the data\n* Impute missing values\n* Impute missing values and \n\n### Dropping columns with missing values\nIt might not be a good idea to drop rows with missing values, as it might introduce some bias into the data (there might be a systematic bias to rows with missing values in certain columns).\n\nAdditionally, some columns might have many missing values, so if we drop rows we might not have much data left.\n","c8bfda3a":"## Step 1: Read and explore data","e5a2758d":"### Before encoding: choose good columns\nWe only want to choose columns where each value in the test data is represented in the training data. For this, we compare the `categorical_cols` in `X` with `X_test`.","6a4b0713":"### Label encode columns with categorical values\n\nLabelEncoder() objects only encode 1 column. We will use a `for` loop to do this.\n\n> Note: LabelEncoder only replaces one column at a time; 'memory' is lost after each iteration. However, this is not a problem; we just have to re-fit_transform the label encoder to the training dataset we eventually use before transforming the label of the test data. Something like:\n\n```\nfor col in [X_test_MV_removed_LE]:\n    if col in categorical_cols:\n        X_MV_removed_LE[col] = label_encoder.fit_transform(X_MV_removed)\n        X_test_MV_removed_LE[col] = label_encoder.transform(X_test_MV_removed)\n```","6e1242d8":"# Recap: Intermediate Machine Learning\n\nThe aim of this notebook is to practice running through the workflows covered in the Intermediate Machine Learning course.\n\n## Scope\n1. Pre-processing (missing and categorical data)\n    - Drop missing and\/or categorical data\n    - Impute missing data (optional: label imputed columns)\n    - Label encoding\n    - One-hot encoding\n2. Pipelines\n3. Cross-validation\n4. XGBoost\n\n## Components and their methods\n```\n    \u251c\u2500\u2500 Missing values\n    \u2502   \u251c\u2500\u2500 (1) Remove values\n    \u2502   \u2502   \u251c\u2500\u2500 isnull().any()\n    \u2502   \u2502   \u251c\u2500\u2500 isnull().sum()\n    \u2502   \u2502   \u251c\u2500\u2500 dropna(axis)\n    \u2502   \u2502   \u2514\u2500\u2500 drop(colsList, axis)\n    \u2502   \u251c\u2500\u2500 (2) Impute values\n    \u2502   \u2502   \u2514\u2500\u2500 SimpleImputer()\n    \u2502   \u2502       \u251c\u2500\u2500 fit_transform(X)\n    \u2502   \u2502       \u2514\u2500\u2500 transform(X)\n    \u2502   \u2514\u2500\u2500 (3) Impute values & track imputed values\n    \u2502       \u251c\u2500\u2500 SimpleImputer()\n    \u2502       \u2514\u2500\u2500 astype() --- convert (0, 1) back to bool\n    \u251c\u2500\u2500 Categorical variables\n    \u2502   \u251c\u2500\u2500 (1) Remove values\n    \u2502   \u2502   \u251c\u2500\u2500 dtypes\n    \u2502   \u2502   \u2514\u2500\u2500 select_dtypes(exclude\/include)\n    \u2502   \u251c\u2500\u2500 (2) Label encoding\n    \u2502   \u2502   \u251c\u2500\u2500 LabelEncoder()\n    \u2502   \u2502   \u2514\u2500\u2500 set() --- generates unique values\n    \u2502   \u2514\u2500\u2500 (3) One-hot encoding\n    \u2502       \u251c\u2500\u2500 OneHotEncoder()\n    \u2502       \u251c\u2500\u2500 pd.concat([pdList], axis=1)\n    \u2502       \u2514\u2500\u2500 nunique() --- unique().count(), cardinality\n    \u251c\u2500\u2500 Cross-validation\n    \u2502   \u2514\u2500\u2500 cross_val_score(pipeline, X, y, cv, scoring)\n    \u2502       \u2514\u2500\u2500 scoring = 'neg_mean_absolute_error'\n    \u2514\u2500\u2500 XGBoost\n```\n\n## Modules and their methods\n```\n\u251c\u2500\u2500 sklearn\n\u2502   \u251c\u2500\u2500 impute\n\u2502   \u2502   \u2514\u2500\u2500 SimpleImputer(strategy, *fill_value)\n\u2502   \u2502       \u2514\u2500\u2500 strategy = 'mean', 'median', 'mostfrequent', 'constant'\n\u2502   \u251c\u2500\u2500 preprocessing\n\u2502   \u2502   \u251c\u2500\u2500 LabelEncoder()\n\u2502   \u2502   \u2514\u2500\u2500 OneHotEncoder(handle_unknown='unknown', sparse=False)\n\u2502   \u251c\u2500\u2500 pipeline\n\u2502   \u2502   \u2514\u2500\u2500 Pipeline(steps) --- 'vertical' integration\n\u2502   \u2502       \u251c\u2500\u2500 steps = [('name', object), ...]\n\u2502   \u2502       \u251c\u2500\u2500 .fit(X, y)\n\u2502   \u2502       \u2514\u2500\u2500 .predict(X)\n\u2502   \u251c\u2500\u2500 compose\n\u2502   \u2502   \u2514\u2500\u2500 ColumnTransformer(transformers) --- 'horizontal' integration\n\u2502   \u2502       \u2514\u2500\u2500 transformers = [('name', object, colList), ...]\n\u2502   \u2514\u2500\u2500 model_selection\n\u2502       \u2514\u2500\u2500 cross_val_score(pipeline, X, y, cv, scoring)\n\u2502           \u2514\u2500\u2500 scoring = 'neg_mean_absolute_error'\n\u2514\u2500\u2500 xgboost\n    \u2514\u2500\u2500 XGBRegressor(n_estimators, learning_rate)\n        \u2514\u2500\u2500 .fit(X_train, y_train, args)\n            \u251c\u2500\u2500 early_stopping_rounds: # consecutive deprovements before halt\n            \u2514\u2500\u2500 eval_set = [(X_valid, y_valid)]\n```","f9b83436":"### Impute columns with missing values\nThis can be done either using: `sklearn.impute.SimpleImputer(strategy, \\*fill_value)`:\n```\nstrategy = 'mean', 'median', 'mostfrequent', 'constant'\n```\nor `pd.DataFrame.fillna(value, method, axis)`:\n```\nvalue = str, int64, float64; dict\nmethod = 'pad'\/'ffill', 'bfill'\/'backfill'\n```\n#### Note:\nImputation removes column names; we need to add them back.","c11b0367":"For interest, here we display the cardinality of the categorical columns. This could be used to define a function to limit the number of additional columns that one-hot encoding would add to the dataset.","7f02d555":"### Impute and indicate columns with missing values "}}