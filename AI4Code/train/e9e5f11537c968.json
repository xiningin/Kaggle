{"cell_type":{"fa85f043":"code","1d36f2a8":"code","2a087b01":"code","3483dbab":"code","82fc2b00":"code","a27b158b":"code","5be46d7e":"code","64c6320d":"code","2d040f99":"code","e2dd2160":"code","b0c72abb":"code","8a25f1fe":"code","f9d6cc34":"code","6f1abe36":"code","62915ce8":"code","64d1a04d":"code","0ea7e5ca":"code","1530de58":"code","0b0a908b":"code","320ce036":"code","eaccf7b8":"code","3195634e":"code","e03b1429":"code","8ab809b7":"code","81e84d89":"code","38fcd579":"code","8a892413":"code","11d392f7":"code","5a06147b":"code","79fc7eb1":"markdown","e1b574c0":"markdown","dcbc03d3":"markdown","b9f52bce":"markdown","f6f568fe":"markdown","cd3d5dfe":"markdown","da19b92f":"markdown","883499cc":"markdown","713efac8":"markdown","5b9ba055":"markdown","fa183b1d":"markdown","930af982":"markdown","b95aa3f0":"markdown","aaa0cf23":"markdown","ad70931f":"markdown","58fef3cd":"markdown","341f2e84":"markdown","29089b5f":"markdown","dc301d7b":"markdown","553c6033":"markdown","a9bff57f":"markdown","234faa66":"markdown","174c5aea":"markdown","79431309":"markdown","886d442d":"markdown","f1123ec7":"markdown","85788dc0":"markdown","df364205":"markdown"},"source":{"fa85f043":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt","1d36f2a8":"data = pd.read_csv('..\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv')\n","2a087b01":"data.head()","3483dbab":"data.describe()","82fc2b00":"sns.countplot(data['class'])\nplt.show()\n","a27b158b":"data_melted = data.melt(id_vars=['class'])\nordered_class = data_melted[\"class\"].value_counts().index\nfacet = sns.FacetGrid(data_melted, col=\"variable\", sharey=False, col_wrap=2, aspect=1.2)\nfacet.map(sns.boxplot, \"class\", \"value\", data=data_melted, palette=[\"#e1812c\", \"#3a923a\", \"#3274a1\"], order=ordered_class)\nplt.show()","5be46d7e":"data = data[data.degree_spondylolisthesis<200]","64c6320d":"sns.pairplot(data, hue=\"class\", height=2.5)\nplt.show()","2d040f99":"corr = data.corr()\nsns.heatmap(corr)\nplt.show()\n","e2dd2160":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=6)\nx_data = data.iloc[:,0:-1]\npcs = pca.fit_transform(x_data)\nplt.plot(np.arange(1,7),pca.explained_variance_ratio_ * 100)\nplt.bar(np.arange(1,7),pca.explained_variance_ratio_ * 100)\nplt.ylabel(\"Inertia (%)\")\nplt.xlabel(\"Dimension\")\nplt.show()","b0c72abb":"data_pca = np.hstack((data[['class']].to_numpy(), pcs))\ndata_pca = pd.DataFrame(data_pca)\ndata_pca = data_pca.rename(columns={i:f'Dim {i}' for i in range(1,7)}).rename(columns={0:'class'})\n\nsns.scatterplot(x='Dim 1', y='Dim 2', hue='class', data= data_pca)\nplt.show()\n","8a25f1fe":"from mlxtend.plotting import plot_pca_correlation_graph\n\nfigure, correlation_matrix = plot_pca_correlation_graph(x_data, \n                                                        x_data.columns, \n                                                        dimensions=(1, 2), \n                                                        figure_axis_size=7, \n                                                        X_pca=pcs[:,0:2], \n                                                        explained_variance=pca.explained_variance_[0:2])\n","f9d6cc34":"from sklearn.cluster import KMeans\n\ndata_pca_quant = data_pca.drop(columns=\"class\")\nkm3 = KMeans (n_clusters=3, init=\"random\")\nkm3.fit(data_pca_quant)\nsns.scatterplot(x='Dim 1', y='Dim 2', data= data_pca_quant, hue=km3.labels_, style=data_pca['class'])\nplt.show()","6f1abe36":"from sklearn.metrics import adjusted_rand_score\n\nprint(f\"Adjusted rand score: {round(adjusted_rand_score(data_pca['class'], km3.labels_),2)}\")","62915ce8":"from sklearn.model_selection import train_test_split\n\nX = data.iloc[:, 0:-1]\ny = data['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/3, random_state=42, stratify=y)","64d1a04d":"X_train_5c = X_train.drop(columns=['pelvic_incidence'])\nX_test_5c = X_test.drop(columns=['pelvic_incidence'])\n","0ea7e5ca":"from sklearn.neighbors import NeighborhoodComponentsAnalysis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\ndef grid_search(model, parameters):\n    def one_grid_search(X, y, model, parameters):\n        clf = GridSearchCV(model, parameters, scoring=\"accuracy\", cv=5, n_jobs=-1, refit=True)\n        clf.fit(X, y)\n        scores = clf.cv_results_['mean_test_score']\n        return clf.best_params_, clf.best_score_\n\n    pipe1 = Pipeline([('standard', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis())])\n    pipe2 = Pipeline([('standard', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis(2))])\n\n    Xs = [ \n        ('original',X_train),\n        ('without pelvic incidence',X_train_5c),\n\n        ('NCA', NeighborhoodComponentsAnalysis().fit_transform(X_train,y_train)),\n        ('without pelvic incidence + NCA', NeighborhoodComponentsAnalysis().fit_transform(X_train_5c,y_train)),\n        \n        ('NCA(2)', NeighborhoodComponentsAnalysis(2).fit_transform(X_train,y_train)),\n        ('without pelvic incidence + NCA(2)', NeighborhoodComponentsAnalysis(2).fit_transform(X_train_5c,y_train)),\n \n        ('standard scaler', StandardScaler().fit_transform(X_train)),\n        ('without pelvic incidence + standard scaler', StandardScaler().fit_transform(X_train_5c)),\n        \n        ('standard scaler + NCA', pipe1.fit_transform(X_train, y_train)),\n        ('without pelvic incidence + standard scaler + NCA', pipe1.fit_transform(X_train_5c, y_train)),\n\n        ('standard scaler + NCA(2)', pipe2.fit_transform(X_train, y_train)),\n        ('without pelvic incidence + standard scaler + NCA(2)', pipe2.fit_transform(X_train_5c, y_train))\n\n    ]\n    print(f'Parameters to test: {str(list(parameters.keys())).strip(\"[]\")}\\n')\n    print(f\"{'Method':55} {'Score':10} Parameters\")\n    for X in Xs:\n        method, X = X\n        best_params, best_score = one_grid_search(X, y_train, model, parameters)\n        print(f'- {method:52}: ({round(100*best_score,2):5} %)  {str(list(best_params.values())).strip(\"[]\")}')","1530de58":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_boundary(clf, X, y):\n    labels = ['Hernia', 'Spondylolisthesis', 'Normal']\n    \n    # Transfom y to categorical [1,2,3]\n    y_cat = y.copy()\n    y_cat[y_cat=='Hernia']=0\n    y_cat[y_cat=='Spondylolisthesis']=1\n    y_cat[y_cat=='Normal']=2\n    y_cat = y_cat.to_numpy()\n    y_cat = y_cat.astype(int)\n\n    h = 20  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['tab:blue','tab:orange' , 'tab:green'])\n    cmap_bold = ListedColormap(['blue', 'darkorange','darkgreen' ])\n\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf.fit(X, y_cat)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 100, X[:, 0].max() + 100\n    y_min, y_max = X[:, 1].min() - 100, X[:, 1].max() + 100\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(10,5))\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y_cat, cmap=cmap_bold,\n                edgecolor='k', s=20, label=labels)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    legend = plt.legend(*scatter.legend_elements(), title=\"Class\")\n    for i, label in enumerate(labels):\n        legend.get_texts()[i].set_text(label)\n    plt.show()\n\n","0b0a908b":"from sklearn.neighbors import KNeighborsClassifier\n\nparameters = {'n_neighbors':list(range(1, 100)), 'weights':['uniform', 'distance']}\ngrid_search(KNeighborsClassifier(n_jobs=-1), parameters)","320ce036":"from sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier(n_neighbors=14, weights='distance')\npipe = Pipeline([('standard', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis(2))])\nX_train_transformed = pipe.fit_transform(X_train, y_train)\nX_test_transformed = pipe.transform(X_test)\n\nknn.fit(X_train_transformed, y_train)\ny_pred = knn.predict(X_test_transformed)\nprint(f'Test score: {round(100*accuracy_score(y_pred, y_test),2)} %')\n","eaccf7b8":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(knn, X_test_transformed, y_test, normalize='true')\nplt.show()","3195634e":"knn = KNeighborsClassifier(n_neighbors=14, weights='distance')\n\nplot_decision_boundary(knn, X_train_transformed, y_train)\n","e03b1429":"from sklearn.linear_model import LogisticRegression\n\nparameters = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}\ngrid_search(LogisticRegression(random_state=42, multi_class=\"auto\", n_jobs=-1, C=1), parameters)","8ab809b7":"nca = NeighborhoodComponentsAnalysis(2)\nX_train_transformed = nca.fit_transform(X_train_5c, y_train)\nX_test_transformed = nca.transform(X_test_5c)\n\nlr = LogisticRegression(random_state=42, multi_class=\"auto\", n_jobs=-1, C=1, solver='newton-cg')\nlr = lr.fit(X_train_transformed, y_train)\n\ny_pred = lr.predict(X_test_transformed) \nprint(f'Test score: {round(100*accuracy_score(y_pred, y_test),2)} %')\n\n","81e84d89":"plot_confusion_matrix(lr, X_test_transformed, y_test, normalize='true')\nplt.show()","38fcd579":"lr = LogisticRegression(random_state=42, multi_class=\"auto\", n_jobs=-1, C=1, solver='newton-cg')\n\nplot_decision_boundary(lr, X_train_transformed, y_train)\n","8a892413":"from sklearn.ensemble import RandomForestClassifier\n\nparameters = {\n    'bootstrap':[True, False],            \n    'criterion':['gini', 'entropy'], \n    'max_features':[2, 3, 4, 5, None],\n    'n_estimators':[10, 100, 200],        \n            }\ngrid_search(RandomForestClassifier(n_jobs=-1, random_state=42), parameters)\n","11d392f7":"nca = NeighborhoodComponentsAnalysis()\nX_train_transformed = nca.fit_transform(X_train, y_train)\nX_test_transformed = nca.transform(X_test)\n\nrf = RandomForestClassifier(bootstrap=False,criterion='gini', max_features=4, n_estimators=100,  random_state=42)\nrf = rf.fit(X_train_transformed, y_train)\n\ny_pred = rf.predict(X_test_transformed)\nprint(f'Test score: {round(100*accuracy_score(y_pred, y_test),2)} %')\n","5a06147b":"plot_confusion_matrix(rf, X_test_transformed, y_test, normalize='true')\nplt.show()","79fc7eb1":"# Data distribution 2 by 2","e1b574c0":"# Data correlation","dcbc03d3":"With K=3, we obtain a partition that is very different from the real partition.```Spondylolisthesis``` is divided in two clusters but ```Normal``` and ```Hernia```are in the same one. \n\nThis is confirmed with the Adjusted rand score. ","b9f52bce":"# Knn\n","f6f568fe":"### Original data","cd3d5dfe":"# Conclusion\nWe obtain the best test score (84.47%) with the logistic regression. Some improvements can be applied to this work:\n* Other algorithms could be tested in order to improve the score (SVM, neural networks for example).\n* We can also add a higher weight for wrong classifications, especially when we classify a patient with a disease as ```Normal```.","da19b92f":"The best validation score (88.82%) is obtained with the NCA transformation, and with ```bootstrap=False, criterion='gini', max_features=4```  and```n_estimators=100```.\nWe can apply this model into the test dataset.","883499cc":"### Adjusted rand score","713efac8":"# K-means","5b9ba055":"# PCA\nIn order to better visualize the data we can try to use the Principal Component Analysis (PCA).","fa183b1d":"We remove the outlier for ```degree spondylolisthesis``` higher than 200","930af982":"# Number of patients in each class","b95aa3f0":"About 80% of the total inertia is explained by the first two dimensions. We can then try to plot the data in these two dimensions.","aaa0cf23":"## Decision boundary function","ad70931f":"# Data distribution","58fef3cd":"# Logistic regression","341f2e84":"## Train\/test split\nWe start by splitting our dataset into a train (2\/3) and test (1\/3) dataset.","29089b5f":"The task consists in classifying patients as belonging to one out of three categories: ```Normal```, ```Disk Hernia``` or ```Spondylolisthesis```.","dc301d7b":"# Supervised learning","553c6033":"From all the previous plots, we can see that ```pelvic radius``` is independent from the other variables. We can also notice a high corrolation between ```sacral slope``` and\n```pelvic incidence```, as well as between ```pelvic incidence``` and ```lumbar lordosis angle```.","a9bff57f":"The best validation score (84.43%) is obtained with the NCA(2) transformation, and with ```solver=newton-cg```.\nWe can apply this model into the test dataset.","234faa66":"The best validation score (90.74%) is obtained with the standard scaler and NCA(2) transformations, and with ```n_neighbors=14``` and ```weights='distance```.\nWe can apply this model into the test dataset.","174c5aea":"## Load CSV file with 3 classes","79431309":"We have 310 patients. Each patient is represented with 6 biomechanical attributes:\n* pelvic tilt\n* sacral slope: angle between the S1 upper vertebra and a horizontal line.\n* pelvic incidence: sum of pelvic tilt and sacral slope.\n* lumbar lordosis angle \n* pelvic radius \n* degree spondylolisthesis","886d442d":"### Data without pelvic incidence\nAs said before, pelvic incidence is the sum of pelvic tilt and sacral slope. This can add redundancy. So it's interesting to see what happens when we drop this variable from our dataset.","f1123ec7":"We can see that ```Spondylolisthesis``` is separated from the two other classes. However ```Hernia``` and ```Normal``` are mixed. ","85788dc0":"## Grid search function\nWe can apply some transformations to our dataset before training. For example:\n* Neighborhood Components Analysis (NCA), with ```n_components``` equal to 2 or ```n_features```\n* Standard Scaler\n\nWe can even combine these transformations using pipes.\n\nWe also want to test different parameters for each algorithm\n\nAll this is done with the ```grid_search``` function.","df364205":"# Random forest\n"}}