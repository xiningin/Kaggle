{"cell_type":{"f9497ba0":"code","9ee0e921":"code","2ee54f0c":"code","e6e54776":"code","987ffd9c":"code","7bab09ba":"code","91968558":"code","50247138":"code","ca03c962":"code","6506165d":"code","42726f78":"code","4cffc5a0":"code","6142d429":"code","8044f5a7":"code","f47c8a75":"code","ad0b91b9":"code","25f910ce":"code","416d4cda":"code","2ac7f626":"code","44fe8b45":"code","7e449b45":"code","4567edb3":"code","7021cfbf":"code","07434dda":"markdown","3acb12a9":"markdown","fa4d8487":"markdown","c799de43":"markdown","10358857":"markdown","3c5bc4f8":"markdown","4cb26594":"markdown"},"source":{"f9497ba0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ee0e921":"train_data=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(\"the shape of train data is:\",train_data.shape)\nprint(\"the shape of test data is:\",test_data.shape)\ntrain_y=train_data['Survived']\np_test_id=test_data['PassengerId']\n ","2ee54f0c":"train_data=train_data.drop([\"PassengerId\",\"Survived\"],axis=1)\ntest_data=test_data.drop('PassengerId',axis=1)\ntotal=pd.concat([train_data,test_data],axis=0,ignore_index=True)                            ","e6e54776":"total['Age'] = total.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\nfair=total.groupby(['Pclass','Sex']).median()['Fare'][3][\"male\"]\ntotal['Fare']=total['Fare'].fillna(fair)\n ","987ffd9c":"# code for family size selection\ntotal['Family_size']=total['SibSp']+total['Parch']+1\ndef check(df):\n    if df['Family_size']<=1:\n        return \"Single\"\n    elif 1<df['Family_size']<5:\n        return \"Small\"\n    elif 5<=df['Family_size']<=6:\n            return \"Medium\"\n    else:\n        return \"Large\"\ntotal['fam_attr']=total.apply(check,axis=1)\n\n \n ","7bab09ba":"# extracting title from name\ntotal['Title'] = total['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0] \ntotal['Title']=total['Title'].replace('Mme','Mrs')\ntotal['Title']=total['Title'].replace('Ms','Miss')\ntotal['Title']=total['Title'].replace('Mlle','Miss')\ntotal['Title']=total['Title'].replace('Dona','Miss')\n \ntotal['Title']=total['Title'].replace(['Capt','Col','Rev','Dr','Major'],'Officer')\ntotal['Title']=total['Title'].replace(['Don','Jonkheer','Sir','Lady','the Countess'],'Royalty')\ntotal['is_married']=0\n \n\ntotal['is_married'].loc[total['Title'] == 'Mrs'] = 1\n ","91968558":"total['Ticket_count'] =total.groupby('Ticket')['Ticket'].transform('count')\ndummies_title=pd.get_dummies(total['Title'],prefix='Title')\ntotal=pd.concat([total,dummies_title],axis=1)\ntotal=total.drop('Title',axis=1)\n ","50247138":"total['Cabin']=total['Cabin'].fillna('U')\ntotal['Cabin'] = total['Cabin'].map(lambda x: x[0])\ndummies=pd.get_dummies(total['Cabin'],prefix='Cabin')\ntotal=pd.concat([total,dummies],axis=1)\ntotal=total.drop('Cabin',axis=1)\ntotal=total.drop(\"Name\",axis=1)\ntotal=total.drop(\"Ticket\",axis=1)","ca03c962":"transform={'male':0,'female':1}\nembark_unique={'S':0,\"C\":1,\"Q\":2}\nfam_vec={'Single':0,\"Small\":1,'Medium':2,'Large':3}\ntotal['fam_attr']=total['fam_attr'].map(fam_vec)\ntotal['Embarked']=total['Embarked'].map(embark_unique)\ntotal['Embarked']=total['Embarked'].fillna(2)\ntotal['Sex']=total['Sex'].map(transform)\ntotal[\"Age\"]=total['Age'].astype(int)\ntotal[\"Fare\"]=total[\"Fare\"].astype(int)","6506165d":"# # get surnames, people with same surnames belong to same family\n# import string\n# def get_surnames(data):\n#     families=[]\n#     for i in range(len(data)):\n#         name=data.iloc[i][1]\n        \n#         if '(' in name:\n#             name=name.split('(')[0]\n#         else:\n#             name=name\n#         surname=name.split(',')[0]\n#         for punctuations in string.punctuation:\n#             surname = surname.replace(punctuations, '').strip()\n#         families.append(surname)\n     \n#     return families\n# total['Surnames']=get_surnames(total)","42726f78":"#data preprocessing\n\ntrain_x=total[:891]\n \ntest=total[891:]\n ","4cffc5a0":"# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.ensemble import RandomForestClassifier\n\n# from sklearn.metrics import accuracy_score\n# clf=RandomForestClassifier()\n\n# clf.fit(train_x,train_y)\n# predicted=clf.predict(train_x)\n# score=accuracy_score(predicted, train_y)\n# print(\"the accuracy score is: \",score)\n# # print(\"The OOB score is: \", clf.oob_score_*100,\" %\")\n","6142d429":"# from sklearn.model_selection import cross_val_score\n# clf=RandomForestClassifier()\n# score=cross_val_score(clf,train_x,train_y,cv=10,scoring='accuracy')\n# print(\"Mean cross validation score is: \", score.mean())\n","8044f5a7":"# from sklearn.model_selection import RandomizedSearchCV\n# clf=RandomForestClassifier()\n# params={\"min_samples_leaf\" : [1, 5,7], \"n_estimators\": [100,170, 200,250, 500, 1000,1500, 1600,1700]}\n\n# param={'bootstrap': [True, False],\n#  'max_depth': [ 60,63,64,65,66,67,68, 70],\n#  \"n_estimators\": [1000,1500, 1600,1650,1700,1750,1760,1770,1800],\n#  'max_features': ['auto', 'sqrt'],\n#  'min_samples_leaf': [1, 2,3,4],\n#  'min_samples_split': [7, 8,9,10]}\n# tuning=RandomizedSearchCV(clf,param,n_jobs=-1,cv=3,random_state=42)\n# tuning.fit(train_x,train_y)\n# print(\"the best parameters are as follows; \",tuning.best_params_)","f47c8a75":" \n# clf=RandomForestClassifier(n_estimators= 1770, max_features='sqrt', max_depth= 63, bootstrap= True,oob_score=True,min_samples_split= 10, min_samples_leaf= 1, n_jobs=-1,random_state=42)\n# clf.fit(train_x,train_y)\n# predicted=clf.predict(train_x)\n# score=accuracy_score(predicted,train_y)\n# print(\"the accuracy score after tuning is : \",score)\n# print(\"The OOB score is: \", clf.oob_score_*100,\" %\")\n# clf.feature_importances_\n# features = pd.DataFrame()\n# features['feature'] = train_x.columns\n# features['importance'] = clf.feature_importances_\n# features.sort_values(by=['importance'], ascending=True, inplace=True)\n# features.set_index('feature', inplace=True)\n\n# features.plot(kind='barh', figsize=(25, 25))","ad0b91b9":"# clf=RandomForestClassifier(n_estimators= 1770, max_features='sqrt', max_depth= 63, bootstrap= True,oob_score=True,min_samples_split= 10, min_samples_leaf= 1, n_jobs=-1,random_state=42)\n \n# score=cross_val_score(clf,train_x,train_y,cv=10,scoring='accuracy')\n# print(\"Mean cross validation score is: \", score.mean())","25f910ce":" \n# x_test=test_data.drop(['PassengerId'],axis=1)\n\n# Model=RandomForestClassifier(n_estimators= 1770, max_features='sqrt', max_depth= 63, bootstrap= True,oob_score=True,min_samples_split= 10, min_samples_leaf= 1, n_jobs=-1,random_state=42)\n \n# Model.fit(train_x,train_y)\n# predicted=Model.predict(test)\n\n# result=pd.DataFrame({'PassengerId':p_test_id,'Survived':predicted})\n# result.to_csv('my_sssubmission.csv', index=False)\n# print(\"Success\")\n \n","416d4cda":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nxgb=XGBClassifier()\nxgb.fit(train_x,train_y)\npredicted=xgb.predict(train_x)\nscore=accuracy_score(predicted,train_y)\nprint(\"the accuracy score with default XGBoosting is:\",score)\nxgb=XGBClassifier()\ncross_val_score=cross_val_score(xgb,train_x,train_y,cv=10,scoring='accuracy')\ncross_val_score=cross_val_score.mean()\nprint(\"the cross validated mean accuracy is : \",cross_val_score)","2ac7f626":"# params={'n_estimators':[10,50,100,150,200],'learning_rate':[0.01,0.05,0.09,0.1,0.2,0.3],'max_depth':[3,5,7,9],'subsample':[0.5,0.7,0.9]}\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nparams={'n_estimators': range(8, 20),\n        \n    'max_depth': range(6, 10),\n    'learning_rate': [.4, .45, .5, .55, .6],\n    'colsample_bytree': [.6, .7, .8, .9, 1]}\nxgb=XGBClassifier()\n# tuning=RandomizedSearchCV(xgb,params,n_jobs=-1,random_state=42,cv=4)\ntuning=GridSearchCV(estimator=xgb, param_grid=params, n_jobs=-1, cv=4 )\ntuning.fit(train_x,train_y)\nprint(\"the best parameters are as follows; \",tuning.best_params_)\nprint(\"the best CV score is : \",tuning.best_score_)","44fe8b45":"xgb=XGBClassifier( n_estimators= 15, max_depth= 6, learning_rate= 0.45,colsample_bytree =1)\nxgb.fit(train_x,train_y)\npredicted=xgb.predict(train_x)\nscore=accuracy_score(predicted,train_y)\nprint(\"the accuracy score with tuning XGBoosting is:\",score)\n \n ","7e449b45":"# x_test=test_data.drop(['PassengerId'],axis=1)\nxgb=XGBClassifier( n_estimators= 15, max_depth= 6, learning_rate= 0.45,colsample_bytree =1)\nxgb.fit(train_x,train_y)\npredicted=xgb.predict(test)\n \n\nresult=pd.DataFrame({'PassengerId':p_test_id,'Survived':predicted})\nresult.to_csv('my_sssubmission.csv', index=False)\nprint(\"Success\")\n","4567edb3":"# result.to_csv('my_ssubmission.csv', index=False)\n# print(\"Success\")\n \n\n ","7021cfbf":" print(\"sagar Ghimire\")","07434dda":"here cross validation score has improved from .81 to .83 which is grood\nNow we will  use same model tok predict the given test data as follows:","3acb12a9":"Since age has some missing values we will deal age first\nApproach: Fill median age accourding to Pclass and gender \nexample: for each Pclass, we have sepated median age for both male and female","fa4d8487":"now we can improve this score by hyperparameter tuning\n","c799de43":"Now we will be doing hyperparameter tuning with XGBoosting to see if it is more accurate than Random Forest or not","10358857":"here the reason behind calculating OOB, out of box score is, it is a very good metrics to check the performance of model in RandomForestClassifier\nbecause this score is calculated from unseen data\/ train data are not involved\nhiger the OOB score, higer the confidence of our model\nthis has reduced train accuracy which was 96 percent earlier, probably over fitting, now we hope to get better score on test data****","3c5bc4f8":"Again I train my model based upon these hyperparameter and find accuracy or outof box score OOB","4cb26594":"Now we train again using these parameters are check accuracy and cross_val score and compare with default value "}}