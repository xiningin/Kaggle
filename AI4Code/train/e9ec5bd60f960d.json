{"cell_type":{"da92501b":"code","538dc875":"code","9e8733b3":"code","b43c50b8":"code","30fddae7":"code","6e3a6dd1":"markdown","af98e05f":"markdown","bab158e8":"markdown","bee2df4d":"markdown"},"source":{"da92501b":"import tensorflow as tf\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler","538dc875":"# Load data (must be in same folder as this file, which it will be if you simply unzip the assignment).\n# Note that we don't have any y_test! This way you cannot \"cheat\"!\n\nx_train = np.load('x_train.npy')\nx_test = np.load('x_test.npy')\ny_train = np.load('y_train.npy')\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nprint(x_train.shape, x_test.shape, y_train.shape)","9e8733b3":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(32,)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax'),\n    ])\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy'],\n    )\n\nmodel.fit(x_train, y_train, epochs=10)","b43c50b8":"y_test_hat = model.predict(x_test)\ny_test_hat = np.argmax(y_test_hat, axis=1)\ny_test_hat_pd = pd.DataFrame({\n    'Id': list(range(5000)),\n    'Category': y_test_hat,\n})","30fddae7":"# After you make your predictions, you should submit them on the Kaggle webpage for our competition.\n# You may also (and I recommend you do it) send your code to me (at tsdj@sam.sdu.dk).\n# Then I can provide feecback if you'd like (so ask away!).\n\n# Below is a small check that your output has the right type and shape\nassert isinstance(y_test_hat_pd, pd.DataFrame)\nassert all(y_test_hat_pd.columns == ['Id', 'Category'])\nassert len(y_test_hat_pd) == 5000\n\n# If you pass the checks, the file is saved.\ny_test_hat_pd.to_csv('y_test_hat.csv', index=False)","6e3a6dd1":"# Assignment - classification\n\nHi there! In this assignment, you will use neural networks (or something else, if you want) to predict values in a classification problem with 10 classes.\n\nIt is quite like the third (and to a lesser extent first and fourth) assignment(s).\n\nTo get you started, I have provided a complete working example, which is decent but not very impressive.\n\nWhen you are done, submit your results on the Kaggle webpage for this competition. If you do not like to show your score to everyone, you may either:\n1. Send your $\\texttt{.csv}$ to be, and I will then do it for you.\n1. Use an anonymous username on Kaggle.\n\nHowever, I suggest you use your real name, after all it is just meant as an exercise and it is more fun that way. You can submit 5 times every day, so you can experiment with some stuff without being \"locked in\".","af98e05f":"The below code makes predictions and then saves them (after checking they are in correct format).\n\nThe argmax converts probabilities to specific class predictions.\n\nAnd finally convert to appropriate $\\texttt{.csv}$ for Kaggle submit.","bab158e8":"# Kaggle\n\nFollowing research and suggestion from your co-student Mikkel Bruun-Jensen, I have created a Kaggle webpage for this competition!\n\nYou can use it to submit your predictions. It will then give you a score (based on **some** of the test data). The final score will be calculated using the **other** part of the test data (so you have to change of looking at the test data for the competition).\n\nYou can visit the webpage here: https:\/\/www.kaggle.com\/c\/nn-intro-classification. You need to create an account (free, just need an email).\n\n**Note**: I have not tried this setup before. I hope it works, but there might be some issues until I figure out how to do this.","bee2df4d":"# Details\n\nThe metric used to score this assignment is accuracy (as in the third assignment).\n\nThe Bayesian error rate is 0 (i.e. 100% accuracy is possible - there is no irreducible error in this assignment). However, this is highly non-trivial to achieve.\n\n(**Advanced**): The data is generated by a process that generates a latent variable $y^*$ which describes a probability distribution over the classes (similar to a softmax layer). This is then passed through an argmax operation to generate $y$. Noteably, the distribution of $y^*$ is *not* heavily weighted towards one class of the distribution. This has potential implications for the loss function you may want to use (**hint**: think carefully about how categorical cross-entropy may decrease even as the accuracy also decreases). With that said, categorical cross-entropy is still very fine to use, so this should only be seen as a bonus thing you may want to explore."}}