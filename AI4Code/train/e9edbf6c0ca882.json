{"cell_type":{"437bf5cb":"code","cbce6985":"code","915a2bff":"code","ebaf7973":"code","089f49db":"code","1d224867":"code","d93c5ad1":"code","5e4d5b35":"code","052dd63f":"code","45e29163":"code","07519be1":"code","b6b748ed":"code","e44752e7":"code","2741ec32":"code","0bd17d24":"code","def9bc1a":"code","701788dd":"code","024960cb":"code","e2513db4":"code","b401f40e":"markdown","45b7af99":"markdown","4a809380":"markdown","231e9dd7":"markdown","4e79e18b":"markdown"},"source":{"437bf5cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\nfrom datetime import datetime\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,scale \nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nimport pickle\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import LSTM,Input,Dense,Flatten,SpatialDropout1D,Dropout,CuDNNLSTM,Reshape,Concatenate,BatchNormalization\nfrom keras.layers import CuDNNLSTM, RepeatVector, TimeDistributed\nfrom keras.callbacks import ModelCheckpoint\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cbce6985":"data = pd.read_csv(\"..\/input\/iotusd\/IOTUSD.csv\",parse_dates=[1], index_col=1, encoding='UTF-8', date_parser=lambda x: datetime.fromtimestamp(int(x) \/ 1e3)).drop(['Unnamed: 0'], axis = 1)[500:]\n","915a2bff":"data.describe()","ebaf7973":"data.describe()\ndata[['OPEN','CLOSE','HIGH','LOW']].plot()\ndata[['OPEN','CLOSE','HIGH','LOW']].hist()\ndata[['VOLUME']].plot()\ndata[['VOLUME']].hist()","089f49db":"prob = stats.probplot(data.OPEN,plot=plt)\nplt.show()\nprob = stats.probplot(data.HIGH,plot=plt)\nplt.show()\nprob = stats.probplot(data.VOLUME,plot=plt)\nplt.show()","1d224867":"data.kurtosis()","d93c5ad1":"from scipy.cluster import hierarchy\nfrom scipy.spatial import distance\nimport seaborn as sns\n\ncorr_matrix = data.corr()\ncorrelations_array = np.asarray(corr_matrix)\n\nlinkage = hierarchy.linkage(distance.pdist(correlations_array), \\\n                            method='average')\n\ng = sns.clustermap(corr_matrix,row_linkage=linkage,col_linkage=linkage,\\\n                   row_cluster=True,col_cluster=True,figsize=(10,10),cmap='Greens')\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n\nlabel_order = corr_matrix.iloc[:,g.dendrogram_row.reordered_ind].columns","5e4d5b35":"pdata = pd.DataFrame()","052dd63f":"split_rate = 0.8\ncolumns = data.columns","45e29163":"pdata['OPEN'] = data['OPEN'].pct_change().clip(0.2,-0.2)\npdata['HIGH'] = data['HIGH'].pct_change().clip(0.2,-0.2)\npdata['LOW'] = data['LOW'].pct_change().clip(0.2,-0.2)\npdata['CLOSE'] = data['CLOSE'].pct_change().clip(0.2,-0.2)\npdata['VOLUME'] = data['VOLUME'].apply(np.log1p).clip(14,0)\npdata.dropna(inplace=True)\n\nX_train = pdata[:int(pdata.shape[0]*split_rate)]\nX_test = pdata[int(pdata.shape[0]*split_rate):]\n\n\n","07519be1":"scaler = MinMaxScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train,columns = columns)\nX_test = pd.DataFrame(X_test,columns = columns)\n","b6b748ed":"import numpy as np\nimport keras\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, X,input_columns, output_columns, batch_size, sequence_lenght,steps_ahead,normalise = False):\n        'Initialization'\n        self.batch_size = batch_size\n        self.X = X\n        self.input_columns = input_columns\n        self.output_columns = output_columns\n        self.sequence_lenght = sequence_lenght\n        self.steps_ahead = steps_ahead\n        self.on_epoch_end()\n        self.normalise = normalise\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor((len(self.X)-self.sequence_lenght)\/self.batch_size))\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.X))\n        \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate samples of the batch\n        data_windows = np.zeros([self.batch_size,self.sequence_lenght,len(self.input_columns)])\n        for i in range(self.batch_size):\n            data_windows[i] = self.X[index*self.batch_size+i:index*self.batch_size+i+self.sequence_lenght]\n        # Generate data\n        #X,X = self.__data_generation(data_windows)\n\n        return data_windows,data_windows\n\n\n    def __data_generation(self, data_windows,):\n        'Generates data containing batch_size samples' # X : (n_samples, lenght, input dim) Y :(n_samples,output_dim)\n        # Initialization\n\n        X = np.empty((self.batch_size, self.sequence_lenght, len(self.input_columns)))\n        # Generate data\n        for i,window in enumerate(data_windows):\n            X[i,:,:] = window.values\n\n                    \n\n        return X,X","e44752e7":"timesteps = 1024\nbatch_size = 128\nn_features = 5\ntrain_generator = DataGenerator(X_train,columns,[],batch_size,timesteps,1,False)\ntest_generator = DataGenerator(X_test,columns,[],batch_size,timesteps,1,False)","2741ec32":"from keras.callbacks import Callback\nfrom keras import backend as K\nimport numpy as np\n\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency.\n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    The amplitude of the cycle can be scaled on a per-iteration or\n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    For more detail, please see paper.\n    # Example for CIFAR-10 w\/ batch size 100:\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    # References\n      - [Cyclical Learning Rates for Training Neural Networks](\n      https:\/\/arxiv.org\/abs\/1506.01186)\n    \"\"\"\n\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 \/ (2.**(x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations \/ (2 * self.step_size))\n        x = np.abs(self.clr_iterations \/ self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)","0bd17d24":"model = Sequential()\nmodel.add(CuDNNLSTM(100,input_shape=(timesteps,n_features),return_sequences=True))\nmodel.add(CuDNNLSTM(50,input_shape=(timesteps,n_features),return_sequences=False))\nmodel.add(RepeatVector(timesteps))\nmodel.add(CuDNNLSTM(50,return_sequences=True))\nmodel.add(CuDNNLSTM(100,return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')\n\n","def9bc1a":"filepath=\"1024-weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncyclic = CyclicLR(mode='triangular')\nmodel.fit_generator(train_generator,epochs = 5, validation_data = test_generator,callbacks = [checkpoint,cyclic])","701788dd":"prediction = model.predict(train_generator[0][0])","024960cb":"plt.plot(train_generator[0][0][6])","e2513db4":"plt.plot(prediction[60])","b401f40e":"Generator","45b7af99":"**Iniciaci\u00f3n del cuaderno interactivo**\n\nSe realiza la importaci\u00f3n de algunas librerias que vamos a utilizar, y se muestran los ficheros de entrada al cuaderno. En nuestro caso nuestro fichero en formato CSV.","4a809380":"**Lectura de fichero CSV**\n\nSe cambia el formato con la que mostramos el tiempo (Estampa de tiempo en ms a Fecha)","231e9dd7":"****Preprocesamos los datos****\n","4e79e18b":"**Exploraci\u00f3n de los datos**\nMostramos las caracteristicas de los datos en bruto."}}