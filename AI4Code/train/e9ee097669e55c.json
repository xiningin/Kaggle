{"cell_type":{"331ba201":"code","ffb727b6":"code","f5abf383":"code","9d79045b":"code","224e15d2":"code","5715c2d8":"code","78213de6":"code","66fa69b9":"code","6b67c135":"code","e70fefed":"markdown"},"source":{"331ba201":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport torch\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ffb727b6":"col_names = ['h', 't', 'r']\ntrain_df = pd.read_csv(\"..\/input\/train.txt\", header=None, delim_whitespace=True, names=col_names)\nval_df = pd.read_csv(\"..\/input\/valid.txt\", header=None, delim_whitespace=True, names=col_names)\ntest_df = pd.read_csv(\"..\/input\/test.txt\", header=None, delim_whitespace=True, names=col_names)\nentity2id = pd.read_csv(\"..\/input\/entity2id.txt\", header=None, delim_whitespace=True)\nrelation2id = pd.read_csv(\"..\/input\/relation2id.txt\", header=None, delim_whitespace=True)\n\nrel_len = len(relation2id)\nentity_len = len(entity2id)\n\nall_entities = entity2id[0].values\n","f5abf383":"entity2id_dict = dict((v,k) for k,v in entity2id.to_dict()[0].items())\nrelation2id_dict = dict((v,k) for k,v in relation2id.to_dict()[0].items())","9d79045b":"def map_triplets(df, entity2id, rel2id):\n    print(\"mapping started\")\n    df['h'] = df.apply(lambda L: entity2id_dict[L[0]], axis=1)\n    df['t'] = df.apply(lambda L: entity2id_dict[L[1]], axis=1)\n    df['r'] = df.apply(lambda L: relation2id_dict[L[2]], axis=1)\n    print(\"mapping end\")","224e15d2":"map_triplets(train_df, entity2id_dict, relation2id_dict)\nmap_triplets(val_df, entity2id_dict, relation2id_dict)\nmap_triplets(test_df, entity2id_dict, relation2id_dict)","5715c2d8":"class TransE(nn.Module):\n    def __init__(self, entity_len, rel_len, embedding_dim, margin=0.5):\n        super(TransE, self).__init__()\n        \n        self.entity_embeddings = nn.Embedding(entity_len, embedding_dim,).cuda()\n        self.rel_embeddings = nn.Embedding(rel_len, embedding_dim).cuda()        \n        \n        embeddings_init_bound = 6 \/ np.sqrt(embedding_dim)\n        nn.init.uniform_(\n            self.entity_embeddings.weight.data,\n            a=-embeddings_init_bound,\n            b=+embeddings_init_bound,\n        )\n\n        nn.init.uniform_(\n            self.rel_embeddings.weight.data,\n            a=-embeddings_init_bound,\n            b=+embeddings_init_bound,\n        )\n        \n        self.criterion = nn.MarginRankingLoss(\n            margin=margin\n        )\n        \n        norms = torch.norm(self.rel_embeddings.weight, p=2, dim=1).data\n        self.rel_embeddings.weight.data = self.rel_embeddings.weight.data.div(\n            norms.view(rel_len, 1).expand_as(self.rel_embeddings.weight))\n        \n        \n    def generate_negative_triplets(self, pos_batch, all_entities):        \n        current_batch_size = len(pos_batch)\n        batch_subjs = pos_batch[:, 0:1]\n        batch_relations = pos_batch[:, 2:3]\n        batch_objs = pos_batch[:, 1:2]\n\n        num_subj_corrupt = len(pos_batch) \/\/ 2\n        num_obj_corrupt = len(pos_batch) - num_subj_corrupt\n        pos_batch = torch.tensor(pos_batch, dtype=torch.long)\n\n        corrupted_subj_indices = np.random.choice(np.arange(0, all_entities.shape[0]), size=num_subj_corrupt)\n        corrupted_subjects = np.reshape(all_entities[corrupted_subj_indices], newshape=(-1, 1))\n        corrupted_converted_subjects = np.apply_along_axis(self.entities_to_ids,1,corrupted_subjects).reshape(num_subj_corrupt,1)\n        subject_based_corrupted_triples = np.concatenate(\n            [corrupted_converted_subjects, (batch_objs[:num_subj_corrupt]).cpu(), (batch_relations[:num_subj_corrupt]).cpu()], axis=1)\n\n        corrupted_obj_indices = np.random.choice(np.arange(0, all_entities.shape[0]), size=num_obj_corrupt)\n        corrupted_objects = np.reshape(all_entities[corrupted_obj_indices], newshape=(-1, 1))\n        corrupted_converted_objects = np.apply_along_axis(self.entities_to_ids,1,corrupted_objects).reshape(num_obj_corrupt,1)\n        object_based_corrupted_triples = np.concatenate(\n            [(batch_subjs[num_subj_corrupt:]).cpu(), corrupted_converted_objects, (batch_relations[num_subj_corrupt:]).cpu()], axis=1)\n        batch_subjs.cuda()\n        batch_relations.cuda()\n        batch_objs.cuda()\n        neg_batch = np.concatenate([subject_based_corrupted_triples, object_based_corrupted_triples], axis=0)\n        neg_batch = torch.tensor(neg_batch, dtype=torch.long).cuda()\n        return neg_batch\n\n    \n    \n    def entities_to_ids(self, entities):\n        return entity2id_dict[entities[0]]\n    \n    def forward(self, pos_batch, neg_batch):\n        pos_score = self.score_triplets(pos_batch)\n        neg_score = self.score_triplets(neg_batch)\n        \n        loss = self.compute_loss(pos_score, neg_score)\n        return loss\n\n    \n    def train(self, triplets, all_entities, batchsize=32, epochs=1):\n        triplets_len = triplets.shape[0]\n        optimiser = optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n        loss_hist = []\n        for epoch in range(epochs):\n            print(\"Epoch: {} is started.\".format(epoch))\n            for i in range(0,triplets_len,batchsize):\n                #raises error if last batch contains only one element!!\n                pos_batch = triplets[i:i+batchsize]\n                neg_batch = self.generate_negative_triplets(pos_batch, all_entities)\n                optimiser.zero_grad()\n\n                loss = self.forward(pos_batch, neg_batch)\n                loss_hist.append(loss)\n                print(\"Calculated loss for iteration {}: {}\".format(i,loss))\n                loss.backward()\n                optimiser.step()\n            \n        return loss_hist\n        \n    \n    def compute_loss(self, pos_scores, neg_scores):\n        y = np.repeat([1], repeats=pos_scores.shape[0])\n        y = torch.tensor(y, dtype=torch.float)\n\n        positive_scores = torch.tensor(pos_scores, dtype=torch.float)\n        negative_scores = torch.tensor(neg_scores, dtype=torch.float)\n\n        loss = self.criterion(pos_scores.cpu(), neg_scores.cpu(), y)\n        \n        return loss\n    \n    def split_triplets(self, triplets):\n        h = triplets[:, 0:1]\n        t = triplets[:, 1:2]\n        r = triplets[:, 2:3]\n        return h, t, r\n\n    #it is very important how to vectorize code. Before, i was using numpy.apply_along_axis function to fetch the embeddings\n    #since it is using numpy, apply function was fetching the embeddings one by one with using vectorization.\n    #however, this was creating a ndarray of tensors which i do not want.\n    #then, i realize it is possible to give a tensor(which has indices of relevant embeddings) to embeddings.weight.data\n    #to fetch the relevant embeddings. Since I am fetching directly from nn.embedding (is of type tensor) now resulting\n    #data is also tensor in the shape I want.\n    \n    def get_embedding_of_triplets(self, triplets):\n        heads, tails, relations = self.split_triplets(triplets)\n        #print(\"SHAPE \",self.entity_embeddings.weight[heads].reshape(heads.shape[0],-1).shape)\n        return self.entity_embeddings.weight[heads].reshape(heads.shape[0],-1), self.entity_embeddings.weight[tails].reshape(heads.shape[0],-1), self.entity_embeddings.weight[relations].reshape(heads.shape[0],-1)   \n        \n    def score_triplets(self, triplets):\n        print(self.entity_embeddings.weight.data)\n        norms = torch.norm(self.entity_embeddings.weight, dim=1).data\n        self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(norms.view(entity_len, 1).expand_as(self.entity_embeddings.weight))\n        print(self.entity_embeddings.weight.data)\n        heads, tails, rels = self.get_embedding_of_triplets(triplets)\n        sum_res = heads + rels - tails\n        distances = torch.norm(sum_res, p=1, dim=1)\n        distances_view = distances.view(size=(-1,))\n\n        return distances_view\n            \n        \n","78213de6":"embedding_dim = 5\nmodel = TransE(entity_len, rel_len,  embedding_dim, margin=1)\npos_triplets = torch.from_numpy(train_df.values).cuda()\nlen_pos_triplets = pos_triplets.shape[0]\nmodel.generate_negative_triplets_from_all_set(pos_triplets[0], all_entities)\n#loss_hist = model.train(pos_triplets, all_entities,len_pos_triplets,epochs=100)\n\n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print (name, param.data)\n    else:\n        print(param.name)","66fa69b9":"plt.plot(loss_hist)\nplt.ylabel('some numbers')\nplt.show()","6b67c135":"subj = entity2id_dict['\/m\/027rn']\nrel = relation2id_dict['\/location\/country\/form_of_government']\nobj = entity2id_dict['\/m\/06cx9']\nprint(\"subj index: \", subj)\nprint(\"rel index: \", rel)\nprint(\"obj index: \", obj)\n\nsubj_embed = model.entity_embeddings.weight[subj]\nrel_embed = model.rel_embeddings.weight[rel]\nobj_embed = model.entity_embeddings.weight[obj]\n\nprint(\"subj embed: \", subj_embed)\nprint(\"rel embed: \", rel_embed)\nprint(\"obj embed: \", obj_embed)\nprint(\"sum embed: \", subj_embed+rel_embed - obj_embed)","e70fefed":"**REMEMBER TO CHECK HEAD TAIL RELATION SEQUENCE!!!!**"}}