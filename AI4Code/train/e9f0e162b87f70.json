{"cell_type":{"239c3a3f":"code","d7d9e845":"code","c05c2961":"code","175cc343":"code","c5a828f5":"code","55f525a6":"code","25f4630f":"code","f4a28168":"code","60cc6513":"code","4c7d88b9":"code","252e12b1":"code","d384835f":"code","7c7ea0b0":"code","6b0d7bb7":"code","a738ee75":"code","23b45480":"code","e357d694":"code","b4b98cb1":"code","035661e4":"code","f3fb54b1":"code","528943aa":"code","3039e345":"code","98e80f22":"code","65f08848":"code","c587cb76":"code","cc1db07b":"code","b769f944":"code","433bb179":"code","d828c9aa":"code","6c2a2515":"code","53215485":"code","4f541aa6":"code","cb2b5d2b":"code","9e516289":"code","ad9c0c2b":"code","3a48b314":"code","55559d2e":"code","c56da336":"code","e22f9caf":"code","e15ba799":"code","61579c8a":"code","4e4319cb":"code","941698f8":"code","6d70b854":"code","e2fec991":"code","2b4aa06f":"code","1d36f697":"code","79c278f8":"code","3e70fc9d":"code","5b1a8fbc":"code","0d8f9801":"code","5b8e30d9":"code","f156f38e":"code","946d8b3c":"code","dd0cb1f5":"code","a835ba70":"code","804306d4":"code","5c71b5ec":"code","384c0447":"code","670cdcaa":"code","491c64c9":"code","de7b97c1":"code","8fd530be":"code","d886d18a":"code","e51ecf60":"code","eb3f3e9a":"code","cb5eb13c":"code","43b72b57":"code","3cf7b91b":"code","50fede9c":"code","2d430eda":"code","ddf2411b":"code","f3761d84":"code","5682e057":"code","9a4bc33b":"code","9bed092a":"code","8f6ff5ca":"code","bfcf19e1":"code","65349931":"code","dc25e0b5":"code","d20aefca":"code","3455ca1d":"code","63d5f313":"code","96b8cec6":"code","b48f3f07":"code","181aa1e7":"code","dde5f246":"code","4bbd9211":"code","44a0d04a":"code","9a8edb20":"code","adfce8da":"code","5ebb573f":"code","7bd7f12b":"code","fe298570":"code","bb1a1bcd":"code","dd805922":"code","e41ccbbb":"code","c1c61649":"code","4c372a0e":"code","524ade77":"code","03086d75":"code","f1cb8d58":"code","aeabdc27":"code","96155152":"code","366bef61":"code","7d1b7800":"code","36ef03e9":"code","785744fa":"code","eee8729a":"code","971d41d8":"code","e9ac0be0":"code","7d90dcee":"code","1435d2ae":"code","1a37e091":"code","ac39321e":"code","96efde62":"code","36bb93dd":"code","852c8d47":"code","0fb5e080":"code","8be8d044":"code","9dfd4d63":"code","6de7700b":"code","40e0ca1a":"code","4d95a3aa":"code","b5188b2b":"code","ea2c165f":"code","ce4ef716":"code","d399dea4":"code","d6a3d978":"code","bcc62e15":"code","f4570e6c":"code","640754b6":"code","593c8d3b":"code","b7da1abe":"code","90a55f68":"code","2d9e19fa":"code","a6e0cb28":"code","445da588":"code","d256e5e2":"code","a9700e71":"code","925882cd":"code","dc568d83":"code","368901a6":"code","83113723":"code","e177d466":"code","c8c85cdc":"code","1fc3a9d1":"code","c6b8ef33":"code","b33d4da8":"code","75c3366f":"code","8dce64cc":"code","c39ec62e":"code","2ceddc3f":"code","56d0f008":"code","f2f78534":"code","8ebd5d8d":"code","5f18dc26":"code","faa1cdf9":"code","4150d9db":"code","9cf2a023":"code","ebf37b68":"code","1c2ba83e":"code","cac86d22":"code","3b1a2d77":"code","b0a65c8f":"code","79bc1c2c":"code","1134e967":"code","c52755d6":"code","aa930ffb":"code","4bee6c69":"code","e6934f31":"code","d5272da0":"code","fb990d6e":"code","ab02c753":"code","9e205163":"code","7d87dc87":"code","523528ed":"code","f9cb4042":"code","95e8da15":"code","40d99db9":"code","1ccf91b3":"code","4f4b92ed":"code","6348886d":"code","58d86720":"code","66674a7f":"code","508473bb":"code","ef56a5a6":"code","d3d03e7c":"code","402bb1a6":"code","3bca581d":"code","c3d1c267":"code","0dc183b5":"code","6882c04d":"code","9de04579":"markdown","4f142be1":"markdown","4b6f351d":"markdown","3f14f428":"markdown","edfabd78":"markdown","3c70b72a":"markdown","a3fca192":"markdown","d8c050c6":"markdown","c844b152":"markdown","4b86bd93":"markdown","3f517101":"markdown","3bfc8d7e":"markdown","9b2ff555":"markdown","ccbfcf41":"markdown","cf9583df":"markdown","ceb7985e":"markdown","95c3eb66":"markdown","6b3239bf":"markdown","84169c32":"markdown","385035de":"markdown","7da312af":"markdown","8878b72d":"markdown","748d7708":"markdown","b7a736a7":"markdown","50dcd0af":"markdown","627d94de":"markdown","8f99626d":"markdown","c7e212c6":"markdown","dc844cdf":"markdown","d6905764":"markdown","f14fec24":"markdown","4741e35f":"markdown","803cac0c":"markdown","c3247e82":"markdown","750febad":"markdown","d7a80ab1":"markdown","9c269a62":"markdown","15fb0076":"markdown","d5671d3a":"markdown","b46e8070":"markdown","5881f704":"markdown","32526e5b":"markdown","08986373":"markdown","25c74d65":"markdown","9b79db3e":"markdown","3d49aa84":"markdown","ded32833":"markdown","fbf27ae5":"markdown","ac026a06":"markdown","d07594e9":"markdown","30fec235":"markdown","346bb878":"markdown","1e3c210b":"markdown","02acab4f":"markdown","7f5201c6":"markdown","7a8e64a0":"markdown","6d5eb5b1":"markdown","1f11cb2a":"markdown","13d34605":"markdown","717b9b35":"markdown","c8c067e1":"markdown","c0dc4b5c":"markdown","dcff5fdc":"markdown","46ba98ec":"markdown","f8e4c152":"markdown","55d218b9":"markdown","13af6405":"markdown","92d5dfcf":"markdown","e1cc8f13":"markdown","7cd6b94a":"markdown","26170ff4":"markdown","6e78b315":"markdown","348587eb":"markdown","2ab3669f":"markdown","a47b7dc3":"markdown","f181b77e":"markdown","1ddd2e03":"markdown","f6553aec":"markdown","b40b6ed8":"markdown","5454f224":"markdown","d0f524f3":"markdown","8b9a3b9b":"markdown","c0eb7299":"markdown","792aecbe":"markdown","d4b848e2":"markdown","750550a9":"markdown","183dfc2d":"markdown","5443ab42":"markdown","27977f45":"markdown","66883e87":"markdown","e5388e71":"markdown","0b1a6b86":"markdown","2ce94a15":"markdown","d0fc9521":"markdown","11faf2e1":"markdown","f65ce6e5":"markdown","536f13da":"markdown","61ce33e6":"markdown","e9893b02":"markdown","d8d53f07":"markdown","ce9a0d2a":"markdown","991b0024":"markdown","0420a0a8":"markdown","16733a44":"markdown","83f51815":"markdown","a1e8e106":"markdown","dbf8f198":"markdown","3b81348c":"markdown","c1ae2fde":"markdown","1437f21a":"markdown","9edf88ae":"markdown","544e54fc":"markdown","ef93270b":"markdown","80554baf":"markdown","85e4f060":"markdown","0be09d14":"markdown","f7c3f127":"markdown","6d8a8e80":"markdown","9fc03b7c":"markdown","fca774c9":"markdown","6e24f3ac":"markdown","41457de5":"markdown","fa34e26a":"markdown","816d2d04":"markdown","ffa164da":"markdown","d8a013b7":"markdown","b3ba3923":"markdown","dbbd19b6":"markdown","ec841fc2":"markdown","bedd76af":"markdown","d115aa33":"markdown","42589252":"markdown","afbd4792":"markdown","2ff9f3e7":"markdown","cb467e66":"markdown","52ecfc8f":"markdown","452253f6":"markdown","1a586911":"markdown","c88a8f1a":"markdown","df737cc1":"markdown","8bd99ed6":"markdown","6a97384e":"markdown","1cad94a5":"markdown","5afdbaf2":"markdown","72a56258":"markdown","57ccebd9":"markdown","9714e951":"markdown","649e27b8":"markdown","4c93414d":"markdown","be4402f0":"markdown","5dc4eceb":"markdown","9d04bc33":"markdown","cd7f9771":"markdown","fd421ac9":"markdown"},"source":{"239c3a3f":"%%capture\n!pip install catboost","d7d9e845":"%%capture\n!pip install xgboost","c05c2961":"%%capture\n!pip install shap","175cc343":"%%capture\n!pip install optuna","c5a828f5":"%%capture\n!pip install -U keras-tuner","55f525a6":"%%capture\n!pip install hyperopt","25f4630f":"%%capture\n!pip install phik","f4a28168":"import pandas as pd\nimport numpy as np \nimport pylab\nimport scipy.stats as stats\nimport warnings\nimport optuna\nimport shap\nimport time\nimport hyperopt\n\nimport pickle\nimport phik\nfrom phik.report import plot_correlation_matrix\nfrom phik import report\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.model_selection import cross_validate, cross_val_predict\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.utils import class_weight, shuffle\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve\n\nfrom keras_tuner.tuners import RandomSearch\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.backend import backend as K\n\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.optimizers import Nadam\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)","60cc6513":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","4c7d88b9":"def get_info(data):\n    display(data.head())\n    display(data.describe().T)\n    print('----')\n    print('Information about gaps and data types')\n    print('----')\n    print()\n    display(data.info())\n    print('----')\n    print()","252e12b1":"get_info(df)","d384835f":"df['customerID'].nunique()","7c7ea0b0":"df['customerID'].duplicated().sum()","6b0d7bb7":"df.columns","a738ee75":"df.columns = ['customer_id', 'gender', 'senior_citizen', 'partner', 'dependents',\n       'tenure', 'phone_service', 'multiple_lines', 'internet_service',\n       'online_security', 'online_backup', 'device_protection', 'tech_support',\n       'streaming_tv', 'streaming_movies', 'contract', 'paperless_billing',\n       'payment_method', 'monthly_charges', 'total_charges', 'label']\ndf.columns","23b45480":"df.head()","e357d694":"df.info()","b4b98cb1":"df.loc[df['total_charges'] == ' ', 'total_charges'].count()","035661e4":"df.loc[df['total_charges'] == ' ', 'total_charges'] = df['monthly_charges']\ndf['total_charges'] = df['total_charges'].astype('float64')","f3fb54b1":"df['senior_citizen'].value_counts()","528943aa":"df['senior_citizen'] = df['senior_citizen'].astype('category')","3039e345":"df.isna().mean().sort_values(ascending=False).plot(\n    kind='bar', figsize=(15,5), \n    grid=True, color='steelblue', \n    edgecolor='black', linewidth=2\n)\nplt.title('Visualization of gaps')\nplt.xlabel('Feature')\nplt.ylabel('Fraction of gaps in a column')\nplt.show()","98e80f22":"df.head()","65f08848":"def col_name(data):\n  data.reset_index(inplace=True)\n  data.columns = ['customer_id', 'gender', 'senior_citizen', 'partner', 'dependents',\n       'tenure', 'phone_service', 'multiple_lines', 'internet_service',\n       'online_security', 'online_backup', 'device_protection', 'tech_support',\n       'streaming_tv', 'streaming_movies', 'contract', 'paperless_billing',\n       'payment_method', 'monthly_charges', 'total_charges', 'label']\n  return data\n\ndef change_types_and_label(data):\n  data.loc[data['total_charges'] == ' ', 'total_charges'] = data['monthly_charges']\n  data['total_charges'] = data['total_charges'].astype('float64')\n  data['senior_citizen'] = data['senior_citizen'].astype('category')\n  data['loyality'] =  data['contract'].apply(lambda x: 0 if x == 'Month-to-month' else 1).astype('category')\n  data['label'] = data['label'].apply(lambda x: 0 if x == 'No' else 1).astype('int64')\n  data['multiple_lines'] = data['multiple_lines'].apply(lambda x: 'No' if x == 'No phone service' else x)\n  cols = ['online_backup', 'online_backup', 'device_protection', 'tech_support', 'streaming_tv', 'streaming_movies', 'online_security']\n  for col in cols:\n    data[col] = data[col].apply(lambda x: 'No' if x == 'No internet service' else x)\n  return data\n\ndef fillnan(data):\n  data['internet_service'] = data['internet_service'].apply(lambda x: 0 if x == 'No' else 1).astype('category')\n  return data\n\ndef drop_cols(data):\n  cols = ['customer_id']\n  data.drop(cols, inplace=True, axis=1)\n  return data","c587cb76":"df = (pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv', index_col='customerID')\n    .pipe(col_name)\n    .pipe(change_types_and_label)\n    .pipe(fillnan)\n    .pipe(drop_cols)\n)","cc1db07b":"df.head()","b769f944":"df.info()","433bb179":"df.head()","d828c9aa":"cols = ['monthly_charges', 'total_charges']","6c2a2515":"df[cols].hist(bins=50, figsize=(15,5), edgecolor='black', linewidth=2)\nplt.show()","53215485":"df[['monthly_charges', 'total_charges']].describe().T","4f541aa6":"df.groupby('label')['monthly_charges'].agg('mean')","cb2b5d2b":"df_churn = df.query('label == 1')\ndf_clients = df.query('label == 0')\n\n\nsample_1 = df_churn['monthly_charges'][df_churn['monthly_charges'].notna()].tolist()\nsample_2 = df_clients['monthly_charges'][df_clients['monthly_charges'].notna()].tolist()\nalpha = .05\nstat, p = stats.levene(sample_1, sample_2)\nresult_levene = p\n\n\n\nif result_levene < alpha:\n    print('Reject H0, variances differ')\nelse:\n    print('We do not reject H0, variances do not differ')\n\nvar_sample = [np.var(x, ddof=1) for x in [sample_1, sample_2]]\nprint('Sample variances: ', var_sample)","9e516289":"alpha = .05 \nresults = stats.mannwhitneyu(sample_1, sample_2, alternative='two-sided')\n\nprint('p-VALUE:', results.pvalue)\n\nif results.pvalue < alpha:\n    print(\"Reject \u041d0\")\nelse:\n    print(\"Do not reject \u041d0\")","ad9c0c2b":"def qq_plt(data):\n  measurements = data\n  stats.probplot(measurements, dist=\"norm\", plot=pylab)\n  plt.title('QQ-plot')\n  plt.xlabel('Theoretical quantiles')\n  plt.ylabel('Ordered values')\n  pylab.show()\n\ndef kde_plot(data, col):\n  plt.figure(figsize=(10,5))\n  sns.kdeplot(data=data, shade=True, \n                 alpha=.5, linewidth=4.2\n  )\n  plt.xlabel(f\"{col}\")\n  plt.title('KDE')\n  plt.show()","3a48b314":"qq_plt(df['total_charges'])","55559d2e":"qq_plt(df['monthly_charges'])","c56da336":"kde_plot(df['total_charges'], 'total_charges')","e22f9caf":"kde_plot(df['monthly_charges'], 'monthly_charges')","e15ba799":"def plot_hist(data, col):\n  plt.figure(figsize=(12,6))\n  data[col].hist(ec='black', alpha=0.84)\n  data[data['label'] == 1][col].hist(ec='black', alpha=0.84)\n  plt.legend(['All users', 'Churn users'])\n  plt.xticks(rotation=42)\n  plt.title(f\"Churn on feature {col}\")\n  plt.ylabel('Quantity of users')\n  plt.show()","61579c8a":"(df.groupby('payment_method')['label'].agg(['sum', 'count'])\n.assign(ratio = lambda x: x['sum'] \/ x['count'])\n).sort_values(by='ratio', ascending=False).style.background_gradient(low=0.42)","4e4319cb":"plot_hist(df, 'payment_method')","941698f8":"(df.groupby('gender')['label'].agg(['sum', 'count'])\n.assign(ratio = lambda x: x['sum'] \/ x['count'])\n).sort_values(by='ratio', ascending=False).style.background_gradient(low=0.42)","6d70b854":"plot_hist(df, 'gender')","e2fec991":"(df.groupby('contract')['label'].agg(['sum', 'count'])\n.assign(ratio = lambda x: x['sum'] \/ x['count'])\n).sort_values(by='ratio', ascending=False).style.background_gradient(low=0.42)","2b4aa06f":"plot_hist(df, 'contract')","1d36f697":"(df.groupby('internet_service')['label'].agg(['sum', 'count'])\n.assign(ratio = lambda x: x['sum'] \/ x['count'])\n).sort_values(by='ratio', ascending=False).style.background_gradient(low=0.42)","79c278f8":"plot_hist(df, 'internet_service')","3e70fc9d":"def plot_hist_groups(data, cols, n_cols):\n\n  fig, ax = plt.subplots(nrows=1, ncols=n_cols, figsize = (15,4))\n  for i, col in enumerate(cols):\n\n      ax[i].hist(data[col], ec='black', alpha=0.84)\n      ax[i].hist(data[data['label'] == 1][col], ec='black', alpha=0.84)\n      ax[i].set_title(f\"Churn on feature {col}\")\n      ax[i].legend(['All users', 'Churn users'])\n\n  plt.ylabel('Quantity of users')\n  fig.autofmt_xdate()\n  plt.tight_layout()\n  plt.show()","5b1a8fbc":"cols = ['online_security', 'online_backup', 'device_protection', 'tech_support']\nplot_hist_groups(df, cols, 4)","0d8f9801":"cols = ['streaming_tv', 'streaming_movies', 'multiple_lines']\nplot_hist_groups(df, cols, 3)\n","5b8e30d9":"cols = ['partner', 'dependents']\nplot_hist_groups(df, cols, 2)","f156f38e":"df.head()","946d8b3c":"corr_check = df[['monthly_charges', 'total_charges', 'tenure']].copy()","dd0cb1f5":"plot_values = df[['monthly_charges', 'total_charges', 'tenure', 'label']].copy()","a835ba70":"map_corr = sns.pairplot(plot_values, hue='label', markers=[\"o\", \"D\"])\n(map_corr.map_lower(sns.kdeplot, levels=4, color=\".1\")\n.fig.suptitle('Scatter plot'))\nplt.show()","804306d4":"corr_check.corr(method='pearson')","5c71b5ec":"phik_overview = corr_check.phik_matrix()\nphik_overview.round(2)","384c0447":"X = df.copy()\ny = X.pop('label')\n\nfor col in list(X.select_dtypes(['object', 'category']).columns):\n    X[col], _ = X[col].factorize()\n\ndiscrete_features = X.dtypes == int","670cdcaa":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name='mutual_information \u043e\u0446\u0435\u043d\u043a\u0430', index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3].to_frame('Mi_scores')","491c64c9":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores, ec='black', alpha=0.84)\n    plt.yticks(width, ticks)\n    plt.title('Mutual information')\n\n\nplt.figure(dpi=100, figsize=(15, 6))\nplot_mi_scores(mi_scores)","de7b97c1":"print('Features for training model and target label')\nprint('___________________________')\nfor col in list(df.columns):\n  print(col)","8fd530be":"df['label'].value_counts(normalize=True).to_frame('Class balance')","d886d18a":"def split_data(data, target_column):\n    return data.drop(columns=[target_column], axis=1), data[target_column]","e51ecf60":"features, target = split_data(df, 'label')\nfeatures.shape, target.shape","eb3f3e9a":"X_train , X_test , y_train , y_test, = train_test_split ( \n                                features, target, test_size=0.25, random_state=42)\n","cb5eb13c":"X_train.shape[0] + X_test.shape[0]","43b72b57":"y_train.shape, y_test.shape","3cf7b91b":"optimal_clusters = X_train[['monthly_charges', 'total_charges']]","50fede9c":"distortion = []\nK = range(1, 8)\nfor k in K:\n    model = KMeans(n_clusters=k, random_state=42)\n    model.fit(optimal_clusters)\n    distortion.append(model.inertia_) ","2d430eda":"plt.figure(figsize=(12, 8))\nplt.plot(K, distortion, 'bx-')\nplt.xlabel('Clasters')\nplt.ylabel('Objective function value')\nplt.show() ","ddf2411b":"model = KMeans(n_clusters=3, random_state=42)\nmodel.fit(optimal_clusters)\nprint(\"Typical segment users for 3 clusters:\")\nprint(np.round(model.cluster_centers_))","f3761d84":"print('Centroids')\ncentroids = pd.DataFrame(model.cluster_centers_, columns=optimal_clusters.columns)\noptimal_clusters['label'] = model.labels_.astype(str)\ncentroids['label'] = ['0 centroid', '1 centroid', '2 centroid']\ndata_cluster = pd.concat([optimal_clusters, centroids], ignore_index=True)\nsns.pairplot(data_cluster, hue='label', diag_kind='hist')\nplt.show()\n","5682e057":"X_train.info()","9a4bc33b":"cluster_cols = ['monthly_charges', 'total_charges']\nnum_features = list(X_train.select_dtypes(include=['int64', 'float64']).columns)\ncat_features = ['payment_method', 'contract']\nbinary_features = list(set(list(X_train.select_dtypes(exclude=['int64', 'float64']).columns)) - set(cat_features) - set(['internet_service']))","9bed092a":"X_train , X_test , y_train , y_test, = train_test_split ( \n                                features, target, test_size=0.25, random_state=42)\n","8f6ff5ca":"class AttributAdder(BaseEstimator, TransformerMixin):\n\n    \n    def __init__(self, attr = True):\n        self.attr = attr\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n\n        kmeans = KMeans(n_clusters=3, random_state=42)\n        X['cluster'] = kmeans.fit_predict(X)\n        X['cluster'] = X['cluster'].astype(\"category\")\n        \n        return X","bfcf19e1":"def delete_cols_array(X):\n    arr = np.array([1, 19, 20])\n    X = np.delete(X, arr, 1)\n    return X\n\n\n\ndef return_DF(X):\n    cols = (['monthly_charges', 'tenure'] + \n            cat_features +  \n            binary_features +\n            ['cluster'])\n\n    X = pd.DataFrame(X, columns=cols)\n    cols_type = (cat_features +  \n                binary_features +\n                ['cluster'])\n    for col in cols:\n         X[col] = X[col].astype('float64')\n    return X    ","65349931":"preprocessor = ColumnTransformer (\n    transformers = [\n        \n        ('std_scaler' , StandardScaler() , num_features) ,\n        ('cat' , OrdinalEncoder() , cat_features) ,\n        ('binary' , OneHotEncoder(drop='first') , binary_features),\n        ('kmeans', AttributAdder(), cluster_cols),\n        \n        ]\n)\n\npipe = Pipeline(steps=\n    \n    [\n     ('preprocessor', preprocessor),\n     ('delete_cols_array', FunctionTransformer(delete_cols_array, validate=False)),\n     ('return_DF', FunctionTransformer(return_DF, validate=False))\n\n     ]\n)\n","dc25e0b5":"X_train.shape","d20aefca":"X_train.head()","3455ca1d":"X_train_prep = pipe.fit_transform(X_train)","63d5f313":"X_train_prep.head()","96b8cec6":"X_train_prep.shape","b48f3f07":"auc = []\naccuracy = []\ntraining_time = []\nprediction_time = []\nmodel_name = []","181aa1e7":"y_train.value_counts(normalize=True).to_frame('Calss balance')","dde5f246":"X_test_prep = pipe.transform(X_test)","4bbd9211":"X_train_prep.shape, X_test_prep.shape, y_train.shape, y_test.shape","44a0d04a":"X, X_val, y, y_val = train_test_split(X_train_prep, y_train, test_size=0.2,\n                                      random_state=42)","9a8edb20":"%%time\nstart = time.time()\n\nclasses_weights = class_weight.compute_sample_weight(\n                  class_weight='balanced',\n                  y = y_train)\nmodel = XGBClassifier(sample_weight=classes_weights,\n                                    random_state=42,\n                                    eval_metric ='logloss')\nmodel.fit(X, y, eval_set = [(X_val, y_val)],\n              early_stopping_rounds=200, verbose=50)\n\nend = time.time()\nelapsed = end - start","adfce8da":"start_pred = time.time()\ny_pred = model.predict_proba(X_test_prep)[:, 1]\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred\n\ny_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\nauc.append(roc_auc_score(y_test, y_pred))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('\u0431\u0430\u0437\u043e\u0432\u044b\u0439')))","5ebb573f":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred))","7bd7f12b":"classes_weights = class_weight.compute_sample_weight(\n                  class_weight='balanced',\n                  y = y_train)\nmodel = XGBClassifier(sample_weight=classes_weights,\n                                    random_state=42,\n                                    eval_metric ='logloss')\nmodel.fit(X, y, eval_set = [(X_val, y_val)],\n              early_stopping_rounds=200, verbose=50)\nprobabilities_valid = model.predict_proba(X_test_prep)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfor threshold in np.arange(0, 0.95, 0.05):\n    predicted_valid = probabilities_one_valid > threshold\n    precision = precision_score(y_test, predicted_valid)\n    recall = recall_score(y_test, predicted_valid)\n    f1 = f1_score(y_test, predicted_valid)\n    print(\"Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f} | F1-score = {:.3f}\".format(\n        threshold, precision, recall, f1))\n\nprecision, recall, thresholds = precision_recall_curve(y_test, probabilities_valid[:, 1])    \nplt.figure(figsize=(10, 10))\nplt.step(recall, precision, where='post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('PR curve')\nplt.show() ","fe298570":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, X)","bb1a1bcd":"%%capture\ntrain_X = features.copy()\ntrain_X.drop('total_charges', axis=1, inplace=True)","dd805922":"X_train , X_test , y_train , y_test, = train_test_split ( \n                                train_X, target, test_size=0.25, random_state=42)","e41ccbbb":"cat_features_light = list(X_train.select_dtypes(include=['object', 'category']))","c1c61649":"len(cat_features_light)","4c372a0e":"X_train_lgbm = X_train.copy()\nX_test_lgbm = X_test.copy()\nX_train_lgbm[cat_features_light] = (X_train[cat_features_light]\n                                    .apply(lambda x: x.astype('category')))\nX_test_lgbm[cat_features_light] = (X_test[cat_features_light]\n                                   .apply(lambda x: x.astype('category')))","524ade77":"X_train_lgbm.info()","03086d75":"model = LGBMClassifier(random_state=42, class_weight = 'balanced')","f1cb8d58":"cv = ShuffleSplit(n_splits=5, test_size=0.25, random_state=42)\nparams_space = {'n_estimators': [500, 1000],\n                  'learning_rate': [0.01, 0.4],\n                  'max_depth': [10, 25],\n                  'num_leaves':[31, 40, 60]}\n\ngrid_cv = GridSearchCV(model, params_space, scoring='roc_auc', cv=cv)\ngrid_cv.fit(X_train_lgbm, y_train)","aeabdc27":"grid_cv.best_params_","96155152":"estimator = grid_cv.best_estimator_","366bef61":"start_pred = time.time()\ny_pred = estimator.predict_proba(X_test_lgbm)[:, 1]\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred\n\ny_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\nauc.append(roc_auc_score(y_test, y_pred))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str('LightGBM')+str(' ')+str('GridSearCVcategory')))","7d1b7800":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred))","36ef03e9":"fi = (pd.DataFrame({'feature': list(X_train.columns), \n                    'importance': estimator.feature_importances_})\n.sort_values(by='importance', ascending=False))","785744fa":"plt.figure(figsize=(12, 6));\nsns.barplot(x=\"importance\", y=\"feature\", data=fi);\nplt.title('LGBM features importance:');","eee8729a":"%%time\nstart = time.time()\n\nclasses_weights = class_weight.compute_sample_weight(\n                  class_weight='balanced',\n                  y = y_train)\n\nest = XGBClassifier(sample_weight=classes_weights,\n                                    random_state=42,\n                                    eval_metric ='logloss')\n\nscore = cross_validate(est, X_train_prep, y_train,\n                           scoring=['roc_auc', 'accuracy'],\n                           n_jobs=-1, verbose=0)\nend = time.time()\nelapsed = end - start\nsorted(score.keys())","971d41d8":"score['test_accuracy']","e9ac0be0":"score['test_roc_auc']","7d90dcee":"start_pred = time.time()\ny_pred = cross_val_predict(est, X_train_prep, y_train, n_jobs=-1, \n                           verbose=0, method='predict_proba')[:, 1]\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred","1435d2ae":"y_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_train, y_bin))\nauc.append(roc_auc_score(y_train, y_pred))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str('XGBClassifier')+str(' ')+str('sklearn CV')))","1a37e091":"fpr, tpr, thresholds = roc_curve(y_train, y_pred)\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_train, y_pred))","ac39321e":"%%time\n\nprediction = np.zeros(X_test_prep.shape[0])\nn_fold = 5 \nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) \nrocauc_report = []\nidx=0\n\nfor train_idx, test_idx in folds.split(X_train_prep, y_train):\n    start = time.time()\n    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))\n    X_train_fold, X_val_fold = X_train_prep.iloc[train_idx], X_train_prep.iloc[test_idx]\n    y_tr_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    classes_weights = class_weight.compute_sample_weight(\n                      class_weight='balanced',\n                      y = y_tr_fold)\n    clone_clf = clone(XGBClassifier(sample_weight=classes_weights,\n                                    random_state=42,\n                                    eval_metric ='logloss'))\n    clone_clf.fit(X_train_fold, y_tr_fold, eval_set = [(X_val_fold, y_val_fold)],\n              early_stopping_rounds=200, verbose=50)\n    y_pred = clone_clf.predict_proba(X_test_prep)[:, 1]\n    prediction += y_pred\n    rocauc_report.append(roc_auc_score(y_val_fold, clone_clf.predict_proba(X_val_fold)[:, 1], average ='macro'))\n    print(idx+1, rocauc_report[idx])\n    idx +=1\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","96efde62":"y_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\nauc.append(roc_auc_score(y_test, prediction, average ='macro'))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed)\nmodel_name.append((str(clone_clf.__class__.__name__)+str(' ')+str('CV')))","36bb93dd":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred, average ='macro'))","852c8d47":"%%capture\ntrain_X = features.copy()\ntrain_X.drop('total_charges', axis=1, inplace=True)","0fb5e080":"X_train , X_test , y_train , y_test, = train_test_split ( \n                                train_X, target, test_size=0.25, random_state=42)","8be8d044":"cat_features = list(X_train.select_dtypes(include=['object', 'category']).columns)","9dfd4d63":"X, X_val, y, y_val = train_test_split(X_train, y_train, \n                                      test_size=0.2, random_state=42)","6de7700b":"start = time.time()\n\ntrain_data = Pool(data=X, \n                  label=y,\n                  cat_features=cat_features)\nvalid_data = Pool(data=X_val, \n                  label=y_val,\n                  cat_features=cat_features)\ntest_data = Pool(data=X_test,\n                 cat_features=cat_features)\n\nparams = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'verbose': 200,\n          'random_seed': 42\n         }\n\nmodel = CatBoostClassifier(**params)\nmodel.fit(train_data,\n          eval_set=valid_data, \n          use_best_model=True\n          )\nend = time.time()\nelapsed = end - start","40e0ca1a":"start_pred = time.time()\ny_pred = model.predict_proba(X_test)[:, 1]\n\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred\n\nauc.append(roc_auc_score(y_test, y_pred, average ='macro'))\ny_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('\u0431\u0430\u0437\u043e\u0432\u044b\u0439')))","4d95a3aa":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred, average ='macro'))","b5188b2b":"fi = model.get_feature_importance(prettified=True)","ea2c165f":"plt.figure(figsize=(12, 6));\nsns.barplot(x=\"Importances\", y=\"Feature Id\", data=fi);\nplt.title('CatBoost features importance:');","ce4ef716":"save_best = list(fi.loc[fi['Importances'] >= 1.5]['Feature Id'])","d399dea4":"%%time\nstart = time.time()\n\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n\nparams_cat = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'verbose': 200,\n          'random_seed': 42\n         }\n\ntest_data = Pool(data=X_test,\n                 cat_features=cat_features)\nscores = []\nprediction = np.zeros(X_test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n    X_train_cat, X_valid_cat = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_cat, y_valid_cat = y_train.iloc[train_index], y_train.iloc[valid_index]\n    \n    train_data = Pool(data=X_train_cat, \n                      label=y_train_cat,\n                      cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                      label=y_valid_cat,\n                      cat_features=cat_features)\n    \n    model = CatBoostClassifier(**params_cat)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    score = model.get_best_score()['validation']['AUC']\n    scores.append(score)\n\n    y_pred = model.predict_proba(test_data)[:, 1]\n    prediction += y_pred\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","d6a3d978":"auc.append(roc_auc_score(y_test, prediction, average ='macro'))\ny_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('CV')))","bcc62e15":"fpr, tpr, thresholds = roc_curve(y_test, prediction) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, prediction, average ='macro'))","f4570e6c":"def objective(trial,data=X_train_prep,target=y_train):\n    \n    train_x, test_x, train_y, test_y = train_test_split(X_train_prep, y_train, test_size=0.25,\n                                                        random_state=42)\n    param = {\n\n\n        'alpha': trial.suggest_uniform('alpha', 15, 22),\n        'gamma': 0.1,\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 0.4),\n        'subsample': trial.suggest_categorical('subsample', [0.7,0.8]),\n        'max_depth': trial.suggest_categorical('max_depth', [3, 5]),\n        'random_state': 42,\n        'booster': 'gbtree',\n        'n_jobs': 4\n    }\n\n    classes_weights = class_weight.compute_sample_weight(\n                      class_weight=None,\n                      y = train_y)\n    model = XGBClassifier(**param, sample_weight=classes_weights)\n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=50,verbose=False)\n    preds = model.predict_proba(test_x)[:, 1]\n    auc = roc_auc_score(test_y, preds, average ='macro')\n    return auc\n","640754b6":"%%capture\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=15)\nprint('Number of completed tests:', len(study.trials))\nprint('Best Trial:', study.best_trial.params)","593c8d3b":"optuna.visualization.plot_optimization_history(study)","b7da1abe":"params_XGB = study.best_trial.params","90a55f68":"%%time\nstart = time.time()\nprediction = np.zeros(X_test_prep.shape[0])\nn_fold = 5 \nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) \nrocauc_report = []\nidx=0\n\nfor train_idx, test_idx in folds.split(X_train_prep, y_train):\n    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))\n    X_train_fold, X_val_fold = X_train_prep.iloc[train_idx], X_train_prep.iloc[test_idx]\n    y_tr_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    classes_weights = class_weight.compute_sample_weight(\n                      class_weight='balanced',\n                      y = y_tr_fold)\n    clone_clf = clone(XGBClassifier(**params_XGB, sample_weight=classes_weights,\n                                    random_state=42,\n                                    eval_metric ='logloss'))\n    clone_clf.fit(X_train_fold, y_tr_fold, eval_set = [(X_val_fold, y_val_fold)],\n              early_stopping_rounds=200, verbose=50)\n    y_pred = clone_clf.predict_proba(X_test_prep)[:, 1]\n    prediction += y_pred\n    rocauc_report.append(roc_auc_score(y_val_fold, clone_clf.predict_proba(X_val_fold)[:, 1], average ='macro'))\n    print(idx+1, rocauc_report[idx])\n    idx +=1\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","2d9e19fa":"auc.append(roc_auc_score(y_test, prediction, average ='macro'))\ny_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed)\nmodel_name.append((str(clone_clf.__class__.__name__)+str(' ')+str('Optuna')))","a6e0cb28":"fpr, tpr, thresholds = roc_curve(y_test, prediction) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, prediction, average ='macro'))","445da588":"cat_features","d256e5e2":"def objective(trial,data=X_train,target=y_train):\n    \n    X_train_cat, X_valid_cat, y_train_cat, y_valid_cat = train_test_split(X_train, y_train, test_size=0.25,\n                                                        random_state=42)\n    params_space = {\n\n        \n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1),\n        'objective': 'Logloss',\n        'depth': trial.suggest_int('depth', 2, 9),\n        'boosting_type': 'Ordered',\n        'bootstrap_type': 'MVS',\n        'l2_leaf_reg': 1,\n        'n_estimators': 200,\n        'learning_rate':trial.suggest_float('learning_rate', 0.1, 0.4)\n    }\n\n\n    train_data = Pool(data=X_train_cat, \n                  label=y_train_cat,\n                  cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                      label=y_valid_cat,\n                      cat_features=cat_features)\n\n    model = CatBoostClassifier(**params_space, random_seed=42, verbose=False)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    preds = model.predict_proba(X_valid_cat)[:, 1]\n    auc = roc_auc_score(y_valid_cat, preds, average ='macro')\n    return auc\n","a9700e71":"%%capture\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=15, timeout=600)\nprint('Number of completed tests:', len(study.trials))\nprint('Best Trial:', study.best_trial.params)","925882cd":"optuna.visualization.plot_optimization_history(study)","dc568d83":"params_cat_2 = study.best_trial.params","368901a6":"%%time\nstart = time.time()\n\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n\n\ntest_data = Pool(data=X_test,\n                 cat_features=cat_features)\nscores = []\nprediction = np.zeros(X_test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n    X_train_cat, X_valid_cat = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_cat, y_valid_cat = y_train.iloc[train_index], y_train.iloc[valid_index]\n    \n    train_data = Pool(data=X_train_cat, \n                      label=y_train_cat,\n                      cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                      label=y_valid_cat,\n                      cat_features=cat_features)\n    \n    model = CatBoostClassifier(**params_cat_2, \n                               loss_function='Logloss',\n                               eval_metric='AUC',\n                               random_seed=42, \n                               verbose=200)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    score = model.get_best_score()['validation']['AUC']\n    scores.append(score)\n\n    y_pred = model.predict_proba(test_data)[:, 1]\n    prediction += y_pred\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","83113723":"auc.append(roc_auc_score(y_test, prediction, average ='macro'))\ny_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('Optuna')))","e177d466":"fpr, tpr, thresholds = roc_curve(y_test, prediction)\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, prediction, average ='macro'))","c8c85cdc":"def create_baseline():\n\n\tmodel = Sequential()\n\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\tmodel.compile(loss='binary_crossentropy', \n               optimizer=Nadam(lr=0.0001), \n               metrics=['accuracy'])\n\treturn model","1fc3a9d1":"blender = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=8, verbose=0)","c6b8ef33":"%%time\n%%capture\nstart = time.time()\nX_train_st, X_val_st, y_train_st, y_val_st = train_test_split(\n        X_train_prep, y_train, test_size=0.25, random_state=42)\nX, X_val_eval, y, y_val_eval = train_test_split(X_train_st, y_train_st, \n                                                test_size=0.20,\n                                                random_state=42)\nclasses_weights = class_weight.compute_sample_weight(\n                  class_weight='balanced',\n                  y = y)\n\nXGB_1 = XGBClassifier(random_state=42, eval_metric ='logloss',\n                      sample_weight=classes_weights)\nXGB_2 = XGBClassifier(**params_XGB, random_state=42, eval_metric ='logloss',\n                      sample_weight=classes_weights)\nCAT_1 = CatBoostClassifier(**params_cat)\nCAT_2 = CatBoostClassifier(**params_cat_2)\nestimators = [XGB_1, XGB_2, CAT_1, CAT_2]\n# training models to build a meta-student training set\nfor estimator in estimators:\n    print('\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435', estimator)\n\n    estimator.fit(X, y, eval_set=[(X_val_eval, y_val_eval)], verbose=200)\n\nX_val_predictions = np.empty((X_val_st.shape[0], len(estimators)), dtype=np.float32)\n# collecting a set of prediction models\nfor index, estimator in enumerate(estimators):\n    X_val_predictions[:, index] = estimator.predict_proba(X_val_st)[:, 1]\n\n# teach the meta-student\nblender.fit(X_val_predictions, y_val_st)\n\nend = time.time()\nelapsed = end - start","b33d4da8":"start_pred = time.time()\n\nX_test_predictions = np.empty((X_test.shape[0], len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_test_predictions[:, index] = estimator.predict_proba(X_test_prep)[:, 1]\n\ny_pred = blender.predict_proba(X_test_predictions)[:, 1]\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred","75c3366f":"auc.append(roc_auc_score(y_test, y_pred, average ='macro'))\ny_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(blender.__class__.__name__)+str(' ')+str('Stacking')))","8dce64cc":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred, average ='macro'))","c39ec62e":"def build_model(hp):\n  model = Sequential()\n  activation_choice = hp.Choice('activation', values=['relu', 'selu', 'elu'])\n  model.add(Dense(units=hp.Int('units_input',    \n                                  min_value=4,    \n                                  max_value=32,   \n                                  step=2),  \n                  input_dim=X_val_predictions.shape[1], activation=activation_choice))\n  model.add(Dense(units=hp.Int('units_input',    \n                              min_value=4,    \n                              max_value=32,   \n                              step=2),  \n                  input_dim=X_val_predictions.shape[1], activation=activation_choice))\n  model.add(Dense(1, activation='sigmoid'))\n  model.compile(loss='binary_crossentropy', \n                optimizer=Nadam(lr=0.001), \n                metrics=['accuracy'])\n  return model","2ceddc3f":"tuner = RandomSearch(\n    build_model,                 \n    objective='loss',    \n                        \n    max_trials=10,                \n    directory='test_directory_01',\n    overwrite=True   \n    )\n\n\nearly_stopping = EarlyStopping(\n\n    mode='min', \n    patience=20,\n    restore_best_weights=True,\n)","56d0f008":"tuner.search_space_summary()","f2f78534":"tuner.search(X_val_predictions, y_val_st,           \n             batch_size=4,           \n             epochs=20, validation_split=0.25\n             )","8ebd5d8d":"models = tuner.get_best_models(num_models=2)","5f18dc26":"for model in models:\n  model.summary()\n  model.evaluate(X_val_predictions, y_val_st)\n  print()  ","faa1cdf9":"def create_baseline():\n\n\tmodel = Sequential()\n\tmodel.add(Dense(22, input_dim=4, activation='selu'))\n\tmodel.add(Dense(22, activation='selu'))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\tmodel.compile(loss='binary_crossentropy', \n\t\t\t\t\t\t\t\toptimizer=Nadam(lr=0.001), \n\t\t\t\t\t\t\t\tmetrics=['accuracy'])\n\treturn model","4150d9db":"blender = KerasClassifier(build_fn=create_baseline, epochs=50, batch_size=4, verbose=0)","9cf2a023":"%%time\n%%capture\nstart = time.time()\nX_train_st, X_val_st, y_train_st, y_val_st = train_test_split(\n        X_train_prep, y_train, test_size=0.25, random_state=42)\nX, X_val_eval, y, y_val_eval = train_test_split(X_train_st, y_train_st, \n                                                test_size=0.20,\n                                                random_state=42)\nclasses_weights = class_weight.compute_sample_weight(\n                  class_weight='balanced',\n                  y = y)\n\nXGB_1 = XGBClassifier(random_state=42, eval_metric ='logloss',\n                      sample_weight=classes_weights)\nXGB_2 = XGBClassifier(**params_XGB, random_state=42, eval_metric ='logloss',\n                      sample_weight=classes_weights)\nCAT_1 = CatBoostClassifier(**params_cat)\nCAT_2 = CatBoostClassifier(**params_cat_2, \n                               loss_function='Logloss',\n                              eval_metric='AUC',\n                               random_seed=42, \n                               verbose=200)\nestimators = [XGB_1, XGB_2, CAT_1, CAT_2]\nfor estimator in estimators:\n    print('\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435', estimator)\n\n    estimator.fit(X, y, eval_set=[(X_val_eval, y_val_eval)], verbose=200)\n\nX_val_predictions = np.empty((X_val_st.shape[0], len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_val_predictions[:, index] = estimator.predict_proba(X_val_st)[:, 1]\n\n\nblender.fit(X_val_predictions, y_val_st)\n\nend = time.time()\nelapsed = end - start","ebf37b68":"start_pred = time.time()\n\nX_test_predictions = np.empty((X_test.shape[0], len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_test_predictions[:, index] = estimator.predict_proba(X_test_prep)[:, 1]\n\ny_pred = blender.predict_proba(X_test_predictions)[:, 1]\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred","1c2ba83e":"auc.append(roc_auc_score(y_test, y_pred, average ='macro'))\ny_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(blender.__class__.__name__)+str(' ')+str('KerasTuner')))","cac86d22":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred, average ='macro'))","3b1a2d77":"train, valid, target_train, target_valid = train_test_split(\n    features, target, test_size=0.25, random_state=42)\n\ndef upsample(features, target, repeat):\n    \n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n    features_upsampled = shuffle(features_upsampled, random_state=42)\n    target_upsampled = shuffle(target_upsampled, random_state=42)\n    return features_upsampled, target_upsampled\n\nfeatures_upsampled, target_upsampled = upsample(train, target_train, 3)\n\nprint(features_upsampled.shape)\nprint(target_upsampled.shape)","b0a65c8f":"target_upsampled.value_counts(normalize=True)","79bc1c2c":"%%time\nstart = time.time()\n\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n\nparams_cat = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'verbose': 200,\n          'random_seed': 42\n         }\n\ntest_data = Pool(data=valid,\n                 cat_features=cat_features)\nscores = []\nprediction = np.zeros(X_test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(features_upsampled, target_upsampled)):\n    X_train_cat, X_valid_cat = features_upsampled.iloc[train_index], features_upsampled.iloc[valid_index]\n    y_train_cat, y_valid_cat = target_upsampled.iloc[train_index], target_upsampled.iloc[valid_index]\n    \n    train_data = Pool(data=X_train_cat, \n                      label=y_train_cat,\n                      cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                      label=y_valid_cat,\n                      cat_features=cat_features)\n    \n    model = CatBoostClassifier(**params_cat)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    score = model.get_best_score()['validation']['AUC']\n    scores.append(score)\n\n    y_pred = model.predict_proba(test_data)[:, 1]\n    prediction += y_pred\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","1134e967":"auc.append(roc_auc_score(target_valid, prediction, average ='macro'))\ny_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(target_valid, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('CV-Upsampling')))","c52755d6":"fpr, tpr, thresholds = roc_curve(y_test, prediction) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, prediction, average ='macro'))","aa930ffb":"def objective(trial,data=X_train,target=y_train):\n    \n    X_train_cat, X_valid_cat, y_train_cat, y_valid_cat = train_test_split(features_upsampled, \n                                                                          target_upsampled, \n                                                                          test_size=0.25,\n                                                                           random_state=42)\n    params_space = {\n\n        \n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1),\n        'objective': 'Logloss',\n        'depth': trial.suggest_int('depth', 2, 9),\n        'boosting_type': 'Ordered',\n        'bootstrap_type': 'MVS',\n        'l2_leaf_reg': 1,\n        'n_estimators': 1000,\n        'learning_rate':trial.suggest_float('learning_rate', 0.1, 0.4)\n    }\n\n\n    train_data = Pool(data=X_train_cat, \n                  label=y_train_cat,\n                  cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                  label=y_valid_cat,\n                  cat_features=cat_features)\n\n    model = CatBoostClassifier(**params_space, random_seed=42, verbose=False)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    preds = model.predict_proba(X_valid_cat)[:, 1]\n    auc = roc_auc_score(y_valid_cat, preds, average ='macro')\n    return auc\n","4bee6c69":"%%capture\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=15, timeout=600)\nprint('Number of completed tests:', len(study.trials))\nprint('Best Trial:', study.best_trial.params)","e6934f31":"optuna.visualization.plot_optimization_history(study)","d5272da0":"params_cat_2 = study.best_trial.params","fb990d6e":"%%time\nstart = time.time()\n\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n\n\ntest_data = Pool(data=X_test,\n                 cat_features=cat_features)\nscores = []\nprediction = np.zeros(X_test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n    X_train_cat, X_valid_cat = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_cat, y_valid_cat = y_train.iloc[train_index], y_train.iloc[valid_index]\n    \n    train_data = Pool(data=X_train_cat, \n                      label=y_train_cat,\n                      cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                      label=y_valid_cat,\n                      cat_features=cat_features)\n    \n    model = CatBoostClassifier(**params_cat_2, \n                               loss_function='Logloss',\n                               eval_metric='AUC',\n                               random_seed=42, \n                               verbose=200,\n                               boosting_type='Ordered',\n                               bootstrap_type='MVS',\n                               l2_leaf_reg= 1,\n                               n_estimators= 1000)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    score = model.get_best_score()['validation']['AUC']\n    scores.append(score)\n\n    y_pred = model.predict_proba(test_data)[:, 1]\n    prediction += y_pred\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","ab02c753":"auc.append(roc_auc_score(y_test, prediction, average ='macro'))\ny_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('Optuna-Upsampling')))","9e205163":"fpr, tpr, thresholds = roc_curve(y_test, prediction)\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, prediction, average ='macro'))","7d87dc87":"log_clf = LogisticRegression(solver=\"liblinear\", random_state=42,\n                            class_weight='balanced', max_iter=500)\nrnd_clf = RandomForestClassifier(n_estimators=500, random_state=42, \n                            class_weight='balanced')\nsvm_clf = SVC(gamma=\"auto\", probability=True, random_state=42, \n                            class_weight='balanced')\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), \n                ('svc', svm_clf)],\n    voting='soft')\n\n\nstart = time.time()\nvoting_clf.fit(X_train_prep, y_train)\nend = time.time()\nelapsed = end - start\n","523528ed":"start_pred = time.time()\npredict = voting_clf.predict_proba(X_test_prep)[:, 1]\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred","f9cb4042":"auc.append(roc_auc_score(y_test, predict, average ='macro'))\ny_bin = (predict >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(voting_clf.__class__.__name__)))","95e8da15":"fpr, tpr, thresholds = roc_curve(y_test, predict) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, predict, average ='macro'))","40d99db9":"X_train.head()","1ccf91b3":"def users_bin (data):\n\n    if data['internet_service'] == 1 and data['multiple_lines']=='Yes':\n        return 0\n    elif data['internet_service'] == 1:\n        return 1\n    elif data['multiple_lines']=='Yes':\n        return 2\n    else:\n        return 3","4f4b92ed":"X_train['user_cat'] = X_train.apply(users_bin, axis=1)\nX_test['user_cat'] = X_test.apply(users_bin, axis=1)","6348886d":"cols_del = list(set(X_train.columns) - set(save_best))","58d86720":"cols_del","66674a7f":"X_train.drop(cols_del, axis=1, inplace=True\n             )\nX_test.drop(cols_del, axis=1, inplace=True\n             )\ncat_features = list(X_train.select_dtypes(include=['object', 'category']).columns)","508473bb":"params_cat_2 = study.best_trial.params","ef56a5a6":"%%time\n%%capture\nstart = time.time()\n\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n\n\ntest_data = Pool(data=X_test,\n                 cat_features=cat_features)\nscores = []\nprediction = np.zeros(X_test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n    X_train_cat, X_valid_cat = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_cat, y_valid_cat = y_train.iloc[train_index], y_train.iloc[valid_index]\n    \n    train_data = Pool(data=X_train_cat, \n                      label=y_train_cat,\n                      cat_features=cat_features)\n    valid_data = Pool(data=X_valid_cat, \n                      label=y_valid_cat,\n                      cat_features=cat_features)\n    \n    model = CatBoostClassifier(**params_cat_2, \n                               loss_function='Logloss',\n                              eval_metric='AUC',\n                               random_seed=42, \n                               verbose=200)\n    model.fit(train_data,\n              eval_set=valid_data, \n              use_best_model=True\n             )\n    score = model.get_best_score()['validation']['AUC']\n    scores.append(score)\n\n    y_pred = model.predict_proba(test_data)[:, 1]\n    prediction += y_pred\n\nprediction \/= n_fold\nend = time.time()\nelapsed = end - start","d3d03e7c":"auc.append(roc_auc_score(y_test, prediction, average ='macro'))\ny_bin = (prediction >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_bin))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('New_feature_shap')))","402bb1a6":"fpr, tpr, thresholds = roc_curve(y_test, prediction)\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, prediction, average ='macro'))","3bca581d":"start = time.time()\nmodel = DummyClassifier(strategy='stratified')\nmodel.fit(X_train_prep, y_train)\n\nend = time.time()\nelapsed = end - start","c3d1c267":"start_pred = time.time()\ny_pred = model.predict_proba(X_test_prep)[:, 1]\n\nend_pred = time.time()\nelapsed_pred = end_pred - start_pred\n\nauc.append(roc_auc_score(y_test, y_pred, average ='macro'))\ny_bin = (y_pred >= 0.5)*1\naccuracy.append(accuracy_score(y_test, y_pred))\ntraining_time.append(elapsed)\nprediction_time.append(elapsed_pred)\nmodel_name.append((str(model.__class__.__name__)+str(' ')+str('stratified')))","0dc183b5":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\n\nprint(\"AUC:\", roc_auc_score(y_test, y_pred, average ='macro'))","6882c04d":"summary = pd.DataFrame({ \n    'model' : model_name , \n    'training_time, sec' : training_time ,\n    'prediction_time, sec' : prediction_time , \n    'AUC' : auc,\n    'Accuracy': accuracy}\n    ).sort_values(by='AUC', ascending=False).reset_index(drop=True)\n\n\n\n\n\nsummary.style.highlight_max( 'AUC' , color = 'green' , axis = 0 )","9de04579":"We have no missing values","4f142be1":"Train the model","4b6f351d":"### XGboost base with transforms","3f14f428":"A strong correlation is observed in two signs - `tenure` and` total_charges` - we will remove `total_charges` - we will get it from days and monthly payment","edfabd78":"## Exploratory data analysis","3c70b72a":"At the final stage of data preparation, we will check the multicollinearity between our numerical data. If the value exceeds 0.8 - a strong correlation - we will get rid of unnecessary features to improve generalization. Let's use Pearson's Correlation and [Phik-correlation](https:\/\/towardsdatascience.com\/phik-k-get-familiar-with-the-latest-correlation-coefficient-9ba0032b37e7) - quite new, but capable, as stated, to eliminate dependencies in nonlinear connections. We will also check the mutual information of categorical features. Let's prepare our data:","a3fca192":"The feature `senior_citizen` is listed as int64 by type - we need to look at the values. Most likely - we need to convert it to object or category","d8c050c6":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","c844b152":"We will replace them with monthly_charges - it is possible that the gaps are due to the fact that the client has not paid for it yet - he uses free time","4b86bd93":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","3f517101":"For the rest of the criteria, we will go through using only histograms.","3bfc8d7e":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","9b2ff555":"\nNow let's turn to mutual information. To do this, we translate our categorical signs to human ones using coding:","ccbfcf41":"\n\n\nThere are no passes, no duplicates. Users are unique, their number corresponds to the number of objects in the sample.\n\n- `TotalCharges` must be cast to float","cf9583df":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","ceb7985e":"<p align=\"center\"><img src='https:\/\/i.ibb.co\/wKyYNpt\/slider.jpg'><\/p>\n\n**Context**\n\"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" \n\n**Content**\nEach row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n\n**The data set includes information about:**\n\nCustomers who left within the last month \u2013 the column is called Churn  \n\nServices that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies  \n\nCustomer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges  \n\nDemographic info about customers \u2013 gender, age range, and if they have partners and dependents\n\nROC_AUC and Accuracy goals\n\n---\n\n\n <font size=\"2\"> You can learn here abot Shap, Mutual information, Stacking, Ensembles, Cross validation and Phik correlation. Pipelines, chaining methods and working with disbalance of classes. Thank you for reading <\/font>\n\n","95c3eb66":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","6b3239bf":"For convenience, we will display a pivot table","84169c32":"1. At the first stage, we prepared and worked out the `pipeline` for the preparation of features. In the future, it is possible to automate downloads using the `chaining` methods provided by` Pandas`\n\n2. We have a class imbalance. There will be no clarifications about the importance of this or that class - we will predict with a standard threshold of `0.5`, while using the stratified selection method and the prediction method for cross-validation. I plan to use boosting model `XGboost`,` Catboost`, `LightGBM`, stacking based on models from support vector family, random forest and Logistic regression` SKlearn` with meta learner based on `Keras` model, shallow. I plan to select parameters `Optuna`\n\n3. We plan to use StandardScaler for quantitative features in order to give them the same scale and normal shape\n\n4. For categorical features I plan to use `OrdinalEncoder`. It was planned to use `OneHotEncoder` for binaries (` Yes-No`) - but it was decided to fill in the gaps with the value `No`\n\n5. These transformations will be performed through the `Pipeline`. Let's measure the training and prediction times. Let's compare different models and draw conclusions","385035de":"Let's see and study the properties of a user who leaves the company","7da312af":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","8878b72d":"Most of the information about the target attribute is contained in `tenure` and` payment_method`. But not so significant as to remove total_days from training","748d7708":"We got the expected result - the output is a transformed `DataFrame Pandas` - this will allow us to use` iloc`, which we need when training models. We can start preparing the models. Let's create auxiliary tables for preparing the report","b7a736a7":"#### Pipe data processing","50dcd0af":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","627d94de":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","8f99626d":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","c7e212c6":"### Catboost tuning","dc844cdf":"`Pipeline` will consist of the following components:\n\n1. `StandardScaler` for numeric features\n2. `OrdinalEncoder` for ordinal features\n3. `OneHotEncoder` for binary signs\n4. `Kmean` to create an additional feature in the form of a cluster\n5. `delete_cols_array` to remove auxiliary signs from `np.ndarray`","d6905764":"Thus, when repeating the study - you can quickly get a set for work - `object` or` category` did not begin to be converted to the same format - there is no need - the data will be loaded into the model via `Pipeline`","f14fec24":"Unfortunately, the results are not the best. We will not adjust the ensemble - there are simpler ways to get the required metric","4741e35f":"Let's try `LGBM` through` GridSearchCV` from `sklearn` - select the best parameters and see the results","803cac0c":"`total_charges` needs to be converted to float64, since it contains the total receipts from a particular user. Let's check if there are empty lines without an entry","c3247e82":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","750febad":"The results improved, relative to the baseline, but it is not the best. It is worth noting that the model generalizes well","d7a80ab1":"\n### Importing required libraries","9c269a62":"#### Pearson Correlation","15fb0076":"Let's try to train a model with a new feature","d5671d3a":"Let's compose lists of features for processing in `Pipeline`","b46e8070":"We got the best parameters from the search - we will form a new model and try to start training again","5881f704":"Let's see what results will be with `StratifiedKFold` in ensemble voting with cross-validation","32526e5b":"**The company is left most often by profitable users, which is used by:**\n\n- electronic checks\n- use a monthly payment\n- do not use additional services in the form of antivirus, cloud storage, dedicated line with technical support and equipment protection\n- are not married.\n\n**The higher the monthly payment** - **the higher the likelihood of leaving**. It may be worth considering the need for incentives to move to better rates.\n\n\nThese studies were also confirmed by the conclusions of the models on the importance of features.\nAccording to the results of the work, all the planned points of the plan were completed:\n\n1. We have prepared a draft `pipeline` for loading the data set,` Pipeline` for processing features - this will allow us to automate the process of processing new information in the future. Pipeline preparation of features allows coding categorical variables, standardizing quantitative ones, due to their not normally distributed nature.\n\n2. Faced class imbalance - fixed this problem by using weighting during training, apply cross-validation with stratified division of the sample, also worked the `Upsampling` technique\n\n3. Created an additional feature - the number of days as a client. They did not take the start and end date - to reduce the likelihood of `Data leaks`. Measurements of mutual information showed that the new feature has `0.3n` of mutual information and will be acceptable for processing. We also added user clusters depending on the size of the monthly payment.\n\n4. A strong correlation was found between the number of days and the total payments - the attribute `total_charges` - has been removed\n\n5. We found out that by removing the following attributes `user_cat`,` streaming_tv`, `online_backup`,` senior_citizen`, `device_protection`,` dependents`, `partner`,` internet_service` - it is possible to get a sufficiently high result. This will reduce the time for collecting information.\n","08986373":"`LGBM` works better and faster without` Ohe` and `Ordinal` coding. Therefore, we will transform our samples for `LGBM` - we will translate everything into` category` except numeric","25c74d65":"By features - everything is identical to `XGboost`, there are no striking changes","9b79db3e":"### XGboost basic with cross-validated transforms","3d49aa84":"The results have improved, relative to the baseline and the result is satisfactory in quality","ded32833":"## Preparing data for training","fbf27ae5":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","ac026a06":"### XGboost base with conversions with `cross_validate`,` cross_val_predict`","d07594e9":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","30fec235":"Thus, we confirm that the average monthly payments of those who left are not equal. Those who leave - pay more","346bb878":"### Catboost basic no cross-validated conversions","1e3c210b":"### Sanity check. DummyClassifier","02acab4f":"Create a `Pipeline`","7f5201c6":"### Traits monthly_charges and total_charges","7a8e64a0":"Let's try to define an additional category - only an Internet user or only a phone user, or another user - for example, only television","6d5eb5b1":"Observe the long tail on the right, the distribution is asymmetric, skewed to the right","1f11cb2a":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","13d34605":"Signs of importance differ little from those used by `XGboost`","717b9b35":"## Loading data. Inspection and processing. Primary analysis","c8c067e1":"<a id='Pipe'><\/a>","c0dc4b5c":"Fiber optic users are more likely to churn","dcff5fdc":"### Checking for multicollinearity and mutual information","46ba98ec":"\n## Data download and initial inspection","f8e4c152":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","55d218b9":"#### Mutual information","13af6405":"Let's try the `stratified` strategy,` 1` is important to us - they talk about churn, predicting churn 1 for everyone as a constant will entail losses for the company, predicting `0` - we will not predict churn. Or just predict `0` and` 1` with probability `0.5`. Indirectly, through the `ROC-AUC`, we have already checked the performance of our models - we will make an additional check","92d5dfcf":"#### Replacing data types","e1cc8f13":"Let's look at the QQ-graph of our features and at kde","7cd6b94a":"Let's remove from the training sample the features that were not included in the best","26170ff4":"Most of users choose electronic checks, more than `45%` of customers who left - used this type of invoice receipt and interaction","6e78b315":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","348587eb":"### Xgboost tuning","2ab3669f":"#### Clustering and finding the optimal number of clusters for `monthly_charges` and` total_charges`","a47b7dc3":"The diagram shows the dependence between the traits `total_charges` and` month_charges` and a strong one between `total_charges` and` tenure`. Let's see the correlation coefficients","f181b77e":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","1ddd2e03":"Users who are not married are more likely to leave than those who are in a legal relationship. It is also worth noting that users who are dependents are less likely to leave the company - they do not choose, they choose for them","f6553aec":"### Upsampling","b40b6ed8":"<a id='shap'><\/a>","5454f224":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","d0f524f3":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","8b9a3b9b":"Our assumption is correct - the attribute `senior_citizen` must be converted to` category`. ","c0eb7299":"In fact - the attribute `tenure` will determine the target to a greater extent. Without this feature, a ROC-AUC estimate of 0.87 was achieved. We will train with this attribute - it is calculated, the attribute `total_charges` will be removed from training. During the training, we will also look at the `features importance` of the models and compare with the research results","792aecbe":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","d4b848e2":"Let's try to tune `Blender` with` Keras Tuner`","750550a9":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","183dfc2d":"Care does not depend on gender, at least not explicitly. The sample turned out to be quite balanced, the strata were observed, the number of men and the number of women were approximately the same - this is good for us.","5443ab42":"### Catboost basic without conversions","27977f45":"Delete the `customer_id` feature","66883e87":"Let's try the same model, but we will make predictions with the help of `StratifiedKFold` - we get an ensemble of models","e5388e71":"### LightGBM \u0441 GridSearch","0b1a6b86":"The best balance is obtained using a resampling factor of `3`","2ce94a15":"Let's add a payment sign - if a client pays for a year in advance - we will consider this a sign of loyalty. We also added this feature to the pipe of the set preparation","d0fc9521":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","11faf2e1":"Let's try to check the performance of the model through `cross_validate` and` cross_val_predict` - this method will help to determine the possibility of using the model in the ensemble","f65ce6e5":"## Pipeline preprocessing","536f13da":"`total_charges` is strongly skewed to the right, we observe a long tail","61ce33e6":"\nThe result is slightly better, but the model generalizes better. But in this case - it is better to use the base model - faster prediction times","e9893b02":"Let's check how the average monthly total payments of customers that left and those who remained are different\nLet us construct hypotheses: Let us formulate it as a hypothesis about the equality of the means of two general populations\n\n**H0 (null hypothesis): Average user payments are equal.**\n\n**H1 (alternative hypothesis): Average user payments are not equal.**","d8d53f07":"We were able to improve the result, quite significantly","ce9a0d2a":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","991b0024":"Unfortunately, the result has not improved significantly again. But it also did not get much worse - it is still acceptable - and this is with a decrease in the number of signs by almost half","0420a0a8":"### Stacking keras tuner","16733a44":"### Exploring of the dependence of care on various traits","83f51815":"The data has been transformed - we can see the mutual information:","a1e8e106":"Since we have an imbalance of classes, it is necessary to take this into account in training. We will weigh the target variables during training. We will also try various ensemble methods - cross-validation, stacking, voting and upsampling.","dbf8f198":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","3b81348c":"Let's prepare a test set using `Pipline`, then select a validation set for our model","c1ae2fde":"With streaming services, not everything is so simple. Users using the services and not leaving are almost the same. Likewise for those users who use multiple lines","1437f21a":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","9edf88ae":"### Ensemble of Logistic Regression, Random Forest and SVC","544e54fc":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","ef93270b":"Let's try ensemble prediction, set the voting type to soft. We will use `Logistic regression`,` Random forest` and `SVC support vectors`","80554baf":"To speed up work in the future, we will make training in the form of pipe methods\nTo do this, we will overwrite the variables of the sets and load them again - to demonstrate the effectiveness of this approach - in the future this can save time. We will do the preparation through the method queue","85e4f060":"Let's try Catboost in the same way - first basic, then on `StratifiedFolds`","0be09d14":"### Creation of additional characteristics and training without the least important","f7c3f127":"## Conclusion","6d8a8e80":"Likewise, let's test for `Catboost`","9fc03b7c":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","fca774c9":"We got a quite expected option - `AUC` is close to` 0.5` - it could have been omitted - but for the interpretation of `ROC-AUC` they made an additional model","6e24f3ac":"In what follows, we will use the manual prediction method for `Fold with CV`.","41457de5":"Now let's try to set up the models. Let's start with `XGboost`, we will use the` Optuna` library","fa34e26a":"We got a fairly high result - satisfying the required quality. Let's try to improve it with the help of predictions on `StratifiedFolds` and tuning parameters\n\nBut first let's try to look at the `PR` curve","816d2d04":"Let's try the upsampling technique. This will allow us to balance the set by the number of tags.","ffa164da":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","d8a013b7":"The nature of our features is far from normally distributed","b3ba3923":"Let's try to improve the results using the stacking technique. As models, we will take those that we used earlier, we will make a shallow neural network as a meta-student.","dbbd19b6":"Let's consider separately the signs that are responsible for customer payments - `monthly_charges` and` total_charges`","ec841fc2":"Let's save the most important features and try to build a model only on the basis of them in [later](#shap)","bedd76af":"For a threshold of 0, the completeness is 1 - all answers are positive. At the threshold of `0.85`, the model stops giving correct answers. The highest indicator `F1` is observed with the threshold` 0.35-0.40`. In the future, we will look at the `ROC-AUC` curve and` AUC` - there have been no clarifications on the importance of the classes. Let's look at the importance of signs.","d115aa33":"### Stacking","42589252":"Divide the sample into training and test samples","afbd4792":"<a href=\"#footer\">Back to content<\/a>\n<footer id=\"footer\"><\/footer>\n\n___","2ff9f3e7":"The optimal number of clusters will be selected using the elbow method","cb467e66":"\nLet's create a helper class for `KMeans` and a function to remove unnecessary features - in order to demonstrate how` Pipeline` works","52ecfc8f":"We got a very good result. Best so far\n","452253f6":"The result is quite high. let's see other models and at the end we will make a conclusion","1a586911":"The results are acceptable in quality, but not the best - let's try to set up a meta-student. Models have already been tuned before","c88a8f1a":"Most often, users who do not use the services leave: `online_security`,` online_backup`, `tech_support` and` device_protection`","df737cc1":"Let's add an additional feature - a cluster for the monthly payment of the client","8bd99ed6":"<footer id=\"footer\"><\/footer>","6a97384e":"Check the functionality of the `Pipeline`","1cad94a5":"Unfortunately, it was not possible to greatly improve the results relative to the overall picture, but better than in the base case","5afdbaf2":"The features are categorical, we will encode in the future - `OrdinalEncoder` for ordinal and` Ohe` for binary","72a56258":"The more users pay per month, the higher the likelihood of leaving, in other words, the most profitable customers leave for an unknown reason","57ccebd9":"#### Phik Correlation","9714e951":"Users with monthly subscriptions are more likely to leave than those who prefer long-term contracts","649e27b8":"The data confirms our findings in the `EDA`:\n\n\nMost often those users leave the company who use:\nshort-term contracts\nelectronic checks\nuse a monthly payment\ndo not use additional services in the form of antivirus, cloud storage, dedicated line with technical support and equipment protection","4c93414d":"Let's see our target groups:","be4402f0":"`Catboost` shows better results than` XGboost` on this set and it does not require transformations for categorical features, which saves time","5dc4eceb":"Let's focus on three clusters","9d04bc33":"#### Handling passes","cd7f9771":"**Conclusions**\n\n\nMost often those users leave the company who use:\n\n- electronic checks\n- use a monthly payment - here you can offer discounts for payment for a longer period\n- do not use additional services in the form of antivirus, cloud storage, a dedicated line with technical support and equipment protection - perhaps these services should be included in offers for users as a recommendation (Internet security: antivirus (***DeviceProtection***) and blocking insecure sites (***OnlineSecurity***); dedicated technical support line (***TechSupport***); Cloud file storage for data backup (***OnlineBackup***)\n- not married\n- the higher the monthly payment, the higher the likelihood of leaving. It may be worth considering the need for incentives to move to better rates.","fd421ac9":"Let's try to configure. The model is poorly generalized - significant retraining is underway - but one way or another - the result is high"}}