{"cell_type":{"898b64fa":"code","2f49437c":"code","a5c0d1ec":"code","e1bbe1f9":"code","4fdd62f4":"code","4e4bc438":"code","7ddb3b0e":"code","773b7f12":"code","19d76a27":"code","2e8be289":"code","3bbae2bd":"code","68e00286":"code","2e7c1796":"code","0a345b7e":"code","ce12b722":"code","4418503f":"code","26f77498":"code","2a567c86":"code","933c1762":"code","79435591":"code","675dcc23":"code","e8bb67db":"code","af253b90":"code","e48d32be":"code","8e7ab9da":"code","9e60a384":"code","8304e663":"code","7f83b2f4":"code","3bf38fec":"code","3c1c5117":"code","56d91961":"code","179d0653":"code","f2052d8f":"code","04407c69":"code","5de7d0e6":"code","00568f26":"code","53e2f1fb":"code","d38b07e5":"code","e22e704b":"code","6ffac856":"code","8d484d67":"code","05c4eb97":"code","bc5fbd24":"code","68d5993c":"code","b5696f24":"code","89d0124b":"code","4d593531":"code","85598783":"code","96a94ae9":"code","5c732729":"code","26c01093":"code","35f8e37a":"code","7bec0115":"code","c93a6172":"code","e1232abc":"code","073f0d91":"code","800b502c":"code","09db05d0":"code","73316e1f":"code","041a3f21":"code","7621047e":"code","54fe0154":"code","2665ee75":"code","a7f86372":"code","21dc5974":"code","ac7c96f4":"code","3fff29b7":"code","1addc4d6":"code","25b5c42d":"code","e8c3b1af":"code","c7b198e4":"code","d93f2b55":"code","433dcce6":"code","e95013f4":"code","3c9ade84":"code","d6bd72b6":"code","3da81bc4":"code","5a218c65":"code","63b4e05a":"code","fdb87ea1":"code","8dddb29b":"code","a45fd7a2":"code","d643f548":"code","c4b37d33":"code","811dd81c":"code","efd91da1":"code","55d5da84":"code","c8b151e2":"code","c57932b1":"code","fc7a849c":"code","56db4903":"code","7e79e3a9":"code","f5c85fe1":"code","74e385d5":"code","d1574282":"code","074085f8":"code","d3860d04":"code","c4102125":"code","1a173f52":"code","03414aee":"code","1a505e84":"code","447d3ff8":"code","26eea1e8":"code","6c570d9f":"code","867fa36e":"code","faaf1627":"code","5539f516":"code","0479d95d":"code","058ecbd8":"code","03fc3fef":"code","76d79dab":"code","071af050":"code","c11710cf":"code","117ab2b3":"code","d6201bbe":"code","e3c0e54a":"code","a79bc1c4":"code","ec9f8a83":"code","33a88173":"code","645aecb2":"code","e7633985":"code","ad1f9305":"code","6da7a8b4":"code","bc7f23d8":"code","15f088ae":"code","e296c66c":"code","1af54352":"code","0bd3b026":"code","7335ea98":"code","578371f6":"code","d01e53c6":"code","053f82d3":"code","ebbab39f":"code","4f327022":"code","8496209d":"code","9f022492":"code","50ea0ba2":"code","034b77ed":"code","9214818e":"code","a5a41f9a":"code","de10510d":"code","4f7b1320":"code","d0ee7764":"code","9c85d308":"code","b3c77854":"code","b889dfc8":"code","43a9d537":"code","4c24c671":"code","9823b36a":"code","50b0e7ed":"code","8e0903e7":"code","2f3bcfc6":"code","86a77149":"code","baba73cd":"markdown","1aefefc9":"markdown","439dabfd":"markdown","20db2dab":"markdown","f4f2e407":"markdown","431ece30":"markdown","a92efcb0":"markdown","d921bc2b":"markdown","268ca11e":"markdown","f977f0d2":"markdown","b8fe0a0e":"markdown","f3dcea9f":"markdown","c46aa91d":"markdown","09447bf8":"markdown","ba246ffc":"markdown","8c2a019a":"markdown","d6dc11e5":"markdown","2600b129":"markdown","7ce7688d":"markdown","48248357":"markdown","aed233ac":"markdown","cda07587":"markdown","3682a301":"markdown","03b410eb":"markdown","3c923188":"markdown","8fa69d96":"markdown","aa5fbb17":"markdown","06d22686":"markdown","b11bed9f":"markdown","07dfa2d6":"markdown","9ea27098":"markdown","30fe4eb9":"markdown","31d1bcb7":"markdown","5d8f2edf":"markdown","8c063270":"markdown","d2eb319e":"markdown","9142309f":"markdown","b45b9118":"markdown","efc6857d":"markdown","f0b6b618":"markdown","9f5ae59e":"markdown","ca4e1be8":"markdown","cdabb9d9":"markdown","8caf8b3e":"markdown","34daeb68":"markdown","f3b34380":"markdown","6b8a9dbe":"markdown","a8aab762":"markdown","453136fc":"markdown","80b0f47d":"markdown","aa6561fa":"markdown","105992f6":"markdown","e05a9b12":"markdown","8c067183":"markdown","38bebcb5":"markdown","80dc2e10":"markdown","4cba853c":"markdown","a139dbc8":"markdown","937fde86":"markdown","6ca3f743":"markdown","89a22c0e":"markdown","1b36573e":"markdown","b5b8b7a5":"markdown","1e686d37":"markdown","4bf92a0b":"markdown","dafc1fd2":"markdown","b5c3e97f":"markdown","b9542247":"markdown","e2ed3358":"markdown","f026e45c":"markdown","836170d3":"markdown","7ce69b24":"markdown","d23602f2":"markdown","a899dbf2":"markdown","8215d7dc":"markdown","5ac7b93a":"markdown","bd105421":"markdown","df9ebe90":"markdown","33a19136":"markdown","b1a2355a":"markdown","5dceeee1":"markdown","d23518f1":"markdown","561202ec":"markdown","bb1f1eb8":"markdown","652ff40c":"markdown","86909fff":"markdown","e061e232":"markdown","3441736a":"markdown","6ca92ba0":"markdown","9d5cc0ca":"markdown","a59cae57":"markdown","3cd757bd":"markdown","82099785":"markdown","57753b6d":"markdown","81f7bffd":"markdown","68261c88":"markdown","8c2d1f42":"markdown","46512f9e":"markdown","2d668f3f":"markdown","41cb6cc7":"markdown","a9071003":"markdown","ac588dc8":"markdown","e1b06f17":"markdown","ebf0805f":"markdown","5dbc70e1":"markdown","c25e7973":"markdown","cd851509":"markdown","5acb051c":"markdown","1d36955a":"markdown","60829873":"markdown","f56a2261":"markdown","3294f1a1":"markdown","e5706afb":"markdown","b383b3d4":"markdown","e80a4848":"markdown","410dc6ec":"markdown","acd59aa7":"markdown","147a494a":"markdown","757464ff":"markdown","7137e761":"markdown","a5cddd58":"markdown","6c8abf86":"markdown","19fc4c52":"markdown","636660fa":"markdown","8c1fd8a6":"markdown","ae84af4f":"markdown","309e204b":"markdown","7d354f21":"markdown","e5c25ac4":"markdown","18ad32fb":"markdown","a295c01e":"markdown","74221c1c":"markdown","96f79e19":"markdown","9bd20126":"markdown","b7340aa7":"markdown","b7422b3f":"markdown","20ffd657":"markdown","4349114d":"markdown","2ac2e2b6":"markdown","9b7a44a7":"markdown","11a92c5f":"markdown","6e7693f9":"markdown","e1254276":"markdown","722ef755":"markdown","7b56bb75":"markdown","15897b0f":"markdown","c221d761":"markdown","a7f16345":"markdown","fea2ef10":"markdown","2a6ad78d":"markdown","1452e053":"markdown","d5f3d57e":"markdown","bbe6dcec":"markdown","93ff6bb0":"markdown","c5c24dd2":"markdown","8342edba":"markdown","71805153":"markdown","6433d47d":"markdown","a815f871":"markdown","2ff41add":"markdown","c1dbc215":"markdown","fea47d33":"markdown","79810a38":"markdown","85aace05":"markdown","21cff1e3":"markdown","70d91165":"markdown","00a82ac9":"markdown","82d5aa01":"markdown","5d35a0b1":"markdown","b1270390":"markdown","abf8e6ff":"markdown","5e6ee7f8":"markdown","f22bbb32":"markdown","9bb83ea6":"markdown","62231c4d":"markdown","326d9d33":"markdown","d1974f7a":"markdown","b3aaa8f7":"markdown","d819a6fb":"markdown","2c0104ed":"markdown","4de0fd1a":"markdown","a37a8e4a":"markdown","0edffddd":"markdown","a61ae3a5":"markdown","637f1367":"markdown","04e6dc61":"markdown","6ba15a4e":"markdown","ec066d12":"markdown","700ebc8f":"markdown","d9fd4c6d":"markdown","14ef9c3f":"markdown","3063af94":"markdown","7a3b92d9":"markdown","d10d9c02":"markdown","88fca27d":"markdown","96a13055":"markdown","233d414f":"markdown","b387a953":"markdown","7dddd9d5":"markdown","d88cf9d3":"markdown","ea48baca":"markdown","059d3a7d":"markdown","2a54c297":"markdown","7796b404":"markdown"},"source":{"898b64fa":"import pandas as pd\nsub=pd.read_csv('..\/input\/cnn-models-petfinder\/lastVersionXGB.csv')\nsub.to_csv('submission.csv', index=False)\n","2f49437c":"pip install --upgrade pip\n","a5c0d1ec":"\n!pip install -U tensorflow-text==2.4.1\n!pip install -U tensorflow-estimator==2.4.1\n!pip install -U tensorflow tensorflow_hub==2.4.1\n\n\n\n\n#libraries\nimport numpy as np \nimport pandas as pd \nimport os\nimport json\nimport seaborn as sns \nimport cv2 \nimport matplotlib.pyplot as plt\nimport glob\nfrom wordcloud import WordCloud\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import cohen_kappa_score \nfrom tabulate import tabulate\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.cluster import KMeans\nimport tensorflow_hub as hub\nimport shutil\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom PIL import Image\nfrom sklearn.cluster import KMeans\nimport tensorflow as tf\nimport tensorflow_text as text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n%matplotlib inline\nplt.style.use('ggplot')\npd.set_option('display.max_columns', None)\npd.options.mode.chained_assignment = None  # default='warn'\n#pd.set_option('display.max_colwidth', -1)\n\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)","e1bbe1f9":"colors = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nbreeds = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\nstates = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')\n\ntrain_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\n\n\ntrainImagesPath = ('..\/input\/petfinder-adoption-prediction\/train_images')\ntestImagesPath = ('..\/input\/petfinder-adoption-prediction\/test_images')\n\n","4fdd62f4":"train_df.info()\n","4e4bc438":"train_df.head()","7ddb3b0e":"plt.figure(figsize=(14, 6));\ng = sns.countplot(x='AdoptionSpeed', data=train_df)\nplt.title('Adoption speed classes rates');\nax=g.axes\nfor p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 \/ train_df.shape[0]:.2f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 8),\n         textcoords='offset points')  \nax=ax.set_xticklabels(['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption after 4 months'])\n","773b7f12":"adopted01_df=train_df.copy()\nadopted01_df['AdoptionSpeed'].replace(\n    to_replace=[ 0, 1, 2 ,3 ],\n    value=0,\n    inplace=True\n)\nadopted01_df['AdoptionSpeed'].replace(\n    to_replace=[ 4 ],\n    value=1,\n    inplace=True\n)\n\nf, ax1 = plt.subplots(1,1,figsize=(20,12))\n\ncorr=train_df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(240, 10, as_cmap=True,n=9)\n\n\n\n\nax1=sns.heatmap(corr, mask=mask, cmap=cmap,  center=0,\n            square=True, linewidths=.5,vmax=0.3, cbar_kws={\"shrink\": .9})\nax1.set_title(\"AdoptionSpeed\",fontsize=14)\n","19d76a27":"ax = sns.countplot(x=\"Type\", data=train_df)\nsns.set(font_scale=1.0)\nax.set(xlabel = 'Animal', ylabel = 'Count')\nax.set_xticklabels([\"Dogs\",\"Cats\"])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()-80),\n                ha='center', va='center', fontsize=11, color='black', xytext=(0, 10),\n                textcoords='offset points')\n\n\n","2e8be289":"fig, ax = plt.subplots(1,2,figsize=(30,10))\nsns.set(font_scale=1.4)\nax[0] = sns.countplot(x=\"Type\",hue='AdoptionSpeed', data=train_df,ax=ax[0])\nax[0].set(xlabel = 'Animal', ylabel = 'Count')\nax[0].set_xticklabels([\"Dogs\",\"Cats\"])\nTypeP=[]\n\nfor i in range(1,3):\n    TypeP.append((train_df['Type']==i).value_counts()[1])\n\nfor i,p in enumerate(ax[0].patches):\n    ax[0].annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()-80),\n                ha='center', va='center', fontsize=11, color='black', xytext=(0, 20),\n                textcoords='offset points')\n    ax[0].annotate(\"%.2f %%\" % (p.get_height() *100  \/ TypeP[i%2]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, -10),\n         textcoords='offset points')  \n\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\n\n\n\nadopted=train_df[train_df['AdoptionSpeed']<=3]\ndogsP=(adopted['Type']==1).value_counts()[1]\/(train_df['Type'].value_counts()[1])\ncatsP=(adopted['Type']==2).value_counts()[1]\/(train_df['Type'].value_counts()[2])\nprecent=[]\nprecent.append(dogsP)\nprecent.append(catsP)\n\nsns.set_color_codes(\"pastel\")\nax[1]=sns.countplot(x=\"Type\", data=train_df,color='b',ax=ax[1])\nfor i,p in enumerate(ax[1].patches):\n    ax[1].annotate(\"%.2f %% adopted \" %precent[i]   , (p.get_x() + p.get_width() \/ 2., p.get_height()-250),\n                ha='center', va='center', fontsize=11, color='black', xytext=(0, -10),\n                textcoords='offset points')\n\nsns.set_color_codes(\"muted\")\nsns.countplot(x=\"Type\", data=adopted, color='b')\nsns.set(font_scale=1.0)\nax[1].set(xlabel = 'Animal', ylabel = 'Count')\nax[1].set_xticklabels([\"Dogs\",\"Cats\"])\nheightlist=[]\nfor p in ax[1].patches:\n    ax[1].annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()-250),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 20),\n                textcoords='offset points')\nplt.legend()\n","3bbae2bd":"train_df['Name'] =train_df['Name'].fillna('NULL')\n\nnoName_df=train_df.copy() #df without any names\nnoName_df['Name']=noName_df['Name'].replace({\"No Name\": \"NULL\", \"No Name Yet\": \"NULL\", \"Unknown\": \"NULL\", \"Baby\": \"NULL\",\"Lucky\": \"NULL\",\"Brownie\": \"NULL\",\"Blackie\": \"NULL\",\"Puppy\": \"NULL\",\"Kitty\": \"NULL\",\"Kittens\":\"NULL\"})\nnoName_df=noName_df[noName_df['Name']=='NULL']\n\nnoName_df['Name'].value_counts()\n\nadoptedNoName=noName_df[noName_df['AdoptionSpeed']<=3]\nadoptedNoNameCount=adoptedNoName.count()[1]\nadoptedNoNamePercent=adoptedNoNameCount\/noName_df.count()[1]\n\nyesName_df=train_df.copy() # df with all the names\nyesName_df.drop(yesName_df.index[yesName_df['Name'] == \"NULL\"], inplace = True)\nadoptedYesName=yesName_df[yesName_df['AdoptionSpeed']<=3]\nadoptedYesNameCount=adoptedYesName.count()[1]\nadoptedYesNamePercent=adoptedYesNameCount\/yesName_df.count()[1]\n\nfig1, ax1 = plt.subplots(1,2,figsize=(15,5))\nexplode = (0, 0.1)\nlabels = \"Weren't Adopted\" ,'Adopted'\nax1[0].set_title(\"Animals without a name Adoptian rate\",fontsize=14)\nax1[0].pie([noName_df.count()[1]-adoptedNoNameCount,adoptedNoNameCount], explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['C3','C9'])\n\nax1[1].set_title(\"Animals with a name Adoptian rate\",fontsize=14)\nax1[1].pie([yesName_df.count()[1]-adoptedYesNameCount,adoptedYesNameCount], explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['C3','C9'])\n\n\nplt.show()\n","68e00286":"plt.figure(figsize=(15, 7));\nsns.set(font_scale=1)\nax = sns.histplot(x=\"Age\", data=train_df)\nax.set(xlabel = 'Age in months', ylabel = 'Count')\nplt.title('Age distribution', fontsize = 'xx-large')","2e7c1796":"trainAge_df=train_df.copy()\n\ntrainAge_df.drop(trainAge_df[trainAge_df['Age'] >121].index, inplace=True)\nplt.figure(figsize=(24, 20));\n\nbins =[i  for i in range(0,13)]\nfor i in range(2,11):\n    bins.append(i*12)\n\ntrainAge_df['Age'] = pd.cut(trainAge_df['Age'], bins)\nax=sns.countplot(x='Age',hue='AdoptionSpeed',data=trainAge_df)\nfor i,p in enumerate(ax.patches):\n    \n    ax.annotate(\"%.2f \" % (p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=90, xytext=(0, 10),\n         textcoords='offset points')  ","0a345b7e":"ageP=[]\n\nfor i in range(0,12):\n    ageP.append((trainAge_df['Age']==pd.Interval(i, i+1, closed='right')).value_counts()[1])\nfor i in range(1,10):\n    ageP.append((trainAge_df['Age']==pd.Interval(i*12, (i+1)*12, closed='right')).value_counts()[1])\n    \n\nplt.figure(figsize=(22, 22))\nax=sns.countplot(data=trainAge_df, x=\"Age\",hue='AdoptionSpeed')\nplt.legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    \n    ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ ageP[i%21]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=90, xytext=(0, 30),\n         textcoords='offset points')  ","ce12b722":"adoptedAge01_df=trainAge_df.copy()\n\nadoptedAge01_df['AdoptionSpeed'].replace(\n    to_replace=[ 0, 1, 2 ,3 ],\n    value=0,\n    inplace=True\n)\nadoptedAge01_df['AdoptionSpeed'].replace(\n    to_replace=[ 4 ],\n    value=1,\n    inplace=True\n)\n\n\nplt.figure(figsize=(20, 8));\n\nsns.set_color_codes(\"pastel\")\nax=sns.countplot(data=adoptedAge01_df, x=\"Age\",order = adoptedAge01_df['Age'].value_counts().sort_index(ascending =True).index,color='b')\nadoptedAge0_df=adoptedAge01_df.loc[adoptedAge01_df['AdoptionSpeed']==0]\n\ndf=adoptedAge0_df['Age'].value_counts().sort_index(ascending =True)\/adoptedAge01_df['Age'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\nfor i,p in enumerate(ax.patches):\n    ratio = df[i]\n    ax.annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax=sns.countplot(data=adoptedAge0_df, x=\"Age\",order = adoptedAge01_df['Age'].value_counts().sort_index(ascending =True).index)","4418503f":"fig, ax = plt.subplots(1,2,figsize=(30,10))\nax[0].set_title(\"Dogs\",fontsize=14)\nax[1].set_title(\"Cats\",fontsize=14)\n\nadoptedAge01_df=trainAge_df.copy()\n\nadoptedAge01_df['AdoptionSpeed'].replace(\n    to_replace=[ 0, 1, 2 ,3 ],\n    value=0,\n    inplace=True\n)\nadoptedAge01_df['AdoptionSpeed'].replace(\n    to_replace=[ 4 ],\n    value=1,\n    inplace=True\n)\n\nadoptedAge01_df.drop(adoptedAge01_df[adoptedAge01_df['Type'] ==1].index, inplace=True)\n\nplt.figure(figsize=(20, 8));\n\nsns.set_color_codes(\"pastel\")\nax[0]=sns.countplot(data=adoptedAge01_df, x=\"Age\",order = adoptedAge01_df['Age'].value_counts().sort_index(ascending =True).index,color='b',ax=ax[0])\nadoptedAge0_df=adoptedAge01_df.loc[adoptedAge01_df['AdoptionSpeed']==0]\n\n\n\ndf=adoptedAge0_df['Age'].value_counts().sort_index(ascending =True)\/adoptedAge01_df['Age'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\n\nfor i,p in enumerate(ax[0].patches):\n    ratio = df[i]\n    ax[0].annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax[0]=sns.countplot(data=adoptedAge0_df, x=\"Age\",order = adoptedAge01_df['Age'].value_counts().sort_index(ascending =True).index,ax=ax[0])\n\n################# Cats\n\n\nadoptedAge01_df=trainAge_df.copy()\n\nadoptedAge01_df['AdoptionSpeed'].replace(\n    to_replace=[ 0, 1, 2 ,3 ],\n    value=0,\n    inplace=True\n)\nadoptedAge01_df['AdoptionSpeed'].replace(\n    to_replace=[ 4 ],\n    value=1,\n    inplace=True\n)\n\nadoptedAge01_df.drop(adoptedAge01_df[adoptedAge01_df['Type'] ==2].index, inplace=True)\n\nplt.figure(figsize=(20, 8));\n\nsns.set_color_codes(\"pastel\")\nax[1]=sns.countplot(data=adoptedAge01_df, x=\"Age\",order = adoptedAge01_df['Age'].value_counts().sort_index(ascending =True).index,color='b',ax=ax[1])\nadoptedAge0_df=adoptedAge01_df.loc[adoptedAge01_df['AdoptionSpeed']==0]\n\n\n\ndf=adoptedAge0_df['Age'].value_counts().sort_index(ascending =True)\/adoptedAge01_df['Age'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\n\nfor i,p in enumerate(ax[1].patches):\n    ratio = df[i]\n    ax[1].annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax[1]=sns.countplot(data=adoptedAge0_df, x=\"Age\",order = adoptedAge01_df['Age'].value_counts().sort_index(ascending =True).index,ax=ax[1])\n\n","26f77498":"print(breeds.info())\nprint(train_df[train_df['Type']==1]['Breed1'].value_counts())\nprint(train_df[train_df['Type']==2]['Breed1'].value_counts())","2a567c86":"print(breeds[breeds['BreedID']==307]['BreedName']) # Mixed Dog\nprint(breeds[breeds['BreedID']==266]['BreedName']) # Domestic short Hair Cat\nprint(breeds[breeds['BreedID']==265]['BreedName']) # Domestic Medium Hair Cat\nprint(breeds[breeds['BreedID']==264]['BreedName']) # Domestic Long Hair Cat\n","933c1762":"\ndogBreedsMixed_df=train_df[(train_df['Breed1']==307)& (train_df['Type']== 1)] #this is a df with only the mixed breeds dogs\ndogBreedsMixed_df['Breed1']='Mixed Dogs'\ndogBreedsPure_df=train_df[(train_df['Breed1']!=307) & (train_df['Type']== 1)] #this is a df with only the Pure breeds dogs\n\ndogBreedsPure_df['Breed1']='PureDogs'\n\ncatBreedsPure_df=train_df[(train_df['Breed1']!=266) & (train_df['Breed1']!=265) & (train_df['Breed1']!=264) & (train_df['Type']== 2)  ] #this is a df with Pure Breeds Cats\ncatBreedsPure_df['Breed1']='PureCats'\n\ncatShortHair_df=train_df[(train_df['Breed1']==266) & (train_df['Type']== 2)]# Domestic Short Hair Cats\ncatShortHair_df['Breed1']='Domestic Short Hair Cats'\ncatMediumHair_df=train_df[(train_df['Breed1']==265) & (train_df['Type']== 2)]#Domestic Medium Hair Cats\ncatMediumHair_df['Breed1']='Domestic Medium Hair Cats'\n\ncatLongHair_df=train_df[(train_df['Breed1']==264) & (train_df['Type']== 2)]# Domestic Long Hair Cats\ncatLongHair_df['Breed1']='Domestic Long Hair Cats'\n\n\nBreeds_df=dogBreedsMixed_df.copy() #combining all dataframes to one \nBreeds_df=Breeds_df.append(dogBreedsPure_df)\nBreeds_df=Breeds_df.append(catBreedsPure_df)\nBreeds_df=Breeds_df.append(catShortHair_df)\nBreeds_df=Breeds_df.append(catMediumHair_df)\nBreeds_df=Breeds_df.append(catLongHair_df)\n\n\nBreedsAdopted_df=Breeds_df[Breeds_df['AdoptionSpeed']<=3] #df of all the adopted animals\nBreedsP=BreedsAdopted_df['Breed1'].value_counts() \/Breeds_df['Breed1'].value_counts() #get the precent of adoption per Breed\n\n","79435591":"plt.figure(figsize=(20, 10));\n\n\n\n\nax=sns.countplot(x=\"Breed1\", data=Breeds_df,color='g',order = Breeds_df['Breed1'].value_counts().index)\nfor i,p in enumerate(ax.patches):\n    \n    ax.annotate(\"%.2f %% adopted\" % BreedsP[i] , (p.get_x() + p.get_width() \/ 2., p.get_height()-100),\n                ha='center', va='center', fontsize=11, color='black', xytext=(0, 20),\n                textcoords='offset points')\n\n    \nax=sns.countplot(x=\"Breed1\", data=BreedsAdopted_df,color='b',order = Breeds_df['Breed1'].value_counts().index)\nplt.xlabel('Breed')","675dcc23":"a=train_df.copy()\ncounts = a['Breed1'].value_counts()\ncounts[counts>100]\na=a[a['Breed1'].isin(counts[counts > 100].index)]\npopBreeds=a['Breed1'].value_counts().index\nBreedNames=[]\n#for i in popBreeds:\n    #print(breeds[breeds['BreedID']==i]['BreedName']) # Mixed Dog\nplt.figure(figsize=(20, 10))\n\n    \nsns.set_color_codes(\"pastel\")\nax=sns.countplot(data=a, x=\"Breed1\",order = a['Breed1'].value_counts().sort_index(ascending =True).index,color='b')\na0=a.loc[a['AdoptionSpeed']!=4]\n\ndf=a0['Breed1'].value_counts().sort_index(ascending =True)\/a['Breed1'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\n\nfor i,p in enumerate(ax.patches):\n    ratio = df[i]\n    ax.annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax=sns.countplot(data=a0, x=\"Breed1\",order = a['Breed1'].value_counts().sort_index(ascending =True).index, color='b')\n\n_=ax.set_xticklabels(['Golden Retriever','Labrador Retriever','Poodle','Shih Tzu','Terrier','Calico','Domestic Long Hair','Domestic Medium Hair','Domestic Short Hair',\n                   'Persian','Siamese','Tabby','Mixed Breed'],rotation=45)\n","e8bb67db":"train_df['Gender'].value_counts()","af253b90":"plt.figure(figsize=(20, 10));\n\nax=sns.countplot(data=train_df, x=\"Gender\", hue=\"AdoptionSpeed\")\nax.set_xticklabels([\"Male\",\"Female\", \"Group\"])\n\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n    \n","e48d32be":"adopted_df=train_df.copy()\nadopted_df['AdoptionSpeed'].replace(\n    to_replace=[ 0, 1, 2 ,3 ],\n    value='Yes',\n    inplace=True\n)\nadopted_df['AdoptionSpeed'].replace(\n    to_replace=[ 4 ],\n    value='No',\n    inplace=True\n)\nfig1, ax1 = plt.subplots(1,3,figsize=(15,5))\nexplode = (0, 0.1)\nlabels = \"Adopted\" ,\"Weren't Adopted\"\nax1[0].set_title(\"Male Adoptation Rate\",fontsize=14)\nax1[1].set_title(\"Female Adoptation Rate\",fontsize=14)\nax1[2].set_title(\"Group Adoptation Rate\",fontsize=14)\n\n\nax1[0].pie([adopted_df[(adopted_df['Gender']==1) & (adopted_df['AdoptionSpeed']=='Yes')].count()[0],adopted_df[(adopted_df['Gender']==1) & (adopted_df['AdoptionSpeed']=='No')].count()[0]], autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['C3','C9'],labels=labels)\n\nax1[1].pie([adopted_df[(adopted_df['Gender']==2) & (adopted_df['AdoptionSpeed']=='Yes')].count()[0],adopted_df[(adopted_df['Gender']==2) & (adopted_df['AdoptionSpeed']=='No')].count()[0]], autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['C1','C2'],labels=labels)\n\nax1[2].pie([adopted_df[(adopted_df['Gender']==3) & (adopted_df['AdoptionSpeed']=='Yes')].count()[0],adopted_df[(adopted_df['Gender']==3) & (adopted_df['AdoptionSpeed']=='No')].count()[0]], autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['C10','C6'],labels=labels)\nplt.show()\n\n\n","8e7ab9da":"\n\n\nplt.figure(figsize=(23, 10));\nsns.set(font_scale=1.4)\nax = sns.countplot(x=\"Color1\",hue='AdoptionSpeed', data=train_df)\nax.set(xlabel = 'Animal', ylabel = 'Count')\nax.set_xticklabels([\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"])\ncolorP=[]\n\nfor i in range(1,8):\n    colorP.append((train_df['Color1']==i).value_counts()[1])\n\n\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f%%\" % (p.get_height() *100  \/ colorP[i%7]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=8, color='black', rotation=0, xytext=(0, 10),\n         textcoords='offset points')  \n\nplt.legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\n\n\n\n\n\n\n\n","9e60a384":"\nfig, ax = plt.subplots(1,2,figsize=(30,10))\nax[0].set_title(\"Cats Color\",fontsize=14)\nax[1].set_title(\"Dogs Color\",fontsize=14)\n\ntrainCats_df=train_df.copy()\n\ntrainCats_df.drop(trainCats_df[trainCats_df['Type'] ==1].index, inplace=True)\n\nax[0]=sns.countplot(data=trainCats_df, x=\"Color1\",hue='AdoptionSpeed',ax=ax[0])\ncolorP=[]\n\nfor i in range(1,8):\n\n    colorP.append((trainCats_df['Color1']==i).value_counts()[1])\n\nmax=0\nfor i,p in enumerate(ax[0].patches):\n    \n    ax[0].annotate(\"%.2f%%\" % (p.get_height() *100  \/ colorP[i%7]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=90, xytext=(0, 20),\n         textcoords='offset points') \n    if p.get_height()>max:\n        max=p.get_height()\n\nax[0].set_xticklabels([\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"])\nax[0].set_ylim(0,max+100)\n\n\n###########Dogs\n    \ntrainDogs_df=train_df.copy()\ntrainDogs_df.drop(trainDogs_df[trainDogs_df['Type'] ==2].index, inplace=True)\n    \nax[1]=sns.countplot(data=trainDogs_df, x=\"Color1\",hue='AdoptionSpeed',ax=ax[1])\ncolorP=[]\n\nfor i in range(1,8):\n    colorP.append((trainDogs_df['Color1']==i).value_counts()[1])\n\nmax=0\nfor i,p in enumerate(ax[1].patches):\n    \n    ax[1].annotate(\"%.2f%%\" % (p.get_height() *100  \/ colorP[i%7]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=90, xytext=(0, 20),\n         textcoords='offset points')\n    if p.get_height()>max:\n        max=p.get_height()\nax[1].set_xticklabels([\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"])\nax[1].set_ylim(0,max+100)\n\n\nax[0].legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nax[1].legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\n\n","8304e663":"\nplt.figure(figsize=(10, 8))\nax=sns.countplot(data=train_df, x=\"MaturitySize\")\n\nfor p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 \/ train_df.shape[0]:.2f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n         textcoords='offset points')  \nax=ax.set_xticklabels([\"Small\",\"Medium\", \"Large\",\" Extra Large\"])\n\n","7f83b2f4":"plt.figure(figsize=(10, 8))\nadoptedYes_df=adopted_df.loc[adopted_df['AdoptionSpeed']=='Yes'] # all adopted pets\n\nsns.set_color_codes(\"pastel\")\nax=sns.countplot(x=\"MaturitySize\", data=adopted_df, color='b',order = adopted_df['MaturitySize'].value_counts().sort_index(ascending =True).index) #plot all the pets by MaturitySize\n\ndf=adoptedYes_df['MaturitySize'].value_counts().sort_index(ascending =True)\/adopted_df['MaturitySize'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\nfor i,p in enumerate(ax.patches):\n    ratio = df[i]\n    ax.annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n        \n    \nsns.set_color_codes(\"muted\")\n\nsns.countplot(x=\"MaturitySize\", data=adoptedYes_df ,color='b',order = adopted_df['MaturitySize'].value_counts().sort_index(ascending =True).index) # plot only the adopted by MaturitySize\n\nax.set_xticklabels([\"Small\",\"Medium\", \"Large\",\" Extra Large\"])\n","3bf38fec":"MaturitySizeP=[]\nMaturitySizeP.append((train_df['MaturitySize']==1).value_counts()[1])\nMaturitySizeP.append((train_df['MaturitySize']==2).value_counts()[1])\nMaturitySizeP.append((train_df['MaturitySize']==3).value_counts()[1])\nMaturitySizeP.append((train_df['MaturitySize']==4).value_counts()[1])\nplt.figure(figsize=(22, 8))\nax=sns.countplot(data=train_df, x=\"MaturitySize\",hue='AdoptionSpeed')\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\n\nax.set_xticklabels([\"Small\",\"Medium\", \"Large\",\" Extra Large\"])\n\nfor i,p in enumerate(ax.patches):\n     ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ MaturitySizeP[i%4]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=10, xytext=(0, 10),\n         textcoords='offset points')  \n","3c1c5117":"\nplt.figure(figsize=(20, 8))\n\nax=sns.countplot(data=train_df, x=\"FurLength\", hue=\"AdoptionSpeed\")\nFurLengthP=[]\n\nfor i in range(1,4):\n    FurLengthP.append((train_df['FurLength']==i).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n    ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, -5),\n         textcoords='offset points')  \nax.set_xticklabels([\"Short\",\"Medium\", \"Long\"])\n\n  \n    \n\n","56d91961":"\nfig, ax = plt.subplots(1,2,figsize=(30,10))\nax[0].set_title(\"Cats FurLength\",fontsize=14)\nax[1].set_title(\"Dogs FurLength\",fontsize=14)\n\ntrainCats_df=train_df.copy()\n\ntrainCats_df.drop(trainCats_df[trainCats_df['Type'] ==1].index, inplace=True)\n\nax[0]=sns.countplot(data=trainCats_df, x=\"FurLength\",hue='AdoptionSpeed',ax=ax[0])\nFurLengthP=[]\n\nfor i in range(1,4):\n\n    FurLengthP.append((trainCats_df['FurLength']==i).value_counts()[1])\n\nfor i,p in enumerate(ax[0].patches):\n    \n    ax[0].annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n         textcoords='offset points')  \n\n###########Dogs\n    \ntrainDogs_df=train_df.copy()\ntrainDogs_df.drop(trainDogs_df[trainDogs_df['Type'] ==2].index, inplace=True)\n    \nax[1]=sns.countplot(data=trainDogs_df, x=\"FurLength\",hue='AdoptionSpeed',ax=ax[1])\nFurLengthP=[]\n\nfor i in range(1,4):\n    FurLengthP.append((trainDogs_df['FurLength']==i).value_counts()[1])\n\nfor i,p in enumerate(ax[1].patches):\n    \n    ax[1].annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n         textcoords='offset points')  \n\n\nax[0].legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nax[1].legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nax[0].set_xticklabels([\"Short\",\"Medium\", \"Long\"])\nax[1].set_xticklabels([\"Short\",\"Medium\", \"Long\"])\n\n\n","179d0653":"\nplt.figure(figsize=(20, 8))\n\nax=sns.countplot(data=train_df, x=\"Vaccinated\", hue=\"AdoptionSpeed\")\nFurLengthP=[]\n\nfor i in range(1,4):\n    FurLengthP.append((train_df['Vaccinated']==i).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n    ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, -5),\n         textcoords='offset points')  \nax.set_xticklabels([\"Yes\",\"No\", \"Not Sure\"])\n\n  \n    \n\n","f2052d8f":"plt.figure(figsize=(10, 8))\n\nax=sns.boxplot(data=train_df, x=\"Vaccinated\",y='Age',hue='AdoptionSpeed')\nax.set_xticklabels(['Yes','No','Not Sure'], fontsize=12)\nax.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\n","04407c69":"\nplt.figure(figsize=(20, 8))\n\nax=sns.countplot(data=train_df, x=\"Dewormed\", hue=\"AdoptionSpeed\")\nFurLengthP=[]\n\nfor i in range(1,4):\n    FurLengthP.append((train_df['Dewormed']==i).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n    ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, -5),\n         textcoords='offset points')  \n\nax.set_xticklabels(['Yes','No','Not Sure'], fontsize=12)","5de7d0e6":"plt.figure(figsize=(10, 8))\n\nax=sns.boxplot(data=train_df, x=\"Dewormed\",y='Age',hue='AdoptionSpeed')\nax.set_xticklabels(['Yes','No','Not Sure'], fontsize=12)\n","00568f26":"\nplt.figure(figsize=(20, 8))\n\nax=sns.countplot(data=train_df, x=\"Sterilized\", hue=\"AdoptionSpeed\")\nax.set_xticklabels([\"Yes\",\"No\",\"Not Sure\"])\n\nFurLengthP=[]\n\nfor i in range(1,4):\n    FurLengthP.append((train_df['Sterilized']==i).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n    ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, -5),\n         textcoords='offset points')  \n\n  \n\n\n","53e2f1fb":"plt.figure(figsize=(10, 8))\n\nax=sns.boxplot(data=train_df, x=\"Sterilized\",y='Age',hue='AdoptionSpeed')\nax.set_xticklabels(['Yes','No','Not Sure'], fontsize=12)\n","d38b07e5":"\nplt.figure(figsize=(20, 8))\n\nax=sns.countplot(data=train_df, x=\"Health\", hue=\"AdoptionSpeed\")\nFurLengthP=[]\nax.set_xticklabels([\"Healthy\",\"Minor Injury\",\"Serious Injury\"])\n\nfor i in range(1,4):\n    FurLengthP.append((train_df['Health']==i).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(\"%.2f\" % p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n    ax.annotate(\"%.2f %%\" % (p.get_height() *100  \/ FurLengthP[i%3]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, -5),\n         textcoords='offset points')  \n\n  \n    \n\n","e22e704b":"train_df['Quantity'].value_counts()\n","6ffac856":"train10_df=adopted01_df.copy()\n\ntrain10_df.loc[train10_df['Quantity']>=10, 'Quantity']=10\n\nplt.figure(figsize=(25, 10))\n\nsns.set_color_codes(\"pastel\")\nax=sns.countplot(data=train10_df, x=\"Quantity\",order = train10_df['Quantity'].value_counts().sort_index(ascending =True).index,color='b')\nplt.legend(title='Adoption Rate', loc='upper right')\nadopted0_df=train10_df.loc[train10_df['AdoptionSpeed']==0]\n\ndf=adopted0_df['Quantity'].value_counts().sort_index(ascending =True)\/train10_df['Quantity'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n    \nfor i,p in enumerate(ax.patches):\n    ratio = df[i]\n    ax.annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nsns.countplot(data=adopted0_df, x=\"Quantity\",order = train10_df['Quantity'].value_counts().sort_index(ascending =True).index, color='b')\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nlabels[-1] = '10+'\nax=ax.set_xticklabels(labels)\n\n   ","8d484d67":"train_df['Fee'].describe()","05c4eb97":"plt.figure(figsize=(16, 10));\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\nsns.scatterplot(data=train_df, x=train_df['AdoptionSpeed'],hue='Type',y=train_df['Fee'],palette=[\"m\", \"g\"])\nplt.legend(title='Pet Type', loc='upper left', labels=['Cat','Dog'])\n\nsns.despine(offset=10, trim=True)\n","bc5fbd24":"fee_df=train_df.copy()\nprint(fee_df['Fee'].value_counts(normalize=True,bins=12)) #seems like after 500 we can cut the outliers\n\nfee_df.drop(fee_df[fee_df['Fee'] >500].index, inplace=True)\n","68d5993c":"plt.figure(figsize=(16, 10));\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\nsns.violinplot(data=fee_df, x=fee_df['AdoptionSpeed'],hue='Type',y=fee_df['Fee'],palette=[\"m\", \"g\"])\nsns.despine(offset=10, trim=True)","b5696f24":"adopted01Fee_df=adopted01_df.copy()\nadopted01Fee_df.drop(adopted01Fee_df[adopted01Fee_df['Fee'] >500].index, inplace=True)\nadopted01Fee_df.loc[adopted01Fee_df['Fee']>=1, 'Fee']='Paid'\nadopted01Fee_df.loc[adopted01Fee_df['Fee']==0, 'Fee']='Free'\n#################### Fee with Binning\nadoptedBinsFee_df=adopted01_df.copy()\nbins=[-1,0,50,100,150,200,250,300,350,400,450,500]\nadoptedBinsFee_df['Fee'] = pd.cut(adoptedBinsFee_df['Fee'], bins)\nadoptedBinsFee_df['Fee'].value_counts()","89d0124b":"fig, ax = plt.subplots(1,2,figsize=(30,10))\nax[0].set_title(\"Binning Adoptation Rate\",fontsize=14)\nax[1].set_title(\"Free or Fee Adoptation Rate\",fontsize=14)\n\n######################Free or Fee Adoptation Rate \n\n\nsns.set_color_codes(\"pastel\")\nax[0]=sns.countplot(data=adoptedBinsFee_df, x=\"Fee\",order = adoptedBinsFee_df['Fee'].value_counts().sort_index(ascending =True).index,color='b',ax=ax[0])\nadopted0BinsFee_df=adoptedBinsFee_df.loc[adoptedBinsFee_df['AdoptionSpeed']==0]\n\ndf=adopted0BinsFee_df['Fee'].value_counts().sort_index(ascending =True)\/adoptedBinsFee_df['Fee'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\nfor i,p in enumerate(ax[0].patches):\n    ratio = df[i]\n    ax[0].annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax[0]=sns.countplot(data=adopted0BinsFee_df, x=\"Fee\",order = adoptedBinsFee_df['Fee'].value_counts().sort_index(ascending =True).index, color='b',ax=ax[0])\n\n######################Binning Adoptation Rate \n\n\nsns.set_color_codes(\"pastel\")\nax[1]=sns.countplot(data=adopted01Fee_df, x=\"Fee\",order = adopted01Fee_df['Fee'].value_counts().sort_index(ascending =True).index,color='b',ax=ax[1])\nadopted0Fee_df=adopted01Fee_df.loc[adopted01Fee_df['AdoptionSpeed']==0]\n\ndf=adopted0Fee_df['Fee'].value_counts().sort_index(ascending =True)\/adopted01Fee_df['Fee'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\n\nfor i,p in enumerate(ax[1].patches):\n    ratio = df[i]\n    ax[1].annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax[1]=sns.countplot(data=adopted0Fee_df, x=\"Fee\",order = adopted01Fee_df['Fee'].value_counts().sort_index(ascending =True).index, color='b',ax=ax[1])","4d593531":"plt.figure(figsize=(20, 15));\n\nax = plt.subplot2grid((2, 2), (0, 0))\n\nax2 = plt.subplot2grid((2, 2), (0, 0))\nsns.boxplot(x=\"AdoptionSpeed\", y=\"PhotoAmt\", hue=\"Type\", data=train_df);\nplt.title('AdoptionSpeed by Type and PhotoAmt');\n\nax3 = plt.subplot2grid((2, 2), (0, 1))\nsns.boxenplot(x=\"AdoptionSpeed\", y=\"PhotoAmt\", data=train_df);\n\nax4 = plt.subplot2grid((2, 2), (1, 0),colspan=2)\n\nax=sns.countplot(data=train_df, x=\"PhotoAmt\")\n\nfor p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 \/ train_df.shape[0]:.1f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n         textcoords='offset points')  ","85598783":"plt.figure(figsize=(20, 8));\nadoptedPhoto01_df=adopted01_df.copy()\n\nsns.set_color_codes(\"pastel\")\nax=sns.countplot(data=adoptedPhoto01_df, x=\"PhotoAmt\",order = adoptedPhoto01_df['PhotoAmt'].value_counts().sort_index(ascending =True).index,color='b')\nadoptedPhoto0_df=adoptedPhoto01_df.loc[adoptedPhoto01_df['AdoptionSpeed']==0]\n\n\ndf=adoptedPhoto0_df['PhotoAmt'].value_counts().sort_index(ascending =True)\/adoptedPhoto01_df['PhotoAmt'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\n\nfor i,p in enumerate(ax.patches):\n    ratio = df[i]\n    ax.annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()+20),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax=sns.countplot(data=adoptedPhoto0_df, x=\"PhotoAmt\",order = adoptedPhoto01_df['PhotoAmt'].value_counts().sort_index(ascending =True).index, color='b')","96a94ae9":"videoAmt_df=train_df.copy()\n\nvideoAmt_df['VideoAmt'] = pd.cut(videoAmt_df['VideoAmt'], bins=[-1,0,10])\nvideoAmt_df['VideoAmt'].value_counts()\n\nplt.figure(figsize=(20, 8));\n\n\nax=sns.countplot(data=videoAmt_df, x=\"VideoAmt\",hue='AdoptionSpeed')\nvideoP=[]\nvideoP.append((videoAmt_df['VideoAmt']==pd.Interval(-1, 0, closed='right')).value_counts()[1])\nvideoP.append((videoAmt_df['VideoAmt']==pd.Interval(0, 10, closed='right')).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nmax=0\nfor i,p in enumerate(ax.patches):\n    \n    ax.annotate(\"%.2f%%\" % (p.get_height() *100  \/ videoP[i%2]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=90, xytext=(0, 30),\n         textcoords='offset points')\n    if p.get_height()>max:\n        max=p.get_height()\nax.set_ylim(0,max+ 660)\n\n","5c732729":"a=train_df.copy()\ncounts = a['RescuerID'].value_counts()\ncounts[counts>50]\na=a[a['RescuerID'].isin(counts[counts > 50].index)]\npopBreeds=a['RescuerID'].value_counts().index\nplt.figure(figsize=(22, 10));\n\nsns.set_color_codes(\"pastel\")\nax=sns.countplot(data=a, x=\"RescuerID\",order = a['RescuerID'].value_counts().sort_index(ascending =True).index,color='b')\na0=a.loc[a['AdoptionSpeed']!=4]\n\ndf=a0['RescuerID'].value_counts().sort_index(ascending =True)\/a['RescuerID'].value_counts().sort_index(ascending =True) # getting the right P%\ndf=df.to_numpy()\n\n\nfor i,p in enumerate(ax.patches):\n    ratio = df[i]\n    ax.annotate(\"%.2f %% \" % ratio  , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                textcoords='offset points')\n    \nsns.set_color_codes(\"muted\")\nax=sns.countplot(data=a0, x=\"RescuerID\",order = a['RescuerID'].value_counts().sort_index(ascending =True).index, color='b')\n_=plt.xticks(rotation=90)","26c01093":"from PIL import Image\nmask = np.array(Image.open('..\/input\/cnn-models-petfinder\/istockphoto-513133900-612x612.jpg'))\n\ntrain_df['Description']=train_df['Description'].fillna('').values\nfig, ax = plt.subplots(figsize = (15, 15))\nDescription = ' '.join(train_df['Description'])\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=mask.shape[1], height=mask.shape[0],mask=mask).generate(Description)\nplt.axis(\"off\");\nplt.imshow(wordcloud)\nplt.title('Top words in description');","35f8e37a":"train_df['DescriptonWordsCount'] = train_df['Description'].apply(lambda x: len(x.split()))\nfig, ax = plt.subplots(1,2,figsize=(30,10))\nax[0].set_title(\"DescriptonWordsCount\",fontsize=14)\nax[1].set_title(\"DescriptonWordsCount\",fontsize=14)\nax[0]=sns.boxenplot(x=\"AdoptionSpeed\", y=\"DescriptonWordsCount\", hue=\"Type\", data=train_df,ax=ax[0]);\nax[1]=plt.hist(train_df.DescriptonWordsCount,bins=100)","7bec0115":"cleanedDescripton=train_df.copy()\ncleanedDescripton=cleanedDescripton[(np.abs(stats.zscore(cleanedDescripton['DescriptonWordsCount'])) < 3)]\nfig, ax = plt.subplots(1,2,figsize=(30,10))\nax[0].set_title(\"DescriptonWordsCount\",fontsize=14)\nax[1].set_title(\"DescriptonWordsCount\",fontsize=14)\nax[0]=sns.boxenplot(x=\"AdoptionSpeed\", y=\"DescriptonWordsCount\", hue=\"Type\", data=cleanedDescripton,ax=ax[0]);\nax[1]=plt.hist(cleanedDescripton.DescriptonWordsCount,bins=10)","c93a6172":"cleanedDescriptonBins=cleanedDescripton.copy()\n\ncleanedDescriptonBins['DescriptonWordsCount'] = pd.cut(cleanedDescriptonBins['DescriptonWordsCount'], bins=10)\ncleanedDescriptonBins['DescriptonWordsCount'].value_counts()\n\nplt.figure(figsize=(20, 8));\n\n    \nax=sns.countplot(data=cleanedDescriptonBins, x=\"DescriptonWordsCount\",hue='AdoptionSpeed')\nwordsP=[]\nwordsP.append((cleanedDescriptonBins['DescriptonWordsCount']==pd.Interval(-0.27, 27, closed='right')).value_counts()[1])\n\nfor i in range(1,10):\n    wordsP.append((cleanedDescriptonBins['DescriptonWordsCount']==pd.Interval(i*27, (i+1)*27, closed='right')).value_counts()[1])\n\n\nplt.legend(title='Adoption Speed', loc='upper right', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nmax=0\nfor i,p in enumerate(ax.patches):\n    \n    ax.annotate(\"%.2f%%\" % (p.get_height() *100  \/ wordsP[i%10]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=90, xytext=(0, 30),\n         textcoords='offset points')\n    if p.get_height()>max:\n        max=p.get_height()\nax.set_ylim(0,max+200)","e1232abc":"train_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\ndata=[]\n\nfor i in range (len(train_sentiment_files)):\n    with open(train_sentiment_files[i]) as jsonFile:\n        jsonObject = json.load(jsonFile)\n        jsonFile.close()\n    score=jsonObject['documentSentiment']['score']\n    magnitude=jsonObject['documentSentiment']['magnitude']\n    path=train_sentiment_files[i]\n    firstpos=path.rfind(\"\/\")\n    lastpos=path.rfind(\".\")\n    PetID=path[firstpos+1:lastpos]\n    data.append([PetID,score,magnitude,score*magnitude])\n\ndata = pd.DataFrame(data, columns=['PetID','Score', 'Magnitude','Sentiment'])\n\ntrain_df=pd.merge(train_df, data, on='PetID',how='outer')\ntrain_df=train_df.fillna(0)","073f0d91":"plt.figure(figsize=(20, 8));\n\ncluster_df=train_df.copy()\ncluster_df=cluster_df.drop(['Type', 'Name', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n       'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'RescuerID',\n       'VideoAmt', 'Description', 'PetID', 'PhotoAmt', \n       'DescriptonWordsCount'],axis=1)\n\n\nkmeans = KMeans(n_clusters=3, random_state=0).fit(cluster_df[['Score','Magnitude']])\ny_kmeans = kmeans.predict(cluster_df[['Score','Magnitude']])\n\nplt.scatter( cluster_df['Magnitude'],cluster_df['Score'], c=y_kmeans, s=10, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.xticks([1,2,3,4,5,6,7,8,9,10,15,20,30])\nplt.grid(which='both')\nplt.xlabel('Magnitude')\nplt.ylabel('Score')\nplt.scatter(centers[:, 1], centers[:, 0], c='black', s=500, alpha=0.5);\n","800b502c":"data=[]\n\nfor i in range (len(train_sentiment_files)):\n    with open(train_sentiment_files[i]) as jsonFile:\n        jsonObject = json.load(jsonFile)\n        jsonFile.close()\n    score=jsonObject['documentSentiment']['score']\n    magnitude=jsonObject['documentSentiment']['magnitude']\n    path=train_sentiment_files[i]\n    firstpos=path.rfind(\"\/\")\n    lastpos=path.rfind(\".\")\n    PetID=path[firstpos+1:lastpos]\n    if score >=0.1:\n        if magnitude < 2:\n            data.append([PetID,'Weak Postive'])\n        elif magnitude < 6 and magnitude>=2:\n            data.append([PetID,'Medium Postive'])\n        elif magnitude >=6:\n            data.append([PetID,'Strong Postive'])\n            \n    elif score <=-0.1 :\n        if magnitude < 2:\n            data.append([PetID,'Weak Negative'])\n        elif magnitude < 6 and magnitude>=2:\n            data.append([PetID,'Medium Negative'])\n        elif magnitude >=6:\n            data.append([PetID,'Strong Negative'])\n            \n    else:\n        data.append([PetID,'Netural'])\n\ndata = pd.DataFrame(data, columns=['PetID','sentiment_TEXT'])\n\ntrain_df=pd.merge(train_df, data, on='PetID',how='outer')\ntrain_df=train_df.drop(['Score','Magnitude','Sentiment'],axis=1)","09db05d0":"train_df.head()","73316e1f":"plt.figure(figsize=(20, 8))\n\nax=sns.countplot(data=train_df, x=\"sentiment_TEXT\", hue=\"AdoptionSpeed\")\nsentimentP=[]\nprint(train_df['sentiment_TEXT'].value_counts())\n\n\n\nplt.legend(title='Adoption Speed', loc='upper left', labels=['Same Day', '1St week','1st Month','2nd & 3rd month','No adoption'])\nfor i,p in enumerate(ax.patches):\n    ax.annotate(  p.get_height() , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10),\n                textcoords='offset points')\n  \n    \n\n","041a3f21":"import en_core_web_sm\n\nnlp = en_core_web_sm.load()\nfrom spacy import displacy\n\n","7621047e":"\"\"\"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\ndef identify_tokens(row):\n    review = row['Description']\n    tokens = nltk.word_tokenize(review)\n    # taken only words (not punctuation)\n    token_words = [w for w in tokens if w.isalpha()]\n    return token_words\n\n\np_stemmer=PorterStemmer()\ns_stemmer=SnowballStemmer(language='english')\nmeow=train_df.copy()\n\nmeow['words'] = meow.apply(identify_tokens, axis=1)\n\nmeow['stemmed'] = meow['words'].apply(lambda x: [s_stemmer.stem(y) for y in x]) # Stem every word.\nmeow['readyS']=meow['stemmed'].apply(lambda x : \" \".join(x))\n\n\nmeow['stemmedP'] = meow['words'].apply(lambda x: [p_stemmer.stem(y) for y in x]) # Stem every word.\nmeow['readyP']=meow['stemmedP'].apply(lambda x : \" \".join(x))\n\"\"\"","54fe0154":"import nltk\ntrain_NLP=train_df.copy()\ntrain_NLP[\"lemmatized\"] = train_NLP['Description'].apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))\ntrain_NLP['lemmatized'] = train_NLP['lemmatized'].str.replace(r'[^\\w\\s]+', '')\ntrain_df['lemmatized'] = train_NLP['lemmatized'].str.replace(r'[^\\w\\s]+', '')\n\n\ntxt = train_NLP.lemmatized.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\nwords = nltk.tokenize.word_tokenize(txt)\nword_dist = nltk.FreqDist(words)\n\nstopwords = nlp.Defaults.stop_words\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n\n","2665ee75":"print('All frequencies:')\nprint('=' * 60)\nrslt = pd.DataFrame(words_except_stop_dist.most_common(100),\n                    columns=['Word', 'Frequency'])\nrslt\n","a7f86372":"wordAdoption=[]\n\nfor word in rslt['Word']:\n    #print(word)\n    if word=='pron':\n        continue\n        \n    YESKeyword=train_NLP[train_NLP['lemmatized'].str.contains(word)]\n    NOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains(word)]\n    wordAdoption.append([word,\n                         YESKeyword.count()[0],\n                         YESKeyword.count()[0]\/train_NLP.count()[0],\n                         YESKeyword['AdoptionSpeed'].value_counts().sort_index(ascending=True)[4],\n                         YESKeyword['AdoptionSpeed'].value_counts().sort_index(ascending=True)[4]\/YESKeyword.count()[0],\n                         NOKeyword['AdoptionSpeed'].value_counts().sort_index(ascending=True)[4],\n                         NOKeyword['AdoptionSpeed'].value_counts().sort_index(ascending=True)[4]\/NOKeyword.count()[0],\n                         (YESKeyword['AdoptionSpeed'].value_counts().sort_index(ascending=True)[4]\/YESKeyword.count()[0])-(NOKeyword['AdoptionSpeed'].value_counts().sort_index(ascending=True)[4]\/NOKeyword.count()[0])])\nwordAdoption = pd.DataFrame(wordAdoption, columns=['Word','Numbers of Rows that has the word','% of rows with that word in the Data', ' Have the word, No Adoption ','Have the word % no Adoption','No Word, No Adoption','No word % no Adoption','Difference between adoption Rate'])\n","21dc5974":"wordAdoption.sort_values('Difference between adoption Rate',ascending=True).head(30)","ac7c96f4":"def compareWords(YESKeyword,NOKeyword):\n    \n    fig, ax = plt.subplots(1,2,figsize=(30,10))\n    ax[0].set_title(\"YES KEYWORD\",fontsize=14)\n    ax[1].set_title(\"NO KEYWORD\",fontsize=14)\n\n    ax[0]=sns.countplot(data=YESKeyword, x='AdoptionSpeed',ax=ax[0])\n\n    for i,p in enumerate(ax[0].patches):\n        \n        ax[0].annotate(\"%.2f %%\" % (p.get_height() *100  \/YESKeyword.count()[0] ) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n             ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n             textcoords='offset points')  \n    \n###########NO\n    ax[1]=sns.countplot(data=NOKeyword, x='AdoptionSpeed',ax=ax[1])\n    P=[]\n    \n    for i in range(0,5):\n        P.append((NOKeyword['AdoptionSpeed']==i).value_counts()[1])\n\n    for i,p in enumerate(ax[1].patches):\n        \n        ax[1].annotate(\"%.2f %%\" % (p.get_height() *100  \/ NOKeyword.count()[0]) , (p.get_x() + p.get_width() \/ 2., p.get_height()),\n             ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n             textcoords='offset points')  ","3fff29b7":"keywords=['interested']\nYESKeyword=train_NLP[train_NLP['lemmatized'].str.contains('|'.join(keywords))]\nNOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains('|'.join(keywords))]\ncompareWords(YESKeyword,NOKeyword)","1addc4d6":"keywords=['neuter','spay']\nYESKeyword=train_NLP[train_NLP['lemmatized'].str.contains('|'.join(keywords))]\nNOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains('|'.join(keywords))]\ncompareWords(YESKeyword,NOKeyword)","25b5c42d":"YESKeyword=train_NLP[(train_NLP['lemmatized'].str.contains('neuter')) | (train_NLP['lemmatized'].str.contains('spay')) & (train_NLP['lemmatized'].str.contains('interested'))]\nNOKeyword=train_NLP[(~train_NLP['lemmatized'].str.contains('neuter')) & (~train_NLP['lemmatized'].str.contains('spay'))& (~train_NLP['lemmatized'].str.contains('interested'))]\ncompareWords(YESKeyword,NOKeyword)","e8c3b1af":"keywords=['train']\nYESKeyword=train_NLP[train_NLP['lemmatized'].str.contains('|'.join(keywords))]\nNOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains('|'.join(keywords))]\ncompareWords(YESKeyword,NOKeyword)","c7b198e4":"keywords=['play','playful','active']\nYESKeyword=train_NLP[train_NLP['lemmatized'].str.contains('|'.join(keywords))]\nNOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains('|'.join(keywords))]\ncompareWords(YESKeyword,NOKeyword)","d93f2b55":"wordAdoption.sort_values('Difference between adoption Rate',ascending=False).head(30)","433dcce6":"keywords=['dog']\nYESKeyword=train_NLP[train_NLP['lemmatized'].str.contains('|'.join(keywords))]\nNOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains('|'.join(keywords))]\ncompareWords(YESKeyword,NOKeyword)","e95013f4":"keywords=['friendly']\nYESKeyword=train_NLP[train_NLP['lemmatized'].str.contains('|'.join(keywords))]\nNOKeyword=train_NLP[~train_NLP['lemmatized'].str.contains('|'.join(keywords))]\ncompareWords(YESKeyword,NOKeyword)","3c9ade84":"YESKeyword=train_NLP[(train_NLP['lemmatized'].str.contains('friendly')) & (train_NLP['lemmatized'].str.contains('dog'))  ]\nNOKeyword=train_NLP[(~train_NLP['lemmatized'].str.contains('friendly')) & (~train_NLP['lemmatized'].str.contains('dog'))]\nplt.figure(figsize=(20, 10))\ncompareWords(YESKeyword,NOKeyword)","d6bd72b6":"train_df.head()\ntrainDropped_df=train_df.copy()\ntrainDropped_df.drop(['Breed2', 'State','RescuerID','Description','Color2','Color3','PetID','lemmatized'], axis=1, inplace=True)\n\ntest_df.head()\ntestDropped_df=test_df.copy()\ntestDropped_df['Description']=testDropped_df['Description'].fillna('').values\ntestDropped_df['DescriptonWordsCount'] = testDropped_df['Description'].apply(lambda x: len(x.split()))\n\ntestDropped_df.drop(['Breed2', 'State','RescuerID','Description','Color2','Color3','PetID'], axis=1, inplace=True)","3da81bc4":"trainDropped_df.head()","5a218c65":"trainCleaned1_df=trainDropped_df.copy()\ntestCleaned1_df=testDropped_df.copy()\n","63b4e05a":"trainCleaned1_df['Name'] =trainCleaned1_df['Name'].fillna('NULL')\ntrainCleaned1_df['Name'].replace({\"No Name\": \"NULL\", \"No Name Yet\": \"NULL\", \"Unknown\": \"NULL\"},inplace=True)\ntrainCleaned1_df.loc[trainCleaned1_df['Name'] !=\"NULL\", 'Name'] = 'HasName'\nprint(trainCleaned1_df['Name'].value_counts())\n\n\ntestCleaned1_df['Name'] =testCleaned1_df['Name'].fillna('NULL')\ntestCleaned1_df['Name'].replace({\"No Name\": \"NULL\", \"No Name Yet\": \"NULL\", \"Unknown\": \"NULL\"},inplace=True)\ntestCleaned1_df.loc[testCleaned1_df['Name'] !=\"NULL\", 'Name'] = 'HasName'\n","fdb87ea1":"\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']!=266) & (trainCleaned1_df['Breed1']!=265) & (trainCleaned1_df['Breed1']!=264) & (trainCleaned1_df['Breed1']!=307) & (trainCleaned1_df['Type']== 2),'Breed1'] =\"Pure Cats\"\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']!=266) & (trainCleaned1_df['Breed1']!=265) & (trainCleaned1_df['Breed1']!=264) & (trainCleaned1_df['Breed1']!=307) & (trainCleaned1_df['Type']== 1),'Breed1'] =\"Pure Dogs\"\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==307),'Breed1']='Mixed Dog'\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==266),'Breed1']='Domestic short Hair Cat'\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==265),'Breed1']='Domestic Medium Hair Cat'\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==264),'Breed1']='Domestic Long Hair Cat'\n\nprint(trainCleaned1_df['Breed1'].value_counts())\n\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']!=266) & (testCleaned1_df['Breed1']!=265) & (testCleaned1_df['Breed1']!=264) & (testCleaned1_df['Breed1']!=307) & (testCleaned1_df['Type']== 2),'Breed1'] =\"Pure Cats\"\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']!=266) & (testCleaned1_df['Breed1']!=265) & (testCleaned1_df['Breed1']!=264) & (testCleaned1_df['Breed1']!=307) & (testCleaned1_df['Type']== 1),'Breed1'] =\"Pure Dogs\"\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==307),'Breed1']='Mixed Dog'\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==266),'Breed1']='Domestic short Hair Cat'\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==265),'Breed1']='Domestic Medium Hair Cat'\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==264),'Breed1']='Domestic Long Hair Cat'\n","8dddb29b":"trainCleaned1_df.loc[(trainCleaned1_df['Gender']==1),'Gender']='Male'\ntrainCleaned1_df.loc[(trainCleaned1_df['Gender']==2),'Gender']='Female'\ntrainCleaned1_df.loc[(trainCleaned1_df['Gender']==3),'Gender']='Mixed'\n\ntestCleaned1_df.loc[(testCleaned1_df['Gender']==1),'Gender']='Male'\ntestCleaned1_df.loc[(testCleaned1_df['Gender']==2),'Gender']='Female'\ntestCleaned1_df.loc[(testCleaned1_df['Gender']==3),'Gender']='Mixed'","a45fd7a2":"colors=[\"Nan\",\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"]\n\nfor i in range(1,8):\n    trainCleaned1_df.loc[(trainCleaned1_df['Color1']==i),'Color1']=colors[i]\n    \nprint(trainCleaned1_df['Color1'].value_counts())\n\ncolors=[\"Nan\",\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"]\n\nfor i in range(1,8):\n    testCleaned1_df.loc[(testCleaned1_df['Color1']==i),'Color1']=colors[i]","d643f548":"trainCleaned1_df","c4b37d33":"trainCleaned1_df.loc[trainCleaned1_df['Fee']>=1, 'Fee']='Paid'\ntrainCleaned1_df.loc[trainCleaned1_df['Fee']==0, 'Fee']='Free'\n\ntestCleaned1_df.loc[testCleaned1_df['Fee']>=1, 'Fee']='Paid'\ntestCleaned1_df.loc[testCleaned1_df['Fee']==0, 'Fee']='Free'","811dd81c":"trainCleaned1_df.loc[trainCleaned1_df['VideoAmt']>=1, 'VideoAmt']='YesVideo'\ntrainCleaned1_df.loc[trainCleaned1_df['VideoAmt']==0, 'VideoAmt']='NoVideo'\n\ntestCleaned1_df.loc[testCleaned1_df['VideoAmt']>=1, 'VideoAmt']='YesVideo'\ntestCleaned1_df.loc[testCleaned1_df['VideoAmt']==0, 'VideoAmt']='NoVideo'","efd91da1":"trainCleaned1_df['Type'] = pd.Categorical(trainCleaned1_df.Type)\ntrainCleaned1_df['Gender'] = pd.Categorical(trainCleaned1_df.Gender)\ntrainCleaned1_df['Vaccinated'] = pd.Categorical(trainCleaned1_df.Vaccinated)\ntrainCleaned1_df['Dewormed'] = pd.Categorical(trainCleaned1_df.Dewormed)\ntrainCleaned1_df['Sterilized'] = pd.Categorical(trainCleaned1_df.Sterilized)\ntrainCleaned1_df['Health'] = pd.Categorical(trainCleaned1_df.Health)\ntrainCleaned1_df=pd.get_dummies(trainCleaned1_df)\n\ntestCleaned1_df['Type'] = pd.Categorical(testCleaned1_df.Type)\ntestCleaned1_df['Gender'] = pd.Categorical(testCleaned1_df.Gender)\ntestCleaned1_df['Vaccinated'] = pd.Categorical(testCleaned1_df.Vaccinated)\ntestCleaned1_df['Dewormed'] = pd.Categorical(testCleaned1_df.Dewormed)\ntestCleaned1_df['Sterilized'] = pd.Categorical(testCleaned1_df.Sterilized)\ntestCleaned1_df['Health'] = pd.Categorical(testCleaned1_df.Health)\ntestCleaned1_df=pd.get_dummies(testCleaned1_df)","55d5da84":"print(trainCleaned1_df['Age'].describe())\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['Age'])) < 3)]\nprint(trainCleaned1_df['Age'].describe())\nsns.histplot(data=trainCleaned1_df, x='Age',kde=True)","c8b151e2":"print(trainCleaned1_df['PhotoAmt'].describe())\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['PhotoAmt'])) < 3)]\nprint(trainCleaned1_df['PhotoAmt'].describe())\n\nsns.histplot(data=trainCleaned1_df, x='PhotoAmt',kde=True)","c57932b1":"print(trainCleaned1_df['Quantity'].describe())\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['Quantity'])) < 3)]\nprint(trainCleaned1_df['Quantity'].describe())\n\nsns.histplot(data=trainCleaned1_df, x='Quantity',kde=True)","fc7a849c":"print(trainCleaned1_df['DescriptonWordsCount'].describe())\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['DescriptonWordsCount'])) < 3)]\nprint(trainCleaned1_df['DescriptonWordsCount'].describe())\n\nsns.histplot(data=trainCleaned1_df, x='DescriptonWordsCount',kde=True)","56db4903":"column_names_to_normalize = ['Age','MaturitySize','FurLength','Quantity','PhotoAmt','DescriptonWordsCount']\nx = trainCleaned1_df[column_names_to_normalize].values\nscaler = MinMaxScaler()\n\nx_scaled = scaler.fit_transform(x)\ndf_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = trainCleaned1_df.index)\ntrainCleaned1_df[column_names_to_normalize] = df_temp\n\ncolumn_names_to_normalize = ['Age','MaturitySize','FurLength','Quantity','PhotoAmt','DescriptonWordsCount']\nx = testCleaned1_df[column_names_to_normalize].values\nscaler = MinMaxScaler()\n\nx_scaled = scaler.fit_transform(x)\ndf_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = testCleaned1_df.index)\ntestCleaned1_df[column_names_to_normalize] = df_temp","7e79e3a9":"trainData=trainCleaned1_df.copy()\ntrainData=trainData.drop(['AdoptionSpeed'],axis=1)\ntrainTarget=trainCleaned1_df.copy()\ntrainTarget=trainTarget['AdoptionSpeed']\n\nx_test=testCleaned1_df.copy()","f5c85fe1":"#making a Dog DS\ndogsTrainData=trainCleaned1_df[(trainCleaned1_df['Type_1']==1)]\ndogsTrainData=dogsTrainData.drop(['Type_2'],axis=1)\ndogsTrainData=dogsTrainData.drop(['Type_1'],axis=1)\ndogsTrainTarget=dogsTrainData.copy()\ndogsTrainTarget=dogsTrainTarget['AdoptionSpeed']\ndogsTrainData=dogsTrainData.drop(['AdoptionSpeed'],axis=1)\n\n#Making a Cat DS\ncatsTrainData=trainCleaned1_df[(trainCleaned1_df['Type_1']==0)]\ncatsTrainData=catsTrainData.drop(['Type_2'],axis=1)\ncatsTrainData=catsTrainData.drop(['Type_1'],axis=1)\ncatsTrainTarget=catsTrainData.copy()\ncatsTrainTarget=catsTrainTarget['AdoptionSpeed']\ncatsTrainData=catsTrainData.drop(['AdoptionSpeed'],axis=1)","74e385d5":"\ndef quadratic_kappa(actuals, preds, N=5):\n    \"\"\"This function calculates the Quadratic Kappa Metric used for Evaluation in the PetFinder competition\n    at Kaggle. It returns the Quadratic Weighted Kappa metric score between the actual and the predicted values \n    of adoption rating.\"\"\"\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)\/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E\/E.sum();\n    O = O\/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num\/den))\n","d1574282":"#combined model\ndata_dmatrix = xgb.DMatrix(data=trainData,label=trainTarget)\nX_train, X_test, y_train, y_test = train_test_split(trainData, trainTarget, test_size=0.25, random_state=123,stratify=trainTarget)\n","074085f8":"# Cats and Dogs Models\ndataDogs_dmatrix = xgb.DMatrix(data=dogsTrainData,label=dogsTrainTarget)\ndataCats_dmatrix = xgb.DMatrix(data=catsTrainData,label=catsTrainTarget)\n\nX_trainDogs, X_testDogs, y_trainDogs, y_testDogs = train_test_split(dogsTrainData, dogsTrainTarget, test_size=0.25, random_state=123)\nX_trainCats, X_testCats, y_trainCats, y_testCats = train_test_split(catsTrainData, catsTrainTarget, test_size=0.25, random_state=123)\n","d3860d04":"model = XGBClassifier( colsample_bytree = 0.3, learning_rate = 0.5,\n                max_depth = 15, alpha = 20, n_estimators = 20,use_label_encoder=False)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nprint(quadratic_kappa(y_test, predictions))\n\n\n","c4102125":"DogsModel = XGBClassifier( colsample_bytree = 0.3, learning_rate = 0.5,\n                max_depth = 15, alpha = 20, n_estimators = 20, use_label_encoder=False)\nDogsModel.fit(X_trainDogs, y_trainDogs)\ny_predDogs = DogsModel.predict(X_testDogs)\npredictionsDogs = [round(value) for value in y_predDogs]\naccuracyDogs = accuracy_score(y_testDogs, predictionsDogs)\nprint('Dogs Model Details:')\nprint(\"Accuracy: %.2f%%\" % (accuracyDogs * 100.0))\nprint(f'Quadratic_kappa {quadratic_kappa(y_testDogs, predictionsDogs)}')\n\n\nCatsModel = XGBClassifier( colsample_bytree = 0.3, learning_rate = 0.5,\n                max_depth = 15, alpha = 20, n_estimators = 20,use_label_encoder=False)\nCatsModel.fit(X_trainCats, y_trainCats)\ny_predCats = CatsModel.predict(X_testCats)\n\npredictionsCats = [round(value) for value in y_predCats]\naccuracyCats = accuracy_score(y_testCats, predictionsCats)\nprint('Cats Model Details:')\nprint(\"Accuracy: %.2f%%\" % (accuracyCats * 100.0))\nprint(f'Quadratic_kappa {quadratic_kappa(y_testCats, predictionsCats)}')","1a173f52":"print(tabulate([['CombinedModel', accuracy * 100.0,quadratic_kappa(y_test, predictions)],\n                ['CatsModel', accuracyCats * 100.0,quadratic_kappa(y_testCats, predictionsCats)],\n                ['DogsModel',accuracyDogs * 100.0,quadratic_kappa(y_testDogs, predictionsDogs)]],\n               headers=['Model Name', 'Accuracy','Quadratic Kappa']))","03414aee":"plt.figure(figsize=(38, 20));\nax2 = plt.subplot2grid((2, 2), (0, 0))\nxgb.plot_importance(DogsModel,ax=ax2)\n\nplt.title('DogsModel');\n\nax3 = plt.subplot2grid((2, 2), (0, 1))\nxgb.plot_importance(CatsModel,ax=ax3)\nplt.title('CatsModel');\n\nax4 = plt.subplot2grid((2, 2), (1, 0),colspan=2)\nplt.title('Combined');\n\nxgb.plot_importance(model,ax=ax4)","1a505e84":"import matplotlib.pyplot as plt\n\nax=xgb.plot_tree(model,num_trees=1)\nfig = ax.figure\nfig.set_size_inches(30, 30)\nplt.show()\n\n","447d3ff8":"trainCleaned1_df","26eea1e8":"train_df.head()\ntrainDropped_df=train_df.copy()\ntrainDropped_df.drop(['Breed2', 'State','Color2','Color3'], axis=1, inplace=True)\n\n\ntest_df.head()\ntestDropped_df=test_df.copy()\ntestDropped_df['Description']=testDropped_df['Description'].fillna('').values\ntestDropped_df['DescriptonWordsCount'] = testDropped_df['Description'].apply(lambda x: len(x.split()))\n\ntestDropped_df.drop(['Breed2', 'State','Color2','Color3',], axis=1, inplace=True)\n\ntrainCleaned1_df=trainDropped_df.copy()\ntestCleaned1_df=testDropped_df.copy()\n\n#name\ntrainCleaned1_df['Name'] =trainCleaned1_df['Name'].fillna('NULL')\ntrainCleaned1_df['Name'].replace({\"No Name\": \"NULL\", \"No Name Yet\": \"NULL\", \"Unknown\": \"NULL\", \"Baby\": \"NULL\",\"Lucky\": \"NULL\",\"Brownie\": \"NULL\",\"Blackie\": \"NULL\",\"Puppy\": \"NULL\",\"Kitty\": \"NULL\",\"Kittens\":\"NULL\"},inplace=True)\ntrainCleaned1_df.loc[trainCleaned1_df['Name'] !=\"NULL\", 'Name'] = 'HasName'\n\n\ntestCleaned1_df['Name'] =testCleaned1_df['Name'].fillna('NULL')\ntestCleaned1_df['Name'].replace({\"No Name\": \"NULL\", \"No Name Yet\": \"NULL\", \"Unknown\": \"NULL\", \"Baby\": \"NULL\",\"Lucky\": \"NULL\",\"Brownie\": \"NULL\",\"Blackie\": \"NULL\",\"Puppy\": \"NULL\",\"Kitty\": \"NULL\",\"Kittens\":\"NULL\"},inplace=True)\ntestCleaned1_df.loc[testCleaned1_df['Name'] !=\"NULL\", 'Name'] = 'HasName'\n#Breed\n\na=train_df.copy()\ncounts = a['Breed1'].value_counts()\ncounts[counts>30] #i have tried many numbers but it seems like 30 is the sweet spot between good accuracy and high training\\infernce time.\na=a[a['Breed1'].isin(counts[counts > 30].index)]\npopBreeds=a['Breed1'].value_counts().index\n    \n    \ntrainCleaned1_dfDogs=trainCleaned1_df.loc[trainCleaned1_df['Type']==1]\ntrainCleaned1_dfCats=trainCleaned1_df.loc[trainCleaned1_df['Type']==2] \n\ntrainCleaned1_dfDogs.loc[~(trainCleaned1_dfDogs['Breed1'].isin(popBreeds)),'Breed1'] =\"Pure Dogs\"\ntrainCleaned1_dfCats.loc[~(trainCleaned1_dfCats['Breed1'].isin(popBreeds)),'Breed1'] =\"Pure Cats\"\n\ntrainCleaned1_df=trainCleaned1_dfCats.append(trainCleaned1_dfDogs)\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==307),'Breed1']='Mixed Dog'\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==266),'Breed1']='Domestic short Hair Cat'\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==265),'Breed1']='Domestic Medium Hair Cat'\ntrainCleaned1_df.loc[(trainCleaned1_df['Breed1']==264),'Breed1']='Domestic Long Hair Cat'\n                                 #Test Breed\na=test_df.copy()\ncounts = a['Breed1'].value_counts()\ncounts[counts>30] #i have tried many numbers but it seems like 30 is the sweet spot between good accuracy and high training\\infernce time.\na=a[a['Breed1'].isin(counts[counts > 30].index)]\npopBreeds=a['Breed1'].value_counts().index\n    \n    \ntestCleaned1_dfDogs=testCleaned1_df.loc[testCleaned1_df['Type']==1]\ntestCleaned1_dfCats=testCleaned1_df.loc[testCleaned1_df['Type']==2] \n\ntestCleaned1_dfDogs.loc[~(testCleaned1_dfDogs['Breed1'].isin(popBreeds)),'Breed1'] =\"Pure Dogs\"\ntestCleaned1_dfCats.loc[~(testCleaned1_dfCats['Breed1'].isin(popBreeds)),'Breed1'] =\"Pure Cats\"\n\ntestCleaned1_df=testCleaned1_dfCats.append(testCleaned1_dfDogs)\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==307),'Breed1']='Mixed Dog'\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==266),'Breed1']='Domestic short Hair Cat'\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==265),'Breed1']='Domestic Medium Hair Cat'\ntestCleaned1_df.loc[(testCleaned1_df['Breed1']==264),'Breed1']='Domestic Long Hair Cat'  \n                                 \n#Gender\ntrainCleaned1_df.loc[(trainCleaned1_df['Gender']==1),'Gender']='Male'\ntrainCleaned1_df.loc[(trainCleaned1_df['Gender']==2),'Gender']='Female'\ntrainCleaned1_df.loc[(trainCleaned1_df['Gender']==3),'Gender']='Mixed'\n\ntestCleaned1_df.loc[(testCleaned1_df['Gender']==1),'Gender']='Male'\ntestCleaned1_df.loc[(testCleaned1_df['Gender']==2),'Gender']='Female'\ntestCleaned1_df.loc[(testCleaned1_df['Gender']==3),'Gender']='Mixed'\n\n#colors\ncolors=[\"Nan\",\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"]\n\nfor i in range(1,8):\n    trainCleaned1_df.loc[(trainCleaned1_df['Color1']==i),'Color1']=colors[i]\n    \n    \ncolors=[\"Nan\",\"Black\",\"Brown\",\"Golden\",\"Yellow\",\"Cream\",\"Gray\",\"White\"]\n\nfor i in range(1,8):\n    testCleaned1_df.loc[(testCleaned1_df['Color1']==i),'Color1']=colors[i]\n\n#fee\ntrainCleaned1_df.loc[trainCleaned1_df['Fee']>=1, 'Fee']='Paid'\ntrainCleaned1_df.loc[trainCleaned1_df['Fee']==0, 'Fee']='Free'\n\ntestCleaned1_df.loc[testCleaned1_df['Fee']>=1, 'Fee']='Paid'\ntestCleaned1_df.loc[testCleaned1_df['Fee']==0, 'Fee']='Free'\n#videoamt\ntrainCleaned1_df.loc[trainCleaned1_df['VideoAmt']>=1, 'VideoAmt']='YesVideo'\ntrainCleaned1_df.loc[trainCleaned1_df['VideoAmt']==0, 'VideoAmt']='NoVideo'\n\ntestCleaned1_df.loc[testCleaned1_df['VideoAmt']>=1, 'VideoAmt']='YesVideo'\ntestCleaned1_df.loc[testCleaned1_df['VideoAmt']==0, 'VideoAmt']='NoVideo'\n\n\n#RescuerID\n# 2 is the sweet spot i will try 50 to reduce gridsearch time\n\na=train_df.copy()\ncounts = a['RescuerID'].value_counts()\ncounts[counts>2]\na=a[a['RescuerID'].isin(counts[counts > 2].index)]\npopRescuers=a['RescuerID'].value_counts().index\n    \n    \ntrainCleaned1_df.loc[~(trainCleaned1_df['RescuerID'].isin(popRescuers)),'RescuerID'] =\"less than 2 listings\"\n\n#RESUCER FOR TEST\na=test_df.copy()\ncounts = a['RescuerID'].value_counts()\ncounts[counts>2]\na=a[a['RescuerID'].isin(counts[counts > 2].index)]\npopRescuers=a['RescuerID'].value_counts().index\n    \n    \ntestCleaned1_df.loc[~(testCleaned1_df['RescuerID'].isin(popRescuers)),'RescuerID'] =\"less than 2 listings\"\n\n# Clean Outliers\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['Age'])) < 3)]\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['PhotoAmt'])) < 3)]\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['Quantity'])) < 3)]\ntrainCleaned1_df=trainCleaned1_df[(np.abs(stats.zscore(trainCleaned1_df['DescriptonWordsCount'])) < 3)]\n\n\n#drop Lemmatized from main XGBBoost models while keeping it for RNN Text classification.\ntrain_RNN=trainCleaned1_df.copy() # to be continued , implement this to the RNN !!!!!!!!!!!!!!!!!!!!!!!!!!\ntrainCleaned1_df.drop(['lemmatized'], axis=1, inplace=True)\n\n\n\n#dummies\ntrainCleaned1_df['Type'] = pd.Categorical(trainCleaned1_df.Type)\ntrainCleaned1_df['Gender'] = pd.Categorical(trainCleaned1_df.Gender)\ntrainCleaned1_df['Vaccinated'] = pd.Categorical(trainCleaned1_df.Vaccinated)\ntrainCleaned1_df['Dewormed'] = pd.Categorical(trainCleaned1_df.Dewormed)\ntrainCleaned1_df['Sterilized'] = pd.Categorical(trainCleaned1_df.Sterilized)\ntrainCleaned1_df['Health'] = pd.Categorical(trainCleaned1_df.Health)\ntrainCleaned1_df['RescuerID'] = pd.Categorical(trainCleaned1_df.RescuerID)\n\n###################################\ntrainCleaned1_df['sentiment_TEXT'] = pd.Categorical(trainCleaned1_df.sentiment_TEXT)\n###########################\n\n\n#trainCleaned1_df=pd.get_dummies(trainCleaned1_df,columns=['Type','Gender','Vaccinated','Dewormed','Sterilized','Health','RescuerID','sentiment_TEXT'])\ntrainCleaned1_df=pd.get_dummies(trainCleaned1_df,columns=['Type','Gender','Vaccinated','Dewormed','Sterilized','Health','RescuerID','sentiment_TEXT','Name', 'Breed1', 'Color1', 'Fee', 'VideoAmt'])\n##################################\n#test adding sentiment_text\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))\n\nprint('num of train sentiment files: {}'.format(len(test_sentiment_files)))\n\n\ndata=[]\n\nfor i in range (len(test_sentiment_files)):\n    with open(test_sentiment_files[i]) as jsonFile:\n        jsonObject = json.load(jsonFile)\n        jsonFile.close()\n    score=jsonObject['documentSentiment']['score']\n    magnitude=jsonObject['documentSentiment']['magnitude']\n    path=test_sentiment_files[i]\n    firstpos=path.rfind(\"\/\")\n    lastpos=path.rfind(\".\")\n    PetID=path[firstpos+1:lastpos]\n    if score >=0.1:\n        if magnitude < 2:\n            data.append([PetID,'Weak Postive'])\n        elif magnitude < 6 and magnitude>=2:\n            data.append([PetID,'Medium Postive'])\n        elif magnitude >=6:\n            data.append([PetID,'Strong Postive'])\n            \n    elif score <=-0.1 :\n        if magnitude < 2:\n            data.append([PetID,'Weak Negative'])\n        elif magnitude < 6 and magnitude>=2:\n            data.append([PetID,'Medium Negative'])\n        elif magnitude >=6:\n            data.append([PetID,'Strong Negative'])\n            \n    else:\n        data.append([PetID,'Netural'])\n\n\ndata = pd.DataFrame(data, columns=['PetID','sentiment_TEXT'])\ntestCleaned1_df=pd.merge(testCleaned1_df, data, on='PetID',how='outer')\n\n\n##################################\n\n\n#testdummies\ntestCleaned1_df['sentiment_TEXT'] = pd.Categorical(testCleaned1_df.sentiment_TEXT)\ntestCleaned1_df['Type'] = pd.Categorical(testCleaned1_df.Type)\ntestCleaned1_df['Gender'] = pd.Categorical(testCleaned1_df.Gender)\ntestCleaned1_df['Vaccinated'] = pd.Categorical(testCleaned1_df.Vaccinated)\ntestCleaned1_df['Dewormed'] = pd.Categorical(testCleaned1_df.Dewormed)\ntestCleaned1_df['Sterilized'] = pd.Categorical(testCleaned1_df.Sterilized)\ntestCleaned1_df['Health'] = pd.Categorical(testCleaned1_df.Health)\ntestCleaned1_df['RescuerID'] = pd.Categorical(testCleaned1_df.RescuerID)\n\ntestCleaned1_df=pd.get_dummies(testCleaned1_df,columns=['Type','Gender','Vaccinated','Dewormed','Sterilized','Health','RescuerID','sentiment_TEXT','Name', 'Breed1', 'Color1', 'Fee', 'VideoAmt'])\n\n\n#Normalize\ncolumn_names_to_normalize = ['Age','MaturitySize','FurLength','Quantity','PhotoAmt','DescriptonWordsCount']\nx = trainCleaned1_df[column_names_to_normalize].values\nscaler = MinMaxScaler()\n\nx_scaled = scaler.fit_transform(x)\ndf_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = trainCleaned1_df.index)\ntrainCleaned1_df[column_names_to_normalize] = df_temp\n\n\ncolumn_names_to_normalize = ['Age','MaturitySize','FurLength','Quantity','PhotoAmt','DescriptonWordsCount']\nx = testCleaned1_df[column_names_to_normalize].values\nscaler = MinMaxScaler()\n\nx_scaled = scaler.fit_transform(x)\ndf_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = testCleaned1_df.index)\ntestCleaned1_df[column_names_to_normalize] = df_temp\n\n\n# DS train\ntrainData=trainCleaned1_df.copy()\ntrainData=trainData.drop(['AdoptionSpeed'],axis=1)\ntrainTarget=trainCleaned1_df.copy()\ntrainTarget=trainTarget['AdoptionSpeed']\n\n# DS test\ntestData=testCleaned1_df.copy()\n\n#################\n\n#dropping petID after copying it for ensemble_df\nensemble_df=trainCleaned1_df.copy()\ntrainData.drop(['PetID','Description'], axis=1, inplace=True)\ntrainCleaned1_df.drop(['PetID','Description'], axis=1, inplace=True)\n\n#ensemble test\nTestEnsemble_df=testCleaned1_df.copy()\ntestData.drop(['PetID','Description'], axis=1, inplace=True)\ntestCleaned1_df.drop(['PetID','Description'], axis=1, inplace=True)\n","6c570d9f":"#Model\n#making a Dog DS\ndogsTrainData=trainCleaned1_df[(trainCleaned1_df['Type_1']==1)]\ndogsTrainData=dogsTrainData.drop(['Type_2'],axis=1)\ndogsTrainData=dogsTrainData.drop(['Type_1'],axis=1)\ndogsTrainTarget=dogsTrainData.copy()\ndogsTrainTarget=dogsTrainTarget['AdoptionSpeed']\ndogsTrainData=dogsTrainData.drop(['AdoptionSpeed'],axis=1)\n\n#Making a Cat DS\ncatsTrainData=trainCleaned1_df[(trainCleaned1_df['Type_1']==0)]\ncatsTrainData=catsTrainData.drop(['Type_2'],axis=1)\ncatsTrainData=catsTrainData.drop(['Type_1'],axis=1)\ncatsTrainTarget=catsTrainData.copy()\ncatsTrainTarget=catsTrainTarget['AdoptionSpeed']\ncatsTrainData=catsTrainData.drop(['AdoptionSpeed'],axis=1)\n#combined model\ndata_dmatrix = xgb.DMatrix(data=trainData,label=trainTarget)\nX_train, X_test, y_train, y_test = train_test_split(trainData, trainTarget, test_size=0.25, random_state=123,stratify=trainTarget)\n# Cats and Dogs Models\ndataDogs_dmatrix = xgb.DMatrix(data=dogsTrainData,label=dogsTrainTarget)\ndataCats_dmatrix = xgb.DMatrix(data=catsTrainData,label=catsTrainTarget)\n\nX_trainDogs, X_testDogs, y_trainDogs, y_testDogs = train_test_split(dogsTrainData, dogsTrainTarget, test_size=0.25, random_state=123,stratify=dogsTrainTarget)\nX_trainCats, X_testCats, y_trainCats, y_testCats = train_test_split(catsTrainData, catsTrainTarget, test_size=0.25, random_state=123,stratify =catsTrainTarget)\n\nmodel = XGBClassifier( colsample_bytree = 0.3, learning_rate = 0.5,\n                max_depth = 10, alpha = 20, n_estimators = 20,use_label_encoder =False)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nprint(quadratic_kappa(y_test, predictions))\nDogsModel = XGBClassifier( colsample_bytree = 0.3, learning_rate = 0.5,\n                max_depth = 10, alpha = 20, n_estimators = 20,use_label_encoder =False)\nDogsModel.fit(X_trainDogs, y_trainDogs)\ny_predDogs = DogsModel.predict(X_testDogs)\npredictionsDogs = [round(value) for value in y_predDogs]\naccuracyDogs = accuracy_score(y_testDogs, predictionsDogs)\nprint('Dogs Model Details:')\nprint(\"Accuracy: %.2f%%\" % (accuracyDogs * 100.0))\nprint(f'Quadratic_kappa {quadratic_kappa(y_testDogs, predictionsDogs)}')\n\n\nCatsModel = XGBClassifier( colsample_bytree = 0.3, learning_rate = 0.5,\n                max_depth = 10, alpha = 20, n_estimators = 50,use_label_encoder =False)\nCatsModel.fit(X_trainCats, y_trainCats)\ny_predCats = CatsModel.predict(X_testCats)\npredictionsCats = [round(value) for value in y_predCats]\naccuracyCats = accuracy_score(y_testCats, predictionsCats)\nprint('Cats Model Details:')\nprint(\"Accuracy: %.2f%%\" % (accuracyCats * 100.0))\nprint(f'Quadratic_kappa {quadratic_kappa(y_testCats, predictionsCats)}')\nfrom tabulate import tabulate\nprint(tabulate([['CombinedModel', accuracy * 100.0,quadratic_kappa(y_test, predictions)],\n                ['CatsModel', accuracyCats * 100.0,quadratic_kappa(y_testCats, predictionsCats)],\n                ['DogsModel',accuracyDogs * 100.0,quadratic_kappa(y_testDogs, predictionsDogs)]],\n               headers=['Model Name', 'Accuracy','Quadratic Kappa']))\n","867fa36e":"params={\n    \"learning_rate\": [0.1],\n    \"colsample_bytree\" : [0.5],\n        'gamma': [0.35],\n        'subsample': [0.9],\n        'colsample_bytree': [0.9],\n        \"min_child_weight\" : [ 1 ],\n        'max_depth': [8,10,12],\n        'n_estimators' : [175],\n        'alpha': [0,0.2,0.5],\n        'objective': 'multi:softprob'\n    \n        }","faaf1627":"from datetime import datetime\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n","5539f516":"\"\"\"\nclassifier=XGBClassifier()\nrandom_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=9,cv=5,verbose=3)\n\nfrom datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X_train, y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable\nprint(random_search.best_estimator_)\nprint(random_search.best_params_)\n\n\"\"\"","0479d95d":"accuracysum=0\nquadratic_kappasum=0\nNumRandomStates=1\nfor i in range(NumRandomStates): #100\n    print(i)\n\n    data_dmatrix = xgb.DMatrix(data=trainData,label=trainTarget)\n    X_train, X_test, y_train, y_test = train_test_split(trainData, trainTarget, test_size=0.20, random_state=1000+1000*i,stratify=trainTarget)\n\n    XGBmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n          colsample_bynode=1, colsample_bytree=0.9, gamma=0.35, gpu_id=-1,\n          importance_type='gain', interaction_constraints='',\n          learning_rate=0.1, max_delta_step=0, max_depth=20,\n          min_child_weight=1, monotone_constraints='()',\n          n_estimators=175, n_jobs=-1, num_parallel_tree=1,\n          objective='multi:softprob', random_state=0, reg_alpha=0,\n          reg_lambda=1, scale_pos_weight=None, subsample=0.9,\n          tree_method='exact', validate_parameters=1,alpha=0.25, verbosity=None,use_label_encoder =False)\n              \n    \n    XGBmodel.fit(X_train, y_train)\n    y_pred = XGBmodel.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    accuracysum+=accuracy *100.0\n    quadratic_kappasum+=quadratic_kappa(y_test, predictions)\n    \n    print(accuracy * 100.0,quadratic_kappa(y_test, predictions))\n    \n\nprint(f'Accuracy Sum over {NumRandomStates} RandomStates is:{accuracysum\/NumRandomStates}')\nprint(f'Quadratic Kappa Sum over {NumRandomStates} RandomStates is:{quadratic_kappasum\/NumRandomStates}')","058ecbd8":"ax=xgb.plot_importance(XGBmodel,max_num_features=40)\nfig = ax.figure\nfig.set_size_inches(20, 20)","03fc3fef":"\"\"\"fig, ax = plt.subplots(figsize=(200, 200))\nxgb.plot_tree(XGBmodel, ax=ax,font_size=20)\nplt.savefig(\"XGBoostTree.pdf\")\nplt.savefig(\"XGBoostTree.png\")\nplt.show()\n\"\"\"","76d79dab":"\"\"\"search_grid={'n_estimators':[200,300,400,500,600],\n             'learning_rate':[0.05, 0.1,0.01,0.2,0.3,0.4,0.5]}\nsearch=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=search_grid,scoring='accuracy',n_jobs=1,cv=4,verbose=3)\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nsearch.fit(X_train, y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable\nprint(search.best_estimator_)\nprint(search.best_params_)\n\"\"\"","071af050":"accuracysum=0\nquadratic_kappasum=0\nNumRandomStates=1\nfor i in range(NumRandomStates): #100\n    X_train, X_test, y_train, y_test = train_test_split(trainData, trainTarget, test_size=0.20, random_state=1000*i,stratify=trainTarget)\n\n    ADAmodel = AdaBoostClassifier( \n              n_estimators=5000,random_state=1000*i,learning_rate =0.5)\n\n    ADAmodel.fit(X_train, y_train)\n    y_pred = ADAmodel.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    accuracysum+=accuracy *100.0\n    quadratic_kappasum+=quadratic_kappa(y_test, predictions)\n    \n    print(accuracy * 100.0,quadratic_kappa(y_test, predictions))\n    \nprint(f'Accuracy Sum over {NumRandomStates} RandomStates is:{accuracysum\/NumRandomStates}')\nprint(f'Quadratic Kappa Sum over {NumRandomStates} RandomStates is:{quadratic_kappasum\/NumRandomStates}')","c11710cf":"predictions=pd.DataFrame(predictions)\npredictions.value_counts()","117ab2b3":"def load_image(path, pet_id): # Load the first image of a pet\n    image = cv2.imread(f'{path}\/{pet_id}-1.jpg')\n    return image\n\ndef load_images(path, pet_id): # Load all images of a pet to a list, return the list and the length of the list\n    pictures=glob.glob(f'{path}\/{pet_id}-*.jpg')\n    #print(files)\n    return pictures,len(pictures)\n\ndef displayImages(imageList): # Gets an ImageList and display all of them maximum 30\n    plt.figure(figsize=(20, 10))\n    for _,i in enumerate(imageList):\n        ax = plt.subplot(6, 5, _ + 1) \n        currentImage=cv2.imread(i)\n        plt.imshow(currentImage)\n        plt.axis(\"off\")\n    plt.show()","d6201bbe":"petID=train_df['PetID'].iloc[8]\nimageList,imageCount = load_images(trainImagesPath, petID)\ndisplayImages(imageList)","e3c0e54a":"trainCNN_df=train_df.copy()\ntrainCNN_df=trainCNN_df[trainCNN_df.PhotoAmt!=0]\n\nimage_df=[]\nfor index, row in trainCNN_df.iterrows(): \n    imageList,imageCount = load_images(trainImagesPath, row['PetID'])\n    if imageCount==0:\n        continue\n    speed=row['AdoptionSpeed']\n    for image in imageList:\n        image_df.append([image,speed])\n                                                             \nprint(f' we found: {len(image_df)} amount of pictures in the database')\nimage_df = pd.DataFrame(image_df, columns=['ImageURL','Speed'])\n\nimage_df['Speed']=image_df['Speed'].astype(str)\n","a79bc1c4":"image_df","ec9f8a83":"               \nimage_df['Speed']=image_df['Speed'].astype(str)\nimage_df0=image_df.loc[image_df['Speed']=='0']\nimage_df1=image_df.loc[image_df['Speed']=='1']\nimage_df2=image_df.loc[image_df['Speed']=='2']\nimage_df3=image_df.loc[image_df['Speed']=='3']\nimage_df4=image_df.loc[image_df['Speed']=='4']\n\nimage_df1=image_df1.sample(n=len(image_df0)+1000)\nimage_df2=image_df2.sample(n=len(image_df0)+1000)\nimage_df3=image_df3.sample(n=len(image_df0)+1000)\nimage_df4=image_df4.sample(n=len(image_df0)+1000)\n\nimage_dfBalanced=image_df0.append([image_df1,image_df2,image_df3,image_df4])\nimage_dfBalanced['Speed'].value_counts()","33a88173":"pics=image_dfBalanced['ImageURL']\nlabel=image_dfBalanced['Speed']\nval_split = 0.20\nX_train, X_val, y_train, y_val = train_test_split(pics, label, test_size=val_split,stratify=label)\n\ntrain_balancedCNN = np.concatenate((X_train, y_train))\nval_balancedCNN = np.concatenate((X_val, y_val))\n\ntrain_balancedCNN=pd.DataFrame(list(zip(X_train, y_train)),\n              columns=['ImageURL','Speed'])\n\nval_balancedCNN=pd.DataFrame(list(zip(X_val,y_val)),\n              columns=['ImageURL','Speed'])\n\nprint(f' the length of train set is: {len(train_balancedCNN)},the length of validation set is: {len(val_balancedCNN)}')","645aecb2":"#VGG16 Model for training\n\ndatagen=ImageDataGenerator(rescale=1.\/255,\n    rotation_range=40,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    fill_mode=\"nearest\",\n    horizontal_flip=True,\n    validation_split=0.2) \ndatagen2=ImageDataGenerator(validation_split=0.99,rescale=1.0\/255.0) \n\ntrain_generator=datagen.flow_from_dataframe(dataframe=train_balancedCNN, x_col=\"ImageURL\", y_col=\"Speed\", class_mode=\"categorical\", target_size=(112,112), batch_size=32,subset=\"training\",shuffle=True,color_mode=\"rgb\")\nvalid_generator=datagen2.flow_from_dataframe(dataframe=val_balancedCNN, x_col=\"ImageURL\", y_col=\"Speed\", class_mode=\"categorical\", target_size=(112,112), batch_size=32,subset=\"validation\",shuffle=True,color_mode=\"rgb\")\n\n## Loading VGG16 model\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(112,112,3))\n#base_model.trainable = False ## Not trainable weights\n\nmodel = models.Sequential([\n    base_model,\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(5, activation='softmax')\n])\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=0.1,\n        decay_steps=10000,\n        decay_rate=0.90)\noptimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer='SGD',\n        metrics=['accuracy']\n    )\n\nfilepath = \"VGG16-{epoch:02d}-{val_accuracy:.2f}.hdf5\"  # the new name for the model\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,\n                                                    mode='max')\ncallbacks_list = [checkpoint]\n\nprint(model.summary())\nepochs=80\nhistory=model.fit(train_generator\n                    ,validation_data=valid_generator, epochs=epochs, batch_size=32, callbacks=callbacks_list)","e7633985":"image = cv2.imread('..\/input\/cnn-models-petfinder\/VGG029.4.png')\nplt.figure(figsize = (10,10))\nplt.imshow(image)\n\n","ad1f9305":"\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(epochs)\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n","6da7a8b4":"bert_preprocess = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nbert_encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1\")","bc7f23d8":"text_train_df=train_df.copy()\n                     \ntext_df0=text_train_df.loc[text_train_df['AdoptionSpeed']==0]\ntext_df1=text_train_df.loc[text_train_df['AdoptionSpeed']==1]\ntext_df2=text_train_df.loc[text_train_df['AdoptionSpeed']==2]\ntext_df3=text_train_df.loc[text_train_df['AdoptionSpeed']==3]\ntext_df4=text_train_df.loc[text_train_df['AdoptionSpeed']==4]\n\nprint(len(text_df0))\nprint(len(text_df1))\nprint(len(text_df2))\nprint(len(text_df3))\nprint(len(text_df4))\ntext_df1=text_df1.sample(n=len(text_df0)+200)\ntext_df2=text_df2.sample(n=len(text_df0)+300)\ntext_df3=text_df3.sample(n=len(text_df0)+200)\ntext_df4=text_df4.sample(n=len(text_df0)+300)\n\ntext_dfBalanced=text_df0.append([text_df1,text_df2,text_df3,text_df4])\n\ntext_train_df=text_dfBalanced[['Description','AdoptionSpeed']]\ntext_train_df\n\nX_train, X_test, y_train, y_test = train_test_split(text_train_df['Description'],text_train_df['AdoptionSpeed'],train_size=0.8,stratify=text_train_df['AdoptionSpeed'])\ntext_train_df","15f088ae":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.2, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(256, activation='relu', name=\"DENSE\")(l)\nl = tf.keras.layers.Dense(5, activation='softmax', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])\nmodel.summary()","e296c66c":"filepath = \"BERTsomebalance200-{epoch:02d}-{val_accuracy:.2f}.hdf5\"  # the new name for the model\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,\n                                                    mode='max')\ncallbacks_list = [checkpoint]\n\nepochs=50\nmodel.compile(optimizer='Adam',\n              loss='SparseCategoricalCrossentropy',\n              metrics=['accuracy'])\nhistory=model.fit(X_train, y_train, epochs=epochs ,validation_data=((X_test,y_test)),\n    batch_size=32,callbacks=callbacks_list)","1af54352":"image = cv2.imread('..\/input\/cnn-models-petfinder\/Bert.png')\nplt.figure(figsize = (10,10))\nplt.imshow(image)","0bd3b026":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(epochs)\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","7335ea98":"\"\"\"\ndef get_sequences(texts, tokenizer, train=True, max_seq_length=None):\n    sequences = tokenizer.texts_to_sequences(texts)\n    \n    if train == True:\n        max_seq_length = np.max(list(map(len, sequences)))\n    \n    sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n    \n    return sequences\ndef preprocess_inputs(df):\n    df = df.copy()\n    \n    # Limit data to only the review and rating column\n    y = df['AdoptionSpeed']\n    X = df['lemmatized'] #changed from Description to lemmatized\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=1)\n    \n    # Fit tokenizer\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(X_train)\n    print(\"Vocab length:\", len(tokenizer.word_index) + 1)\n    \n    # Convert texts to sequences\n    X_train = get_sequences(X_train, tokenizer, train=True)\n    X_test = get_sequences(X_test, tokenizer, train=False, max_seq_length=X_train.shape[1])\n    \n    return X_train, X_test, y_train, y_test, tokenizer\n    \"\"\"","578371f6":"\"\"\"text_df0=train_RNN.loc[train_RNN['AdoptionSpeed']==0]\ntext_df1=train_RNN.loc[train_RNN['AdoptionSpeed']==1]\ntext_df2=train_RNN.loc[train_RNN['AdoptionSpeed']==2]\ntext_df3=train_RNN.loc[train_RNN['AdoptionSpeed']==3]\ntext_df4=train_RNN.loc[train_RNN['AdoptionSpeed']==4]\n\nprint(len(text_df0))\nprint(len(text_df1))\nprint(len(text_df2))\nprint(len(text_df3))\nprint(len(text_df4))\ntext_df1=text_df1.sample(n=len(text_df0)+500)\ntext_df2=text_df2.sample(n=len(text_df0)+500)\ntext_df3=text_df3.sample(n=len(text_df0)+500)\ntext_df4=text_df4.sample(n=len(text_df0)+500)\n\ntext_dfBalanced=text_df0.append([text_df1,text_df2,text_df3,text_df4])\n                                \n\nX_train, X_test, y_train, y_test, t = preprocess_inputs(text_dfBalanced)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n\ninputs = tf.keras.Input(shape=(264,))\nx = tf.keras.layers.Embedding(input_dim=19606,output_dim=64)(inputs)\nx= tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(x)\n\nx= tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True))(x)\n\nx= tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16))(x)\n\nx = tf.keras.layers.Dense(256, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.1, name=\"dropout1\")(x)\n\n\noutputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=0.1,\n        decay_steps=10000,\n        decay_rate=0.90)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\noptimizer='adam'\n    \nmodel.compile(\n    loss='SparseCategoricalCrossentropy', optimizer=optimizer, metrics=['accuracy'])\n   \nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_data=((X_test,y_test)),\n    batch_size=16,\n    epochs=50,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=25,\n            restore_best_weights=True\n        )\n    ]\n)\n\"\"\"","d01e53c6":"def find_percentage_agreement(s1, s2): # Small function to see the differnce between model predictions\n    assert len(s1)==len(s2), \"Lists must have the same shape\"\n    nb_agreements = 0  # initialize counter to 0\n    for idx, value in enumerate(s1):\n        if s2[idx] == value:\n            nb_agreements += 1\n\n    percentage_agreement = nb_agreements\/len(s1)\n\n    return percentage_agreement\n","053f82d3":"XGB_ADA_df=ensemble_df.copy()\nMain_df=ensemble_df.copy()\nMain_df.drop(Main_df.columns.difference(['PetID','AdoptionSpeed']), 1, inplace=True)\n#XGB_ADA_df=XGB_ADA_df.drop(['AdoptionSpeed'],axis=1)\nXGB_ADA_df=XGB_ADA_df.drop(['PetID','Description','AdoptionSpeed'],axis=1)\n#all(XGB_ADA_df==trainData)\nXGB_pred = XGBmodel.predict(XGB_ADA_df)\nADA_pred = ADAmodel.predict(XGB_ADA_df)\n\nMain_df['XGBSpeedPredict'] = XGB_pred\nMain_df['ADASpeedPredict'] = ADA_pred","ebbab39f":"Main_df","4f327022":"bert_preprocess = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nbert_encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1\")\n\n# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.2, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(256, activation='relu', name=\"DENSE\")(l)\n\nl = tf.keras.layers.Dense(5, activation='softmax', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nBERTmodel = tf.keras.Model(inputs=[text_input], outputs = [l])\n\nBERTmodel.load_weights('..\/input\/cnn-models-petfinder\/BERT-0.31.hdf5')","8496209d":"Bert_df=ensemble_df.copy()\n\nBert_pred = BERTmodel.predict(Bert_df['Description'])\n\nBert_pred = Bert_pred.argmax(axis=-1)  # declaring the winner of the elections\nMain_df['BertPrediction'] = Bert_pred\n\n","9f022492":"Main_df","50ea0ba2":"VGGmodel1 = tf.keras.models.load_model('..\/input\/cnn-models-petfinder\/NewModelVGG (1).hdf5VGG 0.297.hdf5') \nVGGmodel2Stra = tf.keras.models.load_model('..\/input\/cnn-models-petfinder\/VGG STRA 0.29Imagnet.hdf5') \n\nFullPredictions=[]\nj=0\n\nclass_names = [\"0\", '1', '2','3','4']\nfor index, row in ensemble_df.iterrows():\n    if(j%1000==0):\n        print(j)\n    j+=1\n    imageList,imageCount = load_images(trainImagesPath, row['PetID'])\n    \n    if imageCount==0:\n        continue\n    SpeedProb1=[0,0,0,0,0]\n    SpeedProb2=[0,0,0,0,0]\n    for image in imageList:\n                    currentImage=cv2.imread(image)\n                    resized_image = cv2.resize(currentImage, (112, 112)) \n                    img_array = tf.keras.preprocessing.image.img_to_array(resized_image)\n                    img_array = tf.expand_dims(img_array, 0)\n                    SpeedProb1 += VGGmodel1.predict(img_array)[0]  \n                    SpeedProb2 += VGGmodel2Stra.predict(img_array)[0]  \n    predictedSpeedVGG1 = class_names[np.argmax(SpeedProb1)]  # declaring the winner of the elections\n    predictedSpeedVGG2 = class_names[np.argmax(SpeedProb2)]  # declaring the winner of the elections\n\n    ##### add predictions from all models\n    FullPredictions.append([row['PetID'],predictedSpeedVGG1,predictedSpeedVGG2])\n\n       \nFullPredictions= pd.DataFrame(FullPredictions, columns=['PetID','AdoptionPredictedVGG1','AdoptionPredictedVGG2'])\n\nMain_df=Main_df.merge(FullPredictions, on='PetID', how='left')\n","034b77ed":"Main_df","9214818e":"\nMain_df['AdoptionPredictedVGG1'] = Main_df['AdoptionPredictedVGG1'].replace(np.nan, 4)\nMain_df['AdoptionPredictedVGG2'] = Main_df['AdoptionPredictedVGG2'].replace(np.nan, 4)\n\nMain_df[['AdoptionPredictedVGG1', 'AdoptionPredictedVGG2']] = Main_df[['AdoptionPredictedVGG1', 'AdoptionPredictedVGG2']].astype('int64')\n\nMain_df.to_csv('Main_train_df.csv',index=False)\nMain_df\n","a5a41f9a":"Main_df = pd.read_csv('..\/input\/cnn-models-petfinder\/Main_train_dfNIGHTT.csv')\nMain_data=Main_df.copy()\nMain_data.drop(['AdoptionSpeed'],axis=1,inplace=True)\n\nbackup=ensemble_df.copy()\nMain_data=ensemble_df.merge(Main_data, on='PetID')\nMain_data","de10510d":"Main_label=Main_df.copy()\nMain_label.drop(['PetID','XGBSpeedPredict','ADASpeedPredict','BertPrediction','AdoptionPredictedVGG1','AdoptionPredictedVGG2'], axis=1, inplace=True)\nMain_data.drop(['XGBSpeedPredict','ADASpeedPredict'], axis=1, inplace=True)\n\n#Main_data['XGBSpeedPredict'] = pd.Categorical(Main_data.XGBSpeedPredict)\n#Main_data['ADASpeedPredict'] = pd.Categorical(Main_data.ADASpeedPredict)\nMain_data['BertPrediction'] = pd.Categorical(Main_data.BertPrediction)\nMain_data['AdoptionPredictedVGG1'] = pd.Categorical(Main_data.AdoptionPredictedVGG1)\nMain_data['AdoptionPredictedVGG2'] = pd.Categorical(Main_data.AdoptionPredictedVGG2)\nMain_data.drop(['PetID','Description','AdoptionSpeed'], axis=1, inplace=True)\n\nMain_data=pd.get_dummies(Main_data,columns=['AdoptionPredictedVGG2','AdoptionPredictedVGG1','BertPrediction'])\nMain_data\n","4f7b1320":"data_dmatrix = xgb.DMatrix(data=Main_data,label=Main_label)\nX_train, X_test, y_train, y_test = train_test_split(Main_data, Main_label, test_size=0.30, random_state=99,stratify =Main_label)\n","d0ee7764":"params={\n    \"learning_rate\": [0.1,0.15,0.2,],\n    \"colsample_bytree\" : [0.5,0.8],\n        'gamma': [0.35,0.4],\n        'subsample': [0.9],\n        'colsample_bytree': [0.9],\n        \"min_child_weight\" : [ 1 ],\n        'max_depth': [3,4,5],\n        'n_estimators' : [40,30,50],\n        'alpha': [0.5],\n        'objective': 'multi:softprob'\n    \n        }","9c85d308":"\"\"\"classifier=XGBClassifier(use_label_encoder=False)\nrandom_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=108,cv=5,verbose=3)\n\nfrom datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X_train, y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable\nprint(random_search.best_estimator_)\nprint(random_search.best_params_)\"\"\"","b3c77854":"accuracysum=0\nquadratic_kappasum=0\nMainXGBmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n          colsample_bynode=1, colsample_bytree=0.8, gamma=0.35, gpu_id=-1,\n          importance_type='gain', interaction_constraints='',\n          learning_rate=0.1, max_delta_step=0, max_depth=20,\n          min_child_weight=1, monotone_constraints='()',\n          n_estimators=175, n_jobs=-1, num_parallel_tree=1,\n          objective='multi:softprob', random_state=0, reg_alpha=0,\n          reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n          tree_method='exact', validate_parameters=1,alpha=0.25, verbosity=None,use_label_encoder =False)\n              \n    \nMainXGBmodel.fit(X_train, y_train)\ny_pred = MainXGBmodel.predict(X_test)\npredictions = [round(value) for value in y_pred]\n#print(predictions)\nprint(len(predictions))\n#print(list(y_test['AdoptionSpeed']))\naccuracy = accuracy_score(y_test, predictions)\nprint(accuracy)\naccuracysum+=accuracy *100.0\nquadratic_kappasum+=quadratic_kappa(list(y_test['AdoptionSpeed']), predictions)\n    \nprint(accuracy * 100.0,quadratic_kappa(list(y_test['AdoptionSpeed']), predictions))","b889dfc8":"ax=xgb.plot_importance(MainXGBmodel,max_num_features=40)\nfig = ax.figure\nfig.set_size_inches(20, 20)\n","43a9d537":"accuracysum=0\nquadratic_kappasum=0\nNumRandomStates=1\nfor i in range(NumRandomStates): #100\n        \n    X_train, X_test, y_train, y_test = train_test_split(Main_data, Main_label, test_size=0.25, random_state=1000*i,stratify =Main_label)\n    Main_ADAmodel = AdaBoostClassifier( \n              n_estimators=1000,random_state=1000*i,learning_rate =0.1)\n\n    Main_ADAmodel.fit(X_train, y_train)\n    y_pred = Main_ADAmodel.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    accuracysum+=accuracy *100.0\n    quadratic_kappasum+=quadratic_kappa(list(y_test['AdoptionSpeed']), predictions)\n    \n    print(accuracy * 100.0,quadratic_kappa(list(y_test['AdoptionSpeed']), predictions))    \n\nprint(f'Accuracy Sum over {NumRandomStates} RandomStates is:{accuracysum\/NumRandomStates}')\nprint(f'Quadratic Kappa Sum over {NumRandomStates} RandomStates is:{quadratic_kappasum\/NumRandomStates}')\n\n    \n  ","4c24c671":"Test_XGB_ADA_df=TestEnsemble_df.copy()\nTest_Main_df=TestEnsemble_df.copy()\nTest_XGB_ADA_df=Test_XGB_ADA_df.drop(['AdoptionSpeed','PetID','Description'],axis=1)\n\nTest_Main_df.drop(Test_Main_df.columns.difference(['PetID']), 1, inplace=True)\nTest_XGB_pred = XGBmodel.predict(Test_XGB_ADA_df)\nTest_ADA_pred = ADAmodel.predict(Test_XGB_ADA_df)\n\nTest_Main_df['XGBSpeedPredict'] = Test_XGB_pred\nTest_Main_df['ADASpeedPredict'] = Test_ADA_pred\nbert_preprocess = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nbert_encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1\")\n\n# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.2, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(256, activation='relu', name=\"DENSE\")(l)\n\nl = tf.keras.layers.Dense(5, activation='softmax', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nBERTmodel = tf.keras.Model(inputs=[text_input], outputs = [l])\n\nBERTmodel.load_weights('..\/input\/cnn-models-petfinder\/BERT-0.31.hdf5')\nTest_Bert_df=TestEnsemble_df.copy()\n\nTest_Bert_pred = BERTmodel.predict(Test_Bert_df['Description'])\n\nTest_Bert_pred = Test_Bert_pred.argmax(axis=-1)  # declaring the winner of the elections\nTest_Main_df['BertPrediction'] = Test_Bert_pred\nVGGmodel1 = tf.keras.models.load_model('..\/input\/cnn-models-petfinder\/NewModelVGG (1).hdf5VGG 0.297.hdf5') \nVGGmodel2Stra = tf.keras.models.load_model('..\/input\/cnn-models-petfinder\/VGG STRA 0.29Imagnet.hdf5') \n\n#CNNmodel = tf.keras.models.load_model('..\/input\/cnn-models-petfinder\/VGG02.68.hdf5') # need to check tomorrow didnt see it yet \nFullPredictions=[]\nj=0\n\nclass_names = [\"0\", '1', '2','3','4']\nfor index, row in TestEnsemble_df.iterrows():\n    if(j%1000==0):\n        print(j)\n    j+=1\n    imageList,imageCount = load_images(testImagesPath, row['PetID'])\n    \n    \n    if imageCount==0:\n        continue\n    SpeedProb1=[0,0,0,0,0]\n    SpeedProb2=[0,0,0,0,0]\n    for image in imageList:\n                    currentImage=cv2.imread(image)\n                    resized_image = cv2.resize(currentImage, (112, 112)) \n                    #gray = cv2.cvtColor(resized_image, cv2.COLOR_RGB2GRAY)\n                    img_array = tf.keras.preprocessing.image.img_to_array(resized_image)\n                    img_array = tf.expand_dims(img_array, 0)\n                    SpeedProb1 += VGGmodel1.predict(img_array)[0]  # each char cast a vote\n                    SpeedProb2 += VGGmodel2Stra.predict(img_array)[0]  # each char cast a vote\n\n    \n    #print(SpeedProb, VGGmodel.predict_classes(img_array))   \n    predictedSpeedVGG1 = class_names[np.argmax(SpeedProb1)]  # declaring the winner of the elections\n    predictedSpeedVGG2 = class_names[np.argmax(SpeedProb2)]  # declaring the winner of the elections\n\n    #print(predictedSpeedVGG1,predictedSpeedVGG2)\n    ##### add predictions from all models\n    FullPredictions.append([row['PetID'],predictedSpeedVGG1,predictedSpeedVGG2])\n\n       \nFullPredictions= pd.DataFrame(FullPredictions, columns=['PetID','AdoptionPredictedVGG1','AdoptionPredictedVGG2'])\n\nTest_Main_df=Test_Main_df.merge(FullPredictions, on='PetID', how='left')\nTest_Main_df['AdoptionPredictedVGG1'] = Test_Main_df['AdoptionPredictedVGG1'].replace(np.nan, 4)\nTest_Main_df['AdoptionPredictedVGG2'] = Test_Main_df['AdoptionPredictedVGG2'].replace(np.nan, 4)\n\nTest_Main_df[['AdoptionPredictedVGG1', 'AdoptionPredictedVGG2']] = Test_Main_df[['AdoptionPredictedVGG1', 'AdoptionPredictedVGG2']].astype('int64')\n\nTest_Main_df.to_csv('TestPredictions.csv',index=False)\n\n","9823b36a":"missing_cols = set( ensemble_df.columns ) - set( TestEnsemble_df.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    TestEnsemble_df[c] = 0\n    \n# Ensure the order of column in the test set is in the same order than in train set\nTestEnsemble_df = TestEnsemble_df[ensemble_df.columns]\nTestEnsemble_df\n\nTest_Main_df = pd.read_csv('..\/input\/cnn-models-petfinder\/TestPredictionNightGood.csv')\nbackup2=TestEnsemble_df.copy()\nTest_Main_df=TestEnsemble_df.merge(Test_Main_df, on='PetID')\nTest_PetID=Test_Main_df.copy()\nTest_Main_df","50b0e7ed":"Test_Main_df.drop(['XGBSpeedPredict','ADASpeedPredict'], axis=1, inplace=True)\n\n#Test_Main_df['XGBSpeedPredict'] = pd.Categorical(Test_Main_df.XGBSpeedPredict)\n#Test_Main_df['ADASpeedPredict'] = pd.Categorical(Test_Main_df.ADASpeedPredict)\nTest_Main_df['BertPrediction'] = pd.Categorical(Test_Main_df.BertPrediction)\nTest_Main_df['AdoptionPredictedVGG1'] = pd.Categorical(Test_Main_df.AdoptionPredictedVGG1)\nTest_Main_df['AdoptionPredictedVGG2'] = pd.Categorical(Test_Main_df.AdoptionPredictedVGG2)\n\nTest_Main_df=pd.get_dummies(Test_Main_df,columns=['AdoptionPredictedVGG2','AdoptionPredictedVGG1','BertPrediction'])\nTest_Main_df.drop(['PetID','Description','AdoptionSpeed'], axis=1, inplace=True)\nTest_Main_df\n","8e0903e7":"Test_predictions = Main_ADAmodel.predict(Test_Main_df)\npredictions2 = [round(value) for value in Test_predictions]\nprint(len(predictions2))\nsubmission = pd.DataFrame({'PetID': Test_PetID['PetID'].values, 'AdoptionSpeed': predictions2})\nsubmission.head()\nsubmission.to_csv('submissionADA.csv', index=False)","2f3bcfc6":"Test_predictions = MainXGBmodel.predict(Test_Main_df)\npredictions = [round(value) for value in Test_predictions]\nprint(len(predictions))\nsubmission = pd.DataFrame({'PetID': Test_PetID['PetID'].values, 'AdoptionSpeed': predictions})\nsubmission.head()\nsubmission.to_csv('submissionXGB.csv', index=False)","86a77149":"find_percentage_agreement(predictions, predictions2)","baba73cd":"Plotting graphs for VGG Model1 ","1aefefc9":"**I Wont Be training the VGG Models here live, it takes a couple hours to do so, I will load the Weights i have trained before**\n\nUncomment the next sections to train the VGG Model","439dabfd":"by using keywords that shows the level of activity of the pet we can increase the adoption chances by a small margin, this comparsion is backed by 57% of the data.","20db2dab":"# Images Functions \n\nthese small functions will helped me load and display images","f4f2e407":"# Does the Fee of adopting a pet has an effect on Adoption Speed? #14","431ece30":"We can see that we have 8132 Dogs and 6861 Cats in the Data set, 54% of the Dataset pets are dogs,there is almost a blance between the listings of cats and dogs.","a92efcb0":"Let's checkout how the Age of the pet effects it adoptionSpeed, we can make a guess that the younger the pet the higher adoptionSpeed it has.\n\nWe can see that most of the pet's are quite young and there is bumps in the dataset every 12 months, we can make a guess that most of the stray pets doesnt have an exact birth date so people have to make an estimate of their age therefore it easier to estimate in years and not in months.\n\n","d921bc2b":"# **Dogs Vs Cats** #1 ","268ca11e":"it's really interesting to see that just by using the words neuter and spay (providing the information) the adoption rate per column changed a lot, My recommendtion is to add this extra information to each pet Description.","f977f0d2":"Tested out stemming the words","b8fe0a0e":"**Let's check the adoption rate per Pet Type:**\n","f3dcea9f":"#  Exploring the TrainSet - EDA Start\n\n","c46aa91d":"# Health Section ","09447bf8":"# Does Gender effects Adoptain speed?#5 ","ba246ffc":"Let's try to train two models one for Dogs and one for Cats","8c2a019a":"# Build XGBoost Model Version #1 DataFrame #1","d6dc11e5":"**Creating a Dataframe to analyze the results**\n\nDF columns:\n\n**Word**- The Word\n\n**Numbers of Rows that has the word**- Count the rows in our database that has the word.\n\n**% of rows with that word in the Data**- display the Precentage of rows that has the word.\n\n**Have the word, No Adoption**- Count how many rows have the word but has AdoptionSpeed=4, not adopted.\n\n**Difference between adoption Rate** -  **Have the word % no Adoption** \"Minus\" **No word % no Adoption**  - Negative value shows that rows that have the word have higher adoption rate than rows that doesnt have the word, Postive value shows the other way around.\n","2600b129":"**What about Color2 and Color 3?**\n\nfor now i will ignore Color2 and Color3 and just use Color1.\n\nIt's possible to explore them and make a column that will represent one colored pets and two or tri-colored pets.\n\n","7ce7688d":"we will use Tensorflow and Keras to make our VGG16 Model first.\n\n* we are using plenty of data augmention techniques to help us prevent overfit.\n* we are resizing each picutre to 112,112(half of the normal size for vgg16) so we can train faster.\n* This model uses imagnet weights as a baseline and train upon them, i also removed the top of VGG16 and add Dense layer and Softmax layer.\n* Training the model for 80 epochs took me around 8 hours so we wont be training the model live in the notebook due to 9 hours maximum sessions of kaggle.\n* we are saving the weights everytime the model improve (determined by the validation accuracy) and loading up the weights that we want next when using the model for predicitons\n\n\n","48248357":"# Does the amount of photos of a pet has an effect on Adoption Speed? #15","aed233ac":"surprsingly we can see some listing of pets with a very high Qauntity, I will bin all the 10+ listing to a one column that will be called 10+ ","cda07587":"Lets first remove the data columns that i will not use for sure atm.","3682a301":"# GridSearchResult for the Combined model.\n\nI will try to find the optimal parameters for our model with GridSeach:\n\n\n\nAfter tuning some HyperParameters for hours we got to good results:\n\nAccuracy Sum over 100 RandomStates is: 44.400700934579426 \n\nQuadratic Kappa Sum over 100 RandomStates is: 0.40919143816050394\n\nwe are in a better spot with the new Hyperparameters!\n\n","03b410eb":"# What I Plan to achieve:\n\n1. Using the rest of the not Tabular data - I Plan on using the pictures with CNN to extract features, Use the Description Column to extract sentiment and work with NLP techniques (woho! cant wait to learn something new), Ensembling all the models to improve the test results.\n\n2. Dive Deeper to the data set and gather more insights from the Tabular Data, Images, Description Column. ( I want to find deep insights like which words should we use in the Pet Description)\n\n3. Try out many more versions of handling the data (Data Cleaning and Binning, Feature Selection) aiming to improving the results.\n\n4. Learn and Deploy more Machine Learning models and hyper tuning them.\n\n5. Edit this whole notebook to make it more visually pleasing!\n\n6. Summarize the Insights in a cool PDF\\PPT and send it to Animal Shelters.\n","3c923188":"here we have a dataframe of the procced words and their frequency, we can se \"pron\" :-PRON- is the default lemma for pronouns in spaCy  https:\/\/spacy.io\/api\/annotation , therefore we will be ignoring \"pron\" when doing our research.","8fa69d96":"# Ensembling all models to one main Model to rule them all!","aa5fbb17":"# Description #17","06d22686":"I used the weights when getting to 0.27 val accuracy, I could take the 0.29 weights but I would have reached overfit according to the graph.","b11bed9f":"# Building Test predictions Dataframe for Kaggle submission","07dfa2d6":"balancing the classes to try and prevent overfit, while adding a bit more data to 1-4 classes","9ea27098":"# Description NLP for finding and research #19","30fe4eb9":"Seems like on AVG here we get better results when using using Cats and Dogs Model Separately , we get around 0.335 Kappa Score instead of 0.30 while using the combined model","31d1bcb7":"let's take a SneakPeak into the head of the Dataset to see some values.","5d8f2edf":"People always say that Black cats or dogs have lower chance of adoption, does it stand with our data?\n\nWell yes and no,\n\nBlack pets are dominating the dataset (in terms of sum of posts) and have 72% adoption rate **BUT** Brown and Yellow colors has lower adoption rate of 70%!\n\nwhile the most liked color is Cream with 77% adoption rate!\n\n\n\n","8c063270":"# Natural language processing Models - Bert, LSTM.","d2eb319e":"we can see some difference between the important features of each model\n1. Woha DescriptionWordsCount is the most important Feature here! That is pretty surprising! that Means that extracting NLP info and Sentiment from the Description Could hold valuable information\n2. Age is the second most important feature but the age of cats is much more important then the age of dogs.\n3. Maturity size of dogs is also more important than of cats, that probably because dogs have a higher variance of sizes.","9142309f":"We can see that the new predictions from VGG and Bert got to the top 40 and even starring in the top 20!","b45b9118":"Just as i thought, Younger pets tend to be not Vaccinated yet, we already know that younger pets are adopted more quickly so the information in Vaccinated chart is correlated with the age of the pet, Might drop this col when cleaning the Data","efc6857d":"**Change Gender Labels to Male, Female, Group**\n","f0b6b618":"Loading up the CSV Files \n","9f5ae59e":"let's bin the data to 10 bins to see it more clearly.\n\nHere we can see a trend that the longer the Descripton the better chances for adoption at all. \n\n**People do prefer to know more about the pet before adopting it, It's better to write a longer summary about the pet! This is an insight that we should display to animal shelers.**","ca4e1be8":"**Bin Video Amount to Have Video Vs No Video**\n* we can also try to drop this column","cdabb9d9":"**Vaccinated** - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\nseems like pets who are not vaccinated yet get adopted quickly and have higher adoption rate compared to vaccinated pets, that surprisng but maybe it's because of that young pets didnt get vaccinated yet? Let's check that too","8caf8b3e":"# And the Winner Model is LastVersionXGB.csv  with 0.2495 kappa score!","34daeb68":"# LSTM - leftovers, not used anymore.\nthe Idea was to use the Lematized column from the EDA instead of using the Description and Bert preprocess, the idea didnt go well, the Accuracy was low and tended to overfit really quickly after overcoming plenty of bug and GPU memory problms...\n\nI will leave the code here maybe i will get an idea how to improve it later.","f3b34380":"**ABORT ABORT THE PLAN!**\nafter many tests i have finaly improved my Kappa score to 0.2495, Kappa score ranges between -1 to 1 so i guess that's fine for my first Kaggle Competition.\n\n* I tried to make a dataframe from all 5 models and put another model on top of it to predict the label, after that i tried to use a voting scheme so all the models would be equal - didnt work it just got worse and fell to 0.2 kappa score.\n* in the end i decided to take Ensemble_df and append to it dummay varaibles based on Bert and two VGG predictions,after that i used XGBModel to predict the labels, this way got me to 0.2495 Kappa score.","6b8a9dbe":"**Training Dogs and Cats Models**","a8aab762":"Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved \u2014 and more happy families created.\n\nIn this Project i will uncover the Secrets behind pet adoption.\n\n# **Project Goals**\n\n1. **Uncover valueable information:**\n\n    extract insights about what makes pets more likely to be adopted, Find the most important variables that effects AdoptionSpeed.\n    \n    Opensource, send the insights I gathered to Animal Sheleters in hope that these insights will help save stray pets lives and help find an Home for all Pets (AMEN).\n    \n    Animal Shelters could use the insights to emphasis Pets profile to maximize their Adoption Speed.\n    \n    \n2. **Build a Model that will manage to predict AdoptionSpeed for Pets and Submit my first Kaggle Competition**\n\n    Build a model that will use the Data provided in this Competition to predict the AdoptionSpeed on the Test data.\n    \n    \n    \n3. **Learn New Techniques for Data visualization, Data Cleaning, ML Models, Try out NLP for the first time and most importantly have fun!!!**  \n    \n","453136fc":"**Let's try to find out which words can hurt the adoption rate of the pet**\n\n\nunfortunately i didn't find major insights or words that should be avoided at all cost, but I will recap some of my conclusions:\n\n1. the keywords dog and friendly have a lower adoption rate but why?\n\n **\"dog\"**: 23.8% of the rows have this word, there is a 7.3% difference in adoption rate. more details below.\n \n **\"friendly\"** :12.7% of the rows have this word, there is a  6.7%% difference in adoption rate. more details below.\n\n\n2. we can see that heart warming or needy words like:  loving, help, need, live, forever, house, hope, family, care\n\n   doesn't provide anything and can even hurt the chances of adoption by using these words or by reflecting the reality too much.\n   \n**I would recommend using the descrption text box to talk about the good qualities of the pet and not writing an heart warming story**\n\n","80b0f47d":"**Results of 3 Models**","aa6561fa":"Playing with some params -39.78% accuracy and 0.325% weighted Kappa is not bad for a first try without tuning anything at the model and using all the Data, We got plenty of room for improvments! (the Weighted Kappa that won the competiton is 45%)","105992f6":" let's look at the percentage of each bar to get some prespective.\n\n**We can see that the age of the pet has a big effect on the AdoptionRate and speed of the pet**\n\nYounger pet's have better chances of adoption, no surprises here.","e05a9b12":"I Stopped training the Bert model when it reached 0.3 Validation Accuracy","8c067183":"___________________ ","38bebcb5":"**Seems like PureDogs and Domestic Long Hair Cats are leading in adoptation percentage**\n\nthere is a 16% difference in Adoptation percentage between Pure Dogs and Mixed Dogs, that a lot! and a bit sad to be honest...","80dc2e10":"I wonder if people should pour their heart into the Description box or being cheap on words.\n\nLet's try to check if the length of the Descripton effects adoption rate (In words Count or in total Length)","4cba853c":"Let's see some pictures from our database","a139dbc8":"I wonder if a pet Name can make a difference on it adoptionSpeed.\n\nSuprisngly enough we find out that animals with a name(any name) are adopted at a better rate than animals without a name!\n\nthere is a 6.3% difference between having a name or not!\n\nthis is an important insights that we discovered and should share with animal shelters!","937fde86":"**Make Dummies for Categorical Data**","6ca3f743":"Same story here,Just as i thought,  Younger pets tend to be not Dewormed yet, we already know that younger pets are adopted more quickly so the information in Dewormed chart is correlated with the age of the pet, Might drop this col when cleaning the Data","89a22c0e":"Seems like that the more Photos a pet has the higher the chances for Adoption.","1b36573e":"# Exploring the Train Tabular Data\n\nseems like we have 24 cols of information about each pet in our dataset.\n","b5b8b7a5":"**We clearly see that \"intersted\" has the biggest impact on the adoption rate, By digging into some descriptions i can find out two main uses for this word:**\n1. interested as a pet trait, describing the pet as a curious and livly pet.\n2. referring to people who are interested in adopting the pet, I can't find a reason why it effects the adoption rate that much but i can assume that it shows more boldness from the post creator and less needy.","1e686d37":"# PetFinder !","4bf92a0b":"I wonder what will be the adoption rate of pets when they have the key words intersted + spay or neuter vs pets who didnt use these keywords.\n\nas we can see there is a big difference between each column and also adoption speed 4 has a huge change of 12.8%. \n\n15.6% of the posts has two (or 3) of the keywords.","dafc1fd2":"# Building the Main_DF from scratch:","b5c3e97f":"Let's look at the adoption rate per gender.\n\nMale Adoption rate is higher in 3.8% than females and 5.2% than groups.\n\ngroups are indeed more challenging to adopt, not surprise here, I was thinking groups will have a lower adoption rate!","b9542247":"We will be using Bert from tensorflow, https:\/\/tfhub.dev\/google\/collections\/bert\/1, we will be using a small bert to train faster.","e2ed3358":"Let's check out the distrubtion of AdoptionSpeed in the DS","f026e45c":"pretty much obvious but pets who are grouped have lower Adoption rate... It might be better to list every pet on their own if possible.\n\nThis is an insight that people should be aware of.","836170d3":"# correlation Matrix ","7ce69b24":"# Does Sterilized status effects Adoption Speed? #11","d23602f2":"Building 3 more layers above Bert to classify our classes","a899dbf2":"seems like most pets has 0 videos, I'm not so sure we would be able to use that column in prediction but we can see that pets with videos (at any amount) do have a higher adoption rate,\n\ntherefore **I can recommend animal shelters to take a small video of the pet**","8215d7dc":"we can see that a young age for a dog is more important than a young age for a cat, in the first 3 months dogs had a higher rate of adoption by 9% in totall compared to cats, while cats have a better adoption rate across the later years.\n\n**Age is an important factor for adoptaion speed so that lead me into thinking about making seprate models for dogs and cats, will be sure to check that out.**","5ac7b93a":"The Plan is to use all 5 Models to create a dataframe who will record all of their predictions.\n\nafter that train a XGB or ADABoost model on the dataframe as the last Model to rule them all!\n\n\n","bd105421":"sneakpeak into the new dataframe:","df9ebe90":"# Breed #4\n","33a19136":"some people prefer small pets because they are easier to handle, does it shown in our database?\n\nSmall Pets have an advantage of 4% adoptation rate from Medium and Large, while extra large have 91% adoption rate, well it doesnt tell us much because Extra large pets are only 0.22% of our database.","b1a2355a":"# Data fields summary\n\n**PetID** - Unique hash ID of pet profile\n\n**AdoptionSpeed** - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n\n**Type** - Type of animal (1 = Dog, 2 = Cat)\n\n**Name** - Name of pet (Empty if not named)\n\n**Age** - Age of pet when listed, in months\n\n**Breed1** - Primary breed of pet (Refer to BreedLabels dictionary)\n\n**Breed2** - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n\n**Gender** - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n\n**Color1** - Color 1 of pet (Refer to ColorLabels dictionary)\n\n**Color2** - Color 2 of pet (Refer to ColorLabels dictionary)\n\n**Color3** - Color 3 of pet (Refer to ColorLabels dictionary)\n\n**MaturitySize** - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n\n**FurLength** - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n\n**Vaccinated** - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n\n**Dewormed** - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n\n**Sterilized** - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n\n**Health** - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n\n**Quantity** - Number of pets represented in profile\n\n**Fee** - Adoption fee (0 = Free)\n\n**State** - State location in Malaysia (Refer to StateLabels dictionary)\n\n**RescuerID** - Unique hash ID of rescuer\n\n**VideoAmt** - Total uploaded videos for this pet\n\n**PhotoAmt** - Total uploaded photos for this pet\n\n**Description** - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n\n# AdoptionSpeed\n\nContestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted.\n\nThe values are determined in the following way:\n\n**0** - Pet was adopted on the same day as it was listed.\n\n**1** - Pet was adopted between 1 and 7 days (1st week) after being listed.\n\n**2** - Pet was adopted between 8 and 30 days (1st month) after being listed.\n\n**3** - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n\n**4** - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).","5dceeee1":"**Bin the Breed1 Column to 6 categories like in the EDA**\n\n* we can also dive deeper and UnBin the top 10-20 categories of Pure Dogs and Cats, see if that will change something","d23518f1":"\n\nHelpful links that i have studied from the topic of NLP:\n\nhttps:\/\/www.udemy.com\/course\/nlp-natural-language-processing-with-python\/ - Great Udemy course who helped me start the journey.\n\nhttps:\/\/jalammar.github.io\/illustrated-word2vec\/\n\nhttps:\/\/www.coursera.org\/learn\/nlp-sequence-models?specialization=deep-learning - coursera course on Sequence Models by DeepLearning.AI\n\nhttps:\/\/www.tensorflow.org\/tutorials\/keras\/text_classification_with_hub - tensorflow text guides\n\nhttps:\/\/www.tensorflow.org\/text\/tutorials\/text_classification_rnn - another tensorflow guide for simple RNN LSTM\n\nhttps:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/ - Bert guide\n","561202ec":"Making predictions with XGB model ","bb1f1eb8":"**How many Dogs and Cats do we have in the Train set?**","652ff40c":"This is also where we train our ADAModel!","86909fff":"train is also a good to have keyword, the train keyword add vital information to wheter or not the pet trained to commands, litter box, cage and more.\n","e061e232":"Lematize the Descriptions words, removing stop words and punction.","3441736a":"Let's check out the RescuerID column, this column may help us classify the pets better.\n\nWe can see that there are some Rescuers with a very high adoption rate and there are two with a very low adoption rate 42%, It's pretty surprsing too see that the RescuerID has such a significant results over the Adoption rate, I will try to use that column in DataFrame 2.\n\nWe have no way to understand what the RescuerID means except the fact that it's a unique Key for each Rescuer...\n\nwe have 19 Rescuers with more than 50 Listings! \n","6ca92ba0":"Let's start by building a dataframe for all ImageURLs and their adoptionSpeed so we can use it to train our models.","9d5cc0ca":"Now we can see that some of our rows has NAN value in the Adoption by Computer Vision, that is because some of the posts doesnt have photos!\nwe will replace all NAN values by 4 , we already know form our EDA that posts without photos had 38% adoption rate! that is 62% posts with the label 4!","a59cae57":"**Let's try to find out which words can help increase the adoption rate of the pet**\n\nwe can see that we have some interesting words that can increase the adoption rate up to 12% but we should also look at the % of rows that have the word in our database to deliver robust insights.\n\nI Will recap here some of the words that i have found usefull:\n\n**Usefull** : Difference between adoption rate - 5% Min, % of rows with that word in the Data - 10% Min, Words should be meaningfull and not numbers or '-'.\n\n\n1. **\"Interested\"**: 17.3% of the rows have this word, there is a  9.6% difference in adoption rate. more details below.\n2. **\"Kitten\"** :17.9% of the rows have this word, there is a  9%% difference in adoption rate. not surprising that Kittens has a higher adoption rate, we already seen that cats has higher adoption rate and the smaller the age the faster the adoption.\n3. **\"Neuter\"** **+** **\"Spay\"**: 22% of the rows have these words, there is a 9% difference in adoption rate. more details below.\n4. **'Train'**: 12% of the rows have this word, there is a 6% difference in adoption rate.  more details below.\n5. **'Play' + Playful' + ''Active'** :57% of the rows have these words 4.8% in adoption rate. more details below.\n\n**Good to know**:\nSibling, Liter, Cage.\n","3cd757bd":"Are there any breeds that are adopted in a better rate?\n\nI will check the Adoption Rate of popular Breeds in our data base.\n\nseems like there are some breeds that can effect the adoption rate even if they are not pure, we will keep the popular breeds information and the less popular breeds will be turned into the value- Mixed dogs .\n\n","82099785":"If we need to build the Test set from scratch use the code below, otherwise continue to load the data that is already prepared later in the code","57753b6d":"**GridSearch**","81f7bffd":"We can see here clearly that non Sterilized pets have a big gap in terms of adoption rate and speed, with about 20% adoption rate increase! \n\n","68261c88":"Seems like we have 307 different breeds for dogs and cats in total.\ncheckingout why some breeds have such high numbers and finding out we have some \"Mixed\" Breeds labels.\n\n","8c2d1f42":"# Preparing the Data for the Model - First Version DataFrame #1","46512f9e":"Here we will balance the classes in our image_df to try and prevent overfit to the popular classes only, i will add 1000 moure pictures for classes 1-4 to combat the underfit that we get by using so little data.","2d668f3f":"Here i will use some techniques that i learned to try and find recommendtion on what people should write in the description text to increase adoption rate!","41cb6cc7":"This colum is also correlated with Age but it seems like the age can be a little higher here.\n\nPeople prefer to adopt not sterilized pets, wheter or not they would like to breed them in the future or just prefer to choose for themselves","a9071003":"Dog keyward charts","ac588dc8":"# Does the amount of videos of a pet has an effect on Adoption Speed? #16","e1b06f17":"**Let's checkout the chart of \"interested\"**","ebf0805f":"well colors are important but Golden color cat is different than a Golden Color Dog, let's test the difference when looking and Cats and Dogs separately!\n\n\n\nthere are some changes between the charts, while cats adoption rate stay somewhat consistent Dogs on the other hand are more effected by the main color.\nBrown dogs have 65% adoption rate, compared to 75% in cats.\nYellow Dogs have 59% adoptin rate compare to 74% in cats.\n\n\n\n\n\n**making two different models for dogs and cats seems reasonable here, i will try that and see if it make sense**","5dbc70e1":"****","c25e7973":"we have 5536 Male Pets, 7277 Female Pets and 2180 Mixed Pets-a group of pets","cd851509":"**Now Lets remove outliers from the rest of the Numeric values**","5acb051c":"**Change Color1 Labels to 1-Black\n2-Brown\n3-Golden\n4-Yellow\n5-Cream\n6-Gray\n7-White**\n","1d36955a":"Using Kmeans clustring to find a way to use Score and Magnitude together in one column.\n\nI have found that using 3 clusters seperate the data pretty good, therefore i will try to use 3 classes when building sentiment text column.","60829873":"**Prepare Train Data and Prepare TestData as valid data.**","f56a2261":"# One Model to rule them all!\n\n# GridSearch for XGBoost Main_df","3294f1a1":"Let's discover the range of Breeds in our dataset!","e5706afb":"**Load the Data and make predictions**\n\nJust like in the training set we will do everything in the same way.\n\nwe have some columns that appeared only at train data so to be able to use the model to predict our test data we need to fill them up with 0 (some of these columns are RescuerID)","b383b3d4":"**Create DataFrames of Cats and dog separately**\n\nLet's try if the model will work better when trained and tested on Cats and Dogs separately.","e80a4848":"**Sentiment Testing methods**\n\n**1. Bin the Magnitude and Score together to one colum that will represent Negative\\ Postive Sentiment and their Magnitude as Netural, Weak, Medium, Strong.**\n\n2. Make a one column called Sentiment that will represent Score * Magnitude, normalize it too.\n\n3. leave both columns \"Score\", \"Magnitude\", normalize them and fit to the model.\n\n\nI Tried all 3 techniques and it seems like method 1 was easier to understand ,better visualized and had better scores when testing DataFrame 2 .\n\n\n","410dc6ec":"Is there any gender perfernce, does it effects the Adoption Speed?\n\n\nMales are adopted more quickly than Femals and Groups, we can see that the mass of Male posts is sitting around adoption 1-2 compared to females wher it is 2-4 .","acd59aa7":"I will build differnet Train DataFrames based on some of the insights from the EDA.\n\n# DataFrame 1 \n\nMain Points:\n1. remove Outliers and Normalize Numerical columns\n2. Work with one hot encoding for categorical data\n","147a494a":"After fiddling around with some testing on ADABoost I couldnt get better results than XGBoost, Tried to optmize the Learning rate and Estimator numbers got to : 5000 Estimators and 0.5 learning rate\n\n\nAccuracy Sum over 1 RandomStates is:40.67177802117561\n\nQuadratic Kappa Sum over 1 RandomStates is:0.37643191497631756","757464ff":"**FurLength** - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n\nFur, The Rival of my IRobot! some people prefer short haired pets and some just want their fav breed, what is the data telling us?\n\n\nWe can see that people prefer Pets with longer hair.\n","7137e761":"Seems like 28% of the pets werent adopted after 100 Days, that means 72% of the pets were adopted before 100 days.\n\n**From now on i will be reffering to pets who werent adopted after 100 as not adopted to make the notebook more clean**\n\n","a5cddd58":"**Remove Outliers from Age**\n\n* we can also Bin Age like we did in the EDA, This is a possbility that i will explore later.","6c8abf86":"Yay! most of the pets are healthy in the data set, Not surpsingly too see that pets with Minor Injury tends to have a lower adoption rate","19fc4c52":"# Overview the Dataset columns:\n\n* the DS has two kinds of animals here, Dogs=1 and Cats=2 represented by the Column 'Type', Pets also has a Name column.\n* each Pet in the DS has information about it physical apperance, we can see the Columns: Age, Breed, Gender, Color, FurLength and MaturitySize.\n* There are some columns concerning the Health status of the pet : Vaccinated, Dewormed, Sterilized and general Health condtion(Healthy, Minor Injury, Serious Injury).\n* Fee and local information, some of the pets has a Fee that is required for adoption, we can also see some local info like the State of where the pet in, RescuerID that is a Unique ID of who rescued the pet.\n* Some of the pets are tagged along with some Pictures and even Videos! We will be able to take advantage of that and use them with some Computer Vision Models.\n* There is a Description field that include free written Text describing the pet, we would be able to analyze some of it with NLP models.\n\n\n# AdoptionSpeed\n\nAdoption Speed is a column that represent how quickly and if a pet were adopted.\n\nWhenever a Pet were listed in \"PetFinder.my\" site PetFinder tracked how much time passed till someone adopted the pet.\n\nThe values of that column were categorized into 5 time slices.\n\n**0** - Pet was adopted on the same day as it was listed.\n\n**1** - Pet was adopted between 1 and 7 days (1st week) after being listed.\n\n**2** - Pet was adopted between 8 and 30 days (1st month) after being listed.\n\n**3** - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n\n**4** - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n\n\n\n","636660fa":"What i have tried:\n\n1. LSTM models in many aritchtures , had problems with GPU memory and low accuracy after solving some of them by making a smaller network -  not used anymore\n\n2. Bert Model - different bert models from tensorflowhub, different split of data and balancing.\n\n* Using the Full Data lead to overfitting label 4 and classifing around 50% of the data to it...\n\n* I also tried to train Bert with a balance between all classes but that lead to low amount of training data due to the low amount of posts labeled as AdoptionSpeed=0 and low accuracy.\n \n* my last attempt was to start with a balance between classes and add 200-300 more description data to the other 4 classes, This attempt was the most successfull but i fear to underfit the data with so little data...\n\n\n","8c1fd8a6":"# OK, now let's Deep Dive into our Columns data!","ae84af4f":"# DataFrame 2","309e204b":"**Now Let's go and Normalize the data**","7d354f21":"We can see that DataFrame 2 is performing better and that our Combinedmodel is close to using two distinct models.\n\nWe got to 41.5% accuracy with our combined model and 0.345 Quadractic Kappa.\n\n**From now on i will continue to work on a combined model and not two distinct models!**","e5c25ac4":"**Interpreting sentiment analysis values**\n\nThe score of a document's sentiment indicates the overall emotion of a document.\n\nThe magnitude of a document's sentiment indicates how much emotional content is present within the document, and this value is often proportional to the length of the document.\n\nhttps:\/\/cloud.google.com\/natural-language\/docs\/basics#interpreting_sentiment_analysis_values\n\n\n","18ad32fb":"# Does Quantity effects adoption Speed? #13","a295c01e":"Let's remove some of the outliers and Bin the age data to see understand it better, I will use bins of 1-12 months as it is a Puppy phase for dogs and cats, after that the bins will be on year basis, \n\n**We can see Clearly that the 'No Adoption Bar' for each month except 1-3 is the highest bar :(**\n\nI'm pretty surprised to see that even dogs that are young 1-2 have such a huge bar","74221c1c":"the past charts were a bit overwhelming, I want to see the data in more simple way, Let's visualize the precentages of Adoption Rate.\n\nseems like pets in age 1-3 has a really low adoption rate, after age 3 the adoptin rate picks up again .\n\ni can guess that the reason behind these adoption rate is that pets in age 1-3 are no longer considered \"puppies\\kittens\" so they are less attractive but do need a lot of attention, compared to older dogs 3+ that considered adults and are more behaved.","96f79e19":"around 55.6% small pets are adopted after a month or less\n\n48.2% of Medium sized pets a are dopted after a month or less.\n\n51.7% of Large sized pets are adopted after a month or less.\n\nseems like small pets are adopted faster than the rest.","9bd20126":"# Does Fur Length effects adoption Speed? #8\n","b7340aa7":"# Does Color effects Adoptation Speed ? #6","b7422b3f":"Let's checkout Dog and Friendly charts, seems like the charts are pretty similar, that strengthen my idea.","20ffd657":"Make predictions with XGBModel and saving it to a CSV","4349114d":"**Adding Bert Model**","2ac2e2b6":"n_iter is the number of random iterations, CV is the number of Cross Validation on each iteration.","9b7a44a7":"**Loading up the JsonFile Data and adding that to our train_df, cleaning some NANS and fill them with 0 too**","11a92c5f":"# Project Overview\n","6e7693f9":"If you reached to this point, you are awesome , this is my first time doing Kaggle Competitions and making a Datasciene project in this scale.\n\n\nI'm not super happy with my Kappa score but the important part is that I learned a lot about the Data sceince world learned new skills in the field of EDA, NLP, CNN, Machine Learning, Deep Learing and I'm super grateful for that.\n\n**This notebook took me countless days and nights to make and I Finally reached my destination!**\n\nI Accomplished all my projects goals and now i can represent the Main Docuemnt I created to help Animal Shelter\\ Individual Resucer make better Adoption post!\n\nhttps:\/\/drive.google.com\/file\/d\/1Ljp4Vg8qjqDdkzfjTGn9FnpyR3Efns6d\/view?usp=sharing\n\nTake this PDF and spread it! let us help our little friends find the Home they Deserve \u2665\n\n\n\nStav Cohen.\n\n\n\n\n","e1254276":"Most of the pets have photos in their post , here i will try to use their photos and build a CNN model and train VGG16 model to predict adoption speed","722ef755":"# Computer Vision Models-  VGG16","7b56bb75":"**Why does the word \"Dog\" has such a bad adoption rate and why the word \"friendly\" who suppose to be a good quality has a bad adoption rate too?**\n\nwe already know that referring to a pet by a generic name or no name (dog, cat) can lower the adoption rate, we also know that dogs has lower adoption rate then cats but what is the main reason here?\n\npeople can tell that the pet is a dog by the pictures of it, there is no need to add the word dog for it.\n\nwhy \"Friendly\" has lower adoptin rate too? are they connected?\n\nHAHA i got an idea!!! \n\nwe know that friendly is a good trait but what if people uses the words Friendly and Dog to say that the pet **is not** dog friendly?","15897b0f":"I Configured the XGBoost model with hyperparemters that i have found.\n\ni wanted to make sure the model is robust and checked out the model average Accuracy and kappa over 100 Random states of train-test split. (even tho i made CV when selecting the parameters)\n\nthis is where we fit or XGBModel too.","c221d761":"# Pets with a Name Vs Pets without a Name #2\n","a7f16345":"# Loading up Main_df file\n\nGetting predictions from all 5 models takes a lot of time, that's why we save the prediction table to CSV and now load it !","fea2ef10":"Our top features changed a bit, while DescriptionWordsCount, PhotoAmt, Age, MatuirtySize, FureLength and Quantity remained Top 6, we can now see Color Black, Gender Female, Sentiment Weak Postive and Color1 Brown sneaking up to Top 10!\n\nwe can also see two RescuerID columns here!","2a6ad78d":"**Let's compare AdoptionSpeed of Pets that are free to non free pets**","1452e053":"Adding predictions from our Computer visions models, **It takes a lot of time to run!**\n\nWe will make predictions on every photo that is tied to a post, after that we will choose the label with the got the highest prediciton score all acros the photos.\n\n","d5f3d57e":"Seems like after 500 we can cut the outliers and settle with 0.996% of the train set\n\n","bbe6dcec":"# RescuerID #17","93ff6bb0":"# Load the Test Data ","c5c24dd2":"# Age Distribution by Type #3","8342edba":"Friendly keyword charts","71805153":"Here is a small usefull function that i have used and can use in the future to check the differnce between two models predictions","6433d47d":"First time working with text! Let's do it! \n\nstarting out with a wordcloud of popular words in a shape of a dog!.\n\nwe can already see the main words are Cat, Dog, Love, Kitten, Found Adoption, Will.","a815f871":"as we can see the sentiment of the text does effect the adoptionSpeed column but it's not super effective in predicting the AdoptionSpeed.\n\nWe dont have a lot of Negative sentiment descriptions in our database, It's not a surprise because people do want to show the adopted pet in a positive light.\n\nthe negative sentiment descriptions tend to have lower adoption rate as we can see in the columns \"Weak Negative\"\n\nThe Big surprise here is that the \"Netural\" column has the best adoption rate over the five periods! \n","2ff41add":"What a cool tree!\n\nAge is the main tree node, in level 1 we can see Type and Color and in level 2 we can see Quantity, Age again, Breed_Domestic long hair cat and a Leaf already.\n\nmost of the leafs are , that is super weird, we should work on a better dataframe and a better model.","c1dbc215":"Training the model and saving checkpoints every val accuracy improvment","fea47d33":"Merging the prediction with ensemble_df","79810a38":"We can see that most of the pet's are Free of fee.","85aace05":"We want to have the same amount of pictures per class in train and validation sets (to prevent situiatons like there are only 100 pictures from classes 0-3 in validation and the rest are from class 4 only)\n\nwe will be using train_test_split with stratify to make sure we have the right balance between sets.","21cff1e3":"**Do people Adopt any kind of Pet faster than the other?**\n\nSeems like Cats are adopted more quickly than dogs, We can see 8% difference between adoption of dogs and cats between 1St week and 1St Month.\n\nwe can also see that 74% of the Cats were adopted while only 70% of the dogs were adopted","70d91165":"Let's take a look at the new column:","00a82ac9":"# Does MaturitySize effects Adoptation Speed?  #7 \nMaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n\n** most of the pets in our database will mature to a Medium size, (not a surprise because we have cats too in our database)**","82d5aa01":"# Does Vaccinated status effects Adoption Speed? #9","5d35a0b1":"# XGBOOST for DF 2\n","b1270390":"Plotting XGBoost trees:\n\nDisplaying these trees in kaggle broke my notebook again and again, I do not recommend running the code below...\n","abf8e6ff":"I Will comment out the GridSearch algorithem becasue we already found the best parameters for our model.","5e6ee7f8":"___________________","f22bbb32":"**So far we got two Models that are based on DF2 the models are ADAmodel and XGBmodel, i will be using both of them in the final prediction**","9bb83ea6":"Use our XGB and ADA Models to make predictions on ensemble database who holds our preprocessed data\n\nwe will build a dataframe called Main_df who will hold all model predictions.","62231c4d":"# NLP #18","326d9d33":"Dropping out XGB and ADA prediction who were trained on the same data and lead to overfit!","d1974f7a":"# Does Dewormed status effects Adoption Speed? #10","b3aaa8f7":"# Building a Dataframe with more focused breeds section :\n1. **Mixed Dogs** -  Dogs with mixed Breed\n2. **Pure Dogs** -  Dogs with a pure Breed (taking all different Breeds into one category) \n3. **Pure Cars** - Cats with a Pure Breed\n4. **Domestin Short Hair Cats** - Domestic short-haired cat is a cat of mixed ancestry\u2014thus not belonging to any particular recognised cat breed\n5. **Domestin Medium Hair Cats** \n6. **Domestin  Hair Cats** ","d819a6fb":"Let's check the Adoption Rate per Photo amount.\n\n**We can see a clear insight that the more photos a pet has the higher is the adoption rate!**","2c0104ed":"# Summary and Ideas after EDA\n\n* It can be a good idea to split the data into Cats and Dogs and try to make seperate models for each of them.\n* The Data need some cleaning of outliers, need to decide how to handle them.\n* There are some categories that can be dropped like states or Color2-3 that we will not use now.\n* I can build a NLP model upon The Description and Lemmatized columns.\n* I can try to use Pets photos to build a CNN model.","4de0fd1a":"# Feature Importance Results and comparsin to DF1:\n\n","a37a8e4a":"Let's plot some Feature Importantance here.\n\nI had to limit the number of features displayed becasue keeping many Unique RescuerId and using OneHot to encode them created many columns.","0edffddd":"What if a pet has both the words dog and friendly?\n\nseems like pets with both words have a lower adoption rate in around 12.3%.\n\nunfortunately the insight is backed by 6.7% of the data only.\n\n\n","a61ae3a5":"I Wonder how much ADABoost and XGBboost model agrees on the labels, \n\n59% that's nice, let's see who is the better model.","637f1367":"Spliting the Train Test to Train and Test","04e6dc61":"**Bin Fee to Paid Pets and Free Pets**\n* we can also try to bin per intervals or leave it as numerical value","6ba15a4e":"**Change Name Column to Name Vs No Name**","ec066d12":"Making a Timer function to see how much time each Fit was taken during Gridsearch","700ebc8f":"# Summary \n\n\n\nIn this notebook you can find a Big EDA on all the tabular data and Text data followed by 5 models that are being used to predict the adoptionSpeed of each pet.\n\n1. XGBOOST\n2. AdaBoost\n3. Bert\n5. VGG16\n5. VGG16 2\n\nIn the End of the notebook i will summarize all my findings and insights on how should animal adoption posts improve!\n\n\n\n","d9fd4c6d":"Building CNN Models:\n\nWhat i have tried so far:\n\nData:\n1. Using all the Data and pictures- heavy overfit  \n2. Balance the Data to have the same amount of pictures per class - low ammount of accuracy around 22-23\n3. Balancing the Data and adding 1000 more pictures for each class except class 0 who has low amounts of pictures - best results so far with 29-31%\n\nModels:\n\n1. Custom CNN Model\n2. VGG16 non trainable\n3. VGG16 all trainable with imagenet weights\n4. VGG16 all trainable with no weights\n5. Resnet.\n\nModels in Use:\n1. VGG16 with trainable Imagenet weights -VGG1\n2. VGG16 with without base Imagnetweights -VGG2","14ef9c3f":"Let's work on another version of the DataFrame and make some changes:\n\n1. we are going to use RescuerID info instead of dropping it.\n2. keeping Breed1 Names for popular breeds.\n3. add Sentiment Text from NLP section\n\nwe will also work on preproccessing the test set!","3063af94":"\nthere are Many intersting feature to check that stands out in the correlation Matrix!\n\n* Quantity X Gender is not surprsing since Gender 3 is only for More than one pet\n* Seems like Dewormed Sterilized and Vaccianted have an high correlection\n* we can see that there is a postive correlection between The age of the pet and the adoption Speed (that means older pets are less likely to be adopted :( )\n* By looking at the binary adoption df (left heatmap) we can see that small amounts of pictures effects the chances of pet adoption\n* People tend to adopt pet's with higher fur length\n\n","7a3b92d9":"# An Example of a Listing in PetFinder.my \nhttps:\/\/www.petfinder.my\/pets\/108197\/\n\n![](https:\/\/i.imgur.com\/IxK3VXx.png)","d10d9c02":"**We can see that Pets Free of fee has higher Adoptation Rate, there is also a downtrend of adoptation rate as the fee increases**","88fca27d":"We can see a differnce in adoption Rate per dogs and per Cats, Seems like the fur length on dogs is more important than on cats","96a13055":"# ADABOOST Model for DF2","233d414f":"Normalizing the Data with Zscore, we can see that most of the Descripton are quite short, It's hard to see any insight here beside that pets that were adopted at the same date has a shorter descripton.\n","b387a953":"seems like we have some outliers in Fees, the Mean is 21 but the max Value is 3000, let's take a closer look","7dddd9d5":"# Training ADABOOST","d88cf9d3":"we can see some outliers that should be removed, let's dive a bit deeper to find out what should be dropped.","ea48baca":"What a Cutie! , Crossing my fingers for him!","059d3a7d":"# Does Health effects adoption Speed? #12","2a54c297":"Followed this great guide on Hyperparameters tunning ! https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","7796b404":"Make predictions with ADAModel and saving it to a CSV"}}