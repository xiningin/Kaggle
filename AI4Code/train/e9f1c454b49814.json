{"cell_type":{"90977b30":"code","836bc5bc":"code","5ad4af56":"code","9c4c30d9":"code","b3c1025f":"code","f78229c9":"code","50bd3ddd":"code","d6afc3f7":"code","ee59515f":"code","8e215ab5":"code","2d019f78":"code","47531e01":"code","8d892cdd":"code","e8dd7088":"code","16278638":"code","8f540dbb":"code","a39fca02":"code","6910dfbf":"code","73878bf7":"code","c240bd51":"code","68e205e4":"code","3f89fbdf":"code","ce5749b2":"code","0f589e4e":"code","d6b9574f":"code","1176db47":"code","853bf942":"code","ce7a72aa":"code","057225a2":"code","de28c0f2":"code","2ab0c469":"code","f3ef4f85":"code","55006b23":"code","afd18c03":"code","c328517f":"code","6c5e90f7":"markdown","40a562f4":"markdown","b01e7404":"markdown","5d7363c5":"markdown","2c9ffb62":"markdown","9323c475":"markdown","0befab75":"markdown","e2771fae":"markdown","1d0a1ec4":"markdown","f9333a04":"markdown","2d821b1a":"markdown","afce2aea":"markdown","32bf6cdc":"markdown"},"source":{"90977b30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","836bc5bc":"import plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt","5ad4af56":"# Import the datasets\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","9c4c30d9":"# Perform EDA on the train\ntrain.shape ,train.dtypes","b3c1025f":"train.head()","f78229c9":"train.excerpt[0]","50bd3ddd":"train.describe(include='all')","d6afc3f7":"# Perform EDA on the test\ntest.head()","ee59515f":"test.shape","8e215ab5":"test.describe(include='all')","2d019f78":"# The histogram provides details on the distribution of the variable. Including the box plot shows key parameter summary values.\n# By using plotly we are able to hover over the values and easily understand how the values compare\nfig = px.histogram(train, x=\"target\",\n                   marginal=\"box\")\nfig.show()\n","47531e01":"# Extract insights from the excerpt variable\nimport spacy\n# Initialise spacy\nnlp = spacy.load('en_core_web_sm')","8d892cdd":"# Perform initial test on one excerpt\nsample1 = train.loc[0, 'excerpt']\nsample1","e8dd7088":"# Create the spacy doc item for review\ndoc = nlp(sample1)\ndoc","16278638":"# Reviewing the token, lemma and stopword for each token (item)\nprint(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\n# Review the first 10  values to test the output\nfor token in doc[:10]:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\\t\\t{len(token)}\")","8f540dbb":"# A few different options for stopwords, spacy and nltk. Lets compare\nimport nltk\nfrom nltk.corpus import stopwords\n# Compare the differences\nnltk_set = set(stopwords.words('english'))\nspacy_set = set(nlp.Defaults.stop_words)\n\n# Union - all values\nunion = nltk_set.union(spacy_set)\n# Intersection - seen in both sets\ninter = nltk_set.intersection(spacy_set)\nprint(f\"Seen in both : {len(inter)} \\n {inter}\")\n# Remainder - differences between sets\nnltk_extra = nltk_set - inter\nspacy_extra = spacy_set - inter\nprint(f\"Extra NLTK : {len(nltk_extra)} \\n {nltk_extra}\")\nprint(f\"Extra Spacy : {len(spacy_extra)} \\n {spacy_extra}\")","a39fca02":"from sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.feature_extraction.text import CountVectorizer\n#instantiate CountVectorizer() \ncv=CountVectorizer() \n\n# this steps generates word counts for the words in the sample doc\nword_count_vector=cv.fit_transform(train.excerpt)\n\nword_count_vector.shape","6910dfbf":"# Compute the IDF values\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True) \ntfidf_transformer.fit(word_count_vector)\n# print idf values \ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n# sort ascending \ndf_idf.sort_values(by=['idf_weights'])\ndf_idf.describe()","73878bf7":"# Lets create a dictionary to review the key phrase outputs\nfrom collections import defaultdict, Counter\n\n# Returns integers that map to parts of speech\ncounts_dict = doc.count_by(spacy.attrs.IDS['POS'])\n\n# Print the human readable part of speech tags\nfor pos, count in counts_dict.items():\n    human_readable_tag = doc.vocab[pos].text\n    print(human_readable_tag, count)","c240bd51":"pos_counts = defaultdict(Counter)\nfor token in doc:\n    pos_counts[token.pos][token.orth] += 1\n    \nfor pos_id, counts in sorted(pos_counts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common():\n        print(pos, count, doc.vocab.strings[orth_id], len(doc.vocab.strings[orth_id]))","68e205e4":"# Expanding named entities\nfor entity in doc.ents:\n    print(entity.text, entity.label_)\n\n# Analyze syntax\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\nprint(\"Number of sentences\", len([*doc.sents]))\nprint(\"Sentiment\", doc.sentiment)\n\n# Understand the length of sentences\nfor sent in doc.sents:\n    print(sent.start_char, sent.end_char, (sent.end_char - sent.start_char))","3f89fbdf":"from spacy import displacy\n\n# Display the entities within a sentence\ndisplacy.render(doc, style='ent', jupyter=True)","ce5749b2":"# Lets apply the nlp instance to each excerpt\ntrain['excerpt_scy'] = train['excerpt'].apply(nlp)","0f589e4e":"# Check the data type for the updated column\ntype(train.loc[0, 'excerpt_scy'])\ntrain.head()","d6b9574f":"# Create the class methods required to run the analysis\nclass NLPMethods():\n    # Create constructor for the class\n#     def __init__():\n    \n    # Number of sentences\n    def number_sentences(self, nlp_text):\n        return len([*nlp_text.sents])\n    \n    # Average length of sentence\n    def average_sentence_length(self, nlp_text):\n        sent_length = list()\n        for sent in nlp_text.sents:\n            sent_length.append(sent.end_char - sent.start_char)\n        return np.mean(sent_length)\n    \n    # Part of speech tags\n    def part_of_speech_tags(self, nlp_text):\n        counts_dict = nlp_text.count_by(spacy.attrs.IDS['POS'])\n        counts_dict1 = {}\n        # Extract the text that matches to the POS value\n        for k, v in counts_dict.items():\n            counts_dict1[nlp_text.vocab[k].text] = v\n        return counts_dict1\n    \n    # Number of spaces\n    def number_spaces(self, nlp_text):\n        dict_pos = self.part_of_speech_tags(nlp_text)\n        if dict_pos.get('SPACE') != None:\n            space = dict_pos.get('SPACE')\n        else:\n            space = 0\n        return space\n    \n    # Part of speech tags - including the word counts\n    def word_counts(self, nlp_text):\n        pos_counts = defaultdict(Counter)\n        for token in nlp_text:\n            pos_counts[token.pos][token.orth] += 1\n        \n        # Create dictionary for the word counts\n        word_counts_dict = {}\n        for pos_id, counts in sorted(pos_counts.items()):\n            pos = nlp_text.vocab.strings[pos_id]\n            for orth_id, count in counts.most_common():\n                word_counts_dict[nlp_text.vocab.strings[orth_id]] = {'count':count, \n                                                                     'length':len(nlp_text.vocab.strings[orth_id]), \n                                                                     'pos':pos}\n        return word_counts_dict\n    \n    # Number of words\n    def number_words(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        return len(dict_word_counts.items())\n    \n    # Longest word\n    def longest_word(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        df = pd.DataFrame(dict_word_counts).T.reset_index().rename(columns={'index':'variable'})\n        return max(df['length'])","1176db47":"# Add columns for the spacy doc\ntrain['num_sentences'] = train['excerpt_scy'].apply(NLPMethods().number_sentences)\ntrain['avg_sentence_length'] = train['excerpt_scy'].apply(NLPMethods().average_sentence_length)\ntrain['pos_dict'] = train['excerpt_scy'].apply(NLPMethods().part_of_speech_tags)\ntrain['num_space'] = train['excerpt_scy'].apply(NLPMethods().number_spaces)\ntrain['wc_dict'] = train['excerpt_scy'].apply(NLPMethods().word_counts)\ntrain['num_words'] = train['excerpt_scy'].apply(NLPMethods().number_words)\ntrain['longest_word'] = train['excerpt_scy'].apply(NLPMethods().longest_word)","853bf942":"train['avg_sentence_length'].mean()","ce7a72aa":"train.sample(5)","057225a2":"# Review the max value target variable\nmax_val = np.max(train['target'])\ntrain_max = train.loc[(train['target']==max_val), :]\ntrain_max","de28c0f2":"\n\n# Check the reason for the largest target value\ntype(train.loc[2829, 'excerpt_scy'])\ntrain.loc[2829, 'excerpt_scy']\ntrain.loc[2829, 'excerpt']\n\n\n\nIt appears that the word \"paleontologists\" could be causing the difficulty?\n","2ab0c469":"# Review the min value target variable\nmin_val = np.min(train['target'])\ntrain_min = train.loc[(train['target']==min_val), :]\ntrain_min","f3ef4f85":"# Check the reason for the smallest target value\ntype(train.loc[1705, 'excerpt_scy'])\ntrain.loc[1705, 'excerpt_scy']","55006b23":"\n\n# Import libraries\nimport seaborn as sns\n\n\n\n# Correlation analysis\ndf = train.loc[:, ['id', 'target', 'num_sentences', 'avg_sentence_length', 'num_space', 'num_words', 'longest_word']]\ndf.head()\n\ncor = df.corr()\nsns.heatmap(cor, annot=True)\nplt.show()\n\n","afd18c03":"X = df.drop('id', axis=1)\nX.dtypes","c328517f":"# Review a scatter matrix\nfig = px.scatter_matrix(X)\nfig.show()","6c5e90f7":"# EDA train data ","40a562f4":"# Stop words :","b01e7404":"# 3. Create the datasets for training the models","5d7363c5":"\n2. Data discovery\n\nKey challenge is to understand the difficulty of the readability challenge. When reviewing how difficult a text is there are a few key areas of interest:\n\n    - Word difficulty\n\n            Vocabulary lists : can be used to highlight the proportion of comman words used. The less common a word is the more difficult it can be perceived and understood to be\n            Word length : longer words are usually seen as more difficult that short. Therefore a correlation could be constructed between the word length and text difficulty\n\n    - Sentence difficulty\n\n            Sentence length : longer sentences lead to more difficult text. Have to be aware that the inclusion of colon and semi-colon can impact sentence length as well as the full stop\n\n","2c9ffb62":"Initial thoughts of the test dataset\n\n    Excerpt is the key variable\n    Seven values mean that the validation of the training set will be key to optimise the model. Developing a hold out sample on this size could help\n# Data Visualizations","9323c475":"# Feature Extraction :\n","0befab75":"\n# EDA of new variables","e2771fae":"# EDA \nFiles\n\n    train.csv - the training set\n    test.csv - the test set\n    sample_submission.csv - a sample submission file in the correct format\n\nColumns\n\n    id - unique ID for excerpt\n    url_legal - URL of source - this is blank in the test set.\n    license - license of source material - this is blank in the test set.\n    excerpt - text to predict reading ease of\n    target - reading ease\n    standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n##  Import packages ","1d0a1ec4":"Spacy appears to cover a wider range of stopwords. Adding the additional 56 words from the NLTK could help to increase the scope of stopwords available for use.","f9333a04":"# Import data :","2d821b1a":"# EDA test data ","afce2aea":"\n\nIt appears that the word \"paleontologists\" could be causing the difficulty?\n","32bf6cdc":"# Initial thoughts\n\n    Extracting information from the \"Excerpt\" column will be key to this analysis\n\n            All of the values are unique\n            EDA required for text will help with future discoveries\n\n    Target column shows a broad range of values\n\n            Distribution shows a larger proportion of negative values (perform a Histogram and box plot to confirm)\n            There appears to be a negative skew with the mean value lower than the median (50th percentile)\n\n"}}