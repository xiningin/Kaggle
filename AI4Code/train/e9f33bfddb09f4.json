{"cell_type":{"2adcaef4":"code","49e96945":"code","69b1c67f":"code","e03192e4":"code","f7641c15":"code","2bad6398":"code","d471be89":"code","078c1ff6":"code","ebd3de51":"code","77b93f6d":"code","ef8fbe24":"code","0a595210":"code","7a18e326":"code","d4c2c773":"code","89d27fe6":"code","514ffd4b":"code","215b0528":"code","5f34a790":"code","9f7d2f40":"code","23fd9fa9":"code","86b4d783":"markdown","9f88dbe6":"markdown","f1c5581f":"markdown","771ab07c":"markdown","0bff7e91":"markdown","94c1f060":"markdown","7447bc8f":"markdown","9c3082cf":"markdown","ba963604":"markdown","e131bafa":"markdown","46b09436":"markdown"},"source":{"2adcaef4":"# Basics imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#import plotly.express as px\n\n# Pre-processing \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# For Baseline algoritm\nfrom sklearn.dummy import DummyClassifier\n\n# Model that i will use\nfrom sklearn.naive_bayes import MultinomialNB\n\n# For shows results\nfrom sklearn.metrics import accuracy_score\n\n# For select features\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# For hyper parameters\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# For validation\nfrom sklearn.model_selection import KFold,cross_validate,cross_val_score\n\n\n# Control the seed\nseed = np.random.seed(777)","49e96945":"data = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")\ndata.head()","69b1c67f":"data.isnull().sum()","e03192e4":"data.isna().sum()","f7641c15":"# Variables. We can see that have 2 types of variables. Numerical variables and \"Str variables\"\n# For this i will make some modifys\n\nNaN = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']\nfor i in NaN:\n    print(i, \":\", data[i].unique())","2bad6398":"# Tranform objects variables on numbers, automatically...\n\nencoder = LabelEncoder()\nfor i in NaN:\n    data[i] = encoder.fit_transform(data[i])\ndata.head()","d471be89":"# droping negative values from feature Oldpeak (i will use a model that is sensible)\n\ndef rename(num):\n    if num < 0:\n        return 0\n    return num\n\n\ndata[\"oldpeak\"] = data[\"Oldpeak\"].map(rename)\ndata.drop('Oldpeak', axis=1, inplace = True)","078c1ff6":"# Separing variables from values\n\nx = data.drop(['HeartDisease'], axis = 1)\ny = data['HeartDisease']\n\nx.head()","ebd3de51":"# I really like this plot, its very simply and beautiful. Tks for https:\/\/www.kaggle.com\/ludovicocuoghi\/pytorch-pytorch-lightning-w-optuna-opt \n'''\ndef num_plot(df, col):\n    fig = px.histogram(df, x=col, color=\"HeartDisease\",\n                       marginal=\"box\")\n    fig.update_layout(height=400, width=500, showlegend=True)\n    fig.update_traces(marker_line_width=1,marker_line_color=\"black\")\n    fig.show()\n    \nfor col in x:\n    num_plot(data,col)\n    \n'''","77b93f6d":"# some outliers can't stay in zero... For example: Cholesterol, everybody have cholesterol... How can a person have a cholesterol equal 0?\n# because of this we need to make a more detailed analysis of this feature\n\nprint(x['Cholesterol'].value_counts())\n\n# have a 172 persos with cholesterol == 0...\n# so... i will fill this with median. What is the median?\nprint('')\nprint('')\nmedian = x['Cholesterol'].median()\nprint(\"The median of cholesterol is:\",median)\n\n\n# with median on hands, i will replace the values with 0\nx['Cholesterol'].replace({0:median}, inplace=True)","ef8fbe24":"print(x['Cholesterol'].value_counts())\nprint('')\nprint('')\nprint('So, now the median is:',x['Cholesterol'].median())\n\n# Now have non values == 0...","0a595210":"x.describe()","7a18e326":"# Now that i analyze the data, make some modifys and drop some things. The dataset it's ready for the machine learn model.\n# First i will pull apart the data for train an test.\n# For this i will use the classic: train_test_split (hahahhah)\n\nxtrain, xtest, ytrain, ytest= train_test_split(x,y,test_size =0.2,shuffle = True, stratify = y)","d4c2c773":"# Now i will create a baseline algorith for compare with the score of the ml algorithm\n# for this i will make a DummyClassifier\n\n# i make a function for automate the results\ndef results (dummy):\n    mean = dummy.mean()\n    std = dummy.std()\n    \n    print('')\n    print('Accuracy mean: %.2f'%(mean\/100))\n    print('')\n    print('Accuracy interval: [%.2f,%.2f]'%(((mean-2*std)\/100),((mean+2*std)\/100)))\n    print('')\n    return round((mean\/100),3)[0]\n\nx = []\n\nfor i in range(21):\n    dummy = DummyClassifier(random_state=seed).fit(xtrain, ytrain)\n    dummy_score = dummy.score(xtest,ytest)*100\n    x.append(round(dummy_score,2))\n    \nbaseline = results(pd.DataFrame(x))\nbaseline\n\n# The baseline are floating around of 0.50,0.51 and 0.52, how we can see....","89d27fe6":"model = MultinomialNB().fit(xtrain, ytrain)\npredict = model.predict(xtest)","514ffd4b":"accuracy_normal_model = accuracy_score(ytest, predict)\n\nprint(\"The accuracy of this model are: \",round(accuracy_normal_model, 3))\nprint(' ')\nprint('While the accuracy of base algorithm are', baseline)\n\n# I think that is a good score, but we need, yet, make a validation about this model.\n# At least, we're having a better result than the base algorithm...","215b0528":"# Now that i have the score of this model, i will try improve the results with selecting, only, the best features and selecting another parameters  (hypeparameters)\n\n# So... first i will take a look in the features.","5f34a790":"sns.heatmap(xtrain.corr())\nplt.show()\n\n# With this graph we can see that the features are very similiar, have a medium correlation\n# i think that all features are important, but to confirm that i will make a experiment with KBest algorithms","9f7d2f40":"select = SelectKBest(chi2,k=10).fit(xtrain, ytrain)\n\ntrain_kbest = select.transform(xtrain)\ntest_kbest = select.transform(xtest)\n\nmodel_kbest = MultinomialNB().fit(train_kbest, ytrain)\npredict = model_kbest.predict(test_kbest)\n\naccuracy_model_kbest = accuracy_score(ytest, predict)\n\naccuracy_model_kbest\n\n\nprint(\"The accuracy of the first model are: \",round(accuracy_normal_model, 3))\nprint(' ')\nprint('The accuracy with the best features are:', round(accuracy_model_kbest,3))\nprint(' ')\nprint('While the accuracy of base algorithm are', baseline)\n\n# The results are worst than the \"original\" model that i made... Because of this i will select the first model","23fd9fa9":"cv = KFold(n_splits =10, shuffle=True)\n\ncv_results = cross_validate(model,xtrain, ytrain, cv=cv, return_train_score=False)\n\ncv_mean = cv_results['test_score'].mean()\ncv_std = cv_results['test_score'].std()\n\nprint('')\nprint('Accuracy mean:',round(cv_mean,2))\nprint('')\nprint('Accuracy interval:',round(cv_mean-2*cv_std,2),round(cv_mean+2*cv_std,2))\nprint('')\n\n# the cross validation results are very close of the model that are the best features....","86b4d783":"# Loading, analyzing and modifying data","9f88dbe6":"![image.png](attachment:53bbfdb8-7a2e-43ba-a83e-63cadfc1caed.png)","f1c5581f":"<p style = \"font-size:190%;\"> Selecting best features","771ab07c":"# Context\n\n<p style = \"font-size:120%;\"> Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n    \n<p style = \"font-size:120%;\"> People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n    \n[Link of the dataset](https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction)","0bff7e91":"<p style = \"font-size:190%;\">  Validation of this model","94c1f060":"# Pre-processing for machine learning","7447bc8f":"<p style = \"font-size:190%;\"> Score of this algorithm","9c3082cf":"<p style = \"font-size:190%;\"> Separing the data ","ba963604":"# Libraries","e131bafa":"<p style = \"font-size:190%;\"> Creating machine learning model","46b09436":"<p style = \"font-size:190%;\"> Creating baseline"}}