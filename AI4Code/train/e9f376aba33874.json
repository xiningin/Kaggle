{"cell_type":{"55667137":"code","85b6ec18":"code","874490fc":"code","33edc1c6":"code","f720c7c4":"code","67eae832":"code","c7fd6d13":"code","0c233add":"code","2ff36394":"code","a61e9e22":"code","53dfb7d9":"code","ed2e59af":"code","30674f1f":"code","cf7cd002":"code","b49e98ba":"code","167589cd":"code","ba921db5":"code","3bb6636f":"code","ad7146eb":"code","d7460798":"markdown","bb07348d":"markdown","1ed9b47b":"markdown","4cbb5ecf":"markdown","4b19f880":"markdown","61d366b4":"markdown","70a0df38":"markdown","80a0df7f":"markdown","5772b086":"markdown","f1364d5f":"markdown","f973171a":"markdown","5d45fe0e":"markdown","783c3641":"markdown","cf3d5608":"markdown","010606a6":"markdown"},"source":{"55667137":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","85b6ec18":"#First, we load the train and test datasets.\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission_df = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","874490fc":"df = pd.concat([train, test], axis = 0)\ndf = df.set_index('PassengerId')\ndf.info()","33edc1c6":"df = df.drop(['Cabin'], axis = 1)","f720c7c4":"import seaborn as sns\nsns.set_theme(font_scale = 1.5)\nsns.displot(data = df, hue = 'Sex', x = 'Age', row = 'Survived', col = 'Pclass')\n","67eae832":"#This creates the child and adult categories.\ninds = df[df['Age'] > 17].index\ndf.loc[inds, 'Kind'] = 'Adult'\ninds = df[df['Age'] <= 17].index\ndf.loc[inds, 'Kind'] = 'Child'\n\n#This creates a survival plot for the two age groupings, broken up further by ticket class and sex.\nsns.catplot(data = df, hue = 'Sex', x = 'Kind', row = 'Survived', col = 'Pclass', kind = 'count')","c7fd6d13":"df['Embarked'] = df['Embarked'].fillna('S')\nsns.catplot(data = df, hue = 'Sex', x = 'Embarked', col = 'Survived', kind = 'count')","0c233add":"sns.catplot(data = df, hue = 'Pclass', x = 'Embarked', col = 'Survived', kind = 'count')","2ff36394":"sns.catplot(data = df, x = 'Fare', col = 'Pclass', row = 'Survived', kind = 'violin', cut = 0, bw = .2)","a61e9e22":"sns.catplot(x = \"Fare\", y = \"Embarked\", hue = \"Sex\", row = \"Pclass\", data = df, orient = \"h\", height = 3, aspect = 3, kind=\"violin\", dodge=True, cut=0, bw=.2)","53dfb7d9":"df = df.drop(['Fare', 'Ticket', 'Name'], axis = 1)","ed2e59af":"def fillna_using_knn_imputer(df):\n    from sklearn.impute import KNNImputer\n    import pandas as pd\n    # initialize imputer:\n    imputer = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')\n    # use integer coding on the string cols to be filled:\n    df['Sex'] = df['Sex'].factorize()[0]\n    df['Embarked'] = df['Embarked'].factorize()[0]\n    df['Kind'] = df['Kind'].factorize()[0]\n    #  divide to train, test and drop survived so it won't be filled:\n    X_train = df[~df['Survived'].isnull()].drop('Survived', axis = 1)\n    X_test = df[df['Survived'].isnull()].drop('Survived', axis = 1)\n    # fit_transform on X_train and transform on test:\n    X_train_trans = pd.DataFrame(imputer.fit_transform(X_train), columns = X_train.columns, index = X_train.index)\n    X_test_trans = pd.DataFrame(imputer.transform(X_test), columns = X_test.columns, index = X_test.index)\n    dff = pd.concat([X_train_trans, X_test_trans], axis = 0)\n    dff['Survived'] = df['Survived']\n    dff = dff.sort_index()\n    return dff\n\ndf = fillna_using_knn_imputer(df)\ndf.info()","30674f1f":"def encode_cols(df, cols = ['Pclass', 'Embarked']):\n    import pandas as pd\n    for col in cols:\n        dumm = pd.get_dummies(data = df[col], prefix = col)\n        df = pd.concat([df, dumm], axis = 1)\n        df = df.drop(col, axis = 1)\n    return df\n\n\ndef scale_all_features(df):\n    from sklearn.preprocessing import MinMaxScaler\n    import pandas as pd\n    X = df.drop('Survived', axis = 1)\n    scaler = MinMaxScaler()\n    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index = X.index)\n    df = pd.concat([X, df['Survived']], axis = 1)\n    return df, scaler\n\ndf = encode_cols(df)\ndf, _ = scale_all_features(df)\ndf.info()","cf7cd002":"def split_to_train_test_X_y(df):\n    X = df[~df['Survived'].isna()].drop('Survived', axis=1)\n    y = df[~df['Survived'].isna()]['Survived']\n    X_test = df[df['Survived'].isna()].drop('Survived', axis=1)\n    return X, y, X_test\nX, y, X_test = split_to_train_test_X_y(df)","b49e98ba":"class ML_Classifier_Switcher(object):\n\n    def pick_model(self, model_name):\n        \"\"\"Dispatch method\"\"\"\n        self.param_grid = None\n        method_name = str(model_name)\n        # Get the method from 'self'. Default to a lambda.\n        method = getattr(self, method_name, lambda: \"Invalid ML Model\")\n        return method()\n\n    def SVM(self):\n        from sklearn.svm import SVC\n        import numpy as np\n        self.param_grid = {'kernel': ['rbf', 'sigmoid', 'linear'],\n                           'C': np.logspace(-2, 2, 10),\n                           'gamma': np.logspace(-5, 1, 14)}\n\n        return SVC()\n\n    def XGR(self):\n        import numpy as np\n        from xgboost import XGBClassifier\n        self.param_grid = {'gamma': np.logspace(-5, 1, 7),\n                           'subsample': [0.5, 0.75, 1.0],\n                           'colsample_bytree': [0.5, 0.75, 1.0],\n                           'eta': [0.1, 0.5, 0.9],\n                           'max_depth': [3, 5]}\n        return XGBClassifier(random_state=42, nthread=7, use_label_encoder=False,\n                             eval_metric='error', tree_method = \"hist\")\n\n    def RF(self):\n        from sklearn.ensemble import RandomForestClassifier\n        # import numpy as np\n        self.param_grid = {\n            'n_estimators': [50, 100, 200, 300],\n            'max_features': ['auto'],\n            'criterion': ['entropy'],\n            'max_depth': [5, 10],\n            'min_samples_split': [5],\n            'min_samples_leaf': [1]\n        }\n\n        return RandomForestClassifier(random_state=42, n_jobs=-1)\n\n    def LR(self):\n        from sklearn.linear_model import LogisticRegression\n        self.param_grid = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n                           'penalty': ['l2'],\n                           'C': [100, 10, 1.0, 0.1, 0.01]}\n        return LogisticRegression(n_jobs=None, random_state=42)\n\n    def KNN(self):\n        from sklearn.neighbors import KNeighborsClassifier\n        self.param_grid = {\n            'n_neighbors': list(range(1, 5)),\n            'weights': ['uniform', 'distance'],\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'leaf_size': list(range(1, 10)),\n            'p': [1, 2]\n        }\n        return KNeighborsClassifier()","167589cd":"def cross_validate(X, y, model_name='RF', cv=5, scoring='accuracy',\n                   gridsearch=True):\n    \"\"\"Options for model_name are : RF, LR, KNN and SVM\"\"\"\n    import pandas as pd\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import GridSearchCV\n    # we instantiate an ML model switcher:\n    switcher = ML_Classifier_Switcher()\n    model = switcher.pick_model(model_name)\n    # we do gridsearch in the HP space and pick the best model:\n    if gridsearch:\n        gr = GridSearchCV(model, switcher.param_grid,\n                          scoring=scoring, cv=cv, n_jobs=-1)\n        gr.fit(X, y)\n        model = gr.best_estimator_\n    # we do another CV with the best model and report the scores via cvr dataframe:\n    cvr = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n    cvr = pd.DataFrame(cvr).T\n    cvr.index = [model_name]\n    cvr.columns = ['fold_{}'.format(x+1) for x in cvr.columns]\n    cvr['mean'] = cvr.mean(axis=1)\n    cvr['std'] = cvr.std(axis=1)\n    return cvr, model","ba921db5":"def cross_validate_models(X, y, cv=5, scoring='accuracy'):\n    import pandas as pd\n    models = ['LR', 'KNN', 'SVM', 'RF']\n    cvrs = []\n    bests = []\n    for model_name in models:\n        print('Optimizing {} model'.format(model_name))\n        cvr, best = cross_validate(X, y, model_name=model_name, cv=cv, scoring=scoring,\n                                   gridsearch=True)\n        cvrs.append(cvr)\n        bests.append(best)\n        cvr = pd.concat(cvrs, axis=0)\n    return cvr, bests","3bb6636f":"cvr, bests = cross_validate_models(X,y)\nprint(cvr)","ad7146eb":"def predict_on_test(X_test, model, submission_df, target='Survived'):\n    preds_test = model.predict(X_test)\n    submission_df.loc[:, target] = [int(x) for x in preds_test]\n    submission_df.to_csv('submission.csv', index=False)\n    return\npredict_on_test(X_test, bests[-1], submission_df)","d7460798":"It definitely seems like children fared better than adults, although other than that, its hard to see a correlation between age and survival. It's possible that Age still mattered at a theoretical level (for non-children, lower age should be associated with a greater physical condition, improving survival likelihood, although the relationship will run in the reverse for children, since small children are presumably less physically capable than teenagers).","bb07348d":"Running the cross-validate loop for all models:","1ed9b47b":"Does the Fare variable provide any additional information beyond that provided by the ticket class? It potentially does, since it appears that fare variation was much more complex than one price each for each ticket class. For the second and third class passengers, there doesn't appear to be any clear relationship between the fare that they paid and their survival odds. Among the first class passengers, however, it appears that for a cluster of extremely expensive ticket outliers (around $500), every single member of this group survived. It's not clear from this data what this corresponds to and why it would provide a survival advantage, it's possible that these rooms were close to lifeboats or these passengers were given especially favorable treatment by the crew during the disaster. However, investigating these observations in a spreadsheet finds that they are no more than a handful of people, at least some of whom were related and in the same cabin. So we don't have enough diverse observations in this fare range to ascribe any greater survival odds to those with extremely high fares. The variable will therefore be dropped from the dataset, as it does not appear to provide any information.","4cbb5ecf":"The data has now been scaled using MinMaxScaler. Now to proceed with modeling.","4b19f880":"Everything looks good above. Now to proceed with preprocessing before modeling:","61d366b4":"As we can see, very few second and first class children perished. So the variable adds some informational value. \nNow, let's look if any patterns jump out relating to Embarked.","70a0df38":"From the graph above, it appears like there might be a small cluster of low-cost fares paid in Southhampton for journeys to either of the other two cities which followed it, but it's not clear. At this point, I believe it is reasonable to move ahead with dropping the Fare variable. Furthermore, now would be a good time to drop the other two string variables that I don't intend to use for this model. I can envision ways of using those variables, but it's a little too complicated for a first pass at this competition.","80a0df7f":"As you can see, Age and Cabin have a lot of missing values. I thought about dropping Embarked since, per some Internet research, there were only a total of 29 passengers who boarded at one of the earlier two embarkment points who disembarked prior to the Titanic venturing into the open Atlantic towards New York. I had hoped there might be some way to distinguish these 29 people within the dataset, (for example, I speculated that the short-distance passengers might have payed lower fares, but I couldn't find any good evidence of this when I examined the Fare column.) but it seems I cannot, but I will leave it in the model, since it is possible that the variable captures some other theoretically significant information (for example, it might correlate with nationalities or ethnicities that were less likely to survive the accident, something the Wikipedia article on the disaster alludes to.)\n\nA quick spreadsheet examination reveals no obvious pattern to the individuals with missing values for Age (which could create a bias), and the full normal range of Age variation appears to be represented. (approx. 0-80)\n\nSince the bulk of passengers appeared to have embarked at Southhampton, I will arbitrarily fill in the two NaN's for Embarked with 'S' (with only two missing values, this shouldn't bias things much)\n\nIt might be possible to engineer deck numbers (which would be theoretically significant, since the lower-alphabetical decks were closer to the lifeboats) from the Cabin variable, but there are simply too many NaN's such that trying to do this would likely add more noise than information. Thus, the Cabin variable will be dropped.","5772b086":"Interestingly, it appears that only third-class passengers boarded at Queentown. Second-class passengers overwhelmingly boarded at Southhampton, and first-class passengers were largely divided in boarding between Cherbourg and Southhampton. In other words, it appears that boarding location is capturing some latent variable(s) that may not be captured by ticket class, and which could be theoretically significant. So we will leave it in.","f1364d5f":"Adapted from the submission of Shlomi Ziskin Ziv.","f973171a":"Let's test various models success at predicting survival.","5d45fe0e":"Before I move on, however, I am curious to see how fares varied by departure point. Presumably, shorter distance journeys should be less expensive. Let's try that below:","783c3641":"Defining the cross-validation function:","cf3d5608":"Now, let's check for missing values. We concatenate the data frames, and then get a summary of the resulting data frame. NaN's in the \"Survived\" column will be used to distinguish the test set from the training set.","010606a6":"It appears that RF is the best model with about %82.72 accuracy. Let's produce the predictions and submit them."}}