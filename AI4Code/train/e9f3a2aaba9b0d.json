{"cell_type":{"87819922":"code","4fdf4384":"code","57d2a653":"code","31c932ca":"code","6cf740ab":"code","d453d3ad":"code","70c23237":"code","1fca5614":"code","e03d0fd6":"code","1fc3163b":"code","0669d9e1":"code","6c4c56b9":"code","c4995b6f":"code","7f82dbd2":"code","0eea0e3f":"code","f093d520":"markdown","c9a1f79f":"markdown","1b6af20e":"markdown","009dd3a8":"markdown","9a9ae513":"markdown","ba04c0ac":"markdown","e785dc86":"markdown","acf14858":"markdown","26b6f034":"markdown","63d61d70":"markdown","832defb9":"markdown","e843a722":"markdown","b98f0602":"markdown","9af6a4a9":"markdown","1d8307ad":"markdown"},"source":{"87819922":"import numpy as np\nimport pandas as pd\nfrom skimage.color import rgb2gray\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import ndimage\nimport os\nimport sys\nimport random\nimport math\nimport numpy as np\nimport skimage.io\nimport matplotlib\nimport matplotlib.pyplot as plt","4fdf4384":"train_sample_metadata = pd.read_json('..\/input\/deepfake-detection-challenge\/train_sample_videos\/metadata.json').T\ntrain_sample_metadata.head()","57d2a653":"train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\nplt.show()","31c932ca":"import cv2\n\nVIDEO_STREAM = \"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/ytddugrwph.mp4\"\n#VIDEO_STREAM_OUT = \"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/Result.mp4\"\n\nvidcap = cv2.VideoCapture(VIDEO_STREAM)\ndef getFrame(sec):\n    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n    hasFrames,image = vidcap.read()\n    if hasFrames:\n        cv2.imwrite(\"image\"+str(count)+\".jpg\", image) # save frame as JPG file\n        plt.imshow(image)\n    return hasFrames\nsec = 0\nframeRate = 0.5 #\/\/it will capture image in each 0.5 second\ncount=1\nsuccess = getFrame(sec)\nwhile success:\n    count = count + 1\n    sec = sec + frameRate\n    sec = round(sec, 2)\n    success = getFrame(sec)","6cf740ab":"!pip install face_recognition","d453d3ad":"import face_recognition\nimport cv2 as cv\nimport os\nimport matplotlib.pylab as plt\nfrom PIL import Image\ntrain_dir = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/'\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\n# video_file = train_video_files[30]\nvideo_file = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4'\ncap = cv.VideoCapture(video_file)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release() \nface_locations = face_recognition.face_locations(image)\n\n# https:\/\/github.com\/ageitgey\/face_recognition\/blob\/master\/examples\/find_faces_in_picture.py\nprint(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n\nfor face_location in face_locations:\n\n    # Print the location of each face in this image\n    top, right, bottom, left = face_location\n    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n    # Access the actual face itself:\n    face_image = image[top:bottom, left:right]\n    fig, ax = plt.subplots(1,1, figsize=(5, 5))\n    plt.grid(False)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n    ax.imshow(face_image)","70c23237":"from PIL import Image, ImageDraw\n\nfig, axs = plt.subplots(19, 2, figsize=(10, 80))\naxs = np.array(axs)\naxs = axs.reshape(-1)\ni = 0\npad = 60 #pad is addded to the plot to zoom out of the face\nfor fn in train_sample_metadata.index[:24]:\n    label = train_sample_metadata.loc[fn]['label']\n    orig = train_sample_metadata.loc[fn]['label']\n    video_file = f'\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/{fn}'\n    ax = axs[i]\n    cap = cv.VideoCapture(video_file)\n    success, image = cap.read()\n    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n    face_locations = face_recognition.face_locations(image)\n    if len(face_locations) > 0:\n        # Print first face\n        face_location = face_locations[0]\n        top, right, bottom, left = face_location\n        face_image = image[top-pad:bottom+pad, left-pad:right+pad]\n        ax.imshow(face_image)\n        ax.grid(False)\n        ax.title.set_text(f'{fn} - {label}')\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        # Find landmarks\n        face_landmarks_list = face_recognition.face_landmarks(face_image)\n        if len(face_landmarks_list) > 0:\n            face_landmarks = face_landmarks_list[0]\n            pil_image = Image.fromarray(face_image)\n            d = ImageDraw.Draw(pil_image)\n            for facial_feature in face_landmarks.keys():\n                d.line(face_landmarks[facial_feature], width=2, fill='yellow')\n            landmark_face_array = np.array(pil_image)\n            ax2 = axs[i+1]\n            ax2.imshow(landmark_face_array)\n            ax2.grid(False)\n            ax2.title.set_text(f'{fn} - {label}')\n            ax2.xaxis.set_visible(False)\n            ax2.yaxis.set_visible(False)\n            i += 2\nplt.grid(False)\nplt.tight_layout()\nplt.show()","1fca5614":"%%capture\n# Install facenet-pytorch\n%pip install \/kaggle\/input\/facenet-pytorch-vggface2\/facenet_pytorch-1.0.1-py3-none-any.whl\n\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p \/tmp\/.cache\/torch\/checkpoints\/\n!cp \/kaggle\/input\/facenet-pytorch-vggface2\/20180402-114759-vggface2-logits.pth \/tmp\/.cache\/torch\/checkpoints\/vggface2_DG3kwML46X.pt\n!cp \/kaggle\/input\/facenet-pytorch-vggface2\/20180402-114759-vggface2-features.pth \/tmp\/.cache\/torch\/checkpoints\/vggface2_G5aNV2VSMn.pt","e03d0fd6":"import os\nimport glob\nimport torch\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n# See github.com\/timesler\/facenet-pytorch:\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu' #checks if GPY is being used or the CPU\nprint(f'Running on device: {device}')\ntorch.cuda.get_device_name(0)","1fc3163b":"# Load face detector\nmtcnn = MTCNN(device=device).eval()\n\n# Load facial recognition model\nresnet = InceptionResnetV1(pretrained='vggface2', num_classes=2, device=device).eval()","0669d9e1":"# Get all test videos\nfilenames = glob.glob('\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/*.mp4')\n\n# Number of frames to sample (evenly spaced) from each video\nn_frames = 10\n\nX = []\nwith torch.no_grad():\n    for i, filename in enumerate(filenames):\n        print(f'Processing {i+1:5n} of {len(filenames):5n} videos\\r', end='')\n        \n        try:\n            # Create video reader and find length\n            v_cap = cv2.VideoCapture(filename)\n            v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            # Pick 'n_frames' evenly spaced frames to sample\n            sample = np.linspace(0, v_len - 1, n_frames).round().astype(int)\n            imgs = []\n            for j in range(v_len):\n                success, vframe = v_cap.read()\n                vframe = cv2.cvtColor(vframe, cv2.COLOR_BGR2RGB)\n                if j in sample:\n                    imgs.append(Image.fromarray(vframe))\n            v_cap.release()\n            \n            # Pass image batch to MTCNN as a list of PIL images\n            faces = mtcnn(imgs)\n            \n            # Filter out frames without faces\n            faces = [f for f in faces if f is not None]\n            faces = torch.stack(faces).to(device)\n            \n            # Generate facial feature vectors using a pretrained model\n            embeddings = resnet(faces)\n            \n            # Calculate centroid for video and distance of each face's feature vector from centroid\n            centroid = embeddings.mean(dim=0)\n            X.append((embeddings - centroid).norm(dim=1).cpu().numpy())\n        except KeyboardInterrupt:\n            raise Exception(\"Stopped.\")\n        except:\n            X.append(None)","6c4c56b9":"bias = -0.4\nweight = 0.068235746\n\nsubmission = []\nfor filename, x_i in zip(filenames, X):\n    if x_i is not None and len(x_i) == 10:\n        prob = 1 \/ (1 + np.exp(-(bias + (weight * x_i).sum())))\n    else:\n        prob = 0.6\n    submission.append([os.path.basename(filename), prob])","c4995b6f":"submission = pd.DataFrame(submission, columns=['filename', 'label'])\nsubmission.sort_values('filename').to_csv('submission.csv', index=False)","7f82dbd2":"plt.hist(submission.label, 20)\nplt.show()\nsubmission","0eea0e3f":"# to be continued....","f093d520":"# 6. Baseline submission using Facenet\n\n#### Baseline code is taked from this kernel: https:\/\/www.kaggle.com\/climbest\/facial-recognition-model-in-pytorch-change-bias","c9a1f79f":"# 1. Imports","1b6af20e":"## Locating a face within an image","009dd3a8":"# 5. Display test examples and labels","9a9ae513":"# 2. Review of Data Files","ba04c0ac":"# Deepfake Detection","e785dc86":"# 7. Process test videos","acf14858":"### Install dependencies","26b6f034":"# 4. Face Detection\n\nSource: https:\/\/www.kaggle.com\/robikscube\/kaggle-deepfake-detection-introduction\n","63d61d70":"### Imports","832defb9":"# 8. Predict classes","e843a722":"## From video to frames\nIn **cv2.VideoCapture(VIDEO_STREAM)**, we just have to mention the video name with it\u2019s extension. \n\nYou can set frame rate which is widely known as fps (frames per second). Here I set 0.5 so it will capture a frame at every 0.5 seconds, means 2 frames (images) for each second.\n\nIt will save images with name as **image1.jpg**, **image2.jpg** and so on.","b98f0602":"# 3. Recognizing people in a video stream","9af6a4a9":"# 9. Output","1d8307ad":"## Create MTCNN and Inception Resnet models"}}