{"cell_type":{"319b35c4":"code","d2595cba":"code","3c4dda23":"code","2d4b7d69":"code","2da04b2a":"code","f8810a74":"code","e1962ec2":"code","718cd420":"code","2f3ab83e":"code","74ca55a3":"code","f4a8b8a4":"code","fc63adb5":"code","7935d5b5":"code","5adb0004":"code","a2574ac4":"code","b0897e75":"code","5a2a4b3d":"code","e21fba55":"code","627b1102":"code","703928d6":"code","cb579fd8":"code","02093246":"code","2ca379f5":"code","148be0e4":"code","4c0a55a8":"code","1f141bec":"code","df8d703e":"code","e0eb07e2":"code","fff18bb6":"code","def48760":"code","ff17195a":"code","0acb04d2":"code","639f987b":"code","ac957fe9":"code","553bb6ef":"code","3b2ec4fb":"code","f0af764f":"code","545403f0":"code","ce7805bd":"code","ce8ba079":"code","4ed66353":"code","c451539c":"code","7a54d94b":"code","0e7e1c47":"code","3b0cb780":"code","fb132b4b":"code","76b2295c":"code","8b0226d4":"code","2b6688a9":"code","8993117f":"code","76bbbc98":"code","f71e6463":"code","fb5ea803":"markdown","ace322bf":"markdown","40ebbe9a":"markdown","25b69ac1":"markdown","a3424d87":"markdown","e24f8caf":"markdown","dface298":"markdown","a6b5c459":"markdown","a44ed742":"markdown","a882a087":"markdown","d6c409d7":"markdown","c67022f3":"markdown","733c098a":"markdown","2b512b79":"markdown","f3b3d8e7":"markdown","d11a533a":"markdown","8ee124d3":"markdown","7cb4d104":"markdown","15bc5a03":"markdown","3da1bf8e":"markdown","81ebf8ee":"markdown","277e45d6":"markdown","e2401838":"markdown","bfe421b9":"markdown","4835c5cb":"markdown","b4d56b5f":"markdown","7519cad4":"markdown","79bd966f":"markdown","c330fdf6":"markdown","e72f1776":"markdown","e13aeb67":"markdown","344d6df8":"markdown","78e81eed":"markdown","7ed89b04":"markdown","0fc6ddc0":"markdown","9d6d272d":"markdown","d43e6241":"markdown","f6ee5e60":"markdown","8dd6ac26":"markdown","f465f3b4":"markdown","4dcf02fc":"markdown","d96bc9a4":"markdown","41dc586d":"markdown","da1d934d":"markdown","36685c2f":"markdown","98519b3b":"markdown","c9afe972":"markdown","0821738b":"markdown","f4cfd9c2":"markdown","37abe02f":"markdown","6396cb49":"markdown","0072e8c7":"markdown","c858a128":"markdown","9ec3acf9":"markdown","3f872ce5":"markdown","a1353a01":"markdown","659ba780":"markdown","adbc50fd":"markdown"},"source":{"319b35c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport re\nimport string\nimport collections\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom time import time\n%matplotlib inline\nimport os\nimport pandas as pd\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport spacy\nimport spacy.cli\nfrom spacy.matcher import Matcher \nfrom spacy.matcher import PhraseMatcher\n\nspacy.cli.download(\"en\")\nspacy.cli.download(\"en_core_web_lg\")\nnlp = spacy.load('en_core_web_lg')","d2595cba":"## set english loanguage\nstop_words = set(stopwords.words('english'))\n\n## declaration of Porter stemmer.\nporter=PorterStemmer()\n\n## Clean Null Record in dataframe\ndef cleanEmptyData(columnName,df):\n    return df[df[columnName].notnull()]\n\n## Remove Punctuation\ndef remove_punctuation(columnName,df):\n    return df.loc[:,columnName].apply(lambda x: re.sub('[^a-zA-z\\s]','',x))\n\n## Convert To Lower Case\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str  \n\n## Remove duplicate item in the dataframe\ndef removeDuplicate(df,list):\n    df.drop_duplicates(list, inplace=True)    \n\n## Remove nlp stop words    \ndef remove_stop_words(columnName,df):\n  return df.loc[:,columnName].apply(lambda x: [word for word in x.split() if word not in stop_words])\n\n##Remove single character from the sentence\ndef remove_one_character_word(columnName,df):\n  return df.loc[:,columnName].apply(lambda x: [i for i in x if len(i) > 1])\n\n## Join as a single text with seperator\ndef join_seperator(columnName,df):\n  seperator = ', '\n  return df.loc[:,columnName].apply(lambda x: seperator.join(x))\n\n## apply stemmer to data frame fields\ndef apply_stemmer(columnName,df):\n  return df.loc[:,columnName].apply(lambda x: [porter.stem(word) for word in x])\n\n## Data Cleaning Process function\ndef dataCleaningProcess(dataFrame):\n    ## remove duplicate records\n    removeDuplicate(dataFrame,['abstract', 'text_body'])\n    \n    ## clean null value records\n    clean_data = cleanEmptyData('text_body',dataFrame)\n    clean_data.loc[:,'text_body_clean'] = clean_data.loc[:,'text_body'].apply(lambda x: lower_case(x))\n    \n    ## removing punctuation \n    clean_data.loc[:,'text_body_clean'] = remove_punctuation('text_body_clean',clean_data)\n    \n    ## apply stop words\n    clean_data.loc[:,'text_body_clean'] = remove_stop_words('text_body_clean',clean_data)\n    \n    ## apply stemmer for each tokens\n    clean_data.loc[:,'text_body_clean'] = apply_stemmer('text_body_clean',clean_data)\n    \n    ## removing single charter word in the sentence\n    clean_data.loc[:,'text_body_clean'] = remove_one_character_word('text_body_clean',clean_data)\n    \n    ## join as a single text from words token\n    clean_data.loc[:,'text_body_clean'] = join_seperator('text_body_clean',clean_data)\n    \n    ## remove coma after join\n    clean_data.loc[:,'text_body_clean'] = remove_punctuation('text_body_clean',clean_data)\n    \n    return clean_data","3c4dda23":"## get words token from text\ndef getWordsFromText(_text):\n    words = []\n    for i in range(0,len(_text)):\n        words.append(str(_text.iloc[i]['text_body']).split(\" \"))\n    return words\n\n# Read Excel data as Data Frame\ndef readExcelToDataFrame(path):\n    research_dataframe = pd.read_csv(path,index_col=False)\n    research_dataframe.drop(research_dataframe.columns[research_dataframe.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n    return research_dataframe\n\n## basic scatter plot\ndef showScatterPlot(_X,title):\n    # sns settings\n    sns.set(rc={'figure.figsize':(15,15)})\n    # colors\n    palette = sns.color_palette(\"bright\", 1)\n    # plot\n    sns.scatterplot(_X[:,0], _X[:,1], palette=palette)\n    plt.title(title)\n    # plt.savefig(\"plots\/t-sne_covid19.png\")\n    plt.show()\n\n## scatter plot with cluster\ndef showClusterScatterPlot(_X, _y_pred, title):\n    # sns settings\n    sns.set(rc={'figure.figsize':(10,10)})\n    # colors\n    palette = sns.color_palette(\"bright\", len(set(_y_pred)))\n    # plot\n    sns.scatterplot(_X[:,0], _X[:,1], hue=_y_pred, legend='full', palette=palette)\n    plt.title(title)\n    # plt.savefig(\"plots\/t-sne_covid19_label.png\")\n    plt.show()\n\n\n## drop clumns\ndef getTargetData(dataFrame):\n    text_body = dataFrame.drop([\"doc_id\", \"source\", \"title\", \"abstract\"], axis=1)\n    return getWordsFromText(text_body)\n\n## train model for tSNE clustering visualization\ndef trainEmbededData(_perplexity,dataFrame,total_cluster, _n_iter):\n    ## convert text to word frequency vectors\n    vectorizer = TfidfVectorizer(max_features=2**12)\n    \n    ## training the data and returning term-document matrix.\n    _X = vectorizer.fit_transform(dataFrame['text_body_clean'].values)\n    \n    ## tsne declartion\n    tsne = TSNE(verbose=1, perplexity=_perplexity,learning_rate=200, random_state=0, n_iter=_n_iter)\n    _X_embeded = tsne.fit_transform(_X.toarray())\n    \n    ## clusterring for tsne\n    _kmeans = MiniBatchKMeans(n_clusters=total_cluster)\n    return _X_embeded,_kmeans,_X\n\n## predicting cluster centers and predict cluster index for each sample\ndef predict(_kmeans,_X):\n    return _kmeans.fit_predict(_X)\n\n## reusable fucntion for TSNE K-Mean Clustering with TF-IDF\ndef analyse(pplexity,data_frame,cluster,iter):\n    ## train model for tSNE clustering visualization\n    embeded,kmeans,x = trainEmbededData(pplexity,data_frame,cluster,iter)\n    pred = predict(kmeans,x)\n    ## visualized the scatter plot\n    showClusterScatterPlot(embeded,pred,'t-SNE Covid-19 - Clustered(K-Means) - Tf-idf with Plain Text')\n    return embeded,kmeans,x","2d4b7d69":"research_dataframe = readExcelToDataFrame('\/kaggle\/input\/coviddata4\/data.csv')\nresearch_dataframe.head()","2da04b2a":"clean_data =dataCleaningProcess(research_dataframe)","f8810a74":"clean_data.head()\nclean_process_data = clean_data.drop([\"doc_id\", \"source\", \"title\", \"abstract\"], axis=1)\nclean_process_data.head(20)","e1962ec2":"meta_data = readExcelToDataFrame('\/kaggle\/input\/covidmeta\/meta.csv')\nmeta_data.head()","718cd420":"def prepare_search_data(_meta_data_frame,research_dataframe):\n    ## add a field doc_id\n    _meta_data_frame[\"doc_id\"] = _meta_data_frame[\"sha\"]\n    \n    ## clean NUll record\n    _meta_data_frame = cleanEmptyData('doc_id', _meta_data_frame)\n    _meta_data_frame = cleanEmptyData('publish_time', _meta_data_frame)\n    \n    ## select only 2019 & 2020 published records\n    meta_data_filter = _meta_data_frame[_meta_data_frame['publish_time'].str.contains('2019') | _meta_data_frame['publish_time'].str.contains('2020')]  \n    \n     ## clean NUll record\n    research_dataframe_clean = cleanEmptyData('doc_id', research_dataframe)\n    research_dataframe_clean = cleanEmptyData('text_body', research_dataframe_clean)\n    \n    ## merging of Research data and meta data on doc_id\n    tmp_data_frame  = research_dataframe_clean.merge(meta_data_filter, on='doc_id', how='right')\n    \n    ## remove un used fields\n    clean_process_data = tmp_data_frame.drop([\"source\", \"abstract_x\",  \"abstract_x\",\"sha\",\"source_x\",\"title_y\",\"pmcid\",\"pubmed_id\",\"license\",\"abstract_y\",\"journal\",\"Microsoft Academic Paper ID\",\"WHO #Covidence\"], axis=1)\n    \n    ## clean NUll record\n    clean_process_data = cleanEmptyData('text_body', clean_process_data)\n    clean_process_data = clean_process_data.rename(columns={'title_x': 'title'}) \n    \n    # reordering the column index\n    columns = [\"doc_id\",\"doi\", \"publish_time\", \"authors\",\"url\",\"title\", \"text_body\"]\n    clean_process_data = clean_process_data.reindex(columns=columns)\n    \n    return clean_process_data","2f3ab83e":"def process_title(x):\n  if not str(x['title_x']).lower() =='nan':\n    return str(x['title_x']) + ' (' +  str(x['url']) + ')'\n  else:\n    return str(x['url'])","74ca55a3":"filter_data = prepare_search_data(meta_data,research_dataframe)\nfilter_data.head()","f4a8b8a4":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n","fc63adb5":"def show_WordCloud(filter_data):\n    comment_words = ' '\n    stopwords = set(STOPWORDS) \n  \n# iterate through the csv file \n    for val in filter_data: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n        #print(val) \n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n         comment_words = comment_words + words + ' '\n         #print(comment_words)\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white',\n                    max_words = 200, \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (10, 10), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n\n","7935d5b5":"show_WordCloud(filter_data.title)","5adb0004":"embeded,kmeans,x = analyse(5000,clean_process_data,10,15000)","a2574ac4":"papers = research_dataframe['text_body'].astype('str')\nlen(papers)\npapers.head()\n#meta_data_filter\u00a0=\u00a0_meta_data_frame[_meta_data_frame['publish_time'].str.contains('2019')\u00a0|\u00a0_meta_data_frame['publish_time'].str.contains('2020')]\u00a0\u00a0 ","b0897e75":"%%time\nimport nltk\nimport tqdm\nnltk.download('wordnet')\n\nstop_words = nltk.corpus.stopwords.words('english')\nwtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\nwnl = nltk.stem.wordnet.WordNetLemmatizer()\n\ndef normalize_corpus(papers):\n    norm_papers = []\n    for paper in tqdm.tqdm(papers):\n        paper = paper.lower()\n        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n        paper_tokens = list(filter(None, paper_tokens))\n        #if paper_tokens:\n        norm_papers.append(paper_tokens)\n            \n    return norm_papers\n    \nnorm_papers = normalize_corpus(papers)\nprint(len(norm_papers))","5a2a4b3d":"import gensim\n\nbigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') # higher threshold fewer phrases.\nbigram_model = gensim.models.phrases.Phraser(bigram)\n\nprint(bigram_model[norm_papers[0]][:50])","e21fba55":"print(bigram_model[norm_papers[1]][:50])","627b1102":"norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n\n# Create a dictionary representation of the documents.\ndictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\nprint('Sample word to number mappings:', list(dictionary.items())[:15])\nprint('Total Vocabulary Size:', len(dictionary))","703928d6":"# Filter out words that occur less than 20 documents, or more than 60% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.6)\nprint('Total Vocabulary Size:', len(dictionary))","cb579fd8":"bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\nprint(bow_corpus[1][:50])","02093246":"print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])","2ca379f5":"print('Total number of papers:', len(bow_corpus))","148be0e4":"import joblib\nlda_model = joblib.load('\/kaggle\/input\/coviddata41\/lda_model.jl')","4c0a55a8":"topics_assigned = lda_model[bow_corpus]","1f141bec":"len(topics_assigned)","df8d703e":"b= pd.DataFrame(topics_assigned,columns = ['T0','T1','T2','T3','T4','T5','T6','T7','T8','T9'])","e0eb07e2":"d= pd.concat([research_dataframe['text_body'],b],axis=1)","fff18bb6":"d.to_csv(\"Topic_paper_07042020_v4.csv\")","def48760":"for topic_id, topic in lda_model.print_topics(num_topics=50, num_words=20):\n    print('Topic #'+str(topic_id+1)+':')\n    print(topic)\n    print()","ff17195a":"import numpy as np\ntopics_coherences = lda_model.top_topics(bow_corpus, topn=20)\navg_coherence_score = np.mean([item[1] for item in topics_coherences])\nprint('Avg. Coherence Score:', avg_coherence_score)","0acb04d2":"topics_with_wts = [item[0] for item in topics_coherences]\nprint('LDA Topics with Weights')\nprint('='*50)\nfor idx, topic in enumerate(topics_with_wts):\n    print('Topic #'+str(idx+1)+':')\n    print([(term, round(wt, 3)) for wt, term in topic])\n    print()","639f987b":"print('LDA Topics without Weights')\nprint('='*50)\nfor idx, topic in enumerate(topics_with_wts):\n    print('Topic #'+str(idx+1)+':')\n    print([term for wt, term in topic])\n    print()","ac957fe9":"cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n                                                      texts=norm_corpus_bigrams,\n                                                      dictionary=dictionary, \n                                                      coherence='c_v')\navg_coherence_cv = cv_coherence_model_lda.get_coherence()\n\numass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n                                                         texts=norm_corpus_bigrams,\n                                                         dictionary=dictionary, \n                                                         coherence='u_mass')\navg_coherence_umass = umass_coherence_model_lda.get_coherence()\n\nperplexity = lda_model.log_perplexity(bow_corpus)\n\nprint('Avg. Coherence Score (Cv):', avg_coherence_cv)\nprint('Avg. Coherence Score (UMass):', avg_coherence_umass)\nprint('Model Perplexity:', perplexity)","553bb6ef":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=2000,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","3b2ec4fb":"with open(\"\/kaggle\/input\/coviddata41\/Topic_paper_07042020_v4.csv\",encoding = 'utf8', errors='ignore') as f:\n    df_topic = pd.read_csv(f)\n    \n#df = df.replace(\"\\n\",\" \").dropna()","f0af764f":"show_WordCloud(df_topic.loc[df_topic['Dominant_topic'] == 0]['text_body'])\n#show_WordCloud(filter_data.title)","545403f0":"show_WordCloud(df_topic.loc[df_topic['Dominant_topic'] == 1]['text_body'])","ce7805bd":"\nfrom __future__ import print_function\n\n__author__ = 'maxim'\n\nimport numpy as np\nimport gensim\nimport string\nfrom gensim.models import Word2Vec\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.utils.data_utils import get_file\nfrom gensim.models import Word2Vec\nimport pandas as pd\nimport numpy as np\nimport re\nimport json\nimport pandas as pd\nfrom gensim.models.fasttext import FastText\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport re\n\nimport gensim\nimport nltk","ce8ba079":"print('loading model')\n\nword_model = Word2Vec.load(\"\/kaggle\/input\/coviddata41\/word2vec_1000ITR.model\")\n\nprint('model loaded')","4ed66353":"print(word_model.most_similar(positive=['covid19'])) ","c451539c":"print(word_model.most_similar(positive=['treatment','options' ])) ","7a54d94b":"\narr_most_similar = [['covid19','medicine','treatment'],['covid19','medicine','treatment','chuanxiong'],['covid19','medicine','treatment','lopinavirritonavir','ribavirin'],['covid19','antivirals','remdesivir']]\narr_predict=[['viral','inhibitors','replication']]     \n\n#Question: 9     \n#Public health mitigation measures that could be effective for control\n#Intial input fed to word2vec  model to extract related words that could lead to the answers of this question\n#['public','health','mitigation','measures',  'effective', 'control','covid19','disease']]\n#RESULT from first Query: [('interventions', 0.7447171807289124), ('prevention', 0.7438710927963257), ('preventive', 0.7241473197937012), ('policies', 0.7131460905075073), ('intervention', 0.7103219628334045), ('management', 0.7056314945220947), ('implementing', 0.6939526200294495), ('policy', 0.6897070407867432), ('quarantine', 0.6833001971244812), ('planning', 0.6516326069831848), ('implementation', 0.641217827796936), ('awareness', 0.6297034025192261), ('containment', 0.6278428435325623), ('preparedness', 0.622099757194519), ('epidemic', 0.6220937967300415), ('timely', 0.6204564571380615), ('community', 0.6184203624725342), ('government', 0.6134730577468872), ('outbreak', 0.6104072332382202), ('pandemic', 0.6079082489013672)]]\n#Updated input taken from first querying of word2vec model after choosing relevant keywords\n#['mitigation','measures','control','covid19','quarantine','containment','awareness','policies']]\n#RESULT from second Query: [[('interventions', 0.7805880308151245), ('intervention', 0.7159266471862793), ('policy', 0.690924346446991), ('preventive', 0.6865450143814087), ('implementing', 0.6757345795631409), ('implementation', 0.6531403064727783), ('planning', 0.6507831811904907), ('practices', 0.6400246620178223), ('prevention', 0.6383914947509766), ('management', 0.6378570199012756), ('government', 0.6369956731796265), ('preparedness', 0.6348874568939209), ('restrictions', 0.6139740943908691), ('campaigns', 0.6061151027679443), ('behaviors', 0.5989149808883667), ('plans', 0.5976110696792603), ('decisions', 0.5968020558357239), ('timely', 0.591980516910553), ('governmental', 0.5905696153640747), ('biosecurity', 0.5887465476989746)]]\n\n#Most Similar Keywrods Detection \n\nlen(arr_most_similar)\narrans=[]\nprint(len(arr_most_similar))\ncount=0\nfor i in arr_most_similar:\n    print('--------->',i)\n    answers=word_model.most_similar(positive=i,topn=30)\n    \n    arrans.append(answers)\n    count +=1\n    print('=========',count)\nprint(arrans)\nprint(len(arrans))\n\n\n#Predicted Keywords Detection\n\narr_predict\nlen(arr_predict)\narrans_arr_predict=[]\nprint(len(arr_predict))\ncount=0\nfor j in arr_predict:\n    print('--------->',j)\n    answers1=word_model.predict_output_word(j,topn=30)\n    \n    arrans_arr_predict.append(answers1)\n    count +=1\n    print('=========',count)\nprint(arrans_arr_predict)\nprint(len(arrans_arr_predict))\n\n","0e7e1c47":"for q in arrans:\n    print(q)","3b0cb780":"#arrans\nfor k in arrans_arr_predict:\n    print(k)","fb132b4b":"## constant for spliting sentence\nalphabets= \"([A-Za-z])\"\nprefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = \"[.](com|net|org|io|gov)\"","76b2295c":"\n \n## spliting to sentence from text\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n    text = re.sub(websites,\"<prd>\\\\1\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    if \"\u201d\" in text: text = text.replace(\".\u201d\",\"\u201d.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n\n## search inference text by key words and return all the matches sentence\ndef search_inference_keys(text, keywords):\n    sentences = split_into_sentences(text)\n    txt = ''\n    for sent in sentences:\n        r = re.compile(keywords,flags=re.IGNORECASE)\n        if len(r.findall(sent))>0:         \n            txt = str(txt) + str(sent)\n    return txt\n\n\n## check key words exist or not in a sentence\ndef check_exist_multiple_keywords(text, keywords):\n    r = re.compile(keywords, flags=re.IGNORECASE)\n    if len(r.findall(text))>0:    \n        return True\n    else:\n        return False\n\n## Search Inference and download results as excel \ndef searc_by_keys_as_excel(keyword, src_data_frame):\n    data_frame = src_data_frame\n    ## check exist to slice down the related contents\n    data_frame['search_key_status'] =data_frame.loc[:,'text_body'].apply(lambda x: check_exist_multiple_keywords(x,keyword))\n    ## select only target data\n    process_data_frame = data_frame.query('search_key_status == True')\n  \n    ## filter on corona and covid 19 related data\n    process_data_frame['search_covid_content'] =process_data_frame.loc[:,'text_body'].apply(lambda x: check_exist_multiple_keywords(x,'covid-19|sars-cov-2|2019-ncov|ncov-19|coronavirus'))\n  \n    ## get only covid-19|sars-cov-2|2019-ncov|ncov-19|coronavirus data\n    process_data_frame = process_data_frame.query('search_covid_content == True')\n    process_data_frame.loc[:,'inference'] = process_data_frame.loc[:,'text_body'].apply( lambda x: search_inference_keys(x,keyword))\n    \n    ## remove unused fields\n    final_data = process_data_frame.drop([\"search_key_status\",\"text_body\"], axis=1)\n    ## download as excel\n    final_data.to_excel(str(keyword) + '_result.xlsx', sheet_name='keyword')\n  \n    return final_data","8b0226d4":"# Search Inference for \"incubation period\" and download results as excel \nsearch_data = prepare_search_data(meta_data,research_dataframe)","2b6688a9":"final_data =searc_by_keys_as_excel('protease|inhibitor|',search_data)\n\n\nfinal_data.head()","8993117f":"final_data =searc_by_keys_as_excel('ribavirin|chloroquine',search_data)\n","76bbbc98":"final_data =searc_by_keys_as_excel('ribavirin|chloroquine',search_data)","f71e6463":"final_data =searc_by_keys_as_excel('remdesivir|lopinavir|ritonavir|oseltamivir|favipiravir|sofosbuvir|corticosteroids',search_data)","fb5ea803":"We perform some basic text wrangling or preprocessing before diving into topic modeling. We keep things simple here","ace322bf":"# PROS and CONS\n\nPROS:\n\n1. Good understanding and iference of the research papers through TSNE and LDA models\n2. Well Trained word embedding models\n3. Accurate keyword extraction with emamples demonstrated with results\n\nCONS:\n\n1. Manual intervention to establish the relationship of search algorithm to derive answers to the questions.","40ebbe9a":"**Loading data(a)-Complete data by reading uploaded CSV** | CSV file is created by parsing the JSON data","25b69ac1":"Visualise Cleaned Data and removing columns","a3424d87":"The Visualization clearly talks about the : **coronavirus, transmission, infection, vaccine, ourbreak etc.. **\nGiving a clear picture of the terminalogies and informations that can be retrived from the research papers.","e24f8caf":"LOAD the Research Dataset","dface298":"**Key HighLight in Search**\n1. **Clinical and bench trials to investigate less common viral inhibitors  against COVID-19**\n\n2020-02-04\tGurjit S. Randhawa; Maximillian P.M. Soltysiak; Hadi El Roz; Camila P.E. de Souza; Kathleen A. Hill; Lila Kari\thttps:\/\/doi.org\/10.1101\/2020.02.03.932350\tMachine learning using intrinsic genomic signatures for rapid classification of novel pathogens: COVID-19 case study\tZAP targets 425 CG dinucleotide sequences, and in vertebrate host cells with the CG suppression in host 426 genomes, this can serve as a mechanism for the distinction of self vs non-self RNA and 427 inhibitory consequences [81] .Due to the high amino acid similarities between 438 COVID-19 and SARS-CoV main protease essential for viral replication and processing, 439 anticoronaviral drugs targeting this protein and other potential drugs have been 440 identified using virtual docking to the protease for treatment of 441 COVID-19 [29, 44, 45, [91] [92] [93] [94] .\n\n2020-02-29\tXiaoyan Liu; Zhe Li; Shuai Liu; Zhanghua Chen; Zhiyao Zhao; Yi-you Huang; Qingling Zhang; Jun Wang; Yinyi Shi; Yanhui Xu; Jing Sun; Huifang Xian; Rongli Fang; Fan Bai; Changxing Ou; Bei Xiong; Andrew M Lew; Jun Cui; Hui Huang; Jincun Zhao; Xuechuan Hong; Yuxia Zhang; Fulin Zhou; Hai-Bin Luo\thttps:\/\/doi.org\/10.1101\/2020.02.27.20027557\tTherapeutic effects of dipyridamole on COVID-19 patients with 1 coagulation dysfunction\tTo date, no agents have been reported to be effective After viral entry to the host cells, the coronavirus messenger RNA is first translated to yield the 94 polyproteins, which are subsequently cleaved by two viral proteinases, 3C-like protease (3CLP, aka 95 nsp5 or Mpro) and papain-like protease (PLP, or nsp3), to yield non-structural proteins essential for 96 viral replication.21 Inhibitors that suppress the activity of these proteases may inhibit viral replication 97 and offer a revenue for the HCoV-19 therapy.32 Third, as a pan-PDE inhibitor, DIP may prevent 105 acute injury and progressive fibrosis of the lung, heart, liver, and kidney.Free energy perturbation and surface plasmon resonance (SPR) assay 114 We virtually screened an FDA-approved drug database using the HCoV-19 protease Mpro as a target 115 and validated the binding affinity by the SPR assay.20027557 doi: medRxiv preprint 229 We virtually screened an FDA approved drug library and found that DIP bound to the HCoV-19 230 protease Mpro (Figure S1 ).\n\n2020-03-08\tWioletta Rut; Katarzyna Groborz; Linlin Zhang; Xinyuanyuan Sun; Mikolaj Zmudzinski; Rolf Hilgenfeld; Marcin Drag\thttps:\/\/doi.org\/10.1101\/2020.03.07.981928\tSubstrate specificity profiling of SARS-CoV-2 M pro protease provides basis for anti-COVID-19 drug design\t[5] The main protease (M pro , also known as 3CL pro ), is one of coronaviral nonstructural proteins (Nsp5) designated as a potential target for drug development.[6] [7] Due to the close phylogenetic relationship between SARS-CoV-2 and SARS-CoV, [2, 8] their main proteases share many structural and functional features.From the perspective of the design and synthesis of new M pro inhibitors, a key feature of both of the enzymes is their ability to cleave the peptide bond following Gln.The SARS-CoV M pro cleaves polyproteins within the Leu-Gln?(Ser, Ala, Gly) sequence (? indicates the cleavage site), which appears to be a conserved pattern of this protease.[6a, 7, 9] The ability of peptide bond hydrolysis after Gln residues is also observed for main proteases of other coronaviruses [10] but is unknown for human enzymes.This observation, along with further studies on the M pro , can potentially lead to new broad-spectrum anti-coronaviral inhibitors with minimum side effects.[11] In the present study, we applied the HyCoSuL (Hybrid Combinatorial Substrate Library) approach to determine the full substrate specificity profile of SARS-CoV M pro and SARS-CoV-2 M pro proteases.The use of natural and a large number of unnatural amino acids with diverse chemical structures allowed an in-depth characterization of the residue preference of the binding pockets within the active sites of the proteases.Moreover, results from our studies clearly indicate that SARS-CoV M pro and SARS-CoV-2 M pro proteases exhibit overlapping substrate specificity.[9, 12] The imidazole of His163, located at the very bottom of the S1 proteases, which explains less stringent specificity.Then, we measured the rate of substrate hydrolysis relevant to each protease (Figure 3) .In summary, we established substrate specificity profiles at the P4-P2 positions of the SARS-CoV M pro and SARS-CoV-2 M pro proteases using a combinatorial approach.Information provided here can be used for the design of inhibitors and activity-based probes against the SARS-CoV-2.\n\n2020-02-27\tZhenyu Fan; Liping Chen; Jun Li; Cheng Tian; Yajun Zhang; Shaoping Huang; Zhanju Liu; Jilin Cheng\thttps:\/\/doi.org\/10.1101\/2020.02.26.20026971\tClinical Features of COVID-19-Related Liver Damage\tThe medications of COVID-19 patients before admission included antibiotics (Levofloxacin, Azithromycin, Cephalosporin), antiviral drugs (Arbidol, Oseltamivir, Acyclovir)and conventional febrifuge (Ibuprofen).In this study, the drugs used by patients before admission are mainly antibacterial drugs (including moxifloxacin, cephalosporins), antiviral drugs (abidol, oseltamivir, acyclovir), and antipyretic drugs with acetaminophen.\n\n2020-03-10\tAnjue Tang; Wenhui Xu; min shen; Peifen Chen; Guobao Li; Yingxia Liu; Lei Liu\thttps:\/\/doi.org\/10.1101\/2020.03.08.20029710\tA retrospective study of the clinical characteristics of COVID-19 infection in 26 children\tMedicines for treating COVID-19 include oseltamivir, ribavirin, interferon, kaletra and traditional Chinese medicine.\n\n**Analysis of DRUGS\/Antivirals providing Clinical Impact**\n\n\n1. **Corticosteroids**\n\n Corticosteroids are used to prevent lung injury for patients with Pneumonia and is being put to trial in nCov-2019. \n Below are some facts Low dose Corticosteroids is being recommended\n Chinese Government Recommends Short treatment in Corticosteriods\n\n **Corticosteroids usage in low doses**\n\n10.1186\/s40169-020-00271-z\t2020-02-20\tShe, Jun; Jiang, Jinjun; Ye, Ling; Hu, Lijuan; Bai, Chunxue; Song, Yuanlin\thttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7033263\/\t\n\nA trial has been initiated quickly to assess the efficacy and safety of remdesivir in patients hospitalized with 2019-nCoV infection.As the cytokine storm was observed in severe 2019-nCoV infection patients, low dose corticosteroids has been used to treat the patients for possible benefit by reducing inflammatory-induced lung injury.However, corticosteroids did not reduce the mortality for SARS-CoV and MERS-CoV infection by WHO interim guidance [34, 35] .\n\n **Low dose is being recommended**\n  \n 10.1186\/s13054-020-2786-x\t2020\tLiu, Yong; Li, Jinxiu; Feng, Yongwen\thttps:\/\/doi.org\/10.1186\/s13054-020-2786-x\n\n Low-dose systematic corticosteroids, lopinavir\/ritonavir, and atomization inhalation of interferon were encouraged.\n\n  **Detailed study of low dose and precautions are available below**\n\n 10.1038\/s41392-020-0127-9\t2020\tGao, Wei Zhou; Yisi, Liu; Dongdong, Tian; Cheng, Wang; Sa, Wang; Jing, Cheng; Ming, Hu; Minghao, Fang; Yue\thttps:\/\/doi.org\/10.1038\/s41392-020-0127-9\t\n\nRemarkably, systematic corticosteroids treatment (methylprednisolone, <1-2 mg per kg body weight, for 3-5 days) was recommended to be an adjuvant therapy 3 , which immediately raised concerns about whether patients infected with this novel coronavirus could benefit from corticosteroids therapy 4 .Some scholars may not support the corticosteroids treatment for novel coronavirus pneumonia (NCP), because observational studies and systematic reviews have indicated inconclusive clinical evidence on the effect of corticosteroids therapy for viral pneumonias (such as SARS, MERS and H1N1).Additionally, pulse-dose therapy or long-term administration to high dose of corticosteroids in early stage were reported to be possibly harmful [6] [7] [8] .However, these conclusions obscured the clinical benefits of corticosteroids on some subgroups of patients, particularly those with severe symptoms, as the clinical effects might be related to the indication (severities of illness), the timing of intervention, the dose and duration of corticosteroids therapy 9 .Of note, as documented in a series of randomized clinical trials (RCT), low or physiologic dose of corticosteroids treatment did not reduce mortality from septic shock caused by primary lung infections, but it could bring clinical benefits to secondary outcomes, such as earlier reversal of shock, shorter duration to exit from ICU and mechanical ventilation 9, 10 .Besides, salvage corticosteroids treatment for severe patients with advanced ARDS could alleviate the pulmonary fibrosis and prevent progressive pathological deterioration 11 , which provides a good framework for explaining why some critical patients with SARS infection benefit from rescue corticosteroids therapy.More importantly, mortality benefit favored the severe HIN1-illness in the adjunctive treatment group with low dose of corticosteroids 12 .Evidently, all these results strongly suggest that proper use of low-dose corticosteroids may bring survival advantages for critically ill patients with 2019-nCoV, but this treatment should be strictly performed on NCP patients with definite clinical indications (such as refractory ARDS, sepsis or septic shock) according to the recommended guidelines.Over the past month, we collaborated with front-line ICU physicians and firstly evaluated the efficacy of corticosteroids treatment for severe or fatal cases with 2019-nCoV infection in Wuhan.According to the guidelines, corticosteroids therapy (median hydrocortisoneequivalent dose of 400.7% (7\/15), closer to that after adjustment for time-varying confounders induced by critically ill patients with MERS without corticosteroids treatment 6 , suggesting that corticosteroids might not improve ICU mortality in critical NCP patients.But meanwhile, systematic corticosteroids therapy in the first 3-5 days could enhance oxygen saturation (SaO 2 ) and arterial oxygen tension (PaO 2 )\/inspiratory oxygen fraction (FiO 2 ), both of which could be further augmented by collaborating with invasive mechanical ventilation (IMV) (Fig.Nevertheless, corticosteroids treatment in the phase of ARDS would effectively inhibit furious inflammatory storm (Fig.1b) and gain valuable time for controlling infection and preventing secondary multiorgan damage and shock, which implies that corticosteroids have synergistic biological effects when combined with other intensivists' treatment against severe or fatal NCP patients.Due to the deficiency of sample size and a matched control group, we could not easily draw an accurate conclusion about the role of corticosteroids in patients with 2019-nCoV by now.However, our clinical experience and available descriptive data from the therapeutic process of the first 15 critical NCP patients are prone to support corticosteroids treatment for specific subgroup of critically ill patients with 2019-nCoV.There is no fixed clinical guideline for the use of corticosteroids in critically ill patients in ICU.The anecdotal experience from SARS and sCAP therapy strongly supports precise corticosteroids management of NCP.Personalized medicine strategy should contain, but not limited to, specific indications, timing and duration, as well as therapeutic monitoring of corticosteroids therapy.As mentioned above, corticosteroids should be avoided unless there are indications for moderate or severe ARDS, sepsis or septic shock, in part consistent with the recommended clinical guidance from World Health Organization (WHO).We also do not suggest the use of corticosteroids for mild or early-stage ARDS, because early corticosteroids application could delay the clearance of virus and increase mortality risk, and corticosteroids are more likely to function on inflammation-mediated lung injury and interstitial fibro-proliferation at late-stage of ARDS 11 .Furthermore, clinical adverse complications in SARS patients with corticosteroids treatment have been reported to be dose-related.Hence, lower dose and short duration of corticosteroids treatment (methylprednisolone, <1 mg\/kg body weight, no more than 7 days), along with adverse drug reaction monitoring, would be more beneficial in clinical management of critical patients with 2019-nCoV.We endorse the potential benefits from low-dose corticosteroids treatment in a subset of critically ill patients with 2019-nCoV based on existing studies and clinical experience, despite there is no significant improvement in overall survival.Certainly, our ongoing welldesigned prospective cohort study with sufficient samples may provide systematic answers to this clinical dilemma-\"to use or not to use corticosteroids for the treatment of lung injury with 2019-nCoV\"-in the near future.\n\n**Chinese Government Recommends Short treatment in Corticosteriods**\n2020-03-06\tChengfeng Qiu; Qian Xiao; Xin Liao; Ziwei Deng; Huiwen Liu; Yuanlu Shu; Dinghui Zhou; Ye Deng; Hongqiang Wang; Xiang Zhao; Jianliang Zhou; Jin Wang; Zhihua Shi; Long Da\thttps:\/\/doi.org\/10.1101\/2020.03.04.20026005\tTransmission and clinical characteristics of coronavirus disease 2019 in 104 outside-Wuhan patients, China\n\n103 of 104 patients of this study received one or more antiviral drugs, including Lopinavar\/ ritonavir, interferon a atomization and Abidol.Lopinavar\/ ritonavir was proved to be substantial clinical benefit against SARS 17 19, 20 .Evidence suggests corticosteroids did not decrease the mortality of patients with SARS and MERS, but rather delayed the clearance of viral 19, 21 .Chinese guideline recommends a short treatment of corticosteroids in server NCP 7 This study has several limitations.\n  \n**2. Remdesivir**\n\n **Studies are going on to check Remdesivir could be a potential antiviral for Covid-19**\n **Evidence of patients in US recovered from 2019-nCov with Remdesivir**\n \n\n 10.3346\/jkms.2020.35.e89\t2020\tLim, Jaegyun; Jeon, Seunghyun; Shin, Hyun Young; Kim, Moon Jung; Seong, Yu Min; Lee, Wang Jun; Choe, Kang Won; Kang, Yu Min; Lee, Baeckseung; Park, Sang Joon\thttps:\/\/doi.org\/10.3346\/jkms.2020.35.e89\t\n1 Our report focused on the decrease of virus loads and the alleviation of the patient's symptoms during lopinavir\/ritonavir (LPV\/r) administration.Favipiravir, ribavirin, remdesivir and galidesivir could be good candidates as potential antiviral agents for the treatment.\n\n10.1097\/cm9.0000000000000777\t2020\tWang, Jian-Wei; Cao, Bin; Wang, Chen\thttps:\/\/doi.org\/10.1097\/cm9.0000000000000777\tScience in the fight against the novel coronavirus disease\n\nAt the time of preparation of this manuscript, the Chinese Academy of Medical Sciences and the China-Japan Friendship Hospital had launched a multi-center, randomized, double-blind, placebocontrolled clinical trial in Wuhan to test the effectiveness of remdesivir as an antiviral drug against 2019-nCoV, [12, 13] and studies have already shown that chloroquine phosphate is an effective treatment for COVID-19.\n\n\n10.1101\/2020.03.02.20030452\t2020-03-06\tGuqin Zhang; Chang Hu; Linjie Luo; Fang Fang; Yongfeng Chen; Jianguo Li; Zhiyong Peng; Huaqin Pan\thttps:\/\/doi.org\/10.1101\/2020.03.02.20030452\tClinical features and outcomes of 221 patients with COVID-19 in Wuhan, China\n\n20030452 doi: medRxiv preprint oseltamivir, arbidol hydrochloride, a-interferon atomization inhalation, and lopinavir\/ritonavir), (4) Extracorporeal membrane oxygenation (ECMO) support was used for the patients with refractory hypoxemia that is difficult to be corrected by prone and protective lung ventilation strategy.4 28 In addition, a case report showed that remdesivir, an adenosine analogue, has shown survival benefits in one severe COVID-19 pneumonia patient 29 .30 Now a couple of clinical trials focus on the efficacy of remdesivir, as well as other therapeutic strategies, such as immunoglobulins, Vitamin C infusion, mesenchymal stem cell treatment, arbidol hydrochloride plus interferon atomization, ritonavir combined with oseltamivir, lopinavir plus ritonavir and arbidol hydroxychloroquine and methylprednisolone.\n\n10.1101\/2020.02.27.20029009\t2020-02-29\tYing Huang; Rui Yang; Ying Xu; Ping Gong\thttps:\/\/doi.org\/10.1101\/2020.02.27.20029009\tClinical characteristics of 36 non-survivors with COVID-19 in Wuhan, China\tTRUE\t\n\n22%) patients received antiviral treatment, including oseltamivir, ganciclovir, ribavirin or umifenovir hydrochloride.Clinical trials on promising regimens for COVID-19, such as remdesivir, lopinavir, and chloroquine phosphate are ongoing, which shed light on conquering the COVID-19 epidemic 12 .\n\n10.1038\/s41422-020-0282-0\t2020-02-04\tWang, Manli; Cao, Ruiyuan; Zhang, Leike; Yang, Xinglou; Liu, Jia; Xu, Mingyue; Shi, Zhengli; Hu, Zhihong; Zhong, Wu; Xiao, Gengfu\thttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7054408\/\t\n\nNotably, two compounds remdesivir (EC 50 = 0.7 Our time-ofaddition assay showed remdesivir functioned at a stage post virus entry (Fig.showed that in NHP model, intravenous administration of 10 mg\/kg dose of remdesivir resulted in concomitant persistent levels of its active form in the blood (10 \u00b5M) and conferred 100% protection against Ebola virus infection.7 Our data showed that EC 90 value of remdesivir against 2019-nCoV in Vero E6 cells was 1.S2 ) showed that remdesivir also inhibited virus infection efficiently in a human cell line (human liver cancer Huh-7 cells), which is sensitive to 2019-nCoV.b Immunofluorescence microscopy of virus infection upon treatment of remdesivir and chloroquine.c and d Time-of-addition experiment of remdesivir and chloroquine.Our findings reveal that remdesivir and chloroquine are highly effective in the control of 2019-nCoV infection in vitro.\n\n3. **Lopinavir-Ritonavir**\n\n There are evidences that the above drug has given clinical benefits.\n \n **Evidence of usage of Korea and Recovery**\n\n 10.3346\/jkms.2020.35.e61\t2020-02-03\tKim, Jin Yong; Choe, Pyoeng Gyun; Oh, Yoonju; Oh, Kyung Joong; Kim, Jinsil; Park, So Jeong; Park, Ji Hye; Na, Hye Kyoung; Oh, Myoung-don\thttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7008073\/\t\n\nAfter the diagnosis of 2019-nCoV infection was made, lopinavir 400 mg\/Ritonavir 100 mg was given from January 21, 2020 (day 4 of illness). Fever persisted for ten days with a maximum temperature of 38.9\u00b0C on day 7 of illness, and then subsided on January 28, 2019 (day 11 of illness). From January 31, 2019 (day 14 of illness), her dyspnea began to improve, reducing oxygen requirement, and the lung lesions also began to diminish in chest radiography.\n\n\n **Lopinavir-Ritonavir is predicted to block Covid-19 multiplication**\n\n10.1101\/2020.01.31.929695\t2020-02-03\tShen Lin; Runnan Shen; Jingdong He; Xinhao Li; Xushun Guo\thttps:\/\/doi.org\/10.1101\/2020.01.31.929695\tMolecular Modeling Evaluation of the Binding Effect of Ritonavir, Lopinavir and Darunavir to Severe Acute Respiratory Syndrome Coronavirus 2 Proteases\n \n **Non-Recommendation for Covid-19**\n\n10.1101\/2020.02.26.20026971\t2020-02-27\tZhenyu Fan; Liping Chen; Jun Li; Cheng Tian; Yajun Zhang; Shaoping Huang; Zhanju Liu; Jilin Cheng\thttps:\/\/doi.org\/10.1101\/2020.02.26.20026971\tClinical Features of COVID-19-Related Liver Damage\t\n\t\nAntiviral therapy can be tried with interferon, lopinavir\/litonavir, abidol and dnrunavir.During the stay in hospitalization, the patients received treatment with antibiotics (Levofloxacin, Meropenem, Moxifloxacin, Cephalosporin), interferon and antiviral drugs (Arbidol, Lopinavir\/ritonavir, Dnrunavir).1%) received treatment with lopinavir\/ritonavir compared with those with normal liver function (25%) (p = 0.In this study, the drugs used by patients before admission are mainly antibacterial drugs (including moxifloxacin, cephalosporins), antiviral drugs (abidol, oseltamivir, acyclovir), and antipyretic drugs with acetaminophen.Comparing with the medication of these patients during hospitalization, our study found that the application rate of lopinavir\/ritonavir in liver damage group was significantly higher than that in patients with normal liver function.In view of another article on antiviral effect in our hospital, lopinavir\/ritonavir has no effect on the negative conversion rate of SARS-CoV-2.28 For this reason, we tend not to recommend lopinavir\/ritonavir as a common clinical treatment drug for COVID-19, even in mild patients with normal liver function.More studies are needed to further evaluate the risks and benefits that lopinavir\/ritonavir may bring.SARS-CoV-2 may cause the liver function damage, and the emerging liver injury after admission has some connection with the application of lopinavir\/ritonavir and the extend length of hospital stay.\n\nhttps:\/\/doi.org\/10.1101\/2020.01.29.924100\tPotential inhibitors for 2019-nCoV coronavirus M protease from clinically approved medicines\t\n\nA previous attempt to predict drugs for the M pro of SARS-CoV has identified two HIV-1 protease inhibitors, namely lopinavir and ritonavir, as potential candidates, both of which bind to the same target site of M pro .Virtual docking of lopinavir\/ritonavir to 2019-nCoV M pro also showed high binding ability to the pocket site (Figure 1c) , similar to previous report for SARS-CoV M pro .Amino acids Thr24, Thr26, and Asn119 were predicted to be the key residues for binding the drugs (Figure 1c and Supplementary Figure 1) , forming 2 hydrogen bonds with lopinavir and 2 hydrogen bonds with ritonavir, respectively.Compared to lopinavir\/ritonavir, most of these predicted drugs could form more hydrogen bounds with 2019-nCoV M pro , thus may have higher mutation tolerance than lopinavir\/ritonavir.\n\n\nAs coronaviruses, including SARS-CoV-2, synthesize polyproteins followed by hydrolyzed to produce their structure and function proteins [11] [12] [13] , it is suggested that ritonavir, lopinavir and darunavir may block the multiplication cycle of SARS-CoV-2 by inhibiting its proteases.\n\n**oseltamivir  - Influenza Antivirus drugs- Article results mention no positive effects on the drug for Covid-19**\n\n**favipiravir - No specific positive effects mentioned on the drug for Covid-19**\n\n**sofosbuvir - No specific positive effects mentioned on the drug for Covid-19**\n\n**Chloroquine - Showing Positive Results for Covid-19**\n\n10.1038\/s41422-020-0282-0\t2020-02-04\tWang, Manli; Cao, Ruiyuan; Zhang, Leike; Yang, Xinglou; Liu, Jia; Xu, Mingyue; Shi, Zhengli; Hu, Zhihong; Zhong, Wu; Xiao, Gengfu\thttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7054408\/\t\tTRUE\t\n\n and chloroquine (EC 50 = 1.10 Our time-of-addition assay demonstrated that chloroquine functioned at both entry, and at postentry stages of the 2019-nCoV infection in Vero E6 cells (Fig.Besides its antiviral activity, chloroquine has an immune-modulating activity, which may synergistically enhance its antiviral effect in vivo.The EC 90 value of chloroquine against the 2019-nCoV in Vero Cytotoxicity of these drugs to Vero E6 cells was measured by CCK-8 assays.b Immunofluorescence microscopy of virus infection upon treatment of remdesivir and chloroquine.c and d Time-of-addition experiment of remdesivir and chloroquine.Our findings reveal that remdesivir and chloroquine are highly effective in the control of 2019-nCoV infection in vitro.\n\n\n10.1101\/2020.02.25.965582\t2020-03-02\tMeehyun Ko; So Young Chang; Soo Young Byun; Inhee Choi; Anne-Laure Pham Hung d\u2019Alexandry d\u2019Orengiani; David Shum; Ji-Young Min; Marc P. Windisch\thttps:\/\/doi.org\/10.1101\/2020.02.25.965582\tScreening of FDA-approved drugs using a MERS-CoV clinical isolate from South Korea identifies potential therapeutic options for COVID-19\tTRUE\n\tScreening was conducted in two-times independently (screen 1 and screen 2) using chloroquine as a reference inhibitor at 100 \u00b5M for the maximum inhibitory concentration (IC90 = 93 \u00b5M) (De Wilde et al.78 indicated a good separation of infected cells treated with the dimethyl sulfoxide (DMSO) control and chloroquine (Fig.\n\n10.1101\/2020.02.04.934232\t2020-02-11\tMiyssa I. Abdelmageed; Abdelrahman H. Abdelmoneim; Mujahed I. Mustafa; Nafisa M. Elfadol; Naseem S. Murshed; Shaza W. Shantier; Abdelrafie M. Makhawi\thttps:\/\/doi.org\/10.1101\/2020.02.04.934232\tOriginal Article\tTRUE\t\nAlthough both flu and anti-HIV drugs are used currently in China for treatment of COVID-19, and chloroquine phosphate, an old drug for treatment of malaria, has recently found to have apparent efficacy and acceptable safety against 79] ; nevertheless more studies are required to standardize these therapies.\n\n10.1101\/2020.02.17.951848\t2020-02-18\tRenhong Yan; Yuanyuan Zhang; Yaning Li; Lu Xia; Qiang Zhou\thttps:\/\/doi.org\/10.1101\/2020.02.17.951848\tStructure of dimeric full-length human ACE2 in complex with B 0 AT1\tTRUE\t\nIt has been shown that chloroquine, which can interfere with the terminal glycosylation of ACE2 (31), inhibits SARS-CoV infection.\n\n10.1101\/2020.03.03.20030353\t2020-03-06\tXu Chen; Fang Zheng; Yanhua Qing; Shuizi Ding; Danhui Yang; Cheng Lei; Zhilan Yin; Xianglin Zhou; Dixuan Jiang; Qi Zuo; Jun He; Jianlei Lv; Ping Chen; Yan Chen; Hong Peng; Honghui Li; Yuanlin Xie; Jiyang Liu; Zhiguo Zhou; Hong Luo\thttps:\/\/doi.org\/10.1101\/2020.03.03.20030353\tTitle: Epidemiological and clinical features of 291 cases with coronavirus disease 2019 in areas adjacent to Hubei, China: a double-center observational study\tTRUE\t\nBesides, chloroquine phosphate was reported to have apparent efficacy and acceptable safety against COVID-19 in a multicenter clinical trials 21 and had just been included in the latest edition of the guidelines for China.According to this guideline, two patients in the study were given chloroquine for antiviral therapy.\n\n\n10.1097\/cm9.0000000000000777\t2020\tWang, Jian-Wei; Cao, Bin; Wang, Chen\thttps:\/\/doi.org\/10.1097\/cm9.0000000000000777\tScience in the fight against the novel coronavirus disease\tTRUE\t\n At the time of preparation of this manuscript, the Chinese Academy of Medical Sciences and the China-Japan Friendship Hospital had launched a multi-center, randomized, double-blind, placebocontrolled clinical trial in Wuhan to test the effectiveness of remdesivir as an antiviral drug against 2019-nCoV, [12, 13] and studies have already shown that chloroquine phosphate is an effective treatment for COVID-19.\n\n**Ribavirin - Our results say no Specific Postive effects for Covid-19**\n\n**Chinese Herbs**\n\n**\"Radix\" is being used and recommended in Chinese Medicine. Below are the prescription from Chinese Doctors for Covid-19.**\n\n2020\tJin, Ying-Hui; Cai, Lin; Cheng, Zhen-Shun; Cheng, Hong; Deng, Tong; Fan, Yi-Pin; Fang, Cheng; Huang, Di; Huang, Lu-Qi; Huang, Qiao; Han, Yong; Hu, Bo; Hu, Fen; Li, Bing-Hui; Li, Yi-Rong; Liang, Ke; Lin, Li-Kai; Luo, Li-Sha; Ma, Jing; Ma, Lin-Lu; Peng, Zhi-Yong; Pan, Yun-Bao; Pan, Zhen-Yu; Ren, Xue-Qun; Sun, Hui-Min; Wang, Ying; Wang, Yun-Yun; Weng, Hong; Wei, Chao-Jie; Wu, Dong-Fang; Xia, Jian; Xiong, Yong; Xu, Hai-Bo; Yao, Xiao-Mei; Yuan, Yu-Feng; Ye, Tai-Sheng; Zhang, Xiao-Chun; Zhang, Ying-Wen; Zhang, Yin-Gao; Zhang, Hua-Min; Zhao, Yan; Zhao, Ming-Juan; Zi, Hao; Zeng, Xian-Tao; Wang, Yong-Yan; Wang, Xing-Huan; Management, for the Zhongnan Hospital of Wuhan University Novel Coronavirus; Research Team, Evidence-Based Medicine Chapter of China International Exchange; Promotive Association for, Medical; Health, Care\thttps:\/\/doi.org\/10.1186\/s40779-020-0233-6\t\t\n\nAnd the recommended prescription is the Huoxiang Zhengqi powder (Yin dampness injuring superficies case from the National Famous Traditional Chinese Medical Doctor Medical Cases); which comprises of perilla leaf 10 g, atractylodes lancea 15 g, radix angelicae dahuricae 10 g, dried tangerine or orange peel 10 g, notopterygium root 10 g, agastache rugosus 10 g (end addition), mangnolia officinalis 10 g, saposhnikovia divaricata 10 g, poria peel 15 g, and Tetrapanax papyriferus 10 g above yielded decoction.Its composition comprises of ephedra with honey fried 10 g, almond 10 g, gypsum 20-30 g, periostracum cicada 10 g, bombyx batryticatus 10 g, rhizoma curcumae longae 10 g, rhubarb stir-fried with wine 10 g, scutellaria baicalensis 10 g, coptis chinensis 5 g, phillyrin 15 g, angelica sinensis 10 g, peach kernel 10 g, radix paeoniae rubra 15 g, and rhizome of rehmannia 15 g above yielded decoction.The recommended prescription comprises of rhizoma pinellinae praeparata 9 g, dried tangerine or orange peel 10 g, Codonopsis pilosula 15 g, radix astragali preparata 30 g, poria cocos 15 g, agastache rugosus 10 g, and fructus amomi 6 g (end addition) above yielded decoction.The recommended prescription is Zhuye Shigao decoction with cogongrass rhizome and rhizoma phragmitis; and the composition of this prescription includes bamboo leaf 15 g, gypsum 15 g (predecoction), Codonopsis pilosula 15 g, radix ophiopogonis 10 g, pinellia ternate 9 g, cogongrass rhizome 15-30 g, rhizoma phragmitis 20 g, licorice 10 g, and polished round-grained rice 30 g above yielded decoction.\n","a6b5c459":"**Creation of Word Cloud Topic wise**","a44ed742":"**WORD CLOUD VISUALIZATION **\n\nWe have filtered 2019-2020 research papers to understand COVID-19 data through the word cloud visualization.","a882a087":"In the above word cloud for the topics were mostly about genome of the covid-19 and possibilties of other corona virus genomes and carriers.\n","d6c409d7":"# MODEL BUILDING AND INFERENCE","c67022f3":"# COVID-19 Open Research Dataset (CORD-19) Analysis\n\n![CORD-19.png](attachment:CORD-19.png)\n\n\n*An example of result snippet is shown below, For each tasks a sperate notebook is created with answers to each questions in a task added to a excel*\n\n\n","733c098a":"**Model Evaluation:**\n\nExample 1:\n\n**\" COVID-19 \"** keyword when tested against our word embedding model gave the following results.\n\n[('ncp', 0.7167163491249084), ('wuhan', 0.7067078351974487), ('2019ncov', 0.7065909504890442), ('sarscov2', 0.6876667737960815), ('mers', 0.6260266304016113), ('sars', 0.6118236780166626), ('2020', 0.6101416349411011), ('hubei', 0.5763685703277588), ('mainland', 0.5460340976715088), ('china', 0.531872034072876)]\n\nnotable keywords from the word2vec output are: 'wuhan','sars','china' relating to the orgin of the virus\n\n\nExample 2:\n\nFor Multiple keywords **'treatment','option'** keyword when tested against our word embedding model gave the following results.\n\n[('treatments', 0.88392174243927), ('therapies', 0.7969704866409302), ('therapy', 0.7752498984336853), ('drugs', 0.7263898253440857), ('medications', 0.7093261480331421), ('antivirals', 0.6795921325683594), ('regimens', 0.6594418287277222), ('prophylaxis', 0.653971254825592), ('medication', 0.6348516941070557), ('antibiotics', 0.6138178706169128)]\n\nnotable keywords from the word2vec outpur are : 'prophylaxis','therapies' relating to preventive medications . \n\nAs shown below:","2b512b79":"**Dimensionality Reduction with t-SNE**\n\nUsing t-SNE we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the text_body_clean can be plotted. t-SNE will attempt to preserve the relations of the higher dimensional data as closely as possible when shrunk to 2D\n\nAnalyse with perplexity of 5000, cluster 10, iteration : 15000\n\nThe optimal cluster parameter is decided after clustering the data from different preplexity and clusters and visualized.","f3b3d8e7":"Import libraries","d11a533a":"**LDA TOPIC WITHOUT WEIGHTS**","8ee124d3":"# SOLUTION APPROACH\n\n![covid.JPG](attachment:covid.JPG)","7cb4d104":"Import libraries ","15bc5a03":"As shown in the above steps, the context of the context words 'treatment','options' are well understood by the word2vec model in predicting the target words 'therapy','antivirals','regimens','prophylaxis' which are very relavant in finding the answers to the questions.","3da1bf8e":"Cleaning of corpus helper functions ","81ebf8ee":"Data Preparation included the below process:\n\n* Load Research & Meta Data\n* Meta Data filter for published time from 2019 to 2020 on doc_id\n* Remove unused fields for the search inference.","277e45d6":"**Inference of some topics **\n\n* Topic 1&5: mainly deals with the virus structure and genome \n* Topic 3: deals with the origin and history \n* Topic 4&10: For public health risk information \n* Topic 9: Has lab research on various subjects","e2401838":"Data Cleaning","bfe421b9":"As shown in the above steps, the context of the context words 'covid19' are well understood by the word2vec model in predicting the target words 'wuhan','sars','china' relating to the orgin of the virus which are very relavant in approaching the answers to the questions. ","4835c5cb":"Re usable Helper functions","b4d56b5f":"word2vec model loading, we are using the training model by loading it.","7519cad4":"Build a Bi-gram Phrase Model","79bd966f":"Filtered Data to be used to visualize the word cloud in the upcoming cells to understand the importance of topics and vocabs used.","c330fdf6":"Question 1: -->**Effectiveness of drugs being developed and tried to treat COVID-19 patients.\nClinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.**","e72f1776":"# CONCLUSION","e13aeb67":"Question 2: -Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.","344d6df8":"**LDA TOPIC WITH WEIGHTS**","78e81eed":"Answering our task","7ed89b04":"**KEYWORDS EXTRACTION:**","0fc6ddc0":"**Transforming corpus into bag of words vectors**\n\nWe can now perform feature engineering by leveraging a simple Bag of Words model.","9d6d272d":"Import libraries","d43e6241":"We tried understanding documents with different clusters with various perplexity and identified the optimal perplexity where the convergence of documents took place. ","f6ee5e60":"# VISUALIZATION","8dd6ac26":"**LDA TOPICS WITH TOPIC ID** ","f465f3b4":"**Topic Models with Latent Dirichlet Allocation (LDA)**","4dcf02fc":"*The Keywords extracted through embedding are passed on to the below functions to extarct the list of research articles to further manually pick the articles relevant to the questions to answer the questions.*\n\n**Following are the keywords picked question wise:**\n\n*   protease|inhibitor| \n*   ribavirin|chloroquine \n*   remdesivir|lopinavir|ritonavir|oseltamivir|favipiravir|sofosbuvir|corticosteroids \n\n\nSeperate Excel is created in the Output section for each questions with its possible research articles that can prove the inference made using the word2vec answering approach.","d96bc9a4":"**TASK 4:**\n\nWhat do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\n\nSpecifically, we want to know what the literature reports about:\n\n1. Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n    * Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n2. Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n3. Exploration of use of best animal models and their predictive value for a human vaccine.\n4. Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n5. Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n6. Efforts targeted at a universal coronavirus vaccine.\n7. Efforts to develop animal models and standardize challenge studies\n8. Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n9. Approaches to evaluate risk for enhanced disease after vaccination\n10. Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics] ","41dc586d":"> TOPIC CSV CREATION","da1d934d":"Looks like we have a lot of unique phrases in our corpus of research papers, based on the preceding output. Several of these terms are not very useful since they are specific to a paper. Hence, we will prune our vocabulary and start removing terms.","36685c2f":"Read the meta data from meta_data CSV","98519b3b":"**SEARCH ALGORITHM **\n\nTo extract the relevant research documents from the keywords extarcted ","c9afe972":"**WORD CLOUD FOR THE DOMINANT TOPIC(S) **\n\nExample: Topic 1 can have 20 documents while Topic 2 may have 200 documents in it.Hence a word cloud is drawn to visualize the intersting vocabs involving each Topic","0821738b":"**Evaluating topic model:**\n\nQuality We can use perplexity and coherence scores as measures to evaluate the topic model. Typically, lower the perplexity, the better the model. Similarly, the lower the UMass score and the higher the Cv score in coherence, the better the model.","f4cfd9c2":"***Building model**\n\n> %%time\n> \n> > TOTAL_TOPICS = 10\n> \n> lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, \n>                                    alpha='auto', eta='auto', random_state=42,\n>                                    iterations=500, num_topics=TOTAL_TOPICS, \n>                                    passes=20, eval_every=None)*","37abe02f":"**Topic 2**","6396cb49":"**WORD2VEC MODEL **\n\nWORD2VEC EMBEDDING IS USED TO PREDICT THE TARGET WORD(S) FORM THE CONTEXT WORDS(Questions from task) USING CBOW \n\nIn this COVID-19 challenge the questions form the context words, we leverage the context words from the questions to predict the target words to form the keywords, which inturn will lead us to the answers for the respective context words(questions). we are using the CBOW approach in word2vec to acheive this.","0072e8c7":"**Topic 1**","c858a128":"In the above Questions, Questions 1 AND 4 are attempted using a combination of word2vec embedding to extract the keywords and search algorithm to extract the research articles related to it.\n\nWord2vec embedding provides the keywords that are closest to answer the questions","9ec3acf9":"EXTRACT CSV","3f872ce5":"**Load the LDA Model **","a1353a01":"> In the above word cloud for the topics were mostly about infection samples and detection of virus in a patient","659ba780":"# EDA\n\nWORD CLOUD FOR THE TITLE OF ARTICLES FROM 2019 - 2020 ","adbc50fd":"**Robust Word2Vec Model with Gensim**\n\nThe __`gensim`__ framework, created by Radim \u0158eh\u016f\u0159ek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our covid-19 corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n\n- __`size`:__ The word embedding dimensionality : 100\n- __`window`:__ The context window size : 20\n- __`min_count`:__ The minimum word count : 1\n- __`iter`:__ Iteration : 1000 \n- __`sg`:__ Training model, 1 for skip-gram otherwise CBOW : CBOW\n\nWe have build a  Word2Vec model on the corpus. "}}