{"cell_type":{"f473f675":"code","174495ce":"code","8d88009c":"code","f7c84796":"code","de7e0b8e":"code","df0c6633":"code","4412cc89":"code","fb24db16":"code","5e39559e":"code","9d1dabe6":"code","4c65a313":"code","ef5d51dc":"code","b74576cf":"code","400c1137":"code","f5bbc2a1":"code","6ed072e5":"code","88b11c27":"code","658458a5":"code","dc908d71":"code","19b94ff3":"code","d4432d1e":"code","c72789d8":"code","29d89b7e":"code","5524b6e5":"code","1b1a9b22":"code","689980eb":"code","f93c231c":"code","f7f68e91":"code","653a61a5":"code","5a4c9d17":"code","cc909110":"code","7d920704":"code","ba6de178":"code","952e1309":"code","d690613b":"code","49b7fd22":"code","a1375a12":"markdown","04ceeab5":"markdown","7867836b":"markdown","923cf604":"markdown","7d664af0":"markdown","4e7f7f60":"markdown"},"source":{"f473f675":"#import library\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing CSV I\\O \n#for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV ,cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')\n","174495ce":"#import datasets\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nSalePrice=train_df['SalePrice']\nall_data=pd.concat([train_df.drop(['SalePrice'],axis=1),test_df])","8d88009c":"train_df.shape,test_df.shape","f7c84796":"train_df.head()","de7e0b8e":"train_df.tail()","df0c6633":"test_df.head()","4412cc89":"# statistical information for numerical data\ntrain_df.describe()","fb24db16":"#statistical information for categorical data \ntrain_df.describe(include=['O'])","5e39559e":"all_data.info()","9d1dabe6":"#to show number of Nulls in data\nNulls=all_data.isnull().sum().sort_values(ascending=False)\npercent=all_data.isnull().sum()\/all_data.isnull().count().sort_values(ascending=False)\nmissing_data=pd.concat([Nulls,percent],axis=1,keys=['Nulls','percent'])\nmissing_data.head(30)","4c65a313":"# New, we can drop missing data > 160 nulls\nall_data=all_data.drop(missing_data[missing_data['Nulls']>160].index,1)","ef5d51dc":"g=sns.FacetGrid(train_df)\ng.map(plt.hist,'GarageCond')","b74576cf":"#this code to show number of variable in features\nlists=['GarageCond','GarageType','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2'\n      ,'BsmtCond','BsmtFinType1','BsmtQual','MasVnrType','Electrical'] \nfor i in lists:\n    count=all_data[i].value_counts()\n    print(count)","400c1137":"#in this lession we will fill nulls in the most frequent variables\nlist1=['Exterior1st','Exterior2nd','SaleType','KitchenQual','Functional','GarageCond','GarageType','GarageQual'\n       ,'BsmtExposure','BsmtFinType2','BsmtCond','MasVnrType','Electrical','MSZoning','Utilities']\nfor i in list1:\n    all_data[i]=all_data[i].fillna(all_data[i].mode()[0])","f5bbc2a1":"# drop some features\nlist2=['Id','GarageFinish','BsmtFinType1','BsmtQual','GarageYrBlt']\nall_data=all_data.drop(list2,axis=1)","6ed072e5":"#we can fill nulls with the mean\nlist3=['BsmtFinSF1','GarageCars','TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','GarageArea','MasVnrArea','BsmtFullBath','BsmtHalfBath']\n\nfor i in list3:\n    all_data[list3]=all_data[list3].fillna(all_data[list3].mean().astype(int))","88b11c27":"#Now ,we can show Nulls \nall_data.isnull().sum().max()","658458a5":"#split all_data\ntrain_df=all_data.iloc[:1460,:]\ntrain_df['SalePrice']=SalePrice\ntest_df=all_data.iloc[1460:,:]","dc908d71":"#in this lesson we will explore categorical data and processing with Label Encoder\nS=(train_df.dtypes == 'object')\nobject_cols=list(S[S].index)\nLE=LabelEncoder()\nfor col in object_cols:\n    train_df[col]=LE.fit_transform(train_df[col])\n    test_df[col]=LE.fit_transform(test_df[col])","19b94ff3":"#we will show correlation matrix\ncorrmat=train_df.corr()\nf,ax=plt.subplots(figsize=(12,10))\nsns.heatmap(corrmat,cmap='RdYlGn')","d4432d1e":"corrmat['SalePrice'].sort_values(ascending=False).head(25)","c72789d8":"#scatterplot\nsns.set()\ncols = ['GarageCond','SalePrice', 'GrLivArea','OverallQual', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show();","29d89b7e":"train_df['SalePrice'].skew()","5524b6e5":"train_df['SalePrice'].kurt()","1b1a9b22":"#histogram and normal probability plot\nfrom scipy import stats\n\nsns.distplot(train_df['SalePrice'],fit=stats.norm)\nfig = plt.figure()\nprob=stats.probplot(train_df['SalePrice'],plot=plt)\nplt.show()","689980eb":"#data transformation\ntrain_df['SalePrice']=np.log(train_df['SalePrice'])\n","f93c231c":"#transformed histogram and normal probability plot\nsns.distplot(train_df['SalePrice'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)","f7f68e91":"#histogram and normal probability plot\nsns.distplot(train_df['GrLivArea'],fit=stats.norm)\nfig = plt.figure()\nprob=stats.probplot(train_df['GrLivArea'],plot=plt)\nplt.show()","653a61a5":"#data transformation\ntrain_df['GrLivArea']=np.log(train_df['GrLivArea'])","5a4c9d17":"#transformed histogram and normal probability plot\nsns.distplot(train_df['GrLivArea'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train_df['GrLivArea'], plot=plt)","cc909110":"corr=corrmat['SalePrice'].sort_values(ascending=False).head(25)\nfeatures=corr.index","7d920704":"X=train_df[features.drop('SalePrice')]\ny=train_df['SalePrice']","ba6de178":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX=scaler.fit_transform(X)","952e1309":"lab_enc =LabelEncoder()\ny = lab_enc.fit_transform(y)","d690613b":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X,y, test_size = 0.3, random_state = 101)\n","49b7fd22":"#Random Forest Regressor\nRandomForestModel = RandomForestRegressor(random_state=99)\n\n#USING GRID SEARCH\nmax_depth = [3, 10, 20, 40]\nn_estimators = [10, 50, 100, 200,400]\nparam_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\nRandomForestModel_GS = GridSearchCV(estimator=RandomForestModel, param_grid=param_grid, cv = 10,n_jobs=-1).fit(X_train, y_train)\nRandomForestModel_best = RandomForestModel_GS.best_estimator_\nprint('MLPRegressor Best Parmas',RandomForestModel_GS.best_params_)\n#calculation predict\ny_pred=RandomForestModel_GS.predict(X_val)\nprint('MAE MLPRegressor : ',mean_absolute_error(y_pred,y_val))\n\nprint('='*50)\n\n#MLPRegressor\nfrom sklearn.neural_network import MLPRegressor\nMLPRegressorModel=MLPRegressor()\n#USING GRID SEARCH\nactivation=['solver','identity','tanh','relu']\nsolver=['lbfgs','sgd','adam']\nlearning_rate=['constant','invscaling','adaptive']\nparam_grid=dict(solver=solver,learning_rate=learning_rate)\ngrid_search_MLPRegressor = GridSearchCV(estimator=MLPRegressorModel, param_grid=param_grid, cv = 10,n_jobs=-1).fit(X_train, y_train)\nMLPRegressor_best = grid_search_MLPRegressor.best_estimator_\nprint('MLPRegressor Best Parmas',grid_search_MLPRegressor.best_params_)\n#calculation predict\ny_pred=grid_search_MLPRegressor.predict(X_val)\nprint('MAE Random Forest : ',mean_absolute_error(y_pred,y_val))\n","a1375a12":"OK, we have positive skewness and does not follow the diagonal line.\nto solve this problem in case of positive skewness, log transformations usually works well. \nlearned from  '*Pedro Marcelino, PhD*'","04ceeab5":"# transform categorical variabels","7867836b":"# Split Data","923cf604":"# Missing Data","7d664af0":"# Feature Scaling","4e7f7f60":"# Data Wrangling"}}