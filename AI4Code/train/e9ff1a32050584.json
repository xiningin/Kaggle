{"cell_type":{"755cd845":"code","e6d1219b":"code","ba7be775":"code","70c94d5b":"code","2279a07c":"code","6ba5b098":"code","cfb6e0f2":"code","9880d511":"code","ec182891":"code","56b1639f":"code","8d0e446c":"code","8e4ff70a":"code","59102182":"code","21321357":"code","a699ec61":"code","f1bf28ba":"code","d78be0b0":"code","9e3050ed":"code","627f509d":"code","0944efef":"code","d8adbcaf":"code","2df26617":"code","3b5a7227":"code","297a8c9a":"code","ef5bb3f5":"code","d95af1be":"code","703a0dd5":"code","3f8f050a":"code","2a49a64e":"code","6ed44bdc":"code","efd53c1e":"code","1a231183":"code","98b8e42e":"code","7ee67782":"code","e5a3bb6a":"code","0cd14ead":"code","e13defde":"code","23042246":"code","fd285dc1":"code","db6db2ba":"code","97f433af":"code","e84cc1e0":"code","cb985445":"code","dfa1a579":"code","5c0cdb8d":"code","8732661d":"code","0a741c91":"code","cd27dc63":"code","fe589772":"code","f18f0574":"markdown","5e2ccace":"markdown","5917e045":"markdown","77ad4ca9":"markdown","8e0fc797":"markdown","99c5738f":"markdown","2a587047":"markdown","fe1a8702":"markdown","e148f6d4":"markdown","6637b4fa":"markdown","0a1670b9":"markdown","ba452382":"markdown","38313d12":"markdown","4766f041":"markdown","f9919757":"markdown","874e6462":"markdown","9abc79d6":"markdown","5acc0b00":"markdown","e67b68c1":"markdown","167da012":"markdown","83d2a682":"markdown","1da3a692":"markdown","57965b34":"markdown","3e245fb5":"markdown"},"source":{"755cd845":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport regex as re\nimport gc\n# Any results you write to the current directory are saved as output.","e6d1219b":"baseline_tree_score = 0.23092278864723115\nbaseline_neuralnetwork_score = 0.5480561937041435","ba7be775":"train = pd.read_csv('..\/input\/kaggletutorial\/covertype_train.csv')\ntest = pd.read_csv('..\/input\/kaggletutorial\/covertype_test.csv')","70c94d5b":"train_index = train.shape[0]","2279a07c":"lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.06,\n    \"num_leaves\": 16,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.7,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}","6ba5b098":"def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims\/\/2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', # one may use 'mean_absolute_error' as alternative\n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef keras_history_plot(history):\n    plt.plot(history.history['loss'], 'y', label='train loss')\n    plt.plot(history.history['val_loss'], 'r', label='val loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='upper right')\n    plt.show()","cfb6e0f2":"def baseline_tree_cv(train):\n    train_df = train.copy()\n    y_value = train_df[\"Cover_Type\"]\n    del train_df[\"Cover_Type\"], train_df[\"ID\"]\n    \n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_iteration = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        evals_result_dict = {} \n        dtrain = lgbm.Dataset(train_x, label=train_y)\n        dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n        clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=3000, valid_sets=[dtrain, dvalid],\n                               early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n        predict = clf.predict(valid_x)\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_iteration = max(best_iteration, clf.best_iteration)\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        lgbm.plot_metric(evals_result_dict)\n        plt.show()\n        \n    print(\"Best Iteration\", best_iteration)\n    print(\"Total LogLoss\", total_score \/ NFOLD)\n    print(\"Baseline model Score Diff\", total_score \/ NFOLD - baseline_tree_score)\n    \n    del train_df\n    \n    return best_iteration\n\ndef baseline_keras_cv(train):\n    train_df = train.copy()\n    y_value = train_df['Cover_Type']\n    del train_df['Cover_Type'], train_df['ID']\n    \n    model = keras_model(train_df.shape[1])\n    callbacks = [\n            EarlyStopping(\n                patience=10,\n                verbose=10)\n        ]\n\n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_epoch = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                            verbose=1, callbacks=callbacks)\n\n        keras_history_plot(history)\n        predict = model.predict(valid_x.values)\n        null_count = np.sum(pd.isnull(predict) )\n        if null_count > 0:\n            print(\"Null Prediction Error: \", null_count)\n            predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_epoch = max(best_epoch, np.max(history.epoch))\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        \n    print(\"Best Epoch: \", best_epoch)\n    print(\"Total LogLoss\", total_score\/NFOLD)\n    print(\"Baseline model Score Diff\", total_score\/NFOLD - baseline_neuralnetwork_score)","9880d511":"all_data = pd.concat([train,test])\n\nfig, axes = plt.subplots(ncols=3, figsize=(16,4))\nsns.distplot(all_data['Elevation'],ax=axes[0])\nsns.distplot(np.log1p(all_data['Elevation']),ax=axes[1])\nsns.distplot(np.sqrt(all_data['Elevation']),ax=axes[2])\naxes[0].set_title('Original Distribution')\naxes[1].set_title('Log Transform')\naxes[2].set_title('Power Transform')\nplt.show()\n\nfig, axes = plt.subplots(ncols=3, figsize=(16,4))\nsns.distplot(all_data.loc[all_data['Cover_Type']==0, 'Elevation'],ax=axes[0])\nsns.distplot(all_data.loc[all_data['Cover_Type']==1, 'Elevation'],ax=axes[0])\nsns.distplot(np.log1p(all_data.loc[all_data['Cover_Type']==0, 'Elevation']),ax=axes[1])\nsns.distplot(np.log1p(all_data.loc[all_data['Cover_Type']==1, 'Elevation']),ax=axes[1])\nsns.distplot(np.sqrt(all_data.loc[all_data['Cover_Type']==0, 'Elevation']),ax=axes[2])\nsns.distplot(np.sqrt(all_data.loc[all_data['Cover_Type']==1, 'Elevation']),ax=axes[2])\n\naxes[0].set_title('Original Distribution')\naxes[1].set_title('Log Transform')\naxes[2].set_title('Power Transform')\nplt.show()","ec182891":"del all_data['oil_Type']\nall_data['Elevation'] = np.log1p(all_data['Elevation'])","56b1639f":"category_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)\n    \ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]","8d0e446c":"baseline_tree_cv(train_df)","8e4ff70a":"all_data = pd.concat([train,test])\n\ndel all_data['oil_Type']\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)\n    \nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\nall_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nsc = StandardScaler()\ntrain_df[numerical_feature] = sc.fit_transform(train_df[numerical_feature])\ntest_df[numerical_feature] = sc.transform(test_df[numerical_feature] )","59102182":"baseline_keras_cv(train_df)","21321357":"all_data = pd.concat([train,test])\n\ndel all_data['oil_Type']\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)\n    \nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))","a699ec61":"for col in numerical_feature:\n    sns.distplot(all_data.loc[all_data[col].notnull(), col])\n    plt.title(col)\n    plt.show()","f1bf28ba":"['Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways']","d78be0b0":"fig, axes = plt.subplots(ncols=2, figsize=(10,4))\naxes[0].plot(np.sort(all_data['Horizontal_Distance_To_Fire_Points']))\naxes[1].plot(np.cumsum(np.sort(all_data['Horizontal_Distance_To_Fire_Points'])))\nplt.title('Horizontal_Distance_To_Fire_Points')\nplt.show()\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,4))\naxes[0].plot(np.sort(all_data['Horizontal_Distance_To_Roadways']))\naxes[1].plot(np.cumsum(np.sort(all_data['Horizontal_Distance_To_Roadways'])))\nplt.title('Horizontal_Distance_To_Roadways')\nplt.show()","9e3050ed":"for percent in np.arange(99,100,0.1):\n    print(np.percentile(all_data['Horizontal_Distance_To_Fire_Points'],percent))","627f509d":"for percent in np.arange(99,100,0.1):\n    print(np.percentile(all_data['Horizontal_Distance_To_Roadways'],percent))","0944efef":"def outlier_binary(frame, col, outlier_range):\n    outlier_feature = col + '_Outlier'\n    frame[outlier_feature] = 0\n    frame.loc[frame[col] > outlier_range, outlier_feature] = 1\n    return frame","d8adbcaf":"all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)","2df26617":"all_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nsc = StandardScaler()\ntrain_df[numerical_feature] = sc.fit_transform(train_df[numerical_feature])\ntest_df[numerical_feature] = sc.transform(test_df[numerical_feature] )","3b5a7227":"baseline_keras_cv(train_df)","297a8c9a":"all_data = pd.concat([train, test])\nhdroad_outlier_index = all_data['Horizontal_Distance_To_Roadways']>=10000\n\nsns.distplot(all_data.loc[hdroad_outlier_index, 'Horizontal_Distance_To_Roadways'])\nplt.show()\n\nhdroad_outlier_median =  all_data.loc[hdroad_outlier_index, 'Horizontal_Distance_To_Roadways'].median()\nhdroad_normal_median = all_data.loc[all_data['Horizontal_Distance_To_Roadways']<10000, 'Horizontal_Distance_To_Roadways'].median()\nprint(hdroad_outlier_median, hdroad_normal_median, hdroad_outlier_median\/hdroad_normal_median)\nhdroad_outlier_ratio = hdroad_outlier_median\/hdroad_normal_median\n\nsns.distplot(all_data.loc[hdroad_outlier_index, 'Horizontal_Distance_To_Roadways']\/hdroad_outlier_ratio)\nplt.show()","ef5bb3f5":"all_data.loc[hdroad_outlier_index, 'Horizontal_Distance_To_Roadways'] = all_data.loc[hdroad_outlier_index, 'Horizontal_Distance_To_Roadways']\/hdroad_outlier_ratio\nsns.distplot(all_data['Horizontal_Distance_To_Roadways'])\nplt.show()\nsns.distplot(all_data.loc[all_data['Cover_Type']==0,'Horizontal_Distance_To_Roadways'])\nsns.distplot(all_data.loc[all_data['Cover_Type']==1,'Horizontal_Distance_To_Roadways'])\nplt.show()","d95af1be":"all_data = pd.concat([train, test])\nhdfpoint_outlier_index = all_data['Horizontal_Distance_To_Fire_Points']>=10000\n\nsns.distplot(all_data.loc[hdfpoint_outlier_index, 'Horizontal_Distance_To_Fire_Points'])\nplt.show()\n\nhdfpoint_outlier_median =  all_data.loc[hdfpoint_outlier_index, 'Horizontal_Distance_To_Fire_Points'].median()\nhdfpoint_normal_median = all_data.loc[all_data['Horizontal_Distance_To_Fire_Points']<10000, 'Horizontal_Distance_To_Fire_Points'].median()\nprint(hdfpoint_outlier_median, hdfpoint_normal_median, hdfpoint_outlier_median\/hdfpoint_normal_median)\nhdfpoint_outlier_ratio = hdfpoint_outlier_median\/hdfpoint_normal_median\n\nsns.distplot(all_data.loc[hdfpoint_outlier_index, 'Horizontal_Distance_To_Fire_Points']\/hdfpoint_outlier_ratio)\nplt.show()","703a0dd5":"all_data.loc[hdfpoint_outlier_index, 'Horizontal_Distance_To_Fire_Points'] = all_data.loc[hdfpoint_outlier_index, 'Horizontal_Distance_To_Fire_Points']\/hdfpoint_outlier_ratio\nsns.distplot(all_data['Horizontal_Distance_To_Fire_Points'])\nplt.show()\nsns.distplot(all_data.loc[all_data['Cover_Type']==0,'Horizontal_Distance_To_Fire_Points'])\nsns.distplot(all_data.loc[all_data['Cover_Type']==1,'Horizontal_Distance_To_Fire_Points'])\nplt.show()","3f8f050a":"def outlier_divide_ratio(frame, col, outlier_range):\n    outlier_index = frame[col] >= outlier_range\n    outlier_median =  frame.loc[outlier_index, col].median()\n    normal_median = frame.loc[frame[col] < outlier_range, col].median()\n    outlier_ratio = outlier_median \/ normal_median\n    \n    frame.loc[outlier_index, col] = frame.loc[outlier_index, col]\/outlier_ratio\n    return frame","2a49a64e":"all_data = pd.concat([train, test])\ndel all_data['oil_Type']\n\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)\n\nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\nall_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nsc = StandardScaler()\ntrain_df[numerical_feature] = sc.fit_transform(train_df[numerical_feature])\ntest_df[numerical_feature] = sc.transform(test_df[numerical_feature] )\n\nbaseline_keras_cv(train_df)","6ed44bdc":"all_data = pd.concat([train, test])\ndel all_data['oil_Type']\n\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data = pd.concat([all_data,pd.get_dummies(all_data[col],prefix=col)],axis=1)\n    category_feature.append(col)\n    del all_data[col]\n    \nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\nall_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nsc = StandardScaler()\ntrain_df[numerical_feature] = sc.fit_transform(train_df[numerical_feature])\ntest_df[numerical_feature] = sc.transform(test_df[numerical_feature] )\n\nbaseline_keras_cv(train_df)","efd53c1e":"all_data = pd.concat([train, test])\ndel all_data['oil_Type']\n\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data = pd.concat([all_data,pd.get_dummies(all_data[col],prefix=col)],axis=1)\n    category_feature.append(col)\n    del all_data[col]\n    \nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\n# all_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)","1a231183":"all_data['Aspect'].isnull().sum()","98b8e42e":"aspect_train = all_data.loc[all_data['Aspect'].notnull()]\naspect_test = all_data.loc[all_data['Aspect'].isnull()]\ndel aspect_train[\"Cover_Type\"], aspect_train['ID']\ndel aspect_test[\"Cover_Type\"], aspect_test['ID']","7ee67782":"numerical_feature_woaspect = numerical_feature[:]\nnumerical_feature_woaspect.remove('Aspect')\n\nsc = StandardScaler()\naspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\naspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )","e5a3bb6a":"y_value = aspect_train['Aspect']\ndel aspect_train['Aspect'], aspect_test['Aspect']","0cd14ead":"knn = KNeighborsRegressor(n_neighbors=7)\nknn.fit(aspect_train,y_value)\npredict = knn.predict(aspect_test)","e13defde":"sns.distplot(predict)\nsns.distplot(all_data['Aspect'].dropna())\nplt.show()","23042246":"all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nsc = StandardScaler()\ntrain_df[numerical_feature] = sc.fit_transform(train_df[numerical_feature])\ntest_df[numerical_feature] = sc.transform(test_df[numerical_feature] )","fd285dc1":"baseline_keras_cv(train_df)","db6db2ba":"all_data = pd.concat([train, test])\ndel all_data['oil_Type']\n\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)\n    \nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\n# all_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\naspect_train = all_data.loc[all_data['Aspect'].notnull()]\naspect_test = all_data.loc[all_data['Aspect'].isnull()]\ndel aspect_train[\"Cover_Type\"], aspect_train['ID']\ndel aspect_test[\"Cover_Type\"], aspect_test['ID']\n\nnumerical_feature_woaspect = numerical_feature[:]\nnumerical_feature_woaspect.remove('Aspect')\n\nsc = StandardScaler()\naspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\naspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\ny_value = aspect_train['Aspect']\ndel aspect_train['Aspect'], aspect_test['Aspect']\n\nknn = KNeighborsRegressor(n_neighbors=7)\nknn.fit(aspect_train,y_value)\npredict = knn.predict(aspect_test)\n\nall_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nbaseline_tree_cv(train_df)","97f433af":"category_feature","e84cc1e0":"all_data = pd.concat([train_df, test_df])","cb985445":"soil_freq_encoding = all_data.groupby(['Soil_Type']).size()\/all_data.shape[0]\nsoil_freq_encoding = soil_freq_encoding.reset_index().rename(columns={0:'Soil_Frequncy'})\nall_data = all_data.merge(soil_freq_encoding, on='Soil_Type', how='left')","dfa1a579":"def frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()\/all_data.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequncy'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')","5c0cdb8d":"all_data = frequency_encoding(all_data, 'Wilderness_Area')","8732661d":"train_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]","0a741c91":"frequency_df = train_df.groupby(['Soil_Frequncy','Cover_Type'])['Soil_Frequncy'].count().unstack('Cover_Type')\nfrequency_df.plot(kind='bar', figsize=(14,5))\nplt.title('SoilType Frequency')\nplt.show()","cd27dc63":"frequency_df = train_df.groupby(['Wilderness_Area_Frequncy','Cover_Type'])['Wilderness_Area_Frequncy'].count().unstack('Cover_Type')\nfrequency_df.plot(kind='bar', figsize=(10,5))\nplt.title('Wilderness_Area_Frequncy')\nplt.show()","fe589772":"all_data = pd.concat([train, test])\ndel all_data['oil_Type']\n\nall_column_set = set(all_data.columns)\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)\n    \nnumerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\n# all_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)\nall_data['Elevation'] = np.log1p(all_data['Elevation'])\n\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\nall_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\nall_data = frequency_encoding(all_data, 'Soil_Type')\nall_data = frequency_encoding(all_data, 'Wilderness_Area')\n\naspect_train = all_data.loc[all_data['Aspect'].notnull()]\naspect_test = all_data.loc[all_data['Aspect'].isnull()]\ndel aspect_train[\"Cover_Type\"], aspect_train['ID']\ndel aspect_test[\"Cover_Type\"], aspect_test['ID']\n\nnumerical_feature_woaspect = numerical_feature[:]\nnumerical_feature_woaspect.remove('Aspect')\n\nsc = StandardScaler()\naspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\naspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\ny_value = aspect_train['Aspect']\ndel aspect_train['Aspect'], aspect_test['Aspect']\n\nknn = KNeighborsRegressor(n_neighbors=7)\nknn.fit(aspect_train,y_value)\npredict = knn.predict(aspect_test)\n\nsns.distplot(predict)\nsns.distplot(all_data['Aspect'].dropna())\nplt.show()\n\nall_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n\ntrain_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]\n\nbaseline_tree_cv(train_df)","f18f0574":"### \uc544\ub798 2\uac1c\uc758 Feature\uac00 \uc774\uc0c1\ud569\ub2c8\ub2e4.","5e2ccace":"### Graph\uac00 \uc5b4\ub290 \uc21c\uac04\uc5d0 \uaebd\uc774\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","5917e045":"\ucf54\ub4dc\uac00 \uc880 \uae34\ub370 \uc798 \ub530\ub77c\uc640\uc8fc\uc138\uc694~<br>\n\uc544\ub798\ub294 \uae4c\uc9c0\ub294 \uae30\ubcf8 Feature \ucd94\uac00\ud55c \uac83\uc785\ub2c8\ub2e4.","77ad4ca9":"### Utility Function \uc785\ub2c8\ub2e4.","8e0fc797":"# \uc774\ubc88 Kernel\uc5d0\uc11c\ub294 \uae30\ubcf8\uc801\uc778 Data \ubcc0\ud658\uc744 \uc2e4\uc2b5\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc81c\uac00 \uc124\uba85\ud558\ub294\ub3d9\uc548 \uc9c1\uc811 \ucf54\ub4dc \uc2e4\ud589 \uc2dc\ud0a4\uc2dc\uba74\uc11c \uc774\uac83\uc800\uac83 \ud574\ubcf4\uc2dc\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4!","99c5738f":"predict\uc640 \uc6d0\ub798 \uac12\uc758 \ubd84\ud3ec\ub97c \ubcf4\uba74 \uac70\uc758 \ube44\uc2b7\ud558\uac8c Impuation \ub41c\uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. predict\uac12\uc744 \uc6d0\ub798 \ub370\uc774\ud130\uc5d0 \ub123\uace0 \uc131\ub2a5\uc744 \uce21\uc815\ud574\ubd05\ub2c8\ub2e4.","2a587047":"## EDA \ud560 \ub54c Numerical Data\uc5d0 \uc544\uc8fc \ud070 \uac12\uc774 \ub4e4\uc5b4 \uc788\ub294 \uac83\uc744 \ub610 \ubcfc \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.","fe1a8702":"### \ub9c8\ucc2c\uac00\uc9c0\ub85c Horizontal_Distance_To_Fire_Points \ub3c4 \uc0b4\ud3b4\ubd05\ub2c8\ub2e4.","e148f6d4":"### Horizontal_Distance_To_Roadways \ubd84\ud3ec\ub97c \ub2e4\uc2dc \uadf8\ub824\ubcf4\ub2c8 \ud070 \uac12\uc774 \ubaa8\ub450 \uc81c\uac70 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\nTarget\uac12\ub3c4 \ub458 \uc0ac\uc774\uc758 \ubd84\ud3ec\uac00 \ub2ec\ub77c\uc11c Algorithm \uc131\ub2a5\uc774 \uc88b\uac8c \ub098\uc62c \uac83 \uac19\uc2b5\ub2c8\ub2e4.","6637b4fa":"# \uc9c0\uae08\uae4c\uc9c0 \uc815\uc81c\ud55c Feature\ub85c Tree Model\uc5d0\ub3c4 \uc0ac\uc6a9\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\ntree \ubaa8\ub378\uc5d4 One Hot\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 Label Encoding\ub9cc \ud574\uc8fc\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","0a1670b9":"KNN\uc740 \uac70\ub9ac\uae30\ubc18 \uc54c\uace0\ub9ac\uc998\uc774\uae30 \ub54c\ubb38\uc5d0 Scale\uc5d0 \ubbfc\uac10\ud569\ub2c8\ub2e4. Aspect \uac12\uc744 \uc81c\uc678\ud558\uace0 \ubaa8\ub450 StandardScale\uc744 \ud574\uc90d\ub2c8\ub2e4.","ba452382":"\uadf8 \ud6c4 Train, Test Set\uc744 \uc900\ube44\ud558\ub294 \uac83\ucc98\ub7fc y\uac12\uc744 \ub530\ub85c \ube7c\uc8fc\uace0 Aspect Column\uc744 \uc81c\uac70\ud569\ub2c8\ub2e4.","38313d12":"## \uc544\uae4c mean\uc73c\ub85c \ucc44\uc6b4 NULL \uac12\uc744 \uc81c\ub300\ub85c \ucc44\uc6cc\ubcfc\uae4c\uc694?\n\ubb38\uc81c\ub97c \ubcc0\ud658\ud574\uc11c Aspect \uac12\uc758 NULL \uac12\uc744 Test\ub85c \ub193\uace0 \uac12\uc774 \uc788\ub294 \uac83\uc744 Train\uc73c\ub85c \ub193\uc2b5\ub2c8\ub2e4.<br>\nKNN\uc744 \uc0ac\uc6a9\ud558\uc5ec Aspect \uac12\uc744 \ucc44\uc6cc\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>","4766f041":"### Horizontal_Distance_To_Roadways\uc758 \ubd84\ud3ec\ub97c \ub2e4\uc2dc\ud55c\ubc88 \uc0b4\ud3b4\ubd05\ub2c8\ub2e4.\n\uc790\uc138\ud788 \ubcf4\ub2c8 \ud070 \uac12\uacfc \uc791\uc740 \uac12\uc758 \ubd84\ud3ec\uac00 \uac70\uc758 \ube44\uc2b7\ud569\ub2c8\ub2e4.<br>\n\ud070 \uac12\uc758 \uc911\uc559\uac12\uacfc \uc791\uc740 \uac12\uc758 \uc911\uc559\uac12\uc758 \ube44\uc728\ub85c \ud070 \uac12\uc744 \ub098\ub220 \uc90d\ub2c8\ub2e4.<br>","f9919757":"## CategoryData\ub97c Frequency Encoding \ud574\ubcfc\uac8c\uc694\nCategory Data\ub294 2\uac1c\uac00 \uc788\uc2b5\ub2c8\ub2e4.","874e6462":"## \ubb54\uac00 \uc774\ub807\uac8c\ub9cc outlier binary feature \ud558\ub098\ub85c\ub9cc\uc740 \uc880 \uc544\uc27d\uc2b5\ub2c8\ub2e4.\n\uc880 \ub354 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","9abc79d6":"Aspect \uac12\uc774 Null\uc778 \uac83\uc740 Test, Null\uc774 \uc544\ub2cc \uac83\uc740 Train\uc73c\ub85c \ub193\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 Cover_Type\uacfc ID\uac12\uc740 \uc9c0\uc6cc\uc90d\ub2c8\ub2e4.","5acc0b00":"## \uc54c\ub824\uc9c4\ub300\ub85c NeuralNetwork\ub294 OneHot\uc744 \ud574\uc57c\ud569\ub2c8\ub2e4\n## \uc131\ub2a5\uc774 \uaf64 \ub9ce\uc774 \uc62c\ub77c\uac00\uc11c \uae30\ubd84\uc774 \uc88b\ub124\uc694","e67b68c1":"### Tree Model\uc5d0\uc11c\ub294 Log Trasnform \ud55c\ub2e4\uace0 \uc131\ub2a5\ucc28\uc774\uac00 \ud06c\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.","167da012":"### \uac19\uc740 Feature\ub97c NeuralNetwork\uc5d0 \uc801\uc6a9\ud574 \ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","83d2a682":"### \uac12\uc774 \ub108\ubb34 \ud070 Elevation Feature Log Transform \uc218\ud589","1da3a692":"Sklearn\uc5d0 KNeighborsRegressor \uc744 \ubd88\ub7ec\uc640\uc11c \uc54c\uace0\ub9ac\uc998\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4. K\uac12\uc740 Tuning\uc774 \ud544\uc694\ud55c\ub370\uc694.<br>\nCV \uac12\uc744 \ubcf4\uace0 \ud558\uc5ec\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ud558\ub4dc\ud558\uac8c \ud29c\ub2dd\ud558\uc9c0 \uc54a\uace0 \uc801\ub2f9\ud788 \ud558\uace0 \ub118\uc5b4\uac00\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>","57965b34":"## Neural Network\uc5d0\uc11c\ub294 \uc544\uc8fc \ud070 \uac12\uc5d0 Log Transform \ud558\uc600\uc744 \ub54c \ub9ce\uc774 \uac1c\uc120\ub41c \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","3e245fb5":"## Neural Network\ub294 One Hot Encoding\uc73c\ub85c \uc131\ub2a5\uc774 \uc62c\ub77c\uac08\uae4c?\nNeural Network Model\uc740 Category Feature\ub97c \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uba74(Label Encoding) \uc131\ub2a5\uc774 \uc548\uc88b\ub2e4\uace0 \ud569\ub2c8\ub2e4.<br>\n\uac19\uc740 \ucc28\uc6d0\uc5d0\uc11c \ub370\uc774\ud130\ub97c \ub098\ub20c \uc218 \uc5c6\uc5b4 \ub3c5\ub9bd\ub41c \ub2e4\ub978 \ucc28\uc6d0\uc73c\ub85c \ubcf4\ub0b4\uc57c \ud55c\ub2e4\uace0 \ud569\ub2c8\ub2e4.<br>\n\uadf8\ub798\uc11c \ud754\ud788 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc774 One Hot Encoding\uacfc Entity Embedding \ubc29\ubc95\uc785\ub2c8\ub2e4. <br>\nOne Hot Encoding\uc740 \ucc28\uc6d0\uc774 \uc544\uc8fc \ub9ce\uc544\uc9c0\ub294 \ub2e8\uc810\uc774 \uc788\uace0 Category\uac04\uc758 \uc720\uc0ac\uc131\uc744 \ud30c\uc545\ud558\uae30 \ud798\ub4ed\ub2c8\ub2e4.<br>\n\uc694\uc0c8 \ub098\uc628 \uc6b0\uc2b9\uc790 \uc194\ub8e8\uc158\ub4e4 \ubcf4\uba74 \uac70\uc758 Entity Embedding\uc744 \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.<br>\nOne Hot\uacfc Entity Embedding\uc758 \uc131\ub2a5\uc744 \ube44\uad50\ud574\ubcf4\ub294 \uac83\uc774 \uc88b\uc740\ub370, \uadf8\uac83\uc740 \uc81c\uac00 \uc774\ubc88\uc5d0 \uc900\ube44\ud558\uc9c0 \ubabb\ud574\uc11c \uacf5\ubd80\ud560 \uc218 \uc788\ub294 Link\ub9cc \uac78\uc5b4\ub4dc\ub9b4\uac8c\uc694<br>\n\uaf2d \uc0ac\uc6a9\ud574\ubcf4\uc138\uc694<br>\nhttps:\/\/www.kaggle.com\/youhanlee\/simple-eda-entity-embedding - \uc774\uc720\ud55c\ub2d8 Kernel<br>\nhttps:\/\/www.kaggle.com\/aquatic\/entity-embedding-neural-net - Joe Eddy Kaggle Kernel<br>\nhttps:\/\/medium.com\/@satnalikamayank12\/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9 - \uc704 Kaggle Kernel\uc744 \uc124\uba85\ud55c <br>\nhttps:\/\/github.com\/entron\/entity-embedding-rossmann <br>\nhttp:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/ <br>\nhttps:\/\/arxiv.org\/abs\/1604.06737 <br>"}}