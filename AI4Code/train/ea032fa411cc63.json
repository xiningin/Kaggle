{"cell_type":{"5a3a42ba":"code","36636ddd":"code","0ab7780a":"code","647b7942":"code","47c4c97b":"code","89a65543":"code","bb397088":"code","a5ba0d64":"code","7e991b90":"markdown","9847ff17":"markdown","5ac4a76f":"markdown","36f68d89":"markdown","fe7a0817":"markdown","8cf66e46":"markdown","b7860382":"markdown","e56db8d2":"markdown","90f9807a":"markdown","fd1408f1":"markdown","736424cc":"markdown","479e3bbd":"markdown","f5cac85a":"markdown","8cd3d253":"markdown"},"source":{"5a3a42ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36636ddd":"!pip install git+git:\/\/github.com\/tensorflow\/examples.git#egg=tensorflow-examples[model_maker]","0ab7780a":"import numpy as np\n\nimport tensorflow as tf\nassert tf.__version__.startswith('2')\n\nfrom tensorflow_examples.lite.model_maker.core.data_util.image_dataloader import ImageClassifierDataLoader\nfrom tensorflow_examples.lite.model_maker.core.task import image_classifier\nfrom tensorflow_examples.lite.model_maker.core.task.model_spec import mobilenet_v2_spec\nfrom tensorflow_examples.lite.model_maker.core.task.model_spec import ImageModelSpec\n\nimport matplotlib.pyplot as plt","647b7942":"#load flower dataset\nimage_path = tf.keras.utils.get_file(\n      'flower_photos',\n      'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz',\n      untar=True)\n\n#below code is for unpacking archives\/zip\/rar\n#!pip install pyunpack\n#!pip install patool\n#from pyunpack import Archive\n#Archive('\/content\/stage 01.rar').extractall('\/content\/new\/')\n\n#load your own dataset\n#image_path= '\/content\/new\/stage 01\/'","47c4c97b":"data = ImageClassifierDataLoader.from_folder(image_path)\ntrain_data, test_data = data.split(0.9)","89a65543":"model = image_classifier.create(train_data,model_spec=mobilenet_v2_spec)","bb397088":"loss, accuracy = model.evaluate(test_data)","a5ba0d64":"model.export(export_dir='.', with_metadata=True)\nfrom IPython.display import FileLink, FileLinks\nFileLinks('.') #generates links of all files\n#FileLinks('model.tflite')\n#FileLinks('labels.txt')","7e991b90":"# **Head over [this](https:\/\/www.tensorflow.org\/lite\/tutorials\/model_maker_image_classification) page for training on specific models, cutomization of models.**\n\n**Change the model**\n\nChange to the model that's supported in this library.\n\nThis library supports EfficientNet-Lite models, MobileNetV2, ResNet50 by now. EfficientNet-Lite are a family of image classification models that could achieve state-of-art accuracy and suitable for Edge devices. The default model is EfficientNet-Lite0.\n\nWe could switch model to MobileNetV2 by just setting parameter model_spec to mobilenet_v2_spec in create method.\n\n*model = image_classifier.create(train_data, model_spec=mobilenet_v2_spec, validation_data=validation_data)*\n","9847ff17":"# Connect me at \"shubham.divakar@gmail.com\" for any queries.\n# Soon I will be publishing android apps using the tflite models from here.","5ac4a76f":"2. Customize the TensorFlow model.","36f68d89":"3. Evaluate the model.","fe7a0817":"**4. Export to TensorFlow Lite model. You could download it in the left sidebar same as the uploading part for your own use.**","8cf66e46":"Prerequisites\n\nTo run this example, we first need to install serveral required packages, including Model Maker package that in github [repo](https:\/\/github.com\/tensorflow\/examples\/tree\/master\/tensorflow_examples\/lite\/model_maker).","b7860382":"# Change to the model in TensorFlow Hub\n\nMoreover, we could also switch to other new models that inputs an image and outputs a feature vector with TensorFlow Hub format.\n\nAs Inception V3 model as an example, we could define inception_v3_spec which is an object of ImageModelSpec and contains the specification of the Inception V3 model.\n\nWe need to specify the model name name, the url of the TensorFlow Hub model uri. Meanwhile, the default value of input_image_shape is [224, 224]. We need to change it to [299, 299] for Inception V3 model.\n\n*inception_v3_spec = ImageModelSpec(\n    uri='https:\/\/tfhub.dev\/google\/imagenet\/inception_v3\/feature_vector\/1')\ninception_v3_spec.input_image_shape = [299, 299]*\n\nThen, by setting parameter model_spec to inception_v3_spec in create method, we could retrain the Inception V3 model.\n\nThe remaining steps are exactly same and we could get a customized InceptionV3 TensorFlow Lite model in the end.\nChange your own custom model\n\nIf we'd like to use the custom model that's not in TensorFlow Hub, we should create and export ModelSpec in TensorFlow Hub.\n\nThen start to define ImageModelSpec object like the process above.\nChange the training hyperparameters\n\nWe could also change the training hyperparameters like epochs, dropout_rate and batch_size that could affect the model accuracy. For instance,\n\n    epochs: more epochs could achieve better accuracy until it converges but training for too many epochs may lead to overfitting.\n    dropout_rate: avoid overfitting.\n    batch_size: number of samples to use in one training step.\n    validation_data: number of samples to use in one training step.\n\nFor example, we could train with more epochs.\n\n*model = image_classifier.create(train_data, validation_data=validation_data, epochs=10)\n*","e56db8d2":"After this simple 4 steps, we can now download the model and label files, and continue to the next step in the [codelab](https:\/\/codelabs.developers.google.com\/codelabs\/recognize-flowers-with-tensorflow-on-android\/#5).\n\nFor a more comprehensive guide to TFLite Model Maker, please refer to this [notebook](https:\/\/colab.research.google.com\/github\/tensorflow\/examples\/blob\/master\/tensorflow_examples\/lite\/model_maker\/demo\/image_classification.ipynb) and its documentation.\n","90f9807a":"# **Flower classification with TensorFlow Lite Model Maker with TensorFlow 2.0**\n","fd1408f1":"Simple End-to-End Example\nGet the data path\n\nLet's get some images to play with this simple end-to-end example. Hundreds of images is a good start for Model Maker while more data could achieve better accuracy.","736424cc":"Import the required packages.","479e3bbd":"**The source of this kernel is [here](https:\/\/www.tensorflow.org\/lite\/tutorials\/model_maker_image_classification)**","f5cac85a":"You could replace image_path with your own image folders. As for uploading data to colab, you could find the upload button in the left sidebar shown in the image below with the red rectangle. Just have a try to upload a zip file and unzip it. The root file path is the current path.\n\nUpload File\n\nIf you prefer not to upload your images to the cloud, you could try to run the library locally following the guide in github.\nRun the example\n\nThe example just consists of 4 lines of code as shown below, each of which representing one step of the overall process.\n\n    1. Load input data specific to an on-device ML app. Split it to training data and testing data.\n\n","8cd3d253":"Model Maker library simplifies the process of adapting and converting a TensorFlow neural-network model to particular input data when deploying this model for on-device ML applications.\n\nThis notebook shows an end-to-end example that utilizes this Model Maker library to illustrate the adaption and conversion of a commonly-used image classification model to classify flowers on a mobile device.\n"}}