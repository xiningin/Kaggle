{"cell_type":{"56e9c6c8":"code","b13f5cfa":"code","d9035274":"code","7917d59e":"code","80b7d548":"code","babeb48b":"code","b188b05d":"code","5e1b61bd":"code","0193fa40":"code","59fef32a":"code","4c7c72d0":"code","efed4811":"code","fc75b9b9":"code","692d856f":"code","eb3a7d54":"code","ddc24a90":"code","87cd06f5":"code","0238a592":"code","a3954837":"code","67e0255b":"code","37710610":"code","d6381b33":"code","662c5133":"markdown","d0070a94":"markdown","7bde9ea4":"markdown","ec5e521b":"markdown","41dfde95":"markdown","1e284121":"markdown","85c30dbb":"markdown","fac3b17d":"markdown","34e5d831":"markdown","43fa8902":"markdown","625ebc3a":"markdown","78b2085f":"markdown","56ee9c87":"markdown","bd36faa5":"markdown","d709a47d":"markdown","1a250f81":"markdown","7d8eb050":"markdown"},"source":{"56e9c6c8":"import pandas as pd\nimport numpy as np\nimport re\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom statistics import mean\n\nimport operator\n\nimport os","b13f5cfa":"# Let's check the mean and median size of the data in the engagement data folder\n\n_files = []\n_lenght_data = []\n_lenght_cols = []\n\nfor _file in os.listdir('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'):\n    _files.append(int(_file[:-4]))\n    df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/' + _file)\n    _lenght_data.append(len(df))\n    _lenght_cols.append(len(df.columns))\n\n_infos = pd.DataFrame(data=zip(_files, _lenght_data, _lenght_cols), columns=['district_id', 'Lines', 'Cols'])\n\nprint(f\"\"\"Files length median is : {int(_infos['Lines'].median())}\nMean is : {int(_infos['Lines'].mean())}\nShorter file has {int(_infos['Lines'].min())} lines\nand the longest one {int(_infos['Lines'].max())}.\nThere are {int(_infos['Cols'].unique())} columns on each file\"\"\")","d9035274":"# Let's merge our data infos on their District ID and pivot the data to see which state and locale provided the most infos\n\ndistrict_df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\n\nmerged = pd.merge(_infos, district_df, on=\"district_id\")\n_pvt = merged.pivot_table(index='state', columns='locale', values='Lines', aggfunc='sum')\n\ndisplay(pd.DataFrame(data=(_pvt.isna().sum())).reset_index().sort_values(by=0).rename(columns={0 : 'Missing data'}))\n\npvt = merged.pivot_table(index=['state', 'locale'], values='Lines', aggfunc='sum').reset_index()\n\nfig = px.bar(pvt, x=\"state\", y=\"Lines\", color=\"locale\")\n\nfig.update_layout(margin=dict(\n                              l=20,\n                              r=20,\n                              b=50,\n                              t=50,\n                              pad=10\n                            ),\n                            paper_bgcolor=\"darkgray\",\n                            title=\"Data distribution by locale type and State\", \n                            title_x=0.5\n                        )\n\nfig.update_xaxes(title=\"\")\nfig.update_yaxes(title=\"\")\n\nfig.show()","7917d59e":"fig, axes = plt.subplots(1, 2, figsize=(20, 10))\nfig.suptitle('Data distribution by state')\n\nsns.set_style(style = \"darkgrid\")\n\ndistrict_per_state = district_df[['state', 'district_id']].groupby(by='state').count().reset_index()\ndistrict_per_state.sort_values(by='district_id', ascending=False, inplace=True)\n\ng1 = sns.barplot(ax=axes[0], data=district_per_state, x='district_id', y='state')\ng1.set(title='Number of School districts available per State')\ng1.set(xlabel='Nb of school districts')\ng1.set(ylabel=None)\n\n_pvt2 = merged.pivot_table(index='state', values='Lines', aggfunc='sum')\n_pvt2 = _pvt2.sort_values(by=\"Lines\", ascending=False)\n\ng2 = sns.barplot(ax=axes[1], data=_pvt2.reset_index(), x='Lines', y='state')\ng2.set(title='Available Data points per State')\ng2.set(xlabel='Data points')\ng2.set(ylabel=None)\nxlabels = ['{:,.0f}'.format(x) for x in g2.get_xticks()]\ng2.set_xticklabels(xlabels)\n\nplt.show()","80b7d548":"# The sectors are grouped, we need to split them, get_dummies is probably the best way to go about this\n# We'll also split the 'Primary Essential Function' column into main and sub category as per the Readme file infos\n# For that Nans need to be dropped, there are a total of 20 out of 372, which is not too bad\n\n_products = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\n\n# Thanks Leonie for the great code !\ntemp_sectors = _products['Sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\n_products = _products.join(temp_sectors)\n\n_products['main pef'] = _products['Primary Essential Function'].dropna().apply(lambda x : x.split(' - ')[0].strip())\n_products['sub pef'] = _products['Primary Essential Function'].dropna().apply(lambda x : x.split(' - ')[1:])\n_products['sub pef'] = _products['sub pef'].dropna().apply(lambda x : \", \".join(x))\n\n_products.drop([\"Sector(s)\", \"Primary Essential Function\"], axis=1, inplace=True)\n\n_products","babeb48b":"# I'll manipulate the _products table to get some graphs out of it\n\n_sectors_data = {\n         'sector_Corporate' : [_products['sector_Corporate'].sum()], \n         'sector_HigherEd' : [_products['sector_HigherEd'].sum()],\n         'sector_PreK-12' : [_products['sector_PreK-12'].sum()]\n         }\n\n_sectors = pd.DataFrame.from_dict(data=_sectors_data, orient='index').rename(columns={0 : 'count'})\n\n_pvt3 = _products.pivot_table(index='Provider\/Company Name', values='LP ID', aggfunc='count').sort_values(by=\"LP ID\", ascending=False).reset_index()\n\nl=[]\nfor i in _products['sub pef'].dropna():\n    l.append(i)\n\n_res={}\nfor i in list(set(l)):\n    _dico = {i : l.count(i)}\n    _res.update(_dico)\n\n_sub_pef = pd.DataFrame.from_dict(data=_res, orient='index', columns=['count']).sort_values(by='count', ascending=False).reset_index()\n_sub_pef.rename(columns={'index' : 'sub'}, inplace=True)\n","b188b05d":"fig, axes = plt.subplots(2, 2, figsize=(20, 10))\nfig.suptitle('Distribution')\n\nsns.set_style(style = \"darkgrid\")\nplt.figure(figsize=(8, 6))\n\ng1 = sns.barplot(ax=axes[0, 1], data=_sectors.sort_values(by='count', ascending=False), x='count', y=_sectors.index)\ng1.set(title='Sector Distribution')\ng1.set(xlabel=None)\ng1.set(ylabel=None)\n\ng2 = sns.countplot(ax=axes[1, 1], data=_products, y='main pef')\ng2.set(title='Main Essential Function')\ng2.set(xlabel=None)\ng2.set(ylabel=None)\n\ng3 = sns.barplot(ax=axes[0, 0], data=_pvt3.head(5), x='LP ID' , y='Provider\/Company Name')\ng3.set(title='Company')\ng3.set(xlabel=None)\ng3.set(ylabel=None)\n\ng4 = sns.barplot(ax=axes[1, 0], data=_sub_pef.head(5), x='count' , y='sub')\ng4.set(title='Other Essential Function')\ng4.set(xlabel=None)\ng4.set(ylabel=None)\n\nplt.show()","5e1b61bd":"for i in range(len(district_df)):\n    _val_race = district_df[\"pct_black\/hispanic\"][i]\n    _val_free = district_df[\"pct_free\/reduced\"][i]\n    _val_pp = district_df[\"pp_total_raw\"][i]\n    try:\n        _val_race = _val_race[1:-1].split(', ')\n        district_df[\"pct_black\/hispanic\"][i] = int(mean(float(i) for i in _val_race) * 100)\n    except:\n        pass\n    try:\n        _val_free = _val_free[1:-1].split(', ')\n        district_df[\"pct_free\/reduced\"][i] = int(mean(float(i) for i in _val_free) * 100)\n    except:\n        pass\n    try:\n        _val_pp = _val_pp[1:-1].split(', ')\n        district_df[\"pp_total_raw\"][i] = mean(int(i) for i in _val_pp)\n    except:\n        pass\n\n\ndistrict_df[\"pct_black\/hispanic\"] = district_df[\"pct_black\/hispanic\"].astype(float)\ndistrict_df[\"pct_free\/reduced\"] = district_df[\"pct_free\/reduced\"].astype(float) \ndistrict_df[\"pp_total_raw\"] = district_df[\"pp_total_raw\"].dropna().astype(int)\n\nprint(f\"Highest pct_black\/hispanic we have is {district_df['pct_black\/hispanic'].max()}\")\n\ndisplay(district_df[district_df['pct_black\/hispanic'] == district_df['pct_black\/hispanic'].max()])\n\n","0193fa40":"district_corr = district_df.corr()\n\ndistrict_corr.style.background_gradient(cmap='coolwarm').set_precision(2)","59fef32a":"race_df = pd.read_csv('..\/input\/elsi-csv-export\/ELSI_csv_export_6376565703827256183522.csv', encoding='cp1252')\n\nrace_df.rename(columns={'State Name [Public School] Latest available year' : 'state',\n'Total Students All Grades (Excludes AE) [Public School] 2019-20' : 'total',\n'Hispanic Students [Public School] 2019-20' : 'hispanic',\n'Black or African American Students [Public School] 2019-20' : 'black'}, inplace=True)\n\nrace_df = race_df.dropna(subset=['state'])\n\n#race_df['state'] = race_df['state'].apply(lambda x : x.lower())\nrace_df['state'] = race_df['state'].apply(lambda x : \" \".join(e.capitalize() for e in x.split()))\nrace_df['state'] = race_df['state'].apply(lambda x : x.strip())\n\n_cols = ['total', 'state', 'hispanic', 'black']\n\nrace_df = race_df[_cols]\n\ndef col_clean(col):\n    for _val in range(len(col)):\n        for i in col[_val]:\n            if i.isnumeric()==False:\n                col[_val] = col[_val].replace(i, '')\n\n    col = col.replace('', 0)\n    col = col.astype(int)\n    \n    return col\n\nrace_df['hispanic'] = col_clean(race_df['hispanic'])\nrace_df['black'] = col_clean(race_df['black'])\nrace_df['total'] = col_clean(race_df['total'])\n\nrace_df = pd.DataFrame(race_df)\n\nrace_df = race_df.groupby(by='state').sum().reset_index()\n\nrace_df['% hispanic'] = round(race_df['hispanic']*100 \/ race_df['total'], ndigits=2)\nrace_df['% black'] = round(race_df['black']*100 \/ race_df['total'], ndigits=2)\n\nrace_df['% black\/hispanic'] = race_df['% hispanic'] + race_df['% black']\n\ndistrict_dff = district_df.dropna(subset=['state'])[['state', 'pct_black\/hispanic', 'pct_free\/reduced']]\ndistrict_dff = district_dff.groupby(by='state').median()\ndistrict_dff.reset_index(inplace=True)\ndistrict_dff['state'] = district_dff['state'].apply(lambda x : \" \".join(e.capitalize() for e in x.split()))\n\njoined = pd.merge(district_dff, race_df, how=\"left\", on=[\"state\", \"state\"])\n\njoined = joined[['state', 'pct_black\/hispanic', '% black\/hispanic']]\n\njoined['difference'] = joined['% black\/hispanic'] - joined['pct_black\/hispanic']\n\njoined.rename(columns={'pct_black\/hispanic' : 'provided % black\/hispanic', '% black\/hispanic' : 'NCES % black\/hispanic'}\n              , inplace=True)\n\njoined","4c7c72d0":"us_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\nstate_map = district_df.dropna(subset=['state'])[['state', 'pct_black\/hispanic', 'pct_free\/reduced', 'pp_total_raw']]\nstate_map = state_map.groupby(by='state').mean().reset_index()\nstate_map['state_abrv'] = state_map['state'].apply(lambda x : us_state_abbrev[x])\n\n\nfig = go.Figure()\nlayout = dict(\n    title_text = f\"Mean pct_black\/hispanic per State\",\n    geo_scope='usa',\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations=state_map[\"state_abrv\"],\n        zmax=1,\n        z = state_map[\"pct_black\/hispanic\"],\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='white',\n        geo='geo',\n        colorscale=px.colors.sequential.Teal, \n    )\n)\n\nfig.update_layout(layout)   \n#fig.show(renderer='notebook')\n\nfig2 = go.Figure()\nlayout = dict(\n    title_text = f\"Mean pct_free\/reduced per State\",\n    geo_scope='usa',\n)\n\nfig2.add_trace(\n    go.Choropleth(\n        locations=state_map[\"state_abrv\"],\n        zmax=1,\n        z = state_map[\"pct_free\/reduced\"],\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='white',\n        geo='geo',\n        colorscale=px.colors.sequential.Teal, \n    )\n)\n\nfig2.update_layout(layout)  \n\ndisplay(fig, fig2)\n","efed4811":"# List of products on the products_info file\nall_products_id = _products['LP ID'].to_list()\n\n# Initiate empty dictionary with the product ID as keys, all values set to 0 \nproducts_hits_dic = {k : 0 for k in all_products_id}\n\n# For each file in the engagement_data folder\nfor _file in os.listdir('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'):\n    # Open the file, drop the NaNs from the 'lp_id' column\n    data = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/' + _file)\n    data = data.dropna(subset=['lp_id'])\n    data['lp_id'] = data['lp_id'].astype(int)\n    # Create a 'hit' column and do the sum ( i avoid potential NaNs that i might encounter if i did a .count() rather than .sum() )\n    # I could have dropped the NaNs but i might have missed too many lines by doing that\n    data[\"hit\"] = 1\n    grouped_data = data.groupby(by='lp_id').sum().reset_index()\n    # List the values and create a dictionary with them\n    _ids = grouped_data['lp_id'].to_list()\n    _hits = grouped_data['hit'].to_list()\n\n    _temp_dic = {_id : _hit for _id, _hit in zip(_ids, _hits)}\n    # Now for each key present in my dictionary that's also present in the products_hits_dic, i increment the value\n    for key, value in _temp_dic.items():\n        if key in products_hits_dic.keys():\n            products_hits_dic[key] += value\n \n","fc75b9b9":"# I'm gonna create a Dataframe with the values of the dictionary and merge the results with the products_info.csv file\n\n_top_products = pd.DataFrame.from_dict(products_hits_dic, orient='index').reset_index()\n_top_products.rename(columns={'index' : 'LP ID', 0 : 'hits'}, inplace=True)\n_top_products.sort_values(by='hits', ascending=False, inplace=True)\n\n_top_products_merged = pd.merge(_top_products, _products, on=\"LP ID\")\n_top_products_merged","692d856f":"fig = px.histogram(_top_products_merged.head(30), x=\"hits\", y=\"Product Name\", height=900)\n\nfig.update_layout(paper_bgcolor=\"darkgray\", title=\"Most popular Learning tools\", title_x=0.5)\n\nfig.update_xaxes(title=\"\")\nfig.update_yaxes(title=\"\", autorange='reversed')\nfig.update_traces(hovertemplate='<i>Total %{y} : <\/i>' + '%{x}')\n\nfig.show()","eb3a7d54":"def nice_graph(category : str, _nb : int):\n\n    _category = _products[_products['sub pef'] == str(category)]\n    _category_lp_id = _category['LP ID'].astype(int).to_list()\n    _category_products = _category['Product Name'].to_list()\n    _category_dic = {a : b for a, b in zip(_category_lp_id, _category_products)}\n\n    _category_data = pd.DataFrame(columns={'time', 'lp_id', 'pct_access', 'engagement_index'})\n\n    for _file in os.listdir('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'):\n        \n        df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/' + _file)\n        df = df[df['lp_id'].isin(_category_lp_id)]\n\n        _category_data = pd.concat([_category_data, df], ignore_index=True)\n\n    _category_data.dropna(inplace=True)\n    _category_data['time'] = pd.to_datetime(_category_data['time'])\n    _category_data = _category_data.sort_values(by='time', ascending=True).reset_index(drop=True)\n    _category_data['lp_id'] = _category_data['lp_id'].astype(int)\n    _category_data['product name'] = _category_data['lp_id'].apply(lambda x : _category_dic[x])\n    _category_data['engagement_index'] = _category_data['engagement_index'].apply(lambda x: int(x))\n\n    _pvt = _category_data[['product name', 'lp_id']].pivot_table(index='product name', values='lp_id', aggfunc='count').sort_values(by='lp_id', ascending=False).reset_index()\n    _pvt.rename(columns={'product name' : category, 'lp_id' : 'hits'}, inplace=True)\n\n    _top_products = _pvt[category][:_nb].to_list()\n\n    _pvt_fig = px.histogram(_pvt.head(_nb), x=\"hits\", y=category)\n    _pvt_fig.update_layout(paper_bgcolor=\"darkgray\", title=category, title_x=0.5)\n    _pvt_fig.update_xaxes(title=\"\")\n    _pvt_fig.update_yaxes(title=\"\", autorange='reversed')\n    _pvt_fig.update_traces(hovertemplate='<i>Total %{y} : <\/i>' + '%{x:.0f}')\n    \n    _category_fig = px.line(_category_data[_category_data['product name'].isin(_top_products)], x=\"time\", y=\"engagement_index\", color='product name')\n    _category_fig.update_layout(title=f'{category} engagement')\n    _category_fig.update_xaxes(title='')\n    _category_fig.update_yaxes(title='Engagement Index')\n    _category_fig.update_traces(hovertemplate=\n                                              '<br><b>Total<\/b>: %{y:.0f}<br>'\n                                              + '<br><b>Date<\/b>: %{x}<br>'\n                                              )\n    \n    return _pvt_fig, _category_fig\n    ","ddc24a90":"_sub_pef.head(5)","87cd06f5":"study_tools_ranking, study_tools_graph = nice_graph('Study Tools', 5)\nvirtual_classromm_ranking, virtual_classromm_graph = nice_graph('Virtual Classroom, Video Conferencing & Screen Sharing', 5)\nc_c_c_ranking, c_c_c_graph = nice_graph('Content Creation & Curation', 5)\ns_r_r_ranking, s_r_r_graph = nice_graph('Sites, Resources & Reference', 5)\nd_l_p_ranking, d_l_p_graph = nice_graph('Digital Learning Platforms', 5)","0238a592":"display(study_tools_ranking, study_tools_graph) ","a3954837":"display(virtual_classromm_ranking, virtual_classromm_graph)","67e0255b":"display(c_c_c_ranking, c_c_c_graph)","37710610":"display(s_r_r_ranking, s_r_r_graph)","d6381b33":"display(d_l_p_ranking, d_l_p_graph) ","662c5133":"## Data distribution check","d0070a94":"### I chose to only display the top 5 products in each category to spare our CPUs, but if you want to see more just type in the number of products you'd like displayed as the second parameter of the 'nice_graph' function in the cell above\n\n### If you're interested in looking at the engagement for one particular product Double-click on its name in the 'product name' legend, add as many as you want if you want to compare them.","7bde9ea4":"### Suburb > city > rural > town. I would have liked more data coming from rural \/ town areas seeing they were the most impacted by travel restrictions \/ school closures, the internet connexion could also be a problem in some areas. Unfortunately the column 'country_connections_ratio' which should be helping us has poor quality data in there (a giant interval) and is of no use to us.","ec5e521b":"## Popular products","41dfde95":"### The correlation between those 2 columns is pretty clear","1e284121":"### The distribution of the data by State is very unequal, that was expected but the disparities are big. We are missing a lot of states and the most populated states are not the ones who provided the most data, definitely something to take into account. \n","85c30dbb":"### Some interesting insights there. 'Corporate education' is far ahead of everyone else (so not exactly your typical school student), 'Learning & Curriculum' dominates all over kinds of Primary Essential Function. There are plenty of other essential functions in the newly created 'sub pef' column but 'Sites, Resources & Reference' seems to be the winner. As far as companies go no surprise there with Google and Microsoft at the top of the chart.","fac3b17d":"### So where does the data come from exactly ?","34e5d831":"### I will now take care of the columns \"pct_black\/hispanic\", \"pp_total_raw\" and \"pct_free\/reduced\", values are represented with intervals and i'll turn them into an average for better readability, the column \"county_connections_ratio\" has poor quality data in it doing an average of those intervals wouldn't make much sense","43fa8902":"### Ok let's take a look at the products, which ones are the most popular ? \n### Let's create a dictionary which will have as keys the ID of all available products and as values the number of times that product is mentionned in the engagement data files.\n","625ebc3a":"### These are the top 'Primary Essential Function' items in the 'sub_pef' column","78b2085f":"### 0.66 correlation score between Black \/ Hispanic districts and students available for free \/ discounted meals, that makes me sad.","56ee9c87":"## Engagement of students\n\n### Time to look at the engagement of the students for different Essential Functions items\n#### Remember how we split the 'Primary Essential Function' column into 2 sub-categories? that's what we'll use here","bd36faa5":"## Data quality check\n\n### I went to the NCES website and downloaded some data to double-check the porportion of Afro-american and Hispanic students in different states. The data can be found at this [link](https:\/\/nces.ed.gov\/ccd\/elsi\/tableGenerator.aspx).\n\n### I'll compare it to the data we are provided with","d709a47d":"### Ok we seem to have some discrepancies here, i'll choose to go with the data we are provided with","1a250f81":"### Let's put those infos on maps\n\n#### In order to do a Plotly Choropleth map we need to change a coupe of infos, like abbreviating the States names, i found a Python dictionary that will save us some time right [here](https:\/\/gist.github.com\/rogerallen\/1583593)\n\n#### Infos on how to do a Plotly Choropleth map can be found [here](https:\/\/plotly.com\/python\/choropleth-maps\/#choropleth-maps-with-gochoropleth)","7d8eb050":"### Let's now take care of the columns with aggregated data in it ( 'sectors' and 'primary essential function'). I'll split it between 'primary essential function' and 'sub primary essential function'. For the sectors i'll get some dummies."}}