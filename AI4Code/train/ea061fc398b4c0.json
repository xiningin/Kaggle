{"cell_type":{"7bdf749f":"code","68d780cb":"code","18ebdc83":"code","0970c22e":"code","b7e3f2f9":"code","e3b12fed":"code","7afbad78":"code","d2d1fae4":"code","ee8d1a5d":"code","af565088":"code","e30124f7":"code","970e52a7":"code","aab1be4d":"code","f47b95c0":"code","8a5bcfcb":"code","b97d6740":"code","fed04e69":"code","1b7e0423":"code","5bc68452":"code","7891ff92":"code","a294cb1b":"code","d9bc8b17":"code","5082bb8d":"code","cfe6fc77":"code","b1689324":"code","59d0f15b":"code","e3951964":"code","965d38da":"code","a6c99d5d":"code","ee6d6b08":"code","6f61bd91":"code","9aef3c4a":"code","c78eb07e":"code","60867dab":"code","ab231e87":"code","efad0209":"code","9799362c":"code","ac045f52":"code","aaebe4ce":"markdown","5a8479ca":"markdown","2a9c5ae7":"markdown","5a799a63":"markdown","84234f6a":"markdown","c85c8517":"markdown","65a927f9":"markdown","c83dae04":"markdown","5dab8675":"markdown","68ff1015":"markdown"},"source":{"7bdf749f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")#\u5ffd\u7565\u6389\u666e\u901a\u7684warning\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","68d780cb":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","18ebdc83":"train=pd.read_csv(\"..\/input\/train.csv\",index_col=0)\ntest=pd.read_csv((\"..\/input\/test.csv\"),index_col=0)\ntest['SalePrice']=-99","0970c22e":"train.head()","b7e3f2f9":"sns.distplot(np.log1p(train['SalePrice']),color=\"r\")","e3b12fed":"var = 'OverallQual'\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=train)\nfig.axis(ymin=0, ymax=800000);","7afbad78":"na_des=train.isna().sum()\ntrain['SalePrice']=np.log1p(train['SalePrice'])\nna_des[na_des>0].sort_values(ascending=False)","d2d1fae4":"new_data=pd.concat([train,test],axis=0,sort=False)\nnew_data.head()","ee8d1a5d":"cols1 = [\"PoolQC\",\"MiscFeature\",'SaleType', \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\nfor col in cols1:\n    new_data[col].fillna(\"None\", inplace=True)\ncols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\nfor col in cols:\n    new_data[col].fillna(1, inplace=True)\nnew_data[\"LotFrontage\"] = new_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\na=new_data.isna().sum()\ncols=a[(a>0) & (a<100)].index\nfor col in cols:\n    new_data[col]=new_data[col].fillna(new_data[col].mode()[0])","af565088":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","e30124f7":"total= 'TotalBsmtSF'\nsns.scatterplot(x=total, y='SalePrice',data=train,style='Street',markers={'Pave':'^','Grvl':'o'});","970e52a7":"sns.scatterplot(x='GrLivArea', y='SalePrice',data=train,color='b',style='Street',markers={'Pave':'^','Grvl':'o'});","aab1be4d":"f, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=train)\nplt.xticks(rotation=90);\n\n","f47b95c0":"k = 10 \ncorrmat = train.corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nax = sns.heatmap(cm, annot=True,annot_kws={'size': 10}, fmt=\".2f\",xticklabels=cols.values,yticklabels=cols.values)\nplt.show()","8a5bcfcb":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','SaleType']\nsns.pairplot(train[cols], size = 2.5,hue=\"SaleType\", palette=\"husl\")\nplt.show();","b97d6740":"sns.violinplot(x=\"SaleType\", y=\"SalePrice\", data=train,hue=\"Street\",palette=\"Set2\")","fed04e69":"f, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"GrLivArea\", data=train)\nplt.xticks(rotation=90);","1b7e0423":"train.groupby(['YearBuilt']).SalePrice.aggregate(['mean','std','max']).plot()","5bc68452":"train.groupby(['YearBuilt']).GrLivArea.aggregate(['mean','std','max']).plot()","7891ff92":"train.sort_values(by = 'GrLivArea', ascending = False)['GrLivArea'][:1]\ntrain = train.drop(train[train.index == 1299].index)","a294cb1b":"fig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\n","d9bc8b17":"sns.distplot(np.log1p(train['GrLivArea']));\nfig = plt.figure()\nres = stats.probplot(np.log1p(train['GrLivArea']), plot=plt)\ntrain['GrLivArea']=np.log1p(train['GrLivArea'])","5082bb8d":"new_data['HasBsmt']=new_data['TotalBsmtSF'].apply(lambda x:1 if x!=0 else 0)","cfe6fc77":"new_data.loc[new_data['HasBsmt']==1,'TotalBsmtSF'] = np.log1p(new_data['TotalBsmtSF'])","b1689324":"fig = plt.figure()\nres = stats.probplot(new_data[new_data['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","59d0f15b":"new_data['GrLivArea'][new_data['GrLivArea']==0]=1\nnew_data['1stFlrSF'][new_data['1stFlrSF']==0]=1\nnew_data['2ndFlrSF'][new_data['2ndFlrSF']==0]=1\nnew_data['1st_GrLivArea']=new_data['1stFlrSF']\/new_data['GrLivArea']\nnew_data['2st_GrLivArea']=new_data['2ndFlrSF']\/new_data['GrLivArea']\nnew_data['1st_2st']=new_data['1stFlrSF']\/new_data['2ndFlrSF']","e3951964":"new_data=pd.get_dummies(new_data)","965d38da":"train=new_data.loc[np.array(train.index)]\ntest=new_data.loc[np.array(test.index)]","a6c99d5d":"x=train.drop('SalePrice',axis=1)\ny=train['SalePrice']\nx_test=test.drop('SalePrice',axis=1)","ee6d6b08":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport time\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, x.values,y.values, scoring=\"neg_mean_squared_error\", cv=kf))\n    return(rmse)\n\ndef eval_model(model, name):\n    start_time = time.time()\n    score = rmsle_cv(model)\n    print(\"{} score: {:.4f} ({:.4f}),     execution time: {:.1f}\".format(name, score.mean(), score.std(), time.time()-start_time))","6f61bd91":"\nmod_lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.005, random_state=1))\neval_model(mod_lasso, \"lasso\")\nmod_enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\neval_model(mod_enet, \"enet\")\nmod_cat = CatBoostRegressor(iterations=10000, learning_rate=0.01,\n                            depth=5, eval_metric='RMSE',\n                            colsample_bylevel=0.7, random_seed = 17, silent=True,\n                            bagging_temperature = 0.2, early_stopping_rounds=200)\neval_model(mod_cat, \"cat\")\nmod_gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=3, max_features='sqrt',\n                                   min_samples_leaf=7, min_samples_split=10, \n                                   loss='huber', random_state=5)\neval_model(mod_gboost, \"gboost\")\nmod_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=7, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=7, nthread=-1)\neval_model(mod_xgb, \"xgb\")\nmod_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=63,\n                              learning_rate=0.05, n_estimators=650,\n                              max_bin=58, bagging_fraction=0.80,max_depth=6,\n                              bagging_freq=5, feature_fraction=0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=7, min_sum_hessian_in_leaf=11)\neval_model(mod_lgb, \"lgb\")","9aef3c4a":"def valid(model):\n    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state=0)\n    model.fit(x_train.values,y_train.values)\n    train_pred = model.predict(test.values)\n    print(rmsle(y, train_pred))\n    return (pred)","c78eb07e":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \nmod_stacked = StackingAveragedModels(base_models = (mod_cat, mod_xgb, mod_gboost, mod_lgb), meta_model = mod_enet)\neval_model(mod_stacked, \"stacked\")","60867dab":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef predict(model):\n    model.fit(x.values,y.values)\n    train_pred = model.predict(x.values)\n    pred = np.expm1(model.predict(x_test.values))\n    print(rmsle(y, train_pred))\n    return (pred)","ab231e87":"pre_lasso = predict(mod_lasso)\npre_enet = predict(mod_enet)\npre_xgb = predict(mod_xgb)\npre_gboost = predict(mod_gboost)\npre_lgb = predict(mod_lgb)\npre_stack1 = predict(mod_stacked)","efad0209":"test['id']=test.index\ntest['SalePrice']=pre_stack1 \ntest[['id','SalePrice']].to_csv('submission_Dragon2.csv', index=False)\ntest[['id','SalePrice']].head()","9799362c":"test['id']=test.index\ntest['SalePrice']=0.7*pre_stack1+0.15*pre_lgb+0.15*pre_xgb\ntest[['id','SalePrice']].to_csv('submission_Dragon3.csv', index=False)\ntest[['id','SalePrice']].head()","ac045f52":"test['id']=test.index\ntest['SalePrice']=0.5*pre_stack1+0.15*pre_lgb+0.2*pre_xgb+0.15*pre_gboost\ntest[['id','SalePrice']].to_csv('submission_Dragon4.csv', index=False)\ntest[['id','SalePrice']].head()","aaebe4ce":"\u4e0a\u9762\u4e00\u90e8\u5206\u6765\u81eakernel\u548c\u6211\u81ea\u5df1\u7684\u4e00\u4e9b\u6539\u8fdb,\u6211\u82f1\u8bed\u4e0d\u600e\u4e48\u597d,70\u4e2a\u7279\u5f81\u63cf\u8ff0\u5bf9\u4e8e\u6211\u6765\u8bf4\u592a\u56f0\u96be\u4e86","5a8479ca":"\u623f\u4ef7\u7684\u4fee\u5efa\u5e74\u4efd\u548c\u9500\u552e\u4ef7\u683c\u6709\u4e00\u4e2a\u8d8b\u52bf","2a9c5ae7":"\u6bcf\u5e74\u7684\u4f4f\u5b85\u9762\u79ef\u6709\u7740\u5f88\u4e00\u4e2a\u8f83\u4e3a\u663e\u8457\u7684\u4e0b\u964d\u8d8b\u52bf,\u8bf4\u660e\u623f\u4ef7\u53ea\u4f1a\u8d8a\u6765\u8d8a\u9ad8","5a799a63":"\u5730\u4e0b\u5ba4\u603b\u9762\u79ef","84234f6a":"\u6574\u4f53\u6750\u6599\u548c\u9970\u9762\u8d28\u91cf ","c85c8517":"\u770b\u8d77\u6765\u53ea\u6709WD\u5728GRVL\u5904\u9500\u552e,\u5e76\u4e14\u52a0\u4e2a\u6ca1\u6709PAVE\u8857\u9053\u7684\u597d,\u6570\u636e\u96c6\u5185\u5927\u90e8\u5206\u7684\u623f\u5b50\u90fd\u662f\u5728Pave\u5904\u9500\u552e","65a927f9":"annot=True,annot_kws={'size': 10}, fmt=\".2f\" \u8fd9\u51e0\u4e2a\u53c2\u6570\u6bd4\u8f83\u91cd\u8981","c83dae04":"\u6570\u636e\u5f88\u660e\u663e\u670d\u4ece\u4e00\u4e2a\u5206\u5e03","5dab8675":"\u5730\u4e0a\u751f\u6d3b\u533a\u9762\u79ef\/\u5355\u4f4d\u662f\u5e73\u65b9\u82f1\u5c3a","68ff1015":"\u5728\u4e0d\u540c\u623f\u5c4b\u7684\u7c7b\u578b\u7684\u57fa\u7840\u4e0a\u505a\u4e86\u53ef\u89c6\u5316,\u53d1\u73b0\u4ef7\u683c\u4e0e\u8fd9\u51e0\u4e2a\u76f8\u5173\u6027\u5f88\u5f3a\u7684\u7279\u5f81\u8054\u7cfb\u5f88\u5927"}}