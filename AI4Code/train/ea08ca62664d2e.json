{"cell_type":{"d83918c0":"code","2da38942":"code","2376437b":"code","7c8f2cbc":"code","7e372ae6":"code","366ba41f":"code","fcd32ca8":"code","68247a40":"code","8bb01f01":"code","10e5d0ab":"code","673b560e":"code","4d3adef4":"code","e6df1c60":"code","4b642575":"code","cbc63045":"code","19bb1aaf":"code","439d4055":"code","52ebd2da":"code","edb8d312":"code","825294ce":"code","a8fd5a17":"code","7102e3cd":"code","16baa164":"code","85fce2bd":"code","ee2908fe":"code","e8e51a73":"code","e8b8dfd7":"code","b2f5ff5e":"code","412938dc":"code","3c8d832c":"code","099b577b":"code","4bd5bc33":"code","63745cdc":"code","0a7a6ea5":"code","9f77077c":"code","b61894d1":"code","a7bc42a5":"code","8df0ccff":"code","2d1976f6":"code","a0dc4fed":"code","b3920044":"code","0e41d982":"code","c6e678bf":"code","6f30aceb":"code","cf1cc4cb":"code","5638add7":"code","b691ab5f":"code","3ad75c78":"code","9f65a6eb":"code","2d325bdd":"code","9a47e4e3":"code","66eb414c":"code","b98f9dd4":"code","62f41df1":"code","a59abdbf":"code","47c356d3":"code","17bf7f36":"code","5f724012":"code","d92e115b":"code","bbd62dd4":"code","215f7537":"markdown","d4ea371a":"markdown","f4783f4e":"markdown","51dfd3c9":"markdown","4c508b57":"markdown","256606a8":"markdown","1c771ea1":"markdown","0a1e90e1":"markdown","77bf0ef4":"markdown","29bb3e0a":"markdown","318d244a":"markdown","dc2a59c8":"markdown","1f9aa0a1":"markdown","bbe82b80":"markdown","ced4b539":"markdown","e486171f":"markdown","57bc4a47":"markdown","2c7db738":"markdown","e889315d":"markdown","0ea2a23c":"markdown","6e9e2b03":"markdown","99a297c9":"markdown","51813396":"markdown","a70e4f56":"markdown","d016fcbc":"markdown","6f91bf17":"markdown","e9c71172":"markdown","d44d2bed":"markdown","8e4b74fa":"markdown","fcdd8874":"markdown","7e71fb2e":"markdown","39aa912c":"markdown","3ea55840":"markdown","dae23cb3":"markdown","4092f017":"markdown","f9d69850":"markdown","7bb98893":"markdown","95eb5c13":"markdown","16fcc507":"markdown","56f04eca":"markdown","6896514b":"markdown","a90bc31c":"markdown","787b8dd9":"markdown","e407be14":"markdown","46c14d4f":"markdown","73debee2":"markdown","906e2bfe":"markdown","74b37f8c":"markdown","4402e601":"markdown","2cb8322e":"markdown","44a25e72":"markdown","07442812":"markdown","d3f9a031":"markdown","e80938a8":"markdown","fe47cfdf":"markdown","05f268d7":"markdown","e7e6c04d":"markdown","cb8ea71e":"markdown","bd3490d3":"markdown","1b91e6c5":"markdown","9da9efb9":"markdown","6e0bac16":"markdown","ed4f3dcf":"markdown","7d044c8a":"markdown","d0ccace6":"markdown","0fd8768f":"markdown","db50d224":"markdown","145ec637":"markdown","20e308ea":"markdown","c8496c86":"markdown","3cf1e343":"markdown","205e2a32":"markdown","1b363018":"markdown","759b0e7c":"markdown","8e6ab779":"markdown","f6735b5e":"markdown"},"source":{"d83918c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","2da38942":"!ls \/kaggle\/input\/CORD-19-research-challenge\/","2376437b":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})","7c8f2cbc":"# Conversion of publish_time column to Datetime and add a column of year of publication\npublish_dates = []\nfor d in meta_df['publish_time']:\n    try:\n        publish_dates.append(datetime.strptime(d,\"%Y-%m-%d\"))\n    except Exception as e:\n        if isinstance(d, str):\n            publish_dates.append(datetime(int(d),1,1))\n        else:\n            publish_dates.append(datetime(1900,1,1))\n\nmeta_df.drop('publish_time',axis=1)\nmeta_df['publish_time'] = publish_dates\nmeta_df['year'] = meta_df['publish_time'].apply(lambda x: x.year)\n\nmeta_df.info()","7e372ae6":"meta_df[meta_df['sha'].isnull()]","366ba41f":"len(meta_df['cord_uid'].unique())","fcd32ca8":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","68247a40":"# dictionary to build df_covid19 dataset for the unsupervised clustering\ndict_1={'cord_uid': [], 'title':[], 'abstract':[], 'body_text':[]}\n\n# Dictionary that will include [paper_id and its cited_papers] useful for the most cited papers calculation\ndict_2={'cord_uid':[], 'year' :[], 'title':[]}\n\n# function that gets paper id based on its sha code or title if sha code does not exit\ndef get_paper_id(meta_df,sha,title):\n    # try to get metadata information thru sha\n    meta_data = meta_df.loc[meta_df['sha'] == sha]['cord_uid']\n    # no metadata, get cord_uid thru the title\n    if len(meta_data) == 0:\n        p_id = meta_df.loc[meta_df['title'] == title]['cord_uid']\n    else:\n        p_id = meta_df.loc[meta_df['sha'] == sha]['cord_uid']\n    return p_id\n\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    # Open the content of each json file\n    content = json.load(open(entry))\n    \n    ################ get the paper id from meta_df\n    paper_id = get_paper_id(meta_df, content['paper_id'], content['metadata']['title'])\n    \n    if paper_id.empty:\n        continue\n    else:\n        paper_id = paper_id.iloc[0]\n    \n    ## get meta_data\n    meta_data = meta_df.loc[meta_df['cord_uid'] == paper_id]\n\n    ################# get title, abstract and body_text\n    # Append paper id\n    dict_1['cord_uid'].append(paper_id)\n    # get paper title\n    paper_title = content['metadata']['title']\n    # if title is empty in json data, get it from meta data\n    if len(paper_title)==0:\n        dict_1['title'].append(meta_data['title'].values[0])\n    else:\n        dict_1['title'].append(paper_title)\n    \n    abstract=[]\n    body_text =[]\n    \n    # if abstract is not provided in json data, get it from meta data\n    try:\n        # try getting data from the json file\n        for a in content['abstract']:\n            abstract.append(a['text'])\n        \n        if len(abstract)==0:\n            abstract = meta_data['abstract'].values[0]\n        else:\n            # join abstract \n            abstract = ''.join(str(abstract))\n    # otherwise get it from meta data\n    except Exception as e:\n        abstract= meta_data['abstract'].values[0]\n    \n    \n    # get body text\n    for t in content['body_text']:\n        body_text.append(t['text'])\n    \n    # join body text\n    body_text = ''.join(body_text)\n    \n    # cap body_text length to 1 million characters (spacy constraint)\n    if len(body_text) > 1000000:\n        body_text = body_text[0:999999]\n    \n    dict_1['abstract'].append(abstract)\n    dict_1['body_text'].append(body_text)\n    \n    ################## get all biliographic references (BIBREF)\n    refs = []\n    for e in content['bib_entries']:\n        refs.append(e)\n        \n    # get cited references of this paper\n    bib_entries = content['bib_entries']\n    \n    for ref in refs:\n        dict_2['cord_uid'].append(paper_id)\n        dict_2['year'].append(str(bib_entries[ref]['year']))\n        dict_2['title'].append(bib_entries[ref]['title'])\n        \n\ndf_covid19 = pd.DataFrame(dict_1,columns=['cord_uid','title','abstract','body_text'])\ndf_bibrefs = pd.DataFrame(dict_2, columns=['cord_uid','year','title'])\ndf_covid19.head()","8bb01f01":"df_covid19.head()","10e5d0ab":"df_bibrefs.head()","673b560e":"# Function to create a foreign key combining the year and the title of each cited article\ndef f(x,y):\n    return str(x) + '-' + str(y)\n\ndf_bibrefs['key'] = df_bibrefs.apply(lambda x: f(x.year, x.title), axis = 1)","4d3adef4":"# groupby and count to estimate the number of occuerence of each cited paper within the corpus\nnew_df_bibrefs = df_bibrefs.groupby('key').count()","e6df1c60":"# drop year and title columns to keep only cord_uid and key as index\nnew_df_bibrefs = new_df_bibrefs.drop(['year', 'title'] , axis = 1)","4b642575":"# creating the foreign key (year - title) within meta_df\nmeta_df['key'] = meta_df.apply(lambda x: f(x.year, x.title), axis =1)","cbc63045":"# we reset key index\nnew_df_bibrefs= new_df_bibrefs.reset_index()","19bb1aaf":"df = pd.merge(meta_df,new_df_bibrefs, on='key', suffixes=('','_y'))\ndf = df.sort_values(by=['cord_uid_y'], ascending = False)\ndf = df.rename(columns= {'cord_uid_y' : 'nb_citations'})","439d4055":"df.shape","52ebd2da":"df.groupby(by=['year']).count()['cord_uid'].plot(kind='bar')","edb8d312":"dict_={'cord_uid' :[], 'authors' : []}\n\nfor i in range(0, len(df)):\n    \n    authors = str(df['authors'].iloc[i]).split(';')\n    paper_id = df['cord_uid'].iloc[i]\n        \n    for author in authors:\n        dict_['cord_uid'].append(paper_id)\n        dict_['authors'].append(author)\n        \ndf_authors = pd.DataFrame(dict_, columns=['cord_uid','authors'])","825294ce":"df_authors.head()","a8fd5a17":"df_authors_pub = df_authors.groupby('authors').count()","7102e3cd":"# we reset key index\ndf_authors_pub= df_authors_pub.reset_index()\n# filter only on rows without NaN\ndf_authors_pub = df_authors_pub.sort_values(by=['cord_uid'], ascending = False)[['authors','cord_uid']]","16baa164":"# remove nan line\ndf_authors_pub= df_authors_pub.drop(index=0)","85fce2bd":"# Merge df_authors with df to get the number of citations per paper\ndf_authors = pd.merge(df_authors, df, on= 'cord_uid', suffixes =('','_y'))","ee2908fe":"df_authors = df_authors[df_authors['authors'].notna()]","e8e51a73":"# Groupby on authors and sum of the number of citations\ndf_authors_cited = df_authors.groupby(by=[\"authors\"])['nb_citations'].sum().reset_index().sort_values(\"nb_citations\", ascending=False)","e8b8dfd7":"df_authors_cited.head()","b2f5ff5e":"df_plot = df_authors_pub.merge(df_authors_cited, on='authors')\ndf_plot = df_plot.drop_duplicates('authors')\ndf_plot = df_plot.rename(columns={'cord_uid': 'nb_publications'})","412938dc":"df_plot = df_plot[df_plot['nb_publications']>10]","3c8d832c":"import bokeh\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, Slider, TapTool, TextInput, RadioButtonGroup\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox\n\noutput_notebook()\nsource = ColumnDataSource(df_plot)\ntools = \"hover, box_zoom, undo, crosshair\"\np = figure()\np.scatter('nb_publications', 'nb_citations', source = source,alpha=1)\n\np.add_tools(\n    HoverTool(\n        tooltips=[('Author', '@authors'), ('Nb citations','@nb_citations'), ('Nb publications', '@nb_publications')]\n    )\n)\nshow(p)","099b577b":"df_scope = df[(df.nb_citations >10) & (df.year >2000)]\ndf_scope = df_scope[['cord_uid','nb_citations']]\ndf_cluster = pd.merge(df_scope,df_covid19, on='cord_uid')\ndf_cluster = df_cluster.sort_values(by=['nb_citations'], ascending = False)\ndf_cluster.head()","4bd5bc33":"df_cluster.drop_duplicates(subset ='cord_uid', keep='first',inplace=True)\ndf_cluster['abstract'] = df_cluster['abstract'].apply(lambda x: str(x))\ndf_cluster.shape","63745cdc":"import spacy\nimport string\nimport re\nimport time\nfrom collections import Counter\n\n# Spacy imports\nfrom spacy.lang.en.examples import sentences\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n#Skit learn imports\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.compose import ColumnTransformer\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    # using str to avoid float values error\n    return str(text).strip().lower()\n\n# Named entities extractors\n# Load English NLP object (long size for entities recognition and GloVe vectorisation)\nnlp= spacy.load('en_core_web_lg')\n\n\n# Create our list of punctuation marks\npunctuations = string.punctuation\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\nparser = English()\n\n# Creating our tokenizer function (w\/o named entities)\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(sentence)\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations  and len(word) > 2]\n    # return preprocessed list of tokens\n    return mytokens\n\n# Custom transformer using spaCy\nclass Cleaner(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\nclusterer = KMeans(n_clusters=10, random_state = 42)\n\npipeline = Pipeline([\n    # Use ColumnTransformer to combine the features from title, abstract and body text\n    ('union', ColumnTransformer(\n        [\n            # Pulling features from the article's title line (first column)\n            ('title', TfidfVectorizer(tokenizer=spacy_tokenizer), 0),\n            \n            # Pipeline for standard bag-of-words model for article (second column)\n            ('abstract_bow', Pipeline([\n                ('clean', Cleaner()),\n                ('tfidf_abstract', TfidfVectorizer(tokenizer=spacy_tokenizer, max_features=2**9, max_df=0.6, min_df=10)),\n                ('best', TruncatedSVD(n_components=50)),\n            ]), 1),\n\n            # Pipeline for standard bag-of-words model for article (second column)\n            ('body_bow', Pipeline([\n                ('clean', Cleaner()),\n                ('tfidf_body', TfidfVectorizer(tokenizer=spacy_tokenizer, max_features=2**12, max_df=0.6, min_df=10)),\n                ('best', TruncatedSVD(n_components=50)),\n            ]), 2),\n\n        ],\n\n        # weight components in ColumnTransformer\n        transformer_weights={\n            'title': 0.25,\n            'abstract_bow':0.15,\n            'body_bow': 0.60,\n            #'named_entities': 1,\n        }\n    )),\n\n    # Use a k-mean clusterer on the combined features\n    ('kmean', clusterer ),\n])","0a7a6ea5":"data_a = df_cluster[['title','abstract','body_text']]\nX_a = pipeline.fit_transform(data_a)\nlabels_a = pipeline.predict(data_a)\ndata_a['labels'] = labels_a","9f77077c":"from sklearn.manifold import TSNE\nimport seaborn as sns\ny = labels_a\ntsne = TSNE(verbose=1)\nX_embedded_a = tsne.fit_transform(X_a)","b61894d1":"# Prepare data for display \ndef get_breaks(content, length):\n    data = \"\"\n    words = str(content).split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\ndf_cluster['title_display'] = df_cluster['title'].apply(lambda x: get_breaks(x,40))\ndf_cluster['abstract_display'] = df_cluster['abstract'].apply(lambda x: ' '.join(x.split(' ')[:100])) \ndf_cluster['abstract_display'] = df_cluster['abstract'].apply(lambda x: get_breaks(x,40)) ","a7bc42a5":"df_temp = df_cluster.merge(meta_df, on='cord_uid', suffixes = ('','_y'))\ndf_cluster['journal'] = df_temp.drop_duplicates('cord_uid')['journal']\ndf_cluster['authors'] = df_temp.drop_duplicates('cord_uid')['authors']","8df0ccff":"# Generating the clustering display\noutput_notebook()\ny_labels = labels_a\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded_a[:,0], \n    y= X_embedded_a[:,1],\n    x_backup = X_embedded_a[:,0],\n    y_backup = X_embedded_a[:,1],\n    desc= y_labels, \n    titles= df_cluster['title_display'],\n    authors = df_cluster['authors'],\n    journal =df_cluster['journal'],\n    abstract = df_cluster['abstract_display'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n], point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Most cited papers last 20 years, Clustered(K-Means), Tf-idf with Title, Abstract and  Plain Text\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend_group = 'labels')\n\n\n#header\nheader = Div(text=\"\"\"<h1> Most cited papers clustering 2000 - 2020 <\/h1>\"\"\")\n\n# show\nshow(column(header,p))","2d1976f6":"# How to save clustering data\ndata_a.to_csv('clustering_A.csv', index=False)\noutcome = pd.DataFrame(X_a)\noutcome.to_csv('X_clustering_A.csv',encoding='utf-8', index = False)\n\n# How to load a saved clustering later\n# data_a = pd.read_csv('clustering_A.csv')\n# labels_a = data['labels']\n#X_a = pd.read_csv('X_clustering_A.csv').to_numpy()","a0dc4fed":"def find_virus(x):\n    count = 0\n    x = str(x)\n    count += x.lower().count('covid-19')\n    count += x.lower().count('sars-cov-2')\n    return count\n\ndf_covid19['covid-19']= df_covid19[['title','abstract','body_text']].apply(lambda x: find_virus(x.title)+find_virus(x.abstract)+find_virus(x.body_text),axis=1)","b3920044":"df_only_covid19 = df_covid19.loc[df_covid19['covid-19']>10].sort_values(by=['covid-19'], ascending=False)\n# Removing duplicates\ndf_only_covid19.drop_duplicates(subset ='cord_uid', keep='first',inplace=True)","0e41d982":"df_only_covid19.shape","c6e678bf":"df_only_covid19 = df_only_covid19.merge(meta_df, on='cord_uid', suffixes=('', '_y'))\ndf_only_covid19 = df_only_covid19[df_only_covid19['year']>2018]\ndf_only_covid19.drop_duplicates(subset ='cord_uid', keep='first',inplace=True)","6f30aceb":"# populating titles that say \"Comment\" with title from meta_df\ndf_only_covid19['title'] = df_only_covid19[['title','title_y']].apply(lambda x: x.title_y if x.title=='Comment' else x.title, axis=1)","cf1cc4cb":"# populating null abstracts by the top 200 words of the body text (it is just a workaround to avoid having a cluster with only papers with null abstract)\ndf_only_covid19['abstract'] = df_only_covid19['abstract'].fillna('missing')\ndf_only_covid19['abstract'] = df_only_covid19[['abstract','body_text']].apply(lambda x: ' '.join(x.body_text.split(' ')[:200]) if x.abstract =='missing' else x.abstract, axis=1)","5638add7":"clusterer = KMeans(n_clusters=6, random_state = 42)\n\n\npipeline = Pipeline([\n    # Use ColumnTransformer to combine the features from title, abstract and body text\n    ('union', ColumnTransformer(\n        [\n            # Pulling features from the article's title line (first column)\n            ('title', TfidfVectorizer(tokenizer=spacy_tokenizer), 0),\n            \n            # Pipeline for standard bag-of-words model for article (second column)\n            ('abstract_bow', Pipeline([\n                ('clean', Cleaner()),\n                ('tfidf_abstract', TfidfVectorizer(tokenizer=spacy_tokenizer, max_features=2**8, max_df=0.6, min_df=10)),\n                ('best', TruncatedSVD(n_components=50)),\n            ]), 1),\n\n            # Pipeline for standard bag-of-words model for article (second column)\n            ('body_bow', Pipeline([\n                ('clean', Cleaner()),\n                ('tfidf_body', TfidfVectorizer(tokenizer=spacy_tokenizer, max_features=2**12, max_df=0.6, min_df=10)),\n                ('best', TruncatedSVD(n_components=50)),\n            ]), 2),\n\n        ],\n\n        # weight components in ColumnTransformer\n        transformer_weights={\n            'title': 0.4,\n            'abstract_bow':0.4,\n            'body_bow': 0.2,\n        }\n    )),\n\n    # Use a k-mean clusterer on the combined features\n    ('kmean', clusterer ),\n])","b691ab5f":"data = df_only_covid19[['title','abstract','body_text']]\nX = pipeline.fit_transform(data)\nlabels = pipeline.predict(data)\ndata['labels'] = labels\nX_embedded = tsne.fit_transform(X)","3ad75c78":"df_only_covid19['title_display'] = df_only_covid19['title'].apply(lambda x: get_breaks(str(x),40))\ndf_only_covid19['abstract_display'] = df_only_covid19['abstract'].apply(lambda x: ' '.join(str(x).split(' ')[:100])) \ndf_only_covid19['abstract_display'] = df_only_covid19['abstract'].apply(lambda x: get_breaks(str(x),40)) ","9f65a6eb":"output_notebook()\ny_labels = labels\n\ntitle = df_only_covid19['title']\ntitle = [text[0:40] for text in title]\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded[:,0], \n    y= X_embedded[:,1],\n    x_backup = X_embedded[:,0],\n    y_backup = X_embedded[:,1],\n    desc= y_labels, \n    titles= df_only_covid19['title_display'],\n    authors = title,\n    journal =title,\n    abstract = df_only_covid19['abstract_display'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Tf-idf with Title, Abstract & Plain Text\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n#header\nheader = Div(text=\"\"\"<h1>COVID-19 Research Papers Cluster - 2019\/2020 <\/h1>\"\"\")\n\n# show\nshow(column(header,p))","2d325bdd":"# How to save clustering data\ndata.to_csv('clustering_B_final.csv', index=False)\noutcome = pd.DataFrame(X)\noutcome.to_csv('X_clustering_B_final.csv',encoding='utf-8', index = False)\n","9a47e4e3":"df_plot = df_plot.loc[df_plot['nb_publications']> 10]","66eb414c":"df_experts = df_plot.loc[df_plot['nb_citations']> 50]","b98f9dd4":"experts = df_experts['authors'].tolist()","62f41df1":"def check_author(x):\n    count = 0\n    authors = str(x).split(';')\n    for author in authors:\n        if author in experts:\n            count+=1\n    return count\n\ndf_only_covid19['by_expert'] = df_only_covid19['authors'].apply(lambda x: check_author(x))","a59abdbf":"pd.set_option('display.max_colwidth', -1)\ndf_top_papers = df_only_covid19[df_only_covid19['by_expert']>0][['title','abstract','authors', 'cord_uid']]\ndf_top_papers.to_excel('100_top_papers.xlsx')","47c356d3":"root_path = '\/kaggle\/input\/100-top-papers-labelling\/'\nlabelling_path = f'{root_path}\/100_top_papers_labelling.xlsx'\ndf_labels = pd.read_excel(labelling_path)","17bf7f36":"df_labels = df_labels.rename(columns = {'Title' : 'title'})\ndf_labels = df_labels.drop(['index'], axis=1)","5f724012":"df_top_papers = df_top_papers.merge(df_labels, on='title')","d92e115b":"df_top_papers","bbd62dd4":"df_top_papers.groupby('label').count()['title'].plot(kind ='bar')","215f7537":"# Clustering (B) : Covid-19 papers","d4ea371a":"### Disclaimer\nThis publication does not represent the thoughts or opinions of my employer, nor uses its assets. It is solely based on our personal views and on our own initiative as citizens of the world","f4783f4e":"A this stage, we acheived the citation count per paper. Now we need to acheive it on the author level. ","51dfd3c9":"Delete rows without an author","4c508b57":"We drop duplicates and caste abstract to avoid float errors","256606a8":"## Generate scatter plot : (publications vs citations) per author \nObjective : Find out who are the experts in the field and renown authors","1c771ea1":"Clustering may vary because the nature of the algorithms used (unsupervised) but more\/less the same clusters tend to clearly appear. As you can see in the figure below, clusters tend to group articles about various respiratory viruses outbreaks during the last 20 years. Others group precious articles about prevention, virus transmission, impact of environment, molecular immune response, ..etc.\n\nThese clusters could be found in the input repo (Clustering A results) or consulted here in [this notebook](https:\/\/www.kaggle.com\/hadimahihenni\/clustering-a-visualisation\/). ","0a1e90e1":"## Citation count per paper","77bf0ef4":"We performed a clear link between each cluster and the requested tasks of the challenge.\n\n![image.png](attachment:image.png)","29bb3e0a":"Get path to all JSON files","318d244a":"We also calculate the number of citations per author (= number of publications x number of citations per paper)","dc2a59c8":"## Main findings and link to the challenge tasks","1f9aa0a1":"This is an overview of the labels found by reading the top 100 papers and their link to the challenge tasks (see also 100-top-covid19-papers dataset in the input repo). This work could be combined with the clustering B in order to build a labeled bigger dataset to train a supervised ML model that could be generalised to the whole corpus.  ","bbe82b80":"Insert a new column about the year of publication of each paper","ced4b539":"[Cite : COVID-19 Litterature Clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering)\n\n@inproceedings {COVID-19 Literature Clustering,\n    author = {Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward},\n    title = {COVID-19 Literature Clustering},\n    year = {2020},\n    month = {April},\n    location = {University of Maryland Baltimore County (UMBC), Baltimore, MD, USA},\n    note={Malware Research Group},\n    url = {\\url{https:\/\/github.com\/MaksimEkin\/COVID19-Literature-Clustering}},\n    howpublished = {TBA}\n}","e486171f":"# Clustering (A) : most cited papers","57bc4a47":"We run the below clustering script to instanciate the data pipeline","2c7db738":"In the following long script, we prepare two dataframes : \n1. df_covid19 that include (cord_uid, title, abstract and body_text) that will feed the clustering pipeline\n2. df_bibrefs that will be used to compute the citations count per paper & author","e889315d":"Two different runs of the covid-19 papers result in the emergence of similar 6 clusters highlighted below : \n- C0 : Models predicting pandemic, transmission dynamics and non pharmacy intervention (related to Task 1)\n- C1 : Clinical characterisation, diagnosis, treatment, symptoms (beyond lungs) (related to Task 2)\n- C2 : Testing, diagnostics & detection protocols (related to Task 1 - Clinical process)\n- C3 : Healthcare workers impact, patients subgroups, coping strategies in different medical departments (Related to Task 2)\n- C4 : Covid-19 features (structure, genetics, origin, evolution, mutation) + Molecular immune response and potential treatments \/ vaccines (related to Task 3 and Task 4)\n- C5 : General reviews\n\n\nData is provided for the clusters on the left in the slide below. To consult the clustering results commented below, please see [this notebook](https:\/\/www.kaggle.com\/hadimahihenni\/clustering-b-visualisation)","0ea2a23c":"... launch the fitting, transformation and dimensionality reduction with T-SNE ...","6e9e2b03":"![image.png](attachment:image.png)","99a297c9":"Finally, merge meta_df and new_df_bibrefs to have citation count in this new datafrme called (df)","51813396":"After plotting the evolution of the number of publications per year, we see a significant increase of scientific research after 2000 with several virus outbreak that happend in the 2000's and 2010's. This justifiy the assumption that we took in our approach to focus only on papers published after 2000; ","a70e4f56":"# Goal & Problem Statement","d016fcbc":"Let's flag the papers that are published by the top experts in the field","6f91bf17":"... and papers that are published after 2018 (ie. in 2019 and 2020)","e9c71172":"# Data preparation for citations count and clustering","d44d2bed":"![image.png](attachment:image.png)\n\n\nOur approach focused on articles published from the year 2000, two years before the emergence of SARS_COV and when the number of publications relating to the subject shows a significant increase in number (see chart below). This seems relevant to our tasks due to the similarity of the recent corona viruses outbreaks to the COVID-19 pandemic. We also assumed that any important information from before would have been considered when dealing with recent outbreaks and hence found in these articles.\n \nSecondly, we considered important to identify the experts in the field and the articles that are viewed by the community as influential. Scientific brilliance of a research project and any articles resulting from it comes from being rigorous in theoretical concepts, methodological design and interpretive discussion. Comprehensive reviews can also hold such weight. Such articles are likely to be highly cited (although this can not be guaranteed) earning their author(s) prestige in the scientific community. From this principle we focused our search on articles that have been cited at least 10 times within the provided corpus.\n \nThis approach provided a neat clustering using kmeans algorithm with 10 clusters and feature engineering using Tf-Idf text vectoriser leveraging information from title, abstract and body text by using the respective weights (25%, 15% and 60%). In fact, in scientific research, title and abstract can accurately summarize the body of the paper that we thought should be considered in this task.\n \nAs cases resulting from SARS-COV2 infection have only been officially reported on December 2019, the above principle cannot be applied to recent papers treating the subject. Therefore, we split our approach into two steps.\n \nFirst we prepared a relevant dataset by considering only the papers that:\n \n\u00b7  \twere published in 2019 and 2020.\n\u00b7  \thave a word count of covid-19 or SARS-CoV-2 greater than 10 times for the algorithm to consider it relevant.\n \nAdditionally, unsupervised clustering (k-mean) and tf-idf transformer was used to group articles by theme, then link them to the requested tasks. We iteratively analysed the found clusters with a domain expert to fine tune the number of clusters and the transformer weights. In fact, we reviewed the weights used above by giving more importance to the title and abstract since the body text of recent papers tends to mention all different aspects of the virus. Missing abstract have been replaced by top 200 words of the body text.\n \nNeater clusters are found with (k = 6) and weights (title = 40%, abstract = 40% and body = 20%) after many iterations. Further clustering could be done on the body text only at the section level to be able to extract detailed information about method and material used, results, findings and discussion aspects.\n \n\nWe combined the results from sections A and B.  Identifying the most cited articles in task A, enabled us to generate a list of experts in the field. As the COVID-19 infection has only been known to us since November 2019, we could not use the number of citations as a calculating factor. To fine tune our selection, we identified authors with more than 10 publications who have been cited at least 50 times. 94 authors emerged from this crossover.  The latter had resulted in 90 articles. The domain expert clustered these articles in 6 more specific clusters and related them to the tasks for the challenge. This step could be considered as labelling for further \u201csupervised clustering\u201d and\/or recommendation system for similar articles within the corpus. Meanwhile, this \u201cmanual\u201d clustering could help any researcher in the field to get started with what the algorithms suggest are the \u201cgolden nuggets\u201d papers. \n\n\n\nBelow you will find the details of the data preparation, transformation and main findings. This work relies as well on other related works that are cited at the end of this publication. ","8e4b74fa":"Let's start by check meta data. First, can we rely on the \"sha\" as primary key for each paper ? ","fcdd8874":"Our analytical approach allowed us to distinguish two main types of information to be dissected later in greater details, articles before and after COVID-19 emergence. This approach enabled us to separate our well established knowledge of viruses that belong to the same family as the COVID-19, their genetics, structure, transmision, evolution, mutation, clinical and epidemiological characteristics and current treatment or prevention protocols. It also give a great insight of non-pharmaceutical methods that can be applied for the prevention of global pandemics, their management and their social and economics impacts. These articles can be used by experts in the field as a starting point and also as a reservoir for generating predictive models. In the absence of time for conventional research methods, we can turn to artificial intelligence. The information gathered on the coronavirus family can be fed into models, that can based on differences and similarities predict vital information on COVID-19 and even used for the development of a vaccine.\n\n\nVarying the number of clusters and the features weights, we managed to cluster the articles published after the surfacing of the first case of COVID-19 infection in 6 prominent categories. This can be of particular importance for researcher with different interests in the infection and can be a great tool to filter through particular topic. Equally, this clustering can be of use to generate clinical, treatment and management protocols, identifying risk factors and patient groups at risk.\n\nThe approach we adopted allowed us to identify the most renowned authors in the field. Due to the limited time scope, we considered articles published by these authors after the emergence of COVID-19 of higher credibility. They can be used as solid references in ongoing research. These can be a great source of information for collaborations across the globe. These articles were manually labelled in accordance with clustering B. Interestingly, there was no need to create new subclusters, which validates our model. \n\nThe next step would be to combine manual and automatic labels found in clustering B to train a supervised model that could be expanded to the whole corpus.","7e71fb2e":"This work has been conducted by myself ([Hadi Mahihenni](https:\/\/www.linkedin.com\/in\/hadimahihenni\/)) and my wife, [Dr. Lilia Mecif](https:\/\/www.linkedin.com\/in\/lilia-mecif-702b4250\/). We teamed up to bring our modest contribution to this challenge facing our livelihood. Lilia holds a PhD in Pharmacy, she brings her scentific experience in biology and scientific research and I am an engineer (with background in mathematics and physics) with high interest in data science. \n\nWe welcome feedback so that we can continue to improve this project","39aa912c":"# Citations Count\n","3ea55840":"[Cite : COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool)","dae23cb3":"We first start by identifying all articles that talk about Covid-19 or SARS-CoV-2 at least once by counting these words in the title, abstract and body text. ","4092f017":"## Data preparation","f9d69850":"First, we need to create a foreign key (year - title) in df_bibrefs to be able to groupby cited articles and count their occurrence within the corpus. Then join meta_df & df_bibrefs on this foreign key to able to have the citation count with the primary key (cord_uid)","7bb98893":"Apply pipeline to the prepared dataset (df_cluster)","95eb5c13":"# Data Loading and Preparation","16fcc507":"Instantiate the pipeline with 6 clusters ...","56f04eca":"Load metadata.csv as it includes more information about papers (e.g. publish_dates, journal, ..etc).  ","6896514b":"... and prepare clustering results display ","a90bc31c":"Adding journal and authors columns, useful for clusters display","787b8dd9":"## Citations count per author (+ number of publications)","e407be14":"## Fetch All of JSON File Path","46c14d4f":"Then we calculate the number of publications by author in the corpus by groupby & count on authors column.","73debee2":"Start by building a datafrme df_authors by splitting authors from meta_df","906e2bfe":"# What are the recent papers published by the field experts and their main topics ? ","74b37f8c":"More than 51000 (over 59000 papers as of April 15th) has got a cord_uid unique code ! We will rely on this \"primary key\" for the rest of the note book.","4402e601":"# Conclusion and way forward","2cb8322e":"## Data Loading (meta data & json files) ","44a25e72":"Too many papers (more than 13000 papers) are without a \"sha\" code ! Let's see if we can use the cord_uid instead ?","07442812":"As mentioned before, we will consider only authors with more than 10 publications and 50 citations in the corpus. ","d3f9a031":"## Clustering pipeline implementation","e80938a8":"Now that we computed our labels (clusters), we need to prepare our clustering display using dimensionality reduction by T-SNE","fe47cfdf":"COVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, about COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https:\/\/pages.semanticscholar.org\/coronavirus-research) and there is a research [challenge on Kaggle](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) where all tasks could be consulted. \n\nThis project builds a data-driven approach to analyse COVID-19 dataset in order to support researchers and scientists in their current active research around SARS-CoV-2 and hence accelerate future scientific discoveries and\/or healthcare policies. \n\nWe refer below to the different challenge tasks as the following : \n* Task 1 : What is known about transmission, incubation and environmental stability ?\n* Task 2 : What do we know about COVID-19 risk factors ?\n* Task 3 : What do we know about virus genetics, origin and evolution ?\n* Task 4 : What do we know about vaccines and therapeutics ?\n* Task 5 : What has been published about medical care ?\n* Task 6 : What do we know about non-pharmaceutical interventions ? ","05f268d7":"### 2. Number of citations","e7e6c04d":"# Introduction","cb8ea71e":"As we can see there is high correlation between number of publications and citations. We also can see there are two top experts in the field with the highest number of publication and citations (Dr. Yuen, Know-Young and Dr Drosten, Christian). For further focus on the best expertise in the field, we will only consider authors who have : \n1. Number of publications > 20\n2. Number of citations > 100\n\nWe will look to later into recent publications about Covid 19 published by these authors. ","bd3490d3":"### 1. Number of publications","1b91e6c5":"![image.png](attachment:image.png)","9da9efb9":"## Main findings and link to tasks","6e0bac16":"We will consider on only papers that talk about covid19 more than 10 times ...","ed4f3dcf":"We can inspect these two important dataframes by looking into their heads","7d044c8a":"Let's plot the number of article per class. As we can see, we need to label more articles for class 2 and 5 to have a balanced sample. We can enrich this sample with articles from Clustering B and carry-out a supervised approach for the whole corpus. This could be done as a next step after Round 1. ","d0ccace6":"# COVID-19 Research Corpus Analysis and Findings","0fd8768f":"Importing important librairies for this notebook ","db50d224":"### Saving clustering results","145ec637":"# Global Approach and Key results","20e308ea":"Let's display the top papers from top experts ! There are 34 papers that are must read to start any research about COVID-19","c8496c86":"We first define the dataset corresponding the scope of this clustering (A), i.e. most cited papers (more than 10 citations), published after 2000","3cf1e343":"# Citations","205e2a32":"## Labelisation phase by an expert","1b363018":"### Saving clustering results \nSince our clustering methodology is not 100% reproducible, we save here the results (output repo). They could be loaded later to reproduce the same results that we comment here. ","759b0e7c":"## Main findings and link to tasks","8e6ab779":"![image.png](attachment:image.png)","f6735b5e":"## Clustering pipeline implementation"}}