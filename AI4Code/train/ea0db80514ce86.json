{"cell_type":{"c6312a28":"code","a63b19d0":"code","d9b52d53":"code","34ca66bd":"code","a0d01436":"code","e0401843":"code","f364c698":"code","1c48bfb5":"code","9bf8b9fc":"code","bfc1cb88":"code","01341f39":"code","b6a2c9df":"code","0f080ad1":"code","030777c6":"code","5b35c6b1":"code","10206409":"code","b04c32cc":"code","cffb4839":"code","8e5dacb4":"code","3fd8e293":"code","98f9b0a8":"markdown"},"source":{"c6312a28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport tensorflow as tf\n\n\n# Any results you write to the current directory are saved as output.","a63b19d0":"SEED = 42\ntf.random.set_random_seed(SEED)\nnp.random.seed(SEED)\n","d9b52d53":"VOCAB_SIZE = 100_000  # Limit on the number vocabulary size used for tokenization\nMAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated\/padded to this length","34ca66bd":"df_twitter = pd.read_csv(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",encoding=\"latin1\", header=None)\ndf_twitter = df_twitter.rename(columns={\n                 0:\"sentiment\",\n                 1:\"id\",\n                 2:\"time\",\n                 3:\"query\",\n                 4:\"username\",\n                 5:\"text\"\n             })\ndf_twitter = df_twitter[[\"sentiment\",\"text\"]]","a0d01436":"df_twitter[\"sentiment_label\"] = df_twitter[\"sentiment\"].map({0: 0, 4: 1})","e0401843":"import re\nfrom tensorflow.keras.preprocessing import text\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nclass TextPreprocessor(object):\n    \n    def __init__(self, vocab_size, max_sequence_length):\n        self._vocab_size = vocab_size\n        self._max_sequence_length = max_sequence_length\n        \n        #imported so the functions are avaialble in the pickle without importing\n        self._tokenizer = text.Tokenizer(num_words=self._vocab_size, oov_token = '<OOV>')\n        self._pad_sequences = pad_sequences\n        self._re_sub = re.sub\n        \n    def _clean_line(self, text):\n        text = self._re_sub(r\"http\\S+\", \"<url>\", text)\n        text = self._re_sub(r\"@[A-Za-z0-9]+\", \"<user>\", text)\n#         text = self._re_sub(r\"#[A-Za-z0-9]+\", \"\", text)\n        text = text.replace(\"RT\",\"\")\n        text = text.lower()\n        text = text.strip()\n        return text\n    \n    def fit(self, text_list):        \n        # Create vocabulary from input corpus.\n        text_list_cleaned = [self._clean_line(txt) for txt in text_list]\n        self._tokenizer.fit_on_texts(text_list)\n\n    def transform(self, text_list):        \n        # Transform text to sequence of integers\n        text_list = [self._clean_line(txt) for txt in text_list]\n        seq = self._tokenizer.texts_to_sequences(text_list)\n        padded_text_sequence = self._pad_sequences(seq, maxlen=self._max_sequence_length)\n        return padded_text_sequence","f364c698":"processor = TextPreprocessor(5, 5)\nprocessor.fit(['hello machine learning','test'])\nprocessor.transform(['hello machine learning',\"lol\"])","1c48bfb5":"from sklearn.model_selection import train_test_split\n\nsents = df_twitter.text\nlabels = np.array(df_twitter.sentiment_label)\n\n# Train and test split\nprint('Splitting Data')\nX_train, X_test, y_train, y_test = train_test_split(sents, labels, test_size=0.2, random_state = SEED)\n\nprint('X_train.shape', X_train.shape)\n\nprocessor = TextPreprocessor(VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\nprocessor.fit(X_train)\n\n# Preprocess the data\nprint('Processing data')\nX_train = processor.transform(X_train)\nprint('X_train.shape', X_train.shape)\n\nprint('Processing data')\nX_test = processor.transform(X_test)\n\nwith open('.\/preprocessor.pkl', 'wb') as f:\n    pickle.dump(processor, f)","9bf8b9fc":"print('Test Processor')\nprint('Size of Word Index:', len(processor._tokenizer.word_index))\nprocessor.transform(['Hello World'])","bfc1cb88":"LEARNING_RATE=.001\nEMBEDDING_DIM=50\nFILTERS=64\nDROPOUT_RATE=0.5\nPOOL_SIZE=2\nNUM_EPOCH=25\nBATCH_SIZE=256\nKERNEL_SIZES=[2,4,8]\n\ndef create_model(\n        embedding_matrix,\n        filters=FILTERS, \n        kernel_sizes=KERNEL_SIZES, \n        dropout_rate=DROPOUT_RATE, \n        pool_size=POOL_SIZE):\n    \n    # Input layer\n    model_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\n    # Embedding layer\n    x = tf.keras.layers.Embedding(\n        input_dim=(embedding_matrix.shape[0]),\n        output_dim=embedding_matrix.shape[1],\n        input_length=MAX_SEQUENCE_LENGTH,\n        weights=[embedding_matrix]\n    )(model_input)\n\n    x = tf.keras.layers.Dropout(rate = dropout_rate)(x)\n\n    # Convolutional block\n    conv_blocks = []\n    for kernel_size in kernel_sizes:\n        conv = tf.keras.layers.Convolution1D(\n                        filters=filters,\n                        kernel_size=kernel_size,\n                        padding=\"valid\",\n                        activation=\"relu\",\n                        strides=1\n                    )(x)\n        conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n        conv = tf.keras.layers.Flatten()(conv)\n        conv_blocks.append(conv)\n        \n    x = tf.keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n\n    x = tf.keras.layers.Dropout(rate = dropout_rate)(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    model_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(model_input, model_output)\n    \n    return model","01341f39":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef get_embedding_matrix(processor, vocab_size, embeddings_index):\n    word_index = processor._tokenizer.word_index\n    nb_words = min(vocab_size, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n\n    for word, i in word_index.items():\n        if i >= VOCAB_SIZE: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n","b6a2c9df":"EMBEDDINGS_INDEX = dict(get_coefs(*o.strip().split()) for o in open(\"..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.50d.txt\",\"r\",encoding=\"utf8\"))","0f080ad1":"EMBEDDING_MATRIX = get_embedding_matrix(processor = processor, vocab_size = VOCAB_SIZE, embeddings_index = EMBEDDINGS_INDEX)","030777c6":"print('EM shape', EMBEDDING_MATRIX.shape)","5b35c6b1":"model = create_model(embedding_matrix=EMBEDDING_MATRIX)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","10206409":"\n#keras train\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs=NUM_EPOCH, \n    batch_size=BATCH_SIZE,\n    validation_data=(X_test, y_test),\n    verbose=2,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_acc',\n            min_delta=0.005,\n            patience=3,\n            factor=0.5\n        ),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=0.005, \n            patience=5,  \n            mode='auto',\n            restore_best_weights = True\n        ),\n        tf.keras.callbacks.History()\n    ]\n)","b04c32cc":"model.save('keras_saved_model.h5')","cffb4839":"print('Y Train Mean', y_train.mean())\nprint('Y Test Mean', y_test.mean())","8e5dacb4":"import matplotlib.pyplot as plt\nplt.plot(history.epoch, history.history['loss'], label='train loss')\nplt.plot(history.epoch, history.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()","3fd8e293":"plt.plot(history.epoch, history.history['acc'], label='train acc')\nplt.plot(history.epoch, history.history['val_acc'], label='val acc')\nplt.legend()\nplt.show()","98f9b0a8":"### Basic Model"}}