{"cell_type":{"f89615ff":"code","ba70c3e8":"code","73a16361":"code","305d6615":"code","5710e82d":"code","876b0c59":"code","b56938be":"code","1d446655":"code","a0590776":"code","1aa7dced":"code","089fdb0d":"code","777a7964":"code","28301d1a":"code","550dac62":"code","cedda812":"code","a00e7cc5":"code","39af626a":"code","6e3314be":"code","ae40d71c":"code","23b2a95a":"code","5c7b3902":"code","d21c57a6":"code","74f08933":"code","0f7b975c":"code","712c4f07":"code","b6f108ef":"code","a5e9defe":"markdown","0fef2429":"markdown","cc036102":"markdown","e6dfced7":"markdown","3323cba1":"markdown","10e7c6bd":"markdown","e8e43cfa":"markdown","27a18348":"markdown","63eea4c6":"markdown","01a927fb":"markdown","00ca67e3":"markdown","4795e3b9":"markdown","8537f64c":"markdown","6d17cb22":"markdown","32ac654f":"markdown","d2376f53":"markdown","7ee575d5":"markdown","6f1bd667":"markdown","cec30b9f":"markdown","a905787e":"markdown","7d26216f":"markdown","95f697f1":"markdown","caad1e69":"markdown","787dc23d":"markdown","8b174fda":"markdown","3f57caad":"markdown","4b2209b7":"markdown","fbcef396":"markdown","b6a14a6b":"markdown","a546fa67":"markdown","1947df60":"markdown","4b5a98a4":"markdown","f799a05d":"markdown","91cbb260":"markdown","8e5ac92c":"markdown","9ca87c36":"markdown","cfdbee9b":"markdown","f3ea6ec5":"markdown","97267344":"markdown","c7a68fa5":"markdown","7582fb5c":"markdown","3824669a":"markdown","d665283d":"markdown","9adb1849":"markdown"},"source":{"f89615ff":"# Data Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objects as pgo\nimport plotly.express as px\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, SCORERS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Other\nfrom scipy import stats\n\n# Import data\ndf = pd.read_csv('..\/input\/telecom-churn\/telecom_churn.csv')","ba70c3e8":"df.head()","73a16361":"df.shape","305d6615":"# Create Table\nvar = {'Description': ['1 if customer cancelled service, 0 if not', 'number of weeks customer has had active account', '1 if customer recently renewed contract, 0 if not', '1 if customer has data plan, 0 if not', 'gigabytes of monthly data usage', 'number of calls into customer service', 'average daytime minutes per month', 'average number of daytime calls', 'average monthly bill', 'largest overage fee in last 12 months', 'average number of roaming minutes'],\n       'Datatype': list(df.dtypes),\n       '# of NaNs': list(df.isna().sum())}\n\ndf_var = pd.DataFrame(var, columns = ['Description', 'Datatype', '# of NaNs'], index = list(df.columns))\ndf_var","5710e82d":"# Create new dataframe for better labeling of plots\ndf['churn'] = np.where(df['Churn'] == 1, 'cancelled', 'not cancelled')\ndf['contract renewal'] = np.where(df['ContractRenewal'] == 1, 'renewed', 'not renewed')\ndf['data plan'] = np.where(df['DataPlan'] == 1, 'data plan', 'no data plan')\n\n# Create pie chart\nlabels = ['cancelled', 'not cancelled']\nvalues = [(df.Churn == 1).sum(), (df.Churn == 0).sum()]\nfig = pgo.Figure(data = [pgo.Pie(labels = labels, values = values, hole = .6, marker_colors = ['coral',                                                                                                                       \n                                                                                               'cornflowerblue'])])\nfig.show()","876b0c59":"# Create barchart for 'ContractRenewal'\nsns.set_style(\"darkgrid\")\ndf_plot = df.groupby(['churn', 'contract renewal']).size().reset_index().pivot(columns = 'churn', index = 'contract renewal', values = 0)\ndf_plot.plot(kind = 'bar', stacked = True, figsize = (10, 7))\nplt.xticks(rotation = 0)","b56938be":"# Create barchart for 'DataPlan'\ndf_plot = df.groupby(['churn', 'data plan']).size().reset_index().pivot(columns = 'churn', index = 'data plan', values = 0)\ndf_plot.plot(kind = 'bar', stacked = True, figsize = (10, 7))\nplt.xticks(rotation = 0)","1d446655":"# Set up figure\nfig = plt.figure(figsize = (20,16))\n\n# Violinplots\nnbin_var = df[['AccountWeeks', 'DataUsage', 'CustServCalls', 'DayMins', 'DayCalls', 'MonthlyCharge', 'OverageFee',\n               'RoamMins']]\nfor i, v in enumerate(nbin_var):\n    axes = fig.add_subplot(4, 2, i+1)\n    sns.violinplot(x = 'churn', y = v, data = df, ax = axes)","a0590776":"# Create boxplots\nsns.catplot(x = 'DataUsage', y = 'data plan', row = 'churn',\n            kind = 'box', orient = 'h', height = 2.5, aspect = 4,\n            data = df)","1aa7dced":"# Create boxplots\nsns.catplot(x = 'MonthlyCharge', y = 'data plan', row = 'churn',\n            kind = 'box', orient = 'h', height = 2.5, aspect = 4,\n            data = df)","089fdb0d":"# Create boxplots\nsns.catplot(x = 'DayMins', y = 'data plan', row = 'churn',\n            kind = 'box', orient = 'h', height = 2.5, aspect = 4,\n            data = df)","777a7964":"# Create scatterplot\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df, x = 'DataUsage', y = 'MonthlyCharge', hue = 'churn')","28301d1a":"# Create scatterplot\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df, x = 'DayMins', y = 'MonthlyCharge', hue = 'churn')","550dac62":"# Define target and features\ny = df['Churn']\nX = df.iloc[:, 1:11]\n\n# Stratified train-test-split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","cedda812":"# Merge X_train and y_train\ndf_train = X_train.merge(y_train.to_frame(), left_index = True, right_index = True)\n\n# Indicies of each class' observations\ndf_train0 = df_train[df_train['Churn'] == 0]\ndf_train1 = df_train[df_train['Churn'] == 1]\n\n# Number of observations in each class\nn_0 = len(df_train0)\nn_1 = len(df_train1)\n\n# Downsampling\ndf_train0_ds = resample(df_train0, replace = False, n_samples = n_1, random_state = 123)\ndf_train_ds = pd.concat([df_train0_ds, df_train1])\nprint(\"Downsampled Dataset Class Counts:\", df_train_ds.Churn.value_counts(), sep = \"\\n\")\n\n# Upsampling\ndf_train1_us = resample(df_train1, replace = True, n_samples = n_0, random_state = 123)\ndf_train_us = pd.concat([df_train1_us, df_train0])\nprint(\"\\nUpsampled Dataset Class Counts:\", df_train_us.Churn.value_counts(), sep = \"\\n\")\n\n# Seperate y and X\ny_train_ds = df_train_ds['Churn']\nX_train_ds = df_train_ds.iloc[:, 0:10]\n\ny_train_us = df_train_us['Churn']\nX_train_us = df_train_us.iloc[:, 0:10]","a00e7cc5":"# Set up function which uses an algo as input and returns the accuracy score and AUROC for the input data\ndef _alg_fit(alg, X_train, y_train, params=None, scoring=None, cv=None):\n    \n    if params: # hyperparameter tuning if params dict is passed\n        if not scoring:\n            raise ValueError(\"If params dict is defined, then also a scoring metric has to be defined.\")\n        if not cv:\n            raise ValueError(\"If params dict is defined, then also the number of folds (cv) has to be defined.\")\n        rscv = RandomizedSearchCV(alg, params, scoring=scoring, random_state=42, n_jobs=-1, cv=cv)\n        mod = rscv.fit(X_train, y_train)\n        print(f\"Best {scoring}-score during GS: {mod.best_score_}\")\n        print(\"Best params from GS:\\n\", mod.best_params_)\n    else: # else simply fit\n        mod = alg.fit(X_train, y_train)\n    \n    # Prediction\n    y_pred = mod.predict(X_test)\n    \n    # Accuracy Score\n    acc_score = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score: \", acc_score)\n    \n    # AUROC\n    prob_y = mod.predict_proba(X_test)\n    prob_y = [p[1] for p in prob_y]\n    auroc_score = roc_auc_score(y_test, prob_y)\n    print(\"AUROC Score:\", auroc_score)","39af626a":"# Set up function which uses function _alg_fit and returns a comparison of the accuracy score and AUROC for imbalanced, downsampled and upsampled data\ndef compare_results(alg, params=None, scoring=None, cv=None):\n    # Unsampled Dataset\n    print(\"IMBALANCED CLASSES\")\n    _alg_fit(alg, X_train, y_train, params=params, scoring=scoring, cv=cv)\n\n    # Downsampled majority class\n    print(\"\\nDOWNSAMPLED MAJORITY CLASS\")\n    _alg_fit(alg, X_train_ds, y_train_ds, params=params, scoring=scoring, cv=cv)\n\n    # Upsampled minority class\n    print(\"\\nUPSAMPLED MINORITY CLASS\")\n    _alg_fit(alg, X_train_us, y_train_us, params=params, scoring=scoring, cv=cv)","6e3314be":"# Logistic Regression\nparams_lr = {\n    'penalty': ['l2', 'none'],\n    'tol': stats.uniform(loc=0.0001, scale=0.0001),\n    'max_iter': stats.randint(low=300, high=800),\n}\ncompare_results(LogisticRegression(n_jobs=-1), params=params_lr, scoring='roc_auc', cv=5)","ae40d71c":"# Gaussian Naive Bayes\nparams_nb = {'var_smoothing': stats.uniform(loc=0.0001, scale=0.0001)}\ncompare_results(GaussianNB(), params=params_nb, scoring='roc_auc', cv=5)","23b2a95a":"# K-nearest Neighbors\nparams_knn = {\n    'n_neighbors': stats.randint(low=3, high=8),\n    'weights': ['uniform', 'distance'],\n    'metric': ['mahalanobis', 'euclidean', 'minkowski'],\n}\ncompare_results(KNeighborsClassifier(n_jobs = -1), params=params_knn, scoring='roc_auc', cv=5)","5c7b3902":"# Decision Tree Classifier\nparams_dt = {\n    'max_depth': stats.randint(low=5, high=20),\n    'min_samples_split': stats.randint(low=2, high=10),\n    'min_samples_leaf': stats.randint(low=1, high=5),\n}\ncompare_results(DecisionTreeClassifier(criterion='entropy'), params=params_dt, scoring='roc_auc', cv=5)","d21c57a6":"# Random Forest Classifier\nparams_rf = {\n    'n_estimators': stats.randint(low=100, high=500),\n    'max_depth': stats.randint(low=5, high=20),\n    'min_samples_split': stats.randint(low=2, high=10),\n    'min_samples_leaf': stats.randint(low=1, high=5),\n}\nrfc = RandomForestClassifier(criterion='entropy', random_state=0, n_jobs=-1)\ncompare_results(rfc, params=params_rf, scoring='roc_auc', cv=5)","74f08933":"# Gradient Boosting Classifier\nparams_gbc = {\n    'learning_rate': stats.uniform(loc=0.03, scale=0.1),\n    'n_estimators': stats.randint(low=100, high=500),\n    'subsample': stats.uniform(loc=0.6, scale=0.4),\n    'max_depth': stats.randint(low=5, high=20),\n    'min_samples_split': stats.randint(low=2, high=10),\n    'min_samples_leaf': stats.randint(low=1, high=5),\n}\ngbc = GradientBoostingClassifier(random_state=0, n_iter_no_change=30)\ncompare_results(gbc, params=params_gbc, scoring='roc_auc', cv=5)","0f7b975c":"# XG-Boost Classifier\nparams_xgb = {\n    'learning_rate': stats.uniform(loc=0.01, scale=0.19),\n    'n_estimators': stats.randint(low=200, high=600),\n    'gamma': stats.uniform(loc=0.1, scale=0.9),\n    'subsample': stats.uniform(loc=0.6, scale=0.4),\n    'colsample_bytree': stats.uniform(loc=0.6, scale=0.4),\n    'max_depth': stats.randint(low=5, high=20),\n    'reg_alpha': [0, 1e-30, 1e-20, 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 50, 100],\n    'reg_lambda': [0, 1e-30, 1e-20, 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 50, 100],\n}\nxgb = XGBClassifier(booster='gbtree', n_jobs=-1, objective = 'binary:logistic')\ncompare_results(xgb, params=params_xgb, scoring='roc_auc', cv=5)","712c4f07":"# List of best algos with parameter tuned\nbest_algs = [rfc, gbc, xgb]\nnames = [\"RandomForestClassifier\", \"GradientBoostingClassifier\", \"XGBClassifier\"]\n\n# Return confusion matrices (absolute), precision and recall\nj = 0\nfor i in best_algs:\n    mod = i.fit(X_train_us, y_train_us)\n    y_pred = mod.predict(X_test)\n    con_mat = confusion_matrix(y_test, y_pred)\n    prec = (con_mat[0][0])\/(con_mat[0][0] + con_mat[0][1])\n    rec = (con_mat[0][0])\/(con_mat[0][0] + con_mat[1][0])\n    print(\"\\n\", names[j], \"Confusion Matrix:\\n\", con_mat, \"\\nPrecision: \", prec, \"\\nRecall: \", rec)\n    j = j + 1","b6f108ef":"# Feature importance\ndf_feature_importance = pd.DataFrame(rfc.feature_importances_, index = X_train_us.columns, \n                                     columns = ['feature importance']).sort_values('feature importance', \n                                                                                   ascending = False)\ndf_feature_importance","a5e9defe":"As predicted, there is a positive correlation between data usage and monthly charge. What we can also observe is that most customers with low to no data usage but a high monthly charge left the company.","0fef2429":"## Relationship: 'Daymins' & 'MonthlyCharge' & 'Churn' \n\nWe will do the same for 'DayMins' and 'MonthlyCharge'.","cc036102":"It is ok that the accuracy metric is lower for the resampled data as it is expected. We will focus more on the AUROC-score as it is a better precision metric for our classification models. Here, upsampling the minority class yields the best result.","e6dfced7":"## K-nearest Neighbors ","3323cba1":"## Logistic Regression \n\nLet's have a look at the first model and how it performs on the data with imbalanced classes, downsampled majority class and upsampled minority class.","10e7c6bd":"## Feature Importance\n\nWe will use the Random-Forest Model to detect the most important features.","e8e43cfa":"We have a new accuracy high with the decision tree. Also, it is achieved with the unsampled training data.","27a18348":"## Relationship: 'DataPlan' & 'MonthlyCharge' & 'Churn'\n\nNow we will look at the differences in the monthly charge between costumers with and without a data plan.","63eea4c6":"The monthly charge is also higher on average for those with a data plan. The more interesting fact about these boxplots is that the average monthly charge of the customers who had no data plan and left the company and was higher (around 62) than those who stayed in the company (around 47).","01a927fb":"## Final Result\n\nOur Random-Forest Model actually had the highest AUROC-Score, Accuracy and Precision. But the Recall is best for sklearns Gradient-Boosting Classifier.","00ca67e3":"There is also a positive relationship between the average daytime minutes per month and the monthly charge. Here we can additionally observe one surprising fact. Almost all customers with a monthly charge between 60 and 80 and average daytime minutes per month above 200 left the company. The other leavers are well distributed in this scatterplot.","4795e3b9":"## Relationship: 'DataPlan' & 'DayMins' & 'Churn' \n\nLastly let's have a look at the differences in their average daytime minutes per month.","8537f64c":"## Binary Features: 'ContractRenewal' and 'DataPlan'\n\nThe feature 'ContractRenewal' tells us wether the customer recently renewed his contract (1) or not (0). The feature 'DataPlan' is 1 if the customer had a data plan and 0 else.\n\nWe will see how many customers renewed their contract and how many had a data plan in this dataset. Additionaly we will use 'Churn' as a third variable in our plots to hopefully gain first insights of who is more likely to leave the company.","6d17cb22":"How many rows and columns do we have?","32ac654f":"Here, the AUROC-score for the resampled data is even lower than with imbalanced classes.","d2376f53":"## Gaussian Naive Bayes","7ee575d5":"As forseeable Random Forest performed better than a single decision tree -> Wisdom of the crowds! Now also training with the upsampled data yielded good results.","6f1bd667":"**This looks great!**\n\nWe don't have any categorical variables and no missing values. So our dataset is ready to be analysed. Of course the variable 'Churn' will be our target feature.","cec30b9f":"* Only around 27% of customers have a data plan in this dataset.\n* There is no significant difference in the proportion of cancelled contracts in these two groups.\n* In the group with no data plan around 16% and in the other group 11% left the company.","a905787e":"# Exploratory Data Analysis\n\nNow we will see what insights and relationships can be discovered through analysing the variables. We will start with our target feature 'Churn'. ","7d26216f":"Of course, customers with a data plan had higher data usage on average than those without. The more valuable insights here are:\n* Customers with a data plan who cancelled their service used more data on average than those who stayed in the company.\n* 50% of those with no data plan who stayed in the company made use of data whereas only outliers of the group which left the company did so.","95f697f1":"## Import packages and data","caad1e69":"* In this dataset around 90% renewed their contract recently. \n* We can see that the proportion of customers who cancelled their contract is much higher in the group of customers who haven't renewed their contract recently. \n* In the group with a contract renewal the proportion is only around 10%, whereas in the other group it is more than one third.","787dc23d":"# What is this project about?\n\nOur data was provided by [Barun Kumar](https:\/\/www.kaggle.com\/barun2104). It contains information on customers of a telecommunication company and whether they left the company or not. In this project we will see which types of customers are most likely to leave the company using classification.","8b174fda":"# ML-Models\n\nAs we will play around and try out multiple models we will define a function which lets us use and compare without having to write the code multiple times. As performence metric we will use the normal accuracy score and \"Area under the ROC-Curve\" as it better represents the performance in a classification problem, espacially with imballanced classes.","3f57caad":"## Conclusion\n\nAfter this analysis we now more about relationships of the variables and what kind of customers the company. We will now use Machine-Learning-Models to have a precise classification of the costumers in there leaving-behavior.","4b2209b7":"## Random Forest ","fbcef396":"**Let's have a closer look at the variables**\n\nThe following table contains all variables with their description, data-type and the number of missing values. ","b6a14a6b":"**Resampling**\n\nAs we saw in our first pie chart the target feature 'churn' has a problem of imbalanced classes. It had 2850 customers who stayed in the company and 483 who left. The problem with imbalanced classes is that the data isn't easily seperable. We must make sacrifices to one class or the other.\nThat's why we will try out Upsampling and Downsampling our train-data prior to fitting our machine-learning-models. Then we will see which model and which resampling method worked best on our test-data.","a546fa67":"## Gradient Boosting ","1947df60":"# How does our data look like? \n\nLet's have a look at the first five rows.","4b5a98a4":"## XG-Boost ","f799a05d":"## Target variable: 'Churn'\n\n**How high is the proprotion of customers which cancelled the service?**\n\nAs described in the table above, this variable is '1' if the customer cancelled the service and '0' if not. ","91cbb260":"## Relationship: 'DataPlan' & 'DataUsage' & 'Churn'\n\nWe will now focus on other relationships and thereby try to better understand our data. We will start with plotting the data usage of customers with data plan vs customers without a data plan. We will keep 'Churn' as a third variable in the plot.","8e5ac92c":"## Decision Tree ","9ca87c36":"* Half of the non-binary variables show no significant difference regarding the behavior of the two groups.\n* The other half give us some interesting insights.\n\n**DataUsage:**\n* The median in both groups is around 0. This is most probably the case because as we already found out, only 27% have a data plan in this dataset.\n* In the group of cancelled contracts the interquartile range is not nearly as large as in the other group. Also the above whisker is very small in comparison.\n* This means that the data usage is more spread out for customers who then stayed in the company.\n\n**CustServCalls:**\n* Those who cancelled their service have one customer-service call more on average (median) than those who stayed in the company. \n\n**DayMins:**\n* They also had higher average daytime minutes per month.\n\n**MonthlyCharge:**\n* The monthly charge was also higher on average in the group of customers who cancelled.","cfdbee9b":"So the k-nearest-neighbor-Classifier didn't perform that good, as expected.","f3ea6ec5":"Surprisingly, the Random-Forest Classifier has still the highest Accuracy and AUROC-score. And also surprising is that the Sampling methods did not improve the two metrics. Lets look at other important metrics:","97267344":"There are surprisingly no improvements with Gradient-Boosting from scikit-learn. Lets compare to Gradient-Boosting from XG-Boost:","c7a68fa5":"## Relationship: 'DataUsage' & 'MonthlyCharge' & 'Churn'\n\nOf course the monthly charge of customers with high data usage will be higher on average. But by adding the variable 'Churn' as a third variable we could observe additional important insights.","7582fb5c":"# Data Preparation\n\n**Define target variable, features and split the data into train and test**","3824669a":"## Confusion Matrix, Recall and Precision\n\nWe will have a look at the confusion matrices of the last three models (Random Forest, Gradient Boosting & XGBoost) as they performed the best. Additionally, we are going to focus on the models with upsampled minority class as for all it yielded the highest AUROC Score.","d665283d":"For the customers who stayed in the company the distribution of daytime minutes per month looks almost the same. For those who cancelled their service the difference between the group with and without a data plan is large. The average of those with a data plan is almost as high as the one of the customers who stayed in the company with 170 minutes. But for those who had no data plan is around 225 minutes. ","9adb1849":"## All non-binary features\n\nNow we will look at the non-binary features and if they can help us identify the reason for leaving the company."}}