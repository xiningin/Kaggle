{"cell_type":{"9b7dd94b":"code","a1edd670":"code","f75e151f":"code","334b9bc6":"code","864c28b1":"code","a933ab19":"code","8b75d91a":"code","9db98ea9":"code","9383b2cc":"code","518b08ec":"code","241aec31":"code","6cbe8d7d":"code","312fceb1":"code","a4680041":"code","aad1b635":"code","ac9bb7ad":"code","c18b35cb":"code","c3ba18e0":"code","a76d7007":"code","d66ffad4":"code","9241ae70":"code","635ce552":"code","6eb4384e":"code","7350c1df":"code","fe1deac7":"code","311cf69b":"code","40fd261f":"code","8448e5f4":"code","bea187d3":"code","6036f1a2":"markdown","fe275fbe":"markdown","306a45ca":"markdown","7ab8b4b7":"markdown","fc76b907":"markdown","f9b469c7":"markdown","91b50ceb":"markdown","58c7d5de":"markdown","423f4af2":"markdown","ab69786b":"markdown","ebf38399":"markdown","288b29f5":"markdown","3f8907c6":"markdown","689b76b5":"markdown","20b90824":"markdown","631522a9":"markdown","bc5daa5f":"markdown","85198239":"markdown","46723920":"markdown","9b88174f":"markdown","60755bc4":"markdown","083bfd79":"markdown","6f57cdcc":"markdown","8423bb4a":"markdown","b13587ff":"markdown"},"source":{"9b7dd94b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1edd670":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree,svm\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')","f75e151f":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# Printing first 10 rows of the dataset\ntrain_data.head(10)\nprint('The shape of our training set: %s passengers and %s features'%(train_data.shape[0],train_data.shape[1]))","334b9bc6":"train_data.info()","864c28b1":"# Checking Null Values\ntrain_data.isnull().sum()","a933ab19":"# We are going to plot a heat map to see the correlation between the parameters and the target variable (Survived)\n\nheatmap = sns.heatmap(train_data[[\"Survived\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"]].corr(), annot = True)\nsns.set(rc={'figure.figsize':(12,10)})","8b75d91a":"# For SibSp Column\n\n# Finding unique values\n\ntrain_data['SibSp'].unique()\nbargraph_sibsp = sns.catplot(x = \"SibSp\", y = \"Survived\", data = train_data, kind=\"bar\", height = 8)","9db98ea9":"# For Age Column\n\nageplot = sns.FacetGrid(train_data, col=\"Survived\", height = 7)\nageplot = ageplot.map(sns.distplot, \"Age\")\nageplot = ageplot.set_ylabels(\"Survival Probability\")","9383b2cc":"# For Gender Column\n\nsexplot = sns.barplot(x=\"Sex\", y=\"Survived\", data=train_data)","518b08ec":"# For Pclass Column\n\npclassplot = sns.catplot(x = \"Pclass\", y=\"Survived\", data = train_data, kind=\"bar\", height = 6)","241aec31":"train_data.isnull().sum()","6cbe8d7d":"mean = train_data[\"Age\"].mean()\nstd = train_data[\"Age\"].std()\n\nrand_age = np.random.randint(mean-std, mean+std, size = 177)\nage_slice = train_data[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ntrain_data[\"Age\"] = age_slice\n\ntrain_data[\"Embarked\"].value_counts()\n# Filling 2 missing values with most frequent value\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].fillna('S')\n\n# Again checking for null values\ntrain_data.isnull().sum()","312fceb1":"col_to_drop = [\"PassengerId\", \"Ticket\", \"Cabin\", \"Name\"]\ntrain_data.drop(col_to_drop, axis=1, inplace=True)\ntrain_data.head(10)","a4680041":"genders = {\"male\":0, \"female\":1}\ntrain_data[\"Sex\"] = train_data[\"Sex\"].map(genders)\n\nports = {\"S\":0, \"C\":1, \"Q\":2}\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].map(ports)\n\ntrain_data.head()","aad1b635":"df_train_x = train_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n\n# Target variable column\ntrain_data['Survived']=train_data['Survived']\ndf_train_y = train_data[['Survived']]\n\nx_train, x_test, y_train, y_test = train_test_split(df_train_x, df_train_y, test_size=0.20, random_state=42)","ac9bb7ad":"y_train.info()","c18b35cb":"clf1 = RandomForestClassifier()\nclf1.fit(x_train, y_train)\nrfc_y_pred = clf1.predict(x_test)\nrfc_accuracy = accuracy_score(y_test,rfc_y_pred) * 100\nprint(\"accuracy=\",rfc_accuracy)","c3ba18e0":"clf3 = KNeighborsClassifier(5)\nclf3.fit(x_train, y_train)\nknc_y_pred = clf3.predict(x_test)\nknc_accuracy = accuracy_score(y_test,knc_y_pred)*100\n\nprint(\"accuracy=\",knc_accuracy)","a76d7007":"clf4 = tree.DecisionTreeClassifier()\nclf4 = clf4.fit(x_train, y_train)\ndtc_y_pred = clf4.predict(x_test)\ndtc_accuracy = accuracy_score(y_test,dtc_y_pred)*100\n\nprint(\"accuracy=\",dtc_accuracy)","d66ffad4":"clf5 = svm.SVC()\nclf5.fit(x_train, y_train)\nsvm_y_pred = clf5.predict(x_test)\nsvm_accuracy = accuracy_score(y_test,svm_y_pred)*100\nprint(\"accuracy=\",svm_accuracy)","9241ae70":"print(\"Accuracy of Random Forest Classifier =\",rfc_accuracy)\nprint(\"Accuracy of K-Neighbor Classifier =\",knc_accuracy)\nprint(\"Accuracy of Decision Tree Classifier = \",dtc_accuracy)\nprint(\"Accuracy of Support Vector Machine Classifier = \",svm_accuracy)","635ce552":"# Importing test.csv\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head(10)","6eb4384e":"test_data.info()","7350c1df":"test_data.isnull().sum()","fe1deac7":"# Replacing missing values of age column\nmean = test_data[\"Age\"].mean()\nstd = test_data[\"Age\"].std()\nrand_age = np.random.randint(mean-std, mean+std, size = 86)\nage_slice = test_data[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ntest_data[\"Age\"] = age_slice\n\n# Replacing missing value of Fare column\ntest_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\n\ntest_data.isnull().sum()","311cf69b":"col_to_drop = [\"PassengerId\", \"Ticket\", \"Cabin\", \"Name\"]\ntest_data.drop(col_to_drop, axis=1, inplace=True)\ntest_data.head(10)","40fd261f":"genders = {\"male\":0, \"female\":1}\ntest_data[\"Sex\"] = test_data[\"Sex\"].map(genders)\n\nports = {\"S\":0, \"C\":1, \"Q\":2}\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].map(ports)\n\ntest_data.head()","8448e5f4":"x_test = test_data\ny_pred = clf1.predict(x_test)\noriginaltest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": originaltest_data[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.head(20)","bea187d3":"submission.to_csv('Titanic-Prediction.csv', index = False)","6036f1a2":"#### ***Now we\u2019ll visualize the impact of each feature on the target variable.***","fe275fbe":"### Support Vector Machine","306a45ca":"### Dropping Columns","7ab8b4b7":"### Now, we are going to fit our model on 5 different classification algorithms namely Random Forest Classifier, K-Neighbor Classifier, Decision Tree Classifier, and Support Vector Machine. ","fc76b907":"#### So a first class passenger has more chances of survival over 2nd and 3rd class passengers & Similarly the 2nd class passengers have more chances of survival over 3rd class passengers. ####","f9b469c7":"# Thank You!!","91b50ceb":"### Handling Missing Values of Age and Embarked Column","58c7d5de":"#### According to the above graph it\u2019s quite obvious to say that man has less chances of survival over females. ####","423f4af2":"### Since we're getting maximum accuracy score with Random Forest Classifier so we choose it for making predictions on test.csv. ###","ab69786b":"# **4. Building Machine Learning Model**","ebf38399":"# **2. Exploratory Data Analysis**","288b29f5":"### Random Forest Classifier","3f8907c6":"# **6. Data Preprocessing for Testing Data**","689b76b5":"#### So, As we can see there are 177 Missing values in Age column, and 687 missing values in Cabin column. ####","20b90824":"### Decision Tree Classifier","631522a9":"### K-Neighbor Classifier","bc5daa5f":"# **7. Final Task**","85198239":"### Accuracy of all 4 Classifiers","46723920":"# **1. Importing Necessary Libraries and Loading Dataset**","9b88174f":"# **3. Data Preprocessing**","60755bc4":"#### 1. Passengers having 1 or 2 siblings have good chances of survival ####\n#### 2. More no. of siblings -> Fewer chances of survival ####","083bfd79":"# **5. Making Prediction for Test.csv**","6f57cdcc":"### Dropping Columns","8423bb4a":"#### More age -> less chances of survival! ####","b13587ff":"#### ***Here, we have Fare whose value of correlation with the Survived variable is 0.26 which shows that the more the fare is, the more are the chances of survival.***"}}