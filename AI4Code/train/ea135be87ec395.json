{"cell_type":{"4148b033":"code","d3e2c9f1":"code","feb555ad":"code","9bfb306f":"code","e16bbe68":"code","daad726d":"code","1fbcd922":"code","f4f1ea90":"code","0197d198":"code","897af59c":"code","dbe16ac3":"code","2bd2b52b":"code","5d436293":"code","bcf0f56f":"code","3f8a3ffe":"code","28e08e6d":"code","211ea440":"code","4f5ce7b7":"code","4ed41935":"code","a950c64d":"code","f5765a79":"code","54f16ed0":"code","c354c69c":"code","7baacc7a":"code","e150aa37":"code","a4244be1":"code","a0f29a40":"code","b2730ce6":"code","fe3e55ef":"code","883a9e7d":"code","aea9264d":"code","646114f7":"code","13e02520":"code","c5e7b91c":"code","123df490":"code","32e2331f":"code","94d0af46":"code","338a4cd1":"code","dc97c805":"code","69128a9a":"code","d9442dd9":"code","ab84b6ec":"code","65851a17":"code","9de4be54":"code","3a6fca82":"code","66b0cda9":"code","7ff37d8b":"code","0b5ead51":"code","a3e086a4":"code","c7ea818a":"code","5b905188":"code","d2f582e7":"code","b796d53c":"code","c71bdcfe":"code","1dfdab58":"code","bd003a42":"code","d2412635":"code","bee1f867":"code","f8e53e8b":"code","a1ad491e":"code","6f90a12f":"code","69269d22":"code","a0b62e3f":"code","4aebd59e":"code","c883e304":"code","2c63974e":"code","0d9a31a4":"code","101dc817":"code","2f3ec5f7":"code","d3a76c44":"code","de297c86":"code","85e8e640":"markdown","6e87f302":"markdown","f38c4e44":"markdown","6109b8ab":"markdown","0a0f9291":"markdown","b7f0cd7f":"markdown","7b5c84a8":"markdown","2518b23d":"markdown","ffef6e79":"markdown","88258a9a":"markdown","5c2ff99f":"markdown","3879fd8c":"markdown","4d6df324":"markdown","bd04d69a":"markdown","2e0d32ac":"markdown","11b0b7db":"markdown","3a781d1e":"markdown","9ee36871":"markdown","883296f4":"markdown","b940cf44":"markdown","a4f38871":"markdown","20cfd90b":"markdown","2e7daf52":"markdown","ff8547d5":"markdown","09249749":"markdown","32cadf38":"markdown","4474789b":"markdown","f787aaa4":"markdown","d69ef76e":"markdown","1d80a241":"markdown","f6503a9f":"markdown"},"source":{"4148b033":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport random\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","d3e2c9f1":"#Importing Dataset\ndf = pd.read_csv(\"..\/input\/bank-full.csv\")","feb555ad":"#Creating User Columns\ndf_user = pd.DataFrame(np.arange(0,len(df)), columns=['user'])\ndf = pd.concat([df_user, df], axis=1)","9bfb306f":"df.info()","e16bbe68":"df.head(5)","daad726d":"df.tail()","1fbcd922":"df.columns.values","f4f1ea90":"df.describe()","0197d198":"df.groupby('y').mean()","897af59c":"df['y'].value_counts()","dbe16ac3":"countNo = len(df[df.y == 'no'])\ncountYes = len(df[df.y == 'yes'])\nprint('Percentage of \"No\": {:.3f}%'. format((countNo\/(len(df.y))*100)))\nprint('Percentage of \"Yes\": {:.3f}%'. format((countYes\/(len(df.y))*100)))","2bd2b52b":"#Verifying null values\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')","5d436293":"df.isna().any()","bcf0f56f":"df.isna().sum()","3f8a3ffe":"#Define X and y\nX = df.drop(['y','user','job','marital', 'education', 'contact', \n             'housing', 'loan', 'day', 'month', 'poutcome' ], axis=1)\ny = df['y']","28e08e6d":"X = pd.get_dummies(X)\ny = pd.get_dummies(y)","211ea440":"X.columns\nX = X.drop(['default_no'], axis= 1)\nX = X.rename(columns = {'default_yes': 'default'})\ny.columns\ny = y.drop(['yes'], axis=1)\ny = y.rename(columns= {'no': 'y'})","4f5ce7b7":"#Age group\nbins = range(0, 100, 10)\nax = sns.distplot(df.age[df.y=='yes'],\n              color='red', kde=False, bins=bins, label='Have Subscribed')\nsns.distplot(df.age[df.y=='no'],\n         ax=ax,  # Overplots on first plot\n         color='blue', kde=False, bins=bins, label=\"Haven't Subscribed\")\nplt.legend()\nplt.show()","4ed41935":"#Age\npd.crosstab(df.age,df.y).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Client Subscribed Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","a950c64d":"pd.crosstab(df.marital,df.y).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\nplt.title('Client Subscribed Frequency for Maritial')\nplt.xlabel('marital')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Subscribed\", \"Have Subscribed\"])\nplt.ylabel('Frequency')\nplt.show()","f5765a79":"plt.scatter(x=df.age[df.y=='yes'], y=df.duration[(df.y=='yes')], c=\"red\")\nplt.scatter(x=df.age[df.y=='no'], y=df.duration[(df.y=='no')])\nplt.legend([\"Have Subscribed\", \"Haven't Subscribed\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Duration\")\nplt.show()\n","54f16ed0":"sns.pairplot(data=df, hue='y', vars= ['age', 'balance', 'duration'])","c354c69c":"sns.countplot(x='y', data=df, label='Count')","7baacc7a":"sns.scatterplot(x='age', y='balance',hue='y', data=df)","e150aa37":"plt.figure(figsize=(20,10))\nsns.heatmap(data=df.corr(), annot=True, cmap='viridis')","a4244be1":"sns.distplot(df.age, bins = 20) ","a0f29a40":"sns.distplot(df.balance, bins = 20) ","b2730ce6":"sns.distplot(df.duration, bins = 20) ","fe3e55ef":"df2 = X\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(df2.shape[1]):\n    plt.subplot(6, 3, i + 1)\n    f = plt.gca()\n    f.set_title(df2.columns.values[i])\n\n    vals = np.size(df2.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    \n    plt.hist(df2.iloc[:, i], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","883a9e7d":"## Correlation with independent Variable \ndf2.corrwith(y.y).plot.bar(\n        figsize = (10, 10), title = \"Correlation with Y\", fontsize = 15,\n        rot = 45, grid = True)","aea9264d":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df2.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","646114f7":"## Pie Plots\ndf.columns\ndf2 = df[['y','job','marital', 'education', 'default', 'housing','loan', 'contact',\n             'month', 'poutcome'\n                    ]]\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Pie Chart Distributions', fontsize=20)\nfor i in range(1, df2.shape[1] + 1):\n    plt.subplot(6, 3, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(df2.columns.values[i - 1])\n   \n    values = df2.iloc[:, i - 1].value_counts(normalize = True).values\n    index = df2.iloc[:, i - 1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","13e02520":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)","c5e7b91c":"print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","123df490":"y_train['y'].value_counts()","32e2331f":"pos_index = y_train[y_train.values == 1].index\nneg_index = y_train[y_train.values == 0].index\n\nif len(pos_index) > len(neg_index):\n    higher = pos_index\n    lower = neg_index\nelse:\n    higher = neg_index\n    lower = pos_index\n\nrandom.seed(0)\nhigher = np.random.choice(higher, size=len(lower))\nlower = np.asarray(lower)\nnew_indexes = np.concatenate((lower, higher))\n\nX_train = X_train.loc[new_indexes]\ny_train = y_train.loc[new_indexes]","94d0af46":"y_train['y'].value_counts()","338a4cd1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = pd.DataFrame(sc.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","dc97c805":"## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression (Lasso)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])","69128a9a":"## K-Nearest Neighbors (K-NN)\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=15, metric='minkowski', p= 2)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['K-Nearest Neighbors (minkowski)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","d9442dd9":"## SVM (Linear)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","ab84b6ec":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf', probability= True)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","65851a17":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Naive Bayes (Gaussian)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","9de4be54":"## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclassifier.fit(X_train, y_train)\n\n#Predicting the best set result\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Decision Tree', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","3a6fca82":"## Random Forest Gini (n=100)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'gini')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","66b0cda9":"## Random Forest Gini (n=200)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 200,\n                                    criterion = 'gini')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=200)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","7ff37d8b":"## Random Forest Gini (n=300)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 300,\n                                    criterion = 'gini')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=300)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","0b5ead51":"## Random Forest Entropy (n=100)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Entropy (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","a3e086a4":"## Random Forest Entropy (n=200)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 200,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Entropy (n=200)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","c7ea818a":"## Random Forest Entropy (n=300)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 300,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Entropy (n=300)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","5b905188":"results","d2f582e7":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train,cv=10)\naccuracies.mean()\naccuracies.std()","b796d53c":"print(\"SVM (Linear) Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","c71bdcfe":"Confusion Matrix\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred)) ","1dfdab58":"#Plotting Cumulative Accuracy Profile (CAP)\ny_pred_proba = classifier.predict_proba(X=X_test)\nimport matplotlib.pyplot as plt\nfrom scipy import integrate\ndef capcurve(y_values, y_preds_proba):\n    num_pos_obs = np.sum(y_values)\n    num_count = len(y_values)\n    rate_pos_obs = float(num_pos_obs) \/ float(num_count)\n    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n    xx = np.arange(num_count) \/ float(num_count - 1)\n    \n    y_cap = np.c_[y_values,y_preds_proba]\n    y_cap_df_s = pd.DataFrame(data=y_cap)\n    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n    \n    print(y_cap_df_s.head(20))\n    \n    yy = np.cumsum(y_cap_df_s[0]) \/ float(num_pos_obs)\n    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n    \n    percent = 0.5\n    row_index = int(np.trunc(num_count * percent))\n    \n    val_y1 = yy[row_index]\n    val_y2 = yy[row_index+1]\n    if val_y1 == val_y2:\n        val = val_y1*1.0\n    else:\n        val_x1 = xx[row_index]\n        val_x2 = xx[row_index+1]\n        val = val_y1 + ((val_x2 - percent)\/(val_x2 - val_x1))*(val_y2 - val_y1)\n    \n    sigma_ideal = 1 * xx[num_pos_obs - 1 ] \/ 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n    sigma_model = integrate.simps(yy,xx)\n    sigma_random = integrate.simps(xx,xx)\n    \n    ar_value = (sigma_model - sigma_random) \/ (sigma_ideal - sigma_random)\n    \n    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n    ax.plot(xx,yy, color='red', label='User Model')\n    ax.plot(xx,xx, color='blue', label='Random Model')\n    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n    \n    plt.xlim(0, 1.02)\n    plt.ylim(0, 1.25)\n    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n    plt.xlabel('% of the data')\n    plt.ylabel('% of positive obs')\n    plt.legend()","bd003a42":"capcurve(y_test,y_pred_proba[:,1])","d2412635":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","bee1f867":"# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\n\n# Model to Test\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\n\n# Select Best X Features\nrfe = RFE(classifier, n_features_to_select=None)\nrfe = rfe.fit(X_train, y_train)","f8e53e8b":"# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)","a1ad491e":"X_train.columns[rfe.support_]","6f90a12f":"# New Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = X_train[X_train.columns[rfe.support_]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})    ","69269d22":"# Fitting Model to the Training Set\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM RFE (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","a0b62e3f":"results","4aebd59e":"# Evaluating Results\n#Making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(data=cm, annot=True)","c883e304":"#Making the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","2c63974e":"# Applying k-Fold Cross Validation (RFE)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,\n                             X = X_train[X_train.columns[rfe.support_]],\n                             y = y_train, cv = 10)","0d9a31a4":"print(\"SVM RFE (Linear) Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","101dc817":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train[X_train.columns[rfe.support_]].columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)   ","2f3ec5f7":"#CAP Curve\ny_pred_proba = classifier.predict_proba(X=X_test[X_train.columns[rfe.support_]])\ncapcurve(y_test,y_pred_proba[:,1])   ","d3a76c44":"### End of Model ####\n\n# Formatting Final Results\nuser_identifier = df['user']\nfinal_results = pd.concat([y_test, user_identifier], axis = 1).dropna()\nfinal_results['predicted'] = y_pred\nfinal_results = final_results[['user', 'y', 'predicted']].reset_index(drop=True)","de297c86":"final_results.head()","85e8e640":"# Feature Selection\n# Recursive Feature Elimination","6e87f302":"# Cumulative Accuracy Profile","f38c4e44":"# Conclusion","6109b8ab":"# Feature Scaling","0a0f9291":"# Data Analysis","b7f0cd7f":"# Introduction","7b5c84a8":"# Balancing the Traing Set","2518b23d":"Taking off some features, our model decreases a little bit the Accuracy as shown below.","ffef6e79":"# Visualising Data","88258a9a":"The Training Set is unbalanced. And so become necessary to organize it.","5c2ff99f":"# Dummy Trap","3879fd8c":"# Cumulative Accuracy Profile (CAP)","4d6df324":"Note below how Dataset is unbalanced","bd04d69a":"# Accuracy Paradox","2e0d32ac":"For feature selection, we wil use the Recursive Feature Elimination (RFE). More about Recursive Feature Elimination (RFE) [here](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html).","11b0b7db":"# Results","3a781d1e":"Note how the CAP result was much smaller than Accuracy result. CAP (55.25%) versus Accuracy (75.54%). Definitely the Accuracy Paradox influence negatively the model.","9ee36871":"As observed, the Bank Marketing Dataset is unbalanced. And so become tough to perform pretty well the model. Another issue was the Accuracy Paradox that provides a false Accuracy final result. Nevertheless, the Cumulative Accuracy Profile showed that our model is performing very worst. \nFor this case, perhaps more data can help balance the dataset and trying to find one best final result. \nPerfomance of Model Accuracy: 79.54%\nCumulative Accuracy Profile: 55.25%","883296f4":"For figure out Accuracy Paradox, we will use the Cumulative Accuracy Profile (CAP). More about Cumulative Accuracy Profile (CAP) [here](http:\/\/en.wikipedia.org\/wiki\/Cumulative_accuracy_profile).","b940cf44":"# Verifying Null Values","a4f38871":"# Splitting the Dataset into Training Set and Test Set","20cfd90b":"This Dataset is a big challenge for a Data Scientist. Besides being unbalanced data, further, we will realize that some datasets don't have a solution. And so, take more information and data become crucial to solve the Data Science problem.","2e7daf52":"1. Title: Bank Marketing\n\nSources Created by: Paulo Cortez (Univ. Minho) and S\u00e9rgio Moro (ISCTE-IUL) @ 2012\n\nPast Usage:\n\nThe full dataset was described and analyzed in:\n\nS. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimar\u00e3es, Portugal, October, 2011. EUROSIS.\n\nRelevant Information:\n\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed.\n\nThere are two datasets: 1) bank-full.csv with all examples, ordered by date (from May 2008 to November 2010). 2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv. The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).\n\nThe classification goal is to predict if the client will subscribe a term deposit (variable y).\n\nNumber of Instances: 45211 for bank-full.csv (4521 for bank.csv)\n\nNumber of Attributes: 16 + output attribute.\n\nAttribute information:\n\nFor more information, read [Moro et al., 2011].\n\nInput variables:\n\nbank client data:\n1 - age (numeric) 2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") 3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed) 4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\") 5 - default: has credit in default? (binary: \"yes\",\"no\") 6 - balance: average yearly balance, in euros (numeric) 7 - housing: has housing loan? (binary: \"yes\",\"no\") 8 - loan: has personal loan? (binary: \"yes\",\"no\")\n\nrelated with the last contact of the current campaign:\n9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") 10 - day: last contact day of the month (numeric) 11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\") 12 - duration: last contact duration, in seconds (numeric)\n\nother attributes:\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) 14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted) 15 - previous: number of contacts performed before this campaign and for this client (numeric) 16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n\nOutput variable (desired target): 17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n\nMissing Attribute Values: None","ff8547d5":"Accuracy is not the best way to measure a perfomance of model. It\u00b4s because Accuracy Paradox. More about Accuracy Paradox [here](http:\/\/towardsdatascience.com\/accuracy-paradox-897a69e2dd9b).","09249749":"# About This File","32cadf38":"# Get Dummies Values","4474789b":"# Model Building\n# Comparing Models","f787aaa4":"The SVM (Linear) was the most powerful method for our model as shown below.","d69ef76e":"# Define X and y","1d80a241":"Dummy Variable Trap can influence negatively in our analyses. Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms, one variable can be predicted from the others. The best definition for Dummy Variable Trap [here](http:\/\/www.youtube.com\/watch?v=qrWx3OjZL3o)\n","f6503a9f":"# Applying K-fold Validation"}}