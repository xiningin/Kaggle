{"cell_type":{"8c324742":"code","e011c3b0":"code","eb246330":"code","8fc82dc1":"code","6a6d31e7":"code","25cc6ba6":"code","5f044f2d":"code","d014621d":"code","d801c6a4":"code","5d8cae4b":"code","a11d716b":"code","f3497acf":"markdown","a707cfb2":"markdown","13b64877":"markdown","e8e43ab6":"markdown","a253b3cb":"markdown","0cdb4435":"markdown"},"source":{"8c324742":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndataset = pd.read_csv('..\/input\/prehackathonsup\/train_data\/train_data.csv')\n\n# \"engine_no\" and \"time_in_cycled\" are not required in prediction.\nmetacol = ['engine_no', 'time_in_cycles']\n\n# ******************* Check Null data *********************** #\nnull_rate = (dataset.isnull().sum()\/dataset.shape[0]) * 100\n\nnull_col = []\nthreshold_ratio = 99.5 #(%)\n\nfor i, x in enumerate(null_rate.index):\n    if null_rate[i] > threshold_ratio:\n        null_col.append(x)\n\n# If Null Ratio is greater than threshold_ratio, drop the column from dataset\ndataset = dataset.drop(columns=null_col)\nprint(f'Droppd columns are: {null_col}')\n# ************************************************************ #\n\n# Get Engine No. List\nengine_list = dataset['engine_no'].drop_duplicates().values","e011c3b0":"# use seaborn to visualize trend between features and target (RUL)\nexplore = sns.PairGrid(data=dataset.query('engine_no < 10') ,\n                 x_vars=['RUL'],\n                 y_vars=dataset.drop(columns=metacol).columns,\n                 hue=\"engine_no\", height=3, aspect=2.5)\nexplore = explore.map(plt.scatter)\nexplore = explore.set(xlim=(500,0))\nexplore = explore.add_legend()","eb246330":"### Draw Histogram with all columns in dataset ###\n#************************************************#\ndef visualize_hist(dataset):\n    fig, ax = plt.subplots(ncols=4, nrows=7, figsize=(20,12))\n    plt.tight_layout()\n    for i, x in enumerate(dataset):\n        row = int(i\/\/4)\n        col = int(i%4)\n        dataset[x].hist(bins=50, ax=ax[row, col])\n        ax[row, col].set_title(x)\n#************************************************#\n\n\n### Draw Plot with three \"plot_list\" columns ###\n#**********************************************#\ndef visualize_plot(dataset):\n    fig, ax = plt.subplots(ncols=4, nrows=7, figsize=(20,12))\n    plt.tight_layout()\n    for i, x in enumerate(dataset):\n        row = int(i\/\/4)\n        col = int(i%4)\n        dataset[x].plot(ax=ax[row, col])\n        ax[row, col].set_title(x)\n#**********************************************#\n\n\nvisualize_hist(dataset)\n\n# Is there a large outlier??\nvisualize_plot(dataset)","8fc82dc1":"# Check correlation features vs RUL\n\ncorr = dataset.corr()\nfig, ax = plt.subplots(figsize=(12,10))\nheatmap = ax.pcolor(corr, cmap=plt.cm.Reds, vmax=1.0)\nax.set_xticks(np.arange(corr.shape[0]))\nax.set_yticks(np.arange(corr.shape[1]))\nax.set_xticklabels(corr.columns, rotation=90)\nax.set_yticklabels(corr.columns)\nfig.colorbar(heatmap, ax=ax)","6a6d31e7":"# Assign Features\/Target variable\n\nX = dataset.drop(columns=metacol).iloc[:,:-1]\ny = dataset['RUL'].values","25cc6ba6":"# See which features are the most important\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nreg = RandomForestRegressor()\nreg = RandomForestRegressor(n_estimators = 200, max_depth = 15)\nreg.fit(X, y)\ny_pred = reg.predict(X)\nprint(\"complete\")\n\nimportances = reg.feature_importances_\ncolumns=['col', 'val']\npd_importances = pd.DataFrame(columns=columns)\nfor i, x in enumerate(X.columns):\n    pd_importances.loc[i] = [x, importances[i]]\n\n# Get the most important columns in top10    \nimportant_col = pd_importances.sort_values(by='val', ascending=False).reset_index()\nprint(important_col)","5f044f2d":"# Get the most important columns in top18 \nimportant_col = important_col.loc[:19, 'col']\nprint(important_col)","d014621d":"# Search the best hyper-parameter using Optuna integrated LightGBM\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport optuna.integration.lightgbm as lgb\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n\nparams = {\n    'objective': 'mean_squared_error',\n    'metric': 'rmse',\n    'verbosity': -1\n}\n\n# Data Preprocessing\nscaler = StandardScaler()\nX = scaler.fit_transform(dataset.loc[:,important_col].values)\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n\ntrains = lgb.Dataset(X_tr, y_tr)\ntests = lgb.Dataset(X_te, y_te)\n\nmodel = lgb.train(params, trains, valid_sets=tests, verbose_eval=False, show_progress_bar=False)\nprint(f'best params = {model.params}')\nprint(f'best score = {model.best_score}')\nbest_params = model.params","d801c6a4":"# XGBoost Parameter tuning\n\nimport xgboost as xgb\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n# XGBoost Parameters \u2014 xgboost 1.4.0-SNAPSHOT documentation https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n\ndef objective(trial):\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'tree_method': 'gpu_hist',\n        'eta': trial.suggest_loguniform('eta', 0.1, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 0.001, 5.),\n        'max_depth': trial.suggest_int('max_depth', 4, 10),\n        'sub_sample': trial.suggest_loguniform('sub_sample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.5, 1.0),\n        'lambda': trial.suggest_loguniform('lambda', 0.001, 0.01),\n        'alpha': trial.suggest_loguniform('alpha', 0.001, 0.01),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.00001, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n    }  \n    \n    xgb_model = xgb.XGBRegressor(**params)\n    xgb_model.fit(X_tr, y_tr, eval_set = [(X_te, y_te)],  early_stopping_rounds=10, verbose=False)\n    y_pred = xgb_model.predict(X)\n    \n    return np.sqrt(mean_squared_error(y, y_pred))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=50, n_jobs=-1)\nxgb_best = study.best_params\nprint(xgb_best)","5d8cae4b":"import lightgbm as lgb\n\nregressor = lgb.LGBMRegressor(**best_params)\nregressor.fit(X_tr, y_tr, eval_set=(X_te, y_te), verbose=0)\nlgb_y_pred = regressor.predict(X)\n\nxgb_model = xgb.XGBRegressor(**xgb_best)\nxgb_model.fit(X_tr, y_tr, eval_set = [(X_te, y_te)],  early_stopping_rounds=10, verbose=0)\nxgb_y_pred = xgb_model.predict(X)\n\ndef draw_rul_graph(y_act, y_pred):\n    fig, ax = plt.subplots(figsize=(8,6))\n    ax.scatter(y_act, y_pred)\n    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n    ax.set_xlabel('Actual RUL')\n    ax.set_ylabel('Predicted RUL')\n    ax.set_title('RUL Actual vs. Predicted')\n    plt.show()\n\ndraw_rul_graph(y, lgb_y_pred)\ndraw_rul_graph(y, xgb_y_pred)","a11d716b":"# K-Folds cross-validation stacking\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef cv_stacking(X, y, model):\n    counter = 0\n\n    for tridx, teidx in cv.split(X):\n        xtr_cv, xte_cv = X[tridx], X[teidx]\n        ytr_cv, yte_cv = y[tridx], y[teidx]\n        \n        try:\n            model.fit(xtr_cv, ytr_cv, eval_set=(xte_cv, yte_cv), verbose=0)\n        except:\n            model.fit(xtr_cv, ytr_cv, eval_set=[(xte_cv, yte_cv)], verbose=0)\n            # I know this try-except architecture is not good. But don't care :P\n        y_pred = model.predict(X)\n\n        if counter == 0:\n            summary = np.zeros(y_pred.shape[0])\n\n        summary += y_pred\n        counter += 1\n\n    return summary\n\nlgb_sum = cv_stacking(X, y, lgb.LGBMRegressor(**best_params))\nxgb_sum = cv_stacking(X, y, xgb.XGBRegressor(**xgb_best))\n\nsummary = (lgb_sum + xgb_sum)\/10\n\ndraw_rul_graph(y, summary)","f3497acf":"# Hyperparameter Tuning","a707cfb2":"# Data Visualization","13b64877":"> best params = {'objective': 'mean_squared_error', 'metric': 'rmse', 'feature_pre_filter': False, 'lambda_l1': 9.209013255699873, 'lambda_l2': 5.861166768736052, 'num_leaves': 22, 'feature_fraction': 0.4, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'num_iterations': 1000, 'early_stopping_round': None}\nbest score = defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 49.767768049741385)])})\n","e8e43ab6":"# Predict RUL with NASA Turbofan Data\n\nUseful documents.\n* [How to Implement Machine Learning For Predictive Maintenance](https:\/\/towardsdatascience.com\/how-to-implement-machine-learning-for-predictive-maintenance-4633cdbe4860)\n* [Predictive Maintenance ML (IIoT)](https:\/\/www.kaggle.com\/billstuart\/predictive-maintenance-ml-iiot)","a253b3cb":"# Train and Fit model","0cdb4435":"> {'eta': 0.12294493010797729, 'gamma': 0.06882131325279922, 'max_depth': 10, 'sub_sample': 0.8058550867403695, 'colsample_bytree': 0.5052660986445803, 'lambda': 0.005195480949665515, 'alpha': 0.0013480986056236027, 'learning_rate': 0.015100504529824567, 'n_estimators': 557}"}}