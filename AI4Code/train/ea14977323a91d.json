{"cell_type":{"cdf34b0f":"code","2352261d":"code","df74309a":"code","de81d482":"code","bd535947":"code","14c38e4a":"code","3fedb0f9":"code","5101a04f":"code","15d1a549":"code","d2160c0a":"code","2d4746b7":"code","9f133140":"markdown","5c249a34":"markdown","d3ae7e1e":"markdown","423a7377":"markdown","b1ef261d":"markdown","2405096c":"markdown","1cd8a1b7":"markdown"},"source":{"cdf34b0f":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n","2352261d":"dir = \"\/kaggle\/input\/asdfff\/data.csv\"","df74309a":"data = pd.read_csv(dir)\ndata.head()","de81d482":"x_full = np.array(data.X)\ny_full = np.array(data.Y)","bd535947":"X , testx, Y, testy = train_test_split(x_full, y_full)\ntrainx, validx, trainy, validy = train_test_split(X, Y) ","14c38e4a":"print(f\"Training length: {len(trainy)}\")\nprint(f\"Validation length: {len(validy)}\")\nprint(f\"Test set length: {len(testy)}\")","3fedb0f9":"class Method1:\n    def __init__(self):\n        self.slope = 0\n        self.intercept = 0\n        # initialize the weights\n    \n    def fit(self, X, Y):\n        self.x = X\n        self.y = Y\n        \n        num = np.sum((self.x - np.mean(self.x))*(self.y - np.mean(self.y)))\n        den = np.sum((self.x - np.mean(self.x))**2)\n        \n        self.m = num\/den\n        self.c = np.mean(self.y) - self.m * np.mean(self.x)\n        # Use the formula to get the value of slope and intercept\n\n    def predict(self,X):\n        return self.m * X + self.c\n        # Use the values to make predictions\n    \n    def evaluate(self, X, Y):\n        print(\"Error = \",np.mean((Y - self.predict(X))**2))\n        # Give out the total mean squared error of the regressor\n    \n    def plot(self, x, y):\n\n        ypreds = self.predict(x)\n\n        plt.scatter(x, y, color='red', alpha=0.2)\n        plt.scatter(x, ypreds, color='green')\n        \n        # plot the predicted values along with the actual values","5101a04f":"regressor = Method1()\nregressor.fit(trainx, trainy)\nregressor.evaluate(validx, validy)\nregressor.plot(testx, testy)","15d1a549":"class Method2:\n    def __init__(self):\n        self.weights = [None]\n    \n    def fit(self, x, y):\n        # adding a bias term along with the features\n        x = np.c_[np.zeros(len(x))+1, x]\n        \n        # geting the weights\n        theta = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)\n        self.weights = theta\n        \n    def predict(self, x):\n        x = np.c_[np.zeros(len(x))+1, x]\n        return x.dot(self.weights)\n    \n    def evaluate(self, x, y):\n        print(\"Mean Square error: \", np.mean((y - self.predict(x))**2))\n        \n    def plot(self, x, y):\n\n        ypreds = self.predict(x)\n\n        plt.scatter(x, y, color='red', alpha=0.2)\n        plt.scatter(x, ypreds, color='green')","d2160c0a":"reg = Method2()\nreg.fit(trainx, trainy)\nreg.evaluate(validx, validy)\nreg.plot(testx,testy)","2d4746b7":"output = pd.DataFrame()\noutput['Id'] = str(x_full)\noutput['Preds'] = reg.predict(x_full)\noutput.to_csv('output.csv', header=True, index=False)","9f133140":"## Get the dataset","5c249a34":"##### We can see that the predicted and the actual values overlap","d3ae7e1e":"## Method 2: Normal equation\n\\\\(\\Large \\theta = (X^T  X)^{-1}X^T Y \\\\)\n#### Where \\\\(\\large \\theta \\\\) contains all the weights required \n#### The feature vector x now has another feature which is always 1\n##### For example. previous x = [1, 2, 3, 4] new x = [[1, 1], [1, 2], [1, 3], [1, 4]]","423a7377":"## Split into features and targets then convert to numpy arrays","b1ef261d":"# Use Linear regression methods to get decent predictions","2405096c":"## Split the set into training set, validation set and test set","1cd8a1b7":"## Method 1: Slope and Intercept equation\n\n### Slope Equation of simple regressor : \\\\(\\Large M =  \\frac{\\sum_{i=0}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=0}^n(X_i - \\bar{X})^2}\\\\)\n\n### Intercept Equation: \\\\(\\Large C = \\bar{Y} - m\\bar{X} \\\\)"}}