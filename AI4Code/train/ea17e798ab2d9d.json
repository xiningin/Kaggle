{"cell_type":{"e6aad098":"code","76fea10d":"code","a205623e":"code","fe79ff7c":"code","409df787":"code","c450c4a8":"code","f9b5cca8":"code","f0bdfb55":"code","82cd7aa8":"code","d8d0ec70":"code","3a4ec59a":"code","2546c108":"code","a9bfc9f8":"code","65b283ac":"code","acf3bcc9":"code","43068009":"code","f77bb340":"code","86882c77":"code","dcc7b16c":"code","d1f37584":"code","b744526f":"code","ebe09a0b":"code","9830a7b4":"markdown"},"source":{"e6aad098":"# Got the code from the link - https:\/\/stackabuse.com\/text-generation-with-python-and-tensorflow-keras\/\n# Post from Dan Nelson\n# I will be altering the code later for my own neural net\n\n# Layered code which adds more functionality to your code \n# Still early days yet still comes out with gibberish im going to alter the layer types over\n# time to get the thing to work not sure whats wrong will try other models \nimport numpy\nimport sys\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.layers import ELU\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint","76fea10d":"file = open(\"..\/input\/gothic-literature\/frankenstein.txt\").read() # Copy the file path from using add data on the right","a205623e":"# Create a function that makes the text easier to manipulate by the computer\n\ndef tokenize_words(input):\n    # lowercase everything to standardize it\n    input = input.lower()\n\n    # instantiate the tokenizer\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(input)\n\n    # if the created token isn't in the stop words, make it part of \"filtered\"\n    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n    return \" \".join(filtered)","fe79ff7c":"# preprocess the input data, make tokens\nprocessed_inputs = tokenize_words(file)","409df787":"# Tokenize make the text easier to process for the computer \n\n# Preprocess the input data, make tokens\n\nprocessed_inputs = tokenize_words(file)","c450c4a8":"# We need to work out vocabulary length and character length\n\nchars = sorted(list(set(processed_inputs)))\nchar_to_num = dict((c, i) for i, c in enumerate(chars))\ninput_len = len(processed_inputs)\nvocab_len = len(chars)\nprint (\"Total number of characters:\", input_len)\nprint (\"Total vocab:\", vocab_len)","f9b5cca8":"# Numbers array store - I\/O arrays list - size seq_length of 100\n\nseq_length = 100\nx_data = []\ny_data = []","f0bdfb55":"# Now let's covert the text into numbers\n\n# loop through inputs, start at the beginning and go until we hit\n# the final character we can create a sequence out of\nfor i in range(0, input_len - seq_length, 1):\n    # Define input and output sequences\n    # Input is the current character plus desired sequence length\n    in_seq = processed_inputs[i:i + seq_length]\n\n    # Out sequence is the initial character plus total sequence length\n    out_seq = processed_inputs[i + seq_length]\n\n    # We now convert list of characters to integers based on\n    # previously and add the values to our lists\n    x_data.append([char_to_num[char] for char in in_seq])\n    y_data.append(char_to_num[out_seq])","82cd7aa8":"# Now we have our input sequences of characters and our output, which is the character \n# that should come after the sequence ends. We now have our training data features and labels, \n# stored as x_data and y_data. Let's save our total number of sequences and check to see how many \n# total input sequences we have:\n\nn_patterns = len(x_data)\nprint (\"Total Patterns:\", n_patterns)\n","d8d0ec70":"# Now we'll go ahead and convert our input sequences into a processed numpy array that our \n# network can use. We'll also need to convert the numpy array values into floats so that the \n# sigmoid activation function our network uses can interpret them and output probabilities from 0 to 1:","3a4ec59a":"# The neural net uses sigmoid activations - lets reshape the data and turn them into floats \n# so the activation function sigmoid can process them \n\nX = numpy.reshape(x_data, (n_patterns, seq_length, 1))\nX = X\/float(vocab_len)","2546c108":"# Use - one hot encoding - to convert them to our output labels to train the neural network like this\n#\n#                                                   10000000\n#                                                   01000000\n#                                                   00000001\n# It makes the neural net easier to train\n\ny = np_utils.to_categorical(y_data)","a9bfc9f8":"# Create our Neuralnet that learns - dont worry about what all that means below for now\n# just remind yourself its a little blackbox you dont need to understand how it does \n# what it does as long as after training it gives the desired outcome\n# as AI gets better you will be able to describe what you want from your\n# blackbox and the AI will create it for you\n\n# Currently AI researchers code at such a low level inorder to get the maths to marry up\n# with the function the neuralnet needs to learn. If you want to do research yes you need a \n# deeper understanding of the below code, but remember what is said above. \n\n# You don't have to understand how to make a pair of trainers inorder to wear them and\n# understand what they do. Treat neuralnets the same way ...\n\nmodel = Sequential()\nmodel.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, return_sequences=True)) # Think of it as a type of complex switch or gate\nmodel.add(Dropout(0.2)) # Stops the Neuralnet over fitting\nmodel.add(LSTM(256))\n \n\n# Drop layer is stop over fitting you want your neuralnet to generalise well with novel inputs\n# the dropout layers helps the neuralnet generalise \n# For more on this type into your search engine neuralnetworks what is overfitting?\n# ----------------------------------------------------------------------------------------------\nmodel.add(Dropout(0.1)) \n# ----------------------------------------------------------------------------------------------\nmodel.add(Dense(y.shape[1], activation='softmax'))","65b283ac":"# Compile the Neural Net calulate how well its working by its loss & optimize it's learning using adam\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","acf3bcc9":"# Create a check point that stores how the neural net our litlle brain is wired up - its neuralnet\n# pathways\n\nfilepath = \"model_weights_saved.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ndesired_callbacks = [checkpoint]","43068009":"# Lets start the training of the neural net feed in the input data in batches 'x' \n# also tell the neural net what output we want it to learn with the labels 'y'\n# The amount of cycles the neuralnet trains with is set with epochs if its set lower than 20 \n# the worse the text and the output turns into gibberish\n\nmodel.fit(X, y, epochs=1, batch_size=64, callbacks=desired_callbacks)","f77bb340":"# Load the saved neuralnet pathways\nfilename = \"model_weights_saved.hdf5\"\nmodel.load_weights(filename)\n# Now we have loaded the pathways again recompile into the neuralnet\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","86882c77":"# Convert the numbers back to characters\nnum_to_char = dict((i, c) for i, c in enumerate(chars))","dcc7b16c":"# Generate some random text to get the neuralnet to produce more\nstart = numpy.random.randint(0, len(x_data) - 1)\npattern = x_data[start]\nprint(\"Random Seed:\")\nprint(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")","d1f37584":"# Now to finally generate text, we're going to iterate through our chosen number of \n# characters and convert our input (the random seed) into float values.\n\n# We'll ask the model to predict what comes next based off of the random seed, \n# convert the output numbers to characters and then append it to the pattern, \n# which is our list of generated characters plus the initial seed:","b744526f":"for i in range(10):\n    x = numpy.reshape(pattern, (1, len(pattern), 1))\n    x = x \/ float(vocab_len)\n    prediction = model.predict(x, verbose=0)\n    index = numpy.argmax(prediction)\n    result = num_to_char[index]\n    seq_in = [num_to_char[value] for value in pattern]\n\n    sys.stdout.write(result)","ebe09a0b":"    pattern.append(index)\n    pattern = pattern[1:len(pattern)]","9830a7b4":"![https:\/\/www.google.com\/search?q=sigmoid+function+images&rlz=1CAIGZW_enGB893&sxsrf=ALeKk03MAXsU-SaQYnjt-zRrvikVaVbYgg:1597652629260&tbm=isch&source=iu&ictx=1&fir=VjglH3kRCVECZM%252CavewwmVOe63F1M%252C_&vet=1&usg=AI4_-kT93sbNoWWf-AveFyir-RNKD7H7LA&sa=X&ved=2ahUKEwik0MPr56HrAhWGOcAKHcjGA6UQ9QEwAHoECAoQJg&biw=1300&bih=572#imgrc=VjglH3kRCVECZM](http:\/\/)"}}