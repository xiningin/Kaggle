{"cell_type":{"2464af84":"code","1a053c6e":"code","1c2b7fd6":"code","a053cab9":"code","91444a90":"code","d2e5780f":"code","c785299f":"code","1bf02356":"code","8dfee038":"code","94803052":"code","920b7b00":"code","5688d6f9":"code","770c6e13":"code","5fe90131":"code","d7a500a5":"code","3ac51604":"code","4f651516":"code","1c0e84f0":"code","25b213cd":"code","ba9ebbff":"code","fd12383a":"code","bb8445b9":"code","e15484bf":"code","0b5270e2":"code","40de2264":"code","7d05d565":"code","cfdc9c39":"code","680d93b5":"code","0b5c5e85":"code","d4e36af5":"code","d29216d2":"code","8f021c3f":"code","b05b2df8":"code","5821c671":"code","6beaa420":"code","a0c00843":"code","73ead1c5":"code","1e5e914f":"code","c7675286":"markdown","5c153657":"markdown","794046ed":"markdown","71bf5763":"markdown","f01e4ffb":"markdown","c2fe6e04":"markdown","8f194850":"markdown","9bd52b84":"markdown","d60dccf8":"markdown","5f672656":"markdown","58392aa6":"markdown","bc44e6a6":"markdown","3e3bf754":"markdown","98987bb0":"markdown","00a64776":"markdown","a459a92e":"markdown","3b2c89b6":"markdown","68eede93":"markdown"},"source":{"2464af84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a053c6e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","1c2b7fd6":"dataset = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')\ndataset.head()","a053cab9":"dataset.shape","91444a90":"dataset.describe(include = 'all')","d2e5780f":"dataset.info()","c785299f":"dataset.isnull().sum()","1bf02356":"for i in dataset:\n    if(dataset[i].nunique()<30):\n        sns.countplot(x = dataset[i])\n        plt.show()","8dfee038":"plt.figure(figsize = (30, 10))\nplt.subplot(331)\nsns.distplot(dataset['battery_power'])\nplt.subplot(332)\nsns.distplot(dataset['clock_speed'])\nplt.subplot(333)\nsns.distplot(dataset['int_memory'])\nplt.subplot(334)\nsns.distplot(dataset['m_dep'])\nplt.subplot(335)\nsns.distplot(dataset['mobile_wt'])\nplt.subplot(336)\nsns.distplot(dataset['px_height'])\nplt.subplot(337)\nsns.distplot(dataset['px_width'])\nplt.subplot(338)\nsns.distplot(dataset['ram'])\nplt.subplot(339)\nsns.distplot(dataset['talk_time'])\nplt.show()","94803052":"dataset.head()","920b7b00":"dataset['isBluetooth'] = ' '\nfor i in range(len(dataset)):\n    if dataset['blue'][i] == 0:\n        dataset['isBluetooth'][i] = 'No'\n    else:\n        dataset['isBluetooth'][i] = 'Yes'\npx.pie(data_frame = dataset, names = 'isBluetooth', title = 'Percentage of devices having Bluetooth', hole = 0.2)","5688d6f9":"dataset['isDualSim'] = ' '\nfor i in range(len(dataset)):\n    if dataset['dual_sim'][i] == 0:\n        dataset['isDualSim'][i] = 'No'\n    else:\n        dataset['isDualSim'][i] = 'Yes'\npx.pie(data_frame = dataset, names = 'isDualSim', title = 'Percentage of devices having Dual Sim', hole = 0.2)","770c6e13":"dataset['isFour_g'] = ' '\nfor i in range(len(dataset)):\n    if dataset['four_g'][i] == 0:\n        dataset['isFour_g'][i] = 'No'\n    else:\n        dataset['isFour_g'][i] = 'Yes'\npx.pie(data_frame = dataset, names = 'isFour_g', title = 'Percentage of devices having 4G Connection', hole = 0.2)","5fe90131":"dataset['isThree_g'] = ' '\nfor i in range(len(dataset)):\n    if dataset['three_g'][i] == 0:\n        dataset['isThree_g'][i] = 'No'\n    else:\n        dataset['isThree_g'][i] = 'Yes'\npx.pie(data_frame = dataset, names = 'isThree_g', title = 'Percentage of devices having 3G Connection', hole = 0.2)","d7a500a5":"dataset['isTouchScreen'] = ' '\nfor i in range(len(dataset)):\n    if dataset['touch_screen'][i] == 0:\n        dataset['isTouchScreen'][i] = 'No'\n    else:\n        dataset['isTouchScreen'][i] = 'Yes'\npx.pie(data_frame = dataset, names = 'isTouchScreen', title = 'Percentage of devices having a Touch Screen', hole = 0.2)","3ac51604":"dataset['isWifi'] = ' '\nfor i in range(len(dataset)):\n    if dataset['wifi'][i] == 0:\n        dataset['isWifi'][i] = 'No'\n    else:\n        dataset['isWifi'][i] = 'Yes'\npx.pie(data_frame = dataset, names = 'isWifi', title = 'Percentage of devices having Wifi service', hole = 0.2)","4f651516":"dataset['Cores'] = ' '\nfor i in range(len(dataset)):\n    if dataset['n_cores'][i] == 1:\n        dataset['Cores'][i] = 'Single-Core'\n    elif dataset['n_cores'][i] == 2:\n        dataset['Cores'][i] = 'Dual-Core'\n    elif dataset['n_cores'][i] == 3:\n        dataset['Cores'][i] = 'Triple-Core'\n    elif dataset['n_cores'][i] == 4:\n        dataset['Cores'][i] = 'Quad-Core'\n    elif dataset['n_cores'][i] == 5:\n        dataset['Cores'][i] = 'Penta-Core'\n    elif dataset['n_cores'][i] == 6:\n        dataset['Cores'][i] = 'Hexa-Core'\n    elif dataset['n_cores'][i] == 7:\n        dataset['Cores'][i] = 'Hepta-Core'\n    else:\n        dataset['Cores'][i] = 'Octa-Core'\npx.pie(data_frame = dataset, names = 'Cores', title = 'Percentage of devices having Different types of Cores', hole = 0.2)","1c0e84f0":"px.histogram(data_frame = dataset, x = 'isBluetooth', color = 'price_range', title = 'Comparison of devices sold having Bluetooth or not')","25b213cd":"px.histogram(data_frame = dataset, x = 'isDualSim', color = 'price_range', title = 'Comparison of devices sold having Dual Sim or not')","ba9ebbff":"px.histogram(data_frame = dataset, x = 'isFour_g', color = 'price_range', title = 'Comparison of devices sold having 4G or not')","fd12383a":"px.histogram(data_frame = dataset, x = 'isTouchScreen', color = 'price_range', title = 'Comparison of devices sold having Touch Screen or not')","bb8445b9":"px.histogram(data_frame = dataset, x = 'isWifi', color = 'price_range', title = 'Comparison of devices sold having Wifi or not')","e15484bf":"px.histogram(data_frame = dataset, x = 'Cores', color = 'price_range', title = 'Comparison of devices sold having Different Cores')","0b5270e2":"dataset.drop(['isBluetooth', 'isDualSim', 'isFour_g', 'isThree_g', 'isTouchScreen', 'isWifi', 'Cores'], axis = 1, inplace = True)\ndataset.head()","40de2264":"plt.figure(figsize = (15, 10))\nsns.heatmap(dataset.corr(), linecolor = 'white', linewidths = 1, cmap = 'coolwarm', annot = True)\nplt.show()","7d05d565":"X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","cfdc9c39":"sns.distplot(X)","680d93b5":"sns.distplot(y)","0b5c5e85":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\nlog.fit(X_train, y_train)\nprint(\"Traiing Score of SVM is: {}\".format(log.score(X_train, y_train) * 100))\ny_predlog = log.predict(X_test)\nam_log = accuracy_score(y_test, y_predlog) * 100\nprint(\"Accuracy of Logistic Regression Classifier is: {}%\".format(am_log))\nprint(\"Confusion Matrix of Logistic Regression Classifier is: \\n{}\".format(confusion_matrix(y_test, y_predlog)))\nprint(\"{}\".format(classification_report(y_test, y_predlog)))","d4e36af5":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nprint(\"Traiing Score of SVM is: {}\".format(nb.score(X_train, y_train) * 100))\ny_prednb = nb.predict(X_test)\nam_nb = accuracy_score(y_test, y_prednb) * 100\nprint(\"Accuracy of Naive Bayes Classifier is: {}%\".format(am_nb))\nprint(\"Confusion Matrix of Naive Bayes Classifier is: \\n{}\".format(confusion_matrix(y_test, y_prednb)))\nprint(\"{}\".format(classification_report(y_test, y_prednb)))","d29216d2":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf')\nsvc.fit(X_train, y_train)\nprint(\"Traiing Score of SVM is: {}\".format(svc.score(X_train, y_train) * 100))\ny_predsvm = svc.predict(X_test)\nam_svm = accuracy_score(y_test, y_predsvm) * 100\nprint(\"Accuracy of SVM Classifier is: {}%\".format(am_svm))\nprint(\"Confusion Matrix of SVM Classifier is: \\n{}\".format(confusion_matrix(y_test, y_predsvm)))\nprint(\"{}\".format(classification_report(y_test, y_predsvm)))","8f021c3f":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(X_train, y_train)\nprint(\"Traiing Score of Decision Tree is: {}\".format(dt.score(X_train, y_train) * 100))\ny_preddt = dt.predict(X_test)\nam_dt = accuracy_score(y_test, y_preddt) * 100\nprint(\"Accuracy of SVM Classifier is: {}%\".format(am_dt))\nprint(\"Confusion Matrix of SVM Classifier is: \\n{}\".format(confusion_matrix(y_test, y_preddt)))\nprint(\"{}\".format(classification_report(y_test, y_preddt)))","b05b2df8":"from sklearn.ensemble import RandomForestRegressor\ntrain_accuracy = []\ntest_accuracy = []\n\nfor i in range(100, 600, 100):\n    c = RandomForestRegressor(n_estimators = i)\n    c.fit(X_train, y_train)\n    train_accuracy.append(c.score(X_train, y_train))\n    test_accuracy.append(c.score(X_test, y_test))\n    \nframe_rf = pd.DataFrame({\"n_neighbors\": range(100, 600, 100), \"train_accuracy\": train_accuracy, \"test_accuracy\": test_accuracy})\nframe_rf","5821c671":"plt.figure(figsize = (10, 7))\nplt.plot(range(100, 600, 100), frame_rf[\"test_accuracy\"], label = 'Test')\nplt.plot(range(100, 600, 100), frame_rf[\"train_accuracy\"], label = 'Training')\nplt.title('Optimal Number of Estimators')\nplt.xlabel('Estimators')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","6beaa420":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 300)\nrf.fit(X_train, y_train)\nprint(\"Traiing Score of Random Forest is: {}\".format(rf.score(X_train, y_train) * 100))\ny_predrf = rf.predict(X_test)\nam_rf = accuracy_score(y_test, y_predrf) * 100\nprint(\"Accuracy of SVM Classifier is: {}%\".format(am_rf))\nprint(\"Confusion Matrix of SVM Classifier is: \\n{}\".format(confusion_matrix(y_test, y_predrf)))\nprint(\"{}\".format(classification_report(y_test, y_predrf)))","a0c00843":"classifiers = ['Logistic Regression', 'Naive bayes', 'SVM', 'Decision Tree', 'Random Forest']\nlst_acc = [am_log, am_nb, am_svm, am_dt, am_rf]\ndf = pd.DataFrame({'Model': classifiers, 'Accuracy': lst_acc})\npx.histogram(data_frame = df, x = 'Model', y = 'Accuracy')","73ead1c5":"kfold = KFold(n_splits=10)\nxyz = []\naccuracy=[]\nclassifiers=['SVM','Logistic Regression','Decision Tree', 'Random forest', 'Naive Bayes']\nmodels=[SVC(kernel='rbf'),LogisticRegression(),\n        DecisionTreeClassifier(), \n        RandomForestClassifier(n_estimators=300,random_state=0), GaussianNB()]\n\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X_train, y_train, cv = kfold, scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    accuracy.append(cv_result)\n\ncv_models_dataframe=pd.DataFrame(xyz, index=classifiers)   \ncv_models_dataframe.columns=['CV Mean']    \ncv_models_dataframe\ncv_models_dataframe.sort_values(['CV Mean'], ascending=[0])","1e5e914f":"box=pd.DataFrame(accuracy,index=[classifiers])\nboxT = box.T\nplt.figure(figsize = (10, 7))\nax = sns.boxplot(data=boxT, orient=\"h\", palette=\"Set2\", width=.6)\nax.set_yticklabels(classifiers)\nax.set_title('Cross validation accuracies with different classifiers')\nax.set_xlabel('Accuracy')\nplt.show()","c7675286":"### Random Forest","5c153657":"### Distplot of Various Columns","794046ed":"## Finding Correlation between various Columns","71bf5763":"#### Finding the optimal number of estimators","f01e4ffb":"# Basic Data Wrangling","c2fe6e04":"# Thank you :)","8f194850":"# Context\nBob has started his own mobile company. He wants to give tough fight to big companies like Apple,Samsung etc.\n\nHe does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem he collects sales data of mobile phones of various companies.\n\nBob wants to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price. But he is not so good at Machine Learning. So he needs your help to solve this problem.\n\nIn this problem you do not have to predict actual price but a price range indicating how high the price is","9bd52b84":"### Countplot of Various Columns","d60dccf8":"### Decision Tree","5f672656":"### Logistic Regression","58392aa6":"# Comapring various Models (based on Accuracy)","bc44e6a6":"### Splitting the Dataset into and Training, Test set and Applying Standard Scalar","3e3bf754":"# Importing Various Libraries and the Dataset","98987bb0":"### **'ram' is having the highest correlation with the column 'price_range', signifying that Ram has a huge impact on the price range of various mobile devices**","00a64776":"### Naive Bayes","a459a92e":"### SVM","3b2c89b6":"# Model","68eede93":"# Exploratory Data Analysis"}}