{"cell_type":{"73c13bd4":"code","73e99082":"code","7999dfd1":"code","6c425531":"code","c3b6b6ef":"code","5db8eb8d":"code","68edde32":"code","aaecd9ad":"code","2806820e":"code","2bb92887":"code","48296a44":"code","a0c32d0f":"code","0a61e0b0":"code","c66574d2":"code","c6ffc183":"code","78275d50":"code","8845c5b6":"code","25ec4eb6":"code","97e7945e":"code","0e69d2b3":"markdown","be39a299":"markdown"},"source":{"73c13bd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","73e99082":"#Importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","7999dfd1":"#Importing our data\n\ndata = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n\nprint(\"Number of features : \",data.shape[1])\nprint(\"Number of samples  :\" ,data.shape[0])","6c425531":"data.info()","c3b6b6ef":"data.columns","5db8eb8d":"data.head(5)","68edde32":"data.shape","aaecd9ad":"y = data['Outcome'].values\n\nx_data = data.drop(['Outcome'],axis=1)\n\n\n","2806820e":"#Visualization\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data.corr(),annot = True , linewidth = .5, fmt = '.2f',ax = ax)\nplt.show()","2bb92887":"data.plot(kind=\"scatter\",x=\"Glucose\",y=\"Outcome\",color=\"blue\")\nplt.xlabel(\"Glucose\")\nplt.ylabel(\"Outcome\")\nplt.show()","48296a44":"#Normalization\n\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values\nx","a0c32d0f":"#train-test split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","0a61e0b0":"#Transpoze\n\nx_train = x_train.T\nx_test  = x_test.T\ny_train = y_train.T\ny_test  = y_test.T","c66574d2":"#initializing parameters and sigmoid function\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b=0.0\n    return w,b\n\n#%%\n\ndef sigmoid(z):\n    \n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\n","c6ffc183":"#Forward and Backward Propogation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    \n    #forward propogation\n    \n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n     \n    # backward propogation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n    ","78275d50":"#Updating parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    \n    cost_list  = []\n    cost_list2 = []\n    index      = []\n    \n    # updating parameters is number_of_iterarion times\n    \n    for i in range(number_of_iterarion):\n        \n        # make forward and backward propagation and find cost and gradients\n       \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        # lets update\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        if i % 30 == 0:\n            \n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","8845c5b6":"#Prediction\ndef predict(w,b,x_test):\n    \n    # x_test is a input for forward propagation\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    # if z is bigger than 0.5, our prediction is one (y_head=1),\n    # if z is smaller than 0.5, our prediction is zero (y_head=0),\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","25ec4eb6":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    \n    # initialize\n    \n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    \n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train,y_train,x_test,y_test,1,700)","97e7945e":"# sklearn with LR\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\n\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T))) ","0e69d2b3":"**INTRODUCTION**\n\n\nThe datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.For outcome,1 is diabete,0 is not diabete\n\n","be39a299":"**Now, we have learned Logistic Regression step by step.Now,we have library for doing this all staf just in a few lines.**"}}