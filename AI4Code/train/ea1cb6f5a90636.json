{"cell_type":{"50e1c660":"code","5efe743c":"code","5e358a48":"code","fae4f065":"code","550f30ae":"code","54536928":"code","bd77b957":"code","5301ec96":"code","f2a51f5a":"code","231ca3be":"code","27c48006":"code","9c5bbbf1":"code","cae30016":"code","8f846689":"code","9c81353c":"code","c236d8d6":"code","51373e87":"code","6642c434":"code","30d795d6":"code","5d434a2a":"code","648d6ea3":"code","f1a6a3d4":"code","a05d2eba":"code","c2050bbe":"code","ce79db71":"code","bd2de94c":"code","c23fde73":"code","64fddab4":"code","7043b428":"code","8d4e5f40":"code","7c22ef88":"code","95a877e0":"code","8112f73f":"code","61c0ff42":"code","29980f62":"code","9da1e073":"code","7de248dc":"code","fb828f30":"code","5afef911":"markdown","680960c2":"markdown","4b44708e":"markdown","6596b225":"markdown","d682c06d":"markdown","6bd41569":"markdown","01230a41":"markdown","7c917b6e":"markdown","ccefa73d":"markdown","21ffd058":"markdown","c3a8ebb5":"markdown","265d7811":"markdown","673fd668":"markdown","2bc9f784":"markdown","4f0f1294":"markdown","9460cd13":"markdown","e7d68f4b":"markdown","e3fdac31":"markdown","f8d5734e":"markdown","578eb369":"markdown","7d6269e3":"markdown","9d8ea71b":"markdown","24fbf6ff":"markdown"},"source":{"50e1c660":"%cd \/kaggle\/working","5efe743c":"%%capture\nHAVE_GPU = True # change according to environment\nif HAVE_GPU:\n    !pip install --user tensorflow-gpu==1.14 -q\nelse:\n    !pip install --user tensorflow==1.14 -q\n# never mind the `ERROR: tensorflow 2.1...` message below","5e358a48":"# make sure we the required packages\n!pip install --user Cython -q\n!pip install --user contextlib2 -q\n!pip install --user pillow -q\n!pip install --user lxml -q\n!pip install --user matplotlib -q","fae4f065":"!wget -O protobuf.zip https:\/\/github.com\/google\/protobuf\/releases\/download\/v3.0.0\/protoc-3.0.0-linux-x86_64.zip -q\n!unzip -o protobuf.zip\n!rm protobuf.zip","550f30ae":"%cd \/kaggle\n!rm -fr models\n!git clone https:\/\/github.com\/tensorflow\/models.git\n!rm -fr models\/.git","54536928":"# compile ProtoBuffers\n%cd models\/research\n!..\/..\/working\/bin\/protoc object_detection\/protos\/*.proto --python_out=.","bd77b957":"import os\n\nos.environ['AUTOGRAPH_VERBOSITY'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONPATH']=os.environ['PYTHONPATH']+':\/kaggle\/models\/research\/slim:\/kaggle\/models\/research'\nos.environ['PYTHONPATH']","5301ec96":"!pwd\n!python object_detection\/builders\/model_builder_test.py","f2a51f5a":"def disable_coco(file):\n    with open(file,'r') as f:\n        file_str = f.read()\n    file_str=file_str.replace('from object_detection.metrics import coco_evaluation',\n                    '#from object_detection.metrics import coco_evaluation')\n    file_str=file_str.replace('object_detection.metrics import coco_tools',\n                    '#object_detection.metrics import coco_tools')\n    file_str=file_str.replace('\\'coco_detection_metrics\\':', '#\\'coco_detection_metrics\\':')\n    file_str=file_str.replace('coco_evaluation.CocoDetectionEvaluator,', '#coco_evaluation.CocoDetectionEvaluator,')\n    file_str=file_str.replace('\\'coco_mask_metrics\\':','#\\'coco_mask_metrics\\':')\n    file_str=file_str.replace('coco_evaluation.CocoMaskEvaluator,','#coco_evaluation.CocoMaskEvaluator,')\n    with open(file,'w') as f:\n        f.write(file_str)\n\ndisable_coco('.\/object_detection\/eval_util.py')","231ca3be":"%cd object_detection\n!wget -O faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz http:\/\/download.tensorflow.org\/models\/object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz -q\n!tar xvzf faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz\n!rm faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz\n%cd ..","27c48006":"!rm object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\/checkpoint","9c5bbbf1":"%cd object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\n!mkdir export\n%cd export\n!mkdir Servo\n%cd ..\/..\/..","cae30016":"#import tensorflow as tf\n\n#input_pattern='\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????1-of-00050.tfrecord;\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????7-of-00050.tfrecord'\n#input_files = tf.io.gfile.glob(input_pattern)\n#data_set = tf.data.TFRecordDataset(input_files)\n#records_n = sum(1 for record in data_set)\nrecords_n = 3075 # takes a long time to run this, so cheating here\nprint(\"records_n = {}\".format(records_n))","8f846689":"import sys\n\nos.environ['DATA_PATH']='\/kaggle\/input\/tensorflow-tfrecords-demystified'\nos.environ['MODEL_PATH']='object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19'","9c81353c":"%%writefile 'object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\/ArTaxOr.config'\nmodel {\n  faster_rcnn {\n    num_classes: 7 # ArTaxOr has 7 classes currently\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 600\n        max_dimension: 1024\n      }\n    }\n    feature_extractor {\n      type: 'faster_rcnn_resnet50'\n      first_stage_features_stride: 16\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        scales: [0.25, 0.5, 1.0, 2.0]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        height_stride: 16\n        width_stride: 16\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_batch_size: 32\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 50\n        max_total_detections: 100\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n  }\n}\n\ntrain_config: {\n  batch_size: 1\n  num_steps: 4000000\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        manual_step_learning_rate {\n          initial_learning_rate: 0.0002\n          schedule {\n            step: 20000\n            learning_rate: .00002\n          }\n          schedule {\n            step: 50000\n            learning_rate: .000002\n          }\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  gradient_clipping_by_norm: 10.0\n  fine_tune_checkpoint: \"\/kaggle\/models\/research\/object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\/model.ckpt\"\n  from_detection_checkpoint: true\n  load_all_detection_checkpoint_vars: true\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n}\n\ntrain_input_reader: {\n  label_map_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????0-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????2-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????3-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????4-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????5-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????6-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????8-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????9-of-00050.tfrecord\"\n  }\n}\n\neval_config: {\n  metrics_set: \"pascal_voc_detection_metrics\"\n  #use_moving_averages: false\n  num_examples: 3075\n}\n\neval_input_reader: {\n  label_map_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr.pbtxt\"\n  shuffle: false\n  num_readers: 1\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????1-of-00050.tfrecord\"\n    input_path: \"\/kaggle\/input\/tensorflow-tfrecords-demystified\/ArTaxOr-????7-of-00050.tfrecord\"\n  }\n}","c236d8d6":"!pwd","51373e87":"# Note! Tensorboard only works in editor mode (kernel running), so we will not be using it here.\n#%load_ext tensorboard\n#%tensorboard --logdir=object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19","6642c434":"old_stdout = sys.stdout\nsys.stdout = open('\/kaggle\/working\/train.log', 'w')\n!python object_detection\/model_main.py \\\n    --pipeline_config_path=object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\/ArTaxOr.config \\\n    --model_dir=object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19 \\\n    --num_train_steps=60000 \\\n    --sample_1_of_n_eval_examples=1 \\\n    --alsologtostderr=False\nsys.stdout = old_stdout","30d795d6":"%%capture cap_out --no-stderr\n!mkdir \/kaggle\/working\/trained\n!python object_detection\/export_inference_graph.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\/ArTaxOr.config \\\n    --trained_checkpoint_prefix object_detection\/faster_rcnn_resnet50_fgvc_2018_07_19\/model.ckpt-60000 \\\n    --output_directory \/kaggle\/working\/trained","5d434a2a":"!ls -al \/kaggle\/working\/trained","648d6ea3":"!tar -cvzf \/kaggle\/working\/trained_model.tar \/kaggle\/working\/trained\n!gzip \/kaggle\/working\/trained_model.tar","f1a6a3d4":"!pip install --user parse -q\nfrom parse import *\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nwith open('\/kaggle\/working\/train.log', 'r') as f:\n    data=f.read()","a05d2eba":"loss=[]\nfor r in findall(\"Loss\/RPNLoss\/localization_loss = {:f}\", data):\n    loss.append(r[0])\nmAP=[]\nfor r in findall(\"\/mAP@0.5IOU = {:f}\", data):\n    mAP.append(r[0])\nstep=[]\nfor r in findall(\"global_step = {:d}\", data):\n    step.append(r[0])\nplt.figure(figsize=(16, 8))\nplt.plot(step,mAP)\nplt.xlabel('Global step')\nplt.legend(['mAP@0.5IOU']);","c2050bbe":"APAraneae=[]\nfor r in findall(\"AP@0.5IOU\/Araneae = {:f}\", data):\n    APAraneae.append(r[0])\nAPColeoptera=[]\nfor r in findall(\"AP@0.5IOU\/Coleoptera = {:f}\", data):\n    APColeoptera.append(r[0])\nAPDiptera=[]\nfor r in findall(\"AP@0.5IOU\/Diptera = {:f}\", data):\n    APDiptera.append(r[0])\nAPHemiptera=[]\nfor r in findall(\"AP@0.5IOU\/Hemiptera = {:f}\", data):\n    APHemiptera.append(r[0])\nAPHymenoptera=[]\nfor r in findall(\"AP@0.5IOU\/Hymenoptera = {:f}\", data):\n    APHymenoptera.append(r[0])\nAPLepidoptera=[]\nfor r in findall(\"AP@0.5IOU\/Lepidoptera = {:f}\", data):\n    APLepidoptera.append(r[0])\nAPOdonata=[]\nfor r in findall(\"AP@0.5IOU\/Odonata = {:f}\", data):\n    APOdonata.append(r[0])\nplt.figure(figsize=(16, 8))\nplt.plot(step,APAraneae)\nplt.plot(step,APColeoptera)\nplt.plot(step,APDiptera)\nplt.plot(step,APHemiptera)\nplt.plot(step,APHymenoptera)\nplt.plot(step,APLepidoptera)\nplt.plot(step,APOdonata)\nplt.xlabel('Global step')\nplt.legend(['AP Araneae', 'AP Coleoptera', 'AP Diptera', 'AP Hemiptera', 'AP Hymenoptera', 'AP Lepidoptera', 'AP Odonata']);","ce79db71":"%%capture cap_out --no-stderr\n!python object_detection\/inference\/infer_detections.py \\\n  --input_tfrecord_paths=\/kaggle\/input\/starter-arthropod-taxonomy-orders-testset\/ArTaxOr_TestSet.tfrecord \\\n  --output_tfrecord_path=\/kaggle\/working\/ArTaxOr_detections.tfrecord \\\n  --inference_graph=\/kaggle\/working\/trained\/frozen_inference_graph.pb \\\n  --discard_image_pixels","bd2de94c":"!ls \/kaggle\/working","c23fde73":"%%capture\nimport pandas as pd\nimport tensorflow as tf\n\nlabels=pd.read_pickle('\/kaggle\/input\/starter-arthropod-taxonomy-orders-testset\/testset_labels.pkl')\ndf=pd.read_pickle('\/kaggle\/input\/starter-arthropod-taxonomy-orders-testset\/testset_filelist.pkl')\nanno=pd.read_pickle('\/kaggle\/input\/starter-arthropod-taxonomy-orders-testset\/testset_objects.pkl')","64fddab4":"pdf=pd.DataFrame(columns=['score', 'label_idx', 'left', 'top', 'right', 'bottom', 'by', 'filename'])\nexample = tf.train.Example()\nfor record in tf.compat.v1.io.tf_record_iterator('\/kaggle\/working\/ArTaxOr_detections.tfrecord'):\n    example.ParseFromString(record)\n    f = example.features.feature\n    score = f['image\/detection\/score'].float_list.value\n    score = [x for x in score if x >= 0.60]\n    l = len(score)\n    pdf=pdf.append({'score': score,\n                    'label_idx': f['image\/detection\/label'].int64_list.value[:l],\n                    'left': f['image\/detection\/bbox\/xmin'].float_list.value[:l],\n                    'top': f['image\/detection\/bbox\/ymin'].float_list.value[:l],\n                    'right': f['image\/detection\/bbox\/xmax'].float_list.value[:l],\n                    'bottom': f['image\/detection\/bbox\/ymax'].float_list.value[:l],\n                    'by': f['image\/by'].bytes_list.value[0].decode(),\n                    'filename': f['image\/filename'].bytes_list.value[0].decode()}, ignore_index=True)","7043b428":"pdf.head()","8d4e5f40":"!pip install --user python-resize-image -q","7c22ef88":"from PIL import Image, ImageFont, ImageDraw\nfrom resizeimage import resizeimage\nimport numpy as np\n\nTSET_PATH = '\/kaggle\/input\/arthropod-taxonomy-orders-object-detection-testset\/ArTaxOr_TestSet\/'\n\n#fontname = 'C:\/Windows\/fonts\/micross.ttf' # Windows\nfontname = '\/usr\/share\/fonts\/truetype\/dejavu\/DejaVuSans.ttf' # Linux\nfont = ImageFont.truetype(fontname, 20) if os.path.isfile(fontname) else ImageFont.load_default()\n\ndef resize_image(file, width, height, stretch=False):\n    with Image.open(file) as im:\n        img = im.resize((width, height)) if stretch else resizeimage.resize_contain(im, [width, height])\n    img=img.convert(\"RGB\")    \n    return img\n\n#draw boundary box\ndef bbox(img, xmin, ymin, xmax, ymax, color, width, label, score):\n    draw = ImageDraw.Draw(img)\n    xres, yres = img.size[0], img.size[1]\n    box = np.multiply([xmin, ymin, xmax, ymax], [xres, yres, xres, yres]).astype(int).tolist()\n    txt = \" {}: {}%\" if score >= 0. else \" {}\"\n    txt = txt.format(label, round(score, 1))\n    ts = draw.textsize(txt, font=font)\n    draw.rectangle(box, outline=color, width=width)\n    if len(label) > 0:\n        if box[1] >= ts[1]+3:\n            xsmin, ysmin = box[0], box[1]-ts[1]-3\n            xsmax, ysmax = box[0]+ts[0]+2, box[1]\n        else:\n            xsmin, ysmin = box[0], box[3]\n            xsmax, ysmax = box[0]+ts[0]+2, box[3]+ts[1]+1\n        draw.rectangle([xsmin, ysmin, xsmax, ysmax], fill=color)\n        draw.text((xsmin, ysmin), txt, font=font, fill='white')\n    \n#prediction\ndef plot_img_pred(img, xres, yres, axes, scores, xmin, ymin, xmax, ymax, classes, title, by=''):\n    wscale = min(1,xres\/yres)\n    hscale = min(1,yres\/xres)\n    for i in range(len(scores)):\n        if scores[i]> 0.5 and classes[i]>0:\n            label = labels.name.iloc[int(classes[i]-1)]\n            color=labels.color.iloc[int(classes[i]-1)]\n            width, height = xmax[i]-xmin[i], ymax[i]-ymin[i]\n            xcenter, ycenter = xmin[i] + width\/2., ymin[i] + height\/2.\n            sxmin = .5+(xcenter-.5)*wscale-.5*wscale*width\n            symin = .5+(ycenter-.5)*hscale-.5*hscale*height\n            sxmax = .5+(xcenter-.5)*wscale+.5*wscale*width\n            symax = .5+(ycenter-.5)*hscale+.5*hscale*height\n            bbox(img, sxmin, symin, sxmax, symax, color, 2, label, 100*scores[i])\n    plt.setp(axes, xticks=[], yticks=[])\n    axes.set_title(title) if by == '' else axes.set_title(title+'\\n'+by)\n    plt.imshow(img)\n\n#ground truth\ndef plot_img_gt(img, axes, boxes, stretch, title, by=''):\n    wscale = 1. if stretch else min(1,boxes.xres.iloc[0]\/boxes.yres.iloc[0])\n    hscale = 1. if stretch else min(1,boxes.yres.iloc[0]\/boxes.xres.iloc[0])\n    for i in range(len(boxes)):\n        label = boxes.label.iloc[i]\n        color=labels.color.iloc[boxes.label_idx.iloc[i]]\n        xmin = .5+(boxes.xcenter.iloc[i]-.5)*wscale-.5*wscale*boxes.width.iloc[i]\n        ymin = .5+(boxes.ycenter.iloc[i]-.5)*hscale-.5*hscale*boxes.height.iloc[i]\n        xmax = .5+(boxes.xcenter.iloc[i]-.5)*wscale+.5*wscale*boxes.width.iloc[i]\n        ymax = .5+(boxes.ycenter.iloc[i]-.5)*hscale+.5*hscale*boxes.height.iloc[i]\n        bbox(img, xmin, ymin, xmax, ymax, color, 2, label, -1)\n    plt.setp(axes, xticks=[], yticks=[])\n    axes.set_title(title) if by == '' else axes.set_title(title+'\\n'+by)\n    plt.imshow(img)\n\ndef pred_batch(idx):\n    if idx + 2 < len(pdf):\n        rows = 3\n    else:\n        rows = len(pdf) - idx\n    fig = plt.figure(figsize=(16,rows*8))\n    for i in range(rows):\n        img = resize_image(TSET_PATH+'positives\/'+pdf.filename.iloc[i+idx], 512, 512, False)\n        by = pdf.by.iloc[i+idx]\n        axes = fig.add_subplot(rows, 2, 1+i*2)\n        boxes = anno[anno.id == df.id.iloc[i+idx]][['label', 'label_idx', 'xres', 'yres', 'xcenter', 'ycenter', 'width', 'height']]\n        plot_img_gt(img, axes, boxes, False, 'Ground truth', by)\n        img = resize_image(TSET_PATH+'positives\/'+pdf.filename.iloc[i+idx], 512, 512, False)\n        axes = fig.add_subplot(rows, 2, 2+i*2)\n        plot_img_pred(img, boxes.xres.iloc[0], boxes.yres.iloc[0], axes, pdf.score[i+idx], pdf.left[i+idx], pdf.top[i+idx], \n                      pdf.right[i+idx], pdf.bottom[i+idx],\n                      pdf.label_idx[i+idx], 'Detections', '')","95a877e0":"pred_batch(0)","8112f73f":"pred_batch(3)","61c0ff42":"pred_batch(6)","29980f62":"pred_batch(9)","9da1e073":"pred_batch(12)","7de248dc":"pred_batch(15)","fb828f30":"pred_batch(18)","5afef911":"Zip it for easy download.","680960c2":"### Config file\nThen we need to define the model `.config` file. Here we set up paths to the dataset and a few other parameters. Thankfully, TFRecords for the ArTaxOr dataset has been created in [this notebook](https:\/\/www.kaggle.com\/mistag\/tensorflow-tfrecords-demystified) so we can link directly to its output files. We will use a 80-20 split for training and evaluation, and since the dataset is sharded in 50 files, we can simply select 10 of them (arbitrary) to go into the evaluation set. We also need to determine how many images there are in the evaluation set to configure the evaluation stage correctly:","4b44708e":"## Install TF Object Detection API\nThe [Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) is at the time of writing not compatible with TF2 , so we need to install TF1.14 first. This notebook produces quite a lot of local files, and to keep a tidy house any large files not required will be removed (`rm -fr`).","6596b225":"## Prediction (detections)\nThe easiest way to make predictions (or detections) with the trained model is to use the API supplied script `infer_detections.py`, which expects images in a TFRecord file. Note that this script is difficult on Windows machines. We will make predictions on the [ArTaxOr TestSet](https:\/\/www.kaggle.com\/mistag\/arthropod-taxonomy-orders-object-detection-testset). The [starter kernel](https:\/\/www.kaggle.com\/mistag\/starter-arthropod-taxonomy-orders-testset) outputs a TFRecord file, so we can simply link to that. The detections are output in a separate TFRecord file, which we will process further down.","d682c06d":"Let's check precision vs. training steps by parsing data from the log file.","6bd41569":"Yohoo, it works!  \nNow, we did not install the Coco API, since we will be using the Pascal VOC evaluation metric. Unfortunately, there are some hardcoded references to the Coco API that needs to be commented out. Alternatively, just install the Coco API.","01230a41":"# TF Object Detection on Custom Data\nIn this notebook we will train a TensorFlow Object Detection model with a (large) custom dataset. We will cover the following steps:  \n* Install TensorFlow and TF Object Detection API\n* Fetch a pre-trained model from the TensorFlow detection model zoo\n* Configure the model and run training with the custom dataset\n* Make predictions with the trained model\n\nNow, the TensorFlow Object Detection API is not for the faint of heart to get started on, but once a few tweaks are in place, it is mostly smooth sailing.","7c917b6e":"Finally we can show some images, with ground truth to the left and detections to the right. Some detections are overlapping, and additional non-max suppression seems to be needed here.","ccefa73d":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> Remove the <b>checkpoint<\/b> file, otherwise training will fail while loading graph.\n<\/div>","21ffd058":"Furthermore, create a directory for saved models (otherwise an error will occur when training is finished).","c3a8ebb5":"Export the trained model to working directory using the supplied script.","265d7811":"Odonata (dragonflies and damselflies) is the class with the highest score, while Hymenoptera (bees, wasps, ants) is the class the model struggles most with.","673fd668":"Then compile the protocol buffer messages needed by the API.","2bc9f784":"Then we define a few helper functions for plotting the test images and bounding boxes.","4f0f1294":"Let's have a look at precision for each class:","9460cd13":"Time to fetch the Object Detection API.\n<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> Move up one level to avoid kernel crash when cloning repositories with deep folder structure.\n<\/div>","e7d68f4b":"20000 steps take about 2h to run. Training will output large amounts of text, and once things are working it is better to dump it to a file rather than having to scroll down past thousands of lines.","e3fdac31":"## Fetch a model from the zoo\nWe will start with a pre-trained model from [Tensorflow detection model zoo](https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/g3doc\/detection_model_zoo.md). Our custom dataset is the [ArTaxOr dataset](https:\/\/www.kaggle.com\/mistag\/arthropod-taxonomy-orders-object-detection-dataset), which contains images of invertebrate animals. Thus it makes sense to choose one of the iNaturalist Species-trained model. ","f8d5734e":"## Training","578eb369":"We need to install the `protoc` compiler. On windows, you can get [precompiled binaries here](https:\/\/github.com\/protocolbuffers\/protobuf\/releases).","7d6269e3":"First, we'll import pickled annotation data from the [ArtAxOr TestSet Starter notebook](https:\/\/www.kaggle.com\/mistag\/starter-arthropod-taxonomy-orders-testset). Then we read in the TFRecord with the detections, and create a Pandas frame with the detected bounding boxes.","9d8ea71b":"## Summary\nAny object detection framework that has all files located in a directory called `research` should ring a few alarm bells when it comes to expectations of a slick user experience. However, we have seen that a few tweaks are all that is needed to get going with the TensorFlow Object Detection API. What about other options for object detection? PyTorch Detectron2 is the only other framework that has a pretrained model zoo, but currently it does not run on Kaggle (and no Windows support). TensorFlow Hub has several pre-trained models, and otherwise one would have to engage in detail implementation of models like YOLO etc. What we really need is to get object detection from research level and into mainstream.","24fbf6ff":"That's it! We can now test our setup by running `model_builder_test.py`."}}