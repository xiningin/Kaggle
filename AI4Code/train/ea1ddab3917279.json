{"cell_type":{"32b12d4b":"code","35757bb1":"code","1ab93ca4":"code","3274a173":"code","8c4be797":"code","2658efb5":"code","4c5195aa":"code","009852f0":"code","a8f64f15":"code","d37ea6ea":"code","38b57675":"code","a587ec09":"code","0eb3e131":"code","b9830387":"code","8f32b2d4":"code","c7eb8672":"code","1856b716":"code","afde5cdd":"code","c85988fb":"code","6365daf0":"code","49ac97c9":"code","5e4fb400":"code","8646580c":"code","277ae85d":"code","7107df1c":"code","326b87f0":"code","e92465b7":"code","fdc4546a":"code","7bf4da69":"code","2c2b6819":"code","c947c4ab":"code","4be29f01":"code","62b34050":"code","e330a0c4":"code","c91b88df":"code","044a3283":"code","300a4473":"code","f1d693a6":"code","c851fa4d":"code","cbf08f1a":"code","30c2246e":"code","e3c62972":"code","62ef36f0":"code","07d9243a":"code","7602b2d2":"code","c25dfc9b":"code","4959b807":"code","9082bba2":"code","d475ef21":"code","eab8394c":"code","c499e9f3":"code","4e5481da":"code","0842eaad":"code","8cec2875":"code","9b85d297":"code","7b991c4b":"code","503a27a2":"code","374767aa":"code","0800aff8":"code","e810e397":"code","b7be530b":"code","8b796be0":"code","de171b5d":"code","cc62ff9d":"code","d9c6e6f4":"code","a2ba69b1":"code","24954faa":"code","5bbf2536":"code","977f05d8":"code","5321e6e3":"code","65781be8":"code","9b75bce8":"code","ffb40cc1":"code","47f0f4be":"code","e016f3bd":"code","d539f458":"code","d497b16b":"markdown","d66b8ec4":"markdown","de5fdd6b":"markdown","7f0bab25":"markdown","6893da2a":"markdown","55175f2b":"markdown","82645be5":"markdown","7cd90d75":"markdown","d11cdc1c":"markdown","45874edc":"markdown","978a3078":"markdown","7ed35c3b":"markdown","3fea5fbf":"markdown","b18634da":"markdown","a919f212":"markdown","302017f4":"markdown","c855649f":"markdown","6a7c05c6":"markdown","6c6917ba":"markdown","c0f77dbc":"markdown","2f879185":"markdown","126551c7":"markdown","508fe150":"markdown","5d324cb5":"markdown","4a5f8b64":"markdown","624d62b9":"markdown","8c298043":"markdown","03f5d511":"markdown","a50bbddf":"markdown","83430633":"markdown","359cb6e9":"markdown","8380246c":"markdown","1aa5db54":"markdown","a5a50589":"markdown","29567564":"markdown","037ea163":"markdown","c199f256":"markdown","fcb4b436":"markdown","30567be9":"markdown","64d9398c":"markdown","cc9a974c":"markdown","91ca685c":"markdown","a62a08d2":"markdown","476ca42a":"markdown","3167ca47":"markdown","6f55920b":"markdown","97b8f40c":"markdown","389eeb9f":"markdown","a07b9c60":"markdown","6c57d9a8":"markdown","9533aa3d":"markdown","d4e46e26":"markdown","60a668e9":"markdown","334c7290":"markdown","bab7706f":"markdown","8acdad72":"markdown","7bbf05df":"markdown","20885e16":"markdown","6d4e3fc2":"markdown","3f05122c":"markdown","f2180ce4":"markdown"},"source":{"32b12d4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35757bb1":"pip install pyxlsb","1ab93ca4":"data = pd.read_excel('\/kaggle\/input\/datml.xlsb', engine='pyxlsb')","3274a173":"data.head()","8c4be797":"data.tail()","2658efb5":"data.columns","4c5195aa":"data.shape","009852f0":"data.nunique()","a8f64f15":"data.count","d37ea6ea":"data.isna().sum()","38b57675":"data.describe().transpose()","a587ec09":"data.isnull().sum()","0eb3e131":"data['converted_in_7days'].nunique()","b9830387":"new_data=data.drop(['client_id', 'country', 'date', 'device', 'region', 'sourceMedium'], axis = 1) ","8f32b2d4":"new_data.head(10)","c7eb8672":"new_data2=new_data.drop(['converted_in_7days'],axis=1)","1856b716":"new_data_x=new_data2.iloc[:,:].values\nnew_data_x","afde5cdd":"new_data_y=new_data['converted_in_7days']","c85988fb":"new_data_y.head(2)","6365daf0":"from sklearn.impute import SimpleImputer\nimputer=SimpleImputer(missing_values=np.nan,strategy=\"median\")\nimputer.fit(new_data_x[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]])\nnew_data_x[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]]=imputer.transform(new_data_x[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]])","49ac97c9":"new_data_x","5e4fb400":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(new_data_x,new_data_y,test_size=0.2)","8646580c":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_tree = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\nclassifier_tree.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_predict = classifier_tree.predict(x_test)","277ae85d":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict))","7107df1c":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_tree=confusion_matrix(y_test,y_predict)\nprint(c_tree)\nAccuracy_tree=sum(np.diag(c_tree))\/(np.sum(c_tree))\nAccuracy_tree","326b87f0":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0,class_weight='balanced')\nclassifier_ensemble.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_predict1 = classifier_ensemble.predict(x_test)","e92465b7":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_ensemble=confusion_matrix(y_test,y_predict1)\nprint(c_ensemble)\nAccuracy_ensemble=sum(np.diag(c_ensemble))\/(np.sum(c_ensemble))\nAccuracy_ensemble","fdc4546a":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict1))","7bf4da69":"data.head(5)","2c2b6819":"data.shape","c947c4ab":"data.converted_in_7days.value_counts()","4be29f01":"data.drop(data[data['converted_in_7days'] == 2].index, inplace = True)","62b34050":"data.shape","e330a0c4":"data.drop(data[data['converted_in_7days'] == 3].index, inplace = True)","c91b88df":"data.shape","044a3283":"data.nunique()","300a4473":"data.describe().transpose()","f1d693a6":"data['converted_in_7days'].nunique()","c851fa4d":"data.shape","cbf08f1a":"new_data1=data.drop(['client_id', 'country', 'date', 'device', 'region', 'sourceMedium','converted_in_7days'], axis = 1) ","30c2246e":"new_data_X=new_data1.iloc[:,:].values\nnew_data_X.shape","e3c62972":"new_data_Y=data['converted_in_7days']\nnew_data_Y.shape","62ef36f0":"from sklearn.impute import SimpleImputer\nimputer=SimpleImputer(missing_values=np.nan,strategy=\"median\")\nimputer.fit(new_data_X[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]])\nnew_data_X[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]]=imputer.transform(new_data_X[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]])","07d9243a":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(new_data_X,new_data_Y,test_size=0.2)","7602b2d2":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble1 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_ensemble1.fit(X_train, Y_train)\n\n# Predicting the Test set results\ny_predict_ensemble1 = classifier_ensemble1.predict(X_test)","c25dfc9b":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_ensemble1=confusion_matrix(Y_test,y_predict_ensemble1)\nprint(c_ensemble1)\nAccuracy_ensemble1=sum(np.diag(c_ensemble1))\/(np.sum(c_ensemble1))\nAccuracy_ensemble1\n\n\n#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test,y_predict_ensemble1))","4959b807":"pip install imblearn","9082bba2":"from imblearn.over_sampling import SMOTE","d475ef21":"smt = SMOTE()\nX_train1, Y_train1 = smt.fit_sample(X_train, Y_train)","eab8394c":"np.bincount(Y_train1)","c499e9f3":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble3 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_ensemble3.fit(X_train1, Y_train1)\n\n# Predicting the Test set results\ny_predict_ensemble3 = classifier_ensemble3.predict(X_test)","4e5481da":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_ensemble3=confusion_matrix(Y_test,y_predict_ensemble3)\nprint(c_ensemble3)\nAccuracy_ensemble3=sum(np.diag(c_ensemble3))\/(np.sum(c_ensemble3))\nAccuracy_ensemble3\n\n\n#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test,y_predict_ensemble3))","0842eaad":"target_count = data.converted_in_7days.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])","8cec2875":"print('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","9b85d297":"# Class count\ncount_class_0, count_class_1 = data.converted_in_7days.value_counts()\n\n# Divide by class\ndf_class_0 = data[data['converted_in_7days'] == 0]\ndf_class_1 = data[data['converted_in_7days'] == 1]","7b991c4b":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndata_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(data_over.converted_in_7days.value_counts())\n\ndata_over.converted_in_7days.value_counts().plot(kind='bar', title='Count (target)')","503a27a2":"new_data4=data_over.drop(['client_id', 'country', 'date', 'device', 'region', 'sourceMedium','converted_in_7days'], axis = 1) ","374767aa":"new_data1_X=new_data4.iloc[:,:].values\nnew_data1_X.shape","0800aff8":"new_data1_Y=data_over['converted_in_7days']\nnew_data1_Y.shape","e810e397":"from sklearn.impute import SimpleImputer\nimputer=SimpleImputer(missing_values=np.nan,strategy=\"median\")\nimputer.fit(new_data1_X[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]])\nnew_data1_X[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]]=imputer.transform(new_data1_X[:,[3,15,19,21,23,26,28,33,35,37,39,41,43,45,47,49,52,54]])","b7be530b":"from sklearn.model_selection import train_test_split\nX1_train,X1_test,Y1_train,Y1_test=train_test_split(new_data1_X,new_data1_Y,test_size=0.2)","8b796be0":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble4 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_ensemble4.fit(X1_train, Y1_train)\n\n# Predicting the Test set results\ny_predict_ensemble4 = classifier_ensemble4.predict(X1_test)","de171b5d":"#Accuracy of our model.\nfrom sklearn.metrics import confusion_matrix\nc_ensemble4=confusion_matrix(Y1_test,y_predict_ensemble4)\nprint(c_ensemble4)\nAccuracy_ensemble4=sum(np.diag(c_ensemble4))\/(np.sum(c_ensemble4))\nAccuracy_ensemble4\n\n\n#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(Y1_test,y_predict_ensemble4))","cc62ff9d":"from sklearn.neural_network import MLPClassifier","d9c6e6f4":"nn=MLPClassifier(activation='logistic',solver='sgd',hidden_layer_sizes=(10,15),random_state=1)\nnn.fit(x_train,y_train)\ny_predict_nn1=nn.predict(x_test)","a2ba69b1":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_nn1))","24954faa":"nn=MLPClassifier(activation='logistic',solver='sgd',hidden_layer_sizes=(10,15),random_state=1)\nnn.fit(X_train,Y_train)\ny_predict_nn2=nn.predict(X_test)","5bbf2536":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test,y_predict_nn2))","977f05d8":"nn=MLPClassifier(activation='logistic',solver='sgd',hidden_layer_sizes=(10,15),random_state=1)\nnn.fit(X1_train,Y1_train)\ny_predict_nn3=nn.predict(X1_test)","5321e6e3":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(Y1_test,y_predict_nn3))","65781be8":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD","9b75bce8":"from keras.utils import to_categorical\ny_binary = to_categorical(new_data_Y)","ffb40cc1":"from sklearn.model_selection import train_test_split\nx2_train,x2_test,y2_train,y2_test=train_test_split(new_data_X,y_binary,test_size=0.2)","47f0f4be":"model = Sequential()\n\nmodel.add(Dense(1500, activation='relu', input_dim=56))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(750, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(125, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='softmax'))\n\nsgd= SGD(lr=0.01,decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n\n\nhistory=model.fit(x2_train,y2_train,epochs=1)\nscore=model.evaluate(x2_test,y2_test)\n","e016f3bd":"\nmodel.metrics_names","d539f458":"score","d497b16b":"Independent variables","d66b8ec4":"Slightly better when compared with the previous one","de5fdd6b":"lets impute the missing values with median","7f0bab25":"Basically what i will be doing is using the MLPClassifier from scikit learn.","6893da2a":"removing the unwanted variables from the dataset","55175f2b":"# Importing the required libraries","82645be5":"Lets split the data into test and train","7cd90d75":"random forest classification","d11cdc1c":"Model no 3 on data of random over sampling","45874edc":"# Splitting into test and train ","978a3078":"Now we have removed some features which we think that they are of no help to us in the analysis part.\n#As mentioned earlier now that we have the data which is good to go with for our further analysis, we will start doing the operations to the data to make it fit for our model building.\n#So, i will list out all the variables which have missing values and the approach i chose to fill those missing values as per my understanding of the data.\n#1bounces_hist---missing values are present and the strategy to replace this is median\n#2help_me_buy_evt_count_hist---median\n#3pageviews_hist---median\n#4paid_hist---median\n#5phone_clicks_evt_count_hist---median\n#6sessionDuration_hist---median\n#7sessions_hist---median\n#8visited_air_purifier_page_hist--median\n#9visited_checkout_page_hist---median\n#10visited_contactus_hist--median\n#11visited_customer_service_amc_login_hist---median\n#12visited_customer_service_request_login_hist---median\n#13visited_demo_page_hist---median\n#14visited_offer_page_hist---median\n#15visited_security_solutions_page_hist---median\n#16visited_storelocator_hist---median\n#17visited_vacuum_cleaner_page_hist---median\n#18visited_water_purifier_page_hist---median\n\nlets do the imputation for these variables and then we can go ahead for further analysis","7ed35c3b":"A 99 % accurate model, but the recall, precision and f1- score are very less which is like a metric trap.","3fea5fbf":"# Basic EDA.","b18634da":"* random forest now over this SMOTE","a919f212":"Now we have to see the independentvariables","302017f4":"Even this is not helping us to treat with an imbalanced dataset. We are falling for this metric trap again.","c855649f":"# lets try with random upsampling.","6a7c05c6":"Null Values are there in our dataset.","6c6917ba":"# Summary of the dataset","c0f77dbc":"* We have removed those two classes and we have got something better now with the random forest classifier\n* Now my model got better when compared to the early one. the metrics are okay when compared to the previous scenario****","2f879185":"Dropping the unwanted columns from the dataset","126551c7":"#From my basic understanding of the dataset I came to a stage where I would like to take some points into consideration and below are mentioned\n#Removing out some features which i think they are not of any help to my analysis.\nfeature selection is done by taking out \n#1)client id----which doesnot affect the buying pattern.\n#2)country\n#3)date--user last visited date---which doesnt affect buying pattern.\n#4)region\n#5)sourcemedium\n#6)device--mobile\/desktop cannot say that it affects the buying pattern.\n\n#So, will go ahead and drop these columns from our dataset and then treat the variables with missing values \n\n","508fe150":"it is not giving any further improvement in metrics","5d324cb5":"Dependent variable","4a5f8b64":"2 classes are there now.","624d62b9":"# Lets build the tree and ensemble model.","8c298043":"Dropping classes 2 and 3","03f5d511":"# Random Forest With Class Weighting","a50bbddf":"Slightly better metrics over the previois models build using MLP classifier.","83430633":"Model no-2 on data with 2 classes","359cb6e9":"Dropping these variables which are not giving anything meaningful to our data.","8380246c":"# lets try with smote upsampling","1aa5db54":"# Importing the dataset","a5a50589":"It is a 4 class problem and as it is a huge margin and the class 2 and class 3 are very less in number I can remove those two classes from my dataset .","29567564":"# Imputing the missing values of the above mentioned variables with the median.","037ea163":"#Now i got my values imputed with median values and the data is good to go with, i will move a head and do the model building and relevant stuff.","c199f256":"# if we carefully observe we are getting into a metric trap which is known as accuracy paradox.\nThe accuracy is very high, but the precision recall and f1 score are very low.\nso what we gonna do here is make the imbalanced dataset a balanced one.","fcb4b436":"# Now let us build using keras package","30567be9":"Now we drop our output or predicted variable.","64d9398c":"It clearly says that we have null values","cc9a974c":"Lets see the count of classes in target variable","91ca685c":"Lets see the proportion of the imbalance of the classes.","a62a08d2":"It is a 4 class problem.","476ca42a":"# Random Forest","3167ca47":"Independent variables","6f55920b":"Now we have removed class 2 and class 3 from our dataset.","97b8f40c":"Same problem exists here as well.","389eeb9f":"Dependent variables","a07b9c60":"We have done random over sampling and this is the result of it. Both the  classes are having same records now.","6c57d9a8":"#We got the best metrics here out of all the models built in various scenarios.\n#I have reached here by taking the random over sampling into the picture.","9533aa3d":"Lets try to implement on imbalanced dataset directly","d4e46e26":"# Seperating the predictor variables and the predicted variable from the dataset.","60a668e9":"Target variable count","334c7290":"model-1 on data with 4 classes","bab7706f":"# Neural Network ","8acdad72":"Lets check which class problem it is","7bbf05df":"*As it is a classification problem and the data size is round off 7 lakh. i rule out the below mentioned algorithms for the reasons mentioned \n1) logistic regression---as it is good for 2 class problem and here we have 4 class problem.\n2) KNN---dataset is too large for knn to handle \n3) Svm---dataset is too large to handle.\n\nAnd now i will be going with tree and ensemble method.\n*","20885e16":"Imputing the missing values ","6d4e3fc2":"Split into train and test.","3f05122c":"#Here we have two options\n#1 upsampling---good to go with \n#2 downsampling---problem is we miss some serious critical information if we go with this.\n\n#Now we will be taking both the samples and check for the best one.\n#1) SMOTE\n#2) Random upsampling","f2180ce4":"Dependent variable"}}