{"cell_type":{"47022eaf":"code","c2d497dc":"code","630207c3":"code","3cd1c9cc":"code","1e8bc868":"code","720f82b4":"code","1521e201":"code","6ce26770":"code","e074e331":"code","d2c13f76":"code","fd807c0b":"code","673153c7":"code","b9199dd6":"code","454655c1":"code","8a7abd18":"code","a29a35d5":"code","f6364f5a":"code","77a0923b":"code","ee7b5c22":"code","e15f136d":"code","051e5b02":"code","e39a4918":"code","6de412d0":"code","b6661837":"code","562ec676":"code","a8484607":"code","bd0ebc63":"code","99937d3f":"code","21e0a3e6":"code","66afab11":"code","e854a598":"code","5126c2ab":"code","6da17beb":"code","1b1adfd9":"code","51ffc809":"markdown","ef7b8c4e":"markdown","21bb938f":"markdown","31ec164b":"markdown","bc104eca":"markdown","bcb08598":"markdown","b3321b9d":"markdown","dae2bb24":"markdown","63ef5a6a":"markdown","7a568f2d":"markdown","a47b13f1":"markdown","de34c4ce":"markdown","152caddc":"markdown","3b5ed004":"markdown","e5d2536f":"markdown","5b5d0b49":"markdown","de778acb":"markdown","9b2ecc3b":"markdown","3480a5f3":"markdown","7dc27dd6":"markdown","21177e91":"markdown","5531a503":"markdown","c2372500":"markdown","ab6f868d":"markdown","9b7c6b68":"markdown","9df7c555":"markdown","91fd2e83":"markdown","d51395d1":"markdown","1366f12a":"markdown","2dff0566":"markdown","d5226174":"markdown","8c1398db":"markdown","50a34ed2":"markdown","8a616a49":"markdown","49e4a591":"markdown","4939c5df":"markdown","f68ab2d3":"markdown","6c04a194":"markdown","32787609":"markdown"},"source":{"47022eaf":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","c2d497dc":"dataset = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","630207c3":"dataset.info()","3cd1c9cc":"dataset.describe()","1e8bc868":"print(\"Distribui\u00e7\u00e3o das classifica\u00e7\u00f5es dos dados:\")\nprint(f\"Transa\u00e7\u00f5es N\u00e3o Fraudulentas: {round(dataset['Class'].value_counts()[0]\/len(dataset) * 100,2)}%.\")\nprint(f\"Transa\u00e7\u00f5es Fraudulentas: {round(dataset['Class'].value_counts()[1]\/len(dataset) * 100,2)}%.\")","720f82b4":"labels = ['N\u00e3o Fraude', 'Fraude']\nsns.countplot('Class', data = dataset, palette = ['red','blue'])\nplt.title('Distribui\u00e7\u00e3o das Classes', fontsize=14)\nplt.xticks(range(2), labels)\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Quantidade\");","1521e201":"fraude = dataset[dataset['Class'] == 1]['Amount']\nn_fraude= dataset[dataset['Class'] == 0]['Amount']","6ce26770":"print(f\"Transa\u00e7\u00f5es N\u00e3o Fraudulentas:\\n{n_fraude.describe()}\")\nprint()\nprint(f\"Transa\u00e7\u00f5es Fraudulentas:\\n{fraude.describe()}\")","e074e331":"fig, ax = plt.subplots(2, 2, figsize = (18,10))\n\n# Dados de todas as transa\u00e7\u00f5es\nvalores = dataset['Amount'].values\ntempo = dataset['Time'].values\n\nsns.distplot(valores, ax = ax[0][0], color = 'red')\nax[0][0].set_title('Distribui\u00e7\u00e3o dos Valores (Todas as transa\u00e7\u00f5es)', fontsize = 12)\nax[0][0].set_xlim([min(valores), max(valores)])\nax[0][0].set_xlabel('Valor da Transa\u00e7\u00e3o')\n\nsns.distplot(tempo, ax = ax[0][1], color = 'green')\nax[0][1].set_title('Distribui\u00e7\u00e3o das Transa\u00e7\u00f5es no Tempo (Todas as transa\u00e7\u00f5es)', fontsize = 12)\nax[0][1].set_xlim([min(tempo), max(tempo)])\nax[0][1].set_xlabel('Segundos desde a primeira transa\u00e7\u00e3o')\n\n# Dados apenas das transa\u00e7\u00f5es fraudulentas\nvalores_fraude = dataset[dataset['Class'] == 1]['Amount'].values\ntempo_fraude = dataset[dataset['Class'] == 1]['Time'].values\n\nsns.distplot(valores_fraude, ax = ax[1][0], color = 'red')\nax[1][0].set_title('Distribui\u00e7\u00e3o dos Valores (Transa\u00e7\u00f5es Fraudulentas)', fontsize = 12)\nax[1][0].set_xlim([min(valores), max(valores)])\nax[1][0].set_xlabel('Valor da Transa\u00e7\u00e3o')\n\nsns.distplot(tempo_fraude, ax = ax[1][1], color = 'green')\nax[1][1].set_title('Distribui\u00e7\u00e3o das Transa\u00e7\u00f5es no Tempo (Transa\u00e7\u00f5es Fraudulentas)', fontsize = 12)\nax[1][1].set_xlim([min(tempo), max(tempo)])\nax[1][1].set_xlabel('Segundos desde a primeira transa\u00e7\u00e3o')\n\nplt.show()","d2c13f76":"# Remo\u00e7\u00e3o da coluna relacionada ao tempo\ndf = dataset.drop(['Time'], axis=1)\n\n# Normaliza\u00e7\u00e3o da coluna relacionada ao valor da transa\u00e7\u00e3o\nsc = StandardScaler()\ndf['Amount'] = sc.fit_transform(df['Amount'].values.reshape(-1, 1))","fd807c0b":"# Separa\u00e7\u00e3o das classes de dados\nfraud_data = df[df['Class'] == 1]\nn_fraud_data = df[df['Class'] == 0]","673153c7":"# Separa\u00e7\u00e3o dos dados de treino e de teste\nX_train, X_test = train_test_split(n_fraud_data, test_size = 0.2)\n\nX_train = X_train.drop(columns = ['Class']).copy()\n\nX_test_full = pd.concat([X_test, fraud_data], ignore_index=True, sort=False)\nX_test_full = shuffle(X_test_full)\n\ny_test = X_test_full['Class'].copy()\nX_test = X_test_full.drop(columns = ['Class'])\n\nprint('Dimensionalidade dos dados:')\nprint(f'Treinamento: {X_train.shape}')\nprint(f'Teste (Fraudulentos): {fraud_data.shape}, Teste (N\u00e3o Fraudulentos): {X_test.shape}')","b9199dd6":"# Dimens\u00f5es dos dados de entrada\ninput_dim = X_train.shape[1]\n\ninput_layer = Input(shape=(input_dim, ))\n\n# Camadas de encoding\nencoder = Dense(18, activation=\"relu\")(input_layer)\nencoder = Dense(14, activation=\"relu\", activity_regularizer=regularizers.l2(10e-5))(encoder) \nencoder = Dense(10, activation=\"relu\")(encoder)\n\n# Camadas de decoding\ndecoder = Dense(14, activation='relu')(encoder)\ndecoder = Dense(18, activation='relu')(decoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\n\nautoencoder = Model(inputs=input_layer, outputs=decoder)\n\nautoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\ncheckpointer = ModelCheckpoint(filepath=\".\/model\/model.h5\", verbose=0, save_best_only=True)\n\ntensorboard = TensorBoard(log_dir='.\/logs', histogram_freq=0, write_graph=True, write_images=True)","454655c1":"autoencoder.summary()","8a7abd18":"# Treinamento da rede utilizando 15 \u00c9pocas e um tamanho de batch de 32\n\nEPOCHS = 50\nBS = 64\n\nhistory = autoencoder.fit(X_train, X_train,\n                          epochs = EPOCHS, \n                          batch_size = BS,      \n                          shuffle=True,\n                          validation_data=(X_test, X_test),\n                          verbose=1).history","a29a35d5":"# Representa\u00e7\u00e3o gr\u00e1fica dos valores de perda para os dados de Treino e de Valida\u00e7\u00e3o ao longo do treinamento\nfig, ax = plt.subplots(figsize=[14,8])\nax.plot(history['loss'], label='Treino')\nax.plot(history['val_loss'], label='Valida\u00e7\u00e3o')\nax.set_title('Valores de Perda', fontdict={'fontsize': 20})\nax.set_ylabel('Perda', fontdict={'fontsize': 15})\nax.set_xlabel('\u00c9poca', fontdict={'fontsize': 15})\nax.legend(fontsize=12, loc='upper right');","f6364f5a":"predictions = autoencoder.predict(X_test)","77a0923b":"mse = np.mean(np.power(X_test - predictions, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})","ee7b5c22":"error_df.describe()","e15f136d":"normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\nfraud_error_df = error_df[error_df['true_class'] == 1]\n\nfig, ax = plt.subplots(1, 2, figsize = (25,8))\nbins=20\n\nax[0].hist(normal_error_df.reconstruction_error.values, bins=bins)\nax[0].set_title('Erros de Reconstru\u00e7\u00e3o (Transa\u00e7\u00f5es N\u00e3o Fraudulentas)', fontsize = 17)\nax[0].set_xlabel('Erro de Reconstru\u00e7\u00e3o', fontsize = 15)\n\nax[1].hist(fraud_error_df.reconstruction_error.values, bins=bins)\nax[1].set_title('Erros de Reconstru\u00e7\u00e3o (Transa\u00e7\u00f5es Fraudulentas)', fontsize = 17)\n_ = ax[1].set_xlabel('Erro de Reconstru\u00e7\u00e3o', fontsize = 15)","051e5b02":"# C\u00e1lculo das curvas de precis\u00e3o e revoca\u00e7\u00e3o\nprecision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n\n# C\u00e1lculo da curva do F1 Score\nF1_score=[]\nF1_score = 2 * (precision * recall) \/ (precision + recall)\nwhere_are_NaNs = np.isnan(F1_score)\nF1_score[where_are_NaNs] = 0\nthreshold_max_f1 = th[np.argmax(F1_score)]","e39a4918":"fig, ax = plt.subplots(figsize = (16,9))\n\nax.plot(recall, precision, 'b', label='Precision-Recall curve')\nax.set_title('Precis\u00e3o vs Revoca\u00e7\u00e3o', fontsize=20)\nax.set_xlabel('Revoca\u00e7\u00e3o', fontsize=15)\n_ = ax.set_ylabel('Precis\u00e3o', fontsize=15)","6de412d0":"fig, ax = plt.subplots(figsize = (16,9))\n\nax.plot(th, precision[1:], 'r', label='Precis\u00e3o')\nax.plot(th, recall[1:], 'b', label='Revoca\u00e7\u00e3o')\nax.plot(th, F1_score[1:], 'g', label='F1 Score')\n\nax.set_title('Precis\u00e3o, Revoca\u00e7\u00e3o e F1 Score Vs Threshold', fontsize=20)\nax.set_xlabel('Erro de Reconstru\u00e7\u00e3o', fontsize=15)\nax.set_ylabel('Precis\u00e3o, Revoca\u00e7\u00e3o e F1', fontsize=15)\n_ = ax.legend(fontsize=12)","b6661837":"print('Valor m\u00e1ximo de  F1 Score: ', str(max(F1_score)))","562ec676":"def plot_reconstruction_error(error_df, threshold):\n    groups = error_df.groupby('true_class')\n    fig, ax = plt.subplots(figsize=(16, 9))\n\n    for name, group in groups:\n        ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n                label= \"Fraude\" if name == 1 else \"Normal\")\n        \n    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Limite')\n    ax.set_title(\"Erros de reconstru\u00e7\u00e3o para as diferentes transa\u00e7\u00f5es\", fontsize=20)\n    ax.set_ylabel(\"Erro de Reconstru\u00e7\u00e3o\", fontsize=15)\n    ax.set_xlabel(\"\u00cdndice dos dados\", fontsize=15)\n    ax.legend(fontsize=12)","a8484607":"def plot_confusion_matrix(error_df, threshold):\n    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n    conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n\n    fig, ax = plt.subplots(figsize=(16, 9))\n\n    sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\");\n    ax.set_title(\"Matriz de Confus\u00e3o\", fontsize=20)\n    ax.set_ylabel('Classe Verdadeira', fontsize=15)\n    ax.set_xlabel('Classe Predita', fontsize=15)\n\n    return y_pred","bd0ebc63":"plot_reconstruction_error(error_df, threshold_max_f1)","99937d3f":"y_pred = plot_confusion_matrix(error_df, threshold_max_f1)","21e0a3e6":"print(classification_report(error_df.true_class, y_pred))","66afab11":"threshold = 3.2","e854a598":"plot_reconstruction_error(error_df, threshold)","5126c2ab":"y_pred = plot_confusion_matrix(error_df, threshold)","6da17beb":"print(classification_report(error_df.true_class, y_pred))","1b1adfd9":"## FIM","51ffc809":"## 4.3. Separar dados de Treino e Teste.","ef7b8c4e":"- \u00c9 poss\u00edvel observar que n\u00e3o h\u00e1 uma grande diferen\u00e7a na distribui\u00e7\u00e3o dos valores das transa\u00e7\u00f5es e no hor\u00e1rio delas entre as fraudulentas e a totalidade de transa\u00e7\u00f5es.\n","21bb938f":"**Fonte**: https:\/\/pt.wikipedia.org\/wiki\/Precis%C3%A3o_e_revoca%C3%A7%C3%A3o","31ec164b":"# 1. Importar Bibliotecas","bc104eca":"- As transa\u00e7\u00f5es n\u00e3o fraudulentas possuem uma gama de valores de transa\u00e7\u00e3o muito maior do que as transa\u00e7\u00f5es fraudulentas.\n- Tamb\u00e9m \u00e9 poss\u00edvel observar que as transa\u00e7\u00f5es com maiores valores s\u00e3o classificadas como n\u00e3o fraudulentas.","bcb08598":"- O dataset \u00e9 originalmente desbalanceado. A maior parte dos dados s\u00e3o representados por transa\u00e7\u00f5es n\u00e3o fraudulentas. Tendo em vista que, analisando os dados de uma empresa fict\u00edcia, podemos ter grande similaridade na distribui\u00e7\u00e3o dos dados de fraude com o apresentado neste dataset.","b3321b9d":"**Refer\u00eancias:**\n\nhttps:\/\/www.pyimagesearch.com\/2020\/03\/02\/anomaly-detection-with-keras-tensorflow-and-deep-learning\/","dae2bb24":"**OBS:**\n\nCaso o Autoencoder seja treinado corretamente, esperamos que o erro entre a sa\u00edda e a entrada seja muito pequeno para dados normais.\n\nEntretanto, esse n\u00e3o \u00e9 o caso das anomalias ou, no nosso caso, transa\u00e7\u00f5es fraudulentas de dados.\n\nDurante o treinamento o codificador autom\u00e1tico aprendeu apenas os conceitos e caracter\u00edsticas da inst\u00e2ncia normal de dados. Isso significa que os pesos do Autoencoder foram ajustados apenas para codificar e reconstruir dados normais (transa\u00e7\u00f5es n\u00e3o fraudulentas).\n\nSe tentarmos codificar uma transa\u00e7\u00e3o fraudulenta, a representa\u00e7\u00e3o latente dessa transa\u00e7\u00e3o seria significativamente diferente da representa\u00e7\u00e3o latente de uma transa\u00e7\u00e3o normal.\n\nComo resultado direto, a entrada reconstru\u00edda diferiria ainda mais da entrada original, resultando em um erro muito maior do que no caso de transa\u00e7\u00f5es normais.\n\nO conhecimento do fato de que dados de entrada fraudulentos resultam em valores de perda mais altos da fun\u00e7\u00e3o de perda de erro quadr\u00e1tico m\u00e9dio (MSE - *Mean Squared Error*) pode ser usado ao nosso favor. Tudo o que precisamos fazer \u00e9 encontrar um limite de perda que distinga os dados normais dos fraudulentos (*threshold*).\n\nNa pr\u00e1tica, isso significa que inst\u00e2ncias de dados para as quais obtemos um valor de perda acima desse limite classificam essa inst\u00e2ncia como uma transa\u00e7\u00e3o fraudulenta.\n\nPor outro lado, inst\u00e2ncias de dados com valores de perda abaixo desse limite podem ser consideradas dados normais ou transa\u00e7\u00f5es n\u00e3o fraudulentas.\n\n    Caso 1: O valor da perda MSE para um recurso de entrada est\u00e1 acima do limiar de perda \u2192 o recurso de entrada \u00e9 uma transa\u00e7\u00e3o fraudulenta\n    Caso 2: O valor da perda MSE para um recurso de entrada est\u00e1 abaixo do limite \u2192 o recurso de entrada \u00e9 uma transa\u00e7\u00e3o n\u00e3o fraudulenta\n\n\n","63ef5a6a":"Uma \u00e1rea alta sob a curva representa alta revoca\u00e7\u00e3o e alta precis\u00e3o, onde alta precis\u00e3o se refere a uma baixa taxa de falsos positivos e alta revoca\u00e7\u00e3o se refere a uma baixa taxa de falsos negativos. As pontua\u00e7\u00f5es altas de ambos mostram que o classificador est\u00e1 retornando resultados precisos (alta precis\u00e3o), al\u00e9m de retornar a maioria de todos os resultados positivos (alta revoca\u00e7\u00e3o).","7a568f2d":"## 7.1 Precis\u00e3o vs Revoca\u00e7\u00e3o (Recall)\n\n<img width=\"303\" height=\"604\" src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/ab\/Precis%C3%A3o_e_revoca%C3%A7%C3%A3o.png\" \/>\n\nPrecis\u00e3o e Recall s\u00e3o definidos das seguintes maneiras:\n\n$$\\text{Precis\u00e3o} = \\frac{\\text{verdadeiros positivos}}{\\text{verdadeiros positivos} + \\text{falsos positivos}}$$\n\n$$\\text{Revoca\u00e7\u00e3o} = \\frac{\\text{verdadeiros positivos}}{\\text{verdadeiros positivos} + \\text{falsos negativos}}$$\n\n**Precis\u00e3o** (tamb\u00e9m chamada de valor preditivo positivo) \u00e9 a fra\u00e7\u00e3o de inst\u00e2ncias recuperadas que s\u00e3o relevantes.\n\n**Revoca\u00e7\u00e3o** (tamb\u00e9m conhecida como sensibilidade) \u00e9 a fra\u00e7\u00e3o de inst\u00e2ncias relevantes que s\u00e3o recuperadas. \n\nTanto precis\u00e3o quanto revoca\u00e7\u00e3o s\u00e3o, portanto, bases para o estudo e compreens\u00e3o da medida de relev\u00e2ncia.\n\nAlto recall, mas baixa precis\u00e3o significa muitos resultados, a maioria dos quais tem pouca ou nenhuma relev\u00e2ncia. Quando a precis\u00e3o \u00e9 alta, mas o recall \u00e9 baixo, temos o oposto - poucos resultados retornados com uma relev\u00e2ncia muito alta. Normalmente, o resultado ideal seria ter uma alta precis\u00e3o e alto recall.","a47b13f1":"\u00c9 poss\u00edvel observar que, \u00e0 medida que o erro de reconstru\u00e7\u00e3o aumenta, nossa precis\u00e3o tamb\u00e9m aumenta.\n\nJ\u00e1 na para a revoca\u00e7\u00e3o temos a situa\u00e7\u00e3o exatamente oposta. \u00c0 medida que o erro de reconstru\u00e7\u00e3o aumenta, a revoca\u00e7\u00e3o diminui. O mesmo acontece com o valor de F1 Score.","de34c4ce":"## 6.1. Demonstra\u00e7\u00e3o dos erros de reconstru\u00e7\u00e3o nos dados de Teste","152caddc":"# 3. An\u00e1lise Explorat\u00f3ria","3b5ed004":"## 5.3. Sum\u00e1rio da Rede Neural","e5d2536f":"## 5.2. Constru\u00e7\u00e3o do Auto Encoder utilizando o framework Keras\n","5b5d0b49":"## 3.1. Distribui\u00e7\u00e3o dos Dados com base na Classe\n","de778acb":"## 5.4. Treinamento do Auto Encoder","9b2ecc3b":"# 4. Processamento dos dados","3480a5f3":"#### AJUSTE FINO: Otimizando valor de Threshold para melhor resultado de Revoca\u00e7\u00e3o\n- O objetivo \u00e9 minimizar o falso negativo, o caso de ser uma transa\u00e7\u00e3o fraudulenta e o algoritmo predizer que \u00e9 uma transa\u00e7\u00e3o normal.","7dc27dd6":"# Autoencoders para Detec\u00e7\u00e3o de Fraudes","21177e91":"# 6. Predi\u00e7\u00e3o","5531a503":"**Resolu\u00e7\u00e3o**: <br>\n\nPara resolu\u00e7\u00e3o do problema, utilizamos Autoendoder, que \u00e9 uma t\u00e9cnica de aprendizado n\u00e3o supervisionado de Deep Learning, para realizar redu\u00e7\u00e3o de dimensionalidade e identificar valores extremos.\n\nA framework Keras foi o escolhido para usar na implementa\u00e7\u00e3o do modelo de rede neural utilizado pelo Autoencoder.","c2372500":"## 7.3 Compara\u00e7\u00e3o das m\u00e9tricas para o modelo gerado","ab6f868d":"## 5.5. Gr\u00e1fico de Erro de Valida\u00e7\u00e3o do Modelo","9b7c6b68":"## 7.2 F1 Score\n\nF1 Score \u00e9 uma medida que combina precis\u00e3o e revoca\u00e7\u00e3o \u00e9 a m\u00e9dia harm\u00f3nica entre precis\u00e3o e revoca\u00e7\u00e3o\n\n$${\\displaystyle F1=2\\cdot {\\frac {\\mathrm {precis\u00e3o} \\cdot \\mathrm {revoca\u00e7\u00e3o} }{\\mathrm {precis\u00e3o} +\\mathrm {revoca\u00e7\u00e3o} }}}$$\n\nEsta medida \u00e9 aproximadamente a m\u00e9dia de ambas quando seus valores est\u00e3o pr\u00f3ximos, e de maneira mais geral, o quadrado da m\u00e9dia geom\u00e9trica dividido pela m\u00e9dia aritm\u00e9tica. H\u00e1 v\u00e1rias raz\u00f5es pelas quais o F1 Score pode ser criticado em casos espec\u00edficos devido ao seu vi\u00e9s como m\u00e9trica de avalia\u00e7\u00e3o.","9df7c555":"- Dataset n\u00e3o possui valores nulos em suas colunas.","91fd2e83":"## 3.3. Distribui\u00e7\u00e3o das Transa\u00e7\u00f5es","d51395d1":"**Autor**: Matheus Jeric\u00f3 Palhares <br>\n**LinkedIn**: https:\/\/linkedin.com\/in\/matheusjerico","1366f12a":"## 7.3. Matriz de Confus\u00e3o\n- O valor de Threshold utilizado foi o valor que representa o maior resultado para a m\u00e9trica de F1-Score.\n","2dff0566":"### Dataset - Credit Card Fraud Detection\n","d5226174":"# 5. Autoencoder\n\n## 5.1. Introdu\u00e7\u00e3o\n\nPara o treinamento do Autoencoder s\u00e3o utilizados apenas os dados referentes \u00e0s transa\u00e7\u00f5es n\u00e3o fraudulentas. Durante o treinamento, o codificador ver\u00e1 milh\u00f5es de transa\u00e7\u00f5es n\u00e3o fraudulentas com cart\u00e3o de cr\u00e9dito.\n\nDessa forma, o Autoencoder codifica as informa\u00e7\u00f5es mais relevantes dos dados n\u00e3o fraudulentos. Intuitivamente, podemos dizer que dessa maneira o algoritimo aprende apenas o que \u00e9 uma transa\u00e7\u00e3o de cart\u00e3o de cr\u00e9dito absolutamente normal.\n\nE esse conceito aprendido de uma transa\u00e7\u00e3o normal com cart\u00e3o de cr\u00e9dito pode ser encontrado como a representa\u00e7\u00e3o latente na camada intermedi\u00e1ria, usada para recriar os recursos originais usados como dados de entrada para o Autoencoder.\n\nAp\u00f3s o treinamento com os dados n\u00e3o fraudulentos, podemos finalmente usar o Auto Encoder para detectar as anomalias (fraudes).\n\nCom isso, \u00e9 poss\u00edvel mostrar para o Auto Encoder os dados de ambos os tipos - anomalias e dados normais. Como antes, os recursos de entrada s\u00e3o codificados pelo Autoencoder em uma representa\u00e7\u00e3o latente que \u00e9 usada para reconstruir a entrada. Os dados de fraude v\u00e3\u00e3o possuir um erro de reconstru\u00e7\u00e3\u00e3o muito superior aos dados normais, pois o Auto Encoder aprendeu a codificar dados de transa\u00e7\u00f5\u00f5es normais.\n","8c1398db":"- Dataset apresenta quantidade significativa de outliers na maioria de suas colunas.","50a34ed2":"## 4.1. Aplicar t\u00e9cnica de padroniza\u00e7\u00e3o na feature 'Amount'.\n- Aplicamos a t\u00e9cnica pois a feature apresenta uma gama grande de valores.","8a616a49":"#### Valor balanceado entre Precis\u00e3o e Revoca\u00e7\u00e3o","49e4a591":"# 2. Carregar dados","4939c5df":"## 4.2. Separar os dados normais dos dados fraudulentos.","f68ab2d3":"# 7. M\u00e9tricas","6c04a194":"## 3.2. Existe difer\u00eancia nos valores das transa\u00e7\u00f5es fraudulentas e n\u00e3o fraudulentas?","32787609":"- Os erros de reconstru\u00e7\u00e3o, em sua maioria, s\u00e3o valores baixos, visto que representam as transa\u00e7\u00f5es n\u00e3o fraudulentas. J\u00e1 a grande parte dos erros mais altos \u00e9 representativa das transa\u00e7\u00f5es fraudulentas.\n- Para a defini\u00e7\u00e3o de melhor valor limite para diferenciar as transa\u00e7\u00f5es fraudulentas das n\u00e3o fraudulentas, \u00e9 preciso utilizar algumas m\u00e9tricas."}}