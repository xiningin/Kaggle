{"cell_type":{"3e0321ca":"code","38f6c768":"code","3d1ec8c9":"code","11105406":"code","99f0c805":"code","3738da15":"code","57df3172":"code","d55b95e4":"code","a5a74312":"code","a1379d7a":"code","11fbad26":"code","2e4531d2":"code","988d311f":"code","8d71f5a2":"code","00cb7f3e":"code","bbf20145":"code","2325974a":"code","1d3bd69a":"code","d42eca19":"code","4a1acbb8":"code","8976bf12":"code","f5650a07":"code","8407f890":"code","57e4b7ba":"code","83ac4f5b":"code","a083182e":"code","0d1042d1":"code","85f1b28d":"code","e4c2a5b0":"code","b17244ca":"code","e6c37d3c":"code","570b9e47":"code","0ede47c8":"code","8edeea92":"code","e6e729b9":"code","3fe61f78":"code","0d3c8c8c":"code","5ceab04a":"code","7ec0afb4":"code","49e01ec7":"code","8cc50984":"code","f2656a42":"code","bb9552b8":"code","f5df7c9e":"code","c920fb5c":"code","73a05c4a":"code","ad2f5d5d":"code","c14529dc":"code","dc19d81e":"code","fdac58ec":"code","473d0c90":"code","56bf295e":"markdown","0261f384":"markdown","dc616b3d":"markdown","dc452f7c":"markdown","5d8b0363":"markdown","c07c1e66":"markdown","785695ed":"markdown","e17b5a76":"markdown","ff08ed58":"markdown","6fc39c79":"markdown","7e0c4629":"markdown","d98286b6":"markdown","866a0176":"markdown","2abe9f09":"markdown","8924bea9":"markdown","c5b98c5f":"markdown","a9bf158f":"markdown","bbee4569":"markdown","e4172dd5":"markdown","de5f7fde":"markdown","a9ee0989":"markdown","05b120be":"markdown","7a2f375f":"markdown","900bffb8":"markdown","b9afc10c":"markdown","10176155":"markdown","173e143d":"markdown","cf5d9ec3":"markdown","974e28e1":"markdown","37171e6a":"markdown","56921a2e":"markdown","ee758302":"markdown","e34daf1f":"markdown","751a9f62":"markdown"},"source":{"3e0321ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport seaborn as sns\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","38f6c768":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d1ec8c9":"data_frame = pd.read_json('\/kaggle\/input\/github-bugs-prediction\/embold_train.json')","11105406":"train_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train.json\").reset_index(drop=True)\ntrain_df.head()","99f0c805":"test_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_test.json\").reset_index(drop=True)\ntest_df.head()","3738da15":"train_ex_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train_extra.json\").reset_index(drop=True)\ntrain_ex_df.head()","57df3172":"def dataset_length_check(data_frame):\n    print(len(data_frame),data_frame.index.shape[-1])\n                 \ndataset_length_check(train_df)\ndataset_length_check(train_ex_df)\ndataset_length_check(test_df)\n","d55b95e4":"data_frame = pd.concat([train_df,train_ex_df],ignore_index=True)\ndata_frame.head()","a5a74312":"data_frame.info()","a1379d7a":"print('Total Counts of label column: \\n'.format(),data_frame['label'].value_counts())\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.countplot(x='label', data=data_frame)","11fbad26":"df_bug = data_frame[data_frame['label']==0]\ndf_feature = data_frame[data_frame['label']==1]\ndf_question = data_frame[data_frame['label']==2]","2e4531d2":"label_counts = data_frame.label.value_counts().sort_index()\nlabel_counts","988d311f":"def fx(x):\n    return x['title'] + \" \" + x['body']   \ndata_frame['text']= data_frame.apply(lambda x : fx(x),axis=1)\ndata_frame['text']= data_frame.apply(lambda x : fx(x),axis=1)","8d71f5a2":"print('Number of datapoints with label as Bug :',label_counts[0])\nprint('Number of datapoints with label as Feature :',label_counts[1])\nprint('Number of datapoints with label as Question :',label_counts[2])","00cb7f3e":"import nltk\nimport re\nimport string\n\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    return text","bbf20145":"from string import punctuation\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(git_text):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_git_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_git_text","2325974a":"from collections import Counter\nimport plotly.express as px\n\ndef visulize_dataset(data_frame, category):\n    \n    # Let's apply the above two functions 'clean_text' and 'remove_stopwords' to the whole dataset\n\n    data_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n    data_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\n    data_frame[\"text\"] = data_frame[\"text\"].apply(remove_stopwords)\n    \n    \n    word_list = []\n    \n    for i, j in data_frame.iterrows():\n        for word in j['text']:\n            word_list.append(word)\n        \n    count_dict = Counter(word_list)\n    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n    fig = px.histogram(most_common_words_df,\n                       x='word', \n                       y='count',\n                       title='Most common terms used while refering to a GitHub {}'.format(category),\n                       color_discrete_sequence=['#843B62'] )\n    fig.show()","1d3bd69a":"\"\"\"\nlabel 0: Bug\nlabel 1: Feature\nlabel 2: Question\n\"\"\"\ndata_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\ndata_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\ndata_frame.head()","d42eca19":"data_frame.head()\ndata_frame.describe","4a1acbb8":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","8976bf12":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    BERT_PATH = \"\/kaggle\/input\/bert-base-uncased\"\n    MODEL_PATH = \"..\/input\/bert-base-uncased\/pytorch_model.bin\"\n    tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n    truncation=True","f5650a07":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=0.005)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.BCEWithLogitsLoss()\n\nmodel.to(device)\ncriterion.to(device)","8407f890":"#loading our BERT model\nBERT_PATH = '\/kaggle\/input\/bert-base-uncased'","57e4b7ba":"#loading the pre-trained BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH)","83ac4f5b":"# some basic operations to understand how BERT converts a sentence into tokens and then into IDs\nsample_body = 'script stopped adding videos saenzramiro abc xyz'\ntokens = tokenizer.tokenize(sample_body)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_body}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","a083182e":"# using encode_plus to add special tokens : [CLS]:101, [SEP]:102, [PAD]:0\nencodings = tokenizer.encode_plus(\n            sample_body,\n            max_length=32,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n)\n\nencodings.keys()","0d1042d1":"print('Input IDs : {}'.format(encodings['input_ids'][0]))\nprint('\\nAttention Mask : {}'.format(encodings['attention_mask'][0]))","85f1b28d":"MAX_LENGTH = 512","e4c2a5b0":"class Git_Message(Dataset):\n    def __init__(self, git_messages, label, tokenizer, max_len):\n        self.git_messages = git_messages\n        self.label = label\n        self.tokenizer = config.tokenizer\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.git_messages)\n\n    def __getitem__(self, item):\n        git_messages = str(self.git_messages[item])\n        label = self.label[item]\n        \n        encoding = self.tokenizer.encode_plus(\n        git_messages,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt')\n        return {\n        'git_messages': git_messages,\n         'input_ids': encoding['input_ids'],\n         'attention_mask': encoding['attention_mask'],\n         'label': torch.tensor(label, dtype=torch.long)\n          }","b17244ca":"from sklearn.model_selection import train_test_split","e6c37d3c":"data_frame.head()","570b9e47":"data_frame = data_frame[:2000]","0ede47c8":"training_data, testing_data = train_test_split(\n    data_frame,\n    test_size=0.1,\n    random_state=RANDOM_SEED\n)\n\ntesting_data, validation_data = train_test_split(\n    testing_data,\n    test_size=0.5,\n    random_state=RANDOM_SEED\n)","8edeea92":"training_data.shape, testing_data.shape, validation_data.shape","e6e729b9":"def create_data_loader(data, tokenizer, max_len, batch_size):\n    \n    ds = Git_Message(git_messages=data.text.to_numpy(),\n    label=data.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len)\n    \n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4)\n\n\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(training_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\ntesting_data_loader = create_data_loader(testing_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\nval_data_loader = create_data_loader(validation_data, tokenizer, MAX_LENGTH, BATCH_SIZE)","3fe61f78":"data_frame = next(iter(train_data_loader))\ndata_frame.keys()","0d3c8c8c":"data_frame['input_ids'].squeeze().shape, data_frame['attention_mask'].squeeze().shape, data_frame['label'].shape","5ceab04a":"print('git_messages  : ', data_frame['git_messages'][0])\nprint('input_ids : ', data_frame['input_ids'].squeeze()[0])\nprint('attention_mask : ', data_frame['attention_mask'].squeeze()[0])\nprint('label : ', data_frame['label'][0])","7ec0afb4":"bert_model = BertModel.from_pretrained(BERT_PATH)","49e01ec7":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encodings['input_ids'],\n  attention_mask=encodings['attention_mask']\n)","8cc50984":"last_hidden_state.shape, pooled_output.shape","f2656a42":"class BugPredictor(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(BugPredictor, self).__init__()\n        self.bert_model = BertModel.from_pretrained(BERT_PATH)\n        self.dropout = nn.Dropout(p=0.0)\n        self.out = nn.Linear(self.bert_model.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert_model(\n        input_ids=input_ids,\n        attention_mask = attention_mask\n        )\n        output = self.dropout(pooled_output)\n        return self.out(output)","bb9552b8":"\"\"\"\nlabel 0: Bug\nlabel 1: Feature\nlabel 2: Question\n\"\"\"\nclass_names = [0, 1, 2]\nbug_predictor_model = BugPredictor(len(class_names))\nbug_predictor_model = bug_predictor_model.to(device)","f5df7c9e":"EPOCHS = 10\n\noptimizer = AdamW(bug_predictor_model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","c920fb5c":"def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d['input_ids'].squeeze().to(device)\n        attention_mask = d['attention_mask'].squeeze().to(device)\n        targets = d['label'].to(device)\n\n        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds==targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","73a05c4a":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].squeeze().to(device)\n            attention_mask = d['attention_mask'].squeeze().to(device)\n            targets = d['label'].to(device)\n\n            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds==targets)\n            losses.append(loss.item())\n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","ad2f5d5d":"%%time\nfrom collections import defaultdict\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}\/{}'.format(epoch+1,EPOCHS))\n    print('-' * 10)\n  \n    train_acc, train_loss = train_model(bug_predictor_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(training_data))\n    \n    print('Train loss : {} accuracy : {}'.format(train_loss, train_acc))\n    \n    val_acc, val_loss = eval_model(bug_predictor_model, val_data_loader, loss_fn, device, len(validation_data))\n    \n    print('Validation loss : {} accuracy : {}'.format(val_loss, val_acc))\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        print('Saving the best model ...')\n        torch.save(bug_predictor_model.state_dict(), 'best_model.bin')\n        best_accuracy = val_acc","c14529dc":"sample_bug_message = \"Script stopped adding video's. A recent change in the youtube layout broke the script. Probably caused by element names being altered.\"","dc19d81e":"class_names = ['bug', 'feature', 'question']","fdac58ec":"def predict_git_category(sample_message, model):\n    encoded_message = tokenizer.encode_plus(sample_bug_message, max_length=MAX_LENGTH, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n    input_ids = encoded_message['input_ids'].to(device)\n    attention_mask = encoded_message['attention_mask'].to(device)\n    \n    output = model(input_ids=input_ids, attention_mask=attention_mask)\n    _, prediction_idx = torch.max(output, dim=1)\n        \n    return class_names[prediction_idx]\n","473d0c90":"print('Sample bug message : ', sample_bug_message)\nprint('Predicted GitHub Category : ', predict_git_category(sample_bug_message, bug_predictor_model))","56bf295e":"# **Getting Started**","0261f384":"# Results","dc616b3d":"# BERT conversion of sentences","dc452f7c":"# Let's Start Training the Model","5d8b0363":"# Adding [CLS]:101, [SEP]:102, [PAD]:0","c07c1e66":"If we look at the label feild , it is basically divided into three segments as we mentioned earlier:\n* Bug - 0\n* Feature - 1\n* Question - 2\n\nLet's categorize the data fields based on their labels as **df_bug** for all the bugs labeled from the dataset. Similarly we do it for features as **df_feature**  and question as **df_question** respectively.","785695ed":"## Load The Dataset","e17b5a76":"# GITHUB BUGS PREDICTION\n[Indrajit Singh](https:\/\/www.linkedin.com\/in\/indrajitsinghds\/) - December 2020\n\n\n----------\n## My First Kaggle Notebook","ff08ed58":"# *  What is BERT?\n\nBERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\n\n# *  What Makes BERT Different?\n\nBERT builds upon recent work in pre-training contextual representations \u2014 including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\n\n# *  Why does pretrained language models makes a difference?\n\nPre-trained representations can either be **context-free** or **contextual**, and contextual representations can further be unidirectional or bidirectional. \n\n* Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word \u201cbank\u201d would have the same context-free representation in \u201cbank account\u201d and \u201cbank of the river.\u201d \n\n* Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence \u201cI accessed the bank account,\u201d a unidirectional contextual model would represent \u201cbank\u201d based on \u201cI accessed the\u201d but not \u201caccount.\u201d \n\nHowever, BERT represents \u201cbank\u201d using both its previous and next context \u2014 \u201cI accessed the ... account\u201d \u2014 starting from the very bottom of a deep neural network, making it deeply bidirectional.\n\nRead more about Basic BERT [here](http:\/\/https:\/\/arxiv.org\/pdf\/1810.04805.pdf).\n\nIn this Notebook we will be using BERT Base Uncased, from [Hugging Face Library](http:\/\/https:\/\/huggingface.co\/bert-base-uncased). \n________________________________________\n**Model: bert-base-uncased**\n\n*        12-layer, 768-hidden, 12-heads, 110M parameters.\n\n*        Trained on lower-cased English text.\n\n```\n@article{DBLP:journals\/corr\/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs\/1810.04805},\n  year      = {2018},\n  url       = {http:\/\/arxiv.org\/abs\/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https:\/\/dblp.org\/rec\/journals\/corr\/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https:\/\/dblp.org}\n}\n```","6fc39c79":"# Removing Stopwords & Punctuations","7e0c4629":"# **Data Set**","d98286b6":"# Let's create a dataframe","866a0176":"Let's ensure we make text lowercase, remove text in square brackets, remove links, remove punctuation and remove words containing numbers.","2abe9f09":"# Workflow\n\n1. Problem Statement.\n1. Geting Started.\n1. Train and Test data \n1. Analyze, identify patterns, and explore the data.\n1. Model, predict and solve the problem.\n1. Visualize, report, and present the problem solving steps and final solution.\n1. Supply or submit the results.","8924bea9":"# Construct a Class for GitHub Messages","c5b98c5f":"## **Problem Statement:**\n\n [MachineHack](http:\/\/https:\/\/www.machinehack.com\/) a Biggest Machine Learning Community For Data Science & AI Enthusiastshas comeup with one of the popular Hackathon on GitHub bugs prediction. \n \nDetecting defects in software systems is an evergreen topic, since there is no real world software without bugs. There are many algorithms that has come up recently to help developers find bugs in software. The challenges of having curated datasets for own domain specific software , or code relates bug finding is rare. You can go through the [research paper](http:\/\/https:\/\/www.researchgate.net\/publication\/304664263_A_Public_Bug_Database_of_GitHub_Projects_and_Its_Application_in_Bug_Prediction) for gaining more ideas on Bug Prediction Databases and related algorithms that can be used.\n\nIn this Hackathon proposed by Machinehack , we need to come up with an algorithm that can predict the bugs, features, and questions based on GitHub titles and the text body. \n\n\nThe Dataset has three columns Title, Body and Label: \n\n*     **-Title** - the title of the GitHub bug, feature question\n\n*     **-Body** - the body of the GitHub bug, feature question\n\n*     **-Label** - Represents various classes of Labels\n\nOnce we clean up the dataset [embold_train.json, embold_test.json] and make it ready for the model we need feed the data to a specifc model (Here we using BERT base uncased as pretrained model to predict the expected outcome )","a9bf158f":"\n\n\n![image.png](attachment:image.png)\n\n\n\n\n    Bug - 0\n    Feature - 1\n    Question - 2\n\n\n\n","bbee4569":"This notebook simplifies the steps to solve github bug prediction through a typical workflow using BERT base uncased model. Before we get started with the workflow , the first question that comes into our mind is how do we get started. Well we will get there but let's first understand what exactly the problem statement is and what is expected here?","e4172dd5":"# Count the Lables :  Bug, Feature abd Questions","de5f7fde":"## **Importing the essential libraries**\n\n\nImporting the required libraries at the begining of your notebook helps you run your code seemless . We frequenly get interrupted by the missing library errors or any other imports which can be avoided by importing them early.","a9ee0989":"# BERT","05b120be":"# Creating Train, Test Validation","7a2f375f":"# Cleaning the Text Field","900bffb8":"Would like to thank and give credits to the Kaggle Kernel by  [ Purva Singh](http:\/\/https:\/\/www.kaggle.com\/purvasingh\/github-bug-prediction-via-bert). I have a taken the ideas and workflow as a guide to bhild my first Kaggle Notebook. \ud83d\ude0a\ud83d\udc4f\ud83d\udc4f\ud83d\udc4f\ud83d\udc4f\ud83d\udc4f\ud83d\udc4f\ud83d\udc4f\ud83d\udc4f","b9afc10c":"# Printing the text feild after Cleaning","10176155":"### Test Set","173e143d":"Currently the title and body is in separate fields. We will combine the title and body to keep them into a single field called text:","cf5d9ec3":"# Visulaize the Dataset","974e28e1":"# Creating Data Loader","37171e6a":"## **Predictor Model**","56921a2e":"# The Model","ee758302":"### Checking the Length of Each Data Frame","e34daf1f":"Looks like both the train and train extra dataframe contains the same content so let's combine them using concatenation command of pandas on index axis","751a9f62":"### Train Set"}}