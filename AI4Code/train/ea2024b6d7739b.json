{"cell_type":{"89aae2ac":"code","71088d70":"code","536f64a5":"code","08ffa5ee":"code","b728a217":"code","9aacfa96":"code","e6527cd8":"code","82634293":"code","2ce30f00":"code","072f133d":"code","16fca403":"code","17a7c6a7":"code","f7a5612b":"code","7253b97b":"code","22a52d0c":"code","606606d0":"code","f44892d9":"code","78022ffa":"code","61294a58":"code","fcc9c159":"code","b4825b27":"code","639fe178":"code","bbb23648":"code","89a1032e":"code","ebcf7029":"code","c9f3e20c":"code","84f18f1f":"code","b3bd3443":"code","cfccec51":"code","999d0a00":"code","52ff8ea5":"code","52c2997a":"code","2421b333":"code","9f8b387d":"code","8b762f8c":"code","d1d3a7f5":"code","b2f16ec4":"code","0e641197":"code","1fcc5b24":"code","9b65dfdc":"code","d4112a15":"code","b1ae1ef2":"code","41209337":"code","5ca9700e":"markdown","b32f0b70":"markdown","db6eb8f3":"markdown","5f199975":"markdown","25a3cf13":"markdown","5004fa65":"markdown","771a82fe":"markdown","6726dc1c":"markdown","1b7b356a":"markdown","4cac3019":"markdown","1f29d753":"markdown","4cfb4bcc":"markdown","ab47351b":"markdown","32a65ffb":"markdown","3c9327d5":"markdown","80b48e7a":"markdown","450d61a9":"markdown"},"source":{"89aae2ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71088d70":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_columns', None)\n\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (15,7)","536f64a5":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndata.head()","08ffa5ee":"print(data.shape, 'This dataset has', data.shape[0], 'rows &', data.shape[1], 'columns')","b728a217":"data.info()","9aacfa96":"data.isnull().sum().sum()","e6527cd8":"sns.countplot(data['target'], palette='Set1')\nplt.title('Patients who has and has not disease')\nplt.xlabel('Target')\nplt.ylabel('Number of patients')\nplt.show()","82634293":"sns.distplot(data['age'], bins = 20)\nplt.title('Dataset Members Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Distribution')\nplt.show()","2ce30f00":"sns.boxplot(data['target'], data['age'], palette='Set1')\nplt.title('Patients who has and has not disease depends on Age')\nplt.xlabel('Target')\nplt.ylabel('Age')\nplt.show()","072f133d":"sns.countplot(data['target'], hue = data['sex'], palette='Set1')\nplt.title('Patients with or without disease depends on Sex')\nplt.xlabel('Target')\nplt.ylabel('Nr of patients')\nplt.show()","16fca403":"columns_feature = ['cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n\nfor i in columns_feature:\n    sns.boxplot(data['target'], data[i], palette='Set1')\n    plt.title('Target depends on ' + i)\n    plt.xlabel('Target')\n    plt.ylabel(i)\n    plt.show()","17a7c6a7":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve","f7a5612b":"data.head()","7253b97b":"X = data.drop('target', axis = 1)\ny = data['target']","22a52d0c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)","606606d0":"logistic = LogisticRegression(random_state=0)\ndt = DecisionTreeClassifier(random_state=1)\nsvc = SVC()\nrandom = RandomForestClassifier(random_state=1)\nxgb = XGBClassifier(random_state = 10)","f44892d9":"logistic.fit(X_train, list(y_train.values))\nlog_pred = logistic.predict(X_test)\nprint('Accuracy: ', logistic.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), log_pred))\nprint('Recall: ', recall_score(list(y_test.values), log_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), log_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), log_pred))","78022ffa":"dt.fit(X_train, list(y_train.values))\ndt_pred = dt.predict(X_test)\nprint('Accuracy: ', dt.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), dt_pred))\nprint('Recall: ', recall_score(list(y_test.values), dt_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), dt_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), dt_pred))","61294a58":"svc.fit(X_train, list(y_train.values))\nsvc_pred = svc.predict(X_test)\nprint('Accuracy: ', svc.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), svc_pred))\nprint('Recall: ', recall_score(list(y_test.values), svc_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), svc_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), svc_pred))","fcc9c159":"random.fit(X_train, list(y_train.values))\nrandom_pred = random.predict(X_test)\nprint('Accuracy: ', random.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), random_pred))\nprint('Recall: ', recall_score(list(y_test.values), random_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), random_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), random_pred))","b4825b27":"xgb.fit(X_train, list(y_train.values))\nxgb_pred = xgb.predict(X_test)\nprint('Accuracy: ', xgb.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), xgb_pred))\nprint('Recall: ', recall_score(list(y_test.values), xgb_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), xgb_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), xgb_pred))","639fe178":"y_score_log = logistic.decision_function(X_test)\ny_score_dt = dt.predict_proba(X_test)[:,1]\ny_score_svc = svc.decision_function(X_test)\ny_score_random = random.predict_proba(X_test)[:,1]\ny_score_xgb = xgb.predict_proba(X_test)[:,1]\n\nfpr_log, tpr_log, _ = roc_curve(y_test.values, y_score_log)\nfpr_dt, tpr_dt, _ = roc_curve(y_test.values, y_score_dt)\nfpr_svc, tpr_svc, _ = roc_curve(y_test.values, y_score_svc)\nfpr_random, tpr_random, _ = roc_curve(y_test.values, y_score_random)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test.values, y_score_xgb)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr_log, tpr_log, label = \"Logistic Regression\")\nplt.plot(fpr_dt, tpr_dt, label = 'Decision Tree')\nplt.plot(fpr_svc, tpr_svc, label = 'Support Vector')\nplt.plot(fpr_random, tpr_random, label = 'Random Forest')\nplt.plot(fpr_xgb, tpr_xgb, label = 'XGBoost')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","bbb23648":"y_score_random = random.predict_proba(X_test)[:,1]\ny_score_xgb = xgb.predict_proba(X_test)[:,1]\n\nfpr_random, tpr_random, _ = roc_curve(y_test.values, y_score_random)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test.values, y_score_xgb)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr_random, tpr_random, label = 'Random Forest')\nplt.plot(fpr_xgb, tpr_xgb, label = 'XGBoost')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","89a1032e":"importances = random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in random.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_test.shape[1]):\n    print(\"%d. feature: %s (%f)\" % (f + 1, data.iloc[:,indices[f]].name, importances[indices[f]]))","ebcf7029":"labels = ['cp', 'ca', 'thalach', 'oldpeak', 'chol', 'age', 'thal', 'trestbps']\n\nX2 = X[labels]","c9f3e20c":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size = 0.2, random_state = 10)","84f18f1f":"random2 = RandomForestClassifier(random_state=1)\nrandom2.fit(X_train2, list(y_train2.values))\nrandom2_pred = random2.predict(X_test2)\n\nprint('Accuracy: ', random2.score(X_test2, list(y_test2.values)))\nprint('Precission: ', precision_score(list(y_test2.values), random2_pred))\nprint('Recall: ', recall_score(list(y_test2.values), random2_pred))\nprint('F1 Score: ', f1_score(list(y_test2.values), random2_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test2.values), random2_pred))","b3bd3443":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso","cfccec51":"feat_sel_model = SelectFromModel(Lasso(alpha = 0.005, random_state = 42))\nfeat_sel_model.fit(X, y)","999d0a00":"feat_sel_model.get_support()","52ff8ea5":"selected_feat = X.columns[feat_sel_model.get_support()]","52c2997a":"X3 = X[selected_feat]\nX3.head()","2421b333":"X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y, test_size = 0.2, random_state = 10)","9f8b387d":"xgb3 = XGBClassifier(random_state = 10)\nxgb3.fit(X_train3, list(y_train3.values))\nxgb_pred3 = xgb3.predict(X_test3)\nprint('Accuracy: ', xgb3.score(X_test3, list(y_test3.values)))\nprint('Precission: ', precision_score(list(y_test3.values), xgb_pred3))\nprint('Recall: ', recall_score(list(y_test3.values), xgb_pred3))\nprint('F1 Score: ', f1_score(list(y_test3.values), xgb_pred3))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test3.values), xgb_pred3))","8b762f8c":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 50)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","d1d3a7f5":"rf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3,\n                              verbose = 2, random_state = 42, n_jobs = -1)\nrf_random.fit(X_train2, y_train2)","b2f16ec4":"rf_random.best_estimator_","0e641197":"random3 = RandomForestClassifier(max_depth=80, max_features='sqrt', min_samples_split=10,\n                       n_estimators=991)\nrandom3.fit(X_train2, list(y_train2.values))\nrandom3_pred = random3.predict(X_test2)\n\nprint('Accuracy: ', random3.score(X_test2, list(y_test2.values)))\nprint('Precission: ', precision_score(y_test2.values, random3_pred))\nprint('Recall: ', recall_score(list(y_test2.values), random3_pred))\nprint('F1 Score: ', f1_score(list(y_test2.values), random3_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test2.values), random3_pred))","1fcc5b24":"n_estimators = [int (x) for x in np.linspace(100, 1500, num = 50)]\nlearning_rate = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25]\ngamma = [0.0, 0.1, 0.2, 0.3, 0.4]\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 10)]\nmin_child_weight = [int(x) for x in np.linspace(1, 5, num = 5)]\nbooster = ['gbtree', 'gblinear']\nbase_score = [0.25, 0.5, 0.75, 1]\n\nhyper_parameter = {\n    'n_estimators': n_estimators,\n    'learning_rate': learning_rate,\n    'gamma': gamma,\n    'max_depth': max_depth,\n    'min_child_weight': min_child_weight,\n    'booster': booster,\n    'base_score': base_score\n}","9b65dfdc":"xgb_optimize = XGBClassifier()\nxgb_random = RandomizedSearchCV(estimator=xgb_optimize, param_distributions=hyper_parameter, n_iter=100,\n                                cv = 3, verbose=2, random_state=42, n_jobs=-1)\nxgb_random.fit(X_train, y_train)","d4112a15":"xgb_random.best_estimator_","b1ae1ef2":"xgb_optimize =XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.25, max_delta_step=0, max_depth=7,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=414, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\nxgb_optimize.fit(X_train, list(y_train.values))\nxgb_optimize_pred = xgb_optimize.predict(X_test)\nprint('Accuracy: ', xgb_optimize.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), xgb_optimize_pred))\nprint('Recall: ', recall_score(list(y_test.values), xgb_optimize_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), xgb_optimize_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), xgb_optimize_pred))","41209337":"y_score_random = random.predict_proba(X_test)[:,1]\ny_score_random2 = random2.predict_proba(X_test2)[:,1]\ny_score_xgb = xgb.predict_proba(X_test)[:,1]\n\nfpr_random, tpr_random, _ = roc_curve(y_test.values, y_score_random)\nfpr_random2, tpr_random2, _ = roc_curve(y_test2.values, y_score_random2)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test.values, y_score_xgb)\n\nplt.plot(fpr_random, tpr_random, label = 'Random Forest', color = 'orange')\nplt.plot(fpr_random2, tpr_random2, label = 'Random Forest Optimize', color = 'blue')\nplt.plot(fpr_xgb, tpr_xgb, label = 'XGBoost', color = 'green')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","5ca9700e":"Support Vector ","b32f0b70":"*We can see that patients who has disease varies from 45 years old to almost 60 years old and for people who has not disease are on age from 52 years old to 63 years old. Age is not a good predictor for this disease*","db6eb8f3":"*We can see that our dataset is balanced distributed: 140 patients has not disease & 163 has disease*","5f199975":"Random Forest after feature selection is the best model","25a3cf13":"Decision Tree","5004fa65":"Random Forest Optimization Tuning","771a82fe":"\nFor better comparison between the models let's draw the ROC curves","6726dc1c":"3.Data Modeling","1b7b356a":"Logistic Regression","4cac3019":"XGBoost","1f29d753":"*This is a very small dataset so when we remove columns we loose information that is why the accuracy is decreasing*","4cfb4bcc":"1. Import Data","ab47351b":"Random Forest","32a65ffb":"XGBoost Feature Importance","3c9327d5":"Let's rank the features in terms of importance","80b48e7a":"XGBoost Optimization Tuning","450d61a9":"2. Data Visualization"}}