{"cell_type":{"b7c3a697":"code","da990a7a":"code","ed9cb675":"code","6b395aef":"code","c2c66ccc":"code","3fa721ae":"code","ec0adef1":"code","2b957a9d":"code","90820b15":"code","8d79519e":"code","becd048b":"code","a95f5ce6":"code","b7d11dbb":"code","95d99fa9":"code","4df5d17f":"code","9e587f26":"code","bc33064c":"code","71813b90":"code","c445dde5":"code","23d1e849":"code","a88b67cd":"code","56c779e5":"code","cb9599d9":"code","f216379f":"code","c1adb701":"code","1efedc60":"code","5aefaca7":"code","ede82d74":"code","e1c40978":"code","3ae91cea":"code","1cf1ad01":"code","55e2de60":"code","378bfe83":"code","603e1a12":"code","50f7eba9":"code","58d9f216":"code","7eb56c82":"code","5ded7d93":"code","fdc0936c":"code","62cbd5e5":"code","9334f4ca":"code","6985f8ed":"code","ad0b3192":"code","ff9489cb":"code","a823ef8d":"code","e834f48b":"code","d15ae7e2":"code","036ed872":"code","383be946":"code","5a607218":"code","724615ea":"code","6cf1524a":"code","277bd079":"code","bb4f6845":"code","2246084d":"code","6b788145":"code","547fdaee":"code","e5f92917":"code","947e7fb0":"code","a185c406":"code","a06fdaee":"code","3300b13b":"code","9a20b6d4":"code","3544161e":"code","81685e11":"code","f3833f4b":"code","5c71fc92":"code","d648c6c5":"markdown","f0b923ca":"markdown","b95ce7e0":"markdown","7af54196":"markdown","c775b555":"markdown","13fe07ab":"markdown","e5414870":"markdown","f43056d2":"markdown","12051271":"markdown","27e4517b":"markdown","6f9f09e4":"markdown","0eef2527":"markdown","76150a02":"markdown","56683834":"markdown","f2a071f2":"markdown","1c9c2a98":"markdown","d81126a8":"markdown","afc4f37b":"markdown","b87efbf0":"markdown","f8a2fbdb":"markdown","66a52987":"markdown","986830b2":"markdown","04d8158d":"markdown"},"source":{"b7c3a697":"# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","da990a7a":"df = pd.read_csv(\"..\/input\/disneyland-reviews\/DisneylandReviews.csv\", encoding='cp1252', na_values = 'missing')","ed9cb675":"df","6b395aef":"df.describe()","c2c66ccc":"df.shape","3fa721ae":"# Dropping rows with missing values\ndf.dropna(inplace = True)","ec0adef1":"df.Review_ID.nunique()","2b957a9d":"df.Review_ID.value_counts()","90820b15":"# Dropping duplicates and only keeping its first occurrence\ndf.drop_duplicates(subset=\"Review_ID\",inplace=True,keep='first')\ndf.info()","8d79519e":"timeline = df['Year_Month'].str.split('-')\ntimeline","becd048b":"df['Year'] = timeline.str[0]\ndf['Month'] = timeline.str[1]","a95f5ce6":"df.drop([\"Year_Month\"], axis = 1, inplace = True)\ndf.head()","b7d11dbb":"year_viz1 = df.groupby('Year', as_index=False).agg({'Rating':'count'})\nyear_viz1","95d99fa9":"# Data visualization of 'Ratings' vs 'Year'\nx = year_viz1['Year']\ny = year_viz1['Rating']\nplt.bar(x, y, color = 'mediumslateblue')\nplt.xticks(x)\nfor i, v in enumerate(x):\n    plt.text(v, y[i], y[i],\n             fontsize = 9,\n             horizontalalignment='center',\n             verticalalignment='bottom') \nplt.show","4df5d17f":"sns.countplot(data = df, x = 'Year', hue = 'Rating')","9e587f26":"month_viz1 = df.groupby('Month', as_index=False).agg({'Rating':'count'})\nmonth_viz1","bc33064c":"# Data Visualization of 'Month' vs 'Ratings'\nx = month_viz1['Month']\ny = month_viz1['Rating']\nplt.bar(x, y, color = 'mediumslateblue')\nplt.xticks(x)\nfor i, v in enumerate(x):\n    plt.text(v, y[i], y[i],\n             fontsize = 9,\n             horizontalalignment='center',\n             verticalalignment='bottom') \nplt.show","71813b90":"import plotly.express as px","c445dde5":"rlocs_viz = df.groupby('Reviewer_Location', as_index=False).agg({'Rating':'mean'}).sort_values('Rating', ascending=False)","23d1e849":"fig = px.choropleth(rlocs_viz, \n                    locations = 'Reviewer_Location', \n                    locationmode = 'country names', \n                    color = 'Rating',\n                    hover_data = ['Rating'], \n                    title = 'Average Reviewer Countries')\nfig.show()","a88b67cd":"df.reset_index(inplace=True)\ndf.shape","56c779e5":"from nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport string\nimport re","cb9599d9":"df.head()","f216379f":"# Lowering the text\ndf['Review']= df['Review_Text'].apply(lambda x: x.lower())","c1adb701":"# Removing punctuation\ndef remove_punctuation(text):\n    text = \" \".join([word.strip(string.punctuation) for word in text.split(\" \")])\n    return text\ndf['Review'] = df['Review'].apply(remove_punctuation)","1efedc60":"# Removing numbers in words\ndef remove_numbers_in_word(text):\n    text = \"\".join([word for word in text if not any(c.isdigit() for c in word)])\n    return text\ndf['Review'] = df['Review'].apply(remove_numbers_in_word)","5aefaca7":"# Converting reviews to tokens, ie, broken into elements of a list\ndef tokenization(text):\n    text = text.split()\n    return text\ndf['Review'] = df['Review'].apply(tokenization)","ede82d74":"import nltk\nnltk.download('stopwords')","e1c40978":"# Stopwords: These words do not add any value to the analysis of the text.\n# Removing stopwords like 'I', 'you', etc.\nstop_words = set(stopwords.words('english'))\npunctuation = string.punctuation\nname = ['disney', 'disneyland', 'iron', 'ironman', 'mickey', 'buz', 'hk', 'california', 'paris', 'hongkong']\n\ndef remove_stopwords(text): \n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop_words and x not in name and x not in punctuation] \n    return text\ndf['Review'] = df['Review'].apply(remove_stopwords)","3ae91cea":"df.head()","1cf1ad01":"import nltk\nnltk.download('wordnet')","55e2de60":"# Lemmatization: converting words to their base word, for example, 'ran', 'runs', 'run' all reduce to 'run'\n# Lemmatizing the reviews\nfrom nltk.stem import WordNetLemmatizer\ndef lemmatize(text):\n    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n    return text\ndf['Review'] = df['Review'].apply(lemmatize)","378bfe83":"# Joining all the pre-processed review text\ndef join_text(text):\n    text = \" \".join(text)\n    return text\ndf['Review'] = df['Review'].apply(join_text)","603e1a12":"df.head()","50f7eba9":"# Finding out how many branches of disneyland are included in the data\ndf.Branch.nunique()","58d9f216":"df.Branch.value_counts()","7eb56c82":"df['Branch'] = df['Branch'].str.slice(start=11)","5ded7d93":"df","fdc0936c":"# Plotting number of reviews against branch\nsns.set_style(\"darkgrid\")\nsns.countplot(x=\"Branch\", data=df)","62cbd5e5":"# Plotting average rating of branches\nbranch_rating = df.groupby('Branch', as_index=False).agg({'Rating':'mean'})\n\nx = branch_rating['Branch']\ny = branch_rating['Rating']\n\nplt.plot(x, y, color = 'mediumslateblue')\nplt.ylim(3,5)\nfor i, v in enumerate(x):\n    plt.text(v, y[i], round(y[i],2),\n             fontsize = 11,\n             horizontalalignment='center',\n             verticalalignment='bottom') \nplt.show","9334f4ca":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n%matplotlib inline","6985f8ed":"california = np.array(Image.open(r\"..\/input\/images\/california.png\"))\nhongkong = np.array(Image.open(r\"..\/input\/images\/hongkong.png\"))\nparis = np.array(Image.open(r\"..\/input\/images\/paris.png\"))","ad0b3192":"# Creating a wordcloud of the words mostly found in reviews of the California branch.\nplt.figure(figsize = (10,10))\nWc = WordCloud(mask = california, background_color='white',\n               max_words = 1000 , width = 500 , height = 400, \n               contour_width = 1, contour_color = 'black', colormap = \"viridis\").generate(\" \".join(df[df.Branch == 'California'].Review))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear')","ff9489cb":"# Creating a wordcloud of the words mostly found in reviews of the Hongkong branch.\nplt.figure(figsize = (10,10))\nWc = WordCloud(mask = hongkong, background_color='white',\n               max_words = 1000 , width = 500 , height = 400, \n               contour_width = 1, contour_color = 'black', colormap = \"plasma\").generate(\" \".join(df[df.Branch == 'HongKong'].Review))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear')","a823ef8d":"# Creating a wordcloud of the words mostly found in reviews of the Paris branch.\nplt.figure(figsize = (10,10))\nWc = WordCloud(mask = paris, background_color='white',\n               max_words = 1000 , width = 500 , height = 400, \n               contour_width = 1, contour_color = 'black', colormap = \"cividis\").generate(\" \".join(df[df.Branch == 'Paris'].Review))\nplt.axis(\"off\")\nplt.imshow(Wc , interpolation = 'bilinear')","e834f48b":"df['Polarity Rating'] = df['Rating'].apply(lambda x: 'Positive' if x > 3 else('Neutral' if x == 3 else 'Negative'))\ndf.head()","d15ae7e2":"import nltk\nnltk.download('vader_lexicon')","036ed872":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsenti_analyzer=SentimentIntensityAnalyzer()","383be946":"v_scores = []\n\nfor i in df['Review']:\n    score = senti_analyzer.polarity_scores(i)\n    v_scores.append(score['compound'])","5a607218":"df['VADER_score'] = v_scores\ndf.groupby(\"Rating\")[\"VADER_score\"].describe()","724615ea":"df['Vader Rating'] = df['VADER_score'].apply(lambda x: 'Positive' if x > 0 else('Neutral' if x == 0 else 'Negative'))\ndf.head()","6cf1524a":"# Using One-Hot Encoding for the branches of disneyland.\nlocation = pd.get_dummies(df['Branch'])\ndf = pd.concat([df,location],axis=1)\ndf","277bd079":"# Dropping unnecessary columns\ndf.drop(['Branch','index','Review_Text'],axis=1,inplace=True)\ndf.dtypes","bb4f6845":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor i in ['Reviewer_Location','Year','Polarity Rating','Vader Rating']:\n    df[i] = le.fit_transform(df[i])","2246084d":"df[\"Month\"] = pd.to_numeric(df[\"Month\"])\ndf","6b788145":"X = df['Review']\ny = df['Polarity Rating']","547fdaee":"from sklearn.model_selection import train_test_split","e5f92917":"# Splitting the data into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n\nprint(X_train.shape, X_test.shape) \nnp.unique(y_train, return_counts=True)","947e7fb0":"from sklearn.feature_extraction.text import TfidfVectorizer","a185c406":"stop_words = stopwords.words('english')\n\nvect = TfidfVectorizer(stop_words=stop_words).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","a06fdaee":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth = 100, random_state = 42)\ndt.fit(X_train_vectorized, y_train)\nprint(dt.score(X_train_vectorized, y_train))\nprint(dt.score(vect.transform(X_test), y_test))","3300b13b":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 100, random_state = 42)\nrf.fit(X_train_vectorized, y_train)\nprint(rf.score(X_train_vectorized, y_train))\nprint(rf.score(vect.transform(X_test), y_test))","9a20b6d4":"X = df['Review']\ny = df['Vader Rating']","3544161e":"# Splitting the data into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.2)\n\nprint(X_train.shape, X_test.shape) \nnp.unique(y_train, return_counts=True)","81685e11":"stop_words = stopwords.words('english')\n\nvect = TfidfVectorizer(stop_words=stop_words).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","f3833f4b":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth = 100, random_state = 42)\ndt.fit(X_train_vectorized, y_train)\nprint(dt.score(X_train_vectorized, y_train))\nprint(dt.score(vect.transform(X_test), y_test))","5c71fc92":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 100, random_state = 42)\nrf.fit(X_train_vectorized, y_train)\nprint(rf.score(X_train_vectorized, y_train))\nprint(rf.score(vect.transform(X_test), y_test))","d648c6c5":"### Loading the Dataset","f0b923ca":"### Dealing with missing values and duplicate data","b95ce7e0":"Training Accuracy : 99.41%\n\nTesting Accuracy : 89.15%","7af54196":"## Using Random Forest Classifier","c775b555":"## Using Decision Tree classifier","13fe07ab":"### WordCloud","e5414870":"### Data Visualization","f43056d2":"# Disneyland Reviews Analysis\nThe aim of this project is to analyse the reviews given by visitors from different countries of the world using Natural Language Processing (NLP) to understand the sentiment of the reviews and classify them using Sentiment Analysis metrics like Sentiment Polarity and VADER Polarity. This processed data is then fed to different classifier models (DecisionTreeClassifier and RandomForestClassifier) to get trained and predict the sentiment of the test reviews.","12051271":"Training Accuracy : 98.45%\n\nTesting Accuracy : 76.03%","27e4517b":"### Finding the VADER Polarity","6f9f09e4":"## Review Analysis on the basis of Sentiment Polarity","0eef2527":"Training Accuracy : 96.08%\n\nTesting Accuracy : 91.59%","76150a02":"### Pre-processing 'Year_Month' column","56683834":"### Pre-processing the 'Reviewer_Location' column","f2a071f2":"### Pre-processing of Reviews","1c9c2a98":"## Conclusion\n\nWe can conclude that VADER Polarity is a better metric than Sentiment Polarity to analyze reviews.\n\nRandom Forest Classifier works better than Decision Tree Classifier in both Sentiment Polarity and VADER Polarity. Even though the training accuracy for Decision Tree Classifier (when trained using VADER Polarity) is higher (99.41%), Random Forest Classifier works better on testing data (accuracy of 91.59%).","d81126a8":"## Using Random Forest Classifier","afc4f37b":"Training Accuracy : 96.30%\n\nTesting Accuracy : 80.13%","b87efbf0":"#### Data Visualization","f8a2fbdb":"### Label Encoding on Reviewer_Location,Year,Polarity Rating, Vader Rating","66a52987":"## Lexicon based approach of Sentiment Analysis\n\n### Getting the VADER score","986830b2":"## Using Decision Tree classifier","04d8158d":"## Review Analysis on the basis of VADER Polarity"}}