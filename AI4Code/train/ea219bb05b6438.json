{"cell_type":{"67fa3425":"code","85f12bbd":"code","5b6a2657":"code","02444191":"code","9cedfcc3":"code","f5febaa2":"code","4d466ebb":"code","a62507b6":"code","73d6fe73":"code","07b914c9":"code","a1241fc0":"code","a8342132":"code","e34cb0c4":"code","dc68d8ce":"code","f1e4b52a":"code","194bdcbb":"code","9c12051b":"code","5d618fee":"code","d5736517":"code","db4c7fe5":"code","016a1b38":"code","57699e0e":"code","05eb2d84":"code","eaff75e7":"code","cd57ad4a":"code","77abd1ec":"code","c8374751":"code","5dce9663":"code","e750ea1b":"code","febd6c10":"code","ee2a2778":"code","122f0c53":"code","66ba844a":"code","89675dee":"code","377f39e5":"markdown","8b0b59a1":"markdown","a3443f5a":"markdown","013bd30e":"markdown"},"source":{"67fa3425":"# For data loading and manipulation\nimport pandas as pd\nimport numpy as np\n\n# For Visualization\/EDA\nimport seaborn as sns\nsns.set(style=\"white\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# For data science and machine learning techniques\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier","85f12bbd":"# Read the train and test datasets from Kaggle to create two DataFrames using Pandas\ntrain_loc = \"..\/input\/titanic\/train.csv\"\ntest_loc = \"..\/input\/titanic\/test.csv\"\ntrain = pd.read_csv(train_loc)\ntest = pd.read_csv(test_loc)\n# Print the first few records of the train and test datasets\nprint(train.head())\nprint(test.head())","5b6a2657":"train.describe()","02444191":"train.info()","9cedfcc3":"train.loc[(train.Age.isnull())&(train.Sex=='female'),'Age'] = train[\"Age\"][train[\"Sex\"] == 'female'].mean()\ntrain.loc[(train.Age.isnull())&(train.Sex=='male'),'Age'] = train[\"Age\"][train[\"Sex\"] == 'male'].mean()\nprint(\"Is any age null after cleaning = \",train.Age.isnull().any())","f5febaa2":"# Check the Cabin distribution\n#train.groupby('Cabin').size()\nlen(train.Cabin.unique())","4d466ebb":"# How much information does it provide?\nprint(100*train.Cabin.isnull().sum()\/train.shape[0],\"% of cabin values are missing hence this attribute wont contribute much and we can remove it\")","a62507b6":"# Dropping Cabin from the analysis\ntrain.drop(['Cabin'],axis=1,inplace=True)","73d6fe73":"# Check the Embarked distribution\ntrain.groupby('Embarked').size()","07b914c9":"# only 2 rows have null Embarked\ntrain.loc[train.Embarked.isnull()]","a1241fc0":"train.loc[train.Embarked.isnull(),\"Embarked\"] = \"S\"","a8342132":"# check the info again to see if all attributes are cleaned\ntrain.info()","e34cb0c4":"# Overall distribution of the survived passengers\ntrain[\"Survived\"].value_counts(normalize = True)","dc68d8ce":"# 62% of the passengers could not survive\nsns.countplot('Survived',data=train).set_title('Survival Count')","f1e4b52a":"# Survival Rates by Gender\npd.crosstab(train.Survived, train.Sex, normalize='index')","194bdcbb":"# Plot the survival count by Gender\nsns.countplot('Sex',hue='Survived',data=train).set_title('Survival by Gender')","9c12051b":"# Next we will explore the Passenger Class variable\npd.crosstab(train.Survived, train.Pclass, normalize='index')","5d618fee":"sns.countplot('Pclass',hue='Survived',data=train).set_title('Survival by Passenger Class')","d5736517":"# Create a boxplot for the Fare distribution of each class\nsns.boxplot(\"Pclass\", \"Fare\", data=train, palette=[\"lightblue\", \"lightpink\", \"lightgreen\"]).set_title('Fare Distribution by Pclass')","db4c7fe5":"# Let's look into the Age factor\n# Like fare, it is a continuous variable so let's plot a histogram to check the distribution\nsns.FacetGrid(train, col='Survived').map(plt.hist, 'Age', bins=30)","016a1b38":"# Next, we explore the Embarked variable\npd.crosstab(train.Survived, train.Embarked, normalize='index')","57699e0e":"# From the 12 columns of the training dataset, we have already dropped Cabin and Ticket\n# We do not need Name and PassengerID, so let's drop them\ntrain.drop(['Name','PassengerId'],axis=1,inplace=True)\ntrain.info()","05eb2d84":"# Convert the male and female groups to integer form\ntrain[\"Gender\"] = 0\ntrain.loc[train['Sex']=='male','Gender']=0\ntrain.loc[train['Sex']=='female','Gender']=1\n\n# Convert the Embarked classes to integer form\ntrain[\"Port\"] = 0\ntrain.loc[train['Embarked']=='S','Port']=0\ntrain.loc[train['Embarked']=='C','Port']=1\ntrain.loc[train['Embarked']=='Q','Port']=2\n\n# Create buckets for Age\ntrain[\"Age_cat\"] = 0\ntrain.loc[train['Age']<=16,'Age_cat']=0\ntrain.loc[(train['Age']>16)&(train['Age']<=30),'Age_cat']=1\ntrain.loc[(train['Age']>30)&(train['Age']<=40),'Age_cat']=2\ntrain.loc[(train['Age']>40)&(train['Age']<=50),'Age_cat']=3\ntrain.loc[train['Age']>50,'Age_cat']=4\n\n# Create buckets for Fare\ntrain[\"Fare_cat\"] = 0\ntrain.loc[train['Fare']<=8,'Fare_cat']=0\ntrain.loc[(train['Fare']>8)&(train['Fare']<=15),'Fare_cat']=1\ntrain.loc[(train['Fare']>15)&(train['Fare']<=31),'Fare_cat']=2\ntrain.loc[train['Fare']>31,'Fare_cat']=3\n\n# Create a new variable family size and buckets for the same as travel_company\ntrain[\"family_size\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntrain[\"travel_company\"] = 0\ntrain.loc[train['family_size']<=1,'travel_company']=0\ntrain.loc[(train['family_size']>1)&(train['family_size']<=4),'travel_company']=1\ntrain.loc[train['family_size']>4,'travel_company']=2","eaff75e7":"# Remove the unneccessary vaiables and make sure the new variables got added\n#train.describe()\n#train.info()\ntrain.drop(['Sex','Age','SibSp','Parch','Fare','Embarked','family_size'],axis=1,inplace=True)\n#train.describe()\ntrain.info()","cd57ad4a":"# Check the correlation among the rest of the available variables\nsns.heatmap(train.corr()).set_title('Correlation Heat map for candidate variables')","77abd1ec":"# Separating the response (y) and explanatory (x) variables\n#X = train[[\"Pclass\", \"Gender\", \"Port\", \"Age_cat\", \"Fare_cat\", \"travel_company\"]].values\n# Removing the Fare category increases the model accuracy, so decided to exclude that from the final models\nX = train[[\"Pclass\", \"Gender\", \"Port\", \"Age_cat\", \"travel_company\"]].values\ny = train[\"Survived\"].values","c8374751":"# Splitting the dataset into test and training with 80% for training the model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=25)","5dce9663":"# Building the Logistic Regression model using the training dataset\nLogReg = LogisticRegression()\nLogReg.fit(X_train, y_train)\n\n# Testing the model with the test dataset (do not confuse with the actual Test dataset)\ny_pred = LogReg.predict(X_test)\nprint('The model accuracy is', metrics.accuracy_score(y_pred, y_test))\nprint('The R-square value is', metrics.r2_score(y_pred, y_test)) \n#Although R-square doesn't provide a lot of info in binary models","e750ea1b":"# Building the Decision tree model using the training dataset\nDecTree = tree.DecisionTreeClassifier()\nDecTree.fit(X_train, y_train)\n\n# Testing the model with the test dataset (do not confuse with the actual Test dataset)\ny_pred = DecTree.predict(X_test)\nprint('The model accuracy is', metrics.accuracy_score(y_pred, y_test))\nprint('The R-square value is', metrics.r2_score(y_pred, y_test))\n#Although R-square doesn't provide a lot of info in binary models","febd6c10":"# Building the Random forest model using the training dataset\nRandFor = RandomForestClassifier(max_depth = 6, min_samples_split=2, n_estimators = 100, random_state = 1)\nRandFor.fit(X_train, y_train)\n\n# Testing the model with the test dataset (do not confuse with the actual Test dataset)\ny_pred = RandFor.predict(X_test)\nprint('The model accuracy is', metrics.accuracy_score(y_pred, y_test))\nprint('The R-square value is', metrics.r2_score(y_pred, y_test))\n#Although R-square doesn't provide a lot of info in binary models","ee2a2778":"# Also compare the feature importance of the Decision tree and Random forest models\nprint(DecTree.feature_importances_)\nprint(RandFor.feature_importances_)","122f0c53":"# Pre-process and transform the data same as the training dataset\ntest.info()","66ba844a":"# Impute missing values of age by the respective average of the genders\ntest.loc[(test.Age.isnull())&(test.Sex=='female'),'Age'] = test[\"Age\"][test[\"Sex\"] == 'female'].mean()\ntest.loc[(test.Age.isnull())&(test.Sex=='male'),'Age'] = test[\"Age\"][test[\"Sex\"] == 'male'].mean()\ntest.Age.isnull().any()\n\n# Fare not included in the final model, but in case we want to revert, need to treat missingness\n# Impute the missing value of fare by the pclass median\ntest.loc[(test.Fare.isnull())&(test.Pclass==1),'Fare'] = test[\"Fare\"][test[\"Pclass\"] == 1].median()\ntest.loc[(test.Fare.isnull())&(test.Pclass==2),'Fare'] = test[\"Fare\"][test[\"Pclass\"] == 2].median()\ntest.loc[(test.Fare.isnull())&(test.Pclass==3),'Fare'] = test[\"Fare\"][test[\"Pclass\"] == 3].median()\ntest.Fare.isnull().any()\n\n# Since Cabin will be dropped so, not required to fill the missing values\n\n# Convert the male and female groups to integer form\ntest[\"Gender\"] = 0\ntest.loc[test['Sex']=='male','Gender']=0\ntest.loc[test['Sex']=='female','Gender']=1\n\n# Convert the Embarked classes to integer form\ntest[\"Port\"] = 0\ntest.loc[test['Embarked']=='S','Port']=0\ntest.loc[test['Embarked']=='C','Port']=1\ntest.loc[test['Embarked']=='Q','Port']=2\n\n# Create buckets for Age\ntest[\"Age_cat\"] = 0\ntest.loc[test['Age']<=16,'Age_cat']=0\ntest.loc[(test['Age']>16)&(test['Age']<=30),'Age_cat']=1\ntest.loc[(test['Age']>30)&(test['Age']<=40),'Age_cat']=2\ntest.loc[(test['Age']>40)&(test['Age']<=50),'Age_cat']=3\ntest.loc[test['Age']>50,'Age_cat']=4\n\n# Create  buckets for family size\/travel company\ntest[\"family_size\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\ntest[\"travel_company\"] = 0\ntest.loc[test['family_size']>=1,'travel_company']=0\ntest.loc[(test['family_size']>1)&(test['family_size']<=4),'travel_company']=1\ntest.loc[test['family_size']>4,'travel_company']=2\n\ntest.describe()","89675dee":"# Extract the features from the test set and predict using the final model\ntest_features = test[[\"Pclass\", \"Gender\", \"Port\", \"Age_cat\", \"travel_company\"]].values\ntest_Survived = RandFor.predict(test_features)\n\n# Create a data frame with two columns: PassengerId & Survived for the final submission\nTitanic_Prediction = pd.DataFrame({'PassengerId' : test.loc[:,'PassengerId'],\n                                   'Survived': test_Survived})\n\n# Checking for the final dimensions : 418 x 2\nprint(Titanic_Prediction.shape)\n\n# Export to a csv file\nTitanic_Prediction.to_csv(\"Titanic_Prediction.csv\", index=False)","377f39e5":"Fill all null ages with mean age","8b0b59a1":"Next we also wanted to check the relationship between the passenger class and fare to see if fare provides obvious\/redundant information. From the boxplot below, we found that the fares are way too high for the upper class (obvious information) but there are still outliers in each class. But basically, the more you pay, the higher class you will get and eventually have better chances to be rescued","a3443f5a":"# **Titanic Survival Analysis**\n\nThis notebook walks you through all the steps required to analyse Titanic dataset.","013bd30e":"\"S\" is the most frequent value, therefore we can fill the missing values with the mode of embarked i.e. \"S\""}}