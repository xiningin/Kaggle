{"cell_type":{"b9433c6e":"code","631812f7":"code","7ebddebd":"code","ff63b8e8":"code","49558776":"code","57e3ed2f":"code","8229cdae":"code","0341ad80":"code","c76a2f43":"code","45b7c978":"code","48d8f2a1":"code","5c50c224":"code","d4049d8b":"code","3711bfe2":"code","5dac29f8":"code","5cb62c60":"code","0a6b94dd":"code","eb40b507":"code","5b1d4224":"code","aeb9b1df":"code","e0efd949":"code","918ae72a":"code","314c010d":"code","c8fad466":"code","a5e82efe":"code","e0417cad":"code","840459f6":"code","b5de13c1":"code","f88e766a":"code","02f6a4b8":"code","b7c1fcf7":"code","879ef93d":"code","c79c9961":"code","1a9609a3":"code","0ba428e1":"code","15d8581e":"code","a8ca404d":"code","d549484a":"code","a0d0fa6a":"code","0f1bb255":"code","1a6a15e4":"code","0f73f67d":"code","2f643729":"code","991ccd8f":"code","8e48394b":"code","768fa1c4":"code","369cff9b":"code","c35e8950":"code","9ddcba9b":"code","0a7acbe0":"code","33473180":"code","b635be64":"code","efed826d":"code","e91847c3":"code","d9475ae8":"code","37de075b":"code","439a31fa":"code","550c7a36":"code","34bd1087":"markdown","6e5a5a6f":"markdown","0b15c706":"markdown","ff3a7bb7":"markdown","97b08872":"markdown","135e6903":"markdown","4da2d7b8":"markdown","83ac68a7":"markdown","ae5120e8":"markdown","edefe39c":"markdown"},"source":{"b9433c6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","631812f7":"import pandas as pd\nimport numpy as np\nfrom catboost import Pool, CatBoostClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","7ebddebd":"import os\ntweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head()","ff63b8e8":"tweet.target.value_counts(normalize=True)","49558776":"tweet.location.value_counts(normalize=True)","57e3ed2f":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","8229cdae":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \",\n\"won't\": \"would not\",\n'dis': 'this',\n'bak': 'back',\n'brng': 'bring'}","0341ad80":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key, value)\n        return x\n    else:\n        return x","c76a2f43":"tweet['text'] = tweet['text'].apply(lambda x: cont_to_exp(x))\n","45b7c978":"def remove_emails(x):\n     return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n\n\ndef remove_urls(x):\n    return re.sub(r'(http|https|ftp|ssh):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '' , x)\n\ndef remove_rt(x):\n    return re.sub(r'\\brt\\b', '', x).strip()\n\ndef remove_special_chars(x):\n    x = re.sub(r'[^\\w ]+', \"\", x)\n    x = ' '.join(x.split())\n    return x\n\n\ndef remove_accented_chars(x):\n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return x\n\ndef remove_stopwords(x):\n    return ' '.join([t for t in x.split() if t not in stopwords])","48d8f2a1":"tweet['text'] = tweet['text'].apply(lambda x: remove_emails(x))\ntweet['text'] = tweet['text'].apply(lambda x: remove_urls(x))\ntweet['text'] = tweet['text'].apply(lambda x: remove_rt(x))\ntweet['text'] = tweet['text'].apply(lambda x: remove_special_chars(x))\n","5c50c224":"\ncat_features = ['keyword','location']\ntext_features = ['text']\n","d4049d8b":"tweet.isna().sum()","3711bfe2":"tweet['keyword'].value_counts()","5dac29f8":"tweet['keyword'] = np.where(tweet['keyword'].isna()==True,'UNK',tweet['keyword'])\ntweet['location'] = np.where(tweet['location'].isna()==True,'UNK',tweet['location'])","5cb62c60":"col = ['target','id']\nX = tweet.drop(col,axis=1)\ny = tweet['target']","0a6b94dd":"X_train, X_test, y_train, y_test = train_test_split(X,y.values,test_size=0.15)","eb40b507":"train_pool = Pool(\n    X_train, \n    y_train, \n    cat_features=cat_features, \n    text_features=text_features, \n    feature_names=list(X_train)\n)\nvalid_pool = Pool(\n    X_test, \n    y_test,\n    cat_features=cat_features, \n    text_features=text_features, \n    feature_names=list(X_train)\n)\n\ncatboost_params = {\n    'iterations': 3000,\n    'learning_rate': 0.01,\n    'eval_metric': 'Accuracy',\n    'task_type': 'GPU',\n    'early_stopping_rounds': 2000,\n    'use_best_model': True,\n    'verbose': 500\n}\nclas_weight = [5,4]\nmodel = CatBoostClassifier(**catboost_params,class_weights=clas_weight)\nmodel.fit(train_pool, eval_set=valid_pool)\n\n","5b1d4224":"from sklearn.metrics import classification_report,accuracy_score\npred = model.predict(X_test)\nprint(classification_report(y_test,pred))\nprint(accuracy_score(y_test,pred))","aeb9b1df":"pred","e0efd949":"test1 = test.drop('id',axis=1)","918ae72a":"test1.isna().sum()","314c010d":"test1['keyword'] = np.where(test1['keyword'].isna()==True,'UNK',test1['keyword'])\ntest1['location'] = np.where(test1['location'].isna()==True,'UNK',test1['location'])","c8fad466":"pred1 = model.predict(test1)\nsub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","a5e82efe":"pred1.shape","e0417cad":"sub.columns","840459f6":"sub.target.value_counts()","b5de13c1":"\nsub['target'] = pred1\n\nsub","f88e766a":"sub.to_csv('submission.csv', index=False)","02f6a4b8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","b7c1fcf7":"# GLOVE_EMB = '\/kaggle\/input\/glove6b\/glove.6B.300d.txt'\n# EMBEDDING_DIM = 300\n# LR = 1e-3\n# BATCH_SIZE = 512","879ef93d":"# !wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","c79c9961":"# !unzip glove.6B.zip","1a9609a3":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","0ba428e1":"X.columns","15d8581e":"X['text'].apply(lambda x:len(str(x).split())).max()","a8ca404d":"def roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","d549484a":"xtrain, xvalid, ytrain, yvalid = train_test_split(X.text.values, y.values, \n                                                  stratify=y.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","a0d0fa6a":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 31\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n#zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","0f1bb255":"\nwith strategy.scope():\n    # A simpleRNN without any pretrained embeddings and one dense layer\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    model.add(SimpleRNN(100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel.summary()","1a6a15e4":"model.fit(xtrain_pad, ytrain, epochs=3,validation_data=(xvalid_pad, yvalid), batch_size=64) #Multiplying by Strategy to run on TPU's","0f73f67d":"scores = model.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))","2f643729":"from sklearn.metrics import classification_report,accuracy_score\npred = model.predict(xvalid_pad)\n# print(classification_report(yvalid,pred))\n# print(accuracy_score(yvalid,pred))","991ccd8f":"pred","8e48394b":"from sklearn.metrics import classification_report,accuracy_score\npred = model.predict(X_test)\nprint(classification_report(y_test,pred))\nprint(accuracy_score(y_test,pred))","768fa1c4":"### Word Embedding","369cff9b":"%%time\nwith strategy.scope():\n    \n    model1 = Sequential()\n    model1.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    model1.add(LSTM(100))\n    model1.add(Dense(1, activation='sigmoid'))\n    model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel1.summary()","c35e8950":"model1.fit(xtrain_pad, ytrain, epochs=3, batch_size=128,validation_data=(xvalid_pad, yvalid)) #Multiplying by Strategy to run on TPU's","9ddcba9b":"scores = model1.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))","0a7acbe0":"yvalid.shape,predicted.shape","33473180":"from sklearn.metrics import classification_report,accuracy_score\npred = model1.predict_classes(xvalid_pad)\nprint(classification_report(yvalid,pred))\nprint(accuracy_score(yvalid,pred))","b635be64":"%%time\nwith strategy.scope():\n    \n    model1 = Sequential()\n    model1.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    \n    model1.add(SpatialDropout1D(0.3))\n    model1.add(GRU(300))\n    model1.add(Dense(1, activation='sigmoid'))\n    model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel1.summary()","efed826d":"model1.fit(xtrain_pad, ytrain, epochs=3, batch_size=128,validation_data=(xvalid_pad, yvalid)) #Multiplying by Strategy to run on TPU's","e91847c3":"from sklearn.metrics import classification_report,accuracy_score\npred = model1.predict_classes(xvalid_pad)\nprint(classification_report(yvalid,pred))\nprint(accuracy_score(yvalid,pred))","d9475ae8":"### Bidirectional RNN","37de075b":"%%time\nwith strategy.scope():\n    \n    model1 = Sequential()\n    model1.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    model1.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n    model1.add(Dense(1, activation='sigmoid'))\n    model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel1.summary()","439a31fa":"model1.fit(xtrain_pad, ytrain, epochs=4, batch_size=128,validation_data=(xvalid_pad, yvalid)) #Multiplying by Strategy to run on TPU's","550c7a36":"from sklearn.metrics import classification_report,accuracy_score\npred = model1.predict_classes(xvalid_pad)\nprint(classification_report(yvalid,pred))\nprint(accuracy_score(yvalid,pred))","34bd1087":"contractions","6e5a5a6f":"### GRU","0b15c706":"### Text Classification with RNN and LSTM ","ff3a7bb7":"**Graphical Representation of dependent Variables**","97b08872":"**Importing DataSet**","135e6903":"**Importing Packages**","4da2d7b8":"**Preprocessing of text data**","83ac68a7":"**Expansion**","ae5120e8":"**Building Catboost Model**","edefe39c":"*Checking Value count of event rate*"}}