{"cell_type":{"01712fb9":"code","fb1abc01":"code","2f29bbdc":"code","bee68d30":"code","7d410eca":"code","9683b99e":"code","ea1405f9":"code","cc5de92d":"code","b2183248":"code","f9925fac":"code","2c589870":"code","28eb5336":"code","b0b9b8f6":"code","5312d6a9":"code","c5936a54":"code","ecd54703":"code","165dc1e4":"code","b869ea90":"code","28ae8b93":"code","e34def64":"code","8c9529d9":"code","36ebbd2e":"code","bace44c4":"code","b15a49df":"code","9371da3c":"code","6603429e":"code","590d1c55":"code","90c56063":"code","52e6a751":"code","a22090c5":"code","baa9a953":"markdown","8a80c39f":"markdown","8a0bd1e9":"markdown","49e3ad5c":"markdown","0c72938a":"markdown","a23ac02f":"markdown","fcba411c":"markdown","f4a62ddf":"markdown","d9ef99b4":"markdown","c16d9dbc":"markdown","0de24c7f":"markdown","5a394d11":"markdown","4a6713d1":"markdown","773a42da":"markdown","eab429d1":"markdown","1afbeed4":"markdown","fe4ea766":"markdown","1afc3c35":"markdown","cdad0a06":"markdown","29c6f90d":"markdown","67636b39":"markdown","a9fec8cd":"markdown","dd8ca239":"markdown","c497b0c3":"markdown","3cb898e5":"markdown","baf81ced":"markdown","3723a901":"markdown","b9fcf316":"markdown","c1b9da7c":"markdown","7f3f6785":"markdown","d69c9c73":"markdown","060d0055":"markdown","b88bc6fe":"markdown"},"source":{"01712fb9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fb1abc01":"# Read input files\n\n\ndataset = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\nfeatures = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip\",sep=',', header=0,\n                       names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4',\n                              'MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\nstores = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\n\n\ntest = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\")\n\n\n#merge the datasets\ndataset_all = dataset.merge(stores, how='left').merge(features, how='left')\n\n#convert to datetime \n\nfeatures['Date'] =pd.to_datetime(features['Date'], format=\"%Y-%m-%d\")\ntest['Date'] = pd.to_datetime(test['Date'], format=\"%Y-%m-%d\")\ndataset_all['Date'] = pd.to_datetime(dataset_all['Date'], format=\"%Y-%m-%d\")\n\n\npd.DataFrame(dataset_all.dtypes, columns=['Type'])\n","2f29bbdc":"#plot distribution for weekly sales\nsns.distplot(dataset_all['weeklySales'])\nplt.ylabel('Distribution');\n\n\n#Quick statistical look\ndataset_all.describe()\n\n","bee68d30":"#correlation matrix\ncorrmat = dataset_all.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.8, square=True, annot=True);","7d410eca":"colours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(dataset_all.isnull(), cmap=sns.color_palette(colours))","9683b99e":"stores_total = stores.groupby(['Type']).sum()\nplot = stores_total.plot.pie(subplots=True, figsize=(8, 6),autopct='%1.1f%%')\n\n","ea1405f9":"# Sales in Holidays\nholi_y = dataset_all['weeklySales'].loc[dataset_all['isHoliday']== True] \n#Sales when it is not holidays\nholi_n = dataset_all['weeklySales'].loc[dataset_all['isHoliday']== False] \nsns.barplot(x='isHoliday', y='weeklySales', data=dataset_all);\n","cc5de92d":"f, ax = plt.subplots(figsize=(10, 6))\nax = sns.violinplot(x=\"isHoliday\", y=\"weeklySales\",\n                    data=dataset_all, palette=\"muted\");","b2183248":"\nsales_avg = dataset_all.groupby(['Store']).mean()\n\nyear2010 = ((dataset_all[dataset_all['Date'].dt.year == 2010]))\nyear2011 = ((dataset_all[dataset_all['Date'].dt.year == 2011]))\nyear2012 = ((dataset_all[dataset_all['Date'].dt.year == 2012]))\n\n# add date as index so we can use the resample function\nyear2010.index = year2010['Date']\nyear2011.index = year2011['Date'] \nyear2012.index = year2012['Date'] \n\n# finally resample it by week \nwsales_2010 = year2010.resample('W').mean()\nwsales_2011 = year2011.resample('W').mean()\nwsales_2012 = year2012.resample('W').mean()\n\nplt.figure(figsize=(15,8))\nax = sns.lineplot(wsales_2010.index.week, wsales_2010['weeklySales'].values,markers=True)\nax2 = sns.lineplot(wsales_2011.index.week, wsales_2011['weeklySales'].values,markers=True)\nax3 = sns.lineplot(wsales_2012.index.week, wsales_2012['weeklySales'].values,markers=True)\nax3.axvspan(7, 7, color='black', alpha=1)\nax3.axvspan(36, 36,  color='black', alpha=1)\nax3.axvspan(47, 47,  color='black', alpha=1)\nax3.axvspan(52, 52,  color='black', alpha=1)\nax3.axvspan(12, 17,facecolor='gray', alpha=0.2)\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best')\nplt.ylabel('Avg Weekly Sales')\nplt.xlabel('Week')\nplt.show()\n\n","f9925fac":"dataset_dropped = dataset_all.drop(columns=['Date', 'Store','Dept','isHoliday','Type'])\n\n#We know that Markdown has NaN, so we fill it with 0\ndataset_dropped = dataset_dropped.fillna(0)\n#dataset_dropped.head()\n\ndef create_plot(feature,width, height):\n    # Set up the matplotlib figure\n    g = sns.jointplot(x=feature, y=\"weeklySales\", data=dataset_dropped)\n    corre = np.corrcoef(dataset_dropped[feature], dataset_dropped[\"weeklySales\"])\n    g.fig.suptitle('Corr: '+str(corre[1,0]))\n\n\ncreate_plot('Temperature',7,7) \ncreate_plot('Size',7,7)    \ncreate_plot('Fuel_Price',7,7)    \ncreate_plot('Unemployment',7,7)    \ncreate_plot('CPI',7,7)    \ncreate_plot('MarkDown1',7,7)    \ncreate_plot('MarkDown2',7,7)   \ncreate_plot('MarkDown3',7,7)   \ncreate_plot('MarkDown4',7,7)   \n","2c589870":"#Take the average per store and department\nstores_avg = dataset_all.groupby(['Store']).mean()\ndept_avg = dataset_all.groupby(['Dept']).mean()\n\nplt.figure(figsize=(20,6))\nplt.xlim([1, len(stores_avg.index)+1])\nplt.xticks(range(len(stores_avg.index)+1))\nplt.bar(stores_avg.index,stores_avg['weeklySales'])\nplt.xlabel('Stores')\nplt.ylabel('Average weekly revenue')\nplt.show()","28eb5336":"plt.figure(figsize=(20,6))\nplt.xlim([0, len(dept_avg.index)+1])\nplt.xticks(dept_avg.index)\nplt.bar(dept_avg.index,dept_avg['weeklySales'])\nplt.xlabel('Departments')\nplt.ylabel('Average weekly revenue')\nplt.show()\n","b0b9b8f6":"dataset_model = dataset_all \n\ndataset_model['year']=dataset_model['Date'].dt.year\ndataset_model['month']=dataset_model['Date'].dt.month\ndataset_model['week']=dataset_model['Date'].dt.week\ndataset_model = dataset_model.drop(['Date'],axis=1)\n\ndataset_model.head()","5312d6a9":"dataset_model = dataset_model.fillna(0)\ndataset_model.head()","c5936a54":"dataset_model.loc[(dataset_model.year==2010) & (dataset_model.week==13), 'isHoliday'] = True\ndataset_model.loc[(dataset_model.year==2011) & (dataset_model.week==16), 'isHoliday'] = True\ndataset_model.loc[(dataset_model.year==2012) & (dataset_model.week==14), 'isHoliday'] = True","ecd54703":"dataset_model = pd.get_dummies(dataset_model)","165dc1e4":"#it's easily done by multiplying the entire dataset by 1\ndataset_model = dataset_model*1\ndataset_model.head()","b869ea90":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nX = dataset_model.drop('weeklySales',axis=1)\ny = dataset_model['weeklySales']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=34)\n\nrf = RandomForestRegressor(n_jobs=-1) #The default value of n_estimators changed from 10 to 100 in 0.22.\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\nprint(r2_score(y_test,y_pred))\n\n\n(pd.DataFrame([X.columns,rf.feature_importances_],columns=['Store', 'Dept', 'isHoliday', 'Temperature', 'Fuel_Price', 'CPI',\n       'Unemployment', 'Size', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'year', 'month','week','Type_A','Type_B','Type_C']).T).plot.bar()","28ae8b93":"dataset_model_final = dataset_model[['Store','Dept','weeklySales','Temperature','week','isHoliday','Size']]\ndataset_model_final.head()","e34def64":"def WMAE(dataset, real, predicted):\n    weights = dataset.isHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)","8c9529d9":"from sklearn.tree import DecisionTreeRegressor\n\nX = dataset_model_final.drop('weeklySales',axis=1)\ny = dataset_model_final['weeklySales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\n\ndef model_hypertuning(max_d,max_estimators):\n    '''hypertune random forest model'''\n\n    print('Random forest model started')\n    error_train = []\n    max_depth = np.arange(10, max_d+10, 10).tolist()\n    n_estimators = np.arange(10, max_estimators, 20).tolist()\n    hyperparams = []\n    \"\"\"Calculating errors for maximum depth and number of estimators parameters.\"\"\"\n    for i in max_depth: \n        print(\"Max Depth: %f\" % i)\n        print()\n        for j in n_estimators: \n            print(\"Estimators: %f\" % j)\n            rf = RandomForestRegressor(max_depth=i, n_estimators=j, n_jobs=-1) \n            rf.fit(X_train, y_train)  \n            y_pred_train_rf = rf.predict(X_train) \n            error_train.append(WMAE(X_train,y_train, y_pred_train_rf)) \n            hyperparams.append({'Maximum Depth':i, 'No. of Estimators':j})\n            \n    return error_train,hyperparams","36ebbd2e":"tuning_model_rf = model_hypertuning(40,140)","bace44c4":"hyperparams = pd.DataFrame(tuning_model_rf[1])\nhyperparams['Train Error']=tuning_model_rf[0]\nhyperparams_pivot = pd.pivot_table(hyperparams,'Train Error','Maximum Depth','No. of Estimators')\n\nplt.figure(figsize=(10,6))\np1 = sns.heatmap(hyperparams_pivot,annot=True, annot_kws={\"size\": 8}, fmt='g') \n\nhyperparams_pivot.head()","b15a49df":"model_rf = RandomForestRegressor(max_depth= 40, n_estimators=110,n_jobs=-1).fit(X_train, y_train) # Fit the model with best hyper parameter values.\ny_pred = model_rf.predict(X_test) # Predict the test data.\nprint('Weighted Mean Absolute Error (WMAE) for Random Forest Regression:', WMAE(X_test,y_test, y_pred)) # Get WMAE score.","9371da3c":"# regression plot using seaborn\nfig = plt.figure(figsize=(10, 7))\nsns.regplot(x=y_test, y=y_pred)","6603429e":"#Re-read the data \ntest = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\", names=['Store','Dept','Date','isHoliday'],sep=',', header=0)\ntrain = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\nfeatures = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip\",sep=',', header=0,\n                       names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4',\n                              'MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\nstores = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\ntest.head()","590d1c55":"# Joining test data with store and features.\ntest_final = test.merge(features, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')\n\n# Converting Date to datetime\ntest_final['Date'] = pd.to_datetime(test_final['Date'])\n\n# Extract date features\ntest_final['year']=test_final['Date'].dt.year\ntest_final['month']=test_final['Date'].dt.month\ntest_final['day']=test_final['Date'].dt.day\ntest_final['week']=test_final['Date'].dt.week\n\n#One hot encoding \ntest_final = pd.get_dummies(test_final)\n\n#fill nans with 0\ntest_final = test_final.fillna(0)\n\n#drop date\ntest_final = test_final.drop(['Date','day'], axis=1)\n\n#add easter holiday\ntest_final.loc[(test_final.year==2013) & (test_final.week==13), 'isHoliday'] = True\n\n#convert binary to numeric\ntest_final = test_final*1\n\ndef check_holidays(year,month):\n    '''\n    For convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\n    Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13 -> ok\n    Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13 -> ok\n    Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13 -> ok\n    Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n    '''\n    return test_final[(test_final.isHoliday == 1)  & (test_final.year == year) & (test_final.month == month)]\n\ncheck_holidays(2013,2) ","90c56063":"test_final = test_final[['Store','Dept','Temperature','week','isHoliday','Size']]\ntest_final.head()","52e6a751":"y_pred_final = model_rf.predict(test_final) # Predict the test data.","a22090c5":"test_file = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\")\n\n# Weekly Sales Prediction is the final file to be submitted in Kaggle.\nsubmission = pd.DataFrame({\n        \"Id\": test_file.Store.astype(str)+'_'+test_file.Dept.astype(str)+'_'+test_file.Date.astype(str), \n        \"Weekly_Sales\": y_pred_final # This is predicetd Weekly Sales on final test data using Random Forest regression.\n    })\n\nsubmission.to_csv('Prediction_final.csv', index=False) # Final submission.","baa9a953":"Here we analyze the weekly sale per year and check how the holidays affect the sales. The black lines represent the exact weeks for the super bowl, labor day, thanksgiving, and christmas, respectively. We notice that the peak of sales fall on the same week of the holidays, except for christmas when the peak is the week before. \n\nAnother important remark we get from the data is another important holiday which is Easter (gray shaded area). The week of this holiday is not aligned with the other ones. Although the Easter holiday brings more revenue to the store than Superbowl, it is not included in the isHoliday data. We will make a small adjustment later on.","8a80c39f":"We also check the weekly sales per store to see if there's any important pattern in this feature","8a0bd1e9":"The metric which will evaluate the models is the following:\n![Screenshot%202020-04-04%20at%2011.30.49.png](attachment:Screenshot%202020-04-04%20at%2011.30.49.png)\nWe implement it in the function below","49e3ad5c":"Taking into consideration the previous analysis of the features and the current, we end up with the following features","0c72938a":"We can notice that the holidays have some big outliers.","a23ac02f":"Heatmap with correlation matrix between features","fcba411c":"# **** Data Analysis\/Exploration****","f4a62ddf":"Here I will basically import the datasets and give a quick check on it ","d9ef99b4":"Now we hypertune the model ","c16d9dbc":" After tuning our model, we re-run it with the best configurations. We see that 110 estimators and 40 is the ideal.","0de24c7f":"Type is still a categorical variable, so we will use One-hot encode to convert it to numeric.","5a394d11":"Final prediction","4a6713d1":"We check in detail some of the features to see if there is any correlation. Unfortunately, there's little of no correlation in almost all of them, except for a moderate correlation with size.","773a42da":"Finally, we only have now holidays as a boolean. We will convert it to binary by multiplying the entire dataframe by 1.","eab429d1":"Finally, we take a look at weekly sales per department","1afbeed4":"# **Machine Learning Modelling**","fe4ea766":"The stores are sperated by 3 types: A, B, and C. We take a look at how this is distributed. However, it is not clear what exactly A, B, and C mean.","1afc3c35":"We split again the dataset into test and train set and create a function in order to hypertune the model","cdad0a06":"Once we have hypertuned the model, we check what is the ideal configuration.","29c6f90d":"# **Feature Selection**","67636b39":"Kaggle says we need to predict with the test data, so we will re-run the model with the test data, but first we need to organize the test dataset. We will apply the same data cleaning as before.","a9fec8cd":"Our data has Nans, so we will fill them with 0.","dd8ca239":"**Author: Michell Germano** \n\nCheck my linkedin: [https:\/\/www.linkedin.com\/in\/michell-fontenelle-germano-1a654159\/](http:\/\/)\n","c497b0c3":"The model we choose for this case is the Random Forest model. Random forests is an ensemble learning method for classification or regression. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \n\nWe chose this model because of its ensemble configuration and relatively easy interpretation\/implementation, makes this model a powerful tool. Although the **ideal** situation would be testing multiple models and see which one performs better, we are limited by computational power on kaggle, so to make it faster, we will try only this model.\n\n","3cb898e5":"Now that we have eveything numeric, we will convert select only the important features we selected initially ","baf81ced":"Here we take a look and check if we have data missing; yellow is missing and blue is not","3723a901":"WeeklySales is one of the most important variables, so we will take a quick look at its distrubtion","b9fcf316":"As we saw in the previous section, our data has NANs and some categorical variables, so we will pre-process this data so we can use later in the model","c1b9da7c":"# **Data Manipulation\/Cleaning**","7f3f6785":"# **Submission**","d69c9c73":"Now we add the Easter holiday","060d0055":"We check what is the difference of weekly sales when it is holiday and when it is not. The revenue is bigger when we have holidays than otherwise.","b88bc6fe":"We ended up with 20 features after our data cleaning process. Here will try to understand what the model (random forest) sees as more important. This, plus the knowledge we acquired during the initial data analysis will guide us in the decision of the most important features.\n\nIn order to achieve it, we will run the random forest model with our entire dataset and check what are the most important features for the model."}}