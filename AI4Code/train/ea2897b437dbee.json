{"cell_type":{"214dfe7c":"code","3a0596da":"code","e313f32a":"code","360e869c":"code","0e24ee66":"code","691a0233":"code","8ee1ff0f":"code","3ed09ca9":"code","c34f8ecc":"code","7b99ed62":"code","e55d41f2":"code","6a742304":"code","2b77151b":"code","464c245d":"markdown","7753a25f":"markdown","9623efb0":"markdown","d1b78f4a":"markdown","95fc8685":"markdown","741a93da":"markdown","fcb77c36":"markdown","b1a4b827":"markdown"},"source":{"214dfe7c":"import os\nimport numpy as np \nimport pandas as pd \nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3a0596da":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data","e313f32a":"train_data.isnull().sum()","360e869c":"test_data.isnull().sum()","0e24ee66":"y_train = train_data['Survived']\nx_train = train_data.drop(['Survived','Name' ,'Ticket', 'Cabin'], axis=1)\nids = test_data['PassengerId']\nx_test = test_data.drop(['Name' ,'Ticket', 'Cabin'], axis=1)\nx_train","691a0233":"labelEncoder1 = LabelEncoder()\nx_train['Sex'] = labelEncoder1.fit_transform(x_train['Sex'])\nx_test['Sex'] = labelEncoder1.transform(x_test['Sex'])\n\nlabelEncoder2 = LabelEncoder()\nx_train['Embarked'] = labelEncoder2.fit_transform(x_train['Embarked'])\nx_test['Embarked'] = labelEncoder2.transform(x_test['Embarked'])\nx_train","8ee1ff0f":"minmax = MinMaxScaler()\nx_train[[\"Age\", \"Fare\"]] = minmax.fit_transform(x_train[[\"Age\", \"Fare\"]])\nx_test[[\"Age\", \"Fare\"]] = minmax.fit_transform(x_test[[\"Age\", \"Fare\"]])","3ed09ca9":"imputer = IterativeImputer(random_state=42, verbose=1)\ntrain_imputed = pd.DataFrame(imputer.fit_transform(x_train), columns = ['PassengerId','Pclass', 'Sex' ,'Age', \n                                                                        'SibSp', 'Parch', 'Fare','Embarked'])\n\ntest_imputed = pd.DataFrame(imputer.transform(x_test), columns = ['PassengerId','Pclass', 'Sex' ,'Age', 'SibSp', \n                                                                  'Parch', 'Fare', 'Embarked'])","c34f8ecc":"run_gs = False\n\nif run_gs:\n    parameter_grid = {\n                 'max_depth' : [2, 4, 6],\n                 'n_estimators': [100, 50],\n                 'criterion' : ['entropy', 'gini'],\n                 'min_samples_split': [2, 4, 6],\n                 'min_samples_leaf': [1, 3, 6]\n                 }\n    forest = RandomForestClassifier()\n    cross_validation = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    grid_search = GridSearchCV(forest,\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=cross_validation,\n                               verbose=1\n                              )\n\n    grid_search.fit(train_imputed, y_train)\n    model = grid_search\n    parameters = grid_search.best_params_\n\n    print('Best score: {}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))","7b99ed62":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\n\nestimator = RandomForestClassifier(max_depth=6, criterion='gini' , min_samples_leaf=1, min_samples_split=4, \n                             n_estimators=50, bootstrap=True, random_state=42)\nselector = RFECV(estimator, step=1, cv=cv, min_features_to_select=1)\nselector = selector.fit(train_imputed, y_train)\nselector.support_","e55d41f2":"train_imputed = train_imputed.drop(['Parch', 'Embarked'], axis=1)\ntest_imputed = test_imputed.drop(['Parch', 'Embarked'], axis=1)","6a742304":"clf = RandomForestClassifier(max_depth=6, criterion='gini' , min_samples_leaf=1, min_samples_split=4, \n                             n_estimators=50, bootstrap=True, random_state=42)\nclf.fit(train_imputed, y_train)\npredictions = clf.predict(test_imputed)","2b77151b":"output = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","464c245d":"**Feature Engineering**\n\nFeature Engineering is used to select features based on statistic or wrapper methods to find the subset of features that can improve the model performance","7753a25f":"**LABEL ENCODING**\n\nLabel Encoding the categorical features using sklearn LabelEncoder class. Learn more about the class here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html","9623efb0":"**DATA IMPUTATION**\n\n**MICE IMPUTER**\n\nAn imputer is used to fill the missing values in a dataset. I favoured the MICE technique as it takes into account the other features in the dataset in order to fill the missing values of a particular feature. It helps in reducing the anomlaies that may occur due to the univariate imputation techiques. Learn more about MICE here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html","d1b78f4a":"Name, Ticket and Cabin features are dropped as they are contain unique values, which do not provide any valuable information on the survived\/not survived classification","95fc8685":"**Hyperparameter Tuning**\n\nIt is used to find the best subset of parameters to find the best model by using all provided combinations of parameters. GridSearch can be used for this purpose","741a93da":"Here, both the train and test sets have missing values. These will be dealt with an imputer model later","fcb77c36":"**DATA SCALING**\n\n**MINMAX SCALER**\n\nA normalization technique is required to scale the continuous features to a range. For this the minmax scaler has been used. Learn more about min max scaler here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html","b1a4b827":"This is the submission v10 of the classic titanic dataset ML model\n\n**This notebook include the following techniques:**\n* Data Imputation using MICE\n* label Encoding\n* Feature Scaling\n* Hyperparameter tuning\n* Feature Selection\n* Ensemble RandomForest Model\n\nWhat it does not include:\n* Extensive EDA - as most other notebooks on the titanic dataset has extensive graphs and analysis"}}