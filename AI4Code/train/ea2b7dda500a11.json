{"cell_type":{"66426c93":"code","28414737":"code","8f6a04a5":"code","19c8aa99":"code","e92292a8":"code","732b7a7e":"code","65afb446":"code","2f9bcca1":"code","e1424b30":"code","2d225c33":"code","8baefb63":"code","9d51869b":"code","e4949b69":"code","78fd121a":"code","a4e1c675":"code","20c8bfb9":"code","a5a8f4d5":"code","84bb50c6":"code","35d0bdc4":"code","348d12ee":"code","7dbd65ee":"code","6d28a019":"code","9b00953d":"code","f53e3b6c":"code","51644393":"code","2dd4b59f":"code","6c4ac6c0":"code","3c5d6bf6":"code","b8de4694":"code","f1cf0ec4":"code","3fab4e17":"code","f4a4dffd":"code","ccc1c5ff":"code","8b9265b6":"code","8628161b":"code","98ef6c6b":"code","00b7c562":"code","fef5c316":"code","137dce86":"code","7d301504":"code","d290f9ae":"code","90659e1b":"code","84d2f32c":"code","e02eaf24":"code","2ae927f1":"code","16ffd9f0":"code","debbc7ff":"code","cfb3e2e0":"code","8d10853b":"code","b275c578":"code","9c2a4f6f":"code","f8d2a88a":"code","b2359d43":"code","15950dc3":"code","5ec4adeb":"code","41560510":"code","0f3c661c":"code","2e1a1592":"code","5f44eb37":"code","8283dd97":"code","46f9ef1c":"code","be6e5273":"code","b1ff15e0":"code","d31bb7eb":"code","5409baaf":"code","1f468b4a":"code","99d02a45":"code","1c9ddce7":"code","4f270803":"code","dd51e8b3":"code","5b8d8447":"code","0e4fa79d":"markdown","e194f5a0":"markdown","c1489763":"markdown","a0740edc":"markdown","ffe2bdd1":"markdown","6a36f819":"markdown","3b212074":"markdown","ec3f17fa":"markdown","051f947d":"markdown","aded96cb":"markdown","ac63c570":"markdown","a153807f":"markdown","dda527ee":"markdown","fef155d5":"markdown","89b75524":"markdown","78391acf":"markdown","2299372d":"markdown","c8356569":"markdown","0389d740":"markdown","6ed1bda6":"markdown","6f1bea8f":"markdown","f58809e4":"markdown","fcdc1ae2":"markdown","979f9b66":"markdown","df0007a6":"markdown","10351e93":"markdown","c67fa0cb":"markdown","1e7d8294":"markdown","1c945f83":"markdown","2609cba5":"markdown","3d9d9794":"markdown","9a57e7c6":"markdown","2674d70f":"markdown","c312cdf5":"markdown","80977f75":"markdown","23dcf02e":"markdown","a5113a45":"markdown","8c9a4c27":"markdown","65ef38b7":"markdown","17b69be2":"markdown","0c1cb659":"markdown","9308a2e9":"markdown","ec2196a4":"markdown","62211fc5":"markdown","03d3e4ca":"markdown","ae00ce72":"markdown","0067223f":"markdown","cfc74a2a":"markdown","bb51a111":"markdown","f6c4b121":"markdown","7dcc7949":"markdown","f0f55db1":"markdown","7a34d2f4":"markdown","72c24f01":"markdown"},"source":{"66426c93":"# Importando bibliotecas que serao utilizadas neste projeto\nimport pandas as pd\nimport numpy as np\n\nfrom itertools import product\nfrom multiprocessing import Pool\nfrom scipy.stats import kurtosis, skew\nfrom scipy.optimize import minimize\nimport scipy.stats as scs\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\n# Stats\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pickle\nimport datetime\nfrom dateutil.relativedelta import relativedelta \nimport time\nimport gc\nimport os\nfrom tqdm import tqdm_notebook\n\n# Ignorar warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Seta algumas op\u00e7\u00f5es no Jupyter para exibi\u00e7\u00e3o dos datasets\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)\n\n# Variavel para controlar o treinamento no Kaggle\nTRAIN_OFFLINE = False\n\n# Variavel para indicar o path local\nLOCAL_DATA_FOLDER  = 'data\/'\nKAGGLE_DATA_FOLDER = '\/kaggle\/input\/m5-forecasting-accuracy\/'","28414737":"# Importando bibliotecas do sklearn\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, StandardScaler\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score, TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\n\n# lib de modelos de machine learning\nimport xgboost as xgb\nimport lightgbm as lgb","8f6a04a5":"# Funcao para reducao da memoria utilizada\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n# Funcao para realizar a leitura dos arquivos LOCAL ou do KAGGLE\ndef read_data():\n    \n    # Se for local\n    if TRAIN_OFFLINE:\n\n        calendar               = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'calendar.csv'))\n        sell_prices            = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'sell_prices.csv'))\n        sales_train_validation = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'sales_train_validation.csv'))\n        submission             = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'sample_submission.csv'))\n\n    # Se estiver no ambiente do Kaggle\n    else:\n        \n        calendar               = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'calendar.csv'))\n        sell_prices            = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sell_prices.csv'))\n        sales_train_validation = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sales_train_validation.csv'))\n        submission             = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sample_submission.csv'))\n\n    calendar               = reduce_mem_usage(calendar)\n    sell_prices            = reduce_mem_usage(sell_prices)\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    submission             = reduce_mem_usage(submission)\n        \n    return calendar, sell_prices, sales_train_validation, submission\n","19c8aa99":"# Leitura dos dados e aplicando redu\u00e7\u00e3o de mem\u00f3ria\ncalendar, sell_prices, sales_train_validation, submission = read_data()","e92292a8":"# Funcao para realizar o merge dos datasets retornando apenas um dataframe\ndef reshape_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows=55000000, merge=False):\n    \n    # realizando o reshape dos dados de venda usando melt\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    print('Melted sales train validation tem {} linhas e {} colunas'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    #sales_trian_validation = reduce_mem_usage(sales_train_validation)\n    \n    # separando os registros de teste e validacao\n    test_rows = [row for row in submission['id'] if 'validation' in row]\n    val_rows = [row for row in submission['id'] if 'evaluation' in row]\n    \n    test = submission[submission['id'].isin(test_rows)]\n    val = submission[submission['id'].isin(val_rows)]\n    \n    # renomeando as colunas\n    test.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', \n                    'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', \n                    'd_1931', 'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', \n                    'd_1940', 'd_1941']\n    val.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', \n                   'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', \n                   'd_1959', 'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', \n                   'd_1968', 'd_1969']\n    \n    # obtendo somente dados do produto e removendo registros duplicados \n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n    # realizando merge com a tabela de produto\n    test = test.merge(product, how = 'left', on = 'id')\n    val = val.merge(product, how = 'left', on = 'id')\n    \n    # realizando o reshape dos dados de test e validacao\n    test = pd.melt(test, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    val = pd.melt(val, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    # criando uma nova coluna para definir dados de treino, teste e validacao\n    sales_train_validation['part'] = 'train'\n    test['part'] = 'test'\n    val['part'] = 'val'\n    \n    # criando um so dataset com a juncao de todos os registros de treino, validacao e teste\n    data = pd.concat([sales_train_validation, test, val], axis = 0)\n    \n    # removendo datasets anteriores\n    del sales_train_validation, test, val\n    \n    # selecionando somente alguns registros para treinamento\n    data = data.loc[nrows:]\n    \n    # removendo os dados de validacao\n    data = data[data['part'] != 'val']\n    \n    # realizando o merge com calendario e preco\n    if merge:\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['d', 'day', 'weekday'], inplace = True, axis = 1)\n        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n        print('Dataset final para treino tem {} linhas e {} colunas'.format(data.shape[0], data.shape[1]))\n    else: \n        pass\n    \n    return data\n\n# Funcao para tratamento valores missing transformacao das features categoricas e numericas\ndef transform(data):\n    \n    # realizando tratamento em valores missing nas features categoricas\n    nan_features_cat = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features_cat:\n        data[feature].fillna('unknown', inplace = True)\n    \n    # realizando tratamento em valores missing na feature sell_price\n    data['sell_price'].fillna(0, inplace = True)\n        \n    # transformando features categorias em numericas para realizar as previsoes\n    encoder = preprocessing.LabelEncoder()\n    data['id_encode'] = encoder.fit_transform(data['id'])\n    \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n           'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data","732b7a7e":"%%time\n\n# Realizando o reshape e o merge dos datasets\ndata = reshape_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows=45000000, merge=True)\n\n# Chamando as funcoes de transformacao dos dados\ndata = transform(data)\n\n# Visualizando o cabecalho do dataset final\ndata.head()\n\n# Limpando dados da mem\u00f3ria\ngc.collect()","65afb446":"# Verificando a data de inicio e fim do dataset\nprint(min(data['date']), max(data['date']))","2f9bcca1":"data[data['id'] == 'FOODS_3_634_WI_2_validation'].head()","e1424b30":"data[(data['id'] == 'FOODS_3_634_WI_2_validation') & (data['demand'] > 0) & (data['part'] == 'train')]","2d225c33":"# Selecionar apenas os dados de treino e valida\u00e7\u00e3o para as analises\n# Selecionando somente 1 item para testes: FOODS_3_634_WI_2\ndf = data[(data['date'] <= '2016-04-24') & (data['id'] == 'FOODS_3_634_WI_2_validation') & (data['demand'] > 0) & (data['demand'] <= 15)]\n\n# Selecionando apenas algumas colunas para a analise e treinamento\ndf = df[['date','demand','dept_id','cat_id','store_id','state_id','event_name_1','event_type_1','snap_WI','sell_price']]\n\n# Transformando a data como index \ndf = df.set_index('date')\n\n# Visualizando o resultado do dataset\ndf.head()","8baefb63":"plt.figure(figsize=(24, 7))\nplt.plot(df['demand'])\nplt.title('Soma das Vendas por dia')\nplt.grid(True)\nplt.xticks(rotation=90)\nplt.show()","9d51869b":"# Visualizando informa\u00e7\u00f5es de distribuicao da variavel \"demand\"\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(12, 8))\n\n# Fit a normal distribution\nmu, std = norm.fit(df['demand'])\n\n# Verificando a distribuicao de frequencia da variavel \"demand\"\nsns.distplot(df['demand'], color=\"b\", fit = stats.norm)\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Demand\")\nax.set(title=\"Demand distribution: mu = %.2f,  std = %.2f\" % (mu, std))\nsns.despine(trim=True, left=True)\n\n# Adicionando Skewness e Kurtosis\nax.text(x=1.1, y=1, transform=ax.transAxes, s=\"Skewness: %f\" % df['demand'].skew(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:poo brown')\nax.text(x=1.1, y=0.95, transform=ax.transAxes, s=\"Kurtosis: %f\" % df['demand'].kurt(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:dried blood')\n\nplt.show()","e4949b69":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","78fd121a":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\n#  realizando as previs\u00f5es dos \u00faltimos 28 dias\nmoving_average(df, 28)","a4e1c675":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond \/ Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","20c8bfb9":"#  Vamos suavizar usando uma janela de 7 dias\nplotMovingAverage(df['demand'], 7)","a5a8f4d5":"#  Vamos suavizar usando uma janela de 28 dias\nplotMovingAverage(df['demand'], 28)","84bb50c6":"plotMovingAverage(df['demand'], 28, plot_intervals=True)","35d0bdc4":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)","348d12ee":"weighted_average(df['demand'], [0.6, 0.3, 0.1])","7dbd65ee":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result","6d28a019":"def plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","9b00953d":"plotExponentialSmoothing(df['demand'], [0.3, 0.05])","f53e3b6c":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result","51644393":"def plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","2dd4b59f":"plotDoubleExponentialSmoothing(df['demand'], alphas=[0.9, 0.02], betas=[0.9, 0.02])","6c4ac6c0":"class HoltWinters:\n    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) \/ self.slen\n        return sum \/ self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)\/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])\/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg\/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","3c5d6bf6":"def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=28):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=2) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], \n                            slen=slen, \n                            alpha=alpha, \n                            beta=beta, \n                            gamma=gamma, \n                            n_preds=len(test))\n        \n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        \n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","b8de4694":"%%time\n\nnew_data = df['demand']\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimizing the loss function \nopt = minimize(timeseriesCVscore, \n               x0=x, \n               args=(new_data, mean_squared_error), \n               method=\"TNC\", \n               bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\n# Take optimal values...\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\n# ...and train the model with them, forecasting for the next 28 days\nmodel = HoltWinters(new_data, \n                    slen = 28, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 28, \n                    scaling_factor = 3)\n\nmodel.triple_exponential_smoothing()","f1cf0ec4":"def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    plt.figure(figsize=(20, 10))\n    plt.plot(model.result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    \n    #error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n    #plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    error = rmse(series.values, model.result[:len(series)])\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n            series.values[series.values<model.LowerBond[:len(series)]]\n        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n            series.values[series.values>model.UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up\/Low confidence\")\n        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n        \n    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-60, len(model.result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);","3fab4e17":"plotHoltWinters(df['demand'])","f4a4dffd":"plotHoltWinters(df['demand'], plot_intervals=True, plot_anomalies=True)","ccc1c5ff":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\");","8b9265b6":"def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey\u2013Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","8628161b":"tsplot(df['demand'], lags=60)","98ef6c6b":"df_diff = df['demand'] - df['demand'].shift(28)\ntsplot(df_diff[28:], lags=60)","00b7c562":"tsplot(df_diff[28+1:], lags=60)","fef5c316":"# setting initial values and some bounds for them\nps = range(2, 3)\nd=1 \nqs = range(2, 3)\nPs = range(0, 2)\nD=1 \nQs = range(0, 2)\ns = 28 # season length is still 28\n\n# creating list with all the possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)","137dce86":"def optimizeSARIMA(parameters_list, d, D, s):\n    \"\"\"\n        Return dataframe with parameters and corresponding AIC\n        \n        parameters_list - list with (p, q, P, Q) tuples\n        d - integration order in ARIMA model\n        D - seasonal integration order \n        s - length of season\n    \"\"\"\n    \n    results = []\n    best_aic = float(\"inf\")\n\n    for param in tqdm_notebook(parameters_list):\n        # we need try-except because on some combinations model fails to converge\n        try:\n            model=sm.tsa.statespace.SARIMAX(df['demand'], \n                                            order=(param[0], d, param[1]), \n                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n        aic = model.aic\n        # saving best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n\n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    # sorting in ascending order, the lower AIC is - the better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table","7d301504":"%%time\nresult_table = optimizeSARIMA(parameters_list, d, D, s)","d290f9ae":"result_table.head()","90659e1b":"# set the parameters that give the lowest AIC\np, q, P, Q = result_table.parameters[0]\n\nbest_model=sm.tsa.statespace.SARIMAX(df['demand'], \n                                     order=(p, d, q),\n                                     seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprint(best_model.summary())","84d2f32c":"tsplot(best_model.resid[28+1:], lags=60)","e02eaf24":"def plotSARIMA(series, model, n_steps):\n    \"\"\"\n        Plots model vs predicted values\n        \n        series - dataset with timeseries\n        model - fitted SARIMA model\n        n_steps - number of steps to predict in the future\n        \n    \"\"\"\n    # adding model values\n    dfCopy = series.copy()\n    \n    dfCopy['arima_model'] = model.fittedvalues\n    # making a shift on s+d steps, because these values were unobserved by the model\n    # due to the differentiating\n    dfCopy['arima_model'][:s+d] = np.NaN    \n    \n    # forecasting on n_steps forward \n    forecast = model.predict(start = dfCopy.shape[0], end = dfCopy.shape[0]+n_steps)\n    forecast = dfCopy['arima_model'].append(forecast)\n\n    # calculate error, again having shifted on s+d steps from the beginning\n    error = rmse(dfCopy['demand'][s+d:], dfCopy['arima_model'][s+d:])\n\n    plt.figure(figsize=(15, 7))\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    #plt.plot(forecast, color='r', label=\"model\")\n    #plt.axvspan(dfCopy.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n    plt.plot(dfCopy['demand'], label=\"actual\")\n    plt.legend()\n    plt.grid(True);","2ae927f1":"plotSARIMA(df, best_model, 28)","16ffd9f0":"# Criar uma copia do dataset original\nnew_df = df.copy()\n\n# Visualizando o dataset\nnew_df.head()","debbc7ff":"# Adicionando features considerando o atraso da demanda de 7 a 28 dias\nfor i in range(7, 29):\n    new_df[\"lag_{}\".format(i)] = new_df['demand'].shift(i)","cfb3e2e0":"# Visualizando o resultado do dataset\nnew_df.tail()","8d10853b":"# para o cross-validation da serie temporal\ntscv = TimeSeriesSplit(n_splits=5)","b275c578":"def timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test  = X.iloc[test_index:]\n    y_test  = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","9c2a4f6f":"y = new_df.dropna()['demand']\nX = new_df.dropna().drop(['demand'], axis=1)","f8d2a88a":"# split com 10% para dados de teste\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.1)","b2359d43":"# machine learning em 2 linhas\nlr = LinearRegression()\nlr.fit(X_train, y_train)","15950dc3":"def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        Plots modelled vs fact values, prediction intervals and anomalies\n    \n    \"\"\"\n    \n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(15, 7))\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_absolute_error\")\n        mae = cv.mean() * (-1)\n        deviation = cv.std()\n        \n        scale = 1.96\n        lower = prediction - (mae + scale * deviation)\n        upper = prediction + (mae + scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond \/ lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    error = rmse(y_test, prediction)\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);\n    \ndef plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","5ec4adeb":"plotModelResults(lr, plot_intervals=True)\nplotCoefficients(lr)","41560510":"new_df.index = pd.to_datetime(new_df.index)\nnew_df[\"day\"] = new_df.index.day\nnew_df[\"weekday\"] = new_df.index.weekday\nnew_df['is_weekend'] = new_df.weekday.isin([5,6])*1\nnew_df.tail()","0f3c661c":"plt.figure(figsize=(16, 5))\nplt.title(\"Encoded features\")\nnew_df['day'].plot()\nnew_df['weekday'].plot()\nnew_df['is_weekend'].plot()\nplt.grid(True);","2e1a1592":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","5f44eb37":"X = new_df.dropna().drop(['demand'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.1)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","8283dd97":"def code_mean(data, cat_feature, real_feature):\n    \"\"\"\n    Returns a dictionary where keys are unique categories of the cat_feature,\n    and values are means over real_feature\n    \"\"\"\n    return dict(data.groupby(cat_feature)[real_feature].mean())","46f9ef1c":"average_day = code_mean(new_df, 'day', \"demand\")\nplt.figure(figsize=(12, 5))\nplt.title(\"M\u00e9dias di\u00e1rias\")\npd.DataFrame.from_dict(average_day, orient='index')[0].plot()\nplt.grid(True);","be6e5273":"def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n    \"\"\"\n        series: pd.DataFrame\n            dataframe with timeseries\n\n        lag_start: int\n            initial step back in time to slice target variable \n            example - lag_start = 1 means that the model \n                      will see yesterday's values to predict today\n\n        lag_end: int\n            final step back in time to slice target variable\n            example - lag_end = 4 means that the model \n                      will see up to 4 days back in time to predict today\n\n        test_size: float\n            size of the test dataset after train\/test split as percentage of dataset\n\n        target_encoding: boolean\n            if True - add target averages to the dataset\n        \n    \"\"\"\n    \n    # copy of the initial dataset\n    new_df = df.copy()\n\n    # lags of series\n    for i in range(7, 29):\n        new_df[\"lag_{}\".format(i)] = new_df['demand'].shift(i)\n\n    # datetime features\n    new_df.index = pd.to_datetime(new_df.index)\n    new_df[\"day\"] = new_df.index.day\n    new_df[\"weekday\"] = new_df.index.weekday\n    new_df['is_weekend'] = new_df.weekday.isin([5,6])*1\n\n    if target_encoding:\n        # calculate averages on train set only\n        test_index = int(len(new_df.dropna())*(1-test_size))\n        new_df['weekday_average'] = list(map(code_mean(new_df[:test_index], 'weekday', \"demand\").get, new_df['weekday']))\n        new_df[\"day_average\"] = list(map(code_mean(new_df[:test_index], 'day', \"demand\").get, new_df['day']))\n\n        # frop encoded variables \n        new_df.drop([\"day\", \"weekday\"], axis=1, inplace=True)\n    \n    # train-test split\n    y = new_df.dropna()['demand']\n    X = new_df.dropna().drop(['demand'], axis=1)\n\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n\n    return X_train, X_test, y_train, y_test","b1ff15e0":"X_train, X_test, y_train, y_test =\\\nprepareData(df, lag_start=1, lag_end=29, test_size=0.1, target_encoding=True)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lr)","d31bb7eb":"X_train, X_test, y_train, y_test =\\\nprepareData(df, lag_start=1, lag_end=29, test_size=0.1, target_encoding=False)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","5409baaf":"plt.figure(figsize=(10, 8))\nsns.heatmap(X_train.corr());","1f468b4a":"from sklearn.linear_model import LassoCV, RidgeCV\n\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)\nplotCoefficients(ridge)","99d02a45":"lasso = LassoCV(cv=tscv)\nlasso.fit(X_train_scaled, y_train)\n\nplotModelResults(lasso, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)\nplotCoefficients(lasso)","1c9ddce7":"from xgboost import XGBRegressor \n\nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train)","4f270803":"plotModelResults(xgb, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)","dd51e8b3":"from lightgbm import LGBMRegressor \n\nlgb = LGBMRegressor()\nlgb.fit(X_train_scaled, y_train)","5b8d8447":"plotModelResults(lgb, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)","0e4fa79d":"# Part 1. Carregando as bibliotecas","e194f5a0":"Os atrasos (lags) simples e a regress\u00e3o linear nos deram previs\u00f5es  distantes do SARIMA em termos de qualidade. Como existem muitas features desnecess\u00e1rias, faremos a Feature Selection daqui a pouco. Por enquanto, vamos continuar a engenharia!","c1489763":"Quando aplicamos a suaviza\u00e7\u00e3o nos dados, podemos ver claramente a din\u00e2mica das vendas no per\u00edodo.","a0740edc":"Agora, vamos ver o que acontece se, em vez de ponderar os \u00faltimos valores de $ k $ da s\u00e9rie temporal, come\u00e7armos a ponderar todas as observa\u00e7\u00f5es dispon\u00edveis enquanto diminu\u00edmos exponencialmente os pesos \u00e0 medida que avan\u00e7amos mais no tempo. Existe uma f\u00f3rmula para isso **[exponential smoothing (https:\/\/en.wikipedia.org\/wiki\/Exponential_smoothing)** que ir\u00e1 nos ajudar com esta f\u00f3rmula:\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n\n\nAqui, o valor do modelo \u00e9 uma m\u00e9dia ponderada entre o valor verdadeiro atual e os valores anteriores do modelo. O peso $\\alpha$ \u00e9 chamado de fator de suaviza\u00e7\u00e3o. Ele define a rapidez com que \"esqueceremos\" a \u00faltima observa\u00e7\u00e3o verdadeira. Quanto menor $\\alpha$, maior a influ\u00eancia das observa\u00e7\u00f5es anteriores e mais suave ser\u00e1 a s\u00e9rie.\n\nA exponencialidade est\u00e1 oculta na recursividade da fun\u00e7\u00e3o - multiplicamos por $(1-\\alpha)$ a cada passada, o que j\u00e1 cont\u00e9m uma multiplica\u00e7\u00e3o por $(1-\\alpha)$ dos valores anteriores do modelo.","ffe2bdd1":"## 5.3 Target encoding\n\nGostaria de adicionar outra variante para codificar features categ\u00f3ricas: encoding por valor m\u00e9dio. Se n\u00e3o for desej\u00e1vel criar um conjunto de dados usando muitas vari\u00e1veis dummy que podem levar \u00e0 perda de informa\u00e7\u00f5es e se elas n\u00e3o puderem ser usadas como valores reais devido a conflitos como \"0 horas < 23 horas\", ent\u00e3o \u00e9 poss\u00edvel codificar uma vari\u00e1vel com valores um pouco mais interpret\u00e1veis. A id\u00e9ia natural \u00e9 codificar com o valor m\u00e9dio da vari\u00e1vel de destino. No nosso exemplo, todos os dias da semana podem ser codificados pelo n\u00famero m\u00e9dio correspondente de vendas durante esse dia. \u00c9 muito importante garantir que o valor m\u00e9dio seja calculado apenas no conjunto de treinamento (ou apenas na valida\u00e7\u00e3o cruzada atual), para que o modelo conhe\u00e7a o futuro.","6a36f819":"# Kaggle Competition: M5 Forecasting \n## Estimate the unit sales of Walmart retail goods\n\n- Analise de Series Temporais\n\n- Autor: Rodrigo de Lima Oliveira\n\n- Refer\u00eancias:\n\n    - [DSA] https:\/\/www.datascienceacademy.com.br\/\n    - [MLCOURSER.AI] https:\/\/mlcourse.ai\/\n    - [KAGGLE] https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python","3b212074":"# Part 3. Feature Engineering","ec3f17fa":"Tamb\u00e9m podemos tra\u00e7ar os intervalos de confian\u00e7a.","051f947d":"## 4.2. Mover, suavizar, avaliar\n\nVamos come\u00e7ar com uma hip\u00f3tese: \"amanh\u00e3 ser\u00e1 o mesmo de hoje\". No entanto, atrav\u00e9s de um modelo como o $ \\ hat {y} _ {t} = y_ {t-1} $, assumiremos que o valor futuro de nossa vari\u00e1vel depende da m\u00e9dia dos valores anteriores de $ k $. Portanto, usaremos a **m\u00e9dia m\u00f3vel**.\n\n\n$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$","aded96cb":"## 4.8 Tratar a N\u00e3o Estacionariedade e construir o modelo SARIMA","ac63c570":"# Part 5. Modelos lineares em s\u00e9ries temporais\n\n\nEsses modelos analisados at\u00e9 aqui demandam muito tempo para a prepara\u00e7\u00e3o de dados (como no SARIMA) ou exigem treinamento frequente sobre novos dados (novamente, SARIMA) ou s\u00e3o dif\u00edceis de ajustar (bom exemplo - SARIMA). Portanto, muitas vezes \u00e9 muito mais f\u00e1cil selecionar algumas features das s\u00e9ries temporais existentes e criar um modelo de regress\u00e3o linear simples ou, por exemplo, uma random forest.\n\nEssa abordagem n\u00e3o \u00e9 apoiada pela teoria e quebra v\u00e1rias suposi\u00e7\u00f5es (por exemplo, o teorema de Gauss-Markov, especialmente para erros n\u00e3o correlacionados), mas \u00e9 muito \u00fatil na pr\u00e1tica e \u00e9 frequentemente usada em competi\u00e7\u00f5es.\n","a153807f":"### Estacionariedade\n\n\nAntes de come\u00e7ar, devo mencionar uma propriedade importante da s\u00e9rie temporal: [**stationarity**] (https:\/\/en.wikipedia.org\/wiki\/Stationary_process).\n\nSe um processo \u00e9 estacion\u00e1rio, isso significa que ele n\u00e3o altera suas propriedades estat\u00edsticas ao longo do tempo, ou seja, sua m\u00e9dia e vari\u00e2ncia. (A const\u00e2ncia da varia\u00e7\u00e3o \u00e9 chamada de [homoscedasticidade] (https:\/\/en.wikipedia.org\/wiki\/Homoscedasticity)) A fun\u00e7\u00e3o de covari\u00e2ncia n\u00e3o depende do tempo, deve depender apenas da dist\u00e2ncia entre as observa\u00e7\u00f5es. Voc\u00ea pode ver issonas imagens do post de [Sean Abu] (http:\/\/www.seanabu.com\/2016\/03\/22\/time-series-seasonal-ARIMA-model-in-python\/):\n\n- O gr\u00e1fico vermelho abaixo n\u00e3o \u00e9 estacion\u00e1rio porque a m\u00e9dia aumenta com o tempo.\n\n<img src=\"https:\/\/habrastorage.org\/files\/20c\/9d8\/a63\/20c9d8a633ec436f91dccd4aedcc6940.png\"\/>\n\n- Observando a varia\u00e7\u00e3o de valores ao longo do tempo\n\n<img src=\"https:\/\/habrastorage.org\/files\/b88\/eec\/a67\/b88eeca676d642449cab135273fd5a95.png\"\/>\n\n- Finalmente, a covari\u00e2ncia do i termo e do (i + m) termo n\u00e3o deve ser uma fun\u00e7\u00e3o do tempo. No gr\u00e1fico a seguir, voc\u00ea notar\u00e1 que o spread se aproxima \u00e0 medida que o tempo aumenta. Portanto, a covari\u00e2ncia n\u00e3o \u00e9 constante como no gr\u00e1fico da direita.\n\n\n<img src=\"https:\/\/habrastorage.org\/files\/2f6\/1ee\/cb2\/2f61eecb20714352840748b826e38680.png\"\/>\n\nEnt\u00e3o, por que a estacionariedade \u00e9 t\u00e3o importante? Porque \u00e9 f\u00e1cil fazer previs\u00f5es em uma s\u00e9rie estacion\u00e1ria, pois podemos assumir que as propriedades estat\u00edsticas futuras n\u00e3o ser\u00e3o diferentes daquelas atualmente observadas. A maioria dos modelos de s\u00e9ries temporais, de uma maneira ou de outra, tenta prever essas propriedades (m\u00e9dia ou varia\u00e7\u00e3o, por exemplo). As previs\u00f5es futuras estariam erradas se a s\u00e9rie original n\u00e3o estivesse estacion\u00e1ria. Infelizmente, a maioria das s\u00e9ries temporais que vemos fora dos livros did\u00e1ticos n\u00e3o \u00e9 estacion\u00e1ria, mas podemos (e devemos) mudar isso.\n\nEnt\u00e3o, vamos ver como detectar a n\u00e3o estacionariedade. Analisaremos o ru\u00eddo branco e os randow walks para aprender a passar de um para outro.","dda527ee":"Adicionaremos algumas features de data. Para fazer isso, precisamos transformar a data no formato `datetime`.","fef155d5":"Aparentemente vemos Overfitting! Algumas features tiveram uma importancia negativa t\u00e3o grande no conjunto de dados de treinamento que o modelo decidiu concentrar todas as suas for\u00e7as nele. Como resultado, a qualidade da previs\u00e3o caiu. Esse problema pode ser resolvido de v\u00e1rias maneiras; por exemplo, podemos calcular a codifica\u00e7\u00e3o de destino n\u00e3o para todo o conjunto de treinamento, mas para algumas janelas. Dessa forma, as codifica\u00e7\u00f5es da \u00faltima janela observada provavelmente descrever\u00e3o melhor o estado atual da s\u00e9rie. Como alternativa, podemos simplesmente descart\u00e1-lo manualmente, pois temos certeza de que isso piora as coisas neste caso.","89b75524":"**Weighted average** \u00e9 uma modifica\u00e7\u00e3o simples da m\u00e9dia m\u00f3vel. Os pesos somam `1` com pesos maiores atribu\u00eddos a observa\u00e7\u00f5es mais recentes.\n\n\n$\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$","78391acf":"Surpreendentemente, as s\u00e9ries iniciais s\u00e3o estacion\u00e1rias; o teste de Dickey-Fuller rejeitou a hip\u00f3tese nula de que uma raiz unit\u00e1ria est\u00e1 presente. Na verdade, podemos ver isso no pr\u00f3prio gr\u00e1fico - n\u00e3o temos uma tend\u00eancia vis\u00edvel; portanto, a m\u00e9dia \u00e9 constante e a varia\u00e7\u00e3o \u00e9 praticamente est\u00e1vel. A \u00fanica coisa que resta \u00e9 a sazonalidade, com a qual temos que lidar antes da modelagem. Para fazer isso, vamos usar a \"diferen\u00e7a sazonal\", que significa uma simples subtra\u00e7\u00e3o da s\u00e9rie de si mesma com um atraso igual ao per\u00edodo sazonal.","2299372d":"## 5.1 Feature Extraction","c8356569":"Podemos ver claramente que alguns coeficientes est\u00e3o se aproximando cada vez mais de zero (embora eles nunca cheguem a ele) \u00e0 medida que sua import\u00e2ncia no modelo diminui.","0389d740":"- [Mean Absolute Error](http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#mean-absolute-error): essa \u00e9 uma m\u00e9trica interpret\u00e1vel porque tem a mesma unidade de medida que a s\u00e9rie inicial, $[0, +\\infty)$\n\n$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$ \n\n```python\nsklearn.metrics.mean_absolute_error\n```\n---\n\n- Mean Absolute Percentage Error: \u00e9 o mesmo que o MAE, mas \u00e9 calculado como uma porcentagem, o que \u00e9 muito conveniente quando voc\u00ea deseja explicar a qualidade do modelo, $[0, +\\infty)$\n\n$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$ ","6ed1bda6":"Vamos analisar alguns dos m\u00e9todos e ver o que podemos extrair dos dados desta s\u00e9rie temporal da competi\u00e7\u00e3o\n\n## 5.2 Atrasos (lags) da s\u00e9rie temporal\n\nMudando a s\u00e9rie $n$ para tr\u00e1s, obtemos uma feature em que o valor atual da s\u00e9rie temporal est\u00e1 alinhado com seu valor no tempo $t-n$. Se fizermos uma mudan\u00e7a de 1 lag e treinarmos um modelo com essa nova feature, o modelo poder\u00e1 prever um passo \u00e0 frente e observar o estado atual da s\u00e9rie. Aumentar o atraso (lag), digamos, at\u00e9 28, permitir\u00e1 que o modelo fa\u00e7a previs\u00f5es 28 passos \u00e0 frente; no entanto, usar\u00e1 os dados observados nas 28 etapas para tr\u00e1s. Se algo mudar fundamentalmente a s\u00e9rie durante esse per\u00edodo n\u00e3o observado, o modelo n\u00e3o capturar\u00e1 essas altera\u00e7\u00f5es e retornar\u00e1 previs\u00f5es com um grande erro. Portanto, durante a sele\u00e7\u00e3o do atraso inicial, \u00e9 preciso encontrar um equil\u00edbrio entre a qualidade ideal da previs\u00e3o e a dura\u00e7\u00e3o de previs\u00e3o.","6f1bea8f":"Aqui est\u00e1 o c\u00f3digo para renderizar os gr\u00e1ficos.","f58809e4":"## 4.6 Time series cross validation\n\nAntes de come\u00e7ar a construir um modelo, vamos verificar como estimar os par\u00e2metros do modelo automaticamente.\n\nN\u00e3o h\u00e1 nada incomum aqui; como sempre, temos que escolher uma fun\u00e7\u00e3o de perda adequada para a tarefa que nos dir\u00e1 qu\u00e3o pr\u00f3ximo o modelo se aproxima dos dados. Em seguida, usando a valida\u00e7\u00e3o cruzada, avaliaremos nossa fun\u00e7\u00e3o de perda escolhida para os par\u00e2metros de modelo fornecidos, calcularemos o gradiente, ajustamos os par\u00e2metros do modelo e assim por diante, eventualmente descendo para o m\u00ednimo global.\n\n\nVoc\u00ea pode estar se perguntando como fazer a valida\u00e7\u00e3o cruzada para s\u00e9ries temporais porque as s\u00e9ries temporais possuem essa estrutura temporal e n\u00e3o \u00e9 poss\u00edvel misturar valores aleatoriamente sem preserva essa estrutura. Com a randomiza\u00e7\u00e3o, todas as depend\u00eancias de tempo entre as observa\u00e7\u00f5es ser\u00e3o perdidas. \u00c9 por isso que teremos que usar uma abordagem mais complicada para otimizar os par\u00e2metros do modelo. N\u00e3o sei se existe um nome oficial para isso, mas em [CrossValidated] (https:\/\/stats.stackexchange.com\/questions\/14099\/using-k-fold-cross-validation-for-time-series-model-selection), \u00e9 poss\u00edvel encontrar algumas respostas para esse m\u00e9todo que \u00e9 \"valida\u00e7\u00e3o cruzada em uma base cont\u00ednua\".\n\nA id\u00e9ia \u00e9 bastante simples - treinamos nosso modelo em um pequeno segmento da s\u00e9rie cronol\u00f3gica desde o in\u00edcio at\u00e9 $t$, fazemos previs\u00f5es para as pr\u00f3ximas etapas de $t+n$ e calculamos um erro. Em seguida, expandimos nossa amostra de treinamento para o valor $t+n$, fazemos previs\u00f5es de $t+n$ at\u00e9 $t+2*n$ e continuamos movendo janela de teste da s\u00e9rie cronol\u00f3gica at\u00e9 atingirmos a \u00faltima observa\u00e7\u00e3o dispon\u00edvel. Como resultado, temos muitas amostras $n$ entre a amostra de treinamento inicial e a \u00faltima observa\u00e7\u00e3o.\n\n<img src=\"https:\/\/habrastorage.org\/files\/f5c\/7cd\/b39\/f5c7cdb39ccd4ba68378ca232d20d864.png\"\/>","fcdc1ae2":"## 4.1. M\u00e9tricas para a previs\u00e3o\n- Antes de come\u00e7ar a realizar as previs\u00f5es, vamos entender como medir o desempenho de nossas previs\u00f5es e dar uma olhada na m\u00e9trica que ser\u00e1 usada.","979f9b66":"Primeiro, vamos ter certeza de que temos features a serem descartadas e que os dados possuem features altamente correlacionadas.","df0007a6":"- $p$ \u00e9 provavelmente 4, j\u00e1 que \u00e9 o atraso mais significativo no PACF\n- $d$ \u00e9 igual a 1 porque tivemos as primeiras diferen\u00e7as\n- $q$ deve estar em torno de 4, assim como no ACF\n- $P$ pode ser 2, uma vez que os atrasos de 28 e 56 s\u00e3o um pouco significativas no PACF\n- $D$ novamente \u00e9 igual a 1 porque realizamos diferencia\u00e7\u00e3o sazonal\n- $Q$ \u00e9 provavelmente 1. O 28\u00ba atraso no ACF \u00e9 significativo, enquanto o 56\u00ba n\u00e3o.","10351e93":"## 4.7 Abordagem Econometrica","c67fa0cb":"# Part 6. XGBoost e LightGBM","1e7d8294":"Infelizmente, n\u00e3o podemos fazer previs\u00f5es do futuro - para obter o pr\u00f3ximo valor, precisamos que os valores anteriores sejam realmente observados. Mas a m\u00e9dia m\u00f3vel tem outro uso - suavizar a s\u00e9rie temporal original para identificar tend\u00eancias. O Pandas tem uma implementa\u00e7\u00e3o dispon\u00edvel com [`DataFrame.rolling (window).mean()`] (http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.rolling.html). Quanto maior a janela, mais suave a tend\u00eancia. No caso de dados com muito ru\u00eddo, geralmente encontrados em dados de finan\u00e7as ou mercado de a\u00e7\u00f5es, esse procedimento pode ajudar a detectar padr\u00f5es comuns.","1c945f83":"# Part 2. Importando os Dados","2609cba5":"Vamos testar v\u00e1rios modelos e ver qual \u00e9 o melhor.","3d9d9794":"Agora temos que ajustar dois par\u00e2metros: $\\alpha$ e $\\beta$. O primeiro \u00e9 respons\u00e1vel pela suaviza\u00e7\u00e3o da s\u00e9rie em torno da tend\u00eancia, a segunda pela suaviza\u00e7\u00e3o da pr\u00f3pria tend\u00eancia. Quanto maiores os valores, maior o peso das observa\u00e7\u00f5es mais recentes e menos suavizada ser\u00e1 a s\u00e9rie.","9a57e7c6":"Agora que sabemos como definir os par\u00e2metros iniciais, vamos dar uma olhada no gr\u00e1fico final mais uma vez e definir os par\u00e2metros:","2674d70f":"# Part 4. Time Series em Python","c312cdf5":"Vamos construir um modelo ARIMA, percorrendo todos os est\u00e1gios para tornar uma s\u00e9rie estacion\u00e1ria.","80977f75":"### Vamos dar uma olhada em como trabalhar com s\u00e9ries temporais usando esta competi\u00e7\u00e3o do Kaggle. Ent\u00e3o vamos analisar:\n- M\u00e9todos e modelos que podemos usar para previs\u00f5es neste dataset\n- Aplicar suaviza\u00e7\u00e3o exponencial dupla e tripla\n- Analisar estacionariedade\n- Criar modelo SARIMA\n- Fazer previs\u00f5es usando o xgboost e lightgbm","23dcf02e":"Agora \u00e9 muito melhor com a sazonalidade vis\u00edvel desaparecida.","a5113a45":"A regress\u00e3o Lasso acabou sendo mais conservadora; removeu algumas lag features importantes o que piorou a qualidade da previs\u00e3o.","8c9a4c27":"## 4.4 Double exponential smoothing","65ef38b7":"Agora, sabendo como configurar a valida\u00e7\u00e3o cruzada, podemos encontrar os par\u00e2metros ideais para o modelo Holt-Winters. Lembre-se de que temos sazonalidade di\u00e1ria, da\u00ed o par\u00e2metro slen = 1.","17b69be2":"## 4.5 Triple exponential smoothing a.k.a. Holt-Winters\n\nVimos a suaviza\u00e7\u00e3o exponencial e a dupla suaviza\u00e7\u00e3o exponencial. Desta vez, vamos entrar em suaviza\u00e7\u00e3o exponencial triplo.\n\n\nA id\u00e9ia \u00e9 adicionar um terceiro componente - a sazonalidade. Isso significa que n\u00e3o devemos usar esse m\u00e9todo se n\u00e3o for esperado que nossa s\u00e9rie temporal tenha sazonalidade. Os componentes sazonais do modelo explicam varia\u00e7\u00f5es repetidas em torno da intercepta\u00e7\u00e3o e tend\u00eancia e ser\u00e3o especificadas pela dura\u00e7\u00e3o da temporada, ou seja, pelo per\u00edodo ap\u00f3s o qual as varia\u00e7\u00f5es se repetem. Para cada observa\u00e7\u00e3o do per\u00edodo, h\u00e1 um componente separado; por exemplo, se o per\u00edodo for de 28 dias (uma sazonalidade mensal), teremos 28 componentes sazonais, um para cada dia do m\u00eas.\n\nCom isso, vamos escrever um novo sistema de equa\u00e7\u00f5es:\n\n$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n\n$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n\n\nO intercept agora depende do valor atual da s\u00e9rie menos qualquer componente sazonal correspondente. A tend\u00eancia permanece inalterada e o componente sazonal depende do valor atual da s\u00e9rie menos o intercept e do valor anterior do componente. Leve em considera\u00e7\u00e3o que o componente \u00e9 suavizado em todas os per\u00edodos dispon\u00edveis; por exemplo, se tivermos um componente de segunda-feira, ser\u00e1 calculado apenas a m\u00e9dia de outras segundas-feiras. Voc\u00ea pode ler mais sobre como a m\u00e9dia funciona e como \u00e9 feita a aproxima\u00e7\u00e3o inicial da tend\u00eancia e dos componentes sazonais [aqui] (http:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section4\/pmc435.htm). Agora que temos o componente sazonal, podemos prever n\u00e3o apenas um ou dois passos \u00e0 frente, mas tamb\u00e9m um futuro arbitr\u00e1rio $m$, o que \u00e9 muito bom.\n\nAbaixo est\u00e1 o c\u00f3digo para um modelo triplo de suaviza\u00e7\u00e3o exponencial, tamb\u00e9m conhecido pelos sobrenomes de seus criadores, Charles Holt e seu aluno Peter Winters. Al\u00e9m disso, o m\u00e9todo Brutlag foi inclu\u00eddo no modelo para produzir intervalos de confian\u00e7a:\n\n\n$$\\hat y_{max_x}=\\ell_{x\u22121}+b_{x\u22121}+s_{x\u2212T}+m\u22c5d_{t\u2212T}$$\n\n$$\\hat y_{min_x}=\\ell_{x\u22121}+b_{x\u22121}+s_{x\u2212T}-m\u22c5d_{t\u2212T}$$\n\n$$d_t=\\gamma\u2223y_t\u2212\\hat y_t\u2223+(1\u2212\\gamma)d_{t\u2212T},$$\n\nonde $T$ \u00e9 a dura\u00e7\u00e3o do per\u00edodo, $d$ \u00e9 o desvio previsto. Outros par\u00e2metros foram obtidos da tripla suaviza\u00e7\u00e3o exponencial. Voc\u00ea pode ler mais sobre o m\u00e9todo e sua aplicabilidade \u00e0 detec\u00e7\u00e3o de anomalias em s\u00e9ries temporais [aqui] (http:\/\/fedcsis.org\/proceedings\/2012\/pliks\/118.pdf).","0c1cb659":"Como agora temos escalas diferentes em nossas features, precisamos transform\u00e1-los na mesma escala para explorar a import\u00e2ncia, posteriormente, a regulariza\u00e7\u00e3o.","9308a2e9":"Podemos visualizar o resultado:","ec2196a4":"O erro permaneceu o mesmo.","62211fc5":"\u00d3timo, geramos um conjunto de dados aqui. Por que agora n\u00e3o treinamos um modelo?","03d3e4ca":"Finalmente, vamos juntar todas as transforma\u00e7\u00f5es em uma \u00fanica fun\u00e7\u00e3o.","ae00ce72":"A julgar pelos gr\u00e1ficos, nosso modelo conseguiu aproximar com sucesso as s\u00e9ries temporais iniciais, capturando a sazonalidade di\u00e1ria, a tend\u00eancia geral de queda e at\u00e9 algumas anomalias. Se voc\u00ea observar os desvios do modelo, poder\u00e1 ver claramente que o modelo reage bastante \u00e0s mudan\u00e7as na estrutura da s\u00e9rie, mas depois retorna rapidamente o desvio aos valores normais, essencialmente \"esquecendo\" o passado. Esse recurso do modelo nos permite criar rapidamente sistemas de detec\u00e7\u00e3o de anomalias, mesmo para ru\u00eddos nos dados da s\u00e9rie, sem gastar muito tempo na prepara\u00e7\u00e3o dos dados e no treinamento do modelo.","0067223f":"No final, obtivemos previs\u00f5es muito boas. Nosso modelo errou 2.96 (RMSE), em m\u00e9dia, o que \u00e9 muito, muito bom. No entanto, os custos gerais da prepara\u00e7\u00e3o dos dados, da prepara\u00e7\u00e3o da s\u00e9rie e da sele\u00e7\u00e3o de par\u00e2metros podem n\u00e3o valer essa precis\u00e3o.","cfc74a2a":"## 4.9 Fam\u00edlia ARIMA\n\nExplicarei esse modelo construindo letra por letra. $SARIMA(p, d, q)(P, D, Q, s)$, M\u00e9dia m\u00f3vel sazonal de regress\u00e3o autom\u00e1tica (Seasonal Autoregression Moving Average model):\n\n\n- $AR(p)$ - modelo de regress\u00e3o autom\u00e1tica, isto \u00e9, regress\u00e3o da s\u00e9rie temporal em si mesma. A suposi\u00e7\u00e3o b\u00e1sica \u00e9 que os valores da s\u00e9rie atual dependem de seus valores anteriores com algum atraso (ou v\u00e1rios atrasos). O atraso m\u00e1ximo no modelo \u00e9 referido como $p$. Para determinar o $p$ inicial, \u00e9 necess\u00e1rio examinar o gr\u00e1fico do PACF e encontrar o maior atraso significativo o qual a maioria dos outros atrasos se torna insignificante.\n\n\n- $ MA(q)$ - modelo de m\u00e9dia m\u00f3vel. Sem entrar em muitos detalhes, modela o erro da s\u00e9rie cronol\u00f3gica, novamente com a suposi\u00e7\u00e3o de que o erro atual depende do anterior com algum atraso, conhecido como $q$. O valor inicial pode ser encontrado no gr\u00e1fico ACF com a mesma l\u00f3gica de antes.\n\n\nVamos combinar nossas 4 primeiras letras:\n\n$AR(p) + MA(q) = ARMA(p, q)$\n\n\nO que temos aqui \u00e9 o modelo de m\u00e9dia m\u00f3vel autorregressiva! Se a s\u00e9rie \u00e9 estacion\u00e1ria, pode ser aproximada com estas 4 letras. Vamos continuar.\n\n\n- $I(d)$ - ordem de integra\u00e7\u00e3o. Este \u00e9 simplesmente o n\u00famero de diferen\u00e7as n\u00e3o sazonais necess\u00e1rias para tornar a s\u00e9rie estacion\u00e1ria. No nosso caso, \u00e9 apenas 1 porque usamos as primeiras diferen\u00e7as.\n\nA adi\u00e7\u00e3o desta letra \u00e0s quatro nos d\u00e1 o modelo $ARIMA$, que pode manipular dados n\u00e3o estacion\u00e1rios com a ajuda de diferen\u00e7as n\u00e3o sazonais. \u00d3timo, mais uma letra pela frente!\n\n\n- $S(s)$ - \u00e9 respons\u00e1vel pela sazonalidade e igual \u00e0 dura\u00e7\u00e3o do tamanho do per\u00edodo da s\u00e9rie\n\nCom isso, temos tr\u00eas parametros: $(P, D, Q)$\n\n- $P$ - ordem de regress\u00e3o autom\u00e1tica para o componente sazonal do modelo, que pode ser derivado do PACF. Mas voc\u00ea precisa observar o n\u00famero de atrasos significativos, que s\u00e3o os m\u00faltiplos da dura\u00e7\u00e3o do per\u00edodo da serie. Por exemplo, se o per\u00edodo \u00e9 igual a 28 dias e vemos que os atrasos de 28 e 56 s\u00e3o significativas no PACF, isso significa que o $P$ inicial deve ser 2.\n\n\n- $Q$ - l\u00f3gica semelhante usando o gr\u00e1fico ACF.\n\n\n- $D$ - ordem de integra\u00e7\u00e3o sazonal. Isso pode ser igual a 1 ou 0, dependendo se as diferen\u00e7as sazonais foram aplicadas ou n\u00e3o.","bb51a111":"Quais as features podemos extrair deste dataset?\n* Lags of time series\n* Window statistics:\n    - Max\/min value of series in a window\n    - Average\/median value in a window\n    - Window variance\n    - etc.\n* Date and time features:\n    - Eventos especiais\n    - Dia da semana, m\u00eas, ano,...\n    - Feriados\n    - etc.\n* Target encoding \n* Forecasts de outros modelos (note que podemos perder o desempenho da previs\u00e3o dessa maneira)","f6c4b121":"\u00c9 claro que os res\u00edduos s\u00e3o estacion\u00e1rios e n\u00e3o h\u00e1 autocorrela\u00e7\u00f5es aparentes. Vamos fazer previs\u00f5es usando nosso modelo.","7dcc7949":"At\u00e9 agora, os m\u00e9todos apresentados foram para uma \u00fanica previs\u00e3o de um ponto futuro (com uma boa suaviza\u00e7\u00e3o). Isso \u00e9 legal, mas tamb\u00e9m n\u00e3o \u00e9 suficiente. Vamos estender a suaviza\u00e7\u00e3o exponencial para que possamos prever dois pontos futuros (e claro, tamb\u00e9m incluiremos mais suaviza\u00e7\u00e3o).\n\nA decomposi\u00e7\u00e3o em s\u00e9rie nos ajudar\u00e1 obtendo dois componentes: intercept (n\u00edvel) $\\ell$ e slope (tend\u00eancia) $b$. Aprendemos a prever intercept (ou valor esperado da s\u00e9rie) com nossos m\u00e9todos anteriores; agora, aplicaremos a mesma suaviza\u00e7\u00e3o exponencial \u00e0 tend\u00eancia, assumindo que a dire\u00e7\u00e3o futura das mudan\u00e7as na s\u00e9rie temporal depende das altera\u00e7\u00f5es ponderadas anteriores. Como resultado, obtemos o seguinte conjunto de fun\u00e7\u00f5es:\n\n$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n\n\nA primeira descreve o intercept, que depende do valor atual da s\u00e9rie. O segundo termo agora est\u00e1 dividido em valores anteriores do n\u00edvel e da tend\u00eancia. A segunda fun\u00e7\u00e3o descreve a tend\u00eancia, que depende das mudan\u00e7as de n\u00edvel na etapa atual e do valor anterior da tend\u00eancia. Nesse caso, o coeficiente $\\beta$ \u00e9 um peso para a suaviza\u00e7\u00e3o exponencial. A previs\u00e3o final \u00e9 a soma dos valores do modelo do intercept e da tend\u00eancia.","f0f55db1":"## 5.4 Regularization e Feature Selection \n\nComo j\u00e1 sabemos, nem todos as features s\u00e3o igualmente importantes - alguns podem levar a overfitting, enquanto outras devem ser removidas. Al\u00e9m da inspe\u00e7\u00e3o manual, podemos aplicar a regulariza\u00e7\u00e3o. Dois dos modelos de regress\u00e3o mais populares com regulariza\u00e7\u00e3o s\u00e3o as regress\u00f5es de Ridge e Lasso. Ambos adicionam mais algumas restri\u00e7\u00f5es \u00e0 nossa fun\u00e7\u00e3o de perda.\n\nNo caso da regress\u00e3o de Ridge, essas restri\u00e7\u00f5es s\u00e3o a soma dos quadrados dos coeficientes multiplicados pelo coeficiente de regulariza\u00e7\u00e3o. Quanto maior o coeficiente de uma feature, maior ser\u00e1 a nossa perda. Portanto, tentaremos otimizar o modelo, mantendo os coeficientes razoavelmente baixos.\n\nComo resultado dessa regulariza\u00e7\u00e3o de $L2$, teremos um vi\u00e9s mais alto e uma menor varia\u00e7\u00e3o, de modo que o modelo generalize melhor (pelo menos \u00e9 o que esperamos que aconte\u00e7a).\n\nO segundo modelo de regress\u00e3o, regress\u00e3o de Lasso, adiciona \u00e0 fun\u00e7\u00e3o de perda, n\u00e3o quadrados, mas valores absolutos dos coeficientes. Como resultado, durante o processo de otimiza\u00e7\u00e3o, os coeficientes das features sem import\u00e2ncia podem se tornar zeros, o que permite a sele\u00e7\u00e3o automatizada de recursos. Esse tipo de regulariza\u00e7\u00e3o \u00e9 chamado $L1$.","7a34d2f4":"## 4.3 Exponential smoothing","72c24f01":"Vamos inspecionar os res\u00edduos do modelo."}}