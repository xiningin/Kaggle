{"cell_type":{"41247313":"code","85c6afca":"code","2f0ae061":"code","6474c52a":"code","3c50523b":"code","329794cb":"code","de902402":"code","c3db8d17":"code","3d5e33b5":"code","03875f85":"markdown","ffb1b669":"markdown","0ffa9eac":"markdown","d43d9641":"markdown","6e6aad2c":"markdown"},"source":{"41247313":"COMMIT = False\n\nif COMMIT:\n    from PIL import Image\n    import numpy as np\n    import json\n    import cv2\n    import tensorflow as tf\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n\n    imgs_path = \"\/kaggle\/input\/ann-and-dl-vqa\/dataset_vqa\/train\"\n    train_json_path = \"\/kaggle\/input\/ann-and-dl-vqa\/dataset_vqa\/train_data.json\"\n    test_json_path = \"\/kaggle\/input\/ann-and-dl-vqa\/dataset_vqa\/test_data.json\"\n    \n    SEED = 1234\n    DATASET_SPLIT = 0.8\n    img_h = 128\n    img_w = 128\n    BATCH_SIZE = 128\n    \n    classes = {'0': 0,\n               '1': 1,\n               '10': 2,\n               '2': 3,\n               '3': 4,\n               '4': 5,\n               '5': 6,\n               '6': 7,\n               '7': 8,\n               '8': 9,\n               '9': 10,\n               'no': 11,\n               'yes': 12}\n    \n    N_CLASSES = len(classes)","85c6afca":"if COMMIT:\n    # ----------------------TRAIN THE TOKENIZER VOCABULARY----------------------\n    if 'tokenizer' not in globals():        # only if it does not exists yet\n        # Use the Tokenizer to transform the text (questions) into sequence\n        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n\n        with open(train_json_path, 'r') as f:\n            data = json.load(f)\n            data = data['questions']\n\n            for question in data:\n                quest = question['question'].split(\" \")\n                for i in range(len(quest)):\n                    quest[i] = quest[i].replace(\"?\", \"\")\n                #print(quest)\n\n                # Updates internal vocabulary based on the questions of the dataset\n                tokenizer.fit_on_texts(quest)            \n        f.close()\n    words_number = len(tokenizer.word_index) + 1","2f0ae061":"class DataGenerator(tf.keras.utils.Sequence):\n    \n    def __init__(self, list_IDs, image_path, train_input_questions, max_length, to_fit=True,\n                 batch_size=16, dim=(100, 150), n_channels=3, n_classes=13, shuffle=True):\n        self.list_IDs = list_IDs\n        self.train_input_questions = train_input_questions\n        self.image_path = image_path\n        self.to_fit = to_fit\n        self.batch_size = batch_size\n        self.dim = dim\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.img_h = dim[0]\n        self.img_w = dim[1]\n        self.max_length = max_length\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        # Generate indexes of the batch\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X = self._generate_X(list_IDs_temp)\n\n        if self.to_fit:\n            y = self._generate_y(list_IDs_temp)\n            return X, y\n        else:\n            return X\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def _generate_X(self, list_IDs_temp):\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        X2 = np.empty((self.batch_size, self.max_length))\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            X[i,] = self._load_image(self.image_path[ID], self.img_w, self.img_h)\n            X2[i,] = (self.train_input_questions[ID]).tolist()\n        ole = [X2, X]\n        \n        return ole\n\n    def _generate_y(self, list_IDs_temp):\n        y = np.empty((self.batch_size, 1), dtype=int)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            y[i] = self.list_IDs[ID]\n\n        return y\n\n    def _load_image(self, image_path, img_w, img_h):\n        if self.to_fit:\n            image = cv2.imread(\"\/kaggle\/input\/ann-and-dl-vqa\/dataset_vqa\/train\/\" + image_path)\n        else:\n            image = cv2.imread(\"\/kaggle\/input\/ann-and-dl-vqa\/dataset_vqa\/test\/\" + image_path)   \n        image = cv2.resize(image, (img_w, img_h))\n        image = image\/ 255.\n        return image","6474c52a":"def readTrainJson(data, first, last):\n        images = []\n        questions = []\n        answers = []\n\n        for question in data[first:last]:\n            name = question['image_filename']\n            quest = question['question'].split(\" \")\n            for i in range(len(quest)):\n                quest[i] = quest[i].replace(\"?\", \"\")\n            ans = question['answer']\n\n            images.append(name)\n            questions.append(quest)\n            answers.append(classes[ans])\n        return images, questions, answers\n\ndef readTestJson(data, first, last):\n    quest_id = []\n    images = []\n    questions = []\n\n    for question in data[first:last]:\n        qid = question['question_id']\n        name = question['image_filename']\n        quest = question['question'].split(\" \")\n        for i in range(len(quest)):\n            quest[i] = quest[i].replace(\"?\", \"\")\n        \n        quest_id.append(qid)\n        images.append(name)\n        questions.append(quest)\n    return images, questions, quest_id","3c50523b":"if COMMIT:\n    #read train JSON file\n    with open(train_json_path, 'r') as f:\n        train_data = json.load(f)\n        train_data = train_data['questions']\n    f.close()\n    \n    #read test JSON file\n    with open(test_json_path, 'r') as f:\n        test_data = json.load(f)\n        test_data = test_data['questions']\n    f.close()\n    \n    \n    TOT_QUESTIONS = len(train_data)\n    TRAIN_QUESTIONS = int(TOT_QUESTIONS*DATASET_SPLIT)\n    VALID_QUESTIONS = TOT_QUESTIONS-TRAIN_QUESTIONS\n\n    #extract images, questions and answer (or quest_id) from the train and test files\n    train_images, train_questions, train_answers = readTrainJson(train_data, 0, TRAIN_QUESTIONS)\n    valid_images, valid_questions, valid_answers = readTrainJson(train_data, TRAIN_QUESTIONS, TOT_QUESTIONS)\n    test_images, test_questions, questions_id = readTestJson(test_data, 0, len(test_data))\n    \n    sequences = tokenizer.texts_to_sequences(train_questions)\n    max_length = max(len(sequence) for sequence in sequences)\n    train_input_questions = pad_sequences(sequences, maxlen=max_length)\n\n    sequences = tokenizer.texts_to_sequences(valid_questions)\n    valid_input_questions = pad_sequences(sequences, maxlen=max_length)\n\n    tokenizer.fit_on_texts(test_questions)\n    sequences = tokenizer.texts_to_sequences(test_questions)\n    test_input_questions = pad_sequences(sequences, maxlen=max_length)\n\n    words_number = len(tokenizer.word_index) + 1\n\n    training_generator = DataGenerator(train_answers, train_images, train_input_questions, max_length, batch_size=BATCH_SIZE, dim=(img_h, img_w), n_classes=N_CLASSES)\n    validation_generator = DataGenerator(valid_answers, valid_images, valid_input_questions, max_length, batch_size=BATCH_SIZE, dim=(img_h, img_w), n_classes=N_CLASSES)\n    test_generator = DataGenerator(questions_id, test_images, test_input_questions,  max_length, to_fit=False, batch_size=1, dim=(img_h, img_w), n_classes=N_CLASSES, shuffle=False)","329794cb":"if COMMIT:\n    # Import Keras \n    # import tensorflow as tensorflow\n    \n    INPUT_SIZE_MERGE = 64\n\n    # Define CNN for Image Input\n    base_model = tf.keras.applications.VGG16(input_shape=(img_h, img_w, 3), include_top=False, weights='imagenet')\n    for i in range(len(base_model.layers)):\n        base_model.layers[i].trainable = False\n        \n    vision_model = tf.keras.models.Sequential()\n    #vision_model.add(tf.keras.layers.Dropout(0.2))\n    #global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n    vision_model.add(base_model)\n    #vision_model.add(global_average_layer)\n    vision_model.add(tf.keras.layers.Dropout(0.1))\n    vision_model.add(tf.keras.layers.Flatten())\n    vision_model.add(tf.keras.layers.Dense(INPUT_SIZE_MERGE))\n\n    image_input = tf.keras.layers.Input(shape=(img_h, img_w, 3))\n    encoded_image = vision_model(image_input)\n\n    # Define RNN for language input\n    question_input = tf.keras.layers.Input(shape=[max_length])\n    embedded_question = tf.keras.layers.Embedding(input_dim=words_number, output_dim=512, input_length=100)(question_input)\n    encoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(INPUT_SIZE_MERGE, dropout=0.1, recurrent_dropout=0.1, unroll=True))(embedded_question)\n\n    # Combine CNN and RNN to create the final model\n    merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n    output = tf.keras.layers.Dense(32)(merged)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(len(classes), activation='softmax')(output)\n    vqa_model = tf.keras.models.Model(inputs=[question_input, image_input], outputs=output)\n    \n    vision_model.summary()\n    vqa_model.summary()","de902402":"if COMMIT:\n    # Optimization params\n    # -------------------\n\n    # Loss\n    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n\n    # learning rate\n    lr = 5e-4\n    #optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr, rho=0.9)\n    # -------------------\n\n    # Validation metrics\n    # ------------------\n\n    metrics = ['accuracy']\n    # ------------------\n\n    # Compile Model\n    #vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    vqa_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","c3db8d17":"if COMMIT:\n    vqa_model.fit_generator(generator=training_generator,\n                            validation_data=validation_generator,\n                            epochs=2)\n    pred = vqa_model.predict_generator(test_generator)","3d5e33b5":"import os\nfrom datetime import datetime\n\ndef create_csv(results, results_dir='.\/'):\n\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n\n        f.write('Id,Category\\n')\n\n        for key, value in results.items():\n            f.write(str(key) + ',' + str(value) + '\\n')\n\nresults = {}\n\nfor i in range(len(pred)):\n    results[test_generator.list_IDs[i]] = np.argmax(pred[i])\n\ncreate_csv(results)","03875f85":"## Description Generator\nWe used a custom generator who take the couple (image, question) as input and its answer as output.","ffb1b669":"## Parameters\nWe play a lot with the learning rate, optimizer and loss to improve our result but they does not seem to change a lot.\n","0ffa9eac":"# Visual Question Answering\nWe split the dataset: 80% train set and 20% validation set. The batch size is set to 128 to speed up a little bit the training part, same thing for the epochs that are only 2.\n","d43d9641":"## CNN & RNN\nWe used the standard network provided by Keras and than we introduce some changing in order to reach a better result. For example: VGG16 model. ","6e6aad2c":"# Custom Generator"}}