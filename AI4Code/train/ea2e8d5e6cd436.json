{"cell_type":{"7ff6d692":"code","fc93bc4e":"code","4999ad2d":"code","9ca27c32":"code","84921514":"code","50192922":"code","9149da26":"code","ea8e4107":"code","497b4925":"code","3b745eca":"code","69812bc2":"code","0376aa59":"code","4dadb50e":"code","1f8ecb2a":"code","33548ac0":"code","943590bd":"code","b7b8be7c":"code","12372f92":"code","087a8276":"code","76350357":"code","947ece98":"code","43b50df8":"code","3aed8c87":"code","80d1a79d":"markdown","44da581d":"markdown","842532e8":"markdown","26034444":"markdown","c198a6e2":"markdown","1aeeb592":"markdown","7d6a4bca":"markdown","7d62418f":"markdown","5817e45b":"markdown","2ec911f4":"markdown","ceaa50ef":"markdown","ff75401c":"markdown","bed11451":"markdown","510191d5":"markdown","a63516f8":"markdown","0baa16ca":"markdown","995bd296":"markdown","db35ee70":"markdown","2f34d0b4":"markdown","323de887":"markdown","cb2a9e65":"markdown","908bb992":"markdown","ce5c8122":"markdown","5a44fb52":"markdown","ab814307":"markdown","86817728":"markdown","48d1d3a6":"markdown","a85e8501":"markdown","b5579753":"markdown","6b8a26e6":"markdown"},"source":{"7ff6d692":"#### PIP INSTALLS ####\n\n#################################################################\n######################## Light Version ##########################\n#################################################################\n# - This installs a very light version of \ud83e\udd17 Transformers. \n# - In particular, no specific machine learning frameworks are installed. \n# - Since we\u2019ll be using a lot of different features of the library, \n#   we recommend installing the development version, which comes \n#   with all the required dependencies for pretty much \n#   any imaginable use case.\n# - Therefore we use transformers[sentencepiece] instead of transformers\n# - Note*: We use a -q argument to quiet the output that is displayed\n#################################################################\n# !pip install --upgrade transformers\n\n# Full Development Version\n!pip install -q --upgrade transformers[sentencepiece]\n\n# Install Flair NLP library - https:\/\/github.com\/flairNLP\/flair\n!pip install -q --upgrade flair\n\n# This prevents TF from eating all the GPU memory which will block PyTorch\nimport tensorflow as tf\ntry:\n    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\nexcept:\n  # Invalid device or cannot modify virtual devices once initialized.\n  pass","fc93bc4e":"import tensorflow as tf\nimport numpy as np\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\n# --------------------------------------------------------------------\n#                 This is OLD Stuff (previous chapter)\n# --------------------------------------------------------------------\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"This course is amazing!\",\n]\nbatch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n# --------------------------------------------------------------------\n\n# --------------------------------------------------------------------\n#                  This is NEW Stuff (this chapter)\n# --------------------------------------------------------------------\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\nlabels = tf.convert_to_tensor([1, 1])\nmodel.train_on_batch(x=batch, y=labels)\n# --------------------------------------------------------------------","4999ad2d":"import torch\nimport numpy as np\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n\n# --------------------------------------------------------------------\n#                 This is OLD Stuff (previous chapter)\n# --------------------------------------------------------------------\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"This course is amazing!\",\n]\nbatch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n# --------------------------------------------------------------------\n\n# --------------------------------------------------------------------\n#                  This is NEW Stuff (this chapter)\n# --------------------------------------------------------------------\nbatch[\"labels\"] = torch.tensor([1, 1])\noptimizer = AdamW(model.parameters())\nloss = model(**batch).loss\nloss.backward()\noptimizer.step()\n# --------------------------------------------------------------------","9ca27c32":"from datasets import load_dataset\n\n# Download and cache the dataset\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\n\n# Print the DatasetDict Object\nprint(\"\\n\\n\\n... RAW DATASET OBJECT - GLUE\/MRPC\\n\")\nprint(raw_datasets)\n\n# Access the train subset of the dataset (Dataset Object)\nraw_train_dataset = raw_datasets[\"train\"]\nprint(\"\\n\\n\\n... TRAIN DATASET\\n\")\nprint(raw_train_dataset)\n\n# Access an example from the train subset of the dataset (Dict)\nraw_train_ex = raw_train_dataset[0]\nprint(\"\\n\\n\\n... TRAIN EXAMPLE - IDX=0\\n\")\nfor k,v in raw_train_ex.items(): print(f\"\\t--> {k:<10}: {v}\")\n    \n# Access features of our Dataset Object (Dict)\nraw_train_features = raw_train_dataset.features\nprint(\"\\n\\n\\n... TRAIN DATASET FEATURES\\n\")\nfor k,v in raw_train_features.items(): print(f\"\\t--> {k:<10}: {v}\")\n\n# Access the indices from the train and val subsets of the dataset\nprint(\"\\n\\n\\n... LABEL - TRAIN SUBSET - IDX=15\\n\")\nfor k,v in raw_datasets[\"train\"][15].items(): \n    print(f\"\\t--> **{k}** : {v} ({raw_train_dataset.features['label'].names[v]})\") if k==\"label\" else print(f\"\\t--> {k:<10}: {v}\")\n\nprint(\"\\n\\n... LABEL - VAL SUBSET - IDX=87\\n\")\nfor k,v in raw_datasets[\"validation\"][87].items(): \n    print(f\"\\t--> **{k}** : {v} ({raw_train_dataset.features['label'].names[v]})\") if k==\"label\" else print(f\"\\t--> {k:<10}: {v}\")\n\n# For spacing\nprint(\"\\n\\n\")","84921514":"from transformers import AutoTokenizer\n\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\ntokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n\nprint(\"\\n\\n... FIRST 3 ORIGINAL SENTENCE PAIRS ...\\n\")\nfor i in range(3): print(f\"\\tSENTENCE 1 --> '{raw_datasets['train']['sentence1'][i]}'\\n\\tSENTENCE 2 --> '{raw_datasets['train']['sentence2'][i]}'\\n\")\n\nprint(\"\\n\\n... FIRST 3 TOKENIZED SENTENCE PAIRS ...\\n\")\nfor i in range(3): print(f\"\\tSENTENCE 1 --> '{tokenized_sentences_1[i]}'\\n\\tSENTENCE 2 --> '{tokenized_sentences_2[i]}'\\n\")\n\ne15_s1 = raw_datasets['train']['sentence1'][15]\ne15_s2 = raw_datasets['train']['sentence2'][15]\n    \nprint(\"\\n\\n\\n... ELEMENT 15 - ORIGINAL SENTENCE PAIRS ...\\n\")\nprint(f\"\\tSENTENCE 1 --> '{e15_s1}'\\n\\tSENTENCE 2 --> '{e15_s2}'\\n\")\n\ne15_tokenize_seperately_s1 = tokenizer.convert_ids_to_tokens(tokenizer(e15_s1)[\"input_ids\"])\ne15_tokenize_seperately_s2 = tokenizer.convert_ids_to_tokens(tokenizer(e15_s2)[\"input_ids\"])\ne15_tokenized_together = tokenizer(e15_s1, e15_s2)\ne15_tokens = tokenizer.convert_ids_to_tokens(e15_tokenized_together[\"input_ids\"])\n\nprint(\"\\n\\n\\n... ELEMENT 15 - TOKENIZED SEPERATELY ...\\n\")\nprint(f\"\\tSENTENCE 1 --> {' '.join(e15_tokenize_seperately_s1)}\\n\\tSENTENCE 2 --> {' '.join(e15_tokenize_seperately_s2)}\\n\")\n    \nprint(\"\\n\\n\\n... ELEMENT 15 - TOKENIZED TOGETHER ...\\n\")\nprint(f\"\\tSENTENCES --> {' '.join(e15_tokens)}\\n\")","50192922":"print(\"\\nFIRST TEN TOKENS   :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"input_ids\"][:10]])\nprint(\"FIRST TEN STRINGS  :\", [f\"{x:<4}\" for x in e15_tokens[:10]])\nprint(\"FIRST TEN TYPE IDS :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"token_type_ids\"][:10]])\nprint(\"\\nTEN TO TWENTY TOKENS   :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"input_ids\"][10:20]])\nprint(\"TEN TO TWENTY STRINGS  :\", [f\"{x:<4}\" for x in e15_tokens[10:20]])\nprint(\"TEN TO TWENTY TYPE IDS :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"token_type_ids\"][10:20]])\nprint(\"\\nTWENTY TO THIRTY TOKENS   :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"input_ids\"][20:30]])\nprint(\"TWENTY TO THIRTY STRINGS  :\", [f\"{x:<4}\" for x in e15_tokens[20:30]])\nprint(\"TWENTY TO THIRTY TYPE IDS :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"token_type_ids\"][20:30]])\nprint(\"\\nLAST 14 TOKENS   :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"input_ids\"][30:]])\nprint(\"LAST 14 STRINGS  :\", [f\"{x:<4}\" for x in e15_tokens[30:]])\nprint(\"LAST 14 TYPE IDS :\", [f\"{x:<5}\" for x in e15_tokenized_together[\"token_type_ids\"][30:]], \"\\n\\n\")\n\n\nfor i, (_id, _tt_id) in enumerate(zip(e15_tokenized_together[\"input_ids\"], e15_tokenized_together[\"token_type_ids\"])):\n    print(f\"\\nTOKEN AT POSITION  ** {i} **\")\n    print(f\"\\tTOKEN AS STRING --> {e15_tokens[i]}\")\n    print(f\"\\tTOKEN RAW VALUE --> {_id}\")\n    print(f\"\\tTOKEN TYPE ID   --> {_tt_id}\")","9149da26":"def tokenize_function(example):\n    \"\"\"\n    This function takes a dictionary (like the items of our dataset) and returns a new dictionary \n    with the keys input_ids, attention_mask, and token_type_ids. \n        - Note that it also works if the example dictionary contains several samples \n          (each key as a list of sentences) since the tokenizer works on lists \n          of pairs of sentences, as seen before. \n    \n    This will allow us to use the option batched=True in our call to map(), \n    which will greatly speed up the tokenization. \n        - The tokenizer is backed by a tokenizer written in Rust from the \n          \ud83e\udd17 Tokenizers library. \n        - This tokenizer can be very fast, but only if we give it lots of \n          inputs at once.\n    \n    Note that we\u2019ve left the padding argument out in our tokenization function for now. \n        - This is because padding all the samples to the maximum length is not efficient\n        - It\u2019s better to pad the samples when we\u2019re building a batch, as then we only need to \n          pad to the maximum length in that batch, and not the maximum length in the entire dataset. \n        - This can save a lot of time and processing power when the inputs have variable lengths.\n    \n    Args:\n        example (Dict): A dictionary containing the items of the dataset.\n    \n    Returns:\n        A new dictionary with the keys input_ids, attention_mask, and token_type_ids.\n    \n    \"\"\"\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n\n# Here is how we apply the tokenization function on all our datasets at once. \n# \n# We\u2019re using batched=True in our call to map so the function is applied to \n# multiple elements of our dataset at once, and not on each element separately. \ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets","ea8e4107":"def glue_sst2_preprocessing():\n    \"\"\" wip \"\"\"\n    pass\n    \ndef general_glue_preprocessing():\n    \"\"\" wip \"\"\"\n    pass","497b4925":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\nsamples = tokenized_datasets[\"train\"][:8]\nsamples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n\nprint(\"\\n\\n\\n... LENGTH OF SAMPLES IN BATCH\")\nprint(\"\\t\", [len(x) for x in samples[\"input_ids\"]])\n\nbatch = data_collator(samples)\nprint(\"\\n\\n... SHAPE OF TENSORFLOW ARRAYS IN BATCH AFTER COLLATING\")\nfor k,v in batch.items(): print(f\"\\t{k:<15} ----> {v.shape}\")","3b745eca":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\nsamples = tokenized_datasets[\"train\"][:8]\nsamples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n\nprint(\"\\n\\n\\n... LENGTH OF SAMPLES IN BATCH\")\nprint(\"\\t\", [len(x) for x in samples[\"input_ids\"]])\n\nbatch = data_collator(samples)\nprint(\"\\n\\n... SHAPE OF PYTORCH ARRAYS IN BATCH AFTER COLLATING\")\nfor k,v in batch.items(): print(f\"\\t{k:<15} ----> {v.shape}\")","69812bc2":"try:\n    print(f\"\\n\\n\\n... TRAINING DATASET ...\\n\\n{tf_train_dataset}\")\n    print(f\"\\n\\n\\n... VALIDATION DATASET ...\\n\\n{tf_validation_dataset}\")\n    \nexcept:    \n    # Step 1: Imports\n    from datasets import load_dataset\n    from transformers import AutoTokenizer, DataCollatorWithPadding\n    import numpy as np\n\n    # Step 2: Initialize --> Dataset, Tokenizer (from ckpt)\n    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n    checkpoint = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    # Step 3: Tokenize the entire datasets and batch\n    #             -  Note:  The tokenize Function was defined above and \n    #                       is not copied here for brevity\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n\n    # Step 4: Instantiate the data collator from the tokenizer to return TF tensors\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n\n    # Step 5A: Create the training dataset\n    tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n        label_cols=[\"labels\"],\n        shuffle=True,\n        collate_fn=data_collator,\n        batch_size=8,\n    )\n    \n    # Step 5B: Create the validation dataset\n    tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n        label_cols=[\"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n        batch_size=8,\n    )\n    \n    # See what we have...\n    print(f\"\\n\\n\\n... TRAINING DATASET ...\\n\\n{tf_train_dataset}\")\n    print(f\"\\n\\n\\n... VALIDATION DATASET ...\\n\\n{tf_validation_dataset}\")","0376aa59":"# Import the model class and plotting library\nfrom transformers import TFAutoModelForSequenceClassification\nimport matplotlib.pyplot as plt\n\n# Instantiate the model from previously defined checkpoint - see warning\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\n# Compile the model\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\n# Define the number of epochs and report the batch size\nnum_epochs = 3\nbatch_size = 8\n\n# Train the model\nhistory = model.fit(\n    tf_train_dataset,\n    validation_data=tf_validation_dataset,\n    epochs=num_epochs,\n)\n\nplt.figure(figsize=(20, 14))\nplt.subplot(2, 1, 1)\nplt.title(\"\\n\\nAccuracy\", fontweight=\"bold\")\nl_1 = plt.plot(history.history[\"accuracy\"], \"-b\", label=\"Training\")\nl_2 = plt.plot(history.history[\"val_accuracy\"], \"-r\", label=\"Validation\")\nplt.ylim(0, 1.0)\nplt.xlabel(\"Epoch #\", fontweight=\"bold\")\nplt.ylabel(\"Accuracy %\", fontweight=\"bold\")\nplt.legend()\nplt.subplot(2, 1, 2)\nplt.title(\"\\n\\nLoss\", fontweight=\"bold\")\nplt.plot(history.history[\"loss\"], \"-b\", label=\"Training\")\nplt.plot(history.history[\"val_loss\"], \"-r\", label=\"Validation\")\nplt.xlabel(\"Epoch #\", fontweight=\"bold\")\nplt.ylabel(\"Loss Value\", fontweight=\"bold\")\nplt.legend()\nplt.tight_layout()\nplt.show()","4dadb50e":"# The number of training steps is the number of samples in the dataset, \n# divided by the batch size then multiplied by the total number of epochs\n#     - Because the dataset is already batched we don't need to divide \nnum_train_steps = len(tf_train_dataset) * num_epochs\nlr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=5e-5, \n    end_learning_rate=0.0,\n    decay_steps=num_train_steps\n)\nopt = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n\n# Plot the learning rate\nplt.figure(figsize=(20,10))\nplt.plot(np.arange(num_train_steps), [lr_scheduler(i) for i in range(num_train_steps)])\nplt.grid(True)\nplt.title(\"Lesrning Rate Scheduler - Polynomial Decay (Linear)\", fontweight=\"bold\")\nplt.xlabel(\"Step\", fontweight=\"bold\")\nplt.ylabel(\"Learning Rate\", fontweight=\"bold\")\nplt.show()\n\n# Redeclare the model, compile and fit\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\nhistory = model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs)\n\nplt.figure(figsize=(20, 14))\nplt.subplot(2, 1, 1)\nplt.title(\"\\n\\nAccuracy\", fontweight=\"bold\")\nl_1 = plt.plot(history.history[\"accuracy\"], \"-b\", label=\"Training\")\nl_2 = plt.plot(history.history[\"val_accuracy\"], \"-r\", label=\"Validation\")\nplt.ylim(0.75, 1.0)\nplt.xlabel(\"Epoch #\", fontweight=\"bold\")\nplt.ylabel(\"Accuracy %\", fontweight=\"bold\")\nplt.legend()\nplt.subplot(2, 1, 2)\nplt.title(\"\\n\\nLoss\", fontweight=\"bold\")\nplt.plot(history.history[\"loss\"], \"-b\", label=\"Training\")\nplt.plot(history.history[\"val_loss\"], \"-r\", label=\"Validation\")\nplt.xlabel(\"Epoch #\", fontweight=\"bold\")\nplt.ylabel(\"Loss Value\", fontweight=\"bold\")\nplt.legend()\nplt.tight_layout()\nplt.show()","1f8ecb2a":"from datasets import load_metric\nimport gc # garbage collector\n\npreds = model.predict(tf_validation_dataset)[\"logits\"]\nclass_preds = tf.argmax(preds, axis=1)\n\nprint(f\"Raw Prediction Shape   : {preds.shape}\")\nprint(f\"Class Prediction Shape : {class_preds.shape}\")\n\nmetric = load_metric(\"glue\", \"mrpc\")\nprint(f\"Validation Metrics:\\n\\t--> {metric.compute(predictions=class_preds, references=raw_datasets['validation']['label'])}\")\n\n\n# CLEANUP\ntf.keras.backend.clear_session(); gc.collect(); gc.collect();","33548ac0":"try:\n    if data_collator.return_tensors==\"pt\":\n        print(f\"\\n\\n\\n... DATA COLLATOR ...\\n\\n{data_collator}\")\n        print(f\"\\n\\n\\n... TOKENIZED DATASETS ...\\n\\n{tokenized_datasets}\")\n    else:\n        raise ValueError()\nexcept:    \n    # Step 1: Imports\n    from datasets import load_dataset\n    from transformers import AutoTokenizer, DataCollatorWithPadding\n    import numpy as np\n\n    # Step 2: Initialize --> Dataset, Tokenizer (from ckpt)\n    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n    checkpoint = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    # Step 3: Tokenize the entire datasets and batch\n    #             -  Note:  The tokenize Function was defined above and \n    #                       is not copied here for brevity\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n\n    # Step 4: Instantiate the data collator from the tokenizer to return TF tensors\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n    \n    # See what we have...\n    print(f\"\\n\\n\\n... DATA COLLATOR ...\\n\\n{data_collator}\")\n    print(f\"\\n\\n\\n... TOKENIZED DATASETS ...\\n\\n{tokenized_datasets}\")","943590bd":"from transformers import TrainingArguments\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import Trainer\n\ntraining_args = TrainingArguments(\"test-trainer\", \n                                  num_train_epochs=3,\n                                  report_to=\"none\") #report_to prevents wandbe\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\ntrainer.train()","b7b8be7c":"from datasets import load_metric\n\ndef compute_metrics(eval_preds):\n    \"\"\" Wrapper function for computing metrics \n    \n    Args:\n        eval_preds: TBD\n        \n    Returns:\n        TBD\n    \"\"\"\n    metric = load_metric(\"glue\", \"mrpc\")\n    \n    if len(eval_preds)==3:\n        logits, labels, _ = eval_preds\n    else:\n        logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Get raw predictions along with ground truth labels\npredictions = trainer.predict(tokenized_datasets[\"validation\"])\nclass_preds = np.argmax(predictions.predictions, axis=-1)\n\nprint(f\"Raw Prediction Shape    : {predictions.predictions.shape}\")\nprint(f\"Class Labels Shape      : {predictions.label_ids.shape}\")\nprint(f\"Class Predictions Shape : {class_preds.shape}\")\nmetric = load_metric(\"glue\", \"mrpc\")\nprint(f\"\\nValidation Metrics:\\n\\t--> {metric.compute(predictions=class_preds, references=predictions.label_ids)}\")\n\n\nprint(f\"\\n\\n... COMPUTE METRICS WRAPPER FUNCTION:\\n\\t--> {compute_metrics(predictions)}\")","12372f92":"training_args = TrainingArguments(\"test-trainer\", \n                                  num_train_epochs=3,\n                                  report_to=\"none\",\n                                  evaluation_strategy=\"epoch\")\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n\n# --------- Try It Out ---------\n#\n#       >>>PLACEHOLDER<<<\n#       >>>PLACEHOLDER<<<\n#       >>>PLACEHOLDER<<<\n#\n# --------- Try It Out ---------","087a8276":"from torch.utils.data import DataLoader\n\nprint(f\"\\n\\n\\nORIGINAL TOKENIZED DATASET COLUMNS & NAMES:\\n\\t--> {tokenized_datasets['train'].column_names}\")\n\n#  Remove the columns corresponding to values the model does not expect\ntokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n\n# Rename the column label to labels\n#      - Because the model expects the argument to be named `labels`\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n\n# Set the format of the datasets so they return PyTorch tensors\ntokenized_datasets.set_format(\"torch\")\n\n# Validate that our column names are correct\nprint(f\"\\n\\n\\nPREPROCESSED TOKENIZED DATASET COLUMNS & NAMES:\\n\\t--> {tokenized_datasets['train'].column_names}\")\n\n# Insantiate our dataloaders\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n)\n\n# check there is no mistake in the data processing by inspecting a single batch\nprint(\"\\n\\n\\nCHECK DATA LOADER BY INSPECTING A SINGLE BATCH:\")\nfor batch in train_dataloader:\n    for k,v in batch.items(): print(f\"\\t{k:<15} --> {v.shape}\")\n    break","76350357":"from transformers import AutoModelForSequenceClassification\nfrom transformers import AdamW\nfrom transformers import get_scheduler\n\n# Instantiate the model using a checkpoint and define the number of label categories\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\n# Test the model\noutputs = model(**batch)\nprint(f\"\\n\\n\\n... MODEL OUTPUT SHAPE : {outputs.logits.shape} \")\nprint(f\"... MODEL OUTPUT LOSS  : {outputs.loss} \")\n\n# Instantiate our optimizer using the defaults\noptimizer = AdamW(model.parameters(), lr=5e-5)\nprint(f\"\\n\\n\\n... ADAMW OPTIMIZER OBJECT : {optimizer} \")\n\n# Instantiate our learning rate scheduler using defaults\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\nprint(f\"\\n\\n\\n... LEARNING RATE SCHEDULER OBJECT   : {lr_scheduler} \")\nprint(f\"... LEARNING RATE SCHEDULE # OF STEPS : {num_training_steps} \")","947ece98":"import torch\nfrom tqdm.auto import tqdm\n\n# If GPU available ensure training occurs on it... otherwise fallback to CPU\n#      - Get device\n#      - Push model to device (CPU or GPU)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Check which device we are using\nprint(f\"\\n\\n\\n... TRAINING WILL OCCUR USING {device} ...\\n\")\n\n# Define the progress bar for training based on # of steps\nprogress_bar = tqdm(range(num_training_steps))\n\n# ----------------------------------------------------------\n# --------------------- TRAINING LOOP ---------------------\n# ----------------------------------------------------------\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n# ----------------------------------------------------------","43b50df8":"from datasets import load_metric\n\n# Load the relevant MRPC metric\nmetric = load_metric(\"glue\", \"mrpc\")\n\n# -----------------------------------------------------------\n# -------------------- VALIDATION LOOP ----------------------\n# -----------------------------------------------------------\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n# -----------------------------------------------------------\n    \n# Compute the validation metrics based on our conducted loop\nprint(\"\\n\\n\\n... MODEL METRICS - GLUE\/MRPC\")\nfor k,v in metric.compute().items(): print(f\"\\t{k:<8} --> {v}\")\n","3aed8c87":"# --------- Try It Out ---------\n#\n#       >>>PLACEHOLDER<<<\n#       >>>PLACEHOLDER<<<\n#       >>>PLACEHOLDER<<<\n#\n# --------- Try It Out ---------","80d1a79d":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","44da581d":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">BASIC TRAINING<\/b>\n\n<p style=\"font-family: Georgia;\"><span style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">TensorFlow<\/span> models imported from \ud83e\udd17 Transformers are already Keras models. Here is a short introduction to Keras.<\/p>\n\n- <a href=\"https:\/\/youtu.be\/P-rZWqcB6CE\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - KERAS INTRODUCTION - HUGGING FACE CHANNEL<\/a>\n\n<p style=\"font-family: Georgia;\">That means that once we have our data, very little work is required to begin training on it.<\/p>\n\n- <a href=\"https:\/\/youtu.be\/P-rZWqcB6CE\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - FINE-TUNING WITH KERAS - HUGGING FACE CHANNEL<\/a>\n\n<p style=\"font-family: Georgia;\">As in the <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">previous chapter<\/a><\/b>, we will use the <b><code>TFAutoModelForSequenceClassification<\/code><\/b> class, with two labels. You will notice that unlike in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">Chapter 2<\/a><\/b>, you get a warning after instantiating this pretrained model. This is because <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been inserted instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.<\/p>\n\n<p style=\"font-family: Georgia;\">To fine-tune the model on our dataset, we just have to <b><code>compile()<\/code><\/b> our model and then pass our data to the <b><code>fit()<\/code><\/b> method. This will start the fine-tuning process (which should take a couple of minutes on a GPU) and report training loss as it goes, plus the validation loss at the end of each epoch.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; padding-left: 5em; padding-right: 5em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>Note that \ud83e\udd17 Transformers models have a special ability that most Keras models don\u2019t - they can automatically use an appropriate loss which they compute internally. They will use this loss by default if you don\u2019t set a loss argument in compile().<br><br>Note that to use the internal loss you\u2019ll need to pass your labels as part of the input, not as a separate label, which is the normal way to use labels with Keras models. You\u2019ll see examples of this in Part 2 of the course, where defining the correct loss function can be tricky. <br><br>For sequence classification, however, a standard Keras loss function works fine, so that\u2019s what we\u2019ll use here.<\/b><br><br>\n<\/div><\/center>\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; padding-left: 5em; padding-right: 5em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>Note a very common pitfall here \u2014 you can just pass the name of the loss as a string to Keras, but by default Keras will assume that you have already applied a softmax to your outputs. Many models, however, output the values right before the softmax is applied, which are also known as the logits. We need to tell the loss function that that\u2019s what our model does, and the only way to do that is to call it directly, rather than by name with a string.<\/b><br><br>\n<\/div><\/center>","842532e8":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPARE FOR TRAINING - THE MODEL<\/b>\n\n<p style=\"font-family: Georgia;\">Now that we\u2019re completely finished with data preprocessing (a satisfying yet elusive goal for any ML practitioner), let\u2019s turn to the model.<\/p>\n\n<p style=\"font-family: Georgia;\">Firstly we must instantiate the model and pass a batch to make sure that everything will go smoothly during training. All \ud83e\udd17 Transformers models will return the loss when labels are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2).\n\n<p style=\"font-family: Georgia;\">We\u2019re almost ready to write our training loop! We\u2019re just missing two things: an <b><i>optimizer<\/i><\/b> and a <b><i>learning rate scheduler<\/i><\/b>. Since we are trying to replicate what the <b><code>Trainer<\/code><\/b> was doing by hand, we will use the same defaults.<\/p> \n<ul style=\"font-family: Georgia;\">\n    <li>The optimizer used by the <b><code>Trainer<\/code><\/b> is <b><code>AdamW<\/code><\/b>, which is the same as Adam, but with a twist for weight decay regularization (see <b><a href=\"https:\/\/arxiv.org\/abs\/1711.05101\">\u201cDecoupled Weight Decay Regularization\u201d<\/a><\/b> by Ilya Loshchilov and Frank Hutter).<\/li> \n    <li>The learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The <b><code>Trainer<\/code><\/b> uses three epochs by default, so we will follow that<\/li>\n<\/ul><br>","26034444":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">IMPROVING TRAINING PERFORMANCE<\/b>\n\n<ul><li><a href=\"https:\/\/youtu.be\/cpzq6ESSM5c\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - LEARNING RATE SCHEDULING WITH TENSORFLOW - HUGGING FACE CHANNEL<\/a><\/li><\/ul>\n\n<p style=\"font-family: Georgia;\">If you try the above code, it certainly runs, but you\u2019ll find that the loss declines only slowly or sporadically. The primary cause is the <b><i>learning rate<\/i><\/b>. As with the loss, when we pass Keras the name of an optimizer as a string, Keras initializes that optimizer with default values for all parameters, including learning rate (or, in the case above, we instantiated it from the correct class but did not overwrite the default learning rate value). This is usually ok, however, we know that transformer models benefit from a much lower learning rate than the default for Adam.<\/p>\n<ul style=\"font-family: Georgia;\">\n    <li>Adam's default starting learning rate is <b><code>1e-3<\/code><\/b>, also written as 10 to the power of -3, or <b>0.001<\/b>.<\/li>\n    <li>Transformers often start their learning rates around <b><code>5e-5<\/code><\/b> <b>(0.00005)<\/b>, which is some twenty times lower than the default!<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">In addition to lowering the learning rate, we have a second trick up our sleeve: We can slowly reduce the learning rate over the course of training. In the literature, you will sometimes see this referred to as <b><i>decaying<\/i><\/b> or <b><i>annealing<\/i><\/b> the learning rate. In Keras, the best way to do this is to use a <b><i>learning rate scheduler<\/i><\/b>. A good one to use is <b><code>PolynomialDecay<\/code><\/b> \u2014 despite the name, with default settings it simply linearly decays the learning rate from the initial value to the final value over the course of training, which is exactly what we want. In order to use a scheduler correctly, though, we need to tell it how long training is going to be (<b><code>num_train_steps<\/code><\/b>)<\/p>\n\n<p style=\"font-family: Georgia;\">Once we have our all-new optimizer, we can try training with it similar to before. We will first need to reload the model, to reset the changes to the weights from the training run we just did, and then we can compile it with the new optimizer and fit it on our datasets.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; padding-left: 5em; padding-right: 5em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>The \ud83e\udd17 Transformers library also has a <code style=\"color: darkgreen;\">create_optimizer()<\/code> function that will create an <i>AdamW Optimizer<\/i> with learning rate decay. This is a convenient shortcut that you\u2019ll see in detail in future sections of the course.<\/b><br><br>\n<\/div><\/center>\n\n<center><div class=\"alert alert-block alert-success\" style=\"padding-left: 5em; padding-right: 5em; margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\ud83d\udca1 &nbsp;&nbsp; If you want to automatically upload your model to the Hub during training, you can pass along a <code style=\"color: darkgreen;\">PushToHubCallback<\/code> in the <code style=\"color: darkgreen;\">model.fit()<\/code> method. We will learn more about this in <a href=\"#toc\">Chapter 4<\/a><\/b><br><br>\n<\/div><\/center>","c198a6e2":"<br><b style=\"color: gray; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">LIBRARY AGNOSTIC<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","1aeeb592":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">EVALUATION<\/b>\n\n<p style=\"font-family: Georgia;\">Let\u2019s see how we can build a useful <b><code>compute_metrics()<\/code><\/b> function and use it the next time we train. The function must take an <b><code>EvalPrediction object<\/code><\/b> (which is a named tuple with a predictions field and a <b><code>label_ids<\/code><\/b> field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the <b><code>Trainer.predict()<\/code><\/b> command.<\/p>\n\n<p style=\"font-family: Georgia;\">The output of the <b><code>predict()<\/code><\/b> method is another named tuple with three fields: <b><code>predictions<\/code><\/b>, <b><code>label_ids<\/code><\/b>, and <b><code>metrics<\/code><\/b>. The <b><code>metrics<\/code><\/b> field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our <b><code>compute_metrics()<\/code><\/b> function and pass it to the <b><code>Trainer<\/code><\/b>, that field will also contain the metrics returned by <b><code>compute_metrics()<\/code><\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">You'll see that the predictions are a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). This array contains the logits for each element of the dataset we passed to <b><code>predict()<\/code><\/b> (as you saw in the <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\/\">previous chapter<\/a><\/b>, all Transformer models return logits). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis<\/p>\n\n<p style=\"font-family: Georgia;\">After transforming our predictions, we will compare them to the labels. To build our <b><code>compute_metric()<\/code><\/b> function, we will rely on the metrics from the \ud83e\udd17 Datasets library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the <b><code>load_metric()<\/code><\/b> function. The object returned has a <b><code>compute()<\/code><\/b> method we can use to do the metric calculation.<\/p>\n\n<p style=\"font-family: Georgia;\">The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. When you calculate the model performance, you will see that it has an accuracy of 85-86% on the validation set and an F1 score of 89-90. These are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the <b><a href=\"https:\/\/arxiv.org\/pdf\/1810.04805.pdf\">BERT<\/a><\/b> paper reported an F1 score of 88.9 for the base model. That was the uncased model while we are currently using the cased model, which explains the better result.<\/p>\n\n<p style=\"font-family: Georgia;\">Wrapping everything together, we get our <b><code>compute_metrics()<\/code><\/b> function. And to see it used in action to report metrics at the end of each epoch, here is how we define a new Trainer with this <b><code>compute_metrics()<\/code><\/b> function:<\/p>","7d6a4bca":"<img src=\"https:\/\/repository-images.githubusercontent.com\/155220641\/a16c4880-a501-11ea-9e8f-646cf611702e\"><\/img>\n\n---\n\n<p style=\"font-family: Georgia;\">This collection of notebooks will walk you through the entire <a src=\"https:\/\/huggingface.co\/course\/chapter0\/1?fw=tf\" style=\"font-weight: bold;\"><b>Hugging Face Transformers Course:<\/b><\/a><\/p>\n\n---\n\n<b style=\"font-family: Georgia;\">Links To Chapter Notebooks&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color: red; font-size: 11px;\">(This Notebook Will Cover Chapter 3)<\/span><\/b>\n<ul style=\"font-family: Georgia;\">\n    <li><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch#chapter_0\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">0<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">SEtUP<\/span><\/b><\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch#chapter_1\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">1<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Transformer Models<\/span><\/b><\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">2<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Using \ud83e\udd17 Transformers<\/span><\/b><\/a><\/li>\n    <li><a href=\"#toc\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">3<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Fine-Tuning A Pretrained Model<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">4<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Sharing Models And Tokenizers<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">5<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">The \ud83e\udd17 Datasets Library<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">6<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">The \ud83e\udd17 Tokenizers Library<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">7<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Main NLP Tasks<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">8<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">How To Ask For Help<\/span><\/b><\/a><\/li>    \n<\/ul>\n\n---\n\n<p style=\"font-family: Georgia;\">To find the original course please <a src=\"https:\/\/huggingface.co\/course\/chapter0\/1?fw=tf\" style=\"font-weight: bold; text-decoration: underline;\">>>>click here<<<<\/a><\/p>\n    \n---\n    \n<br>\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;The vast majority of the text in this notebook will come directly from the HuggingFace Transformers course. If I would like to add in (or change anything), I will insert the information in a blue-box similar to this one.<\/b><br><br>\n<\/div><\/center>\n   \n    \n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;Hugging Face uses green blocks to inject notation into their course.<\/b><br><br>\n<\/div><\/center>\n","7d62418f":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">MODEL PREDICTIONS<\/b>\n\n<ul><li><a href=\"https:\/\/youtu.be\/nx10eh4CoOs\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - PREDICTIONS AND METRICS - HUGGING FACE CHANNEL<\/a><\/li><\/ul>\n\n<p style=\"font-family: Georgia;\">Training and watching the loss go down is all very nice, but what if we want to actually get outputs from the trained model, either to compute some metrics, or to use the model in production? To do that, we can just use the <b><code>predict()<\/code><\/b> method. This will return the logits from the output head of the model, one per class. We can convert these logits into the model\u2019s class predictions by using <b><code>argmax<\/code><\/b> to find the highest logit, which corresponds to the most likely class.<\/p>\n\n<p style=\"font-family: Georgia;\">Now, we can use those predictions to compute some metrics! We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the <b><code>load_metric()<\/code><\/b> function. The object returned has a <b><code>compute()<\/code><\/b> method we can use to do the metric calculation.<\/p>\n\n<p style=\"font-family: Georgia;\">The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85 to 86% on the validation set and an F1 score of 89 to 90. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the BERT paper reported an F1 score of 88.9 for the base model. That was the uncased model while we are currently using the cased model, which explains the better result.<\/p>\n\n<p style=\"font-family: Georgia;\">This concludes the introduction to fine-tuning using the Keras API. An example of doing this for most common NLP tasks will be given in <b><a href=\"toc\">Chapter 7<\/a><\/b>. If you would like to hone your skills on the Keras API, try to fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in <b><a href=\"#3_1\">Section 3.1<\/a><\/b><\/p>","5817e45b":"<a id=\"3_3\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">3.3 FINE-TUNING A MODEL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b style=\"color: #EE4C2C; font-family: Verdana; font-size: 16px; letter-spacing: 0.35em;\">PYTORCH TRAINER API<\/b>&nbsp;&nbsp;<b style=\"color: black; font-family: Verdana; font-size: 16px; letter-spacing: 0.2em;\">VERSION<\/b>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_3\">&#10514;<\/a><\/h3>\n\n---\n\n- <a href=\"https:\/\/youtu.be\/nvBXf7s7vTI\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - THE TRAINER API - HUGGING FACE CHANNEL<\/a><br>\n\n\n<p style=\"font-family: Georgia;\">\ud83e\udd17 Transformers provides a <b><code>Trainer<\/code><\/b> class to help you fine-tune any of the pretrained models it provides on your dataset. Once you\u2019ve done all the data preprocessing work in the last section, you have just a few steps left to define the <b><code>Trainer<\/code><\/b>. The hardest part is likely to be preparing the environment to run <b><code>Trainer.train()<\/code><\/b>, as it will run very slowly on a CPU. If you don\u2019t have a GPU set up, you can get access to free GPUs or TPUs on Kaggle and Google Colab.<\/p>\n\n<p style=\"font-family: Georgia;\"><p style=\"font-family: Georgia;\">The code examples below assume you have already executed the examples in the previous section. The code below will use a <b><code>try\/except<\/code><\/b> to validate the datasets have been created. The <b><code>except<\/code><\/b> branch of this code provides an excellent recap of the required steps from the previous section.<\/p>","2ec911f4":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase;\">BUT WE ONLY HAVE TWO SENTENCES...<\/b>\n\n<p style=\"font-family: Georgia;\">Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.<\/p>\n\n<p style=\"font-family: Georgia;\">In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We\u2019ve selected it for this chapter because it\u2019s a small dataset, so it\u2019s easy to experiment with training on it.<\/p>","ceaa50ef":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">THE EVALUATION LOOP<\/b>\n\n<p style=\"font-family: Georgia;\">As we did earlier, we will use a metric provided by the \ud83e\udd17 Datasets library. We\u2019ve already seen the <b><code>metric.compute()<\/code><\/b> method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method <b><code>add_batch()<\/code><\/b>. Once we have accumulated all the batches, we can get the final result with <b><code>metric.compute()<\/code><\/b>. Here\u2019s how to implement all of this in an evaluation loop.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp; Modify the previous training loop to fine-tune your model on the SST-2 dataset.<\/b><br><br>\n<\/div><\/center>","ff75401c":"<a id=\"3_4\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">3.4 TRAINING WITH PURE&nbsp;&nbsp;&nbsp;&nbsp;<b style=\"color: #EE4C2C; font-family: Verdana; font-size: 16px; letter-spacing: 0.35em;\">PYTORCH<\/b>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_3\">&#10514;<\/a><\/h3>\n\n---\n\n- <a href=\"https:\/\/youtu.be\/Dh9CL8fyG80\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - TRAINING LOOP - HUGGING FACE CHANNEL<\/a><br>\n\n\n<p style=\"font-family: Georgia;\">Now we\u2019ll see how to achieve the same results as we did in the last section without using the <b><code>Trainer<\/code><\/b> class. Again, we assume you have done the data processing in <b><a href=\"#3_1\">Section 3.1<\/a><\/b>. Here is a short summary covering all the concepts\/topics\/objects you will need.<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>Loading datasets from the Hub<\/li>\n    <li>Loading a pretrained <b><code>tokenizer<\/code><\/b> from a checkpoint<\/li>\n    <li>Tokenizing our dataset (<b><code>.map()<\/code><\/b>) using our previously created <b><code>tokenize_function<\/code><\/b><\/li>\n    <li>Instantiating a data collator using <b><code>DataCollatorWithPadding<\/code><\/b> and passing our <b><code>tokenizer<\/code><\/b><\/li>\n<\/ul>\n\n<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPARE FOR TRAINING - PREPROCESS THE TOKENIZED DATASET<\/b>\n\n<p style=\"font-family: Georgia;\">Before actually writing our training loop, we will need to define a few objects. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our <b><code>tokenized_datasets<\/code><\/b>, to take care of some things that the <b><code>Trainer<\/code><\/b> did for us automatically. Specifically, we need to:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>Remove the columns corresponding to values the model does not expect (like the <b><code>sentence1<\/code><\/b> and <b><code>sentence2<\/code><\/b> columns).<\/li>\n    <li>Rename the column <b><code>label<\/code><\/b> to <b><code>labels<\/code><\/b> (because the model expects the argument to be named <b><code>labels<\/code><\/b>).<\/li>\n    <li>Set the format of the datasets so they return PyTorch tensors instead of lists.<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">Our <b><code>tokenized_datasets<\/code><\/b> has one method for each of those steps. Once we have preprocessed our <b><code>tokenized_datasets<\/code><\/b> object, we can easily check that the result only has columns that our model will accept.<\/p>\n    \n<p style=\"font-family: Georgia;\">After verifying our <b><code>tokenized_datasets<\/code><\/b> object has been preprocessed correctly, we can then define our dataloaders and produce batches to validate that everything looks correct.<\/p>\n","bed11451":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","510191d5":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">THE TRAINING LOOP<\/b>\n\n<p style=\"font-family: Georgia;\">One last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a device we will put our model and our batches on.<\/p>\n\n<p style=\"font-family: Georgia;\">We are now ready to train! To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the tqdm library. You'll see that the core of the training loop looks a lot like the one in the introduction. We didn\u2019t ask for any reporting, so this training loop will not tell us anything about how the model fares. We need to add an evaluation loop for that.<\/p>","a63516f8":"<p id=\"toc\"><\/p>\n<a id=\"chapter_3\"><\/a>\n\n<br>\n\n<h1 style=\"font-family: Georgia; font-size: 30px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\">TABLE OF CONTENTS<\/h1>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Georgia; font-size: 24px; font-style: normal; font-weight: bolder; text-decoration: none; text-transform: none; letter-spacing: 2px; color:  navy; background-color: #ffffff;\"><a href=\"#toc\">CHAPTER &nbsp;#3&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp;TRANSFORMER MODELS<\/a><\/h2>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#3_0\">3.0 INTRODUCTION<\/a><\/h3>\n\n\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#3_1\">3.1 PROCESSING THE DATA<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#3_2\">3.2 FINE-TUNING A MODEL &nbsp;&nbsp;<span style=\"color: #ff6f00; font-weight: bold;\">[TENSORFLOW & KERAS]<\/span><\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#3_3\">3.3 FINE-TUNING A MODEL &nbsp;&nbsp;<span style=\"color: #EE4C2C; font-weight: bold;\">[PYTORCH TRAINER API]<\/span><\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#3_4\">3.4 FULL MODEL TRAINING &nbsp;&nbsp;<span style=\"color: #EE4C2C; font-weight: bold;\">[PYTORCH]<\/span><\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#3_5\">3.5 FINE-TUNING, CHECK!<\/a><\/h3>","0baa16ca":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPROCESSING A DATASET - TOKNEIZE AN ENTIRE DATASET<\/b>\n\n<p style=\"font-family: Georgia;\">Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">previous chapter<\/a><\/b>, we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">Chapter 2<\/a><\/b>.<\/p>\n    \n<p style=\"font-family: Georgia;\">This works well, but it has the disadvantage of returning a dictionary (with our keys: <b><code>input_ids<\/code><\/b>, <b><code>attention_mask<\/code><\/b>, and <b><code>token_type_ids<\/code><\/b>, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the \ud83e\udd17 Datasets library are <b><a href=\"https:\/\/arrow.apache.org\/\">Apache Arrow<\/a><\/b> files stored on the disk, so you only keep the samples you ask for loaded in memory).<\/p>\n\n<p style=\"font-family: Georgia;\">To keep the data as a dataset, we will use the <b><code>Dataset.map()<\/code><\/b> method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The <b><code>map()<\/code><\/b> method works by applying a function on each element of the dataset.<\/p>\n\n<p style=\"font-family: Georgia;\">The way the \ud83e\udd17 Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function.<\/p>\n\n<p style=\"font-family: Georgia;\">You can even use multiprocessing when applying your preprocessing function with map() by passing along a num_proc argument. We didn\u2019t do this here because the \ud83e\udd17 Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.<\/p>\n\n<p style=\"font-family: Georgia;\">Take for example the code in the next cell, the <b><code>tokenize_function<\/code><\/b> returns a dictionary with the keys: <b><code>input_ids<\/code><\/b>, <b><code>attention_mask<\/code><\/b>, and <b><code>token_type_ids<\/code><\/b>, so those three fields are added to all splits of our dataset. Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied <b><code>map()<\/code><\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together \u2014 a technique we refer to as <b><i>dynamic padding<\/i><\/b>... we will discuss this in the next section.<\/p>","995bd296":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">TRAINING WITH BETTER EVALUATION<\/b>\n\n<p style=\"font-family: Georgia;\">Now that we've created our <b><code>compute_metrics<\/code><\/b> wrapper function, we can use it to report metrics at the end of each epoch. We will shortly define a new <b><code>Trainer<\/code><\/b> with this <b><code>compute_metrics<\/code><\/b> function. Note that we create a new <b><code>TrainingArguments<\/code><\/b> with its <b><code>evaluation_strategy<\/code><\/b> set to \"epoch\" and a new model \u2014 otherwise, we would just be continuing the training of the model we have already trained.<\/p>\n\n<p style=\"font-family: Georgia;\">This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy\/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark.<\/p>\n\n<p style=\"font-family: Georgia;\">The <b><code>Trainer<\/code><\/b> will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use <b><code>fp16 = True<\/code><\/b> in your training arguments). We will go over everything it supports in <b><a href=\"#toc\">Chapter 10<\/a><\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">This concludes the introduction to fine-tuning using the <b><code>Trainer<\/code><\/b> API. An example of doing this for most common NLP tasks will be given in <b><a href=\"#toc\">Chapter 7<\/a><\/b>, but for now let\u2019s look at how to do the same thing in <i><b>pure PyTorch<\/b><\/i>.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp; Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in <a href=\"#3_1\">Section 3.1<\/a><\/b><br><br>\n<\/div><\/center>","db35ee70":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPROCESSING A DATASET - TOKENIZING AN ENTIRE DATASET<\/b>\n\n<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 16px; letter-spacing: 0.35em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 16px; letter-spacing: 0.2em;\">&nbsp;NEXT STEPS<\/b>\n\n<p style=\"font-family: Georgia;\">Now that we have our dataset and a data collator, we need to put them together. We could manually load batches and collate them, but that\u2019s a lot of work, and probably not very performant either. Instead, there\u2019s a simple method that offers a performant solution to this problem: <b><code>to_tf_dataset()<\/code><\/b>. This will wrap a <b><code>tf.data.Dataset<\/code><\/b> around your dataset, with an optional collation function. <b><code>tf.data.Dataset<\/code><\/b> is a native TensorFlow format that Keras can use for <b><code>model.fit()<\/code><\/b>, so this one method immediately converts a \ud83e\udd17 Dataset to a format that\u2019s ready for training.<\/p>\n\n<p style=\"font-family: Georgia;\">Once this has been completed, we can take the processed datasets forward into the next section, where training will be pleasantly straightforward after all the hard work of data preprocessing.<\/p>\n\n<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 16px; letter-spacing: 0.35em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 16px; letter-spacing: 0.2em;\">&nbsp;NEXT STEPS<\/b>\n\n<p style=\"font-family: Georgia;\">The Pytorch <b><code>Trainer<\/code><\/b> API will use the dataset and collator in their current state for training\/fine-tuning, so no further coercion is required. We will explore how to finetune a model using this API as well as how to train <b>without<\/b> leveraging this API.<\/p>\n","2f34d0b4":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPROCESSING A DATASET - TOKEN TYPE IDS<\/b>\n\n<p style=\"font-family: Georgia;\">We discussed the <b><code>input_ids<\/code><\/b> and <b><code>attention_mask<\/code><\/b> keys in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">Chapter 2<\/a><\/b>, but we put off talking about <b><code>token_type_ids<\/code><\/b>. In the following example, this is what will tell the model which part of the input is the first sentence and which is the second sentence.<\/p>\n\n<p style=\"font-family: Georgia;\">In the previous code example, we saw that when we tokenized the sentences together, we received a slightly different output result. In the next code cell we will take the previous code example and align the <b><code>token_type_ids<\/code><\/b> with the decoded words.<\/p>\n\n<b><code><span style=\"color: darkblue\">[CLS]<\/span> <span style=\"color: darkgreen\">sentence1<\/span> <span style=\"color: darkblue\">[SEP]<\/span> <span style=\"color: darkred\">sentence2<\/span> <span style=\"color: darkblue\">[SEP]<\/span><\/code><\/b>\n\n<p style=\"font-family: Georgia;\">As you can see, the parts of the input corresponding to <b><code><span style=\"color: darkblue\">[CLS]<\/span> <span style=\"color: darkgreen\">sentence1<\/span> <span style=\"color: darkblue\">[SEP]<\/span><\/code><\/b> all have a token type ID of <b><code>0<\/code><\/b>, while the other parts, corresponding to <b><code><span style=\"color: darkred\">sentence2<\/span> <span style=\"color: darkblue\">[SEP]<\/span><\/code><\/b>, all have a token type ID of <b><code>1<\/code><\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">Note that if you select a different checkpoint, you won\u2019t necessarily have the <b><code>token_type_ids<\/code><\/b> in your tokenized inputs (for instance, they\u2019re not returned if you use a <b><a href=\"https:\/\/arxiv.org\/abs\/1910.01108\">DistilBERT<\/a><\/b> model). They are only returned when the model will know what to do with them, because it has seen them during its pretraining.<\/p>\n\n<p style=\"font-family: Georgia;\">Here, <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> is pretrained with token type IDs, and on top of the masked language modeling objective we talked about in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch\">Chapter 1<\/a><\/b>, it has an additional objective called <b><i>next sentence prediction<\/i><\/b>. The goal with this task is to model the relationship between pairs of sentences.<\/p>\n\n<p style=\"font-family: Georgia;\">With next sentence prediction, the model is provided pairs of sentences (with randomly masked tokens) and asked to predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents.<\/p>\n\n<p style=\"font-family: Georgia;\">In general, you don\u2019t need to worry about whether or not there are <b><code>token_type_ids<\/code><\/b> in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.<\/p>","323de887":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">BASIC TRAINING<\/b>\n\n<p style=\"font-family: Georgia;\">The first step before we can define our <b><code>Trainer<\/code><\/b> is to define a <b><code>TrainingArguments<\/code><\/b> class that will contain all the hyperparameters the <b><code>Trainer<\/code><\/b> will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning.<\/p>\n\n<p style=\"font-family: Georgia;\">The second step is to define our model. As in the <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">previous chapter<\/a><\/b>, we will use the <b><code>AutoModelForSequenceClassification<\/code><\/b> class, with two labels. You will notice that unlike in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">Chapter 2<\/a><\/b>, you get a warning after instantiating this pretrained model. This is because <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.<\/p>\n\n<p style=\"font-family: Georgia;\">Once we have our model, we can define a <b><code>Trainer<\/code><\/b> by passing it all the objects constructed up to now \u2014 the model, the <b><code>training_args<\/code><\/b>, the training and validation datasets, our <b><code>data_collator<\/code><\/b>, and our <b><code>tokenizer<\/code><\/b>.<\/p> \n\n<p style=\"font-family: Georgia;\">Note that when you pass the <b><code>tokenizer<\/code><\/b> object, the default <b><code>data_collator<\/code><\/b> used by the <b><code>Trainer<\/code><\/b> will be a <b><code>DataCollatorWithPadding<\/code><\/b> as defined previously, so you can optionally skip this line <b><code>data_collator=data_collator<\/code><\/b> when calling the Trainer. It was still important to show you this part of the processing in <b><a href=\"2_1\">Section 2.1<\/a><\/b>!<\/p>\n\n<p style=\"font-family: Georgia;\">To fine-tune the model on our dataset, we just have to call the <b><code>train()<\/code><\/b> method of our <b><code>Trainer<\/code><\/b>. This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It won\u2019t, however, tell you how well (or badly) your model is performing. This is because:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>We are not telling the <b><code>Trainer<\/code><\/b> to evaluate during training.<\/li><ul>\n        <li><b><code>evaluation_strategy<\/code><\/b> set to <b>\"steps\"<\/b> will evaluate every <b><code>eval_steps<\/code><\/b><\/li>\n        <li><b><code>evaluation_strategy<\/code><\/b> set to <b>\"epoch\"<\/b> will evaluate at the end of each epoch.<\/li><\/ul>\n    <li>We didn\u2019t provide the <b><code>Trainer<\/code><\/b> with a <b><code>compute_metrics()<\/code><\/b> function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number).<\/li>\n<\/ul>","cb2a9e65":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPROCESSING A DATASET - COLLATING AND DYNAMIC PADDING<\/b>\n\n<ul><li><a href=\"https:\/\/www.youtube.com\/watch?v=7q5NyFT8REg\" style=\"font-family: Georgia; color: darkred; font-weight: bold;\">VIDEO LINK - DYNAMIC PADDING - HUGGING FACE CHANNEL<\/a><\/li><\/ul>\n\n<p style=\"font-family: Georgia;\">The function that is responsible for putting together samples inside a batch is called a <b><i>collate function<\/i><\/b>. The default collator is a function that will just convert your samples to <b><code>tf.Tensor<\/code><\/b> and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won\u2019t be possible in our case since the inputs we have won\u2019t all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you\u2019re training on a TPU it can cause problems \u2014 TPUs prefer fixed shapes, even when that requires extra padding.<\/p>\n\n<p style=\"font-family: Georgia;\">To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the \ud83e\udd17 Transformers library provides us with such a function via <b><code>DataCollatorWithPadding<\/code><\/b>. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need.<\/p>\n\n<p style=\"font-family: Georgia;\">To test this new function, we will need to grab a few samples from our training set that we would like to batch together. Before collating them, we will remove the <b><code>columns idx<\/code><\/b>, <b><code>sentence1<\/code><\/b>, and <b><code>sentence2<\/code><\/b> as they won\u2019t be needed and contain strings (and we can\u2019t create tensors with strings)<\/p>\n\n<p style=\"font-family: Georgia;\">No spoilers... but we will get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of our samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b><br><br>Replicate the preprocessing on the GLUE SST-2 dataset. It\u2019s a little bit different since it\u2019s composed of single sentences instead of pairs, but the rest of what we did should look the same.<br><br>For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks.<\/b><br><br>\n<\/div><\/center>","908bb992":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Supercharge your training loop with \ud83e\udd17 Accelerate<\/b>\n\n- <a href=\"https:\/\/youtu.be\/s7dy8QRgjJ0\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - SUPERCHARGE YOUR PYTORCH TRAINING LOOP WITH \ud83e\udd17 ACCELERATE - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\"><b><i>STILL TO BE DONE - CONVERTING THIS TO NOTEBOOK FORMAT ISN'T SUPER STRAIGHTFORWARD<\/i><\/b><\/p>\n","ce5c8122":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","5a44fb52":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPROCESSING A DATASET - SENTENCE PAIRS<\/b>\n\n- <a href=\"https:\/\/youtu.be\/P-rZWqcB6CE\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - PREPROCESSING SENTENCE PAIRS - HUGGING FACE CHANNEL<\/a>\n- <a href=\"https:\/\/youtu.be\/0u3ioSwev3s\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - PREPROCESSING SENTENCE PAIRS - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">previous chapter<\/a><\/b>, this is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair.<\/p>\n\n<p style=\"font-family: Georgia;\">However, we can\u2019t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. <b>We need to handle the two sequences as a pair<\/b>, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> model expects.<\/p>\n\n<p style=\"font-family: Georgia;\">If we decode the IDs inside <b><code>input_ids<\/code><\/b> back to words, we see the model expects the inputs to be of the form <b><code><span style=\"color: darkblue\">[CLS]<\/span> <span style=\"color: darkgreen\">sentence1<\/span> <span style=\"color: darkblue\">[SEP]<\/span> <span style=\"color: darkred\">sentence2<\/span> <span style=\"color: darkblue\">[SEP]<\/span><\/code><\/b> when there are two sentences.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp; Take element 15 of the training set and tokenize the two sentences separately and as a pair. What\u2019s the difference between the two results?<\/b><br><br>\n<\/div><\/center>","ab814307":"<a id=\"3_5\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">3.5 FINE-TUNING, CHECK!&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_3\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">That was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li><b style=\"color: black;\">BOTH<\/b> - Learned about datasets in the <b><a href=\"https:\/\/huggingface.co\/datasets\">Hub<\/a><\/b><\/li>\n    <li><b style=\"color: black;\">BOTH<\/b> - Learned how to load and preprocess datasets, including using dynamic padding and collators<\/li>\n    <li><b style=\"color: #EE4C2C;\">PT<\/b> - Implemented your own fine-tuning and evaluation of a model<\/li>\n    <li><b style=\"color: #ff6f00;\">TF<\/b> - Learned how to fine-tune and evaluate a model with Keras<\/li>\n    <li><b style=\"color: #ff6f00;\">TF<\/b> - Implemented a custom metric<\/li>\n    <li><b style=\"color: #EE4C2C;\">PT<\/b> - Implemented a lower-level training loop<\/li>\n    <li><b style=\"color: #EE4C2C;\">PT<\/b> - <strike>Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs<\/strike> <b>[TBD]<\/b><\/li>\n<\/ul>","86817728":"<a id=\"3_1\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">3.1 PROCESSING THE DATA&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_3\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">Continuing with the example from the <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">previous chapter<\/a><\/b>, here is how we would train a <b>sequence classifier<\/b> on one batch in <b style=\"color: #ff6f00;\">TensorFlow<\/b> and <b style=\"color: #EE4C2C;\">PyTorch<\/b>.<\/p>","48d1d3a6":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","a85e8501":"<a id=\"3_2\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">3.2 FINE-TUNING A MODEL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b style=\"color: #ff6f00; font-family: Verdana; font-size: 16px; letter-spacing: 0.35em;\">TENSORFLOW & KERAS<\/b>&nbsp;&nbsp;<b style=\"color: black; font-family: Verdana; font-size: 16px; letter-spacing: 0.2em;\">VERSION<\/b>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_3\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">Once you\u2019ve done all the data preprocessing work in the last section, you have just a few steps left to train the model. Note, however, that the <b><code>model.fit()<\/code><\/b> command will run very slowly on a CPU. If you don\u2019t have a GPU set up, you can get access to free GPUs or TPUs on Kaggle or on Google Colab.<\/p>\n\n<p style=\"font-family: Georgia;\">The code examples below assume you have already executed the examples in the previous section. The code below will use a <b><code>try\/except<\/code><\/b> to validate the datasets have been created. The <b><code>except<\/code><\/b> branch of this code provides an excellent recap of the required steps from the previous section.<\/p>","b5579753":"<a id=\"3_0\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">3.0 INTRODUCTION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_3\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">In <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">Chapter 2<\/a><\/b> we explored how to use tokenizers and pretrained models to make predictions. But what if you want to fine-tune a pretrained model for your own dataset? That\u2019s the topic of this chapter! You will learn:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li><b style=\"color: black;\">BOTH<\/b> - How to prepare a large dataset from the Hub<\/li>\n    <li><b style=\"color: #ff6f00;\">TF<\/b> - How to use Keras to fine-tune a model<\/li>\n    <li><b style=\"color: #EE4C2C;\">PT<\/b> - How to use the high-level Trainer API to fine-tune a model<\/li>\n    <li><b style=\"color: #ff6f00;\">TF<\/b> - How to use Keras to get predictions<\/li>\n    <li><b style=\"color: #EE4C2C;\">PT<\/b> - How to use a custom training loop<\/li>\n    <li><b style=\"color: #ff6f00;\">TF<\/b> - How to use a custom metric<\/li>\n    <li><b style=\"color: #EE4C2C;\">PT<\/b> - How to leverage the \ud83e\udd17 Accelerate library to easily run that custom training loop on any distributed setup<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">In order to upload your trained checkpoints to the <b><a href=\"https:\/\/huggingface.co\/models\">Hugging Face Hub<\/a><\/b>, you will need a huggingface.co account: <b><a href=\"https:\/\/huggingface.co\/join\">create an account<\/a><\/b><\/p>","6b8a26e6":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase;\">LOADING A DATASET FROM THE HUB<\/b>\n\n- <a href=\"https:\/\/youtu.be\/W_gMJF0xomE\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - DATASETS OVERVIEW - HUGGING FACE CHANNEL<\/a>\n- <a href=\"https:\/\/youtu.be\/_BZearw7f0w\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - DATASETS OVERVIEW - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">The Hub doesn\u2019t just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets <b><a href=\"https:\/\/huggingface.co\/datasets\">here<\/a><\/b>, and we recommend you try to load and process a new dataset once you have gone through this section (see the general documentation <b><a href=\"https:\/\/huggingface.co\/docs\/datasets\/loading_datasets.html#from-the-huggingface-hub\">here<\/a><\/b>). But for now, let\u2019s focus on the <b><a href=\"https:\/\/paperswithcode.com\/dataset\/mrpc\">Microsoft Research Paraphrase Corpus (MRPC) Dataset<\/a><\/b>! This is one of the 10 datasets composing the <b><a href=\"https:\/\/gluebenchmark.com\/\">GLUE Benchmark<\/a><\/b>, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.<\/p>\n\n<p style=\"font-family: Georgia;\">The \ud83e\udd17 Datasets library provides a very simple command to download and cache a dataset on the Hub. We can easily download the MRPC dataset and we will get a <b><code>DatasetDict<\/code><\/b> object which contains the training set, the validation set, and the test set. Each of those contains several columns (<b><code>sentence1<\/code><\/b>, <b><code>sentence2<\/code><\/b>, <b><code>label<\/code><\/b>, and <b><code>idx<\/code><\/b>) and a variable number of rows, which are the number of elements in each set (so, there are <b><code>3,668<\/code><\/b> pairs of sentences in the training set, <b><code>408<\/code><\/b> in the validation set, and <b><code>1,725<\/code><\/b> in the test set).<\/p>\n\n<p style=\"font-family: Georgia;\">Note that when we download the datset, we also automatically cache the dataset, by default in <code>~\/.cache\/huggingface\/dataset<\/code>. Recall from <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-2-tf-torch\">Chapter 2<\/a><\/b> that you can customize your cache folder by setting the <b><code>HF_HOME<\/code><\/b> environment variable.<\/p>\n\n<p style=\"font-family: Georgia;\">Once we have our <b><code>DatasetDict<\/code><\/b>, we can access each pair of sentences by simply indexing, like with a dictionary. When we index into our dataset, we will see the labels are already integers, so we won\u2019t have to do any preprocessing there.<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>To know which integer corresponds to which label, we can inspect the features of our <b><code>raw_train_dataset<\/code><\/b>. This will tell us the type of each column.<\/li> \n    <li>Behind the scenes, label is of type <b><code>ClassLabel<\/code><\/b>, and the mapping of integers to label name is stored in the names folder.\n        <ul style=\"font-family: Georgia;\">\n            <li><b><code>0<\/code><\/b> corresponds to not_equivalent<\/li>\n            <li><b><code>1<\/code><\/b> corresponds to equivalent<\/li>\n        <\/ul>\n    <\/li>\n<\/ul>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp; Look at element 15 of the training set and element 87 of the validation set. What are their labels?<\/b><br><br>\n<\/div><\/center>"}}