{"cell_type":{"1de005b6":"code","776b5425":"code","5cbb5d9a":"code","d5484884":"code","046c5195":"code","ed032651":"code","f0ae6e99":"code","78c68b20":"code","a3cf0fc5":"code","ce5ac347":"code","0d2c7208":"code","2d9f103d":"code","cbc81511":"code","d1b0bbcd":"code","034cb439":"code","1ab7024f":"code","ef18bcc8":"code","9cf0be7e":"code","8131b50f":"code","56e7fb7a":"code","ebe507c7":"code","28453a06":"code","f0e0819d":"code","0c1378fe":"code","4ddf542c":"code","bf0e7116":"code","aefa5b40":"code","eea35124":"code","340bd07b":"code","f1809268":"code","83ff29ba":"code","148eb95b":"markdown","b751289c":"markdown","c8553aea":"markdown","07f8196c":"markdown","b9db7a09":"markdown","f1af5879":"markdown","3f1b078f":"markdown","d9fc3ff5":"markdown","ade3c664":"markdown","433a9fb5":"markdown","e4dfcf4a":"markdown","5a09f19b":"markdown","a828f5c5":"markdown","7670e1cb":"markdown","7d8cf7ca":"markdown","a78a655a":"markdown","31d1cfcc":"markdown","6b186ced":"markdown","c2f919ed":"markdown","6081f22a":"markdown","f07be45a":"markdown","9f8cc2a0":"markdown"},"source":{"1de005b6":"import re\nimport numpy as np\nimport string\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\ndata =\"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\"\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(data)\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 24))\naxes[0].imshow(wordcloud)\naxes[0].axis('off')\naxes[1].imshow(wordcloud)\naxes[1].axis('off')\naxes[2].imshow(wordcloud)\naxes[2].axis('off')\nfig.tight_layout()","776b5425":"sentences = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\"","5cbb5d9a":"# remove special characters\nsentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n\n# remove 1 letter words\nsentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n\n# lower all characters\nsentences = sentences.lower()","d5484884":"# list of words\nwords = sentences.split()\n\n# get vocabulary (set(words)) - unique words\nvocab = set(words)","046c5195":"# word to id\nword_to_ix = {w: i for i, w in enumerate(vocab)}\n\n# id to word\nix_to_word = {i: w for i, w in enumerate(vocab)}","ed032651":"word_to_ix['the']","f0ae6e99":"ix_to_word[83]","78c68b20":"vocab_size = len(set(vocab))\nembedding_dimension = 10","a3cf0fc5":"#embedding matrix = vocabulary_size x embedding_dimension\nembeddings =  np.random.random_sample((vocab_size, embedding_dimension))","ce5ac347":"trigrams = [([words[i], words[i+1]], words[i+2]) for i in range(len(words) - 2)]","0d2c7208":"# inputs = ['word1', 'word2']\n# targets = 'target'\n# ( ['word1', 'word2'], 'target')\ntrigrams[0]","2d9f103d":"def relu(x):\n    return x * (x > 0)\n\ndef drelu(x):\n    return 1. * (x > 0)","cbc81511":"def log_softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return np.log(e_x \/ e_x.sum())","d1b0bbcd":"def NLLLoss(logs, targets):\n    out = logs[range(len(targets)), targets]\n    return -out.sum()\/len(out)","034cb439":"def linear1(x, theta):\n    w1, w2 = theta\n    return np.dot(x, w1.T)\n\ndef linear2(o, theta):\n    w1, w2 = theta\n    return np.dot(o, w2.T)","1ab7024f":"def forward(x, theta):\n    m = embeddings[x].reshape(1, -1)\n    n = linear1(m, theta)\n    o = relu(n)\n    p = linear2(o, theta)\n    q = log_softmax(p)\n    \n    params = m, n, o, p, q\n    return(params)","ef18bcc8":"def log_softmax_crossentropy_with_logits(logits,target):\n\n    out = np.zeros_like(logits)\n    out[np.arange(len(logits)),target] = 1\n    \n    softmax = np.exp(logits) \/ np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- out + softmax) \/ logits.shape[0]","9cf0be7e":"def backward(x, y, theta, params):\n    m, n, o, p, q = params\n    w1, w2 = theta\n    \n    dlog = log_softmax_crossentropy_with_logits(p, y)\n    drelu = relu(n)\n    \n    # dw2 = dlog * o\n    do = np.dot(dlog,w2)\n    dw2 = np.dot(dlog.T, o)\n    \n    # dw1 = do * drelu * m\n    dn = do * drelu\n    dw1 = np.dot(dn.T, m)\n    \n    return dw1, dw2","8131b50f":"def optimize(theta, grads, lr=0.03):\n    w1, w2 = theta\n    dw1, dw2 = grads\n    \n    w1 -= dw1 * lr\n    w2 -= dw2 * lr\n    \n    return theta","56e7fb7a":"theta = np.random.uniform(-1,1, (128, 20)), np.random.uniform(-1,1, (85, 128))","ebe507c7":"context, target  = trigrams[0]","28453a06":"# convert context('word1', 'word2') into [#1, #2] word to ix\ncontext_ix = [word_to_ix[c] for c in context]\n\n# convert to numpy array and convert from (2,) to (1,2)\ncontext_ix = np.array(context_ix)\ncontext_ix = context_ix.reshape(1, 2)\n\n# forward propagation (predict)\nparams = forward(context_ix, theta)\n\n# get the looses and append to epoch losses\nloss = NLLLoss(params[-1], [word_to_ix[target]])\n\n# get the gradients from back propagation\ngrads = backward(context_ix, [word_to_ix[target]], theta, params)","f0e0819d":"params[-2].shape","0c1378fe":"[word_to_ix[target]]","4ddf542c":"log_softmax_crossentropy_with_logits(params[-2], [word_to_ix[target]])","bf0e7116":"losses = {}\n\nfor epoch in range(1000):\n\n    epoch_losses = []\n    \n    for context, target in trigrams:\n        \n        # convert context('word1', 'word2') into [#1, #2] word to ix\n        context_ix = [word_to_ix[c] for c in context]\n\n        # convert to numpy array and convert from (2,) to (1,2)\n        context_ix = np.array(context_ix)\n        context_ix = context_ix.reshape(1, 2)\n\n        # forward propagation (predict)\n        params = forward(context_ix, theta)\n\n        # get the looses and append to epoch losses\n        loss = NLLLoss(params[-1], [word_to_ix[target]])\n        epoch_losses.append(loss)\n\n        # get the gradients from back propagation\n        grads = backward(context_ix, [word_to_ix[target]], theta, params)\n\n        # optimize the weights Stochastic gradient descent (SGD)\n        theta = optimize(theta, grads)\n        \n    losses[epoch] = epoch_losses","aefa5b40":"ix = np.arange(0,35)\n\nfig = plt.figure()\nfig.suptitle('Epoch\/Losses', fontsize=20)\nplt.plot(ix,[losses[i][0] for i in ix])\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Losses', fontsize=12)","eea35124":"def predict(words):\n    context_ix = [word_to_ix[c] for c in words]\n    params = forward(context_ix, theta)\n    word = ix_to_word[np.argmax(params[-1])]\n    \n    return word","340bd07b":"# 'And dig deep trenches in thy beauty's field'\n# expected answer 'deep'\npredict([\"and\", \"dig\"])","f1809268":"def accuracy():\n    wrong = 0\n\n    for context, target in trigrams:\n        if(predict(context) != target):\n            wrong += 1\n            \n    return (1 - (wrong \/ len(trigrams)))","83ff29ba":"accuracy()","148eb95b":"### Test prediction","b751289c":"### Linear models","c8553aea":"### Log-Softmax Cross-Entropy\n\nCross Entropy = NLLoss(log_softmax(p))\n\nWe could also say that Cross Entropy is NLLoss + log_softmax","07f8196c":"<h1 id=\"implementation\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Implementation\n        <a class=\"anchor-link\" href=\"#implementation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","b9db7a09":"Word embeddings are dense vectors of real numbers, one per word in your vocabulary. In NLP, it is almost always the case that your features are words! But how should you represent a word in a computer? You could store its ascii character representation, but that only tells you what the word is, it doesn\u2019t say much about what it means (you might be able to derive its part of speech from its affixes, or properties from its capitalization, but not much). Even more, in what sense could you combine these representations? We often want dense outputs from our neural networks, where the inputs are |V| dimensional, where V is our vocabulary, but often the outputs are only a few dimensional (if we are only predicting a handful of labels, for instance)","f1af5879":"### Prepare the embeddings matrix","3f1b078f":"### Prepare data","d9fc3ff5":"### Input\/Targets","ade3c664":"Accuracy of 96.49%, please don't forget to up-vote if you enjoy it :)\n\nIf you want to learn more about this integration, read more on this pytorch tutorial:<br>\nhttps:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html","433a9fb5":"<h1 id=\"prepare\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Embeddings and Trigrams\n        <a class=\"anchor-link\" href=\"#prepare\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e4dfcf4a":"### Activation functions","5a09f19b":"### Loss function","a828f5c5":"### Backward function","7670e1cb":"### Optimizer function","7d8cf7ca":"### Predict function","a78a655a":"<h1 id=\"definition\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Definition\n        <a class=\"anchor-link\" href=\"#definition\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","31d1cfcc":"### Forward function","6b186ced":"### Prepare the dictionaries","c2f919ed":"<h1 id=\"training\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","6081f22a":"### Plot losses","f07be45a":"<h1 id=\"analyze\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","9f8cc2a0":"<h1 id=\"dataset\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}