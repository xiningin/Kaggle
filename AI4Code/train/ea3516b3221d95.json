{"cell_type":{"6bae9c41":"code","5acd9ff1":"code","239b3b6a":"code","6099f770":"code","ff742ff0":"code","f1795268":"code","3fa5cc33":"code","7b9e6124":"code","814ca64d":"code","aa1d552f":"code","53f48a74":"code","fcc805df":"code","a9eafc5e":"code","972051b5":"code","f071eb8c":"code","66ef584b":"code","04484037":"code","90589ad3":"code","242636ae":"code","e6dea1e9":"code","39a4d907":"code","9ace3560":"code","3cf43d8f":"code","c86dfe97":"code","092ab727":"code","776bd4df":"code","655f7cd2":"code","7262b5ce":"code","999542fc":"code","9addce2c":"code","e8a48536":"code","33afe494":"code","1cfee518":"code","e246f150":"code","478f1ccd":"code","42ac0e1b":"code","d2717680":"code","0166fcec":"code","b73c4925":"code","50124d2b":"code","da157188":"code","cf9dcfe7":"code","2f0c5101":"code","66168c9a":"markdown","1a0ea3a7":"markdown","083ed140":"markdown","828e526c":"markdown","0a1f55e6":"markdown","914b8ace":"markdown","2db1c64d":"markdown","e049ce9b":"markdown","1347d4f1":"markdown","9ba3c274":"markdown","53121dd1":"markdown","5decf673":"markdown","d7d5c0ee":"markdown","65cbc203":"markdown","3bae2d0d":"markdown"},"source":{"6bae9c41":"# importing libraries:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport phik\nfrom phik.report import plot_correlation_matrix\nfrom phik import report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","5acd9ff1":"# loading datasets:\n\ntrain_orig = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col=0)\ntest_orig = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col=0)","239b3b6a":"print(f'train_orig df size - {train_orig.shape}')\nprint(f'test_orig df size - {test_orig.shape}')","6099f770":"train_orig.head()","ff742ff0":"test_orig.head(5)","f1795268":"train_orig.describe()","3fa5cc33":"train_orig['Survived'].value_counts()","7b9e6124":"# estimation of the number of missing values.\n\ndef na_values(data):\n    report = data.isna().sum().to_frame()\n    report = report.rename(columns = {0: 'missing_values'})\n    report = report.loc[report['missing_values'] != 0]\n    report['% of total'] = (report['missing_values'] \/ data.shape[0]).round(2)\n    return report.sort_values(by = 'missing_values', ascending = False)","814ca64d":"na_values(train_orig)","aa1d552f":"na_values(test_orig)","53f48a74":"def hist_boxplot(data,column,interval = [0,0]):\n    fig, axes = plt.subplots(2,1,figsize=(10,5))\n    axes[0].hist(data[column], bins = 500)\n    axes[0].grid()\n    axes[0].set_title(f'Feature distribution \"{column}\"',fontsize=15)\n    sns.boxplot(x=column, data=data, orient=\"h\",showmeans=True).set(xlabel=None)\n    fig.tight_layout()\n    plt.grid()\n    if interval == [0,0]:\n        plt.show()\n    else:\n        axes[0].set_xlim(interval)\n        plt.xlim(interval)\n        plt.show()","fcc805df":"hist_boxplot(train_orig,'Age')","a9eafc5e":"hist_boxplot(train_orig,'Fare')","972051b5":"train = train_orig.copy()\ntest = test_orig.copy()","f071eb8c":"# make feature of social status\n\ndef name_group(x):\n    if 'mr' in x:\n        return 'mr'\n    if 'mrs' in x:\n        return 'mrs'\n    if 'miss' in x:\n        return 'miss'\n    if 'master' in x:\n        return 'master'\n    if 'master' in x:\n        return 'master'\n    if 'dr' in x:\n        return 'dr'\n    if 'rev' in x:\n        return 'rev'\n    else:\n        return 'misc'","66ef584b":"train['name_group'] = train['Name'].str.lower().apply(name_group)\ntest['name_group'] = test['Name'].str.lower().apply(name_group)","04484037":"train[train['name_group'] == 'misc']","90589ad3":"drop_features_list = ['Name','Cabin','Ticket']\ncat_features_list = ['Pclass','Sex','Embarked','name_group']\nnum_features_list = ['Age','SibSp','Parch','Fare']\ntarget_feature = 'Survived'","242636ae":"# drop unused features\n\ntrain = train.drop(drop_features_list, axis=1)\ntest = test.drop(drop_features_list, axis=1)","e6dea1e9":"# make train and test subsets\n\nfeatures_train = train.drop(target_feature, axis=1)\ntarget_train = train[target_feature]\nfeatures_test = test","39a4d907":"# fill NA in Embarked\n\nmost_pop_port = features_train['Embarked'].value_counts().index[0]\n\nfeatures_train.loc[features_train['Embarked'].isna(), 'Embarked'] = features_train.loc[features_train['Embarked'].isna(), 'Embarked'].fillna(most_pop_port)","9ace3560":"# fill NA in Fare\n\nmedian_fare = features_train['Fare'].median()\nfeatures_test.loc[features_test['Fare'].isna(), 'Fare'] = features_test.loc[features_test['Fare'].isna(), 'Fare'].fillna(median_fare)","3cf43d8f":"# encoding categorical features\n\nfeatures_train[cat_features_list] = OrdinalEncoder().fit_transform(features_train[cat_features_list])\nfeatures_test[cat_features_list] = OrdinalEncoder().fit_transform(features_test[cat_features_list])","c86dfe97":"# check NA\ndisplay(na_values(features_train))\ndisplay(na_values(features_test))","092ab727":"phik_matrix = features_train.phik_matrix()","776bd4df":"fig, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(phik_matrix,annot=True, cmap='Blues')\nplt.show()","655f7cd2":"# age_high_corr_features_list = phik_matrix.loc[phik_matrix['Age'] >= 0.4, 'Age'].index\n# use only highly correlated features is a bad idea\n\nage_high_corr_features_list = phik_matrix.loc[phik_matrix['Age'] >= 0, 'Age'].index","7262b5ce":"imputer = KNNImputer(n_neighbors=10)","999542fc":"def filling_df(data,high_corr_features_list):\n    fill_array = imputer.fit_transform(data[high_corr_features_list])\n    fill_df = pd.DataFrame(fill_array, columns = high_corr_features_list, index = data.index)\n    append_columns = data.drop(high_corr_features_list, axis=1)\n    append_columns_name = append_columns.columns\n    fill_df[append_columns_name] = append_columns\n    return fill_df","9addce2c":"features_train = filling_df(features_train,age_high_corr_features_list)\nfeatures_test = filling_df(features_test,age_high_corr_features_list)","e8a48536":"# check NA\ndisplay(na_values(features_train))\ndisplay(na_values(features_test))","33afe494":"# scaling\n\nscaler = StandardScaler()\n\ndef scaling_data(data,numerical_features):\n    scaler.fit(data[numerical_features])\n    data[numerical_features] = scaler.transform(data[numerical_features])","1cfee518":"scaling_data(features_train,num_features_list)\nscaling_data(features_test,num_features_list)","e246f150":"model = CatBoostClassifier(silent = True)","478f1ccd":"parametrs = {'n_estimators': range (50, 1000, 50),\n             'max_depth': range (1, 15),\n             'learning_rate': [x \/ 1000.0 for x in range(1, 11, 1)]}","42ac0e1b":"# selection of the best parameters\n\ndef rs_model(model,param,features_train,target_train):\n    rs = RandomizedSearchCV(model,\n                            param,\n                            scoring='roc_auc',\n                            cv=5, \n                            verbose=1,\n                            random_state=12345)\n    rs.fit(features_train,target_train)\n    best_model = rs.best_estimator_\n    return best_model","d2717680":"best_model = rs_model(model,parametrs,features_train,target_train)\nbest_model.get_all_params()","0166fcec":"def model_quality(model,features,target):\n    probs = model.predict_proba(features)\n    probs = probs[:, 1]\n    fpr, tpr, treshold = roc_curve(target, probs)\n    auc_score = auc(fpr, tpr)\n    predict = model.predict(features)\n    accuracy = model.score(features,target)\n    recall = recall_score(target,predict)\n    precision = precision_score(target,predict)\n    \n    print(f'ROC AUC: {auc_score}')\n    print(f'Accuracy: {accuracy}')\n    print(f'Recall: {recall}')\n    print(f'Precision: {precision}')\n    \n    plt.plot(fpr, tpr, color='darkorange',\n         label='ROC curve (area = %0.2f)' % auc_score)\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve\\n',fontsize=15)\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    cf_matrix = confusion_matrix(target, predict)\n    text = np.asarray([['TN', 'FP'], ['FN', 'TP']])\n    labels = (np.asarray([\"{0}\\n{1:}\".format(text,cf_matrix) \n                          for text, cf_matrix in zip(text.flatten(), cf_matrix.flatten())])).reshape(2,2) \n    ax = sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt=\"\")\n    ax.set_title('\u0421onfusion matrix\\n',fontsize=15);\n    ax.set_xlabel('Predicted Values')\n    ax.set_ylabel('Actual Values ');\n    ax.xaxis.set_ticklabels(['False','True'])\n    ax.yaxis.set_ticklabels(['False','True'])\n    plt.show()","b73c4925":"model_quality(best_model,features_train,target_train)","50124d2b":"def plot_feature_importances(model,features_train):\n    feature_names = list(features_train)\n    importances = model.feature_importances_\n    model_importances = pd.Series(importances, index=feature_names)\n    model_importances = model_importances.sort_values(ascending=False).head(10)\n    model_importances.plot.bar(figsize=(10,5))\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Importances',fontsize=15)\n    plt.title(f'Top-10 {model}', fontsize=15)\n    plt.grid()\n    plt.show()","da157188":"plot_feature_importances(best_model,features_train)","cf9dcfe7":"model = best_model\npredictions_array = model.predict(features_test)\nsubmission = pd.DataFrame(data = zip(features_test.index, predictions_array), columns = ['PassengerId', 'Survived'])","2f0c5101":"submission.to_csv('.\/submission.csv', index = False)","66168c9a":"<a id = '2.0'><\/a>\n# 2. Primary analysis","1a0ea3a7":"Goal:\n<br>Predict if a passenger survived the sinking of the Titanic or not.\n<br>For each in the test set, you must predict a 0 or 1 value for the variable.\n\nMetric:\n<br>Your score is the percentage of passengers you correctly predict. This is known as accuracy.","083ed140":"<a id = '5.0'><\/a>\n# 5. Predicting","828e526c":"- Imbalance of classes\n- Missing values. Need to remove the feature \"Cabin\" and fill in the rest.\n- Outliers in the feature \"Fare\"","0a1f55e6":"- Imbalance of classes.","914b8ace":"# Contents","2db1c64d":"<a id = '1.0'><\/a>\n# 1. Importing libraries and loading datasets","e049ce9b":"- Outliers in the feature \"Fare\".","1347d4f1":"Feature correlation:\n\nhttps:\/\/towardsdatascience.com\/phik-k-get-familiar-with-the-latest-correlation-coefficient-9ba0032b37e7","9ba3c274":"<a id = '3.0'><\/a>\n# 3. Pre-processing","53121dd1":"* [1. Importing libraries and loading datasets](#1.0)\n* [2. Primary analysis](#2.0)\n    * [2.1 \u0421onclusion](#2.1)\n* [3. Pre-processing](#3.0)\n* [4. Modeling](#4.0)\n* [5. Predicting](#5.0)","5decf673":"<a id = '2.1'><\/a>\n## 2.1 \u0421onclusion","d7d5c0ee":"<a id = '4.0'><\/a>\n# 4. Modeling","65cbc203":"# Problem statement","3bae2d0d":"- Missing values. Need to remove the feature \"Cabin\" and fill in the rest."}}