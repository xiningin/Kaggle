{"cell_type":{"96c7f84f":"code","76e7d632":"code","6eda82b3":"code","f7d8c871":"code","981abcc8":"code","1fb20be0":"code","b280e875":"code","774ed64a":"code","abb67808":"markdown","2810132f":"markdown","919cfbc6":"markdown","51b02475":"markdown","920bcd26":"markdown"},"source":{"96c7f84f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","76e7d632":"import os\nimport time\nfrom IPython.display import clear_output\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom matplotlib.pyplot import imshow, show, figure\nfrom PIL import Image\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import RandomCrop, ToTensor, Compose, RandomHorizontalFlip, RandomVerticalFlip, ToPILImage","6eda82b3":"NUM_EPOCHS = 100\nBATCH_SIZE = 32\n\nPATCH_SIZE = 40\nNOISE_SIGMA = 25\nCROPS_PER_IMAGE = 10\n\nTRAIN_DATA_PATH = \"\/kaggle\/input\/kaggle-days-denoise\/data\/train\/\"\nVALIDATION_DATA_PATH = \"\/kaggle\/input\/kaggle-days-denoise\/data\/val\/\"","f7d8c871":"class DenoisingDataset(Dataset):\n    def __init__(self, data_path):\n        self._images = [np.array(Image.open(os.path.join(data_path, filename))) for filename in os.listdir(data_path)]\n        self._sigma = NOISE_SIGMA\n        self._transforms = Compose([\n            ToPILImage(),\n            RandomCrop(PATCH_SIZE),\n            RandomHorizontalFlip(),\n            RandomVerticalFlip(),\n            ToTensor()])\n\n    def __len__(self):\n        return len(self._images) * CROPS_PER_IMAGE\n\n    def __getitem__(self, item):\n        image = self._images[item % len(self._images)]\n        \n        image = self._transforms(image)\n        noise = torch.randn(image.size()).mul_(self._sigma \/ 255)\n        \n        noisy_image = (image + noise).clamp(0, 1)\n        \n        return noisy_image, image","981abcc8":"class DnCNN(nn.Module):\n    def __init__(self, depth=7, n_channels=16, image_channels=3, use_bnorm=True, kernel_size=3):\n        super(DnCNN, self).__init__()\n        kernel_size = 3\n        padding = 1\n        layers = []\n\n        layers.append(nn.Conv2d(in_channels=image_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=True))\n        layers.append(nn.ReLU(inplace=True))\n        for _ in range(depth-2):\n            layers.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding, bias=False))\n            layers.append(nn.BatchNorm2d(n_channels, eps=0.0001, momentum = 0.95))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(in_channels=n_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding, bias=False))\n        self.dncnn = nn.Sequential(*layers)\n        self._initialize_weights()\n\n    def forward(self, x):\n        out = self.dncnn(x)\n        return x - out\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.orthogonal_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)","1fb20be0":"def visualize_validation(model):\n    model.eval()\n    image = Image.open(os.path.join(VALIDATION_DATA_PATH, \"pier.png\"))\n    image = np.array(image).astype(\"float32\") \/ 255.\n    model_input = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n\n    with torch.no_grad():\n        result = model(model_input)\n\n    result_image = result[0].clamp(0, 1).permute(1, 2, 0).numpy()\n    result_image = (result_image * 255).astype(\"uint8\")\n    \n    stacked_images = np.zeros((image.shape[0], image.shape[1] * 2, image.shape[2]), dtype=\"uint8\")\n    stacked_images[:, :image.shape[1]] = (image * 255).astype(\"uint8\")\n    stacked_images[:, image.shape[1]:] = result_image\n    \n    \n    clear_output(wait=True)\n    figure(figsize=(18, 18))\n    imshow(stacked_images)\n    show()\n","b280e875":"model = DnCNN()\ndataset = DenoisingDataset(TRAIN_DATA_PATH)\ncriterion = nn.MSELoss(reduction=\"sum\")\noptimizer = Adam(model.parameters(), lr=0.001)","774ed64a":"for epoch_id in range(NUM_EPOCHS):\n    # First, we will use current model to predict a validation image\n    visualize_validation(model)\n\n    # Let's optimize parameters of our model\n    model.train()\n    \n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n\n    for iter_id, (input_images, target_images) in enumerate(data_loader):\n        predicted_images = model(input_images)\n        loss = criterion(predicted_images, target_images)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        print(\"\\rEpoch {} Iteration {} Loss {}\".format(epoch_id, iter_id, loss.item() \/ BATCH_SIZE), end=\"\")","abb67808":"### Let's train our model!","2810132f":"### Define model architecture","919cfbc6":"### Set the model, dataset, loss function and optimizer","51b02475":"### Some useful stuff for visualization","920bcd26":"### Define a dataset, augmentations, noise simulation"}}