{"cell_type":{"8f00b968":"code","34e10186":"code","da2713b3":"code","2e739f3e":"code","e52abefb":"code","0f517d2c":"code","573869bc":"code","214c0f58":"code","389abc47":"code","8f218a21":"code","709f55fc":"code","cfd3cd87":"code","97472bc1":"code","9c191cdc":"code","eaf65fb5":"code","a90d5e55":"code","a95e032e":"code","467266ee":"code","715ceb61":"code","b6cd0141":"code","9c6f0cf5":"markdown","15d63ec7":"markdown","14b4398d":"markdown","f2b86179":"markdown","4169f5ca":"markdown","be5f1a10":"markdown","975d2fa9":"markdown","bcb4d79c":"markdown","d1f58ac0":"markdown","3deaf2e4":"markdown","bc5e2377":"markdown","10d072fb":"markdown","ddeb390a":"markdown","247e1da8":"markdown","bde9af66":"markdown","33bb8dda":"markdown","78b7c649":"markdown"},"source":{"8f00b968":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34e10186":"from keras.layers import Conv2D, LeakyReLU, Dense, Flatten, Dropout, MaxPool2D # Layers\nfrom keras.models import Sequential # Sequential Model\nfrom keras.optimizers import Adam,RMSprop # optimizer\nfrom keras.preprocessing.image import ImageDataGenerator # data generator\nfrom keras.callbacks import ReduceLROnPlateau ","da2713b3":"train = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_test.csv\")\nprint(\"\\n\",train.info())\nprint(test.info())","2e739f3e":"train.head()","e52abefb":"print(train.shape)\ntrain.describe()","0f517d2c":"x_train = train.drop(['label'], axis=1) # x_train'e label d\u0131\u015f\u0131ndaki t\u00fcm px de\u011ferlerimi al\u0131yorum\ny_train = train.label # label de\u011feri yani say\u0131n\u0131n de\u011ferini \nx_test = test.drop(['label'], axis=1) # x_train ile ayn\u0131 \u015fekilde \ny_test = test.label # y_train ile ayn\u0131 \u015fekilde ","573869bc":"print(\"x_train shape before reshape:\", x_train.shape)\nprint(\"x_test shape before reshape:\", x_test.shape)\nprint(\"y_train shape before reshape:\", y_train.shape)\nprint(\"y_test shape before reshape:\", y_test.shape)\n\nx_train = np.array(x_train).reshape(-1, 28, 28, 1)\nx_test = np.array(x_test).reshape(-1, 28, 28, 1)","214c0f58":"from keras.utils.np_utils import to_categorical # one-hot-encoding'a \u00e7evirmek i\u00e7in \ny_train = to_categorical(y_train, num_classes = 10) # label encoding \ny_test = to_categorical(y_test, num_classes = 10)","389abc47":"print(\"x_train shape after reshape:\", x_train.shape)\nprint(\"x_test shape after reshape:\", x_test.shape)\nprint(\"y_train shape after reshape:\", y_train.shape)\nprint(\"y_test shape after reshape:\", y_test.shape)","8f218a21":"x_train = x_train\/255.0\nx_test = x_test\/255.0","709f55fc":"epochs = 30\nbatch_size = 250\nmodel = Sequential()\n\n# Block 1\nmodel.add(Conv2D(32,3, padding  =\"same\", input_shape=(28,28,1)))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(32,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n#Block 2\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n#Flatten Block\nmodel.add(Flatten())\n\n#Output Block\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(10,activation=\"softmax\"))\nmodel.summary()","cfd3cd87":"# kullan\u0131labilir optimizer featurelar\u0131\n\"\"\"\nsgd = keras.optimizers.SGD(lr=1e-4, momentum=0.9)\nrms_prop = keras.optimizers.RMSprop(lr=1e-4)\nadam = keras.optimizers.adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \nadamax = keras.optimizers.Adamax(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\nadadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001) \"\"\"\n\nadam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n#rms_prop = RMSprop(lr=0.001)\nlearning_rate_reduce = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.4, \n                                            min_lr=0.00001)\nloss = \"categorical_crossentropy\"","97472bc1":"model.compile( optimizer= adam, loss=loss ,metrics=['accuracy'])","9c191cdc":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(x_train)","eaf65fb5":"history = model.fit_generator(datagen.flow(x_train,y_train, batch_size = batch_size),\n                              epochs = epochs, validation_data = (x_test, y_test),\n                              steps_per_epoch = x_train.shape[0] \/\/ batch_size,\n                              callbacks=[learning_rate_reduce])\nresult = model.evaluate(x = x_train, y = y_train)\n","a90d5e55":"print('Accuracy:', result[1])","a95e032e":"plt.plot(history.history['loss'])\nplt.title(\"Loss Plot\", fontsize = 15)\nplt.xlabel(\"Epochs\", fontsize = 12)\nplt.ylabel(\"Loss\", fontsize = 12)\nplt.grid(alpha=0.3)\nplt.legend([\"Train\", \"Test\"])\nplt.show()\n\nplt.plot(history.history[\"accuracy\"])\nplt.title(\"Accuracy Plot\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\", fontsize = 12)\nplt.grid(alpha=0.3)\nplt.legend([\"Train\",\"Test\"])\nplt.show()","467266ee":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(x_test)\n\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n\n# Convert validation observations to one hot vectors\ny_true = np.argmax(y_test,axis = 1) \n\n# compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred_classes) \n\n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\n\nsns.heatmap(cm, annot = True, linewidths = 0.01, cmap = \"Greens\", linecolor = \"gray\", fmt = '.1f',ax = ax)\n\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\n\nplt.show()","715ceb61":"test_y = np.argmax(model.predict(x_test),axis =1)","b6cd0141":"df_submission = pd.DataFrame([test.index+1,test_y],[\"ImageId\",\"Label\"]).transpose()\ndf_submission.to_csv(\"submission.csv\",index=False)","9c6f0cf5":"###### Improvement Learning Rate\n* ReduceLROnPlateau() keras'ta bulunan bir callback fonksiyon, bu fonksiyon a\u015famal\u0131 olarak accuracy'e bakarak, (e\u011fer improvement durduysa)lr'yi d\u00fc\u015f\u00fcrerek \u00f6\u011frenmenin daha iyi olmas\u0131n\u0131 sa\u011fl\u0131yor.\n\n\n    1. monitor: bir metric'i monitor ediyor, bu bizde val_accuracy\n    2. patience: ne kadar epochs s\u00fcresince bir improvement olmad\u0131, bunu belirliyoruz, \u00f6rne\u011fin 2 epochs sonras\u0131nda bir improvement olmad\u0131ysa lr d\u00fc\u015f\u00fcr\u00fcyoruz. \n    3. factor: ne kadar de\u011ferde lr d\u00fc\u015f\u00fcr\u00fclecek new_lr = lr * factor (0.5)\n    4. min_lr: en d\u00fc\u015f\u00fck lr band'\u0131 (0.00001)","15d63ec7":"###### Optimizers: \n\n    1. SGD (Stochastic gradient descent optimizer)\n    2. RMSprop\n    3. Adam\n    4. Adamax","14b4398d":"# Building Model","f2b86179":"# Training the model","4169f5ca":"# Normalization Data\n* Datam\u0131z\u0131 0-1 aras\u0131na scale ediyoruz","be5f1a10":"# Submission","975d2fa9":"##### CNN ile train etmek i\u00e7in np.array'e at\u0131p reshape etmemiz gerekiyor. Yukar\u0131da yapt\u0131\u011f\u0131m\u0131z reshape i\u015flemi 1 boyutlu olan pixellerimizi 3 boyut'a ta\u015f\u0131yor. Array'imizde 60000\/10000 olarak belirtilen ka\u00e7 tane veri oldu\u011funu tutuyor. Di\u011fer k\u0131s\u0131m ise 3 boyutlu input'u g\u00f6steriyor. Bizim verimiz 28x28x1'lik 3 boyutlu 60000\/10000 tane inputtan olu\u015fuyor. \n* K\u0131saca (sample_size, 28x28x1)","bcb4d79c":"# Image Generate","d1f58ac0":"# Loading Dataset\n\n#### MNIST dataset:\n\n* 60000 tane el yazmas\u0131 0-9'a kadar say\u0131lar bulunmaktad\u0131r.\n* Her foto\u011fraf 28*28 px'den olu\u015fmaktad\u0131r ve gri renktedir.\n* Her pixel 0-255 aras\u0131nda bir de\u011fer almaktad\u0131r. '0' siyah\u0131, '255' beyaz\u0131 temsil etmektedir.\n* Her bir say\u0131n\u0131n label'i 0-9 olarak tan\u0131mlanm\u0131\u015ft\u0131r.\n* Data 758 s\u00fctun'dan olu\u015fmaktad\u0131r. \u0130lk s\u00fctun label olarak tan\u0131mlanm\u0131\u015ft\u0131r, di\u011fer 784 tane s\u00fctun ise pixellerden olu\u015fmaktad\u0131r.","3deaf2e4":"# Import Library\/Packages\n\n* Keras -> Model\/Prediction\/Layers\n* numpy -> Hesaplamalar ve pandas'a yard\u0131mc\u0131 olmas\u0131 i\u00e7in \n* pandas -> Data i\u00e7in\n* matplotlib -> G\u00f6rselle\u015ftirmek i\u00e7in","bc5e2377":"###### Model olu\u015fturman\u0131n iki yolu var,\n    1.  Sequential Model ile,\n    2.  Function API kullanarak\n* Functional API daha komplike daha zorlu modellerde ve \u00e7ok output var ise kullan\u0131labilir. Basit d\u00fczeyde model kullanmak istiyoruz. \n* Sequential'da modelimize istedi\u011fimiz \u015fekilde s\u0131ras\u0131yla layer ekleyebiliyoruz.\n\n###### Modelimiz ,\n* 2 tane Conv. Blok'tan olu\u015facak.\n* Her blokt 2 tane Conv2D Layer'dan olu\u015facak ve aktivasyon fonksiyonu olarak LeakyRelu kullanaca\u011f\u0131z. Sonra MaxPool2D layerimiz ve son olarak Dropout yapaca\u011f\u0131z. \n* Sonras\u0131nda Flatten Layer'\u0131m\u0131z bulunacak ard\u0131ndan outputmuzun oldu\u011fu Dense Layerlar\u0131m\u0131z\u0131 koyaca\u011f\u0131z\n\n* MaxPool2D layer'\u0131 image'in boyutunu reduce etmemize yar\u0131yoru. Pool size(2,2) default olarak uyguland\u0131\u011f\u0131nda (28,28) olan image'imiz (14,14)' d\u00fc\u015f\u00fcyor. Bir nevi feature reducing.\n\n* Dropout layer ise node'lar\u0131 random \u015fekilde sizin verdi\u011finiz oranda kapat\u0131yor. Regularization layer'\u0131, over-fit engellemek i\u00e7in kullan\u0131yoruz. \n\n* Output layer'\u0131m\u0131z ise 10 node'dan olu\u015facak, sigmoid func kullanaca\u011f\u0131z output(y_train) 10 tane outputtan olu\u015fuyor.","10d072fb":"# Performance plotting","ddeb390a":"##### Learning Rate\n\n    1. E\u011fer learning rate \u00e7ok y\u00fcksek ise loss'umuz azal\u0131rken bir anda y\u00fckselebilir. En az loss'a sahip olamayabiliriz.\n    2. E\u011fer learning rate \u00e7ok az ise learning \u00e7ok yava\u015f olur. D\u00fczg\u00fcn bir rate de\u011feri se\u00e7memiz gerekiyor.\n    3. lr de\u011feri 0.001 yeterli bir de\u011ferdir. E\u011fer i\u015fe yaramazsa daha y\u00fcksek ya da d\u00fc\u015f\u00fck de\u011fer verilebilir.\n    4. Biz adam optimizer kullanaca\u011f\u0131z.","247e1da8":"##### Loss Functions: \n\n    1. binary_crossentropy: Bu loss fonksiyonu daha \u00e7ok tek node'lu output layerlarda kullan\u0131l\u0131yor. 0 ve 1 classificationlar\u0131nda \n    2. categorical_crossentropy: Multi-class yani birden \u00e7ok outputlarda kullan\u0131ly\u0131or. \n    3. sparse_categorical_crossentropy: Yukar\u0131daki ile ayn\u0131 a\u015fa\u011f\u0131da fark\u0131ndan bahsediliyor.\n  \n    \n    \n* E\u011fer target'im one-hot encoded ise categorical_crossentropy kullanabilirsin.\n\u00d6rne\u011fin:\n [1,0,0]\n [0,1,0]\n [0,0,1]\n\n* E\u011fer targetim integer ise sparse_categorical_crossentropy kullanabilirsin. \n\u00d6rne\u011fin:\n1\n2\n3\n\n","bde9af66":"# Compiling Model","33bb8dda":"# Prepare Train and Test ","78b7c649":"# Confusion Matrix"}}