{"cell_type":{"30000921":"code","23218477":"code","194021dd":"code","3c498c82":"code","176be66d":"code","c8508220":"code","0a400167":"code","e687c435":"code","fb4b6e1c":"code","44da11c0":"code","a9dcaf99":"code","dcdc6c18":"code","f27ed510":"code","03b122ef":"code","4018e5a7":"code","10a6437e":"code","9f8be33b":"code","35fa20e6":"code","0612cbd3":"code","dc8f20eb":"code","05458bbe":"code","4c40a9a8":"code","5cb69959":"code","db2f45a4":"code","b4da679b":"code","270ace46":"code","adedc430":"code","86bc6709":"code","27a57cf1":"code","ab01c2b5":"code","407783f3":"code","55062810":"code","f73daf11":"code","f19c3999":"code","2647afbd":"code","0acbd6af":"code","7bdbcd59":"code","e227ee79":"code","83e75903":"code","7c3721bc":"code","5984b48b":"code","ef1b2947":"code","41cf717c":"code","64df605f":"markdown","a258103b":"markdown","e1b3522c":"markdown","bd211983":"markdown","7d7a24da":"markdown","8cb5098f":"markdown","119baced":"markdown","2e21476f":"markdown","29f8ac88":"markdown","e0caa53a":"markdown","4e649830":"markdown","4fadb564":"markdown","75130ab6":"markdown","3303afa2":"markdown","71ddd4d7":"markdown","633d0b1b":"markdown","dd76e15d":"markdown","22524890":"markdown","cca675f9":"markdown","ea29e8b2":"markdown","7ea42a75":"markdown","f4a6aa5c":"markdown","93041a51":"markdown","30bdddf2":"markdown","f7c14c53":"markdown","0f123eb7":"markdown","5891bbb2":"markdown","282c88f0":"markdown"},"source":{"30000921":"#Necessary packages\nimport numpy as np #linear Algebra\nimport pandas as pd #data manipulation\nimport seaborn as sns #multiple plots\nimport matplotlib.pyplot as plt #plotting\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline \n\ntrain_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")","23218477":"train_df.head()\n","194021dd":"train_df.tail()\n","3c498c82":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms= ms[ms[\"Percent\"] > 0]\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='80')\n    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"red\",alpha=0.8)\n    plt.xlabel('Independent variables', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('NaN exploration', fontsize=15)\n    return ms\n\nmissingdata(train_df)","176be66d":"missingdata(test_df)","c8508220":"fig=sns.scatterplot(train_df.index, train_df[\"Age\"],color=\"blue\",alpha=0.8)\nplt.xlabel('Occurances', fontsize=15)\nplt.ylabel('Values of Age', fontsize=15)\nplt.title('Variable exporation', fontsize=15)","0a400167":"test_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)\n\n\n#For example when trying to exclude the cabin column we are going to receive unnecessary warnings. Exclude! (thats why we imported it)\ndrop_column = ['Cabin']\ntrain_df.drop(drop_column, axis=1, inplace = True)\ntest_df.drop(drop_column,axis=1,inplace=True)\n\ntest_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)\n\n","e687c435":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","fb4b6e1c":"print(train_df.isnull().sum())\nprint(test_df.isnull().sum())","44da11c0":"# in order to apply the function only once\nall_data=[train_df,test_df]","a9dcaf99":"import re\n# A way to think about textual preprocessing is: Given my character column, what are some regularities that occur often. In our case we see titles (miss,mr etc).\n# Let us then extract second word from every row and assign it to a new column. Not only that let us make it categorical (so that we can one-hot encode it) where we observe the most frequent ones.\n\n\ndef title_parser(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # Check whether title exists, then return it, if not \"\"\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# New column named Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(title_parser)\n# Irrelevant titles should be just called irrelevant (in sence that they do not occur often)\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'irrelevant')\n# Let us make sure they are categorical, where we replace similiar names\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","dcdc6c18":"g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n#as a general rule with classification problems, we will always put the target variable (to be classified) in hue\n","f27ed510":"all_data=[train_df,test_df]\n\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n    ## create bin for age features\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n    \n    \n  \n","03b122ef":"# create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n                                                                                      'Average_fare','high_fare'])","4018e5a7":"traindf=train_df\ntestdf=test_df\n\nall_dat=[traindf,testdf]\n\nfor dataset in all_dat:\n    drop_column = ['Name','Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)\n    \n   ","10a6437e":"train_df.head()","9f8be33b":"traindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])\n\ntestdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])\n\ntraindf.head()\ntestdf.head()","35fa20e6":"sns.heatmap(traindf.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","0612cbd3":"type(traindf[\"Age\"])","dc8f20eb":"from sklearn.preprocessing import MinMaxScaler\n\n\ntraindf[['Age','Fare']] = traindf[['Age',\"Fare\"]].apply(pd.to_numeric)\nscaler = MinMaxScaler()\ntraindf[['Age','Fare']] = scaler.fit_transform(traindf[['Age',\"Fare\"]])\n\ndrop_column = ['PassengerId']#id of any kind will be always dropped since it does not have predictive power\ntraindf.drop(drop_column, axis=1, inplace = True)\ntrain_X = traindf.drop(\"Survived\", axis=1)#we do not need train test splitting with skicit learn (in nomral setting test_df and train_df will be concatanted and then use it)\ntrain_Y=traindf[\"Survived\"]\ntest_X  = testdf.drop(\"PassengerId\", axis=1).copy() \ntrain_X.shape, train_Y.shape, test_X.shape\n\n\ntraindf.head()","05458bbe":"from sklearn.model_selection import train_test_split #split the dat in test and train sets\nfrom sklearn.model_selection import cross_val_score #score evaluation with cross validation\nfrom sklearn.model_selection import cross_val_predict #prediction with cross validation\nfrom sklearn.metrics import confusion_matrix #for confusion matrix (metric of succes)\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n\n\nall_features = traindf.drop(\"Survived\",axis=1) #all of the independent variables are necessary for the cross_val function\nTargeted_feature = traindf[\"Survived\"]\n\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targeted_feature,test_size=0.3,random_state=42)#why do we have to do it cant we just use test_df ? NO, since we do not have the predictions (that si checked internally in Kaggle) we can not have accuracy on hold-out test)","4c40a9a8":"train_X = traindf.drop(\"Survived\", axis=1)\ntrain_Y=traindf[\"Survived\"]\ntest_X  = testdf.drop(\"PassengerId\", axis=1).copy()\ntrain_X.shape, train_Y.shape, test_X.shape\n\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300,400],\n              'learning_rate': [0.1, 0.05, 0.01,0.001],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.2,0.1] \n              }\n\nmodelf = GridSearchCV(model,param_grid = param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodelf.fit(train_X,train_Y)\n\n\n# Best Estimator\nmodelf.best_estimator_","5cb69959":"modelf.best_score_ #accuracy metric is straightforeward, how much did I predict corrrectly?","db2f45a4":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nn_estim=range(100,1000,100)\n\n#This is the grid\nparam_grid = {\"n_estimators\" :n_estim}\n\n\nmodel_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodel_rf.fit(train_X,train_Y)\n\n\n#best estimator\nmodel_rf.best_estimator_","b4da679b":"model_rf.best_score_","270ace46":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel =LinearDiscriminantAnalysis()\nparam_grid = {'tol':[0.001,0.01,.1,.2]}\n\nmodell = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodell.fit(train_X,train_Y)\n\n# Best Estimator\nmodell.best_estimator_","adedc430":"modell.best_score_ ","86bc6709":"from sklearn.linear_model import LogisticRegression # Logistic Regression\nfrom sklearn.metrics import accuracy_score  #for accuracy_score\n\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_lr,y_test)*100,2))\nresult_lr=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_lr.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","27a57cf1":"from sklearn.svm import SVC, LinearSVC\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nprediction_svm=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_svm,y_test)*100,2))\nresult_svm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_svm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","ab01c2b5":"from sklearn.neighbors import KNeighborsClassifier\n\n\nmodel = KNeighborsClassifier(n_neighbors = 5)\nmodel.fit(X_train,y_train)\nprediction_knn=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_knn,y_test)*100,2))\nresult_knn=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_knn.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","407783f3":"from sklearn.naive_bayes import GaussianNB\nmodel= GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_gnb=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_gnb,y_test)*100,2))\nresult_gnb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_gnb.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","55062810":"from sklearn.tree import DecisionTreeClassifier\nmodel= DecisionTreeClassifier(criterion='gini', \n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto')\nmodel.fit(X_train,y_train)\nprediction_tree=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_tree,y_test)*100,2))\nresult_tree=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_tree.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","f73daf11":"from sklearn.ensemble import AdaBoostClassifier\nmodel= AdaBoostClassifier()\nmodel.fit(X_train,y_train)\nprediction_adb=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_adb,y_test)*100,2))\nresult_adb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_adb.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","f19c3999":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel= LinearDiscriminantAnalysis()\nmodel.fit(X_train,y_train)\nprediction_lda=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_lda,y_test)*100,2))\nresult_lda=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_lda.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","2647afbd":"from sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nprediction_gbc=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_gbc,y_test)*100,2))\nresult_gbc=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_gbc.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","0acbd6af":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(criterion='gini', n_estimators=700,\n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto',oob_score=True,\n                             random_state=1,n_jobs=-1)\nmodel_rf.fit(X_train,y_train)\nprediction_rm=model.predict(X_test)\n\n\nprint('Accuracy',round(accuracy_score(prediction_rm,y_test)*100,2))\nresult_rm=cross_val_score(model_rf,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","7bdbcd59":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nmodel1 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nmodel1.fit(X_train,y_train)\n\nprediction_rm1=model1.predict(X_test)\n\nprint('Accuracy',round(accuracy_score(prediction_rm1,y_test)*100,2))\nresult_rm1=cross_val_score(model1,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score',round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model1,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"cool\")\nplt.title('Confusion matrix', y=1.05, size=15)","e227ee79":"models = pd.DataFrame({\n    'Model': [\"support vector machine\",'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', \n              'Gradient Decent', 'Linear Discriminant Analysis', \n              'Decision Tree',\"Tuned RF\"],\n    'Score': [result_svm.mean(),result_knn.mean(), result_lr.mean(), \n              result_rm.mean(), result_gnb.mean(), result_adb.mean(), \n              result_gbc.mean(), result_lda.mean(), result_tree.mean(),result_rm1.mean()]})\nmodels.sort_values(by='Score',ascending=False) #pd.DAtaFrame() is a function that takes a dictionary as an input. Within this list we determine key-values paires (column name-values within column)\n","83e75903":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrandom_forest.fit(train_X, train_Y)\nY_pred_rf = random_forest.predict(test_X)\nrandom_forest.score(train_X,train_Y)\nacc_random_forest = round(random_forest.score(train_X, train_Y) * 100, 2)\nprint(acc_random_forest)","7c3721bc":"\n\nprint(\"Feature selection\")\npd.Series(random_forest.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8) #in a series x (theirs relative importance) and y values are taken\n","5984b48b":"\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_rf})","ef1b2947":"submission.head()","41cf717c":"submission.to_csv('submission.csv', index=False)","64df605f":"Kaggle offers very nice interface to look at the data. Basically what we have here is:\n\n\nsurvival Survival 0 = No, 1 = Yes\n\npclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex: Male or Female\n\nAge: Age in years\n\nsibsp: # of siblings \/ spouses aboard the Titanic\n\nparch: # of parents \/ children aboard the Titanic\n\nticket: Ticket number\n\nfare: Passenger fare\n\ncabin: Cabin number\n\nembarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n**Missing Values**\nLet us define a function that will find the missing values and also plot the frequency of them. With this information we can impute the values accordingly.  OR as in the case of Cabin feature delete it, since imputing does not make much sense.\n","a258103b":"There are couple of outliers here, we can procede bothways. Here we can learn another lesson regarding modeling. When in doubt try both methods and see what works better.\n","e1b3522c":"Fare and Embarked are two columns where test and train differ in missing values. Nevertheless we should impute both of them. Interesting thing is that embarked is a character column. For that we are going to impute with the .mode() method. Basically we impute the column with the single most occuring element in that column.","bd211983":"![image.png](attachment:image.png)","7d7a24da":"**LogiticRegression**\n\n\n","8cb5098f":"**Data imputation**\n\nFirst of all we need to think is it instincitive to do it? For example Cabin variable should be excluded. \nAge variable can be imputed, but which value? First things that come to mind are mean and median imputation. But one should note that they are not automatically correct and that imputing with these two method can lead to more damage than leaving the data untouched. We are going to focus on these two simple method of imputing numerical data, but for more on imputation please consult [](https:\/\/en.wikipedia.org\/wiki\/Imputation_(statistics))\n\n\nChoosing between mean or median imputation we should look for outliers. If there are signs of them then median is the choice since mean will be heavily affected by them. Depending on the context (and that is general rule!) mean could also be unfavorable because is produces(possibly) value in data set that did not exist beforehand. Let us plot our Age variable","119baced":"Before proceeding let us have a look at how our variables interact with each other. (not the correlation\/heat matrix)\n","2e21476f":"There is a possibility that test and train datasets do not have sama NaN values, especially if they are small. Let us check:\n","29f8ac88":"There are two main steps:\n1.  **Data preparation** We need to make sure that the data is in a format understandable to machine learning algorithms. That implies converting the data in numerical form. Not only that we should also think about whether there are some features (independent variables) that can be improved or even derived from existing ones in order to improve predictive power.\n2. **Modeling** Here we start using our algorithms. There are couple of thinks that we should bare in mind. Are all pre-processing steps satisfactory (i.e. if I am using k-means clustering I am going to need to scale my data etc.), what are my hyper-parameter values and how can I optimize them (we will see use of randomizedSearch() and GridSearch(). How do I evaluate my accuracy? Did I use cross-validation that ensures \"complete data usage\" etc.\n\n**NOTE**\nAs we can see there are a couple of things that we should bare in mind. Ofcourse this list is far from complete but the goal of this kernel is to give a comprehensible start into Kaggle predictive challenges.","e0caa53a":"**Introduction**\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Can we predict who will survive based on certain characteristics?\nGoal of this Kernel is to give a workflow, and a directions how one should procede when faced with any predictive challenge (not only classification as here).\n","4e649830":"**Modeling**\n\nSo how do we procede with model selection. First of all we should define our goal specifically. We want to classify Surivived variable given other features. We are performing supervised classification. There exists a following group of algorithms that we can select from:\n\n\n1. Logistic Regression\n\n2. KNN\n\n3. Support Vector Machines\n\n4. Naive Bayes classifier\n\n5. Decision Tree\n\n6. Random Forrest\n\n7. Linear Discriminant Analysis\n\n8. Ada Boost Classifier\n\n9. Gradient Boosting Classifier\n\n\nBut why choose one of them, let us try all of them and see which one has the best predictive power:\n\n","4fadb564":"Let us now put all of our results in one dataframe and compare the (cross-validated) accuracy.","75130ab6":"Now we compare the tuned parameters and not the tuned ones","3303afa2":"**NOTE** we need to differentiate two data sets here. First of all X_train etc... was created with sklearn in order to obtain accuracy scores. train_X etc. is basically original training data that we use to eventually give predictions. Why did we seperate it? Well our original test data did not include predictions, so we could not use it as hold-out set and test the accuracy. That is why we had to create our own","71ddd4d7":"One last thing before we start with modeling. We have a lot of variables, but question is, is there a lot of correlation between them. Because if there is some of them are redundant, i.e. using onge of them is waste of resources. (Highly positive or negative correlated).  In the following we are going to show-case heat-map of correlation, to get a view of the correlation. Pearson correlation gives quantifies the relationships.\n","633d0b1b":"Good thing about randomForest model is that can be used as PCA, where we can select the most important features with a simple command line. That is important (and can confirm our intution) about what varibles predict the Survive column the best.","dd76e15d":"One possibility is to pair all of the varibales that have coefficient larger or smaller than 0.5 and -0.5 respectively. To keep things simple we are going to skip this step.","22524890":"Now that we know the best parameters for some of our models, let us then use this information to get an accuracy scores! as well as use some other models","cca675f9":"Now we use our best model to create output set (predictions!!!) Please note that here we are working with train_X etc. because that is exactly the original train set with 891 observations (a bit of difference)","ea29e8b2":"Finally the really last step is scaling and data-transformation. In other words when dealing with numerical data on scale, certain algortihms do not like the fact that two seprate columns are not on the same unit of measure, furthermore if we are measuring distance (which is the case with for example KNN than scale will greatly influence the results-- because outlier in one scale is nothing compared to outlier on other scale) then it is important to scale the data.  Before we proceede with scaling, let us just say that log-transformation (generally Cox-Box transformation) is not necessary for 2 main reasons: 1. We do not need to assume normality 2. We are not dealing with prices (the whole discussion with multiplicative vs. additive interpretation can be found online.\n\nThat is OK, but what do we do with our 2 numerical columns, Age and Fare?\n","7ea42a75":"We are slowly getting our dataset ready. What variables are useless, drop them! and can we create new features?\n","f4a6aa5c":"Let us check that we do not have anymore missing values and procede with feature engineering.\n\n","93041a51":"One hot encoding so that algorithm can understand","30bdddf2":"**Feature engineering**\n\nQuestion that we should as ourselves is how can I combine&extract additional information&features from the existing ones. Let us us start with character column Namne. We can see that there are some reguarities, i.e. titles like Mrs, Miss etc. are repetative. Surely they can give us some information. We need to extract it. For that we will use re library (regular expression). In a nutshell we just extrapolate the text of interest from a text corpus using regular expressions and synthax in re library. We will be defining a function that does the heavy lifting.\n","f7c14c53":"Comprehensive titanic challenge. Additionally I show how you can deploy the titanic machine learning solution on my free udemy course [Udemy ML-deployment](https:\/\/www.udemy.com\/course\/differentially-private-web-applications\/)","0f123eb7":"We got our optimal parameters for the first algorithm. Let us check for all of them. Before we proceede one should note that more effective grid search (search for parameters) can be done with randomizedSearch() method. Please see the discussion here [http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html](http:\/\/)","5891bbb2":"**Random Forest parameter tuning and other models:**","282c88f0":"**eXtreme Gradient Boosting** \nLet us start with one of the favorite algortihms on Kaggle, xGB. Now this is a implementation tutorial and we will not describe the inner-workings of this algorithm. Let us just say that gradient boosting algorithms are ensambling (\"add-on\") algorithms, where each consecutive simple model (algo) is trained on residuals of the previous one. And in the sense it learns from previous mistakes.  Below we will define parameter Grid, i.e. all the possible paramters that our algorithm can take, and with help from sklearn library tools we are going to determine what are the most optimal hyper-parameters.\nDisclaimer!: Do not get confused between hyper-parameters and \"normal\" parameters ,i.e. weights. Hyper-parameters are manually selected and can not te made better with training set. Weights (as in for example in simple linear regression or neural networks) are exactly what model learns as we are training it.\n"}}