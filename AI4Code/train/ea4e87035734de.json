{"cell_type":{"01c63f8d":"code","07885a6f":"code","7187a162":"code","37b6a94f":"code","89c77394":"code","f6b3bc6c":"code","4cff7268":"code","c4c1d6f0":"code","e8a414c1":"code","20f382a9":"code","486bbf6e":"code","9c6f8660":"code","e64d558b":"code","42c04d70":"code","945626b0":"code","039ce733":"code","e8213c9d":"code","59c5bc42":"code","503c1b94":"code","1cab1f5c":"markdown","04dbeb40":"markdown","ed9fc65b":"markdown","9359dd83":"markdown","d2f26a53":"markdown","e33cf13d":"markdown","27011b6a":"markdown","f95e031b":"markdown","37587da8":"markdown"},"source":{"01c63f8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import  PCA\nimport tensorflow as tf\nimport os\nos.listdir('..\/input')\n# Any results you write to the current directory are saved as output.","07885a6f":"df=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n","7187a162":"df.info()","37b6a94f":"df.isnull().sum().head()","89c77394":"test.isnull().sum().head()","f6b3bc6c":"print(df['label'].value_counts())","4cff7268":"plt.figure(figsize=(15,7))\nsns.countplot(data=df,x='label',palette='Paired')\nplt.xlabel('Digits')\nplt.title('Frequency of Digits')","c4c1d6f0":"plt.figure(figsize=(15,5))\nsns.kdeplot(df.iloc[:,0])","e8a414c1":"def show_digits(x,nrows,ncols):\n    fig,axis=plt.subplots(nrows,ncols)\n    fig.set_figheight(18)\n    fig.set_figwidth(15)\n    for i in range(nrows):\n        for j in range(ncols):\n            count=(np.random.rand(1)*100).astype('int')\n            sns.heatmap(x.iloc[count,1:].values.reshape(28,28),ax=axis[i][j],xticklabels=False,yticklabels=False,cbar=False)\n            axis[i,j].title.set_text(x.iloc[count,0].values[0])\n            count+=1  \nprint('random images of given pixes in heatmap ')\nshow_digits(df,5,3)      \n","20f382a9":"df=pd.concat([df,test])","486bbf6e":"x=df.iloc[:,1:].values\ny=df.iloc[:,0].values","9c6f8660":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nx=sc.fit_transform(x)","e64d558b":"def comm_sum(x):\n    total=0\n    arr=np.array([])\n    for i in x:\n        total+=i\n        arr=np.append(arr,total)\n    return arr\ndef com_var(x,max_comp):\n    pc=PCA(n_components=max_comp).fit(x)\n    var=pc.explained_variance_ratio_\n    var=comm_sum(var)\n    return var\nvar=com_var(x[:42000,:],500)","42c04d70":"fig=plt.figure(figsize=(12,7))\nplt.plot(var*100,lw=5,c='red')\nplt.xlabel('number of components')\nplt.ylabel('comm. variance percentage')\nplt.xlim(xmin=1)\nplt.title('comulative variance ')","945626b0":"\n# pca=PCA(n_components=300)\n# x=pca.fit_transform(x)\n","039ce733":"xtrain=x[:42000,:]\nxtest=x[42000:,:]\nytrain=y[:42000]","e8213c9d":"\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import  shuffle\n\n\nytrain=ytrain.astype('int')\nt_train=pd.get_dummies(ytrain).values\n\nsc=StandardScaler()\nxtrain=sc.fit_transform(xtrain)\nxtest=sc.transform(xtest)\n\n\ndef forward_test(X,w1,b1,w2,b2,w3,b3,w4,b4): \n    layer1=tf.add(tf.matmul(X,w1),b1)\n    z1=tf.nn.relu(layer1)\n    layer2=tf.add(tf.matmul(z1,w2),b2)\n    z2=tf.nn.relu(layer2)\n    layer3=tf.add(tf.matmul(z2,w3),b3)\n    z3=tf.nn.relu(layer3)\n    layer4=tf.add(tf.matmul(z3,w4),b4)\n    \n    return layer4\n\ndef forward(X,w1,b1,w2,b2,w3,b3,w4,b4):\n    \n    layer1=tf.add(tf.matmul(X,w1),b1)\n    z1=tf.nn.relu(layer1)\n    z1=tf.nn.dropout(x=z1,keep_prob=0.5)\n    layer2=tf.add(tf.matmul(z1,w2),b2)\n    z2=tf.nn.relu(layer2)\n    z2=tf.nn.dropout(x=z2,keep_prob=0.5)\n    layer3=tf.add(tf.matmul(z2,w3),b3)\n    z3=tf.nn.relu(layer3)\n    z3=tf.nn.dropout(x=z3,keep_prob=0.5)\n    layer4=tf.add(tf.matmul(z3,w4),b4)\n    \n    return layer4\n  \ng1=tf.Graph()\n\n\nwith g1.as_default():\n    m1=1500\n    m2=1300\n    m3=1100\n    d=xtrain.shape[1]\n    k=t_train.shape[1]\n    \n    w1=tf.Variable(tf.random_normal([d,m1])*tf.sqrt(2\/d))\n    w2=tf.Variable(tf.random_normal([m1,m2])*tf.sqrt(2\/m1))\n    w3=tf.Variable(tf.random_normal([m2,m3])*tf.sqrt(2\/m2))\n    w4=tf.Variable(tf.random_normal([m3,k]))\n\n\n    b1=tf.Variable(tf.random_normal([m1]))\n    b2=tf.Variable(tf.random_normal([m2]))\n    b3=tf.Variable(tf.random_normal([m3]))\n\n    b4=tf.Variable(tf.random_normal([k]))\n\n    \n    tfx=tf.placeholder(tf.float32,[None,d])\n    tfy=tf.placeholder(tf.float32,[None,k])\n    \n    logits=forward(tfx,w1,b1,w2,b2,w3,b3,w4,b4)\n    t_logits=forward_test(tfx,w1,b1,w2,b2,w3,b3,w4,b4)\n    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tfy,logits=logits))\n    opt=tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n    predict=tf.argmax(t_logits,axis=1)\n       \n    \ndef model():\n    ypred=[]\n    with  tf.Session(graph=g1) as sess:\n        init=tf.global_variables_initializer()\n        sess.run(init)\n        epoch=1000\n        batch_size=300\n        n_batches=xtrain.shape[0]\/\/batch_size\n        x,y=xtrain,t_train\n        x,y=shuffle(x,y)\n        for i in range(epoch):\n            for j in range(n_batches):\n                xt=x[j*batch_size:(j*batch_size+batch_size)]\n                yt=y[j*batch_size:(j*batch_size+batch_size)]\n                sess.run(opt,feed_dict={tfx:xt,tfy:yt})\n            print('accuracy-'+str(np.mean(ytrain==sess.run(predict,feed_dict={tfx:xtrain}))))\n        yhat=sess.run(predict,feed_dict={tfx:xtest})\n        sess.close()\n    ypred=yhat\n    return ypred\n        \nytest=model()\n\n\n\n\n    ","59c5bc42":"y=pd.DataFrame(ytest)","503c1b94":"y.to_csv('output.csv')","1cab1f5c":"- We can see that 1 has the highest frequency\n-  The data set is pretty stable with lables categories","04dbeb40":"This dataset is a 28*28 =784  pixels reperesentation of handwritten digits\n- The input features are pixels intesity and the we have to predict the digit \n- hence its a classification problem where we have to find the the number digit from input pixels[](http:\/\/)","ed9fc65b":"### Visualising random images from given 785 pixes\n - Train set has preety well accurate labels for confusing images","9359dd83":"### Missing values\n> dataset is pretty much  cleaned\n> it is compromised of huge dimetions","d2f26a53":"Frequency of unique numbers(labels) in decending order","e33cf13d":"#### Feature Selection","27011b6a":"> ## Kernel desity estimation","f95e031b":"# MNIST Classification","37587da8":"> There are 786 columns which  out of which first is our label and 785 are the pixels and are input features\n"}}