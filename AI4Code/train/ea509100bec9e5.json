{"cell_type":{"57334644":"code","905d7c39":"code","317c303c":"code","f078197c":"code","dfeb1366":"code","f05bdcb0":"code","d42e2eb8":"code","97279695":"code","6e63158c":"code","3cf153a4":"code","4939f94f":"code","4a1838b1":"code","50cd0601":"code","eaa8cb4d":"code","0e95278a":"code","3ff5fd50":"code","cd989204":"code","b01c9644":"code","e28ef3ec":"code","bfa08191":"code","8b7bfdb3":"code","20748926":"code","be1e448f":"code","3a8c6e35":"code","4c68e187":"code","0185a7eb":"code","594f742c":"code","00fe0211":"code","bbd5072d":"code","9401b750":"code","896cd4e7":"code","70b16c8b":"code","7ac1c22b":"code","1389cb4b":"code","9a8a3640":"code","4224e60b":"code","ea0633a0":"code","6d22234b":"code","e58a6f29":"code","1d9c6b84":"code","78d02a93":"code","6bc6edc3":"code","c2c47f4f":"code","559f665b":"code","de58fe08":"code","c543c69c":"code","b2bebdf9":"code","6359167f":"code","20ad2774":"code","b624a207":"code","25f94ffd":"code","4a59496c":"code","9e7226a0":"code","7809ee45":"code","179005a7":"code","d625e054":"code","ace40ec9":"code","f2a8491a":"code","3366394e":"markdown","602ab6d6":"markdown","efc12a9b":"markdown","59d7fe26":"markdown","937c5e8c":"markdown","a33cb7d5":"markdown","100ebeba":"markdown"},"source":{"57334644":"import numpy as np                                                 # Implemennts milti-dimensional array and matrices\nimport pandas as pd                                                # For data manipulation and analysis\nimport pandas_profiling\nimport matplotlib.pyplot as plt                                    # Plotting library for Python programming language and it's numerical mathematics extension NumPy\nimport seaborn as sns                                              # Provides a high level interface for drawing attractive and informative statistical graphics\n%matplotlib inline\nsns.set()\n\nfrom subprocess import check_output\nfrom collections import Counter\nimport os","905d7c39":"# Uncomment on kaggle to know data path\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","317c303c":"base_path='\/kaggle\/input\/titanic\/'","f078197c":"train = pd.read_csv(base_path+'train.csv')\ntest = pd.read_csv(base_path+'test.csv')","dfeb1366":"print(train.shape)\nprint(test.shape)","f05bdcb0":"test_PassengerId = test[\"PassengerId\"]","d42e2eb8":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers  ","97279695":"Outliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","6e63158c":"train[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].describe()","3cf153a4":"train.loc[Outliers_to_drop] # Show the outliers rows","4939f94f":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","4a1838b1":"print(train.shape)\nprint(test.shape)","50cd0601":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","eaa8cb4d":"# dataset = dataset.fillna(np.nan)","0e95278a":"miss = dataset.isna().sum().sort_values().reset_index()\nmiss.columns = ['column', 'total_miss']\npercent = (dataset.isna().sum()\/dataset.shape[0])*100\npercent = percent.sort_values().reset_index()\npercent.columns = ['column', 'percent_miss']\nmiss = pd.merge(left=miss, right=percent, on='column')\nmiss.sort_values(by='percent_miss', ascending=False)","3ff5fd50":"#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","cd989204":"dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(dataset['Embarked'].mode()[0])\n","b01c9644":"index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med\n","e28ef3ec":"dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']+1","bfa08191":"dataset['GenderClass'] = dataset.apply(lambda x: 'child' if x['Age'] < 15 else x['Sex'],axis=1)","8b7bfdb3":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].head()","20748926":"dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","be1e448f":"dataset = pd.get_dummies(dataset, columns=['GenderClass','Embarked'])","3a8c6e35":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","4c68e187":"# dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()","0185a7eb":"# dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","594f742c":"# dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","00fe0211":"# Ticket = []\n# for i in list(dataset.Ticket):\n#     if not i.isdigit() :\n#         Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n#     else:\n#         Ticket.append(\"X\")\n        \n# dataset[\"Ticket\"] = Ticket\n# dataset[\"Ticket\"].head()","bbd5072d":"# dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","9401b750":"dataset = dataset.drop(['Name','Sex','SibSp','Parch', 'PassengerId', 'Cabin','Ticket'], axis = 1)\n","896cd4e7":"dataset.head(5)","70b16c8b":"train = dataset.iloc[:train_len]\ntest = dataset.iloc[train_len:]\ntest.drop(['Survived'], axis=1, inplace=True)","7ac1c22b":"train.Survived = train.Survived.astype('int64')","1389cb4b":"print(train.shape)\nprint(test.shape)","9a8a3640":"sns.pairplot(train[[\"Fare\",\"Age\",\"Survived\"]],vars = [\"Fare\",\"Age\"],hue=\"Survived\", dropna=True,markers=[\"o\", \"s\"])\nplt.title('Pair Plot')","4224e60b":"corr = train.corr()[['Age', 'Fare', 'FamilySize', 'Title', 'GenderClass_female', 'Embarked_Q', 'Pc_1', 'Survived']]","ea0633a0":"plt.figure(figsize=(10,10))\nsns.heatmap(corr,vmax=.8,linewidth=.01, square = True, annot = True,cmap='YlGnBu',linecolor ='black')\nplt.title('Correlation between features')\nplt.ylim(10, 0)","6d22234b":"corr['Survived'].sort_values().plot(kind='bar')","e58a6f29":"sns.countplot(x='Survived',data=train)","1d9c6b84":"X = train.loc[:,train.columns != 'Survived']\nX.head()","78d02a93":"y = train.Survived ","6bc6edc3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","c2c47f4f":"print(X_train.shape)\nprint(y_train.shape)","559f665b":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(max_iter=300)\nlogreg.fit(X_train,y_train)","de58fe08":"y_pred_train = logreg.predict(X_train)  ","c543c69c":"y_pred_test = logreg.predict(X_test)                                                           # make predictions on the testing set","b2bebdf9":"from sklearn.metrics import accuracy_score\nprint('Accuracy score for test data is:', accuracy_score(y_test,y_pred_test))","6359167f":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_test))\n\nprint(confusion_matrix)","20ad2774":"confusion_matrix.index = ['Actual Died','Actual Survived']\nconfusion_matrix.columns = ['Predicted Died','Predicted Survived']\nprint(confusion_matrix)","b624a207":"preds1 = np.where(logreg.predict_proba(X_test)[:,1]> 0.75,1,0)\nprint('Accuracy score for test data is:', accuracy_score(y_test,preds1))","25f94ffd":"preds2 = np.where(logreg.predict_proba(X_test)[:,1]> 0.25,1,0)\nprint('Accuracy score for test data is:', accuracy_score(y_test,preds2))","4a59496c":"test.shape","9e7226a0":"test.info()","7809ee45":"final_pred = logreg.predict(test)                                                           # make predictions on the testing set","179005a7":"final_pred","d625e054":"sub = pd.DataFrame()\nsub['PassengerId'] = test_PassengerId\nsub['Survived'] = final_pred\nsub.to_csv('submission.csv',index=False)","ace40ec9":"sub.shape","f2a8491a":"sub.head(5)","3366394e":"# Submission","602ab6d6":"# Feature Engineering","efc12a9b":"# Model Evaluation","59d7fe26":"# Missing Values and NA treatment","937c5e8c":"# joining train and test","a33cb7d5":"# Outlier treatment","100ebeba":"# Modeling"}}