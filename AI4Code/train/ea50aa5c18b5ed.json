{"cell_type":{"20a047e7":"code","c68ae0c5":"code","c9db2ad6":"code","e4ddcd4b":"code","48bc4fc4":"code","4d3ef711":"code","a562a5d6":"code","1e8f2519":"code","e5bfd47b":"code","fe55a0db":"code","bf86bba9":"code","cc706f92":"code","bd734f7c":"code","4d457f87":"code","b8a80d3e":"code","5e9d221c":"markdown","4bd8e4fe":"markdown"},"source":{"20a047e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nnp.random.seed(0)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c68ae0c5":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngs = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\n#drop not much useful columns\ntrain = train.drop(['Cabin','Ticket'],axis=1)\ntest = test.drop(['Cabin','Ticket'],axis=1)","c9db2ad6":"def binAges(x):\n    if x<18:\n        return 1\n    elif 17<x<35:\n        return 2\n    elif 34<x<51:\n        return 3\n    elif 50<x<66:\n        return 4\n    elif x<65:\n        return 5\n\n#get Mr mrs etc\ndef stripit(x):\n    return x.split(',')[1].split('.')[0].strip()    ","e4ddcd4b":"title_dict = {\"Capt\": \"Rare\",\"Col\": \"Rare\",\"Major\": \"Rare\",\"Jonkheer\": \"Rare\",\"Don\": \"Rare\",\"Dona\": \"Miss\",\n    \"Sir\" : \"Rare\",\"Dr\": \"Rare\",\"Rev\": \"Rare\",\"the Countess\":\"Rare\",\"Mme\": \"Mrs\",\"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\"Mr\" : \"Mr\",\"Mrs\" : \"Mrs\",\"Miss\" : \"Miss\",\"Master\" : \"Master\",\"Lady\" : \"Rare\"}\ntitle_map = {\"Mr\": 1, \"Master\": 2, \n    \"Mrs\": 3, \"Miss\": 4, \n    \"Rare\": 5}","48bc4fc4":"def preprocess(df):\n    df['desig'] = df.Name.apply(stripit)\n    df.desig = df.desig.map(title_dict)\n    df.desig = df.desig.map(title_map)\n\n    #bin ages and fill with mean\n    mean = df.Age.mean()\n    df.Age = df.Age.fillna(mean)\n    df.Age = df.Age.apply(binAges)\n\n    #create family and individual features\n    df['Family'] = df.Parch + df.SibSp\n    return df","4d3ef711":"processed_train = preprocess(train).dropna(axis=0)","a562a5d6":"X = processed_train.drop(['PassengerId','Name','Survived','SibSp','Parch'],axis=1)\ny = processed_train.Survived\ntrain.corr().style.background_gradient()","1e8f2519":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_d = pd.get_dummies(X)\nmms = MinMaxScaler().fit(X_d)\nX_d = mms.transform(X_d)\nX_train, X_val, y_train, y_val = train_test_split(X_d,y,test_size=0.2,random_state=0)","e5bfd47b":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=111)\nrf.fit(X_train, y_train)\nrf.score(X_val, y_val)","fe55a0db":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, recall_score\n\nparam_grid = { \n    'n_estimators': [100, 200, 300],\n    'max_depth' : [5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, scoring='accuracy', cv=5)\ngrid_search.fit(X_train, y_train)\nprint('best params:{}\\nbest score:{}'.format(str(grid_search.best_params_), str(grid_search.best_score_)))","bf86bba9":"rf = RandomForestClassifier(n_estimators=100,max_depth=5,criterion='gini',random_state=111)\nrf.fit(X_train, y_train)\nrf.score(X_val, y_val)","cc706f92":"processed_test = preprocess(test).fillna(0)\nX_test = processed_test.drop(['PassengerId','Name','SibSp','Parch'],axis=1)\nX_test = pd.get_dummies(X_test)\nX_test = mms.transform(X_test)\npred = rf.predict(X_test)\nsubmit = pd.DataFrame({'PassengerId':test.PassengerId,'Survived':sample}).reset_index(drop=True)\nsubmit.to_csv('Submission.csv',index=False)","bd734f7c":"from keras.models import Sequential \nfrom keras.layers import Activation, Dense \nmodel = Sequential()\nmodel.add(Dense(50, input_shape=(10,), activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","4d457f87":"model.fit(X_train,y_train,validation_split=0.33,epochs=400,validation_data=(X_val, y_val),batch_size=32)","b8a80d3e":"processed_test = preprocess(test).fillna(0)\nX_test = processed_test.drop(['PassengerId','Name','SibSp','Parch'],axis=1)\nX_test = pd.get_dummies(X_test)\nX_test = mms.transform(X_test)\nsample = map(int,np.round(model.predict(X_test)).reshape(418,))\nsubmit = pd.DataFrame({'PassengerId':test.PassengerId,'Survived':sample}).reset_index(drop=True)\nsubmit.to_csv('Submission.csv',index=False)","5e9d221c":"# Random Forest","4bd8e4fe":"# Neural nets"}}