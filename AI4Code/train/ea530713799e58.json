{"cell_type":{"4d2b412d":"code","26ca419e":"code","8f9a9576":"code","d675bd88":"code","c2fdc15a":"code","0b36aaae":"code","03e3dc63":"code","c922ad75":"code","96342898":"code","7e9452dc":"code","f45d62b4":"code","78624797":"code","d1c58917":"code","78ca8926":"code","9bb417fe":"code","b2d6be3a":"code","8904f042":"code","152fd9c0":"code","7a2be47a":"code","76bfe1fc":"code","da7c2e3d":"code","26c44d54":"code","123c3e73":"code","535a6aaf":"code","23bf6a66":"markdown","4764df4f":"markdown","6c5bbee9":"markdown","6feb03c4":"markdown","fc5625cb":"markdown","74164ba7":"markdown","62540905":"markdown","71be5361":"markdown"},"source":{"4d2b412d":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))","26ca419e":"df = pd.read_csv('..\/input\/athlete_events.csv')\ndf.head()","8f9a9576":"df.info()","d675bd88":"df.isnull().sum()","c2fdc15a":"df.dropna(inplace=True)","0b36aaae":"df.describe()","03e3dc63":"y = np.array([len(df[df.Sex =='F']),len(df[df.Sex =='M'])])\nx = ['Female','Male']\nplt.bar(x,y)","c922ad75":"fig = plt.subplots(figsize=(10,5))\nd = sns.countplot(x= 'Sport', data=df, hue='Season',palette='bright')\n_ = plt.setp(d.get_xticklabels(),rotation = 90)","96342898":"fig = plt.subplots(figsize=(10,5))\nd = sns.countplot(x='Team' ,data=df.head(100), hue='Medal',palette='bright')\n_ = plt.setp(d.get_xticklabels(),rotation=90)","7e9452dc":"plt.scatter(df.Height,df.Weight)\nplt.xlabel('Heighy')\nplt.ylabel('Weight')","f45d62b4":"fig = plt.subplots(figsize=(10,5))\nd = sns.countplot(x='Age',data=df,hue='Medal',palette='bright')\n_ = plt.setp(d.get_xticklabels(),rotation=90)","78624797":"d = sns.countplot(x='Season',data=df , hue='Medal',palette='muted')","d1c58917":"fig = plt.subplots(figsize=(10,5))\nd = sns.countplot(x='City',data=df,hue='Season',palette='muted')\n_ = plt.setp(d.get_xticklabels(),rotation=90)","78ca8926":"fig = plt.subplots(figsize=(10,5))\nd = sns.countplot(x='Year',data=df,hue='Season',palette='muted')\n_ = plt.setp(d.get_xticklabels(),rotation=90)","9bb417fe":"df['Sex'] = df['Sex'].apply(lambda x: int(str(x).replace('F','1')) if 'F' in str(x) else x)\ndf['Sex'] = df['Sex'].apply(lambda x: int(str(x).replace('M','0')) if 'M' in str(x) else x)\ndf['Season'] = df['Season'].apply(lambda x: int(str(x).replace('Winter','1')) if 'Winter' in str(x) else x)\ndf['Season'] = df['Season'].apply(lambda x: int(str(x).replace('Summer','0')) if 'Summer' in str(x) else x)\ndf.info()","b2d6be3a":"x = df[['Year','Height','Weight','Age','Sex']]\ny = df['Season']","8904f042":"from sklearn.model_selection import train_test_split","152fd9c0":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","7a2be47a":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=10)","76bfe1fc":"lcr = LogisticRegression()\nlcr.fit(x_train,y_train)\npredict_lcr = lcr.predict(x_test)\nprint('Score:',lcr.score(x_test,y_test))\nprint('f1 score:',f1_score(y_test,predict_lcr,average='micro'))\nprint('precision score:',precision_score(y_test,predict_lcr,average='micro'))\nprint('recall score:',recall_score(y_test,predict_lcr,average='micro'))\n\ncm_lcr = confusion_matrix(y_test,predict_lcr)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_lcr,ax=ax,annot=True,linewidth=0.5,fmt='.0f')","da7c2e3d":"svm = SVC()\nsvm.fit(x_train,y_train)\npredict_svm = svm.predict(x_test)\nprint('Score:',svm.score(x_test,y_test))\nprint('f1 score:',f1_score(y_test,predict_svm,average='micro'))\nprint('precision score:',precision_score(y_test,predict_svm,average='micro'))\nprint('recall score:',recall_score(y_test,predict_svm,average='micro'))\n\ncm_svm = confusion_matrix(y_test,predict_svm)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svm,ax=ax,annot=True,linewidth=0.5,fmt='.0f')","26c44d54":"gnb = GaussianNB()\ngnb.fit(x_train,y_train)\npredict_gnb = gnb.predict(x_test)\nprint('Score:',gnb.score(x_test,y_test))\nprint('f1 score:',f1_score(y_test,predict_gnb,average='micro'))\nprint('precision score:',precision_score(y_test,predict_gnb,average='micro'))\nprint('recall score:',recall_score(y_test,predict_gnb,average='micro'))\n\ncm_gnb = confusion_matrix(y_test,predict_gnb)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_gnb,ax=ax,annot=True,linewidth=0.5,fmt='.0f')","123c3e73":"dtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\npredict_dtc = dtc.predict(x_test)\nprint('Score:',dtc.score(x_test,y_test))\nprint('f1 score:',f1_score(y_test,predict_dtc,average='micro'))\nprint('precision score:',precision_score(y_test,predict_dtc,average='micro'))\nprint('recall score:',recall_score(y_test,predict_dtc,average='micro'))\n\ncm_dtc = confusion_matrix(y_test,predict_dtc)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_dtc,ax=ax,annot=True,linewidth=0.5,fmt='.0f')","535a6aaf":"rfc = RandomForestClassifier(n_estimators=100,random_state=10)\nrfc.fit(x_train,y_train)\npredict_rfc = rfc.predict(x_test)\nprint('Score:',rfc.score(x_test,y_test))\nprint('f1 score:',f1_score(y_test,predict_rfc,average='micro'))\nprint('precision score:',precision_score(y_test,predict_rfc,average='micro'))\nprint('recall score:',recall_score(y_test,predict_rfc,average='micro'))\n\ncm_rfc = confusion_matrix(y_test,predict_rfc)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_rfc,ax=ax,annot=True,linewidth=0.5,fmt='.0f')","23bf6a66":"advantages of decision trees:\nSimple to understand and to interpret. Trees can be visualised.\nRequires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\nUses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\nPossible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.","4764df4f":"A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","6c5bbee9":"Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem.","6feb03c4":"Bayes theorem provides a way of calculating the posterior probability, P(c|x), from P(c), P(x), and P(x|c). Naive Bayes classifier assume that the effect of the value of a predictor (x) on a given class \u00a9 is independent of the values of other predictors. This assumption is called class conditional independence\nhere is great explaination:https:\/\/medium.com\/machine-learning-researcher\/naive-bayes-classifier-in-machine-learning-8d77b2fd1434","fc5625cb":"![](http:\/\/)","74164ba7":"The advantages of support vector machines are:\n\nEffective in high dimensional spaces.\nStill effective in cases where number of dimensions is greater than the number of samples.\nUses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\nVersatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\nThe disadvantages of support vector machines include:\n\nIf the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\nSVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation","62540905":"drop nun","71be5361":"let`s see the propotion about gender"}}