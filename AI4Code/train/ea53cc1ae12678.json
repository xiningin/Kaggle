{"cell_type":{"7e397c9f":"code","90b071cf":"code","602014e8":"code","0ee63f2c":"code","fc0acb97":"code","49363c70":"code","4b016bc4":"code","b6e4cf47":"code","ac5dbaf7":"code","b2370ba9":"code","5c0ad7a0":"code","7f76a9bd":"code","6972c24f":"code","651afbdd":"code","b595626a":"code","1f138041":"code","d2dd5094":"code","43a896c2":"code","e31d1b61":"code","0c3f6a2b":"code","e5c6d3c1":"code","6446ccc6":"code","8840b223":"code","9c26b3b5":"code","4b242f60":"code","7676182f":"code","c00108a5":"code","cc8780c7":"code","a3ed33d3":"code","d595f21f":"code","aa70cbad":"code","b88881d7":"code","3cbc5f57":"code","98efe128":"code","d896c89e":"code","5ac33d9b":"code","dc3669bc":"code","dffb4194":"code","c71bb8de":"code","6909c0de":"code","fd173431":"code","7ef26cc4":"code","32561929":"code","40b7ce7e":"code","a2dd16f6":"code","d4840a22":"code","e82c72b0":"code","e5f61f1d":"code","b5210e9d":"code","39be6cba":"code","56a35d13":"code","98fb21a9":"code","608817a8":"code","1b59c0ab":"code","247d1792":"markdown","247814b7":"markdown","5da23730":"markdown","a4d5b38a":"markdown","32056288":"markdown","eaf8858b":"markdown","0aa4acde":"markdown","2f7f791e":"markdown","eff4520b":"markdown","35b885e7":"markdown","613aae64":"markdown","92099b53":"markdown","730a896a":"markdown","2a95c36a":"markdown","5e5e56fc":"markdown","8a245cc2":"markdown","47652181":"markdown","14be4336":"markdown","d6cf3ebe":"markdown","67ba613e":"markdown","ece66b57":"markdown","9d66ed57":"markdown","7a021e0e":"markdown","fccf7983":"markdown","f612ceac":"markdown","699501ee":"markdown","e27640e9":"markdown","6b6d0ba8":"markdown","d035c78a":"markdown","79184121":"markdown","21bd893d":"markdown","dc798327":"markdown","eeb2ce30":"markdown","db89bf6b":"markdown","5a4f9aa7":"markdown"},"source":{"7e397c9f":"# Data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning\nimport sklearn as skl\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_absolute_error\n#from sklearn.preprocessing import PolynomialFeatures\n#from sklearn.linear_model import Lasso\n#from sklearn.linear_model import LinearRegression\n#from sklearn.metrics import normalized_mutual_info_score\n#from sklearn.ensemble import RandomForestRegressor\n#from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# File handling\nimport os\nimport gc\ngc.enable()\nprint(os.listdir(\"..\/input\"))","90b071cf":"training_df = pd.read_csv(\"..\/input\/train_V2.csv\")\ntesting_df = pd.read_csv(\"..\/input\/test_V2.csv\")","602014e8":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","0ee63f2c":"training_df.head()","fc0acb97":"training_df.info(verbose=True)\nprint('_'*40)\ntesting_df.info(verbose=True)\n","49363c70":"training_df.describe()","4b016bc4":"training_df = training_df.drop(['Id', 'longestKill', 'rankPoints', 'numGroups', 'matchType'], axis=1)\ntesting_df = testing_df.drop(['Id', 'longestKill', 'rankPoints', 'numGroups', 'matchType'], axis=1)","b6e4cf47":"training_df['kills'].value_counts().tail(10)","ac5dbaf7":"training_df = training_df.drop(training_df[training_df.kills > 20].index)\ntraining_df['kills'].value_counts()","b2370ba9":"training_df = training_df.drop(training_df[training_df.DBNOs > 15].index)\ntraining_df['DBNOs'].value_counts()","5c0ad7a0":"training_df = training_df.drop(training_df[training_df.weaponsAcquired > 30].index)\ntraining_df['weaponsAcquired'].value_counts()","7f76a9bd":"training_df.info(verbose=True)","6972c24f":"training_df.isnull().sum()\ntraining_df = training_df.dropna(how='any',axis=0) \ntraining_df.winPlacePerc.isnull().sum()","651afbdd":"features = list(training_df.columns)\nfeatures.remove(\"matchId\")\nfeatures.remove(\"groupId\")\nfeatures.remove(\"winPlacePerc\")\n\n#Getting match mean features\nprint(\"get match mean feature\")\nmatch_mean = training_df.groupby(['matchId'])[features].agg('mean').reset_index()\ntraining_df = training_df.merge(match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\nmatch_mean = testing_df.groupby(['matchId'])[features].agg('mean').reset_index()\ntesting_df = testing_df.merge(match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n\n#Getting match size features\nprint(\"get match size feature\")\nmatch_size = training_df.groupby(['matchId']).size().reset_index(name='match_size')\ntraining_df = training_df.merge(match_size, how='left', on=['matchId'])\nmatch_size = testing_df.groupby(['matchId']).size().reset_index(name='match_size')\ntesting_df = testing_df.merge(match_size, how='left', on=['matchId'])\n\ndel match_mean, match_size\ngc.collect()","b595626a":"print(\"get group size feature\")\ngroup_size = training_df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\ntraining_df = training_df.merge(group_size, how='left', on=['matchId', 'groupId'])\ngroup_size = testing_df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\ntesting_df = testing_df.merge(group_size, how='left', on=['matchId', 'groupId'])\n\n#print(\"get group mean feature\")\n#group_mean = training_df.groupby(['matchId','groupId'])[features].agg('mean')\n#group_mean_rank = group_mean.groupby('matchId')[features].rank(pct=True).reset_index()\n#training_df = training_df.merge(group_mean.reset_index(), suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\n#training_df = training_df.merge(group_mean_rank, suffixes=[\"\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n#group_mean = testing_df.groupby(['matchId','groupId'])[features].agg('mean')\n#group_mean_rank = group_mean.groupby('matchId')[features].rank(pct=True).reset_index()\n#testing_df = testing_df.merge(group_mean.reset_index(), suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\n#testing_df = testing_df.merge(group_mean_rank, suffixes=[\"\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n\nprint(\"get group max feature\")\ngroup_max = training_df.groupby(['matchId','groupId'])[features].agg('max')\ngroup_max_rank = group_max.groupby('matchId')[features].rank(pct=True).reset_index()\ntraining_df = training_df.merge(group_max.reset_index(), suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ntraining_df = training_df.merge(group_max_rank, suffixes=[\"\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\ngroup_max = testing_df.groupby(['matchId','groupId'])[features].agg('max')\ngroup_max_rank = group_max.groupby('matchId')[features].rank(pct=True).reset_index()\ntesting_df = testing_df.merge(group_max.reset_index(), suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ntesting_df = testing_df.merge(group_max_rank, suffixes=[\"\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n\nprint(\"get group min feature\")\ngroup_min = training_df.groupby(['matchId','groupId'])[features].agg('min')\ngroup_min_rank = group_min.groupby('matchId')[features].rank(pct=True).reset_index()\ntraining_df = training_df.merge(group_min.reset_index(), suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ntraining_df = training_df.merge(group_min_rank, suffixes=[\"\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\ngroup_min = testing_df.groupby(['matchId','groupId'])[features].agg('min')\ngroup_min_rank = group_min.groupby('matchId')[features].rank(pct=True).reset_index()\ntesting_df = testing_df.merge(group_min.reset_index(), suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ntesting_df = testing_df.merge(group_min_rank, suffixes=[\"\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n\ndel group_size, group_max, group_max_rank, group_min, group_min_rank\ngc.collect()","1f138041":"#print(\"get group mean feature\")\n#group_mean = training_df.groupby(['matchId','groupId'])[features].agg('mean')\n#group_mean_rank = group_mean.groupby('matchId')[features].rank(pct=True).reset_index()\n#training_df = training_df.merge(group_mean.reset_index(), suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\n#training_df = training_df.merge(group_mean_rank, suffixes=[\"\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])","d2dd5094":"#training_df = reduce_mem_usage(training_df)\n#print('_'*40)\n#testing_df = reduce_mem_usage(testing_df)","43a896c2":"#group_mean = testing_df.groupby(['matchId','groupId'])[features].agg('mean')\n#group_mean_rank = group_mean.groupby('matchId')[features].rank(pct=True).reset_index()\n#testing_df = testing_df.merge(group_mean.reset_index(), suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\n#testing_df = testing_df.merge(group_mean_rank, suffixes=[\"\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n\n#We don't need matchId and groupId anymore\ntraining_df.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\ntesting_df.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)","e31d1b61":"training_df = reduce_mem_usage(training_df)\nprint('_'*40)\ntesting_df = reduce_mem_usage(testing_df)","0c3f6a2b":"training_df['headshotRate'] = training_df['headshotKills'] \/ training_df['kills']\ntraining_df['headshotRate'].fillna(0, inplace=True)\ntraining_df['headshotRate'].replace(np.inf, 0, inplace=True)\ntesting_df['headshotRate'] = testing_df['headshotKills'] \/ training_df['kills']\ntesting_df['headshotRate'].fillna(0, inplace=True)\ntesting_df['headshotRate'].replace(np.inf, 0, inplace=True)\n\ntraining_df['totalDistance'] = training_df['rideDistance'] + training_df['swimDistance'] + training_df['walkDistance']\ntesting_df['totalDistance'] = testing_df['rideDistance'] + testing_df['swimDistance'] + testing_df['walkDistance']\n\ntraining_df['items'] = training_df['heals'] + training_df['boosts']\ntesting_df['items'] = testing_df['heals'] + testing_df['boosts']\n\ntraining_df['healsPerWalkDistance'] = training_df['heals'] \/ training_df['walkDistance']\ntraining_df['healsPerWalkDistance'].fillna(0, inplace=True)\ntraining_df['healsPerWalkDistance'].replace(np.inf, 0, inplace=True)\ntesting_df['healsPerWalkDistance'] = testing_df['heals'] \/ testing_df['walkDistance']\ntesting_df['healsPerWalkDistance'].fillna(0, inplace=True)\ntesting_df['healsPerWalkDistance'].replace(np.inf, 0, inplace=True)\n\ntraining_df['killsPerWalkDistance'] = training_df['kills'] \/ training_df['walkDistance']\ntraining_df['killsPerWalkDistance'].fillna(0, inplace=True)\ntraining_df['killsPerWalkDistance'].replace(np.inf, 0, inplace=True)\ntesting_df['killsPerWalkDistance'] = testing_df['kills'] \/ testing_df['walkDistance']\ntesting_df['killsPerWalkDistance'].fillna(0, inplace=True)\ntesting_df['killsPerWalkDistance'].replace(np.inf, 0, inplace=True)\n\ntraining_df.head()","e5c6d3c1":"# every feature with low number of discrete values (<100). \nfeature_comparison = [\n    'assists',\n    'boosts',\n    'DBNOs',\n    'headshotKills',\n    'heals',\n    'killPlace',\n    'kills',\n    'killStreaks',\n    'maxPlace',\n    'revives',\n    'roadKills',\n    'teamKills',\n    'vehicleDestroys',\n    'weaponsAcquired',\n    'items'\n    ]\n\n#We will store every comparison table in this list\ntable_comparison = []\nrow_axis = 0\ncolumn_axis = 0\n\n#graph individual features\nfig, saxis = plt.subplots(4, 4,figsize=(16,12))\n\n#Creating the comparison dataframes with two columns : feature, and the mean of the winplace percentage\nfor feature in feature_comparison:\n    table_comparison.append(training_df[[feature, 'winPlacePerc']].groupby([feature], as_index=False).mean().sort_values(by=feature, ascending=True))    \n\n#Plotting the win place percentage as a function of each feature\nfor table in table_comparison: \n    sns.scatterplot(x = table.iloc[:,0], y = table.winPlacePerc, ax = saxis[row_axis,column_axis])\n    row_axis += 1\n    if row_axis > 3:\n        row_axis = 0\n        column_axis += 1","6446ccc6":"# every feature with continuous value. \nfeature_comparison_2 = [\n    'damageDealt',\n    'killPoints',\n    'rideDistance',\n    'swimDistance',\n    'walkDistance',\n    'winPoints',\n    'headshotRate',\n    'totalDistance',\n    'healsPerWalkDistance',\n    'killsPerWalkDistance'\n    ]\n\n#We will store every comparison table in this list\ntable_comparison_2 = []\nrow_axis = 0\ncolumn_axis = 0\n\n#graph individual features\nfig, saxis = plt.subplots(4, 3,figsize=(16,12))\n\n#Creating the comparison dataframes with two columns : feature, and the mean of the winplace percentage\nfor feature in feature_comparison_2:\n    table_comparison_2.append(training_df[[feature, 'winPlacePerc']].groupby([feature], as_index=False).mean().sort_values(by=feature, ascending=True))  \n    table_comparison_2[-1][feature + '_binned'] = pd.cut(table_comparison_2[-1][feature], bins = 100, labels=False)\n    table_comparison_2[-1] = table_comparison_2[-1].groupby([feature + '_binned'], as_index=False).mean().sort_values(by=feature + '_binned', ascending=True)\n\n#Plotting the win place percentage as a function of each feature\nfor table in table_comparison_2: \n    sns.scatterplot(x = table.iloc[:,1], y = table.winPlacePerc, ax = saxis[row_axis,column_axis])\n    row_axis += 1\n    if row_axis > 3:\n        row_axis = 0\n        column_axis += 1","8840b223":"feature_MI = [\n    'assists',\n    'boosts',\n    'damageDealt',\n    'DBNOs',\n    'headshotKills',\n    'heals',\n    'killPlace',\n    'killPoints',\n    'kills',\n    'killStreaks',\n    'matchDuration',\n    'maxPlace',\n    'revives',\n    'rideDistance',\n    'roadKills',\n    'swimDistance',\n    'teamKills',\n    'vehicleDestroys',\n    'walkDistance',\n    'weaponsAcquired',\n    'winPoints',\n    'winPlacePerc',\n    'headshotRate',\n    'totalDistance',\n    'items',\n    'healsPerWalkDistance',\n    'killsPerWalkDistance'\n    ]\n\nmutual_info_df = training_df.truncate(after=-1)\n\n#for feature in feature_MI:\n    #mutual_info_df.loc[feature] = pd.Series([np.nan])\n\n#for feature1 in feature_MI:\n    #for feature2 in feature_MI:\n        #mutual_info = normalized_mutual_info_score(training_df[feature1], training_df[feature2], average_method='arithmetic')\n        #if mutual_info == 1:\n            #print('OK')\n        #mutual_info_df[feature1][feature2] = mutual_info","9c26b3b5":"#plt.figure(figsize=(9,7))\n#sns.heatmap(\n    mutual_info_df,\n    xticklabels=mutual_info_df.columns.values,\n    yticklabels=mutual_info_df.columns.values,\n    linecolor='white',\n    linewidths=0.1,\n    cmap=\"RdBu\"\n)\n#plt.show()","4b242f60":"#mutual_info_target_df = abs(mutual_info_df[['winPlacePerc']])\n#mutual_info_target_df = mutual_info_target_df.drop(['winPlacePerc'])\n#mutual_info_target_df['feature'] = mutual_info_target_df.index\n\n#plt.figure(figsize=(10, 6))\n#sns.barplot(x='winPlacePerc', y='feature', data=mutual_info_target_df.sort_values(by=\"winPlacePerc\", ascending=False))\n#plt.title('Mutual Information between each feature and the target value')\n#plt.tight_layout()","7676182f":"#corr_df = training_df.corr()\n\n#plt.figure(figsize=(9,7))\n#sns.heatmap(\n    corr_df,\n    xticklabels=corr_df.columns.values,\n    yticklabels=corr_df.columns.values,\n    linecolor='white',\n    linewidths=0.1,\n    cmap=\"RdBu\"\n)\n#plt.show()","c00108a5":"#corr_target_df = abs(corr_df[['winPlacePerc']])\n#corr_target_df = corr_target_df.drop(['winPlacePerc'])\n#corr_target_df['feature'] = corr_target_df.index\n\n#plt.figure(figsize=(10, 6))\n#sns.barplot(x='winPlacePerc', y='feature', data=corr_target_df.sort_values(by=\"winPlacePerc\", ascending=False))\n#plt.title('Pearson Correlation between each feature and the target value')\n#plt.tight_layout()","cc8780c7":"#training_df_truncated = training_df.truncate(before=50000,after=60000)\n#X_train_truncated = np.asarray(training_df_truncated[['walkDistance','killPlace', 'damageDealt', 'boosts', 'weaponsAcquired']])\n#X_train_truncated = np.float32(X_train_truncated)\n#X_train_truncated = PolynomialFeatures(2, interaction_only=False).fit_transform(X_train_truncated)\n#X_train_truncated[0:5]","a3ed33d3":"#y_train_truncated = np.asarray(training_df_truncated[['winPlacePerc']])\n#y_train_truncated[0:5]","d595f21f":"#print ('Train set truncated:', X_train_truncated.shape,y_train_truncated.shape)","aa70cbad":"#Split the model\ncross_validation_split = model_selection.ShuffleSplit(n_splits = 5, test_size = .3, train_size = .6, random_state = 0)\n#Create dataframe to store results according to degree of polynomial features.\nlasso_results = pd.DataFrame(data = {'degree': [], 'test_score_mean': [], 'fit_time_mean': []})\n#lasso_results = pd.DataFrame(data = {'degree': [], 'test_score_mean': [], 'fit_time_mean': [], 'mean_absolute_error': []})\n\n#Evaluate the model for different dataframes. Each step increases the degree of the PolynomialFeatures function and outputs the accuracy of the model. \nfor degree in range (1,6):\n    X_train_truncated = np.asarray(training_df_truncated[['walkDistance','killPlace', 'damageDealt', 'boosts', 'weaponsAcquired']])\n    X_train_truncated = np.float32(X_train_truncated)\n    X_train_truncated = PolynomialFeatures(degree, interaction_only=False).fit_transform(X_train_truncated)\n    #Evaluate the model\n    cross_validation_results = model_selection.cross_validate(Lasso(alpha = 0.00001, max_iter=10000, normalize=True), X_train_truncated, y_train_truncated, cv = cross_validation_split, return_train_score = True)\n    #The line below is here if you want to see the effect of LASSO compared to a classic linear regression.\n    #cross_validation_results = model_selection.cross_validate(LinearRegression(), X_train_truncated, y_train_truncated, cv = cross_validation_split, return_train_score = True)\n    #Predicts the target value\n    #y_hat_truncated = Lasso(alpha=0.00001, max_iter=10000, normalize=True).fit(X_train_truncated, y_train_truncated).predict(X_train_truncated)\n    lasso_results = lasso_results.append({'degree' : degree,\n                                          'test_score_mean' : cross_validation_results['test_score'].mean(), \n                                          'fit_time_mean' : cross_validation_results['fit_time'].mean()}, ignore_index=True) \n                                          #'mean_absolute_error' : mean_absolute_error(y_train_truncated, y_hat_truncated)}, ignore_index=True)\n    print('OK degree ' + str(degree))\n        \nsns.pointplot(x = lasso_results.degree, y = lasso_results.test_score_mean)\n#sns.pointplot(x = lasso_results.degree, y = lasso_results.fit_time_mean)\n\n#This part was here to find a good value of alpha where the test_score converge.\n#It showed that alpha = 0.00001 is a good value, in terms of convergence and fit time\n#------------------------------------------------------------------------------------------------------\n#lasso_results = pd.DataFrame(data = {'1 \/ alpha': [], 'test_score_mean': [], 'fit_time_mean': []})\n#lasso_alpha = 1\n#denominator = 1\n\n#for i in range (1,7):\n    #cross_validation_results = model_selection.cross_validate(Lasso(alpha = (lasso_alpha \/ denominator), max_iter=10000, normalize=True), X_train_truncated, y_train_truncated, cv = cross_validation_split, return_train_score = True)\n    #lasso_results = lasso_results.append({'1 \/ alpha' : (denominator), 'test_score_mean' : cross_validation_results['test_score'].mean(), 'fit_time_mean' : cross_validation_results['fit_time'].mean()}, ignore_index=True)\n    #i += 1\n    #denominator *= 10\n#------------------------------------------------------------------------------------------------------","b88881d7":"feature_FS = [\n    'assists',\n    'DBNOs',\n    'headshotKills',\n    'heals',\n    'killPoints',\n    'kills',\n    'killStreaks',\n    'matchDuration',\n    'maxPlace',\n    'revives',\n    'rideDistance',\n    'roadKills',\n    'swimDistance',\n    'teamKills',\n    'vehicleDestroys',\n    'winPoints',\n    'headshotRate',\n    'totalDistance',\n    'items',\n    'healsPerWalkDistance',\n    'killsPerWalkDistance'\n    ]\n\n#Split the model\ncross_validation_split = model_selection.ShuffleSplit(n_splits = 5, test_size = .3, train_size = .6, random_state = 0)\n#Create dataframe to store results according to degree of polynomial features.\nFS_results = pd.DataFrame(data = {'feature': [], 'test_score_mean': [], 'fit_time_mean': [], 'mean_absolute_error': []})\n#Create X_train_truncated. The best new features will be appended to this array\nX_train_truncated = np.asarray(training_df_truncated[['walkDistance','killPlace', \n                                                      'damageDealt', 'boosts', \n                                                      'weaponsAcquired']])\nX_train_truncated = np.float32(X_train_truncated)\n#Number of feature we want to add in the model\nfeatures_to_add = 6\n\n#Loop for adding new feature into the model\n#for i in range(1,features_to_add + 1):\n    #Loops through each feature and computes cross_validation test score with LASSO regression model.\n    #for feature in feature_FS:\n        #Creates a temporary array\n        X_temp = X_train_truncated\n        #Add a new feature to the temporary array, and apply PolynomialFeatures function\n        added_feat = np.asarray(training_df_truncated[[feature]])\n        X_temp = np.append(X_temp, added_feat, axis = 1)\n        X_temp = PolynomialFeatures(3, interaction_only=False).fit_transform(X_temp)\n        #Evaluate the model\n        cross_validation_results = model_selection.cross_validate(Lasso(alpha = 0.00001, max_iter=10000, normalize=True), X_temp, y_train_truncated, cv = cross_validation_split, return_train_score = True)\n        #Predicts the target value\n        y_hat_truncated = Lasso(alpha=0.00001, max_iter=10000, normalize=True).fit(X_temp, y_train_truncated).predict(X_temp)\n        FS_results = FS_results.append({'feature' : feature, \n                                        'test_score_mean' : cross_validation_results['test_score'].mean(), \n                                        'fit_time_mean' : cross_validation_results['fit_time'].mean(), \n                                        'mean_absolute_error' : mean_absolute_error(y_train_truncated, y_hat_truncated)}, ignore_index=True)\n        print('OK for ' + feature)\n    \n    #Store the results into a dataframe, sort it, and choose the best feature to add to the model.\n    FS_results = FS_results.sort_values(by='mean_absolute_error', ascending=True)\n    new_feat = FS_results.feature.iloc[0]\n    new_score = FS_results.test_score_mean.iloc[0]\n    new_MAE = FS_results.mean_absolute_error.iloc[0]\n    new_fit_time = FS_results.fit_time_mean.iloc[0]\n    X_train_truncated = np.append(X_train_truncated, np.asarray(training_df_truncated[[new_feat]]), axis = 1)\n    print(new_feat + ' feature has been added to the model. Test score mean is now ' + str(new_score) + '. Mean absolute error is now ' + str(new_MAE) + '. Fit time mean is now ' + str(new_fit_time) + '.')\n    i += 1","3cbc5f57":"training_df_truncated = training_df.truncate(after=100000)\nX_train_truncated = np.asarray(training_df_truncated.drop(['winPlacePerc'], axis = 1))\nX_train_truncated[0:1]","98efe128":"#X_train = np.asarray(training_df.drop(['winPlacePerc'], axis = 1))\n#X_train[0:1]","d896c89e":"y_train_truncated = np.asarray(training_df_truncated[['winPlacePerc']])\ny_train_truncated[0:5]","5ac33d9b":"#y_train = np.asarray(training_df[['winPlacePerc']])\n#y_train[0:5]","dc3669bc":"del training_df\ngc.collect()","dffb4194":"X_test = np.asarray(testing_df)\nX_test[0:1]","c71bb8de":"del testing_df\ngc.collect()","6909c0de":"print ('Train set truncated:', X_train_truncated.shape,y_train_truncated.shape)\nprint ('Train set:', X_train.shape,y_train.shape)\nprint ('Test set:', X_test.shape)","fd173431":"models = [ \n    RandomForestRegressor(n_estimators=10, criterion = 'mse', oob_score = True, random_state = 1)\n    ]\n\nmodel_results = pd.DataFrame(data = {'test_score_mean': [], 'fit_time_mean': [], 'mean_absolute_error': []})\n\n# Spliting the model\ncross_validation_split = model_selection.ShuffleSplit(n_splits = 5, test_size = .3, train_size = .6, random_state = 0 )\n# Performing shufflesplit cross validation, with the whole training set (the cross_validate function coupled with ShuffleSplit take care of spliting the training set) \n#for model in models:\n    #cross_validation_results = model_selection.cross_validate(model, X_train_truncated, y_train_truncated, \n                                                              cv= cross_validation_split, return_train_score=True)\n    #Predicts the target value on the whole training set\n    y_hat = model.fit(X_train_truncated, y_train_truncated).predict(X_train)    \n    # Checking the mean of test scores for each iteration of the validation    \n    model_results = model_results.append({'test_score_mean' : cross_validation_results['test_score'].mean(), \n                                          'fit_time_mean' : cross_validation_results['fit_time'].mean(), \n                                          'mean_absolute_error' : mean_absolute_error(y_train, y_hat)}, ignore_index=True) \n \nmodel_results","7ef26cc4":"#A first iteration (see below) gave these results: \n#0.9099475158875073\n#{'n_estimators': 30, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_depth': 30}\n#--------------------------------------------------------------------------------------------------\n#RFR = RandomForestRegressor(criterion = 'mse', oob_score = True, random_state = 1)\n#param_grid = {'min_samples_leaf' : [1, 10, 50, 100, 500, 1000], \n              #'min_samples_split' : [2, 20, 100, 200, 1000, 2000], \n              #'max_depth': [10, 20, 30, 40, 50, None],\n              #'n_estimators': [10, 20, 30]}\n\n#RS = RandomizedSearchCV(estimator = RFR, \n                        #param_distributions = param_grid, \n                        #n_iter = 100, \n                        #cv = cross_validation_split, verbose = 5, random_state = 0, n_jobs = -1)\n\n#RS = RS.fit(X_train_truncated, y_train_truncated)\n\n#print(RS.best_score_)\n#print(RS.best_params_)\n#--------------------------------------------------------------------------------------------------\n\nRFR = RandomForestRegressor(criterion = 'mse', oob_score = True, random_state = 1)\nparam_grid = {'min_samples_leaf' : [5, 10, 20, 40, 70, 100], \n              'min_samples_split' : [10, 20, 40, 60, 80, 100], \n              'max_depth': [10, 20, 30, 40, 50, None],\n              'n_estimators': [30, 40, 50]}\n\n#RS = RandomizedSearchCV(estimator = RFR, \n                        param_distributions = param_grid, \n                        n_iter = 100, \n                        cv = cross_validation_split, verbose = 5, random_state = 0, n_jobs = -1)\n\n#RS = RS.fit(X_train_truncated, y_train_truncated)\n\nprint(RS.best_score_)\nprint(RS.best_params_)","32561929":"param_grid = {'min_samples_leaf' : [5, 10, 15], \n              'min_samples_split' : [10, 15, 20], \n              'max_depth': [30, 35, 40, None],\n              'n_estimators': [40]}\n\n#GS = GridSearchCV(estimator = RFR, param_grid = param_grid, cv = cross_validation_split, verbose = 5, n_jobs = -1)\n#GS = GS.fit(X_train_truncated, y_train_truncated)\n\nprint(GS.best_score_)\nprint(GS.best_params_)","40b7ce7e":"#best_model = RandomForestRegressor(n_estimators=40, \n                                    oob_score = True,\n                                    min_samples_leaf = 5,\n                                    min_samples_split = 15,\n                                    max_depth = 30,\n                                    random_state = 1).fit(X_train_truncated,y_train_truncated)\n#yhat = best_model.predict(X_train)\nprint(\"%.4f\" % best_model.oob_score_)\nprint (\"%.4f\" % mean_absolute_error(y_train, y_hat))\nimportance_df = pd.concat((pd.DataFrame(training_df_truncated.drop(['winPlacePerc'], axis=1).columns, columns = ['variable']), \n           pd.DataFrame(best_model.feature_importances_, columns = ['importance'])), \n           axis = 1).sort_values(by='importance', ascending = False)\nimportance_df","a2dd16f6":"plt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y='variable', data=importance_df.sort_values(by=\"importance\", ascending=False))\nplt.title('Feature Importance')\nplt.tight_layout()","d4840a22":"#training_df_truncated = training_df.truncate(after=10000)\n#X_train_truncated = np.asarray(training_df_truncated.drop(['winPlacePerc'], axis = 1))\n#X_train_truncated = np.float32(X_train_truncated)\n#X_train_truncated = preprocessing.StandardScaler().fit(X_train_truncated).transform(X_train_truncated)\n#X_train_truncated[0:1]","e82c72b0":"#y_train_truncated = np.asarray(training_df_truncated[['winPlacePerc']])\n#y_train_truncated[0:5]","e5f61f1d":"print ('Train set truncated:', X_train_truncated.shape,y_train_truncated.shape)\nprint ('Train set:', X_train.shape,y_train.shape)\nprint ('Test set:', X_test.shape)","b5210e9d":"models = [ \n    #lgb.LGBMRegressor(boosting_type='gbdt', n_estimators=1000, learning_rate=0.05, bagging_fraction = 0.9, max_bin = 127, metric = 'mae', n_jobs=-1, \n                      #max_depth=-1, num_leaves=200, min_data_in_leaf = 100),\n    #lgb.LGBMRegressor(boosting_type='gbdt', n_estimators=50, learning_rate=0.003, metric = 'mae', n_jobs=-1, \n                      #max_depth=-1, num_leaves=200, min_data_in_leaf = 100),\n    #lgb.LGBMRegressor(boosting_type='gbdt', n_estimators=100, learning_rate=0.003, metric = 'mae', n_jobs=-1, \n                      #max_depth=-1, num_leaves=200, min_data_in_leaf = 100)\n    ]\n\nmodel_results = pd.DataFrame(data = {'test_score_mean': [], 'fit_time_mean': [], 'mean_absolute_error': []})\n\n# Spliting the model\ncross_validation_split = model_selection.ShuffleSplit(n_splits = 4, test_size = .3, train_size = .6, random_state = 0 )\n# Performing shufflesplit cross validation, with the whole training set (the cross_validate function coupled with ShuffleSplit take care of spliting the training set) \nfor model in models:\n    cross_validation_results = model_selection.cross_validate(model, X_train_truncated, y_train_truncated, cv= cross_validation_split, \n                                                              scoring = 'neg_mean_absolute_error', return_train_score=True)\n    #Predicts the target value on the whole training set\n    y_hat = model.fit(X_train_truncated, y_train_truncated).predict(X_train)    \n    # Checking the mean of test scores for each iteration of the validation    \n    model_results = model_results.append({'test_score_mean' : cross_validation_results['test_score'].mean(), \n                                          'fit_time_mean' : cross_validation_results['fit_time'].mean(), \n                                          'mean_absolute_error' : mean_absolute_error(y_train, y_hat)}, ignore_index=True) \n \nmodel_results","39be6cba":"LGBM = lgb.LGBMRegressor(learning_rate=0.003, metric = 'mae', n_estimators = 100, n_jobs=-1)\n#early_stopping_rounds = 100, \nparam_grid = {'boosting_type' : ['gbdt', 'dart', 'goss'],\n              'max_depth' : [10, 20, 30, -1],\n              'min_data_in_leaf' : [10, 50, 100, 500, 1000],\n              'num_leaves' : [50, 100, 200, 500, 1000]\n             }\n\n#RS = RandomizedSearchCV(estimator = LGBM, param_distributions = param_grid, \n                        n_iter = 50, scoring = 'neg_mean_absolute_error',\n                        cv = cross_validation_split, verbose = 10, random_state = 0, n_jobs = -1)\n\n#RS = RS.fit(X_train_truncated, y_train_truncated)\n\nprint(RS.best_score_)\nprint(RS.best_params_)","56a35d13":"param_grid = {'boosting_type' : ['goss'],\n              'max_depth' : [20, 30, 40, -1],\n              'min_data_in_leaf' : [10, 20, 50, 100],\n              'num_leaves' : [400, 500, 600]}\n\n#GS = GridSearchCV(estimator = LGBM, param_grid = param_grid, cv = cross_validation_split, verbose = 10, scoring = 'neg_mean_absolute_error', n_jobs = -1)\n#GS = GS.fit(X_train_truncated, y_train_truncated)\n\nprint(GS.best_score_)\nprint(GS.best_params_)","98fb21a9":"#best_model = lgb.LGBMRegressor(learning_rate=0.003, metric = 'mae', n_estimators = 2000, n_jobs=-1,\n                               boosting_type = 'gbdt',\n                               max_depth = 30,\n                               min_data_in_leaf = 10,\n                               num_leaves = 500).fit(X_train_truncated,y_train_truncated)\n\n#y_hat = best_model.predict(X_train)\nprint (\"%.4f\" % mean_absolute_error(y_train, y_hat))\nimportance_df = pd.concat((pd.DataFrame(training_df_truncated.drop(['winPlacePerc'], axis=1).columns, columns = ['variable']), \n           pd.DataFrame(best_model.feature_importances_, columns = ['importance'])), \n           axis = 1).sort_values(by='importance', ascending = False)\nimportance_df","608817a8":"plt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y='variable', data=importance_df.sort_values(by=\"importance\", ascending=False))\nplt.title('Feature Importance')\nplt.tight_layout()","1b59c0ab":"# Predicting the results of the testing set with the model\n#yhat_test = lgb.LGBMRegressor(learning_rate=0.05, bagging_fraction = 0.9, max_bin = 127, metric = 'mae', n_estimators = 1000, n_jobs=-1,\n                              boosting_type = 'gbdt',\n                              max_depth = 30,\n                              min_data_in_leaf = 10,\n                              num_leaves = 200).fit(X_train_truncated, y_train_truncated).predict(X_test)\n# Submitting\ntesting_df = pd.read_csv(\"..\/input\/test_V2.csv\")\nsubmission = testing_df.copy()\nsubmission['winPlacePerc'] = yhat_test\nsubmission.to_csv('submission.csv', columns=['Id', 'winPlacePerc'], index=False)\nsubmission[['Id', 'winPlacePerc']].head()","247d1792":"### **<div id=\"III1\">1. Saving time and memory with big datasets<\/div>**\n\nThe size of the dataset is pretty big. Implementing a script to make the dataset smaller without losing information can save us a lot of time. I did not create this script, the credit goes to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\nThis script uses the following approach:\n* Iterate over every column\n* Determine if the column is numeric\n* Determine if the column can be represented by an integer\n* Find the min and the max value\n* Determine and apply the smallest datatype that can fit the range of values\n","247814b7":"**Results**: 4th degree terms and further don't seem to improve the accuracy of our model so much. Let's stick to 3rd degree polynomial terms and below. \n\n### **<div id=\"V4\">4. Forward search for feature selection<\/div>**\n\n**Forward search** is a **wrapper method** for feature selection. It allows you to search for the best feature model performance and add them to your feature subset one after the other.\n\nFor data with n features:\n* On first round \u2018n\u2019 models are created with individual feature and the best predictive feature is selected.\n* On second round, \u2018n-1\u2019 models are created with each feature and the previously selected feature.\n* This is repeated till a best subset of \u2018m\u2019 features are selected.","5da23730":"Now we can plot the associated heatmap, and see which feature are the most dependent on the target value.","a4d5b38a":"## **<div id=\"VIII\">VIII. Model Submission<\/div>**","32056288":"According to **Pearson correlation**, we can see which feture are the most correlated to the target:\n* walkDistance,\n* killPlace,\n* totalDistance (but it is obviously highly correlated to walkDistance),\n* boosts,\n* weaponsAcquired,\n* items (but it is obviously highly correlated to boosts),\n* damageDealt\n\nExcept for *maxPlace*, results are very similar with this technique. An interpretation of this similarity is that we don't have in this dataset features that are highly dependent but non-linearly correlated.\n\n#### **c. Conclusion**\n\nHere are the feature we will be using for our first model. Then, we may add more features and see how they impact the performance of the model (Wrapper Method):\n* **walkDistance**,\n* **killPlace**,\n* **damageDealt**,\n* **boosts**,\n* **weaponsAcquired**,","eaf8858b":"### **<div id=\"V3\">3. Lasso regression analysis method<\/div>**\n\nLASSO is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. The basic idea is to penalize the model coefficients such that they don\u2019t grow too big and overfit the data. Using LASSO regression, we are essentially eliminating the higher-order terms in the more complex models. \n\n**Basically, LASSO regression is similar to Linear Regression, but with a penalization coefficient at the end of the formula, eliminating the least important terms.**\n\nHere, we want to evaluate the best model complexity (order of polynomial degree) for our LASSO regression model. Do we need linear regression with 7th degree order terms to reach the best accuracy, or is 2nd degree enough? Let's see.","0aa4acde":"## **<div id=\"IV\">IV. Perform Exploratory Analysis and visualize the data<\/div>**\n\n### **<div id=\"IV1\">1. Visualize relation between each feature and the mean of the target<\/div>**\n\nFirst of all, I want to see how each feature is related with the target value. I am drawing scatter plots between each feature and the mean of the target value.","2f7f791e":"## **<div id=VII\">VII. Model data, attempt n\u00b03: Light GBM<\/div>**\n\n**LightGBM** is a relatively new algorithm. It is a gradient boosting framework that uses tree based learning algorithm. Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. Light GBM is prefixed as \u2018Light\u2019 because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n\nUseful Ressources: \n* https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n* https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html#scikit-learn-api\n* https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\n\n### **<div id=\"VII1\">1. Creating and normalizing matrices for our model<\/div>**","eff4520b":"### **<div id=\"III5\">5. Completing missing values for each feature<\/div>**\n\nThere is only one missing value in the entire dataset (winPlacePerc column). Here, we choose to remove its associated row.","35b885e7":"When submitting this model to Kaggle, we get a score of **0,06454**. This is way better! Now let's try light GBM.","613aae64":"According to this forward search algorithm, we choose to add to our model the following features:\n* **kills** (killsPerWalkDistance if sorting according to min Mean Absolute Error value)\n* **matchDuration** (maxPlace if sorting according to MAE)\n* **maxPlace** (matchDuration if sorting according to MAE)\n* **totalDistance** (totalDistance if sorting according to MAE)\n* **killsPerWalkDistance** (kills if sorting according to MAE)\n* **killStreaks** (assists if sorting according to MAE)","92099b53":"### **<div id=\"III4\">4. Dropping irrelevant or marginal data<\/div>**\n\nIn a game of PUBG, we can observe that games with more than 20 kills represents less than 0,01% of the data. Basically, if you score more than 20 kills in a game, either you are the best of the world, or a cheater (this). Let's remove data with more than 20 kills per game.","730a896a":"When submitting this model to Kaggle, we get a score of **0,09127**. This is not very good, and we can't use more than 8-9 features in our model without making the computing time skyrocket (each time we add a feature in our model, it adds like 1000 polynomial features and the model computing time takes forever).\nPlus, it seems like adding more feature won't take us much further, since the relations between features in our problem seem complex and can't be modelled with only linear and low degree polynomial functions.\n\nLet's try something else!","2a95c36a":"### **<div id=\"III7\">7. Creating new potentially relevant values for feature engineering<\/div>**\n\nHere we want to create new features to determine if they can potentially provide new signals to predict our outcome. For this dataset, a few ideas come in mind:\n* **headshotRate** = headshotKills \/ kills\n* **totalDistance** = rideDistance + swimDistance + walkDistance\n* **totalItems** = heals + boosts\n* **healsPerWalkDistance** = heals \/ walkDistance\n* **killsPerWalkDistance** = kills \/ walkDistance","5e5e56fc":"# Table of content\n\n[0.Disclaimer](#0)\n\n[I. Define the problem](#I)\n* [1. Problem description](#I1)\n* [2. Methodology](#I2)\n* [3. Tools importing](#I3)\n\n[II. Gather the data](#II)\n\n[III. Wrangle, cleanse and Prepare Data for Consumption](#III)\n* [0. The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#III0)\n* [1. Saving time and memory with big datasets](#III1)\n* [2. Descriptive analysis of the data](#III2)\n* [3. Dropping features](#III3)\n* [4. Dropping irrelevant or marginal data](#III4)\n* [5. Completing missing values for each feature](#III5)\n* [6. Grouping features by group\/match and getting their mean\/size\/min\/max](#III6)\n* [7. Creating new potentially relevant values for feature engineering](#III7)\n\n[IV. Perform Exploratory Analysis and visualize the data](#IV)\n* [1. Visualize relation between each feature and the mean of the target](#IV1)\n* [2. Feature selection](#IV2)\n\n[V. Model data, attempt n\u00b01: LASSO Regression (Outdated)](#V)\n* [1. Initial toughts](#V1)\n* [2. Creating a new array with polynomial features](#V2)\n* [3. Lasso regression analysis method](#V3)\n* [4. Forward search for feature selection](#V4)  \n\n[VI. Model data, attempt n\u00b02: Random Forest (Outdated)](#VI)\n* [1. Creating and normalizing matrices for our model](#VI1)\n* [2. Modeling](#VI2)\n\n[VII. Model data, attempt n\u00b03: Light GBM](#VII)\n* [1. Creating and normalizing matrices for our model](#VII1)\n* [2. Modeling](#VII2)\n\n[VIII. Model Submission](#VIII)\n\n## **<div id=\"0\">0. Disclaimer<\/div>**\n\nIf you want to see the clean and short version of my work, **go here : https:\/\/www.kaggle.com\/toldo171\/pubg-top-35-with-lgbm**\nThis kernel contains my whole thinking process (thus it is also a mess :-) ).\n\n## **<div id=\"I\">I. Define the problem<\/div>**\n\n### **<div id=\"I1\">1. Problem description<\/div>**\n\nBattle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.\n\nPlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.\n\nThe team at PUBG has made official game data available for the public to explore and scavenge outside of \"The Blue Circle.\" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the PUBG Developer API.\n\nYou are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.\n\nWhat's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!\n\nIn a PUBG game, up to 100 players start in each match (matchId). Players can be on teams (groupId) which get ranked at the end of the game (winPlacePerc) based on how many other teams are still alive when they are eliminated. In game, players can pick up different munitions, revive downed-but-not-out (knocked) teammates, drive vehicles, swim, run, shoot, and experience all of the consequences -- such as falling too far or running themselves over and eliminating themselves.\n\nYou are provided with a large number of anonymized PUBG game stats, formatted so that each row contains one player's post-game stats. The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group.\n\nYou must create a model which predicts players' finishing placement based on their final stats, on a scale from 1 (first place) to 0 (last place). \n\n#### **Data fields**\n\n* **DBNOs** - Number of enemy players knocked.\n* **assists** - Number of enemy players this player damaged that were killed by teammates.\n* **boosts** - Number of boost items used.\n* **damageDealt** - Total damage dealt. Note: Self inflicted damage is subtracted.\n* **headshotKills** - Number of enemy players killed with headshots.\n* **heals** - Number of healing items used.\n* **Id** - Player\u2019s Id\n* **killPlace** - Ranking in match of number of enemy players killed.\n* **killPoints** - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a \u201cNone\u201d.\n* **killStreaks** - Max number of enemy players killed in a short amount of time.\n* **kills** - Number of enemy players killed.\n* **longestKill** - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n* **matchDuration** - Duration of match in seconds.\n* **matchId** - ID to identify match. There are no matches that are in both the training and testing set.\n* **matchType** - String identifying the game mode that the data comes from. The standard modes are \u201csolo\u201d, \u201cduo\u201d, \u201csquad\u201d, \u201csolo-fpp\u201d, \u201cduo-fpp\u201d, and \u201csquad-fpp\u201d; other modes are from events or custom matches.\n* **rankPoints** - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API\u2019s next version, so use with caution. Value of -1 takes place of \u201cNone\u201d.\n* **revives** - Number of times this player revived teammates.\n* **rideDistance** - Total distance traveled in vehicles measured in meters.\n* **roadKills** - Number of kills while in a vehicle.\n* **swimDistance** - Total distance traveled by swimming measured in meters.\n* **teamKills** - Number of times this player killed a teammate.\n* **vehicleDestroys** - Number of vehicles destroyed.\n* **walkDistance** - Total distance traveled on foot measured in meters.\n* **weaponsAcquired** - Number of weapons picked up.\n* **winPoints** - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a \u201cNone\u201d.\n* **groupId** - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n* **numGroups** - Number of groups we have data for in the match.\n* **maxPlace** - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n* **winPlacePerc** - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.\n\n\n### **<div id=\"I2\">2. Methodology<\/div>**\n\n* **1. Define the Problem**: If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n* **2. Gather the Data**: John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are \u201cdrowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n* **3. Prepare Data for Consumption**: This step is often referred to as data wrangling, a required process to turn \u201cwild\u201d data into \u201cmanageable\u201d data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n* **4. Perform Exploratory Analysis**: Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n* **5. Model Data**: Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that\u2019s used as actionable intelligence) at worst.\n* **6. Validate and Implement Data Model**: After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our model overfit, generalize, or underfit our dataset.\n* **7. Optimize and Strategize**: This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your \u201ccurrency exchange\" rate.\n\n### **<div id=\"I3\">3. Tools importing<\/div>**\n\nHere we are importing every useful tool needed during our research process.","8a245cc2":"### **<div id=\"VII2\">2. Modeling<\/div>**\n\n#### **a. Cross validation**","47652181":"#### **b. Pearson correlation**\n\nThis technique is very time consuming. It took like 30min to compute the mutual information dataframe. Let's do the same work with the Pearson's correlation and compare the results, to see if it is worth computing the mutual information dataframe.\n\n**Pearson correlation** is a measure of the linear correlation between two variables X and Y. It has a value between +1 and \u22121, where 1 is total positive linear correlation, 0 is no linear correlation, and \u22121 is total negative linear correlation.","14be4336":"Now let's just apply the script to our traning and testing dataframes.","d6cf3ebe":"### **<div id=\"IV2\">2. Feature selection<\/div>**\n\n#### **a. Mutual information**\n\nFeature Selection is a very critical component in a Data Scientist\u2019s workflow. When presented data with very high dimensionality, models usually choke because:\n* Training time increases exponentially with number of features.\n* Models have increasing risk of overfitting with increasing number of features.\n\nFeature Selection methods helps with these problems by reducing the dimensions without much loss of the total information. It also helps to make sense of the features and its importance.\n\nHere we are going to use a **filter method, mutual information**, to select our features.\n\n**Mutual Information** between two variables measures the dependence of one variable to another. If X and Y are two variables:\n* If X and Y are independent, then no information about Y can be obtained by knowing X or vice versa. Hence their mutual information is 0.\n* If X is a deterministic function of Y, then we can determine X from Y and Y from X with mutual information 1.\n   \nWe can then select our features from feature space by ranking their mutual information with the target variable.\n\nAdvantage of using mutual information is it does well with the non-linear relationship between feature and target variable.","67ba613e":"## **<div id=\"V\">V. Model data, attempt n\u00b01: LASSO Regression (Outdated)<\/div>**\n\n### **<div id=\"V1\">1. Initial toughts<\/div>**\n\nResource: https:\/\/towardsdatascience.com\/machine-learning-with-python-easy-and-robust-method-to-fit-nonlinear-data-19e8a1ddbd49\n\nNow that we have acquired, analyzed and prepared the data, we are ready to train a model and predict the required solution. There are many predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Here we are performing supervised learning, and our problem is a regression problem.\nHere we are facing two challenges:\n* Our dataset is very large, this we need our model to be scalable to big datasets,\n* Our dataset is high-dimensionnal, and the relations between features are highly non-linear. One solution is to fit a model with polynomial degree terms, and potentially cross-coupled features. This leads us to few questions:\n    * How to decide up to what polynomials are necessary?\n    * When to stop if we start by incorporating 1st degree, 2nd degree, 3rd-degree terms one by one?\n    * How to decide if any of the cross-coupled terms are important (for example, X1\u00b2, X2\u00b3, X1.X2, X1\u00b2.X3...)?","ece66b57":"We get theses results:\n* **best score**: -0.07856967910566438\n* **bests params**: {'num_leaves': 500, 'min_data_in_leaf': 10, 'max_depth': 30, 'boosting_type': 'goss'}","9d66ed57":"#### **b. Tune Model with Hyper-Parameters**","7a021e0e":"According to **mutual information**, we can see which feture are the most dependant on the target:\n* maxPlace,\n* killPlace,\n* walkDistance\n* totalDistance, killsPerWalkDistance, healsPerWalkDistance, rideDistance (but they are obviously highly dependent on walkDistance),\n* damageDealt\n* boosts,\n* weaponsAcquired,\n","fccf7983":"## **<div id=\"III\">III. Wrangle, cleanse and prepare Data for Consumption<\/div>**\n\n### **<div id=\"III0\">0. Reminder: the 4 C's of data cleaning<\/div>**\n\n* **Correcting**: Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n* **Completing**: There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value.\n* **Creating**: Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome.\n* **Converting**: Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations.","f612ceac":"Let's go through the same process for some other features : \n* **weaponsAcquired** > 30\n* **DBNOs** > 15","699501ee":"### **<div id=\"VI2\">2. Modeling<\/div>**\n\nLogistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution.\n\n#### **a. Cross validation**\n\nFor a prediction problem, a model is generally provided with a data set of known data, called the training data set, and a set of unknown data against which the model is tested, known as the test data set. The target is to have a data set for testing the model in the training phase and then provide insight on how the specific model adapts to an independent data set. A round of cross-validation comprises the partitioning of data into complementary subsets, then performing analysis on one subset. After this, the analysis is validated on other subsets (testing sets). To reduce variability, many rounds of cross-validation are performed using many different partitions and then an average of the results are taken. **Cross-validation is a powerful technique in the estimation of model performance technique.**\n\nHere, we are using the ShuffleSplit function from Scikit Learn. ShuffleSplit will randomly sample your entire dataset during each iteration to generate a training set and a test set. The test_size and train_size parameters control how large the test and training test set should be for each iteration. Since you are sampling from the entire dataset during each iteration, values selected during one iteration, could be selected again during another iteration.  \n\nThe main difference between ShuffleSplit and K-Fold is that In KFold, during each round you will use one fold as the test set and all the remaining folds as your training set. However, in ShuffleSplit, during each round n you should only use the training and test set from iteration n. **As your data set grows, cross validation time increases, making shufflesplits a more attractive alternate**. If you can train your algorithm, with a certain percentage of your data as opposed to using all k-1 folds, ShuffleSplit is an attractive option.","e27640e9":"### **<div id=\"V2\">2. Creating a new array with polynomial features<\/div>**\n\nFirst, let's truncate our training dataframe, to speed up computing. Then, we want to create a new array based on our training dataframe, with each 2nd degree (for now) polynomial features in it. We allow every cross-coupled terms. For degree 2 and 5 features, we will have 21 features in our new array:\n* our 5 initial features,\n* 5 squared features, \n* 5+4+3+2+1 = 15 cross-coupled features.\n* one last feature, with every features powered zero.","6b6d0ba8":"## **<div id=\"II\">II. Gather the data<\/div>**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames.","d035c78a":"#### **b. Tune Model with Hyper-Parameters**\n\nIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.\n\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. Model hyperparameters are set manually and are used in processes to help estimate model parameters.\n**Tuning an hyperparameter means trying to get the closest possible to its best value, in order to maximize the accuracy of the model** (for larger dataset, time computing may also be taken into account for parameter tuning).\n\nFirst, we are using **RandomizedSearchCV**. We need to create a parameter grid to sample from during fitting. On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2x6x6x8x8 = 4608 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.","79184121":"We get theses results:\n* **best score**: 0.9111606731899772\n* **bests params**: {'n_estimators': 40, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_depth': 40}\n\nRandom search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with **GridSearchCV**, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:","21bd893d":"### **<div id=\"III6\">6. Grouping features by group\/match and getting their mean\/size\/min\/max<\/div>**\n\nThe match and group feature seems interesting, and one way to get some information from it is to compute the mean of each feature on each match, and the size of the match feature.","dc798327":"We get theses results:\n* **best score**: 0.9112120273422432\n* **bests params**: {'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 40}","eeb2ce30":"### **<div id=\"III2\">2. Descriptive analysis of the data<\/div>**\n\nLet's:\n* display the first rows of the training dataset, in order to have an overview of the parameters,\n* display the data type of each feature,\n* display a quick descriptive representation of the dataset","db89bf6b":"### **<div id=\"III3\">3. Dropping features<\/div>**\n\nFirst of all, we want to drop some feature which may not be relevant for our problem:\n* **ID, groupId**: in the future, we may want to treat our data set as a \"cluster\" of many games (grouping by matches or by player ID), but for this first iteration let's just drop these features and not take into account these dependencies.\n* **longestKill, rankPoints**: in the description, these features are stated as \"inconsistent\", so let's just drop them.\n* **numGroups**: this feature is very similar to maxPlace. Let's drop it.\n* **matchType**: this feature is somewhat redundant with maxPlace, since for example, on a duo-type game, maxPlace ~ 50. Let's drop it.","5a4f9aa7":"## **<div id=\"VI\">VI. Model data, attempt n\u00b02: Random Forest (Outdated)<\/div>**\n\n### **<div id=\"VI1\">1. Creating and normalizing matrices for our model<\/div>**\n\nFirst, we need to prepare and normalize our train and test matrices, which we are then going to use for our models.\n\nNormalizing the data has two purposes :\n* Making training less sensitive to the scale of features. If we don't normalize the data when we face features with different scales (for example, age and house price), our ML algorithms might take too much care to features with large scales.\n* Accelerating optimization. Most machine learning optimizations are solved using gradient descent, or a variant thereof. And the speed of convergence depends on the scaling of features. Normalization makes the problem better conditioned, improving the convergence rate of gradient descent.\n"}}