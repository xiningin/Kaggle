{"cell_type":{"e1561e30":"code","fbb558aa":"code","985df080":"code","3daf327e":"code","f2fecdc5":"code","6718fe39":"code","61ca3b75":"code","6e96cdf7":"code","e52918b1":"code","466041f5":"code","942f3e48":"code","17686448":"code","ee6feb91":"code","936d365f":"code","db844869":"code","887fcce8":"code","111e3abc":"code","ae986357":"code","7f4a3ab9":"code","0af1aea3":"code","6bb830c1":"code","465c4e4a":"code","1981a230":"code","ecba1552":"code","f0a7568e":"code","2f3b0e2d":"code","9ed635b4":"code","dccbbd20":"code","edea69fc":"code","29966bb7":"code","348c937b":"code","8f63145e":"code","9bf5ea15":"code","9699686f":"code","55e0c098":"code","7e5bfcd6":"code","26e9d5b6":"code","ac57415c":"code","c84727dd":"code","9047d816":"code","4e5b20c0":"code","8e789e1c":"code","356dd466":"code","61961e9c":"code","b8cee5b0":"code","d9587743":"code","c999928c":"code","ec3d9ce3":"code","975b3c76":"code","a96f98cb":"markdown","a52c0cdc":"markdown","94bd487e":"markdown","f9fa7051":"markdown","75124dbf":"markdown","68322e33":"markdown","e7ba2bb9":"markdown","1824656f":"markdown","e2cb25ee":"markdown","acfbf59d":"markdown","04378050":"markdown","0d5cd414":"markdown","33a2a4a1":"markdown"},"source":{"e1561e30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbb558aa":"data=pd.read_csv(\"..\/input\/cwurData.csv\")","985df080":"data.info() # Memory and data types used","3daf327e":"data.head(10) #head function calls default  first 5 samples.","f2fecdc5":"data.tail() #Last 5 samples.","6718fe39":"data.shape  # Row-Column","61ca3b75":"data.describe() # Only Numeric Statistics","6e96cdf7":"data.columns # Column names","e52918b1":"data.isnull().sum() # Missing values control ","466041f5":"data.corr() # Indicates the presence, direction and intensity of the relationship between the two variables","942f3e48":"#Correlation heatmap\nf,ax= plt.subplots(figsize=(18,18))\nsns.heatmap(data.corr(),annot=True,linewidths=.6,fmt='.2f',ax=ax) #annot=True :Visible numbers \nplt.show() ","17686448":"data.cov() #Covaryans ","ee6feb91":"#Line plot\nplt.plot(data.score,data.world_rank,color='blue',label='world_rank',alpha=0.7)\nplt.plot(data.score,data.patents,color='red',label='patents',alpha=0.7)\nplt.plot(data.score,data.quality_of_faculty,color='green',label='quality_of_faculty',alpha=0.7)\nplt.legend()\nplt.xlabel('Score')\nplt.ylabel('y axis')\nplt.title('Line plot')\nplt.show()","936d365f":"#Scatter plot\ndata.plot(kind='scatter',x='world_rank',y='patents',alpha=0.5,color='blue')\nplt.xlabel('world_rank')\nplt.ylabel('patents')\nplt.title('world_rank - patents Scatter Plot')\nplt.show()","db844869":"#Histogram \ndata.score.plot(kind='hist',bins=40,figsize=(15,15),color='green')\nplt.show()\n#Score frequency ","887fcce8":"#Boxplot\nplt.figure()\nplt.boxplot(data.score, 0, 'gD')\nplt.show()","111e3abc":"#Score frequency and the change of outliers according to years\ndf = data[data.year.isin(data.year.value_counts().head(10).index)]\n\nsns.boxplot(\n    x='year',\n    y='score',\n    data=df\n)\nplt.show()","ae986357":"P = np.percentile(data.score, [10, 100])\nP","7f4a3ab9":"data[(data['year']==2012)& (data['score']>=85) & (data['country']=='USA')].sort_values('score',axis=0,ascending=False) #Filtering and ordering by score\n","0af1aea3":"data['country'].unique() #Unique method doesn't show the same country again.","6bb830c1":"filter_1=data['year']==2012\nfilter_2=data['score']<45\nfilter_3=data['country']=='United Kingdom'\ndata[filter_1 & filter_2 & filter_3]\n#List of universities in the United Kingdom that were below 45 scores in 2012","465c4e4a":"#Let's add a new feature\nscore_average=sum(data.score) \/len(data.score)\nmedian=data.score.median()\ndata['score_level']=['high' if i>score_average else 'Average'if median<i<=score_average else 'Low' for i in data.score]\ndata1=data.head()\ndata2=data.tail()\nconc_data_row=pd.concat([data1,data2],axis=0,ignore_index=True) # axis=0 adds dataframes in row\n#Let's see the data again.\nconc_data_row","1981a230":"#For example lets look frequency of country types\nprint(data['country'].value_counts(dropna=False))","ecba1552":"data.describe() #ignore null entries and not number features\n","f0a7568e":"data_new=data.head(10)\ndata_new","2f3b0e2d":"#melt function\n#frame : DataFrame\n#id_vars : tuple, list, or ndarray, optional\n#Column(s) to use as identifier variables.\n#value_vars : tuple, list, or ndarray, optional\n#Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.\nmelted=pd.melt(frame=data_new,id_vars='institution',value_vars=['score','patents'])\nmelted","9ed635b4":"#Index is name\n#Column to use to make new frame\u2019s columns.\n#values:Column(s) to use for populating new frame\u2019s values. \n#If not specified, all remaining columns will be used and the result will have hierarchically indexed columns.\n#Return reshaped DataFrame organized by given index \/ column values.\nmelted.pivot(index='institution',columns='variable',values='value')","dccbbd20":"#Lets make a sample\ndata1=data.head()\ndata2=data.tail()\nconc_data_row=pd.concat([data1,data2],axis=0,ignore_index=True) # axis=0 adds dataframes in row\nconc_data_row","edea69fc":"data1=data['institution'].head(7)\ndata2=data['world_rank'].head(7)\nconc_data_col=pd.concat([data1,data2],axis=1) # axis=1 : adds dataframes in column\nconc_data_col","29966bb7":"#Let's see the distribution of countries by groupby method.\ndata.groupby(\"country\").size()","348c937b":"data.dtypes","8f63145e":"#Convert object(str) to categorical and int to float\ndata['country']=data['country'].astype('category')\ndata['quality_of_faculty']=data['quality_of_faculty'].astype('float')\n","9bf5ea15":"data.dtypes\n#country converted from object to categorical\n#quality_of_faculty converted from int to float","9699686f":"#Let's look at  does World University Ranking data have non value\ndata.isnull().sum() # Missing values control","55e0c098":"#Copy the data, opening a new memory\ndata1=data.copy()","7e5bfcd6":"#Convert categorical(str) to object again.\ndata1['country']=data1['country'].astype('object')","26e9d5b6":"#Let's drop the broad_impact feature\ndata1_target = data1.broad_impact\ndata1_predictors = data1.drop(['broad_impact'], axis=1)\n\n# For the sake of keeping the example simple, we'll use only numeric predictors. \ndata1_numeric_predictors = data1_predictors.select_dtypes(exclude=['object'])","ac57415c":"data1_numeric_predictors #Only numeric features and non null","c84727dd":"from sklearn.model_selection import KFold\n#Sample dataset\nds = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n\n#K-Fold Cross Validation\nkfold = KFold(3, True, 1)\n\n#Return on the folds\nfor egitim, test in kfold.split(ds):\n    print('egitim: %s, test: %s' % (ds[egitim], ds[test]))","9047d816":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn import model_selection\n\n#We select attribute values for training\nX = data1_numeric_predictors.iloc[:, :-1].values\n\n#We select attribute values for classification\n#I selected score_level feature\nY = data.iloc[: ,-1].values","4e5b20c0":"Y","8e789e1c":"#Separation of training and test data sets\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)","356dd466":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()","61961e9c":"#Calculation of ACC with K-fold cross validation of NB model\nscoring = 'accuracy'\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)","b8cee5b0":"cv_results","d9587743":"msg = \"%f (%f)\" % (cv_results.mean(), cv_results.std())\nmsg","c999928c":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nmodel.fit(X_train, Y_train)\n\nY_pred = model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(Y_test, Y_pred))\nprint(confusion_matrix(Y_test, Y_pred))\n# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(\"ACC: \",accuracy_score(Y_pred,Y_test))\n","ec3d9ce3":"from sklearn.linear_model import LinearRegression ","975b3c76":"x=data1_numeric_predictors.iloc[:,[0,1,2,3,4,5,6,7,8,10]].values #all features except score\ny=data1_numeric_predictors.score.values.reshape(-1,1) #Score\n#fitting data\nmultiple_linear_regression=LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\nprint(\"b0 :\",multiple_linear_regression.intercept_) #Intercept\nprint(\"b1,b2,b3,b4,b5,b6,b7,b8,b9,b10 :\",multiple_linear_regression.coef_) #coefficients","a96f98cb":"**Accuracy** :  Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.909 which means our model is approx. 90% accurate.\n\nAccuracy = TP+TN\/TP+FP+FN+TN\n\n**Precision** : Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.  High precision relates to the low false positive rate. We have got Average=0.86 , Low=0.95, high=0.88  precision which is pretty good.\n\nPrecision = TP\/TP+FP\n\n**Recall** (Sensitivity) -:  Recall is the ratio of correctly predicted positive observations to the all observations in actual class - We have got Average=0.85 , Low=0.94 , high=0.91 recall  which is good for this model as it\u2019s above 0.5.\n\nRecall = TP\/TP+FN\n\n**F1 score**  :  F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it\u2019s better to look at both Precision and Recall. In our case, F1 score : Average=0.86, Low=0.95 ,high=0.89 \n\nF1 Score = 2*(Recall * Precision) \/ (Recall + Precision)\n\n","a52c0cdc":"Best Positive correlation is between **world_rank** and** broad_impact** =**0.94** <br\/>\nBest Negative correlation is between ** score** and **quality_of_faculty** = -**0.69**","94bd487e":"**MISSING DATA and TESTING WITH ASSERT** <br\/>\n   \n   How do we handle missing data :\n*  Removing entire observations containing one or more unknown values\n* Filling in unknown values with the most frequent values\n* Filling in unknown values by exploring correlations\n* Filling in unknown values by exploring similarities between cases","f9fa7051":"**Exploratory Data Analysis**","75124dbf":"**DATA TYPES** <br\/>\nThere are  5 basic data types: object(string),boolean,integer,float and categorical","68322e33":"**CREATION OF NAIVE BAYES MODEL**","e7ba2bb9":"**PIVOTING DATA** <br\/>\n  Reverse of melting.","1824656f":"**Data Exploration and Visualization**","e2cb25ee":"**Multiple Linear Regression** <br\/>\n\nMultiple linear regression is the most common form of linear regression analysis.  As a predictive analysis, the multiple linear regression is used to explain the relationship between one continuous dependent variable and two or more independent variables.  The independent variables can be continuous or categorical .\n","acfbf59d":"**K-FOLD CROSS VAL\u0130DAT\u0130ON**","04378050":"There is a large number of positive correlation between the sections in the data set. That means that when one section increases and the other increases in the same direction.","0d5cd414":"**CONCATENATING DATA** <br\/>\nTo concatenate two or more  data sets means to stack one \"on top\" of the other into a single  data set. \n ","33a2a4a1":"**TIDY DATA**"}}