{"cell_type":{"757c0c69":"code","8236d3ce":"code","72f876ed":"code","59ba93e2":"code","541d3261":"code","be6273f1":"code","48140d31":"code","ae226c21":"code","09e0ca43":"code","4de4b3e6":"code","35817805":"code","9d171fb4":"code","995713b1":"code","5e9601fc":"code","69aab29e":"code","1ae98f9f":"code","0b696489":"code","b06064cb":"code","b1ee7ab8":"code","5d3f1653":"code","6fbfcbcd":"code","3d34ae14":"code","5f008794":"code","db1f2476":"code","7d0f5199":"code","1fe28763":"code","4d53b861":"code","66f6ed64":"code","41acdcb1":"code","8d9e9bc8":"code","08b650b4":"code","dc4f6fe2":"code","6c6e03ab":"code","0300bd4e":"code","2f7eeae9":"code","781e2a92":"code","f30a4c91":"code","68696184":"code","31e8985e":"code","b6de95ee":"code","4bf84a4b":"markdown","0fe36298":"markdown","51e31130":"markdown","44602305":"markdown","081ae02d":"markdown","98ed34b7":"markdown","466a2882":"markdown","c42851a3":"markdown","4a6f3838":"markdown","85dd65e0":"markdown","ed02582b":"markdown","7a45a4d2":"markdown","caac9632":"markdown","3499f6b8":"markdown","fe0495ca":"markdown","05ee8111":"markdown","5986611c":"markdown","6a921b3e":"markdown","0061c53a":"markdown","8a6cf27e":"markdown","c41c9852":"markdown","ba40a255":"markdown","e31e6896":"markdown","3ddedebc":"markdown","7956dbbc":"markdown","f438e300":"markdown","e4f67d5e":"markdown","f7c30a48":"markdown","4c17f813":"markdown","3706525c":"markdown","c793c655":"markdown","e348ea2b":"markdown","e4bd4ef5":"markdown","3c5aefb1":"markdown","8ecee896":"markdown","fa23ccb1":"markdown","1cc315ec":"markdown","7b7b8595":"markdown","5a91bdd1":"markdown","eed13a3d":"markdown","63738a53":"markdown","472af5d6":"markdown","bc2448fd":"markdown","c3b866d8":"markdown","32ff8894":"markdown","20aa28c9":"markdown","ae8a08bb":"markdown","90d40d3d":"markdown","c3ef14c2":"markdown","0f4f169a":"markdown","4d2a74d6":"markdown","5644559d":"markdown","6efd994d":"markdown","53149d04":"markdown","435b866b":"markdown","4aa72e38":"markdown","b04f28c3":"markdown","516dabfa":"markdown","904148d3":"markdown","1df91709":"markdown","de173c14":"markdown","70de9227":"markdown","a785e74b":"markdown","d8d4f82e":"markdown","54d47516":"markdown","cdd1b698":"markdown","ad903418":"markdown"},"source":{"757c0c69":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor=sns.color_palette()#helps to give colors to the plots\nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning","8236d3ce":"dt=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndt.head()","72f876ed":"dt.head()","59ba93e2":"fig,ax=plt.subplots(1,1,figsize=(14,5))#fig-specifies whole graph whereas ax represents the subplot that are in graph as:-ax\n#count of people having diseases\nsns.countplot(data=dt,x='target',ax=ax,palette='Set1')\nax.set_xlabel(\"No.of people has heart disease\")\nax.set_ylabel(\"count\")\nax.set_title(\"Frequency of people having heart diseases\")\n","541d3261":"fig,ax=plt.subplots(1,1,figsize=(7,5))\ndt['target'].value_counts().plot.pie(ax=ax,autopct='%1.1f%%')#or we can have explode parameter to have some separation between two","be6273f1":"fig,ax=plt.subplots(1,2,figsize=(12,5))\nsns.countplot(data=dt,x='sex',hue='target',palette='Set1',ax=ax[0])\nax[0].set_xlabel(\"0 is for feamle and 1 is for male\")\nax[0].set_ylabel(\"count\")\ndt['sex'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')\nax[0].set_title(\"sex vs survived\")\nax[1].set_title(\"male vs female survived\")","48140d31":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nsns.countplot(data=dt,x='cp',hue='target',palette=\"Set2\",ax=ax[0])\nax[0].set_xlabel(\"chest pain category\")\nax[0].set_ylabel(\"count\")\nax[0].set_title(\"cp vs survival\")\ndt['cp'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')\nax[1].set_title(\"cp category vise percentage\")","ae226c21":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nsns.countplot(x='restecg',data=dt,hue='target',palette='Set1',ax=ax[0])#different sets gives different colors\nax[0].set_xlabel(\"resting electrocardiographic\")\ndt.restecg.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')\nax[1].set_title(\"resting electrocardiographic\")","09e0ca43":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nsns.countplot(data=dt,x='fbs',hue='target',ax=ax[0],palette=\"Set3\")\nax[0].set_xlabel(\"0 for fps <120 , 1 for fps>120\")\nax[0].set_ylabel(\"count\")\nax[0].set_title(\"fps vs heart disease\")\ndt['fbs'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')","4de4b3e6":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nsns.countplot(x='ca',data=dt,hue='target',palette='Set2',ax=ax[0])\nax[0].set_xlabel(\"number of major vessels colored by flourosopy\")\ndt['ca'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')\nax[1].set_title(\"number of major vessels colored by flourosopy\")","35817805":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nsns.countplot(data=dt,x='slope',palette='Set1',hue='target',ax=ax[0])\nax[0].set_xlabel('slope of the peak')\ndt['slope'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')\nax[1].set_title('slope of peaks')","9d171fb4":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nsns.countplot(data=dt,x='thal',hue='target',palette='Set3',ax=ax[0])\nax[0].set_xlabel(\"thalassemia levels\")\ndt['thal'].value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%')\nax[1].set_title('thalassemia levels')","995713b1":"fig,ax=plt.subplots(2,2,figsize=(16,8))\nsns.swarmplot(y='trestbps',x='ca',hue='target',data=dt,ax=ax[0,0])\nsns.swarmplot(y='trestbps',x='sex',hue='target',data=dt,ax=ax[1,0])\nsns.swarmplot(y='trestbps',x='cp',hue='target',data=dt,ax=ax[0,1])\nsns.swarmplot(y='trestbps',x='exang',hue='target',data=dt,ax=ax[1,1])","5e9601fc":"fig,ax=plt.subplots(2,2,figsize=(14,8))\nsns.swarmplot(y='chol',x='sex',hue='target',data=dt,ax=ax[0,0])\nsns.swarmplot(y='chol',x='cp',hue='target',data=dt,ax=ax[1,0])\nsns.swarmplot(y='chol',x='ca',hue='target',data=dt,ax=ax[0,1])\nsns.scatterplot(x=dt['target'],y=dt['chol'],ax=ax[1,1])","69aab29e":"sns.scatterplot(x=dt['target'],y=dt['oldpeak'])","1ae98f9f":"fig,ax=plt.subplots(4,3,figsize=(16,16))\nfor i in range(12):\n    plt.subplot(4,3,i+1)\n    sns.kdeplot(data=dt.iloc[:,i],shade=True)","0b696489":"dt.isna().sum()","b06064cb":"plt.figure(figsize=(16,16))\nsns.heatmap(dt.corr(),annot=True)","b1ee7ab8":"dt.sex=dt.sex.astype('category')\ndt.cp=dt.cp.astype('category')\ndt.fbs=dt.fbs.astype('category')\ndt.restecg=dt.restecg.astype('category')\ndt.exang=dt.exang.astype('category')\ndt.ca=dt.ca.astype('category')\ndt.slope=dt.slope.astype('category')\ndt.thal=dt.thal.astype('category')","5d3f1653":"model_label=dt['target']\nmodel_label=pd.DataFrame(model_label)\ndel dt['target']","6fbfcbcd":"dt1=pd.get_dummies(dt,drop_first=True)\ndt1.head()","3d34ae14":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\ndt1_scaled=MinMaxScaler().fit_transform(dt1)\ndt1_scaled=pd.DataFrame(data=dt1_scaled, columns=dt1.columns)\ndt1_scaled.head()","5f008794":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(dt1_scaled,model_label,random_state=25,stratify=model_label)#stratify makes equal proportions to go onto both sets","db1f2476":"from sklearn.metrics import confusion_matrix#matrix of true positives,true negatives,false positives,false negatives...\nfrom sklearn.metrics import precision_recall_curve#x-axis->precision,y-axis->recall where precision means what fraction of +ve predictions are correct and recall means what fraction of positives are correctly identified.\nfrom sklearn.metrics import average_precision_score#weighted mean of precision with weight as increase in the amount of recall from previous one\nfrom sklearn.metrics import roc_curve#x-axis->False positives,y-axis->true positives\nfrom sklearn.metrics import auc#Area under the Roc curve\nfrom sklearn.model_selection import cross_val_score#cross validation score\nfrom sklearn.metrics import f1_score#harmonic mean of precision and recall","7d0f5199":"def CrossValidate(dtX,dtY,model,cv=5):#defalut cv=5(folds),model means algorithm that we applied\n    score=cross_val_score(model,dtX , dtY, cv=cv, scoring='accuracy')#return scores of all folds\n    return(np.mean(score))#taking average of all.","1fe28763":"def plotting(true_values,pred_values):\n    fig,axe=plt.subplots(1,2,figsize=(12,6))\n    precision,recall,threshold = precision_recall_curve(true_values,pred_values[:,1])#returns three arrays of precision,recalls and thresholds with respect to which those are attained\n    axe[0].plot(precision,recall,'r--')\n    axe[0].set_xlabel('Precision')\n    axe[0].set_ylabel('Recall')\n    axe[0].set_title(\"Average Precision Score : {}\".format(average_precision_score(true_values,pred_values[:,1])))#probabilities of 1's-pred[:,1] means\n    fpr,tpr,threshold = roc_curve(true_values,pred_values[:,1])#fpr->false positives,tpr->true positive\n    axe[1].plot(fpr,tpr)\n    axe[1].set_title(\"AUC Score is: {}\".format(auc(fpr,tpr)))#area under curve\n    axe[1].set_xlabel('False Positive Rate')\n    axe[1].set_ylabel('True Positive Rate')","4d53b861":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf=KNeighborsClassifier(n_neighbors=18)#Checking different values of n_neighbors gives the maximum of them\nknn_score=CrossValidate(X_train,np.ravel(y_train),knn_clf)\nprint(knn_score)\nknn_clf.fit(X_train,np.ravel(y_train))\nplotting(y_test,knn_clf.predict_proba(X_test))\n\n#Now calculate F1 scores:-\nfig=plt.figure(figsize=(10,5))\nsns.heatmap(confusion_matrix(y_test,knn_clf.predict(X_test)),annot=True)\nF1_knn=f1_score(y_test,knn_clf.predict(X_test))\nprint(F1_knn)","66f6ed64":"from sklearn.linear_model import LogisticRegression\nlg_clf=LogisticRegression(C=10,solver='lbfgs')\nlg_score=CrossValidate(X_train,np.ravel(y_train),lg_clf)\nprint(lg_score)\nlg_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),lg_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,lg_clf.predict(X_test)),annot=True)\nF1_lg=f1_score(y_test,lg_clf.predict(X_test))\nprint(F1_lg)","41acdcb1":"from sklearn.tree import DecisionTreeClassifier\ndt_clf=DecisionTreeClassifier(max_depth=3)\ndt_score=CrossValidate(X_train,np.ravel(y_train),dt_clf)\nprint(dt_score)\ndt_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),dt_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,dt_clf.predict(X_test)),annot=True)\nF1_dt=f1_score(y_test,dt_clf.predict(X_test))\nprint(F1_dt)","8d9e9bc8":"from sklearn.svm import SVC\nsvm_clf=SVC(kernel='rbf',C=0.1,gamma=0.1,probability=True)#To call Predict_proba we must keep probability as True which makes the output to be given as probability estimates.\nsvm_score=CrossValidate(X_train,np.ravel(y_train),svm_clf)\nprint(svm_score)\nsvm_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),svm_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,svm_clf.predict(X_test)),annot=True)\nF1_svm=f1_score(y_test,svm_clf.predict(X_test))\nprint(F1_svm)","08b650b4":"from sklearn.ensemble import RandomForestClassifier\nrf_clf=RandomForestClassifier(max_features=3,random_state=25,n_estimators=10)\nrf_score=CrossValidate(X_train,np.ravel(y_train),rf_clf)\nprint(rf_score)\nrf_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),rf_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,rf_clf.predict(X_test)),annot=True)\nF1_rf=f1_score(y_test,rf_clf.predict(X_test))\nprint(F1_rf)","dc4f6fe2":"from sklearn.naive_bayes import GaussianNB\nnb_clf=GaussianNB()\nnb_score=CrossValidate(X_train,np.ravel(y_train),nb_clf)\nprint(nb_score)\nnb_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),nb_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,nb_clf.predict(X_test)),annot=True)\nF1_nb=f1_score(y_test,nb_clf.predict(X_test))\nprint(F1_nb)","6c6e03ab":"#we can also use batch gradient descent but it takes a lot more time when compared to this as weights gets updated after each batch.\nfrom sklearn.linear_model import SGDClassifier\nsgd_clf=SGDClassifier(alpha=0.3, random_state=37, penalty= \"l2\",loss='log')#applies l2 regulrazation,we need apply log as loss function to get probablities\nsgd_score=CrossValidate(X_train,np.ravel(y_train),sgd_clf)\nprint(sgd_score)\nsgd_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),sgd_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,sgd_clf.predict(X_test)),annot=True)\nF1_sgd=f1_score(y_test,sgd_clf.predict(X_test))\nprint(F1_sgd)","0300bd4e":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf=GradientBoostingClassifier(learning_rate=0.1,max_depth=6,n_estimators=150,random_state=0,tol=1e-10)#increasing learning rate will make model more complex.\ngb_score=CrossValidate(X_train,np.ravel(y_train),gb_clf)\nprint(gb_score)\ngb_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),gb_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,gb_clf.predict(X_test)),annot=True)\nF1_gb=f1_score(y_test,gb_clf.predict(X_test))\nprint(F1_gb)\n","2f7eeae9":"from sklearn.neural_network import MLPClassifier\nnn_clf=MLPClassifier(hidden_layer_sizes=[150,100,100,100,100],solver='lbfgs',random_state=20,activation='tanh',alpha=0.01)\nnn_score=CrossValidate(X_train,np.ravel(y_train),nn_clf)\nprint(nn_score)\nnn_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),nn_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,nn_clf.predict(X_test)),annot=True)\nF1_nn=f1_score(y_test,nn_clf.predict(X_test))\nprint(F1_nn)\n","781e2a92":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf_clf, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","f30a4c91":"fig= plt.figure(figsize=(10,10))\nimportant_feature=pd.Series(rf_clf.feature_importances_, index=X_train.columns)\nimportant_feature.sort_values().plot.bar()","68696184":"models_accuracy=pd.Series(data=[knn_score,lg_score,dt_score,svm_score,rf_score,nb_score,sgd_score,gb_score,nn_score],index=[\"knn\",\"lg\",\"dt\",\"svm\",\"rf\",\"nb\",\"sgd\",\"gb\",\"nn\"])\nmodels_accuracy.sort_values().plot.bar()","31e8985e":"models_accuracy=pd.Series(data=[F1_knn,F1_lg,F1_dt,F1_svm,F1_rf,F1_nb,F1_sgd,F1_gb,F1_nn],index=[\"knn\",\"lg\",\"dt\",\"svm\",\"rf\",\"nb\",\"sgd\",\"gb\",\"nn\"])\nmodels_accuracy.sort_values().plot.bar()","b6de95ee":"from sklearn.ensemble import VotingClassifier\nvc_clf=VotingClassifier(estimators=[('knn',knn_clf),('lg',lg_clf),('sgd',sgd_clf),('svm',svm_clf),('nn',nn_clf)],voting='soft')\nvc_score=CrossValidate(X_train,np.ravel(y_train),vc_clf)\nprint(vc_score)\nvc_clf.fit(X_train,np.ravel(y_train))\nplotting(np.ravel(y_test),vc_clf.predict_proba(X_test))\n\nfig=plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test,vc_clf.predict(X_test)),annot=True)\nF1_vc=f1_score(y_test,vc_clf.predict(X_test))\nprint(F1_vc)","4bf84a4b":"<h2>1.Gathering the data:-<\/h2>","0fe36298":"<h3>2.1)Analysis of features:-<\/h3>","51e31130":"<h3>Let's explore the slope of the peak exercise ST segment (slope)(Category)<\/h3>","44602305":"You can see that cp changed to cp_1,cp_2,cp_3 and remaining categorical features are also changed in that way.And drop_first makes it won't show the sex_0 as actually sex will be encoded as (sex_0 representing 0 and sex_1 representing 1 but by dropping sex_0 also we don'y loss any information because still 1 represents male and if sex_1=0 then that means it is other than male that is feamle.\n\nWe can see that the features values are on different scales so we need to normalize them because for some algorithms to train the normalised data works very well.\n\n<h3>Normalization (To get value b\/w 0 and 1)<h3>","081ae02d":"<h3>Now we will explore  resting electrocardiographic results(restecg){0,1,2}(category):-<\/h3>","98ed34b7":"<h4>First let's do the train test split:<\/h4>","466a2882":"<h3>Let's explore cholestrol(chol):-<\/h3>","c42851a3":"<h3>Sex Feature:-<\/h3>","4a6f3838":"To see the distribution of each feature we can either use histogram or density plot(kde plots or smoothen histograms)","85dd65e0":"And that's not it you can do much more analysis to get more insights of the data but for now the knowledge we got from analysis is enough to work on....So by checking the similarity between the each of the two features we can stop our analysis.","ed02582b":"So,in this way we can analyse the continuous variable with a categorial variable using swarmplot and the conclusions we can get are:\n\nSo, with respect to blood pressure the gender plays a minor role where as the chest pain(cp),exang,ca(number of major vessels) plays an important role.\n\nAnd we can get many other useful insights from this plot ...","7a45a4d2":"<h3>Trestbps (Resting blood pressure)<\/h3>","caac9632":"<h3>2)Logistic Regression<\/h3>\n\nYou might think that it is a regression algorithm but it is not, it is purely an classification algorithm....","3499f6b8":"Wow .... we can see that there are no null values so we can start our exploration and find the correlation or similarity between features.....","fe0495ca":"So above graph shows that if the person has high old peak he is less prone to heart diseases in the similar way we can check with other categorical variables vs this feature to get more insights.\n","05ee8111":"<h3>Now explore cp(chest pain){0,1,2,3} (category):-<\/h3>","5986611c":"<h3>The number of major vessels<\/h3>\n\nLet's explore this..","6a921b3e":"Now well we have trained a lot of models ........ There are other classifiers also like BaggingClassifier,ExtraTree Classifiers etc...,.But random forest gives you the best of all in most cases...So let's stop here...\n\nNow we will see what are the important features......Please check out the mini course in kaggle the machine learning Explainability it is a very wonderful course....\n\nNow let's move....","0061c53a":"Wow....we got pretty good F1-score,precision and AUC scores....\n\nNext we will train against the Decision Tree classifier\n\n<h3>3)Decision Tree Classifier<\/h3>","8a6cf27e":"<h3>7)Stochastic Gradient Descent<\/h3>","c41c9852":"If you once the check the description of the dataset that we are using we can see that the features given in this dataset are scientifically checked that whether they has any effect on heart diseases and they make sure of removing not useful information of the patients like weight etc..,.. \n\nSo we can leave the steps 2.3)Adding new features and 2.4)Removing redundant features as they are actually done before giving the dataset to us.\n\nIn competitions every model accuracy is defined by those steps that is adding new features and removing redundant features(Feature Engineering).I'll tell you a example my model for titanic survival problem(Getting started in ML) in kaggle got around 78percent accuracy how many different algorithms i tried and fine tuned parameters but after careful feature engineering by adding a feature family survival(accuracy rised to 83) and i got upto top 4% from 52% that's how important it is.\n\nThat feature engineering you can do only by finding some correlations among given features through analysis by plotting against each other....\n\nPlease checkout kaggle mini course (feature engineering)....\n","ba40a255":"So the features that contribute more to a heart disease are mainly thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect),oldpeak: ST depression induced by exercise relative to rest, The person's maximum heart rate achieved........Now we will see another feature extraction techinque","e31e6896":"We can say that accuracy vise knn,sgd and svm are doing good but based on accuracy we can't say it is good for classification tasks F1 scores can be good criteria","3ddedebc":"From last plot we can conclude that cholestral levels won't effect the heart disease much and \"Females has more cholestral when compared to men and if number of major vessels are zero has more prone to heart diseases and we can get many useful insights from the above plots","7956dbbc":"Now convert the categorical data into one hot encoding using the get_dummies() function and please check the above said article which tells why we need to do one hot encoding. As the models won't operate on string items we need to make them some numerical values that will be useful for the computer for modelling.","f438e300":"Now we do the step 2.5)\n\n<h3>2.5)Converting all the features into suitable forms for modelling:-<\/h3>","e4f67d5e":"Wow....we got great precision and recall values......Now let's checkout other algorithms also.","f7c30a48":"To compute cross validation scores and to get plots for precision_recall curve and roc_curve we will define the helper functions for each.","4c17f813":"<h3>9)Neural Network<\/h3>","3706525c":"<h3>5)Random Forest Classifier<\/h3>","c793c655":"<h4>Let's check how many people in total have heart problems with respect to all people in dataset<\/h4>","e348ea2b":"So 54.5% people are suffering from heart diseases...oh well let's now look into other insights.Now on further we will plot the above two plots in only plot by using subplots() by specifying arguments as 1,2 specify 1 row 2 columns that means in one row we will have 2 diagramas....","e4bd4ef5":"First of all please checkout this wonderful article [Handling categorical data](https:\/\/www.datacamp.com\/community\/tutorials\/categorical-data) in which there is detailed explanation on how to convert the categorical into suitable forms for modelling and please checkout that article it is very useful for all your ml models.","3c5aefb1":"Now we successfully defined our evalution metrics.....So now let's start training models...\n\nIn this notebook i didn't explained the hyperparameter tuning because as if you are novic learner i thought you could explore more things and learn much more if you do that on your own.....\n\nBut i will tell you the way how to find best parameters to your solution...\n\nThe things you need to learn are \n\n1)GridSearchCV(Basic)\n\n2)write every algorithm parameters on a paper and work on them.\n\nIf you do those things you can build your own awesome models.....","8ecee896":"<h3>2.2)Finding or Exploring all the trends between features:-<\/h3>\n\nIf you are interested please check out the wonderful course on coursera:-[Data visualization](https:\/\/www.coursera.org\/learn\/python-plotting\/home\/welcome) gives all the insights on plotting and visualization or check out mini courses in kaggle","fa23ccb1":"<h3>8)Gradient Boosted Decision Trees<\/h3>","1cc315ec":"Now after training each model we will evaluate that model simantenuosly...... [model evalution](https:\/\/www.coursera.org\/learn\/python-machine-learning?specialization=data-science-python) check the week-3 of this course to get in depth knowledge on model evalution metrics.\n\nOr Please know about ROC curve,Precision recall curve and confusion matrix.......\n\nSo now we can move further we will now set our Evalution metrics for our models.\n\nWe must understand that always accuracy is not a good metric for evalution.\n\n<h3>Evalution Metrics<\/h3>","7b7b8595":"so we can see that people having fbs<120 are more prone to heart diseases and also the number of people in this category are more","5a91bdd1":"Continuous data can be explored using the scatter plots,histograms,headmaps,swarmplots etc..,. and we can't do with count plots,pie charts etc.,. as they tend to more values.","eed13a3d":"well...we see that more than half of the people in the dataset are suffering from heart diseases...So now let's try to be precise.Let's find the percentage of people who are suffering from heart diseases by using autopct argument in pie plot we can have that..","63738a53":"(0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nHere people having ST-T wave abnormality are more prone to the heart diseases.An electrocardiogram (ECG) is a test which measures the electrical activity of your heart to show whether or not it is working normally. An ECG records the heart's rhythm and activity on a moving strip of paper or a line on a screen","472af5d6":"Now Let's see Exploration as a whole dataset:-","bc2448fd":"So from these we can tell which two features are highly related and which are not....and we can see that most values are positives it shows most features are related to each other.\n\n","c3b866d8":"<h3>Now Explore  Oldpeak (continuous feature) <\/h3>","32ff8894":"There the blue bar shows how many have disease and the red shows whom donot have disease and the first pair of bars are feamle and the next are male .....\n\nso we conclude men population is more .... and 3 in 4 women has heart disease .... so this feature is very useful","20aa28c9":"so if category is 0 they are mostly safe but if they are of 1 or 2 or 3 category they are more prone to heart disease.So this feature is useful for model.Here most important thing very few people could get through the 2nd level of chest pain most people will die here so the safe side is o level chest pain people and remaining all levels of people are prone to heart diseases.","ae8a08bb":"Now know some details about every feature:-\n\n    age: The person's age in years\n    sex: The person's sex (1 = male, 0 = female)\n    cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n    trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n    chol: The person's cholesterol measurement in mg\/dl\n    fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n    restecg: Resting electrocardiographic measurement (0,1,2)\n    thalach: The person's maximum heart rate achieved\n    exang: Exercise induced angina (1 = yes; 0 = no)\n    oldpeak: ST depression induced by exercise relative to rest \n    slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n    ca: The number of major vessels (0-3)\n    thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n    target: Heart disease (0 = no, 1 = yes)\n   \nIf you are interested find some more information on each of these features:-","90d40d3d":"Now load the dataset..","c3ef14c2":"Wow...Now we have just completed our analysis on categorical data ...now move on to the continuous data.","0f4f169a":"<h1>               Diagnosing Heart Disease with ML          <\/h1>\n\n\n<h2>Introduction<\/h2>\n\nThe main objective of this kernel is make sure even a naive person without having much of ml knowledge can understand what was going and after completing this notebook i am confident that you can build your own ml models.Be confident and don't bother by seeing the size of this notebook it contains pure simple language so everyone can understand it......I explained each and every step of ML model building...\nSo please spend time and try it complete it ....If there are any suggestions you can mention it in comments and in the middle of the notebook you will some links please checkout those links also......\n\nIf you find this notebook useful please upvote it.\n\n<h2>Data Science in Medical Field :-<\/h2>\n\nIn the future days the Data Science could play an important role in Medical Field.Like many fields it's going to transform the medical\nfield also.Of all the applications of machine-learning, diagnosing any serious disease using a model is always going to be a hard thing. If the output from a model is the particular course of treatment (potentially with side-effects), or surgery, or the absence of treatment, people are going to want to know why we should trust that model or it's result.So we need the best models to apply in the field of medical.\n\nNow In this kernel we will build a baseline model for the prediction of whether the given person has heart disease or not.\n\nWe will do this step by step from  Exploratory Data Analysis to Validation of Model.I will try to make every concept very clear for you\nto understand in simple language and please upvote it if you find it useful.\n\nNow we will first look at the basic workflow that everyone should follow in the model building:-\n\n**1)Primary thing is collecting the data** and as for now we have the large datasets to work on ... so it's not an problem. \n\n\n**2)Next thing is data preprocessing** and it is the most important phase of all phases:It includes:-\n\n      2.1) Analysis of features(knowing the meaning of each and every feature).\n\n      2.2) Finding or Exploring all the trends between the features(using different kinds of plots gives an insight into the given data).\n    \n      2.3) Adding new features by combining given features(if they seem to be helpful)\n  \n      2.4) Removing redundant features(Features which may have no effect on output of the model)\n  \n      2.5) Converting all the features into suitable form for modelling(I'll explain in depth about this later on).\n\n\n**3)Researching the model that will be best for the type of data**\n\n       3.1)checking out on all the algorithms we know and tuning the parameters.\n   \n\n**4)Training and testing the model**\n\n       4.1) Cross Validation.\n   \n\n**5)Evaluation**\n    \n       5.1) Plotting ROC Curve, Precision\/Recall Curve, AUC\n   \n       5.2) Model Comparison (Accuracy + F1 Score)\n   \n\n**6)optional(ensemble)**\n\n       6.1) Then we can ensemble the best models we got into a powerful model....\n       \n       6.2) Important Features Extraction(To best explain the model with explanitory resources of ml).\n\nSo now let's start....\n","4d2a74d6":"<h3>Let's explore fasting blood sugar (Fbs) (Category)<\/h3>","5644559d":"Before that to apply heatmap we need check all values are there or not...\n\n<h3> Checking missing values<\/h3>","6efd994d":"Now the data is ready for modelling....So we move to next step\n\n<h3>3.checking for the best algorithm and parameter tuning:-<\/h3>","53149d04":"<h3>thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)<\/h3>","435b866b":"people having fixed defect(6) tends be prone to heart diseases.And this shows a great value to our model as more people having this fixed defect are prone to heart diseases.","4aa72e38":"We can see that if the number of vessels are 0 then that person is most prone to heart diseases when compared to remaining other cases","b04f28c3":"**First convert all the features having categorical values into category type:- **","516dabfa":"Now we can check how the distributions are there in each feature and we can also find the values which are most occuring in each feature. ","904148d3":"<h2>Summary<\/h2>\n\nWow we finally got Average precision score of 95 and AUC 93 which is wonderful with the small dataset we have.......and we got the F1 score nearly as 89.....\n\nSo by combining the best models into voting classifier we got the best results on our test set....\n\nWe can also ensemble the models into one complex model but it is of not much use when we have a small dataset like we have now ....So it's better using the voting classifier and we got one of the best classifier for heart disease prediction...\n\nI hope you guys like it....If you find it useful upvote it and mention any suggestions and feedback........\n","1df91709":"Now remove the output target column from our dataset","de173c14":"Wow......Neural networks,gaussian boosted,Sgd are doing great and remaining also giving good F1 scores.......\n\nSo now we will combine the results of these with Voting classifier\n\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n\n<h3>10)Voting Classifier<\/h3>","70de9227":"<h3>6)Naive Bayes Classifier<\/h3>","a785e74b":"Wow....we got a great insight here that if the person slope value is 2 than he's mostly likely to suffer with heart disease.","d8d4f82e":"Yes.....Now we find the same features as important .....So, we can say that maximum heart beat,thalassemia effect greatly.\n\nNow check which models are performing greatly by using plots.","54d47516":"We are going to use Heart Disease UCI Dataset and let's load the libraries that we need:-","cdd1b698":"<h3>1)We will start with K-Nearest Neighbors<\/h3>","ad903418":"<h3>4)Support vector machine<\/h3>"}}