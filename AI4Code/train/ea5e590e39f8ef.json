{"cell_type":{"048a10ee":"code","a0d8cb21":"code","0be03f31":"code","ead2460b":"code","6e5a4e4e":"code","6bdb415b":"code","cf3e1fc5":"code","d187c0e6":"code","e1221c5b":"code","c9e32b17":"code","004838d8":"code","fb187b3f":"code","604ee65f":"code","71fb230e":"code","1a8a994f":"code","d3c23957":"code","c89587f2":"code","0a0a781f":"code","2baec76d":"code","92080c53":"code","f6ab1db5":"code","c6e58e93":"code","5d682881":"code","acf50cef":"code","d3d438ae":"code","607ad912":"code","3e7f6003":"code","4d5029f0":"code","6248b6ff":"code","74d0fdd2":"code","65f2c5bf":"code","2ff7cd72":"code","49ab3454":"code","437227cf":"code","82e27ec8":"code","5b39f622":"code","ce5c405b":"code","d350c54b":"code","e10631d0":"code","6e67f384":"code","9d5260f3":"code","8248a5f2":"code","e7971403":"code","ee057aaf":"code","0757e048":"code","229f38bc":"code","bbc41964":"code","7263a5c3":"code","25c9783a":"code","17bb7af8":"code","43436cda":"code","76a06be3":"code","5a39b92e":"code","0d742679":"code","fca4658b":"code","15250226":"code","49d02ae3":"markdown","9747aedd":"markdown","12183fff":"markdown","baf85cca":"markdown","5fd1c023":"markdown","85f7e8a4":"markdown","daf42355":"markdown","d484c244":"markdown","b7d09060":"markdown"},"source":{"048a10ee":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a0d8cb21":"# Importing the dataset\ndataset = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\", delimiter=\",\")","0be03f31":"# Viewing the top 5 rows in the dataset\ndataset.head()","ead2460b":"# To find out if there are any missing values \ndataset.isna().any()","6e5a4e4e":"# Building HeatMap to identify which features are relevant for determining quality of red wine\nplt.figure(figsize=(20,10))\ng = sns.heatmap(data      = dataset.corr(),  \n            square    = True, \n            cbar_kws  = {'shrink': .3}, \n            annot     = True, \n            annot_kws = {'fontsize': 12},\n           )\ng.set(ylim=(0,12))\ng.set(xlim=(0,12))\nplt.show()","6bdb415b":"# Plotting the distribution of dependent variable (quality)\nsns.distplot(dataset['quality'])\nplt.show()","cf3e1fc5":"# Plotting the distribution of independent variable (alcohol)\nsns.distplot(dataset['alcohol'])\nplt.show()","d187c0e6":"# Plotting the distribution of independent variable (sulphates)\nsns.distplot(dataset['sulphates'])\nplt.show()","e1221c5b":"# Plotting the distribution of independent variable (citric acid)\nsns.distplot(dataset['citric acid'])\nplt.show()","c9e32b17":"# some more visualisation\n# Alcohol vs Quality\nsns.barplot(\"quality\", y=\"alcohol\", data=dataset, saturation=.5, palette = 'inferno')\nplt.show()","004838d8":"# Citric Acid vs Quality\nsns.barplot(\"quality\", y=\"citric acid\", data=dataset,saturation=.5, palette = 'inferno')\nplt.show()","fb187b3f":"# Suplhates vs Quality\nsns.barplot(\"quality\", y=\"sulphates\", data=dataset, saturation=.5, palette = 'inferno')\nplt.show()","604ee65f":"Features = ['citric acid','sulphates','alcohol']","71fb230e":"# Creating features(x) and dependent variable(y)\nx = dataset.iloc[:,[2,9,10]].values\ny = dataset.iloc[:,-1].values","1a8a994f":"print(x)","d3c23957":"print(y)","c89587f2":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","0a0a781f":"print(x_train[0])","2baec76d":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:,[2]] = sc.fit_transform(x_train[:,[2]])\nx_test[:,[2]] = sc.transform(x_test[:,[2]])","92080c53":"print(x_train[0])","f6ab1db5":"# Running for loop to determine the number of neighbors for best accuracy\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor neighbors in range(3,20,1):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,20,1)), list1)\nplt.show()","c6e58e93":"# Training the K Nearest Neighbor Classifier on the Training set with n_neighbors = 10\nclassifier = KNeighborsClassifier(n_neighbors=10, metric='minkowski')\nclassifier.fit(x_train, y_train)","5d682881":"# Predicting the Test Set results\ny_pred = classifier.predict(x_test)\nprint(y_pred)","acf50cef":"# Printing the predicted test set results and actual test results\nnp.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","d3d438ae":"# Making the confusion matrix and accuracy\nmylist = []\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","607ad912":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\", hist_kws = {\"align\": \"right\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"left\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","3e7f6003":"# Training the Naive Bayes Classifier on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","4d5029f0":"# Predicting the Test Set result\ny_pred = classifier.predict(x_test)\nprint(y_pred)","6248b6ff":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","74d0fdd2":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","65f2c5bf":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\",  hist_kws = {\"align\": \"left\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\",  hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","2ff7cd72":"# Training the Support Vector Classifier on the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state=0, kernel = 'rbf')\nclassifier.fit(x_train, y_train)","49ab3454":"y_pred = classifier.predict(x_test)\nprint(y_pred)","437227cf":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","82e27ec8":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","5b39f622":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\",  hist_kws = {\"align\": \"left\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","ce5c405b":"# Determining the max_leaf_nodes using for loop for leaves 2-25\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,25):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,25)), list1)\nplt.show()","d350c54b":"# we can see the optimum number of max_leaf_nodes = 5","e10631d0":"# Training the Decision Tree Classifier on the Training set\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 5, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","6e67f384":"y_pred = classifier.predict(x_test)\nprint(y_pred)","9d5260f3":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","8248a5f2":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","e7971403":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\", hist_kws = {\"align\": \"left\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","ee057aaf":"# Determining the optimum number of n_estimators\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(100,200):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(100,200)), list1)\nplt.show()","0757e048":"# Training the Random Forest Classifier on the Training set\nclassifier = RandomForestClassifier(n_estimators=180, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","229f38bc":"y_pred = classifier.predict(x_test)\nprint(y_pred)","bbc41964":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","7263a5c3":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","25c9783a":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\", hist_kws = {\"align\": \"left\"} )\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","17bb7af8":"# Training the XGBoost Classifier on the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(x_train,y_train)\ny_pred = classifier.predict(x_test)","43436cda":"print(y_pred)","76a06be3":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","5a39b92e":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","0d742679":"# Accuracy score of different models\nmylist","fca4658b":"mylist2 = [\"KNearestNeighbours\",\"NaiveBayes\",\"SupportVector\",\"DecisionTree\",\"RandomForest\",\"XGBoost\"]","15250226":"# Plotting the accuracy score for different models\nplt.rcParams['figure.figsize']=10,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","49d02ae3":"-----","9747aedd":"From above graph we can see that optimal number of neighbors is 10","12183fff":"From above HeatMap we can see that 'alcohol', 'sulphates', 'citric acid' are important features in determining 'quality' of red wine","baf85cca":"-----","5fd1c023":"----","85f7e8a4":"----","daf42355":"----","d484c244":"----","b7d09060":"----"}}