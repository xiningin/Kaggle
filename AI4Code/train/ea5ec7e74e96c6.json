{"cell_type":{"ab7c8757":"code","5e5d053a":"code","68a399c5":"code","b7f7846b":"code","221d3789":"code","e9a6a8e7":"code","0fa04a44":"code","b5eb0787":"code","98a4106d":"code","d443954e":"code","6f1a5540":"code","5a103450":"code","a99c7c5a":"code","97e3ec72":"code","b797a0fe":"code","ddd99470":"code","44d77921":"markdown","918035d5":"markdown","8e92d3eb":"markdown","59c5487f":"markdown","df296a46":"markdown","cb7ee4c9":"markdown","4667ccb7":"markdown","632c4dad":"markdown","f01f7be9":"markdown","d4de88af":"markdown","d7e1c4b6":"markdown","25135949":"markdown","401e82f3":"markdown","ede2b959":"markdown","348b59f5":"markdown","aaa3c6b1":"markdown","bc826779":"markdown"},"source":{"ab7c8757":"import glob\nimport numpy as np\nimport pandas as pd ","5e5d053a":"glob.glob('..\/input\/intel-image-classification\/seg_train\/seg_train\/*')","68a399c5":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array","b7f7846b":"def prepare_dataset(path,label):\n    x_train=[]\n    y_train=[]\n    all_images_path=glob.glob(path+'\/*.jpg')\n    for img_path in all_images_path :\n            img=load_img(img_path, target_size=(150,150))\n            img=img_to_array(img)\n            img=img\/255.0\n            x_train.append(img)\n            y_train.append(label)\n    return np.array(x_train),np.array(y_train)","221d3789":"paths=glob.glob('..\/input\/intel-image-classification\/seg_train\/seg_train\/*')\nl=len('..\/input\/intel-image-classification\/seg_train\/seg_train\/')\nlabels=[]\nfor path in paths:\n    labels.append(path[l:])\n    print(labels)","e9a6a8e7":"trainX_building, trainY_building  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/\",0)\ntrainX_forest,trainY_forest  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/\",1)\ntrainX_glacier,trainY_glacier  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/\",2)\ntrainX_mount,trainY_mount  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/\",3)\ntrainX_sea,trainY_sea  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/\",4)\ntrainX_street,trainY_street  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/\",5)\n\nprint('train building shape ', trainX_building.shape, trainY_building.shape) \nprint('train forest', trainX_forest.shape ,trainY_forest.shape)\nprint('train glacier', trainX_glacier.shape,trainY_glacier.shape)\nprint('train mountain', trainX_mount.shape, trainY_mount.shape)\nprint('train sea',     trainX_sea.shape, trainY_sea.shape)\nprint('train street', trainX_street.shape ,trainY_street.shape)","0fa04a44":"x_train=np.concatenate((trainX_building,trainX_forest,trainX_glacier,trainX_mount,trainX_sea,trainX_street),axis=0)\ny_train=np.concatenate((trainY_building,trainY_forest,trainY_glacier,trainY_mount,trainY_sea,trainY_street),axis=0)","b5eb0787":"print(x_train.shape)\nprint(y_train.shape)","98a4106d":"# from sklearn.model_selection import train_test_split\n# train_tes_split(x_train)","d443954e":"testX_building, testY_building  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/buildings\/\",0)\ntestX_forest,testY_forest  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/forest\/\",1)\ntestX_glacier,testY_glacier  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/glacier\/\",2)\ntestX_mount,testY_mount  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/mountain\/\",3)\ntestX_sea,testY_sea  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/sea\/\",4)\ntestX_street,testY_street  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/street\/\",5)\n\nx_test=np.concatenate((testX_building,testX_forest,testX_glacier,testX_mount,testX_sea,testX_street),axis=0)\ny_test=np.concatenate((testY_building,testY_forest,testY_glacier,testY_mount,testY_sea,testY_street),axis=0)","6f1a5540":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.optimizers import RMSprop\n\nlocal_weights_file = '\/kaggle\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n     layer.trainable = False\n        \n# pre_trained_model.summary()","5a103450":"last_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)           \n\nmodel = Model(pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])\n\nhistory=model.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))","a99c7c5a":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.optimizers import RMSprop\n\n# local_weights_file = '\/kaggle\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = \"imagenet\")\n\n# pre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n     layer.trainable = False\n        \n# pre_trained_model.summary()\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)           \n\nmodel = Model(pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])\n\nhistory=model.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))","97e3ec72":"from tensorflow.keras.applications import VGG16\n\npretrained_model=VGG16(input_shape = (150, 150, 3), \n                        include_top = False, \n                        weights = 'imagenet')\n\nfor layer in pretrained_model.layers:\n     layer.trainable = False\n\n# pretrained_model.summary()\nlast_layer = pretrained_model.get_layer('block5_pool')\nprint('last layer of vgg : output shape: ', last_layer.output_shape)\nlast_output= last_layer.output\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)           \n\nmodel_vgg = Model(pretrained_model.input, x) \n\n\nmodel_vgg.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])\n\n# model_vgg.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))\n","b797a0fe":"file='\/kaggle\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_model=VGG16(input_shape = (150, 150, 3), \n                        include_top = False, \n                        weights =None)\npretrained_model.load_weights(file)\n\nfor layer in pretrained_model.layers:\n     layer.trainable = False\n\nlast_layer = pretrained_model.get_layer('block5_pool')\nprint('last layer of vgg : output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)           \n\nmodel_vgg = Model(pretrained_model.input, x) \n\n\nmodel_vgg.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])\n\n# model_vgg.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))","ddd99470":"from tensorflow.keras.applications import ResNet50\n\n#step1\n# file_resnet='\/kaggle\/input\/vgg16\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_model=ResNet50( input_shape=(150,150,3),\n                                  include_top=False,\n                                  weights='imagenet'\n                                   )\n#step2\nfor layer in pretrained_model.layers:\n     layer.trainable = False\n\n# pretrained_model.summary()\n        \n#step3        \nlast_layer = pretrained_model.get_layer('conv5_block3_out')\nprint('last layer of vgg : output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\n#step4\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)\n\n#step5\nmodel_resnet = Model(pretrained_model.input, x) \n\n#step6\nmodel_resnet.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])\n\n#step7\n# model_resnet.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))\n","44d77921":"## 2.1 InceptionV3 ","918035d5":"#### 2.2.1 : VGG16 with inbuilt pretrained weights by the Imagenet","8e92d3eb":"You can also refer to this link. It has a documentation about fine-tuning the pretrained model\nhttps:\/\/keras.io\/applications\/","59c5487f":"# Preparing the dataset","df296a46":"#### 2.2.2 InceptionV3 with inbuilt pretrained weights by the Imagenet","cb7ee4c9":"* the name 'mixed7' in below code is a name of one of the layers of Inceptionv3. You can check this out in the summary of the pretraine model by executing ***pre_trained_model.summary()***\n<br><br>PS : You can also choose different layer ","4667ccb7":"<div>\nI guess,You would have noticed the difference between approach 2.1.1 and 2.1.2 !!\n<br>\n<br> All the things will remain same.The only change is that we used pretrained file of weights in 2.2.1 and in 2.2.2 we use 'imagenet' in this line: <br>pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = \"imagenet\")","632c4dad":"### Now you would have learned how to fine tune the pretrained model... Right!\nTry to do the second approach <b>Resnet using the pretrained weight's file<\/b>* by your own. All the best!\n<br>Here you will find the file : https:\/\/www.kaggle.com\/keras\/resnet50","f01f7be9":"## 2.3 ResNet50","d4de88af":"\n# **In this notebook you will learn how to fine tune different pretrained models like Resnet, InceptionV3, VGG**\n<br>\n<span style=\"color:green\"> I have made this tutorial for the beginners in computer vision. Hope you would learn something new from my notebook!!<\/span>","d7e1c4b6":"Test Dataset","25135949":"## 2.2 VGG16","401e82f3":"#### 2.3.1 Resnet with inbuilt pretrained weights by the Imagenet","ede2b959":"# 2. Transfer learning - Different approaches","348b59f5":"<div><span style=\"color:green\">You can improve the accuracy by changing the architecture of the model <span><\/div>","aaa3c6b1":"#### 2.1.1 InceptionV3 with pretrained weights file \n<br>You can find the ptretraiend weight's file here: https:\/\/www.kaggle.com\/keras\/inceptionv3","bc826779":"#### 2.2.2 : VGG16 with pretrained weights file \nYou will find this file from this link: https:\/\/www.kaggle.com\/keras\/vgg16"}}