{"cell_type":{"20c51c54":"code","5ee676c9":"code","b70bd559":"code","3c7de7fa":"code","17de76f6":"code","febd2ee7":"code","5371f3f2":"code","bd745d74":"code","72576d1f":"code","c2bf7e1d":"code","8ebbeaae":"code","973887a5":"code","2043bf6c":"code","c0bb9af9":"code","388be16d":"code","a874a639":"code","7eb41fa2":"code","67077227":"code","0fe6b206":"code","c9dc45db":"code","24405e57":"code","36666973":"code","813505ca":"code","e41a0f07":"code","fb3fac75":"code","6ca014f9":"code","98bb08a3":"code","4a3e6b6b":"code","553d7a17":"code","8b582bb2":"code","6c45143a":"code","9bdb7225":"code","beb9f27d":"code","77354c7a":"code","1b5cee3d":"code","f747acc9":"code","7fd81599":"code","ede77252":"code","9c166b39":"code","271acacb":"code","bda250e4":"code","bd3e1208":"code","870affdc":"code","9cfe81a6":"code","1a2d3a0e":"code","d0ac8c10":"code","02b8465c":"code","9934fac5":"markdown","df3b2aae":"markdown","2699c753":"markdown","84ff8a6e":"markdown","ed6da712":"markdown","ef9d441d":"markdown","5b4fb13b":"markdown","bd50e8ac":"markdown","4607e8cd":"markdown","1228b07d":"markdown","2ba11671":"markdown","628e4fcc":"markdown","e790eff8":"markdown","9e3a9d1b":"markdown","d9873ab0":"markdown","60f97af6":"markdown","2fb44655":"markdown","6399dbcf":"markdown","de606c05":"markdown","983ca3ed":"markdown","0f977bf0":"markdown","6716d0fd":"markdown","ee450224":"markdown","be3529bc":"markdown","48889a34":"markdown","55b640c4":"markdown","b6902a34":"markdown","09f011b8":"markdown","dbc49435":"markdown","a44f5106":"markdown","641d9c59":"markdown"},"source":{"20c51c54":"# pip install chart_studio","5ee676c9":"import plotly.graph_objs as go\nimport time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\nimport time\n\n\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')","b70bd559":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","3c7de7fa":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('\/kaggle\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('\/kaggle\/input\/demand-forecasting-kernels-only\/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)","17de76f6":"holiday = pd.read_csv('\/kaggle\/input\/usbankholidays\/USBankHolidays', index_col=0, header=None, names=['date','name'])\nholiday.columns = ['date', 'holiday']\nholiday['date'] = holiday['date'].apply(pd.to_datetime)\ndf = df.merge(holiday, how='left', on='date')","febd2ee7":"df[\"date\"].min()","5371f3f2":"df[\"date\"].max()","bd745d74":"check_df(train)","72576d1f":"check_df(test)","c2bf7e1d":"check_df(sample_sub)","8ebbeaae":"check_df(df)","973887a5":"daily_sales_sc = go.Scatter(x=df['date'], y=df['sales'])\nlayout = go.Layout(title='Daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=[daily_sales_sc], layout=layout)\niplot(fig)","2043bf6c":"store_daily_sales_sc = []\nfor store in df['store'].unique():\n    current_store_daily_sales = df[(df['store'] == store)]\n    store_daily_sales_sc.append(go.Scatter(x=current_store_daily_sales['date'], y=current_store_daily_sales['sales'], name=('Store %s' % store)))\n\nlayout = go.Layout(title='Store daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=store_daily_sales_sc, layout=layout)\niplot(fig)","c0bb9af9":"# how is sales distribution\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])","388be16d":"# how many stores ?\ndf[[\"store\"]].nunique()","a874a639":"# how many items\ndf[[\"item\"]].nunique()","7eb41fa2":"# every store has same item counts\ndf.groupby([\"store\"])[\"item\"].nunique()","67077227":"# every store hsa same sales count ?\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})","0fe6b206":"# store-item sales stats\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","c9dc45db":"def create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n\n    df['quarter_of_year'] = df.date.dt.quarter\n    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(int)\n    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(int)\n    df['is_year_start'] = df.date.dt.is_year_start.astype(int)\n    df['is_year_end'] = df.date.dt.is_year_end.astype(int)\n    df['daysinmonth'] = df.date.dt.daysinmonth\n\n    df['holiday_bool'] = pd.notnull(df['holiday']).astype(int)\n\n    return df","24405e57":"df = create_date_features(df)","36666973":"df.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","813505ca":"def random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","e41a0f07":"df.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)","fb3fac75":"check_df(df)","6ca014f9":"df[\"sales\"].head(10)\n","98bb08a3":"df.groupby([\"store\", \"item\"])['sales'].head()\ndf.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(1))\n","4a3e6b6b":"def lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe","553d7a17":"#df = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])\ndf = lag_features(df, [89, 90, 91, 98, 105, 112, 119, 126, 182, 273, 364, 455, 546, 728])\ncheck_df(df)","8b582bb2":"df[df[\"sales\"].isnull()]","6c45143a":"def roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n            transform(lambda x: x.rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(dataframe)\n    return dataframe","9bdb7225":"#df = roll_mean_features(df, [365, 546])\ndf = roll_mean_features(df, [90, 180, 365, 546, 730])\n\n# df.sales.head()\n# df.sales.shift(1).rolling(3, win_type=\"triang\").mean()","beb9f27d":"df['expanding_sales_mean'] = df.groupby(['store', 'item'])['sales'].transform(lambda x: x.shift(1).expanding(2).mean()).astype(np.float16) + random_noise(df)\n","77354c7a":"# df['daily_avg_sold'] = df.groupby(['store', 'item','date'])['sales'].transform('mean').astype(np.float16)\n# df['avg_sold'] = df.groupby(['store', 'item'])['sales'].transform('mean').astype(np.float16)\n# df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16) + random_noise(df)\n# df.drop(['daily_avg_sold', 'avg_sold'], axis=1, inplace=True)\n# This always leads model to overfit, so for this data i wont use that feature eng.\n\n\n# df['selling_trend'].plot()\n# plt.show()\n# df['selling_trend'].describe()\n################################################\n# Exponentially Weighted Mean Features\n################################################\n\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe","1b5cee3d":"alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n# lags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\nlags = [89, 90, 91, 98, 105, 112, 119, 126, 182, 273, 364, 455, 546, 728]","f747acc9":"df = ewm_features(df, alphas, lags)\ncheck_df(df)","7fd81599":"#df = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month', 'quarter_of_year', 'daysinmonth', 'holiday'])\n","ede77252":"import plotly.express as px\nfig = px.histogram(df, x=df['sales'], nbins=50)\nfig.update_layout(bargap=0.2)\nfig.show()","9c166b39":"df['sales'] = np.log1p(df[\"sales\"].values)\ncheck_df(df)\n","271acacb":"fig = px.histogram(df, x=df['sales'], nbins=50)\nfig.update_layout(bargap=0.2)\nfig.show()","bda250e4":"def smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val","bd3e1208":"def lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n","870affdc":"# 2017 as ending point for train set\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# 2017 first 3 months as val set\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\n# kontrol\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape\n","9cfe81a6":"# LightGBM parameters\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 20000,\n                #, 10000, 15000]\n              'early_stopping_rounds': 200,\n              'nthread': -1}\n\n\n\n\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)\n\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))\n","1a2d3a0e":"def plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\nplot_lgb_importances(model, num=30)\nplot_lgb_importances(model, num=30, plot=True)\n\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()","d0ac8c10":"train = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {\n              'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              'force_col_wise': True,\n              'num_boost_round': model.best_iteration\n              }\n\n","02b8465c":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration,verbose_eval=100)\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n# [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.327707 seconds.\n# You can set `force_col_wise=true` to remove the overhead.\n\n# Create submission\nsubmission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\nsubmission_df.to_csv('submission_demand2.csv', index=False)\nsubmission_df.head(20)","9934fac5":"#### Rolling Mean Features","df3b2aae":"#### Expanding Window Mean Features","2699c753":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">References<\/h1>","84ff8a6e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Data Preprocessing<\/h3>","ed6da712":"#### Custom Cost Function","ef9d441d":"\nThis section explains how the preprocessing stage of dataset work has been done on the original dataset. This includes various steps performed during the predictions which are as follows:\n\n1. Analyze the data-types, dimensions, and missing values in all data-sets.\n2. Overview of the trends of total sales of all stores and items over time.\n3. Visualize the data distribution of individual features including stores, items, price, and holiday events.\n4. Explore the different individual features that affects sale volumes.\n5. Modify the original training data-set.\n6. Change the data type of the modified train data-set.\n7. Build models to forecast daily grocery sales and test on validation data-set.","5b4fb13b":"#### One-Hot Encoding","bd50e8ac":"* It was merged into the \"USBankHolidays\" dataset to examine the effect of holiday time on shopping.","4607e8cd":"#### Converting sales to log(1+sales)","1228b07d":"* MAE: mean absolute error\n* MAPE: mean absolute percentage error\n* SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)","2ba11671":"* Now we have month feature and we can see details wrt month","628e4fcc":"<h7> Making an accurate demand forecast for production planning is a very important parameter. Customers' future demand trends can be affected by many factors, such as market conditions and seasonality. In addition, the requested product must be produced instantly in accordance with the needs. Planning plays an important role in maintaining production reliably. Production planning is the planning of production policies, production programs and production-related processes in line with the objectives of the enterprises. \n\nMaking an accurate demand forecast is of critical importance and will enable more efficient use of resources. Demand forecasting methods are grouped under two main headings, quantitative and qualitative. Quantitative estimation method is a method of making predictions based on the knowledge of people's own experiences. Qualitative method, on the other hand, is a method of making predictions based on the results obtained by supporting numerical data with mathematical models. The artificial neural network model is among the quantitative forecasting methods. In this framework, it may be appropriate to use methods and algorithms such as machine learning methods, especially support vector machine, nearest n-neighbor, regression and artificial neural networks and bayesian networks. In this article, the demand forecasting problem is solved with neural networks that give the minimum error by using the artificial neural network method. The artificial neural network method aims to predict a demand dependent on certain variables, and to make a forward-looking demand forecast by teaching artificial neural networks with the data of previous examples.<\/h7>\n\n","e790eff8":"#### LightGBM Model","9e3a9d1b":"* You can follow my Github Repo. https:\/\/github.com\/Sam-Power","d9873ab0":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Loading the data<\/h4>","60f97af6":"* In order to prevent overfitting we inject small noise to data# inorder to prevent overfitting we inject small noise to data","2fb44655":"#### Random Noise","6399dbcf":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Feature Engineering<\/h3>","de606c05":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Loading Necessary Libraries<\/h4>","983ca3ed":"#### Lag\/Shifted Features","0f977bf0":"* Sort df to store item and date.","6716d0fd":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Model Building<\/h3>","ee450224":"* [[1] Huber, J. and Stuckenschmidt, H., 2020. Daily retail demand forecasting using machine learning with emphasis on calendric special days. International Journal of Forecasting, 36(4), pp.1420-1438.](#0)\n* [[2] Vairagade, N., Logofatu, D., Leon, F. and Muharemi, F., 2019, September. Demand forecasting using random forest and artificial neural network for supply chain management. In International Conference on Computational Collective Intelligence (pp. 328-339). Springer, Cham.](#0)\n* [[3] Carbonneau, R., Vahidov, R. and Laframboise, K., 2007. Machine learning-Based Demand forecasting in supply chains. International Journal of Intelligent Information Technologies (IJIIT), 3(4), pp.40-57.](#0)\n* [[4] Mupparaju, K., Soni, A., Gujela, P. and Lanham, M.A., 2008. A Comparative Study of Machine Learning Frameworks for Demand Forecasting. In CONFERENCE PROCEEDINGS BY TRACK (p. 186).](#0)\n","be3529bc":"#### Feature Importance","48889a34":"![cover_1_3vEBqwk-thumbnail-1200x1200.png](attachment:cover_1_3vEBqwk-thumbnail-1200x1200.png)","55b640c4":"#### Time-Based Validation Sets","b6902a34":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">1. Introduction<\/h1>","09f011b8":"## Thank you!...","dbc49435":"* metric mae: l1, absolute loss, mean_absolute_error, regression_l1\n* l2, square loss, mean_squared_error, mse, regression_l2, regression\n* rmse, root square loss, root_mean_squared_error, l2_root\n* mape, MAPE loss, mean_absolute_percentage_error","a44f5106":"#### Trends Features","641d9c59":"#### Final Model"}}