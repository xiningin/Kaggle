{"cell_type":{"845184f0":"code","79a7a987":"code","a85be946":"code","8d854208":"code","de124b9e":"code","ba22cae3":"code","1bd78b54":"code","ec2d79da":"code","b0f6cea3":"code","15f6035e":"code","672f01af":"code","75c46d21":"code","8b090e33":"code","aaf6e84d":"code","00a082e7":"code","4c9162b3":"code","99571422":"code","5b84b2bb":"code","de6b531b":"code","0d969bf6":"code","c3ecb13d":"code","1041ece1":"code","1b4e6f97":"code","8ba6f5a9":"code","73468148":"code","9326c447":"code","4afabcff":"code","fc65a551":"code","7a59bf84":"code","642559e4":"code","6ac03f39":"code","23a332a6":"code","0a87e938":"code","6df252d0":"code","fa315e18":"code","08089697":"code","3d94e175":"code","d34f8a59":"code","1f4ec83d":"code","e01343c9":"code","e3085b76":"code","0377014c":"code","2577a874":"code","62c10ca8":"code","2f6f3c97":"code","3aa34492":"code","db9e4705":"code","06f78de8":"code","84a9ae95":"code","b753c1b5":"code","1154dd45":"code","f4eb6729":"code","6ba68b8a":"code","7ca3ef91":"code","d94724b5":"code","78790f51":"code","7dd14a16":"code","e824cedf":"code","6f8474bb":"code","d602da06":"code","2880d6a0":"code","a501e72b":"code","8c319829":"code","7166b44a":"code","7885d83e":"code","9ca859e5":"code","81b7c253":"code","831ee651":"code","a743ae71":"code","c7aa2049":"code","f16e6851":"code","75b4daf9":"code","96515210":"code","90308dc9":"code","7447ddbf":"code","7cd292e0":"code","da6ead65":"code","ecb5848a":"code","aedf4eb5":"code","65efe241":"code","a3f25821":"code","3e02de5a":"code","1b194b7d":"code","c9913b2b":"code","85e78e48":"code","d50727b8":"code","b2a83684":"code","df032739":"code","d0684106":"code","7decc929":"code","16a3126b":"code","c69c5f89":"code","89d3ad36":"code","e0e869b3":"code","e13f19d9":"code","fa4a4bf7":"code","e57901ee":"code","5072b7fc":"code","bf591caf":"markdown","32e048f3":"markdown","fc2bed41":"markdown","038c1d9d":"markdown","c0c6c2ed":"markdown","4567da69":"markdown","6894f67e":"markdown","92f07f3b":"markdown","7596ff91":"markdown","8dec1322":"markdown","dd817ec4":"markdown","11c7909e":"markdown","aa6094f2":"markdown","d240bb79":"markdown","f6c40661":"markdown","c26db565":"markdown","d5149075":"markdown","2b632e67":"markdown","fabf69ea":"markdown","214ee123":"markdown","09539a58":"markdown","c24cb11e":"markdown","28007e29":"markdown","394f495e":"markdown","cacca22c":"markdown","6b14716c":"markdown","3926ac8b":"markdown","585042d7":"markdown","018c2a6e":"markdown","027fcb80":"markdown","7b2a7d26":"markdown","90255fcb":"markdown","7dc24c4d":"markdown","25023eb9":"markdown","59d94ce2":"markdown","2fa97f37":"markdown","71103c51":"markdown","7da39cad":"markdown","762309a7":"markdown","940b5b94":"markdown","33550d55":"markdown","ed9f9c16":"markdown","95918e3d":"markdown"},"source":{"845184f0":"# Check Data Directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","79a7a987":"import numpy as np                       # For Linear Algebra\nimport pandas as pd                      # For data manipulation\nimport seaborn as sns                    # For data visualization\nimport matplotlib.pyplot as plt          # For data visualization      \n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")        #To ignore any warnings","a85be946":"train = pd.read_csv(\"..\/input\/loan-prediction-practice-av-competition\/train_csv.csv\") #Read train file\ntest  = pd.read_csv(\"..\/input\/loan-prediction-practice-av-competition\/test.csv.csv\") # Read test file\nsubmission=pd.read_csv(\"..\/input\/loan-prediction-practice-av-competition\/sample_submission.csv\") #Read Submission file\n\n#Lets Make Copy of file for not losing original data\n\ntrain_df = train.copy()\ntest_df  = test.copy()","8d854208":"train_df.head()           #Display top five rows in the train_df dataset","de124b9e":"test_df.head()         #Display top five rows in the test_df dataset.","ba22cae3":"train_df.columns # Display coloumn names in train_df.","1bd78b54":"test_df.columns # Display coloumn names in test_df.","ec2d79da":"print (train_df.shape) # Check the shape of train_df dataset\nprint (test_df.shape)  # Check the shape of test_df dataset","b0f6cea3":"train_df.info()  # Print data type of each variable in train_df dataset","15f6035e":"train_df.describe().T      #To Display the Basic Description of train_df.","672f01af":"train_df.describe(include=['object'])  # To see the statistics of non-numerical features","75c46d21":"test_df.describe().T        #To Display the Basic Description of test_df.","8b090e33":"train_df.nunique()  #To dislay the uniqe categories in train_df","aaf6e84d":"test_df.nunique() #To dislay the uniqe categories in test_df","00a082e7":"train_df.isnull().sum()","4c9162b3":"# Fill missing values in the categorical variable using mode function.\n\ntrain_df['Gender'].fillna(train_df['Gender'].mode()[0], inplace=True) \n\ntrain_df['Married'].fillna(train_df['Married'].mode()[0], inplace=True) \n\ntrain_df['Dependents'].fillna(train_df['Dependents'].mode()[0], inplace=True) \n\ntrain_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0], inplace=True) \n\ntrain_df['Credit_History'].fillna(train_df['Credit_History'].mode()[0], inplace=True)\n","99571422":"train_df['Loan_Amount_Term'].value_counts()","5b84b2bb":"# Use mode function to fill the missing value in Loan_Amount_Term.\n\ntrain_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].mode()[0], inplace=True)","de6b531b":"# Print mean and median value of LoanAmount.\n\nprint(train_df['LoanAmount'].mean())\nprint(train_df['LoanAmount'].median())","0d969bf6":"# The median value is smaller as compared to mean value in LoanAmount variable.\n\ntrain_df['LoanAmount'].fillna(train_df['LoanAmount'].median(), inplace=True)","c3ecb13d":"train_df.isnull().sum()","1041ece1":"test_df.isnull().sum()","1b4e6f97":"# Fill missing values in the test_df dataset.\n\ntest_df['Gender'].fillna(test_df['Gender'].mode()[0], inplace=True) \n\ntest_df['Dependents'].fillna(test_df['Dependents'].mode()[0], inplace=True) \n\ntest_df['Self_Employed'].fillna(test_df['Self_Employed'].mode()[0], inplace=True) \n\ntest_df['Credit_History'].fillna(test_df['Credit_History'].mode()[0], inplace=True)\n\ntest_df['Loan_Amount_Term'].fillna(test_df['Loan_Amount_Term'].mode()[0], inplace=True)\n\ntest_df['LoanAmount'].fillna(test_df['LoanAmount'].median(), inplace=True)","8ba6f5a9":"test_df.isnull().sum()","73468148":"# Visualize Target Variable ['Loan_Status']\n\ntrain_df['Loan_Status'].value_counts()","9326c447":"train_df['Loan_Status'].value_counts(normalize=True).plot.bar()","4afabcff":"# Visualize Independent Variable (Categorical)\n\nplt.figure(figsize = (20,10))\n\nplt.subplot(221)\ntrain_df['Gender'].value_counts(normalize=True).plot.bar(figsize= (20,10), title = 'Gender')\n\nplt.subplot(222)\ntrain_df['Married'].value_counts(normalize=True).plot.bar(title= 'Married') \n\nplt.subplot(223)\ntrain_df['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed')\n\nplt.subplot(224)\ntrain_df['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History')\n\nplt.show()","fc65a551":"# Visualize Independent Variable (Ordinal)\n\nplt.figure(figsize = (20,10))\n\nplt.subplot(221)\ntrain_df['Dependents'].value_counts(normalize=True).plot.bar(figsize= (20,10), title = 'No of Dependents')\n\nplt.subplot(222)\ntrain_df['Education'].value_counts(normalize=True).plot.bar(figsize= (20,10), title = 'Type of Education')\n\nplt.subplot(223)\ntrain_df['Property_Area'].value_counts(normalize=True).plot.bar(figsize= (20,10), title = 'Property_Area')\n\nplt.show()","7a59bf84":"# Visualization of ApplicantIncome Variable (Numerical)\n\nplt.figure(figsize = (20,10))\n\nplt.subplot(121) \nsns.distplot(train_df['ApplicantIncome'],color=\"m\", ) \n\nplt.subplot(122) \ntrain_df['ApplicantIncome'].plot.box(figsize=(16,5)) \n\nplt.show()","642559e4":"print('Mean value of ApplicantIncome is  :',train_df['ApplicantIncome'].mean())\nprint('Median value of ApplicantIncome is  :',train_df['ApplicantIncome'].median())","6ac03f39":"# Visualization of CoapplicantIncome Variable (Numerical)\n\nplt.figure(figsize = (20,10))\n\nplt.subplot(121) \nsns.distplot(train_df['CoapplicantIncome'],color=\"r\", ) \n\nplt.subplot(122) \ntrain_df['CoapplicantIncome'].plot.box(figsize=(16,5)) \n\nplt.show()","23a332a6":"# Visualization of LoanAmount Variable (Numerical)\n\nplt.figure(figsize = (20,10))\n\nplt.subplot(121) \nsns.distplot(train_df['LoanAmount'],color=\"b\", ) \n\nplt.subplot(122) \ntrain_df['LoanAmount'].plot.box(figsize=(16,5)) \n\nplt.show()","0a87e938":"train_df['LoanAmount_log'] = np.log(train_df['LoanAmount']) \n\ntrain_df['LoanAmount_log'].hist(bins=20) \n\n","6df252d0":"test_df['LoanAmount_log'] = np.log(test_df['LoanAmount']) \n\ntest_df['LoanAmount_log'].hist(bins=20) ","fa315e18":"train_df['Loan_Amount_Term'].value_counts()","08089697":"# Visualization of Gender Variable vs Loan_Status\n\nGender=pd.crosstab(train_df['Gender'],train_df['Loan_Status']) \n\nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(6,5))\n\nplt.show()\n","3d94e175":"# Visualization of Married Variable vs Loan_Status\n\nMarried=pd.crosstab(train_df['Married'],train_df['Loan_Status']) \n\nMarried.div(Married.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(6,4)) \n\nplt.show()\n","d34f8a59":"# Visualization of Dependents Variable vs Loan_Status\n\nDependents=pd.crosstab(train_df['Dependents'],train_df['Loan_Status']) \n\nDependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,figsize=(6,4))\n\nplt.show()","1f4ec83d":"# Visualization of Self_Employed Variable vs Loan_Status\n\nSelf_Employed=pd.crosstab(train_df['Self_Employed'],train_df['Loan_Status']) \n\nSelf_Employed.div(Self_Employed.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\n\nplt.show()","e01343c9":"# Visualization of Education Variable vs Loan_Status\n\nEducation =pd.crosstab(train_df['Education'],train_df['Loan_Status']) \n\nEducation.div(Education.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(6,4))\n\nplt.show()","e3085b76":"# Visualization of Credit_History vs Loan_Status\n\nCredit_History=pd.crosstab(train_df['Credit_History'],train_df['Loan_Status'])\n\nCredit_History.div(Credit_History.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(6,4)) \n\nplt.show()","0377014c":"# Visualization of Property_Area vs Loan_Status\n\nProperty_Area=pd.crosstab(train_df['Property_Area'],train_df['Loan_Status'])\n\nProperty_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \n\nplt.show()","2577a874":"# Visualization of ApplicantIncome vs Loan_Status\n\nbins=[0,2500,4000,6000,81000] \n\ngroup=['Low','Average','High', 'Very high'] \n\ntrain_df['Income_bin']=pd.cut(train_df['ApplicantIncome'],bins,labels=group)\n\nIncome_bin=pd.crosstab(train_df['Income_bin'],train_df['Loan_Status']) \n\nIncome_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n\nplt.xlabel('ApplicantIncome') \n\nP = plt.ylabel('Percentage')","62c10ca8":"# Visualization of CoapplicantIncome vs Loan_Status\n\nbins=[0,1000,3000,42000] \n\ngroup=['Low','Average','High'] \n\ntrain_df['Coapplicant_Income_bin']=pd.cut(train_df['CoapplicantIncome'],bins,labels=group)\n\nCoapplicant_Income_bin=pd.crosstab(train_df['Coapplicant_Income_bin'],train_df['Loan_Status']) \n\nCoapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \n\nplt.xlabel('CoapplicantIncome') \n\nP = plt.ylabel('Percentage')","2f6f3c97":"# Visualization of Total_Income vs Loan_Status\n\ntrain_df['Total_Income']=train_df['ApplicantIncome']+train_df['CoapplicantIncome']\n\nbins=[0,2500,4000,6000,81000] \n\ngroup=['Low','Average','High', 'Very high'] \n\ntrain_df['Total_Income_bin']=pd.cut(train_df['Total_Income'],bins,labels=group)\n\nTotal_Income_bin=pd.crosstab(train_df['Total_Income_bin'],train_df['Loan_Status']) \n\nTotal_Income_bin.div(Total_Income_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \n\nplt.xlabel('Total_Income') \n\nP = plt.ylabel('Percentage')","3aa34492":"# Visualization of LoanAmount vs Loan_Status\n\nbins=[0,100,200,700] \n\ngroup=['Low','Average','High'] \n\ntrain_df['LoanAmount_bin']=pd.cut(train_df['LoanAmount'],bins,labels=group)\n\nLoanAmount_bin=pd.crosstab(train_df['LoanAmount_bin'],train_df['Loan_Status']) \n\nLoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True) \n\nplt.xlabel('LoanAmount') \n\nP = plt.ylabel('Percentage')","db9e4705":"#Lets drop the variables (Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income') in the train_df\n\ntrain_df=train_df.drop([ 'Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income'], axis=1)\n","06f78de8":"\ntrain_df['Dependents'].replace('3+', 3,inplace=True)  #Replace Dependents (3+ as 3)\n \ntrain_df['Loan_Status'].replace('N', 0,inplace=True)  #Replace Loan Status (N as 0)\n\ntrain_df['Loan_Status'].replace('Y', 1,inplace=True)  ##Replace Loan Status (Y as 1)\n\ntrain_df.head()","84a9ae95":"# Lets visualize the correlation of variables using heatmap\n\nmatrix = train_df.corr()\n\nax = plt.subplots(figsize=(9, 6)) \n\nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"BuPu\");","b753c1b5":"train_df.head()","1154dd45":"train_lgr=train_df.drop(['Loan_ID','Income_bin'],axis=1) \n\nX = train_lgr.drop('Loan_Status',1)\ny = train_lgr.Loan_Status\n\nX = pd.get_dummies(X)\nX.head()","f4eb6729":"X.head()","6ba68b8a":"print(X.shape)\nprint(y.shape)","7ca3ef91":"# Use  train_test_split function from sklearn to divide our train dataset.\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size =0.2,random_state=1)","d94724b5":"print(x_train.shape)\nprint(y_train.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)","78790f51":"#Import LogisticRegression and accuracy_score from sklearn and fit the logistic regression model.\n\nfrom sklearn.linear_model import LogisticRegression \n\nfrom sklearn.metrics import accuracy_score\n\nmodel_log = LogisticRegression(random_state=1)\n\nmodel_log.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_lgr = model_log.predict(x_test)\n\nacc_log = accuracy_score(y_test,pred_lgr)*100 \n\nacc_log","7dd14a16":"#Predict the test file using Log Regression Model\n\ntest_lr=test_df.drop('Loan_ID',axis=1)\n\ntest_lgr=pd.get_dummies(test_lr)\n\ntest_lgr.shape\n\npred_test_lr = model_log.predict(test_lgr)\n","e824cedf":"#Final Submission\n\nsubmission['Loan_Status'] = pred_test_lr \n\nsubmission['Loan_ID']     = test_df['Loan_ID']\n\nsubmission['Loan_Status'].replace(0, 'N',inplace=True)\n\nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)\n\npd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('logistic.csv')\n\nsubmission.head()","6f8474bb":"from sklearn.svm import SVC\n\nmodel_svc=  SVC(gamma='auto')\n\nmodel_svc.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_svc = model_svc.predict(x_test)\n\nacc_svc = accuracy_score(y_test,pred_svc)*100\n\nacc_svc","d602da06":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel_knn= KNeighborsClassifier(n_neighbors = 3)\n\nmodel_knn.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_knn = model_knn.predict(x_test)\n\nacc_knn = accuracy_score(y_test,pred_knn)*100\n\nacc_knn","2880d6a0":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rfc= RandomForestClassifier(n_estimators=100,random_state = 1)\n\nmodel_rfc.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_rfc = model_rfc.predict(x_test)\n\nacc_rfc = accuracy_score(y_test,pred_rfc)*100\n\nacc_rfc","a501e72b":"from sklearn.naive_bayes import GaussianNB\n\nmodel_gnb= GaussianNB()\n\nmodel_gnb.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_gnb = model_gnb.predict(x_test)\n\nacc_gnb = accuracy_score(y_test,pred_gnb)*100\n\nacc_gnb","8c319829":"from sklearn.linear_model import Perceptron\n\nmodel_ptn= Perceptron()\n\nmodel_ptn.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_ptn = model_ptn.predict(x_test)\n\nacc_ptn = accuracy_score(y_test,pred_ptn)*100\n\nacc_ptn","7166b44a":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_dtc= DecisionTreeClassifier(random_state=1)\n\nmodel_dtc.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_dtc = model_dtc.predict(x_test)\n\nacc_dtc = accuracy_score(y_test,pred_dtc)*100\n\nacc_dtc","7885d83e":"import lightgbm as lgb\n\nmodel_lgb=lgb.LGBMClassifier()\n\nmodel_lgb.fit(x_train, y_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_lgb = model_lgb.predict(x_test)\n\nacc_lgb = accuracy_score(y_test,pred_lgb)*100\n\nacc_lgb","9ca859e5":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_rfc, acc_gnb, acc_ptn, \n             acc_dtc,acc_lgb]})\nmodels.sort_values(by='Score', ascending=False)","81b7c253":"#Adding total income by combining applicant's income and coapplicant's income\n\ntrain_df['Total_Income']=train_df['ApplicantIncome']+train_df['CoapplicantIncome'] \n\ntest_df['Total_Income']=test_df['ApplicantIncome']+test_df['CoapplicantIncome']","831ee651":"# Drop  Loan_Id,Income_bin and LoanAmount_log variables in train_fe \n\ntrain_fe = train_df.drop(['Loan_ID','Income_bin','LoanAmount_log'],axis=1)\n\n# Drop  Loan_Id and LoanAmount_log variables in test_fe \n\ntest_fe  = test_df.drop(['Loan_ID','LoanAmount_log'],axis=1)","a743ae71":"train_fe.head()","c7aa2049":"#Convert Dependent Column into numeric feature\n\ntrain_fe = train_fe.replace({'Dependents': r'3+'}, {'Dependents': 3}, regex=True)\n\ntest_fe = test_fe.replace({'Dependents': r'3+'}, {'Dependents': 3}, regex=True)","f16e6851":"# process column, apply LabelEncoder to categorical features\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlbl = LabelEncoder()\n\nlbl.fit(list(train_fe[\"Dependents\"].values))\n\ntrain_fe[\"Dependents\"] = lbl.transform(list(train_fe[\"Dependents\"].values))\n\nlbl.fit(list(test_fe[\"Dependents\"].values))\n\ntest_fe[\"Dependents\"] = lbl.transform(list(test_fe[\"Dependents\"].values))\n\n# shape \n\nprint('Shape all_data: {}'.format(train_fe.shape))\n\nprint('Shape all_data: {}'.format(test_fe.shape))","75b4daf9":"train_fe = pd.get_dummies(train_fe)\n\ntest_fe = pd.get_dummies(test_fe)\n\ntrain_fe.head()","96515210":"test_fe.head()","90308dc9":"train_fe.shape,test_fe.shape","7447ddbf":"from sklearn.feature_selection import SelectKBest,f_classif\n\nX_fe  = train_fe.drop(['Loan_Status'],axis=1)\n\ny_fe  = train_fe.Loan_Status\n\nselector = SelectKBest(f_classif, k=6)\n\nX_new = selector.fit_transform(X_fe, y_fe)\n\nprint(X_new)\n\nX_new.shape\n","7cd292e0":"# Get back the features we've kept, zero out all other features\n\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=X_fe.index, \n                                 columns=X_fe.columns)\nselected_features.head()","da6ead65":"# Dropped columns have values of all 0s, so var is 0, drop them\n\nselected_columns = selected_features.columns[selected_features.var() != 0]\n\n# Get the dataset with the selected features.\n\nX_fe[selected_columns].head()","ecb5848a":"from sklearn.model_selection import train_test_split\n\nxf_train, xf_test, yf_train, yf_test = train_test_split(X_fe[selected_columns],y_fe, test_size =0.2,random_state=1)\n\nxf_train.shape,xf_test.shape,yf_train.shape,yf_test.shape","aedf4eb5":"#Import LogisticRegression and accuracy_score from sklearn and fit the logistic regression model.\n\nfrom sklearn.linear_model import LogisticRegression \n\nfrom sklearn.metrics import accuracy_score\n\nmodel_log1 = LogisticRegression(random_state=1)\n\nmodel_log1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\nyf_pred = model_log1.predict(xf_test)\n\nacc_log1 = accuracy_score(yf_test,yf_pred )*100 \n\nacc_log1","65efe241":"#Support Vector Classifier\n\nfrom sklearn.svm import SVC\n\nmodel_svc1=  SVC(gamma='auto')\n\nmodel_svc1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_svc1 = model_svc1.predict(xf_test)\n\nacc_svc1 = accuracy_score(yf_test,pred_svc1)*100\n\nacc_svc1","a3f25821":"#KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel_knn1= KNeighborsClassifier(n_neighbors = 3)\n\nmodel_knn1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_knn1 = model_knn1.predict(xf_test)\n\nacc_knn1 = accuracy_score(yf_test,pred_knn1)*100\n\nacc_knn1","3e02de5a":"#Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rfc1= RandomForestClassifier(n_estimators=100,random_state = 1)\n\nmodel_rfc1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_rfc1 = model_rfc1.predict(xf_test)\n\nacc_rfc1 = accuracy_score(yf_test,pred_rfc1)*100\n\nacc_rfc1","1b194b7d":"#Gaussian NB\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel_gnb1= GaussianNB()\n\nmodel_gnb1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_gnb1 = model_gnb1.predict(xf_test)\n\nacc_gnb1 = accuracy_score(yf_test,pred_gnb1)*100\n\nacc_gnb1","c9913b2b":"#Perceptron\n\nfrom sklearn.linear_model import Perceptron\n\nmodel_ptn1= Perceptron()\n\nmodel_ptn1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_ptn1 = model_ptn1.predict(xf_test)\n\nacc_ptn1 = accuracy_score(yf_test,pred_ptn1)*100\n\nacc_ptn1","85e78e48":"#Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel_dtc1= DecisionTreeClassifier(random_state=1)\n\nmodel_dtc1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_dtc1 = model_dtc1.predict(xf_test)\n\nacc_dtc1 = accuracy_score(yf_test,pred_dtc1)*100\n\nacc_dtc1","d50727b8":"#Lightgbm Classifier\n\nimport lightgbm as lgb\n\nmodel_lgb1=lgb.LGBMClassifier()\n\nmodel_lgb1.fit(xf_train, yf_train)\n\n# Use Prdict method to predict the loan status and calculate accuracy score in the validation set.\n\npred_lgb1 = model_lgb1.predict(xf_test)\n\nacc_lgb1 = accuracy_score(yf_test,pred_lgb1)*100\n\nacc_lgb1","b2a83684":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Score': [acc_svc1, acc_knn1, acc_log1, \n              acc_rfc1, acc_gnb1, acc_ptn1, \n             acc_dtc1,acc_lgb1]})\nmodels.sort_values(by='Score', ascending=False)","df032739":"test_ptn = test_fe[selected_columns] \n\ntest_ptn.head()","d0684106":"#Final Submission\n\npred_ptn = model_ptn1.predict(test_ptn)\n\nsubmission['Loan_Status'] = pred_ptn\n\nsubmission['Loan_ID']     = test_df['Loan_ID']\n\nsubmission['Loan_Status'].replace(0, 'N',inplace=True)\n\nsubmission['Loan_Status'].replace(1, 'Y',inplace=True)\n\npd.DataFrame(submission, columns=['Loan_ID','Loan_Status']).to_csv('perceptron.csv')\n\nsubmission.head()","7decc929":"#Cross Validation using Logistic Regression\n\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.model_selection import cross_val_score\n\n\nskfold = StratifiedKFold (n_splits=10,shuffle=True, random_state= 1)\n\nLog_skf = LogisticRegression()\n\nLog_skf1 = cross_val_score(Log_skf, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(Log_skf1)\n\nacc_log2 = Log_skf1.mean()*100.0\n\nacc_log2 ","16a3126b":"#Cross Validation using SVC\n\nskfold = StratifiedKFold (n_splits=5,shuffle=True, random_state= 1)\n\nsvc_sk = SVC(gamma='auto')\n\nsvc_sk1 = cross_val_score(svc_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(svc_sk1)\n\nacc_svc2 = svc_sk1.mean()*100.0\n\nacc_svc2 ","c69c5f89":"#Cross Validation using KNN\n\nskfold = StratifiedKFold (n_splits=10,shuffle=True, random_state= 1)\n\nknn_sk = KNeighborsClassifier(n_neighbors = 3)\n\nknn_sk1 = cross_val_score(knn_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(knn_sk1)\n\nacc_knn2 = knn_sk1.mean()*100.0\n\nacc_knn2\n","89d3ad36":"#Cross Validation using RandomForest Classifier\n\nskfold = StratifiedKFold (n_splits=10,shuffle=False, random_state= None)\n\nrfc_sk = RandomForestClassifier(n_estimators=100,random_state = 1)\n\nrfc_sk1 = cross_val_score(rfc_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(rfc_sk1)\n\nacc_rfc2 = rfc_sk1.mean()*100.0\n\nacc_rfc2\n\n","e0e869b3":"#Cross Validation using GaussianNB\n\nskfold = StratifiedKFold (n_splits=5,shuffle=False, random_state= None)\n\ngnb_sk = GaussianNB()\n\ngnb_sk1 = cross_val_score(gnb_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(gnb_sk1)\n\nacc_gnb2 = gnb_sk1.mean()*100.0\n\nacc_gnb2\n","e13f19d9":"#Cross Validation using Perceptron\n\nskfold = StratifiedKFold (n_splits=5,shuffle=True, random_state= None)\n\nptn_sk = Perceptron()\n\nptn_sk1 = cross_val_score(ptn_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(ptn_sk1)\n\nacc_ptn2 = ptn_sk1.mean()*100.0\n\nacc_ptn2\n","fa4a4bf7":"#Cross Validation using DecisionTree Classifier\n\nskfold = StratifiedKFold (n_splits=5,shuffle=True, random_state= None)\n\ndt_sk = DecisionTreeClassifier(random_state=1)\n\ndt_sk1 = cross_val_score(dt_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(dt_sk1)\n\nacc_dt2 = dt_sk1.mean()*100.0\n\nacc_dt2","e57901ee":"#Cross Validation using LGBMClassifier\n\nskfold = StratifiedKFold (n_splits=10,shuffle=True, random_state= None)\n\nlgb_sk = lgb.LGBMClassifier()\n\nlgb_sk1 = cross_val_score(lgb_sk, X_fe[selected_columns], y_fe, cv=skfold)\n\nprint(lgb_sk1)\n\nacc_lgb2 = lgb_sk1.mean()*100.0\n\nacc_lgb2","5072b7fc":"#Model Validation\n\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Mean Accuracy': [acc_svc2, acc_knn2, acc_log2, \n              acc_rfc2, acc_gnb2, acc_ptn2, \n             acc_dt2,acc_lgb2]})\nmodels.sort_values(by='Mean Accuracy', ascending=False)","bf591caf":"* It seems people with credit history as 1 are more likely to get their loans approved.","32e048f3":"* More number of applicants choose the loan term 360 months.","fc2bed41":"* It can be inferred that Applicant income does not affect the chances of loan approval ","038c1d9d":"# 9.Feature Engineering\n\n* Feature Engineering is used for identify the right features for the target variable inorder to improve the prediction accuracy.","c0c6c2ed":"Inference:\n* The distribution of applicant income is Right skewed and the mean value(5403.459283) is more than the median value (3812.500000).\n* The box plot explains most of the applicant income range is below 30,000 and it shows extreme outliers presence in the data.\n","4567da69":"Summary:\n\n* Random Forest,LGBMClassifier and Naive Bayes are the top 3 models in the mean accuracy score.\n","6894f67e":"# 10.Cross Validation\n\n* Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it.\n\n* Some of the common methods for validation are listed below:\n\n 1. The validation set approach\n 1. k-fold cross validation\n 1. Leave one out cross validation (LOOCV)\n 1. Stratified k-fold cross validation\n \n* In this section I use Stratified k-fold validation for testing our dataset.\n****","92f07f3b":"* Now let\u2019s try to find a way to fill the missing values in Loan_Amount_Term. We will look at the value count of the Loan amount term variable","7596ff91":"* The distribution of Coapplicant Income  follws the same kind of the distribution of Applicants Income.\n* Most of the  Coapplicant Income ranges from o to 5000.\n* Box plot shows ouliers occurs in the Coapplicant Income.\n* The distribution of Coapplicant Income variable is not normally distributed.","8dec1322":"Overall  614 rows and 13 columns in train_df dataset and  367 rows and 12 columns in test_df dataset.\n\nThe goal is to predict the loan status.So the target variable is ( Loan_Status) and the predictor variables are predictor variables (Gender,Married,Dependents,Education,Self_Employed,ApplicantIncome,CoapplicantIncome, LoanAmount,Loan_Amount_Term, Credit_History, Property_Area)\n\n1. train_df dataset contains 1 target variable (Loan_Status) and 12 predictor variables.\n2. test_df dataset  cotains only 12 predictor variables  not the target variable.\n\nI am going to use train_df dataset to train the model and apply the model in test_df dataset to predict the target variable.","dd817ec4":"* Distribution of applicants with 1 or 3+ are similar loan status.\n* Applicants with 2 dependents are higher loan approval.","11c7909e":"# 5.Understanding Data ","aa6094f2":"* There are no missing values in the test_df dataset.","d240bb79":"From the above Categorical variable we can infer that.\n\n* 80% of applicants in the train_df dataset are male.\n* 65% of applicants in the train_df dataset are married.\n* Around 85% applicants in the train_df dataset are not self_employed.\n* Around 85% applicants in the train_df dataset are positive credit score.","f6c40661":"* Lets drop the Loan_ID variable as it do not have any effect on the loan status. We will do the same changes to the test dataset which we did for the training dataset.","c26db565":"* Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided\n* Let's build the log transformation for test_df.","d5149075":"* Now there are no missing value in the train_df dataset.\n* Follow the same procedure for test_df dataset.","2b632e67":"# 2.Identification of Problem Type\n\n* It is a classification problem where we have to predict whether a loan would be approved or not. \n* In a classification problem, we have to predict discrete values based on a given set of independent variable(s).","fabf69ea":"* The distribution of Coapplicantincome shows that higher chances of loan approval are coapplicants having low income range.But this does not look right.\n\n* The possible reason behind this may be that most of the applicants don\u2019t have any coapplicant so the coapplicant income for such applicants is 0 and hence the loan approval is not dependent on it. So we can make a new variable in which we will combine the applicant\u2019s and coapplicant\u2019s income to visualize the combined effect of income on loan approval.\n\n* Let us combine the Applicant Income and Coapplicant Income and see the combined effect of Total Income on the Loan_Status.","214ee123":"* Proportion of married applicants are higher for approved loans.","09539a58":"* There is nothing we can infer from self employed vs loan status.","c24cb11e":"* Logistic Regression model shows top accuracy score  80.48.So I used the same model to predict the test_df dataset and run the submission file.","28007e29":"* We see that the most correlated variables are (ApplicantIncome - LoanAmount),(ApplicantIncome - LoanAmount_log) (Credit_History - Loan_Status) and LoanAmount is also correlated with CoapplicantIncome.","394f495e":"* We can see that Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High and Very High Income.","cacca22c":"* The dataset has been divided into training(x_train ,y_train) and validation part(x_mv ,y_mv).","6b14716c":" We can inferred from the above bar chart\n \n* Most of the applicants in the train_df dataset don't have any dependents.\n* Around 75% of applicants in the train_df dataset are Graduate.\n* Most of the applicants are from semiurban.\n","3926ac8b":" * The model shows 80% accuracy of prediction.We have identified 80% of the loan status correctly.","585042d7":"* It can be seen that the proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount. ","018c2a6e":"* It seems like there is a higher approval for Graduate applicants.","027fcb80":"# 7.Data Visualization\n\n* Analyse each categorical variable using frequency table.\n* Use bar graph to visualize the number of categorical features.\n* Use probability density chart  to understand the ditribution of numerical features.","7b2a7d26":"* Proportion of loans getting approved in semiurban area is higher as compared to that in rural or urban areas.","90255fcb":"\n* It can be seen that in loan amount term variable, the value of 360 is repeating the most. So we will replace the missing values in this variable using the mode of this variable.\n\n* Now we will see the LoanAmount variable. As it is a numerical variable, we can use mean or median to impute the missing values. We will use median to fill the null values as earlier we saw that loan amount have outliers so the mean will not be the proper approach as it is highly affected by the presence of outliers.\n","7dc24c4d":"* Most of the applicants are male eventhough the proportion of Loan approved and unapproved are same for both male and female. ","25023eb9":"The loan of 69% of applicant  which is 422 applicants out of 614 was approved.","59d94ce2":"We can see three types of data in both datasets:\n1. object : Refers to categorical variable.\n    Loan_ID,Gender,Married,Dependents,Education,Self_Employed,Property_Area are categorical variables.\n\n2. int64  : ApplicantIncome refers to integer variables.\n\n3. float64: CoapplicantIncome,LoanAmount,Loan_Amount_Term,Credit_History refers to floating values.","2fa97f37":"# 1.Problem Statement\n\n* A company wants to automate the loan eligibility process(Y\/N) on real time basis on the detail provided while filling online application form.\n* These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others.\n ","71103c51":"* There are missing values in Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term and Credit_History features.\n \n* We will treat the missing values in all the features one by one.\n \n* We can consider these methods to fill the missing values:\n \n* For numerical variables: imputation using mean or median\n* For categorical variables: imputation using mode\n* There are very less missing values in Gender, Married, Dependents, Credit_History and Self_Employed features so we can fill them using the mode of the features.","7da39cad":"* We will use scikit-learn (sklearn) for making different models which is an open source library for Python. \n\n* Sklearn requires the target variable in a separate dataset. So, we will drop our target variable from the train dataset and save it in another dataset.","762309a7":"* The Distribution of LoanAmount variable is fairly normal.\n* Box Plot explains there is a strong outliers exist in the Loan amount.\n* Due to these outliers bulk of the data in the loan amount is at the left and the right tail is longer.\n* One way to remove the skewness is by doing the log transformation. As we take the log transformation, it does not affect the smaller values much, but reduces the larger values. So, we get a distribution similar to normal distribution.","940b5b94":"# 3.Import Libraries","33550d55":"# 4.Reading Datasets\n\nWe have three datasets in the data repository\n1. train_csv.csv  \n2. test.csv.csv\n3. sample_submission.csv","ed9f9c16":"# 8.Model Building\n\n*  Let us make our first model to predict the target variable. We will start with Logistic Regression which is used for predicting binary outcome.\n\n* Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables.\n* Logistic regression is an estimation of Logit function. Logit function is simply a log of odds in favor of the event.\n* This function creates a s-shaped curve with the probability estimate, which is very similar to the required step wise function","95918e3d":"# 6. Missing Value Imputation\n\n"}}