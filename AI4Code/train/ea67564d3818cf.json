{"cell_type":{"95669b1d":"code","119642c3":"code","dbf07a4c":"code","90446350":"code","aa0c6f27":"code","a7bc21e4":"code","8cdef2ea":"code","c81d4511":"code","175994a1":"code","d1748711":"code","6a891b18":"code","aa3e18b4":"code","ef3060d7":"code","2f71dc86":"code","0b78e9a0":"code","234a0d60":"code","4da8b77b":"code","3f63c4b4":"code","ce2d7dc9":"code","d5e47c05":"code","a6c810ba":"code","8e11c9a7":"code","436e6321":"code","d6a78284":"code","54eeca84":"code","bbb00fb7":"code","a232fe00":"code","54cc0439":"code","77e7e214":"code","f58e436e":"code","a87cc60f":"code","10fc73a1":"code","369f0de7":"code","c4ec116d":"code","4d8a9df1":"code","304a45e7":"code","26085778":"code","95610439":"code","514edb7a":"code","d8008333":"code","12010e87":"code","3d76b928":"code","688e8e88":"code","2959351a":"code","9204e7f4":"code","788da9ea":"code","5ad10925":"code","fb763069":"code","1dc131e1":"code","77ffa6f1":"code","040a53d4":"code","cd45107d":"code","37bffed2":"code","2efb120d":"code","17da27db":"code","e0493836":"markdown","ca0b81f5":"markdown","ca149a02":"markdown","cdd70e99":"markdown","1647c158":"markdown","fed43f3b":"markdown","93e5cfa1":"markdown","55118023":"markdown","86cf7df1":"markdown","1bddf876":"markdown","898a898d":"markdown","57ffbbc6":"markdown","87194d88":"markdown","c25898b4":"markdown","8df9b989":"markdown","3c76a7a0":"markdown","e39d1fa7":"markdown","c406d807":"markdown","29766847":"markdown","9d2d4798":"markdown","020c2ecf":"markdown","cc0b3a22":"markdown","a98055bc":"markdown","ff6666df":"markdown","8dcd0f05":"markdown","d305fc60":"markdown","e316f288":"markdown","84d5f0a3":"markdown","02da148b":"markdown","a7e2bd90":"markdown","488c470e":"markdown","58439dec":"markdown","0e2f0664":"markdown","f8556cb8":"markdown","edef20c4":"markdown","f1738d5d":"markdown"},"source":{"95669b1d":"print(\"Hello Capstone Project Course!\")","119642c3":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import MarkerCluster\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set()","dbf07a4c":"!wget -O data.csv \"https:\/\/opendata.arcgis.com\/datasets\/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\"","90446350":"data = pd.read_csv(\"data.csv\")\ndata.info()","aa0c6f27":"map = folium.Map(location=[47.60, -122.33], zoom_start=12)\nmarker_cluster = MarkerCluster().add_to(map)\nlocations = data[['Y', 'X']][data['Y'].notna()].head(1000)\nlocationlist = locations.values.tolist()\nfor point in range(len(locations)):\n    folium.Marker(locationlist[point]).add_to(marker_cluster)\nmap","a7bc21e4":"data['WEATHER'].value_counts().to_frame('count')","8cdef2ea":"data['ROADCOND'].value_counts().to_frame('count')","c81d4511":"data['LIGHTCOND'].value_counts().to_frame('count')","175994a1":"data['SPEEDING'].value_counts().to_frame()","d1748711":"data['SEVERITYCODE'].value_counts().to_frame('count')","6a891b18":"data['UNDERINFL'].value_counts().to_frame('count')","aa3e18b4":"data['PERSONCOUNT'].describe()","ef3060d7":"data['VEHCOUNT'].describe()","2f71dc86":"data['PEDCOUNT'].describe()","0b78e9a0":"data['PEDCYLCOUNT'].describe()","234a0d60":"data.isna().sum()","4da8b77b":"data.duplicated().sum()","3f63c4b4":"data_clean = data[['X', 'Y', 'WEATHER', 'ROADCOND', 'LIGHTCOND',\n                   'SPEEDING', 'SEVERITYCODE', 'UNDERINFL',\n                   'SERIOUSINJURIES', 'FATALITIES', 'INJURIES',\n                   'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT']]\ndata_clean.info()","ce2d7dc9":"data_clean['SPEEDING'] = data_clean['SPEEDING'].map({'Y': 1})\ndata_clean['SPEEDING'].replace(np.nan, 0, inplace=True)\ndata_clean['SPEEDING'].value_counts().to_frame()","d5e47c05":"data_clean.replace('Unknown', np.nan, inplace=True)\ndata_clean.replace('Other', np.nan, inplace=True)\ndata_clean['SEVERITYCODE'].replace('0', np.nan, inplace=True)","a6c810ba":"sns.heatmap(data_clean.isnull(), cmap='YlGnBu_r')\nplt.show()","8e11c9a7":"data_clean.dropna(axis=0, inplace=True)","436e6321":"sns.heatmap(data_clean.isnull(), cmap='YlGnBu_r')\nplt.show()","d6a78284":"data_clean['UNDERINFL'] = data_clean['UNDERINFL'].map({'N': 0, '0': 0, 'Y': 1, '1': 1})","54eeca84":"data_clean.info()","bbb00fb7":"ax = sns.countplot(data_clean['WEATHER'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, \n                   horizontalalignment='right')\nplt.show()","a232fe00":"ax = sns.countplot(data_clean['ROADCOND'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, \n                   horizontalalignment='right')\nplt.show()","54cc0439":"ax = sns.countplot(data_clean['LIGHTCOND'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, \n                   horizontalalignment='right')\nplt.show()","77e7e214":"sns.countplot(data_clean['UNDERINFL'])","f58e436e":"ax = plt.scatter(data_clean['VEHCOUNT'], data_clean['PERSONCOUNT'])\nplt.xlabel('VEHCOUNT')\nplt.ylabel('PERSONCOUNT')\nplt.show()","a87cc60f":"ax = plt.scatter(data_clean['VEHCOUNT'], data_clean['INJURIES'])\nplt.xlabel('VEHCOUNT')\nplt.ylabel('INJURIES')\nplt.show()","10fc73a1":"ax = plt.scatter(data_clean['PEDCOUNT'], data_clean['PERSONCOUNT'])\nplt.xlabel('PEDCOUNT')\nplt.ylabel('PERSONCOUNT')\nplt.show()","369f0de7":"sns.heatmap(data_clean.corr(), cmap='YlGnBu_r')\nplt.show()","c4ec116d":"data_clean = pd.concat([data_clean.drop(['WEATHER', 'ROADCOND', 'LIGHTCOND'], axis=1), \n           pd.get_dummies(data_clean['ROADCOND']),\n           pd.get_dummies(data_clean['LIGHTCOND']),\n           pd.get_dummies(data_clean['WEATHER'])], axis=1)","4d8a9df1":"data_clean = data_clean.sample(frac=1).reset_index(drop=True)","304a45e7":"data_clean.head(5).T","26085778":"sns.heatmap(data_clean.corr(), cmap='YlGnBu_r')\nplt.show()","95610439":"from sklearn import preprocessing\nx = data_clean.drop(['SEVERITYCODE'], axis=1)\ny = data_clean[['SEVERITYCODE']]\ndata_clean_scaled = preprocessing.StandardScaler().fit(x).transform(x)\ndata_clean_scaled[0:3]","514edb7a":"from sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data_clean_scaled, y, \n                                                    test_size=0.2, random_state=42)","d8008333":"from sklearn.tree import DecisionTreeClassifier\ndTreeModel = DecisionTreeClassifier(criterion='entropy', max_depth=5)\ndTreeModel.fit(x_train, y_train)\ndTreeModel","12010e87":"yHat = dTreeModel.predict(x_test)","3d76b928":"print(classification_report(y_test, yHat))","688e8e88":"from sklearn.ensemble import RandomForestClassifier\nrfcModel = RandomForestClassifier(n_estimators=75)\nrfcModel.fit(x_train, y_train)","2959351a":"yHat = rfcModel.predict(x_test)","9204e7f4":"print(classification_report(y_test, yHat))","788da9ea":"from sklearn.linear_model import LogisticRegression\nlogRegModel = LogisticRegression(C=0.01)\nlogRegModel.fit(x_train, y_train)\nlogRegModel","5ad10925":"yHat = logRegModel.predict(x_test)","fb763069":"print(classification_report(y_test, yHat))","1dc131e1":"import tensorflow as tf\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(32, input_dim=x_train.shape[1], activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(4, activation='sigmoid')\n])\n\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy']\n)","77ffa6f1":"num_epochs = 10\nhistory = model.fit(x_train, tf.keras.utils.to_categorical(\n    y_train['SEVERITYCODE'].map({\n        '1': 0,\n        '2': 1,\n        '2b': 2,\n        '3': 3\n    }), dtype='float32'\n), epochs=num_epochs, batch_size=50, validation_split = 0.2)","040a53d4":"loss_train = history.history['loss']\nloss_validation = history.history['val_loss']\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, loss_train, 'g', label='Training')\nplt.plot(epochs, loss_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","cd45107d":"acc_train = history.history['accuracy']\nacc_validation = history.history['val_accuracy']\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, acc_train, 'g', label='Training')\nplt.plot(epochs, acc_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()","37bffed2":"yHat = model.predict(x_test)\nyPred = [np.argmax(y) for y in yHat]","2efb120d":"print(classification_report(y_test.SEVERITYCODE.map({\n        '1': 0,\n        '2': 1,\n        '2b': 2,\n        '3': 3\n}), yPred))","17da27db":"plt.bar(['DTC', 'RFC', 'LogReg', 'ANN'], [1.,1.,1.,1.])\nplt.show()","e0493836":"### Downloading and Loading the Data\n\nWe download the dataset to our project directory and take a look at the data types and the dimensionality of the data. We can see that the dataset contains 221,389 records and 40 fields.\n\nThe metadata of the dataset can be found from the website of the [Seattle Department of Transportation](https:\/\/www.seattle.gov\/Documents\/Departments\/SDOT\/GIS\/Collisions_OD.pdf). On reading the dataset summary, we can determine the description of each of the fields and their possible values.","ca0b81f5":"### Modelling and Evaluation","ca149a02":"We now drop the records having null values in order to clean the data.","cdd70e99":"As the dataset has possibly been sourced from a database table, several unique identifiers and spatial features are present in the database which may be irrelevant in further statistical analysis. These fields are `OBJECTID`, `INCKEY`, `COLDETKEY`, `INTKEY`, `SEGLANEKEY`, `CROSSWALKKEY`, and `REPORTNO`. Other fields suchs as `EXCEPTRSNCODE`, `SDOT_COLCODE`, `SDOTCOLNUM` and `LOCATION` and their corresponding descriptions (if any) are categorical but have a large number of distinct values that shall not be that much useful for analysis. The `INCDATE` and `INCDTTM` denote the date and the time of the incident but may not be of use in further analyses. The data needs to be pre-processed.","1647c158":"The `SEVERITYCODE` field contains a code that corresponds to the severity of the\ncollision. and `SEVERITYDESC` contains a detailed description of the severity of the collision.\n\nWe can conclude that there were 349 collisions that resulted in at least one fatality, and 3,102 collisions that resulted in serious injuries. The following table lists the meaning of each of the codes used in the `SEVERITYCODE` field:\n\n| SEVERITYCODE Value | Meaning |\n| :-: | --- |\n| 1 | Accidents resulting in property damage |\n| 2 | Accidents resulting in injuries |\n| 2b | Accidents resulting in serious injuries |\n| 3 | Accidents resulting in fatalities |\n| 0 | Data Unavailable i.e. Blanks |","fed43f3b":"Fixing the `SPEEDING` field by encoding it to `0` for the blanks and `1` for the `Y` values.","93e5cfa1":"## Data\n\nThe dataset is available as comma-separated values (CSV) files, KML files, and ESRI shapefiles that can be downloaded from the Seattle Open GeoData Portal. The data is also available from RESTful API services in formats such as GeoJSON.","55118023":"As the `UNDERINFL` field had data inconsistency, it is cleaned by converting all `N` and `0` values to `0` and all `Y` and `1` values to `1`.","86cf7df1":"The `WEATHER` field contains a description of the weather conditions during\nthe time of the collision. ","1bddf876":"### Results","898a898d":"## Introduction: Business Undertanding\n\nThe Open Data Program makes the data generated by the City of Seattle has been openly available to the public for the purpose of increasing the quality of life for the residents, increasing transparency, accountability and comparability, promoting economic development and research, and improving internal performance management.\n\nThe Traffic Records Group, Traffic Management Division, Seattle Department of Transportation, provides data for all collisions and crashes that have occured in the state from 2004 to the present day. The data is updated weekly and can be found at the [Seattle Open GeoData Portal](https:\/\/data-seattlecitygis.opendata.arcgis.com\/datasets\/5b5c745e0f1f48e7a53acec63a0022ab_0?geometry=-122.326%2C47.592%2C-122.318%2C47.594).\n\nThe objective is to exploit this data to extract vital features that would enable us to end up with a good model that would enable the prediction of the severity of future accidents that take place in the state. This would further enable the Department of Transportation to prioritise their SOPs and channel their energy to ensure that fewer fatalities result in automobile collisions.","57ffbbc6":"Checking for blanks and duplicated records.","87194d88":"Visualizing the dataset after dropping the null values shows that there are no more blanks.","c25898b4":"The `ROADCOND` field describes the condition of the road during the collision. ","8df9b989":"Selecting relevant fields and dropping others.","3c76a7a0":"The `LIGHTCOND` field describes the light conditions during the collision.","e39d1fa7":"Before encoding the values of the categorical fields, we take a look at their distribution.","c406d807":"The `X` and `Y` fields denote the longitude and latitude of the collisions. We can visualize the first few non-null collisions on a map.","29766847":"The data contains several categorical fields and corresponding descriptions which could help us in further analysis. We make an attempt at understanding the data in terms of the fields that we shall take into account for later stages of model building.","9d2d4798":"Shuffling of the dataset is necessary as it is an unbalanced dataset.","020c2ecf":"Neural networks can be used to capture non-linearity between features. We have used a Sequential ANN where there are 2 hidden layers. The `relu` and `sigmoid` activation functions are used. The loss function that is used is `categorical_crossentropy` as the target is integer-coded.","cc0b3a22":"The `PERSONCOUNT` and `VEHCOUNT` indicate how many people and vehicles were involved in a collision respectively.","a98055bc":"Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. Finding the correlation among the features of the dataset helps understand the data better. For example, in the heatmap shown below, it can be observed that some features have a strong positive \/ negative correlation while most of them have weak \/ no correlation.","ff6666df":"### Conclusion\n\nInitially, the classifiers had an prediction accuracy of 66%-71%, however, upon going back to the data preparation phase, minor tweaking and taking additional fields in the dataset improved the overall accuracy of all models.\n\nThe accuracy of the classifiers is excellent, i.e. 100%. This means that the model has trained well and fits the training data and performs well on the testing set as well as the training set. We can conclude that this model can accurately predict the severity of car accidents in Seattle.\n\n### Future Work\n\nThe trained model can be deployed onto governance and monitoring web and mobile applications to predict the accident severity for a given set of parameters.","8dcd0f05":"Records containing values as `Unknown` and `Other` can be considered as null values. Severity Code of 0 corresponds to unknown severity, which can also be treated as null.","d305fc60":"The `UNDERINFL` field describes whether or not a driver involved was under the\ninfluence of drugs or alcohol. The values `0` and `N` denote that the driver was not under any influence while `1` and `Y` that they were.","e316f288":"### Data Pre-processing","84d5f0a3":"We can quickly have an overview of the dataset and look at the frequency of missings records.","02da148b":"The datasets `x` and `y` are constructed. The set `x` contains all the training examples and `y` contains all the labels. Feature scaling of data is done to normalize the data in a dataset to a specific range.\n\nAfter normalization, they are split into `x_train`, `y_train`, `x_test`, and `y_test`. The first two sets sahll be used for training and the last two shall be used for testing. Upon choosing a suitable split ratio, 80% of data is used for training and 20% of is used for testing.","a7e2bd90":"Random Forest Classifier is an ensemble (algorithms which combines more than one algorithms of same or different kind for classifying objects) tree-based learning algorithm. RFC is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object. Used for both classification and regression.\n\nSimilar to DTC, RFT requires an input that specifies a measure that is to be used for classification, along with that a value for the number of estimators (number of decision trees) is required. A hyper parameter RFT was used to determine the best choices for the above mentioned parameters. RFT with 75 DT\u2019s using entropy as the measure gave the best accuracy when trained and tested on pre-processed accident severity dataset.","488c470e":"We now do an one-hot encoding of the `WEATHER`, `ROADCOND`, and `LIGHTCOND` fields as they are categorical.","58439dec":"# IBM Applied Data Science Capstone Project\n\nThis notebook shall be mainly used for completing the capstone project.\n\nFeel free to reach me at [@ScientificGhosh](https:\/\/twitter.com\/ScientificGhosh) on Twitter.","0e2f0664":"Decision Tree makes decision with tree-like model. It splits the sample into two or more homogenous sets (leaves) based on the most significant differentiators in the input variables. To choose a differentiator (predictor), the algorithm considers all features and does a binary split on them (for categorical data, split by category; for continuous, pick a cut-off threshold). It will then choose the one with the least cost (i.e. highest accuracy), and repeats recursively, until it successfully splits the data in all leaves (or reaches the maximum depth).\n\nInformation gain for a decision tree classifier can be calculated either using the Gini Index measure or the Entropy measure, whichever gives a greater gain. A hyper parameter Decision Tree Classifier was used to decide which tree to use, DTC using entropy had greater information gain; hence it was used for this classification problem.","f8556cb8":"The `SPEEDING` field classifies collisions based on whether or not speeding was a factor in the collision. Blanks indicate cases where the vehicle was not speeding.","edef20c4":"The accuracies of all models was 100% which means we can accurately predict the severity of an accident. A bar plot is plotted below with the bars representing the accuracy of each model.","f1738d5d":"Logistic Regression is a classifier that estimates discrete values (binary values like 0\/1, yes\/no, true\/false) based on a given set of an independent variables. It basically predicts the probability of occurrence of an event by fitting data to a logistic function. Hence it is also known as logistic regression. The values obtained would always lie within 0 and 1 since it predicts the probability.\n\nThe chosen dataset has more than two target categories in terms of the accident severity code assigned, one-vs-one (OvO) strategy is employed."}}