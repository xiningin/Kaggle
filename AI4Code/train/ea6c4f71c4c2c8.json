{"cell_type":{"c0b917b5":"code","0d05b839":"code","34f35849":"code","3e806d8e":"code","6cc4b46c":"code","1cc7f0ee":"code","908d74b0":"code","13201524":"code","a02907af":"code","ac997517":"code","34d7164c":"code","faf98a37":"code","ad8375a9":"code","0979173e":"code","903c6304":"code","ff4f44f5":"code","a1277cb3":"code","7f0f0043":"code","80805983":"code","b1061411":"code","7060a5df":"code","29603ee2":"code","7a3260f8":"code","0d208d38":"code","7031e14d":"code","3320af68":"code","f8646cb3":"code","e6363fbd":"code","db6e6a32":"code","7bb9a047":"code","5922eb37":"code","949be49a":"code","606522b8":"code","56273edd":"code","3a5659a9":"code","f74f9434":"code","ee370965":"code","f5e93cd4":"code","07e580d6":"code","22638eea":"markdown","4e7b167e":"markdown","a923257e":"markdown","cb89040c":"markdown","ec8596b7":"markdown","2d1adfaf":"markdown","80e9e2ee":"markdown","f6ff857e":"markdown","1509fed0":"markdown","492f85cc":"markdown","30078758":"markdown","6cf6b670":"markdown","f0ea1d0e":"markdown","fde764ae":"markdown","b226e0df":"markdown","81f34921":"markdown","f1379958":"markdown","14d8abb0":"markdown","3a6b1773":"markdown","91ec1eab":"markdown","1db7a3cc":"markdown","d9e2868f":"markdown","45b69ca1":"markdown","d7be5ad6":"markdown","8a7e44fd":"markdown","948b9300":"markdown","e9a97b9d":"markdown","cfcf61aa":"markdown","5ed712f7":"markdown","55a6be39":"markdown","1001e00a":"markdown","88601e9d":"markdown","a7c31a70":"markdown","fe72d22a":"markdown","54259205":"markdown","7ba5df63":"markdown","4ee5bb44":"markdown","366c2b14":"markdown","2e2eac59":"markdown","b8adfff5":"markdown","a2d3cc2a":"markdown","748f1652":"markdown"},"source":{"c0b917b5":"!pip install talos # hyperparameter tuning\n!pip install --upgrade pip","0d05b839":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # showing and rendering figures\n# io related\nimport seaborn as sns # for Visualizing my data\nfrom keras.utils import to_categorical # Using keras to_categorical because I need to convert by labels \n# into categorical form\nimport os # for taking input to my dataframe\nimport cv2 # for resizing my iamges\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# not needed in Kaggle, but required in Jupyter\n%matplotlib inline \n\n","34f35849":"# Here I am building my dataframe (taking patient id  from image coloum and seperating side of image \n# left or right specefying path of image and at last converting my labels into categorical labels)\ntemp_df=pd.read_csv('..\/input\/diabetic-retinopathy-detection\/trainLabels.csv') # uploading csv to my pandas dataframe\nprint(temp_df.head()) # displaying first 5 objects in dataframe\nimage=temp_df['image'].str.split('_',n=1,expand=True) #splitting Side and Patient ID \ndf = pd.DataFrame()# creating new dataframe object\ndf['eye_side']=image[1] #taking side of Image\ndf['patient_id']=image[0]#taking patient id of an Image\n\ndf['path']='..\/input\/diabetic-retinopathy-detection\/'#Giving paths of the images \ndf['path']=df['path'].str.cat(temp_df['image']+'.jpeg')#adding Image path and format \ndf['exists'] = df['path'].map(os.path.exists)\ndf=df[df['exists']]\ndf['level']=temp_df['level']# taking levels of Image\ndf['level_cat'] = df['level'].map(lambda x: to_categorical(x, 1+df['level'].max()))#converting my \n# labels to categorical_labels\ndf.head()","3e806d8e":"im = plt.imread(df.path.values[2]) # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.show()","6cc4b46c":"sizes = df['level'].values #taking values from series because I only want to visualize levels nt index\nprint(sizes[0:5])#printing first 5 values\n","1cc7f0ee":"sns.distplot(sizes, kde=False); # Visualizing levels in dataset","908d74b0":"pd.value_counts(sizes) # viewing the values of levels","13201524":"import PIL\nfrom PIL import Image\nbaseheight = 128\nimg = Image.open('..\/input\/diabetic-retinopathy-detection\/1192_right.jpeg')\n\nwsize = 128\nimg = img.resize((wsize, baseheight), PIL.Image.ANTIALIAS)\nimg.save('resized_image.jpg')# i need this for storing my previous image also ","a02907af":"im = plt.imread('resized_image.jpg') # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.title(\"Resized Image\")\nplt.show()\n\nim = plt.imread('..\/input\/diabetic-retinopathy-detection\/1192_right.jpeg') # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.title(\"Orignal Image\")\nplt.show()","ac997517":"#total Examples of LEVEL [1,2,3,4] So that we are able to make balanced dataset\nsum_E=0\nfor i in range (1,5):\n    L1_df=pd.DataFrame()# creating new dataframe object\n    L1_df =df [df.level==i]\n    x=len(L1_df)\n    sum_E=x+sum_E\nprint(sum_E)","34d7164c":"B_df=pd.read_csv('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary Dataframe')\nB_df=B_df.drop('Unnamed: 0',axis=1)\nB_df.head(10)","faf98a37":"sizes =B_df['level'].values\nsns.distplot(sizes, kde=False); # Visualizing levels in dataset","ad8375a9":"Binary_90 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_90.npz')\nX_90=Binary_90['a']\nBinary_128 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_128.npz')\nX_128=Binary_128['a']\nBinary_264 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_264.npz')\nX_264=Binary_264['a']\ny=B_df['level'].values\n\n\nprint(X_90.shape)\nprint(X_128.shape)\nprint(X_264.shape)\nprint(y.shape)","0979173e":"# we need to resize our X because we load array in 2 diminsional and we need it in 4 diminsional\nprint(\"Shape before reshaping X_90\" +str(X_90.shape))\nX_90=X_90.reshape(1000,90,90,3)\nprint(\"Shape after reshaping X_90\" +str(X_90.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_128\" +str(X_128.shape))\nX_128=X_128.reshape(1000,128,128,3)\nprint(\"Shape after reshaping X_128\" +str(X_128.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_264\" +str(X_264.shape))\nX_264=X_264.reshape(1000,264,264,3)\nprint(\"Shape after reshaping X_264\" +str(X_264.shape))\n","903c6304":"im = plt.imread(B_df['path'][1]) # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.title(\"Orignal Image\")\nplt.show()","ff4f44f5":"plt.title(\"90*90*3 Image\")\nplt.imshow(X_90[1])\nplt.show()\n\nplt.title(\"128*128*3 Image\")\nplt.imshow(X_128[1])\nplt.show()\n\nplt.title(\"264*264*3 Image\")\nplt.imshow(X_264[1])\nplt.show()","a1277cb3":"y.shape","7f0f0043":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_128,y, test_size=0.10, random_state=42)\ny_train = to_categorical(y_train, num_classes=2)\ny_test_Categorical=to_categorical(y_test)\ny_categorical =to_categorical(y)","80805983":"from keras.models import Sequential,Model\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,Activation\nfrom keras import losses\nfrom keras.optimizers import Adam, Adagrad\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\nfrom sklearn.model_selection import GridSearchCV\nimport keras\n#import talos as ta","b1061411":"\ndef Talos_Model(X_train, y_train, X_test, y_test, params):\n    #parameters defined\n    lr = params['lr']\n    epochs=params['epochs']\n    dropout_rate=params['dropout']\n    optimizer=params['optimizer']\n    loss=params['loss']\n    last_activation=params['last_activation']\n    activation=params['activation']\n    clipnorm=params['clipnorm']\n    decay=params['decay']\n    momentum=params['momentum']\n    l1=params['l1']\n    l2=params['l2']\n    No_of_CONV_and_Maxpool_layers=params['No_of_CONV_and_Maxpool_layers']\n    No_of_Dense_Layers =params['No_of_Dense_Layers']\n    No_of_Units_in_dense_layers=params['No_of_Units_in_dense_layers']\n    Kernal_Size=params['Kernal_Size']\n    Conv2d_filters=params['Conv2d_filters']\n    pool_size_p=params['pool_size']\n    padding_p=params['padding']\n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, Kernal_Size ,padding=padding_p))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=pool_size_p,strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    \n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    out = model.fit(X_train, y_train, epochs=params['epochs'])\n\n    return out,model","7060a5df":"\n\nparams = {'lr': (0.1, 0.01,1 ),\n     'epochs': [10,5,15],\n     'dropout': (0, 0.40, 0.8),\n     'optimizer': [\"Adam\",\"Adagrad\",\"sgd\"],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"mean_absolute_error\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-2),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_CONV_and_Maxpool_layers':[2,3],\n     'No_of_Dense_Layers': [2,3,4],\n     'No_of_Units_in_dense_layers':[128,64,32,256],\n     'Kernal_Size':[(2,2),(4,4),(6,6)],\n     'Conv2d_filters':[60,40,80,120],\n     'pool_size':[(2,2),(4,4)],\n     'padding':[\"valid\",\"same\"]\n    }\n","29603ee2":"import talos as ta\nh = ta.Scan(X_train, y_train, params=params,\n            model=Talos_Model,\n            dataset_name='DR',\n            experiment_no='1',\n            grid_downsample=.01)","7a3260f8":"accessing the results data frame\nh.data.head()\n\naccessing epoch entropy values for each round\nh.peak_epochs_df\n\naccess the summary details\nh.details","0d208d38":"r = ta.Reporting(h)\n\nr.best_params()\n\n","7031e14d":"\ndef Randomized_Model(lr=0.01,dropout=0.5,optimizer=\"adam\",loss='mean_squared_error',\n                    last_activation=\"softmax\",activation=\"relu\",clipnorm=0.1,\n                    decay=1e-2,momentum=0.5,l1=0.01,l2=0.001,No_of_CONV_and_Maxpool_layers=3,\n                    No_of_Dense_Layers=3,No_of_Units_in_dense_layers=24,Conv2d_filters=60):\n       \n    \n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, (2,2) ,padding=\"same\"))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    model.add(Dropout(dropout))\n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","3320af68":"\n\nparams = {'lr': (0.1, 0.01,1,0.001 ),\n     'epochs': [10,5,15,30],\n     'dropout': (0, 0.40, 0.8),\n     'optimizer': [\"Adam\",\"Adagrad\",\"sgd\"],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"mean_absolute_error\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-2),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_CONV_and_Maxpool_layers':[2,3],\n     'No_of_Dense_Layers': [2,3,4,5],\n     'No_of_Units_in_dense_layers':[128,64,32,256],\n     \n     'Conv2d_filters':[60,40,80,120,220]\n     \n     \n    }\n","f8646cb3":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import make_scorer\n# model class to use in the scikit random search CV \nmodel = KerasClassifier(build_fn=Randomized_Model, epochs=10, batch_size=20, verbose=1)\ngrid = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=params, \n                          verbose=20,  n_iter=10, n_jobs=1)\n","e6363fbd":"grid_result = grid.fit(X_train, y_train)","db6e6a32":"best_params=grid_result.best_params_\nbest_params","7bb9a047":"\nfrom sklearn.metrics import accuracy_score\n\ny=grid_result.predict(X_test)\nrandom=accuracy_score(y, y_test)\nprint(\"Base Accuracy \",random)\n\nbest_random = grid_result.best_estimator_\ny1=best_random.predict(X_test)\nBest=accuracy_score(y1, y_test)\nprint(\"Best Accuracy \" ,Best)\n\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (Best - random) \/ random))","5922eb37":"def Best_param_Model(best_params):\n       \n    lr=best_params[\"lr\"]\n    dropout=best_params[\"dropout\"]\n    optimizer=best_params[\"optimizer\"]\n    loss=best_params[\"loss\"]\n    last_activation=best_params[\"last_activation\"]\n    activation=best_params[\"activation\"]\n    clipnorm=best_params[\"clipnorm\"]\n    decay=best_params[\"decay\"]\n    momentum=best_params[\"momentum\"]\n    l1=best_params[\"l1\"]\n    l2=best_params[\"l2\"]\n    No_of_CONV_and_Maxpool_layers=best_params[\"No_of_CONV_and_Maxpool_layers\"]\n    No_of_Dense_Layers=best_params[\"No_of_Dense_Layers\"]\n    No_of_Units_in_dense_layers=best_params[\"No_of_Units_in_dense_layers\"]\n    Conv2d_filters=best_params[\"Conv2d_filters\"]\n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, (2,2) ,padding=\"same\"))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    \n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","949be49a":"\nBinary_model=Best_param_Model(best_params)\nhistory =Binary_model.fit(X_train, y_train, epochs=100, validation_data=(X_test,y_test_Categorical))\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\n\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","606522b8":"Binary_model.evaluate(X_test,y_test_Categorical)","56273edd":"y=B_df['level'].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_128,y, test_size=0.10, random_state=42)\ny_train = to_categorical(y_train, num_classes=2)\ny_test_Categorical=to_categorical(y_test)","3a5659a9":"\nmodel = Sequential()\nmodel.add(Conv2D(16,kernel_size = (5,5),activation = 'relu', activity_regularizer=regularizers.l2(1e-8)))\nmodel.add(Conv2D(32,kernel_size = (5,5),activation = 'relu', activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(MaxPooling2D(3,3))\nmodel.add(Conv2D(64,kernel_size = (5,5),activation = 'relu', activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(MaxPooling2D(3,3))\nmodel.add(Conv2D(128,activation = 'relu',kernel_size = (3,3),activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64,activation = 'tanh',activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16,activation = 'tanh',activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2,activation = 'softmax'))\nmodel.compile(loss=keras.losses.binary_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(X_train,y_train, epochs = 10 ,batch_size = 16,validation_data=(X_test,y_test_Categorical))\nmodel.summary()\n","f74f9434":"from sklearn.metrics import confusion_matrix\nprediction=model.predict(X_test)\ny_pred=[]\nfor i in prediction:\n    y_pred.append(i.argmax())\ny_pred=np.asarray(y_pred)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)\n\n","ee370965":"a=np.expand_dims( X_train[10],axis=0)\na.shape\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(a)\n","f5e93cd4":"def display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index])\n            activation_index += 1\ndisplay_activation(activations, 4, 4,1)","07e580d6":"top_layer = model.layers[0]\nplt.imshow(top_layer.get_weights()[0][:, :, :,15 ])","22638eea":"**Now I have my X and Y. Now its time for spliting and training**","4e7b167e":"**Saving Binary Dataset**","a923257e":"**Randomized Search For Hyperparameter tuning**","cb89040c":"len(B_df)","ec8596b7":"**Best Params of Randomized search**","2d1adfaf":"**Visualizing 15th filter of my first cov2d layer**","80e9e2ee":"**Model is Giving 0% accuracy After all of this If someone know the solution please let me know**\n1. According to me less images will be the problem so that cnn is not getting enough images to train their weights well","f6ff857e":"**Talos Model For HyperParameter Optimization**","1509fed0":"First of All I am going to Import **libraries** that will be usefull for this project","492f85cc":"**CNN MODEL **","30078758":"**Visualization**\n","6cf6b670":"**Talos for hyperparameter tuning**","f0ea1d0e":"**almost 70% of 0 DR and 30% of 1DR **","fde764ae":"**Parameters of Randomized Search**","b226e0df":"Accuracy Measures\n[Helping Material](http:\/\/comprna.upf.edu\/courses\/Master_AGB\/2_ClassificationAlgorithms\/Lecture_Accuracy.pdf)","81f34921":"**Saving Numpy array to files**","f1379958":"**loading My numpy array which I saved**","14d8abb0":"So we will take **261** **examples from eye having** **level 0 and 261 examples from  level[1,2,3,4]**","3a6b1773":"**Visualizing my CNN model**","91ec1eab":"\nB_df=pd.DataFrame()\n#making secondary dataset having small number of sample \n#taking examples where level=0\n\nL0_df=df[df.level==0 ]\n\n\n\n\n\nL1_df=pd.DataFrame()# creating new dataframe object\nL1_df=df[df.level==1 ]\nL1_df=L1_df.drop('level_cat',axis=1)\n\n\n\nL2_df=pd.DataFrame()# creating new dataframe object\nL2_df=df[df.level==2 ]\n\n\n\n\nL3_df=pd.DataFrame()# creating new dataframe object\nL3_df=df[df.level==3] \nL3_df=L3_df.drop('level_cat',axis=1)\n\n\n\nL4_df=pd.DataFrame()# creating new dataframe object\nL4_df=df[df.level==4 ]\nL4_df=L4_df.drop('level_cat',axis=1)\n\n\n#Combining Examples of level [1,2,3,4]\nframes = [ L1_df, L2_df,L3_df,L4_df]\nL1_df = pd.concat(frames)\nL1_df=L1_df.replace(({'level' : { 2 : 1, 3 : 1, 4 : 1 }}))\n\n\nframes = [ L0_df,L1_df]\nB_df = pd.concat(frames,sort=True)\nB_df=B_df.drop('level_cat',axis=1)\nB_df['level_cat'] = B_df['level'].map(lambda x: to_categorical(x, 1+B_df['level'].max()))#converting my \n# labels to categorical_labels\n#Adding New Categorical Lables according to Binary Dataset\n\nprint(\"Total Numbe of examples in Binary DataFrame = \" + str(len(B_df)))  # Total Examples","1db7a3cc":"**Confirming that image path on 1st index of data frame is same as the image at 1st index of the Np Arrays**","d9e2868f":"#resizing my training examples to (128,128)\n#train examples\nfrom PIL import Image\nfrom skimage.transform import resize \n\nimage_list_128=[]\nfor i in B_df['path']:\n    image=plt.imread(i)\n    image = resize(image, (128, 128))\n    image_list_128.append(image)","45b69ca1":"Talos take 1500 different scans which is so much expansive even my kaggle kernal die because of talos scans ","d7be5ad6":"#resizing my training examples to (90,90)\n#train examples\nfrom PIL import Image\nfrom skimage.transform import resize \n\nimage_list=[]\n\nfor i in range (len(B_df)):\n    image=plt.imread(B_df['path'].iloc[i])\n    image = resize(image, (90, 90))\n    image_list.append(image)\n    ","8a7e44fd":"#resizing my training examples to (264,264)\n#train examples\nfrom PIL import Image\nfrom skimage.transform import resize \n\nimage_list_264=[]\nfor i in B_df['path']:\n    image=plt.imread(i)\n    image = resize(image, (264, 264))\n    image_list_264.append(image)","948b9300":"**loading my saving Binary data**","e9a97b9d":"**Achiving 76% accuracy till now on 1000 examples**","cfcf61aa":"**Its seem to be same now thats good for us**","5ed712f7":"**Saving My Binary dataset and binary sizes (90,128,264) of images which are on same indexs **","55a6be39":"sizes =B_df['level'].values\nsns.distplot(sizes, kde=False); # Visualizing levels in dataset","1001e00a":"##### **Installing Libraries**","88601e9d":"**Making of Binary dataframe**","a7c31a70":"**Only resizing took my 3:00 hours in Kaggle** So I am storing my preprocess data","fe72d22a":"img_list=np.asarray(image_list)\nimg_list=img_list.reshape(1000,90*90*3)\nprint(img_list.shape)\ntype(img_list)\nnp.savez_compressed('1000_Binary_images_data_90', a=img_list)\n\nimg_list_128=np.asarray(image_list_128)\nimg_list_128=img_list_128.reshape(1000,128*128*3)\nprint(img_list_128.shape)\ntype(img_list_128)\nnp.savez_compressed('1000_Binary_images_data_128', a=img_list_128)\n\nimg_list_264=np.asarray(image_list_264)\nimg_list_264=img_list_264.reshape(1000,264*264*3)\nprint(img_list_264.shape)\ntype(img_list_264)\nnp.savez_compressed('1000_Binary_images_data_264', a=img_list_264)","54259205":"B_df.to_csv(\"Binary Dataframe\")\n","7ba5df63":"please visualize 1 and 3 shape their will be a 6d array of filters but first 16 filter not present in the weights array","4ee5bb44":"**Model**","366c2b14":"**Hyperparameter Tuning  by using Talos**\n\n[https:\/\/github.com\/autonomio\/talos\/blob\/master\/examples\/Hyperparameter%20Optimization%20with%20Keras%20for%20the%20Iris%20Prediction.ipynb](http:\/\/)","2e2eac59":"**Resizing image array to (90,90)**","b8adfff5":"**Reporting of talos library**","a2d3cc2a":"**Resizing image array to (264,264)**","748f1652":"**Resizing image array to (128,128)**"}}