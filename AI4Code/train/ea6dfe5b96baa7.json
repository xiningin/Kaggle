{"cell_type":{"67f4f8b4":"code","08748592":"code","898e1474":"code","9a6882fd":"code","853d91e4":"code","eae695db":"code","9b1aa0d9":"code","cbb02867":"code","cfa10b81":"code","0c1ce45b":"code","0abefed1":"code","61433a02":"code","1051824a":"code","230ce997":"code","b40e8183":"code","7a616595":"code","a1ec5d46":"code","82e30882":"code","0556214c":"code","ecaf42dd":"code","0cc89ca7":"code","2a6efb14":"code","dfdd6950":"code","a436780e":"code","96a44468":"code","a56d5cbb":"code","3ac57da2":"code","069e576c":"code","3a5dbf0e":"code","6237da48":"code","912f3ba9":"markdown","adca470a":"markdown","054f6b2c":"markdown","af0a8020":"markdown","31db51a7":"markdown","5eed71e8":"markdown","57f0d07f":"markdown","2017c697":"markdown","e1dfbed0":"markdown","acfe067a":"markdown","095bea85":"markdown","3e9a276e":"markdown","8af8a697":"markdown","0b5d0f74":"markdown","0f1b92f3":"markdown","018dd872":"markdown","3d0fc0f2":"markdown","26c05d52":"markdown","6f9a7032":"markdown","218dcdf6":"markdown","98c1ce00":"markdown","e8ae8cb7":"markdown"},"source":{"67f4f8b4":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nimport os\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nimport tensorflow as tf\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 63\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","08748592":"os.listdir('..\/input\/nlp-getting-started')","898e1474":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ncombined = pd.concat([train,test], axis=0)\ncombined.drop('target',inplace=True, axis=1)\ncombined.info()","9a6882fd":"y = train.target.copy()\nX = train.drop('target',axis=1)\npath = '..\/input\/roberta-transformers-pytorch\/roberta-base'","853d91e4":"sns.countplot(y)\nplt.show()","eae695db":"train_targets = train.target.values.tolist()\nmax_len = 90\n\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-transformers-pytorch\/roberta-base')\n\ninput_ids = [tokenizer.encode(\n        text=i,           \n        add_special_tokens=True, \n        max_length=max_len,  \n        truncation=True,     \n        padding=False\n    ) for i in train.text]       ","9b1aa0d9":"unsorted_lengths = [len(x) for x in input_ids]\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style='darkgrid')\n\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples BEFORE Sorting')\n\nplt.show()","cbb02867":"sorted_input_ids = sorted(zip(input_ids, train_targets), key=lambda x: len(x[0]))\nprint('Shortest sample:', len(sorted_input_ids[0][0]))\nprint('Longest sample:', len(sorted_input_ids[-1][0]))\nsorted_lengths = [len(s[0]) for s in sorted_input_ids]","cfa10b81":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.plot(range(0, len(sorted_lengths)), sorted_lengths)\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples after Sorting')\n\nplt.show()","0c1ce45b":"batch_size = 32\nimport random\n\nbatch_ordered_sentences = []\nbatch_ordered_labels = []\n\nprint('Creating training batches of size {:}'.format(batch_size))\n\nwhile len(sorted_input_ids) > 0:  \n    if ((len(batch_ordered_sentences) % 50) == 0):\n        print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n\n    to_take = min(batch_size, len(sorted_input_ids))\n    select = random.randint(0, len(sorted_input_ids) - to_take)\n    batch = sorted_input_ids[select:(select + to_take)]\n    batch_ordered_sentences.append([s[0] for s in batch])\n    batch_ordered_labels.append([s[1] for s in batch])\n    del sorted_input_ids[select:select + to_take]\n\nprint('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))","0abefed1":"inputs = []\nattn_masks = []\ntargets = []\n\nfor (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n    batch_padded_inputs = []\n    batch_attn_masks = []\n    max_size = max([len(sen) for sen in batch_inputs])\n    for sen in batch_inputs:\n        num_pads = max_size - len(sen)\n        padded_input = sen + [tokenizer.pad_token_id]*num_pads\n        attn_mask = [1] * len(sen) + [0] * num_pads\n        batch_padded_inputs.append(padded_input)\n        batch_attn_masks.append(attn_mask)\n    inputs.append(torch.tensor(batch_padded_inputs))\n    attn_masks.append(torch.tensor(batch_attn_masks))\n    targets.append(torch.tensor(batch_labels))","61433a02":"train_text = train.text.values.tolist()\npadded_lengths = [len(s) for batch in inputs for s in batch]\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) \/ float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))\n","1051824a":"# Essential Imports\nimport random\nimport numpy as np\nimport multiprocessing\nimport more_itertools\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, Dataset, DataLoader","230ce997":"class SmartBatchingDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super(SmartBatchingDataset, self).__init__()\n        self._data = (\n            f\"{tokenizer.bos_token} \" + df.text + f\" {tokenizer.eos_token}\" \n        ).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list()\n        self._targets = None\n        if 'target' in df.columns:\n            self._targets = df.target.tolist()\n        self.sampler = None\n\n    def __len__(self):\n        return len(self._data)\n\n    def __getitem__(self, item):\n        if self._targets is not None:\n            return self._data[item], self._targets[item]\n        else:\n            return self._data[item]\n\n    def get_dataloader(self, batch_size, max_len, pad_id):\n        self.sampler = SmartBatchingSampler(\n            data_source=self._data,\n            batch_size=batch_size\n        )\n        collate_fn = SmartBatchingCollate(\n            targets=self._targets,\n            max_length=max_len,\n            pad_token_id=pad_id\n        )\n        dataloader = DataLoader(\n            dataset=self,\n            batch_size=batch_size,\n            sampler=self.sampler,\n            collate_fn=collate_fn,\n            num_workers=(multiprocessing.cpu_count()-1),\n            pin_memory=True\n        )\n        return dataloader","b40e8183":"class SmartBatchingSampler(Sampler):\n    def __init__(self, data_source, batch_size):\n        super(SmartBatchingSampler, self).__init__(data_source)\n        sample_lengths = [len(seq) for seq in data_source]\n        argsort_inds = np.argsort(sample_lengths)\n        batches = list(more_itertools.chunked(argsort_inds, n=batch_size))\n        if batches:\n            last_batch = batches.pop(-1)\n            np.random.shuffle(batches)\n            batches.append(last_batch)\n        self._inds = list(more_itertools.flatten(batches))\n        self._backsort_inds = None\n    \n    def __iter__(self):\n        it = iter(self._inds)\n        return it\n\n    def __len__(self):\n        return len(self._inds)\n    \n    @property\n    def backsort_inds(self):\n        if self._backsort_inds is None:\n            self._backsort_inds = np.argsort(self._inds)\n        return self._backsort_inds","7a616595":"class SmartBatchingCollate:\n    def __init__(self, targets, max_length, pad_token_id):\n        self._targets = targets\n        self._max_length = max_length\n        self._pad_token_id = pad_token_id\n        \n    def __call__(self, batch):\n        if self._targets is not None:\n            sequences, targets = list(zip(*batch))\n        else:\n            sequences = list(batch)\n        \n        input_ids, attention_mask = self.pad_sequence(\n            sequences,\n            max_sequence_length=self._max_length,\n            pad_token_id=self._pad_token_id\n        )\n        \n        if self._targets is not None:\n            output = input_ids, attention_mask, torch.tensor(targets)\n        else:\n            output = input_ids, attention_mask\n        return output\n    \n    def pad_sequence(self, sequence_batch, max_sequence_length, pad_token_id):\n        max_batch_len = max(len(sequence) for sequence in sequence_batch)\n        max_len = min(max_batch_len, max_sequence_length)\n        padded_sequences, attention_masks = [[] for i in range(2)]\n        attend, no_attend = 1, 0\n        for sequence in sequence_batch:\n            # As discussed above, truncate if exceeds max_len\n            new_sequence = list(sequence[:max_len])\n            \n            attention_mask = [attend] * len(new_sequence)\n            pad_length = max_len - len(new_sequence)\n            \n            new_sequence.extend([pad_token_id] * pad_length)\n            attention_mask.extend([no_attend] * pad_length)\n            \n            padded_sequences.append(new_sequence)\n            attention_masks.append(attention_mask)\n        \n        padded_sequences = torch.tensor(padded_sequences)\n        attention_masks = torch.tensor(attention_masks)\n        return padded_sequences, attention_masks","a1ec5d46":"dataset = SmartBatchingDataset(train, tokenizer)\ntrain_data_loader = dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)","82e30882":"padded_lengths = []\nfor batch_idx, (input_ids, attention_mask, targets) in enumerate(train_data_loader):\n    for s in input_ids:\n        padded_lengths.append(len(s))\n\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) \/ float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","0556214c":"class SentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)\n        self.config = self.roberta.config\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.config.hidden_size, n_classes)\n        \n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n        output = self.drop(sequence_output)\n        return self.out(output)","ecaf42dd":"model = SentimentClassifier(2) # 2 classes 1 for disaster and 0 for not\nmodel = model.to(device)","0cc89ca7":"# torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)","2a6efb14":"EPOCHS = 3\noptimizer = AdamW(model.parameters(), betas = (0.99, 0.98), lr=2e-5)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","dfdd6950":"# This is an example. Tailor this example to your training functions\n# optimizer.zero_grad()                               # Reset gradients tensors\n# for i, (inputs, labels) in enumerate(training_set):\n#     predictions = model(inputs)                     # Forward pass\n#     loss = loss_function(predictions, labels)       # Compute loss function\n#     loss = loss \/ accumulation_steps                # Normalize our loss (if averaged)\n#     loss.backward()                                 # Backward pass\n#     if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n#         optimizer.step()                            # Now we can do an optimizer step\n#         optimizer.zero_grad()                           # Reset gradients tensors\n#         if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...\n#             evaluate_model()                        # ...have no gradients accumulated","a436780e":"def train_epoch(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_examples\n):\n  model = model.train()\n  losses = []\n  correct_predictions = 0\n  for i, d in enumerate(data_loader):\n    input_ids = d[0].to(device)\n    attention_mask = d[1].to(device)\n    targets = d[2].to(device)\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n    loss.backward()\n    # gradient accumulation\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    if (i+1) % 2 == 0:                     # Wait for several backward steps\n        optimizer.step()                    # Now we can do an optimizer step\n        optimizer.zero_grad()               # Reset gradients tensors\n        scheduler.step()\n\n\n  return correct_predictions.double() \/ n_examples, np.mean(losses)","96a44468":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for d in data_loader:\n        input_ids = d[0].to(device)\n        attention_mask = d[1].to(device)\n        targets = d[2].to(device)\n        outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n  return correct_predictions.double() \/ n_examples, np.mean(losses)","a56d5cbb":"from sklearn.model_selection import train_test_split\nX_train, X_val = train_test_split(train, test_size=0.10, random_state=RANDOM_SEED)\n\ntrain_dataset = SmartBatchingDataset(X_train, tokenizer)\ntrain_data_loader = train_dataset.get_dataloader(batch_size=16, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\nval_dataset = SmartBatchingDataset(X_val, tokenizer)\nval_data_loader = val_dataset.get_dataloader(batch_size=16, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\nhistory = defaultdict(list)\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(X_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n\n    val_acc, val_loss = eval_model(\n      model,\n      val_data_loader,\n      loss_fn,\n      device,\n      len(X_val)\n    )\n    print(f'Val loss {val_loss} accuracy {val_acc}')\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)","3ac57da2":"# plt.plot(history['train_acc'], label='train accuracy')\n# plt.plot(history['val_acc'], label = 'val accurracy')\n# plt.title('Training history')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend()\n# plt.ylim([0, 1]);","069e576c":"encodes = test.text.apply(lambda x: tokenizer.encode_plus(\n            x, \n            add_special_tokens=True,\n            max_length = max_len,\n            truncation=True,\n            padding='max_length',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ))\ninput_ids = [i['input_ids'] for i in encodes]\nattention_mask = [i['attention_mask'] for i in encodes]","3a5dbf0e":"predictions = []\nfor i, j in zip(input_ids, attention_mask):\n    i = i.to(device)\n    j = j.to(device)\n    output = model(i, j)\n    _, prediction = torch.max(output, dim=1)\n    predictions.append(prediction.item())","6237da48":"submission = pd.concat([test.id, pd.Series(predictions)], axis=1)\nsubmission.rename(columns = {0:'target'}, inplace=True)\nsubmission.to_csv('submission.csv',index=False)","912f3ba9":"id - a unique identifier for each tweet  \ntext - the text of the tweet  \nlocation - the location the tweet was sent from (may be blank)  \nkeyword - a particular keyword from the tweet (may be blank)  \ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)  \n\nNo missing values in the most important feature, text, but...  \n87 missing values in keyword  \n3638 missing values in location  ","adca470a":"SmartBatchingSampler sorts sequences by length, make batches of specified size, shuffle the batch, then return indices","054f6b2c":"# Gradient Accumulation\nOnly useful if you need increase batch size to reduce computation time. Mainly used when fine tuning your models.  \nBefore calling optimizer.step(), you sum the gradients of several backward operations in the parameter.grad tensors. Implementation is easy ","af0a8020":"Pytorch dataloader implementation saved a little more memory","31db51a7":"What a mess. Now we'll sort the tokens by length","5eed71e8":"Some recommendations for fine tuning from the BERT paper  \nBatch size: 16, 32  \nLearning rate (Adam): 5e-5, 3e-5, 2e-5  \nNumber of epochs: 2, 3, 4","57f0d07f":"# Optimization Goals\n1. Implement Dynamic + Uniform Padding - Improved Model Performance and Training Times\n2. Freeze Model Embeddings to Preserve Model Weights and Reduce Gradient Propagation - Reduced Model Performance\n3. Gradient Accumilation","2017c697":"Negatives outnumber positives by ~1000","e1dfbed0":"# Test Predictions","acfe067a":"# Training","095bea85":"train_data_loader orders its data like this: **input_ids, attention_mask, targets**","3e9a276e":"## Putting it all together","8af8a697":"# Model Creation","0b5d0f74":"Remember, the batches are still not ordered according to length","0f1b92f3":"The output is the probabilities that a text is either a 0 or a 1","018dd872":"SmartBatchingDataset stores samples by tokenizing text and converting to sequences","3d0fc0f2":"## Padding","26c05d52":"## Class Imbalance?","6f9a7032":"## Comparison","218dcdf6":"# Dataset Creation + Dynamic\/Uniform Padding  \nFirst, we have to sort our data by sequence length. We'll tokenize our data and plot the length of these tokens first.","98c1ce00":"## Random Batch Selection","e8ae8cb7":"SmartBatchingCollate adds padding up to max_length, make attention masks, and targets for each sample batch"}}