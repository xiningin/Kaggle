{"cell_type":{"218be0f4":"code","f00178d7":"code","1dec836e":"code","cd0c13f2":"code","be3829a5":"code","6e298273":"code","052b09cd":"code","74ececa4":"code","8e67e6c1":"code","2b96f19e":"code","00ddea53":"code","2187872f":"code","ea118abe":"code","25546d08":"code","2adc7e05":"code","d5da317b":"code","4a7481ee":"code","4ba9c176":"code","7b4be1c0":"code","45132e0e":"code","d2a3ca7d":"code","3aeac938":"code","fbd819a2":"code","b1076d55":"code","376ea5c4":"code","2259d4bd":"code","31bbca3f":"code","89127ea4":"code","f8f4857e":"code","18e79ebf":"code","d17eace9":"code","9ad102b2":"code","4f9f0012":"code","38a88cc4":"code","00d69a7d":"code","ac96501b":"code","caf647a9":"code","16cdf788":"code","2daadbf1":"code","af034f31":"code","8bdd00f0":"code","2e6804b3":"code","f6d7fd0a":"code","5e4f0ab1":"code","0feb8631":"code","ee2bf968":"code","dcf3fa6c":"markdown","770cf2bc":"markdown"},"source":{"218be0f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f00178d7":"tr_data=pd.read_csv('\/kaggle\/input\/electric-power-consumption-data-set\/household_power_consumption.txt', sep=';', parse_dates={'DateTime':['Date','Time']},infer_datetime_format=True,na_values=['nan','?'], index_col='DateTime')\ntr_data.head()","1dec836e":"tr_data.index","cd0c13f2":"# Time resampling\ntr_data.info()","be3829a5":"tr_data.shape","6e298273":"tr_data.isnull().sum()","052b09cd":"cols=list(tr_data.columns)\n","74ececa4":"for col in cols:\n    tr_data[col].fillna(tr_data[col].mean(), inplace=True)","8e67e6c1":"tr_data.isnull().sum()","2b96f19e":"tr_data[cols]=tr_data[cols].astype('float64')","00ddea53":"tr_data.info()","2187872f":"import matplotlib.pyplot as plt\nfrom collections import defaultdict","ea118abe":"df_list=[]\nrules={'D':'daily','M':'monthly','Q':'quarterly','A':'annually'}\n\nd=defaultdict(list)\nfor rule in list(rules.keys()):    \n       d[rules[rule]]=[tr_data[col].resample(rule) for col in cols]\n    \n\n","25546d08":"def resample_data(dict_val, freq, func):\n    if func=='mean':\n        \n        for i in range(len(dict_val)):\n            title=func+ ' of '+cols[i]+' sampled '+freq\n            dict_val[freq][i].mean().plot(figsize=(20,6), title=title)\n            plt.show()\n    elif func=='min' :\n        \n        for i in range(len(dict_val)):\n            title=func+ ' of '+cols[i]+' sampled '+freq\n            dict_val[freq][i].min().plot(figsize=(15,6), title=title)\n            plt.show()\n    elif func=='max' :\n        \n        for i in range(len(dict_val)):\n            title=func+ ' of '+cols[i]+' sampled '+freq\n            dict_val[freq][i].max().plot(figsize=(15,6), title=title)\n            plt.show()\n    elif func=='sum' :\n        \n        for i in range(len(dict_val)):\n            title=func+ ' of '+cols[i]+' sampled '+freq\n            dict_val[freq][i].sum().plot(figsize=(15,6), title=title)\n            plt.show()\n        ","2adc7e05":"resample_data(d,'daily','mean')","d5da317b":"resample_data(d,'monthly','mean')","4a7481ee":"resample_data(d,'quarterly','mean')","4ba9c176":"resample_data(d,'annually','max')","7b4be1c0":"#tr_data['6-month-SMA']=tr_data.Global_intensity.rolling(window=6).mean()\n#tr_data['12-month-SMA']=tr_data.Global_intensity.rolling(window=12).mean()","45132e0e":"\n#tr_data[['Global_intensity','6-month-SMA','12-month-SMA']].plot(figsize=(30,20))\n#plt.show()","d2a3ca7d":"#tr_data['12-month-EWMA']=tr_data.Global_intensity.ewm(span=12).mean()\n#tr_data[['Global_intensity','12-month-EWMA']].plot(figsize=(30,20))\n#plt.show()","3aeac938":"data=tr_data.loc['2010-11-01':]","fbd819a2":"# Split the data\n","b1076d55":"test_idx=24*60*2","376ea5c4":"train=data.iloc[:-test_idx]\ntest=data.iloc[-test_idx:]","2259d4bd":"train","31bbca3f":"test","89127ea4":"#Scale data\nfrom sklearn.preprocessing import MinMaxScaler","f8f4857e":"sc=MinMaxScaler()\nsc.fit(train)\ntrain_sc=sc.transform(train)\ntest_sc=sc.transform(test)","18e79ebf":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator","d17eace9":"# define generartor\nlength=24*60\nbatch_size=1\ngen=TimeseriesGenerator(train_sc,train_sc, length=length, batch_size=batch_size )","9ad102b2":"x,y=gen[0]","4f9f0012":"# Create model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM","38a88cc4":"train_sc.shape","00d69a7d":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","ac96501b":"from keras import backend as K\nK.tensorflow_backend._get_available_gpus()","caf647a9":"model=Sequential()\nmodel.add(LSTM(100, input_shape=(length, train_sc.shape[1])))\nmodel.add(Dense(train_sc.shape[1]))\nmodel.compile(optimizer='adam',loss='mse')\n","16cdf788":"model.summary()","2daadbf1":"# Early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop=EarlyStopping(monitor='val_loss',patience=1)\nval_gen=TimeseriesGenerator(test_sc,test_sc, length=length, batch_size=batch_size)\nmodel.fit_generator(gen, epochs=10, validation_data=val_gen, callbacks=[early_stop])","af034f31":"model.save('mul_var.h5')","8bdd00f0":"# plot the losses\n\nloss=pd.DataFrame(model.history.history)\nloss.plot()","2e6804b3":"#Evaluate on Test data\nn_features=train_sc.shape[1]\ntest_pred=[]\nfirst_eval_batch=train_sc[-length:]\ncurr_batch=first_eval_batch.reshape((1, length, n_features))\nfor i in range(len(test)):\n    curr_pred=model.predict(curr_batch)[0]\n    test_pred.append(curr_pred)\n    curr_batch=np.append(curr_batch[:,1:,:], [[curr_pred]], axis=1)\n    \n\n","f6d7fd0a":"test_pred=sc.inverse_transform(test_pred)","5e4f0ab1":"test_pred_df=pd.DataFrame(test_pred, columns=test.columns)","0feb8631":"test_pred_df","ee2bf968":"test","dcf3fa6c":"When the resampling window is large, the peridicity is lost.","770cf2bc":"WIP"}}