{"cell_type":{"f9c2c876":"code","f2283e11":"code","0f2d3bdc":"code","74c4e2cb":"code","600c430f":"code","cbb7cd30":"code","0e0e56e0":"code","7618ead2":"code","7be78606":"code","42e6e72d":"code","2678e659":"code","ff448b9f":"code","350f4dad":"code","0149ce4e":"code","cf726df5":"code","00508be3":"code","40f24c89":"code","7cd2d793":"code","60a6dc32":"code","e584e04d":"code","541dbbec":"code","7afb43d2":"code","d4191905":"code","b2efed1e":"code","096a5f7c":"code","7afa7e7d":"code","1bc92803":"code","97dcaa1a":"code","6040e202":"code","eddef0c7":"code","dd09d322":"code","1c313dd8":"code","1a65b13b":"code","0317015e":"code","69cc1f08":"code","e684894c":"code","63a25098":"code","c0a80564":"code","771591c4":"code","66cd9659":"code","92cb8a61":"code","6900f3d9":"code","943c54af":"code","31807627":"code","3d43a661":"code","84b897d8":"code","1693e6c9":"code","8249696a":"code","261b77f4":"code","b0815163":"code","4ee81ff7":"code","a167e7a9":"code","b35801c0":"code","fdf63fdf":"code","fdb245ce":"code","4b1c8e05":"code","f8ebd28a":"code","cb492470":"code","cb3d4863":"code","375b4fb6":"code","4126bdff":"code","5fdab19e":"code","ea06b2a0":"code","8ca41595":"code","dc3d4688":"code","8cf5702c":"code","b8f99f62":"code","6dd00001":"code","7493086c":"code","86afedcd":"code","5a98e24b":"markdown","9dfdceea":"markdown","85311e7f":"markdown","515a0d7b":"markdown","751086cb":"markdown","b67048e8":"markdown","9672c7b0":"markdown","2861692f":"markdown","5c3e9b57":"markdown","e21c1d19":"markdown","0a044f75":"markdown","034c5635":"markdown","972824ad":"markdown","6394f067":"markdown","6d37103d":"markdown","7f9b1006":"markdown","6acb508e":"markdown","d69b1554":"markdown"},"source":{"f9c2c876":"# Importing Libraries\nimport os\nimport pandas as pd\nimport string,re\nfrom urllib.parse import urlparse\nimport spacy\nimport itertools\nfrom torch import nn\nfrom torchtext import data  \nfrom nltk.corpus import stopwords \nimport torch\nfrom torchtext.data import BucketIterator\nfrom torch import nn\nfrom torch.optim import Adam\nfrom matplotlib import pyplot as plt\nfrom torch.backends import cudnn","f2283e11":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","0f2d3bdc":"cudnn.benchmark = True","74c4e2cb":"device","600c430f":"#DataFrame Options for display\npd.set_option('display.max_columns', None)  \npd.set_option('display.max_rows', None)  \npd.set_option('display.max_colwidth', -1)  ","cbb7cd30":"# loading spacy model\nnlp = spacy.load('en_core_web_sm') ","0e0e56e0":"# Dataset\npath  = r'..\/input\/nlp-getting-started\/'\ntrainDataset = pd.read_csv(os.path.join(path,'train.csv'),index_col=0)","7618ead2":"trainDataset.head(5)","7be78606":"# The keyword\/location column is inconsistent\ntrainDataset = trainDataset[['text','target']]","42e6e72d":"trainDataset.head()","2678e659":"from unidecode import unidecode\ndef remove_non_ascii(text):\n    return unidecode(text)","ff448b9f":"def clean(text):\n    # removing the spaces\n    text = ' '.join(text.split()) \n    # removing url \n    text = ' '.join([token for token in text.split() if not urlparse(token).scheme]) \n    # removing token starts with @\n    text = ' '.join([token for token in text.split() if not (re.match(r'^@',token))])\n    # remove all punctuation ( except . and ,)\n    text = text.translate(str.maketrans('', '','!\"$%&\\'()*+-\/:;<=>?@[\\\\]^_`{|}~')) \n    # keep only alphabets\n    text = re.sub(r\"\\d\", \"\", text) \n    text = ' '.join(list(itertools.chain.from_iterable([[s for s in re.split(\"([A-Z][^A-Z]*)\", token.replace('#','')) if s] if (re.match(r'^#',token)) else [token] for token in text.split()])))\n    # removing token less than 3 char length and removing non-ascii character\n    text = ' '.join([remove_non_ascii(token) for token in text.split()]) \n    text = text.lower().strip()\n    return text","350f4dad":"text = '''#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count'''   ","0149ce4e":"flatten = [[s for s in re.split(\"([A-Z][^A-Z]*)\", token.replace('#','')) if s] if (re.match(r'^#',token)) else [token] for token in text.split()] # removing token starts with @\nflattened = list(itertools.chain(*flatten))\n' '.join(flattened)","cf726df5":"trainDataset['text'] = trainDataset['text'].apply(lambda x:clean(x))","00508be3":"trainDataset.head()","40f24c89":"len(trainDataset)","7cd2d793":"trainDataset = trainDataset[trainDataset['text']!='']","60a6dc32":"trainDataset.shape","e584e04d":"trainDataset = trainDataset.drop_duplicates(subset =\"text\", \n                     keep = False) ","541dbbec":"trainDataset.shape","7afb43d2":"# The classes are two 0 and 1","d4191905":"trainDataset.to_csv(os.path.join(os.getcwd(),'input.csv'),index=False)","b2efed1e":"trainDataset = pd.read_csv(os.path.join(os.getcwd(),'input.csv'))\ntrainDataset.head()","096a5f7c":"# Stopwords = spacy stopword + nltk stopword\nnlp.Defaults.stop_words|= set(stopwords.words('english'))\nnlp.Defaults.stop_words|= {'the',}\n\n#list(nlp.Defaults.stop_words)\nlen(nlp.Defaults.stop_words)","7afa7e7d":"# Bert Tokenizer\ntokenizer = torch.hub.load('huggingface\/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache.","1bc92803":"# Pipelines used for Text Field\npad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nunk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\nmaxSequenceLength = 64 # not used","97dcaa1a":"TEXT = data.Field(use_vocab=False,tokenize=tokenizer.encode,batch_first=True,include_lengths=True,pad_token=pad_index,unk_token=unk_index)","6040e202":"# Pipelines used for LabelField Field\nLABEL = data.LabelField(dtype = torch.long,batch_first=True)","eddef0c7":"fields = [('text',TEXT),('target', LABEL)]","dd09d322":"# Reading and transforming the data\ntraining_data=data.TabularDataset(path=os.path.join(os.getcwd(),'input.csv'),format = 'csv',fields = fields,skip_header = True)","1c313dd8":"vars(training_data.examples[5])","1a65b13b":"# This will be used to match the results\n# Sequential is False to treat it as a number and no one hot encode\nID = data.Field(sequential=False,dtype = torch.int,batch_first=True,use_vocab=False)","0317015e":"# Reading and transforming the test data","69cc1f08":"test_data = pd.read_csv(os.path.join(path,'test.csv'),index_col=0)","e684894c":"test_data.head()","63a25098":"test_data = test_data[['text']]","c0a80564":"test_data.head()","771591c4":"test_data['text'] = test_data['text'].apply(lambda x:clean(x))","66cd9659":"test_data.head()","92cb8a61":"test_data.to_csv(os.path.join(os.getcwd(),'test.csv'))","6900f3d9":"test_data = data.TabularDataset(path=os.path.join(os.getcwd(),'test.csv'),format = 'csv',fields =[('id', ID),('text',TEXT)],skip_header = True)","943c54af":"vars(test_data.examples[2])","31807627":"# This is replaced by bert tokenizer \n#TEXT.build_vocab(training_data,min_freq=3,vectors = \"glove.6B.100d\")  \nLABEL.build_vocab(training_data,)","3d43a661":"#No. of unique tokens in text\n#print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))","84b897d8":"#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))","1693e6c9":"# Bert tokenizer numerical to string\ntokenizer.decode([101,\n  2045,\n  3224,\n  2543,\n  3962,\n  8644,\n  28519,\n  2024,\n  14070,\n  2408,\n  1996,\n  2395,\n  3685,\n  3828,\n  2068,\n  2035,\n  102])","8249696a":"# Bert tokenizer string to label\ntokenizer.encode(['there','forest','fire'])\n","261b77f4":"#set batch size\nBATCH_SIZE = 128","b0815163":"# All same size text grouped together\n#Load an iterator\ntrain_iterator = data.BucketIterator(\n    training_data, \n    batch_size = BATCH_SIZE,\n    sort_key = lambda x: len(x.text),\n    sort_within_batch=True,\n    device = device)","4ee81ff7":"# Each batch has 128 texts \n#vars(next(iter(train_iterator)))","a167e7a9":"# Iteration loop\nfor (trainX,Length),trainY in train_iterator:\n    print(trainX.shape,trainY)\n    bertTrainX = trainX\n    bertTrainY = trainY\n    break","b35801c0":"# All same size text grouped together\n#Load an iterator\ntest_iterator = data.BucketIterator(\n    test_data, \n    batch_size = BATCH_SIZE,\n    sort_key = lambda x: len(x.text),\n    sort_within_batch=True,\n    device = device)","fdf63fdf":"#vars(next(iter(test_iterator)))","fdb245ce":"for (trainID,(trainX,Length)),trainY in test_iterator:\n    print(trainID,trainX.shape,Length,trainY)\n    break","4b1c8e05":"# BERT BASE MODEL\nbertLayer = torch.hub.load('huggingface\/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased')  # Update configuration during loading","f8ebd28a":"# BERT BASE MODEL\nclass BertModel(nn.Module):\n    def __init__(self,):\n        super(BertModel,self).__init__()\n        self.bertLayer = bertLayer\n        \n        # This step is important as it stops training the bert layers\n        for name, param in self.bertLayer.named_parameters():\n            if 'classifier' not in name:\n                param.requires_grad = False   \n            \n    def forward(self,x,y):\n        return self.bertLayer(x,labels=y)","cb492470":"# BERT BASE MODEL\nepochs=10\nlosses=[]","cb3d4863":"# BERT BASE MODEL\nmodelObj = BertModel()\nmodelObj.to(device)","375b4fb6":"# BERT BASE MODEL\noptimizer = torch.optim.Adam(modelObj.parameters(), lr=0.000001)","4126bdff":"# BERT BASE MODEL\nfor (trainX,text_lengths),trainY in train_iterator:\n    modelObj.eval()\n    loss,ypred= modelObj(trainX,trainY)\n    break\nprint(loss,ypred)","5fdab19e":"# Bert Model\nfor epoch in range(epochs):\n    modelObj.train()\n    for (trainX,text_lengths),trainY in train_iterator:\n        loss,ypred= modelObj(trainX,trainY)\n        loss.backward()\n        with torch.no_grad():\n            optimizer.step()\n    losses.append(loss)\n    print(f'Epoch {epoch} Loss is {loss}')","ea06b2a0":"# Converting float for displaying PLOT\nlossesCPU = [float(loss.to('cpu')) for loss in losses]","8ca41595":"# Pyplot\nplt.plot(lossesCPU)","dc3d4688":"submit_frame = pd.DataFrame(columns=['id','target'])","8cf5702c":"for (testID,(testX,text_lengths)),trainY in test_iterator:\n    modelObj.eval()\n    output = modelObj(testX,trainY)[0]\n    submit_frame_temp = pd.DataFrame(columns=['id','target'])\n    submit_frame_temp['id'] = testID.to('cpu').numpy()\n    submit_frame_temp['target'] = torch.argmax(output,dim=1).to('cpu').numpy()\n    submit_frame = submit_frame.append(submit_frame_temp)","b8f99f62":"submit_frame = submit_frame.sort_values(by=['id'],ascending=True).reset_index(drop=True)","6dd00001":"submit_frame.to_csv(os.path.join(os.getcwd(),'submission.csv'),index=False)","7493086c":"os.getcwd()","86afedcd":"submit_frame.head()","5a98e24b":"### Obeservation :\n\n1. We got the relation of vocab and the pretrained vectors.\n2. These vectors can be used directly as an input to the embedding layer.","9dfdceea":"## Future Steps:\n\n1. Detailed EDA\n2. Increase the accuracy by changing the architecture.\n","85311e7f":"1. The empty rows have to be removed \n2. The duplicates have to be removed","515a0d7b":"### Initialize ","751086cb":"## Lets create Iterator and DataLoader","b67048e8":"# Lets create Dataset and DataLoader using Torchtext","9672c7b0":"## Creating the Model","2861692f":"## Preperation of Answer Frame","5c3e9b57":"## Lets understand the data ","e21c1d19":"####  Check complex logic information extraction of tags","0a044f75":"Torchtext : Preprocessor Pipeline\n\nEach .Field will be used for pre\/post processing of feature and target columns.\n* Sequence of the columns also mapped [(None, None),(None,None),(None,None),('text',TEXT),('target', LABEL)]\n\n1. **tokenizer :** spacy : small\n2. **batch_first :** Whether to produce tensors with the batch dimension first. Default: False.\n3. **preprocessing :** Clean function integrated\n4. **stopwords :** for stopwords\n\n.LabelField is for LabelField\n\n1. **sequential**\n2. **vocab** \n\nPlease refer to  : https:\/\/torchtext.readthedocs.io\/en\/latest\/data.html","034c5635":"### Torchtext Pipelines","972824ad":" ### Building Vocab required for LABEL field as the TEXT field is managed by bertTokenizer","6394f067":"### Utilities ","6d37103d":"## Cleaning TextColumn","7f9b1006":"## Reference \n\n1. Please refer to  : https:\/\/torchtext.readthedocs.io\/en\/latest\/data.html\n2. Please refer to  : https:\/\/pytorch.org\/hub\/huggingface_pytorch-transformers\/","6acb508e":"# About the Notebook\n\n### **DATASET** : *Disaster News Classification*\n\n#### **Classifying** FAKE VS REAL NEWS\n\n1. Cleaning the individual rowText : Removing stopwords,urls,@tokens and special attention to #tags.\n2. Using Torchtext for integrating the tokenizer(SPACY) and preprocessor.\n3. Bucket iterrator is used for grouping similar size of sentences together for minimum padding.\n4. Bert Base model for transfer learning.\n5. Updating the weights of the last layer of the bert base model.\n6. The model is trained and the output is saved in a csv.\n\n[![image.png](attachment:image.png)](http:\/\/)","d69b1554":"\n#### BERT Classification Model\n\nIts a bert based model with a Linear Layer at the end for classification.\n\n"}}