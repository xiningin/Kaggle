{"cell_type":{"18063638":"code","61f978f1":"code","6c179cb1":"code","c3b88086":"code","f55cbe10":"code","082f3e29":"code","9ae30127":"code","731437d6":"code","f0c4b554":"code","1979e70a":"code","2147e65c":"code","5c3d3c60":"code","c311304c":"code","39c28601":"code","69a55302":"code","a07d3479":"code","42b2f77b":"code","c58f0f54":"code","462333ef":"code","fbb97491":"code","99339234":"code","102d687d":"code","e4844516":"code","9d15e928":"code","e8646aa4":"markdown","bd4cccde":"markdown","3fa41330":"markdown","e45dc5da":"markdown","75b501a1":"markdown","183c2f60":"markdown","7349f955":"markdown","2e7ddd03":"markdown","214b0c8c":"markdown","98b82324":"markdown","feb18578":"markdown","9d7b2b25":"markdown","9d6cd779":"markdown","e9bc6488":"markdown","82ec74ea":"markdown","eab4ca38":"markdown","9d195899":"markdown","13542221":"markdown","f072d899":"markdown","4813a96a":"markdown","f31aa701":"markdown","f06c9fea":"markdown","7c484018":"markdown","177b6906":"markdown"},"source":{"18063638":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61f978f1":"#loading libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.filterwarnings('ignore')","6c179cb1":"data = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv')\ndata.head()","c3b88086":"for col in data.columns:\n    if ' ' in col:\n        data = data.rename(columns={col:col.replace(' ','_')})","f55cbe10":"data.head()","082f3e29":"print(f'Shape of given data is {data.shape}\\n\\n')\nprint(f'Columns in given data are {data.columns}\\n\\n')\nprint(f'Info regarding the given data \\n')\ndata.info()","9ae30127":"data.describe().plot(kind = \"area\",fontsize=20, figsize = (20,9),colormap=\"rainbow\")\nplt.xlabel('Statistics',)\nplt.ylabel('Value')\nplt.title(\"General Statistics of Admissions\")\nplt.show()\ndata.describe()","731437d6":"sns.pairplot(data)","f0c4b554":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data.corr(), ax=ax, annot=True)","1979e70a":"\nplt.figure(figsize=(40,35))\nplt.subplot(6, 2, 1)\nsns.distplot(data['GRE_Score'],bins=30,color='Blue',  kde_kws={\"color\": \"y\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 2,\"alpha\": 0.3 })\nplt.subplot(6, 2, 2)\nsns.distplot(data['TOEFL_Score'],bins=12,color='Green' ,kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 7,\"alpha\": 0.3 })\nplt.subplot(6, 2, 3)\nsns.distplot(data['University_Rating'],color='Red' ,kde_kws={\"color\": \"g\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 7,\"alpha\": 0.3 })\nplt.subplot(6, 2, 4)\nsns.distplot(data['SOP'],color='Blue',  kde_kws={\"color\": \"y\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 2,\"alpha\": 0.3 })\nplt.subplot(6, 2, 5)\nsns.distplot(data['LOR_'],color='Green' ,kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 7,\"alpha\": 0.3 })\nplt.subplot(6, 2, 6)\nsns.distplot(data['CGPA'],color='Red' ,kde_kws={\"color\": \"g\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 7,\"alpha\": 0.3 })\n","2147e65c":"plt.figure(figsize=(12,6))\nsns.lineplot(y=\"CGPA\", x=\"University_Rating\",hue=\"Research\",data=data)\nplt.show()","5c3d3c60":"plt.figure(figsize=(7,7))\nplt.pie(data.University_Rating.value_counts().values,labels=data.University_Rating.value_counts().index,autopct='%1.1f%%')\nplt.title('University Rating')\nplt.show()","c311304c":"data.groupby('University_Rating')[['SOP','LOR_','CGPA', 'GRE_Score', 'TOEFL_Score']].mean()","39c28601":"fig = sns.scatterplot(x=\"GRE_Score\", y=\"TOEFL_Score\",hue=\"University_Rating\",data= data)\nplt.title(\"GRE Score vs TOEFL Score\")\nplt.show()\n\nfig = sns.scatterplot(x=\"GRE_Score\", y=\"CGPA\",hue=\"University_Rating\", data= data)\nplt.title(\"GRE Score vs CGPA\")\nplt.show()\n\nfig = sns.scatterplot(x=\"GRE_Score\", y=\"Chance_of_Admit_\", hue=\"University_Rating\", data= data)\nplt.title(\"GRE Score vs Chance of Admit\")\nplt.show()","69a55302":"\ns = data[data[\"Chance_of_Admit_\"] >= 0.25][\"University_Rating\"].value_counts()\nplt.title(\"University Ratings of Candidates with an 25% acceptance chance\")\ns.plot(kind='bar',figsize=(5, 3))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.xticks(rotation=360)\nplt.show()\n\np = data[data[\"Chance_of_Admit_\"] >= 0.50][\"University_Rating\"].value_counts()\nplt.title(\"University Ratings of Candidates with an 50% acceptance chance\")\np.plot(kind='bar',figsize=(5, 3))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.xticks(rotation=360)\nplt.show()\n\nd = data[data[\"Chance_of_Admit_\"] >= 0.75][\"University_Rating\"].value_counts()\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\nd.plot(kind='bar',figsize=(5, 3))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.xticks(rotation=360)\nplt.show()\n\nf = data[data[\"Chance_of_Admit_\"] >= 0.90][\"University_Rating\"].value_counts()\nplt.title(\"University Ratings of Candidates with an 90% acceptance chance\")\nf.plot(kind='bar',figsize=(5, 3))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.xticks(rotation=360)\nplt.show()","a07d3479":"data = data.drop(['Serial_No.'], axis=1)","42b2f77b":"X=data.drop('Chance_of_Admit_',axis=1)\ny=data['Chance_of_Admit_']\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nX_n=preprocessing.normalize(X)\nX_train,X_test,y_train,y_test=train_test_split(X_n,y,test_size=0.20,random_state=101)","c58f0f54":"from sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\nfrom sklearn.svm import SVR,SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error","462333ef":"regressors=[['Linear Regression :',LinearRegression()],\n            ['Decision Tree Regression :',DecisionTreeRegressor()],\n            ['Random Forest Regression :',RandomForestRegressor()],\n            ['Gradient Boosting Regression :', GradientBoostingRegressor()],\n            ['Ada Boosting Regression :',AdaBoostRegressor()],\n            ['Extra Tree Regression :', ExtraTreesRegressor()],\n            ['K-Neighbors Regression :',KNeighborsRegressor()],\n            ['Support Vector Regression :',SVR()],\n            ['Xgboost: ', XGBRegressor()],\n            ['CatBoost: ', CatBoostRegressor(logging_level='Silent')]]\n\nprint(\"Results...\")\n\nr_pred =[]\nfor name,model in regressors:\n    model = model\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(name, (np.sqrt(mean_squared_error(y_test, predictions))))\n    r_pred.append(np.sqrt(mean_squared_error(y_test, predictions)))","fbb97491":"y_ax=['Linear Regression' ,'Decision Tree Regression', 'Random Forest Regression','Gradient Boosting Regression', 'Ada Boosting Regression','Extra Tree Regression' ,'K-Neighbors Regression', 'Support Vector Regression', 'Xgboost', 'CatBoost' ]\nx_ax=r_pred\nsns.barplot(x=x_ax,y=y_ax,linewidth=1.5,edgecolor=\"0.1\")\n","99339234":"classifier = RandomForestRegressor()\nclassifier.fit(X,y)\nfeature_names = X.columns\nimportance_frame = pd.DataFrame()\nimportance_frame['Features'] = X.columns\nimportance_frame['Importance'] = classifier.feature_importances_\nimportance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)\n4\n\nplt.barh([1,2,3,4,5,6,7], importance_frame['Importance'], align='center', alpha=0.5)\nplt.yticks([1,2,3,4,5,6,7], importance_frame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()","102d687d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nfrom sklearn.linear_model import LinearRegression\n\n\n#dataset = pd.read_csv('Admission_Predict.csv')\ndataset = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv')\nfor col in dataset.columns:\n    if ' ' in col:\n        dataset = dataset.rename(columns={col:col.replace(' ','_')})\ndataset = dataset.drop(['Serial_No.'], axis=1)\n\nX = dataset.drop('Chance_of_Admit_',axis='columns')\ny = dataset['Chance_of_Admit_']\n\nregressor = LinearRegression(normalize=True)\nregressor.fit(X, y)\n\n# Saving model to disk\npickle.dump(regressor, open('model.pkl','wb'))\n\n# Loading model to compare the results\nmodel = pickle.load(open('model.pkl','rb'))\nprint(model.predict([[337, 118, 4, 4.5, 4.5, 9.65, 1]]))","e4844516":"'''\nimport numpy as np\nfrom flask import Flask, request, jsonify, render_template\nimport pickle\n\napp = Flask(__name__)\nmodel = pickle.load(open('model.pkl', 'rb'))\n\n@app.route('\/')\ndef home():\n    return render_template('index.html')\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    \n    (For rendering results on HTML GUI)\n    \n    int_features = [float(x) for x in request.form.values()]\n    final_features = [np.array(int_features)]\n    prediction = model.predict(final_features)\n\n    output = round(prediction[0]*100, 2)\n\n    return render_template('index.html', prediction_text='Expected Probablity of your admission is {}'.format(output))\n\n@app.route('\/predict_api',methods=['POST'])\ndef predict_api():\n    \n    (For direct API calls trought request)\n    \n    data = request.get_json(force=True)\n    prediction = model.predict([np.array(list(data.values()))])\n\n    output = prediction[0]\n    return jsonify(output)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n'''","9d15e928":"'''\nimport requests\n\nurl = 'http:\/\/localhost:5000\/predict_api'\nr = requests.post(url,json={'gre':337, 'toefl':118, 'rating':4, 'sop':4.5, 'lor':4.5, 'cgpa':9.65, 'research':1})\n\nprint(r.json())\n'''","e8646aa4":"**view this github link to depoy your very own model**\n* [link to code](https:\/\/github.com\/Vaishnavi-Dixit-12\/Predict-Probability-of-Admission-simple-model-and-flask-deployment)","bd4cccde":"<h4 style=\"text-align:center;color: green\">Feature Imporatnce shows that CGPA as the most important and Research as the least<\/h4>","3fa41330":"<a id='intro'><\/a>\n<h2 style=\"background-color:#3377ff;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Introduction<\/h2>\n\n[Back To Table of Contents](#toc)","e45dc5da":"<a id='flask'><\/a>\n<h2 style=\"background-color: gray;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Flask Deployment<\/h2>\n\n[Back To Table of Contents](#toc)","75b501a1":"<h4 style=\"text-align:center;color: green\">Here we hhave used linear regression, fit the model and save the model in model.pkl <\/h4>","183c2f60":"<h4 style=\"text-align:center;color: green\">Chance of Admission of 1 to 5 university ranking keeps on inceasing gradually with 5 highest towards end of X axis and 1 towards the start of X axis<\/h4>","7349f955":"<h4 style=\"text-align:center;color: green\">Linear Regression and CatBoost are amongst the best suited Regressors\/Models <\/h4>","2e7ddd03":"<h4 style=\"text-align:center;color: green\">Mean Value for all the attributes incrases gradually with increase in University Ranking<\/h4>","214b0c8c":"<h4 style=\"text-align:center;color: green\">The dataset has 9 attributes, it has no null values, all the data in numeric form (int and float)<\/h4>\n<h4 style=\"text-align:center;color: green\">It is a comparitively small dataset with only 400 observations<\/h4>","98b82324":"<h4 style=\"text-align:center;color: green\">There is a linear relationship for Chance of Admit with TOEFL Score, CGPA and GRE Score <\/h4>\n<h4 style=\"text-align:center;color: green\">There is a gradual increase in Univeristy Ranking, SOP and LOR with increase in TOEFL Score, CGPA and GRE Score <\/h4>","feb18578":"_**The parameters used in determining probability for admission are-**_\n\n* GRE Scores ( out of 340 )\n* TOEFL Scores ( out of 120 )\n* University Rating ( out of 5 )\n* Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n* Undergraduate GPA ( out of 10 )\n* Research Experience ( either 0 or 1 )\n* Chance of Admit ( ranging from 0 to 1 )\n\n_**Acknowledgements**_\n* This dataset is inspired by the UCLA Graduate Dataset. The test scores and GPA are in the older format.The dataset is owned by Mohan S Acharya.","9d7b2b25":"<a id='eda'><\/a>\n<h2 style=\"background-color:pink;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">EDA<\/h2>\n\n[Back To Table of Contents](#toc)","9d6cd779":"<a id='load'><\/a>\n<h2 style=\"background-color:green;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Loading Data and Cleaning<\/h2>\n\n[Back To Table of Contents](#toc)","e9bc6488":"<h4 style=\"color: red\">model.py<\/h4>\n","82ec74ea":"<h4 style=\"text-align:center;color: green\">In the above 6 graphs we can see the trends for Attribute Vs Density.<\/h4>","eab4ca38":"<h4 style=\"text-align:center;color: green\">Above graph traces the trend for University Ranking vs CGPA for students with No or One Research Topic<\/h4>","9d195899":"<a id='model'><\/a>\n<h2 style=\"background-color: yellow;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Selecting a Model<\/h2>\n\n[Back To Table of Contents](#toc)","13542221":"<h4 style=\"text-align:center;color: green\">With the increase in acceptance chance, lower ranked university decreases<\/h4>","f072d899":"<h4 style=\"color: red\">request.py<\/h4>","4813a96a":"<h4 style=\"text-align:center;color: green\">Above is a summary for the given dataset, here Univeristy Ranking SOP and LOR are acting as categorical attributes.<\/h4>","f31aa701":"<h4 style=\"text-align:center;color: green\">Percentage represenation for Number of Rankings Presesnt <\/h4>","f06c9fea":"<h4 style=\"color: red\">app.py<\/h4>","7c484018":"<h4 style=\"text-align:center;color: green\">Other than serial No. all other attributes share significant correlation with each other. Each attribute is imp for predicting right probability<\/h4>","177b6906":"<a id='toc'><\/a>\n<h2 style=\"background-color: orange ;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Table of Contents<\/h2>\n\n* [Introduction](#intro)\n* [Loading Data and Cleaning ](#load)\n* [EDA](#eda)\n* [Selecting a Model](#model)\n* [Flask Deployment](#flask)"}}