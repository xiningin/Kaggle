{"cell_type":{"fc2251a5":"code","ab7b4de9":"code","63e5227e":"code","9d8bbaea":"code","40bd2bc8":"code","5351c715":"code","e319a98e":"code","3686cafb":"code","0d7f9529":"code","070b2632":"code","260c232f":"code","e304d8aa":"code","8b52eeaf":"code","61da2d31":"code","42f8e825":"code","53a70e11":"code","f3686f6f":"code","0581a2da":"code","30dbd87d":"code","63303d13":"code","a8b496a9":"code","3717c426":"code","56ead3df":"code","34b2d2a8":"code","ee9706c0":"code","fabaa836":"code","38ac3a79":"code","46664b5b":"code","c9848591":"code","f29ea049":"code","0f015486":"code","0311ad0a":"code","d747f4cd":"code","9f7bb676":"code","ac4b0acc":"code","c9ac6870":"code","b3b33ec0":"code","41f0a7ee":"code","ef964786":"code","252f64ce":"code","d4f92fbf":"code","8244da9a":"code","22f76107":"code","b9a2f6d7":"code","3ce0364a":"code","c7025b4b":"code","dc548a43":"code","847a8d6a":"code","037c5765":"code","a63a2699":"code","ab9b6b98":"code","3f04b050":"code","24d3c8ea":"code","7375b662":"code","9fe904c1":"code","a4b9ec3e":"code","633e69cc":"code","61740722":"code","927d3291":"code","f3cbf9d6":"code","773b5122":"code","d0cf65ec":"code","f7cd8d98":"code","73f894eb":"code","68a17e23":"code","f5b3024c":"code","925626b8":"code","786f1f43":"code","89349014":"markdown","51d496d5":"markdown","0f8ee394":"markdown","56f756e9":"markdown","95d038b6":"markdown","3f1ae4e7":"markdown","560a227f":"markdown","f271de34":"markdown","e2bbdcb9":"markdown","44003189":"markdown","360ed458":"markdown","64104747":"markdown","738d0a36":"markdown","afbb3d47":"markdown","255a3dbc":"markdown","5eabbf21":"markdown","9873cb8e":"markdown","7da3ba5d":"markdown","d235af52":"markdown","1eb7c01f":"markdown","ecb61bb9":"markdown","270edbc0":"markdown","042cc725":"markdown","943d8ac6":"markdown","413422f1":"markdown","5c1bd7eb":"markdown","9e11a1f8":"markdown"},"source":{"fc2251a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab7b4de9":"import datetime\nx= datetime.datetime.now()\nprint(x)","63e5227e":"for n in range( 1,50):\n    if n %3==0 and n %5==0:\n        print( 'FizzBuzz')\n    elif n %3==0:\n        print('Fizz')\n    elif n %5==0:\n        print ('Buzz')\n    else:\n        print(n)","9d8bbaea":"def df(n):\n    if n %3 == 0 and n %5 ==0:\n        return'FizzBuzz'\n    elif n % 3==0:\n        return'Fizz'\n    elif n % 5==0:\n        return \"Buzz\"\n    else:\n        return n\nfor n in range(1,50):\n    print(df(n))","40bd2bc8":"#Method 1: using linalg.norm()\nimport numpy as np\n#numpy arrays\npoint1= np.array((1,2,3))\npoint2=np.array((3,2,1))\n# calculate Euclidean distance\ndist= np.linalg.norm(point1-point2)\n# print Euclidean distance\nprint(dist)","5351c715":"#Method 2: using dot()\nimport numpy as np\npoint1= np.array((1,2,3))\npoint2=np.array((3,2,1))\n#subtracting vector\ntemp= point1 - point2\n# doing dot product\n# for finding\n# sum of the squares\nsum_sq = np.dot(temp.T, temp)\n \n# Doing squareroot and\n# printing Euclidean distance\nprint(np.sqrt(sum_sq))","e319a98e":"# Method 3: using sum() and square()\nimport numpy as np\npoint1= np.array((1,2,3))\npoint2=np.array((3,2,1))\n#finding the sumsquare\nsumsq= np.sum(np.square(point1- point2))\n# calculate Euclidean distance\ndist= np.sqrt(sumsq)\nprint(dist)","3686cafb":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans","0d7f9529":"# Import data\ndata ={'Country':['USA','Canada','France','UK','Germany','Australia'],\n                   'Latitude' : [44.97,62.40,46.75,54.01,51.15,-25.45],\n                   'Longtitude':[-103.77,-96.80,2.40,-2.53,10.40,133.11],\n                   'Lanuage':['English','English','French','English','German','English']\n                  }\ndf=pd.DataFrame(data)      \ndf.head()\n","070b2632":"# Plotting the data \nplt.scatter(x='Longtitude', y= 'Latitude', data = df, color='red', marker = '*')\nplt.xlabel( 'Latitude')\nplt.ylabel('Longtitude')\nplt.figure(figsize=(10,20))\nplt.show()","260c232f":"K= range(1,7)\nsse=[]\nfor k in K:\n    km=KMeans(n_clusters=k)\n    km.fit(df[['Longtitude','Latitude']])\n    sse.append(km.inertia_)","e304d8aa":"sse","8b52eeaf":"# plot the elbow plot\nplt.xlabel('K')\nplt.ylabel('Sum of squared error')\nplt.plot(K,sse,'bx-')\nprint('Kmeans is 3')","61da2d31":"kmean=KMeans(3)\nkmean.fit(df[['Longtitude','Latitude']])\na= kmean.cluster_centers_\na\n","42f8e825":"y_predicted=kmean.fit_predict(df[['Longtitude','Latitude']])\ny_predicted","53a70e11":"# add the cluster column to the data \ndf['cluster']= y_predicted\ndf","f3686f6f":"#visualize the new clusters \ndf1=df[df.cluster==0]\ndf2=df[df.cluster==1]\ndf3=df[df.cluster==2]\nplt.scatter(df1.Longtitude,df1.Latitude,color='green')\nplt.scatter(df2.Longtitude,df2.Latitude,color='blue')\nplt.scatter(df3.Longtitude,df3.Latitude,color='red')\nplt.scatter(a[:,0],a[:,1],marker='*',color='purple',label='centroid')\nplt.xlabel('Longtitude')\nplt.ylabel('Latitude')\nplt.legend()\nplt.show()\n","0581a2da":"# IMPORT LIBRARY\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n","30dbd87d":"data={'a':[3, 1, 1, 2, 1, 6, 6, 6, 5, 6, 7, 8, 9, 8, 9, 9, 8],\n     'b':[5, 4, 5, 6, 5, 8, 6, 7, 6, 7, 1, 2, 1, 2, 3, 2, 3]}\nd_data=pd.DataFrame(data)\nd_data.head()","63303d13":"# Plotting the data \nplt.scatter('a','b', data = d_data, color='red', marker = '*')\nplt.xlabel( 'a')\nplt.ylabel('b')\nplt.figure(figsize=(10,20))\nplt.show()","a8b496a9":"# k means determine k\nsil=[]\nK= range(2,10)\nfor k in K:\n    kmeanModel=KMeans(k)\n    kmeanModel.fit(d_data)\n    labels = kmeanModel.labels_\n    sil.append(silhouette_score(d_data,labels,metric='euclidean'))","3717c426":"sil","56ead3df":"# plot the elbow plot\nplt.xlabel('K')\nplt.ylabel('Silhouette score')\nplt.plot(K,sil,'bx-')\nplt.title('Silhouette analysis For Optimal k')\nplt.show()","34b2d2a8":"km=KMeans(3)\nkm.fit(d_data)\nc=km.cluster_centers_\nc","ee9706c0":"clusters=kmean.fit_predict(d_data[['a','b']])\nclusters","fabaa836":"d_data['n_cluster']= clusters\nd_data","38ac3a79":"#visualize the new clusters \nd1=d_data[d_data.cluster==0]\nd2=d_data[d_data.cluster==1]\nd3=d_data[d_data.cluster==2]\nplt.scatter(d1.a,d1.b,color='green')\nplt.scatter(d2.a,d2.b,color='blue')\nplt.scatter(d3.a,d3.b,color='red')\nplt.scatter(c[:,0],c[:,1],marker='*',color='purple',label='centroid')\nplt.xlabel('a')\nplt.ylabel('b')\nplt.legend()\nplt.show()","46664b5b":"import numpy as np\nimport pandas as pd","c9848591":"from sklearn.datasets import load_boston\nboston_dataset=load_boston()\n","f29ea049":"df_boston=pd.DataFrame(boston_dataset.data)\ndf_boston.columns= boston_dataset.feature_names\ndf_boston.head()","0f015486":"df_boston['Price'] = boston_dataset.target\ndf_boston.head()","0311ad0a":"#assign feature on X axis\nX_features= boston_dataset.data\n# assign feature on Y axis\nY_target= boston_dataset.target","d747f4cd":"# import linear model the estimator\nfrom sklearn.linear_model import LinearRegression\nlineReg = LinearRegression()","9f7bb676":"# fit data into the eatimator\nlineReg.fit(X_features, Y_target)","ac4b0acc":"# print the intercept\nprint ( 'the estimated intercept is :', lineReg.intercept_)","c9ac6870":"# print the coefficient\nprint('the estimated coefficient is :', len(lineReg.coef_))","b3b33ec0":"# train model split the whole dataset into train and test datasets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_features, Y_target)","41f0a7ee":"# print the dataset shape\nboston_dataset.data.shape","ef964786":"# print shapes of the training and testing data sets\ndisplay(X_train.shape)\ndisplay(X_test.shape)\ndisplay(Y_train.shape)\ndisplay(Y_test.shape)","252f64ce":"# fit the training sets into the model\nlineReg.fit(X_train,Y_train)","d4f92fbf":"# the mean square error(MSE) or residual sum of squares\n#print( 'the mean square error is:' %np.mean((lineReg.predict(X_test)-Y_test)**2)))","8244da9a":"# calculate variance\nprint('the variance score is:', lineReg.score(X_test,Y_test))","22f76107":"import numpy as np\nimport pandas as pd","b9a2f6d7":"from sklearn.datasets import load_iris\niris_dataset=load_iris()","3ce0364a":"iris_dataset.DESCR","c7025b4b":"iris_dataset.feature_names","dc548a43":"iris_dataset.target","847a8d6a":"# assign features data to X axis\nX_feature= iris_dataset.data\nX_feature.shape","037c5765":"Y_target= iris_dataset.target\nY_target.shape","a63a2699":"# first use KNN classifier method-import it from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier","ab9b6b98":"knn= KNeighborsClassifier(n_neighbors=1)","3f04b050":"knn","24d3c8ea":"knn.fit(X_feature, Y_target)","7375b662":"X_new=[[3,5,4,1],[5,3,4,2]]","9fe904c1":"knn.predict(X_new)","a4b9ec3e":"from sklearn.linear_model import LogisticRegression\nlogReg=LogisticRegression()","633e69cc":"logReg.fit(X_feature, Y_target)","61740722":"from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs","927d3291":"n_samples=30\nrandom_state=20\nX,y= make_blobs(n_samples=n_samples,n_features=5, random_state=None)\npredict_y=KMeans(n_clusters=3, random_state=random_state).fit_predict(X)\npredict_y","f3cbf9d6":"from sklearn.decomposition import PCA","773b5122":"n_sample=20\nrandom_state=20","d0cf65ec":"X,y=make_blobs(n_samples= n_sample, n_features=10, random_state=None)","f7cd8d98":"X.shape","73f894eb":"pca=PCA(n_components =3)","68a17e23":"pca.fit(X)\npca.explained_variance_ratio_","f5b3024c":"first_pca= pca.components_[0]\nfirst_pca","925626b8":"pca_reduced=pca.transform(X)","786f1f43":"pca_reduced.shape","89349014":"## Machine Learning: example of linear regression\n","51d496d5":"**1. Supervised and Unsupervised Learning: What's the difference?**\n\nThe main difference is one uses labeled data to help predict outcomes, while the other does not. \n> https:\/\/www.ibm.com\/cloud\/blog\/supervised-vs-unsupervised-learning","0f8ee394":"17. **How should you maintain your deployed model?**\n* Monitor: constant monitoring of all the models is needed to determine the performance accuarcy of the models\n* Evaluate: eveluation metrics of the current model is calculated to determine if new algorithm is needed\n* Compare: The new models are compared against each other to determine which model performs the best\n* Rebuld: the best performing model is re-buld on current state of data","56f756e9":"**4. How to build a random forest model?**\n* randomly select 'k' features from the total 'm' features where k<<'m'\n\n* among the 'k' features, calculate the node 'd' using the best split point\n\n* split the node into daughter nodes using the best split\n\n* repeat step 2 and 3 until leaf nodes are finalized\n\n* build forest by repeating step 1 to 4 for 'n' number times to create 'n' number of trees\n\n                                                                      \n                                                                      \n                                     ","95d038b6":"**11. For the given points, how will you calculate the Eucledian Distance, in python?**\n\n\nEuclidean distance is the shortest between the 2 points irrespective of the dimensions. \n\nTo answer this question, we will use the NumPy library to calcuate the Eucledian Distance\n![Eucledian Distance](https:\/\/vitalflux.com\/wp-content\/uploads\/2020\/12\/Euclidean-Distance-formula-2.png)\n> https:\/\/www.geeksforgeeks.org\/calculate-the-euclidean-distance-using-numpy\/","3f1ae4e7":"**8. What are the feature selection methods to select the right variables?**\n\n**Feature selection** methods are intended to **reduce the number of input variables** to those that are believed to be most useful to a model in order to predict the target variable.\n\nThere are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into 3 major buckets\n* Filter based: we specify some metric and based on that filter features. ex: correlation, chi-square\n* Wrapper-based: consider the selection of a set of features as a search problem. ex: recursive feature elimination\n* Emvedded: use algorithms that have built-in feature selection methods. ex: lasso, RF\n> https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/\n> https:\/\/towardsdatascience.com\/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2\n> https:\/\/www.analyticsvidhya.com\/blog\/2016\/12\/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables\/","560a227f":"18. **What are recomender system?**\n* <span style ='color: green'> Recommender systems \n    \n    Algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).\n\n* <span style ='color: green'> How does a recommender system work\n    * Characteristic information: information about items(keywords, categories,..) and users(preferences, profiles...)\n    * User-items interaction: ratings, number of purchases, likes...\n \n Based on this, we can distinguish betwween three algorithms used in recommender systems:\n    * Content_based filtering, which used characteristic information\n    * Collaborative filtering, which are based on user-item interaction\n    * Hybird fileting: best of both world\n    \n* <span style = 'color: red'> **Collaborative filtering:** \n\n_ The past interactions recorded between users and items in order to produce new recommendation. These interactioons are stored un the so-called 'user-item interaction matrix'.\n    \n_ In short, collaborative filtering systems are based on the assumption that if a user likes item A and another user likes the same item A as well as item B.  The first user could also be interested in the second item. Hence, they aim to predict new interactions based on historical ones.\n\n_ Advantage of collaborative approach : no information about users or items is required\n_ Disadvantage of collaborative approach: suffer from 'cold start problem': it is impossible to recommend anything to new users  or to recommend a new item to any users and many users or items have too few interactions to be efficiently hanfded.\n \n    \n* <span style = 'color: red'> **Content-based filtering**:\n- Make recommendations using a user's item and profile feature.\n- The idea of content-based methods is try to build a model, based on the avaiable 'feature', that explain the observed user-item interactions\n- Content-based filtering suffer less from cold start problem thn collarborative approach: new user or items can be described by their characteristic so relevent suggestions can be done for these new entities.\n> https:\/\/towardsdatascience.com\/introduction-to-recommender-systems-6c66cf15ada\n> https:\/\/tryolabs.com\/blog\/introduction-to-recommender-systems    ","f271de34":"# PCA method","e2bbdcb9":"20) **How can you select k for k-means?**\n\n* k-means clustering is an unsupervised algorithm.\n* the aim of k-means clustering is to find these k clusters and their centers whide reducing the totoal error.\n* There are two most common Method to select k for k-means: Elbow Curve Method and Silhouette analysis ","44003189":"**5. How can you avoid overfitting model**\n* ###   **<span style=\"color:purple\"> What is overfitting**\n    When machine learning algorithms are constructed, they* <span style = \"color:green\">leverage a sample dataset to train* the model. \n    \n    However, when the <span style = \"color:green\">model trains for too long on sample data or when the <span style = \"color:green\"> model is too complex,<span style = \"color:black\"> it can start to learn the \u201cnoise,\u201d or irrelevant information, within the dataset. \n    \n    When the model memorizes the noise and fits too closely to the training set, the model becomes \u201coverfitted,\u201d and <span style = \"color: red\"> it is unable to generalize well to new data. \n* ###  **<span style=\"color:purple\"> How to delect the overfitted model**\n    \n    To understand the accuracy of machine learning models, <span style = \"color:green\">it\u2019s important to test for model fitness. \n    \n    <span style =\"color: green\"> K-fold cross-validation <span style = \"color:black\">is one of the most popular techniques to assess accuracy of the model.\n* ###   **<span style=\"color:purple\"> How to avoid overfitting**\n     Keep model simple: take into account fewer varibales thereby removing some of the noise in the training data\n     \n     Use cross-validation technique such as k-folds cross validation\n        \n     use regularization technique such as LASSO,  L1 regularization, and dropout to identify and reduce the noise within data\n        \n     Train with more data: include more data can increase the accuracy of the model by providing more opportunities to parse out the dominant relationship among the input and output variables. \n        \n     Feature selection: Feature selection is the process of identifying the most important ones within the training data and then eliminating the irrelevant or redundant ones\n     \n     Ensemble methods: made up of a set of classifiers\u2014e.g. decision trees\u2014and their predictions are aggregated to identify the most popular result\n> https:\/\/machinelearningmastery.com\/overfitting-and-underfitting-with-machine-learning-algorithms\/\n        ","360ed458":"14. **Calculating Euclian Distance Example**","64104747":" ## **1. THE ELBOW METHOD**\n\nThe elbow method runs k-means clustering on the dataset for a range of values of k (say 1 to 10).\n\nPerform K-means clustering with all these different values of K. For each of the K values, we calculate average distances to the centroid across all data points.\nPlot these points and find the point where the average distance from the centroid falls suddenly (\u201cElbow\u201d).\n\n### Example1:\n","738d0a36":"13. What is Linear Regression?\n\nLinear Regression is a statistical model used to predict a relationship between explanatory variables and reponse variable\n\nExamine 2 factor: \n\n* Which variables in particular are significant predictors of the outcome variables?\n+ How significant is the regression line to make predictions with highedt possible accuracy?","afbb3d47":"**9. Python code:**\n\nIn your choice of language: write a program that prints the number from 1 to 50. but for multiples of three print \" Fizz\" instead of the number and for the multiples of five print \"Buzz\". For the numbers which are mutiples of both three and five print \"FizzBuzz\"","255a3dbc":"**6. Machine Learning Approach:**\n* Understand proplem\/Dataset\n* Extract feature from the dataset\n* Identify the problem type\n* Choose the right model\n* Train and test the model\n* Strive for the accuracy\n> https:\/\/lms.simplilearn.com\/courses\/2772\/Data-Science-with-Python\/syllabus","5eabbf21":"**2. How Logistic regression done?**\n\nLogistic regression uses an equation as the representation, very much like linear regression.\n\n\nInput values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter Beta) to predict an output value (y). \n\nA key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value.\n> https:\/\/machinelearningmastery.com\/logistic-regression-for-machine-learning\/\n","9873cb8e":"12. **Application of Linear Regression**\n\n* Economic growth: determine economic grwoth of a country or a state in the up coming quarter, year\n* Housing sales: to estimate the number of house a buider would sell and at what price in the coming months\n* Product price: can be used to predict what would be the price of a product in the future\n* Score prediction: to predict the number of runs a player would score in the coming matches based on previous performance\n\n\n","7da3ba5d":"<h1><center><span style ='color:red'>Data Science Interview Question<\/center><\/h1>","d235af52":"**7. Differentiate between Univariate, Bivariate and Multivariate Analysis**\n### <span style = 'color: purple'> Univariate Analysis\n    This type of data contains only **one variable**. The main purpose of the analysis is to **describe the data and find patterns that exist within it**.\n    \n    The most common univariate analysis is central tendency(mean, median, mode) and dispersion or spread of data (range, minimum, maximum, quartiles, variance and standard deviation).\n    \n    The most common visual technique used for univariate analysis is histogram, box plot, pie chart, bar chart.\n\n    ### <span style = 'color:purple'> Bivariate Analysis\n    \n    This type of data contain **two diferrent variables**. \n    \n    The analysis of this type of data **deals with causes and relationships** and the analysis is done to **find out the relationship among the two variables**\n    \n    Bivariate data analysis involves comparisons, relationships, causes and explanations.\n    \n    The most common visual technique for bivariate analysis is a scatter plot where one variable is on the x-axis and the other is on the y-axis.\n    \n    ### <span style = 'color:purple'> Multivariate Analysis\n    \n    This type of data involes 3 or more variables. \n    \n    Multivariate analysis is similar to Bivariate analysis but you are comparing more than two variables.\n    \n    Common Mutivariate analysis are Principle Component Analysis or logistic regression, linear regression, cluster analysis...\n    \n    Common visual technique for multivariate analysis  is pairplots (in seaborns library in Python)\n> https:\/\/www.geeksforgeeks.org\/univariate-bivariate-and-multivariate-data-and-its-analysis\/\n> https:\/\/medium.com\/analytics-vidhya\/univariate-bivariate-and-multivariate-analysis-8b4fc3d8202c    \n","1eb7c01f":"15. **Explain demensionality reduction, and list its benefits**\n\n\n<span style =\"color: green\"> **what is dimenionality reduction?**\n    \nDimensionality reduction refers to techniques for reducing the number of input variables in training data( convey similar information concisely.)\n> https:\/\/machinelearningmastery.com\/dimensionality-reduction-for-machine-learning\/    \n    \nThere are two components of dimensionality reduction:\n\nFeature selection: In this, we try to find a subset of the original set of variables, or features, to get a smaller subset which can be used to model the problem. It usually involves three ways:\n* Filter\n* Wrapper\n* Embedded\n    \nFeature extraction: This reduces the data in a high dimensional space to a lower dimension space, i.e. a space with lesser no. of dimensions.\n\n<span style='color: green'> **Benefits of demensionality reduction**\n* It helps in data compressing and reducing the storage space\n* It reduces computation times\n* It removes redundant features","ecb61bb9":"## Machine Learning: Example of logistic regression","270edbc0":"## **2. SILHOUETTE ANALYSIS**\n\nThe silhouette coefficient is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation).\n\n* The value of the silhouette coef\ufb01cient is between [-1, 1].\n* A score of 1 denotes the best meaning that the data point i is very compact within the cluster to which it belongs and far away from the other clusters.\n* The worst value is -1. Values near 0 denote overlapping clusters.\n\nLet us see the python code with help of an example.","042cc725":"**3. Explain the step in decisions tree:**\n* take entire data as input\n\n* Calculate entropy of target variables as well as predictor attributes\n\n* calcuate information gain of all the attributes\n\n* Choose the attribute with highest information gain as the root node\n\n* repeat the same process on every branch till the decision node of each branch is finallized\n","943d8ac6":"Visually we can see that the optimal number of clusters shoule be around 3. But visualling the data alone cannot always give the right answer. \nWe will find out the maximize clusters using elbow method:\n","413422f1":"**10. You are given a dataset consisting of variables having more than 30% of missing values? how will you deal with them?\n\n\n<span style = 'color: purple'> **How does missing data affect your algorithm?**\n\n- Missing value provive a wrong idea about the data itself, causing ambiguity. For example, we want to calcualate an average of a column with half of the information unavailable gives thr wrong metric.\n- When data is unavailable, some algorithm do not work. \n- If the missing information is systematical, all analysis is bias\n\n<span style = 'color: purple'> **What Causes Missing Data?**\n- Surveys\n- IoT\n- Restricted acess\n- Manual eror\n\n<span style = 'color: purple'>**How to deal with them?**\n    \n*The easiest way to handle missing data is frop the rows or columns where there is missing information using dropna function in python *\n+ data.dropna() # to removes rows with missing data\n+ data.dropna(axis=1) # to removes columns with missing data\n\n*Imputaion is another method to filling missing values with numbers using a specific strategy. Sopme options to consider for imputation are:*\n + Mean, median, mode from that column\n + A distinct value such as 0\n \n*Algorithms that support missing values:*\n + kNN(k-nearest Neighbor)\n + Naive Bayes \n> https:\/\/phoenixnap.com\/kb\/handling-missing-data-in-python\n> https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/03.04-missing-values.html","5c1bd7eb":"19. **How to find RMSE and MSE in linear regression model?**\n\n<span style = 'color: green'>**MSE(Mean Squared Error)**: \n \nMSE represents the difference between the original and predicted values.\n    \nMSE  measures how close a fitted line is to actual data points.\n    \n<span style ='color: green'> **RMSE(Rooted Mean Square Error)**:\n\n RMSE is the error rate by the square root of MSE.\n \n This is the same as MSE but  is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable( Y-axis).\n \n ![RMSE](https:\/\/1.bp.blogspot.com\/-kL42RjXdOEc\/XMELxXVMe3I\/AAAAAAAABRw\/mx2RoIheodwWj0CPAqg9chwXJmpOyPyJQCLcBGAs\/s1600\/Loss_Functions.PNG)","9e11a1f8":"16. **How will you calculate eigen values and egen vectors of 3 by 3 matrix**"}}