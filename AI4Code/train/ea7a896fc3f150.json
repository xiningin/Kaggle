{"cell_type":{"7452a499":"code","22bcf361":"code","953a555b":"code","dbad7f4f":"code","b76c1443":"code","94d5bb27":"code","8e3f8b7d":"code","4638a280":"code","ea2ecba2":"code","a5e6d6a4":"code","78c7b1a5":"code","f0347a6b":"code","ede2ab91":"code","3a5f0826":"code","c4710924":"code","9e898416":"code","92f9c723":"code","1baa1d46":"code","ccf06ece":"code","199df6bb":"code","82678e40":"code","1c6e0720":"code","23640d39":"code","81c71c34":"code","4f5738d5":"code","5fd2a886":"code","caf97525":"code","b670df7c":"code","9556991f":"code","2c5a2ab6":"code","8389fd7f":"code","c279c59a":"code","9e62a6e2":"code","c5dbc75a":"code","f16fa542":"code","47ddb4d5":"code","a857dc69":"code","6006346a":"markdown","142ca4a5":"markdown","1abb9ecb":"markdown","f309ce79":"markdown","cbc95dc7":"markdown","285d1b06":"markdown","d19d659d":"markdown","f875fa72":"markdown"},"source":{"7452a499":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","22bcf361":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","953a555b":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","dbad7f4f":"train_df.shape , test_df.shape","b76c1443":"train_df.head()","94d5bb27":"print(f'There are {train_df.isnull().any().sum()} columns in train dataset with missing values.')\nprint(f'There are {test_df.isnull().any().sum()} columns in test dataset with missing values.')","8e3f8b7d":"sns.pairplot(train_df)","4638a280":"print('Training Data')\nms = train_df['Age'].isnull().sum()\nms1 = train_df['Cabin'].isnull().sum()\nms2 = train_df['Embarked'].isnull().sum()\nprint(f'There are {ms} missing values in train Age column ')\nprint(f'There are {ms1} missing values in train Cabin column ')\nprint(f'There are {ms2} missing values in train Embarked column ')\nprint('--------------------------------------------------------------------')\nprint('Test Data')\nmst = test_df['Age'].isnull().sum()\nmst1 = test_df['Cabin'].isnull().sum()\nmst2 = test_df['Embarked'].isnull().sum()\nprint(f'There are {mst} missing values in train Age column ')\nprint(f'There are {mst1} missing values in train Cabin column ')\nprint(f'There are {mst2} missing values in train Embarked column ')","ea2ecba2":"train_df.Age = train_df.Age.fillna(-999)\ntrain_df['Age'].isna().sum()","a5e6d6a4":"test_df['Age'].isna().sum()","78c7b1a5":"test_df.Age = test_df.Age.fillna(test_df.Age.mean())\ntest_df.Fare = test_df.Fare.fillna(test_df.Fare.mean())\ntest_df['Age'].isna().sum()","f0347a6b":"test_df['Fare'].isna().sum()","ede2ab91":"print(train_df.isna().sum())\nprint('**************************')\nprint(test_df.isna().sum())","3a5f0826":"train_df.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\ntrain_df['Embarked']= train_df['Embarked'].fillna('S')","c4710924":"train_df.head()","9e898416":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset[\"Fare\"].astype(int)","92f9c723":"test_df.drop(['Cabin'], axis=1,inplace=True)","1baa1d46":"test_df.drop(['Name','Ticket'],axis=1,inplace=True)","ccf06ece":"train_df.head()","199df6bb":"sex_dummies_train = pd.get_dummies(train_df['Sex'])\nsex_dummies_test = pd.get_dummies(test_df['Sex'])\nemb_dummies_train = pd.get_dummies(train_df['Embarked'])\nemb_dummies_test = pd.get_dummies(test_df['Embarked'])","82678e40":"train_df = pd.concat([train_df,sex_dummies_train,emb_dummies_train],axis=1)\ntest_df = pd.concat([test_df,sex_dummies_test,emb_dummies_test],axis=1)","1c6e0720":"test_df.info()","23640d39":"test_df.drop(['Sex','Embarked'],axis=1,inplace=True)","81c71c34":"X_train = train_df.drop(['Sex','Embarked'],axis=1)\n","4f5738d5":"data = [X_train, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","5fd2a886":"final_X = X_train.drop('Survived',axis=1)\nfinal_y = X_train['Survived']","caf97525":"final_X.shape,final_y.shape","b670df7c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(final_X,final_y, test_size=0.3,random_state=15)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(final_X,final_y)\nY_pred = model.predict(test_df)\nacc_log = round(model.score(final_X,final_y) * 100, 2)\nacc_log","9556991f":"# import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nclf = XGBClassifier()\n# eval_set = [(X_train, y_train), (X_test, y_test)]\n# eval_metric = [\"auc\",\"error\"]\n\nmodel = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10)\nmodel.fit(final_X,final_y,  verbose=True)\n\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","2c5a2ab6":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(final_X,final_y)\nY_pred = random_forest.predict(test_df)\nrandom_forest.score(final_X,final_y)\nacc_random_forest = round(random_forest.score(final_X,final_y) * 100, 2)\nacc_random = print(round(acc_random_forest,2,), \"%\")\nacc_random","8389fd7f":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(final_X,final_y)\n\nY_predit = decision_tree.predict(test_df)\n\nacc_decision_tree = round(decision_tree.score(final_X,final_y) * 100, 2)\nacc_rando = print(round(acc_decision_tree,2,), \"%\")\nacc_rando","c279c59a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor(random_state=0)\nregressor.fit(final_X,final_y)\nY_pred = regressor.predict(X_test)\nreg_decision_tree = round(regressor.score(final_X,final_y) * 100, 2)\nprint(round(reg_decision_tree,2,), \"%\")","9e62a6e2":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(final_X,final_y)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(final_X,final_y) * 100, 2)\nacc_gaussia = print(round(acc_gaussian,2,), \"%\")\nacc_gaussia","c5dbc75a":"test_df.shape,train_df.shape","f16fa542":"test_df.head()","47ddb4d5":"# models = pd.DataFrame({\n#     'Model': ['Logistic Regression', \n#               'SVC', 'Random forest', 'Gaussian',  \n#               ],\n#     'Score': [acc_log, acc_svc, acc_rando, \n#              acc_gaussia \n#               ]})\n# models.sort_values(by='Score', ascending=False)","a857dc69":"submission = pd.DataFrame({\n    \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_predit\n})","6006346a":"# SVC","142ca4a5":"**Statement: Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster. Can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.****","1abb9ecb":"#### DecisionTreeRegressor","f309ce79":"**Any missing values or nan in our dataset ? lets find out**","cbc95dc7":"# Gaussian Naive Bayes","285d1b06":"# LogisticRegression","d19d659d":"# Random Forest","f875fa72":"# Decision Tree\n#### DecisionTreeClassifier"}}