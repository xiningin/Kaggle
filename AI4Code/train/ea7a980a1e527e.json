{"cell_type":{"9e875060":"code","e705bc1a":"code","68bf0845":"code","be1a2970":"code","ff4857f8":"code","986f03cb":"code","819e53c3":"code","2b85707b":"code","4d3bd709":"code","18303a0f":"code","8ade9f98":"code","94eefce5":"code","abdb3915":"code","ac238b56":"code","6a493cf3":"code","9d000fa7":"code","1e47d1e1":"code","4bcacebb":"code","92daeb7d":"code","e637483f":"code","c862c3a8":"code","5153bbe6":"code","07904992":"code","660d7fe5":"code","3cf88c22":"code","821ed3c9":"code","1cd16613":"code","71cac4e5":"code","e1465138":"code","a1063e6c":"code","4ddea8a9":"code","1a82cea9":"code","d888fc18":"code","0d9f317f":"code","b6bac1c3":"code","673dcdd2":"code","5f925d21":"code","a513b98a":"code","4a47d3e7":"code","5b62753c":"code","6c52a981":"code","fd14659d":"code","1aeb750e":"code","a87c6d32":"code","9b80443e":"code","c36d4710":"code","981e1885":"code","36a73601":"code","086d960e":"code","dee87df8":"code","f3165284":"code","6b5dc336":"code","719a9627":"code","0182de4d":"code","2261da3e":"code","3f36a404":"code","2c3d453b":"code","765ca43c":"code","9c591bfd":"code","bf60a233":"code","9f3c71b5":"code","d4a0cfd5":"code","1e85553a":"code","3ebef2ff":"code","ba8613e2":"code","b4b56a0d":"code","3abca778":"code","c616301d":"code","ea5a6659":"code","65a7ce56":"code","cb517fa2":"code","ed602b6b":"code","93ab8fea":"code","ddf0df0f":"code","b628bef9":"code","1b6ad7b4":"markdown","be8312f2":"markdown","5e6aa516":"markdown","97aab596":"markdown","ad9a3e2c":"markdown","8967ffd1":"markdown","67642a4b":"markdown","c3042e4b":"markdown","80fd4947":"markdown","cb02868d":"markdown","93cc8f10":"markdown","49e10e12":"markdown","1c0c3433":"markdown","ebb3e27c":"markdown","878e175b":"markdown","43b1b35f":"markdown","346f6baa":"markdown","67983692":"markdown","85a8749c":"markdown","d5a339f8":"markdown","4687ccb4":"markdown","b46493a3":"markdown","a3b96df5":"markdown","270fa592":"markdown","19f023dc":"markdown","c1906cd4":"markdown","6c9bb514":"markdown","40b5a577":"markdown","5373d75b":"markdown","8d92cb6a":"markdown","541006a1":"markdown","8f10764b":"markdown","67619248":"markdown","188cec76":"markdown","d115530f":"markdown","988bb29c":"markdown","a6a539fa":"markdown","2d13d750":"markdown","d7bbe52f":"markdown","03bb98e3":"markdown","be217be1":"markdown","53f78c70":"markdown","bcb5caa4":"markdown","e0c69507":"markdown","6e6a59e6":"markdown","488ef2bf":"markdown","a4048eb3":"markdown"},"source":{"9e875060":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Disabling warnings\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\n    \n# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.machine_learning.ex7 import *\n\n\n\n# train data\niowa_file_path = '..\/input\/train.csv'\ntrain_data = pd.read_csv(iowa_file_path)\n\n# test data\ntest_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)","e705bc1a":"df = pd.concat([train_data, test_data], ignore_index=True)\ndf","68bf0845":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","be1a2970":"df['PoolQC'].replace(np.nan, 'No', inplace=True)\ndf['PoolQC'].value_counts(dropna=False)","ff4857f8":"df['Alley'].replace(np.nan, 'No', inplace=True)\ndf['Alley'].value_counts(dropna=False)","986f03cb":"df['MiscFeature'].replace(np.nan, 'No', inplace=True)\ndf['MiscFeature'].value_counts(dropna=False)","819e53c3":"df['GarageType'].replace(np.nan, 'No', inplace=True)\ndf['GarageType'].value_counts(dropna=False)","2b85707b":"# Recomendation\n\n# cast the data types to the correct ones.\n# remove irrelevant columns. (But it is not exactly)\n# process those NaNs about which you know something for certain. !!!(try to skip this point)\n# algorithm don't handle types: category, datetime64, timedelta[ns], maby bool(not tested)\n# algorithm can handle features with 96.30% missing data, but can't handle 99.52% missing data. It DEPENDS on the number of raws.\n\n\n\n\ndef NaN_predict(df,\n                skip_features_with_missing_data_percentage=100, # If error, decrease to 99 or 90\n                include_features_as_predictors_where_pec_miss_data_less=50,): \n    '''iteratively predict missing values in cells'''\n    \n    \n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n\n    import lightgbm as lgb\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import f1_score\n    from sklearn.preprocessing import LabelEncoder\n\n    \n    global counter_all_predicted_values\n    counter_all_predicted_values = 0\n\n\n    def NaN_info(df):\n        global null_view\n        try:\n            null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n            null_view = pd.DataFrame(null_view, columns=['NANs'])\n            null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n            null_view[['TYPE']] = df.dtypes\n        except:\n            return null_view\n        return null_view\n\n\n    def encoding(work_predictors, df):\n        for j in work_predictors:\n            el_type = df[j].dtype\n            if el_type == 'object':\n                df[j].replace(np.nan, '0', inplace=True)\n                labelencoder = LabelEncoder()\n                df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        return df, work_predictors\n\n\n    def predict_regressor(X, y, miss_df):\n        global pred_miss\n        lgbm = lgb.LGBMRegressor()\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 200 predicted missing values: \\n{pred_miss[:200]}\")\n        return pred_miss\n\n\n    def predict_classifier(X, y, miss_df):\n        global pred_miss\n        lgbm = lgb.LGBMClassifier()\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 200 predicted missing values: \\n{pred_miss[:200]}\")\n        return pred_miss\n\n\n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        counter = 0\n        for idx in miss_indeces:\n            df.loc[idx, el] = pred_miss[counter]\n            counter += 1\n        return df\n\n\n\n\n \n    print(NaN_info(df))\n    print('\\n\\n\\n')\n\n    delete_miss_features = list((null_view.loc[null_view['PERCENT'] > skip_features_with_missing_data_percentage]).index)\n    print(f'Exclude from the prediction, because missing data more than \\\n    {skip_features_with_missing_data_percentage}% :\\n{delete_miss_features}')\n    print('')\n    all_miss_features = list(null_view.index)\n\n    for delete_feature in delete_miss_features:\n        all_miss_features.remove(delete_feature)\n\n    lot_of_miss_features = list((null_view.loc[null_view['PERCENT'] > include_features_as_predictors_where_pec_miss_data_less]).index)\n    print(f'Unused as predictor, because missing data more than \\\n    {include_features_as_predictors_where_pec_miss_data_less}% :')\n    print(lot_of_miss_features)\n    print('')\n\n    all_features = list(df.columns)\n    now_predictors = list(set(all_features)-set(lot_of_miss_features))\n\n    df_indeces = list(df.index)\n\n    for el in all_miss_features:    \n        work_predictors = list(set(now_predictors) - set([el]))\n\n        # missing data (data for prediction)\n        miss_indeces = list((df[pd.isnull(df[el])]).index)\n        miss_df = df.iloc[miss_indeces][:]\n        miss_df = miss_df[work_predictors]\n        encoding(work_predictors, df=miss_df)\n\n        # data without NaN rows (X data for train and evaluation of model)\n        work_indeces = list(set(df_indeces) - set(miss_indeces))\n        work_df = df.iloc[work_indeces][:] \n        encoding(work_predictors, df=work_df)\n\n        X = work_df[work_predictors]\n        y = work_df[el]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n        # Info\n        feature_type = df[el].dtypes\n        print('\\n\\n')\n        perc = null_view['PERCENT'][el]\n        print(f'feature: {el},   type: {feature_type},   missing values: {perc}%')    \n        print(f'X.shape: {X.shape},   y.shape: {y.shape}')\n\n        # Predictions\n        if feature_type == 'object' or feature_type == 'bool':   # or feature_type == 'category' - error\n            print('classifier:')\n            predict_classifier(X, y, miss_df)\n            counter_all_predicted_values += len(miss_indeces)\n            imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n        elif feature_type == 'float64' or feature_type == 'int64':\n            print('regressor:')\n            predict_regressor(X, y, miss_df)\n            counter_all_predicted_values += len(miss_indeces)\n            imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n        else:\n            print(f\"unprocessed feature: {el} - {feature_type} type\")\n\n\n    print('\\n\\n\\n')\n    print(f'{counter_all_predicted_values} values have been predicted and replaced')\n    print('\\n')\n    print(f'These features have not been processed, because missing data more than {skip_features_with_missing_data_percentage}%')\n    print(NaN_info(df))\n    print('\\n\\n\\n')\n\n    \n    return df","4d3bd709":"NaN_predict(df)","18303a0f":"df['PoolStatus']   = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['SeFlrStatus']  = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['GarageStatus'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['BsmtStatus']   = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['FirePlaceStatus'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n\ndf['RemodNBuild'] = df['YearBuilt'] + df['YearRemodAdd']\ndf['SF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ndf['AreaInFt'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\ndf['NumBathroom'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                     df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\ndf['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                        df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\ndf.shape","8ade9f98":"df[['RemodNBuild', 'YearBuilt', 'YearRemodAdd']].head(2)","94eefce5":"df[['SF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF']].head(2)","abdb3915":"df[['AreaInFt', 'BsmtFinSF1', 'BsmtFinSF2', '1stFlrSF', '2ndFlrSF']].head(2)","ac238b56":"df[['NumBathroom', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']].head(2)","6a493cf3":"df[['Total_porch_sf', 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF']].head(2)","9d000fa7":"target_column = ['SalePrice']\npredictors = list(set(list(df.columns))-set(target_column))","1e47d1e1":"from sklearn.preprocessing import LabelEncoder\n\n\nfor el in predictors:\n    el_type = df[el].dtype\n    if el_type == 'object':\n        labelencoder = LabelEncoder()\n        df.loc[:, el] = labelencoder.fit_transform(df.loc[:, el])","4bcacebb":"from sklearn import linear_model\n\nfor el in predictors:\n    plt.figure(figsize=(10, 8))\n    sl_regr = linear_model.LinearRegression()\n\n    train_x = np.asanyarray(df[[el]])\n    train_y = np.asanyarray(df.SalePrice)\n    sl_regr.fit(train_x, train_y)\n    \n    plt.scatter(df[[el]], df.SalePrice)\n    plt.plot(train_x, sl_regr.coef_[0]*train_x + sl_regr.intercept_, '-r')\n    \n    plt.xlabel(el)\n    plt.ylabel(\"PRICE\")\n    plt.show()","92daeb7d":"fig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","e637483f":"a = df.loc[df['GrLivArea'] > 4500]\nprint(a['GrLivArea'])\nidx = a['GrLivArea'].index[0]\nprint(idx)","c862c3a8":"print(df.shape)","5153bbe6":"df = df.drop(df.index[[523, 1298]])\n\n# reset index, because we droped some rows \ndf.reset_index(drop=True, inplace=True)\n\nprint(df.shape)","07904992":"fig, ax = plt.subplots()\nax.scatter(df['GrLivArea'], df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","660d7fe5":"fig, ax = plt.subplots()\nax.scatter(df['GarageYrBlt'], df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GarageYrBlt')\nplt.show()","3cf88c22":"a = df.loc[df['GarageYrBlt'] > 2200]\nprint(a['GarageYrBlt'])\nidx = a['GarageYrBlt'].index[0]","821ed3c9":"df.loc[idx,'GarageYrBlt'] = 2007.0\nprint(df.loc[idx,'GarageYrBlt'])","1cd16613":"fig, ax = plt.subplots()\nax.scatter(df['GarageYrBlt'], df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GarageYrBlt')\nplt.show()","71cac4e5":"from scipy import stats\nfrom scipy.stats import norm, skew \n\n\nsns.distplot(df['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(df['SalePrice'], plot=plt)\nplt.show()","e1465138":"print(df['SalePrice'])\ndf['SalePrice'] = np.log1p(df['SalePrice'])\nprint('------------------------------------------------')\nprint(df['SalePrice'])","a1063e6c":"\nsns.distplot(df['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(df['SalePrice'], plot=plt)\nplt.show()","4ddea8a9":"# train target \ny = df.loc[:1457,target_column]\ny = np.ravel(y)                   \n\ndf_y = df[target_column]\ny","1a82cea9":"df = df[predictors] \n","d888fc18":"from sklearn.preprocessing import LabelEncoder\n\nfeature_power = 0.5  # Skew handling\n\nfor el in predictors:\n    el_type = df[el].dtype\n    if el_type == 'object':\n        labelencoder = LabelEncoder()\n        df.loc[:, el] = labelencoder.fit_transform(df.loc[:, el])\n    else:\n        df[el] = df[el]**feature_power","0d9f317f":"NaN_info(df)","b6bac1c3":"NaN_predict(df)","673dcdd2":"for el in predictors:\n    plt.figure(figsize=(10, 8))\n    sl_regr = linear_model.LinearRegression()\n\n    train_x = np.asanyarray(df[[el]])\n    train_y = np.asanyarray(df_y.SalePrice)\n    sl_regr.fit(train_x, train_y)\n    \n    plt.scatter(df[[el]], df_y.SalePrice)\n    plt.plot(train_x, sl_regr.coef_[0]*train_x + sl_regr.intercept_, '-r')\n    \n    plt.xlabel(el)\n    plt.ylabel(\"PRICE\")\n    plt.show()\n    ","5f925d21":"df.drop(['MSSubClass', 'BsmtFinType1', 'BsmtHalfBath', 'Heating', 'RoofMatl', \n         'GarageType', 'Id', 'Electrical', 'MiscFeature', '3SsnPorch', \n         'SeFlrStatus', 'Utilities', 'Condition2', 'Street', 'GarageFinish', \n         'BsmtFinSF1', 'BsmtFullBath', 'GarageCond', 'LandSlope', \n         'MiscVal', 'PoolStatus', '2ndFlrSF', 'LotShape'], axis=1, inplace=True)","a513b98a":"X = df.loc[:1457,:]      # (we droped 2 raws)\nX","4a47d3e7":"y = df_y.loc[:1457,target_column]   # (we droped 2 raws) \ny = np.ravel(y) \ny","5b62753c":"# The data must be preprocessed as before machine learning!!! \n\n# With each loop, cv_score is either incremented or equal. \n# If cv_score is less than the previous one, the loop will stop and will not drop any functures.\n\n\n\ndef features_selection(X, y):\n    '''select the best features for prediction'''\n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n    import lightgbm as lgb\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import cross_val_score\n\n    \n\n    \n    global droped_features\n    droped_features = []\n        \n    \n    def cross_val(X, y, features):\n        CV = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n        test_df = X[features]\n        model = lgb.LGBMRegressor(random_state=0)\n        model.fit(test_df, y)\n        cv_score  = cross_val_score(model, test_df, y, cv=CV)\n        cv_score = cv_score.mean()\n        return cv_score\n    \n    \n    \n    \n    # GO\n    features = list(X.columns)\n    SCORES = pd.DataFrame(index = features, columns=['cv_score'])\n    score_with_all_features = cross_val(X, y, features)\n    print(f'{score_with_all_features}     General cv_score with all features')\n    \n    cv_score_of_monstrous_feature = 1    # :)\n    \n    while score_with_all_features <= cv_score_of_monstrous_feature:\n        print('\\n\\n')\n        features = list(X.columns)\n        SCORES = pd.DataFrame(index = features, columns=['cv_score'])\n        score_with_all_features = cross_val(X, y, features)\n        print(f'{len(features)}                 === number of features')\n        print(f'{score_with_all_features} === score with all features' )\n\n        print('\\n')\n        print('cv_score     without feature')\n        print('')\n        for without_feature in features:\n            without_feature = [without_feature]\n            fit_faetures = features[:]\n            fit_faetures = list(set(fit_faetures) - set(without_feature))\n            score = cross_val(X, y, fit_faetures)\n            SCORES.loc[without_feature] = score\n\n            print(\"{:1.8f}   {:20}  \".format(score, without_feature[0]))\n\n        SCORES = SCORES.sort_values(by=['cv_score'], ascending=False)\n        \n        print('\\n __SORTED SCORES__')\n        print(SCORES)\n        monstrous_feature = SCORES.index[0]\n        cv_score_of_monstrous_feature = SCORES.iloc[0][0]\n        \n        if score_with_all_features <= cv_score_of_monstrous_feature:\n            X.drop([monstrous_feature], axis=1, inplace=True)\n            droped_features.append(monstrous_feature)\n            \n            print('--------------------------------------------')\n            print(f'    DROP   ====    {monstrous_feature}')           # :)\n            print('--------------------------------------------')\n        else:\n            print('\\n\\n')\n            print(f'These features have been droped:\\n{droped_features}')\n            print('\\n\\n')\n            return X, droped_features\n        ","6c52a981":"df.shape","fd14659d":"features_selection(X, y)","1aeb750e":"droped_features","a87c6d32":"df.drop(droped_features, axis=1, inplace=True)","9b80443e":"df.shape","c36d4710":"# train data     (we droped 2 raws)\nX = df.loc[:1457,:]\nX","981e1885":"# data for prediction   (we droped 2 raws)\ntest_X = df.loc[1458:,:]\ntest_X","36a73601":"from lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\n\n","086d960e":"def constriction_hyperparameters_space(X,\n                                       y,\n                                       algorithm,\n                                       params,\n                                       n_iter_RandomizedSearchCV=100,\n                                       times_for_estimate=10):\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.metrics import mean_absolute_error\n\n    \n    print('scores:\\n')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n    CV = ShuffleSplit(n_splits=4, test_size=0.25, random_state=0)\n    \n    estimator = algorithm()\n        \n    RESULTS_DF = None\n    PARAMS_DICT = None\n    PARAMS_DICT = {}\n    for i in range(0, times_for_estimate):\n        RANDOMIZED_MODEL = RandomizedSearchCV(estimator=estimator, \n                                            param_distributions=params, \n                                            n_iter=n_iter_RandomizedSearchCV, \n                                            scoring='neg_mean_squared_error', \n                                            cv=CV, \n                                            verbose=0, \n                                            n_jobs = -1)\n\n\n        RANDOMIZED_FITED_BEST_MODEL = RANDOMIZED_MODEL.fit(X_train, y_train)\n        best_parameters = RANDOMIZED_FITED_BEST_MODEL.best_params_\n        pred_test = RANDOMIZED_FITED_BEST_MODEL.predict(X_test)\n        MAE = mean_absolute_error(y_test,pred_test)\n        print(f'     MAE: {MAE}')\n        \n        model = algorithm(**best_parameters)\n        cv_score  = cross_val_score(model, X, y, cv=CV)\n        std_per = (cv_score.std()) * 2\n        cv_score = cv_score.mean()\n        print(f'cv_score: {cv_score}')\n        print(f'  params: {best_parameters}')\n\n        PARAMS_DICT[cv_score] = best_parameters \n        columns = ['cv_score', 'std_per', 'MAE',]\n        values = []\n        values.append(cv_score)\n        values.append(std_per)\n        values.append(MAE)\n\n        for k,v in best_parameters.items():\n            columns.append(k)\n            values.append(v)\n\n        if RESULTS_DF is None:\n            RESULTS_DF = pd.DataFrame(columns=columns)\n        else:\n            pass\n        RESULTS_DF.loc[i] = values\n        print('\\n')\n    RESULTS_DF = RESULTS_DF.sort_values(by='cv_score', ascending = False)\n    RESULTS_DF.reset_index(drop=True, inplace=True)\n    return RESULTS_DF","dee87df8":"# params  =  {'num_leaves':  np.arange(1, 100001, step=5000),\n#             'max_depth': np.arange(10, 30, step=2),\n#             'learning_rate': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n#             'n_estimators': np.arange(500, 1000, step=100),\n#             'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n#             'feature_fraction': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n#             'bagging_fraction': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n#             'bagging_seed': np.arange(1, 20, step=1),\n#             'lambda_l1': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n#             'lambda_l2': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n#             'min_child_samples': np.arange(5, 105, step=25),\n#             'min_split_gain': [0.00001, 0.0001, 0.001, 0.01, 0.1]\n#             }","f3165284":"params  =  {'num_leaves': np.arange(2000, 3200, step=100),\n            'max_depth': [16, 18, 20, 22, 24],\n            'learning_rate': [0.001, 0.01, 0.1],\n            'n_estimators': np.arange(1000, 2500, step=100),\n            'subsample': [0.3, 0.35, 0.4],\n            'feature_fraction': [0.2, 0.25, 0.3],\n            'bagging_fraction': [0.2, 0.3, 0.4, 0,5],\n            'bagging_seed': [10, 12, 14, 16, 18],\n            'lambda_l1': [0.2, 0.3, 0.4],\n            'lambda_l2': [0.1, 0.2, 0.3],\n            'min_child_samples': [20, 24, 28, 32, 36],\n            'min_split_gain': [0.00001, 0.0001, 0.001, 0.01],\n            'n_jobs':[-1],\n            }\n\n\nconstriction_hyperparameters_space(X,\n                                   y,\n                                   LGBMRegressor,\n                                   params,\n                                   n_iter_RandomizedSearchCV=10,\n                                   times_for_estimate=5)","6b5dc336":"# params  =  {'subsample': [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n#             'learning_rate': [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n#             'max_depth': [0, 4, 5, 6, 7, 8, 10, 12, 14, 16, 18, 20, 22, 25, 28, 30],\n#             'gamma': [0, 1, 2, 3, 5, 10, 20, 50, 200],\n#             'n_estimators': np.arange(500, 5000, step=500),\n#             'colsample_bytree': [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n#             'colsample_bylevel': [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n#             'colsample_bynode': [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n#             'min_child_weight': [1, 2, 3, 5, 10, 20],\n#             'seed': np.arange(5, 50, step=5),\n#             'reg_alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n#             }","719a9627":"# params  =  {'subsample': [.4],\n#             'learning_rate': [0.005, 0.01],\n#             'max_depth': np.arange(8, 18, step=2),\n#             'gamma': [0.01],\n#             'n_estimators': np.arange(2800, 3200, step=100),\n#             'colsample_bytree': [.7, .75, .8],\n#             'colsample_bylevel': [.25, .3],\n#             'colsample_bynode': [0.5, 0.6],\n#             'min_child_weight': [1],\n#             'seed': np.arange(30, 35, step=1),\n#             'reg_alpha': [0.001],\n#             'n_jobs':[-1],\n#             }\n\n\n# constriction_hyperparameters_space(X,\n#                                    y,\n#                                    XGBRegressor,\n#                                    params,\n#                                    n_iter_RandomizedSearchCV=100,\n#                                    times_for_estimate=5) \n","0182de4d":"# params   = {'n_estimators': np.arange(100, 10000, step=100),\n#             'learning_rate': [.01, .05, 0.1, .2, .3, .4, .5], \n#             'max_depth': np.arange(1, 20, step=1),         \n#             'max_features': ['sqrt'],                                  \n#             'min_samples_leaf': [5, 10, 15, 20, 25],                  \n#             'min_samples_split': [5, 10, 15, 20, 25],                  \n#             'loss': ['ls', 'lad', 'huber', 'quantile'],\n#             'criterion': ['friedman_mse', 'mse', 'mae'],\n#             }","2261da3e":"# params   = {'n_estimators': [1000],\n#             'learning_rate': [.01], \n#             'max_depth': [4, 6, 8, 10, ], \n#             'max_features': ['sqrt'], \n#             'min_samples_leaf': [10, 12,], \n#             'min_samples_split': [10, 20, 25], \n#             'loss': ['huber'],\n#             'criterion': ['mae'],\n#             }\n\n\n# constriction_hyperparameters_space(X,\n#                                    y,\n#                                    GradientBoostingRegressor,\n#                                    params,\n#                                    n_iter_RandomizedSearchCV=10,\n#                                    times_for_estimate=3)\n","3f36a404":"# params  = { 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n#             'leaf_size': [2, 5, 10, 15, 20, 30, 40, 50],\n#             'weights': ['uniform', 'distance'],\n#             'metric': ['euclidean', 'manhattan'],\n#             'n_neighbors': [2, 3, 4, 5, 6, 8, 10, 12, 14, 16],\n#             }","2c3d453b":"# params  = { 'algorithm': ['auto'],\n#             'leaf_size': [10],\n#             'weights': ['distance'], \n#             'metric': ['manhattan'],\n#             'n_neighbors': [20],\n#             'n_jobs':[-1], \n#             }\n\n# constriction_hyperparameters_space(X,\n#                                    y,\n#                                    KNeighborsRegressor,\n#                                    params,\n#                                    n_iter_RandomizedSearchCV=1,\n#                                    times_for_estimate=1)","765ca43c":"# params = {'bootstrap': [True, False],\n#          'max_depth': [10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n#          'max_features': ['auto', 'sqrt'],\n#          'min_samples_leaf': [1, 2, 3, 4, 5, 7, 9, 11, 13, 15],\n#          'min_samples_split': [2, 3, 4, 5, 7, 8, 10],\n#          'n_estimators': [200, 400, 800, 1200, 1600, 2000],\n#          'max_leaf_nodes' : [, 2, 3, 4, 5, 7, 9, 11, 13, 15],\n#          'max_samples' : [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n#          'n_jobs':[-1],\n#          'oob_score':[True]\n#          } ","9c591bfd":"# params = {'bootstrap': [True],\n#          'max_depth': [None],\n#          'max_features': ['sqrt'], \n#          'min_samples_leaf': [1],\n#          'min_samples_split': [4],\n#          'n_estimators': [400, 800, 1200, 1600, 2000, 2400, 2800],   \n#          'n_jobs':[-1],\n#          'oob_score':[True],\n#          }\n\n\n\n# constriction_hyperparameters_space(X,\n#                                    y,\n#                                    RandomForestRegressor,\n#                                    params,\n#                                    n_iter_RandomizedSearchCV=5,\n#                                    times_for_estimate=5)\n","bf60a233":"\n# params = {'learning_rate': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n#           'depth': [6, 7, 8, 10, 12, 14, 16, 18, 20, 22, 24],\n#           'l2_leaf_reg': [1, 3, 5, 7, 9],\n#           'rsm' : [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n          \n#           } \n\n# model = CatBoostRegressor()\n# randomized_search_result = model.randomized_search(params, \n#                                                    X, \n#                                                    y, \n#                                                    plot=False, \n#                                                    verbose=False, \n#                                                    n_iter=10,\n#                                                    )\n\n# best_parameters = randomized_search_result['params']\n# best_parameters\n","9f3c71b5":"\n# best_parameters = {'depth': 6, 'l2_leaf_reg': 7, 'rsm': 0.6, 'learning_rate': 0.01}\n# model = CatBoostRegressor(**best_parameters)\n# model.fit(X, y)\n# pred_test = np.expm1(model.predict(test_X))\n# pred_test[:10]\n","d4a0cfd5":"\nBAGGING_DF = pd.DataFrame()\n\ndef predictions(X,           # train\n                y,           # train\n                test_X,      # test  for final prediction\n                BAGGING_DF, \n                algorithm,\n                params,\n                n_iter_RandomizedSearchCV=5,\n                times_for_estimate=5):\n    \n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.metrics import mean_absolute_error\n    \n    \n    \n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n    CV = ShuffleSplit(n_splits=4, test_size=0.25, random_state=0)\n    estimator = algorithm(random_state=0)   \n    PRED_df = pd.DataFrame()\n    \n    print('MAEs:')\n    for i in range(0, times_for_estimate):\n        RANDOMIZED_MODEL_MSE = RandomizedSearchCV(estimator=estimator, \n                                                    param_distributions=params, \n                                                    n_iter=n_iter_RandomizedSearchCV, \n                                                    scoring='neg_mean_squared_error',\n                                                    cv=CV, \n                                                    verbose=0, \n                                                    n_jobs = -1)\n\n        RANDOMIZED_FITED_BEST_MODEL = RANDOMIZED_MODEL_MSE.fit(X_train, y_train)\n        best_parameters = RANDOMIZED_FITED_BEST_MODEL.best_params_\n\n        pred_test = RANDOMIZED_FITED_BEST_MODEL.predict(X_test)\n        MAE = mean_absolute_error(y_test,pred_test)\n        print(MAE)\n\n        best = algorithm(**best_parameters, random_state=0)\n        best.fit(X, y)\n        pred_test = np.expm1(best.predict(test_X))\n        PRED_df[[MAE]] = pd.DataFrame(pred_test)  \n\n    a = list(PRED_df.columns)\n    a.sort(reverse=False)\n    \n    from_ = 1\n    to = int(times_for_estimate\/1.3)\n    a = a[from_:to]\n    \n    test_preds = PRED_df[a[:]].mean(axis=1)\n    BAGGING_DF[[str(algorithm.__module__)]] = pd.DataFrame(test_preds)\n    test_preds = test_preds.to_numpy()\n    return BAGGING_DF, test_preds","1e85553a":"params  =  {'num_leaves': np.arange(2000, 3200, step=100),\n            'max_depth': [18, 20, 22],\n            'learning_rate': [0.01],\n            'n_estimators': np.arange(1000, 2500, step=100),\n            'subsample': [0.35, 0.4],\n            'feature_fraction': [0.25],\n            'bagging_fraction': [0.3,],\n            'bagging_seed': [14],\n            'lambda_l1': [0.2,],\n            'lambda_l2': [0.1,],\n            'min_child_samples': [28,],\n            'min_split_gain': [0.00001],\n            'n_jobs':[-1],\n            }\n\npredictions(X,\n            y,\n            test_X,\n            BAGGING_DF,\n            LGBMRegressor,\n            params,\n            n_iter_RandomizedSearchCV=5,\n            times_for_estimate=10)","3ebef2ff":"params  =  {'subsample': [.4],\n            'learning_rate': [0.005, 0.01],\n            'max_depth': np.arange(8, 18, step=2),\n            'gamma': [0.01],\n            'n_estimators': np.arange(2800, 3200, step=100),\n            'colsample_bytree': [.7, .75, .8],\n            'colsample_bylevel': [.25, .3],\n            'colsample_bynode': [0.5, 0.6],\n            'min_child_weight': [1],\n            'seed': np.arange(30, 35, step=1),\n            'reg_alpha': [0.001],\n            'n_jobs':[-1],\n            }\n\npredictions(X,\n            y,\n            test_X,\n            BAGGING_DF,\n            XGBRegressor,\n            params,\n            n_iter_RandomizedSearchCV=5,\n            times_for_estimate=10)","ba8613e2":"params   = {'n_estimators': [1000],\n            'learning_rate': [.01], \n            'max_depth': [4, 6, 8, 10], \n            'max_features': ['sqrt'], \n            'min_samples_leaf': [10, 12,], \n            'min_samples_split': [10, 25], \n            'loss': ['huber'],\n            'criterion': ['mae'],\n            }\n\npredictions(X,\n            y,\n            test_X,\n            BAGGING_DF,\n            GradientBoostingRegressor,\n            params,\n            n_iter_RandomizedSearchCV=3,\n            times_for_estimate=6)","b4b56a0d":"# params  = { 'algorithm': ['auto'],\n#             'leaf_size': [10],\n#             'weights': ['distance'], \n#             'metric': ['manhattan'],\n#             'n_neighbors': [20],\n#             'n_jobs':[-1], \n#             }\n\n# predictions(X,\n#             y,\n#             test_X,\n#             BAGGING_DF,\n#             KNeighborsRegressor,\n#             params,\n#             n_iter_RandomizedSearchCV=1,\n#             times_for_estimate=1) ","3abca778":"# params = {'bootstrap': [True],\n#          'max_depth': [None],\n#          'max_features': ['sqrt'], \n#          'min_samples_leaf': [1, 2],\n#          'min_samples_split': [4],\n#          'n_estimators': [400, 800, 1200,  2000, 2400],  \n#          'n_jobs':[-1],\n#          'oob_score':[True],\n#          }\n\n\n# predictions(X,\n#             y,\n#             test_X,\n#             BAGGING_DF,\n#             RandomForestRegressor,\n#             params,\n#             n_iter_RandomizedSearchCV=4,\n#             times_for_estimate=5) ","c616301d":"# best_parameters = {'depth': 6, 'l2_leaf_reg': 7, 'rsm': 0.6, 'learning_rate': 0.01}\n# model = CatBoostRegressor(**best_parameters)\n# model.fit(X, y)\n# test_preds = np.expm1(model.predict(test_X))\n# BAGGING_DF[['CatBoostReg']] = pd.DataFrame(test_preds) ","ea5a6659":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import ShuffleSplit\n\nCV = ShuffleSplit(n_splits=20, test_size=0.25, random_state=0)","65a7ce56":"from sklearn.linear_model import RidgeCV\n\n\nalpha = [14.5, 14.6, 14.7, 14.8, 14.9, 15, \n         15.1, 15.2, 15.3, 15.4, 15.5, 15.6]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alpha, \n                              cv=CV)\n                      )\n\nridge.fit(X, y)\ntest_preds = np.expm1(ridge.predict(test_X))\nBAGGING_DF[['RidgeCV']] = pd.DataFrame(test_preds)\ntest_preds ","cb517fa2":"from sklearn.linear_model import LassoCV\n\nalpha = [5e-05, 0.0001, 0.0002, 0.0003, \n         0.0004, 0.0005, 0.0006, 0.0007]\n\nlasso = make_pipeline(RobustScaler(), \n                      LassoCV(alphas=alpha, \n                              max_iter=1e7, \n                              random_state=42, \n                              cv=CV)\n                      )\n\nlasso.fit(X, y)\ntest_preds = np.expm1(lasso.predict(test_X))\nBAGGING_DF[['LassoCV']] = pd.DataFrame(test_preds)\ntest_preds","ed602b6b":"# from sklearn.linear_model import ElasticNetCV\n\n# alpha = [0.0001, 0.0002, 0.0003, \n#              0.0004, 0.0005, 0.0006, 0.0007]\n\n# e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# elasticnet = make_pipeline(RobustScaler(), \n#                            ElasticNetCV(max_iter=1e7, \n#                                         alphas=alpha, \n#                                         cv=CV, \n#                                         l1_ratio=e_l1ratio)\n#                             ) \n\n# elasticnet.fit(X, y)\n# test_preds = np.expm1(elasticnet.predict(test_X))\n# BAGGING_DF[['ElasticNetCV']] = pd.DataFrame(test_preds)\n# test_preds ","93ab8fea":"BAGGING_DF","ddf0df0f":"test_preds = BAGGING_DF.mean(axis=1)\ntest_preds = test_preds.to_numpy()\ntest_preds","b628bef9":"output = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","1b6ad7b4":"# Function for constriction of hyperparameters space","be8312f2":"### CatBoostRegressor prediction","5e6aa516":"## Hi everyone\n## Everything that you see in this notebook you can freely take, copy, modify,  and create your own great solutions!\n\n## Don't forget to upvote and follow me if this notebook or functions were useful for you. \ud83d\ude0a\n\n\n## Please feel free to comment and criticize this code.","97aab596":"# Work on outliers","ad9a3e2c":"### CatBoostRegressor tuning","8967ffd1":"### GradientBoostingRegressor tuning","67642a4b":"# Algorithm for NaN prediction","c3042e4b":"### Removed features already known to me in advance to reduce data output in the terminal and runtime.","80fd4947":"# Thanks a lot to the Kaggle team for their coolest project!","cb02868d":"### RandomForestRegressor prediction","93cc8f10":"### ElasticNetCV prediction","49e10e12":"### RandomForestRegressor tuning","1c0c3433":"## Please feel free to contact me [on LinkedIn](https:\/\/www.linkedin.com\/in\/artyomkolas\/)","ebb3e27c":"### LassoCV prediction","878e175b":"# Example of constriction of hyperparameters space (cyclical process)   \n### After the first return you should constrict hyperparameters space and repeat again\n### Output is sorted by cv_scores","43b1b35f":"![%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202020-11-26%20%D0%B2%2005.44.56.png](attachment:%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202020-11-26%20%D0%B2%2005.44.56.png)","346f6baa":"# Skew handling for target (borrowed)","67983692":"# Predictions","85a8749c":"# Downloading","d5a339f8":"### LGBM prediction","4687ccb4":"# Data overview and preprocessing","b46493a3":"# GRATITUDE","a3b96df5":"# Data for prediction","270fa592":"### KNeighborsRegressor prediction","19f023dc":"# Housing Prices Competition","c1906cd4":"### KNeighborsRegressor tuning","6c9bb514":"### GradientBoostingRegressor prediction","40b5a577":"# Recomendation :)\n\n- cast the data types to the correct ones.   \n- remove irrelevant columns. (But it is not exactly)\n- process those NaNs about which you know something for certain. !!!(try to skip this point)\n- algorithm don't handle types: category, datetime64, timedelta[ns], maby bool(not tested)\n- algorithm can handle features with 96.30% missing data, but can't handle 99.52% missing data. It DEPENDS on the number of raws.  ","5373d75b":"# DISCLAIMER","8d92cb6a":"### ! we should recall that indexes of raws 0:1460 are train data and we can drop only included these raws in 0:1460 as outliers","541006a1":"### RidgeCV prediction","8f10764b":"# Borrowed Feature Engineering","67619248":"# Preprocessing","188cec76":"### LGBM tuning","d115530f":"# Features selection","988bb29c":"### What's new in this notebook?\n- Algorithm for NaN prediction (function)  [15 times faster than before]\n- Feature selection algorithm (function)\n- My approach for tuning of hyperparameters space (function)\n- My approach in prediction (function)","a6a539fa":"# Encoding","2d13d750":"# Thank's a lot these guys for their best practices! \nKirill Aleksandrovich  https:\/\/www.kaggle.com\/aleksandrovich\/top-8-ensemble-of-models\/comments?select=submission.csv   \nSonni Simmelsgaard  https:\/\/www.kaggle.com\/sonnihs\/house-prices\/notebook?select=1_submission.csv","d7bbe52f":"# NEW (my personal work)","03bb98e3":"# Hyperparameters tuning","be217be1":"# STORY","53f78c70":"# Example of features selection","bcb5caa4":"### XGBoost prediction","e0c69507":"### XGBoost tuning","6e6a59e6":"###  !!!  The data must be preprocessed as before machine learning.\n\nWith each loop, cv_score is either incremented or equal.\nIf cv_score is less than the previous one, the loop will stop and will not drop any functures.","488ef2bf":"### This function and approach may not be the best practice","a4048eb3":"# Contact for joint processing"}}