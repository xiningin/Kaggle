{"cell_type":{"a7488b53":"code","b16a1a4b":"code","b2bb2844":"code","29c5742d":"code","5055e1ef":"code","38cff8fb":"code","c76e4e4d":"code","3c982267":"code","fce132ed":"code","89635385":"code","28e4f7fe":"code","754aeb4b":"code","643aedb3":"code","a03bac10":"code","3354ec5e":"code","479763c5":"code","ac72d287":"code","9826097b":"code","5881dff0":"code","f077c57b":"markdown","8f1100c8":"markdown"},"source":{"a7488b53":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns\nsns.set(style='darkgrid')\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b16a1a4b":"nltk.download('stopwords')","b2bb2844":"DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndf = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\ndf.head()","29c5742d":"df.shape","5055e1ef":"mapping = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return mapping[int(label)]\ndf.target = df.target.apply(lambda x: decode_sentiment(x))","38cff8fb":"df.head()","c76e4e4d":"## Plotting the bar char to identify the frequnecy of values\nsns.countplot(df.target)\n##prinitng number of values for each type\nprint(df.target.value_counts())","3c982267":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\n","fce132ed":"df.text = df.text.apply(lambda x: preprocess(x))\ndf=df.filter(['target','text'], axis=1)","89635385":"df_train, df_test = train_test_split(df, test_size=1-0.8, random_state=42)\nprint(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","28e4f7fe":"documents = [_text.split() for _text in df_train.text] ","754aeb4b":"w2v_model = gensim.models.word2vec.Word2Vec(size=300, \n                                            window=5, \n                                            min_count=5, \n                                            workers=3)","643aedb3":"w2v_model.build_vocab(documents)","a03bac10":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","3354ec5e":"%%time\nw2v_model.train(documents, total_examples=len(documents), epochs=20)","479763c5":"w2v_model.most_similar(\"love\")","ac72d287":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)","9826097b":"%%time\nSEQUENCE_LENGTH=300\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)","5881dff0":"POSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nlabels = df_train.target.unique().tolist()\nlabels.append(NEUTRAL)\nencoder = LabelEncoder()\nencoder.fit(df_train.target.tolist())\n\ny_train = encoder.transform(df_train.target.tolist())\ny_test = encoder.transform(df_test.target.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","f077c57b":"<h5>Gensim Word2VEc paramters <\/h5>\nsize: The number of dimensions of the embeddings and the default is 100. <br\/>\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.<br\/>\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.<br\/>\nworkers: The number of partitions during training and the default workers is 3.<br\/>","8f1100c8":"I tried to learn Deep learning sentiment analysis using the notbook of https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis"}}