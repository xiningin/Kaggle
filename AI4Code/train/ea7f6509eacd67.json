{"cell_type":{"a834b278":"code","6d3ce6e0":"code","bb68517a":"code","6b2528e5":"code","bb4832e8":"code","cdfa9846":"code","f4e91ca8":"code","d860f1fc":"code","2e082ca6":"code","300be375":"code","bda6e9c9":"code","f20c71e5":"code","fc95d34c":"code","9c9e178d":"code","5ee5b334":"code","6122fa86":"code","3a9081a3":"code","b0732acd":"code","8b10892b":"code","245cdc42":"code","77a6713a":"code","050c4afc":"code","652b920d":"code","696947a3":"code","3f299408":"code","d71b0429":"code","5dd359de":"code","ea07929f":"code","7cbf48b1":"code","b610beec":"code","c6ad9be3":"code","bf8648a8":"code","25a96b6f":"code","62daad15":"code","a039bbbd":"code","d3fbbb7c":"code","ab257e7e":"code","037320e2":"code","2b6f758d":"code","e75d99d2":"code","0e58c184":"code","542008a0":"code","be853675":"code","d28dd6bc":"code","b9b73a42":"code","b16f6aaa":"code","a915829c":"code","8fc8bf99":"code","8db36c59":"code","cce337b4":"code","f3e31e66":"code","efe78d75":"code","9871ae62":"code","900d75ab":"code","80317850":"code","32bd93d1":"code","6d8a0752":"code","c4a7b570":"code","e628ff71":"code","f3a1bb85":"code","d8cd8777":"markdown","548d8b03":"markdown","5a531179":"markdown","11a8abc7":"markdown","96205922":"markdown","186fb018":"markdown","f7dbab7b":"markdown","df44a8ef":"markdown","c3df75ac":"markdown","382af2a5":"markdown","7f25d201":"markdown","4b44ea9d":"markdown","aad6c974":"markdown","ad5ab135":"markdown","67d1e9e7":"markdown","4ff492f2":"markdown","8a764aa5":"markdown","6ca1db51":"markdown","9d68aeb3":"markdown","65535bb2":"markdown","3bb5c2e6":"markdown","9f6b016b":"markdown","d41037ab":"markdown","71eed805":"markdown","e1da4d0b":"markdown","4bcd391d":"markdown","f035859a":"markdown","869de32d":"markdown","190e906b":"markdown","db1780a3":"markdown","5b9c51e4":"markdown","9bdf1cc3":"markdown","ea62d71c":"markdown","8b21f4dc":"markdown","865ed44c":"markdown","8a724595":"markdown","caa67bb7":"markdown","f372e7bf":"markdown","65cecf00":"markdown","0f81d4cd":"markdown","f6949335":"markdown","66f00581":"markdown","2ebfb031":"markdown","c5ae7aea":"markdown","3b9de1ec":"markdown","49939b7d":"markdown","93c8b725":"markdown","cf82ec77":"markdown","ca270b35":"markdown","b1ca91f9":"markdown","6c0e7cba":"markdown","c967ca20":"markdown","28fcef15":"markdown","027a610a":"markdown","2a8c43b5":"markdown","cf3148b0":"markdown","b5aa89cc":"markdown"},"source":{"a834b278":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data preprocessing \nfrom sklearn.preprocessing import MinMaxScaler\n\n#Feature selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# Models\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n#Model parameters selection\nfrom sklearn.model_selection import RandomizedSearchCV","6d3ce6e0":"# Read datasets\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","bb68517a":"train.head(5)","6b2528e5":"test.head(5)","bb4832e8":"train.isnull().sum()","cdfa9846":"test.isnull().sum()","f4e91ca8":"train.describe()","d860f1fc":"test.describe()","2e082ca6":"df_combined = train.append(test).reset_index()\ndf_combined.drop('index', 1, inplace = True)","300be375":"df_combined['Name'].head(10)","bda6e9c9":"def get_title(df):\n    df['Title'] = df['Name'].str.extract(r'(\\,\\s\\w+\\W\\s)')\n    df['Title'] = df['Title'].str.replace(', ','', regex=True)\n    return \n\ndef get_first_name(df):\n    df['First_Name'] = df['Name'].str.extract(r'(\\.\\s.*$)')\n    df['First_Name'] = df['First_Name'].str.replace('.','', regex=True)\n    return\n\ndef get_second_name(df):\n    df['Second_Name'] = df['Name'].str.extract(r'(\\w+\\,)')\n    df['Second_Name'] = df['Second_Name'].str.replace(',','', regex=True)\n    return\n\nget_title(df_combined) \nget_first_name(df_combined)\nget_second_name(df_combined)","f20c71e5":"#One record does not fit regex for 'Title':\ndf_combined[pd.isnull(df_combined['Title'])]","fc95d34c":"#Fill in this value manually:\ndf_combined[['Title']] = df_combined[['Title']].fillna(value = 'Countess')","9c9e178d":"def get_husbands_name(df):\n    \n    df['Husband_Name'] = df['Name'].str.extract(r'(\\.\\s\\w.*\\()')\n    df['Husband_Name'] = df['Husband_Name'].str.replace(r'(\\.\\s)','', regex=True)\n    df['Husband_Name'] = df['Husband_Name'].str.replace(r'(\\s\\()','', regex=True)\n    #Combine first and second name, title of a husband\n    df['Husband_Name'] = df['Second_Name'] + str(', Mr. ') + df['Husband_Name'].str.replace(r'(\\s\\()','',\n                                                                                            regex=True)    \n    return\n\ndef spouse_on_board(df):\n    \n    #Split dataframe into 2 dataframes for men and women each\n    males_only = df[df['Sex'] == 'male'].copy()\n    females_only = df[df['Sex'] == 'female'].copy()\n    \n    #Check if a female passenger has a husband on board, if no one is found - set value to zero\n    df['Has_Husband'] = females_only['Husband_Name'].isin(males_only['Name']).astype(int)\n    df['Has_Husband'].fillna(value=0, inplace=True)\n    \n    #Check if a male passenger has a wife on board, if no one is found - set value to zero\n    df['Has_Wife'] = males_only['Name'].isin(females_only['Husband_Name']).astype(int)\n    df['Has_Wife'].fillna(value=0, inplace=True)\n    \n    #Combine results in order to get information about spouses on board (regardles of their gender)\n    df['Spouse_on_Board'] = df['Has_Husband'] + df['Has_Wife']\n    return\n\nget_husbands_name(df_combined)\nspouse_on_board(df_combined)","5ee5b334":"df_combined['Ticket'].head(5)","6122fa86":"def clean_tickets(df, target_column, new_column):\n    #Drop any non-word character OR digits -> get prefixes\n    df[new_column[0]] = df[target_column].replace(to_replace='\\W+|\\d+', value='', regex=True)\n    \n    #Drop all characters before whitespace (including this whitespace) OR drop all letter characters  \n    # -> get ticket numbers only\n    df[new_column[1]] = df[target_column].replace(to_replace='\\S+\\s|[a-zA-Z]', value='', regex=True)\n    \n    #If there are no ticket numbers, replace NaN with 1\n    df[new_column[1]] = df[new_column[1]].replace(to_replace='', value='1', regex=True).astype(int)\n    return\n\nclean_tickets(df=df_combined, \n              target_column='Ticket', new_column=['Ticket_Pref', 'Ticket_Num'])\n\ndef apply_scaler(scaler, df, target_column, new_column):\n    \n    scaler.fit(df[target_column].values.reshape(-1, 1))\n    df[new_column] = scaler.transform(df[target_column].values.reshape(-1, 1))\n    return\n    \napply_scaler(scaler=MinMaxScaler(),\n             df=df_combined,\n             target_column='Ticket_Num', new_column='Ticket_Num_Scaled')","3a9081a3":"#Calculate group size of passengers who travelled with the same joint ticket\ndf_combined['Group_Size'] = df_combined.groupby('Ticket_Num')['PassengerId'].transform('size')\n\n#Sum number of Parch and SibSp to calculate family size of each passenger\ndf_combined['Family_Size'] = df_combined['Parch'] + df_combined['SibSp']\n\n#Choose max value between group size and family size to gather groups of passengers who travelled together\ndf_combined['Travelled_Together'] = df_combined[['Group_Size', 'Family_Size']].max(axis=1)","b0732acd":"# Child = 1, Adult = 0\ndef is_child(df, target_column, new_column):\n    \n    df[new_column] = np.nan\n    \n    for age, family, title, parents in zip(df[target_column[0]].iteritems(),\n                                            df[target_column[1]].iteritems(),\n                                            df[target_column[2]].iteritems(),\n                                            df[target_column[3]].iteritems()\n                                           ):\n    \n        #passengers who are 14 y.o. and under\n        if age[1] <= 14:\n            df.loc[age[0], new_column] = 1\n        #passengers who are older than 14 y.o.    \n        elif age[1] > 14:\n            df.loc[age[0], new_column] = 0\n        #passengers with title \"Master\"    \n        elif (title[1] == 'Master. '):\n            df.loc[title[0], new_column] = 1   \n        #passengers with title \"Mr.\" or \"Mrs.\", or \"Dr.\"\n        elif (title[1] == 'Mr. ' or title[1] == 'Mrs. ' or title[1] == 'Dr. '):\n            df.loc[title[0], new_column] = 0\n        #family size is 1 person    \n        elif family[1] == 1:\n            df.loc[family[0], new_column] = 0\n        #passengers who travelled with at least 1 parent\n        elif parents[1] > 0:\n            df.loc[parents[0], new_column] = 1    \n        else:\n            df.loc[age[0], new_column] = 0\n    return\n\nis_child(df_combined, \n         target_column = ['Age', 'Travelled_Together', 'Title', 'Parch'], \n         new_column='Is_Child')","8b10892b":"def get_deck(df):\n\n    #Replace all characters except the first one to get name of a deck\n    df['Deck'] = df['Cabin'].replace(to_replace='\\B.+', value='', regex=True)\n    df['Deck'] = df['Deck'].str.extract(r'(^\\w{1})')\n    return\n\nget_deck(df_combined)","245cdc42":"#Calculate fare per each passenger\ndf_combined['Fare_per_Passenger'] = df_combined['Fare'] \/ df_combined['Group_Size']","77a6713a":"#Convert categorical values into numerical\nsex_dict = {'male' : 0, 'female' : 1}\ndf_combined['Sex_Scaled'] = df_combined['Sex'].map(sex_dict)","050c4afc":"#Check missing values one more time\ndf_combined.isnull().sum()","652b920d":"df_combined['Embarked'].unique()","696947a3":"# Look for NaN values in 'Embarked' column\ndf_combined[pd.isnull(df_combined['Embarked'])][['PassengerId', 'Name', 'Embarked', 'Fare_per_Passenger']]","3f299408":"#Dataframe with NaN values in'Embarked' that should be predicted\nEmbarked_test_data = df_combined[pd.isnull(df_combined['Embarked'])].copy() \n\n#Dataframe with first-class passengers only\nfirst_Pclass = df_combined[df_combined['Pclass'] == 1].copy()\n#Drop our 'test' data - remove rows with NaN values in 'Embarked'\nfirst_Pclass.dropna(subset=['Embarked'], inplace=True)\n#Dataframe with 'train' data\nEmbarked_train_data = first_Pclass.copy()","d71b0429":"#Quantile fare values for each port of embarkation\nfare_quantiles = Embarked_train_data.groupby(['Embarked'])['Fare_per_Passenger'].quantile([0.25, 0.5, 0.75])\n\nports = ['S', 'C', 'Q']\n\ndef boxplot_embarkation_fees_of_first_Pclass(df, ports):\n    embarkation_fees = []  \n    for port in ports:\n        embarkation_fee = df['Fare_per_Passenger'][df['Embarked'] == port]\n        embarkation_fees.append(embarkation_fee)\n    plt.boxplot(embarkation_fees, labels=ports, patch_artist=True)\n    plt.grid(color = 'gray', linestyle = '--')\n    return                 \n\nprint(\"Quantile 'Fare_per_Passenger' values for each port of embarkation:\\n {}\".format(fare_quantiles))\nboxplot_embarkation_fees_of_first_Pclass(Embarked_train_data, ports)","5dd359de":"Embarked_train_data_with_Deck = Embarked_train_data.copy()\nEmbarked_test_data_with_Deck = Embarked_test_data.copy()\n\n#Combine train and test data to get the same dummy codes, test data are in two last rows\nEmbarked_all_data_with_Deck = Embarked_train_data_with_Deck.append(Embarked_test_data_with_Deck)\nEmbarked_all_data_with_Deck.reset_index(inplace = True)\n\n#Drop rows with NaN in 'Deck'\nEmbarked_all_data_with_Deck.dropna(subset=['Deck'], inplace=True)\n\n#Get dummies for the 'Deck' data\nDeck_dummies = pd.get_dummies(Embarked_all_data_with_Deck['Deck'], prefix='Deck')\nEmbarked_all_data_with_Deck = Embarked_all_data_with_Deck.merge(Deck_dummies, left_index=True, right_index=True)\n\n#Split dataframe into train and test sets\nEmbarked_train_data_with_Deck = Embarked_all_data_with_Deck[:254]\nEmbarked_test_data_with_Deck = Embarked_all_data_with_Deck[254:]","ea07929f":"#Model 1\nX_train = Embarked_train_data[['Fare_per_Passenger', 'Ticket_Num']]\ny_train = Embarked_train_data['Embarked']\nX_test = Embarked_test_data[['Fare_per_Passenger', 'Ticket_Num']]\n\ndecision_tree = DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\ny_pred","7cbf48b1":"#Model 2\nX_train = Embarked_train_data_with_Deck[['Fare_per_Passenger',\n                                        'Ticket_Num', \n                                        'Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_T']]\ny_train = Embarked_train_data_with_Deck['Embarked']\nX_test = Embarked_test_data_with_Deck[['Fare_per_Passenger',\n                                      'Ticket_Num',\n                                      'Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_T']]\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\ny_pred","b610beec":"# Replace NaN values with 'S' (Southampton)\ndf_combined[['Embarked']] = df_combined[['Embarked']].fillna(value = 'S')","c6ad9be3":"# Look for NaN value in 'Fare' column\ndf_combined[pd.isnull(df_combined['Fare_per_Passenger'])][['PassengerId', \n                                                           'Name', \n                                                           'Pclass',\n                                                           'Embarked',\n                                                           'Ticket_Num',\n                                                           'Fare_per_Passenger']] ","bf8648a8":"# Occasions of \"free\" tickets\nfree_tickets = df_combined[df_combined['Fare_per_Passenger'] == 0].sort_values(by = ['Pclass'])\nfree_tickets[['Name', 'Pclass', 'Ticket_Num', 'Fare_per_Passenger']]","25a96b6f":"Thompson = df_combined[df_combined['Second_Name'] == 'Thompson']\nThompson[['Name', 'Ticket_Num', 'Fare_per_Passenger']]","62daad15":"Carver = df_combined[df_combined['Second_Name'] == 'Carver']\nCarver[['Name', 'Ticket_Num', 'Fare_per_Passenger']]","a039bbbd":"Cardeza = df_combined[df_combined['Second_Name'] == 'Cardeza']\nCardeza[['Name', 'Ticket_Num', 'Fare_per_Passenger']]","d3fbbb7c":"Fare_data = df_combined.copy()\n\n#Convert alphabetical values of ports into numerical\ndf_dummies = pd.get_dummies(Fare_data['Embarked'], prefix='Embarked')\nFare_data = Fare_data.merge(df_dummies, left_index=True, right_index=True)\n\n#Create mask to filter out NaN and zero values\nmask = (Fare_data['Fare_per_Passenger'] == 0) | (pd.isnull(Fare_data['Fare_per_Passenger']))\nFare_test_data = Fare_data[mask].copy() #Dataframe with Fare values to be predicted\nFare_train_data = Fare_data[~mask].copy()","ab257e7e":"Fare_independent_variables = ['Sex_Scaled',\n                              'Pclass',\n                              'Ticket_Num',\n                              'Embarked_C', 'Embarked_Q', 'Embarked_S']\nFare_dependent_variable = 'Fare_per_Passenger'\n\n# Apply Ridge Regression for fare prediction.\nridge = Ridge(normalize=True)\nridge.fit(Fare_train_data[Fare_independent_variables], \n          Fare_train_data[Fare_dependent_variable])\n\n#Add predicted values to test dataframe\nFare_test_data.loc[Fare_test_data.index,\n                   'Fare_per_Passenger'] = ridge.predict(Fare_test_data[Fare_independent_variables])\n\nFare_test_data[['Name', 'Fare_per_Passenger']]","037320e2":"#Add test dataframe with predicted values to main dataframe\ndf_combined = Fare_train_data.append(Fare_test_data).sort_values(by = ['PassengerId'])","2b6f758d":"def get_bins(df, target_column, new_column):\n    bins = list(range(1,7))\n    df[new_column] = pd.qcut(x=df[target_column], q=6, labels = bins).astype(int)\n    return\n\nget_bins(df_combined, target_column='Fare_per_Passenger', new_column='Fare_Bins')","e75d99d2":"train = df_combined[:891]","0e58c184":"#Code for getting number of survivors and victims\ndef count_victims_and_survivors(criteria):\n    \n    survivors = []\n    victims = []\n    \n    for criterion in criteria:\n        survivors.append(criterion['Survived'].sum())\n        victims.append(len(criterion['Survived']) - criterion['Survived'].sum())\n        \n    return survivors, victims","542008a0":"#Code for creation of bar plots\ndef create_bar_plot(labels, survivors, victims):\n    \n    fig, ax = plt.subplots()\n\n    x = np.arange(len(labels))\n    width=0.35\n    ax.bar(x - width\/2, survivors, width, label='Survivors', color='darkcyan')\n    ax.bar(x + width\/2, victims, width, label='Victims', color='indianred')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels)\n    ax.set_ylabel('Number of passengers')\n    ax.legend()\n    \n    plt.grid(color='gray', linestyle='dotted')\n    \n    plt.show()","be853675":"first_class = train[train['Pclass'] == 1]\nsecond_class = train[train['Pclass'] == 2]\nthird_class = train[train['Pclass'] == 3]\n\ncriteria = [first_class, second_class, third_class]\n\npassengers = count_victims_and_survivors(criteria=criteria)\ncreate_bar_plot(labels=['Pclass 1', 'Pclass 2', 'Pclass 3'],\n                survivors=passengers[0], victims=passengers[1])","d28dd6bc":"males = train[train['Sex'] == 'male']\nfemales = train[train['Sex'] == 'female']\n\ncriteria = [males, females]\n\npassengers = count_victims_and_survivors(criteria=criteria)\ncreate_bar_plot(labels=['Male', 'Female'],\n                survivors=passengers[0], victims=passengers[1])","b9b73a42":"children = train[train['Is_Child'] == 1]\nadults = train[train['Is_Child'] == 0]\n\ncriteria = [children, adults]\n\npassengers = count_victims_and_survivors(criteria=criteria)\ncreate_bar_plot(labels=['Children', 'Adults'],\n                survivors=passengers[0], victims=passengers[1])","b16f6aaa":"married = adults[adults['Spouse_on_Board'] == 1]\nnot_married = adults[adults['Spouse_on_Board'] == 0]\n\ncriteria = [married, not_married]\n\npassengers = count_victims_and_survivors(criteria=criteria)\ncreate_bar_plot(labels=['Married', 'Not Married'],\n                survivors=passengers[0], victims=passengers[1])","a915829c":"solo_travelers = train[train['Family_Size'] >= 0]\nsmall_family = train[(train['Family_Size'] > 1) & (train['Family_Size'] <= 3)]\nbig_family = train[train['Family_Size'] > 3]\n\ncriteria = [solo_travelers, small_family, big_family]\n\npassengers = count_victims_and_survivors(criteria=criteria)\ncreate_bar_plot(labels=['Solo Travelers', 'Small Family', 'Big Family'], \n                survivors=passengers[0], victims=passengers[1])","8fc8bf99":"solo_travelers = train[train['Travelled_Together'] == 1]\nsmall_group = train[(train['Travelled_Together'] > 1) & (train['Travelled_Together'] <= 3)]\nbig_group = train[train['Travelled_Together'] > 3]\n\ncriteria = [solo_travelers, small_family, big_family]\n\npassengers = count_victims_and_survivors(criteria=criteria)\ncreate_bar_plot(labels=['Solo Travelers', 'Small Group', 'Big Group'], \n                survivors=passengers[0], victims=passengers[1])","8db36c59":"survivors = train[train['Survived'] == 1]\nvictims = train[train['Survived'] == 0]\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nfig.set_size_inches(12, 5)\n\nax1.scatter(x=list(survivors['PassengerId']), y=list(survivors['Fare_per_Passenger']), color='darkcyan')\nax1.scatter(x=list(victims['PassengerId']), y=list(victims['Fare_per_Passenger']), color='indianred')\nax1.set_xlabel('Passenger ID')\nax1.set_ylabel('Fare per each passenger')\n\nax2.plot(list(survivors['Fare_per_Passenger'].sort_values(ascending=True)), color='darkcyan')\nax2.plot(list(victims['Fare_per_Passenger'].sort_values(ascending=True)), color='indianred')\nax2.set_xlabel('Number of passengers')\nax2.set_ylabel('Fare per each passenger')\n\nax1.grid(color='gray', linestyle='dotted')\nax2.grid(color='gray', linestyle='dotted')\n\nplt.show()","cce337b4":"def create_heatmap(df, feature_names):\n        \n    train_features = df[feature_names]\n        \n    plt.figure(figsize=(10,8))\n    cor = train_features.corr()\n    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, xticklabels=True, yticklabels=True)\n    plt.show()\n    \ncreate_heatmap(df=train, feature_names=['Pclass',\n                                        'Sex_Scaled',\n                                        'Is_Child',\n                                        'Spouse_on_Board', 'Family_Size', 'Travelled_Together',\n                                        'Ticket_Num_Scaled',\n                                        'Fare_per_Passenger', 'Fare_Bins',\n                                        'Survived'])","f3e31e66":"def feature_selection(X_train, y_train, feature_names):\n\n    tree_clf = DecisionTreeClassifier()\n    tree_clf.fit(X_train,y_train)\n\n    importances = tree_clf.feature_importances_\n    forest_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n    \n    fig, ax = plt.subplots()\n    forest_importances.plot.bar(color='darkcyan')\n    ax.set_title(\"Feature importances using MDI\")\n    ax.set_ylabel(\"Mean decrease in impurity\")\n    \n    plt.show()\n    return","efe78d75":"feature_names = ['Pclass', \n                 'Sex_Scaled',\n                 'Is_Child',\n                 'Spouse_on_Board', 'Family_Size', 'Travelled_Together',\n                 'Ticket_Num_Scaled',\n                 'Fare_per_Passenger', 'Fare_Bins']\n\nX_train = train[feature_names]\ny_train = train['Survived']\n\nfeature_selection(X_train, y_train, feature_names)","9871ae62":"selector = SelectKBest(score_func=chi2, k=5)\nselector.fit(X_train, y_train)\nX_train.columns[selector.get_support(indices=True)].tolist()","900d75ab":"train = df_combined[:891].copy()\ntest = df_combined[891:].copy()\n\n#Variables\nvariables = ['Pclass', 'Sex_Scaled', 'Is_Child', 'Travelled_Together', 'Fare_Bins']","80317850":"#This function consists of parameters of different models\ndef apply_RandomizedSearchCV(X_train, y_train, classifier, n_iter):\n    \n    ''' Alert for beginners #1! \n        The greater the number of iterations (value of n_iter) or\/and types of classifiers, or\/and parameters,\n        the slower this function will work. \n    '''\n    \n    classifiers = {'LogisticRegression()' : {'params':\n                                             {'C':[0.1, 1, 10, 20]}\n                                            },\n                   'GaussianNB()' : {'params':\n                                     {}\n                                    },\n                   'KNeighborsClassifier()' : {'params':\n                                               {'n_neighbors':list(range(2,16))}\n                                              },\n                   'DecisionTreeClassifier()' : {'params':\n                                                 {'max_depth':list(range(2,16)), \n                                                  'min_samples_leaf':list(range(2,16))}\n                                                },\n                   'SVC()' : {'params' :\n                              {'C':[0.1, 1, 10, 20], 'kernel':['rbf', 'linear']}\n                             }\n                   }\n    \n    rs = RandomizedSearchCV(classifier, classifiers[str(classifier)]['params'], n_iter=n_iter)\n    rs.fit(X_train, y_train)\n    df = pd.DataFrame(rs.cv_results_)[['params',\n                                       'mean_test_score',\n                                       'rank_test_score']].sort_values(by='rank_test_score')[:1]\n    return df","32bd93d1":"#This function is for getting information about optimal parameters of each chosen model\ndef get_params_for_models(df, independent_variables, dependent_variable, models):\n    \n    ''' Alert for beginners #2! \n        Be careful with variables that you pass into this function, because some classifiers \n        may work badly with them. For example, Support Vector Classification cannot handle continuous data \n        or huge amount of variables; this function would work VERY slow.\n        In this case choose only appropriate classifiers or convert your data in appropriate form \n        (e.g. use bins for continuous data).\n    '''\n    \n    best_model_scores = {}\n    \n    for model in models:\n        \n        best_test_score = apply_RandomizedSearchCV(df[independent_variables],\n                                                   df[dependent_variable],\n                                                   model,\n                                                   n_iter=4)\n        best_model_scores[model] = best_test_score\n\n    return best_model_scores","6d8a0752":"models = [LogisticRegression(), GaussianNB(), KNeighborsClassifier(), DecisionTreeClassifier(), SVC()]\n\nprint('Parameters for train data:')\nget_params_for_models(df=train,\n                      independent_variables=variables,                    \n                      dependent_variable='Survived',\n                      models=models)","c4a7b570":"svm = SVC()\nsvm.fit(train[variables],\n        train['Survived'])\n\ny_pred = svm.predict(test[variables])\ntest['Survived'] = y_pred.astype(int)","e628ff71":"results = test.copy()\nfor_submission = results[['PassengerId','Survived']]\nfor_submission.head(10)","f3a1bb85":"# for_submission.to_csv(r'\/[...]\/for_submission_final.csv', index=False)","d8cd8777":"Most of solo travelers as well as big group of passengers perished. Small groups were more lucky to survive.","548d8b03":"Here I have used RandomizedSearchCV() in order to find optimal parameters of different models and select model with best test score. \nFor final prediction I will choose only one classifier that shows the best results: support vector machine. I will ignore 'optimal' parameters provided by RandomizedSearch, because they seem to work worse - still need to figure out why is it so.","5a531179":"As long as those are continuous data, to fill in one missing value of 'Fare' in test dataset, we can use ridge regression with the variables:\n- 'Embarked' - because the longer distance, the higher the price of a voyage.\n- 'Pclass' - depending of the class, there were more (or less) conveniences for passengers, thus 1st class tickets were most expensive and 3rd class tickets were cheapest accordingly.\n- 'Ticket' - tickets with similar numbers or with near-standing numbers may have the same price.\n\nRemembering that there was weiredness in the tickets' price data (like free tickets), I think those values should be removed from the model when predicting fare values. But before we do that, let us look if there a lot of such values.","11a8abc7":"ANOVA estimator determines that the most significant features are 'Pclass', 'Sex_Scaled', 'Spouse_on_Board', 'Fare_per_Passenger', 'Fare_Bins'.","96205922":"Majority of first-class passengers were rescued, whilst majority of third-class passenger perished. For second-class passengers the chances were 50-50:","186fb018":"Let us discover data in the dataset, look for missing values and think about what additional features we can extract from it.","f7dbab7b":"This time we have got something, but it is confusing that shipmates of the group with \"scheduling problems\" had absolutely different ticket numbers. If we look at their [biographies](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/alfred-john-carver.html) they actually shared the ticket no. 370160. Maybe there is a mistake in the dataset? \n\nThis case shows that information about ticket numbers might be inaccurate, and we should rely on these data carefully when building models.","df44a8ef":"## 1. Import of all necessary libraries, load of raw data","c3df75ac":"As far as there are missing values in both train and test datasets, but it is necessary to fill in some missing values and to extract additional features, I need to create a combined dataset - for data preprocessing purpose only.","382af2a5":"## 4. Modelling","7f25d201":"## Table of Contents\n__________________\n* [Executive Summary](#Executive-Summary)\n* [1. Import of all necessary libraries, load of raw data](#1.-Import-of-all-necessary-libraries,-load-of-raw-data)\n* [2. Data-preprocessing](#2.-Data-preprocessing)\n    * [2.1. Extracting data from existing values](#2.1.-Extracting-data-from-existing-values)\n    * [2.2. Filling in missing values](#2.2.-Filling-in-missing-values)\n        * [2.2.1 NaN in 'Embarked'](#2.2.1-NaN-in-'Embarked')\n        * [2.2.2 NaN in 'Fare'](#2.2.2-NaN-in-'Fare')\n* [3. Data analysis and feature selection](#3.-Data-analysis-and-feature-selection)\n     * [3.1. Graphical data analysis](#3.1.-Graphical-data-analysis)\n     * [3.2. Feature selection with estimators](#3.2.-Feature-selection-with-estimators)\n* [4. Modelling](#4.-Modelling)\n    * [4.1. Selection of optimal model parameters](#4.1.-Selection-of-optimal-model-parameters)\n    * [4.2. Final model](#4.2.-Final-model)\n* [Afterword](#Afterword)\n__________________","4b44ea9d":"### 4.2. Final model","aad6c974":"From what we already have, we can find out next information: \n- from 'Name': 'Title' of a person, 'First_Name', 'Second_Name', 'Husband_Name';\n- from 'Sex', 'Husband_Name': 'Spouse_on_Board';\n- from 'Ticket': 'Group_Size' (group of passengers who travelled together);\n- from 'Parch' and 'SibSp': 'Family_Size';\n- from 'Group_Size' and 'Family_Size': 'Travelled_Together';\n- from 'Age', 'Title', 'Travelled_Together', 'Parch': whether each passenger 'Is_Child';\n- from 'Cabin': 'Deck' letter;\n- from 'Fare' and 'Group_Size': 'Fare_per_Passenger'.\n\nTo extract data from 'Name' column, we can use regular expressions, thanks to more or less standardized records:","ad5ab135":"Model 2 gave the same result. I guess ticket numbers also somehow correlate with decks' names, so maybe it was even not necessary to use the second model.","67d1e9e7":"#### 2.2.2 NaN in 'Fare'\nNext task is to fill in missing values in 'Fare_per_Passenger' variable.","4ff492f2":"Next is 'Ticket' column. The values in this column are unstandardized (some of them are numerical, others have letter characters), so we need to clean them:","8a764aa5":"The same situation is with the spouses on board:","6ca1db51":"There are quite a lot of such occasions, moreover passengers with \"free\" tickets were all males travelled in different classes. Besides if we look at their ticket's numbers, we can notice there are several groups of travellers. \n\nFurther investigation shows these groups are explainable.\n\n- [Titanic Guarantee Group](https:\/\/www.encyclopedia-titanica.org\/titanic-guarantee-group) was the Belfast team sent by shipbuilders Harland & Wolff to accompany the Titanic on her maiden voyage. The team consisted of Mr. Andrews, Chisholm, Parr, Parkes, Cunningham, Campbell, Frost, Knight, Watson and Thompson (but the last one is not in the table above). They were travelling in different classes because of the different occupations.\n- [Ismay, Mr. Joseph Bruce](https:\/\/en.wikipedia.org\/wiki\/J._Bruce_Ismay) served as a chairman and managing director of the White Star Line (the company operated the Titanic). [Fry, Mr. Richard](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/richard-fry.html) was the personal valet to Joseph Bruce Ismay, and [Harrison, Mr. William](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/william-harrison.html) was a private secretary of Mr. Ismay.\n- [Reuchlin, Jonkheer. John George](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/johan-george-reuchlin.html) was on the Titanic to evaluate the Olympic-class liners, his ticket was complimentary.\n- Finally, 3-rd class passengers from the table above [were shipmates forced to travel aboard Titanic as passengers because the coal strike caused scheduling problems](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/lionel-leonard.html). They were 6 persons: Lionel Leonard, August (Alfred) Johnson, William Cahoone Jr. Johnson,  William Henry T\u00f6rnquist (as we can see, their tickets' fare value is 0), Thomas Storey (he is the one passenger with NaN value of 'Fare' feature) and Alfred John Carver (but the last one is not in the table above).\n\nI assume these zeroes in fare values should be treated as NaN values, because it is not about the fact that a person had paid for the ticket or not, but more about the information of cabin location or social status that is connected with 'Fare' data. That is why I am going to predict actual fare values of these \"free\" tickets too. \n\nWhere are Mr. Thompson and Mr. Carver? If they are not among the passengers with \"free\" tickets, maybe we can look at fare values of their tickets to get an overall picture?","9d68aeb3":"For the future model I suggest the next data are somehow valuable to predict survival of each passenger:\n- 'Sex' and 'Is_Child', because it was told to rescue women and children first;\n- 'Spouse_on_Board', 'Family_Size', 'Travelled_Together'. I assume solo travellers as well as big families had less chances to survive. I guess it is because big families would not separate from each other to evacuate. As for solo travelers, they had less support\/informing;\n- 'Pclass', because first-class passengers were evacuated first. At the same time third-class passengers and some security members were misinformed about the real situation, that is why exit gates between 1&2 classes section and 3 class section were closed, so third-class passengers had less chances to evacuate and survive;\n- 'Fare_per_Passenger' or 'Fare_Bins', because these data can provide more stratification information within the same class;\n- 'Ticket_Num_Scaled': 1) these data may be connected with a location of each cabin and thereby with the distance to emergency exit; 2) passengers who shared the same joint ticket might survive together or perish together.\n\nFirst, let us conduct graphical data analysis. We will discover more train dataset and try to define what data influence the chance to survive.","65535bb2":"For the first approach, let us split first-class passengers into three groups by three ports of embarkation. Then we can calculate quantiles of fare values per each port of embarkation. This approach is simplified, because it does not include type of passenger's cabin (presumably the more convenient the cabin is the more expensive is the ticket). \n\nThe passengers _Icard, Miss. Amelie_ and _Stone, Mrs. George Nelson (Martha Evelyn)_ paid \u00a340 each. As could be seen below, in this case both Southampton and Cherbourg may be possible ports. For Cherbourg \u00a340-ticket seems to be more common one (most of fare values lay between \u00a327.72085 and \u00a339.60000). \n\nBesides, the visualization gives us insights about extremely low or high ticket prices, that will be useful further.","3bb5c2e6":"As we can see, ticket prices for the passenger with \"scheduling problems\" were predicted if not ideally, but rather good. The model could not predict ticket prices for J.Bruse Ismay and passengers accompanying him, but because there were only few cases with such a high value, I think it is the best that we can get from the model.","9f6b016b":"Final step - train the model and get predictions.","d41037ab":"### 3.2. Feature selection with estimators","71eed805":"By estimating feature importances with tree models, the most important features are 'Ticket_Num_Scaled', 'Sex_Scaled', 'Fare_per_Passenger', 'Pclass', 'Family_Size'. Because it is a tree-based estimator, it works good with continuous data as 'Ticket_Num_Scaled', 'Fare_per_Passenger', but I should be careful with these features when applying different classifiers.","e1da4d0b":"### 4.1. Selection of optimal model parameters","4bcd391d":"## 2. Data preprocessing","f035859a":"### 2.2. Filling in missing values","869de32d":"Now from all we have got we can determine whether or not a passenger was a child:\n- passengers under 14 y.o. were children ([according to classification](https:\/\/www.encyclopedia-titanica.org\/children-on-titanic\/));\n- passengers with title 'Master' were all young boys;\n- passengers with title 'Mr.' were adult men, 'Mrs.' - married women, 'Dr.' - people with science degree, thus also adults;\n- passengers who travelled alone were most likely adults (family size = 1);\n- passengers who travelled with at least 1 parent are children ('Parch'>0). This criterion is tricky, because 'Parch' contains information about children too, or it may be adult passenger who travelled with older parents, but I will take a risk to assume such cases were filtered out by the previous conditions. (This step consists of only ~20 non-married women passengers, manual check showed good results);\n- others are adults.","190e906b":"If we could not get exact answer by the first approach, let us try the second approach. \nTo predict port of embarkation I will use decision tree method with such independent variables as Fare, Ticket and Deck. As far as there are missing values in 'Cabin' (and eventually in 'Deck'), I will try 2 models: with the variable 'Deck' and without it. \nDecision tree classifier would not work with alphabetical values, so we should create dummy codes for 'Deck' letters.","db1780a3":"Because there were some tickets with extremely high prices, and because some classifiers will work badly with continuous data (actual ticket prices), I will split all the fare values into 6 bins. ","5b9c51e4":"As it was noticed by fellow researcher, [information about marital status of each passenger may also be useful](https:\/\/www.kaggle.com\/reisel\/save-the-families). The idea is _all wifes whose husbands survived survived as well and vise versa all husbands whose wifes did not survive didn\u2019t survive as well_. \n\nThe names of married women passengers are recorded according to the same principle: \n\n|Second Name| Title| Name of a husband| First Name|\n|----|----|----|----|\n|Johnson,| Mrs.| Oscar W |(Elisabeth Vilhelmina Berg)|\n\nI will extract data about husbands' names by using regular expressions, and then check if there were any male passengers with such name.","9bdf1cc3":"## 3. Data analysis and feature selection","ea62d71c":"### 2.1. Extracting data from existing values","8b21f4dc":"Last but not least, categorical 'Sex' values should be converted into numerical, because models would not work with categorical values.","865ed44c":"The amount of fare in the dataframe is calculated per a ticket (or more particularly, 'joint ticket'), not per a passenger. (The evidence is from Encyclopedia Titanica; manual checks with neighbouring tickets also show that fares calculated per each passenger were equal, while fares per neighbouring tickets were different). \nThat is why we need to create column 'Fare_per_Passenger' and find out these values by dividing total fare to group size.","8a724595":"Passengers with expensive tickets had higher chances to survive, a lot of passengers with cheap tickets perished:","caa67bb7":"This research is based on the real event - sinking of the RMS Titanic. This catastrophe happened almost 110 years ago, but it still arouses interest concerning its causes and consequences. \nAs far as the data about passengers of the RMS Titanic are available, now we can apply different Machine Learning algorithms to determine the decisive factors of chance to survive for each passenger. In other words, we can estimate (or at least try to) the fate with the help of the numerical and categorical data.","f372e7bf":"### 3.1. Graphical data analysis ","65cecf00":"#### 2.2.1 NaN in 'Embarked'\nLet us begin with missing values in 'Embarked'. First of all we need to look for all possible values of this feature.","0f81d4cd":"## Afterword\n\nIt is my very first independent project on building ML models. \nMy aim was to deepen the knowledge gained in ML courses, to learn more about data preprocessing, feature selection and model selection, and of course to create model with good predictive ability. \n\nIndeed, I have learnt a lot about different techniques how to clean and scale data, figured out how feature selection and model selection algorithms work, improved my coding skills.\n\nI have got a lot of ideas and inspiration from these kernels and discussions of them (this list was compiled in random order):\n\n- [Titanic [0.82] - [0.83]](https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83);\n- [Titanic: 2nd degree families and majority voting](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting);\n- [12,500 Feet Under the Sea](https:\/\/www.kaggle.com\/frederikh\/12-500-feet-under-the-sea);\n- [Save the Families!](https:\/\/www.kaggle.com\/reisel\/save-the-families);\n- [Titanic using Name only [0.81818]](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818);\n- [Titanic Using Ticket Groupings](https:\/\/www.kaggle.com\/jack89roberts\/titanic-using-ticket-groupings); \n- [Beginner Tutorial Using VotingClassifier ~ 82.27](https:\/\/www.kaggle.com\/samratp\/beginner-tutorial-using-votingclassifier-82-27).\n\nFor beginners like me: hope that some ideas were helpful. \nFor experienced researchers: constructive criticism is welcomed! :)\nThank you.","f6949335":"# Introduction","66f00581":"There are a lot of missing values in 'Cabin' column, but I guess we can extract data about deck (that is represented by the letter in the cabin number) at least from existing values.","2ebfb031":"There are missing data in columns: 'Embarked', 'Fare', 'Age', 'Cabin'. \nI am not sure whether I want to use 'Embarked' and 'Cabin' features in the final model, but this information will probably be useful for filling in NaN vaues in other columns that seem more interesting (e.g. 'Fare'). ","c5ae7aea":"It seems there are no data of [Mr. Joseph Thompson](https:\/\/www.encyclopedia-titanica.org\/titanic-biography\/joey-thompson.html) in train or test datasets. It turned out he was lucky to disembark the Titanic before the voyage even started. ","3b9de1ec":"From 'Ticket' column we can draw conclusion about groups of passengers who travelled together and calculate size of each group. Let us call this variable 'Group_Size'. I decided to group passengers by tickets, not by same second names, because some passengers may have different second names, but were somehow related, or vice versa they were just namesakes.\n\nMoreover, relatives could have different ticket numbers, so it is also important to check 'Family_Size' by summing up number of parents\/childern ('Parch') and siblings\/spouses ('SibSp'). \n\nMaximum value between this two groups will be an actual number of passengers who travelled together ('Travelled_Together').","49939b7d":"## Executive Summary\n\n* __Final score of the model__: 0.78229\n* __Model description__: Support Vector Classifier, selected features: passenger class, sex, age category (child or adult), number of passengers with whom each passenger travelled, fare. I have also tried features: spouses on board, ticket numbers, ticket prices, but they did not improve the final score.\n* __Data-preprocessing work__. I have managed to extract additional information about the passengers from the dataset: spouses on board, family size and size of passengers groups who travelled together, fare per each passenger, decks (see more detailed explanation in [2.1.](#2.1.-Extracting-data-from-existing-values)). For filling in gaps in 'Embarked' column I applied decision tree classifier (features: passenger class, fare per passenger, ticket number, deck). For filling in gaps in 'Fare' column (to be more precise, 'Fare per Passenger') I applied ridge regression (features: passenger class, sex, ticket number, port of embarkation). I did not fill in gaps in 'Age' column, instead I split passengers into two categories: children and adults (supportive indicators were age, title, group size, number of parents on board.\n* __Ways to improve model__: \n    * 'Age' column definitely needs more investigation. It should be better to use at least additional categories as middle age and old passengers.\n    * I used scaling for ticket numbers, but I guess use of bins would be more appropriate. It seems there is no particular order in those numbers, but still tickets might represent cabin locations of each passengers and the ability to reach evacuation exit in time.\n    * There are differences in survivor rate among passenger categories from different classes, and I assume combined model (separately trained for each passenger class) should be applied to catch those differences. For now my combined models showed worse results; the biggest problem is to choose appropriate weights for each model.\n    * I also need to learn more about selection of optimal parameters for each model and to try to use more complicated models.","93c8b725":"As we can see there are some pieces of information about Titanic passengers: their names, sex, age, family relations, tickets price, etc. Obviously test dataset does not have 'Survived' data, because those are the values we should predict.\n\nFirst of all we need to check if there are any missing values and decide how to fill those gaps:","cf82ec77":"It seems to be 3 ports of embarkation, and indeed as Wikipedia says they were [Southampton, Cherbourg and Queenstown](https:\/\/en.wikipedia.org\/wiki\/Titanic).","ca270b35":"According to Encyclopedia Titanica, [Icard, Miss. Amelie](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html) and [Stone, Mrs. George Nelson (Martha Evelyn)](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html) travelled together and embarked at **Southampton**. \n\nHowever, for some reasons I am not sure if I can just search for these data and then fill in all the missing values. First, it would be cheating, and second it would be just impossible in other ML-projects because of the absence of the data on the Internet (and we are all here to master machine learning, aren't we?). Instead let us try to predict those values. \n\nI am going to use two approaches:\n1) to discover distribution properties of the available data and determine port, where passengers with same characteristics embarked;  \n2) to predict missing values with decision tree model.\n\nIt is most logical to assume that main pricing factors are passenger's class and port of embarkation. As long as _Miss Amelie and Mrs George Nelson_ were first-class passengers, I think analysis of the first-class passengers only would be more accurate in this case.","b1ca91f9":"Although it was told to rescue children first, only half of them survived. Considering that majority of passengers were adults, for now I doubt whether the feature 'Is_Child' is significant.","6c0e7cba":"Model 1 is pretty accurate, it has already predicted that our target passengers embarked at Southampton:","c967ca20":"For females the chances to survive were higher:","28fcef15":"From the heatmap we can see that features most correlated with chance to survive are 'Sex', then 'Pclass' and 'Fare'.","027a610a":"As we can see, ticket price per each passenger with \"scheduling problems\" probably was \u00a37.25, and ticket price for J.Bruse Ismay (and passengers accompanying him) was \u00a3128.0823. We will keep these values in mind while predicting them with model to evaluate its accuracy.","2a8c43b5":"According to Wikipedia, [the chairman of the White Star Line, J. Bruce Ismay, occupied the port-side \"Deluxe\" Suite on the Titanic, while the starboard suite was occupied by the American millionaire Mrs. Charlotte Drake Cardeza](https:\/\/en.wikipedia.org\/wiki\/First-class_facilities_of_the_Titanic). Let us find out how much money had Mrs. Charlotte Drake Cardeza paid for her suite?","cf3148b0":"As for existing values, we can see that minimum fare value is 0, that is a little bit suspicious. Are those tickets free or is it a mistake? Below we will discover this feature more. Maximum fare value also significantly differs from the values of upper quartile (\u00a3512.3 against \u00a331.5). On the one hand, this difference can be explained by bigger amount of third-class passenger, but on the other hand it may influence badly the future model. So we should carefully use this feature.","b5aa89cc":"To standardize values of 'Ticket' variable I will split all symbols, spaces, letter characters and digitals into two columns ('Ticket_Pref' and 'Ticket_Num'). \n\nStraight away I will also create additional column 'Ticket_Num_Scaled' with ticket numbers scaled with the MinMaxScaler, because later it will be necessary when applying some of the models. Surprising fact appeared here: if you look at the distribution of scaled ticket numbers, most of them lays between 0.00 and 0.25, but there are some tickets that equals 1 or almost 1. All that tickets (actual numbers start with 3101, some of them also have prefix STON\/O2.) were probably sold in Finland (the evidence is from the names and biographies of the passengers with those tickets). Thus by the ticket number you can determine, where it had been sold."}}