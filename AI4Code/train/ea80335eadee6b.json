{"cell_type":{"a4badbb0":"code","441a26bf":"code","9c70b0e2":"code","b83fb8f4":"code","1949145f":"code","a096c278":"code","24f20db0":"code","982d802d":"code","ca4a8490":"code","9a2f33f8":"code","ccb22822":"code","4a0a0b20":"code","5b74bc3a":"code","80bcd61a":"code","55c40f98":"code","fcef3cb4":"code","a08facb1":"code","d464c5f8":"markdown","31a46174":"markdown","1315b224":"markdown","bf7204dc":"markdown","97edf2dc":"markdown","33d848c7":"markdown","76e654fd":"markdown","4631ae7e":"markdown","870ac574":"markdown","6a2d5091":"markdown","4f86d0c8":"markdown","0ff93ab0":"markdown","c151e8c0":"markdown","52da9f11":"markdown","a53f7830":"markdown"},"source":{"a4badbb0":"import numpy as np\nimport pandas as pd\nimport os\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import RNN\nfrom keras.utils import np_utils\nimport re\nfrom langdetect import detect\nimport matplotlib.pyplot as plt","441a26bf":"metacritic_game_user_comments = pd.read_csv(\"..\/input\/metacritic-video-game-comments\/metacritic_game_user_comments.csv\")\nmetacritic_game_user_comments.head()","9c70b0e2":"xc2_reviews = metacritic_game_user_comments[(metacritic_game_user_comments['Title'] == 'Xenoblade Chronicles 2')]\nxc2_reviews = xc2_reviews[(xc2_reviews['Userscore'] == 10)]\nprint(xc2_reviews.shape)","b83fb8f4":"# Print the last row to view the noice of the \"This review contains spoilers.... \" line\n# And remove that part of the strings containing the substring\nprint(xc2_reviews.tail(1))\nxc2_reviews['Comment'] = xc2_reviews['Comment'].str.replace('            This review contains spoilers, click expand to view.        ', '')\n\n# Detect language and select only English reviews.\nxc2_reviews['Language'] = xc2_reviews['Comment'].apply(detect)\nxc2_reviews = xc2_reviews[(xc2_reviews['Language'] == 'en')]['Comment']\nprint(xc2_reviews.shape)","1949145f":"# Convert all column values into one lowercase string\nxc2_reviews_string = '\\n'.join(xc2_reviews.values).lower()\nprint(xc2_reviews_string[:500])","a096c278":"# to count words in string \nres = len(re.findall(r'\\w+', xc2_reviews_string)) \nres","24f20db0":"characters = sorted(list(set(xc2_reviews_string)))\n\nn_to_char = {n:char for n, char in enumerate(characters)}\nchar_to_n = {char:n for n, char in enumerate(characters)}","982d802d":"X = []\nY = []\ninput_length = len(xc2_reviews_string)\nseq_length = 100\n\n# Loop through the entire input string and create sequences of characters\nfor i in range(0, input_length - seq_length, 1):\n    sequence = xc2_reviews_string[i:i + seq_length]\n    label = xc2_reviews_string[i + seq_length]\n    \n    X.append([char_to_n[char] for char in sequence])\n    Y.append(char_to_n[label])\n\nprint(\"Total Patterns:\", len(X))","ca4a8490":"X_modified = np.reshape(X, (len(X), seq_length, 1))\nX_modified = X_modified \/ float(len(characters))\nY_modified = np_utils.to_categorical(Y)","9a2f33f8":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import RNN\n\nmodel = Sequential()\nmodel.add(LSTM(600, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(600))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(Y_modified.shape[1], activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","ccb22822":"history = model.fit(X_modified, Y_modified, epochs=100, batch_size=100)\nmodel.save_weights('xc2_review_generator_model_with_bigger_layers.h5')","4a0a0b20":"# https:\/\/keras.io\/visualization\/\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()","5b74bc3a":"def combine_string(character_array):\n    return_string = \"\"\n    for char in character_array:\n        return_string = return_string + char\n    return return_string\n\ndef generate_review(starting_sequence):\n    generated_review_mapped = [n_to_char[value] for value in starting_sequence]\n    print(f'Starting sequence: {combine_string(generated_review_mapped)}')\n    \n    for i in range(400):\n        x = np.reshape(starting_sequence,(1,len(starting_sequence), 1))\n        x = x \/ float(len(characters))\n\n        pred_index = np.argmax(model.predict(x, verbose=0))\n        seq = [n_to_char[value] for value in starting_sequence]\n        generated_review_mapped.append(n_to_char[pred_index])\n\n        starting_sequence.append(pred_index)\n        starting_sequence = starting_sequence[1:len(starting_sequence)]\n\n    return generated_review_mapped","80bcd61a":"generated_review_1 = generate_review(X[99].copy())\nprint(combine_string(generated_review_1))","55c40f98":"generated_review_2 = generate_review(X[0].copy())\nprint(combine_string(generated_review_2))","fcef3cb4":"generated_review_3 = generate_review(X[10].copy())\nprint(combine_string(generated_review_3))","a08facb1":"custom_starting_sequence = list('xenoblade chronicles 2 is a great game, like all games, it has its issues, but it runs smoothly and ')\ncustom_starting_sequence_mapped = [char_to_n[value] for value in custom_starting_sequence]\n\ngenerated_custom_review = generate_review(custom_starting_sequence_mapped)\nprint(combine_string(generated_custom_review))","d464c5f8":"### Data Preprocessing\nTurning our data into sequences for the LSTM layers.","31a46174":"# Data selection\nI chose to use only the Xenoblade Chronicles 2 reviews that gave a rating of 10. I chose to train on reviews from this game only, because this leaves a smaller dataset (+\/- 150 reviews) which is needed to reduce training time. I chose to generate positive reviews only. I think this should result in a model trained for a more specific purpose and therefore should generate better texts.\n* Filter out all reviews except those of Xenoblade Chronicles 2.\n* Filter out all reviews that give a rating of 10. \n","1315b224":"Using only the Xenoblade Chronicles 2 reviews with a rating of 10 results in a dataset of 159 reviews.","bf7204dc":"### Creating a method to generate text\nThe first method creates a single string from the seperate characters.\nThe second method generates a text of 400 characters, given an starting input text of 100 characters.","97edf2dc":"### Training the model\nI've reset the batch size to 100, as it heavily speeds up training and didn't show any significant improvements regarding loss or the text generation. I've found out that the maximum runtime for a commit is actually 9 hours instead of 6, therefore I can let it train longer. \n","33d848c7":"Finally, we generate a review from a custom starting sequence to see what result that leads to.","76e654fd":"As shown above, filtering out the non-english reviews removes 31 reviews from the dataset, leaving us with a nice, smaller dataset of 128 english reviews.","4631ae7e":"### Cleaning the data\n* Remove the spoilers warning from the input reviews containing spoilers.\n* Detecting the language of reviews and selecting only English reviews.\n* Concatenating all column values into a single lowercase string.","870ac574":"### Character mapping\nMapping the characters to numbers for our model.","6a2d5091":"# Creating a game review generator using Metacritic game reviews\n\nIn this notebook I create a review generator based on game reviews from the \"Metacritic Video Game Comments\" dataset from Kaggle.\nMost of the code here is based on the following tutorial on text generation: https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/text-generation-using-python-nlp\/","4f86d0c8":"Checking how many words this text contains to get an insight on the amount of training data.","0ff93ab0":"## Generating some reviews\nNow its time to generate some reviews. We generate reviews from some starting sequences from our dataset. ","c151e8c0":"After fitting the model, we use the model history to plot the loss values during the training.","52da9f11":"### Creating the model\nAdding an extra LSTM layer and another dropout layer resulted in the text generator starting to repeat itself. This is probably due to the model needing more training time, because the model became bigger. I will now train my model with 2 LSTM layers of 600 neurons isntead. Since the model is bigger, I will give it more training time.","a53f7830":"Loading the data"}}