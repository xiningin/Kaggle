{"cell_type":{"840caada":"code","4fb43010":"code","4e8166c6":"code","eb5722ab":"code","ec261256":"code","05374ba3":"code","f211c371":"code","589bccd3":"code","d66966b4":"code","04b57753":"code","78a79e99":"code","796b52b2":"code","68b06afd":"code","7a2772d2":"code","3da4add3":"code","f01a9aff":"code","8fd1b6d9":"code","1c9386fa":"code","05ef8f79":"code","019889d2":"code","31bad5fb":"code","4e2b0762":"code","ffbbf82b":"code","c774857e":"code","f9d6f59b":"code","b3320555":"code","db1f5f66":"code","4e446731":"code","4e92f209":"code","09e78138":"code","30aec511":"code","0cb8f5e4":"code","665803ed":"code","2f74d3a6":"code","013b7238":"code","caf7d10e":"code","36ca6933":"code","cc952da4":"code","3d97efdc":"code","aa4fa4d7":"code","3b443d65":"code","2009a09d":"code","c67f1d32":"markdown","aa26a584":"markdown","1e4f0d44":"markdown","0ecef983":"markdown","c1256a01":"markdown","c3bf1970":"markdown","26714916":"markdown","d9cb94a8":"markdown","95ffa099":"markdown","da2fcbd2":"markdown","4ebb4ca1":"markdown","5c193d10":"markdown","82aadc94":"markdown","85290c62":"markdown","6c871015":"markdown","b1097937":"markdown","ebedd626":"markdown","b358943f":"markdown","e1bf7f21":"markdown","ac24cc50":"markdown","fefdbf6d":"markdown","d11edfe7":"markdown","7486de42":"markdown","6c9aa898":"markdown","1320eb74":"markdown","b41cbe8d":"markdown","98a2b707":"markdown","b0c375ff":"markdown"},"source":{"840caada":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nwarnings.filterwarnings('ignore')","4fb43010":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsomter =pd.read_csv('..\/input\/smoter\/train_smogn1.csv')\n#result = pd.concat([df1, s2, s2, s2], axis=1)\n# Preview the data\ntrain.head()","4e8166c6":"somter.info()","eb5722ab":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nf, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n\nf.suptitle('Target', fontsize=16)\ng = sns.kdeplot(somter['target'], shade=True, label=\"%.2f\"%(somter['target'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(somter['target'], plot=axes[1])\nsns.boxplot(x='target', data=somter, orient='h', ax=axes[2]);\nplt.tight_layout()\nplt.show()","ec261256":"train.duplicated(subset='id', keep='first').sum()","05374ba3":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","f211c371":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","589bccd3":"train['target'].describe()","d66966b4":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nf, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n\nf.suptitle('Target', fontsize=16)\ng = sns.kdeplot(train['target'], shade=True, label=\"%.2f\"%(train['target'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train['target'], plot=axes[1])\nsns.boxplot(x='target', data=train, orient='h', ax=axes[2]);\nplt.tight_layout()\nplt.show()","04b57753":"y=train['target']\nplt.figure(figsize=(12,6))\nsns.boxplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,.5), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,1), label='1%', c='green', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99.9), label='99.9%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","78a79e99":"np.percentile(y,25)","796b52b2":"np.percentile(y,75)","68b06afd":"y= train['target']\nfactor = 1.2\nqmin=y.quantile(0.2)\nqmax=y.quantile(0.98)\nq1 = y.quantile(0.25)\nq3 = y.quantile(0.75)\niqr = q3 - q1\nlower_bound = q1 - (factor * iqr)\nupper_bound = q3 + (factor * iqr)\nprint(lower_bound)\nprint(upper_bound)","7a2772d2":"bins = [0.1403287728456096,  6.656851961255325,9.81385368620504,10.411991752210524]\n# Bin labels\nlabels1 = [ 'Minority ', 'Majority', 'Minority']\ntrainessai=train.copy()\n# Bin the continuous variable ConvertedSalary using these boundaries\ntrainessai['target_binned'] = pd.cut(trainessai['target'], \n                                bins=bins,labels=labels1 )","3da4add3":"labels = trainessai['target_binned'].astype('category').cat.categories.tolist()\ncounts = trainessai['target_binned'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","f01a9aff":"trainessai.groupby('target_binned').count() ","8fd1b6d9":"trainessai['target_binned'].value_counts()","1c9386fa":"scale_pos_weight1 = 296676\/(2866+457)\nscale_pos_weight1","05ef8f79":"scale_pos_weight2_smooth = np.sqrt(296676\/(2866+457)) \nscale_pos_weight2_smooth ","019889d2":"print(lower_bound)\nprint(upper_bound)\n\ntrainessai['wieght'] = [1 if x =='Majority' else 1.1 for x in trainessai['target_binned']]","31bad5fb":"trainessai['wieght'].value_counts()","4e2b0762":"# Create arrays for the features and the response variable\ntrain['weight'] = trainessai['wieght'] \n","ffbbf82b":"train['weight'].value_counts()","c774857e":"y = train['target']\nX = train.drop(['id','target'], axis=1)","f9d6f59b":"# Split the dataset and labels into training and test sets\nX_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train1.shape[0], X_test.shape[1]))","b3320555":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","db1f5f66":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","4e446731":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","4e92f209":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","09e78138":"train.describe(percentiles=[0,0.01,0.05,0.25,0.5,0.75,0.95,0.99,1])","30aec511":"def condense_category(col, min_freq=0.1, new_name='other'):\n    series = pd.value_counts(col)\n    mask = (series\/series.sum()).lt(min_freq)\n    return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\ntrain_condense=train.copy()\ntrain_condense[cat_columns]=train_condense[cat_columns].apply(condense_category, axis=0)\ntrain_condense[train_condense.select_dtypes(['float64']).columns] = train_condense[train_condense.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain_condense[train_condense.select_dtypes(['object']).columns] = train_condense.select_dtypes(['object']).apply(lambda x: x.astype('category'))","0cb8f5e4":"# Create arrays for the features and the response variable\ny_condense = train_condense['target'].to_numpy()\nX_condense = train_condense.drop(['id','target'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train_condense , X_test_condense , y_train_condense , y_test_condense  = train_test_split(X_condense , y_condense , test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test_condense.shape[0], X_train_condense.shape[0], X_test_condense.shape[1]))","665803ed":"cross_validation_design = KFold(n_splits=7,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","2f74d3a6":"class OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal)","013b7238":"from xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\n# Random HyperParameters\nxgb_paramsbaseline = {'n_estimators': 7000,\n            'learning_rate': 0.16,\n            'subsample': 0.96,\n            'colsample_bytree': 0.12,\n            'max_depth': 2,\n            'booster': 'gbtree', \n            'reg_lambda': 100.1,\n            'reg_alpha': 15.9,\n            'random_state':40}\nXGBRbaseline = XGBRegressor(**xgb_paramsbaseline,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\"\n                   )","caf7d10e":"print(cat_columns)\nprint(num_columns )","36ca6933":"cat_columns2=['cat0', 'cat1', 'cat2']\ncat_columns1=['cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nnum_columns1=[ 'cont1', 'cont2', 'cont3', 'cont4', 'cont5',  'cont7',  'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nnum_columns2=['cont0','cont6','cont8']\nall_columns1=cat_columns1+cat_columns2+num_columns1+num_columns2\nif set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","cc952da4":"X.columns","3d97efdc":"# 3-featureengineer-featuresselectionpart3(version 17\/18) 0.71812\n# Cat Features  \nOrdinalencoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n           # SparseInteractions(degree=2)\n              )\n# Num Features \nRobustscaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        RobustScaler()\n)\nOrdinalEncoder_RobustScaler = make_column_transformer(\n    ( Ordinalencoder , cat_columns),\n    ( Robustscaler, num_columns))\ncat_columns_reduced=[ 'cat1',  'cat3',  'cat5',  'cat7', 'cat8',\n       'cat9']\nOrdinalEncoder_RobustScaler_reduced = make_column_transformer(\n    ( Ordinalencoder , cat_columns_reduced),\n    ( Robustscaler, num_columns))\n    \nUseful_Features=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nReduced_Features=[ 'cat1',  'cat3',  'cat5',  'cat7', 'cat8',\n       'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n# test: 0.7044010096971889 \/  Public : 17\/17\nN_FOLD = 10\ntest_final= test.drop(['id'], axis=1)\n#Setting the kfold parameters\nkf = KFold(n_splits=N_FOLD, shuffle=True, random_state = 50)\n\noof_preds = np.zeros((X.shape[0],))\npredictions = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(kf.split(X)):\n    # split the train data into train and validation\n    X_train = X.iloc[train_idx][Reduced_Features]\n    X_valid = X.iloc[valid_idx][Reduced_Features]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n    X_train_weight=X.iloc[train_idx]['weight']\n    # Define the model \n    xgb_params1 = xgb_params3 = {\n    #'tree_method':'gpu_hist',         ## parameters for gpu\n    #'gpu_id':0,                       #\n    #'predictor':'gpu_predictor',      #\n    'n_estimators': 10000,\n    'learning_rate': 0.03628302216953097,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3,\n    'booster': 'gbtree', \n    'reg_lambda': 0.0008746338866473539,\n    'reg_alpha': 23.13181079976304,\n    'n_jobs':-1,\n    'random_state':40}\n    XGBR1 = XGBRegressor(**xgb_params1,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\",\n                    #eval_set = ((X_valid,y_valid)),\n                    verbose = -1, \n                     #early_stopping_rounds = 200\n                   )\n    XGBROrdinalEncoderRobust= Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler_reduced),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR1)]) \n    # Train the model\n    XGBROrdinalEncoderRobust.fit(X_train,\n                                 y_train,\n                                 #classifier__eval_metric=\"rmse\"\n                                 #classifier__eval_set = ((X_valid,y_valid)),\n                                 #classifier__early_stopping_rounds = 200,\n                                 classifier__sample_weight=X_train_weight)\n\n\n    #Mean of the predictions\n    predictions += XGBROrdinalEncoderRobust.predict(test_final[Reduced_Features]) \/ N_FOLD\n    \n    #Mean of feature importance\n    #model_fi += XGBROrdinalEncoderRobustScalerwithoutreduction_features144.feature_importances_ \/ N_FOLD \n    \n    #Out of Fold predictions\n    oof_preds[valid_idx] = XGBROrdinalEncoderRobust.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ N_FOLD\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","aa4fa4d7":"from sklearn import set_config\nset_config(display='diagram')\n#XGBRpreprocessordinalquantiletransformerbaseline","3b443d65":"preds_valid = XGBROrdinalEncoderRobust.predict(X)\nprint(mean_squared_error(y, preds_valid, squared=False))","2009a09d":"test_final= test.drop(['id'], axis=1)\n# Use the model to generate predictions\npredictions = XGBROrdinalEncoderRobust.predict(test_final)\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,'target': predictions})\noutput.to_csv('XGBROrdinalEncoderRobust1.csv', index=False)","c67f1d32":"# Select Best Pipe and retrain on all data ","aa26a584":"### Num Features ","1e4f0d44":"###  Conculsion : \nSeveral real world prediction problems involve forecasting rare values of a target variable. When this variable is nominal we have a problem of class imbalance that was already studied thoroughly within machine learning. For regression tasks, where the target variable is continuous, few works exist addressing this type of problem. Still, important application areas involve forecasting rare extreme values of a continuous target variable. Namely, we propose to address such tasks by sampling approaches. These approaches change the distribution of the given training data set to decrease the problem of imbalance between the rare target cases and the most frequent ones. We present a modification of the well-known Smote algorithm that allows its use on these regression tasks\n\n\n\n####  Data Centric approch : \nA Python implementation of Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise (SMOGN). Conducts the Synthetic Minority Over-Sampling Technique for Regression (SMOTER) with traditional interpolation, as well as with the introduction of Gaussian Noise (SMOTER-GN). Selects between the two over-sampling techniques by the KNN distances underlying a given observation. If the distance is close enough, SMOTER is applied. If too far away, SMOTER-GN is applied. Useful for prediction problems where regression is applicable, but the values in the interest of predicting are rare or uncommon. This can also serve as a useful alternative to log transforming a skewed response variable, especially if generating synthetic data is also of interest. \n#### Add sample weight :\n            fit(df[\"Var1\"], df[\"Var2\"], sample_weight=df[\"N\"])\n            \nThe weights enable training a model that is more accurate for certain values of the input (e.g., where the cost of error is higher). Internally, weights w are multiplied by the residuals in the loss function \n   \n![image.png](attachment:ff5293c3-4362-42c1-8ea3-326a683b94b9.png)!\n\n         Create equal weights and then augment the last 2 ones\n        sample_weight = np.ones(n_samples) * 20\n        sample_weight[-2:] *= 30\n\n        plt.scatter(X, y, s=sample_weight, c='grey', edgecolor='black')\n\n        # The unweighted model\n        regr = LinearRegression()\n        regr.fit(X, y)\n        plt.plot(X, regr.predict(X), color='blue', linewidth=3, label='Unweighted model')\n\n         The weighted model\n        regr = LinearRegression()\n        regr.fit(X, y, sample_weight)\n        plt.plot(X, regr.predict(X), color='red', linewidth=3, label='Weighted model')\n\n         The weighted model - scaled weights\n        regr = LinearRegression()\n        sample_weight = sample_weight \/ sample_weight.max()\n        regr.fit(X, y, sample_weight)\n        plt.plot(X, regr.predict(X), color='yellow', linewidth=2, label='Weighted model - scaled', linestyle='dashed')\n        plt.xticks(());plt.yticks(());plt.legend();\n        \n        model = XGBRegressor()\n        model.fit(df[['A','B']],df['D'],sample_weight=df['C'])\n        sudo apt-get install python3.6\n        sudo apt-get install git\n        git clone \u2013recursive https:\/\/github.com\/dmlc\/xgboost\n        cd xgboost; make -j4\n        cd python-package; python3 setup.py install\n        \nMore explanation : \n\n19\n\nThe sample_weight parameter allows you to specify a different weight for each training example. The scale_pos_weight parameter lets you provide a weight for an entire class of examples (\"positive\" class).\n\nThese correspond to two different approaches to cost-sensitive learning. If you believe that the cost of misclassifying positive examples (missing a cancer patient) is the same for all positive examples (but more than misclassifying negative ones, e.g. telling someone they have cancer when they actually don't) then you can specify one single weight for all positive examples via scale_pos_weight.\n\nXGBoost treats labels = 1 as the \"positive\" class. This is evident from the following piece of code:\n\nif (info.labels[i] == 1.0f) w *= param_.scale_pos_weight\n\nSee this question.\n\nThe other scenario is where you have example-dependent costs. One example is detecting fraudulent transactions. Not only a false negative (missing a fraudulent transaction) is more costly than a false positive (blocking a legal transaction), but the cost of missing a false negative is proportional to the amount of money being stolen. So you want to give larger weights to positive (fraudulent) examples with higher amounts. In this case, you can use the sample_weight parameter to specify example-specific weights.\n\n\n\n\nAll the documentation says that is should be:\n\n    scale_pos_weight = count(negative examples)\/count(Positive examples)\n\nIn practice, that works pretty well, but if your dataset is extremely unbalanced I'd recommend using something more conservative like:\n\n    scale_pos_weight = sqrt(count(negative examples)\/count(Positive examples)) \n\nThis is useful to limit the effect of a multiplication of positive examples by a very high weight.\n","0ecef983":"In order to more understand this notbook it will be more better to see this notebook first : \nhttps:\/\/www.kaggle.com\/bannourchaker\/4-featureengineer-featuresselectionpart4   \n**Upvote if you find them useful :)**","c1256a01":"    # Preprocess\n        preprocessordinalbinaryquantiletransformer = make_column_transformer(\n            ( ordinalencoder , cat_columns1),\n            ( binaryencoder , cat_columns2),\n            ( quantiletransformer , num_columns))\n    wieght 5.5 factor 1.2\n    0.7506995616992462\n    weight 2 and factor 1.5  :\n    Overall RMSE: 0.7182965850726257\n    \n    1.2 \n    Overall RMSE: 0.71776189577583\n    \n    Overall RMSE: 0.7176093013991661\n","c3bf1970":"# Smong Data: ","26714916":"0.6512159619901037","d9cb94a8":"### Num\/Cat Features ","95ffa099":"# Final Data Centirc Pipe : \n**factor1.2**\n\nfeatures to elimnate  : \n\n['cat0', 'cat2', 'cont1', 'cat4'] \n\n    X_test.pop(\"cat2\") X_test.pop(\"cat6\") X_test.pop(\"cat4\") X_test.pop(\"cat3\")\n    \n    cat2,cat4,cat6,cat0,cat1,\n    \n    cat0,cat2 ,cat4,cat6 ","da2fcbd2":"# Convert Dtypes : ","4ebb4ca1":"## Compelete prerocess pipe for  Cat dara ","5c193d10":"## Define the model features and target\n### Extract X and y ","82aadc94":"### Duplicates ","85290c62":"## check that we have all column","6c871015":"### Box plot of target data with percentile of .1% and 99.9%","b1097937":"# Weight 2","ebedd626":"##  Target \n###  exploring target data main statistics","b358943f":"        ## specify phi relevance values\n        rg_mtrx = [\n            [6.26222674563661,  1, 0],  ## over-sample (\"minority\")\n            [10.208478901823753, 1, 0],  ## over-sample (\"minority\")\n        ]\n        ## conduct smogn\n        train_smogn = smogn.smoter(\n\n            ## main arguments\n            data = trainmong,           ## pandas dataframe\n            y = 'target',             ## string ('header name')\n            k = 3,                    ## positive integer (k < n)\n            samp_method = 'extreme',  ## string ('balance' or 'extreme')\n           # phi relevance arguments\n            rel_thres = 0.80,         ## positive real number (0 < R < 1)\n            rel_method = 'auto',      ## string ('auto' or 'manual')\n            rel_xtrm_type = 'both',   ## string ('low' or 'both' or 'high')\n            rel_coef = 1.5           ## positive real number (0 < R)\n        )","e1bf7f21":"### Bin target ","ac24cc50":"reference : \nhttps:\/\/stackoverflow.com\/questions\/48079973\/xgboost-sample-weights-vs-scale-pos-weight\n\nhttps:\/\/stackoverflow.com\/questions\/67303447\/how-to-use-downsampling-and-configure-class-weight-parameter-when-using-xgboost\n","fefdbf6d":"## Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","d11edfe7":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","7486de42":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","6c9aa898":"# Train Catboost \/ Xgboost \/ Lgbm\n## Define Baseline XGBR ","1320eb74":"### Distribution of Target","b41cbe8d":"# Weight 1","98a2b707":"# Data Modeling\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.","b0c375ff":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n"}}