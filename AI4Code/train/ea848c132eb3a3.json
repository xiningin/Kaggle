{"cell_type":{"be755538":"code","fa6a97ab":"code","94c4a3fa":"code","9cf5d91d":"code","30f9b52c":"code","0457a7f4":"code","df51a6a3":"code","a7f46bc9":"code","69ed5138":"code","6660eb0f":"code","0cfac321":"code","07593802":"code","797872c3":"code","654cb960":"code","aaa40443":"code","88971e4c":"code","1a204781":"markdown","3d8e427b":"markdown","fbfacaee":"markdown","95354e7b":"markdown","635cb177":"markdown","6f5e3fdc":"markdown","6bb82c1b":"markdown","617888fc":"markdown","732b4d32":"markdown","4a5e335f":"markdown","13b0769a":"markdown","b5c6ab09":"markdown","6c3edb46":"markdown","17f966f5":"markdown","34541192":"markdown","ba914d20":"markdown"},"source":{"be755538":"import re\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nimport numpy as np\nfrom tqdm import tqdm\nimport bz2\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport nltk\nimport pandas as pd\n","fa6a97ab":"def splitReviewsLabels(lines):\n    reviews = []\n    labels = []\n    for review in tqdm(lines):\n        rev = reviewToX(review)\n        label = reviewToY(review)\n        reviews.append(rev[:512])\n        labels.append(label)\n    return reviews, labels","94c4a3fa":"def reviewToY(review):\n    return 0 if review.split(' ')[0] == '__label__1' else 1 ","9cf5d91d":"def reviewToX(review):\n    review = review.split(' ', 1)[1][:-1].lower()\n    review = re.sub('\\d','0',review)\n    if 'www.' in review or 'http:' in review or 'https:' in review or '.com' in review:\n        review = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", review)\n    return review","30f9b52c":"train_file = bz2.BZ2File('..\/input\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/test.ft.txt.bz2')","0457a7f4":"train_lines = train_file.readlines()\ntest_lines = test_file.readlines()","df51a6a3":"train_lines = [x.decode('utf-8') for x in train_lines[:20000]]\ntest_lines = [x.decode('utf-8') for x in test_lines[:2000]]","a7f46bc9":"# Load from the file\ntrain_x, train_y = splitReviewsLabels(train_lines)\ntest_x,test_y = splitReviewsLabels(test_lines)","69ed5138":"train_x[6]\nprint(train_y[6])","6660eb0f":"encoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\ntest_y = encoder.fit_transform(test_y)","0cfac321":"print(np.unique(train_y))","07593802":"pi=np.array([sum(train_y==0)\/len(train_y),sum(train_y==1)\/len(train_y)])\npi","797872c3":"count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(train_x)\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(train_x).todense()\nxtest_count =  count_vect.transform(test_x).todense()","654cb960":"wordFreq = pd.DataFrame(columns=['words','class1','class2'])\nwordFreq['words'] = count_vect.get_feature_names()\n\nx_train_class1 = xtrain_count[train_y==0]\nx_train_class2 = xtrain_count[train_y==1]\n\ncount_class1 = np.sum(x_train_class1,axis=0)\ncount_class2 = np.sum(x_train_class2,axis=0)\n\nvocab_size1 = len(np.where(count_class1==0)[1])\nvocab_size2 = len(np.where(count_class2==0)[1])\n\nalpha=10\ncount_class1 = np.array( (count_class1+alpha) \/(np.sum(count_class1)+vocab_size1 +1))\ncount_class2 = np.array( (count_class2+alpha) \/(np.sum(count_class2)+vocab_size2 +1))\n\nwordFreq['class1'] = pd.Series(count_class1.ravel())\nwordFreq['class2'] = pd.Series(count_class2.ravel())","aaa40443":"train_preds = np.zeros(len(xtrain_count))\nfor i in range(len(xtrain_count)):\n    idx = np.where(xtrain_count[i,:]!=0)[1]\n    lh1 = wordFreq['class1'].iloc[idx].prod()\n    lh2 = wordFreq['class2'].iloc[idx].prod()\n    posterior1 = lh1*pi[0]\n    posterior2 = lh2 * pi[1]\n\n    if posterior1>posterior2:\n        train_preds[i] = 0\n    else:\n        train_preds[i] = 1\n\n\nmatches = np.sum(train_y==train_preds)\nprint('Train accuracy is: '+str(matches\/len(train_preds)))","88971e4c":"test_preds = np.zeros(len(xtest_count))\nfor i in range(len(xtest_count)):\n    idx = np.where(xtest_count[i,:]!=0)[1]\n    lh1 = wordFreq['class1'].iloc[idx].prod()\n    lh2 = wordFreq['class2'].iloc[idx].prod()\n    posterior1 = lh1*pi[0]\n    posterior2 = lh2 * pi[1]\n\n    if posterior1>posterior2:\n        test_preds[i] = 0\n    else:\n        test_preds[i] = 1\n\n    temp = 1\n\nmatches = np.sum(test_y==test_preds)\nprint('Validation accuracy is: '+str(matches\/len(test_preds)))","1a204781":"\nOne of the most import libraries for natural language processing is NLTK. It provides more than 50 corpora and lexical resources and interfaces to work with them, also it provides text processing libraries including tokenization, stemming, parsing, classification and etc.","3d8e427b":"We know that we can not feed text as string to the model, so we need a way to represent text as numerical value to the model. One of the easiest encoding that we can use is the Bag of Words (BoW) encoding. It takes into account words and their frequency of occurrence in the sentence.","fbfacaee":"The next step is to calculate the probability of occurrence of each word per class, i.e., likelihood. To do that we are going to create a dataframe that contain three columns: words,class1,and class2. The class1 column will be containing the likelihood for class1 (negative reviews) and class2 column is going to contain the likelihood for class2 (positive reviews).","95354e7b":"\nValidation accuracy is 0.812, which is very promising given the simplicity of the model. Our objective was to learn naive bayes and implement it from scratch without library. we succeded in doing that.\n","635cb177":"However, one important technicality must be considered. The thing is that since some words might not occur in one of the classes, then the likelihood of those words will become zero. Having a zero likelihood for a word is very damaging to the model's performance because it causes the posterior to be zero which does not make sense. To elevate this problem a kind of smoothing which is called Laplace smoothing is used which is defined as follows.\n\n$$ P(X_i \\mid C_j) = \\frac{count_{ij}+\\alpha}{count_j+|V|+1} $$","6f5e3fdc":"In the above formula $ count_{ij} $ is the number of occurrences of word i in class j. $ count_j $ is total number of words in class j and |v| is the vocab size in class j.","6bb82c1b":"Due to limited computing capacity,we are truncating the dataset into 40000 training set and 4000 test set.","617888fc":"The CountVectorizer function from sklearn.feature_extraction.text is the function that encode text using the BoW method. It basically returns the matrix of token counts.","732b4d32":"However for improving performance there are more sophisticated methods like RNN,LSTM Transformers and so on","4a5e335f":"Now changing to labels.0 for negative,1 for poisitive reviews.","13b0769a":"Function to read the dataset","b5c6ab09":"The next step is to calculate the probability of each class. Doing so we will obtain priors in the Bayes formula. It can be easily done by counting number of occurrences of each class divided by the total number of reviews.","6c3edb46":"let's see how are the reviews.","17f966f5":"The next step is to load a data set to work with.Amazon reviews dataset which contains 360000 text reviews and their tags is used. This dataset contains positive and negative reviews. The task is to classify reviews into either of classes. This task is known as sentiment analysis in NLP.","34541192":"Now it is time to iterate through all sentences for both train and validation data to calculate the accuracy on both sets.","ba914d20":"We need to encode the labels into numerical values since the values of the labels are categorical."}}