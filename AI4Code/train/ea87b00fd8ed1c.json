{"cell_type":{"28066cdb":"code","33ca7f20":"code","df787bb3":"code","6fd16199":"code","d21245e9":"code","efd51a0b":"code","f7b257e1":"code","89a810a3":"code","2f93247a":"code","077692c4":"code","1e90df96":"code","85e90f16":"code","7d44dc88":"code","df75fe27":"code","8c60687b":"code","c635318b":"code","3d524574":"code","f2ea9d9d":"code","c7fa01a4":"code","6256a2c2":"code","b7d21965":"code","9da6f696":"code","de1fb221":"code","d36d27bb":"code","2d15afeb":"code","490e156f":"code","815ca440":"code","272caa30":"code","fc19b8e5":"code","9df34018":"code","4d78df47":"code","279519cb":"code","e2b25aaf":"code","edbe7225":"code","ebf8d34d":"code","f64294c3":"code","f6e9fca1":"code","4545778a":"code","9c6a458b":"code","66723796":"code","94660c42":"code","6936bbc4":"code","ae916eed":"code","d9f1d713":"code","498430e5":"code","27b1d133":"code","d013c62f":"code","f17504c5":"code","726d0473":"code","cd8652a7":"code","e1074755":"code","85651c76":"code","3fedd50f":"code","2dddf3c8":"code","0e0a3beb":"code","8fe18f1e":"code","f2f20986":"code","6622a811":"code","a79a1c06":"code","c417a6b9":"code","a20293e1":"code","cb8a0f85":"code","26b6fe50":"code","8b13e268":"code","5fff6177":"code","d1a90834":"code","2f804717":"markdown","8e5ea41e":"markdown","0c969016":"markdown","7c3e05a6":"markdown","bcd8b882":"markdown","21907dca":"markdown","c49935cc":"markdown","98488765":"markdown","c8615ffc":"markdown","e9b3e88f":"markdown","85a8c054":"markdown","2d67bc39":"markdown"},"source":{"28066cdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33ca7f20":"# Import necessary packages\nimport pandas as pd\nimport numpy as np\nimport os\nimport PIL\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nfrom PIL import *\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\nfrom IPython.display import display\nfrom tensorflow.python.keras import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers,optimizers\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import backend as K\nfrom keras import optimizers\nimport matplotlib.pyplot as plt","df787bb3":"# Load the facial Key points Data\nkeyfacial_df=pd.read_csv(\"\/kaggle\/input\/emotion-ai\/Emotion AI Dataset\/data.csv\")","6fd16199":"keyfacial_df.head()","d21245e9":"# obtain the Relavent information about the data frame\nkeyfacial_df.info()","efd51a0b":"# Check if any Null value exists on the Dataframe\nkeyfacial_df.isnull().sum()","f7b257e1":"keyfacial_df[\"Image\"].shape","89a810a3":"# Since values for the image are given as space separated string, separate the values using ' ' as separator.\n# Then convert this into numpy array using np.fromstring and convert the obtained 1D array into 2D array of shape (96, 96)\nkeyfacial_df[\"Image\"]=keyfacial_df[\"Image\"].apply(lambda x: np.fromstring(x,dtype=int,sep=\" \").reshape(96,96))","2f93247a":"# Obtain the shape of the Image\nkeyfacial_df[\"Image\"][0].shape","077692c4":"#Obtain the average, minimum and maximum values for 'right_eye_center_x'\nmaax=keyfacial_df['right_eye_center_x'].max()\nmiin=keyfacial_df['right_eye_center_x'].min()\navg=keyfacial_df['right_eye_center_x'].mean()\nprint(f\"Max = {maax} Min= {miin} Average = {avg}\")","1e90df96":"# Plot a random image from the dataset along with facial keypoints.\n# Image data is obtained from df['Image'] and plotted using plt.imshow\n# 15 x and y coordinates for the corresponding image \n# since x-coordinates are in even columns like 0,2,4,.. and y-coordinates are in odd columns like 1,3,5,..\n# we access their value using .loc command, which get the values for coordinates of the image based on the column it is refering to.\ni=np.random.randint(1,len(keyfacial_df))\n\nplt.imshow(keyfacial_df[\"Image\"][i],cmap='gray')\nfor j in range(1,31,2):\n    plt.plot(keyfacial_df.loc[i][j-1],keyfacial_df.loc[i][j],'rx')","85e90f16":"# Let view More Image in a Grid\nfig=plt.figure(figsize=(20,20))\n\nfor i in range(16):\n    ax=fig.add_subplot(4,4,i+1)\n    image=plt.imshow(keyfacial_df[\"Image\"][i],cmap='gray')\n    for j in range(1,31,2):\n        plt.plot(keyfacial_df.loc[i][j-1],keyfacial_df.loc[i][j],'rx')","7d44dc88":"#Perform a sanity check on the data by randomly visualizing 64 new images along with their cooresponding key points\nfig=plt.figure(figsize=(20,20))\nfor i in range(64):\n    k=np.random.randint(1,len(keyfacial_df))\n    ax=fig.add_subplot(8,8,i+1)\n    image=plt.imshow(keyfacial_df[\"Image\"][k],cmap='gray')\n    for j in range(1,31,2):\n        plt.plot(keyfacial_df.loc[k][j-1],keyfacial_df.loc[k][j],'rx')","df75fe27":"# Create a new copy of the DataFrame\nimport copy\nkeyfacial_df_copy=copy.copy(keyfacial_df)","8c60687b":"# Obtain Columns in dataframe\ncolumns=keyfacial_df_copy.columns[:-1]\ncolumns","c635318b":"# Horizontal Flip - flip the Images along y axis\nkeyfacial_df_copy[\"Image\"]=keyfacial_df_copy[\"Image\"].apply(lambda x : np.flip(x,axis=1))\n\n# since we are flipping horizontally, y coordinate values would be the same\n# Only x coordiante values would change, all we have to do is to subtract our initial x-coordinate values from width of the image(96)\nfor i in range(len(columns)):\n    if i%2==0:\n        keyfacial_df_copy[columns[i]]=keyfacial_df_copy[columns[i]].apply(lambda x:96. - float(x))\n","3d524574":"# Show The Original Image\nplt.imshow(keyfacial_df[\"Image\"][0],cmap='gray')\nfor j in range(1,31,2):\n    plt.plot(keyfacial_df.loc[0][j-1],keyfacial_df.loc[0][j],'rx')","f2ea9d9d":"# Show the Horizontally Flipped Image\nplt.imshow(keyfacial_df_copy[\"Image\"][0],cmap=\"gray\")\nfor j in range(1,31,2):\n    plt.plot(keyfacial_df_copy.loc[0][j-1],keyfacial_df_copy.loc[0][j],'rx')","c7fa01a4":"# Concatenate the original dataframe with the augmented dataframe\naugmented_df=np.concatenate((keyfacial_df,keyfacial_df_copy))","6256a2c2":"augmented_df.shape","b7d21965":"# Randomingly increasing the brightness of the images\n# We multiply pixel values by random values between 1.5 and 2 to increase the brightness of the image\n# we clip the value between 0 and 255\nimport random\nkeyfacial_df_copy=copy.copy(keyfacial_df)\nkeyfacial_df_copy[\"Image\"]=keyfacial_df_copy[\"Image\"].apply(lambda x : np.clip(random.uniform(1.5,2)*x,0.0,255.0))\naugmented_df=np.concatenate((augmented_df,keyfacial_df_copy))\naugmented_df.shape","9da6f696":"# show Image with Increased Brightness\nplt.imshow(keyfacial_df_copy[\"Image\"][0],cmap='gray')\nfor j in range(1,31,2):\n    plt.plot(keyfacial_df_copy.loc[0][j-1],keyfacial_df_copy.loc[0][j],'rx')","de1fb221":"keyfacial_df_copy=copy.copy(keyfacial_df)\n# Vertical flip along x axis\nkeyfacial_df_copy[\"Image\"]=keyfacial_df_copy[\"Image\"].apply(lambda x : np.flip(x,axis=0))\n# since we are flipping vertically, x coordinate values would be the same\n# Only y coordiante values would change, all we have to do is to subtract our initial y-coordinate values from height of the image(96)\nfor j in range(len(columns)):\n    if(j%2!=0):\n        keyfacial_df_copy[columns[j]]=keyfacial_df_copy[columns[j]].apply(lambda x : 96.- float(x))\n        \n        \n","d36d27bb":"# show the Original Image\nplt.imshow(keyfacial_df[\"Image\"][1],cmap='gray')\nfor j in range(1,31,2):\n    plt.plot(keyfacial_df.loc[1][j-1],keyfacial_df.loc[1][j],'rx')","2d15afeb":"# Now see the vertically flipped Image\nplt.imshow(keyfacial_df_copy[\"Image\"][1],cmap='gray')\nfor j in range(1,31,2):\n    plt.plot(keyfacial_df_copy.loc[1][j-1],keyfacial_df_copy.loc[1][j],'rx')","490e156f":"# Obtain the value of images which is present in the 31st column (since index start from 0, we refer to 31st column by 30)\nimg=augmented_df[:,-1]\n\n# Normalize the Image\nimg=img\/255.\n\n# Create an empty array of shape (x, 96, 96, 1) to feed the model\nX=np.empty((len(img),96,96,1))\n\n# Iterate through the img list and add image values to the empty array after expanding it's dimension from (96, 96) to (96, 96, 1)\nfor i in range(len(img)):\n    X[i,]=np.expand_dims(img[i],axis=2)\n    \n# Convert the Array type to float32\nX=np.asarray(X).astype(np.float32)\nX.shape","815ca440":"# Obtain the value of x & y coordinates which are to used as target.\ny=augmented_df[:,:30]\ny=np.asarray(y).astype(np.float32)\ny.shape","272caa30":"# Split the data into train and test data\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","fc19b8e5":"print(X_train.shape)\nprint(y_train.shape)","9df34018":"print(X_test.shape)\nprint(y_test.shape)","4d78df47":"def res_block(X,filter,stage):\n    # convolutional Block\n    X_copy=X\n    f1,f2,f3=filter\n    \n    # Main Path\n    X=Conv2D(f1,(1,1),strides=(1,1),name='res_'+str(stage)+'_conv_a',kernel_initializer=glorot_uniform(seed=0))(X)\n    X=MaxPool2D((2,2))(X)\n    X=BatchNormalization(axis=3,name='bn_'+str(stage)+'_conv_a')(X)\n    X=Activation('relu')(X)\n    \n    X=Conv2D(f2,(1,1),strides=(1,1), padding='same',name='res_'+str(stage)+'_conv_b',kernel_initializer=glorot_uniform(seed=0))(X)\n    X=BatchNormalization(axis=3,name='bn_'+str(stage)+'_conv_b')(X)\n    X=Activation('relu')(X)\n    \n    X=Conv2D(f3,kernel_size=(1,1),strides=(1,1),name='res_'+str(stage)+'_conv_c',kernel_initializer=glorot_uniform(seed=0))(X)\n    X=BatchNormalization(axis=3,name='bn_'+str(stage)+'_conv_c')(X)\n    \n    \n    # short Path\n    X_copy=Conv2D(f3,kernel_size=(1,1),strides=(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n    X_copy=MaxPool2D((2,2))(X_copy)\n    X_copy=BatchNormalization(axis=3,name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n    \n    # Add\n    X=Add()([X,X_copy])\n    X=Activation('relu')(X)\n    \n    # Identity Block 1\n    X_copy = X\n\n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n\n    # ADD\n    X = Add()([X,X_copy])\n    X = Activation('relu')(X)\n    \n    # Identity Block 2\n    X_copy=X\n    \n    # Identity Block 2\n    X_copy = X\n\n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n\n    # ADD\n    X = Add()([X,X_copy])\n    X = Activation('relu')(X)\n\n    return X\n    \n","279519cb":"input_shape=(96,96,1)\n\n# Input Tensor shape\nX_input=Input(input_shape)\n\n# Zero Padding\nX=ZeroPadding2D((3,3))(X_input)\n\n# 1- stage\nX=Conv2D(64,(7,7),strides=(2,2),name='conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\nX = BatchNormalization(axis =3, name = 'bn_conv1')(X)\nX = Activation('relu')(X)\nX = MaxPooling2D((3,3), strides= (2,2))(X)\n\n# 2 - stage\nX=res_block(X,filter=[64,64,256],stage=2)\n\n# 3 - stage\nX = res_block(X, filter= [128,128,512], stage= 3)\n\n# Average Pooling\nX = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n\n# Final layer\nX = Flatten()(X)\nX = Dense(4096, activation = 'relu')(X)\nX = Dropout(0.2)(X)\nX = Dense(2048, activation = 'relu')(X)\nX = Dropout(0.1)(X)\nX = Dense(30, activation = 'relu')(X)\n\nmodel_1_facialKeyPoints=Model(inputs=X_input,outputs=X)\nmodel_1_facialKeyPoints.summary()\n\n","e2b25aaf":"adam=tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.9,beta_2=0.999,amsgrad=False)\nmodel_1_facialKeyPoints.compile(loss=\"mean_squared_error\",optimizer=adam,metrics=['accuracy'])\n","edbe7225":"# save the best model with least validation loss\ncheckpointer=ModelCheckpoint(filepath=\"FacialKeyPoints_weights.hdf5\",verbose=1,save_best_only=True)","ebf8d34d":"history=model_1_facialKeyPoints.fit(X_train,y_train,batch_size=64,epochs=70,validation_split=0.1,callbacks=[checkpointer])","f64294c3":"# Get the model keys\nhistory.history.keys()","f6e9fca1":"# Plot the training artifacts\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train_loss','val_loss'], loc = 'upper right')\nplt.show()","4545778a":"with open('..\/input\/emotion-ai\/Emotion AI Dataset\/detection.json','r') as json_file:\n    json_savedModel=json_file.read()\n    \n# Load the Model Architecture\nmodel_1_facialKeyPoints=tf.keras.models.model_from_json(json_savedModel)\nmodel_1_facialKeyPoints.load_weights('..\/input\/emotion-ai\/Emotion AI Dataset\/weights_keypoint.hdf5')\nadam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\nmodel_1_facialKeyPoints.compile(loss=\"mean_squared_error\", optimizer= adam , metrics = ['accuracy'])","9c6a458b":"# Evaluate the model\nresult=model_1_facialKeyPoints.evaluate(X_test,y_test)\nprint(\"Accuracy : {}\".format(result[1]))","66723796":"# Read the Csv files for the Facial Expression data\nfacialexpression_df=pd.read_csv('..\/input\/emotion-ai\/Emotion AI Dataset\/icml_face_data.csv')","94660c42":"facialexpression_df.head()","6936bbc4":"facialexpression_df[\" pixels\"][0] # String Format","ae916eed":"# function to convert pixel values in string format to array format\ndef string2array(x):\n    return np.array(x.split(' ')).reshape(48,48,1).astype('float32')","d9f1d713":"# Resize images from (48, 48) to (96, 96)\ndef resize(x):\n    img=x.reshape(48,48)\n    return cv2.resize(img,dsize=(96,96),interpolation=cv2.INTER_CUBIC)","498430e5":"facialexpression_df[' pixels']=facialexpression_df[' pixels'].apply(lambda x : string2array(x))","27b1d133":"facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: resize(x))","d013c62f":"facialexpression_df.head()","f17504c5":"# Check the shape of the Dataframe\nfacialexpression_df.shape","726d0473":"# check for the presence of null values in the data frame\nfacialexpression_df.isnull().sum()","cd8652a7":"label_to_text={0:'anger',1:'disgust',2:'sad',3:'happiness',4:'surprise'}","e1074755":"plt.imshow(facialexpression_df[' pixels'][1],cmap='gray');","85651c76":"emotions=[0,1,2,3,4]\n\nfor i in emotions:\n    data=facialexpression_df[facialexpression_df['emotion']==i][:1]\n    img=data[\" pixels\"].item()\n    img=img.reshape(96,96)\n    plt.figure()\n    plt.title(label_to_text[i])\n    plt.imshow(img,cmap='gray')\n","3fedd50f":"facialexpression_df.emotion.value_counts().index","2dddf3c8":"facialexpression_df.emotion.value_counts()","0e0a3beb":"plt.figure(figsize=(10,10))\nsns.barplot(x=facialexpression_df.emotion.value_counts().index,y=facialexpression_df.emotion.value_counts());","8fe18f1e":"# Split the Dataframe into features and labels\nfrom keras.utils import to_categorical\n\nX=facialexpression_df[' pixels']\ny=to_categorical(facialexpression_df['emotion'])","f2f20986":"X[0]","6622a811":"y","a79a1c06":"X=np.stack(X,axis=0)\nX=X.reshape(24568,96,96,1)\n\nprint(X.shape , y.shape)","c417a6b9":"# split the dataframe in to train, test and validation data frames\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_Test,y_train,y_Test=train_test_split(X,y,test_size=0.1,shuffle=True)\nX_val,X_Test,y_val,y_Test=train_test_split(X_Test,y_Test,test_size=0.5,shuffle=True)","a20293e1":"print(X_val.shape,y_val.shape)","cb8a0f85":"print(X_Test.shape,y_Test.shape)","26b6fe50":"print(X_train.shape,y_train.shape)","8b13e268":"# image pre-processing\nX_train=X_train\/255\nX_val=X_val\/255\nX_test=X_test\/255","5fff6177":"# X_train","d1a90834":"train_datagen=ImageDataGenerator(\n                rotation_range=15,\n                width_shift_range=0.1,\n                height_shift_range=0.1,\n                shear_range=0.1,\n                zoom_range=0.1,\n                horizontal_flip=True,\n                vertical_flip=True,\n                brightness_range=[1.1,1.5],\n                fill_mode='nearest')","2f804717":"# **Perform Data Normalization And Data Preparation**","8e5ea41e":"# **PART 2. FACIAL EXPRESSION DETECTION**\n>The second model will classify people's Emotion.\n>Data contains images belongs to 5 categories\n\n>>0=Angry  \n>>1=Disgust  \n>>2=Sad  \n>>3=Happy  \n>>4=Surprise  ","0c969016":"# **ASSESS TRAINED KEY FACIAL POINTS DETECTION MODEL PERFORMANCE**","7c3e05a6":"#### Augment images by flipping them vertically (Hint: Flip along x-axis and note that if we are flipping along x-axis, x co-ordinates won't change)","bcd8b882":"# **PERFORM DATA PREPARATION AND IMAGE AUGMENTATION**","21907dca":"# **COMPILE AND TRAIN KEY FACIAL POINTS DETECTION DEEP LEARNING MODEL**","c49935cc":"# ** Perform Image Augmentation **","98488765":"# **BUILD DEEP RESIDUAL NEURAL NETWORK KEY FACIAL POINTS DETECTION MODEL**","c8615ffc":"### **IMPORT & EXPLORE DATASET FOR FACIAL EXPRESSION DETECTION**\n\n","e9b3e88f":"### Plot the Bar chart outline How many samples(images) are present per emotion\n","85a8c054":"# **Visualize The Images**","2d67bc39":"## VISUALIZE IMAGES AND PLOT LABELS"}}