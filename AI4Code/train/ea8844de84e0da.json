{"cell_type":{"8ad01bbb":"code","14c7bb71":"code","fb713190":"code","93f04466":"code","45377ec5":"code","bbd994e7":"code","e5913e5e":"code","9e2d5c44":"code","60a34dc7":"code","c323269f":"code","edbdfd7f":"code","33356ea3":"code","c615dd60":"code","82b9399d":"code","e2087982":"code","0aee1630":"code","011fbc7b":"code","ac496f6a":"code","443e9906":"code","74f7d72a":"code","e86bd901":"code","9da410f4":"code","470fcbe8":"code","47389be3":"code","72edfd15":"markdown","a17bcaa3":"markdown","5e3d962c":"markdown","126fb7dc":"markdown","ae31305b":"markdown","f714ca3e":"markdown","a25d479b":"markdown","d0a033c7":"markdown","7544107d":"markdown","c0267a45":"markdown"},"source":{"8ad01bbb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom scipy.optimize import minimize\n\nfrom datetime import datetime\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, RandomizedSearchCV\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport os","14c7bb71":"X = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ncomp = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n\nprint(\"Train set size:\", X.shape)\nprint(\"Test set size:\", comp.shape)\nprint('START data processing',   )","fb713190":"from scipy.stats import norm\nsns.distplot(X[\"SalePrice\"],fit=norm)\nmu,sigma= norm.fit(X['SalePrice'])\nprint(\"mu {}, sigma {}\".format(mu,sigma))","93f04466":"########## REMOVING SKEWEENESS ###########\n\nX['SalePrice']=np.log1p(X['SalePrice'])\nsns.distplot(X[\"SalePrice\"],fit=norm)\nmu,sigma= norm.fit(X['SalePrice'])\nprint(\"mu {}, sigma {}\".format(mu,sigma))","45377ec5":"X_ID = X['Id']\ncomp_ID = comp['Id']\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\nX.drop(['Id'], axis=1, inplace=True)\ncomp.drop(['Id'], axis=1, inplace=True)\n\n# Deleting outliers\nX = X[X.GrLivArea < 4500]\nX.reset_index(drop=True, inplace=True)\n\n# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\nX[\"SalePrice\"] = np.log1p(X[\"SalePrice\"])\ny = X.SalePrice.reset_index(drop=True)\nX = X.drop(['SalePrice'], axis=1)\nX_comp = comp","bbd994e7":"X_all = pd.concat([X, X_comp]).reset_index(drop=True)\nprint(X_all.shape)\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nX_all['MSSubClass'] = X_all['MSSubClass'].apply(str)\nX_all['YrSold'] = X_all['YrSold'].astype(str)\nX_all['MoSold'] = X_all['MoSold'].astype(str)\n\nX_all['Functional'] = X_all['Functional'].fillna('Typ')\nX_all['Electrical'] = X_all['Electrical'].fillna(\"SBrkr\")\nX_all['KitchenQual'] = X_all['KitchenQual'].fillna(\"TA\")\nX_all['Exterior1st'] = X_all['Exterior1st'].fillna(X_all['Exterior1st'].mode()[0])\nX_all['Exterior2nd'] = X_all['Exterior2nd'].fillna(X_all['Exterior2nd'].mode()[0])\nX_all['SaleType'] = X_all['SaleType'].fillna(X_all['SaleType'].mode()[0])\n\nX_all[\"PoolQC\"] = X_all[\"PoolQC\"].fillna(\"None\")\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    X_all[col] = X_all[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    X_all[col] = X_all[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    X_all[col] = X_all[col].fillna('None')\n\nX_all['MSZoning'] = X_all.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nobjects = list(X_all.select_dtypes(include='object').columns)\n\nX_all[objects]=X_all[objects].fillna('None')\n\nX_all['LotFrontage'] = X_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Filling in the rest of the NA's\nnumerics = list(X_all.select_dtypes(exclude='object').columns)\nX_all[numerics] = X_all[numerics].fillna(0)","e5913e5e":"numerics2 = list(X_all.select_dtypes(exclude='object').columns)\n\nskew_features = X_all[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    X_all[i] = boxcox1p(X_all[i], boxcox_normmax(X_all[i] + 1))\n\nX_all = X_all.drop(['Utilities', 'Street', 'PoolQC', ], axis=1)\n\nX_all['YrBltAndRemod'] = X_all['YearBuilt'] + X_all['YearRemodAdd']\nX_all['TotalSF'] = X_all['TotalBsmtSF'] + X_all['1stFlrSF'] + X_all['2ndFlrSF']\n\nX_all['Total_sqr_footage'] = (X_all['BsmtFinSF1'] + X_all['BsmtFinSF2'] +\n                                 X_all['1stFlrSF'] + X_all['2ndFlrSF'])\n\nX_all['Total_Bathrooms'] = (X_all['FullBath'] + (0.5 * X_all['HalfBath']) +\n                               X_all['BsmtFullBath'] + (0.5 * X_all['BsmtHalfBath']))\n\nX_all['Total_porch_sf'] = (X_all['OpenPorchSF'] + X_all['3SsnPorch'] +\n                    X_all['EnclosedPorch'] + X_all['ScreenPorch'] + X_all['WoodDeckSF'])\n\n# simplified features\nX_all['haspool'] = X_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nX_all['has2ndfloor'] = X_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nX_all['hasgarage'] = X_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nX_all['hasbsmt'] = X_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nX_all['hasfireplace'] = X_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint(X_all.shape)\nfinal_X_all = pd.get_dummies(X_all).reset_index(drop=True)\nprint(final_X_all.shape)\n\nX = final_X_all.iloc[:len(y), :]\nX_comp = final_X_all.iloc[len(X):, :]\n\nprint('X', X.shape, 'y', y.shape, 'X_comp', X_comp.shape)\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_comp = X_comp.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_comp', X_comp.shape)","9e2d5c44":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","60a34dc7":"niter=20\nncv = 3","c323269f":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nxgb = XGBRegressor(objective='reg:squarederror')\n\nparams = {'n_estimators': sp_randint(100, 6000),\n         'max_depth' :sp_randint(1, 50),\n         'sub_sample': sp_uniform(0, 1),\n         'colsample_bytree': sp_uniform(0, 1),\n         'reg_alpha': sp_uniform(0,1)}\n\nrsearch = RandomizedSearchCV(xgb, param_distributions=params, n_iter=niter, cv=ncv, \n                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n#rsearch.fit(X, y)\n#xgb_params = rsearch.best_params_","edbdfd7f":"xgb_params = {'colsample_bytree': 0.529945192578405,\n 'max_depth': 1,\n 'n_estimators': 2377,\n 'reg_alpha': 0.22958335326689505,\n 'sub_sample': 0.824538323669472}","33356ea3":"lgbm = LGBMRegressor(objective='regression')\n\nparams = {'n_estimators': sp_randint(100, 6000),\n         'max_depth' :sp_randint(1, 150),\n          'num_leaves': sp_randint(10, 2500),\n         'bagging_fraction': sp_uniform(0, 1),\n         'feature_fraction': sp_uniform(0, 1),\n         'learning_rate': sp_uniform(0,1)}\n\nrsearch = RandomizedSearchCV(lgbm, param_distributions=params, n_iter=niter, cv=ncv, \n                             scoring='neg_root_mean_squared_error')\n#rsearch.fit(X, y)\n#lgbm_params = rsearch.best_params_","c615dd60":"lgbm_params = {'bagging_fraction': 0.3826025415942872,\n 'feature_fraction': 0.4152600001975988,\n 'learning_rate': 0.03491619499427312,\n 'max_depth': 2,\n 'n_estimators': 3496,\n 'num_leaves': 1323}","82b9399d":"gbr = GradientBoostingRegressor()\n\nparams = {'n_estimators': sp_randint(100, 6000),\n         'max_depth' :sp_randint(1, 150),\n          'max_features': sp_randint(10, 330),\n         'min_samples_split': sp_randint(2, 100),\n         'min_samples_leaf': sp_randint(1, 100),\n         'learning_rate': sp_uniform(0,1)}\n\nrsearch = RandomizedSearchCV(gbr, param_distributions=params, n_iter=niter, cv=ncv, \n                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n#rsearch.fit(X, y)\n#gbr_params = rsearch.best_params_","e2087982":"gbr_params = {'learning_rate': 0.3762972471885566,\n 'max_depth': 63,\n 'max_features': 121,\n 'min_samples_leaf': 19,\n 'min_samples_split': 73,\n 'n_estimators': 5546}","0aee1630":"rfr = RandomForestRegressor()\n\nparams = {'n_estimators': sp_randint(100, 6000),\n         'max_depth' :sp_randint(1, 16),\n         'min_samples_leaf': sp_randint(1, 100), \n         'min_samples_split': sp_randint(2, 100), \n         'max_features': sp_randint(10,330)}\n\nrsearch = RandomizedSearchCV(rfr, param_distributions=params, n_iter=niter, cv=ncv, \n                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n#rsearch.fit(X, y)\n#rfr_params = rsearch.best_params_","011fbc7b":"rfr_params = {'max_depth': 5,\n 'max_features': 185,\n 'min_samples_leaf': 1,\n 'min_samples_split': 22,\n 'n_estimators': 959}","ac496f6a":"gbr = ensemble.GradientBoostingRegressor(loss='ls', **gbr_params)\n\nxgb = XGBRegressor(objective='reg:squarederror', **xgb_params)\n\nlgbm = LGBMRegressor(objective='regression', **lgbm_params)\n\nrfr = RandomForestRegressor(**rfr_params)\n\ncb = CatBoostRegressor(loss_function='RMSE', logging_level='Silent')","443e9906":"def mean_cross_val(model, X, y):\n    score = -cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n    mean = score.mean()\n    return mean\n\nclfs = []\n\ngbr.fit(X_train, y_train)   \npreds = gbr.predict(X_test) \nrmse_gbr = np.sqrt(mean_squared_error(y_test, preds))\nscore_gbr = gbr.score(X_test, y_test)\ncv_gbr = mean_cross_val(gbr, X, y)\nprint('gbr:  ', cv_gbr)\nclfs.append(gbr)\n\nxgb.fit(X_train, y_train)   \npreds = xgb.predict(X_test) \nrmse_xgb = np.sqrt(mean_squared_error(y_test, preds))\nscore_xgb = xgb.score(X_test, y_test)\ncv_xgb = mean_cross_val(xgb, X, y)\nprint('xgb:  ', cv_xgb)\nclfs.append(xgb)\n\n\nlgbm.fit(X_train, y_train)   \npreds = lgbm.predict(X_test) \nrmse_lgbm = np.sqrt(mean_squared_error(y_test, preds))\nscore_lgbm = lgbm.score(X_test, y_test)\ncv_lgbm = mean_cross_val(lgbm, X, y)\nprint('lgbm:  ', cv_lgbm)\nclfs.append(lgbm)\n\n\nrfr.fit(X_train, y_train)   \npreds = rfr.predict(X_test) \nrmse_rfr = np.sqrt(mean_squared_error(y_test, preds))\nscore_rfr = rfr.score(X_test, y_test)\ncv_rfr = mean_cross_val(rfr, X, y)\nprint('rfr:  ', cv_rfr)\nclfs.append(rfr)\n\n\ncb.fit(X_train, y_train)   \npreds = cb.predict(X_test) \nrmse_cb = np.sqrt(mean_squared_error(y_test, preds))\nscore_cb = rfr.score(X_test, y_test)\ncv_cb = mean_cross_val(rfr, X, y)\nprint('cb:  ', cv_cb)\nclfs.append(cb)","74f7d72a":"model_performances = pd.DataFrame({\n    \"Model\" : [\"GradBRegr\", \"XGBoost\", \"LGBM\", \"RandomForest\"],\n    \"CV(5)\" : [str(cv_gbr)[0:5], str(cv_xgb)[0:5], str(cv_lgbm)[0:5], str(cv_rfr)[0:5]],\n    \"RMSE\" : [str(rmse_gbr)[0:5], str(rmse_xgb)[0:5], str(rmse_lgbm)[0:5], str(rmse_rfr)[0:5]],\n    \"Score\" : [str(score_gbr)[0:5], str(score_xgb)[0:5], str(score_lgbm)[0:5], str(score_rfr)[0:5]]\n})\n\nprint(\"Sorted by Score:\")\nprint(model_performances.sort_values(by=\"Score\", ascending=False))\n\ndef blend_models_predict(X1, a, b, c, d, e):\n    return (a*gbr.predict(X1) + b*xgb.predict(X1) + c*lgbm.predict(X1) + d*rfr.predict(X1) + e*cb.predict(X1))","e86bd901":"## Optimization\n\npredictions = []\nfor clf in clfs:\n    predictions.append(clf.predict(X_test))\n\ndef mse_func(weights):\n    #scipy minimize will pass the weights as a numpy array\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n            final_prediction += weight*prediction\n    #return np.mean((y_test-final_prediction)**2)\n    return np.sqrt(mean_squared_error(y_test, final_prediction))\n    \nstarting_values = [0]*len(predictions)\n\ncons = ({'type':'ineq','fun':lambda w: 1-sum(w)})\n#our weights are bound between 0 and 1\nbounds = [(0,1)]*len(predictions)\n\nres = minimize(mse_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n\nprint('Ensamble Score: {best_score}'.format(best_score=res['fun']))\nprint('Best Weights: {weights}'.format(weights=res['x']))","9da410f4":"yy = 'abcde'\ndd={yy[i]: k for i,k in enumerate(res['x'])}\nprint(dd)","470fcbe8":"def blend_models_predict(X1, **dd):\n    return (dd['a']*gbr.predict(X1) + dd['b']*xgb.predict(X1) + \n            dd['c']*lgbm.predict(X1) + dd['d']*rfr.predict(X1) + dd['e']*rfr.predict(X1))","47389be3":"subm = np.exp(blend_models_predict(X_comp, **dd))\nsubmission = pd.DataFrame({'Id': comp_ID, 'SalePrice': subm})\nsubmission.to_csv(\"submission36.csv\", index=False)\nprint('complete')","72edfd15":"The objective of this notebook is to show some examples of Hyperparameter tuning and optimal determination of weights (ensemble). First of all, I learnt a lot from other notebooks. Let me mention few of the useful notebooks below.\n\nhttps:\/\/www.kaggle.com\/hsperr\/finding-ensamble-weights\n\nhttps:\/\/www.kaggle.com\/mrmorj\/blend-skills-top-1-house-price\n\nhttps:\/\/www.kaggle.com\/namanj27\/how-i-ended-up-in-top-3-on-lb\n\nhttps:\/\/www.kaggle.com\/gowrishankarin\/learn-ml-ml101-rank-4500-to-450-in-a-day\n","a17bcaa3":"****We will prepare few features to reduce data size and increase interpretability","5e3d962c":"Five models are fitted and found to have different levels of accuracy. The idea of ensemble is to bring many models together to create a better predictor. A critical input of ensemble model is the weights being used to combine consitutent models. Given below is a framework that applies optimization to estimate the optimal weights.","126fb7dc":"# Submit","ae31305b":"Each of the models are fitted on the Train set and evaluated using Test. It is also evaluated based on Cross Validation on the entire dataset. ","f714ca3e":"## Advanced Regression: House Prices - lGBM, Gradient Boosting, Random Forest, CatBoost, XGBoost\n","a25d479b":"Each of the models are tuned using Random Search. Since it is lengthy, it is disengaged after the first solution. Why not try to expand the space and try to improve the solution??","d0a033c7":"# Models","7544107d":"# Detecting best weight to blending","c0267a45":"# Fit models"}}