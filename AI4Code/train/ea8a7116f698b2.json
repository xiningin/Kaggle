{"cell_type":{"4befb6fd":"code","ed81f0f9":"code","43fda29b":"code","f58c9986":"code","a2b16d4b":"code","8a6301b1":"code","ef09fa32":"code","db8bad96":"code","d95dbe72":"code","d692eeaf":"code","0006ade6":"code","af604f6d":"code","42888d82":"code","177f14a5":"code","798cafb1":"code","f2ef875b":"code","e12d11e3":"code","92db6a17":"code","89645d8c":"code","03c9db46":"code","b8264676":"code","1135475b":"code","7710b98d":"code","fee56af6":"code","0e6de59e":"code","0e84c8c3":"code","3dedd693":"code","e99c564d":"code","4eaec9b4":"code","181b1c35":"code","8b8f3510":"code","433c8490":"code","4aefc274":"code","bee73330":"code","1bac0df9":"code","af979821":"code","45e874e9":"code","60255d01":"code","227affde":"code","15421346":"code","aaecf312":"markdown","e6d10346":"markdown","39c827f8":"markdown","16c186ac":"markdown","6bf64718":"markdown","474fb431":"markdown","3532f455":"markdown","68262690":"markdown","a1b1fe3f":"markdown","ee98a222":"markdown","4688fb01":"markdown","de38d779":"markdown","8365d961":"markdown","6a822edd":"markdown","d9e15c68":"markdown","d098d1a8":"markdown","6bab81c1":"markdown","ee28569f":"markdown","e9403482":"markdown","1eec1615":"markdown","77d8c561":"markdown","ed3253ed":"markdown","6ae30667":"markdown","dc921c04":"markdown","9b247092":"markdown","2c527da7":"markdown","0267b672":"markdown","6501f033":"markdown","363b3f35":"markdown","1e465d19":"markdown","7751ab59":"markdown"},"source":{"4befb6fd":"!pip install matplotlib==3.4.3 #google colab version of plt is deprecated and doesn't support some methods used in this notebook","ed81f0f9":"!pip install xgboost==1.4.2 #let's guarantee that the colab version of xgboost is not depracated ","43fda29b":"#imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport scipy\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\nimport os","f58c9986":"#importing dataset \n\npwd = os.getcwd()\ndataset = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\noriginal_dataset = dataset.copy()\ndataset","a2b16d4b":"dataset.info()","8a6301b1":"#visualizing with null values\ndataset.isnull().sum().reset_index()","ef09fa32":"#imputing missing values\ndataset.fillna(value=dataset.median(), inplace=True)\ndataset","db8bad96":"dataset.isnull().sum().reset_index()","d95dbe72":"#defining color palette for this EDA\ncolors = ['salmon', 'tab:blue', 'tab:purple', 'tab:orange', 'tab:green', 'tab:pink', 'tab:grey', 'tab:olive', 'tab:red', 'tab:cyan']","d692eeaf":"corr = dataset.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nfig, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(corr, ax =ax, annot = True, mask = mask)\n\n\nplt.show()","0006ade6":"dataset.describe()","af604f6d":"dataset[dataset.ph < 5].groupby('Potability')['ph'].count()","42888d82":"dataset[dataset.ph > 9].groupby('Potability')['ph'].count()","177f14a5":"dataset.head()","798cafb1":"dataset = dataset.drop(dataset[dataset.ph > 9].index, axis = 'index')\ndataset = dataset.drop(dataset[dataset.ph < 5].index, axis = 'index')","f2ef875b":"dataset[((dataset.ph < 5) | ((dataset.ph > 9)))].groupby('Potability')['ph'].count()","e12d11e3":"fig, ax = plt.subplots(figsize = (10,8))\n\ndataset.groupby('Potability').size().plot(kind='pie', ax = ax, labels = ['Not Potable', 'Potable'], autopct = '%1.2f%%', colors = {'salmon', 'tab:blue'}, shadow = True, explode = (0.08, 0))\n\nplt.title('Water Potability')\nfig.set_facecolor('white')\nplt.tight_layout()\nplt.show()","92db6a17":"print(dataset.columns)","89645d8c":"fig, ax = plt.subplots(3,3, figsize = (12,24), squeeze = False)\ncolumns = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n\nj = 0\n\nfor column in columns:\n    sns.violinplot(y = dataset[column], ax = ax.flat[j],color = colors[j])\n    sns.swarmplot(y = dataset[column], ax = ax.flat[j], size = 2, color = 'black')\n    ax.flat[j].set_title(column + ' values:')\n    j = j + 1\n    \n\nplt.tight_layout()\nplt.show()","03c9db46":"#checking distribuition of data for scaling method selection\nfig, axes = plt.subplots(5, 2, squeeze=False, figsize = (16,16))\nfig.delaxes(axes.flat[9])\ncolumn = 0\n\nfor ax in axes.flat:\n    p = sns.kdeplot(dataset.iloc[:, column], ax = ax, color = colors[column])\n    \n    #ploting median line\n    x,y = p.get_lines()[0].get_data()\n    cdf = scipy.integrate.cumtrapz(y, x, initial = 0)\n    nearest_05 = np.abs(cdf - 0.5).argmin()\n    x_median = x[nearest_05]\n    y_median = y[nearest_05]\n    ax.vlines(x_median, 0, y_median, colors = 'black')\n\n    column +=1\n\nplt.suptitle('Graph shows that some variables have skewed distribuitions', size = 22)\nplt.tight_layout()\nplt.show()","b8264676":"#scaling variables\nfrom sklearn.preprocessing import StandardScaler\nX = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1:]\n#X = np.log(X + 1) #we add 1 becase log of numbers between 0 and 1 are NaN\n\n#separating trainning and test datasets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n\nsc = StandardScaler()\nsc.fit_transform(X_train)\nsc.transform(X_test)","1135475b":"#code to visualize model performance\n\ndef view_performance (y_test, y_pred, model = \"\"):\n\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, xticklabels=['Negative' , 'Positive'], yticklabels=['Negative' , 'Positive'], cmap = 'rocket', fmt = 'g')\n    plt.ylabel(\"Label\")\n    plt.xlabel(\"Predicted\")\n    title = 'Confusion Matrix for classifier'\n    plt.title(title)\n    plt.show\n    model = str(model)\n    cls_report = classification_report(y_test, y_pred)\n    model_recall = recall_score(y_test, y_pred, average = 'binary')\n    print(model, 'classifier results: \\n\\n\\n', cls_report)\n    return model_recall","7710b98d":"from sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(random_state = 0)\nclassifier_lr.fit(X_train, y_train)\ny_pred = classifier_lr.predict(X_test)","fee56af6":"recall_log= view_performance(y_test, y_pred, model = 'Logistic')","0e6de59e":"# Training the Kernel SVM model on the Training set\nfrom sklearn.svm import SVC\nclassifier_svm = SVC(kernel = 'rbf', random_state = 1, degree=1)\nclassifier_svm.fit(X_train, y_train)\ny_pred = classifier_svm.predict(X_test)","0e84c8c3":"recall_sv = view_performance(y_test, y_pred, model = 'SVM')","3dedd693":"# Training the Naive Bayes model on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB()\nclassifier_nb.fit(X_train, y_train)\ny_pred = classifier_nb.predict(X_test)","e99c564d":"recall_nb = view_performance(y_test, y_pred, model = \"Na\u00efve Bayes\")","4eaec9b4":"from sklearn.ensemble import RandomForestClassifier\nclassifier_rt = RandomForestClassifier(n_estimators=100, random_state=1)\nclassifier_rt.fit(X_train, y_train)\ny_pred = classifier_rt.predict(X_test)","181b1c35":"recall_rf = view_performance(y_test, y_pred, model = \"Random Forest\")","8b8f3510":"import xgboost as xg\n\nclassifier_xg = xg.XGBClassifier(use_label_encoder=False)\nclassifier_xg.fit(X_train, y_train)\ny_pred = classifier_xg.predict(X_test)","433c8490":"recall_xg = view_performance(y_test, y_pred, model = 'XGBoost')","4aefc274":"perf = pd.DataFrame.from_dict({'scores':[recall_nb, recall_xg, recall_log, recall_rf, recall_sv], 'models': ['Naive Bayes', 'XGBoost', 'Logistic', 'Random Forest', 'SVM']})\n\nfig, ax = plt.subplots(1,1, figsize= (12,8), squeeze=False)\nplot = sns.barplot(data = perf, y = 'scores', x='models')\nplt.bar_label(plot.containers[0], fmt ='%.4f', size = 14)\n\nplt.title('XGBoost has the best recall-score of all models', size = 22)\nplt.ylabel('recall-score', size = 12)\nplt.xlabel('Regressor model', size = 12)\nplt.tight_layout()\nplt.show()","bee73330":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparameters = { 'eta': np.arange(0.01, 0.4, 0.01),\n'min_child_weight': np.arange(1, 10, 1),\n'max_depth': np.arange(2, 10, 1),\n'gamma': np.arange(0.5, 1, 0.05),\n'subsample': np.arange(0.5, 1, 0.05),\n'colsample_bytree': np.arange(0.5, 1, 0.05),\n'lambda': np.arange(1, 2, 0.1),\n}\n\nmodel = XGBClassifier(verbosity =0)\nclassifier = RandomizedSearchCV(model, parameters, n_iter = 2000, cv = 5, verbose=0, scoring = 'recall')","1bac0df9":"classifier.fit(X_train, y_train)\nprint(classifier.best_estimator_)","af979821":"#saving model with best parameters\nimport pickle\n\nfilename = 'classifier.sav'\n\npickle.dump(classifier, open(filename, 'wb'))","45e874e9":"y_pred = classifier.predict(X_test)\n\nrecall_xghp = view_performance (y_test, y_pred, model = \"XGBoost with hyperparameter optimization\")","60255d01":"final_perf = pd.DataFrame.from_dict({'score': [recall_xghp, recall_xg, recall_rf], 'model': ['XGBoost w\/ hyperopt', 'XGBoost Default', 'Random Tree']})\n\nimprovement = 100 - (recall_xg \/ recall_xghp)*100 \n\nfig, ax = plt.subplots(1,1, figsize = (12,8))\n\nplot = sns.barplot(data = final_perf, x = 'model', y='score', ax = ax)\nplt.title(f'We achieved a improvement of {improvement:.2f}% in recall-score using Hyperparameter Optmization', size = 18)\nplt.ylabel('Recall-score', size=18)\nplt.xlabel('Classification model', size=18)\nplt.bar_label(plot.containers[0], fmt ='%.6f', size = 14)\n\nplt.tight_layout()\nplt.show()","227affde":"!pip install shap","15421346":"import shap\n# explain the model's predictions using SHAP\nexplainer = shap.Explainer(classifier.best_estimator_)\nshap_values = explainer(X)\nshap.plots.bar(shap_values)","aaecf312":"### Random Forest","e6d10346":"From our Exploratory data analysis, and the study of our machine learning model we can take the following conclusions: \n\n - As expected the average Ph of the samples is around  7.0 (neutral)\n - The low correlation of variables in this dataset causes linear classifiers to be very ineffective for this case. \n - The levels of Sulfate, Hardness and Ph of the water are the biggest contributors for the potability of the samples. So the study of the \n - Even with hyperparameter optmization, I would not recommend the usage of this model in real situations. Our recall-score is less than 50%, and the implications of consumptions of samples that resulted in false-positives outweight any gain from the use of ML. \n\n - It is possible that the addition of features that have more correlation in this dataset can improve our accuracy. ","39c827f8":"According to the NGO Water.org, 785 million people lack access to safe water and 2 billion people lack access to a toilet. Also, Nearly 1 million people die each year from water, sanitation and hygiene-related diseases. It is a health crisis that is not given too much attention in mainstream media. \n\n![image](https:\/\/cloudfront-eu-central-1.images.arcpublishing.com\/larazon\/Q3KPFJKNJVHN7LR5ECYD5C7J4U.jpg)\n\nSource: La Raz\u00f3n\n\nImproving the access to potable water can reduce child and maternal mortality rates, increase home income and interrupt one aspect of the cicle of poverty. And increasing the capacity of water testing could facilitate this proccess. It is estimated that annually lack of water quality generates 260mi USD in losses (Sadoff et al).\n ","16c186ac":"The query above returns a Null series, this shows that we succesfully deleted the indexes with unreliable Ph values","6bf64718":"The code below is for a function that we will use to visualize each classification model's performance using confusion matrix, and the recall as the main metric. \n\nWe will use recall because the implications of false positives are way worse than false negatives, since recommending the consumption of non-potable water can lead to health problems. \n\nAt the end, the model with the best recall-score will be chosen for hyperparameter tunning. ","474fb431":"## Testing classifiers with standard hyperparameters","3532f455":"### Logistic Regression","68262690":"## Feature Details","a1b1fe3f":"# Predicting the Potability of Water using Machine Learning","ee98a222":"# Introduction","4688fb01":"Since our data is skewed, we will use logarithimic scaling. ","de38d779":"No more Null values.","8365d961":"It seems that some samples have Ph o 0, according to my sources this is quite an extreme value of accidity (similar to the accidity of a battery). Also we can see that the maximum value of the samples is 14 which is also an extreme alkaline value (comparable to bleach). \n\nSource: https:\/\/www.healthline.com\/health\/ph-of-drinking-water\n\nLet's investigate these values  5 < Ph and Ph > 9 for a sanity check.","6a822edd":"This dataset shows a light imbalance towards not potable results, but a 6:4 proportion does not require the use of supersampling techniques. ","d9e15c68":"### Na\u00efve Bayes","d098d1a8":"## Hyperparameter tunning","6bab81c1":"## Context","ee28569f":"## Data imports and cleaning","e9403482":"We will be dealing with only numerical variables. \n\nIt also can be noted that this dataset has columns with Null values. ","1eec1615":"As we can see in the correlation matrix, no variable represents strong linear co-relation with another. We can assume that linear models will be outperformed by probabilistical models for this study. ","77d8c561":"### SVC","ed3253ed":"### XGBoost Classifier","6ae30667":"## Exploratory analysis","dc921c04":"We can see that 184 samples even tho they are in the extremes of the Ph spectrum, are classified as potable. This could be result of input error, measurament error, etc. \n\nI decided to avoid these instances and will drop them from our dataset.","9b247092":"<a href=\"https:\/\/colab.research.google.com\/github\/rafaelhora\/water-potability\/blob\/main\/EDA_water_potability.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","2c527da7":"## Scaling numeric variables\n","0267b672":"## Comparing recall scores ","6501f033":"\n**pH value**: PH is an important parameter in evaluating the acid\u2013base balance of water. **WHO has recommended maximum permissible limit of pH from 6.5 to 8.5.** The current investigation ranges were 6.52\u20136.83 which are in the range of WHO standards.\n\n**Hardness**: Hardness is mainly caused by calcium and magnesium salts. These salts are dissolved from geologic deposits through which water travels. \n\n**Solids** (Total dissolved solids - TDS): Water has the ability to dissolve a wide range of inorganic and some organic minerals or salts such as potassium, calcium, sodium, bicarbonates, chlorides, magnesium, sulfates etc. This is the important parameter for the use of water. **Desirable limit for TDS is 500 mg\/l and maximum limit is 1000 mg\/l which prescribed for drinking purpose.**\n\n**Chloramines**: Chlorine and chloramine are the major disinfectants used in public water systems. **Chlorine levels up to 4 milligrams per liter (mg\/L or 4 parts per million (ppm)) are considered safe in drinking water.**\n\n**Sulfate**: Sulfates are naturally occurring substances that are found in minerals, soil, and rocks. It ranges from 3 to 30 mg\/L in most freshwater supplies, although much higher concentrations (1000 mg\/L) are found in some geographic locations.\n\n**Conductivity**: **Pure water is not a good conductor of electric current rather\u2019s a good insulator.** Increase in ions concentration enhances the electrical conductivity of water. **According to WHO standards, EC value should not exceeded 400 \u03bcS\/cm.**\n\n**Organic_carbon**: Total Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. **According to US EPA < 2 mg\/L as TOC in treated \/ drinking water, and < 4 mg\/Lit in source water which is use for treatment.**\n\n**Trihalomethanes**: THMs are chemicals which may be found in water treated with chlorine. **THM levels up to 80 ppm is considered safe in drinking water.**\n\n**Turbidity**: The turbidity of water depends on the quantity of solid matter present in the suspended state. **The WHO recommended value is < 5.00 NTU.**\n\n**Potability**: Indicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.","363b3f35":"We will inpute the median value for each column in the Null cells. \n\nBut please note that it could be possible that the people responsible for the application of this study can request for unreliable measures to be completely dropped, due to the risks contained in the application of this model. ","1e465d19":"# Conclusions","7751ab59":"The logistic and SVM models have been heavily ineffective for this dataset, since in our EDA we observed that the variables have no linear correlation. \n \n**Tree-based models (xbgboost and random forest) gave the best recall-scores.**\n\nAlso, analyzing the confusion matrix of all classification models we have a very high rate of false positives, since we are studying of a water potability prediction, this is a importante issue. The implication of a false positive is that people will consume water that may harm their health, so we should keep this in mind when tunning our model."}}