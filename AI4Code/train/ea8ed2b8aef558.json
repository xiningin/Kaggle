{"cell_type":{"5ac158ce":"code","9b9b7451":"code","420123dd":"code","cb469e8b":"code","629b8f6b":"code","d65d6dc4":"code","74451909":"code","5423e1c6":"code","dc92a165":"code","0c60b81f":"code","101d7a0b":"code","4ac7d86d":"markdown"},"source":{"5ac158ce":"import time\n\nimport torch\nfrom torch import nn, optim \nimport torch.utils.data as data\nfrom torch.autograd import Variable\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","9b9b7451":"# random seed everything\ndef random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu vars\n    random.seed(seed_value) # Python\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n\n# Helper function for Execution time of the script\ndef execution_time(start):\n    _ = time.time()\n    hours, _ = divmod(_-start, 3600)\n    minutes, seconds = divmod(_, 60)\n    print(\"Execution Time:  {:0>2} hours: {:0>2} minutes: {:05.2f} seconds\".format(int(hours),int(minutes),seconds))\n    \n\nstart = time.time()\nrandom_seed(42, False)","420123dd":"# Model Class\n\n# output = (input + 2*padding - kernel_size - (kernel_size-1)*(dilation-1))\/stride + 1\n\nclass MnistModel(nn.Module):\n    def __init__(self, classes):\n        super(MnistModel, self).__init__()\n        \n        self.classes = classes\n        \n        # initialize the layers in the first (CONV => RELU) * 2 => POOL + DROP\n        self.conv1A = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)  # (N,1,28,28) -> (N,16,24,24)\n        self.act1A = nn.ReLU()\n        self.conv1B = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0) # (N,16,24,24) -> (N,32,20,20)\n        self.act1B = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2) # (N,32,20,20) -> (N,32,10,10)\n        self.do1 = nn.Dropout(0.25)\n        \n        # initialize the layers in the second (CONV => RELU) * 2 => POOL + DROP\n        self.conv2A = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0) # (N,32,10,10) -> (N,64,8,8)\n        self.act2A = nn.ReLU()\n        self.conv2B = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0) # (N,64,8,8) -> (N,128,6,6)\n        self.act2B = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2) # (N,128,6,6) -> (N,128,3,3)\n        self.do2 = nn.Dropout(0.25)\n        \n        # initialize the layers in our fully-connected layer set\n        self.dense3 = nn.Linear(128*3*3, 32) # (N,128,3,3) -> (N,32)\n        self.act3 = nn.ReLU()\n        self.do3 = nn.Dropout(0.25)\n        \n        # initialize the layers in the softmax classifier layer set\n        self.dense4 = nn.Linear(32, self.classes) # (N, classes)\n    \n    def forward(self, x):\n        \n        # build the first (CONV => RELU) * 2 => POOL layer set\n        x = self.conv1A(x)\n        x = self.act1A(x)\n        x = self.conv1B(x)\n        x = self.act1B(x)\n        x = self.pool1(x)\n        x = self.do1(x)\n        \n        # build the second (CONV => RELU) * 2 => POOL layer set\n        x = self.conv2A(x)\n        x = self.act2A(x)\n        x = self.conv2B(x)\n        x = self.act2B(x)\n        x = self.pool2(x)\n        x = self.do2(x)\n        \n        # build our FC layer set\n        x = x.view(x.size(0), -1)\n        x = self.dense3(x)\n        x = self.act3(x)\n        x = self.do3(x)\n        \n        # build the softmax classifier\n        x = nn.functional.log_softmax(self.dense4(x), dim=1)\n        \n        return x","cb469e8b":"# Dataset \nclass MnistDataset(data.Dataset):\n    def __init__(self, df, target, test=False):\n        self.df = df\n        self.test = test\n        \n        # if test=True skip this step\n        if not self.test:        \n            self.df_targets = target\n        \n    def __len__(self):\n        # return length of the dataset\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # if indexes are in tensor, convert to list\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # if test=False return bunch of images, targets\n        if not self.test:\n            return torch.Tensor(self.df[idx].astype(float)), self.df_targets[idx]\n        # if test=True return only images\n        else:\n            return torch.Tensor(self.df[idx].astype(float))","629b8f6b":"# Loss Function\ndef loss_fn(outputs, targets):\n    return nn.NLLLoss()(outputs, targets)","d65d6dc4":"# Train Model Loop\ndef train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n    \n    # set model to train\n    model.train()\n    # iterate over data loader\n    for bi, d in enumerate(data_loader):\n        ids = d[0]\n        targets = d[1]\n        \n        # sending to device (cpu\/gpu)\n        ids = ids.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.long)\n        \n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n        # Forward pass to get output\/logits\n        outputs = model(x=ids)\n        # Calculate Loss: softmax --> negative log likelihood loss\n        loss = loss_fn(outputs, targets)\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n        # Updating parameters\n        optimizer.step()\n        if scheduler is not None:\n            # Updating scheduler\n            if type(scheduler).__name__ == 'ReduceLROnPlateau':\n                scheduler.step(loss)\n            else:\n                scheduler.step()\n        #if bi % 100 == 0:\n            #print(f'Iter [{bi}], Loss: {loss}')\n    print(f'Loss on Train Data: {loss}')","74451909":"# Validation Model Loop\ndef eval_loop_fn(data_loader, model, device):\n    \n    # full list of targets, outputs\n    fin_targets = []\n    fin_outputs = []\n    # set model to eveluate\n    model.eval()  # as model is set to eval, there will be no optimizer and scheduler update\n    \n    # iterate over data loader\n    for _, d in enumerate(data_loader):\n        ids = d[0]\n        targets = d[1]\n\n        ids = ids.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.long)\n\n        outputs = model(x=ids)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        \n        # Get predictions from the maximum value\n        _, outputs = torch.max(outputs.data, 1)\n        \n        # appending the values to final lists \n        fin_targets.append(targets.cpu().detach().numpy())\n        fin_outputs.append(outputs.cpu().detach().numpy())\n    \n    return np.vstack(fin_outputs), np.vstack(fin_targets)","5423e1c6":"# Predicition for Test Data\ndef test_loop_fn(test, model, device):\n    \n    model.eval()\n    # convert test data to FloatTensor\n    test = Variable(torch.Tensor(test))\n    test = test.to(device, dtype=torch.float)\n    \n    # Get predictions\n    pred = model(test)\n    # Get predictions from the maximum value\n    _, predlabel = torch.max(pred.data, 1)\n    # converting to list\n    predlabel = predlabel.tolist()\n    \n    # Plotting the predicted results\n    L = 5\n    W = 5\n    _, axes = plt.subplots(L, W, figsize = (12,12))\n    axes = axes.ravel() # \n\n    for i in np.arange(0, L * W):  \n        axes[i].imshow(test[i].reshape(28,28))\n        axes[i].set_title(\"Prediction Class = {:0.1f}\".format(predlabel[i]))\n        axes[i].axis('off')\n    \n    plt.suptitle('Predictions on Test Data')\n    plt.subplots_adjust(wspace=0.5)\n    \n    return predlabel","dc92a165":"# Running Model\ndef run(path):\n    # reading train an test data\n    dfx = pd.read_csv(path+'train.csv')\n    df_test = pd.read_csv(path+'test.csv')\n    \n    # variables for training model\n    RANDOM_STATE = 42\n    BATCH_SIZE = 100\n    N_ITERS = 8500\n    NUM_EPOCHS = int(N_ITERS \/ (len(dfx) \/ BATCH_SIZE))\n    # target variable\n    target = 'label' \n    classes = dfx[target].nunique()\n    \n    # spliting train data to train, validate\n    df_train, df_valid = train_test_split(dfx, random_state= RANDOM_STATE, test_size = 0.2)\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n    \n    # target labels\n    train_targets = df_train[target].values\n    valid_targets = df_valid[target].values\n    \n    # reshaping data to 28 x 28 images\n    df_train = df_train.drop(target, axis=1).values.reshape(len(df_train), 1, 28, 28)\n    df_valid = df_valid.drop(target, axis=1).values.reshape(len(df_valid), 1, 28, 28)\n    df_test = df_test.values.reshape(len(df_test), 1, 28, 28)\n    \n    # Creating PyTorch Custom Datasets\n    train_dataset = MnistDataset(df=df_train, target=train_targets)\n    valid_dataset = MnistDataset(df=df_valid, target=valid_targets)\n    \n    # Creating PyTorch DataLoaders\n    train_data_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    valid_data_loader = data.DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    # device (cpu\/gpu)\n    device = \"cpu\"\n    # learning rate\n    lr = 1e-02\n    # instatiate model and sending it to device\n    model = MnistModel(classes=classes).to(device)\n    # instantiate optimizer\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    # instantiate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                     patience=3, \n                                                     verbose=0, \n                                                     factor=0.5, \n                                                     min_lr=0.00001)\n    \n    # loop through epochs\n    for epoch in range(NUM_EPOCHS):\n        print(f'Epoch [{epoch+1}\/{NUM_EPOCHS}]')\n        # train on train data\n        train_loop_fn(train_data_loader, model, optimizer, device, scheduler=None)\n        # evaluate on validation data\n        o, t = eval_loop_fn(valid_data_loader, model, device)\n        print(f'Accuracy on Valid Data : {(o == t).mean() * 100} %')\n    \n    # Predict on test data\n    preds = test_loop_fn(df_test, model, device)\n    return preds","0c60b81f":"# Submission function\ndef submission(path, preds):\n    sub = pd.read_csv(path+'sample_submission.csv')\n    sub['Label'] = preds\n    sub.to_csv('submission.csv', index=False)","101d7a0b":"path = '..\/input\/digit-recognizer\/'\npreds = run(path)\n\nsubmission(path, preds)\n\nexecution_time(start)","4ac7d86d":"Model Building Steps in PyTorch\n-------------------------------\n\n1. Load Dataset\n2. Make Dataset Iterable (DataLoader)\n3. Create Model Class\n4. Instantiate Model Class\n5. Instantiate Loss Class\n6. Instantiate Optimizer Class\n7. Train Model\n8. Validate Model\n9. Predictions from Model"}}