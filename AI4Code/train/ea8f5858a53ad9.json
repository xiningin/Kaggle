{"cell_type":{"96368f05":"code","76262088":"code","4d0b34db":"code","f5b393e4":"code","a0384e7b":"code","9b2f965d":"code","d0e5a6c7":"code","15c7a821":"code","f0d39480":"code","3ea19cf2":"code","e09ec13f":"code","e6bfb0ec":"code","28801d7b":"code","8ffba999":"code","6529d659":"code","2861a609":"code","49ce4850":"code","e837f45f":"code","2946f642":"code","065552e3":"code","dc96e398":"code","9ff30c82":"code","3b03a73c":"code","a53f6a1e":"code","88eb4d1c":"code","57563a37":"code","467de3c5":"code","94264435":"markdown","231dce99":"markdown","931b8b40":"markdown","e6bd7818":"markdown","ebbe5587":"markdown","1067f8f0":"markdown","2de43290":"markdown","95c49c33":"markdown","cb58b615":"markdown","3fdb3bc8":"markdown","3ad316ba":"markdown","5071ab59":"markdown","a2d9daff":"markdown","5d7357d9":"markdown","50638a1f":"markdown","77e0eb02":"markdown","445ce62b":"markdown","41c61880":"markdown","59207415":"markdown","b93c30c4":"markdown","8731c882":"markdown","e08163a9":"markdown","877b0f90":"markdown","51704094":"markdown","b29511e8":"markdown","eae8c77b":"markdown","6e46f777":"markdown","ac7db0a9":"markdown","ce76b796":"markdown","ef295353":"markdown"},"source":{"96368f05":"import sys\nsys.path.insert(1, '..\/input\/emotes\/')\nimport emo_unicode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(font_scale=1.3)\n\n# Text preprocessing packages\nimport nltk # Text libarary\n# nltk.download('wwords')\nimport string # Removing special characters {#, @, ...}\nimport re # Regex Package\nfrom nltk.corpus import stopwords # Stopwords\nfrom nltk.stem import WordNetLemmatizer # Stemmer & Lemmatizer\nfrom emo_unicode import *\n\n# Text Embedding and Feature Extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Modelling\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport scipy.sparse as sparse\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nimport html\n# Saving Model\nimport pickle\n","76262088":"#Importing the dataset\ntweets = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', names = ['Target', 'ID','Date','Query','Username', 'tweet'], encoding='latin-1')","4d0b34db":"tweets.Query.unique()","f5b393e4":"tweets = tweets.drop(['Query','Username'], axis = 1)","a0384e7b":"sns.countplot(tweets['Target'])\nplt.ylim([790000,810000])","9b2f965d":"tweets.Date = tweets.Date.str.replace('PDT', '')\ntweets.Date = pd.to_datetime(tweets.Date)","d0e5a6c7":"tweetie = tweets[['Date','Target']]\nfig, ax = plt.subplots(1,1,figsize=(24,8))\nobj = tweetie.groupby([tweets.Date.dt.hour])\nax.plot(obj.mean())\ne = ax.twinx()\ne.plot(obj.count())\ne.grid(None)\nplt.legend(['Average Tweet Sentiment', 'Tweet Count'], loc = 'lower left', fancybox = True)\nax.set_xlabel('Hour of Day', fontsize=18)\nax.set_ylabel('Average Sentiment [0-4]', fontsize=16)\ne.set_ylabel('Average Number of Tweets', fontsize=16);","15c7a821":"tweetie = tweets[['Date','Target']]\nfig, ax = plt.subplots(1,1,figsize=(24,8))\nobj = tweetie.groupby([tweets.Date.dt.weekday])\nax.plot(obj.mean())\ne = ax.twinx()\ne.plot(obj.count())\ne.grid(None)\nplt.legend(['Average Tweet Sentiment', 'Tweet Count'], loc = 'lower left', fancybox = True)\nfig.suptitle('', fontsize=20)\nax.set_xlabel('Day of the Week', fontsize=18)\nax.xaxis.set(ticks=range(0,7) ,ticklabels=['Monday' , 'Tuesday', \"Wednesday\", 'Thursday', \"Friday\", \"Saturday\", \"Sunday\"])\nax.set_ylabel('Average Sentiment [0-4]', fontsize=16)\ne.set_ylabel('Average Number of Tweets', fontsize=16);","f0d39480":"stop= set(stopwords.words('english'))\n\nprocessing_dict  = {\n        r\"[(http(s)?):\\\/\\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&\/\/=]*)\" : 'URL' , #Links\n        r\"&\\w{2,5};\": ' ', #User mentions\n        r\"@[^\\s]+\":  'USER', #HTML Special Entities\n}\ndef process_text(txt):\n        txt = txt.lower()\n        txt = html.unescape(txt)\n        for toreplace,replacement  in processing_dict.items():\n            txt = re.sub(toreplace,replacement,txt)  \n        for emoji,feeling  in EMOTICONS_EMO.items():\n            txt = txt.replace(emoji,feeling)\n        return txt\n\ntweets['processed_tweet'] = tweets.tweet.apply(process_text)","3ea19cf2":"tweets[\"Length\"] = tweets.processed_tweet.str.len()\nplt.figure(figsize=(26,5))\nplt.scatter(tweets.sort_values('Length')['Length'],tweets.Target)\nplt.gca().set_xlabel('Tweet Length', fontsize=18)\nplt.gca().set_ylabel('Sentiment (positive to negative)', fontsize=18);","e09ec13f":"lemmatizer = WordNetLemmatizer()\ndef remove_stopwords(text):\n    stopwordsfree=\" \".join([ x for x in text.split() if x not in stop])\n    return stopwordsfree\n\ndef lemmetize_sentence(text):\n    word_list = nltk.word_tokenize(text)\n    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n    return lemmatized_output\n\ndef clean_text(txt):\n        txt = re.sub(r\"(.)\\1\\1+\",r\"\\1\\1\",txt)\n         #Typo, or exaggeration fixer, replace any charcters repeated more than twice with just two\n         #characters eg: Cottton -> Cotton  or  Amazingggg to Amazingg }\n        txt = re.sub(r'[^a-zA-Z0-9]',' ',txt)\n        return txt\ntweets['lemmetized_processed_tweet'] = tweets.processed_tweet.apply(lambda x:clean_text(remove_stopwords(lemmetize_sentence(x))))","e6bfb0ec":"#Make the feature dicts\nweekday_map= {0:'MON', 1:'TUE', 2:'WED', 3:'THU', 4:'FRI', 5:'SAT', 6:'SUN'}\nhour_map={n: 'Hour {0}'.format(n) for n in range(24)}\n\n#Extract the features from the data\ntweets['dotw'] = tweets.Date.dt.weekday\ntweets['hourofday'] = tweets.Date.dt.hour\n\n#Translate them into words instead of integers for better readibility\ntweets.dotw =tweets.dotw.apply(lambda x: weekday_map[x])\ntweets.hourofday =tweets.hourofday.apply(lambda x: hour_map[x])\n\nenc =OneHotEncoder(handle_unknown='ignore')\n\n#one hot encode the large number of values to feed into our model easier and not require as much memory\nday =enc.fit_transform(tweets['dotw'].values.reshape(-1, 1))\ntime =enc.fit_transform(tweets['hourofday'].values.reshape(-1, 1))\n\n##tape them together =)\ntime_day = sparse.hstack((day,time))\n#Vectorizing and extracting the day of the week feature","28801d7b":"#Normalizing target to be from 0 to 1 as it's easier to deal with for the algorithms\ntweets.Target = tweets.Target.apply(lambda x : 0 if x ==0 else 1)\nfrom wordcloud import WordCloud\nplt.figure(figsize = (22,22)) \nax = plt.gca()\nax.grid(False)\nax.set_xticks([])\nax.set_yticks([])\nstop.update({'url','user'})\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800,collocations=False,stopwords= stop).generate(\" \".join(tweets[tweets.Target == 0].processed_tweet))\nplt.imshow(wc , interpolation = 'bilinear');\nax.set_title('Most Occuring Words in Negative Tweets', fontsize=20);","8ffba999":"from wordcloud import WordCloud\nplt.figure(figsize = (22,22)) \nax = plt.gca()\nax.grid(False)\nax.set_xticks([])\nax.set_yticks([])\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800,collocations=False,stopwords= stop).generate(\" \".join(tweets[tweets.Target == 1].processed_tweet))\nplt.imshow(wc , interpolation = 'bilinear');\nax.set_title('Most Occuring Words in Positive Tweets', fontsize=20);","6529d659":"tweets = pd.concat([tweets, pd.DataFrame(time_day.toarray(), columns = (list(weekday_map.values()) + list(hour_map.values())) )], axis = 1)\nX = tweets[['lemmetized_processed_tweet'] + list(weekday_map.values()) + list(hour_map.values())]\ny = tweets['Target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","2861a609":"## TFIDF embedding for the Description\nvectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=1000000)\n# fit on training (such vectorizer will be saved for deployment)\nvectorizer_tfidf = vectorizer.fit(X_train['lemmetized_processed_tweet'].values,y_train.values)\n# transform on training text data\nX_train_text = vectorizer.transform(X_train['lemmetized_processed_tweet'].values)\n# transform on testing text data\nX_test_text = vectorizer.transform(X_test['lemmetized_processed_tweet'].values)","49ce4850":"finalxtrain = sparse.hstack((sparse.csr_matrix(X_train[list(weekday_map.values()) + list(hour_map.values())].values), X_train_text))\nfinalxtest= sparse.hstack((sparse.csr_matrix(X_test[list(weekday_map.values()) + list(hour_map.values())].values), X_test_text))","e837f45f":"prediction = dict()","2946f642":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB().fit(finalxtrain, y_train)\nprediction['Multinomial'] = model.predict(finalxtest)","065552e3":"from sklearn.svm import LinearSVC\nSVCmodel = LinearSVC()\nSVCmodel.fit(finalxtrain, y_train)\nprediction['SVC'] = SVCmodel.predict(finalxtest)","dc96e398":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(solver='liblinear', C = 2, max_iter = 1000)\nlogreg.fit(finalxtrain, y_train)\nprediction['Logistic'] = logreg.predict(finalxtest)","9ff30c82":"#Plotting an AUC-ROC curve to compare algorithms\ncmp = 0\ncolors = ['b', 'g', 'y','cyan']\nplt.figure(figsize=(15,8))\nfor model, predicted in prediction.items():\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predicted)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n    cmp += 1\nplt.title('Classifiers comparaison with ROC')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","3b03a73c":"print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))","a53f6a1e":"model_name = 'logistic_model.pk'\nvectorizer_name = 'tfidf_vectorizer.pk'\nmodel_path = '.\/'+ model_name\nvect_path = '.\/'+vectorizer_name","88eb4d1c":"pickle.dump(vectorizer_tfidf , open(vect_path,'wb'))\npickle.dump(logreg , open(model_path,'wb'))","57563a37":"loaded_model = pickle.load(open(model_path,'rb'))\nloaded_vect = pickle.load(open(vect_path,'rb'))\ndef analyze_tweet(txt):\n    txt = clean_text(remove_stopwords(lemmetize_sentence(process_text(txt))))\n    week = np.zeros(31)\n    week[pd.Timestamp.now().weekday()] = 1\n    week[pd.Timestamp.now().hour] = 1\n    mx = loaded_vect.transform([txt]) \n    pos,neg  = loaded_model.predict_proba(sparse.hstack((mx,sparse.csr_matrix(week))))[0]\n    print(pos,neg)\n    if pos <=0.58 and pos >=0.42:\n        return 1\n    elif pos>0.52:\n        return 2\n    else:\n        return 0","467de3c5":"twt = tweets.iloc[5656,:]\ncleaned, target =process_text(twt.tweet), twt.Target\nprint(twt.tweet , twt.Target)\nprint(analyze_tweet(cleaned), '<-- Model prediction (0,1)')","94264435":"Let's convert the date string to pandas' DateTime and save it to observe and visualize the data","231dce99":"#### Let's try to find relationships between our variables and interesting facts about our data..","931b8b40":"The few visualization prior showed that both weekdays AND time of the day had a significant impact on tweet sentiment, it was not at all random especially  since it's based on the average of such a large population of tweets...\n\nWe decided to extract these features and inculde them in our model, hoping for better performance.","e6bd7818":"Then we prepare the data for model selection, to X and Y, our independant and dependant variables respectively and, by dividing it into two sets, training and test, and attaching the features like weekday and hour to it:","ebbe5587":"## Text pre-processing and feature extraction","1067f8f0":"Lets' prepare some lemmatizing and stop-word removal functions that work on a per sentence basis to use on our tweets to \"normalize\" our text data..","2de43290":"Above and below we try to visualize our text data with the most occuring words in both positive and negative tweets","95c49c33":"The average amount of tweets trends downwards as people progress into the weekday, and they start tweeting more again as the weekend starts it seems, they're even more positive! Is this because a certain group of positive tweeters come into the mix which are freer on the weekends? Or is it the same set of people but grumpier? A much more thorough analysis is needed to answer that, we will leave it at that for now and focus on the sentiment prediction portion of this notebook","cb58b615":"The approach taken to pre process the text was the following:\n\n#### <b>Links<\/b>:\nTo not lose any data, links were replaced with the word \"URL\", this could possibly be impactful if useres for example posted a link, which can also be a picture due to how twitter works, or if a user posted multiple links it could indicate a more positive or a more negative tweet.\n\n#### <b>Hashtags<\/b>:\nHashtags are left as they are with the \"#\" symbol removed as a hashtag can either represent a word that shows a feeling like \"#lol\" or \"#excited\" or can represent an either positive or negative trend.\n\n#### <b>User mentions<\/b>:\nFor a similar reason as the ones above, all user mentions were changed to all be the word \"USER\", since it could be possible that a person doesn't mention as much, or even mentions more when there's a specific polarity to the tweet.\n\n#### <b>HTML Special Entities<\/b>:\nThese represent only symbols, which isn't very useful to our word based sentimental analysis algorithm. However, they can be part of an emoticon, which is the only reason they're mapped to the characters they represent in our processing functions can signify either positive or negative sentiment.\n\n#### <b>Emoticons<\/b>:\n\nAgain, to not lose any information, we map these emoticons to their meaning \/ how they could possibly be interpreted by the average person, for example:\n- \":\u2011)\"  <b>_maps to_<\/b>  \"Happy face or smiley\"\n\n- \"}:)\" <b>_maps to_<\/b> \"Evil or devilish\"\n\n- \"& gt;:\/\" (after HTML Entity processing: \" >:\/ \") <b>_maps to_<\/b> \"Skeptical, annoyed, undecided, uneasy or hesitant\" \n","3fdb3bc8":"Our positive (with target variable 4) \/ negative reviews (with target variable 0) are balanced ith 800,000 samples for each class, Great! There will be no imbalance to impact our predictive algorithms!","3ad316ba":"## Model Training and Selection","5071ab59":"A quick piece of visualization below, about tweet length and sentiment\n\nAre people angrier the more they talk in a tweet turning it into a rant? ","a2d9daff":"It seems like that people tweet more on average after the usual business hours at the end of the day and late into the night, although it seems like the most positive tweets occure at night and when people aren't working! People seem grumpier at work..","5d7357d9":"## Data Exploration","50638a1f":"An average tweet consists of the following special instances of text, besides language:\n\n- Links\n- Hashtags\n- User Mentions (@)\n- HTML Special Entities eg: \"& gt;\" which translates to a greater than sign (>)\n- Emoticons eg: \"=)\"","77e0eb02":"### Saving the model for production","445ce62b":"Finally, the data is attached in a sparse matrix to be fed to the model","41c61880":"In this notebook you'll find the exploration, analysis and the modelling for the Sentiment140 dataset, a collection of 1.6 million dataset positive and negative tweets, noisily labelled by the emoticons in their text as positive or negative.","59207415":"We then vectorize our text data only with a TF-IDF vectorizer which reduced the importance of the more often repeated words","b93c30c4":"Since the value for \"Query\" is redundant, we drop it.. and it doesn't make sense to assume a username would incline a person to be more negative or positive so we drop that column as well","8731c882":"Above, we split our data for training and testing and below we vectorize the lemmetized tweets for model training","e08163a9":"Hope it was an enjoyable read!\n\nBy Omar Ibrahim\nhttps:\/\/www.linkedin.com\/in\/omar-png\/","877b0f90":"### Loading and testing the model again","51704094":"At what time of day do people tweet the most? Are tweets at a certain time more negative than others?","b29511e8":"We chose fairly popular models for sentiment analysis which are Logistic Regression as well as SVC and MultiNomial Naive Bayes and we store their results in a handy dictionary","eae8c77b":"With this visualization now enabled because any links, longer user mentions and HTML entities processed I would've expect the longer tweets to be more rant-like but surprisingly it's the other way around! Shorter tweets are always more negative than longer ones \n\nEven more interesting is that this graph is shaped much like a logistic function, behind the scenes of making this notebook though, it actually worsened the logistic algorithm result!","6e46f777":"After transforming the TFIDF model on the text only we concatenate it back to the samples in a sparse matrix to save space\n\nAnd we do the same for the test data","ac7db0a9":"The AUC-ROC curve above indicates that Logistic Regression gave us the best possible performance of the three.\n\n## We achieved an AUC of .81 and an f1 score of .805, not bad!","ce76b796":"#### More preprocessing!\n\n\nNext up we remove all stop words, like 'and', 'or', or 'the' as they don't show any sentiment and are repeated in almost all text. They're useless to our model\n\nThen we lemmetize all words to their root word, this was chosen instead of stemming in hopes that the extra context it can provide will provide better results than the faster stemming algorithm.\n\nLastly, we remove all symbols and special charcters, as they don't help the model.","ef295353":"## Exploration, Analysis and Classification of the Sentiment140 Dataset"}}