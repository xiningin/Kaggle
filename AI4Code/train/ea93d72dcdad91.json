{"cell_type":{"128f7ecb":"code","a6069dc7":"code","d3e7f493":"code","e9d4e32a":"code","1b230b86":"code","3bf9bc81":"code","5a3d404a":"code","06c5fdf1":"code","4441b8a0":"code","0c93b00b":"code","b008a863":"code","128e58fe":"code","3f431542":"code","d789b7ba":"code","ba169bf5":"code","c66a4c1a":"code","7e75143a":"code","d94cad0d":"code","3a8d5a7d":"code","ea53dd8e":"code","25ab0e35":"code","66617f92":"code","701b0082":"code","776b2bd7":"code","3c3a5502":"code","e8441972":"code","43611da5":"code","249c49e7":"code","8a08d4a5":"code","a205446e":"code","0f7f223b":"code","51ccf65f":"code","9022e5a0":"code","694e7531":"code","6198d1d5":"code","d9a176d2":"code","1bbdb791":"code","0a90c19f":"code","93ad170a":"code","aa76bf1b":"code","a918aba5":"code","2aa665af":"code","01267aa4":"code","5e713544":"code","db94fb3b":"code","a38338a5":"code","0e02b0cf":"code","6096bce3":"code","501596b3":"code","1753acb0":"code","c4be738a":"code","06bb177d":"code","e4de5413":"code","a214c17f":"code","912a7758":"code","28a58912":"code","d49e437a":"code","9e5334a2":"code","3b73c57e":"code","89a21669":"code","987513d6":"code","95e48d22":"code","ec39616d":"code","eac643fd":"code","7d6c4ec2":"code","37d20346":"code","f55e47b2":"code","584c7219":"code","86532e90":"code","a3959eee":"code","e51e8a4f":"code","8e2c1972":"code","08bdf65c":"code","23d15b12":"code","ce763a03":"code","b93149da":"code","edb001bb":"code","69435d6c":"code","cddc5edd":"code","add7485c":"code","bdedd62a":"code","170c332e":"code","34384066":"code","2a490e4f":"code","0a71ff14":"code","c4a3db5a":"code","fc802d9f":"code","59a13832":"code","675707ba":"code","245efa91":"code","46ec963b":"code","6eb72fe7":"code","0f7b2b27":"code","3943a6d8":"code","076c4db3":"code","575ef8c5":"code","afecfb4d":"code","1452732d":"code","643d7718":"code","63680b8e":"code","04fd981f":"code","3a7f0c5e":"code","7cf902df":"code","0cf31960":"code","4d958a30":"code","bfa8f758":"code","25760c47":"code","81f84ea4":"code","42be7084":"code","914f289e":"code","0f7e7dc7":"code","77e9919a":"code","4f001356":"code","24ace28a":"code","0086b8c3":"code","c9d2768e":"code","636ff71b":"code","4b087c58":"code","c0e25642":"markdown","fe8e97a4":"markdown","ec4c837d":"markdown","43155dbf":"markdown","94a303f8":"markdown","11da3795":"markdown","cff011db":"markdown","55618e67":"markdown","509c200f":"markdown","4bff32f2":"markdown","3bd75bbb":"markdown","b0146813":"markdown","dcbced0f":"markdown","ddbcc821":"markdown","faf129b6":"markdown","a1fb1a1c":"markdown","811b7186":"markdown","86e7852f":"markdown","8b6a6f81":"markdown","35fdcd9d":"markdown","9258dd46":"markdown","f8bae9f4":"markdown","4f8060b2":"markdown","5012c9b9":"markdown","663de2a0":"markdown","a2df1806":"markdown","25e94ce2":"markdown","8f0e3a01":"markdown","3a31b582":"markdown","b4021d7f":"markdown","d1e53d3f":"markdown","4bb1f9d8":"markdown","3daefa5b":"markdown","5ce82aab":"markdown","2ea180da":"markdown","ee64a585":"markdown","2c3770c2":"markdown","cf81bd78":"markdown","8d337961":"markdown","8ce6154e":"markdown","99991c67":"markdown","e010d925":"markdown","d85d3d1f":"markdown","05985c6c":"markdown","b4b5a906":"markdown","7385487b":"markdown","47823a05":"markdown","7b46ff7a":"markdown","8f7e5213":"markdown","aee27d2c":"markdown","ab8f9828":"markdown","a2ce5df7":"markdown","7eeebc3b":"markdown","cfbde12c":"markdown","47930b23":"markdown","d3fe96a8":"markdown","f0e6be09":"markdown","5dffd890":"markdown","498b46b9":"markdown","69f2a289":"markdown","6bfcf848":"markdown","a0f52d23":"markdown","9f7c2b63":"markdown","16b9e8db":"markdown","49412182":"markdown","2dc4133c":"markdown","bd17fa74":"markdown","d2494b4c":"markdown","b721c933":"markdown","703bbcde":"markdown","1b698709":"markdown","198c1576":"markdown","00001600":"markdown","212402c8":"markdown"},"source":{"128f7ecb":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\n\nimport missingno\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom scipy import stats\n\nfrom functools import reduce\n\nimport warnings\nwarnings.filterwarnings('ignore')","a6069dc7":"from sklearn.base import TransformerMixin, BaseEstimator, clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\n\nfrom xgboost import XGBRegressor\n\nimport optuna","d3e7f493":"sns.set_theme(context='notebook', style='whitegrid', palette='rocket')\nplt.rcParams[\"figure.figsize\"] = (30,10)","e9d4e32a":"print(open('\/kaggle\/input\/home-data-for-ml-course\/data_description.txt', 'r').read())","1b230b86":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col = 'Id')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col = 'Id')\nprint(f'train dimensions: {train.shape}')\nprint(f'test dimensions: {test.shape}')","3bf9bc81":"set(train.columns).difference(set(test.columns))","5a3d404a":"target = train.pop('SalePrice')\nset(train.columns) == set(test.columns)","06c5fdf1":"# Backup original data\noriginal_train = train.copy()\noriginal_test = test.copy()\noriginal_target = target.copy()","4441b8a0":"train.info(verbose=False)","0c93b00b":"test.info(verbose=False)","b008a863":" train[test.select_dtypes(include='float').columns].sample(4)","128e58fe":" test[test.select_dtypes(include='float').columns].sample(4)","3f431542":"target.dtype","d789b7ba":"target.describe()","ba169bf5":"target.isna().any()","c66a4c1a":"# Thanks to Artem Prikhodko for the probability plot idea\n# https:\/\/www.kaggle.com\/aipi12\/top-1-approach\/comments#2.1-Target-variable-and-numerical-data\ndef hist_prob_plot(series, name=None):\n    if not name:\n        name = series.name\n    \n    fig = plt.figure(figsize=(24,8), constrained_layout=True)\n    fig.suptitle(name + f' [mode: {series.mode()[0]:.0f}, median: {series.median():.0f}, mean: {series.mean():.0f}, skew: {series.skew():.2f}]', fontsize=18)\n    fig.set_constrained_layout_pads(w_pad=0.2, w_space=0.2, h_pad=0.08, h_space=0.2)\n\n    # Histogram with KDE\n    ax1 = fig.add_subplot(1,2,1)\n    ax1.set(title='Histogram')\n    hist = sns.histplot(series, stat='count', kde=True, \n                        element='step', color='gold', \n                        ax=ax1)\n\n    # Probability Plot\n    ax2 = fig.add_subplot(1,2,2)\n    ax2.set(title='Probability Plot')\n    stats.probplot(series, plot=sns.lineplot(ax=ax2))\n    ax2.get_lines()[0].set(color='mediumaquamarine', alpha=0.5)\n    ax2.get_lines()[1].set_color('black')","7e75143a":"hist_prob_plot(target)","d94cad0d":"hist_prob_plot(np.log(target), 'log(SalePrice)')","3a8d5a7d":"target = np.log(original_target)","ea53dd8e":"# Transformer to select specific features of a dataframe while preserving structure\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, features):\n        self.features = features\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        output_df = X[self.features]\n        return output_df","25ab0e35":"# Thanks to Julie Michelman's talk on how use Scikit's pipelines effectively with Pandas\n# https:\/\/www.youtube.com\/watch?v=BFaadIqWlAg\n# https:\/\/github.com\/jem1031\/pandas-pipelines-custom-transformers\/tree\/master\/code\nclass DataFrameFeatureUnion(TransformerMixin):\n\n    def __init__(self, transformers):\n        self.transformers = transformers\n\n    def fit(self, X, y=None):\n        for (name, t) in self.transformers:\n            t.fit(X, y)\n        return self\n\n    def transform(self, X):\n        Xts = [t.transform(X) for _, t in self.transformers]\n        Xunion = reduce(lambda X1, X2: pd.merge(X1, X2, left_index=True, right_index=True), Xts)\n        Xunion = Xunion[X.columns]\n        return Xunion","66617f92":"def check_na(data, matrix=False):\n    \"\"\"\n    Reports missing values for input dataframe.\n    \"\"\"\n    if len(data) > 0:\n        cols_with_missing = [col for col in data.columns if data[col].isna().any()]\n        \n        missing = pd.DataFrame(index=cols_with_missing, columns=['missing_count', 'percent'])\n        missing.missing_count = [data[col].isna().sum() for col in cols_with_missing]\n        missing.percent = round(missing.missing_count \/ len(data) * 100, ndigits=2)\n        missing.sort_values(by='missing_count', inplace=True, ascending=False)\n        if cols_with_missing:\n            display(missing.T.style.background_gradient(cmap='OrRd', axis=1).format('{:.2f}'))\n        print(f'\\n{len(cols_with_missing)} out of {len(data.columns)} features contain missing values.\\n')\n        if matrix:\n            missingno.matrix(data[missing.index], figsize=(30,10), fontsize=12, labels=True)","701b0082":"print('all missing values:\\n')\ncheck_na(test)","776b2bd7":"print('train missing values:\\n')\ncheck_na(train, matrix=True)","3c3a5502":"numericals = train.select_dtypes(exclude='object').columns.tolist()\ncategoricals = train.select_dtypes(include='object').columns.tolist()","e8441972":"# Some features with numerical datatypes belong to categorical featureset\nnum_to_cat = ['MSSubClass', 'OverallQual', 'OverallCond']\nnumericals = list(set(numericals) - set(num_to_cat))\ncategoricals.extend(num_to_cat)\n(len(numericals) + len(categoricals)) == len(list(original_train.columns))","43611da5":"# Nones according to data_description.txt\nnones = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', \n         'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n# Remaining categorical features\nnot_nones = list(set(categoricals) - set(nones))","249c49e7":"class DataFrameImputer(TransformerMixin):\n    \"\"\"\n    Provide SimpleImputer strategy and fill_value.\n    \"\"\"\n    \n    def __init__(self, strategy=None, fill_value=None):\n        self.imputer = None\n        self.strategy = strategy\n        self.fill_value = fill_value\n        \n    def fit(self, X, y=None):\n        self.imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value).fit(X)\n        return self\n    \n    def transform(self, X):\n        imputed = self.imputer.transform(X)\n        df_imputed = pd.DataFrame(imputed, index=X.index, columns=X.columns)\n        df_imputed = df_imputed.astype(X.dtypes.to_dict())\n        return df_imputed","8a08d4a5":"imputer_pipeline = Pipeline([\n    ('features', DataFrameFeatureUnion([\n        ('numericals', Pipeline([\n            ('select', FeatureSelector(numericals)),\n            ('num_imputer', DataFrameImputer(strategy='constant', fill_value=0))\n        ])),\n        ('cats_nones', Pipeline([\n            ('select', FeatureSelector(nones)), \n            ('none_imputer', DataFrameImputer(strategy='constant', fill_value='None'))\n        ])),\n        ('cats_freqs', Pipeline([\n            ('select', FeatureSelector(not_nones)),\n            ('freq_imputer', DataFrameImputer(strategy='most_frequent'))   \n        ]))\n    ]))\n])","a205446e":"ordinals = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', \n            'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', \n            'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n\nnominals = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n            'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', \n            'Heating', 'CentralAir', 'Electrical', 'GarageType', 'MiscFeature', 'SaleType', 'SaleCondition']","0f7f223b":"# Ordinals' custom ranks\nten_ranks = [str(n) for n in range(1,11)]\nfive_ranks = ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\nfin_ranks = ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']\n\nordinal_ranks = {'LotShape' : ['IR3', 'IR2', 'IR1', 'Reg'],\n                 'Utilities' : ['ELO', 'NoSeWa', 'NoSeWr', 'AllPub'],\n                 'LandSlope' : ['Gtl', 'Mod', 'Sev'],\n                 'OverallQual' : ten_ranks,\n                 'OverallCond' : ten_ranks,\n                 'ExterQual' : five_ranks,\n                 'ExterCond' : five_ranks,\n                 'BsmtQual' : five_ranks,\n                 'BsmtCond' : five_ranks,\n                 'BsmtExposure' : ['None', 'No', 'Mn', 'Av', 'Gd'],\n                 'BsmtFinType1' : fin_ranks,\n                 'BsmtFinType2' : fin_ranks,\n                 'HeatingQC' : five_ranks,\n                 'KitchenQual' : five_ranks,\n                 'Functional' : ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n                 'FireplaceQu' : five_ranks,\n                 'GarageFinish' : ['None', 'Unf', 'RFn', 'Fin'],\n                 'GarageQual' : five_ranks,\n                 'GarageCond' : five_ranks,\n                 'PavedDrive' : ['N', 'P', 'Y'],\n                 'PoolQC' : five_ranks,\n                 'Fence' : ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']}","51ccf65f":"class DataFrameCategoricalEncoder(TransformerMixin):\n    \n    def __init__(self, cat_features=[], ranks={}):\n        self.cat_features = cat_features\n        self.ranks = ranks\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        transformed = X.copy()\n        # All cats as str and category\n        transformed[self.cat_features] = transformed[self.cat_features].astype(str).astype('category')\n        # All ordinals ordered\n        for feature, categories in self.ranks.items():\n            transformed[feature] = transformed[feature].astype(CategoricalDtype(categories, ordered=True))\n        return transformed","9022e5a0":"# check cardinality of nominative categoricals\npd.concat((train, test))[nominals].nunique().sort_values(ascending=False)","694e7531":"encoder_pipeline = Pipeline([\n    ('cats_ords_encoder', DataFrameCategoricalEncoder(categoricals, ordinal_ranks))\n])","6198d1d5":"baseline_pipeline = Pipeline([\n    ('imputer', imputer_pipeline),\n    ('encoder', encoder_pipeline)\n])","d9a176d2":"train = baseline_pipeline.fit_transform(train)","1bbdb791":"# Modified from Kaggle Learn's Feature Engineering course\ndef score_dataset(X, y, model=XGBRegressor()):\n\n    X=X.copy()\n    # Extract ranks of categoricals\n    for categorical in X.select_dtypes([\"category\"]):\n        X[categorical] = X[categorical].cat.codes\n    # Metric: RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","0a90c19f":"baseline_score = score_dataset(train, target)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","93ad170a":"scores = {'baseline' : baseline_score}","aa76bf1b":"# Modified from Kaggle Learn's Feature Engineering course\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for categorical in X.select_dtypes([\"category\"]):\n        X[categorical] = X[categorical].cat.codes\n\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=42)\n    mi_scores = pd.Series(mi_scores, name=\"MIS\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","a918aba5":"def plot_mi_scores(mi_scores, threshold=0):\n    mi_scores = mi_scores[mi_scores>threshold].sort_values(ascending=False)  \n    sns.barplot(x=mi_scores.values, y=mi_scores.index, color='gold', alpha=0.5)\n    sns.despine(left=True, bottom=True)\n    plt.title(f\"Mutual Information Scores (Above {threshold})\")   ","2aa665af":"def num_feature_plots(data, target, feature):\n    \"\"\"\n    For given feature, display univariate and bivariate plots.\n    'data' is dataframe including all features.\n    'target' is series of prediction target.\n    'feature' is string of specified column in 'data'.\n    Plots include:\n        - Histogram with KDE\n        - Probability Plot\n        - Scatter plot with 'feature' and 'target'. 3rd order regression included (not robust).\n        - Top correlations using Pearson metrics. Direction of correlation denoted. Displays all correlations above magnitude of 0.5, or at least 5 top.\n    \"\"\"\n    mis = make_mi_scores(pd.DataFrame(data[feature]), target)[0]\n    \n    fig = plt.figure(figsize=(24,5), constrained_layout=True)\n    fig.suptitle(f'{feature} (MIS: {mis:.3f})', fontsize=18)\n    fig.set_constrained_layout_pads(w_pad=0.2, w_space=0.2, h_pad=0.08, h_space=0.2)\n    \n    # Histogram with KDE\n    ax1 = fig.add_subplot(1,4,1)\n    ax1.set(title='Histogram')\n    sns.histplot(data[feature], stat='count', kde=True, \n                 element='step', color='gold', \n                 ax=ax1)\n    \n    # Probability Plot\n    ax2 = fig.add_subplot(1,4,2)\n    ax2.set(title='Probability Plot')\n    stats.probplot(data[feature], plot=sns.lineplot(ax=ax2))\n    ax2.get_lines()[0].set(color='mediumaquamarine', alpha=0.5)\n    ax2.get_lines()[1].set_color('black')\n    \n    # Scatter with Regression\n    ax3 = fig.add_subplot(1,4,3)\n    ax3.set(title='Scatter and Regression (x\\u00b3) with Target')\n    sns.regplot(x=feature, y=target, data=data,\n                x_jitter=0.1,\n                color='gold', scatter_kws={'alpha':1\/3},\n                order=3, line_kws={'color':'black'}, ci=None,\n                ax=ax3)\n    \n    # Top Correlations\n    ax4 = fig.add_subplot(1,4,4)\n    ax4.set(title='Top Correlations (Pearson)', xlim=(0,1))\n    # Sort correlations by magnitude and note direction (ignore self correlation) \n    corr = pd.concat([data, target], axis=1).corr()[feature].drop(feature)\n    corr = corr.reindex(corr.abs().sort_values(ascending=False).index)\n    corr = pd.DataFrame({'Correlation': corr.abs(), 'Positive': corr >= 0})\n    highly_corr = corr[corr.Correlation >= 0.5]\n    # Plot at least the top 5 correlated features\n    top_corr = corr[:5] if (len(highly_corr) < 5) else highly_corr\n    sns.barplot(data=top_corr, x='Correlation', y=top_corr.index, \n                hue='Positive', dodge=False, palette={True:'gold', False:'mediumaquamarine'}, alpha=0.5,\n                ax=ax4)\n    ax4.legend(title='Positive Corr', loc='lower right')","01267aa4":"# test num_feature_plots()\n#num_feature_plots(baseline_train[numericals], target, '1stFlrSF')","5e713544":"def cat_feature_plots(data, target, feature):\n    \"\"\"\n    For given categorical feature, display univariate and bivariate plots.\n    'data' is dataframe including all features.\n    'target' is series of prediction target.\n    'feature' is string of specified column in 'data'.\n    Plots include:\n        - Count plot\n        - Distribution plot: constant-width violin plot overlayed with point plot (y-range: 1st to 99th quantile)\n    \"\"\"\n    palette = sns.blend_palette(['gold', 'thistle', 'mediumaquamarine'], n_colors=data[feature].nunique())\n    count_order = data[feature].value_counts().index\n    median_order = pd.concat([data[feature], target], axis=1).groupby(feature).median().sort_values(by=target.name, ascending=True).index.tolist()\n    \n    mis = make_mi_scores(pd.DataFrame(data[feature]), target)[0]\n    \n    cat_fig = plt.figure(figsize=(24,5), constrained_layout=True)\n    cat_fig.suptitle(f'{feature} (MIS: {mis:.3f})', fontsize=18)\n    cat_fig.set_constrained_layout_pads(w_pad=0.2, w_space=0.2, h_pad=0.08, h_space=0.2)\n    \n    # Univariate\/Counts\n    ax1 = cat_fig.add_subplot(1,2,1)\n    ax1.set(title='Category counts (ordered by count)')\n    sns.countplot(data=data, x=feature, \n                  order=count_order, palette=palette, alpha=0.5, linewidth=2, edgecolor='gray',\n                  ax=ax1)\n    \n    # Bivariate\/Distributions\n    ax2 = cat_fig.add_subplot(1,2,2)\n    ax2.set(title='Category distributions (ordered by median)')\n    # constant violin width\n    sns.violinplot(data=data, x=feature, y=target,\n                   order=median_order, palette=palette,\n                   inner=None, scale='width', cut=0,\n                   ax=ax2)\n    sns.pointplot(data=data, x=feature, y=target,\n                  order=median_order,\n                  ax=ax2)\n    ax2.set(ylim=(target.quantile(0.01),target.quantile(0.99)))\n    \n    # (Set violin plot opacity)\n    for violin in ax2.collections[::1]:\n        violin.set_alpha(0.5)\n    \n    # (Account for high cardinality)\n    if data[feature].nunique() > 10:\n        ax1.tick_params(axis='x', labelrotation=45)\n        ax2.tick_params(axis='x', labelrotation=45)\n    \n    # (Hide x labels)\n    ax1.set_xlabel('')\n    ax2.set_xlabel('')","db94fb3b":"# test cat_feature_plots\n#cat_feature_plots(baseline_train[categoricals], target, 'Neighborhood')","a38338a5":"def plot_set(data, target, features):\n    for feature in data[features].select_dtypes(exclude='category').columns:\n        num_feature_plots(data, target, feature)\n    for feature in data[features].select_dtypes(include='category').columns:\n        cat_feature_plots(data, target, feature)  ","0e02b0cf":"def compare_score(dataset, selected=''):\n    if selected == '':\n        lowest = min(scores.values())\n        print(f\"{'Lowest RMSE':<16}{lowest:.5f}\")\n    else:\n        lowest = scores[selected]\n        print(f\"{'Selected RMSE':<16}{lowest:.5f}\")\n        \n    score = score_dataset(dataset, target)\n    print(f\"{'New RMSE':<16}{score:.5f}\")\n    if score < lowest:\n        print(f\"{'Better by':<16}{lowest-score:.5f}\")\n    else:\n        print(f\"{'Worse by':<16}{score-lowest:.5f}\")","6096bce3":"baseline_mi_scores = make_mi_scores(train, target)\nplot_mi_scores(baseline_mi_scores, 0.1)","501596b3":"class Sulov:\n    def __init__(self):\n        self.correlations = train.corr()\n        self.correlations[self.correlations == 1] = 0\n    \n    def fit(self, cutoff):\n        keeps = set()\n        for feature in self.correlations.index:\n            # for each feature, get list of features with correlation above cutoff\n            for index in self.correlations[(self.correlations.abs() >= cutoff)[feature]].index.tolist():\n                # compare index-target with feature-target mi scores\n                if baseline_mi_scores[index] > baseline_mi_scores[feature]: \n                    keeps.add(index)\n                else:\n                    keeps.add(feature)\n        drops = set(self.correlations.index) - keeps\n        return keeps, drops","1753acb0":"def sulov_objective(trial):\n    cutoff = trial.suggest_uniform('cutoff', 0.1, 1)\n    \n    sulov = Sulov()\n    keeps, drops = sulov.fit(cutoff)\n    \n    y = target.copy()\n    X = train.drop(drops, axis=1).copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    \n    reg = XGBRegressor()\n    fitted = reg.fit(X, y)\n    \n    score = cross_val_score(fitted, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","c4be738a":"#sulov_study = optuna.create_study(direction='minimize')\n#sulov_study.optimize(sulov_objective, n_trials=100, timeout=600)","06bb177d":"#best_cutoff = sulov_study.best_trial.params.get('cutoff')\nbest_cutoff = 0.215 # best trial {'cutoff': 0.2127053918449865} with RMSE 0.13306083219194825\n\nsulov = Sulov()\nsulov_keeps, sulov_drops = sulov.fit(best_cutoff)\nscores['sulov'] = score_dataset(train.drop(sulov_drops, axis=1), target)\nscores","e4de5413":"sulov_drops","a214c17f":"# Future Mark here. Keeping BsmtFinSF2 in the set for EDA, removing after.\nsulov_drops.remove('BsmtFinSF2')","912a7758":"cat_feature_plots(train,  target, 'Condition1')","28a58912":"train.Condition1.value_counts(normalize=True)","d49e437a":"compare_score(train.drop('Condition1', axis=1), 'baseline')","9e5334a2":"balances = {f: train[f].value_counts(normalize=True).tolist()[0] for f in train[categoricals].columns}\nbalances = pd.Series(data=balances.values(), index=balances.keys(), name='bal').sort_values(ascending=False)\nbalances = pd.concat((balances, baseline_mi_scores), axis=1)\nbalances[balances.bal > 0.95]","3b73c57e":"balance_drops = set(balances[balances.bal>0.98].index)\nbalance_drops","89a21669":"selected_drops = sulov_drops.union(balance_drops)\ntrain = train.drop(selected_drops, axis=1)\ncompare_score(train)\nscores['selected_drops'] = score_dataset(train, target)","987513d6":"len(selected_drops)","95e48d22":"compare_score(train.drop('Condition1', axis=1))","ec39616d":"group_location = {'MSZoning', 'Neighborhood', 'LotConfig', 'Condition1', 'Condition2'}\ngroup_property = {'MSSubClass', 'Street', 'Alley', 'LotShape', 'LandContour', 'LandSlope'}\ngroup_external = {'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence'}\ngroup_exterior = {'BldgType', 'HouseStyle',  'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'Foundation'}\ngroup_interior = {'Utilities', 'Heating', 'CentralAir', 'Electrical', 'Fireplaces'}\ngroup_size = {'LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd'}\ngroup_qc = {'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu'}\ngroup_bsmt = {'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF'}\ngroup_garage = {'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive'}\ngroup_bath = {'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'}\ngroup_other = {'YearBuilt', 'YearRemodAdd', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'}\nall_groups = [group_location, group_property, group_external, group_exterior, group_interior, group_size, group_qc, group_bsmt, group_garage, group_bath, group_other]\nfor group in all_groups:\n    group.difference_update(selected_drops)","eac643fd":"plot_set(train, target, group_location)","7d6c4ec2":"(pd.concat((target, train), axis=1)\n.groupby('Neighborhood')\n.mean()\n.sort_values(by='SalePrice', ascending=True)\n).style.background_gradient(cmap='coolwarm')","37d20346":"train = train.assign(SF = lambda x: (train.TotalBsmtSF + train.GrLivArea + train.GarageArea))\ncompare_score(train)\nscores['add_SF'] = score_dataset(train, target)","f55e47b2":"sns.lmplot(x='SF', y='SalePrice', col=\"Neighborhood\", col_wrap=5, height=8, sharey=True,\n           data=pd.concat((train, target), axis=1))","584c7219":"# I found these indices manually\noutlier_ids = [524, 1299, 633, 1325, 689]","86532e90":"score_dataset(train.drop(outlier_ids, axis=0), target.drop(outlier_ids, axis=0))","a3959eee":"score_dataset(train.drop('SF', axis=1).drop(outlier_ids, axis=0), target.drop(outlier_ids, axis=0))","e51e8a4f":"train = train.drop(outlier_ids, axis=0)\ntarget = target.drop(outlier_ids, axis=0)\nscores['drop_outliers_1'] = score_dataset(train, target)","8e2c1972":"scores","08bdf65c":"group_size.add('SF')","23d15b12":"plot_set(train, target, group_size)","ce763a03":"plot_set(train, target, group_bsmt)","b93149da":"compare_score(train.assign(BS1 = lambda x: (train.BsmtFinType1.cat.codes * train.BsmtFinSF1),\n                           BS2 = lambda x: (train.BsmtFinType2.cat.codes * train.BsmtFinSF2)))","edb001bb":"compare_score(train.assign(BS1 = lambda x: (train.BsmtFinType1.cat.codes * train.BsmtFinSF1),\n                           BS2 = lambda x: (train.BsmtFinType2.cat.codes * train.BsmtFinSF2))\n                   .drop(['BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2'], axis=1))","69435d6c":"train = (train.assign(BsmtFin1 = lambda x: (train.BsmtFinType1.cat.codes * train.BsmtFinSF1),\n                      BsmtFin2 = lambda x: (train.BsmtFinType2.cat.codes * train.BsmtFinSF2))\n              .drop(['BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2'], axis=1))\nscores['add_BsmtFin1_BsmtFin2_drop_FinType_FinSF'] = score_dataset(train, target)","cddc5edd":"compare_score(train.assign(BsmtSFQual = lambda x: (train.BsmtQual.cat.codes * train.TotalBsmtSF)))","add7485c":"train = train.assign(BsmtSFQual = lambda x: (train.BsmtQual.cat.codes * train.TotalBsmtSF))\nscores['add_BsmtSFQual'] = score_dataset(train, target)","bdedd62a":"plot_set(train, target, group_garage)","170c332e":"compare_score(train.assign(GarageFinishCars = lambda x: train.GarageFinish.cat.codes * train.GarageCars))","34384066":"compare_score(train.assign(GarageFinishCars = lambda x: train.GarageFinish.cat.codes * train.GarageCars).drop('GarageCars', axis=1))","2a490e4f":"train = (train.assign(GarageFinishCars = lambda x: train.GarageFinish.cat.codes * train.GarageCars)\n              .drop('GarageCars', axis=1))\nscores['add_GarageFinishCars_drop_GarageCars'] = score_dataset(train, target)","0a71ff14":"plot_set(train, target, group_property)","c4a3db5a":"plot_set(train, target, group_external)","fc802d9f":"plot_set(train, target, group_exterior)","59a13832":"plot_set(train, target, group_interior)","675707ba":"plot_set(train, target, group_qc)","245efa91":"compare_score(train.assign(IntuitiveFeature=lambda x: train.OverallQual.cat.codes * train.SF))","46ec963b":"compare_score(train.assign(KitchenQualOverallCond=lambda x: train.KitchenQual.cat.codes * train.OverallCond.cat.codes))","6eb72fe7":"train = train.assign(KitchenQualOverallCond=lambda x: train.KitchenQual.cat.codes * train.OverallCond.cat.codes)\nscores['add_KitchenQualOverallCond'] = score_dataset(train, target)","0f7b2b27":"plot_set(train, target, group_bath)","3943a6d8":"compare_score(train.assign(TotalBaths=lambda x: train.FullBath + 0.5 * train.HalfBath))","076c4db3":"train = train.assign(TotalBaths=lambda x: train.FullBath + 0.5 * train.HalfBath)\nscores['add_TotalBaths'] = score_dataset(train, target)","575ef8c5":"plot_set(train, target, group_other)","afecfb4d":"scores","1452732d":"new_features = {'SF', 'BsmtFin1', 'BsmtFin2', 'BsmtSFQual', 'GarageFinishCars', 'KitchenQualOverallCond', 'TotalBaths'}","643d7718":"plot_set(train, target, new_features)","63680b8e":"compare_score(train.drop('BsmtFin2', axis=1))","04fd981f":"# Remember target transform!\n# original_target -> np.log() -> .drop(outlier_ids, axis=0) -> target","3a7f0c5e":"class FeatureEngineering(TransformerMixin):\n    \"\"\"\n    Pipeline to consolidate feature engineering transformations.\n    \"\"\"\n    \n    def __init__(self, drops=[]):\n        self.drops = drops\n        \n    def fit(self, X):\n        return self\n    \n    def transform(self, X):\n        new_X = X.copy()\n        \n        # New features\n        new_X = new_X.assign(SF = lambda x: (new_X.TotalBsmtSF + new_X.GrLivArea + new_X.GarageArea))\n        \n        new_X = new_X.assign(BS1 = lambda x: (new_X.BsmtFinType1.cat.codes * new_X.BsmtFinSF1),\n                             BS2 = lambda x: (new_X.BsmtFinType2.cat.codes * new_X.BsmtFinSF2))\n        \n        new_X = new_X.assign(BsmtSFQual = lambda x: (new_X.BsmtQual.cat.codes * new_X.TotalBsmtSF))\n        \n        new_X = new_X.assign(GarageFinishCars = lambda x: new_X.GarageFinish.cat.codes * new_X.GarageCars)\n        \n        new_X = new_X.assign(KitchenQualOverallCond=lambda x: new_X.KitchenQual.cat.codes * new_X.OverallCond.cat.codes)\n        \n        new_X = new_X.assign(TotalBaths=lambda x: new_X.FullBath + 0.5 * new_X.HalfBath)\n        \n        # Features to drop\n        new_X = new_X.drop(self.drops, axis=1)\n        new_X = new_X.drop(['BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2'], axis=1)\n        new_X = new_X.drop('GarageCars', axis=1)\n        \n        return new_X","7cf902df":"feature_engineering_pipeline = Pipeline([\n    ('feature_engineering', FeatureEngineering(selected_drops))\n]) # selected_drops was union of features to drop after SULOV and considering class balances","0cf31960":"full_pipeline = Pipeline([\n    ('imputer', imputer_pipeline),\n    ('encoder', encoder_pipeline),\n    ('feature_engineering', feature_engineering_pipeline)\n])","4d958a30":"# I ran the search (in Appendix) for these params\nselected_params = {'n_estimators': 751, \n                   'learning_rate': 0.12177128646131516, \n                   'max_depth': 2, \n                   'gamma': 0.000251073210724123, \n                   'min_child_weight': 1.2266087138381812, \n                   'reg_lambda': 1.7772392866942126, \n                   'reg_alpha': 0.115889838065404}\nscores['tuned_XGBoost'] = score_dataset(train, target, XGBRegressor(**selected_params))","bfa8f758":"final_train = full_pipeline.fit_transform(original_train)\nfinal_target = np.log(original_target)\n\nfinal_train = final_train.drop(outlier_ids, axis=0)\nfinal_target = final_target.drop(outlier_ids, axis=0)\n\nfor categorical in final_train.select_dtypes([\"category\"]):\n        final_train[categorical] = final_train[categorical].cat.codes","25760c47":"tuned_XGBR = XGBRegressor(**selected_params)\ntrained_XGBR = tuned_XGBR.fit(final_train, final_target)","81f84ea4":"trained_XGBR.get_params()","42be7084":"feature_importances = pd.Series(trained_XGBR.feature_importances_, index=final_train.columns).sort_values(ascending=False)\nsns.barplot(x=feature_importances.values, y=feature_importances.index, color='gold', alpha=0.5)\nsns.despine(left=True, bottom=True)\nplt.title(f\"Feature Importances\")  ; ","914f289e":"final_test = full_pipeline.fit_transform(original_test)\nfor categorical in final_test.select_dtypes([\"category\"]):\n        final_test[categorical] = final_test[categorical].cat.codes","0f7e7dc7":"predictions = np.exp(trained_XGBR.predict(final_test))\npredictions = pd.Series(predictions, index=final_test.index, name='SalePrice')","77e9919a":"# Save to CSV\npredictions.to_csv('submission.csv', index=True)","4f001356":"Stop Run All","24ace28a":"def objective(trial):\n    param = {\n        'n_estimators' : trial.suggest_int('n_estimators', 100, 1000), # I tried ranges up to 5000\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.5),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'gamma': trial.suggest_uniform('gamma', 0, 1),\n        'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n        'reg_lambda': trial.suggest_uniform('reg_lambda', 1, 5),\n        'reg_alpha': trial.suggest_uniform('reg_alpha', 0, 2)\n    }\n    \n    # Temporary--\n    y = target.copy()\n    X = train.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # --Temporary\n    \n    reg = XGBRegressor(**param)\n    fitted = reg.fit(X, y)\n    \n    score = cross_val_score(\n        fitted, X, y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","0086b8c3":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=200, timeout=3600)","c9d2768e":"study.optimize(objective, n_trials=200, timeout=900)","636ff71b":"best_trial = study.best_trial\nprint(f\"Out of {len(study.trials)} trials, the best trial had:\")\nprint(\"  RMSE score of: {}\".format(best_trial.value))\nprint(\"  with parameters: \")\nfor key, value in best_trial.params.items():\n    print(\"    {}: {}\".format(key, value))","4b087c58":"best_trial.params","c0e25642":"I check cardinality to consider one-hot encoding.  \nSeveral features have relatively **high cardinality**.  \nXGBoost doesn't necessarily perform better with OH encoding anyways.  \nI'll just label encode all for now.","fe8e97a4":"# Preprocessing\n***\nMy first round of preprocessing involves **handling missing values** and rudimentary **encoding** of categoricals.","ec4c837d":"*train* and *test* have similar instances of missing values so I can set up a pipeline to impute both.","43155dbf":"*BsmtFinType* basically describes how comfortable the basement is to live in.\n<br>I'll try using *BsmtFinType1 and 2* to emphasize *BsmtFinSF1 and 2*.","94a303f8":"#### Group: Interior","11da3795":"#### Group: Bath","cff011db":"# Missing Values\n***\nAiming to understand **completeness** of the data.","55618e67":"Looks fine. Will handle later.","509c200f":"## Feature Engineering to Pipeline","4bff32f2":"#### Group: External","3bd75bbb":"## Class Imbalance\n***\n<br>A large portion of the feature set is imbalanced. As a result, those features are less informative and more difficult to analyze.\n<br>\n<br>[In the following plots, the category distribution plot gives an idea of the distribution of values of each category across the target.\n<br>The pointplot connecting the violins indicates each category's central tendency while indicating its uncertainty with error bars.\n<br>The link between uncertainty and imbalanced categories is noticable when comparing both plots.]\n<br>\n<br>Consider *Condition1*.","b0146813":"*Train* and *test* have differing numerical feature dtypes?","dcbced0f":"#### Group: Other","ddbcc821":"## Numerical and Categorical Imputation\n<br>The *data_description.txt* clearly denotes which values of NA actually indicate a \"not present\" label: *None*\n<br>I'll transform those features explicitly.","faf129b6":"This is the only time I look at *test*, as significant completeness disparities would need to be addressed in the preprocessing pipeline I intend to make.  \nTo **avoid leakage**, I'll ignore *test* until the model is trained.","a1fb1a1c":"The **outliers had a large impact** on the new *SF* feature.","811b7186":"# First Impression\n***\nA quick and shallow glance at the data.  \nI want to know what kind of **datatypes** I'm working with and how they look.\n### Input Data","86e7852f":"I'll try the same idea with *BsmtQual* on *TotalBsmtSF*.","8b6a6f81":"#### Group: Quality and Condition","35fdcd9d":"### Target ","9258dd46":"## Grouping Features\nAs there are so many features, it's difficult to consider more than a handful at a time.  \nI'll **subjectively group** features and look at one group at a time. ","f8bae9f4":"## Categorical Feature Identification and Encoding\nNext, I opt to **encode** the categoricals using Panda's CategoricalDtype.  \nCategoricalDtype retains values and allows to assign custom ranks.  \nWhere the feature is **ordinal**, I want to **provide the ranks** myself.  \nBy refering to *data_description.txt*, I made some subjective decisions on whether they were ordinal or nominal.","4f8060b2":"I investigated *MSSubClass* as it distributes well across *SalePrice*, but didn't find anything useful.","5012c9b9":"{'n_estimators': 751,\n 'learning_rate': 0.12177128646131516,\n 'max_depth': 2,\n 'gamma': 0.000251073210724123,\n 'min_child_weight': 1.2266087138381812,\n 'reg_lambda': 1.7772392866942126,\n 'reg_alpha': 0.115889838065404}","663de2a0":"### (Functions for EDA)\n***","a2df1806":"#### Group: Basement","25e94ce2":"**Right Skewed** Distribution: Mode < Median < Mean","8f0e3a01":"Then I'll try to optimize the *cutoff* with the baseline model score as the loss function.","3a31b582":"#### Group: Exterior","b4021d7f":"With a little brute force searching, it seems excluding features that have a class with over 98% dominance leads to further improvement after SULOV. ","d1e53d3f":"It seems the target is **continuous** despite its integer type.","4bb1f9d8":"## Feature Engineering Recap\nIf I found any features to create or drop that improved upon the baseline score, I've included them in *train*.","3daefa5b":"Feature selection reduces the feature set's size significantly.","5ce82aab":"# Conclusion\n***\nAs this project was largely experimental, I was often surprised on my findings.  \nI admit I did not spend much time analysing my findings but this was intentional given my lack of domain knowledge in the housing market.  \n<br>\nIf I return to this project I would invest more time in feature engineering as I suspect I would find the biggest gains there.  \nI'd also try new models and ensemble them, as seems to be the approach at the top of the leaderboard.  \nBefore returning I would have to see how others approached feature engineering as I tried to go in blindly.","2ea180da":"### Group: Location\nFirst, I'll look at some features related to location of the property.","ee64a585":"Here, I've aggregated averages across *Neighborhood's* classes and sorted by the target.  \nThe averages of the features describing size seem to correlate with the averages of *SalePrice* across classes of *Neighborhood*.    \n<br>I'll take a closer look at those next.","2c3770c2":"How I'll **handle missing values**:\n- attempt to identify and handle actual missing entries, if needed\n- identify which features use NA's to denote an absence (by refering to data_description.txt)\n    - impute zero for numericals\n    - impute 'None' for categoricals\n\n\nAt this stage, I could have simply used Panda's .fillna() to perform the desired transforms.  \nHowever, I took this opportunity to practice building a custom transformer and pipeline to merge the functionality of Sklearn and Pandas.  ","cf81bd78":"# APPENDIX\n***","8d337961":"*LotConfig's* categories hardly vary across *SalePrice* [ignore].  \n<br>\n*MSZoning* and *Condition1* seems to spread nicely across *SalePrice*, but may be too imbalanced to draw reliable conclusions.  \n<br>\n*Neighborhood* has great diversity and distribution spread, supporting its high MIS. [explore]","8ce6154e":"I'll look at the percentage of the dominant value in each feature to help determine which categorical features to consider for selection.\n<br>I'm looking for features with low MIS and imbalanced classes that further improve the score after the SULOV selection.","99991c67":"It seems these new composites capture enough information to warrant dropping their constituents.","e010d925":"# Predict on Test\n***","d85d3d1f":"I was **surprised** to find an optima in my searches that had a relatively low n_estimators and max_depth!\n<br>Seems like quite a lightweight model.  \nI'm hoping it generalizes well on *test*.","05985c6c":"Finally, train a model with *selected_params*.","b4b5a906":"I've omitted a lot of trial and error, looking for feature combinations.  \nThe takeaway is that many combinations that may seem intuitive do not seem to be informative.  \nFor instance, I would have expected *OverallQual* combined with total indoor surface area *SF* to be informative.  \nMy reasoning being \"having more of a good thing\".","7385487b":"Whereas, a combination I tried by chance seems helpful yet unintuitive.","47823a05":"Seems like a small training set.","7b46ff7a":"### Transformer Setup\n***\nI want to make some **custom transformers** to enable use of pandas dataframes with sklearn's transformers, estimators, and pipelines.  \nI'm new to it and want to **practice**.","8f7e5213":"Log of *SalePrice* is much more **normally distributed**.","aee27d2c":"![sulov_method.jpeg](attachment:976a4584-99aa-4652-867b-d685cae61387.jpeg)","ab8f9828":"Although categories appear to have a desirable spread across values of *SalePrice*, the *Norm* category is overwhelming (86%) and MIS is low (0.02).\n<br>Dropping such a feature improves baseline model performance.","a2ce5df7":"# Modelling\n***\nWith quick analysis and transformations made to *train*, I feel ready to train a model.  \nI did a hyperparameter search and moved that code to the appendix.","7eeebc3b":"I'll plot figures for each feature, looking out for: \n- distribution quality (diversity in values, skew, spread across target) \n- trends\n- outliers\n- correlations (redundancy)\n- predictive usefulness","cfbde12c":"# Workflow\n***\nThis notebook is informal and experiemental.     \nThe workflow used looks something like:  \n\n**Preprocessing**\n   -  look at data dimensions, types, missing values\n   - clean, impute, encode  \n\n**EDA and Feature Engineering**\n   - identify predictive features\n   - combine and transform to improve featureset\n   - drop redundant or unpredictive features  \n\n**Modelling**\n   - obtain baseline score using cross validation\n   - monitor score during feature engineering\n   - search and optimize XGBoost","47930b23":"# Load Data\n***","d3fe96a8":"#### Group: Property","f0e6be09":"I tried exploring *LotArea*, but didn't find anything useful yet.","5dffd890":"tune and search in [default value]:\n1. eta (learning rate) [0.3]\n2. gamma (decision boundary) [0]\n3. max_depth (tree depth) [6]\n4. min_child_weight [1]\n5. lambda (L2 reg) [1]\n6. alpha (L1 reg, high dimensionality) [0] \n7. n_estimators","498b46b9":"[Eyebrows raised when I saw *FireplaceQU*, will investigate if I return to this project.  \nAlso want to get more comfortable interpreting feature importances first.]","69f2a289":"My goal for new features was to have a nice distribution across *SalePrice*.\n<br>For a quick pass, a few decent features were found.\n<br>*BsmtFin2* doesn't look promising from the graphs but seems to be somewhat informative. I'll keep it.","6bfcf848":"# Baseline Score\n***\nFrom Kaggle Learn's course on feature engineering, I learned how desirable it is to acquire a **baseline metric** as early as possible.  \nThis will be my \"North Star\" for any changes I make. It isn't true north, but will keep me heading in the right direction.","a0f52d23":"trained_XGBR.get_params\n- {'objective': 'reg:squarederror',\n- 'base_score': 0.5,\n- 'booster': 'gbtree',\n- 'colsample_bylevel': 1,\n- 'colsample_bynode': 1,\n- 'colsample_bytree': 1,\n- 'gamma': 0.000251073210724123,\n- 'gpu_id': -1,\n- 'importance_type': 'gain',\n- 'interaction_constraints': '',\n- 'learning_rate': 0.12177128646131516,\n- 'max_delta_step': 0,\n- 'max_depth': 2,\n- 'min_child_weight': 1.2266087138381812,\n- 'missing': nan,\n- 'monotone_constraints': '()',\n- 'n_estimators': 751,\n- 'n_jobs': 4,\n- 'num_parallel_tree': 1,\n- 'random_state': 0,\n- 'reg_alpha': 0.115889838065404,\n- 'reg_lambda': 1.7772392866942126,\n- 'scale_pos_weight': 1,\n- 'subsample': 1,\n- 'tree_method': 'exact',\n- 'validate_parameters': 1,\n- 'verbosity': None}","9f7c2b63":"#### Group: Size","16b9e8db":"#### Group: Garage","49412182":"I can clearly spot some outliers which I'll omit in training.","2dc4133c":"## Mutual Information","bd17fa74":"# Exploratory Data Analysis & Feature Engineering\n***\nAt this stage I haven't discovered much about the dataset.  \nI want to discover how features interact with each other and the target.  \nI'll perform some **univariate and multivariate analysis**.  \n<br><br>\nI'll also try to **create some new features** by combining those that stand out to me during analysis.  \nI generally try to leverage an informative feature by combining it with low MIS features to try to expose some interactions.","d2494b4c":"## Correlation and SULOV\nFeatureWiz is a python package for feature selection.  \nI wanted to see if I could implement part of it myself, using its pseudocode.","b721c933":"Surprisingly, the previous example of dropping *Condition1* does not improve the score after feature selection via SULOV and imbalance.  \nLooking at class imbalance and MIS alone is not sufficient to guide feature selection.  \nThis demonstrates the intricacies of feature selection, requiring case by case study.  \nXGBoost does a better job than most algorithms at learning to ignore uninformative features.  \nI'll let the boosted trees do the rest.","703bbcde":"# Hyperparameter Tuning and Search","1b698709":"I referred to *data_description.txt* to discover which categorical features had integer values.","198c1576":"High MIS features are likely useful, but even low MIS features may have usefulness that can be exposed with feature engineering.  \nUnsurprisingly, some features describing size, quality, and location of the building score quite well.","00001600":"# Preface\n***\nMy name is Mark-Felix, thanks for checking out my notebook!  \n<br>\nIn this notebook I'll try to predict the housing prices without having a deep understanding of the data.  \nMy goal here is to practice techniques I've acquired from various MOOCs, blogs, talks, and highly regarded notebooks.  \n<br>\nI'll only use XGBoost to narrow the scope of the project.  \nIn particular, I aim to **become more comfortable with**\n- the kaggle and notebook **workflow**\n- understanding datasets that I am not a subject matter expert on\n- learning how to implement custom **transformers and pipelines**\n- **feature engineering** strategies\n- defining a baseline score and iterating to improve it","212402c8":"# Feature Selection\n***\nGenerally, I would perform feature selection after EDA and feature engineering, before training.  \nHowever, I think trimming some features now would speed up EDA, if I can drop those unlikely to be useful in feature engineering.  \n<br>\nMy objective is to reduce the feature set to include **informative and non-redundant** features.  \nA useful metric for this is the **Mutual Information Score (MIS)** of a feature and the target.  \nFor numerical features, I'll look at MIS and correlations (SULOV).  \nFor categorical features, I'll look at MIS and class balances.  "}}