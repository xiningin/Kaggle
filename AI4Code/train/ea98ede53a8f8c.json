{"cell_type":{"caa0d167":"code","d6cd97c0":"code","623a67be":"code","916d989b":"code","1943f279":"code","3169088e":"code","458146fc":"code","c2d75742":"code","d328bc1a":"code","98c59635":"code","e420f727":"code","a70e9ddd":"code","94e1eb11":"code","396ea6b2":"code","35f9b966":"code","ca1b1c3b":"code","c3ac2bc9":"code","a33e90a6":"code","4b8ed66a":"code","d5e7ec48":"code","d0161c83":"code","f3975edd":"code","9289e23c":"code","a85d6b2a":"code","9ea6bb50":"code","e5b7882c":"code","26805720":"code","a44d7534":"code","543bc518":"code","176762f2":"code","9e2d475a":"code","0568d43f":"code","f4bb8e97":"code","6fac6ad1":"code","b56ea857":"code","2566da20":"code","af66c2da":"code","4abe9ad2":"code","64bbe902":"markdown","1117c900":"markdown"},"source":{"caa0d167":"!pip install dataprep by","d6cd97c0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport warnings\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings(\"ignore\")","623a67be":"df=pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf.head()","916d989b":"#Drop id \ndf.drop(columns=['id'],inplace=True) ","1943f279":"df.info()","3169088e":"df.describe()","458146fc":"df.isna()","c2d75742":"df.isnull().sum()","d328bc1a":"# Chack if we have miss data\nplot_missing(df)","98c59635":"# Imputing the missing values with the mean\ndf=df.fillna(np.mean(df['bmi']))\ndf.info()","e420f727":"import pandas_profiling as pp\npp.ProfileReport(df)","a70e9ddd":"plot(df)","94e1eb11":"df","396ea6b2":"# plots the distribution of column x in various ways and calculates column statistics\nplot(df, 'stroke')","35f9b966":"plot(df, 'smoking_status')","ca1b1c3b":"plot(df, 'bmi')","c3ac2bc9":"plot(df, 'heart_disease')","a33e90a6":"plot_correlation(df)","4b8ed66a":"create_report(df)","d5e7ec48":"df['stroke'].value_counts()\n\ndf['stroke'].value_counts() * 100 \/ len(df)\n\n\nsns.countplot(x='stroke', data=df, palette='viridis')","d0161c83":"plt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), annot=True)","f3975edd":"# Convert Marrital Status, Residence and Gender into 0's and 1's\ndf['gender']=df['gender'].apply(lambda x : 1 if x=='Male' else 0) \ndf[\"Residence_type\"] = df[\"Residence_type\"].apply(lambda x: 1 if x==\"Urban\" else 0)\ndf[\"ever_married\"] = df[\"ever_married\"].apply(lambda x: 1 if x==\"Yes\" else 0)","9289e23c":"# Removing the observations that have smoking_status type unknown. \ndf=df[df['smoking_status']!='Unknown']","a85d6b2a":"df","9ea6bb50":"# used One Hot encoding smoking_status, work_type\ndata_dummies = df[['smoking_status','work_type']]\ndata_dummies=pd.get_dummies(data_dummies)\ndf.drop(columns=['smoking_status','work_type'],inplace=True)","e5b7882c":"data_dummies","26805720":"df","a44d7534":"y=df['stroke']\ndf.drop(columns=['stroke'],inplace=True)\nx=df.merge(data_dummies,left_index=True, right_index=True,how='left')\n","543bc518":"x","176762f2":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nx, y = oversample.fit_resample(x, y)","9e2d475a":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size=0.20,random_state=0)","0568d43f":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","f4bb8e97":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6fac6ad1":"X_train","b56ea857":"def models(X_train,Y_train):\n  \n  #Using Logistic Regression Algorithm to the Training Set\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n    \n  #Using SVC method of svm class to use Support Vector Machine Algorithm\n  from sklearn.svm import SVC\n  svc_lin = SVC(kernel = 'linear', random_state =0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Kernel SVM Algorithm\n  from sklearn.svm import SVC\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\n  from sklearn.naive_bayes import GaussianNB\n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n \n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n  forest.fit(X_train, Y_train)\n  \n    \n  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n    \n    \n    \n    \n #Using xgboostClassifier of tree class to use Decision Tree Algorithm\n  from xgboost import XGBClassifier \n  xgboost = XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0, \n                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005)\n  xgboost.fit(X_train, Y_train)\n    \n    \n    \n #Using  SGDClassifierr of tree class to use Decision Tree Algorithm    \n  from sklearn.linear_model import SGDClassifier\n  SGD = SGDClassifier()\n  SGD.fit(X_train, Y_train)\n    \n    \n  #Using  AdaBoostClassifier of tree class to use Decision Tree Algorithm    \n  from sklearn.ensemble import AdaBoostClassifier\n  Ada = AdaBoostClassifier(n_estimators=2000, random_state = 0)\n  Ada.fit(X_train, Y_train)\n    \n    \n  \n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train)*100)\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train)*100)\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train)*100)\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train)*100)\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train)*100)\n  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train)*100)\n  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train)*100)\n  print('[7]Xgboost Classifier Training Accuracy:', xgboost.score(X_train, Y_train)*100)\n  print('[8]SGD Classifier Training Accuracy:', SGD.score(X_train, Y_train)*100)\n  print('[9]AdaBoost Classifier Training Accuracy:', Ada.score(X_train, Y_train)*100)\n\n  return log, knn, svc_lin, svc_rbf, gauss,tree,forest,xgboost,SGD,Ada\n\nmodel = models(X_train,Y_train)","2566da20":"#Show the confusion matrix and accuracy for all of the models on the test data\n#Classification accuracy is the ratio of correct predictions to total predictions made.\nfrom sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  TN = cm[0][0]\n  TP = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  print(cm)\n  print('Model[{}] Testing Accuracy = \"{}!\"'.format(i,  (TP + TN) \/ (TP + TN + FN + FP)))\n  print()# Print a new line\n\n#Show other ways to get the classification accuracy & other metrics \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(len(model)):\n  print('Model ',i)\n  #Check precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print( accuracy_score(Y_test, model[i].predict(X_test)))\n  print()#Print a new line","af66c2da":"#Print Prediction of Random Forest Classifier model\npred = model[6].predict(X_test)\nprint(pred)\n#Print a space\nprint()\n#Print the actual values\nprint(Y_test)","4abe9ad2":"#Accuracy Score\nacc_1 = 0.8982169390787519\nacc_2 = 0.9279346210995543\nacc_3 = 0.8952451708766717\nacc_4 = 0.9071322436849926\nacc_5 = 0.5520059435364042\nacc_6 = 0.9346210995542348\nacc_7 = 0.9531946508172363\nacc_8 = 0.9041604754829123\nacc_9 = 0.9019316493313522\nacc_10 = 0.9234769687964339\n\n\nresults = pd.DataFrame([[\"Logistic Regression\",acc_1],[\"K-Nearest Neighbor\",acc_2],[\" SVM(Linear)\",acc_3],\n                       [\"SVM(RBF)\",acc_4],[\"Gaussian Naive Bayes\",acc_5],\n                       [\"Decision Tree\",acc_6],[\"Random Forest\",acc_7],[\"Xgboost\",acc_8],[\"SGD\",acc_9] ,[\"AdaBoost\",acc_10]],\n                        columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\n\n\nresults.style.background_gradient(cmap='Blues')","64bbe902":"<h1 style='background-color:#6495ED ; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;' > Exploratory Data Analysis <\/h1>\n\nExploratory Data Analysis (EDA) is the method of examining a dataset to learn about its main characteristics. The dataprep.eda package makes this step easier by allowing users to explore key characteristics using simple APIs. Each API allows the user to examine the dataset at various levels, from high to low, and from various angles. Especially.\n\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcREYfRclj8FnDD3fE9BQNivtsj1XBoFg89AWeGgYRgyldGk_nKvfGe0q-OVDw-8deNpSA4&usqp=CAU\" width=\"800px\">\n\n\n\n\n<h1 style='background-color:#6495ED ; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;' > Stroke Prediction with 10 Algorithms <\/h1>\n\n\n* 1- Logistic Regression \n* 2- K Nearest Neighbor \n* 3- Support Vector Machine with linear\n* 4- Support Vector Machine with rbf\n* 5- Gaussian Naive Bayes \n* 6- Decision Tree\n* 7- Random Forest Classifier\n* 8- Xgboost Classifier\n* 9- SGD Classifie\n* 10-AdaBoost Classifier\n\n\n\n\n\n\n### Attribute Information\n* 1) id: unique identifier.\n* 2) gender: \"Male\", \"Female\" or \"Other\".\n* 3) age: age of the patient.\n* 4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension.\n* 5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease.\n* 6) ever_married: \"No\" or \"Yes\".\n* 7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\".\n* 8) Residence_type: \"Rural\" or \"Urban\".\n* 9) avg_glucose_level: average glucose level in blood.\n* 10) bmi: body mass index.\n* 11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*.\n* 12) stroke: 1 if the patient had a stroke or 0 if not.\n\n\n\n### Dataset Link\n\n##### [Herer](https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset)\n","1117c900":" \n<h1 style='background-color: #6495ED; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;' > Stroke Prediction  <\/h1>\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.Doctors can predict patients' risk for ischemic stroke based on the severity of their metabolic syndrome, a conglomeration of conditions that includes high blood pressure, abnormal cholesterol levels and excess body fat around the abdomen and waist, a new study finds.\n\n<img src=\"https:\/\/guardian.ng\/wp-content\/uploads\/2017\/06\/brainomixstroke.jpg\" width=\"800px\">"}}