{"cell_type":{"2f5ace7d":"code","8d4f290b":"code","2d2a8576":"code","ab18e6b7":"code","47f8ed98":"code","be520ed1":"code","99364352":"code","534e0276":"code","9805310c":"code","93e3bbb3":"code","55eddf9f":"code","3409ab84":"code","5a0dcae1":"code","effa8f3d":"code","adc0604b":"code","ef51d3f0":"code","da2b7d7a":"code","bf4031ea":"code","aa1e7384":"code","14eec76c":"code","78f09c20":"code","84a49ac7":"code","8d45f48b":"code","d4d7d5a2":"code","cff5a80f":"code","4eb31cf6":"code","81e63794":"code","3af9ad34":"code","9a0648d2":"code","500ddf8d":"code","8c68e59e":"code","f19cc99d":"code","cb4a6835":"code","12c8e055":"code","047da9f0":"code","6b21c47d":"code","d9c2c4e2":"code","f54546bb":"code","b58fbede":"code","72723398":"code","a6aa5e15":"code","5dfafdda":"code","d89ed307":"code","915071d0":"code","5572e566":"code","baa817b5":"code","369dbdc9":"code","65747d61":"code","94a721f6":"code","a99f339c":"code","afac88c6":"code","eb98fb47":"markdown","9fdd5e2d":"markdown","3848c2ed":"markdown","b2e509c4":"markdown","b07c0c39":"markdown","cea0f2e0":"markdown","33a18d85":"markdown","2e0ee4dc":"markdown","45bf30c7":"markdown","28040a6b":"markdown","97f4c712":"markdown","7d5d86ed":"markdown","9ec4a78c":"markdown","677f212d":"markdown","d966bb7d":"markdown","98099515":"markdown","886b8c3e":"markdown","cb672060":"markdown","5a4f6844":"markdown","ac1d6a61":"markdown","bd433fdb":"markdown","8a3af042":"markdown","8d998ad9":"markdown","d04841e1":"markdown","05dbd612":"markdown","ed602c3a":"markdown","2119d271":"markdown","44197405":"markdown","48b4d7a4":"markdown","4c2c5aec":"markdown","314cf7cd":"markdown"},"source":{"2f5ace7d":"import os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils import resample\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","8d4f290b":"df_train = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')","2d2a8576":"# Juntando dados de treino e teste\ndf_all = df_train.append(df_test)\ndf_all.info()","ab18e6b7":"# Tamanhos das bases de treino e teste\n# O n\u00famero de dados de treino \u00e9 menor que os dados de teste\n# Em tese, essa caracter\u00edstica torna o trabalho de classifica\u00e7\u00e3o mais dif\u00edcil\ndf_train.shape, df_test.shape","47f8ed98":"## Informa\u00e7\u00f5es do dataset \n# 9 vari\u00e1veis float, 129 inteiras e 5 object\ndf_all.info()","be520ed1":"# Vari\u00e1veis do tipo 'object': Id, idhogar, dependency, edjefe, edjefa\n# Id e idhogar s\u00e3o identificadores de pessoa e casa - ok\n# dependency, edjefe, edjefa requerem tratamento\ndf_all.select_dtypes('object').head(5)","99364352":"# Substituindo 'yes' e 'no'\nyes_no_map = {'no': 0, 'yes': 1}\n\ndf_all['edjefe'] = df_all['edjefe'].replace(yes_no_map).astype(np.float32)\ndf_all['edjefa'] = df_all['edjefa'].replace(yes_no_map).astype(np.float32)\n","534e0276":"df_all['dependency'].value_counts()","9805310c":"# Os valores 0 e 1 n\u00e3o est\u00e3o na lista acima, confirmando que a regra 'yes'=1 e 'no'=0 se aplica\n# \u00c9 feita a substitui\u00e7\u00e3o:\ndf_all['dependency'] = df_all['dependency'].replace(yes_no_map).astype(np.float32)","93e3bbb3":"# Confirmando os resultados\n# => n\u00e3o existem outras vari\u00e1veis do tipo object ap\u00f3s os tratamentos, \u00e0s exce\u00e7\u00f5es dos identificadores de pessoa (Id) e casa (idhogar)\ndf_all.select_dtypes('object').head(5)","55eddf9f":"# Do resultado, as vari\u00e1veis com valores nulos s\u00e3o: rez_esc, v18q1, v2a1, SQBmeaned, meaneduc\n# os valores nulos da vari\u00e1vel Target se referem aos dados de teste => comportamento esperado\ndf_all.isnull().sum().sort_values(ascending=False).head(7)","3409ab84":"# Uma visualiza\u00e7\u00e3o mais sofisticada....\n\ndf_all_null = df_all.isnull().sum()\n\n# filtra apenas as vari\u00e1veis que cont\u00eam valores nulos, representadas com rela\u00e7\u00e3o ao total\ndf_all_null_non_zero = (df_all_null[df_all_null>0] \/ df_all.shape[0]).sort_values(ascending=False)\n\nsns.barplot(x=df_all_null_non_zero, y=df_all_null_non_zero.index)\n_ = plt.title('Percentual de Valores Nulos')","5a0dcae1":"# O valor zero (0) \u00e9 presente no dados => n\u00e3o \u00e9 poss\u00edvel aplicar uma regra de neg\u00f3cio espec\u00edfica\ndf_all['rez_esc'].describe()","effa8f3d":"# Investiga\u00e7\u00e3o do outlier 99 (valor m\u00e1ximo verificado no describe)\ndf_all['rez_esc'].value_counts()","adc0604b":"# Apenas uma ocorr\u00eancia deste outlier, provavelmente um erro de alimenta\u00e7\u00e3o de dados\n# este ser\u00e1 substitu\u00eddo pelo pior valor de anos atrasados presente na base (5)\ndf_all['rez_esc'].replace(to_replace=99, value=5, inplace=True)\ndf_all['rez_esc'].value_counts()","ef51d3f0":"# solu\u00e7\u00e3o: atribuir o valor -1 para os valores nulos de rez_esc\n# isso \u00e9 uma estrat\u00e9gia v\u00e1lida para algoritmos baseados em \u00e1rvore\ndf_all['rez_esc'].fillna(-1, inplace=True)","da2b7d7a":"# O valor zero (0) \u00e9 presente no dados => n\u00e3o \u00e9 poss\u00edvel aplicar uma regra de neg\u00f3cio espec\u00edfica\ndf_train['v2a1'].describe()","bf4031ea":"# solu\u00e7\u00e3o: atribuir um valor -1 para os valores nulos de v2a1\ndf_all['v2a1'].fillna(-1, inplace=True)","aa1e7384":"# correlacionada com  a vari\u00e1vel 'v18q', que identifica se a fam\u00edlia possui um tablet\n# => valores nulos correspondem a zero tablet\ndf_all['v18q1'].fillna(0, inplace=True)","14eec76c":"# se referem a uma fra\u00e7\u00e3o muito pequena do dataset (0,1%)\ndf_all['meaneduc'].isnull().sum() \/ df_all['meaneduc'].count() * 100","78f09c20":"# imputa\u00e7\u00e3o de 'meaneduc' pela mediana\ndf_all['meaneduc'].fillna(df_all['meaneduc'].median(), inplace=True)","84a49ac7":"## SQBmeaned: derivado de 'meaneduc'\n# imputa\u00e7\u00e3o pela mediana\ndf_all['SQBmeaned'].fillna(df_all['SQBmeaned'].median(), inplace=True)","8d45f48b":"## Verifica\u00e7\u00e3o do dataset ap\u00f3s os tratamentos dos missing values\n# pelo resultado, percebe-se que as vari\u00e1veis com valores ausentes foram devidamente tratadas\ndf_all.isnull().sum().sort_values(ascending=False).head(7)","d4d7d5a2":"# Confirma\u00e7\u00e3o que n\u00e3o existem  valores nulos em 'Target'\ndf_train['Target'].isnull().sum()","cff5a80f":"# Contagem dos valores 'Target'\ndf_train['Target'].value_counts().sort_index(ascending=False)","4eb31cf6":"# Distribui\u00e7\u00e3o dos valores 'Target'\nax = sns.countplot(x='Target', data=df_train)\n_ = plt.title('Distribui\u00e7\u00e3o dos valores Target')","81e63794":"# N\u00famero e percentual de chefes de fam\u00edlia na base de treino\nn_chefes_familia = df_train[df_train['parentesco1']==1]['parentesco1'].sum()\nn_chefes_familia, n_chefes_familia \/ df_train.shape[0] * 100","3af9ad34":"# N\u00famero e percentual de chefes de fam\u00edlia na base de teste\nn_chefes_familia = df_test[df_test['parentesco1']==1]['parentesco1'].sum()\nn_chefes_familia, n_chefes_familia \/ df_test.shape[0] * 100","9a0648d2":"# Exclus\u00e3o vari\u00e1veis SQBxxx\ncol_excluir = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned']\ndf_all.drop(col_excluir, axis=1, inplace=True)\ndf_train.drop(col_excluir, axis=1, inplace=True)\ndf_test.drop(col_excluir, axis=1, inplace=True)","500ddf8d":"## Confer\u00eancia final do dataframe\n# sem valores ausentes; sem vari\u00e1veis object\ndf_all.info()","8c68e59e":"correlacao = df_train.corr()\ncorrelacao = correlacao['Target'].sort_values(ascending=False)\n\nprint(f'10 features positivas mais relevantes: \\n{correlacao.head(10)}')\nprint('\\n', '=' * 50, '\\n')\nprint(f'10 features negativas mais relevantes:: \\n{correlacao.tail(10)}')","f19cc99d":"# Observa\u00e7\u00e3o: do resultado acima, chamou aten\u00e7\u00e3o o valor NaN da vari\u00e1vel 'elimbasu5'\n# Investigando-se a mesma nota, nota-se que ela possui apenas um valor para toda a base de treinamento, portanto sem nenhuma fun\u00e7\u00e3o explicativa\ndf_train['elimbasu5'].value_counts()","cb4a6835":"# Decis\u00e3o: eliminar essa vari\u00e1vel\ndf_all.drop('elimbasu5', axis=1, inplace=True)\ndf_train.drop('elimbasu5', axis=1, inplace=True)\ndf_test.drop('elimbasu5', axis=1, inplace=True)","12c8e055":"# Separa as bases de treino e teste\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]\n\ntrain = df_all[~df_all['Target'].isnull()]\ntest = df_all[df_all['Target'].isnull()]\n\ntrain.shape, test.shape","047da9f0":"# Aplica over-sampling SMOTE\n# sm = SMOTE()\n# X_train, y_train = sm.fit_resample(X_train,y_train)\n# X_valid, y_valid = sm.fit_resample(X_valid, y_valid)","6b21c47d":"# Separa os dados de treino de acordo com a classifica\u00e7\u00e3o de pobreza (valor de Target)\ntrain_1 = train[train['Target'] == 1]\ntrain_2 = train[train['Target'] == 2]\ntrain_3 = train[train['Target'] == 3]\ntrain_4 = train[train['Target'] == 4]\n\ntrain_1.shape, train_2.shape,train_3.shape,train_4.shape","d9c2c4e2":"# Aplica over-sampling na base de treino\ntrain_1_over = resample(train_1,                 # aumenta a classe menor\n                       replace=True,             # sample com replacement\n                       n_samples=len(train_4),   # iguala \u00e0 maior classe (4)\n                       random_state=42)\n\ntrain_2_over = resample(train_2,               \n                       replace=True,             \n                       n_samples=len(train_4),  \n                       random_state=42)\n\ntrain_3_over = resample(train_3,               \n                       replace=True,             \n                       n_samples=len(train_4),  \n                       random_state=42)\n\n\ntrain_1_over.shape,train_2_over.shape,train_3_over.shape,train_4.shape","f54546bb":"# Junta os dados\ntrain = pd.concat([train_1_over, train_2_over, train_3_over, train_4])\ntrain.shape","b58fbede":"# Separa a base de treino em treino e valida\u00e7\u00e3o\ntrain, valid = train_test_split(train, test_size=0.20, random_state=42)\n\nX_train = train[feats]\ny_train = train['Target'].astype(int)\n\nX_valid = valid[feats]\ny_valid = valid['Target'].astype(int)\n\nX_test = test[feats]\n\ntrain.shape, valid.shape, test.shape","72723398":"# Fun\u00e7\u00e3o que avalia F1-score e acur\u00e1cia do modelo\ndef avalia_modelo (val_true, val_pred):\n    acuracia = accuracy_score(y_true=val_true, y_pred=val_pred)\n    f1score = f1_score(y_true=val_true, y_pred=val_pred, average='macro')\n    return f1score, acuracia","a6aa5e15":"## Random Forest \nrf = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)\nrf.fit(X_train, y_train)\n\n# Avalia modelo\nf1score, acuracia = avalia_modelo(y_valid, rf.predict(X_valid))\n\n# Apresenta\u00e7\u00e3o dos resultados\nprint (\"Random Forest\")\nprint(\"F1-Score:\", f1score)\nprint(\"Acur\u00e1cia:\", acuracia)","5dfafdda":"## Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)\ngbm.fit(X_train, y_train)\n\n# Avalia modelo\nf1score, acuracia = avalia_modelo(y_valid, gbm.predict(X_valid))\n\n# Apresenta\u00e7\u00e3o dos resultados\nprint (\"Gradient Boosting\")\nprint(\"F1-Score:\", f1score)\nprint(\"Acur\u00e1cia:\", acuracia)","d89ed307":"## XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators=200, learning_rate=0.09, random_state=42)\nxgb.fit(X_train, y_train)\n\n# Avalia modelo\nf1score, acuracia = avalia_modelo(y_valid, xgb.predict(X_valid))\n\n# Apresenta\u00e7\u00e3o dos resultados\nprint (\"XGBoost\")\nprint(\"F1-Score:\", f1score)\nprint(\"Acur\u00e1cia:\", acuracia)","915071d0":"## AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\nabc = AdaBoostClassifier(n_estimators=200, learning_rate=1.0, random_state=42)\nabc.fit(X_train, y_train)\n\n# Avalia modelo\nf1score, acuracia = avalia_modelo(y_valid, abc.predict(X_valid))\n\n# Apresenta\u00e7\u00e3o dos resultados\nprint (\"AdaBoost\")\nprint(\"F1-Score:\", f1score)\nprint(\"Acur\u00e1cia:\", acuracia)","5572e566":"## CatBoost\nfrom catboost import CatBoostClassifier\n\ncbc = CatBoostClassifier(random_state=42)\ncbc.fit(X_train, y_train)\n\n# Avalia modelo\nf1score, acuracia = avalia_modelo(y_valid, cbc.predict(X_valid))\n\n\n# Apresenta\u00e7\u00e3o dos resultados\nprint (\"CatBoost\")\nprint(\"F1-Score:\", f1score)\nprint(\"Acur\u00e1cia:\", acuracia)","baa817b5":"## LightGBM\n# par\u00e2metros derivados de https:\/\/www.kaggle.com\/denismarcio\/improve-vision-with-lightgbm\/\nimport lightgbm as lgb\n\nclf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                                 random_state=None, silent=True, metric='None', \n                                 n_jobs=4, n_estimators=5500, class_weight='balanced',\n                                 colsample_bytree =  0.89, min_child_samples = 90, num_leaves = 56, subsample = 0.96)\nclf.fit(X_train, y_train)\n\n# Avalia modelo\nf1score, acuracia = avalia_modelo(y_valid, clf.predict(X_valid))\n\n# Apresenta\u00e7\u00e3o dos resultados\nprint (\"LightGBM\")\nprint(\"F1-Score:\", f1score)\nprint(\"Acur\u00e1cia:\", acuracia)","369dbdc9":"# Recupera as bases de treino e teste\n\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]\n\n# Recupera a base de treino com oversampling \ntrain_full = pd.concat([train_1_over, train_2_over, train_3_over, train_4])\ntest = df_all[df_all['Target'].isnull()]\n\nX_train_full = train_full[feats]\ny_train_full = train_full['Target'].astype(int)\n\nX_test = test[feats]\n\ntrain_full.shape, test.shape","65747d61":"## Random Forest \nrf = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)\nrf.fit(X_train_full, y_train_full)\n","94a721f6":"## LightGBM\nclf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                                 random_state=None, silent=True, metric='None', \n                                 n_jobs=4, n_estimators=5500, class_weight='balanced',\n                                 colsample_bytree =  0.89, min_child_samples = 90, num_leaves = 56, subsample = 0.96)\nclf.fit(X_train_full, y_train_full)\n","a99f339c":"## Realiza previs\u00e3o do valor de Target na base de teste \n\n# Random Forest\n#test['Target'] = rf.predict(X_test).astype(int)\n\n# LightGBM\ntest['Target'] = clf.predict(X_test).astype(int)\n","afac88c6":"# Cria o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","eb98fb47":"# Exclus\u00e3o de Vari\u00e1veis\n* Para redu\u00e7\u00e3o da dimensionalidade, as vari\u00e1veis que representam o quadrado de outras vari\u00e1veis (SQBxxx) s\u00e3o descartadas\n* Eles seriam \u00fateis para um modelo linear, mas s\u00e3o in\u00fateis para um modelo baseado em \u00e1rvore e podem confundi-lo\n* Fonte: https:\/\/www.kaggle.com\/mlisovyi\/feature-engineering-lighgbm-with-f1-macro","9fdd5e2d":"## An\u00e1lise dos resultados - com base de valida\u00e7\u00e3o\n#### Random Forest:\n    F1-Score: 0.9939934343153598\n    Acur\u00e1cia: 0.9940072954663888\n\n#### LightGBM\n    F1-Score: 0.9932042106373568\n    Acur\u00e1cia: 0.993225638353309\n\n### CatBoost\n    F1-Score: 0.976593475869052\n    Acur\u00e1cia: 0.9768108389786347\n\n### Gradient Boosting\n    F1-Score: 0.6199833610920772\n    Acur\u00e1cia: 0.6206357477853048\n\n### XGBoost\n    F1-Score: 0.939220952973292\n    Acur\u00e1cia: 0.9395518499218343\n\n### AdaBoost\n    F1-Score: 0.5263839922419775\n    Acur\u00e1cia: 0.52892131318395\n* Dos resultados acima, os algoritmos com melhor desempenho foram o Random Forest e o LightGBM\n* Estes dois algoritmos ser\u00e3o utilizados para o treino com toda a base de treino (sem valida\u00e7\u00e3o) e depois aplicados na base de teste, para submiss\u00e3o ao Kaggle","3848c2ed":"# Chefes de fam\u00edlia\n* Somente os chefes de fam\u00edlia s\u00e3o usados na pontua\u00e7\u00e3o. \n* Todos os membros da fam\u00edlia s\u00e3o inclu\u00eddos no teste + o envio da amostra, mas apenas os chefes de fam\u00edlia s\u00e3o pontuados.\n* Vari\u00e1vel parentesco1=1 indica se \u00e9 chefe de fam\u00edlia \n","b2e509c4":"### Vari\u00e1vel v18q1\n* N\u00famero de tablets que a fam\u00edlia possui","b07c0c39":"## Tratamento das Vari\u00e1veis Object","cea0f2e0":"## Considera\u00e7\u00f5es iniciais\n- Cada linha representa uma pessoa\n- M\u00faltiplas pessoas podem fazer parte de uma \u00fanica resid\u00eancia\n- Mais de uma fam\u00edlia pode viver na mesma casa\n- A previs\u00e3o deve se dar apenas para os chefes de fam\u00edlia ","33a18d85":"### Vari\u00e1veis edjefe e edjefa\n* Anos de escolaridade do chefe \/ da chefe de fam\u00edlia \n* Do dicion\u00e1rio de dados, yes=1 e no=0 => basta fazer a substitui\u00e7\u00e3o","2e0ee4dc":"# Tratamento dos Valores Ausentes ","45bf30c7":"### T\u00e9cnica SMOTE\n* N\u00e3o obteve resultados satisfat\u00f3rios\n* F1-Score: 0.8288343045151485\n* Acur\u00e1cia: 0.830938292476754","28040a6b":"Os resultados acima indicam um problema de classifica\u00e7\u00e3o com classes desbalanceadas. A quantidade de fam\u00edlias em situa\u00e7\u00e3o n\u00e3o vulner\u00e1vel \u00e9 muito superior ao n\u00famero das demais fam\u00edlias","97f4c712":"# Separa\u00e7\u00e3o das bases de treino e teste","7d5d86ed":"# Distribui\u00e7\u00e3o da vari\u00e1vel 'Target'","9ec4a78c":"### Vari\u00e1vel dependency\n* Taxa de depend\u00eancia: (membros da fam\u00edlia  potencialmente dependentes) \/ (n\u00famero de trabalahadores potencialmente ativos)\n* F\u00f3rmula: (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)","677f212d":"* 31.1% da base de treino \u00e9 composta de chefes de fam\u00edlia\n* 30.7% da base de treino \u00e9 composta de chefes de fam\u00edlia\n* Nota-se o balanceamento da distribui\u00e7\u00e3o entre as quantidades de chefes de fam\u00edlia nas bases de treino e teste\n","d966bb7d":"### Vari\u00e1vel rez_esc\n* Anos atrasados na escola","98099515":"### Vari\u00e1vel v2a1\n* Valor do aluguel","886b8c3e":"Considera\u00e7\u00f5es:\n* As vari\u00e1veis mais correlacionadas com taxa de pobreza est\u00e3o relacionadas com a educa\u00e7\u00e3o dos adultos (meaneduc, escolari), n\u00famero de criancas na casa (hogar_nin, r4t1), se a casa tem teto (cielorazo). \n* Isso coincide com o senso comum","cb672060":"### T\u00e9cnica de over-sampling tradicional (sklearn)","5a4f6844":"# An\u00e1lise de Correla\u00e7\u00e3o","ac1d6a61":"Valores poss\u00edveis:\n* 1: pobreza extrema\n* 2: pobreza moderada\n* 3: fam\u00edlias vulner\u00e1veis\n* 4: fam\u00edlias n\u00e3o vulner\u00e1veis","bd433fdb":"# Bibliotecas","8a3af042":"# Sugest\u00f5es de Trabalhos Futuros\n* Melhorar a etapa de feature engineering, fazendo consolida\u00e7\u00e3o de vari\u00e1veis \n* Aprimorar a an\u00e1lise relacionada aos chefes de fam\u00edlia, dado que somente estes que s\u00e3o avaliados pelo Kaggle. Avaliar a possibilidade de criar mais colunas, trazendo informa\u00e7\u00f5es gerais dos membros de fam\u00edlia para o chefe de fam\u00edlia","8d998ad9":"### Vari\u00e1vel meaneduc\n* M\u00e9dia de anos de educa\u00e7\u00e3o para adultos (+18)","d04841e1":"# Carregando os dados","05dbd612":"# An\u00e1lise Explorat\u00f3ria \/ Pr\u00e9-processamento dos dados","ed602c3a":"# Resultados Finais\n\n* LightGBM: 0.40331  \n* Random Forest: 0.37312 \n* Embora o Random Forest tenha obtido um resultado ligeiramente melhor na base de valida\u00e7\u00e3o, o algoritmo LightGBM obteve melhor desempenho na base de teste, ou seja, soube generalizar melhor\n","2119d271":"\n# Treinamento Inicial - com dados de valida\u00e7\u00e3o\n* Utiliza base de valida\u00e7\u00e3o para a escolha do melhor algoritmo\n* Os dois melhores algoritmos selecionados nesta fase ser\u00e3o utilizado para treinar com a base completa de treino\n* O melhos dos dois resultados ser\u00e1 submetido \u00e0 competi\u00e7\u00e3o ","44197405":"# Submiss\u00e3o para Competi\u00e7\u00e3o","48b4d7a4":"# IESB - Miner II - Trabalho Final\n* Objetivo: elaborar modelo preditivo em Python para prever o n\u00edvel de pobreza de chefes de fam\u00edlia da Costa Rica, a partir de base de dados disponibilizada no Kaggle\n* Conte\u00fado:\n* * An\u00e1lise explorat\u00f3ria dos dados\n* * Pr\u00e9-processamento dos dados\n* * Aplica\u00e7\u00e3o de t\u00e9cnicas de tratamento de classes desbalanceadas \n* * Avalia\u00e7\u00e3o dos melhores algoritmos de classifica\u00e7\u00e3o, usando base de valida\u00e7\u00e3o\n* * Aplica\u00e7\u00e3o dos melhores algoritmos na base de teste\n* * Submiss\u00e3o dos resultados ao Kaggle","4c2c5aec":"# Treino dos melhores algorimos - toda a base de treino","314cf7cd":"# Balanceamento das Classes"}}