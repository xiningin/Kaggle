{"cell_type":{"4e12d671":"code","e22fd1c3":"markdown"},"source":{"4e12d671":"import collections\nimport random\nimport nltk \nnltk.download('punkt')\nimport re\nfrom collections import Counter\nfrom itertools import islice\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', -1)\nfrom time import time\nimport re\nimport string\nimport os\nimport emoji\nfrom pprint import pprint\nimport collections\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\nfrom sklearn.metrics import f1_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nimport gensim\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(37)  \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder                           \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nimport nltk\nfrom wordcloud import WordCloud\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.decomposition import LatentDirichletAllocation\n\"\"\"\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', str(text))\n    return text\n\n\n\ndef remove_repeating_char(text):\n    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n\nnegative_file = open(\"C:\/Users\/121\/.spyder-py3\/text_mining\/negative_tweets.txt\", encoding =\"utf8\")\npositive_file = open(\"C:\/Users\/121\/.spyder-py3\/text_mining\/positive_tweets.txt\", encoding =\"utf8\")\n\n\n\n\n\n\nall_features = list()\ntexts = list()\ndata_labels = list()\n\n\nfor line in positive_file:\n    \n    text_features= nltk.tokenize.sent_tokenize(negative_file)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    all_features += text_features\n    texts.append(text_features)\n    data_labels.append('pos')\n    \nfor line in negative_file:\n    \n    text_features= nltk.tokenize.sent_tokenize(negative_file)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    all_features += text_features\n    texts.append(text_features)\n    data_labels.append('neg')\n       \n\n\n    \ndf = pd.DataFrame(list(zip(texts, data_labels)),\n              columns=['texts','data_labels'])    \n\ndf['texts'] = remove_diacritics(df['texts'])\ndf['texts'] = remove_repeating_char(df['texts'])\n\"\"\"\ndf = pd.read_excel('..\/input\/tunisian-texts\/tun.xlsx')   \n\n\ndf['Length'] = df['texts'].apply(len)\ndescribe_df = df.groupby('Length').describe()\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.distplot(df['Length'], color = 'purple')\nplt.title('The Distribution of Length over the Texts', fontsize = 20)\n\n# wordcloud\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color = 'lightcyan',\n                      width = 1200,\n                      height = 700).generate(str(df['texts']))\n\nplt.figure(figsize = (15, 10))\nplt.imshow(wordcloud)\nplt.title(\"WordCloud \", fontsize = 20)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer()\nwords = cv.fit_transform(df['texts'])\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\ncolor = plt.cm.twilight(np.linspace(0, 1, 20))\nfrequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = color)\nplt.title(\"Most Frequently Occuring Words - Top 20\")\n\"\"\"\nnltk.download('stopwords')\n\n#arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n#st = ISRIStemmer()\ndf['texts'] = df['texts'].str.split()\n#df['verse'] = df['verse'].map(lambda x: [w for w in x if w not in arb_stopwords])\n# Remove harakat from the verses to simplify the corpus\n#df['verse'] = df['verse'].map(lambda x: re.sub('[\u0651\u064e\u064b\u064f\u064c\u0650\u064d\u06d9~\u0652\u06d6\u06d7]', '', x))\n\n\n# You can filter for one surah too if you want!\nverses = df['texts'].values.tolist()\n# train the model\nmodel = Word2Vec(verses, min_count=15, window=7, workers=8, alpha=0.22)\n\n# fit a 2d PCA model to the vectors\nX = model[model.wv.vocab]\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n# create a scatter plot of the projection\nplt.scatter(result[:, 0], result[:, 1])\nwords = list(model.wv.vocab)\n# Pass list of words as an argument\nfor i, word in enumerate(words):\n    reshaped_text = arabic_reshaper.reshape(word)\n    artext = get_display(reshaped_text)\n    plt.annotate(artext, xy=(result[i, 0], result[i, 1]))\nplt.show()\n\"\"\" \nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['texts'])\n# reduce dimensions\nsvd = TruncatedSVD(n_components= 5, random_state = 0)\nX_2d = svd.fit_transform(X)\nn_clusters = 20 \ntrue_k = 20\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X_2d)\nX_clustered = model.fit_predict(X_2d)\ndf_plot = pd.DataFrame(list(X_2d), list(X_clustered))\ndf_plot = df_plot.reset_index()\ndf_plot.rename(columns = {'index': 'Cluster'}, inplace = True)\ndf_plot['Cluster'] = df_plot['Cluster'].astype(int)\n\nprint(df_plot.head())\n\nprint(df_plot.groupby('Cluster').agg({'Cluster': 'count'}))\nelement_each_cluster = df_plot.groupby('Cluster').agg({'Cluster': 'count'})\n\n\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :20]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\n\nY = vectorizer.transform([\"\u0627\u0644\u062a\u0648\u064a\u062a\u0631 \u0639\u062c\u0628\u0646\u064a \u062c\u062f\u0627\"])\nprediction = model.predict(Y)\nprint(prediction)\n\nY = vectorizer.transform([\"\u0627\u0644\u0644\u0647 \u0639\u0644\u064a\u0643 \u064a\u0627  \u0627\u0644\u063a\u0627\u0644\u064a\"])\nprediction = model.predict(Y)\nprint(prediction)\n\n\n\"\"\"\n# make a column for color by clusters\ncol = df_plot['Cluster'].map({0:'b', 1:'r', 2: 'g', 3:'purple', 4:'gold'})\n\n# variable for first n dimensions we want to plot\nn = 5\n\n# visualize the clusters by first n dimensions (reduced)\nfig, ax = plt.subplots(n, n, sharex=True, sharey=True, figsize=(15,15))\nfig.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n# plot it\nk = 0\nfor i in range(0,n):\n    for j in range(0,n):\n        if i != j:\n            df_plot.plot(kind = 'scatter', x=j, y=i, c=col, ax = ax[i][j], fontsize = 18)\n        else:\n            ax[i][j].set_xlabel(i)\n            ax[i][j].set_ylabel(j)\n            ax[i][j].set_frame_on(False)\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n        \nplt.suptitle('2D clustering view of the first {} components'.format(n), fontsize = 20)\nfig.text(0.5, 0.01, 'Component n', ha='center', fontsize = 18)\nfig.text(0.01, 0.5, 'Component n', va='center', rotation='vertical', fontsize = 18)\n\n\"\"\"\n# Logistic Regression approach\n\ndf['Cluster'] = df_plot['Cluster'] \n# function for finding most significant words for each cluster\ndef generate_text(cluster):\n    \n    df_s = df['texts']\n    y = df['Cluster'].map(lambda x: 1 if x == cluster else 0)\n    count = len(df_s)\n    \n    tfidf = TfidfVectorizer()\n    X = tfidf.fit_transform(df_s)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state = 0).fit(X_train, y_train)\n    clf_d = DummyClassifier().fit(X_train, y_train)\n    acc = clf.score(X_test, y_test)\n    acc_d = clf_d.score(X_test, y_test)\n    coef = clf.coef_.tolist()[0]\n    w = tfidf.get_feature_names()\n    coeff_df = pd.DataFrame({'words' : w, 'score' : coef})\n    coeff_df = coeff_df.sort_values(['score', 'words'], ascending=[0, 1])\n    coeff_df = coeff_df[:30]\n    d = coeff_df.set_index('words')['score'].to_dict()\n    return d, acc, acc_d , coeff_df\n\n# visualized it by word clouds\nfig, ax = plt.subplots(n_clusters, sharex=True, figsize=(15,10*n_clusters))\n\nfor i in range(0, n_clusters):\n    d, acc, acc_d, coeff_df = generate_text(i)\n    wordcloud = WordCloud(max_font_size=40, collocations=False, colormap = 'Reds', background_color = 'white').fit_words(d)\n    ax[i].imshow(wordcloud, interpolation='bilinear')\n    ax[i].set_title('Cluster {} \\nLR accuracy: {} \\nDummy classifier accuracy: {}'.format(i, acc, acc_d), fontsize = 20)\n    ax[i].axis(\"off\")\n\n\n\n# LDA\nno_topics = 5\n\nc = CountVectorizer()\nX_text_c = c.fit_transform(df['texts'])\n\nlda = LatentDirichletAllocation(learning_method = 'online', n_components=no_topics, random_state=0).fit(X_text_c)\nX_text_c_feature_names = c.get_feature_names()\n\ndef display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print (\"Topic %d:\" % (topic_idx))\n        print (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 10\ndisplay_topics(lda, X_text_c_feature_names, no_top_words)\n\n\n\n\n\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nvec = TfidfVectorizer()\nvec.fit(df.texts.values)\nfeatures = vec.transform(df.texts.values)\nrandom_state = 0 \ncls = MiniBatchKMeans(n_clusters=10, random_state=random_state)\ncls.fit(features)\n# predict cluster labels for new dataset\n\ncls.predict(features)\n\n# to get cluster labels for the dataset used while\n# training the model (used for models that does not\n# support prediction on new dataset).\nlabels = cls.labels_\n\n# reduce the features to 2D\npca = PCA(n_components=2, random_state=random_state)\nreduced_features = pca.fit_transform(features.toarray())\n\n# reduce the cluster centers to 2D\n\nreduced_cluster_centers = pca.transform(cls.cluster_centers_)\n\nplt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.predict(features))\nplt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')\n\nfrom sklearn.metrics import homogeneity_score\nhomogeneity_score(df.data_labels, cls.predict(features))\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(features, labels=cls.predict(features))\n\"\"\"","e22fd1c3":"*THE graphs are not good ! I am working on that !! "}}