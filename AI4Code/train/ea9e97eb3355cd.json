{"cell_type":{"893dd581":"code","0d95cd61":"code","ad960d4a":"code","16725909":"code","68b8719b":"code","fc0eb87a":"code","f828ca7e":"code","b3e5d205":"code","b5cfbc6a":"code","f431a979":"code","62eef425":"code","fbc3a174":"code","b6b67230":"code","a6cad486":"code","99940578":"markdown","14473b1d":"markdown","0f4e5cf1":"markdown","c5205d8c":"markdown","024b9850":"markdown","9de09535":"markdown","0381193a":"markdown","b080ddde":"markdown","fc3f272f":"markdown","98e53680":"markdown","847a3861":"markdown"},"source":{"893dd581":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d95cd61":"import re\nimport string\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, backend as K, optimizers\n\nfrom nltk.corpus import stopwords\n\neng_stopwords = stopwords.words('english')\neng_stopwords += [word.title() for word in eng_stopwords]\nno_punct_table = str.maketrans(dict.fromkeys(string.punctuation)) # translation table to remove punctuation\n\n%matplotlib inline","ad960d4a":"def preprocess(x):\n    x = x.replace(\"<br \/>\", '')\n    x = x.translate(no_punct_table)\n    x = ' '.join(list(filter(lambda y: y not in eng_stopwords, x.split(' '))))\n    x = re.sub(r'[A-Z][a-z]+', \"<name>\", x)\n    return x\n\n\ndef make_vocab(column):\n    vocab_ = set()\n    max_len_ = 0\n    for sent in column:\n        vocab_.update(sent.split(' '))\n        max_len_ = max(max_len_, len(sent.split(' ')))\n    \n    vocab_.update(['<PAD>', '<UNK>']) # <PAD> and <UNK> tokens for padding and unknown words found in sentences\n    \n    return sorted(vocab_), max_len_\n\n\ndef pad_index_closure(mapping, ref_len):\n    def inner(x):\n        new_x = [mapping['<PAD>']] * (ref_len - len(x.split(' ')))\n        \n        for word in x.split(' '):\n            try:\n                new_word = mapping[word]\n            except:\n                new_word = mapping['<UNK>']\n            \n            new_x.append(new_word)\n        \n        return new_x\n    \n    return inner\n\n\ndef fix_array(arr):\n    x_ = []\n    for row in arr:\n        x_.append(row)\n\n    return np.array(x_)","16725909":"df = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv', header=0)\ndf.head()","68b8719b":"df.sentiment = df.sentiment.apply(lambda x: 1 if x == 'positive' else 0)\ndf.head()","fc0eb87a":"print(\"Before preprocessing:\\n\")\nprint(df.review[1])\nprint()\nprint(\"=\" * 140)\ndf[\"review_preprocessed\"] = df.review.apply(preprocess)\n\nprint(\"\\nAfter preprocessing:\\n\")\nprint(df.review[1])","f828ca7e":"vocab, max_len = make_vocab(df.review_preprocessed)\n\nvocab_itos = dict(enumerate(vocab))\nvocab_stoi = {val: key for key, val in vocab_itos.items()}","b3e5d205":"df[\"review_indexed\"] = df.review_preprocessed.apply(pad_index_closure(vocab_stoi, max_len))\ndf.head()","b5cfbc6a":"x = df.review_indexed.values\ny = df.sentiment.values\n\n# turn x into numpy array\nx = fix_array(x)","f431a979":"def get_model(input_length, embed_input_dim, embed_dim):\n    K.clear_session()\n    X_input =  layers.Input(shape=(input_length,))\n    \n    # An embedding layer that returns `embed_dim` sized vectors for each word\n    X = layers.Embedding(input_dim=embed_input_dim, output_dim=embed_dim, input_length=input_length)(X_input)\n    \n    # 1D Conv layer to compute local features for each word\n    X = layers.Conv1D(filters=10, kernel_size=3)(X)\n    \n    # LSTM layers to build a context vector of the input sentence\n    X = layers.LSTM(X.shape[-1] \/\/ 2, return_sequences=True)(X)\n    X = layers.LSTM(X.shape[-1] \/\/ 2)(X)\n    \n    # output Dense layer used for classification\n    X = layers.Dense(1, activation='sigmoid')(X)\n    \n    # make the Model instance and return it\n    return models.Model(inputs=X_input, outputs=X)","62eef425":"# make an instance of the model\nmodel = get_model(x.shape[-1], len(vocab), 25)\n\n# compile the model with Adam optimizer and Binary Cross Entropy loss with Accuracy metric\nmodel.compile(optimizer=optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x, y, epochs=12, batch_size=8, validation_split=0.2).history\n\n# Save the model in an h5 file\nmodel.save(\"imdb_analyse_sentiment.h5\")","fbc3a174":"nrows = 1\nncols = 2\nfig_size = plt.rcParams['figure.figsize']\n\nfig, ax = plt.subplots(nrows, ncols, figsize=(fig_size[0] * ncols, fig_size[1] * nrows))\n\n_ = ax[0].plot(range(len(history['loss'])), history['loss'], label='Training')\nif 'val_loss' in history.keys():\n    _ = ax[0].plot(range(len(history['val_loss'])), history['val_loss'], label='Validation')\n    _ = ax[0].legend()\n_ = ax[0].set_title('Loss Curve')\n\n_ = ax[1].plot(range(len(history['accuracy'])), history['accuracy'], label='Training')\nif 'val_accuracy' in history.keys():\n    _ = ax[1].plot(range(len(history['val_accuracy'])), history['val_accuracy'], label='Validation')\n    _ = ax[1].legend()\n_ = ax[1].set_title('Accuracy Curve')","b6b67230":"import random\n\nNUM_EXAMPLES = int(0.1 * df.shape[0])\nsampled_indices = random.sample(range(df.shape[0]), NUM_EXAMPLES)\n\n# I have already preprocessed the text and converted them into sequences of integers and put them in a column called `review_indexed`.\n# So, I'll take the examples from there.\n\n# sampling the test examples and the ground truths\nsampled_examples = fix_array(df.loc[sampled_indices, \"review_indexed\"].values)\ny_true = df.loc[sampled_indices, \"sentiment\"].values\n\n# Feeding the test examples to the models\ny_pred = np.squeeze(model.predict(sampled_examples))\n\n\nshowing_indices = random.sample(range(NUM_EXAMPLES), 3)\nfor index in showing_indices:\n    print(f\"Review:\\n{df.loc[index, 'review']}\\n\")\n    print(f\"Ground Truth: {'positive' if y_true[index] else 'negative'}\")\n    print(f\"Predicted {'positive' if y_pred[index] >= 0.5 else 'negative'} with {(y_pred[index] if y_pred[index] >= 0.5 else 1 - y_pred[index]) * 100:.2f}% confidence\\n\")\n    print(\"=\" * 140)\n    print()\n\ny_pred = (y_pred >= 0.5).astype(int)","a6cad486":"import seaborn as sns\nfrom sklearn import metrics\n\naccuracy = sum(y_true == y_pred) \/ NUM_EXAMPLES\nconf_matrix = metrics.confusion_matrix(y_true, y_pred)\nprecision = metrics.precision_score(y_true, y_pred)\nrecall = metrics.recall_score(y_true, y_pred)\nf1_score = metrics.f1_score(y_true, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1_score:.2f}\")\n\n_ = plt.figure(figsize=(fig_size[0] * 1.5, fig_size[1] * 1.5))\nax = sns.heatmap(conf_matrix, annot=True, fmt='d')\n_ = ax.set_xticklabels([\"negative\", \"positive\"], fontsize=13)\n_ = ax.set_yticklabels([\"negative\", \"positive\"], fontsize=13)\n_ = ax.set_xlabel(\"Predicted Labels\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Labels\", fontsize=15)","99940578":"### Preprocessing\n\n* Remove `<br \/>` tokens that are present in the text.\n* Remove punctuation from the text.\n* Remove stopwords.\n* Substitute names of people with a special `<name>` token.","14473b1d":"### Building the vocabulary of the dataset.\nIn the below cell, `vocab` is just a set.<br>\n\nSo, I will make a dictionary that will map each unique token in vocab to an integer.<br>\n`vocab_itos` maps given integer to a word<br>\n`vocab_stoi` maps given word to an integer","0f4e5cf1":"### Testing the model with randomly sampled examples from the dataset","c5205d8c":"### Relabelling sentiment column\nReplacing positive with 1 and negative with 0","024b9850":"## Get input and output arrays for the model","9de09535":"### Converting the sequence of words to a sequence of integers\n* Padding each sentence to make all the training examples of same length.\n* Substituting each word for its corresponding integer","0381193a":"### Evaluating the performance of the model","b080ddde":"# Helper Functions","fc3f272f":"# Plotting the results","98e53680":"# Reading the input dataset","847a3861":"# Build and train the model\n* 1D Convolution\n* LSTM layer\n* Dense network\n* Binary Cross Entropy loss with accuracy metric\n* Adam optimizer"}}