{"cell_type":{"e5a4e2a7":"code","ed764b1a":"code","ea973dda":"code","1032391b":"code","ed889cfd":"code","c906308c":"code","427d652d":"code","6846aab3":"code","5e949269":"code","255f7a06":"code","f87e79ff":"code","4ce31b7f":"markdown","6ac6e42c":"markdown","cf67854a":"markdown","3619a3da":"markdown","2943d128":"markdown","09fd0acd":"markdown","887eb1e4":"markdown","f9267dc5":"markdown","1abf3dc6":"markdown","fee74c34":"markdown","fc675dc3":"markdown","3c276d4c":"markdown"},"source":{"e5a4e2a7":"!curl -O https:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n!rm -r aclImdb\/train\/unsup","ed764b1a":"import os, pathlib, shutil, random\nfrom tensorflow import keras\nbatch_size = 32\nbase_dir = pathlib.Path(\"aclImdb\")\nval_dir = base_dir \/ \"val\"\ntrain_dir = base_dir \/ \"train\"\nfor category in (\"neg\", \"pos\"):\n    os.makedirs(val_dir \/ category)\n    files = os.listdir(train_dir \/ category)\n    random.Random(1337).shuffle(files)\n    num_val_samples = int(0.2 * len(files))\n    val_files = files[-num_val_samples:]\n    for fname in val_files:\n        shutil.move(train_dir \/ category \/ fname,\n                    val_dir \/ category \/ fname)\n\ntrain_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb\/train\", batch_size=batch_size\n)\nval_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb\/val\", batch_size=batch_size\n)\ntest_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb\/test\", batch_size=batch_size\n)\ntext_only_train_ds = train_ds.map(lambda x, y: x)","ea973dda":"from tensorflow.keras import layers\n\nmax_length = 600\nmax_tokens = 20000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)","1032391b":"import tensorflow as tf\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = tf.one_hot(inputs, depth=max_tokens)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()","ed889cfd":"'''\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\nmodel = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n'''","c906308c":"ds = int_val_ds.take(1)\nfor sentence, label in ds:  # example is (image, label)\n  print(sentence.shape, label)","427d652d":"sentence[2].shape","6846aab3":"model.predict(sentence[2])","5e949269":"embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)","255f7a06":"inputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=2, callbacks=callbacks)\nmodel = keras.models.load_model(\"embeddings_bidir_gru.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")","f87e79ff":"model.predict(sentence[2])","4ce31b7f":"**Training a first basic sequence model**","6ac6e42c":"**Downloading the data**","cf67854a":"### Processing words as a sequence: The sequence model approach","3619a3da":"**Preparing integer sequence datasets**","2943d128":"**Instantiating an `Embedding` layer**","09fd0acd":"#### Learning word embeddings with the Embedding layer","887eb1e4":"#### A first practical example","f9267dc5":"#### Understanding word embeddings","1abf3dc6":"This is a companion notebook for the book [Deep Learning with Python, Second Edition](https:\/\/www.manning.com\/books\/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n\n**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n\nThis notebook was generated for TensorFlow 2.6.","fee74c34":"**Preparing the data**","fc675dc3":"**A sequence model built on one-hot encoded vector sequences**","3c276d4c":"**Model that uses an `Embedding` layer trained from scratch**"}}