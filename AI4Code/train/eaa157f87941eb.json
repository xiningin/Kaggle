{"cell_type":{"3710f7a2":"code","78c135bd":"code","35fad232":"code","31c7d5d4":"code","a9b09647":"code","69eb8248":"code","48fb51ab":"code","e4b23304":"code","a37deea4":"code","f443b821":"code","5bb688cd":"code","f5f08361":"code","52dac7e7":"code","93b9118f":"code","7cea670d":"code","2407e90d":"code","1731ec07":"code","721621a2":"code","9cb48e72":"code","35db7e26":"code","897784a9":"code","e3fc4e78":"code","78d7e5d6":"code","1b7c3ba5":"code","a82f1ad3":"code","689914c5":"code","47528aa3":"code","6f0de34e":"code","1e0cc8ac":"code","e78e7158":"code","6ae0fc9b":"code","f7f881cb":"code","c6e4bf02":"code","0ad37d03":"code","178a7323":"code","56759391":"code","72814114":"code","89534043":"code","157d2e00":"code","91c8e32a":"code","fa981f34":"code","e82aaf32":"code","08a24551":"code","f7d3e204":"code","89a99582":"code","04d4d6ad":"code","fc932894":"code","c7748d7d":"code","331f47ea":"code","2e6fe930":"code","764b8a2e":"code","edffbc5e":"code","6cd10741":"code","50fbc8ee":"code","0b1a4515":"code","f0afc239":"code","290be8dd":"code","43933a45":"code","dec89535":"code","638b031e":"code","5f1a1fee":"code","982c1efc":"code","26a028d3":"code","c2b1be9a":"code","88acee0f":"code","c2c86bdd":"code","b4ed8810":"code","942e94dd":"code","a8cf8286":"code","d359efe6":"code","5fea2e5a":"code","7a494758":"code","a5530bc1":"code","09fdc5ea":"code","2e6696bd":"code","042fbefd":"code","a0290899":"code","90fc6927":"code","a077a993":"code","16a957af":"code","cde4cdde":"code","acf4e21e":"code","ff2c66cc":"code","c1014677":"markdown","5963c653":"markdown","ecae83f9":"markdown","0e1efe66":"markdown","e9464352":"markdown","afda421c":"markdown","de97b4aa":"markdown","c610bcc6":"markdown","67a89b57":"markdown","a0eaf475":"markdown","73840e79":"markdown","ec5aade4":"markdown","ae96cb23":"markdown","8388df1f":"markdown","83569a32":"markdown"},"source":{"3710f7a2":"import re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk # for text manipulation\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","78c135bd":"df = pd.read_csv(\"..\/input\/TwitterSentimentAnalysis.csv\")","35fad232":"df.shape","31c7d5d4":"df.head()","a9b09647":"df['label'].value_counts()","69eb8248":"df.iloc[0:2,2]","48fb51ab":"df.rename(columns={'tweet_text':'tweet'},inplace=True)","e4b23304":"##  Basic feature extraction using text data \n# Number of words\n# Number of characters\n# Average word length\n# Number of stopwords\n# Number of special characters\n# Number of numerics\n# Number of uppercase words","a37deea4":"df.info()","f443b821":"# Number of words\ndf['Word_Count']=df['tweet'].apply(lambda x:len(str(x).split(\" \")))","5bb688cd":"df.head()     ","f5f08361":"# Number of characters (means calculating the length of the tweet.)\ndf['char_count']=df.tweet.str.len()","52dac7e7":"df.head()  ","93b9118f":"# Average word length\ndef avg_word(sentence):\n    words=sentence.split()\n    return (sum(len(word) for word in words)\/len(words))","7cea670d":"df['Avg_word_length']=df.tweet.apply(lambda x:avg_word(x))","2407e90d":"df.head()","1731ec07":"# Number of stopwords\n# Stopwords are individual words such as and, is , am ,are etc.., it is available under nltk.corpus package\nfrom nltk.corpus import stopwords\nstop=stopwords.words(\"english\")\ndf['stop_word_count']=df['tweet'].apply(lambda x:len([x for x in x.split() if x in stop]))","721621a2":"df.head(3)","9cb48e72":"# Number of special characters (say number of HashTags)\n# Number of special characters\n# One more interesting feature which we can extract from a tweet is calculating the number of hashtags or mentions present in it. This also helps in extracting extra information from our text data.\n# Here, we make use of the \u2018starts with\u2019 function because hashtags (or mentions) always appear at the beginning of a word.","35db7e26":"df['HashTags']=df.tweet.apply(lambda x:len([x for x in x.split() if x.startswith('#')]))","897784a9":"df.head(3)","e3fc4e78":"#Number of numerics\ndf['numerics']=df.tweet.apply(lambda x:len([x for x in x.split() if x.isdigit()]))","78d7e5d6":"df.head(2)","1b7c3ba5":"# Number of Uppercase words\ndf['upperword']=df.tweet.apply(lambda x:len([x for x in x.split() if x.isupper()]))","a82f1ad3":"df.head()","689914c5":"# Basic Text Pre-processing of text data \n# Lower casing\n# Punctuation removal\n# Stopwords removal\n# Frequent words removal\n# Rare words removal\n# Spelling correction\n# Tokenization\n# Stemming\n# Lemmatization","47528aa3":"# So far, we have learned how to extract basic features from text data.\n# Before diving into text and feature extraction, our first step should be cleaning the data in order to obtain better features.\n# We will achieve this by doing some of the basic pre-processing steps on our training data.","6f0de34e":"df['tweet']=df.tweet.apply(lambda x:\" \".join(x.lower() for x in x.split()))","1e0cc8ac":"df['tweet'].head()","e78e7158":"df['tweet']=df.tweet.str.replace('[^\\w\\s]','')","6ae0fc9b":"from nltk.corpus import stopwords\nstop=stopwords.words(\"english\")","f7f881cb":"df['tweet']=df.tweet.apply(lambda x:\" \".join(x for x in x.split() if x not in stop))","c6e4bf02":"df.head()","0ad37d03":"freq = pd.Series(' '.join(df['tweet']).split()).value_counts()[:10]\nfreq","178a7323":"freq = list(freq.index)\nfreq","56759391":"df.head(3)","72814114":"df['tweet']=df.tweet.apply(lambda x:\" \".join(x for x in x.split() if x not in freq))","89534043":"df.head(3)","157d2e00":"freq = pd.Series(\" \".join(df['tweet']).split()).value_counts()[-10:]","91c8e32a":"freq","fa981f34":"freq = list(freq.index)\ndf['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ndf['tweet'].head()","e82aaf32":"# In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple\n# copies of words. For example, \u201cAnalytics\u201d and \u201canalytcs\u201d will be treated as different words even if they are used in the\n# same sense.\n# To achieve this we will use the textblob library","08a24551":"from textblob import TextBlob","f7d3e204":"from textblob import TextBlob\ndf['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))","89a99582":"df['tokenized_tweet'] = df['tweet'].apply(lambda x: x.split()) ","04d4d6ad":"df.head()","fc932894":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\ndf['stemmed_text'] = df['tweet'].apply(lambda x: [stemmer.stem(i) for i in x]) ","c7748d7d":"df.iloc[:,5:15].head()","331f47ea":"from nltk.stem.wordnet import WordNetLemmatizer\nlem=WordNetLemmatizer()\n\ndf['Lemmatized_word'] = df.tokenized_tweet.apply(lambda x: [lem.lemmatize(i) for i in x]) ","2e6fe930":"df.iloc[:,10:16].head()","764b8a2e":"# N-grams\n# N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams.\n# Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used. Unigrams do not usually contain\n# as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that \n# they capture the language structure, like what letter or word is likely to follow the given one. The \n# longer the n-gram (the higher the n), the more context you have to work with. Optimum length really \n# depends on the application \u2013 if your n-grams are too short, you may fail to capture important differences. \n# On the other hand, if they are too long, you may fail to capture the \u201cgeneral knowledge\u201d and only stick to\n# particular cases.","edffbc5e":"def generate_ngrams(text, n):\n    words = text.split()\n    output = []  \n    for i in range(len(words)-n+1):\n        output.append(words[i:i+n])\n    return output","6cd10741":"generate_ngrams('this is a sample text', 2)","50fbc8ee":"# This is in case of our dataframe\nTextBlob(df['tweet'][0]).ngrams(2)","0b1a4515":"df.info()","f0afc239":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt","290be8dd":"all_words = ' '.join(text for text in df['tweet'].astype(str))\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=2500,\n                      height=2000\n                     ).generate(all_words)\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","43933a45":"all_words = ' '.join(text for text in df['tweet'].astype(str))\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=2500,\n                      height=2000\n                     ).generate(all_words)\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","dec89535":"all_words = ' '.join(text for text in df['tokenized_tweet'].astype(str))\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=2500,\n                      height=2000\n                     ).generate(all_words)\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","638b031e":"all_words = ' '.join(text for text in df['stemmed_text'].astype(str))\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=2500,\n                      height=2000\n                     ).generate(all_words)\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","5f1a1fee":"all_words = ' '.join(text for text in df['Lemmatized_word'].astype(str))\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","982c1efc":"df.shape","26a028d3":"df.head(1)","c2b1be9a":"len(df['Lemmatized_word'])","88acee0f":"df1=df[['id','label','tweet','Lemmatized_word']]","c2c86bdd":"df1['label'].value_counts()","b4ed8810":"df1.head()","942e94dd":"df1.values","a8cf8286":"words_lem=[]\nfor i in df['Lemmatized_word'].astype(str):\n    WNlemma = nltk.WordNetLemmatizer()\n    words_lem.append(WNlemma.lemmatize(i))","d359efe6":"x=[]\nfor j in words_lem:\n    for i in df1.tweet:\n        if j in i:\n            x.append(1)\n        else:\n            x.append(0)","5fea2e5a":"import numpy as np\nx=np.array(x)","7a494758":"len(x)","a5530bc1":"y=df['label']\nx=df['tweet']","09fdc5ea":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=101)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","2e6696bd":"X_train.head(2)","042fbefd":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)\n# transform the documents in the training data to a document-term matrix\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)","a0290899":"X_train_vectorized","90fc6927":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","a077a993":"# kNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=2)\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","16a957af":"# Naive Bayes\nfrom sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","cde4cdde":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","acf4e21e":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","ff2c66cc":"# Bagging Classifier\nfrom sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","c1014677":"# Spelling correction","5963c653":"#### <b> Stemming<\/b> : Stemming refers to the removal of suffices, like \u201cing\u201d, \u201cly\u201d, \u201cs\u201d, etc. by a simple rule-based approach. \n#### For this purpose, we will use PorterStemmer from the NLTK library.","ecae83f9":"Bag of Words - Count Vectorization","0e1efe66":"# 1. Lower casing","e9464352":"Count Vectorization - In Python Library","afda421c":"# 3.  stopword removal (remove am,is,are, the etc from text)","de97b4aa":"#### Lemmatization : Lemmatization is a more effective option than stemming because it converts the word into its root word,\n#### rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the\n#### root word. Therefore, we usually prefer using lemmatization over stemming.","c610bcc6":"### Tokenization : Tokenisation refers to dividing the text into sequence of words or sentences","67a89b57":"# 2. punctuation removal\n#### The next step is to remove punctuation, as it doesn\u2019t add any extra information while treating text data. \n#### Therefore removing all instances of it will help us reduce the size of the training data.","a0eaf475":"# WordCloud","73840e79":"Logistic Regression","ec5aade4":"## Rare words removal","ae96cb23":"# N-grams","8388df1f":"# Manual Way of Count Vectorization","83569a32":"# Common word removal : \n####  these words as their presence will not of any use in classification of our text data."}}