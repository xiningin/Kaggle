{"cell_type":{"9bcc32f5":"code","bae9aed0":"code","62b15987":"code","d4aa0f2a":"code","15201007":"code","7e4718a3":"code","d63ce8f9":"code","a2cd8fc7":"code","331d899a":"code","b4104137":"code","13c76229":"code","8443b4ee":"code","429569d1":"code","4d550219":"code","35055aca":"code","eda72f77":"code","b6b76fea":"code","2fa4d121":"code","2a55ba29":"code","e37dbf75":"code","4e94d501":"code","7bd6260c":"code","f430c57e":"code","6ae7ae1a":"code","1139c948":"code","85b6c446":"code","c766dd36":"code","6ae8545d":"code","58a87cae":"code","1063ffdb":"code","ddd1dc76":"code","c3947674":"code","54369548":"code","f96833a7":"code","edeffba3":"code","89af2d51":"code","ceaa10a5":"code","833517ca":"code","280655c4":"code","91bf4dd5":"code","7f690434":"code","2acb36fc":"code","ef5255ff":"code","f7634ee3":"code","b1e62e90":"code","e4c70d55":"code","6e835dac":"code","b87bfd4f":"code","e09dd5d1":"code","264c51f2":"code","d876bd2e":"code","936abaeb":"code","fb312bab":"code","3298713c":"code","b410ce10":"code","27428069":"code","9c09917f":"code","f14d0789":"code","e61003db":"code","d8a3aa86":"code","2180d5f0":"code","f391299e":"code","c2a2acae":"markdown","4849f16b":"markdown","adb4c4a6":"markdown","3cfd18d7":"markdown","7330618a":"markdown","f68c15eb":"markdown","fe7937a3":"markdown","da8564f5":"markdown","358e2645":"markdown","580e3ff3":"markdown","cf7dfb3b":"markdown","30f60785":"markdown","b4e6ffca":"markdown","24b7a4f1":"markdown","c8a03a7f":"markdown","63296581":"markdown","543afa40":"markdown","d7e61706":"markdown","aee4473e":"markdown","fcd8f2c0":"markdown","417ab319":"markdown","870a9493":"markdown","6f6489ad":"markdown","a101718e":"markdown","3e472b12":"markdown","a19bf7e9":"markdown","90846b35":"markdown","5eff1081":"markdown","dae23529":"markdown","e5bfc8e4":"markdown","d8a8e21d":"markdown","8106ebd4":"markdown","35068c8f":"markdown","550ddc7c":"markdown","7aa68062":"markdown","8e5f256e":"markdown","5f4a1f53":"markdown","6acd9260":"markdown","b203565f":"markdown","437db87f":"markdown","92641a4c":"markdown"},"source":{"9bcc32f5":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport math, time, datetime\n# from math import sqrt\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\n\nfrom matplotlib import pyplot as plt\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.model_selection import (train_test_split, cross_val_predict, KFold, cross_val_score)\nfrom sklearn.metrics import (mean_squared_error, r2_score, classification_report,confusion_matrix,\n                             accuracy_score, roc_auc_score, roc_auc_score, roc_curve, precision_recall_curve,\n                             auc, log_loss)\n\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.feature_selection import (mutual_info_classif, SelectKBest, chi2, SequentialFeatureSelector)\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\n\n# from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict\n# from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, roc_auc_score, \n#                              precision_recall_curve, auc, log_loss, accuracy_score, f1_score)\n# from sklearn.feature_selection import (mutual_info_classif, SelectKBest, chi2, RFE, RFECV)\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n","bae9aed0":"df_first = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test_first = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_gender_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_first.head()","62b15987":"fig, ax = plt.subplots(figsize = (12, 2))\nax.barh(df_first['Survived'].unique(), df_first['Survived'].value_counts(), align='center', color=['red', 'green'])\nax.text(530, 0, df_first['Survived'].value_counts()[0], ha='center', va='center', color='w', size=20)\nax.text(320, 1, df_first['Survived'].value_counts()[1], ha='center', va='center', color='w', size=20)\nax.set_yticks(df_first['Survived'].unique())\nax.set_yticklabels(df_first['Survived'].unique())\nax.invert_yaxis()\nax.set_ylabel('Survived')\nax.set_title('How many people survived?')\n\nplt.show()","d4aa0f2a":"df_first.info()\nprint(\"----------------------------\")\ndf_test_first.info()","15201007":"## plot graphic of missing values\nmsno.matrix(df_first, figsize=(12, 6))","7e4718a3":"# plot graphic of missing values\nmsno.matrix(df_test_first, figsize=(12, 6))","d63ce8f9":"# drop unnecessary columns, which won't be used in analysis and prediction\ndf = df_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)\ndf_test = df_test_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","a2cd8fc7":"# Checking for missing values in train data\ndf.isnull().sum()","331d899a":"# Checking for missing values in test data\ndf_test.isnull().sum()","b4104137":"# getting the Pclass of the Fare missing value\nPclass_fare_mis = df_test[df_test['Fare'].isnull()]['Pclass'].values[0]\nprint(Pclass_fare_mis)\nprint(\"----------------------------\")\n# getting the mean of Fare for the Pclass 3\ndf_Pclass_mean = df_test.groupby(['Pclass']).mean().loc[Pclass_fare_mis,'Fare']\n\n# fill NaN values in Fare column with mean\ndf_test.loc[152, 'Fare'] = df_Pclass_mean\n\ndf_test.isnull().sum()","13c76229":"# getting the Pclass of the Embarked missing value\nPclass_embarked_mis = df[df['Embarked'].isnull()]['Pclass'].values\nprint(Pclass_embarked_mis)\nprint(\"----------------------------\")\n# distribution in Embarked by their Pclass\nprint(df[df['Pclass'] == 1]['Embarked'].value_counts())\n\n# for Pclass = 1, the most occurring values are S and C. \n# Therefore, one is filled as 'S', and the other as 'C'. \ndf.loc[61, 'Embarked'] = 'S'\ndf.loc[829, 'Embarked'] = 'C'\n\ndf.isnull().sum()","8443b4ee":"mis_val_female = df[df['Sex'] == 'female']['Age'].isna().sum()\nmis_val_male = df[df['Sex'] == 'male']['Age'].isna().sum()\n\nprint(f'Missing values in Age for female: {mis_val_female}')\nprint(f'Missing values in Age for male: {mis_val_male}')","429569d1":"def generate_random_numbers(df):\n    nan_count = df.isna().sum()\n    \n    ax = df.hist(bins=10, density=True, stacked=True, color='teal', alpha=0.6)\n    ax1 = df.plot(kind='kde', color='teal')\n    \n    # mean age\n    df_mean = df.mean(skipna=True)\n    # median age\n    df_median = df.median(skipna=True)\n    # std age\n    df_std = df.std(skipna=True)\n        \n    # getting density peak values\n    density = stats.gaussian_kde(df.dropna())\n    xs = np.linspace(df.min(),df.max(),200)\n    ys = density(xs)\n    index = np.argmax(ys)\n    max_y = ys[index]\n    max_x = xs[index]\n    \n    # density peak plot\n    ax.axvline(max_x, 0, 1, color='blue', label='peak(x): {:.2f}'.format(max_x))\n    \n    # once the median is closer than the mean to the x value of the density peak, median will be used.\n    # otherwise, the mean will be used to fill in the missing values by generated random numbers.\n    if abs(max_x - df_mean) <=  abs(max_x - df_median):\n        rand_numbers = np.random.randint(df_mean - df_std, df_mean + df_std, \n                                   size = nan_count)\n        ax.axvline(df_mean, 0, 1, color='red', label='mean: {:.2f}'.format(df_mean))\n        # we will generate random numbers using the mean {between (mean - std) & (mean + std)}\n        ax.axvspan(df_mean - df_std, df_mean + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n    else: \n        rand_numbers = np.random.randint(df_median - df_std, df_median + df_std, \n                                   size = nan_count)\n        ax.axvline(df_median, 0, 1, color='red', label='median: {:.2f}'.format(df_median))\n        \n        # we will generate random numbers using the median {between (median - std) & (median + std)}\n        ax.axvspan(df_median - df_std, df_median + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n\n    ax.set(xlabel='Age')\n    ax.legend(title='Female')\n    plt.show()\n\n    return (rand_numbers)","4d550219":"df_female_age = df[df['Sex'] == 'female']['Age']\ndf_female_rand_numbers = generate_random_numbers(df_female_age)\n\ndf_female_null = (df[df['Age'].isnull()]['Sex'] == 'female')\ndf_female_ind_null = df_female_null[df_female_null].index\ndf.loc[df_female_ind_null, 'Age'] = df_female_rand_numbers\n\nprint(df.isnull().sum())","35055aca":"df_male_age = df[df['Sex'] == 'male']['Age']\ndf_male_rand_numbers = generate_random_numbers(df_male_age)\n\ndf_male_null = (df[df['Age'].isnull()]['Sex'] == 'male')\ndf_male_ind_null = df_male_null[df_male_null].index\ndf.loc[df_male_ind_null, 'Age'] = df_male_rand_numbers\n\nprint(df.isnull().sum())","eda72f77":"df_test_female_age = df_test[df_test['Sex'] == 'female']['Age']\ndf_test_female_rand_numbers = generate_random_numbers(df_test_female_age)\n\ndf_test_female_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'female')\ndf_test_female_ind_null = df_test_female_null[df_test_female_null].index\ndf_test.loc[df_test_female_ind_null, 'Age'] = df_test_female_rand_numbers\n\nprint(df_test.isnull().sum())","b6b76fea":"df_test_male_age = df_test[df_test['Sex'] == 'male']['Age']\ndf_test_male_rand_numbers = generate_random_numbers(df_test_male_age)\n\ndf_test_male_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'male')\ndf_test_male_ind_null = df_test_male_null[df_test_male_null].index\ndf_test.loc[df_test_male_ind_null, 'Age'] = df_test_male_rand_numbers\n\nprint(df_test.isnull().sum())","2fa4d121":"## Create categorical variable for traveling alone\ndf['TravelAlone']=np.where((df[\"SibSp\"]+df[\"Parch\"])>0, 0, 1)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.head()","2a55ba29":"## Create categorical variable for traveling alone\ndf_test['TravelAlone']=np.where((df_test[\"SibSp\"]+df_test[\"Parch\"])>0, 0, 1)\ndf_test.drop('SibSp', axis=1, inplace=True)\ndf_test.drop('Parch', axis=1, inplace=True)\ndf_test.head()","e37dbf75":"col_order = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone', 'Survived']\ndf=df[col_order]\ndf.head()","4e94d501":"for col in df:\n    unique_vals = np.unique(df[col])\n    nr_values = len(unique_vals)\n    if nr_values < 10:\n        print(f'The number of values for feature {col} :{nr_values} -- {unique_vals}')\n    else:\n        print(f'The number of values for feature {col} :{nr_values}')","7bd6260c":"axes = pd.plotting.scatter_matrix(df, alpha=0.7, figsize=(14, 8), diagonal='kde')\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(0)\n    ax.yaxis.label.set_rotation(90)\n    ax.yaxis.label.set_ha('right')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.gcf().subplots_adjust(wspace=0, hspace=0)\nplt.show()","f430c57e":"def subplot_graph(df, features):\n    f_count = len(features)\n    cols = f_count\n    if f_count < 4:\n        rows = 1\n    else:\n        rows = math.ceil(f_count\/3)\n        cols = 3\n                \n    # Set up the matplotlib figure\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n        \n    sub_rows = 0\n    sub_cols = 0    \n    for f in features:\n        if sub_cols > cols-1:\n            sub_cols = 0\n            sub_rows += 1\n            \n        unique_vals = len(np.unique(df[f]))\n        if unique_vals < 10:\n            df_sub = df.groupby(df.columns[-1])[f].value_counts().sort_index(level=0)\n            x_no = list(df_sub.xs(0, level=0).index)\n            y_no = list(df_sub.xs(0, level=0).values)\n            x_yes = list(df_sub.xs(1, level=0).index)\n            y_yes = list(df_sub.xs(1, level=0).values)\n            list_no = list(zip(x_no, y_no))\n            list_yes = list(zip(x_yes, y_yes))\n            df_no = pd.DataFrame(list_no)\n            df_yes = pd.DataFrame(list_yes)\n\n            df_plot = pd.merge(df_no, df_yes, on=df_no.columns[0], how=\"outer\")\n            df_plot = df_plot.fillna(0)\n            try:\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                   label='1', color='steelblue')\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n            except:\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                             label='1', color='steelblue')  \n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        else:    \n            df_3 = df[df[df.columns[-1]] == 0][f]\n            df_4 = df[df[df.columns[-1]] == 1][f]\n            try:\n                df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_xlim(xmin=0)\n            except:\n                df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n            \n        sub_cols += 1\n        \n    fig.tight_layout()\n    fig.show()\n\nfeatures = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone']\nsubplot_graph(df, features)","6ae7ae1a":"f_count = len(features)\ncols = f_count\nif f_count < 4:\n    rows = 1\nelse:\n    rows = math.ceil(f_count\/3)\n    cols = 3\n    \n# Set up the matplotlib figure\nfig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n\n\nsub_rows = 0\nsub_cols = 0    \nfor f in features:\n    if sub_cols > cols-1:\n        sub_cols = 0\n        sub_rows += 1\n        \n    \n    unique_vals = len(np.unique(df[f]))\n    if unique_vals < 10:\n        df_yy = pd.DataFrame()\n        df1 = df.groupby([f])[df.columns[-1]].value_counts().sort_index(level=0)\n        df_uniq = df1.index.levels[0]\n        f_uniqs = []\n        sur_rates = []\n        for uniq in df_uniq:\n            try:\n                df2 = df1.xs(uniq, level=0)\n                df_sum = df2.sum()\n                df_lev_1_each = df2[1]\n                sur_rate = round((df_lev_1_each \/ df_sum)*100, 1)\n                sur_rates.append(sur_rate)\n                f_uniqs.append(uniq)\n            except:\n                pass\n\n        df_xx = pd.DataFrame(sur_rates, index=f_uniqs, columns=[f])\n        df_yy = pd.concat([df_yy, df_xx], axis=1)\n        \n        df_sub = df_yy[f].dropna()\n#         print(df_sub)\n\n        try:\n            axes[sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        except:\n            axes[sub_rows, sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_rows, sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n    else:    \n        df_3 = df[df[df.columns[-1]] == 0][f]\n        df_4 = df[df[df.columns[-1]] == 1][f]\n        try:\n            df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].legend(title=df.columns[-1])\n            axes[sub_cols].set_xlim(xmin=0)\n        except:\n            df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n            axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n        \n    sub_cols += 1\n    \nfig.tight_layout()\nfig.show()","1139c948":"def create_heatmap(hm, figsize=(7, 6)):\n    fig, ax = plt.subplots(figsize=figsize)\n\n    im = ax.imshow(hm, \n    #                vmin=0, vmax=10, \n                   cmap='viridis', aspect='auto')\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(hm.columns)))\n    ax.set_yticks(np.arange(len(hm.columns)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(hm.columns)\n    ax.set_yticklabels(hm.columns)\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n    ax.set_xticks(np.arange(hm.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(hm.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(hm.columns)):\n        for j in range(len(hm.columns)):\n            hm_val = round(hm.values[i, j], 2)\n            if hm_val > 0.85:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"black\", size=16)\n            else:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"w\", size=16)\n\n    fig.tight_layout()\n    plt.show()","85b6c446":"hm = df.corr()\ncreate_heatmap(hm)","c766dd36":"X = df[df.columns[:-1]]\ny = df[df.columns[-1]]\n\n# first one\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=15)\n\nX_kaggle = df_test.copy()\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","6ae8545d":"# There is no categorical columns in the dataframe\ncategorical_cols = list(set(X_train.columns) - set(X_train._get_numeric_data().columns))\ncategorical_cols","58a87cae":"numerical_cols = list(df._get_numeric_data().columns)\nnumerical_cols","1063ffdb":"from sklearn.compose import make_column_transformer\n\ncolumn_trans = make_column_transformer((OneHotEncoder(), categorical_cols),\n                                      remainder='passthrough')\nX_train_trans = column_trans.fit_transform(X_train)\nX_test_trans = column_trans.fit_transform(X_test)\nX_kaggle_trans = column_trans.fit_transform(X_kaggle)\n\ncols = []\nfor i in column_trans.transformers_[0][2]:\n    cols_enc = sorted(X_train[i].unique())\n    for col_enc in cols_enc:\n        col_name = i + '_' + col_enc\n        cols.append(col_name)\n        \nfor i in column_trans.transformers_[1][2]:\n    col_name = X_train.columns[i]\n    cols.append(col_name)\n    \nX_train_enc = pd.DataFrame(X_train_trans, columns=cols)\nX_test_enc = pd.DataFrame(X_test_trans, columns=cols)\nX_kaggle_enc = pd.DataFrame(X_kaggle_trans, columns=cols)\n\n\nprint(X_train_enc.shape)\nprint(X_test_enc.shape)\nprint(X_kaggle_enc.shape)","ddd1dc76":"X_train_enc.head(3)","c3947674":"X_test_enc.head(3)","54369548":"X_kaggle_enc.head(3)","f96833a7":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_train_enc)\nX_train_enc.columns[var_thres.get_support()]\n\nconstant_columns = [column for column in X_train_enc.columns\n                    if column not in X_train_enc.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","edeffba3":"acc_list = []\nfor k in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn_mean = cross_val_score(knn, X_train_trans, y_train, cv=10, scoring='accuracy').mean()\n    acc_list.append(knn_mean)\n    \nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),acc_list,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nplt.show()","89af2d51":"k_max = acc_list.index(max(acc_list))\nprint(k_max)\nknn = KNeighborsClassifier(n_neighbors=k_max)\npipe = make_pipeline(column_trans, knn)\nknn_acc = cross_val_score(pipe, X_train, y_train, cv=10, scoring='accuracy').mean()\nprint('the best accuracy:{:.1%}'.format(knn_acc))","ceaa10a5":"pipe.fit(X_train, y_train)\ny_pred_test = pipe.predict(X_test)\n\nprint(\"Test Accuracy: {:.1%}\".format(pipe.score(X_test,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","833517ca":"hm_X_train = X_train_enc.corr()\ncreate_heatmap(hm_X_train, figsize=(10, 6))","280655c4":"#  to select highly correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                \n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","91bf4dd5":"corr_features = correlation(X_train_enc, 0.85)\ncorr_features","7f690434":"X_train_enc = X_train_enc.drop(corr_features, axis=1)\nX_test_enc = X_test_enc.drop(corr_features, axis=1)\nX_kaggle_enc = X_kaggle_enc.drop(corr_features, axis=1)\n\nprint(X_train_enc.shape)\nprint(X_test_enc.shape)\nprint(X_kaggle_enc.shape)","2acb36fc":"# Run a Tree-based estimators (i.e. decision trees & random forests)\ndt = DecisionTreeClassifier(random_state=15, criterion = 'entropy', max_depth = 10)\ndt.fit(X_train_enc, y_train)\n\nfi_col = []\nfi = []\nfor i,column in enumerate(X_train_enc):\n#     print('The feature importance for {} is: {:.2%}'.format(column, dt.feature_importances_[i]))    \n    fi_col.append(column)\n    fi.append(dt.feature_importances_[i])\n\n# Creating a Dataframe\nfi_df = zip(fi_col, fi)\nfi_df = pd.DataFrame(fi_df, columns = ['Feature','Feature Importance'])\n\n# Ordering the data\nfi_df = fi_df.sort_values('Feature Importance', ascending = False).reset_index(drop=True)\nfi_df","ef5255ff":"# determine the mutual information\nmutual_info = mutual_info_classif(X_train_enc.values, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train_enc.columns\nmutual_info.sort_values(ascending=False)","f7634ee3":"def model_evaluation(cols):\n    categorical_cols = list(set(X_train_enc[cols].columns) - set(X_train_enc[cols]._get_numeric_data().columns))\n    column_trans = make_column_transformer((OneHotEncoder(), categorical_cols), remainder='passthrough')\n    \n    k_max = acc_list.index(max(acc_list))\n    knn = KNeighborsClassifier(n_neighbors=k_max)\n    pipe = make_pipeline(column_trans, knn)\n    knn_acc = cross_val_score(pipe, X_train_enc[cols], y_train, cv=10, scoring='accuracy').mean()\n#     print('the best accuracy:{:.1%}'.format(knn_acc))\n    \n    pipe.fit(X_train_enc[cols], y_train)\n    \n    y_pred_train = pipe.predict(X_train_enc[cols])\n    knn_mse = mean_squared_error(y_train, y_pred_train)\n    knn_r2 = r2_score(y_train, y_pred_train)\n#     print(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_train, y_pred_train)))\n#     print(\"r2_score: {:.3}\".format(r2_score(y_train, y_pred_train)))\n#     print('='*10)\n\n    return knn_acc, knn_mse, knn_r2","b1e62e90":"import itertools\n\nstuff = X_train_enc.columns\n\nbest_cols = []\nbest_acc = []\nbest_mse = []\nbest_r2 = []\nfor L in range(0, len(X_train_enc)+1):\n    for subset in itertools.combinations(stuff, L):\n        sub_list = list(subset)\n        if len(sub_list) != 0:\n            knn_acc, knn_mse, knn_r2 = model_evaluation(sub_list)\n            best_cols.append(sub_list)\n            best_acc.append(knn_acc)\n            best_mse.append(knn_mse)\n            best_r2.append(knn_r2)","e4c70d55":"best_zip = zip(best_cols, best_acc, best_mse, best_r2)\ndf_best_acc = pd.DataFrame(best_zip, columns=['columns', 'accuracy', 'mse', 'r2'])\ndf_best_acc = df_best_acc.sort_values('accuracy', ascending = False).reset_index(drop=True)\npd.set_option('max_colwidth', -1)\ndf_best_acc.head(10)","6e835dac":"cols_final = ['Embarked_Q', 'Embarked_S', 'Sex_female', 'Pclass']\n\nX_train_final = X_train_enc[cols_final]\nX_test_final = X_test_enc[cols_final]\nX_kaggle_final = X_kaggle_enc[cols_final]\n\nprint(X_train_final.shape)\nprint(X_test_final.shape)\nprint(X_kaggle_final.shape)","b87bfd4f":"categorical_cols = list(set(X_train_final.columns) - set(X_train_final._get_numeric_data().columns))\ncolumn_trans = make_column_transformer((OneHotEncoder(), categorical_cols), remainder='passthrough')\n\nk_max = acc_list.index(max(acc_list))\nknn = KNeighborsClassifier(n_neighbors=k_max)\npipe = make_pipeline(column_trans, knn)\nknn_acc = cross_val_score(pipe, X_train_final, y_train, cv=10, scoring='accuracy').mean()\nprint('the best accuracy:{:.1%}'.format(knn_acc))\n\npipe.fit(X_train_final, y_train)\n\npred_proba_train = pipe.predict_proba(X_train_final)\ny_pred_train = pipe.predict(X_train_final)\n\nknn_mse = mean_squared_error(y_train, y_pred_train)\nknn_r2 = r2_score(y_train, y_pred_train)\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_train, y_pred_train)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_train, y_pred_train)))\nprint('='*10)","e09dd5d1":"pred_proba_test = pipe.predict_proba(X_test_final)\ny_pred_test = pipe.predict(X_test_final)\nprint(\"Test Accuracy: {:.1%}\".format(pipe.score(X_test_final,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","264c51f2":"def confusion_matrix_func(cm, cm_title):\n    fig, ax = plt.subplots(figsize=(4, 4))\n\n    # Plot the heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap='Reds', aspect='auto')\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(cm.tolist())))\n    ax.set_yticks(np.arange(len(cm.tolist())))\n\n\n    thresh = cm.max() \/ 1.5\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(cm.tolist())):\n        for j in range(len(cm.tolist())):\n            text = ax.text(j, i, cm.tolist()[i][j],\n                           ha=\"center\", va=\"center\", size=16,\n                           color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n\n    plt.xlabel('Actual value', size=16)\n    plt.ylabel('Predicted value', size=16)\n    plt.title(cm_title, size=20, x=0.2, y=1.2)\n    plt.show()\n# https:\/\/matplotlib.org\/stable\/gallery\/images_contours_and_fields\/image_annotated_heatmap.html","d876bd2e":"def Confusion_matrix_metrics(TP, FP, FN, TN):\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP \/ (TP + FN)\n    print('The True Positive Rate is: {:.2%}'.format(TPR))\n    # Specificity, selectivity or true negative rate (TNR)\n    TNR = TN \/ (TN + FP)\n    print('The True Negative Rate is: {:.2%}'.format(TNR))\n    print('='*10)\n\n    # accuracy (ACC)\n    ACC = (TP + TN) \/ (TP + TN + FP + FN)\n    print('The Accuracy is: {:.2%}'.format(ACC))\n    # balanced accuracy (BA)\n    BA = (TPR + TNR) \/ 2\n    print('The Balanced Accuracy is: {:.2%}'.format(BA))\n    print('='*10)\n\n    # Precision or positive predictive value\n    PPV = TP \/ (TP + FP)\n    print('The Precision is: {:.2%}'.format(PPV))\n    # negative predictive value (NPV)\n    NPV = TN \/ (TN + FN)\n    print('The Negative Predictive Value is: {:.2%}'.format(NPV))\n    # false discovery rate (FDR)\n    FDR = 1 - PPV\n    print('The False Discovery Rate is: {:.2%}'.format(FDR))\n    # false omission rate (FOR)\n    FOR = 1 - NPV\n    print('The False Omission Rate is: {:.2%}'.format(FOR))\n    print('='*10)\n\n    # prevalence threshold (PT)\n    PT = (math.sqrt(TPR*(1 - TNR)) + TNR - 1)\/(TPR + TNR - 1)\n    print('The Prevalence Threshold is: {:.2}'.format(PT))\n    # F1 score\n    F1 = 2*TP \/ (2*TP + FP + FN)\n    print('The F1 Score is: {:.2}'.format(F1))\n    # Matthews correlation coefficient (MCC) or phi coefficient\n    MCC = ((TP*TN) - (FP*FN)) \/ math.sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))\n    print('The Matthews Correlation Coefficient is: {:.2}'.format(MCC))\n    print('='*10)\n\n    # False positive rate or False alarm rate\n    FPR = FP \/ (FP + TN)\n    print('The False positive rate is: {:.2}'.format(FPR))\n    # False negative rate or Miss Rate\n    FNR = FN \/ (FN + TP)\n    print('The False Negative Rate is: {:.2%}'.format(FNR))","936abaeb":"cm = confusion_matrix(y_train, y_pred_train).T\nconfusion_matrix_func(cm, cm_title=\"Confusion Matrix_Train\")","fb312bab":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","3298713c":"cm_test = confusion_matrix(y_test, y_pred_test).T\nconfusion_matrix_func(cm_test, cm_title=\"Confusion Matrix_Test\")","b410ce10":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm_test.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","27428069":"# TRAIN\n# calculate scores\nlr_auc = roc_auc_score(y_train, pred_proba_train[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nlr_fpr, lr_tpr, thresholds = roc_curve(y_train, pred_proba_train[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc = pd.DataFrame({\n    'False Positive Rate': lr_fpr,\n    'True Positive Rate': lr_tpr\n}, index=thresholds)\ndf_roc.index.name = \"Thresholds\"\ndf_roc.columns.name = \"Rate\"\n\n\n# TEST\n# calculate scores\nlr_auc_test = roc_auc_score(y_test, pred_proba_test[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc_test))\n# calculate roc curves\nlr_fpr_test, lr_tpr_test, thresholds_test = roc_curve(y_test, pred_proba_test[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc_test = pd.DataFrame({\n    'False Positive Rate': lr_fpr_test,\n    'True Positive Rate': lr_tpr_test\n}, index=thresholds_test)\ndf_roc_test.index.name = \"Thresholds\"\ndf_roc_test.columns.name = \"Rate\"\n\n\n# GRAPH\n# Set up the matplotlib figure\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# row=0, col=0\naxes[0, 0].plot(df_roc.iloc[:,0], df_roc.iloc[:,1], color='red', linewidth=2, \n                label=f'AUC={lr_auc:.2f}')\naxes[0, 0].fill_between(df_roc.iloc[:,0], df_roc.iloc[:,1], 0, color='red', alpha=0.3)\naxes[0, 0].plot(df_roc_test.iloc[:,0], df_roc_test.iloc[:,1], color='black', linewidth=2, \n                label=f'AUC_test={lr_auc_test:.2f}')\naxes[0, 0].plot([0, 1], [0, 1], color='green', linestyle='--', linewidth=1,\n                label='No Skill')\n\n# index of the first threshold for which the sensibility > 0.90\nidx = np.min(np.where(lr_tpr > 0.90))\naxes[0, 0].plot([0,lr_fpr[idx]], [lr_tpr[idx],lr_tpr[idx]], 'k--', color='blue')\naxes[0, 0].plot([lr_fpr[idx],lr_fpr[idx]], [0,lr_tpr[idx]], 'k--', color='blue')\n# Annotation\naxes[0, 0].annotate('(%.2f, %.2f)'%(lr_fpr[idx], lr_tpr[idx]),\n            (lr_fpr[idx], lr_tpr[idx]), \n            xytext =(-2 * 50, -30),\n            textcoords ='offset points',\n            bbox = dict(boxstyle =\"round\", fc =\"0.8\"), \n            arrowprops = dict(arrowstyle = \"->\"))\n\naxes[0, 0].set_xlabel('False Positive Rate', size=12)\naxes[0, 0].set_ylabel('True Positive Rate (recall)', size=12)\naxes[0, 0].legend(title='kNN')\naxes[0, 0].set_title('ROC curve', color='red', size=14)\n\n# row=0, col=1\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"True Positive Rate\"][1:], color='blue', linewidth=2, \n                label='TPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"True Positive Rate\"][1:], color='black', linewidth=2, \n                label='TPR_test')\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"False Positive Rate\"][1:], color='orange', linewidth=2, \n                label='FPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"False Positive Rate\"][1:], color='black', linewidth=2, \n                label='FPR_test')\n\naxes[0, 1].set_xlabel('Threshold', size=12)\naxes[0, 1].legend()\naxes[0, 1].set_title('TPR and FPR at every threshold', color='red', size=14)\n\n# row=1, col=0\nprecision, recall, thresholds = precision_recall_curve(y_test, pred_proba_test[:, 1])\n\naxes[1, 0].plot(recall, precision, color='green', linewidth=2, \n                label=f'PR_Curve (AUC={auc(lr_fpr, lr_tpr):.2f})')\naxes[1, 0].fill_between(recall, precision, 0, color='green', alpha=0.3)\n\naxes[1, 0].set_xlabel('Recall', size=12)\naxes[1, 0].set_ylabel('Precision', size=12)\naxes[1, 0].legend()\naxes[1, 0].set_title('Precision-Recall Curve', color='red', size=14)\n\nfig.tight_layout()\nfig.show()","9c09917f":"# Running Log loss on training\nprint('The Log Loss on Training is: {:.2}'.format(log_loss(y_train, pred_proba_train[:, 1])))\n\n# Running Log loss on testing\nprint('The Log Loss on Testing Dataset is: {:.2}'.format(log_loss(y_test, pred_proba_test[:, 1])))","f14d0789":"df_gender_sub.head()","e61003db":"submission = df_gender_sub.drop('Survived', axis=1)\ny_pred_kaggle = pipe.predict(X_kaggle_final)\nsubmission['Survived'] = y_pred_kaggle\nsubmission.head()","d8a3aa86":"# Are our test and submission dataframes the same length?\nif len(submission) == len(df_gender_sub):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","2180d5f0":"# Convert submisison dataframe to csv for submission to csv for Kaggle submisison\nsubmission.to_csv('titanic_submission2.csv', index=False)\nprint('Submission CSV is ready!')","f391299e":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"titanic_submission2.csv\")\nsubmissions_check.head()","c2a2acae":"<a id='313'><\/a>\n#### 3_1_3 Age - Missing Values","4849f16b":"<a id='48'><\/a>\n### 4.8. Confusion matrix","adb4c4a6":"<a id='32'><\/a>\n### 3_2 Combine SibSp and Parch for Simplicity","3cfd18d7":"<a id='34'><\/a>\n### 3_4 Investigate all the elements within each Feature ","7330618a":"##### 3_1_2_1 Missing values in train data","f68c15eb":"<a id='321'><\/a>\n#### 3_1_1 Fare - Missing Values","fe7937a3":"<a id='2'><\/a>\n## 2 Read CSV train\/test files into DataFrame","da8564f5":"<a id='5'><\/a>\n## 5. Submission","358e2645":"Mutual information (MI) measures the dependency between the variables. Higher values mean higher dependency.","580e3ff3":"##### 3_1_3_1 Missing values in train data","cf7dfb3b":"<a id='465'><\/a>\n#### 4_6_5 Feature_Selection - Final","30f60785":"<a id='462'><\/a>\n#### 4_6_2 Feature_Selection - Tree-based","b4e6ffca":"#### 3_2_1 Train data","24b7a4f1":"<a id='4_5'><\/a>\n### 4_5 k-Nearest Neighbours Model","c8a03a7f":"PLife -> Life is unfortunately not fair. If you're a 3rd class person in this world, it's a chance for you to survive.\n\nSex -> Positive discrimination has been made against women here as well.\n\nAge -> The age distribution for survivors and deceased is very similar except for children. The travelers strived for the survival of the children.\n\nFare -> Passengers who pay lower fare seem less likely to survive.\n\nEmbarked -> Although the number of boarders in Southhampton is higher than those in Cherbourg, the survival rate of Cherbourg is higher. This is probably related to the socioeconomic situation.","63296581":"#### 3_2_2 Test data","543afa40":"<a id='47'><\/a>\n### 4.7. Evaluating the Model","d7e61706":"<a id='41'><\/a>\n### 4_1 Separate the dataset into train and test","aee4473e":"<a id='3'><\/a>\n## 3 Data Preprocessing","fcd8f2c0":"<a id='410'><\/a>\n### 4.10. Logarithmic loss","417ab319":"##### 3_1_1_1 Missing values in test data","870a9493":"![titanic_data_dict.png](attachment:titanic_data_dict.png)","6f6489ad":"##### 3_1_3_2 Missing values in test data","a101718e":"pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","3e472b12":"<a id='31'><\/a>\n### 3_1 Missing Values","a19bf7e9":"<a id='312'><\/a>\n#### 3_1_2 Embarked - Missing Values","90846b35":"<a id='44'><\/a>\n### 4_4 What value for k?","5eff1081":"<a id='46'><\/a>\n### 4_6 Feature Selection","dae23529":"## Index\n\n[1 Importing packages](#1)<br>\n[2 Read CSV train\/test files into DataFrame](#2)<br>\n[3 Data Preprocessing](#3)<br>\n    <ul>\n        <li>[3_1 Missing Values](#31)<\/li>\n            <ul><li>[3_1_1 Fare - Missing Values](#311)<\/li>\n            <li>[3_1_2 Embarked - Missing Values](#12)<\/li>\n            <li>[3_1_3 Age - Missing Values](#313)<\/li><\/ul>\n        <li>[3_2 Combine SibSp and Parch for Simplicity](#32)<\/li>\n        <li>[3_3 Move the dependent column to the end](#33)<\/li>\n        <li>[3_4 Investigate all the elements within each Feature](#34)<\/li>\n        <li>[3_5 Exploratory Data Analysis](#35)<\/li>\n    <\/ul>\n[4 Regressions and Results](#4)<br>\n    <ul>\n        <li>[4_1 Separate the dataset into train and test](#41)<\/li>\n        <li>[4_2 Check categorical columns](#42)<\/li>\n        <li>[4_3 Check zero variance features](#43)<\/li>\n        <li>[4_4 What value for k?](#44)<\/li>\n        <li>[4_5 k-Nearest Neighbourns Model](#45)<\/li>\n        <li>[4_6 Feature Selection](#46)<\/li>\n            <ul><li>[4.6.1 Feature Selection - Drop Features Using Pearson Correlation](#461)<\/li>\n            <li>[4_6_2 Feature_Selection - Tree-based](#462)<\/li>\n            <li>[4_6_3 Feature_Selection - Mutual information](#463)<\/li>\n            <li>[4_6_4 Feature_Selection - Feature_Selection - Evaluation of all column combinations](#464)<\/li>\n            <li>[4_6_5 Feature_Selection - Final](#465)<\/li><\/ul>\n        <li>[4_7 Evaluating the Model](#47)<\/li>\n        <li>[4_8 Confusion matrix](#48)<\/li>\n        <li>[4_9 roc curve and auc](#49)<\/li>\n        <li>[4_10 Logarithmic loss](#410)<\/li>\n[5. Submission](#5)<br>","e5bfc8e4":"<a id='464'><\/a>\n#### 4_6_4 Feature_Selection - Evaluation of all column combinations","d8a8e21d":"<a id='4'><\/a>\n## 4 Regressions and Results","8106ebd4":"<a id='35'><\/a>\n### 3_5 Exploratory Data Analysis","35068c8f":"**The goal** is to predict the target variable(Survived) using logistic regression.","550ddc7c":"<a id='463'><\/a>\n#### 4_6_3 Feature_Selection - Mutual information","7aa68062":"<a id='43'><\/a>\n### 4_3 Check zero variance features","8e5f256e":"<a id='33'><\/a>\n### 3_3 Move the dependent column to the end","5f4a1f53":"<a id='42'><\/a>\n### 4_2 Check categorical columns","6acd9260":"<a id='461'><\/a>\n#### 4_6_1 Feature Selection - Drop Features Using Pearson Correlation","b203565f":"<a id='1'><\/a>\n## 1 Importing packages","437db87f":"<a id='49'><\/a>\n### 4.9. roc curve and auc","92641a4c":"{'Sex_male'} and {'Sex_female'} are highly correlated features in the dataset."}}