{"cell_type":{"e82cbae6":"code","b08f1b50":"code","18ff7264":"code","8f578d09":"code","a23c6bfb":"code","9453d5a7":"code","83b52c40":"code","c3fba7fc":"code","c11fc703":"code","68769fc7":"code","204cb9ae":"code","bf1b6d02":"code","14656681":"code","906c255f":"code","ff6669cc":"code","ae588926":"code","9b6ea982":"code","767ccb14":"code","54768909":"code","57c91b5d":"code","4b3bc1e1":"code","1daf1ea2":"code","511d68f9":"code","70b3f36f":"code","7ab585f8":"code","aa308a5f":"code","6b4dd6a5":"code","9508d52b":"code","4ccce627":"code","43a2b8bc":"code","d0de90e6":"code","487e7624":"code","b5f2be8f":"markdown","7de20e5a":"markdown","8b518f6d":"markdown","e58b8428":"markdown"},"source":{"e82cbae6":"#! pip install kaggle\n#! mkdir ~\/.kaggle\n#! cp kaggle.json ~\/.kaggle\/\n#! chmod 600 ~\/.kaggle\/kaggle.json\n","b08f1b50":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor\nfrom sklearn.metrics import mean_squared_error\n","18ff7264":"#! kaggle datasets download -d mohansacharya\/graduate-admissions","8f578d09":"#! unzip graduate-admissions.zip","a23c6bfb":"df=pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf","9453d5a7":"df.drop('Serial No.', axis='columns', inplace=True) # Since this column is irrelevant to our model","83b52c40":"def describe(df):                        # Function to explore major elements in a Dataset\n                                         # Will help to find null values present and deal with them\n  columns=df.columns.to_list()           # Function will help to directly find numerical and categorical columns\n  ncol=df.describe().columns.to_list()\n  ccol=[]\n  for i in columns:\n    if(ncol.count(i)==0):\n      ccol.append(i)\n    else:\n      continue\n  print('Name of all columns in the dataframe:')\n  print(columns)\n  print('')\n  print('Number of columns in the dataframe:')\n  print(len(columns))\n  print('')\n  print('Name of all numerical columns in the dataframe:')\n  print(ncol)\n  print('')\n  print('Number of numerical columns in the dataframe:')\n  print(len(ncol))\n  print('')\n  print('Name of all categorical columns in the dataframe:')\n  print(ccol)\n  print('')\n  print('Number of categorical columns in the dataframe:')\n  print(len(ccol))\n  print('')\n  print('------------------------------------------------------------------------------------------------')\n  print('')\n  print('Number of Null Values in Each Column:')\n  print('')\n  print(df.isnull().sum())\n  print('')\n  print('')\n  print('Number of Unique Values in Each Column:')\n  print('')\n  print(df.nunique())\n  print('')\n  print('')\n  print('Basic Statistics and Measures for Numerical Columns:')\n  print('')\n  print(df.describe().T)\n  print('')\n  print('')\n  print('Other Relevant Metadata Regarding the Dataframe:')\n  print('')\n  print(df.info())\n  print('')\n  print('')","c3fba7fc":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# We are creating 3 categories for better vizualisation\n# Split was chosen after Personal research and is subject to change \n\ndf['Admit Possibility']=0\nfor i in range(0,len(df)):\n  if(df['Chance of Admit '][i]>0.80):\n    df['Admit Possibility'][i]='High'\n  elif(df['Chance of Admit '][i]<0.60):\n    df['Admit Possibility'][i]='Low'\n  else:\n    df['Admit Possibility'][i]='Fair'\n\n    \n","c11fc703":"describe(df)","68769fc7":"fig = plt.figure(figsize=(16,10))\noe=['g','y','r']\nplt.subplot(2,2,1)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"Research\", hue=\"Admit Possibility\",multiple=\"stack\",palette=oe)\n\nplt.subplot(2,2,2)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"LOR \", hue=\"Admit Possibility\",multiple=\"stack\",palette=oe)\n\nplt.subplot(2,2,3)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"SOP\", hue=\"Admit Possibility\",multiple=\"stack\",palette=oe)\n\nplt.subplot(2,2,4)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"SOP\", hue=\"Admit Possibility\",multiple=\"stack\",palette=oe)\n\nplt.subplot(2,2,2)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"University Rating\", hue=\"Admit Possibility\",multiple=\"stack\",palette=oe)\n\n\n","204cb9ae":"oe=['g','y','r']\nplt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('CGPA vs Admit Chance')\nsns.scatterplot( x=\"CGPA\",y='Chance of Admit ', hue=\"Admit Possibility\",data=df,palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Admit Possibility')","bf1b6d02":"oe=['g','y','r']\nplt.tight_layout()\nfig, ax = plt.subplots(figsize=(8, 6))\nstyle.use('ggplot')\nsns.set_context('talk')\nplt.title('GRE Score vs Admit Chance')\nsns.scatterplot(x=\"GRE Score\", y=\"Chance of Admit \", hue=\"Admit Possibility\", data=df, palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Admit Possibility')","14656681":"oe=['g','y','r']\nplt.tight_layout()\nfig, ax = plt.subplots(figsize=(8, 6))\nstyle.use('ggplot')\nsns.set_context('talk')\nplt.title('TOEFL Score vs Admit Chance')\nsns.scatterplot(x=\"TOEFL Score\", y=\"Chance of Admit \", hue=\"Admit Possibility\", data=df, palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Admit Possibility')","906c255f":"# Heat Map : To find out correlation among all variables (i.e. dependent and independent)\n\ncorr = df.corr()\noe=['r','y','g']\nfig, ax = plt.subplots(figsize=(8, 6))\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\nsns.set_style(\"white\")\nsns.heatmap(corr, cmap=oe, linewidths=.5, annot=True, fmt=\".2f\", mask=dropSelf)\nplt.show()","ff6669cc":"def outliers(df_column):\n  q75, q25 = np.percentile(df_column, [75 ,25]) \n  iqr = q75 - q25\n  print('q75: ',q75)\n  print('q25: ',q25)\n  print('Inter Quartile Range: ',round(iqr,2))\n  print('Outliers lie before', q25-1.8*iqr, 'and beyond', q75+1.8*iqr) \n\n  # Usually 1.5 times IQR is considered, but we have used 1.8 for broader range since datapoints are very less\n\n  print('Number of Rows with Left Extreme Outliers:', len(df[df_column <q25-1.8*iqr]))\n  print('Number of Rows with Right Extreme Outliers:', len(df[df_column>q75+1.8*iqr]))\n  plt.tight_layout()\n  plt.style.use('seaborn')\n  sns.set_context('notebook')\n  sns.histplot(data=df, x=df_column, hue=\"Admit Possibility\",multiple=\"stack\",palette=oe)\n  print('')\n  \n\n  ","ae588926":"outliers(df['GRE Score'])","9b6ea982":"outliers(df['TOEFL Score'])","767ccb14":"outliers(df['CGPA'])","54768909":"outliers(df['LOR '])","57c91b5d":"outliers(df['SOP'])","4b3bc1e1":"outliers(df['Research'])","1daf1ea2":"outliers(df['University Rating'])","511d68f9":"# Find VIF (Variance Inflation Factor) To Check for collinearity among independent variables\n# We dropped TOEFL Score becaue it was Highly Correlated to GRE Score with a VIF Value > 1000\nvif = df.copy()\nvif.drop(columns=['Chance of Admit ','Admit Possibility','TOEFL Score'],axis=1,inplace=True)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(vif.values, i)\n                          for i in range(len(vif.columns))]","70b3f36f":"vif_data","7ab585f8":"# Scale Data For Higher Efficiency\nfrom sklearn.preprocessing import StandardScaler # Converts Columnar Data into Standard Normal Distribution\nscaler=StandardScaler()\nscaler.fit(vif)\nscaled_data=scaler.transform(vif)\nscaled_data","aa308a5f":"from sklearn.decomposition import PCA # Reduce Dimensions by Principal Component Analysis To Compensate for Variables with High VIF\npca=PCA(n_components=3)\npca.fit(scaled_data)\nx_pca=pca.transform(scaled_data)\nx_pca","6b4dd6a5":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Chance of Admit '], test_size=0.2, random_state=42)\nreg = LinearRegression()\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Linear Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Linear Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","9508d52b":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Chance of Admit '], test_size=0.2, random_state=42)\nreg=Lasso(alpha=0.005)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Lasso Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Lasso Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","4ccce627":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Chance of Admit '], test_size=0.2, random_state=42)\nreg=Ridge(alpha=0.01)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Ridge Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Ridge Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","43a2b8bc":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Chance of Admit '], test_size=0.2, random_state=42)\nreg=ElasticNet(alpha=0.005)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of ElacticNet Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of ElacticNet Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","d0de90e6":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Chance of Admit '], test_size=0.2, random_state=42)\nreg = RandomForestRegressor(random_state=42)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Random Forest Regressor Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Random Forest Regressor Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","487e7624":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Chance of Admit '], test_size=0.2, random_state=42)\nimport tensorflow as tf\ntf.random.set_seed(0)\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    loss=tf.keras.losses.binary_crossentropy,\n    optimizer=tf.keras.optimizers.Adam(lr=0.03),\n    metrics=[]\n)\n\nhistory = model.fit(x_train, y_train, epochs=100)\ny_pred=model.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))\n","b5f2be8f":"The Following Code Explores Various Models and Effective Vizualisations That are Easy to Understand\n","7de20e5a":"Models are Subject to Betterment with Stringent Hyper parameter tuning, Like using **GridSearchCV** to Run through multiple combinations of potential hyperparameters","8b518f6d":"The Best Mean Squared Error (MSE) Numeric Obtained By a Few Models was: **0.0641** and an Accuray of About **80%**","e58b8428":"Concepts Like **VIF** and **PCA** have been used to transform the dataset, Vizualizations Include **Scatter Plots** and **Hist Plots**"}}