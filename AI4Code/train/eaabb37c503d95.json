{"cell_type":{"36078335":"code","f30a840e":"code","06ebac6b":"code","32a826cc":"code","f2aa1c96":"code","ad3e3fe9":"code","3eeb3396":"code","2622469d":"code","73b4bc81":"code","549d9c8f":"code","e4212e70":"code","ef244ed1":"code","5f47baa5":"code","7154be97":"code","7db52acc":"code","e021e0e1":"code","329e44c5":"code","89a3cd18":"code","4929ef7e":"code","971f6660":"code","3f58fd2d":"code","736223f0":"code","dc3dc336":"code","9857662d":"code","d05d6af3":"code","e1325637":"code","b1a2a7b0":"code","02a93a15":"code","65f33043":"code","98a6367c":"code","0e58389f":"code","1a97a9be":"code","f1e14d62":"code","d3840f15":"code","b60b3ddb":"code","c74682eb":"code","d3e51fac":"code","33b3d442":"code","9b4e1f58":"code","df0aa2dc":"code","071042ab":"code","36212f96":"markdown","3ddfcba2":"markdown","15a1b7ff":"markdown","01422a8b":"markdown","39b77791":"markdown","416a9527":"markdown","0e1baa1c":"markdown","99d5f73d":"markdown","c05bad29":"markdown","b7e317f5":"markdown","a3406e55":"markdown","691d1770":"markdown","31315502":"markdown","0187e17d":"markdown","a9f16ef6":"markdown","81e2341a":"markdown","21b5efba":"markdown","a974c790":"markdown","a0780bbd":"markdown","faed057a":"markdown","7ca3ca8a":"markdown","849f3288":"markdown","340b0254":"markdown","1e2a34bb":"markdown","def25fe2":"markdown","e79ece4e":"markdown","c3e6a668":"markdown","c761326e":"markdown","e243d248":"markdown","7f132c50":"markdown","7f9d1aa3":"markdown","19fa5e96":"markdown","9ce068a3":"markdown","cddc5e57":"markdown","1ece69a1":"markdown","dccba520":"markdown","f9975a6d":"markdown","7894be91":"markdown","c7692953":"markdown","4f63e717":"markdown"},"source":{"36078335":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f30a840e":"#Read the data\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","06ebac6b":"#From priliminary glance, Unnamed:32 looks like all NaN values.Confirming the same:\ndf['Unnamed: 32'].value_counts()","32a826cc":"#Removing unnamed:32\ndf.drop(['Unnamed: 32'], axis = 1, inplace= True)\ndf.head()","f2aa1c96":"#Checking the dimension of the dataset\ndf.shape","ad3e3fe9":"#Checking the datatypes\ndf.dtypes","3eeb3396":"df.nunique()","2622469d":"#number of missing values, if any\ndf.isna().sum()","73b4bc81":"df['diagnosis'].unique()","549d9c8f":"df['diagnosis'].value_counts()","e4212e70":"plt.figure(figsize = (25,25))\nsns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)","ef244ed1":"#Standardizing the data first to view it in a single graph plot\nstd = (df - df.mean())\/ (df.std())\nstd_data =pd.concat([df['diagnosis'], std], axis=1)\nstd_data.describe()","5f47baa5":"#Creating set1 dataframe\nset1 = pd.concat([df['diagnosis'], std['radius_mean'], std['perimeter_mean'], std['area_mean'],std['radius_se'], std['perimeter_se'],std['area_se'], std['radius_worst'],std['perimeter_worst'],std['area_worst']], axis=1)\nset1.head()","7154be97":"plt.figure(figsize = (15, 8))\nsns.boxplot(x=\"variable\", y=\"value\", hue = 'diagnosis' , data=pd.melt(set1, id_vars='diagnosis'))","7db52acc":"#Taking a closer look at radius_mean\n\nsns.distplot(x=df['radius_mean'] )","e021e0e1":"#Defining upper and lower limits to identify outliers\n#Tail =1\ndef outlier_limits(df, variable_name):\n    q1 = df[variable_name].quantile(0.25)\n    q3 = df[variable_name].quantile(0.75)\n    iqr = q3-q1\n    interval=3*iqr\n    upper_l= q3+interval\n    lower_l= q1-interval\n    return lower_l, upper_l","329e44c5":"#Checking the upper and lower limits\n\nlower, upper = outlier_limits(df, 'radius_mean')\nprint(upper, lower)","89a3cd18":"#Calculating the number of outliers, including -ve values\noutliers=[i for i in df['radius_mean'] if i>upper or i<0]\nlen(outliers)","4929ef7e":"#SET 2 \nset2 = pd.concat([df['diagnosis'], std['texture_mean'], std['texture_worst']], axis=1)\nset2","971f6660":"plt.figure(figsize = (15, 8))\nsns.boxplot(x= \"variable\", y = \"value\", hue = \"diagnosis\", data = pd.melt(set2, id_vars = \"diagnosis\"))","3f58fd2d":"#Deeper look at texture_worst\nsns.distplot(x=df['texture_worst'] )","736223f0":"#performing outlier check to confirm\nlower, upper = outlier_limits(df, 'texture_worst')\noutliers=[i for i in df['texture_worst'] if i>upper or i<0]\nlen(outliers)","dc3dc336":"#SET 3\nset3 = pd.concat([df['diagnosis'], std['smoothness_mean'], std['smoothness_worst']], axis=1)\nset3.head()","9857662d":"plt.figure(figsize = (15, 8))\nsns.boxplot(x= \"variable\", y = \"value\", hue = \"diagnosis\", data = pd.melt(set3, id_vars = \"diagnosis\"))","d05d6af3":"#Deeper look at smoothness_worst\nsns.distplot(x=df['smoothness_worst'] )","e1325637":"#performing outlier check to confirm\nlower, upper = outlier_limits(df, 'smoothness_worst')\noutliers=[i for i in df['smoothness_worst'] if i>upper or i<0]\nlen(outliers)","b1a2a7b0":"#SET 4 \nset4 = pd.concat([df['diagnosis'], std['compactness_mean'], std['concavity_mean'], std['concave points_mean'], std['compactness_worst'], std['concavity_worst'], std['concave points_worst']], axis=1)\nset4.head()","02a93a15":"plt.figure(figsize = (15, 8))\nsns.boxplot(x= \"variable\", y = \"value\", hue = \"diagnosis\", data = pd.melt(set4, id_vars = \"diagnosis\"))","65f33043":"data_for_pca = pd.concat([df['compactness_mean'], df['concavity_mean'], df['concave points_mean'], df['compactness_worst'], df['concavity_worst'], df['concave points_worst']], axis=1)\n\n#Standarding the data\nscaler = StandardScaler()\nscaler.fit_transform(data_for_pca)\ndata_for_pca.head()","98a6367c":"pca= PCA()\npca.n_components = 6\npca_data = pca.fit_transform(data_for_pca)\n\nvariance_explained=pca.explained_variance_ratio_\ncumulative_ve=np.cumsum(variance_explained)\n\ndef pca_graph(cumulative_variance):    \n    plt.figure(1, figsize=(6, 4))\n    plt.clf()\n    plt.plot(cumulative_variance, linewidth=2)\n    plt.axis('tight')\n    plt.grid()\n    plt.xlabel('n_components')\n    plt.ylabel('Cumulative_variance_explained')\n    plt.show()\n    \npca_graph(cumulative_ve)","0e58389f":"#Adding some more more concavity variables to see if we're able to retain this much information\ndata_for_pca2 = pd.concat([df['compactness_mean'], df['concavity_mean'], df['concave points_mean'], df['compactness_worst'], df['concavity_worst'], df['concave points_worst'], df['concave points_se'], df['concavity_se'], df['compactness_se']], axis=1)\n\n#Standarding the data\nscaler = StandardScaler()\nscaler.fit_transform(data_for_pca2)\ndata_for_pca2.head()","1a97a9be":"pca= PCA()\npca.n_components = 9\npca_data2 = pca.fit_transform(data_for_pca2)\n\nvariance_explained=pca.explained_variance_ratio_\ncumulative_ve=np.cumsum(variance_explained) \npca_graph(cumulative_ve)","f1e14d62":"#Applying PCA and reducing it to 3 variables\n\npca.n_components = 3\npca_data2=pca.fit_transform(data_for_pca2)\np_components = pd.DataFrame(data=pca_data2, columns = ['PC 1', 'PC 2', 'PC 3'])\np_components","d3840f15":"#Confirming the data from the graph \n\npca.explained_variance_ratio_","b60b3ddb":"#Putting together all the chosen variables and the principal components:\nfinal_df=pd.concat([df['diagnosis'],df['radius_mean'], df['texture_worst'], df['smoothness_worst'],p_components], axis=1)\nfinal_df","c74682eb":"#Deleting the outliers from the final dataset\nlower, upper = outlier_limits(df, 'radius_mean')\nfinal_df.drop(final_df.loc[df['radius_mean']>upper].index, inplace = True)\noutliers=[i for i in final_df['radius_mean'] if i>upper or i<0]\nlen(outliers)","d3e51fac":"final_df","33b3d442":"#Importing the relevant libraries\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nimport xgboost\n\n\n#Splitting the data\n\ny = final_df['diagnosis']\nX = final_df.drop(['diagnosis'], axis =1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=42)\n\n\n#Creating a function to calculate accuracy\/precision scores\n\ndef scores(target_test, predicted):\n    ac=accuracy_score(target_test, predicted)\n    precision = precision_recall_fscore_support(target_test, predicted, labels = ['M'])[0]\n    recall = precision_recall_fscore_support(target_test, predicted, labels = ['M'])[1]\n    fscore = precision_recall_fscore_support(target_test, predicted, labels = ['M'])[2]\n    print(f\" Accuracy is {ac}\")\n    print(f\" Precision is {precision}\")\n    print(f\" Recall is {recall}\")\n    print(f\" F Score is {fscore}\")","9b4e1f58":"#Applying Logistic Regression\n\nlr = LogisticRegression()\nlr = lr.fit(X_train,y_train)\npred = lr.predict(X_test)\nscores(y_test, pred)","df0aa2dc":"#Applying Random Forest \n\nrf = RandomForestClassifier(random_state = 42)\nrf = rf.fit(X_train, y_train)\npred = rf.predict(X_test)\nscores(y_test, pred)","071042ab":"xgb = xgboost.XGBClassifier()\nxgb = xgb.fit(X_train, y_train)\npred = xgb.predict(X_test)\nscores(y_test, pred)","36212f96":"Looking at outliers and variable overlap, texture_worst looks like a better variable for the model\n","3ddfcba2":"> There's doesn't seem to be a huge imbalanace in data","15a1b7ff":"> Looks like all variables have the right datatypes assigned to them.","01422a8b":"# Applying PCA \n\nApplying Principal component analysis (PCA) to the variables to reduce the dimensionality without losing a lot of information. First step is to ensure the variables are standardised:\n\n* Standardising the variables using StandardScaler() to unit scale that is a mean of 0, and variance of 1\n","39b77791":"In this last stage, we'll be applying the various classification machine learning models on our final dataset, and pick the best performing model.","416a9527":"# III. Modeling ","0e1baa1c":"# SET 4 - Compactness, Concavity","99d5f73d":"# I. High-Level Analysis\n\nPerforming high-level analysis to find the shape of the data, check the data-types, and find missing values, if any.","c05bad29":"# Dataset\n\nThe dataset is taken from Kaggle repository - https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data","b7e317f5":"# III. Modifications","a3406e55":"# Reading the data\n\nReading the data using pd.read_csv","691d1770":"> Since there's just one outlier, we'll go ahead and delete it","31315502":"Overall, Random Classifier gives the best results with an  accuracy of 96.7% and a precision of 94.3%","0187e17d":"> Looks like there are no missing values.","a9f16ef6":"# XGBoost","81e2341a":"Some of the observations from the above graph:\n1. **radius_mean** is highly correlated with **perimeter_mean** and **area_mean**, which makes sense because perimeter_mean and area_mean are derived using radius as the only variable. It's also highly correlated with radius_worst, perimteter_worst and area_worst\n2. **texture_mean** and **texture_worst** are highly correlated\n3. **smoothness_mean** and **smoothness_worst** are correlated.\n4. **compactness_mean** shows reasonably high correlation with **concavity_mean**, **concave points_mean**, **compactness_worst**, **concavity_worst** and **concave points_worst**.\n5. **concave points_mean** shows a good correaltion with most variables \n\nLet's look at how these variables are individually related to the target variable to determine which of them can prove to be more significant to the model performance.\n","21b5efba":"Since there's not a very high correlation, a better approach might be to apply PCA on these variables, instead of compeltely discarding few of them\n","a974c790":"# II. In-depth Data Exploration\n\n**1. Target Variable**","a0780bbd":"Categorizing the data into the following sets of variables, for ease of analysis, and visualizing:\n*     **SET 1** : radius_mean, perimeter_mean, area_mean, radius_se, area_se, perimeter_se, radius_worst, perimeter_worst, area_worst\n*     **SET 2** : texture_mean, texture_worst\n*     **SET 3** : smoothness_mean, smoothness_worst\n*     **SET 4** : compactness_mean, concavity_mean, concave_points_mean, compactness_worst, concavity_worst, concave_points_worst","faed057a":"# Logistic Regression","7ca3ca8a":"# Removing Outliers","849f3288":"# Methodology\n\nSEMMA approach was used in analyzing and modeling this particular dataset. More specifically, the following steps were taken :\n\n1. **High-Level Data Analysis** : The dataset characteristics - dimension, missing values, data types were observed.\n2. **Data Exploration** : Individual variables were analysed for patterns, and hidden correations.\n3. **Modifications** : Outliers were removed and PCA was performed where necessary.\n4. **Modeling** : Logistic Regression, Random Forest, and XG Boost was applied, and fit performance was evaluated using metrics like Accuracy, Precision, Recall.\n5. **Conclusion** : Best model was chosen based on the performance and evaluation metrics.","340b0254":"> From the above graph, we can deduce that using just three out of the nine components,we're able to retain more than 98% variance","1e2a34bb":"**2. Predictor Variables:**\n\nTo analyze the predictor variables and identify the variables significant to the model, we'll follow the below process.\n*     Check the distribution of the individual variables\n*     Find Outliers\n*     Check co-relation with target\n\nBefore starting, we'll take a look at the overall correlation between variables to check if we need to deal with multi-collinearity. A heatmap is used for this purpose.","def25fe2":"# IV. Conclusion","e79ece4e":">Looks almost normal without any outliers","c3e6a668":"# SET 1 - Radius, Area, Perimeter","c761326e":"Analysing outliers in radius_mean :","e243d248":"> Almost linear with one or more points that seem to be outliers","7f132c50":"> Target variable has two classes, namely M and B. M stands for Malignant and B for Benign. \nM is the class of importance to us.","7f9d1aa3":"Observations:\n* Almost all variables show a good amount of separation, and no overlap\n* Few variables like radius_se, perimeter_se, and area_se have a lot of outliers.\n* radius_mean might be a good predictor to go ahead with","19fa5e96":"# SET 3 - Smoothness","9ce068a3":"# Importing relevant libraries\n\nWe'll be making use of pandas and numpy to store and perform data operations, matplotlib and seaborn to visualise the data, and sci-kit learn for standardizing data, and applying the machine learning models like Regression, XG Boost, and Random Classification","cddc5e57":"# Breast Cancer Classification\n\nThis is a classsification problem aimed to identify and distinguish malignant cancer from benign, using measures of area, smoothness, texture etc., from a digitized image of a cell nuclei.","1ece69a1":"# Random Forest Classifier","dccba520":"# SET 2 - Texture","f9975a6d":"> *image source* : https:\/\/www.verywellhealth.com\/what-does-malignant-and-benign-mean-514240 ","7894be91":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/verywellhealth.com\/thmb\/i8iFcl6CJWvKS7BFWgluoSCyLCI=\/614x0\/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)\/514240-article-img-malignant-vs-benign-tumor2111891f-54cc-47aa-8967-4cd5411fdb2f-5a2848f122fa3a0037c544be.png\" alt=\"Heat beating\" style=\"height:300px;margin-top:3rem;\"> <\/div>\n","c7692953":"Quite an overlap within the two varibles, might not act as good predictor variables. smoothness_worst, however, seems to be the better option considering separation and outlier distribution\n","4f63e717":">Looks normal without any outliers"}}