{"cell_type":{"4a1248ce":"code","360c91ac":"code","10a28edd":"code","43777dcb":"code","ad5e1fdf":"code","791b14e5":"code","43cb70c7":"code","05406629":"code","2277b7fa":"code","90914c75":"markdown","6bccb89a":"markdown","df9f68ba":"markdown","507dd1bd":"markdown","63f20f46":"markdown","434acaa6":"markdown","ca1c6af6":"markdown","2782dcb0":"markdown","fbf5fb2e":"markdown","c9fc55bb":"markdown"},"source":{"4a1248ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.externals import joblib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import auc\nfrom sklearn import metrics\n\nimport scikitplot as skplt\n\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nHDdata=pd.read_csv('..\/input\/heart.csv')\n\n","360c91ac":"HDdata.head()","10a28edd":"y = HDdata['target'].as_matrix()\n\ndel HDdata['target']\n\n# We will need this later\nfeaturenames = HDdata.columns\n\nX = HDdata.as_matrix()\n\n# Split the data set in a training set (70%) and a test set (30%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","43777dcb":"# instantiate the decision tree classifier\nclf = tree.DecisionTreeClassifier()\n\n# Parameters we want to try\nparam_grid = {\n    'max_depth': [4, 3, 5, 6, 7, 8, 9],\n    'min_samples_leaf': [3, 5, 7, 9, 2],\n    'min_samples_split':[2, 4, 5, 7, 10, 12, 15],\n    'max_features': [0.1, 0.01, 0.05, 0.001, 1],\n    'criterion': ['gini', 'entropy']\n}\n\nGS_clf = GridSearchCV(clf, param_grid, n_jobs=4)\n\nGS_clf.fit(X_train, y_train)\n\n# The best score and the best Params\nprint('best score: ', GS_clf.best_score_)\nprint(\"best params -\", GS_clf.best_params_)\n","ad5e1fdf":"#You could just use GS_clf.fit(X_train, y_train), but i wanted to make sure the params are shown. \n \nclfBestParams = tree.DecisionTreeClassifier(criterion= 'entropy', max_depth= 6, max_features= 0.001, min_samples_leaf=9, min_samples_split= 10)\n\nclfBestParams.fit(X_train, y_train)\n\ndecisionTreePredictionsTrain = clfBestParams.predict(X_train)\n\ndecisionTreePredictionsTest = clfBestParams.predict(X_test)\n\nprint(\"Dec Tree accuracy on the training set:\", accuracy_score(y_train, decisionTreePredictionsTrain))\n\nprint(\"Dec Tree accuracy on the test set:\", accuracy_score(y_test, decisionTreePredictionsTest))\n","791b14e5":"\nprint(pd.DataFrame(\n    confusion_matrix(y_test, decisionTreePredictionsTest),\n    columns=['Predicted No HD', 'Predicted HD'],\n    index=['True Not HD', 'True HD']\n))\n\n\n\n","43cb70c7":"specificity = 36\/(36+5)\nsensitivity = 34\/(34+16)\n\nprint(\"sensitivity\", sensitivity)\nprint(\"Specificity\", specificity)","05406629":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = clfBestParams.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic Testing')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('(1 - Specificity) False Positive Rate')\nplt.show()\n","2277b7fa":"import graphviz\ndot_data = tree.export_graphviz(clfBestParams, out_file=None, feature_names= featurenames,\n                                class_names= ['0', '1'],\n                                filled=True, rounded=True,\n                                special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph.render(\"HeartDisease\")","90914c75":"As this is a medical dataset and we would like to know how the classification decisions are being made; ie some \"explainability\", we will start with a decision tree model.  The aim being to produce a decision tree that can be quickly consulted to provide assistance in determining the risk of heart disease.    At this stage we will leave the data as is, not normalizing or doing any other feature engineering that will make the decision tree less friendly for the end user. \n\nDecision trees will overfit the data if we don't set some parameters to keep to the tree complexity under control.   \n\nWe will use grid search to iterate through a bunch of hyperparemeter combinations and output the most effective one.   \nThis will take some time if you don't have parralell processing...","6bccb89a":"**Load the data into a pandas dataframe**\n\nI like to load all the required libraries at the start, no real reason, just what I am used to from other programming environments. ","df9f68ba":"Create the target and features dataframes.  Then split into the train and test sets.","507dd1bd":"sensitivity 0.68\nSpecificity 0.8780487804878049\n\nAs expected.  Let's have a look at the ROC curve. ","63f20f46":"best params - {'criterion': 'entropy', 'max_depth': 6, 'max_features': 0.001, 'min_samples_leaf': 9, 'min_samples_split': 10}\nbest score:  0.8349056603773585\n\nOk we have the best Params, let's see how the model performs.   It is important compare the test and train accuracy to asses for underfitting and overfitting. ","434acaa6":"(https:\/\/postimg.cc\/w3ktn7kC)\nI hope this can be seen. ","ca1c6af6":"~80% for both training\n~77% for  testing.\n\nBut remeber this is a medical application, we are more interested in precision\/specificity and recall\/sensitivity than  accuracy. \n\nLet's look at the ROC area and create a confusion matrix to see how useful the model is. ","2782dcb0":"Ok we have a nice model, now let's have a look at how it comes to it's predictions. We shall show the decision tree. ","fbf5fb2e":"Ok from a quick glance we can see there are a lot of false negatives; this test is not very sensitive, but seems to be quite precise, ie if the patient does not have the disease it is unlikley they will test positive. \n\nLet's work out these scores. ","c9fc55bb":"Let's have a look at the data\n"}}