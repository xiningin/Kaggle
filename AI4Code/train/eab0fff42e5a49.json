{"cell_type":{"dd11df66":"code","824d31c3":"code","eb1f0ca7":"code","f585a798":"code","50a90fbc":"code","f3455983":"code","71dc39e4":"code","9cc00e3f":"code","a1c21923":"code","06633f37":"code","92cff778":"code","07ae26e5":"code","3ec6701a":"code","61c9c4e7":"code","3475468c":"code","7f256ea1":"code","ff992c7a":"code","39153757":"code","49f1cdd4":"code","5b376f1a":"code","2098e631":"code","31fcd373":"code","a2396502":"code","b4c26ce2":"code","df0f5059":"code","ef229daf":"code","701b50ee":"code","8cff93ab":"code","831ef1f9":"code","3f5ead3b":"code","fa0d8539":"code","bf37b81a":"code","3c0aa36a":"code","b95bda73":"code","83d94697":"markdown","24ea91a4":"markdown","2e761aca":"markdown","5622909b":"markdown","6f562e51":"markdown","6793a07e":"markdown","bff9c3fc":"markdown","e5e4816e":"markdown","2cea21f6":"markdown","d6ec59fe":"markdown","a205d40c":"markdown","7a09554c":"markdown","0fe42619":"markdown","9e343111":"markdown","8353191c":"markdown","9298119b":"markdown"},"source":{"dd11df66":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nimport torchvision.transforms as tfs\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","824d31c3":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print('{} CONTAINS {} files'.format(dirname, len(filenames)))","eb1f0ca7":"# define some constants\n\nINPUT_DIR = '\/kaggle\/input\/hymenoptera-data\/hymenoptera_data\/'\n# Imagenet mean and standard (are calculated from all of images)\nIMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\nIMAGENET_STD = np.array([0.229, 0.224, 0.225])\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nBATCH_SIZE = 4","f585a798":"data_tfs = {\n    'train': tfs.Compose([\n        tfs.Resize(300),\n        tfs.RandomCrop(244),\n        tfs.RandomHorizontalFlip(),\n        tfs.ToTensor(),\n        tfs.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ]),\n    'val': tfs.Compose([\n        tfs.Resize(256),\n        tfs.CenterCrop(244),\n        tfs.ToTensor(),\n        tfs.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ]),\n}\n\ndataset = {ds: torchvision.datasets.ImageFolder(\n    root=os.path.join(INPUT_DIR, ds),\n    transform=data_tfs[ds]\n) for ds in ['train', 'val']}\n\ndataset_size = {ds: len(dataset[ds]) for ds in ['train', 'val']}\ndataset_classes = dataset['train'].classes\nprint('classes:', dataset_classes, '\\nsize', dataset_size)\n\ndataloader = {\n    'train': torch.utils.data.DataLoader(\n        dataset=dataset['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n    ),\n    'val': torch.utils.data.DataLoader(\n        dataset=dataset['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n    ),\n}","50a90fbc":"def get_batches(dataloader, num_of_batches=3):\n    assert(num_of_batches > 0)\n    images, classes = next(iter(dataloader))\n    for _ in range(num_of_batches - 1):\n        image_batch, cls_batch = next(iter(dataloader))\n        images = torch.cat([images, image_batch], dim=0)\n        classes = torch.cat([classes, cls_batch], dim=0)\n    return images, classes","f3455983":"def show_batches(images, classes):\n    cols = (images.shape[0] + 3) \/\/ 4\n    images = images.numpy().transpose(0,2,3,1)\n    images = images * IMAGENET_STD + IMAGENET_MEAN\n    fig, axes = plt.subplots(cols, 4, figsize=(12, cols * 3))\n    for ax, img, img_cls in zip(axes.flatten(), images, classes.numpy()):\n        ax.imshow(img)\n        ax.set_xticks([])\n        ax.set_xlabel(dataset_classes[img_cls], fontsize=20)\n    plt.tight_layout()\n    plt.show()","71dc39e4":"# show 12 images\nimages, classes = get_batches(dataloader['train'])\nshow_batches(images, classes)","9cc00e3f":"def show_metrics(metrics):\n    plt.figure(figsize=(12, 8))\n    plt.plot(metrics['train'], label='train')\n    plt.plot(metrics['val'], label='val')\n    plt.grid()\n    plt.legend()\n    plt.show()","a1c21923":"def train_model(\n    model: torch.nn.Module,\n    criterion: torch.nn.Module,\n    optimizer: torch.nn.Module,\n    sheduler: torch.nn.Module,\n    epochs: int = 25\n) -> (torch.nn.Module, dict):\n    r\"\"\"Training the model. Returns best model, dictionary of train and validation losses, dictionary of train and validation accuracies.\n    Args:\n        model (torch.nn.Module): Neural network\n        criterion (torch.nn.Module): Cost function\n        optimizer (torch.nn.Module): Optimization algorithm\n        sheduler (torch.nn.Module): Learning rate change policy\n        epochs (int): Number of training iterations. Default: 25\n    \"\"\"\n    \n    losses = {'train': [], 'val': []}\n    accuracies = {'train': [], 'val': []}\n    best_model_weights = model.state_dict()\n    best_accuracy = 0.\n    progress = tqdm(range(epochs), desc='Epoch:')\n    \n    for epoch in progress:\n        for phase in ['train', 'val']:\n            loss_accum = 0.\n            corr_accum = 0\n            model.train(mode=(phase == 'train'))\n            \n            for inputs, labels in tqdm(dataloader[phase], desc=f'Phase {phase}:'):\n                inputs = inputs.to(DEVICE)\n                labels = labels.to(DEVICE)\n            \n                if phase == 'train':\n                    optimizer.zero_grad()\n                    outp = model(inputs)\n                    loss = criterion(outp, labels)\n                    loss.backward()\n                    optimizer.step()\n                else:\n                    with torch.no_grad():\n                        outp = model(inputs)\n                        loss = criterion(outp, labels)\n                \n                preds = torch.argmax(outp, -1)\n                loss_accum += loss.item()\n                corr_accum += (preds == labels.data).sum()\n\n            if phase == 'train':\n                sheduler.step()\n                \n            epoch_loss = loss_accum \/ dataset_size[phase]\n            epoch_accuracy = corr_accum \/ dataset_size[phase]\n            losses[phase].append(epoch_loss)\n            accuracies[phase].append(epoch_accuracy)\n            progress.set_description('loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_accuracy))\n            if phase == 'val' and epoch_accuracy > best_accuracy:\n                best_accuracy = epoch_accuracy\n                best_model_weights = model.state_dict()\n    \n    model.load_state_dict(best_model_weights)\n    return model, losses, accuracies","06633f37":"model = torchvision.models.alexnet(pretrained=True)","92cff778":"model","07ae26e5":"num_in_features = 9216\nnum_out_features = 2\n\nmodel.classifier = torch.nn.Linear(num_in_features, num_out_features)\nmodel.to(DEVICE)\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nsheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nmodel, losses, accuracies = train_model(model, loss, optimizer, sheduler)","3ec6701a":"show_metrics(losses)","61c9c4e7":"show_metrics(accuracies)","3475468c":"max(accuracies['val'])","7f256ea1":"model = torchvision.models.alexnet(pretrained=True)","ff992c7a":"# by default, all layers of the neural network are retrained\nfor name, param in model.named_parameters():\n    print(f'{name:30}{param.requires_grad}')","39153757":"for param in model.parameters():\n    param.requires_grad = False\nmodel.classifier = torch.nn.Linear(num_in_features, num_out_features)\nmodel.to(DEVICE)\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nsheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nmodel, losses, accuracies = train_model(model, loss, optimizer, sheduler)","49f1cdd4":"show_metrics(losses)\nshow_metrics(accuracies)\nmax(accuracies['val'])","5b376f1a":"model = torchvision.models.alexnet(pretrained=True)","2098e631":"layers_to_freeze = 6\nfor i, (name, param) in enumerate(model.features.named_parameters()):\n    if i < layers_to_freeze:\n        param.requires_grad = False\n    print(f'{name:30}{param.requires_grad}')\nmodel.classifier = torch.nn.Linear(num_in_features, num_out_features)\nmodel.to(DEVICE)\nloss = torch.nn.CrossEntropyLoss()\nretrain_layers = list(model.features.parameters())[layers_to_freeze:] + list(model.classifier.parameters())\noptimizer = torch.optim.Adam(retrain_layers, lr=1e-4)\nsheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nmodel, losses, accuracies = train_model(model, loss, optimizer, sheduler)","31fcd373":"show_metrics(losses)\nshow_metrics(accuracies)\nmax(accuracies['val'])","a2396502":"model = torchvision.models.alexnet(pretrained=True)","b4c26ce2":"model.classifier = torch.nn.Linear(num_in_features, num_out_features)\nmodel.to(DEVICE)\nloss = torch.nn.CrossEntropyLoss()\noptimizer_params = [\n    {'params': model.features.parameters(), 'lr': 1e-5},\n    {'params': model.classifier.parameters()},\n]\noptimizer = torch.optim.Adam(optimizer_params, lr=1e-4)\nsheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nmodel, losses, accuracies = train_model(model, loss, optimizer, sheduler)","df0f5059":"show_metrics(losses)\nshow_metrics(accuracies)\nmax(accuracies['val'])","ef229daf":"model_vgg16 = torchvision.models.vgg16(pretrained=True)","701b50ee":"# model_vgg16","8cff93ab":"model_incept = torchvision.models.inception_v3(pretrained=True)","831ef1f9":"# model_incept","3f5ead3b":"model_incept.AuxLogits.fc = torch.nn.Linear(768, 2)\nmodel_incept.fc = torch.nn.Linear(2048, 2)\nmodel_incept.aux_logits = False","fa0d8539":"model_resnet50 = torchvision.models.resnet50(pretrained=True)","bf37b81a":"#model_resnet50","3c0aa36a":"!pip install efficientnet_pytorch","b95bda73":"from efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b2')","83d94697":"# Efficient Net\n\n[EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling](https:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html)","24ea91a4":"## Net as Feature Extractor\n\n1. freeze Feature Extractor so that no backpropagation is applied to them\n2. replace the classifier","2e761aca":"# VGGNet\n\n[Review: VGGNet \u2014 1st Runner-Up (Image Classification), Winner (Localization) in ILSVRC 2014](https:\/\/medium.com\/coinmonks\/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11)","5622909b":"## Inception v.3\n\n[A Simple Guide to the Versions of the Inception Network](https:\/\/towardsdatascience.com\/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n\nNB! Reinitialize dataloaders to 299 x 299 size.","6f562e51":"# Resnet","6793a07e":"# AlexNet\n\n![AlexNet](https:\/\/i.stack.imgur.com\/DIQWD.png)\n\nAlexNet is the name of a convolutional neural network architecture, designed by Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton.\n\nAlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. \n\n[The original paper](https:\/\/proceedings.neurips.cc\/paper\/2012\/file\/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)","bff9c3fc":"See you later!","e5e4816e":"# Load data","2cea21f6":"Using the writing learning loop and visualization features, you can explore more modern networks on your own. I'll leave some gaps for you to experiment with.\nIf you want me to finish the article, write me in the comments.\n+1 notepad if you like it.\n\nLinks:\n\n* [TORCHVISION.MODELS](https:\/\/pytorch.org\/vision\/stable\/models.html)","d6ec59fe":"# ResNeXt\n\n* [A Tutorial on Filter Groups (Grouped Convolution)](https:\/\/blog.yani.ai\/filter-group-tutorial\/)\n* [Original paper](https:\/\/arxiv.org\/pdf\/1611.05431.pdf)","a205d40c":"Look, in_features for the classifier is 9216. We have to do the same.\n\n## Retrain","7a09554c":"We will slightly transform the input images so that the network does not learn them, but highlights common features. For this we use transforms.","0fe42619":"# Intro\n\n**Transfer learning** is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.\n\nHow you can use already trained neural networks to apply them in your work? I will show using this kernel as an example, using PyTorch and various Convolutional Neural Networks (CNNs), such as Alexnet, VGG, Inception etc.\n\nAny CNN network consists of a feature extraction part (convolution layers + max pooling layers) and a classifier (fully connected layers). How CNN works your may look at kernel [How CNN works: Feature Maps visualization](https:\/\/www.kaggle.com\/imcr00z\/how-cnn-works-feature-maps-visualization)\n\nThere are several approaches to solving the problem:\n\n1. Prepare the net again: describe the structure, set random weights and train - for a very long time.\n2. Use an already trained network. If our datasets overlap very much, we will get a good result, otherwise, alas, we will get low accuracy.\n3. CNN as a feature extraction tool. Since important commonalities are highlighted on convolutional layers, we can remove the classifier (for example, trained in 1000 classes, in the case of ImageNet) and train our own (for the number of classes we need).\n4. Fine tuning. We can not only replace and train the classifier, but also refine the information at several levels of the extractor so that it better matches our dataset (as long as there is power and time). It is better not to touch the first layers, because the most frequent signs stand out there and this will take a long time.\n\nFor each network, we get a trained model and compare the loss and accuracy on [hymenoptera dataset](https:\/\/www.kaggle.com\/ajayrana\/hymenoptera-data):\n\n* Using a ready-made neural network \"out of the box\".\n* Using a neural network as a **Feature Extractor**.\n* **Fine Tuning** for neural networks.","9e343111":"Show some images.","8353191c":"## Fine Tuning","9298119b":"## Tune optimizer\n\nWe can not freeze the layers, but make them smaller learning rate."}}