{"cell_type":{"95021063":"code","98427c75":"code","71169c2d":"code","43b4cd19":"code","4e55ab14":"code","30ea2250":"code","db9fe6ca":"markdown","72ec9ef3":"markdown","d2326e9d":"markdown"},"source":{"95021063":"# (1) define the model function\n# to build Grid of GLM models and Standardization + CrossValidation\n\nimport sklearn.metrics as metrics\nimport pandas as pd\nfrom plotnine import *\nfrom plotnine.data import meat\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, Imputer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport mlflow\nimport mlflow.spark\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.linalg import Vectors\n\n# setting the parameters\nmaxIter = 10\nelasticNetParam = 0\nregParam = 0.3\n  \n  ## we start with mlflow.start_run() which essentially start tracking what we are doing in this notebook in databricks\nwith mlflow.start_run():\n    labelCol = \"default_loan\"\n    indexers = list(map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = \"keep\"), categoricals))\n    ohes = list(map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"), categoricals))\n    imputers = Imputer(inputCols = numerics, outputCols = numerics)\n    featureCols = list(map(lambda c: c+\"_class\", categoricals)) + numerics\n    model_matrix_stages = indexers + ohes + \\\n                          [imputers] + \\\n                          [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), \\\n                           StringIndexer(inputCol= labelCol, outputCol=\"label\")]\n    \n    scaler = StandardScaler(inputCol=\"features\",\n                            outputCol=\"scaledFeatures\",\n                            withStd=True,\n                            withMean=True)\n    \n    ## here, we build the logistic regression model with parameters equal to variables for elasticNet regression\n    lr = LogisticRegression(maxIter=maxIter, elasticNetParam=elasticNetParam, regParam=regParam, featuresCol = \"scaledFeatures\")\n    \n    ##now, we define a pipline which includes everything from standardazing the data, imputing missing values and encoding for categorical columns\n    pipeline = Pipeline(stages=model_matrix_stages+[scaler]+[lr])\n    \n    glm_model = pipeline.fit(train)\n    \n    ## Log Params and Model\n    ## The important part for mlflow of model tracking and reproduceability of the input parameters that we may want to review and take an action.  \n    mlflow.log_param(\"algorithm\", \"SparkML_GLM_regression\") # we put a name for the algorithm that we used\n    mlflow.log_param(\"regParam\", regParam)\n    mlflow.log_param(\"maxIter\", maxIter)\n    mlflow.log_param(\"elasticNetParam\", elasticNetParam)\n    mlflow.spark.log_model(glm_model, \"glm_model\")           # here we log the model itself\n    \n    ##Evaluate and Log ROC Curve\n    lr_summary = glm_model.stages[len(glm_model.stages)-1].summary\n    roc_pd = lr_summary.roc.toPandas()\n    fpr = roc_pd[\"FPR\"]\n    tpr = roc_pd[\"TPR\"]\n    roc_auc = metrics.auc(roc_pd[\"FPR\"], roc_pd[\"TPR\"])\n   \n    ## Set Max F1 Threshold  (for predicting the loan default with a balance between true-positives and false-positives)\n    fMeasure = lr_summary.fMeasureByThreshold\n    maxFMeasure = fMeasure.groupBy().max(\"F-Measure\").select(\"max(F-Measure)\").head()\n    madFMeasure = maxFMeasure[\"max(F-Measure)\"]\n    fMeasure = fMeasure.toPandas()\n    bestThreshold = float ( fMeasure[ fMeasure[\"F-Measure\"] == maxFMeasure] [\"threshold\"])\n    lr.setThreshold(bestThreshold)\n    \n    \n     ## Evaluate and Log Metrics  (here we score the customers)\n    def extract(row):\n      return (row.remain,) + tuple(row.probability.toArray().tolist()) + (row.label,) + (row.prediction,)\n\n    def score(model,data):\n      pred = model.transform(data).select(\"remain\", \"probability\", \"label\", \"prediction\")\n      pred = pred.rdd.map(extract).toDF([\"remain\", \"p0\", \"p1\", \"label\", \"prediction\"])\n      return pred\n\n    def auc(pred):\n      metric = BinaryClassificationMetrics(pred.select(\"p1\", \"label\").rdd)\n      return metric.areaUnderROC\n    \n   \n    glm_train = score(glm_model, train)\n    glm_valid = score(glm_model, valid)\n    \n    glm_train.registerTempTable(\"glm_train\")\n    glm_valid.registerTempTable(\"glm_valid\")\n    \n    print( \"GLM Training AUC :\" + str( auc(glm_train)))\n    print( \"GLM Validation AUC :\" + str(auc(glm_valid)))\n    \n    ## here we log the auc values and the area under the curve for the models metrics as we defined before for training as well as validation dataset\n    mlflow.log_metric(\"train_auc\", auc(glm_train))\n    mlflow.log_metric(\"valid_auc\", auc(glm_valid))","98427c75":"pandas_df = glm_valid.toPandas()\ntxt = 'This table represents the \"CONFUSION MATRIX\" from Ridge Regression'\nprint(txt.title())\npd.crosstab(pandas_df.label, pandas_df.prediction, values=pandas_df.remain, aggfunc=\"count\").round(2)","71169c2d":"pandas_df_sum_net = glm_valid.groupBy(\"label\", \"prediction\").agg((sum(col(\"remain\"))).alias(\"sum_net\")).toPandas()\r\ntxt = 'This table represents the \"SUM NET\" from Ridge Regression'\r\nprint(txt.title())\r\npd.crosstab(pandas_df_sum_net.label, pandas_df_sum_net.prediction, values=pandas_df_sum_net.sum_net , aggfunc=\"sum\").round(2)","43b4cd19":"# (1) define the model function\n# to build Grid of GLM models and Standardization + CrossValidation\n\nimport sklearn.metrics as metrics\nimport pandas as pd\nfrom plotnine import *\nfrom plotnine.data import meat\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, Imputer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport mlflow\nimport mlflow.spark\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.linalg import Vectors\n\n# setting the parameters\nmaxIter = 10\n\n\n\n  \n  ## we start with mlflow.start_run() which essentially start tracking what we are doing in this notebook in databricks\nwith mlflow.start_run():\n    labelCol = \"default_loan\"\n    indexers = list(map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = \"keep\"), categoricals))\n    ohes = list(map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"), categoricals))\n    imputers = Imputer(inputCols = numerics, outputCols = numerics)\n    featureCols = list(map(lambda c: c+\"_class\", categoricals)) + numerics\n    model_matrix_stages = indexers + ohes + \\\n                          [imputers] + \\\n                          [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), \\\n                           StringIndexer(inputCol= labelCol, outputCol=\"label\")]\n    \n    scaler = StandardScaler(inputCol=\"features\",\n                            outputCol=\"scaledFeatures\",\n                            withStd=True,\n                            withMean=True)\n    \n    ## here, we build the logistic regression model with parameters equal to variables for elasticNet regression\n        \n    lr = LogisticRegression(maxIter=maxIter, featuresCol = \"scaledFeatures\")\n        \n   \n    # Create parameter grid\n       \n    params = ParamGridBuilder() \\\n             .addGrid(lr.regParam, [0.3]) \\\n             .addGrid(lr.elasticNetParam, [0.0]) \\\n             .build()\n    \n    \n    ##now, we define a pipline which includes everything from standardazing the data, imputing missing values and encoding for categorical columns\n    pipeline = Pipeline(stages=model_matrix_stages+[scaler]+[lr])\n    cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=BinaryClassificationEvaluator(), numFolds=5)\n    glm_model = cv.fit(train)\n   \n   \n    ## Log Params and Model\n    ## The important part for mlflow of model tracking and reproduceability of the input parameters that we may want to review and take an action.  \n    mlflow.log_param(\"algorithm\", \"SparkML_GLM_regression\") # we put a name for the algorithm that we used\n\n        \n     ## Evaluate and Log Metrics  (here we score the customers)\n    def extract(row):\n      return (row.remain,) + tuple(row.probability.toArray().tolist()) + (row.label,) + (row.prediction,)\n\n    def score(model,data):\n      pred = model.transform(data).select(\"remain\", \"probability\", \"label\", \"prediction\")\n      pred = pred.rdd.map(extract).toDF([\"remain\", \"p0\", \"p1\", \"label\", \"prediction\"])\n      return pred\n\n    def auc(pred):\n      metric = BinaryClassificationMetrics(pred.select(\"p1\", \"label\").rdd)\n      return metric.areaUnderROC\n    \n   \n    glm_train = score(glm_model, train)\n    glm_valid = score(glm_model, valid)\n    \n  #  glm_train.registerTempTable(\"glm_train\")\n  #  glm_valid.registerTempTable(\"glm_valid\")\n    \n    print( \"GLM Training AUC :\" + str( auc(glm_train)))\n    print( \"GLM Validation AUC :\" + str(auc(glm_valid)))\n    \n    ## here we log the auc values and the area under the curve for the models metrics as we defined before for training as well as validation dataset\n    mlflow.log_metric(\"train_auc\", auc(glm_train))\n    mlflow.log_metric(\"valid_auc\", auc(glm_valid))\n","4e55ab14":"pandas_df = glm_valid.toPandas()\ntxt = 'This table represents the \"CONFUSION MATRIX\" from GLM models and Standardization with CrossValidation'\nprint(txt.title())\npd.crosstab(pandas_df.label, pandas_df.prediction, values=pandas_df.remain, aggfunc=\"count\").round(2)\n\n","30ea2250":"pandas_df_sum_net = glm_valid.groupBy(\"label\", \"prediction\").agg((sum(col(\"remain\"))).alias(\"sum_net\")).toPandas()\r\ntxt = 'This table represents the \"SUM NET\" from GLM models and Standardization with CrossValidation'\r\nprint(txt.title())\r\npd.crosstab(pandas_df_sum_net.label, pandas_df_sum_net.prediction, values=pandas_df_sum_net.sum_net , aggfunc=\"sum\").round(2)","db9fe6ca":"Links: \n*[ResearchGate](https:\/\/www.researchgate.net\/profile\/Afshin-Ashofteh-2)\n*[Kaggle](https:\/\/www.kaggle.com\/aashofteh)\n*[Google Scholar](https:\/\/scholar.google.com\/citations?user=oIa1W0gAAAAJ&hl=en)\n*[Data Science Discussion Group](https:\/\/www.linkedin.com\/groups\/12420006)\n*[email](aashofteh@novaims.unl.pt)","72ec9ef3":"# Machine Learning Codes for Credit Scoring - Ridge regression !\n\nFor citation: https:\/\/doi.org\/10.1016\/j.eswa.2021.114835 (Journal - Expert Systems with Applications.)\n\nAsk for full-text in [ResearchGate](https:\/\/www.researchgate.net\/profile\/Afshin-Ashofteh-2)\n\nAfshin Ashofteh [email](aashofteh@novaims.unl.pt)\n\nSubject: Credit Risk and Credit Scoring.\n\nDatasource: loan.csv - Each loan includes applicant information provided by the applicant as well as current loan status (Current, Late, Fully Paid, etc.) and latest payment information.\n\n","d2326e9d":"# Ridge regression "}}