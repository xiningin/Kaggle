{"cell_type":{"74ac5467":"code","2f7eb4a5":"code","bfd8d338":"code","3402e2b6":"code","1dd042e7":"code","b2e1a211":"code","92172df6":"code","d2fc022a":"code","3bf16419":"code","63f249d4":"code","196cfaeb":"code","c06634d3":"code","c1a938b6":"code","14629404":"code","b85b9f56":"code","d9ac3d2d":"code","1be53a00":"code","76aacc78":"code","8d5164a0":"code","ce728cdb":"code","24e0d235":"code","3dacd5c9":"code","8ea770f8":"code","d4039f21":"code","497e8399":"code","3064386d":"code","24b58688":"code","be8daaf5":"code","23e28735":"code","07a95d93":"code","f2af9621":"code","b7887d0c":"markdown","33348e20":"markdown","b058f14d":"markdown","29e3de1d":"markdown","86836551":"markdown","ce2a2260":"markdown","bf641fe2":"markdown","d4d0585f":"markdown","1c51322b":"markdown","1e150d4e":"markdown","bb846d1b":"markdown","48a4c199":"markdown","6174e3de":"markdown","3050881b":"markdown","bc6700f0":"markdown","b13e43de":"markdown","6f8d91f0":"markdown","eb12342f":"markdown","b1fe9ef4":"markdown","b33fda06":"markdown","7010c59d":"markdown","6a0b52da":"markdown","e4d962c5":"markdown","11757b33":"markdown","80612af4":"markdown","7d5cb4cb":"markdown","334f18a7":"markdown","9c6b57d5":"markdown","f56da1f6":"markdown","0289d7a1":"markdown","3038f091":"markdown","80858620":"markdown","61a8a6b4":"markdown","13cd81b4":"markdown"},"source":{"74ac5467":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","2f7eb4a5":"raw_df = pd.read_csv('..\/input\/closing-price-of-indexes-time-series-data\/Index closing price from 1994 to 2021.csv')\ndf = raw_df.copy()","bfd8d338":"df.head()","3402e2b6":"df.describe()","1dd042e7":"df.isnull().sum()","b2e1a211":"plt.figure(figsize=(20,5))\ndf.spx.plot(title = 'spx price')\nplt.show()","92172df6":"plt.figure(figsize=(20,5))\ndf.ftse.plot(title = 'ftse price')\nplt.show()","d2fc022a":"import scipy.stats\nimport pylab","3bf16419":"scipy.stats.probplot(df['spx'] , plot = pylab)\nplt.show()","63f249d4":"df.head()","196cfaeb":"df.info()","c06634d3":"df['Date'] = pd.to_datetime(df['Date'] , dayfirst=True)","c1a938b6":"df.info() , df.head()","14629404":"df.set_index(\"Date\", inplace=True)\ndf.head()","b85b9f56":"df = df.asfreq('d')\ndf.head()","d9ac3d2d":"df = df.asfreq('b')\ndf.head()","1be53a00":"df.isna().sum()","76aacc78":"df = df.fillna(method = 'ffill')","8d5164a0":"df['Market_Value'] = df['spx']","ce728cdb":"\ndel df['spx'] , df['dax'] , df['ftse'] , df['nikkei']","24e0d235":"df","3dacd5c9":"from sklearn.model_selection import train_test_split\ndf_train,df_test = train_test_split(df,test_size=0.2,shuffle=False)","8ea770f8":"df_train","d4039f21":"df_test","497e8399":"wn = np.random.normal(loc = df.Market_Value.mean(), scale = df.Market_Value.std(), size = len(df))\ndf['wn'] = wn\ndf.wn.plot(figsize = (20,5))\nplt.title(\"White Noise Time-Series\", size= 24)\nplt.show()","3064386d":"import statsmodels.tsa.stattools as sts\nsts.adfuller(df.Market_Value)","24b58688":"import statsmodels.tsa.stattools as sts\nsts.adfuller(df.wn)","be8daaf5":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ns_dec_multiplicative = seasonal_decompose(df.Market_Value, model = \"multiplicative\")\ns_dec_multiplicative.plot()\nplt.show()","23e28735":"import statsmodels.graphics.tsaplots as sgt\nsgt.plot_acf(df.Market_Value , lags = 40 , zero = False)\nplt.title('Autocorrelation')\nplt.show()","07a95d93":"import statsmodels.graphics.tsaplots as sgt\nsgt.plot_acf(df.wn , lags = 40 , zero = False)\nplt.title('Autocorrelation')\nplt.show()","f2af9621":"import statsmodels.graphics.tsaplots as sgt\nsgt.plot_pacf(df.Market_Value , lags = 40 , zero = False,method='OLS')\nplt.title('PACF')\nplt.show()","b7887d0c":"# End of Time Series Analysis in Python : Part 1\n\nAs this Notebook is already big enough to follow in one go, i will be publishing remaining notes in other notebook called Time Series Analysis in Python : Part 2.\n\nAlso sorry for any spelling mistake or bad grammer.\n\nHope this notebook added some value in you work\/study. Thanks for reading.\n\nPlease let me know if there is any correction required in this notebook.\n\nHappy Coding !!","33348e20":"We use Pandas to_datetime() function to achieve this. [You can find detailed explanation of Working with Date and Time in Python in this Notebook.](https:\/\/www.kaggle.com\/omkarborikar\/working-with-date-and-time-in-python)","b058f14d":"If we check ACF for White Noise then we would get completely different graph describing no auto correlation.","29e3de1d":"In order to fill this NaN values , we can use ffill  , bfill or mean ( There are lots of other ways we can fill the NaN data ).\n\nGenerally , mean is not used to fill NaN values in Time Series analysis because there are underlying time variant patterns in the data.\n\nffill - If value for 8-may is missing , then we we fill it by value of 7-may.\n\nbfill - If value for 8-may is missing , then we we fill it by value of 9-may.","86836551":"# Basic Data Analysis ","ce2a2260":"From above cell we can see that some extra rows are addedd in the dataset like 2nd and 3rd row with all NaN values.\n\nIt is because market remains close on saturday and sunday and hence there is no data recorded.\n\nIn this case we can use 'b' as argument in asfreq() which stands for 'business day'.","bf641fe2":"# Decomposition of Time Series into Trend, Seasonality and Residual\n\nTrend - describes whether the time series is decreasing, constant, or increasing over time.\n\nSeasomality - Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every specific time period.\n\nResidual\/Noise - describes what remains behind the separation of seasonality and trend from the time series. In other words, it\u2019s the variability in the data that cannot be explained by the model.\n\n\n* Additive Decomposition - \n\n    Observed value = Trend + Seasonality + Residual\n\n* Multiplicative Decomposition - \n\n    Observed value = Trend x Seasonality x Residual\n    \n Seasonal_Decomposition is used to check if time series has any seasonality or check which type of trend is present in Time Series.","d4d0585f":"For simplicity , lets consider data of SPX only and name this column as Market_Value.","1c51322b":"# Converting Dataset into a Time Series","1e150d4e":"# What is Time series Data ?\n\n* It is a sequence of information which attaches a time period to each value. Value can be anything that can be measured over time like price , humidity etc.\n\n* Unit of time for which data is measured can be anything like seconds , date , year oe even decades.\n\n* Patterns observed inn time-series are most likely to persist in the future. That's why it is used to predict future values.\n\n* In time series , intervals between 2 observations is called as **Time Period**. Time Period (Interval between 2 observations) must be equal throughout the dataset.    If that is not the case , then it is possible that data contains missing values.\n\n* In Time Series , we cannot split train and test data randomly by shuffling as , time series data is chronological. I.e it is dependant on the previous observation. Hence we split train and test data bu simply cutting of one portion of data to train and other to test as shown below.\n","bb846d1b":"# Splitting Dataset into Train and Test set.\n\nAs discussed, we cannot shuffle and split data because, Time Series data relies on keeping the chronological order of the values.\n\nSince we cannot shuffle data , training set must containg data starting from beginning to some specific point while the testing set contains rest.","48a4c199":"# Autocorrelation in Time Series\n\nAutocorrelation is simply correlation of time sries with lagged version of same time seires.\n\n**The Autocorrelation Function** is used to find and plot Autocorrelation of Time Series\n\nacf(time_series_name , lags =  , zero = )\n\n   Here , lags parameter describes the number of lags over which ACF is calculated . first lag can be t+1 , sencond would be t+2 etc. Default va;ue of this parameter      is length of time series.\n   \n   Zero = False indicates whether we include corrent value index in graph. This is because correlation with itself wi;; always be 1.","6174e3de":"From below we can see that date column has data type **String** and not **datetime**. In order to work with dates it is important to convert this string values to date objects.","3050881b":"# Handling Missing Values","bc6700f0":"# Partial Autocorrelation in Time series","b13e43de":"We can identify if Time series is stationry or not by looking at the plot. But, there is mathematical method to prove if time series is stationary or not.\n\nThis method is called ADF (Augmented Dicky Fuller)\n\n* This method works on rejecting Null Hypothesis.\n\n* Here Null Hypothesis is that, given time series is not stationary.\n\n* We reject Null Hypothesis if, test statistic < critical statistic.\n\n* adfuller() method returns -\n        \n1. test statistic (3.1239181959766964)\n\n2. P value describing if time series is Stationary or not (Greater the value , higher chances of Time series being Non stationary. 0 means Time series is                  stationary. 1 means Time series is not stationary) (1.0)\n\n3. Number of lags. Zero value of this parameter suggests that Time Series is stationary (35)\n\n4. Number of datapoints considered. (7219)\n\n5. 1% 3% and 5% critical values(Significance values). These values are compared with test statistic value (1st value) \n   ({'1%': -3.431256167997925,\n   '5%': -2.8619404553644747,\n   '10%': -2.566983158209179})\n\n6. Lower this value easier it is to make future predictions.( 62737.6283516755)","6f8d91f0":"QQ plot - Usually describes how the data fits Normal distribution.","eb12342f":"# What is Stationariity ?\n\nThe Time series is said to be stationary if it satisfies below 3 assumptions -\n\n1. Mean is constant\n2. Standard deviation is constant\n3. Consistent covariance between periods which are at an identical distance from one another.\n        \n**As White noise follows all three assumptions , Every white Noise is Stationary time series but every stationary time series is not white noise time series.**","b1fe9ef4":"* We want this date column to be used as index of the dataset . Doing that will make it easy to select data for perticular time period or date. \n\n* To do this we use set_index() method in which we pass column name .","b33fda06":"The dataset used in this Notebook can be found here -  [Closing price of Top Indexes | Time Series Data |](https:\/\/www.kaggle.com\/omkarborikar\/closing-price-of-indexes-time-series-data)","7010c59d":"As we know that, White Noise is stationary time series lets use this method on white noise column 'wn' from out dataset.\n\nWe can see that -\n\n* Test statistic (-84.03687594361077) < critical\/significance values (-3.4312517942574)\n* P-value is zero\n* Value of 3rd parameter , which represents number of lag where correlation occured is also zero.","6a0b52da":"The QQ graph below contains - \n\nY-axis -> Open price for SPX index.\nX-axis -> How much standard deviation away from the mean this value are.\nRed Line -> Represents what data points should follow if they are Normally distributed.\nBlue Line -> Actual distribution of the data points (Open price of SPX here)\n\nAs from below graph we can see that data is not Normally distributed as there are more data points with value = 500. So we cannot use statistical method to predict that we use on Normally distributed data.","e4d962c5":"As we have analysed the data we have , lets begin to work on it to create Time Series out of it.\n\nLets print first five rows of dataset.","11757b33":"As can be seen from above graph , Only first lag has same result as ACF. After that in ACF graph previous lag values are considered while calculating correlation but in PACF only direct correlation is calculated.","80612af4":"As we can see from below , there are some missing values present in this dataset.\n\nAs we checked earlier in this notebook , there were no Null values in the dataset. But after setting the frequency , we got 8 rows that is 8 days where data is not recorded.\n\nHence after setting the frequency it is important to check for NaN values","7d5cb4cb":"Why to use Partial Autocorrelation function and what are the limitations of ACF ?\n\nIn ACF when we calculate autocorrelation between time series and its lagged versoin, other lagged version also influence the correlation. \n\nFor e.g, if we calculate correlation between price on 29th Oct and 28th Oct price , this correlation will get affected by 27th Oct price because 28th Oct price depends upon 27th Oct price and so on.\n\nIn Partial Autocorrelation Function (PACF), we calculate direct correlation between lagged version and actual time series.","334f18a7":"Each row in dataset represents date and closing price of indexes spx , dax , ftse and nikkel on that date.","9c6b57d5":"From below we can see that datatype of date column is now changed to datetime","f56da1f6":"# What is White Noise ?\n\n* Time series data are expected to contain some white noise component on top of the signal generated by the underlying process.\n\n* y(t) = signal + Noise . where y(t) is prediction. Signal is input signal.\n\n* From above expression we get value of Noise as - Noise = y(t) - signal. If this Noise is White Noise then our model which is used to predict time series cannot be improved more and if it is not White Noise then there is some rrom for improvement.\n\n* Residual (y(t) - signal) is white Noise if it follows all three points mentioned below - \n\n    1. It has zero mean\n    2. Standard variation is constant over time period.\n    3. There is no correlation between time series and its lagged version.\n\n        \n* Found this great video on Youtube which explains White Noise in Most simpler way - [Explanation of white noise]()\n\n**In Short , Time series is White Noise if we cannot make Future predictions out of it.**","0289d7a1":"In above cell , we have selected random points from Normal\/Gaussian distribution.(size = length of dataframe) . From above plot we can see that, Mean is constant , standard variation is also constant and there is no corelation between time series and its lagged version.","3038f091":"Basic notations in Time Series -\n\nT - Entire time period of the dataset \n\nt - specific value of time in the dataset\n\nFor exaample we have a dataset of Daily open price for google stock for year 2010 , lets call this dataset as X.\n\nX = Daily open price for Google stock for year 2010\n\nT = Entire year\n\nt = Single day","80858620":"# What is Random Walk ?\n\n* A random walk is different from a list of random numbers because the next value in the sequence is a modification of the previous value in the sequence.\n\n* A simple model of a random walk is as follows:\n\n  Start with a random number of either -1 or 1.\n  Randomly select a -1 or 1 and add it to the observation from the previous time step.\n  Repeat step 2 for as long as you like.","61a8a6b4":"In above plot , \n\n* values on X-axis represents lags (1 to 40)\n\n* Values on Y-axis represents possible values of Autocorrelation coefficient. Value of this parameter ranges from -1 to 1\n\n* The blue shaded region in the plot is the significance level. The lags that lie above the blue line are the significant lags. As lag is increased , significance level also increases. This because , there can be high correlation between todays price and price before 1 day, but that wouldn't be true for price 1 moth ago.\n\n* In above graph , as Autocorrelation hardly diminishes over 40 lags, we can say that prices 1 month ago still affects todays prices.","13cd81b4":"# Setting the Desired Frequency\n\nNow we have to set the Frequency of data in this dataset and that can done using **asfreq**() function.\n\nSyntax of this function is as follows = \n\ndf = df.asfreq()\n\nThis function take alphbets as arguments. For e.g 'h' means hourly , 'w' - weekly , 'd' - daily , m - monthly , a - 'annualy' etc\n\nFor this dataset we ave daily opening price for indexes , so we will use 'd'."}}