{"cell_type":{"4b50753c":"code","8e28f503":"code","7c7cac51":"code","9df7e6fc":"code","7423db54":"code","7a7e32eb":"code","290d25be":"code","0903cf6d":"code","c4545d1f":"code","ec4d0776":"code","756a9ef3":"code","27507594":"code","9f6314b5":"code","35861c44":"code","08e30ef0":"code","b347aeb3":"code","f27b264b":"code","7b7db44b":"code","9606d2e7":"code","48a62223":"code","c859d34e":"code","bd5c6984":"code","5bbc7863":"code","fc3c975e":"code","e72ad5e0":"code","1490b12b":"code","8bc97522":"code","118a7ef8":"code","b9fd2520":"code","346867a6":"code","d5a50108":"code","823ee660":"code","cdc52473":"code","bab63933":"markdown","d0092a17":"markdown","e907a041":"markdown","4a57a6d1":"markdown","a4960bfb":"markdown","a636fa33":"markdown","30ed0140":"markdown","025275fc":"markdown","54d3afcb":"markdown","c838e131":"markdown"},"source":{"4b50753c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as pl\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8e28f503":"train_identity = pd.read_csv(\"..\/input\/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"..\/input\/train_transaction.csv\")\ntest_identity = pd.read_csv(\"..\/input\/test_identity.csv\")\ntest_transaction = pd.read_csv(\"..\/input\/test_transaction.csv\")","7c7cac51":"train_transaction.head()","9df7e6fc":"print(train_transaction.info())","7423db54":"train_transaction.isnull().sum()","7a7e32eb":"#Let's merge our data sets for Future!\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","290d25be":"clean_df = train_transaction.dropna(axis=1)\nclean_df.head()","0903cf6d":"print(clean_df.shape)\nprint(train_transaction.shape)","c4545d1f":"clean_df.isnull().sum()","ec4d0776":"# Wow, the data looks good to go!\n# let's check \"isFraud\" data balance in respect to our target\ntarget_balancing = clean_df['isFraud'].value_counts().values\nsns.barplot([0,1],target_balancing);\npl.title(\"How much is our target balanced??\");","756a9ef3":"#Let's try seeing the Fraud and non fraud transactions based on Products\nsns.barplot(x = clean_df.index,y = 'ProductCD',hue='isFraud',data = clean_df);","27507594":"pl.figure(figsize=(16,7))\nsns.boxplot(data = clean_df.drop(columns = ['TransactionID','isFraud']))\npl.yscale('log')\npl.xticks(rotation=90);","9f6314b5":"pl.hist(train['TransactionDT'], label='train');\npl.hist(test['TransactionDT'], label='test');\npl.legend();\npl.title('Distribution of transaction dates');","35861c44":"train.head()","08e30ef0":"clean_df.head()","b347aeb3":"pl.figure(figsize=(16,7))\ncor  =clean_df.corr()\nsns.heatmap(cor,annot=True);","f27b264b":"fraudlent = clean_df[clean_df['isFraud']==1]\nnon_fraudlent = clean_df[clean_df['isFraud']==0]\nfraudlent.head()","7b7db44b":"#Let's see the Transaction AMount for the Fraudlent and Non Fraudlent Transactions!\npl.figure(figsize=(16,7))\npl.subplot(1,2,1)\nsns.distplot(fraudlent['TransactionAmt'],rug=True);\npl.title('Transaction Amount for Fraudlent Transactions!');\npl.subplot(1,2,2)\nsns.distplot(non_fraudlent['TransactionAmt'],rug=True);\npl.title('Transaction Amount for Non Fraudlent Transactions!');","9606d2e7":"mean_fraud = fraudlent['TransactionAmt'].mean()\nmean_non_fraud = non_fraudlent['TransactionAmt'].mean()\nprint(mean_fraud)\nprint(mean_non_fraud)","48a62223":"# Correlation for each\ncor_fraud = fraudlent.corr()\ncor_non_fraud = non_fraudlent.corr()\npl.figure(figsize=(20,10))\npl.subplot(2,1,1)\nsns.heatmap(cor_fraud,annot=True)\npl.subplot(2,1,2)\nsns.heatmap(cor_non_fraud,annot=True);","c859d34e":"# We will go on to find the Top 5 Attributes that are highly Correlated fro each of the Category!\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations for Fraudlent Transactions!\")\nprint(get_top_abs_correlations(fraudlent.drop(columns = ['ProductCD','isFraud']), 5))\n\n# I love this peice of code as it makes the Corelation much easier to look!","bd5c6984":"# We will go on to find the Top 5 Attributes that are highly Correlated fro each of the Category!\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations for Non Fraudlent Transactions!\")\nprint(get_top_abs_correlations(non_fraudlent.drop(columns = ['ProductCD','isFraud']), 5))","5bbc7863":"train_identity.head()","fc3c975e":"# Let's get rid of Null values on AXIS = 1\ntrain_identity_clean_df  = train_identity.dropna(axis=0).reset_index(drop=True)\ntrain_identity_clean_df.head()","e72ad5e0":"train_identity_clean_df.isnull().sum()","1490b12b":"# Well I'm thinking of mergign the cleaned Transaction and Identity dataframes as this gives us better view\nmerged_train = pd.merge(clean_df,train_identity_clean_df,on = 'TransactionID',how='left')\nmerged_train.head()","8bc97522":"merged_train = merged_train.dropna(axis = 0).reset_index(drop=True)\nmerged_train.head()","118a7ef8":"merged_train.shape","b9fd2520":"balancing_merge_df = merged_train['isFraud'].value_counts().values\nsns.barplot([0,1],balancing_merge_df);\npl.title('Checking balance of our target in the merged Data Set!');","346867a6":"merged_train['TransactionAmt'].describe()","d5a50108":"pl.figure(figsize=(16,7))\npl.subplot(1,2,1)\npl.plot('TransactionAmt','ProductCD', data=merged_train, marker='o', alpha=0.4)\npl.subplot(1,2,2)\npl.plot('TransactionAmt','ProductCD', data=merged_train, linestyle='none', marker='o', color=\"orange\", alpha=0.3);\npl.title('Different types of Products on Transaction Amount!');","823ee660":"sns.set_style(\"white\")\nsns.kdeplot(merged_train.TransactionAmt, merged_train.card1);","cdc52473":"# top correlating Attributes for the merged Data Frame!\npl.figure(figsize=(20,10))\ncorr_merge = merged_train.corr()\nsns.heatmap(corr_merge,annot=True)\nprint(\"Top Absolute Correlations for Merged Data frame!!\")\nnumeric_df_merged = merged_train.select_dtypes(include=['int'])\nprint(get_top_abs_correlations(numeric_df_merged, 5))","bab63933":"## Next we can go onto create metrics for the Group of Amounts and based on that, we an find the Fraudlent and Non Fraudlent in that Amount Group!\n\n## Till then think and contribute some methods\/explorations and help me in making this kernel better!","d0092a17":"# Let's Try our Data Cleaning!","e907a041":"# Hi,\n\n## We start this Kernel with the basic steps towards the Data Science technology of Exploring the Data Set, and going onto wrangle around with it and build a model as required for the Competition.\n## This is my First ever Kaggle Competition excluding the Titanic Competition. Your support,suggestions and upvotes will get me closer to the right approach to be used.\n\n### Let's Start!","4a57a6d1":"## Now we see that the sahpe is reduced and has been reduced drastically when we tried to merge based on the Valid ID's for which in both the dataframes, Entry was present.\n## We can still fill out the missing values, but for 60 rows, it will be explicitally hard coding.\n### Do let me know if there is any better way to keep the shape intact and be free of Null values or suggest me a better approach!","a4960bfb":"## So we see that the number of rows are intact, whereas the attributes are filtered on their own with null values.\n## Next we can do that is check the missing values again, and try finding the balance in our data set!","a636fa33":"## So we can say from the graph that *fraudlent transactions can be brought in light if the transaction is within the limit of 5000* whereas the *non fraudlents can be considered if the transactions are reasonably above 5.5k*\n \n### Let's chec the mean value to support our hypothesis!","30ed0140":"## The top 5 pair of attributes with High Correlation! for the whole data set are:\n* C8 and C6\n* C1 and C6\n* C1 and C8\n* C8 and C7\n* C6 and C11\n\n## Let's now split the data frame based on Fraudlent and Non Fraudlent and then check for their correlation.\n### This gives us as how many of those attributes still hold true!","025275fc":"## Next is to deal with the outliers and see the correlation.\n## Moving on them we will try to build a model to get the scores of prediction and accuracy.","54d3afcb":"### So we see that on Average a *Fradlent ID can perform 149 Rs. Transaction* but a *Non Fraudlent perform approx 135Rs. transaction!*\n\n## We can use this figure to compare on respective Attribute!","c838e131":"### We have highest number of Fraud Transactions for the Product **S** and **C**.\n## Next we check for outliers in our data frame!"}}