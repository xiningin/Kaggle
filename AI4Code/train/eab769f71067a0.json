{"cell_type":{"9391492c":"code","7911fe84":"code","8942f5da":"code","dcf9a7f3":"code","024d5701":"code","aeed6b53":"code","74981b2b":"code","19e2ce35":"code","1dbf5bcd":"code","10893673":"code","e883e452":"code","3e688ca8":"code","e99ac204":"code","b29445a4":"code","90b6667e":"code","4433475b":"code","22ff8f36":"code","1e728717":"code","7fcc1daf":"code","d0c01e44":"code","c71ef461":"code","9a9c8f39":"code","53e20d51":"code","7f9616d1":"code","aaafa631":"code","fff3234c":"code","ed755a1c":"code","ef3b3248":"code","7290f620":"code","5d6aebd5":"code","a7f4b726":"code","f92e4d44":"code","3405900b":"code","5b82ae6f":"code","1091bdf9":"code","9dbf3fda":"code","11d91801":"code","cfb6c1f2":"code","ac4cc49c":"code","c93f4965":"code","7f574901":"code","226372cc":"code","c7d14efe":"code","a1c548d1":"code","3a1c4de6":"code","fb0e440a":"code","ab477f61":"code","a7dae70e":"markdown","f91b58b8":"markdown","98d17297":"markdown","037f9602":"markdown","4c9e654e":"markdown","46a0f29a":"markdown","6e9dc74c":"markdown","374a029d":"markdown","38de61c8":"markdown","afae53f9":"markdown","8ac4baea":"markdown","0d935631":"markdown","e07feddc":"markdown","b618e4e3":"markdown","7ad98725":"markdown","b8692f75":"markdown","70fb1d38":"markdown","ec33786d":"markdown","31f12a7d":"markdown","c3c4c336":"markdown","db619365":"markdown","f11494ff":"markdown","bd6e5d1d":"markdown","6b9d2973":"markdown","b71b0e3d":"markdown","7ccea275":"markdown","ee3b1733":"markdown","87210274":"markdown","d49a78ab":"markdown","b9f09855":"markdown","68ec9831":"markdown","31f6dd90":"markdown","b78a7a23":"markdown","53844ce6":"markdown","4983bcf4":"markdown","dbc914a2":"markdown","e6050f45":"markdown","1a94a0cc":"markdown","0c0f2f9b":"markdown","fe2a1b70":"markdown","9ae3176b":"markdown","ebd93069":"markdown","113d8784":"markdown","0096d302":"markdown","3c61a161":"markdown","6a53fc82":"markdown"},"source":{"9391492c":"# load some default Python modules\n%matplotlib inline\n\nimport time\n\nfrom sklearn.metrics import mean_squared_error\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode()\n\nfrom math import radians, cos, sin, asin, sqrt\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom fbprophet import Prophet\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')","7911fe84":"train = pd.read_csv(\"..\/input\/train.csv\", nrows = 2_000_000)\n\nprint(\"shape of train data\", train.shape)\ntrain.head()\n","8942f5da":"# datatypes\ntrain.dtypes","dcf9a7f3":"train.describe()","024d5701":"# Checking for valid fare amount\nprint('Old size: %d' % len(train))\ntrain = train.drop(train[train['fare_amount']<2.5].index, axis=0)\nprint('New size after dropping invalid fare amount: %d' % len(train))","aeed6b53":"# check missing data\ntrain.isnull().sum()","74981b2b":"print(\"old size: %d\" % len(train))\ntrain = train.dropna(how='any', axis=0)\nprint(\"New size after dropping missing value: %d\" % len(train))","19e2ce35":"# checking for passanger count\ntrain.passenger_count.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Passanger Count\")\nplt.ylabel(\"Frequency\")","1dbf5bcd":"# checking for passanger count greater than 7\ntrain[train.passenger_count >7].passenger_count.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Passanger Count\")\nplt.ylabel(\"Frequency\")","10893673":"print('Old size: %d' % len(train))\ntrain = train.drop(train[train['passenger_count']>7].index, axis = 0)\ntrain = train.drop(train[train['passenger_count']<1].index, axis = 0)\nprint('New size: %d' % len(train))","e883e452":"# checking for taxi fare\ntrain.fare_amount.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Taxi Fare\")\nplt.ylabel(\"Frequency\")","3e688ca8":"# checking for taxi fare more than 250 USD\ntrain[train.fare_amount >250].fare_amount.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Taxi Fare\")\nplt.ylabel(\"Frequency\")","e99ac204":"print('Old size: %d' % len(train))\ntrain = train.drop(train[train['fare_amount']>250].index, axis = 0)\nprint('New size: %d' % len(train))","b29445a4":"# Lets see the distribution of fare amount less than 100\ntrain[train.fare_amount <100 ].fare_amount.hist(bins=100, figsize = (16,8))\nplt.xlabel(\"Fare Amount\")\nplt.ylabel(\"Number of courses\")","90b6667e":"print('Old size: %d' % len(train))\ntrain = train[(train['pickup_longitude'] >= -74.259090) & (train['pickup_longitude'] <= -73.700272)]\ntrain = train[(train['dropoff_longitude'] >= -74.259090) & (train['dropoff_longitude'] <= -73.700272)]\ntrain = train[(train['pickup_latitude'] >= 40.477399) & (train['pickup_latitude'] <= 40.917577)]\ntrain = train[(train['dropoff_latitude'] >= 40.477399) & (train['dropoff_latitude'] <= 40.917577)]\nprint('New size: %d' % len(train))","4433475b":"print('Old size: %d' % len(train))\ntrain = train[(train['pickup_longitude'] != train['dropoff_longitude']) | (train['pickup_latitude'] != train['dropoff_latitude'])]\nprint('New size: %d' % len(train))","22ff8f36":"train.describe()","1e728717":"test = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"shape of test data\", test.shape)\ntest.head()","7fcc1daf":"#check for missing value\ntest.isnull().sum()","d0c01e44":"# checking for basic stats\ntest.describe()","c71ef461":"# For XGBoost, later\ndef time_features(dataframe):\n    dataframe['pickup_datetime'] = dataframe['pickup_datetime'].astype(str).str.slice(0, 16)\n    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    dataframe['hour_of_day'] = dataframe.pickup_datetime.dt.hour\n    dataframe['month'] = dataframe.pickup_datetime.dt.month\n    dataframe[\"year\"] = dataframe.pickup_datetime.dt.year\n    dataframe[\"weekday\"] = dataframe.pickup_datetime.dt.weekday    \n    return dataframe","9a9c8f39":"# calculate distance between two latitude longitude points haversine formula \n# Returns distance in kilometers\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi\/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)\/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) \/ 2\n    return 12742 * np.arcsin(np.sqrt(a))   # 2*R*asin...","53e20d51":"train['distance_miles'] = distance(train.pickup_latitude, train.pickup_longitude, \\\n                                      train.dropoff_latitude, train.dropoff_longitude)","7f9616d1":"test['distance_miles'] = distance(test.pickup_latitude, test.pickup_longitude, \\\n                                      test.dropoff_latitude, test.dropoff_longitude)","aaafa631":"print(\"Average $USD\/Km : {:0.2f}\".format(train.fare_amount.sum()\/train.distance_miles.sum()))","fff3234c":"# scatter plot distance - fare\nplt.scatter(train.distance_miles, train.fare_amount, alpha=0.2)\nplt.xlabel('distance mile')\nplt.ylabel('fare $USD')\nplt.show()","ed755a1c":"prophet_df = train.iloc[:1000000]","ef3b3248":"prophet_df = prophet_df.reset_index()[[\"pickup_datetime\", \"fare_amount\"]]\nprophet_df.columns = [\"ds\", \"y\"]","7290f620":"prophet_df.head()","5d6aebd5":"prophet_df['ds'] = pd.to_datetime(prophet_df['ds'].sort_values())\nprophet_df['y'] = pd.to_numeric(prophet_df['y'],errors='ignore')\nprophet_df.head()","a7f4b726":"df_train = prophet_df.iloc[:round(len(prophet_df)*0.8)]\ndf_test = prophet_df.iloc[round(len(prophet_df)*0.8):]","f92e4d44":"model = Prophet(changepoint_prior_scale=2.5, daily_seasonality=True)\n\nstart = time.time()\nmodel.fit(df_train)\nprint(\"Fitting duration : {:.3f}s\".format(time.time() - start) )","3405900b":"future_data = df_test.drop(\"y\", axis=1)\nstart = time.time()\nforecast_data = model.predict(future_data)\nprint(\"Predict duration : {:.3f}s\".format(time.time() - start) )","5b82ae6f":"forecast_data[\"y\"] = df_test[\"y\"].values\nforecast_data[['ds', 'y', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","1091bdf9":"py.iplot([\n    go.Scatter(x=df_test['ds'], y=df_test['y'], name='y'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat'], name='yhat'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['trend'], name='Trend')\n])","9dbf3fda":"mse = []\nfor i in range(0, len(forecast_data), 48):\n    mse.append(mean_squared_error(\n                    forecast_data.loc[i:i+48, \"y\"],\n                    forecast_data.loc[i:i+48, \"yhat\"]\n                ))\n\nplt.figure(figsize=(20,12))\nplt.plot(mse) # mse per day during 2 years\nplt.title(\"Evolution of MSE during year 2016 - 2017\")\nplt.show()","11d91801":"model.plot_components(forecast_data)\nplt.show()","cfb6c1f2":"# Calculate root mean squared error.\nprint('RMSE: %f' % np.sqrt(np.mean((forecast_data.loc[:800, 'yhat']-prophet_df['y'])**2)) )","ac4cc49c":"X = train[['distance_miles']]\ny = train[['fare_amount']]\nX['default'] = 1\n","c93f4965":"from sklearn.linear_model import LinearRegression\nmodelRegression = LinearRegression(normalize=True)\nmodelRegression.fit(X,y)\n","7f574901":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\npredictions = modelRegression.predict(X)\nprint(\"RMSE : \" + str(sqrt(mean_absolute_error(predictions, y))))\nprint(\"R2 : \" + str(r2_score(predictions, y)))","226372cc":"plt.plot(X[['distance_miles']], predictions, 'r')\nplt.scatter(train.distance_miles, train.fare_amount, alpha=0.2)\nplt.xlabel('distance mile')\nplt.ylabel('fare $USD')\nplt.show()","c7d14efe":"train = time_features(train)\n\n\nfrom sklearn.model_selection import train_test_split\ny = train.fare_amount\nX = train.drop(['fare_amount', 'key', 'pickup_datetime'], axis=1)\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.2)\n\n\n\n","a1c548d1":"#train_y = train.fare_amount\n#train_X = train.drop(['fare_amount', 'key', 'pickup_datetime'], axis=1)\n#sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\n#test_X = test.drop(['key', 'pickup_datetime' ], axis=1)\n#test_y = sample_submission.drop(['key'], axis=1)\n","3a1c4de6":"from xgboost import XGBRegressor\nfrom math import sqrt\n\ndef XGBoost(train_X, test_X, train_y, test_y, max_depth, nb_estimators, learning_rate, early_stopping_rounds):\n    model = XGBRegressor(max_depth = max_depth, nb_estimators = nb_estimators, learning_rate = learning_rate)\n    model.fit(train_X, train_y, early_stopping_rounds = early_stopping_rounds , eval_set=[(test_X, test_y)], verbose=False)\n    return model\n\ndef errorXGBoost(model, test_X, test_y):\n    predictions = model.predict(test_X)\n    return str(mean_absolute_error(predictions, test_y))","fb0e440a":"\nmodel = XGBoost(train_X, test_X, train_y, test_y, 5, 500, 0.05, 5)\nprint(errorXGBoost(model, test_X, test_y))\n","ab477f61":"test = pd.read_csv(\"..\/input\/test.csv\")\ntest['distance_miles'] = distance(test.pickup_latitude, test.pickup_longitude, \\\n                                      test.dropoff_latitude, test.dropoff_longitude)\n\ntest = time_features(test)\ntest = test.drop(['key', 'pickup_datetime' ], axis=1)\ntest= test.as_matrix()\n\nprediction = model.predict(test)\ntest = pd.read_csv(\"..\/input\/test.csv\")\nholdout = pd.DataFrame({'key': test['key'], 'fare_amount': prediction})\nholdout.to_csv('predictionTest.csv', index=False)\n","a7dae70e":"According to Kaggle competition, we have a RSE of 6.05 $. ","f91b58b8":"The bounding box around New York city is :\n<p>\n    <ul>\n        <li>North Latitude: 40.917577<\/li>\n        <li>South Latitude: 40.477399<\/li> \n        <li>East Longitude: -73.700272 <\/li>\n        <li>West Longitude: -74.259090<\/li>\n    <\/ul>       \n<\/p>\nWe remove ride out of New York city:","98d17297":"We convert 'ds' column to datastamp and sort the values.","037f9602":"Submission to Kaggle competition : ","4c9e654e":"We can see that there are some outliers in the dataset.\n\nFor example : \n<p> \n    <ul>\n        <li> The minimum of fare amount is negative and the maximum is more than 60,000 USD<\/li>\n        <li> The maximum of passenger count is 208 and minimum is 0 <\/li>\n        <li> Some latitude and longitude are very high <\/li>\n    <\/ul>\n<\/p>","46a0f29a":"We start by importing the data. The original file train.csv contains more than 55 millions rows. Because we use a Kaggle kernel we take only 22 millions rows.","6e9dc74c":"As we can see,  majority of taxi rides cost around 7 USD that means people use it on short distances. ","374a029d":"It seems that there are taxi with more than 200 passanger","38de61c8":"It seems that the relation is linear between the distance and the fare amount","afae53f9":"## Can we predict a rider's taxi fare?","8ac4baea":"## XGBoost model","0d935631":"We look if there is some missing data.","e07feddc":"### Fitting the model","b618e4e3":"Mean square error has been chosen because it gives score in the square fare amount.\nSome tests has been done with different parameters. Max depth and the number of estimators are the most important parameters in XGBoost. It has been changed many times to approximate a good score, be robust, trying to not be in overfitting. ","7ad98725":"Now let's compare the prediction and confidence to the real data (you can zoom, pan on the plot)","b8692f75":"XGBoost model is a well-known model on Kaggle competitions. It is composed both of decision trees and boosting and can give good accuracy. What we want do with this model is to predict the price of the course according to our variables. Before, we split the model within the training set and testing set. X are the data in  which we do the prediction and y what we want to predict (the fare amount).","70fb1d38":"Now we look closer at passanger count","ec33786d":"We look what is the type of each feature","31f12a7d":"## FbProphet model\n\nProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data.\nProphet is open source software released by Facebook.\n\nFbProphet is a very ressource consuming algorithm, so we need to split the train data set. \nWe take 1 million sample to process FbProphet","c3c4c336":"## Import data","db619365":"In FbProphet library we must use 'ds' and 'y' as column names. So we rename the existing columns.","f11494ff":"We also remove rides where pickup and dropoff location are exactly the same:","bd6e5d1d":"Plot of our prediction : ","6b9d2973":"We remove it","b71b0e3d":"## Train a linear model","7ccea275":"Let's check if it is better !","ee3b1733":"Our model will take the form  $X\u22c5w=y$  where  $X$  is a matrix of input features, and  $y$ is a column of the target variable, fare_amount, for each row. The weight column  $w$  is what we will \"learn\".\n\nFirst let's setup our input matrix $X$  and target column  $y$  from our training set. The matrix  $X$  should consist of the two GPS coordinate differences, plus a third term of 1 to allow the model to learn a constant bias term. \n\nIn a way, the column of 1s is a hack to extend the model to support a bias term.\n\nA simpler example is in 2D space, where $x\u2208\u211d$ is your \"input\" and $y\u2208\u211d$ is your \"target\". If you try to capture this relationship with a linear model of form $y=ax$, where $a\u2208\u211d$, your model could only be lines that pass through the origin (0,0) and you could not effectively capture most 2D relationships.\n\nHowever if you extend the model to have a second variable $b$ -- sometimes called the bias term -- say $y=ax+b$, then your model can be (almost) any 2D line, and the model can now capture most 2D linear relationships.\n\nNote that if we write $\\vec{x}=\\begin{pmatrix} x & 1 \\end{pmatrix}$ and $\\vec{w}=\\begin{pmatrix} a\\\\b \\end{pmatrix}$ then the following two models are equivalent:\n\n$$y=ax+b$$\n$$y=\\vec{x} \\cdot \\vec{w} $$\nSo adding the column of 1s to our inputs $\\vec{x}$  allows us to write the model in a more concise way (just $\\vec{w}$  instead of a and b), while still allowing the model (encoded by the $\\vec{x}$  column) to learn the additional bias term. The column  $y$  should consist of the target fare_amount values.","87210274":"The maximum capacity for taxi is 7 so we remove data above ","d49a78ab":"# New York City Taxi Fare Prediction","b9f09855":"<p>\n    <ol>\n        <li>First we add a distance in kilometers<\/li>\n         <li>Second we add time feature<\/li>\n     <\/ol>\n <\/p>\n        ","68ec9831":"Training of Linear model using sklearn library : ","31f6dd90":"We look the test set:","b78a7a23":"We look more closely at the data.","53844ce6":"## Data cleaning","4983bcf4":"### Metric\n\nFor the metric, I'll compute the MSE for every 2 days (48 hours of data) to check how it evolves during the complete year","dbc914a2":"We have now 4 analysis.\n\n  <ul>\n    <li>A global trend which is increasing during years. This is logical as we predict over 6 years of data, and it is known that taxi fare increases (a little over years).\n    <\/li> \n    <li>A yearly trend that shows that the prices are quite equal from may to december and starts to decrease because it is the least touristic periods<\/li>\n    <li> A weekly trend which shows that the taxi fare are high especially the week end <\/li>\n    <li> A daily trend that show that the highest rates fare are near to 4 a.m<\/li>\n\n","e6050f45":"For Kaggle competition : ","1a94a0cc":"Now we look for outliers on taxi fare","0c0f2f9b":"## Feature engineering","fe2a1b70":"The result seems predictable because FbProphet take only timestamp parameters so that it could be interesting to take also distance and number of passengers. ","9ae3176b":"In New York City, minimum taxi fare is 2.5 USD. We remove data where fare_amount is less than 2.5 USD","ebd93069":"It seems find !","113d8784":"The RMSE shows the strong correlation between distance and price. \n\nThe R-squared score is relatively close to 1. The nearer, the more correlated. This indicator shows the variation in relation to the regression line. Yet, we see the good relation between distance and price. ","0096d302":"\n\nWe can see that we have a correct MSE during the summer but every winter have a lot more error. This can be explained with weather. We have peaks of error which may be explained by the lack of wind. This is more difficult to predict. As a result, the pollution generated by heating system stacked over the city and decrease quickly when wind is back. The rest of the time we have a quite good approximation\n","3c61a161":"### Split train\/test\n\nThe train set will be the 80% firsts values, and the test set the 20% last values.","6a53fc82":"Then, We created two fonctions. The first function is the creation of the model with specific parameters : \n* max_depth : the depth of the tree ;\n* nb_estimators is the numbers of trees ;\n* learning_rate is speed learning, it means mutiply the prediction of model before we add them together. It can reduce overfit ;\n* early_stopping_rounds : the number of rounds maximum before the error rise. It means if we have the minimum error for a round and after, the error rises, this parameter will stop the training to come back at the step when we had the minimum error.\n\nThe second function is the error returned by the first model, in which the mean square error is calculated. We want to reduce this value to the maximum. \n"}}