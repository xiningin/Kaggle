{"cell_type":{"2621972c":"code","bf90022f":"code","857428d4":"code","43d572aa":"code","8aeb68d5":"code","051930a0":"code","56168951":"code","b17daa54":"code","e467aa8c":"code","61d411a1":"code","e0482dda":"code","fff29620":"code","e4c2360d":"code","1ee67759":"code","26deccc1":"code","288a5092":"code","03349dc4":"code","c3c086eb":"code","b8d5efd9":"code","3cd57c39":"code","8c81b005":"code","af3a828a":"code","9f625cce":"code","9917ae8c":"code","c523845d":"code","a1413f12":"code","cef4b079":"code","c809c2f4":"code","c45ed90e":"code","50c7c947":"code","6c09dc32":"code","4df4874a":"markdown","80c4b7ea":"markdown","cb6b1653":"markdown","0bb0fcca":"markdown","623039f1":"markdown","031ca0cb":"markdown","f6977079":"markdown","1e0f5d1a":"markdown","89ce3be1":"markdown","10b004da":"markdown","522aa30a":"markdown","23680fb0":"markdown","ac119ce0":"markdown","be71b8ce":"markdown","f243e72b":"markdown"},"source":{"2621972c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\n\nimport lightgbm as lgb\n\nfrom sklearn.feature_selection import mutual_info_classif, chi2, f_classif\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score","bf90022f":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","857428d4":"df.info()","43d572aa":"df.isna().sum()","8aeb68d5":"num_cols = ['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']\n\ncat_cols = ['RestingECG','ST_Slope','ChestPainType','Sex','ExerciseAngina','HeartDisease']","051930a0":"df_ans = df.copy()\n\n#make string tags numerical  \ndf_ans.Sex = df.Sex.replace({'M':0,'F':1})\ndf_ans.ExerciseAngina = df.ExerciseAngina.replace({'N':0,'Y':1})\n\ndf_ans.RestingECG=df.RestingECG.replace({'Normal':0,'LVH':1,'ST':2})\ndf_ans.ST_Slope=df.ST_Slope.replace({'Flat':0,'Up':1,'Down':2})\ndf_ans.ChestPainType=df.ChestPainType.replace({'ASY':0,'NAP':1,'ATA':2,'TA':3})\n\n#list distributions\nsns.pairplot(df_ans,hue=\"HeartDisease\");","56168951":"df_ans[num_cols].describe()","b17daa54":"#numeric variable cov\ndf_ans[num_cols].corr()","e467aa8c":"def num_plot(df, col):\n    fig = px.histogram(df, x=col, color=\"HeartDisease\",\n                       marginal=\"box\")\n    fig.update_layout(height=400, width=500, showlegend=True)\n    fig.update_traces(marker_line_width=1,marker_line_color=\"black\")\n    fig.show()","61d411a1":"for col in num_cols:\n    num_plot(df_ans,col)","e0482dda":"#chi test of cat_cols  \ndef chi_test(x, y):\n    from scipy.stats import chi2_contingency\n    \"\"\"Pearson Chi-Square Independent Test: Measure the discrimination of features\n    Params:\n    -----------\n    x: array-like, One-dimensional, discrete feature variable\n    y: array-like\uff0cOne-dimensional, the other discrete feature variable, \n       this test can measure the discrimination of features when y is target variable\n    return:\n    ----------\n    chi_result: dict\n        'Value': chi2 values,\n        'Prob' : Probility, The smaller the value, the stronger the probility of rejecting the zero hypothesis\n        'LLP' : -log(Prob), The larger the value, the stronger the probility of rejecting the zero hypothesis\n        'DF' : degree of freedom.\n    \"\"\"\n    tab = pd.crosstab(x, y).fillna(0)\n    chi_value, p_value, def_free, expected = chi2_contingency(tab)\n    return {'DF': def_free, 'Value': chi_value, 'Prob': p_value, 'LLP': -np.log(p_value)}\n\ndef get_random_index(n, x):\n    \"\"\"Range of index [0, n)\n    n: range\n    x: size\n    \n    return: range of index\n    \"\"\"\n    index = np.random.choice(np.arange(n), size=x, replace=False)\n    return index\n\n#get 100 rows sample randomly\nindex = get_random_index(len(df_ans),100)\n\n#zero hypothesis\uff1a independence x1 and x2 in cat_cols\nchi_df_1 = pd.DataFrame(columns=['name1','name2','chi_value','chi_p','chi_DF','chi_LLP']) \nfor i in range(0,len(cat_cols)): \n    \n    for j in range(i+1,len(cat_cols)):  \n        chi_df = pd.DataFrame() \n        chi_df['name1'] = [list(cat_cols)[i]] \n        chi_df['name2'] = [list(cat_cols)[j]]\n        chi_result = chi_test(df_ans[list(cat_cols)[i]][index].values.reshape(-1),\n                              df_ans[list(cat_cols)[j]][index].values.reshape(-1))\n        chi_df['chi_value'] = [chi_result['Value']]\n        chi_df['chi_p'] = [chi_result['Prob']]\n        chi_df['chi_DF'] = [chi_result['DF']]\n        chi_df['chi_LLP'] = [chi_result['LLP']]\n        chi_df_1 = chi_df_1.append(chi_df)\n        \nchi_df_1.sort_values(by='chi_p')","fff29620":"#mutual information of features \nmutual = pd.Series(mutual_info_classif(df_ans, df_ans.pop('HeartDisease'), random_state=0),\n                   index=df_ans.columns, name='mutual_info')","e4c2360d":"mutual.sort_values(ascending=False).plot.bar(figsize=(15,5))\nplt.title('features mutual information with HeartDisease.')\nplt.ylabel('mutual_info')\nplt.show()","1ee67759":"df.Sex = df.Sex.replace({'M':0,'F':1})\ndf.ExerciseAngina = df.ExerciseAngina.replace({'N':0,'Y':1})","26deccc1":"encoded_df = pd.get_dummies(df, drop_first=True)\nencoded_df","288a5092":"X_df = encoded_df.drop('HeartDisease', axis = 1)\ny_df = encoded_df['HeartDisease']","03349dc4":"#scaling before KNNImputer\nscaler = StandardScaler()\nscaler_X = scaler.fit_transform(X_df)\n\nscaler_X_df= pd.DataFrame(scaler_X,columns=X_df.columns)\n#get index from original dataframe, encoded_df\nscaler_X_df['Cholesterol'][encoded_df['Cholesterol']==0] = np.nan\nscaler_X_df.Cholesterol.isna().sum()\n\n#KNNImputer\nimputer = KNNImputer(n_neighbors=5)\nimputed_X = imputer.fit_transform(scaler_X_df) ","c3c086eb":"#KMF Definition\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n\nclass KMeansFeaturizer:\n    \"\"\"\n    KMeansFeaturizer gets new cluster id by features clustering\n    type of input features: numerical \n    \"\"\"\n    def __init__(self, k = 100, target_scale = 5.0, random_state = None):\n        \"\"\"\n        k: Integer\n            number of clusters\n        target_scale: Float\n            scale of y, y is target information when before fitting model\n        random_state: Integer\n            random seed\n        model: None\n            saving model after fitting\n        \"\"\"\n        self.k = k\n        self.target_scale = target_scale\n        self.random_state = random_state\n        self.model = None\n    def fit(self, X, y = None):\n        \"\"\"\n        method: data clustering and find clusters\n        \n        X: numpy.ndarray (n, r)\n            features data\n        y: numpy.ndarray (n, )\n            target information\n        \"\"\"\n        if y is None:\n            #fitting directly without y\n            km_model = KMeans(n_clusters = self.k, n_init = 20, random_state = self.random_state)\n            km_model.fit(X)\n            #updating km_model\n            self.km_model = km_model\n            self.cluster_centers_ = km_model.cluster_centers_\n            \n            return self\n        #scaling y information if y exists\n        data_with_target = np.hstack((X,y[:,np.newaxis]*self.target_scale))\n        \n        #fitting pre_model with X and y info\n        km_model_pretrain = KMeans(n_clusters = self.k, n_init = 20, init = \"k-means++\", random_state = self.random_state)\n        km_model_pretrain.fit(data_with_target)\n        \n        #fitting final model with X and pre_model_cluster_centers\n        km_model = KMeans(n_clusters = self.k, init = km_model_pretrain.cluster_centers_[:,:-1], n_init = 1, random_state = self.random_state)\n        km_model.fit(X)\n        self.km_model = km_model\n        self.cluster_centers_ = km_model.cluster_centers_\n        return self\n    \n    def transform(self, X):\n        \"\"\"\n        method: getting clusters by transform data using fitted model\n        \n        X: numpy.ndarray (n, r)\n            features data\n        \"\"\"\n        clusters = self.km_model.predict(X)\n        return clusters[:, np.newaxis]\n    \n    def fit_transform(self, X, y = None):\n        \"\"\"\n        method: fitting and transforming model\n        \n        X: numpy.ndarray (n, r)\n            features data\n        y: numpy.ndarray (n, )\n            target information\n        \"\"\" \n        self.fit(X, y)\n        return self.transform(X)","b8d5efd9":"from tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow import random\nfrom tensorflow.nn import relu,leaky_relu,sigmoid\nimport numpy as np\nimport tensorflow as tf\n\ntf.compat.v1.reset_default_graph()\ntf.compat.v1.set_random_seed(2019)\n\nclass DenseAutoencoderFeaturizer:\n    \"\"\"\n    method: building a autoencoder for features extracting\n    \n    input_shape: Integer\n        number of input data features\n    denses_shape: Integer tuple\n        hidden layers and their units setting\n    latend_dim: Integer\n       number of extracting features\n    activated: tf.nn \n        refer to tf.keras.layers\n    drop_rate: float\n        refer to tf.keras.layers\n    kernel_regularizer: tf.nn\n        refer to tf.keras.layers\n    name: String\n        name of autoencoder\n    \"\"\"\n    def build(input_shape, denses_shape=(64, 32), latent_dim=10, \n              activated = relu, drop_rate = 0.5, kernel_regularizer=None, name = \"\"):\n        # initialize the input shape to be \"channels last\" along with\n        # the channels dimension itself\n        # channels dimension itself\n        chan_dim = -1\n        \n        # define the input to the encoder\n        inputs = Input(shape=input_shape)\n        x = inputs\n        # loop over the number of filters\n        for unit in denses_shape:\n            # combine encoder hidden layers\n            x = Dense(unit, kernel_regularizer= kernel_regularizer)(x)\n            x = Activation(activated)(x)\n            x = BatchNormalization(axis=chan_dim)(x)     \n        #latent dim\n        x = Dense(latent_dim, kernel_regularizer= kernel_regularizer)(x)\n        latent = Dropout(drop_rate)(x)\n        # build the encoder model\n        encoder = Model(inputs, latent, name=\"encoder_\"+name)\n        \n        # start building the decoder model which will accept the\n        # output of the encoder as its inputs\n        latent_inputs = Input(shape=(latent_dim,))\n        x = latent_inputs\n        # loop over our number of filters again, but this time in\n        # reverse order\n        for unit in denses_shape[::-1]:\n            # apply a Dense => activated_function => BN operation\n            x = Dense(unit, kernel_regularizer= kernel_regularizer)(x)\n            x = Activation(activated)(x)\n            x = BatchNormalization(axis=chan_dim)(x)\n            \n        x = Dense(input_shape, kernel_regularizer= kernel_regularizer)(x)\n        outputs = Activation(\"sigmoid\")(x)\n        # build the decoder model\n        decoder = Model(latent_inputs, outputs, name=\"decoder_\"+name)\n        # our autoencoder is the encoder + decoder\n        autoencoder = Model(inputs, decoder(encoder(inputs)),\n            name=\"autoencoder_\"+name)\n        # return a 3-tuple of the encoder, decoder, and autoencoder\n        return (encoder, decoder, autoencoder)","3cd57c39":"#extracting KMeans Feature\nKMF = KMeansFeaturizer(k=20, random_state = 2019)\nKMF_target = KMF.fit_transform(imputed_X)\n\n#scaler inversing\nX_df = pd.DataFrame(scaler.inverse_transform(imputed_X),\n                    columns=X_df.columns).round(1)\n\n\nX_df = pd.concat(\n                [\n                    X_df[['Age','Sex','RestingBP','Cholesterol','FastingBS','MaxHR','ExerciseAngina','Oldpeak']],\n                    df[['RestingECG','ST_Slope','ChestPainType']]\n                ],\n                 axis=1)\n\n#combining X_df and KMF_targets\nX_df[\"KMF_target\"] = KMF_target","8c81b005":"imputed_X.shape","af3a828a":"#extracting DAF Feature\nencoder, decoder, autoencoder = DenseAutoencoderFeaturizer.build(input_shape=15, denses_shape=(64,16), latent_dim=4, \n              activated = leaky_relu, drop_rate = 0.015, kernel_regularizer=None, name=\"autoencoder1\")\n\nprint(encoder.summary())\nprint(decoder.summary())\nprint(autoencoder.summary())\n\nEPOCHS = 800\nBS = 100\n\nfrom tensorflow.keras.optimizers import Adam,RMSprop\n\nopt = Adam(learning_rate=1e-3)\n# opt = RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07)\nautoencoder.compile(loss=\"mse\", optimizer=opt, metrics=[\"mae\"])\n\n# train the convolutional autoencoder\nH = autoencoder.fit(\n    imputed_X, imputed_X,\n    epochs=EPOCHS,\n    batch_size=BS,\n    verbose=0)\n\nDAF_targets = encoder.predict(imputed_X)\n\n#combining X_df and KMF_targets\nX_df = pd.concat(\n    [\n        X_df,\n        pd.DataFrame(DAF_targets,columns=['DAF_target_1','DAF_target_2','DAF_target_3','DAF_target_4'])\n    ],\n    axis=1)\nX_df","9f625cce":"X_df.RestingECG=X_df.RestingECG.replace({'Normal':0,'LVH':1,'ST':2})\nX_df.ST_Slope=X_df.ST_Slope.replace({'Flat':0,'Up':1,'Down':2})\nX_df.ChestPainType=X_df.ChestPainType.replace({'ASY':0,'NAP':1,'ATA':2,'TA':3})\n\n# X_df = pd.get_dummies(X_df, drop_first=True)\nX_df","9917ae8c":"# get train feature\ndel_feature = ['HeartDisease']\nfeatures = [i for i in X_df.columns if i not in del_feature]\n\n# split data to [0.9,0.1]\nx_train, x_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.1, random_state=100)\n\n#train dataset concating and reseting index\ntrain_all = pd.concat([x_train,y_train],axis=1).reset_index(drop=True)\nx_train = train_all.drop(['HeartDisease'], axis = 1)\ny_train = train_all[\"HeartDisease\"]\n\n#test dataset concating and reseting index\ntest_all = pd.concat([x_test,y_test],axis=1).reset_index(drop=True)\nx_test = test_all.drop(['HeartDisease'], axis = 1)\ny_test = test_all[\"HeartDisease\"]","c523845d":"params = {'num_leaves': 50, #\u7ed3\u679c\u5bf9\u6700\u7ec8\u6548\u679c\u5f71\u54cd\u8f83\u5927\uff0c\u8d8a\u5927\u503c\u8d8a\u597d\uff0c\u592a\u5927\u4f1a\u51fa\u73b0\u8fc7\u62df\u5408\n          'min_data_in_leaf': 30,\n          'objective': 'binary', #\u5b9a\u4e49\u7684\u76ee\u6807\u51fd\u6570\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"min_sum_hessian_in_leaf\": 8,\n          \"boosting\": \"gbdt\",\n          \"feature_fraction\": 0.8,  #\u63d0\u53d6\u7684\u7279\u5f81\u6bd4\u7387\n          \"bagging_freq\": 1,\n          \"bagging_fraction\": 1,\n          \"bagging_seed\": 8,\n#            \"lambda_l1\": 0.01,             #l1\u6b63\u5219\n           'lambda_l2': 0.001,     #l2\u6b63\u5219\n          \"verbosity\": -1,\n          \"nthread\": -1,                #\u7ebf\u7a0b\u6570\u91cf\uff0c-1\u8868\u793a\u5168\u90e8\u7ebf\u7a0b\uff0c\u7ebf\u7a0b\u8d8a\u591a\uff0c\u8fd0\u884c\u7684\u901f\u5ea6\u8d8a\u5feb\n          'metric': {'binary_logloss', 'auc'},  ##\u8bc4\u4ef7\u51fd\u6570\u9009\u62e9\n          \"random_state\": 2019, #\u968f\u673a\u6570\u79cd\u5b50\uff0c\u53ef\u4ee5\u9632\u6b62\u6bcf\u6b21\u8fd0\u884c\u7684\u7ed3\u679c\u4e0d\u4e00\u81f4\n          # 'device': 'gpu' ##\u5982\u679c\u5b89\u88c5\u7684\u4e8bgpu\u7248\u672c\u7684lightgbm,\u53ef\u4ee5\u52a0\u5feb\u8fd0\u7b97\n          }","a1413f12":"#Scores\ndef get_scores(y, y_pred):\n    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n    'Precision':np.round(precision_score(y, y_pred),2),\n    'Recall':np.round(recall_score(y, y_pred),2),\n    'F1':np.round(f1_score(y, y_pred),2),\n    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n    scores_df = pd.Series(data).to_frame('scores')\n    return scores_df","cef4b079":"folds = KFold(n_splits=15, shuffle=True, random_state=2019)\nprob_oof = np.zeros((x_train.shape[0], ))\ntest_pred_prob = np.zeros((x_test.shape[0], ))\n\ncat_features = ['RestingECG','ST_Slope','ChestPainType','Sex','ExerciseAngina','FastingBS','KMF_target']\n## train and predict\nfeature_importance_df = pd.DataFrame()\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train,y_train)):\n    print(\"fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(x_train.iloc[trn_idx], label=y_train[trn_idx])\n    val_data = lgb.Dataset(x_train.iloc[val_idx], label=y_train[val_idx])\n\n    clf = lgb.train(params,\n                    trn_data,\n                    num_boost_round=30,\n                    valid_sets=[trn_data, val_data],\n                    categorical_feature=cat_features,\n                    verbose_eval=20,\n                    early_stopping_rounds=20)\n    prob_oof[val_idx] = clf.predict(x_train.iloc[val_idx], num_iteration=clf.best_iteration)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    test_pred_prob += clf.predict(x_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nthreshold = 0.480\nypred=[]\nfor pred in test_pred_prob:\n    result = 1 if pred > threshold else 0\n    ypred.append(result)\nget_scores(y_test,ypred)","c809c2f4":"import shap\n\nshap_values = shap.TreeExplainer(clf).shap_values(x_test[features])\n\nshap.summary_plot(shap_values, x_test[features])","c45ed90e":"feature_importance_df.groupby(['Feature'])\\\n                    .sum()\\\n                    .drop(['fold'],axis=1)\\\n                    .sort_values(by='importance')","50c7c947":"def conf_matrix(y, y_pred):\n    fig, ax =plt.subplots(figsize=(3,3))\n    labels=['No','Yes']\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False)\n    plt.title('Heart Failure?')\n    ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n    ax.set_ylabel('Test')\n    ax.set_xlabel('Predicted')","6c09dc32":"conf_matrix(y_test, ypred)","4df4874a":"## 1. Data Exploring","80c4b7ea":"**Thanks:**\n\nLUDOVICO CUOGHI: https:\/\/www.kaggle.com\/ludovicocuoghi\/plotly-viz-pytorch-neural-net-auc-90-f1-91","cb6b1653":"### 3.2 Model Parameters Setting","0bb0fcca":"### 2.1 Encoding","623039f1":"As shown above, we could find out the distributions and the histogram of OldPeak, Cholesterol, ST_Slope, MaxHR, ChestPainType and ExerciseAngina look something different in whether samples have heart disease.\n\nLook at the line, the scatterplots of MaxHR look that the larger the value, the greater the quantity of non heart disease \n\nthe samples with no heart disease mainly concentrate on ST_Slope == 1 (UP)\n\nwe could guess the features of OldPeak, Cholesterol, ST_Slope, MaxHR, ChestPainType and ExerciseAngina have stronger relation with heart disease","031ca0cb":"We use features mutual information to enhance our conjecture. \n\nObviously, now we could say ST_Slope, ExcerciseAngina, ChestPainType, Oldpeak MaxHR and Cholesterol have a great relation with heart disease.\n\n**BUT** from the SCATTERPLOTS, they seem to tell us that multiple features have corelation, and they could be more distinguished whether sample is heart disease or not.\n\nSo, as below, after processing data, we use KNN and AutoEncoder to compress this corelation from high dimension\/multiple features to low dimension, even if losing interpretability in model","f6977079":"RestingECG, ST_Slope and ChestPainType need OHC","1e0f5d1a":"## 3. Data Modeling","89ce3be1":"Here, we just randomly choose 100 samples from dataset to have a chi test\n\nWe found that ST_Slope, ChestPainType and ExerciseAngina in catogory cols have relations with HeartDisease. Their chi_values are high and chi_ps are low","10b004da":"## 2. Data Processing","522aa30a":"Sex and ExerciseAngina both are two classes, we can encode by 0 and 1","23680fb0":"More detail about numeric cols as shown above. **BUT** cholesterol has zero number, is it wrong? \n\nNormally cholesterol should not be zero, it should be a number in a range, so I consider that it is wrong data, replaced it to a related number by using KNNImputer\n\nVia more detail from numeric cols\u2018 histograms, we have a bigger confirmation of OldPeak, MaxHR and Cholesterol having relation with Heart Disease","ac119ce0":"### 3.3 Model Training","be71b8ce":"#### Attribute Information\n\n**Categorical Variables**\n\nSex: sex of the patient [M: Male, F: Female]\n\nChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n\nFastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n\nRestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n\nExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n\nST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\n**(Target)** HeartDisease: output class [1: heart disease, 0: Normal]\n\n**Numeric Variables**\n\nAge: age of the patient [years]\n\nRestingBP: resting blood pressure [mm Hg]\n\nCholesterol: serum cholesterol [mm\/dl]\n\nMaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n\nOldpeak: oldpeak = ST [Numeric value measured in depression]","f243e72b":"### 3.1 Data Splitting"}}