{"cell_type":{"de69bdd4":"code","300fe59f":"code","04c2dc29":"code","587f9b93":"code","36da6532":"code","aedb6742":"code","06715bdd":"code","2038201e":"code","a419edef":"code","e5f39ba8":"code","7e3edf98":"code","57630540":"code","1bcc39d1":"code","6fd1a5c0":"code","2dc69235":"code","f2a38803":"code","2813406d":"code","6ac1d24f":"code","7f6ec63d":"code","3789bf04":"code","4ddcf252":"code","ef94d0c1":"code","0d6c1c7e":"code","024edece":"code","6ff3c865":"code","89e89046":"code","c2c7c32d":"code","af081e7b":"code","201e4ef1":"code","503822f2":"code","6d9079c4":"code","0d27165d":"code","53767b5e":"code","affaf194":"code","5215f617":"code","3b041404":"code","98f662b5":"code","ce264304":"code","95cfd9b6":"code","320684dc":"code","e20c1149":"markdown","39badc36":"markdown","00e696d2":"markdown","f9d6d734":"markdown","31b60c2c":"markdown","87fbddc1":"markdown","f40a7bdc":"markdown","689d3708":"markdown","65b1ecf7":"markdown","8d8971ab":"markdown","ace6640b":"markdown","b4789737":"markdown","829d1c46":"markdown","a313c180":"markdown","3f8e677a":"markdown","4af60207":"markdown","a29429d4":"markdown","a17b9fd8":"markdown","a397ce5b":"markdown","6ab169ed":"markdown","ae065f5f":"markdown","e2f5f1cc":"markdown","83eb8118":"markdown","805c1f77":"markdown","7950d09b":"markdown","32f2ded1":"markdown","6a625f5f":"markdown","b84b8421":"markdown","b1000b11":"markdown","fa34036a":"markdown","7f1e6eb0":"markdown","56233a3a":"markdown","3e2da983":"markdown","07c7f288":"markdown","5333b47c":"markdown","e413c50b":"markdown","e85d02e2":"markdown","2455d00e":"markdown","5097e263":"markdown"},"source":{"de69bdd4":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Get version python\/keras\/tensorflow\/sklearn\nfrom platform import python_version\nimport sklearn\nimport keras\nimport tensorflow as tf\n\n# Folder manipulation\nimport os\n\n# Garbage collector\nimport gc\n\n# Linear algebra and data processing\nimport numpy as np\nimport pandas as pd\nfrom pandas import datetime\n\n# Visualisation of picture and graph\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Keras importation\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, RNN, Bidirectional, concatenate, GRUCell, LSTMCell\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\n\n# Others\nfrom tqdm import tqdm, tqdm_notebook","300fe59f":"print(os.listdir(\"..\/input\"))\nprint(\"Keras version : \" + keras.__version__)\nprint(\"Tensorflow version : \" + tf.__version__)\nprint(\"Python version : \" + python_version())\nprint(\"Sklearn version : \" + sklearn.__version__)","04c2dc29":"MAIN_DIR = \"..\/input\/bitcoin-historical-data\/\"\nDATA = \"bitstampUSD_1-min_data_2012-01-01_to_2019-08-12.csv\"\n\n# Size of the dataset in percentage\nTEST_SIZE = 5\nVAL_SIZE = 5\n\nOUTPUT_SIZE = 30\nINPUT_SIZE = 30\n\n# Set graph font size\nsns.set(font_scale=1.3)","587f9b93":"def load_data():\n    df = pd.read_csv(MAIN_DIR+DATA)\n    # We don't take all the data in order to reduce training time\n    df = df[-25000:].reset_index(drop=True)\n    return df","36da6532":"data_raw = load_data()","aedb6742":"print(f\"Shape of dataset : {data_raw.shape}\")","06715bdd":"data_raw.head()","2038201e":"data_raw.isna().sum()","a419edef":"data_raw = data_raw.dropna()","e5f39ba8":"def plot_curves(df, var_y, var_x='MJD'):\n    fig, ax = plt.subplots(figsize=(16,5))\n    sns.lineplot(x=var_x,y=var_y, data=df, ax=ax)\n\n    ax.set_title(f\"'{var_y}' value evolution in function of time\")\n    ax.set_xlabel(f'{var_x}')\n    ax.set_ylabel(f\"'{var_y}' value\")","7e3edf98":"plot_curves(data_raw, 'Weighted_Price', 'Timestamp')","57630540":"def feature_engineering(data):\n    drop_feat = ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)',\n       'Volume_(Currency)']\n    \n    df = data.copy()\n    \n    # Drop useless feature\n    df = df.drop(drop_feat, axis=1)\n    df.columns = ['target']\n    \n    # Drop Nan for simplicity\n    df = df.dropna()\n    \n    return df","1bcc39d1":"data_raw = load_data()\ndata = feature_engineering(data_raw)","6fd1a5c0":"data.head()","2dc69235":"print(f\"New dataset shape : {data.shape}\")","f2a38803":"# Get index limit of train\/val\/test index in the dataset\ndef get_limit_split(data, val_size, test_size, output_size):\n    # Convert percentage into value\n    val_size = int((val_size*0.01)*data.shape[0])\n    test_size = int((test_size*0.01)*data.shape[0])\n\n    limit_train = data.shape[0] - val_size - test_size - output_size + 1\n    limit_val = limit_train + val_size\n    limit_test = limit_val + test_size\n    \n    return limit_train, limit_val, limit_test","2813406d":"# It would be better to use Keras or Sklearn normalize implementation...\ndef normalize(data, val_size, test_size, output_size):\n    def apply(X, mean, std):\n        X = (X - mean) \/ std\n        return X\n    \n    df = data.copy()\n    \n    val_size = int((val_size*0.01)*df.shape[0])\n    test_size = int((test_size*0.01)*df.shape[0])\n    \n    limit_train = df.shape[0] - val_size - test_size - output_size + 1\n    limit_val = limit_train + val_size\n    limit_test = limit_val + test_size\n    \n    mean = df.iloc[0:limit_train]['target'].mean()\n    std = df.iloc[0:limit_train]['target'].std()\n    \n    df.iloc[0:limit_train]['target'] = apply(df.iloc[0:limit_train]['target'].values, mean, std)\n    df.iloc[limit_train:limit_val]['target'] = apply(df.iloc[limit_train:limit_val]['target'].values, mean, std)\n    df.iloc[limit_val:limit_test]['target'] = apply(df.iloc[limit_val:limit_test]['target'].values, mean, std)\n    \n    return df, mean, std","6ac1d24f":"# It would be better to use Keras or Sklearn normalize implementation...\ndef denormalize(X, mean, std):\n    X = (X * std) + mean\n    return X","7f6ec63d":"def plot_data_split(data, val_size, test_size, output_size):\n    df = data.copy()\n    \n    limit_train, limit_val, limit_test = get_limit_split(df, val_size, test_size, output_size)\n    \n    df.at[0:limit_train, 'dataset'] = 'train'\n    df.at[limit_train:limit_val, 'dataset'] = 'val'\n    df.at[limit_val:limit_test, 'dataset'] = 'test'\n    \n    fig, ax = plt.subplots(figsize=(16,5))\n    sns.lineplot(x=df.index,y='target', data=df, ax=ax, hue='dataset')\n\n    ax.set_title(f\"Bitcoin value evolution in function of time\")\n    ax.set_xlabel(f'Index in dataset')\n    ax.set_ylabel(f\"Bicoin value\")","3789bf04":"def train_val_test_split(X, y, val_size=10, test_size=10, input_size=1, output_size=0):\n    # Convert percentage into value\n    val_size = int((val_size*0.01)*X.shape[0])\n    test_size = int((test_size*0.01)*X.shape[0])\n    \n    limit_train = X.shape[0] - val_size - test_size - output_size + 1\n    limit_val = limit_train + val_size\n    limit_test = limit_val + test_size\n    \n    # TRAINING SET\n    X_train = []\n    y_train = []\n    for i in range(input_size,limit_train):\n        X_train.append(X[i-input_size:i,:])\n        y_train.append(y[i:i+output_size,:])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    \n    # VALIDATION SET\n    X_val = []\n    y_val = []\n    for i in range(limit_train,limit_val):\n        X_val.append(X[i-input_size:i,:])\n        y_val.append(y[i:i+output_size,:])\n    X_val, y_val = np.array(X_val), np.array(y_val)\n    \n    # TEST SET\n    X_test = []\n    y_test = []\n    for i in range(limit_val,limit_test):\n        X_test.append(X[i-input_size:i,:])\n        y_test.append(y[i:i+output_size,:])\n    X_test, y_test = np.array(X_test), np.array(y_test)\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test","4ddcf252":"# Normalize data\ndata_norm, mean, std = normalize(data, val_size=VAL_SIZE, test_size=TEST_SIZE, output_size=OUTPUT_SIZE)","ef94d0c1":"# Reshape data and get different set (train, validation and test set)\nX_norm = data_norm.values\nX_train, y_train, X_val, y_val, _, _= train_val_test_split(X_norm, X_norm, \n                                                           val_size=VAL_SIZE, \n                                                           test_size=TEST_SIZE, \n                                                           input_size=INPUT_SIZE, \n                                                           output_size=OUTPUT_SIZE)","0d6c1c7e":"print(f\"X_train shape : {X_train.shape}\")\nprint(f\"y_train shape : {y_train.shape}\")","024edece":"plot_data_split(data, val_size=VAL_SIZE, test_size=TEST_SIZE, output_size=OUTPUT_SIZE)","6ff3c865":"def build_model(layers, n_in_features=1, n_out_features=1, gru=False, bidirectional=False):\n    \n    keras.backend.clear_session()\n    \n    n_layers = len(layers)\n    \n    ######################\n    # MODEL\n    ######################\n    \n    ## Encoder\n    encoder_inputs = Input(shape=(None, n_in_features))\n    \n    if(gru):\n        rnn_cells = [GRUCell(hidden_dim) for hidden_dim in layers]\n    else:\n        rnn_cells = [LSTMCell(hidden_dim) for hidden_dim in layers]\n        \n    if bidirectional:\n        encoder = Bidirectional(RNN(rnn_cells, return_state=True), merge_mode=None)\n        \n        encoder_outputs_and_states = encoder(encoder_inputs)\n        encoder_states = []\n        \n        if(gru):\n            bi_encoder_states = encoder_outputs_and_states[2:]\n            sep_states = int(len(bi_encoder_states)\/2)\n        \n            for i in range(0, sep_states):\n                temp = concatenate([bi_encoder_states[i],bi_encoder_states[sep_states + i]], axis=-1)\n                encoder_states.append(temp)\n        else:\n            bi_encoder_states = encoder_outputs_and_states[2:]\n            sep_states = int(len(bi_encoder_states)\/2)\n            \n            for i in range(sep_states):\n                temp = concatenate([bi_encoder_states[i],bi_encoder_states[2*n_layers + i]], axis=-1)\n                encoder_states.append(temp)\n        \n    else:  \n        encoder = RNN(rnn_cells, return_state=True)\n        encoder_outputs_and_states = encoder(encoder_inputs)\n        encoder_states = encoder_outputs_and_states[1:]\n    \n    ## Decoder\n    decoder_inputs = Input(shape=(None, n_out_features))\n    \n    if(gru):\n        if bidirectional:\n            decoder_cells = [GRUCell(hidden_dim*2) for hidden_dim in layers]\n        else:\n            decoder_cells = [GRUCell(hidden_dim) for hidden_dim in layers]\n    else:\n        if bidirectional:\n            decoder_cells = [LSTMCell(hidden_dim*2) for hidden_dim in layers]\n        else:\n            decoder_cells = [LSTMCell(hidden_dim) for hidden_dim in layers]\n        \n    decoder = RNN(decoder_cells, return_sequences=True, return_state=True)\n\n    decoder_outputs_and_states = decoder(decoder_inputs,\n                                         initial_state=encoder_states)\n    decoder_outputs = decoder_outputs_and_states[0]\n\n    decoder_dense = Dense(n_out_features, activation='linear') \n    decoder_outputs = decoder_dense(decoder_outputs)\n    \n    model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n    \n    ######################\n    # INFERENCE ENCODER\n    ######################\n    \n    encoder_model = Model(encoder_inputs, encoder_states)\n    \n    ######################\n    # INFERENCE DECODER\n    ######################\n    \n    if(gru):\n        if bidirectional:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim*2)) for hidden_dim in layers]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n        else:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim)) for hidden_dim in layers]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n    else:\n        layers_repeat = np.repeat(np.array(layers), 2)\n        if bidirectional:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim*2)) for hidden_dim in layers_repeat]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n        else:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim)) for hidden_dim in layers_repeat]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n        \n    decoder_states = decoder_outputs_and_states[1:]\n    decoder_outputs = decoder_outputs_and_states[0]\n    \n    decoder_outputs = decoder_dense(decoder_outputs)\n    \n    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n    \n    model.summary()\n    \n    return model, encoder_model, decoder_model","89e89046":"def predict(infenc, infdec, source, n_steps):\n    output = np.empty((0, n_steps, 1), np.float64)\n    \n    for row in tqdm(range(source.shape[0])):\n        \n        states = infenc.predict(source[row:row+1])\n        states = [np.reshape(state, (1, 1, state.shape[-1])) for state in states]\n        \n        output_row = np.empty((1, 0, 1), np.float64)\n        target_seq = np.zeros((1, 1, 1))\n        input_states = [target_seq] + states\n    \n        for t in range(n_steps):\n            output_states = infdec.predict(input_states)\n            output_row = np.concatenate((output_row, output_states[0]), axis=1)\n        \n            # update state\n            states = output_states[1:]\n            \n            # update target sequence\n            target_seq = output_states[0]\n            input_states = output_states\n        \n        output = np.concatenate((output, output_row), axis=0)\n            \n    return output","c2c7c32d":"def train(X_train, y_train, X_val, y_val):\n    bidirectional = True\n    layers = [512, 512, 512]\n    epochs = 20\n\n    cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-6, verbose=0),\n           EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=10, verbose=1, mode='min', restore_best_weights=True)]#,\n    \n    model, encoder, decoder = build_model(layers, X_train.shape[2], y_train.shape[2], gru=True, bidirectional=True)\n    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mean_absolute_error', metrics=['mae'])\n    \n    X_train_bis = np.pad(y_train, ((0, 0), (1, 0), (0, 0)),\n                         mode='constant')[:, :-1]\n    X_val_bis = np.pad(y_val, ((0, 0), (1, 0), (0, 0)),\n                       mode='constant')[:, :-1]\n    \n    history = model.fit([X_train, X_train_bis], y_train,\n                        validation_data=([X_val, X_val_bis],y_val),\n                        epochs=epochs,\n                        batch_size=32,\n                        shuffle=True,\n                        callbacks=cbs)\n    \n    return history, model, encoder, decoder","af081e7b":"history, model, encoder, decoder = train(X_train, y_train, X_val, y_val)","201e4ef1":"# Plotting learning curve\ndef plot_loss(history):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot train\/val MAE\n    ax[0].plot(history.history['mean_absolute_error'])\n    ax[0].plot(history.history['val_mean_absolute_error'])\n    ax[0].set_title('Model accuracy')\n    ax[0].set_ylabel('MSE')\n    ax[0].set_xlabel('Epochs')\n    ax[0].legend(['Train', 'Test'], loc='upper left')\n    \n    # Plot train\/val loss\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(['Train', 'Test'], loc='upper left')","503822f2":"plot_loss(history)","6d9079c4":"def plot_lr(history, info):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    \n    # Plot learning rate\n    ax.plot(history.history['lr'])\n    ax.set_title(f\"{info} learning rate evolution in function of epoch\")\n    ax.set_ylabel('Learning rate value')\n    ax.set_xlabel('Epochs')\n    ax.legend(['Train'], loc='upper right')","0d27165d":"plot_lr(history, info=\"Model\")","53767b5e":"# Some functions to help out with\ndef plot_predictions(y_true, y_pred, title, inter_start, inter_end):\n    \n    if(inter_start and inter_end):\n        y_true = y_true.ravel()[inter_start:inter_end]\n        y_pred = y_pred.ravel()[inter_start:inter_end]\n    \n    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n    \n    y_true = y_true.ravel()\n    y_pred = y_pred.ravel()\n    \n    err = np.mean(np.abs(y_true - y_pred))\n    \n    ax.plot(y_true)\n    ax.plot(y_pred)\n    ax.set_title(f\"{title} : {err} (MAE)\")\n    ax.set_ylabel('Value')\n    ax.set_xlabel('Index')\n    ax.legend(['Real', 'Predict'], loc='upper left')","affaf194":"def reshape(X):\n    return np.reshape(X,(1, X.shape[0], 1))","5215f617":"limit_train, limit_val, limit_test = get_limit_split(data, \n                                                     val_size=VAL_SIZE, \n                                                     test_size=TEST_SIZE, \n                                                     output_size=OUTPUT_SIZE)","3b041404":"input_val_norm = reshape(X_norm[limit_train-OUTPUT_SIZE:limit_train])\ninput_test_norm = reshape(X_norm[limit_val-OUTPUT_SIZE:limit_val])\n\n# Make prediction\ny_pred_val_normalize = predict(infenc=encoder, \n                               infdec=decoder, \n                               source=input_val_norm, \n                               n_steps=limit_val-limit_train)\ny_pred_test_normalize = predict(infenc=encoder, \n                                infdec=decoder, \n                                source=input_test_norm, \n                                n_steps=limit_test-limit_val)\n\n# Denormalize data\ny_pred_val = denormalize(y_pred_val_normalize, mean, std)\ny_pred_test = denormalize(y_pred_test_normalize, mean, std)\nX = denormalize(X_norm, mean, std)","98f662b5":"# We perform prediction on all the validation set and compare on all the validation set\nplot_predictions(X[limit_train:limit_val], y_pred_val[:,:,0], 'Model', None, None)","ce264304":"# We perform prediction on all the validation set and compare only on the 30 first examples\nplot_predictions(X[limit_train:limit_train+30], y_pred_val[:,0:30,0], 'Model', None, None)","95cfd9b6":"# We perform prediction on all the test set and compare on all the test set\nplot_predictions(X[limit_val:limit_test], y_pred_test[:,:,0], 'Model', None, None)","320684dc":"# We perform prediction on all the test set and compare only on the 30 first examples\nplot_predictions(X[limit_val:limit_val+30], y_pred_test[:,0:30,0], 'Model', None, None)","e20c1149":"# 3. Informations <a id=\"informations\"><\/a>","39badc36":"A popular common way to train a Seq2Seq network is to use teacher forcing. This is the method we will use.\n\n<p style=\"text-align:center;\">\n    <img src=\"https:\/\/drive.google.com\/uc?id=1aYV3IzNjOhDbhVRNDn71zRVonpkaOPZb\" style=\"height:75%; width:75%\"\/>\n<\/p>\n<p style=\"text-align:center;font-style:italic\">Figure 1 : Learning with \"teacher forcing\" on RNN Seq2Seq with one layer<\/p>\n\nSo in order to reuse the last example your training set will looks like :\n\n```python\nX_train = [[0, 1]\n           [1, 2]\n           [2, 3]]\ny_train = [[2, 3, 4, 5, 6]\n           [3, 4, 5, 6, 7]\n           [4, 5, 6, 7, 8]]\nX_train_bis = [[_, 1]\n               [_, 2]\n               [_, 3]]\n```\n\nGenerally we replace the _ with a 0 or 1 for training. In this case we will use a zero.","00e696d2":"```python\n# ...some code\nmodel, _, _ = model.fit([X_train, np.zeros((y_train.shape[0], y_train.shape[1], y_train.shape[2]))])\n# ...some code\n```","f9d6d734":"# 11. References <a id=\"references\"><\/a>","31b60c2c":"# 10. Conclusion <a id=\"conclusion\"><\/a>","87fbddc1":"As you can see we have no reinsertion in the decoder part, it's a **RNN Many to Many** with different input\/output.","f40a7bdc":"## 5.3 Visualization <a id=\"visualization\"><\/a>","689d3708":"# <span style=\"color:red\">Warning !<\/span>","65b1ecf7":"<p style=\"text-align:center;\">\n    <img src=\"https:\/\/unsplash.com\/photos\/iGYiBhdNTpE\/download?force=true\" style=\"height:100%; width:100%\"\/>\n<\/p>\n<p style=\"text-align:right;\">Credits : Photo by Andr\u00e9 Fran\u00e7ois McKenzie on Unsplash<\/p>","8d8971ab":"## 8.1 Learning <a id=\"learning\"><\/a>","ace6640b":"The goal of this notebook is to show and explain how to implement a Seq2Seq model with GRU\/LSTM cells (bidirectionnal or not). In this examples we will used the Bitcoin dataset and try to predict the future value a Bitcoin thanks to the past.\n\nThis notebook will not focus on data preprocessing, bitcoin data are only use as data example.","b4789737":"For more information about the architecture of a \"BiRNN\" or \"Bidirectionnal RNN\" (stacked or not), see more on [[2]](https:\/\/towardsdatascience.com\/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918)","829d1c46":"## 8.2 Learning curves <a id=\"learning_curves\"><\/a>","a313c180":"# 9. Results <a id=\"results\"><\/a>","3f8e677a":"## 5.1 Import <a id=\"import\"><\/a>","4af60207":"In this case we will use the last the time t-60 at t to predict the time t+1 at t+30. In other words, we will use the last 60 days to predict the next 30 days.","a29429d4":"# 2. Importations <a id=\"importations\"><\/a>","a17b9fd8":"I will not continue the analysis because the goal of this notebook is not to try to predict Bitcoin evolution but to show an implementation of Seq2Seq GRU\/LSTM on timeseries.","a397ce5b":"```python\n# ...some code\ny_pred_train = model.predict([X_train, np.zeros((y_train.shape[0], y_train.shape[1], y_train.shape[2]))])\ny_pred_val = model.predict([X_val, np.zeros((y_val.shape[0], y_val.shape[1], y_val.shape[2]))])\ny_pred_test = model.predict([X_test, np.zeros((y_test.shape[0], y_test.shape[1], y_test.shape[2]))])\n# ...some code\n```","6ab169ed":"# Table of Contents\n\n1. [Context](#context)  \n2. [Importations](#importations)  \n3. [Informations](#informations)\n4. [Set parameters](#set_parameters)\n5. [Data exploration](#data_exploration)  \n    5.1 [Import](#import)  \n    5.2 [General analysis](#general_analysis)  \n    5.3 [Visualization](#visualisation)  \n6. [Feature engineering](#feature_engineering)\n7. [Data preparation](#data_preparation)\n8. [Modelisation](#modelisation)  \n    8.1 [Learning](#learning)    \n    8.2 [Learning curves](#learning_curves)  \n    8.3 [Learning rate](#learning_rate)  \n9. [Results](#results)  \n10. [Conclusion](#conclusion)\n11. [References](#references)  ","ae065f5f":"## Seq2Seq timeseries bidirectionnal GRU\/LSTM Keras explains on Bitcoin datasets","e2f5f1cc":"[[1]](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/) MachineLearningMastery blog post on how to handle missing data  \n[[2]](https:\/\/towardsdatascience.com\/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918) MachineLearningMastery blog post on Seq2Seq for timeseries  \n[[3]](https:\/\/towardsdatascience.com\/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918) Towards Data science post on Deep stacked unidirectional and bidirectional lstms","83eb8118":"# 5. Data exploration <a id=\"data_exploration\"><\/a>","805c1f77":"## 5.2 General analysis <a id=\"general_analysis\"><\/a>","7950d09b":"# 8. Modelisation <a id=\"modelisation\"><\/a>","32f2ded1":"Now you know how to implement a complete Seq2Seq bidirectionnal model with GRU\/LSTM cells and use \"Teacher Forcing\" methods for learning in Keras. For more informations see the reference parts.","6a625f5f":"# 6. Feature engineering <a id=\"feature_engineering\"><\/a>","b84b8421":"Do training and prediction like this will give you a **RNN \"Many to Many\"** not a **RNN Seq2Seq** because each output is not reinserted into the decoder. You will get the following network (Figure 2) :","b1000b11":"# 4. Set parameters <a id=\"set_parameters\"><\/a>","fa34036a":"# 1. Context <a id=\"context\"><\/a>","7f1e6eb0":"After training you can do prediction like this :","56233a3a":"## 8.3 Learning rate <a id=\"learning_rate\"><\/a>","3e2da983":"# 6. Data preparation <a id=\"data_preparation\"><\/a>","07c7f288":"For simplicity I drop the **Nan** values. I recommend this post for further information [[1]](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/).","5333b47c":"Suppose you have a timeseries like this :\n```python\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n\nYou want to predict 5 days with the last two days. So, you will get a training dataset in this shape :\n```python\nX = [[0, 1]\n     [1, 2]\n     [2, 3]\n     [3, 4]\n     [4, 5]]\ny = [[2, 3, 4, 5, 6]\n     [3, 4, 5, 6, 7]\n     [4, 5, 6, 7, 8]\n     [5, 6, 7, 8, 9]\n     [6, 7, 8, 9, 10]]\n```\n\nIf you decide to split your data in training and validation set you will get (for example) :\n```python\nX_train = [[0, 1]\n           [1, 2]\n           [2, 3]]\ny_train = [[2, 3, 4, 5, 6]\n           [3, 4, 5, 6, 7]\n           [4, 5, 6, 7, 8]]\n\nX_val = [[3, 4]\n         [4, 5]]\ny_val = [[5, 6, 7, 8, 9]\n         [6, 7, 8, 9, 10]]\n```\n\nIn pratice you need to add a dimension at the the end for feature. So the input shape will be **(number of batch, timestep input, number of features).** Note this is not the only way to split your dataset. ","e413c50b":"As you can see the model performs **poorly**. You can improve the performance by adding more feature, perform some feature engineering, ...etc this is not the goal of this notebook here.","e85d02e2":"In Keras it's tricky to have different input\/ouput dimension. A way to circumvent the problem is to create this type of architecture (like above) and do training like this :","2455d00e":"<p style=\"text-align:center;\">\n    <img src=\"https:\/\/drive.google.com\/uc?id=1UV9gReji3YcZN_rumYwCi2qnvcxPJzVM\" style=\"height:75%; width:75%\"\/>\n<\/p>\n<p style=\"text-align:center;font-style:italic\">Figure 3 : RNN Seq2Seq with one layer used for prediction<\/p>","5097e263":"<p style=\"text-align:center;\">\n    <img src=\"https:\/\/drive.google.com\/uc?id=1rzyINUsUSGCxB4lGf1VN6UxANYC1D-W6\" style=\"height:75%; width:75%\"\/>\n<\/p>\n<p style=\"text-align:center;font-style:italic\">Figure 2 : RNN \"Many to Many\" with on layer in Keras<\/p>"}}