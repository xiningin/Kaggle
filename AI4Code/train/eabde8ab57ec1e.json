{"cell_type":{"dc68d34d":"code","5022d33a":"code","c26ded25":"code","90e3d975":"code","a716726c":"code","8a29279b":"code","26ef03d7":"code","bd1b3519":"code","e3083e00":"code","33d2d7d2":"code","5b4350c2":"code","673c5f35":"code","20628d23":"code","bd75d741":"code","68b39e24":"code","3f6f6060":"code","eda3bb25":"code","feb31c8b":"code","b9884df8":"code","0f001284":"code","3203e1b7":"code","640cb364":"code","999e55b9":"code","01ecb49e":"code","dcee640f":"code","7b6cc1ac":"code","35ad1907":"code","4075b9c3":"code","1980f765":"code","d186498c":"code","2a8b415c":"code","d25d75fa":"code","f58ee822":"code","af410bce":"code","cb036d63":"code","97df3b5d":"code","4726c1c7":"code","2531b6af":"code","6bb9efb5":"markdown","1150f13c":"markdown","a740711a":"markdown","c3dec478":"markdown","89c0f5c9":"markdown","183a7625":"markdown","37ec2b2a":"markdown","d78d1ac0":"markdown","c4429109":"markdown","8d889576":"markdown","3e261abf":"markdown","0f3a6c51":"markdown","a6666736":"markdown","1fe0da90":"markdown"},"source":{"dc68d34d":"# import library\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor   # data model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split #data spliter\nfrom learntools.core import *\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer","5022d33a":"# read data\n# train data path\ntrain_data_path = '..\/input\/train.csv'\ntest_data_path = '..\/input\/test.csv'\n\n# read train data\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)","c26ded25":"# let see the first few rows of data \ntrain_data.head()","90e3d975":"# let see the first few rows of data \ntest_data.head()","a716726c":"#removing ID and sales price form  the features\"\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']\n\ny = train_data['SalePrice']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)","8a29279b":"null_columns=train_data.columns[train_data.isnull().any()]\nf, ax = plt.subplots(figsize=(20,5))\nplt.xticks(rotation='vertical')\nplt.bar(null_columns, train_data[null_columns].isnull().sum())","26ef03d7":"missing_data = train_data.isnull().sum()\nmissing_data = missing_data.drop((missing_data[missing_data == 0].index)).sort_values(ascending=False)\ncol_null_value = pd.DataFrame({'Missing Ratio' :missing_data})\ncol_null_value['% value'] = (col_null_value['Missing Ratio'] \/ len(train_data))*100\ncol_null_value","bd1b3519":"# As We can see the Alley, PoolQC, MiscFeature, 'Fence', 'FireplaceQu', 'LotFrontage' contain most of null values so we will drop them. \ntrain_data = train_data.drop(columns=['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'LotFrontage' ], axis=1) \ntest_data = test_data.drop(columns=['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'LotFrontage' ], axis=1) ","e3083e00":"correlation = train_data.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(correlation, vmax=0.9, square=True)","33d2d7d2":"# delete high correlated column \ntrain_data = train_data.drop(columns=['GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea', '1stFlrSF' ], axis=1) \ntest_data = test_data.drop(columns=['GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea', '1stFlrSF' ], axis=1) ","5b4350c2":"corre_sal = train_data.corr()['SalePrice']\ncorrelation['SalePrice'].sort_values(ascending=False)","673c5f35":"# selecting cols which are correlated to price\nselected_num_col = []\ndorp_col_list = []\nfor col in correlation: \n    if (correlation['SalePrice'][col]) > 0.1 or (correlation['SalePrice'][col]) < -0.1 :\n        selected_num_col.append(col)\n    else: \n        dorp_col_list.append(col)\n        \ndorp_col_list","20628d23":"#removing columns with low correlation with sales price\n# delete high correlated column \ntrain_data = train_data.drop(columns=dorp_col_list, axis=1) \ntest_data = test_data.drop(columns=dorp_col_list, axis=1) ","bd75d741":"sale_price = train_data['SalePrice']\ntrain_data.drop(\"SalePrice\", axis = 1, inplace = True)","68b39e24":"#  lets Select numerical columns Again\nnumerical_cols = [cname for cname in train_data.columns if \n                train_data[cname].dtype in ['int64', 'float64']]\nnumerical_cols","3f6f6060":"# some of numerical columns should be categorical column lets convert them\ntrain_data['YearBuilt'] = train_data['YearBuilt'].astype(str)\ntrain_data['YearRemodAdd'] = train_data['YearRemodAdd'].astype(str)\n\n# some of numerical columns should be categorical column lets convert them\ntest_data['YearBuilt'] = test_data['YearBuilt'].astype(str)\ntest_data['YearRemodAdd'] = test_data['YearRemodAdd'].astype(str)","eda3bb25":"#  lets Select numerical columns Again\nnumerical_cols = [cname for cname in train_data.columns if \n                train_data[cname].dtype in ['int64', 'float64']]\nnumerical_cols","feb31c8b":"# plotting all the numerical features against the sales price\nf, ax = plt.subplots(figsize=(20,20))\ncpt  = 0\nfor col in numerical_cols: \n    cpt  = cpt + 1\n    ax = plt.subplot(4, 5, cpt)\n    plt.plot(sale_price, train_data[col], 'o')\n    plt.xlabel('Sale Price')\n    plt.ylabel(col)","b9884df8":"null_num_columns=train_data[numerical_cols].columns[train_data[numerical_cols].isnull().any()]\nnull_num_columns","0f001284":"#let plot distibution data for the MasVnrArea columna\nf, ax = plt.subplots(figsize=(7,5))\nsns.distplot(train_data['MasVnrArea'].dropna())","3203e1b7":"#As we can see the max of the values are zero for MasVsArea so we will fill it with zero\ntrain_data.MasVnrArea = train_data.MasVnrArea.fillna(0)","640cb364":"#select data\nX = train_data\n\nX.head()","999e55b9":"X_train = pd.get_dummies(train_data)\nX_test = pd.get_dummies(test_data)","01ecb49e":"X, test_X = X_train.align(X_test, join='left', axis=1)","dcee640f":"# lets fill the empty values in test data set with mean value\ntest_X.head()","7b6cc1ac":"test_null_col = [test_X.columns[test_X.isnull().any()]]\ntest_null_col","35ad1907":"test_X.head()","4075b9c3":"#SimpleImputer","1980f765":"test_data_imputed_values = test_X.copy()\ntrain_imputer = X.copy()\n\nimp = Imputer(missing_values='NaN', strategy='mean')\nimp.fit(train_imputer)\ntest_data_imputed_values= pd.DataFrame(imp.transform(test_data_imputed_values))\ntest_data_imputed_values.columns = test_X.columns","d186498c":"# split test and train data \nx_train, x_val, y_train, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","2a8b415c":"xbr_model = XGBRegressor(n_estimators=500, learning_rate=0.06, random_state=5) \nxbr_model.fit(x_train, y_train)\nxbr_preds = xbr_model.predict(x_val)\nxbr_mea = mean_absolute_error(y_val, xbr_preds)\nxbr_mea","d25d75fa":"mean_value = []\nmean_value.append({ \"Algorithm\" : 'XGBRegressor', \"Mean\" : xbr_mea })","f58ee822":"lin_model = LinearRegression()\nlin_model.fit(x_train, y_train)\nlin_pred = lin_model.predict(x_val)\nlin_mean = mean_absolute_error(y_val, lin_pred)\nlin_mean","af410bce":"mean_value.append({ \"Algorithm\" : 'LinearRegression', \"Mean\" : lin_mean })","cb036d63":"\ngbr_model = GradientBoostingRegressor(n_estimators=1250, learning_rate=0.04) \ngbr_model.fit(x_train, y_train)\ngbr_pred = gbr_model.predict(x_val)\ngbr_mean = mean_absolute_error(y_val, gbr_pred)\ngbr_mean","97df3b5d":"mean_value.append({ \"Algorithm\" : 'GradientBoostingRegressor', \"Mean\" : gbr_mean })","4726c1c7":"mean_df = pd.DataFrame(mean_value)\nmean_df.set_index('Algorithm', inplace=True)\npd.options.display.float_format = '{:,.2f}'.format\nmean_df","2531b6af":"test_preds= xbr_model.predict(test_data_imputed_values)\n\noutput = pd.DataFrame({'Id': test_ID,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","6bb9efb5":"## Import Data","1150f13c":"Analyse and list the column with the null values. Lets list all column with the number of null values and plot a bar chart","a740711a":"## Visualize the data","c3dec478":"Drop Column with most null values","89c0f5c9":"## Analysing and processing feature","183a7625":"## Importing Libraries","37ec2b2a":"1. ## Model Prepration and Training","d78d1ac0":"Find % of missing values for each column","c4429109":"Since XGBRegressor model is the best model let use this for prediction","8d889576":"## Analysing and processing Categorical feature","3e261abf":"Convert the categorical columns to numerica column using the one hot encoding","0f3a6c51":"Analysing the correlation between the feature and the sales price and select most correlated feature only","a6666736":"Find correlation with in the features and drop highly correlated feature","1fe0da90":"From above graph we can see some feather which are coorelated to each other we will drop them \n* YrBuilt and GarageYrBlt\n* GrLivArea and TotRmsAbvGrd\n* GarageCars and GarageArea\n* TotalBsmtSF and 1stFlrSF\n"}}