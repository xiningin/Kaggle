{"cell_type":{"c61b904e":"code","3614fe12":"code","6c49c0cf":"code","5272df89":"code","0dd88b4a":"code","4a8e4a46":"code","cf03f4b9":"code","7423fa7e":"code","d25accef":"code","d6c58e11":"code","52ebb3f2":"code","a552eccf":"code","80a11a70":"code","43dbd256":"code","d6742650":"code","0b5bb261":"code","5b308f3e":"code","beba89ab":"markdown","fa681966":"markdown","09b52ee2":"markdown","1a10e0f3":"markdown","c54124a7":"markdown","6221ff24":"markdown","09f7993d":"markdown","62a66a67":"markdown","a175d644":"markdown","9021f415":"markdown","18d10967":"markdown","8ce9e1e8":"markdown","a5cfe1ab":"markdown","ca823319":"markdown","64bde6d1":"markdown","5470bb12":"markdown","91c52e12":"markdown","9bc2acb5":"markdown"},"source":{"c61b904e":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport torchaudio\nimport librosa\nimport librosa.display\n\nimport IPython.display as ipd\nfrom IPython.display import display, HTML, clear_output\n\nimport ipywidgets as widgets\nfrom ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n\nfrom fastcore.all import *","3614fe12":"base_path = Path('\/kaggle\/input\/rfcx-species-audio-detection\/')","6c49c0cf":"rec_id = 'c12e0a62b'\naudio, sample_rate = torchaudio.load(base_path\/f'train\/{rec_id}.flac')","5272df89":"print(f'audio size: {audio.size()}, sample_rate {sample_rate}')","0dd88b4a":"ipd.display(ipd.Audio(data=audio[0], rate=sample_rate))","4a8e4a46":"def get_labels(base_path):\n    df_train_fp = (pd.read_csv(base_path\/'train_fp.csv')\n                   .assign(positive=True))\n    df_train_tp = (pd.read_csv(base_path\/'train_tp.csv')\n                   .assign(positive=False))\n    return pd.concat((df_train_fp, df_train_tp), ignore_index=True)","cf03f4b9":"%%time\ndf = get_labels(base_path)\ndf.head()","7423fa7e":"mask = df['recording_id'] == rec_id\ndf_sounds = df.loc[mask]","d25accef":"def plot_spectrogram(audio, sample_rate, df_sounds, rec_id,\n                     positive_background_color='white',negative_background_color='white',\n                     positive_frame_color='green', negative_frame_color='red',\n                     positive_font_color='green', negative_font_color='red', figsize=(12,4), dpi=150):\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize, dpi=dpi)\n    ax.tick_params(axis='x', labelsize=10)\n    ax.tick_params(axis='y', labelsize=10)\n\n    tmp = librosa.stft(audio[0].numpy())\n    tmp_db = librosa.amplitude_to_db(abs(tmp))\n    img = librosa.display.specshow(tmp_db, sr=sample_rate, x_axis='time',\n                             y_axis='hz',ax=ax)\n\n    boxes = {'positive': [], 'negative': []}\n    frames = {'positive': [], 'negative': []}\n    for i, row in df_sounds.iterrows():\n        xy = (row['t_min'], row['f_min'])\n        width = row['t_max'] - row['t_min']\n        height = row['f_max'] - row['f_min']\n        box = mpl.patches.Rectangle(xy, width, height)\n        frame = mpl.patches.Rectangle(xy, width, height)\n        if row['positive']:\n            boxes['positive'].append(box)\n            frames['positive'].append(frame)\n        else:\n            boxes['negative'].append(box)\n            frames['negative'].append(frame)\n        msg = f'species: {row[\"species_id\"]}\\nsong: {row[\"songtype_id\"]}'\n        c = positive_font_color if row['positive'] else negative_font_color\n        ax.annotate(msg, xy=(xy[0],xy[1]+height), color=c, fontsize=12)\n    for k,c in zip(['positive','negative'],[positive_background_color, negative_background_color]):\n        _boxes = mpl.collections.PatchCollection(boxes[k],facecolor='white', lw=2, alpha=.2)\n        ax.add_collection(_boxes)\n    for k,c in zip(['positive','negative'],[positive_frame_color, negative_frame_color]):\n        _frames = mpl.collections.PatchCollection(frames[k],facecolor='None',edgecolor=c, lw=2, alpha=.7)\n        ax.add_collection(_frames)\n    cax = fig.colorbar(img)\n    cax.ax.set_title('dB')\n    ax.set_title(f'Spectrogram for recording_id {rec_id}', fontsize=20)\n    return fig, ax","d6c58e11":"%%time\nplot_spectrogram(audio, sample_rate, df_sounds, rec_id);","52ebb3f2":"def get_flac_files(path:Path, condition=lambda x: x.suffix == '.flac'):\n    return {val.name: val for val in path.ls().filter(condition)}","a552eccf":"%%time\nfiles = {k: get_flac_files(base_path\/k) for k in ['train','test']}","80a11a70":"class SoundInspectorTemplate:\n    'Template for inspector GUIs'\n    def __init__(self, files, df):\n        self.files = files\n        self.df = df\n            \n    def plot(self, change):\n        'Define what is plotted here'\n        pass\n    \n    def render(self):\n        'Define widgets and their arrangement here'\n        pass","43dbd256":"@delegates() # fun decorator from fastcore: https:\/\/fastcore.fast.ai\/meta.html#delegates\nclass SingleRecordingInspector(SoundInspectorTemplate):\n    'Inspecting a single recording'\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n    def _update_file_options(self, new_set, f_min, f_max, sample_type):\n        if len(new_set) == 0: return []\n        if (f_min == f_max) or (f_max < f_min): return []\n        if len(sample_type) == 0: return []\n        files = list(self.files[new_set].keys())\n        types = [True] if sample_type == 'positive' else [False] if sample_type == 'negative' else [True,False]\n        if new_set == 'train':\n            mask_fmin = (self.df['f_min']>=f_min) \n            mask_fmax = (self.df['f_max']<=f_max) \n            mask_type = (self.df['positive'].isin(types))\n            rec_ids = self.df.loc[mask_fmin & mask_fmax & mask_type,'recording_id'].unique()\n            files = [f'{v}.flac' for v in rec_ids if f'{v}.flac' in files]\n        return files\n    \n    def update_file_options_set(self, new_set):\n        return self._update_file_options(new_set, self.f_min.value, self.f_max.value, self.samples.value)\n    \n    def update_file_options_fmin(self, f_min):\n        return self._update_file_options(self.set.value, f_min, self.f_max.value, self.samples.value)\n    \n    def update_file_options_fmax(self, f_max):\n        return self._update_file_options(self.set.value, self.f_min.value, f_max, self.samples.value)\n    \n    def update_file_options_sample_type(self, sample_type):\n        return self._update_file_options(self.set.value, self.f_min.value, self.f_max.value, sample_type)    \n    \n    def plot(self, change):\n        audio, sample_rate = torchaudio.load(self.files[self.set.value][self.file.value])\n        rec_id = self.file.value.split('.')[0]\n        mask = self.df['recording_id'] == rec_id\n        msg = f'''\n        <b>Visualizing {self.set.value}\/{self.file.value}<\/b><br>\n        <ul>\n        <li>Path: {self.files[self.set.value][self.file.value]}<\/li>\n        <li>Sampling rate: {sample_rate}<\/li>\n        <li>Tensor size: {\" x \".join(map(str,tuple(audio.size())))}<\/li>\n        '''\n        mask = mask \n        if self.samples.value == 'positive':\n            mask = mask & (self.df['positive'] == True)\n        if self.samples.value == 'negative':\n            mask = mask & (self.df['positive'] == False)\n        if self.set.value == 'train':\n            mask = mask & (self.df['f_min'] >= self.f_min.value) & (self.df['f_max'] <= self.f_max.value) \n            msg += f'''\n        <li># species: {self.df.loc[mask,\"species_id\"].nunique()}<\/li>\n        <li># song types: {self.df.loc[mask,\"songtype_id\"].nunique()}<\/li>\n        <\/ul>\n        {self.df.loc[mask].to_html()}\n        '''\n        df = self.df.loc[mask]\n        with self.out:\n            clear_output()\n            self.text.value = msg\n            ipd.display(\n                ipd.Audio(data=audio[0], rate=sample_rate),\n                plot_spectrogram(audio, sample_rate, df, rec_id)\n            );\n            show_inline_matplotlib_plots() # shortest code i've found to remove matplotlib charts during update\n    \n    def render(self):\n        self.set = widgets.Dropdown(value='train', options=['train','test'])\n        self.file = widgets.Combobox(placeholder='Choose a file', value='c12e0a62b.flac',\n                                     options=list(self.files['train'].keys()))\n        self.samples = widgets.Dropdown(value='all', options=['positive', 'negative', 'all'])\n        self.f_min = widgets.FloatText(value=0, placeholder='enter f_min')\n        self.f_max = widgets.FloatText(value=5e4, placeholder='enter f_ax')\n        self.dlink_set_files = widgets.dlink((self.set,'value'),(self.file,'options'),self.update_file_options_set)\n        self.dlink_fmin_files = widgets.dlink((self.f_min,'value'),(self.file,'options'),self.update_file_options_fmin)\n        self.dlink_fmax_files = widgets.dlink((self.f_max,'value'),(self.file,'options'),self.update_file_options_fmax)\n        self.dlink_sample_files = widgets.dlink((self.samples,'value'),(self.file,'options'),self.update_file_options_sample_type)\n        self.out = widgets.Output()\n        self.text = widgets.HTML()\n        self.submit = widgets.Button(description='Submit')\n        self.submit.on_click(self.plot)\n        \n        return widgets.HBox([\n            widgets.VBox([widgets.Label('source'),widgets.Label('to highlight'),\n                          widgets.Label('f_min (Hz)'),widgets.Label('f_max (Hz)'),widgets.Label('file')]),\n            widgets.VBox([self.set,self.samples,self.f_min,self.f_max,self.file,self.submit,self.out,self.text])\n        ])","d6742650":"gui = SingleRecordingInspector(files, df)\ngui.render()","0b5bb261":"@delegates() # fun decorator from fastcore: https:\/\/fastcore.fast.ai\/meta.html#delegates\nclass SongOrSpeciesRecordingInspector(SoundInspectorTemplate):\n    'Inspecting a single recording'\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n                \n    def plot(self, change):\n        mask = self.df['recording_id'].isin(self.recordings.value)\n        mask = mask & (self.df[f'{self.mode.value}_id'] == int(self.selection.value))\n        df = self.df.loc[mask]\n        msg = f'''\n        <b>Visualizing {self.mode.value}_id == {self.selection.value}<\/b> in recordings: {\", \".join(self.recordings.value)}\n        '''\n        self.text.value = msg\n        for i,(rec_id, _df) in enumerate(df.groupby('recording_id')):\n            with self.out.children[i]:            \n                clear_output()\n                audio, sample_rate = torchaudio.load(self.files['train'][f'{rec_id}.flac'])\n                ipd.display(\n                    ipd.Audio(data=audio[0], rate=sample_rate),\n                    plot_spectrogram(audio, sample_rate, _df, rec_id)\n                );\n                show_inline_matplotlib_plots() # shortest code i've found to remove matplotlib charts during update\n    \n    def update_selection_options(self,change):\n        'updating self.selection.options based on self.mode.value'\n        if len(change) == 0: return []\n        col = f'{change}_id'\n        return tuple(map(str,self.df[col].unique()))\n    \n    def _update_recording_options(self,selection,sample):\n        if (len(selection) == 0) or (len(sample) == 0): return []\n        col = f'{self.mode.value}_id'\n        sample_options = [True] if (sample == 'positive') else [False] if (sample == 'negative') else [True,False]\n        mask = (self.df[col] == int(selection)) & (self.df['positive'].isin(sample_options))\n        return tuple(self.df.loc[mask,'recording_id'].unique())\n        \n    def update_recording_options_selection(self,change):\n        'updating self.recording.options based on self.selection.value'\n        return self._update_recording_options(change,self.samples.value)\n    \n    def update_recording_options_samples(self,change):\n        'updating self.recording.options based on self.selection.value'\n        return self._update_recording_options(self.selection.value,change)\n                \n    def _update_tab_titles(self):\n        rec_ids = self.recordings.value\n        self.out.children = [widgets.Output() for _ in rec_ids]\n        for i, rec_id in enumerate(rec_ids):\n            self.out.set_title(i,f'rec_id: {rec_id}')\n        \n    def update_tab_titles(self, change):\n        if (change['owner'] != self.recordings) or (change['type'] != 'change') or isinstance(change['new'],dict) or (change['new']==change['old']) or (len(change['new']) == 0):\n            return\n        self._update_tab_titles()\n    \n    def render(self):\n        self.mode = widgets.Dropdown(value='species', options=['species','songtype'])\n        self.selection = widgets.Combobox(placeholder='Choose a species\/song type', value='3',\n                                     options=list(map(str,self.df['species_id'].unique())))\n        self.dlink_mode_selection = widgets.dlink((self.mode,'value'),(self.selection, 'options'), self.update_selection_options)\n        recs = self.df.loc[self.df['species_id']==3,'recording_id'].unique()\n        self.recordings = widgets.SelectMultiple(options=recs,value=['c12e0a62b'])\n        self.samples = widgets.Dropdown(value='all', options=['positive', 'negative', 'all'])\n        self.dlink_selection_recordings = widgets.dlink((self.selection, 'value'),(self.recordings,'options'), self.update_recording_options_selection)\n        self.dlink_samples_recordings = widgets.dlink((self.samples, 'value'),(self.recordings,'options'), self.update_recording_options_samples)\n        self.out = widgets.Accordion(children=[widgets.Output() for _ in self.recordings.value])\n        self._update_tab_titles()\n        self.recordings.observe(self.update_tab_titles)\n        self.text = widgets.HTML()\n        self.submit = widgets.Button(description='Submit')\n        self.submit.on_click(self.plot)\n        \n        return widgets.HBox([\n            widgets.VBox([widgets.Label('id type'),widgets.Label('species\/song id'),widgets.Label('to highlight'),\n                          widgets.Label('recording ids'),]),\n            widgets.VBox([self.mode,\n                          self.selection,\n                          self.samples,\n                          self.recordings,\n                          self.submit,self.text,self.out]),\n        ])","5b308f3e":"gui = SongOrSpeciesRecordingInspector(files, df)\ngui.render()","beba89ab":"Fourth, let's plot the spectrogram and on top of it the labels.","fa681966":"### Inspecting a single recording","09b52ee2":"Lastly, let's collect all the file paths.","1a10e0f3":"First, let's code something to parse flac files using `torchaudio`.","c54124a7":"## The big show using `ipywidgets`\n> Here we \"only\" re-use the above functions.","6221ff24":"**How to use:**\n\nFirst set each of the fields:\n* `source` = source of the recording (`train` or `test` set set) \n* `to high...(light)` = which labels to draw boxes for (`positive` = tp, `negative` = fp, `all` = \u00af\\\\\\_(\u30c4)\\_\/\u00af)\n* `f_min` = the lowest allowed frequency for a box (only `train` set)\n* `f_max` = the highest allowed frequency for a box (only `train` set)\n* `file` = the filename of the recording to render (the list will update based on the values in the other fields)\n\nHit \"Submit\"!","09f7993d":"## Setup","62a66a67":"That's it! \ud83e\udd73","a175d644":"## Inspecting a single file\n> Preamble to our two `Inspector` \ud83e\uddd0 classes.","9021f415":"## TL;DR\n\nThis notebook mainly implements two classes, `SingleRecordingInspector` and `SongOrSpeciesRecordingInspector`. These make use of `ipywidgets` and provide an interactive tool to inspect audio files, highlighting the regions with known species\/song labels. \n\nWhat they do:\n* `SingleRecordingInspector`: \n    * displays one specific spectrogram of interest\n    * renders labeled boxes filtered by `f_min`, `f_max` and whether or not the box label is tp or fp\n    * provides audio playback of the file\n* `SongOrSpeciesRecordingInspector`: \n    * displays your choice of one or more spectrograms and audio for a specific `species_id` or `songtype_id`\n    * renders labeled boxes\n    \nSo, `SingleRecordingInspector` helps to understand what is in a single audio file and `SongOrSpeciesRecordingInspector` helps to understand what a specific signal (by species or song type) looks like across audio files.\n\nHappy clicking! \ud83d\ude01","18d10967":"### Inspecting multiple recordings","8ce9e1e8":"Third, let's collect our label data.","a5cfe1ab":"**How to use:**\n\nFirst enter stuff:\n* `id type` = id (`species` or `songtype`) to filter the recordings for \n* `species\/...(songtype)` = which id of species or song type to filter the recordings for\n* `to high...(light)` = which labels to draw boxes for (`positive` = tp, `negative` = fp, `all` = \u00af\\\\\\_(\u30c4)\\_\/\u00af)\n* `recording` = `recording_id` values to select (using ctrl or shift you can select multiple!!!) (also: the list of available `recording_id` values is updated based on the other fields (sometimes this isn't instantaneous))\n\nHit \"Submit\"!\n\nNote: To not overwhelm the output for each selected recording is hidden in a designated clickable field. If you want to inspect the results don't forget to click on the respective field and \ud83e\uddd0.","ca823319":"Both inspector (\ud83e\uddd0) classes have the same structure. So let's create a template class. ","64bde6d1":"## References\n\n\n- blog article about the mel spectrogram ([TDS](https:\/\/towardsdatascience.com\/getting-to-know-the-mel-spectrogram-31bca3e2d9d0))\n\nSteve Brunton explainer videos: \n- Spectrogram example in python ([yt](https:\/\/www.youtube.com\/watch?v=TJGlxdW7Fb4))\n- The Gabor transform ([yt](https:\/\/www.youtube.com\/watch?v=EfWnEldTyPA))\n\nAnalysis notebooks:\n* https:\/\/www.kaggle.com\/prokaggler\/rfcx-species-audio-detection-eda-pytorch\n* https:\/\/www.kaggle.com\/lakshya91\/basic-audio-and-data-analysis\n* [](http:\/\/)https:\/\/www.kaggle.com\/alkahapur\/exploratory-data-analysis-and-modelling-using-cnn","5470bb12":"## Imports","91c52e12":"Second, let's listen to the audio file.","9bc2acb5":"# Interactive inspection of rainforest recordings\n> A (\u00bfhopefully?) helpful but certainly entertaining tool to inspect rainforest recordings. "}}