{"cell_type":{"cef5182c":"code","2d991632":"code","8cdd3388":"code","9ef79717":"code","5a2aed03":"code","e5b17875":"code","38b2f16b":"code","2e8a4b1a":"code","39d6e12f":"code","b6eaf9ab":"code","b1850897":"code","7060851b":"code","79dfc697":"code","572df598":"code","99a41ff3":"code","c44cbae1":"code","39e30ce4":"code","7dbb3224":"code","d524112f":"code","6a52a638":"code","e0675615":"code","673d62d2":"code","3cab721a":"code","1c01bf59":"code","19a67eef":"code","d2806c0c":"code","75aad2f4":"code","69bdc619":"code","6a343d82":"code","2668b261":"code","b691c59e":"code","96c8407d":"code","1e53a47d":"code","35972859":"code","082157ce":"code","1bc2fbdd":"code","7d0af6c1":"markdown","797cae0c":"markdown","b0650c73":"markdown","e0fe93d6":"markdown","8e153791":"markdown","32afb97a":"markdown","c1ffdbcb":"markdown","046ec2cc":"markdown","0873b86f":"markdown","c2ab4dea":"markdown","0b72563a":"markdown","34c929cf":"markdown","db8cf237":"markdown","794795d0":"markdown"},"source":{"cef5182c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom utils import *\n\nimport os\nimport time\nimport numpy as np\n\nfrom mxnet import nd, autograd, gluon\nfrom mxnet.gluon import nn, rnn\nimport mxnet as mx\nimport datetime\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\n\nimport math\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2d991632":"#This method is widely used by other kernels to implement data because the data is too large\n\ntrain = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n","8cdd3388":"#This method is widely used by other kernels to seperate the data.\n\nrows = 150_000\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nx_train = pd.DataFrame(index=range(segments), dtype=np.int16,\n                       columns=['acoustic_data'])\n                       \ny_train = pd.DataFrame(index=range(segments), dtype=np.float32, columns=['time_to_failure'])\n                       \nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    y_train.loc[segment, 'time_to_failure'] = y\n    x_train.loc[segment, 'acoustic_data']=x.mean()                   ","9ef79717":"#My god this is 10 GB guys !! What the hell, delete it\ndel train\n#Collect garbage\ngc.collect()","5a2aed03":"plt.plot(x_train['acoustic_data'][0:35])","e5b17875":"def get_technical_indicators(dataset):\n    # Create 7 and 21 days Moving Average\n    dataset['ma7'] = dataset['acoustic_data'].rolling(window=7).mean()\n    dataset['ma21'] = dataset['acoustic_data'].rolling(window=21).mean()\n    \n    # Create MACD\n    dataset['26ema'] = dataset['acoustic_data'].ewm(span=26).mean() #pd.ewma(dataset['Close'], span=26)\n    dataset['12ema'] = dataset['acoustic_data'].ewm(span=12).mean() #pd.ewma(dataset['Close'], span=12)\n    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n\n    # Create Bollinger Bands\n    dataset['20sd'] = dataset['acoustic_data'].rolling(20).std() #pd.stats.moments.rolling_std(dataset['Close'],20)\n    dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n    dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n    \n    # Create Exponential moving average\n    dataset['ema'] = dataset['acoustic_data'].ewm(com=0.5).mean()\n    \n    # Create Momentum\n    dataset['momentum'] = dataset['acoustic_data']-1\n    \n    return dataset","38b2f16b":"\n\n#Lets get these technical indicators\ntechnical_train = get_technical_indicators(x_train)","2e8a4b1a":"#Drop the NA values got from the rolling averages\ntechnical_train_na = technical_train.dropna()","39d6e12f":"def plot_technical_indicators(dataset, last_days):\n    plt.figure(figsize=(16, 10), dpi=100)\n    shape_0 = dataset.shape[0]\n    xmacd_ = shape_0-last_days\n    \n    #dataset = dataset.iloc[-last_days:, :]\n    x_ = range(3, dataset.shape[0])\n    x_ =list(dataset.index)\n    \n    # Plot first subplot\n    #plt.subplot(2, 1, 1)\n    plt.plot(dataset['ma7'],label='MA 7', color='g',linestyle='--')\n    plt.plot(dataset['acoustic_data'],label='Real Data', color='b')\n    plt.plot(dataset['ma21'],label='MA 21', color='r',linestyle='--')\n    plt.plot(dataset['upper_band'],label='Upper Band', color='c')\n    plt.plot(dataset['lower_band'],label='Lower Band', color='c')\n    plt.plot((y_train['time_to_failure']\/10)+4,label='Time to failure',color = 'y')\n    plt.fill_between(x_, dataset['lower_band'], dataset['upper_band'], alpha=0.35)\n    plt.title('Technical indicators')\n    plt.ylabel('Signal')\n    plt.legend()\n\n    \n    plt.show()","b6eaf9ab":"plot_technical_indicators(technical_train_na,1000)","b1850897":"#Let's use the FFT to get more features.\ndata_FT = x_train[['acoustic_data']]\n\nclose_fft = np.fft.fft(np.asarray(data_FT['acoustic_data'].tolist()))\nfft_df = pd.DataFrame({'fft':close_fft})\nfft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\nfft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))","7060851b":"plt.figure(figsize=(14, 7), dpi=100)\nfft_list = np.asarray(fft_df['fft'].tolist())\nfor num_ in [3, 6, 9, 25, 100]:\n    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n    plt.plot(np.fft.ifft(fft_list_m10), label='Fourier transform with {} components'.format(num_))\n    \n#plt.plot(data_FT['acoustic_data'],  label='Real')\nplt.xlabel('rows')\nplt.ylabel('amplitude')\nplt.title('Fourier transform')\nplt.legend()\nplt.show()","79dfc697":"from collections import deque\nitems = deque(np.asarray(fft_df['absolute'].tolist()))\nitems.rotate(int(np.floor(len(fft_df)\/2)))\nplt.figure(figsize=(10, 7), dpi=80)\nplt.stem(items)\nplt.title('Components of Fourier transforms')\nplt.show()","572df598":"from statsmodels.tsa.arima_model import ARIMA\nfrom pandas import DataFrame\nfrom pandas import datetime","99a41ff3":"series = data_FT['acoustic_data']\nmodel = ARIMA(series, order=(5, 1, 0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","c44cbae1":"from pandas.tools.plotting import autocorrelation_plot\nautocorrelation_plot(series)\nplt.figure(figsize=(10, 7), dpi=80)\nplt.show() ","39e30ce4":"from pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\nX = series.values\nsize = int(len(X) * 0.01)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n    model = ARIMA(history, order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)","7dbb3224":"error = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)","d524112f":"plt.figure(figsize=(12, 6), dpi=100)\nplt.plot(test, label='Real')\nplt.plot(predictions, color='red', label='Predicted')\nplt.xlabel('Rows')\nplt.ylabel('Amplitude')\nplt.title('ARIMA model on Amplitude')\nplt.legend()\nplt.show()","6a52a638":"arima = [np.NaN for i in range(41)] + list(test) #complete list for total database concatenation\nARIMA = pd.DataFrame(data=arima, columns=['ARIMA'])\n","e0675615":"df_total = pd.concat([technical_train,ARIMA,fft_df.drop(['fft'],axis=1),y_train],ignore_index=False,sort=False,axis=1)","673d62d2":"df_total.head()","3cab721a":"df_total_withoutna=df_total.dropna()\ndf_total_withoutna.iloc[:,:14].head()","1c01bf59":"def get_feature_importance_data(data_income):\n    data = data_income.copy()\n    y = data['time_to_failure']\n    X = data.iloc[:, :14]\n    \n    train_samples = int(X.shape[0] * 0.65)\n \n    X_train = X.iloc[:train_samples]\n    X_test = X.iloc[train_samples:]\n\n    y_train = y.iloc[:train_samples]\n    y_test = y.iloc[train_samples:]\n    \n    return (X_train, y_train), (X_test, y_test)","19a67eef":"# Get training and test data\n(X_train_FI, y_train_FI), (X_test_FI, y_test_FI) = get_feature_importance_data(df_total_withoutna)\nregressor = xgb.XGBRegressor(gamma=0.0,n_estimators=150,base_score=0.7,colsample_bytree=1,learning_rate=0.05)\nxgbModel = regressor.fit(X_train_FI,y_train_FI, \\\n                         eval_set = [(X_train_FI, y_train_FI), (X_test_FI, y_test_FI)], \\\n                         verbose=False)\neval_result = regressor.evals_result()\ntraining_rounds = range(len(eval_result['validation_0']['rmse']))","d2806c0c":"plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\nplt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\nplt.xlabel('Iterations')\nplt.ylabel('RMSE')\nplt.title('Training Vs Validation Error')\nplt.legend()\nplt.show()","75aad2f4":"fig = plt.figure(figsize=(8,8))\nplt.xticks(rotation='vertical')\nplt.bar([i for i in range(len(xgbModel.feature_importances_))], xgbModel.feature_importances_.tolist(), tick_label=X_test_FI.columns)\nplt.title('Feature importance of indicators.')\nplt.show()","69bdc619":"df_final = df_total_withoutna.drop(['acoustic_data','12ema','ema','momentum','ARIMA'],axis=1)\n","6a343d82":"import keras\nfrom keras.layers import Dense, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom tensorflow.python.keras import Sequential\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler","2668b261":"def get_feature_importance_data(data_income):\n    data = data_income.copy()\n    y = data['time_to_failure']\n    X = data.iloc[:, :9]\n    \n    train_samples = int(X.shape[0] * 0.65)\n \n    X_train = X.iloc[:train_samples]\n    X_test = X.iloc[train_samples:]\n\n    y_train = y.iloc[:train_samples]\n    y_test = y.iloc[train_samples:]\n    \n    return (X_train, y_train), (X_test, y_test)","b691c59e":"(X_train_FI, y_train_FI), (X_test_FI, y_test_FI) = get_feature_importance_data(df_final)\n# Get training and test data\nregressor = xgb.XGBRegressor(gamma=0.0,n_estimators=150,base_score=0.7,colsample_bytree=1,learning_rate=0.05)\nxgbModel = regressor.fit(X_train_FI,y_train_FI, \\\n                         eval_set = [(X_train_FI, y_train_FI), (X_test_FI, y_test_FI)], \\\n                         verbose=False)\neval_result = regressor.evals_result()\ntraining_rounds = range(len(eval_result['validation_0']['rmse']))","96c8407d":"lol = regressor.predict(X_test_FI)\nplt.plot(lol-5)\nplt.plot(y_test_FI.values)","1e53a47d":"\nXt=X_train_FI.values\nYt=y_train_FI.values\n\nXt.shape\n","35972859":"model = Sequential()\nmodel.add(Dense(32,input_shape = (9,),activation = 'relu'))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(1))\nmodel.compile(loss = 'mae',optimizer = 'adam')\nmodel.fit(Xt, Yt, epochs=300, verbose=0)\n\n\n","082157ce":"y_pred = model.predict(X_test_FI.values)","1bc2fbdd":"plt.plot((y_pred-5)*2.5,color='g')\nplt.plot(y_test_FI.values,color='r')","7d0af6c1":"**Wow looks good ! We'll try to use it in our model**","797cae0c":"**Here we use the Fourrier transform to get more features**","b0650c73":"**We drop** the features which are not useful","e0fe93d6":"That's what XGBoost can do","8e153791":"The input data","32afb97a":"**Let's build the network now that we have some features**","c1ffdbcb":"Small plot to see what it looks like","046ec2cc":"**That's** what a simple NN can do, hopefully we can find something better because it just looks like noise...","0873b86f":"**Make some space**","c2ab4dea":"\n**Hi, how are you?**\nI'm a big advocate of using algorithms from other fields (in this case finnace) for uncanny applications (in this case to predict earthquakes). \nI would like to put a big shout out to this git hub : https:\/\/github.com\/borisbanushev\/stockpredictionai\nWhich is amazing and from which I took out most of the code to adapt it to this problem, and you should definitely check that repo out !","0b72563a":"**The ARIMA model** is big for stock prediction, let's see if it can also predict earthquake amplitude.","34c929cf":"### Technical Analysis\n**Technical indicators** are a big part of financial analysis, this computes them.","db8cf237":"**SO**:\nBasically, this doesn't give good results, but anyway, I hope it was still useful to you and give you ideas to solve this problem, I just finished this and will update as time goes on, thanks for the read, please please please coment if you have ideas on how to use this and maybe we'll be able to work something out. Much luck to you on this project !","794795d0":"**Man does it look good ! **"}}