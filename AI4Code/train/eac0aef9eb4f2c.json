{"cell_type":{"e27f0337":"code","b8ae6a15":"code","a2b96deb":"code","3984e763":"code","f02eb6eb":"code","7bfe8e2d":"code","e40f8520":"code","086421c1":"code","01dc9bf6":"code","bd0736d4":"code","1ec4f3d1":"code","8a3e4f93":"code","a3200f20":"code","94046190":"code","2a8abd10":"code","ddb4cfb0":"code","a763e619":"code","db2b2f19":"code","f67a1673":"code","2b3210fa":"code","6ba53a40":"code","a861dba7":"code","c8183e08":"code","212da049":"code","f9609ec6":"code","f0928d35":"code","2d8d07a2":"code","36638cd6":"code","797ebb84":"markdown","a1e5f86a":"markdown","b5f0c816":"markdown","f9fbf37b":"markdown","890af44f":"markdown","3cab0e9e":"markdown","abe38a05":"markdown","b6e37ed9":"markdown","9f5b12f5":"markdown","c030208e":"markdown","a33766eb":"markdown","b1404bc5":"markdown","346e9ee3":"markdown","9bdd7868":"markdown","5054c1e8":"markdown"},"source":{"e27f0337":"# Import common libraries\nimport sys # access to system parameters\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd # functions for data processing and analysis\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport numpy as np # foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp # collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \nimport scipy.stats as ss\n\nimport sklearn # collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n\n#misc libraries\nimport random\nimport time\nimport datetime\nimport os\nimport glob\nimport math\n\n\n# Visualisation\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport matplotlib.pyplot as plt\nimport plotly\nprint(\"plotly version: {}\". format(plotly.__version__))\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot # Offline mode\ninit_notebook_mode(connected=True)\nimport seaborn as sns\n\n\n# Import common MLA libraries\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection, model_selection, metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, roc_curve, auc\nfrom scipy.stats import pearsonr\nimport skopt\n#from script_step2 import train_evaluate\n\n# Default Global settings\npd.set_option('max_columns', None)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","b8ae6a15":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","a2b96deb":"# Identify variable types\ncol_int = ['LotFrontage',\n 'LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n 'TotalBsmtSF','1stFlrSF',\n '2ndFlrSF','LowQualFinSF',\n 'GrLivArea','BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',\n 'HalfBath',\n 'BedroomAbvGr',\n 'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF',\n 'OpenPorchSF',\n 'EnclosedPorch',\n '3SsnPorch',\n 'ScreenPorch',\n 'PoolArea','MiscVal','YearBuilt',\n 'YearRemodAdd', 'MasVnrArea'] # create a list of column names to convert to integer\n\ncol_float = [] # create a list of column names to convert to float\n\ncol_ordinal = ['MSSubClass',\n 'MSZoning','Street',\n 'Alley','LotShape',\n 'LandContour',\n 'Utilities','LandSlope','OverallQual',\n 'OverallCond','ExterQual',\n 'ExterCond', 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1','BsmtFinType2','Functional','FireplaceQu',\n 'GarageType','GarageYrBlt','GarageFinish','GarageQual',\n 'GarageCond',\n 'PavedDrive','PoolQC',\n 'Fence'] # create a list of column names to convert to ordinal\n\ncol_nominal = ['LotConfig','Neighborhood',\n 'Condition1',\n 'Condition2',\n 'BldgType',\n 'HouseStyle','RoofStyle',\n 'RoofMatl',\n 'Exterior1st',\n 'Exterior2nd',\n 'MasVnrType',\n 'Foundation','Heating','HeatingQC',\n 'CentralAir', 'Electrical','KitchenQual', 'MiscFeature','SaleType','SaleCondition'] # create a list of column names to convert to nominal","3984e763":"# save target, drop train id, save test id, drop test id, concat\n\n# Save the target variable\ntarget = train['SalePrice']\n\n# Drop the ID and Target from the training set\ntrain.drop(['Id', 'SalePrice'],inplace=True, axis=1)\n\n# Store then drop the Id variable from the test set\ntest_Id = test['Id']\ntest.drop('Id', inplace=True, axis=1)\n\n# Store the number of training set rows to later identify train rows vs test rows\ntrain_nrows = train.shape[0]\n\n# Combine training and test set for data cleaning\ndata = pd.concat([train, test], axis=0)","f02eb6eb":"# Change the Year Sold, Month Sold, Day Sold values to one date field\n\ndata[\"DaySold\"] = 1\ndf2 = data[[\"YrSold\", \"MoSold\", \"DaySold\"]].copy()\ndf2.columns = [\"year\", \"month\", \"day\"]\npd.to_datetime(df2)\n\ndf2['dateInt']= df2['year'].astype(str) + df2['month'].astype(str).str.zfill(2)+ df2['day'].astype(str).str.zfill(2)\ndata['SoldDate'] = pd.to_datetime(df2['dateInt'], format='%Y%m%d')\n\ndata.drop(columns=['YrSold','MoSold','DaySold'],inplace=True)","7bfe8e2d":"# Fix variables with NA instead of None\nNAcol = ['Alley', 'PoolQC', 'Fence', 'FireplaceQu','MiscFeature']\n\nfor col in NAcol:\n    data[NAcol] = data[NAcol].replace(np.nan,\"None\")","e40f8520":"# change to correct data types\ndef change_dtypes(col_int, col_float, col_ordinal, col_nominal, df): \n    '''\n    AIM    -> Changing dtypes to save memory\n    INPUT  -> List of int column names, float column names, df\n    OUTPUT -> updated df with smaller memory  \n    '''\n    df[col_int] = df[col_int].astype('int32')\n    df[col_float] = df[col_float].astype('float32')\n    df[col_ordinal] = df[col_ordinal].astype('object')\n    df[col_nominal] = df[col_nominal].astype('object')\n\n    change_dtypes(col_int, col_float, col_ordinal, col_nominal, data)","086421c1":"# Dropping observations with too many NA, and surfacing the observations possibly requiring some imputation\n\n# If a row has at least 25% of its values as NA, then drop the row\nthreshold = len(data.columns) * .8\ndata = data.dropna(thresh=threshold)\n\nnrows = data.shape[0]\nnull_cat_col = []\nnull_int_col = []\n\ndef nullimputer(data):\n    '''\n    AIM    -> Impute null values where required, drop columns with too many nulls\n    INPUT  -> dataframe\n    OUTPUT -> dataframe with no null values\n    '''\n    ### Possible improvement: columns with null between 15-30% surfaced with recommendation to feature engineer instead of dropping\n    \n    for col in data.columns:\n        x = data[col].isnull().sum()\n        numnull = x \/ nrows\n\n        if numnull >= .15:\n            print(\"Dropped column(s):\",col)\n            data.drop([col],axis=1, inplace=True) # If a column has at least 15% of its values as NA, then drop the column\n        elif 0 < numnull < .15:\n            if col in col_int:\n                null_int_col.append(col)\n            elif col in col_ordinal:\n                null_cat_col.append(col)\n            elif col in col_nominal:\n                null_cat_col.append(col)\n            else:\n                print(\"Column with 0 to 15% null:\",col) # If a column has 0 to 15% of its values as NA, display the name of the column\n        else:\n            continue #If a column has no null then continue\n\n\n    print(\"Categorical columns with nulls:\", null_cat_col)\n    print(\"Numerical columns with nulls:\", null_int_col)\n    \n    print(\"Imputing values...\")\n    for col in null_int_col:\n        data[col] = data[col].fillna(data[col].median()) # Fill integer NA with median\n\n    for col in null_cat_col:\n        data[col] = data[col].fillna(data[col].mode()[0]) # Fill categorical NA with mode\n    \n    print(\"{} null values remain\".format(data.isna().sum().sum()))\n    \nnullimputer(data)","01dc9bf6":"# Encoding ordinal variables\nlabel = LabelEncoder()\n\nfor col in col_ordinal:\n   data[col] = label.fit(data[col].values).transform(data[col].values)","bd0736d4":"# Not going to use sold date for this version - but very useful in the future\ndata = data.drop(columns=['SoldDate'])","1ec4f3d1":"data['OverTot'] = data['OverallQual'] + data['OverallCond']\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['Bathrms'] = data.BsmtFullBath + (data.BsmtHalfBath\/2) + data.FullBath + (data.HalfBath\/2)","8a3e4f93":"# Create a new feature with the average sqr foot per neighbourhood\n# This code not currently working, needs to be looked at\n\n#train['price_per_sqrf'] = target \/ train['LotArea']\n#train['price_per_sqrf'] = train.groupby('Neighborhood')['price_per_sqrf'].transform(lambda x: x.median())\n\n#d = {}\n#for indice_fila, x_train in train.iterrows():\n#    d.update({x_train['Neighborhood']:x_train['price_per_sqrf']})\n    \n#test['price_per_sqrf'] = 0.00\n#for indice, x_test in test.iterrows():\n#    test.loc[test.index == indice ,'price_per_sqrf'] = d[x_test['Neighborhood']]","a3200f20":"# Encoding nominal variables\n\nfor col in col_nominal:\n    data_dummies = pd.get_dummies(data[col])\n    data = pd.concat([data, data_dummies], axis=1)\n    data.drop(col, inplace=True, axis=1)","94046190":"# Drop duplicate columns - find out later why this happened\ndata = data.loc[:,~data.columns.duplicated()]","2a8abd10":"# Seperate back out the train and test datasets to visualise the training set\n\n# train dataset\ntrain = data.iloc[:train_nrows,:]\n\n# test dataset\ntest = data.iloc[train_nrows:,:]","ddb4cfb0":"# Histogram of target\nfig = go.Figure(data=[go.Histogram(x=target)])\nfig.update_layout(title=\"Histogram of the target variable\")\nfig.show()","a763e619":"# Fix the skew of the target variable\ntarget = np.log(target)","db2b2f19":"# Top 10% strongest correlations with target\ncorrdata = pd.concat([train,target],axis=1)\ncorr_matrix = abs(corrdata.corr())\n\ncolnosaleprice = train.columns\ncolnosaleprice = colnosaleprice[colnosaleprice != 'SalePrice']\n#corr_matrix = corr_matrix.drop(colnosaleprice)\n\nnocorr = []\n\ncorr_matrix = corr_matrix.replace(1,0)\nfor col in corr_matrix.columns:\n    for i in corr_matrix[col]:\n        if corr_matrix[col].max() < .8:\n            nocorr.append(col)\n        else:\n            continue\n\ncorr_matrix = corr_matrix.drop(columns=nocorr, axis=1)\ncorr_matrix_saleprice = corr_matrix[corr_matrix.index=='SalePrice']\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_matrix_saleprice, cmap=\"Blues\", vmax=1,\n            square=True, linewidths=.2, cbar_kws={\"shrink\": .7})","f67a1673":"#top 10% strongest correlations with any other variable\nc = data.corr().abs().unstack().sort_values(kind=\"quicksort\")\nc[(c > .8) & (c != 1)].tail(10)","2b3210fa":"# Regression problem\n\nMLA = [\n    \n    # Linear Regression\n    linear_model.LinearRegression(),\n    linear_model.Ridge(),\n    \n    # Decision Tree\n    tree.DecisionTreeRegressor(random_state = 0),\n    \n    #Ensemble Methods\n    ensemble.BaggingRegressor(random_state = 0),\n    ensemble.ExtraTreesRegressor(random_state = 0),\n    ensemble.GradientBoostingRegressor(random_state = 0),\n    ensemble.RandomForestRegressor(random_state = 0),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsRegressor(),\n\n    #xgboost\n    XGBRegressor(random_state = 0), \n    \n    ]","6ba53a40":"X_train, X_test, y_train, y_test = train_test_split(train, target, random_state = 0, test_size = 0.5, train_size = 0.5)","a861dba7":"#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy', 'MLA Test Accuracy', 'MLA RMSE']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\n\n\nfor alg in MLA:\n    #index through MLA and save performance to table\n    \n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model\n    fitted_alg = alg.fit(X_train, y_train)\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = fitted_alg.score(X_train, y_train)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = fitted_alg.score(X_test, y_test)\n    \n    # RMSE score\n    MLA_compare.loc[row_index, 'MLA RMSE'] = np.sqrt(mean_squared_error(y_test, alg.predict(X_test)))\n\n    row_index+=1\n\n    \n#print and sort table\nMLA_compare.sort_values(by = ['MLA RMSE'], ascending = True, inplace = True)\nMLA_compare","c8183e08":"submission = pd.DataFrame(test_Id)","212da049":"submission[\"SalePrice\"] = \"\"","f9609ec6":"submission","f0928d35":"# Select the best test scoring algorithm\nGBR = ensemble.GradientBoostingRegressor(random_state = 0)\nGBR.fit(train, target)\nlog_predict = GBR.predict(test)\nsubmission['SalePrice'] = np.exp(log_predict)","2d8d07a2":"submission","36638cd6":"submission.to_csv('submission.csv', index=False)","797ebb84":"## Prepare submission","a1e5f86a":"## Data Visualisation","b5f0c816":"Machine Learning Model","f9fbf37b":"### Correcting","890af44f":"### Finish Pre-processing","3cab0e9e":"## Data Cleaning","abe38a05":"Pre-Processing","b6e37ed9":"### Tune Hyperparameters","9f5b12f5":"## Import Common Libraries","c030208e":"### Evaluate Model Performance","a33766eb":"# House Price Prediction","b1404bc5":"## Import the data","346e9ee3":"### Converting","9bdd7868":"The methods and code in this notebook have been influenced by public notebooks in the kaggle competition: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques","5054c1e8":"## Some Feature Engineering"}}