{"cell_type":{"baea8afe":"code","269d760b":"code","ec4fc970":"code","ff6d55f5":"code","ececa517":"code","8bb97076":"code","078b836d":"code","c050506a":"code","8abb9cb4":"code","b12d9256":"code","bba35699":"code","2d62c2da":"code","dbc2889d":"code","aaf0cf96":"code","8a726388":"code","190089be":"code","2a9bad15":"code","d1245c71":"code","a0bf64af":"code","99a009be":"code","4dff763d":"code","c8b187dd":"code","a861e61e":"code","14754faf":"code","3952b8e9":"code","1756ed44":"code","0a569536":"code","85b5f18a":"code","66a5985e":"code","51bae982":"code","c611d13c":"code","d2032b4b":"code","67408c9d":"code","8cae688d":"code","b09ee8b8":"code","fb6347b0":"code","4a4f4eb8":"code","ed575078":"code","7f4346dc":"code","39a4a98f":"code","83704fb8":"code","91195661":"code","b967f37f":"code","26a7025f":"code","aec890cf":"code","7d17730d":"code","78217898":"code","a8bfdfa4":"code","a0a2ddb9":"code","447c4ea2":"code","a77b2b4b":"code","575cc07e":"code","1249f78a":"code","4d9ac2e9":"code","bc600b26":"code","11bfaea0":"code","d129f2f4":"code","5959320b":"code","d78fc735":"code","acabda5e":"code","bcf84ace":"code","6d59574a":"code","ecad9b35":"code","469a476a":"markdown","e274d7ac":"markdown","9e703d7c":"markdown","a37d24cc":"markdown","907ccac6":"markdown","8652cb88":"markdown","15646ac3":"markdown","5bb4c297":"markdown","39f61900":"markdown","78fd73e3":"markdown"},"source":{"baea8afe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\n%matplotlib inline\nfrom matplotlib import rcParams\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")","269d760b":"rcParams['figure.figsize'] = (8.0, 5.0)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","ec4fc970":"file_1 = pd.read_csv('\/kaggle\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')","ff6d55f5":"df_orig = pd.DataFrame(file_1)","ececa517":"df_orig.head()","8bb97076":"df = df_orig.copy()","078b836d":"# Dropping the id and name columns.\ndf.drop('CustomerId', axis=1, inplace=True)\ndf.drop('Surname', axis=1, inplace=True)\ndf.head()","c050506a":"df.info()","8abb9cb4":"# Converting NumOfProducts column to categorical.\ndf['NumOfProducts'] = df['NumOfProducts'].astype(int)\ndf['NumOfProducts'] = df['NumOfProducts'].astype(object)","b12d9256":"# Creating seperate columns for categories\ndf = pd.get_dummies(df)\ndf.head()","bba35699":"# Dropping excess columns\ndf.drop('Geography_Spain', axis=1, inplace=True)\ndf.drop('Gender_Male', axis=1, inplace=True)\ndf.drop('NumOfProducts_2', axis=1, inplace=True)\ndf.head()","2d62c2da":"df.columns","dbc2889d":"df = df[['CreditScore', 'Age', 'Tenure', 'Balance', 'HasCrCard',\n       'IsActiveMember', 'EstimatedSalary', 'Geography_Germany',\n       'Geography_France', 'Gender_Female', 'NumOfProducts_1',\n       'NumOfProducts_4', 'NumOfProducts_3', 'Exited']] ","aaf0cf96":"# Correlation Matrix\ncorr = df.corr()\ncorr.style.background_gradient()","8a726388":"# Converting all Balances more than 0 to 1\ndf['Balance'] = df['Balance'].clip(upper=1)","190089be":"# Dropping insignificant features as decided during previous excercises.\n# Age p-value = 0.0\n# Credit Score p-value = 0.0085\n# Balance p-value = 0.0\n# Estimated Salary p-value = 0.1222\ndf.drop('EstimatedSalary', axis=1, inplace=True)\ndf.drop('HasCrCard', axis=1, inplace=True)\n#df.drop('NumOfProducts', axis=1, inplace=True)\ndf.drop('Tenure', axis=1, inplace=True)\ndf.head()","2a9bad15":"df = df.applymap(np.int64)","d1245c71":"df.loc[df.Balance == 0, 'Balance'] = -1\ndf.loc[df.IsActiveMember == 0, 'IsActiveMember'] = -1\ndf.loc[df.Geography_Germany == 0, 'Geography_Germany'] = -1\ndf.loc[df.Geography_France == 0, 'Geography_France'] = -1\ndf.loc[df.Gender_Female == 0, 'Gender_Female'] = -1\ndf.loc[df.NumOfProducts_1 == 0, 'NumOfProducts_1'] = -1\ndf.loc[df.NumOfProducts_3 == 0, 'NumOfProducts_3'] = -1\ndf.loc[df.NumOfProducts_4 == 0, 'NumOfProducts_4'] = -1\ndf.loc[df.Exited == 0, 'Exited'] = -1\ndf.head()","a0bf64af":"# Scaling the data\nfrom sklearn.preprocessing import scale\n\ndf['CreditScore'] = scale(df['CreditScore'])\ndf['Age'] = scale(df['Age'])\n#df['Tenure'] = scale(df['Tenure'])\n#df['NumOfProducts'] = scale(df['NumOfProducts'])\n\ndf.head()","99a009be":"df.columns","4dff763d":"X = df[['CreditScore', 'Age', 'Balance', 'IsActiveMember', 'Geography_Germany',\n       'Geography_France', 'Gender_Female', 'NumOfProducts_1',\n       'NumOfProducts_4', 'NumOfProducts_3']]\ny = df['Exited']","c8b187dd":"# Using Lasso to know features significance.\nfrom sklearn.linear_model import Lasso\nnames = df.columns\nlasso = Lasso(alpha=0.1)\nlasso_coef = lasso.fit(X, y).coef_\n_ = plt.plot(range(len(names)-1), lasso_coef)\nplt.xticks(range(len(names)-1), names, rotation=60)","a861e61e":"from imblearn.over_sampling import SMOTE","14754faf":"smote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)","3952b8e9":"a = pd.Series(y_sm)\na.value_counts()","1756ed44":"# Splitting the data in test data and train data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=51)","0a569536":"from sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","85b5f18a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","66a5985e":"# Fitting the data\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_test)\naccuracy_score(y_pred, y_test)","51bae982":"# Using GridSearch to find the best parameters\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\nlogreg_cv=GridSearchCV(logreg,grid,cv=10, scoring='accuracy', refit=True, n_jobs=-1)\nlogreg_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)\nprint(logreg_cv.best_estimator_)","c611d13c":"# ROC Curve\n\ny_pred_prob = logreg_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.legend()","d2032b4b":"# ROC AUC score. The area under ROC curve.\n\nroc_auc_score(y_test, y_pred_prob)","67408c9d":"\nprint(confusion_matrix(y_test, logreg_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","8cae688d":"# Classification Report\n\nprint(classification_report(y_test, logreg_cv.predict(X_test)))","b09ee8b8":"knn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X_train,y_train)\naccuracy_score(knn.predict(X_test), y_test)","fb6347b0":"knn = KNeighborsClassifier()\nk_grid={'n_neighbors':np.arange(2,20)}\nknn_cv=GridSearchCV(knn, k_grid, cv=10, refit=True, n_jobs=-1)\nknn_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\nprint(\"accuracy :\",knn_cv.best_score_)\nprint(knn_cv.best_estimator_)","4a4f4eb8":"y_pred_prob = knn_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='k Nearest Neighbors')\nplt.legend()","ed575078":"roc_auc_score(y_test, y_pred_prob)","7f4346dc":"print(confusion_matrix(y_test, knn_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","39a4a98f":"print(classification_report(y_test, knn_cv.predict(X_test)))","83704fb8":"\nCs = [0.1, 1, 10, 100]\ngammas = [0.001, .01, 0.1, 1, 10]\nparam_grid = {'C': Cs, 'gamma': gammas,'kernel': ['rbf'], 'probability':[True]}\n\nSVM_rbf_cv = GridSearchCV(SVC(), param_grid, cv=3, refit=True, n_jobs=-1)\nSVM_rbf_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",SVM_rbf_cv.best_params_)\nprint(\"accuracy :\",SVM_rbf_cv.best_score_)\nprint(SVM_rbf_cv.best_estimator_)","91195661":"y_pred_prob = SVM_rbf_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='SVM')\nplt.legend()","b967f37f":"roc_auc_score(y_test, y_pred_prob)","26a7025f":"print(confusion_matrix(y_test, SVM_rbf_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","aec890cf":"print(classification_report(y_test, SVM_rbf_cv.predict(X_test)))","7d17730d":"\nCs = [0.1, 1, 10, 100]\ngammas = [0.001, .01, 0.1, 0.5]\n\nparam_grid = {'C': Cs, 'gamma': gammas,'probability':[True],'kernel': ['poly'],'degree':[2,3] }\nSVM_poly_cv = RandomizedSearchCV(estimator = SVC(), param_distributions = param_grid, n_iter = 10, cv = 3, random_state=51, n_jobs = -1, refit=True)\nSVM_poly_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",SVM_poly_cv.best_params_)\nprint(\"accuracy :\",SVM_poly_cv.best_score_)\nprint(SVM_poly_cv.best_estimator_)","78217898":"y_pred_prob = SVM_poly_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='SVM')\nplt.legend()","a8bfdfa4":"roc_auc_score(y_test, y_pred_prob)","a0a2ddb9":"print(confusion_matrix(y_test, SVM_poly_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","447c4ea2":"print(classification_report(y_test, SVM_poly_cv.predict(X_test)))","a77b2b4b":"\nn_est = [int(x) for x in np.linspace(start = 50, stop = 1000, num = 10)]\nm_depth = [int(x) for x in np.linspace(5, 50, num = 5)]\nmin_samp = [3, 5, 6, 7, 10, 11]\nm_ftr = ['auto']\n\nparam_grid = {'max_depth': m_depth, 'max_features': m_ftr,'n_estimators': n_est,'min_samples_split': min_samp}\nRF_cv = RandomizedSearchCV(estimator = RandomForestClassifier(), n_iter=100, param_distributions =  param_grid, random_state=51, cv=3, n_jobs=-1, refit=True)\nRF_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",RF_cv.best_params_)\nprint(\"accuracy :\",RF_cv.best_score_)\nprint(RF_cv.best_estimator_)","575cc07e":"n_est = [366]\nm_depth = [38]\nmin_samp = [5]\nm_ftr = ['auto']\n\nparam_grid = {'max_depth': m_depth, 'max_features': m_ftr,'n_estimators': n_est,'min_samples_split': min_samp}\nRF_cv_10 = RandomizedSearchCV(estimator = RandomForestClassifier(), n_iter=200, param_distributions =  param_grid, random_state=51, cv=10, n_jobs=-1, refit=True)\nRF_cv_10.fit(X_train,y_train)","1249f78a":"y_pred_prob = RF_cv_10.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='Random Forest')\nplt.legend()","4d9ac2e9":"roc_auc_score(y_test, y_pred_prob)","bc600b26":"print(confusion_matrix(y_test, RF_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","11bfaea0":"print(classification_report(y_test, RF_cv.predict(X_test)))","d129f2f4":"\nm_dep = [5,6,7,8]\ngammas = [0.01,0.001,0.001]\nmin_c_wt = [1,5,10]\nl_rate = [0.05,0.1, 0.2, 0.3]\nn_est = [5,10,20,100]\n\nparam_grid = {'n_estimators': n_est, 'gamma': gammas, 'max_depth': m_dep,\n              'min_child_weight': min_c_wt, 'learning_rate': l_rate}\n\nxgb_cv = RandomizedSearchCV(estimator = XGBClassifier(), n_iter=100, param_distributions =  param_grid, random_state=51, cv=3, n_jobs=-1, refit=True)\nxgb_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",xgb_cv.best_params_)\nprint(\"accuracy :\",xgb_cv.best_score_)\nprint(xgb_cv.best_estimator_)","5959320b":"m_dep = [7]\ngammas = [0.01]\nmin_c_wt = [1]\nl_rate = [0.2]\nn_est = [100]\n\nparam_grid = {'n_estimators': n_est, 'gamma': gammas, 'max_depth': m_dep,\n              'min_child_weight': min_c_wt, 'learning_rate': l_rate}\n\nxgb_cv_10 = RandomizedSearchCV(estimator = XGBClassifier(), n_iter=100, param_distributions =  param_grid, random_state=51, cv=10, n_jobs=-1, refit=True)\nxgb_cv_10.fit(X_train,y_train)","d78fc735":"y_pred_prob = xgb_cv_10.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='xgb')\nplt.legend()","acabda5e":"roc_auc_score(y_test, y_pred_prob)","bcf84ace":"print(confusion_matrix(y_test, xgb_cv_10.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","6d59574a":"print(classification_report(y_test, xgb_cv_10.predict(X_test)))","ecad9b35":"algos = [logreg_cv, knn_cv, SVM_rbf_cv, SVM_poly_cv, RF_cv, xgb_cv]\nlabels = ['Logistic Regression', 'knn', 'SVM rbf', 'SVM poly', 'Random Forest', 'XGB']\n\nplt.figure(figsize = (12,8))\nplt.plot([0,1], [0,1], 'k--')\n\nfor i in range(len(algos)):\n    y_pred_prob = algos[i].predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    plt.plot(fpr, tpr, label=labels[i])\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')","469a476a":"## Logistic Regression:","e274d7ac":"## Random Forest","9e703d7c":"# Predicting Churn:","a37d24cc":"## SVM with 'poly' Kernal","907ccac6":"## Extreme Gradient boosting:","8652cb88":"## SVM with 'rbf' Kernal:","15646ac3":"## The XGB Classifier gives the best roc_auc score of 0.95. Also the best precision and recall.","5bb4c297":"# Applying:","39f61900":"## kNN:","78fd73e3":"## Over Sampling using SMOTE"}}