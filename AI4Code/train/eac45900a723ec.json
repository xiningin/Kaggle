{"cell_type":{"23cac6cf":"code","a96706e8":"code","25ef87b0":"code","3bdd43d2":"code","4888d8ce":"code","9b3b7bac":"code","90c60480":"code","4bd10a0f":"code","feab9a60":"code","b4e38551":"code","3091b2ed":"code","b3bfa385":"code","4fdc475e":"code","f75e7c2d":"code","72221d0c":"code","68551263":"code","76ce16b3":"markdown","08ec3bda":"markdown","07b4692d":"markdown","982a05cb":"markdown","d4f995fc":"markdown"},"source":{"23cac6cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a96706e8":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom mpl_toolkits.mplot3d import Axes3D","25ef87b0":"df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n# print(df.isna().values.any())\n# print(X_test.isna().values.any())\n","3bdd43d2":"label = df.label\nX = df.drop(['label'],axis=1)\nX.shape","4888d8ce":"scaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\nX_test = scaler.transform(X_test)","9b3b7bac":"pca2 = PCA(n_components=331)\npca2.fit(X)\nX_new2 = pca2.transform(X)\n\npca3 = PCA(n_components=3)\npca3.fit(X)\nX_new3 = pca3.transform(X)\n","90c60480":"plt.scatter(X_new2[:,0],X_new2[:,1],c=df.label,alpha=0.1)\nplt.title('Digitizer Plot in 2D')\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.show()","4bd10a0f":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_new3[:,0],X_new3[:,1], zs=X_new3[:,2],c=df.label)\nplt.show()","feab9a60":"pca = PCA().fit(X)\nqsum = np.cumsum(pca.explained_variance_ratio_) #list of the cumulative sum of the variance\nthreshold = 0.99 #threshold of variance retention\n\nnum_features = np.argmax(qsum>threshold) + 1 #number of features needed to retain variance within the threshold\nprint(num_features)","b4e38551":"plt.plot(qsum)\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.xlim(0,600)\nplt.grid(True)\nplt.show()","3091b2ed":"pca533 = PCA(n_components=533)\npca533.fit(X)\nX_new533 = pca533.transform(X)\n\nX_test_new = pca533.transform(X_test)","b3bfa385":"X_train, X_valid, y_train, y_valid = train_test_split(X_new533, label, test_size=0.2, random_state=1)\nclf = MLPClassifier(hidden_layer_sizes=(100,50,25),solver='lbfgs',alpha=1, random_state=1).fit(X_train, y_train)\nclf.predict(X_valid)\nprint(clf.score(X_valid,y_valid))","4fdc475e":"test_predict = clf.predict(X_test_new)","f75e7c2d":"test_predict","72221d0c":"len(test_predict)","68551263":"\nsubmission = pd.Series(test_predict,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),submission],axis = 1)\nsubmission.to_csv(\"final_submission.csv\",index=False)\nsubmission.head()","76ce16b3":"Originally when I ran the simulations, I didnt scale the the data and when I ran PCA I got a value of features to keep being 331. When I implemented standard scaler I ended up with a larger value for number of features to keep, which i find very interesting. I didn't think there would be much difference in the overall accuracy of the algorithm since all of the value are within a specific range.","08ec3bda":"Using a threshold of 99% (or 0.99) it was seen that you can reduce the number of features from 784 to 533 and still retain 99% of the variance in the data.\n\nBelow shows a graph of the retained variance as you increase the number of features. As a results our 2D and 3D representation of the data was not a very good true representation of our data. But unfortunately we can not visualize data above 3 dimensions.","07b4692d":"The data has 42000 data points with each point having 784 features. Next I will run Principal Component Analysis into 2D & 3D just to visualize the data. Since I am reducing the features from 784 to 2 and 3, I wouldnt expect to gain much info by seeing the graph but can be something interesting to see.","982a05cb":"PCA can be used to speed up machine learning algorithms by reducing the amount of features. When you reduce the amount of features you lose valuable information but you can find the optimal amount of features by ensuring the retained variance in the data does not fall below a certain value. Often values in the .90 to .99 are commonly used.","d4f995fc":"Using 533 features we will complete a neural network classification"}}