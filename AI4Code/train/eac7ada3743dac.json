{"cell_type":{"5c7e768f":"code","ada46e58":"code","83807aa5":"code","178b9b1a":"code","6ece821b":"code","86134bd6":"code","54bfb1db":"code","14027d8a":"code","231d58d6":"code","9732db69":"code","3b1b9bfe":"code","dbe8a01f":"code","7bf8b785":"code","566d723c":"code","50981f48":"code","58389f4c":"code","0b08f8dc":"code","0d4c00a0":"code","fa54c95c":"code","58874ab6":"code","b7b8653e":"code","e0e2603f":"code","328c0dcd":"code","b38c8653":"code","dd2bbc10":"code","453ccf27":"code","b12aa256":"code","c575184f":"code","25d09a02":"code","740e2cb7":"code","9cb5774b":"code","af0407ad":"code","4445d1aa":"markdown","71fed293":"markdown","1cc30f25":"markdown","41e9c27d":"markdown","3381321e":"markdown","7660726f":"markdown","9dd39637":"markdown","2880ca2a":"markdown","ffd36668":"markdown","6d856781":"markdown","cbedc56d":"markdown","db809e59":"markdown","6f79b799":"markdown","b4e54cba":"markdown","f583c053":"markdown","862f6565":"markdown","f6737eb0":"markdown","104d2381":"markdown"},"source":{"5c7e768f":"#importing modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","ada46e58":"#Loading the dataset\ndf = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf","83807aa5":"#getting first impression on the dataset\n\ndf.info()","178b9b1a":"#Let's confirm whether our suspect about missing value is true\n\ndf.isna().sum()","6ece821b":"df.drop(columns=[\"Cabin\"],inplace=True)\ndf","86134bd6":"#Let's visualize the most frequently appeared value in Embarked column\nplt.style.use(\"seaborn-whitegrid\")\ndf[\"Embarked\"].value_counts().plot.bar(figsize=(10,6),color=\"red\");","54bfb1db":"df[\"Embarked\"].fillna('S',inplace=True)","14027d8a":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\ncol_feature = [\"Age\"]\nimputer = SimpleImputer(strategy=\"mean\")\ntransformer = ColumnTransformer([(\"imputer\", imputer,col_feature)])\ndf[\"Age\"]=transformer.fit_transform(df)","231d58d6":"df","9732db69":"df.isna().sum()","3b1b9bfe":"df.info()","dbe8a01f":"sex_category = df[\"Sex\"].unique()\nsurvibality= []\nfor val in sex_category:\n    survibality.append(df[\"Sex\"].loc[(df[\"Sex\"]==val) & (df[\"Survived\"]==1)].count())\n    \nfig,ax = plt.subplots(figsize=(10,6))\nax.bar(sex_category,survibality,color=\"palevioletred\");","7bf8b785":"age_category = df[\"Age\"].unique()\nsurvibality_age= []\nfor val in age_category:\n    survibality_age.append(df[\"Age\"].loc[(df[\"Age\"]==val) & (df[\"Survived\"]==1)].count())\n    \nfig,ax = plt.subplots(figsize=(10,6))\nax.bar(age_category,survibality_age,color=\"indigo\");","566d723c":"#let's visualize data distribution among the numerical columns\ndf.plot.hist(figsize=(10,18),bins=50,subplots=True);","50981f48":"from sklearn.model_selection import train_test_split\nX = df.drop(columns=\"Survived\")\ny= df[\"Survived\"]\nX.drop(columns=[\"Ticket\",\"Name\",\"PassengerId\"],inplace=True)\nX","58389f4c":"X[\"Sex\"] = pd.Categorical(X[\"Sex\"])\nX[\"Embarked\"] = pd.Categorical(X[\"Embarked\"])\nX[\"Sex_cat\"] = X[\"Sex\"].cat.codes\nX[\"Embarked_cat\"] = X[\"Embarked\"].cat.codes\nX.drop([\"Sex\",\"Embarked\"],axis=1,inplace=True)\nX","0b08f8dc":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","0d4c00a0":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(X_train,y_train)","fa54c95c":"clf.score(X_test,y_test)","58874ab6":"from sklearn.model_selection import RandomizedSearchCV\nclf.get_params()","b7b8653e":"grid = {\n    'n_estimators':[200,400,600,800,1200],\n    'max_features':['auto','sqrt'],\n    'max_depth':[None,5,15,25,30,60,90],\n    'min_samples_split':[2,5,10,15,100],\n    'min_samples_leaf':[1,2,4,8,10],\n    'bootstrap':[True,False]\n}\n\nrandomcv_clf = RandomizedSearchCV(estimator=clf,param_distributions=grid,n_iter=20,cv=5,verbose=2,random_state=42)\nrandomcv_clf.fit(X_train,y_train)","e0e2603f":"randomcv_clf.score(X_test,y_test)","328c0dcd":"from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,roc_curve","b38c8653":"y_preds=randomcv_clf.predict(X_test)\ny_preds\n","dd2bbc10":"accuracy_score(y_test,y_preds)","453ccf27":"precision_score(y_test,y_preds)","b12aa256":"recall_score(y_test,y_preds)","c575184f":"f1_score(y_test,y_preds)","25d09a02":"roc_auc_score(y_test,y_preds)","740e2cb7":"fpr,tpr,thresholds = roc_curve(y_test,y_preds)\nplt.plot(fpr,tpr);\n","9cb5774b":"from sklearn.metrics import confusion_matrix,classification_report\n\nprint(classification_report(y_test,y_preds))","af0407ad":"print(confusion_matrix(y_test,y_preds))","4445d1aa":"**how about checking survibality `age-wise`**","71fed293":"**For this problem we will use `RandomForestClassifier` to train the model and evaluate score**","1cc30f25":"## **Evaluating our model**","41e9c27d":"**`687` out of `891` missing values in `Cabin` column, `177` out of `891` missing values in `Age` column and 2 in `Embarked` column. \nDataset about disaster itself contains lots of disaster**","3381321e":"**Time to turn our `Sex` and `Embarked` column to categories**","7660726f":"## Exploratory Data Analysis","9dd39637":"**Time to train our model but before that we'll split our `X` and `y` to `train` and `test` set. we keep 20% for `test` set \nand rest of the data we will use for training and evaluate our score on `test` data**","2880ca2a":"**We can fill this missing values with other values but that will lead to not so perfect machine learning model. Filling 687 values,\nwhich is more than 50% data, not so wise move afterall, atleast in my point of view. We have no other choice left but to drop the entire\n`Cabin` column**","ffd36668":"**Still not done yet missing datas, we still have two more columns with missing data.\nLet's begin with `Embarked` column where we have only 2 missing values. we can drop the entire row, but that'll remove data from other columns too and as data is expensive and we already sacrificed a column, how about filling these two missing values.**\n\n**But, what should we fill? How about the most frequently appeared value in that column,i think that'll be perfect**","6d856781":"**`Female` survibality is 2x more than `Male` survibality**","cbedc56d":"**Hyperparameter Tuning on our classifier model**","db809e59":"**Finally,all we need to fill `Age` column but here is also sort of same problem exist like in `Cabin` column.\nFilling 177 values with `mean` or `median` will definitely lead to erroneous model. What we can drop those entire row related\nto the `Age` column since that'll be better option here.**\n\n**On this notebook,i'm still filling these values with `mean` but better option is dropping the entire row**","6f79b799":"**So now we finally dealt with the missing values, next challenge upfront us is string values. For `Machine Learning` , we need to be\nsure all columns must have numerical values. But, before fix this problem lets visualize our data.**","b4e54cba":"## End-To-End Machine Learning Project (**Titanic Disaster**) Notebook ","f583c053":"## From the above result it seems like we have some missing data in our dataset","862f6565":"**First we visualize being `male` or `female` gave any advantage on survibality or not**","f6737eb0":"**Let's split our data in `X` and `y` where `X` contains independent variables and `y` contain dependent variables**\n**As the `Name` and `Ticket` column not so helpful regarding detecting survibality, we simply drop those columns**","104d2381":"**We have a clear winner, value of `S`[Southampton Port] in `Embarked` column has a total count of more than 600 out of 891. Let's fill those two\nmissing values with `S`**"}}