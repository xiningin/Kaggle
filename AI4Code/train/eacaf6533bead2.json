{"cell_type":{"fa8e4254":"code","caa370ea":"code","ddf01183":"code","539606eb":"code","fb94d04a":"code","ceb822dc":"code","86298021":"code","ef1fcc81":"code","af5ee9ef":"code","ae0bad8c":"code","c53213c8":"code","7417e88e":"code","057ff3d0":"code","e438f08d":"code","af4db943":"code","217ec7a4":"code","6a84ad0b":"code","b50e4664":"code","a1e7a863":"code","335b70d7":"code","c8c94562":"code","b9db3ed7":"code","2548c900":"code","f8210ba4":"code","d402dc67":"code","54f4e781":"code","5c83362b":"code","efde82e0":"code","83070596":"code","cf4e4087":"code","8fecf8f6":"code","be233993":"code","d1176b26":"code","3cb428b8":"code","6447fcd5":"code","59fc9ef0":"code","3edfe343":"code","c18664fe":"code","87cf76c8":"code","0ed468ed":"code","2acc2eda":"code","656e9c5f":"code","ee53b724":"code","bbd63a56":"code","9a44976b":"code","13f2cde3":"code","dbf7c678":"code","0d64c111":"code","e17c14bf":"code","9fc50383":"code","0eb8613c":"code","3acc741a":"code","463e43bb":"code","51d3722c":"code","fd233f1e":"code","677db8f2":"code","4c8b3b3d":"markdown","26e07766":"markdown","d48881c3":"markdown","e6ad3e58":"markdown","357e6236":"markdown","b4b7e346":"markdown","e7eca576":"markdown","d45f0202":"markdown","ecdb8a29":"markdown","d8c279ef":"markdown","39155b83":"markdown","59bff1aa":"markdown","d8488329":"markdown","3462aa2d":"markdown","18611b56":"markdown","897879a4":"markdown","faa47bf0":"markdown","2f63bf5e":"markdown","20323569":"markdown","4c1c86e2":"markdown","1a23536b":"markdown","430f35ee":"markdown","cd54e952":"markdown","ab13d9a6":"markdown","79f23b40":"markdown","5bffc304":"markdown","866fa7c0":"markdown","6c3087ca":"markdown","bf47f2f0":"markdown","4611821e":"markdown","3dd19541":"markdown","9b8885c5":"markdown","9c08bbe7":"markdown","661251c2":"markdown","73a7be2c":"markdown","0d824b05":"markdown","e0969d78":"markdown","8ddbe940":"markdown","76300bb9":"markdown","734f72b6":"markdown","72c97bd7":"markdown","9c84180a":"markdown","0f0d3656":"markdown","5979f878":"markdown","faf8cd08":"markdown","9955d18b":"markdown","ce7c6609":"markdown","cbd752bb":"markdown","183edc65":"markdown","7ca5d6cc":"markdown","a31b5bab":"markdown","a5770b1d":"markdown","032b8989":"markdown","38519473":"markdown","5158716b":"markdown"},"source":{"fa8e4254":"import pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n%matplotlib inline","caa370ea":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","ddf01183":"#loading data\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\ndata=[train, test]","539606eb":"train.head()","fb94d04a":"train.info()","ceb822dc":"train.describe(include='all')","86298021":"import missingno as msno\n\nmsno.bar(train)","ef1fcc81":"msno.bar(test)","af5ee9ef":"#Set sns style\nplt.style.use('seaborn')\nsns.set(font_scale=1.5)","ae0bad8c":"sns.countplot('Survived', data=train)","c53213c8":"print(train['Survived'].value_counts(normalize=True))","7417e88e":"fig, ax=plt.subplots(1,2,figsize=(20,5))\n(train[['Survived', 'Pclass']].groupby(['Pclass']).mean()).plot.bar(ax=ax[0], color='orange')\nax[0].set_title('Mean Survival Rate of Passengers by Pclass')\nax[0].set_ylabel('Mean Survival Rate')\nsns.countplot('Pclass', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Dead\/Survived count by Pclass')","057ff3d0":"train[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(color='orange')\nplt.title('Female \/ Male Survival Rate')\nplt.ylabel('Survival Rate')\n\nprint(train[['Sex','Survived']].groupby(['Sex']).mean())","e438f08d":"g=sns.pointplot('Pclass','Survived',hue='Sex',data=train)\ng.legend(bbox_to_anchor=(0.95, 1), ncol=1)","af4db943":"train[['Embarked','Survived']].groupby(['Embarked']).mean().plot.bar(color='orange')","217ec7a4":"fig, ax=plt.subplots(1,2,figsize=(20,7))\nsns.countplot('Embarked', hue='Sex',data=train, ax=ax[0])\nax[0].set_title('Sex, Embarked Together')\nsns.countplot('Embarked',hue='Pclass',data=train, ax=ax[1])\nax[1].set_title('Pclass, Embarked Together')","6a84ad0b":"train['Ticket'].value_counts()","b50e4664":"train['Cabin'].isnull().sum()\/len(train['Cabin'])","a1e7a863":"train.loc[train['Survived']==0,'Age'].plot.hist(bins=20, alpha=0.5)\ntrain.loc[train['Survived']==1,'Age'].plot.hist(bins=20, alpha=0.5)\nplt.legend(['Dead','Survived'])\nplt.title('Distribution of Age by Survival')\nplt.xlabel('Age')","335b70d7":"for dataset in data:\n    dataset['FamilySize']=dataset['SibSp']+dataset['Parch']+1","c8c94562":"sns.barplot('FamilySize','Survived',data=train)","b9db3ed7":"train['Fare'].plot.hist(bins=20)","2548c900":"test['Fare']=test['Fare'].fillna(test['Fare'].median())","f8210ba4":"for dataset in data:\n    dataset['Fare']=dataset['Fare'].map(lambda x: np.log(x) if x > 0 else 0)","d402dc67":"train['Fare'].plot.hist(bins=20)","54f4e781":"train.loc[train['Survived']==0,'Fare'].plot.hist(bins=20, alpha=0.5)\ntrain.loc[train['Survived']==1,'Fare'].plot.hist(bins=20, alpha=0.5)\nplt.legend(['Dead','Survived'])\nplt.title('Distribution of Fare by Survival')\nplt.xlabel('Fare')","5c83362b":"pd.concat([train['Survived'], pd.cut(train['Fare'], 4)], axis=1).groupby(['Fare']).mean().plot.bar(color='orange', rot=45)\nplt.title('Survival rate by Fare intervals')","efde82e0":"sns.barplot('Pclass', 'Fare', data=train)","83070596":"train['Name'].head()","cf4e4087":"train['Title']=[each[1].split('.')[0].strip() for each in train['Name'].str.split(',')]\ntest['Title']=[each[1].split('.')[0].strip() for each in test['Name'].str.split(',')]\n\n\ntrain['Title'].value_counts()","8fecf8f6":"for dataset in data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain['Title'].value_counts()","be233993":"sns.barplot('Title','Survived',data=train)","d1176b26":"print(train['Embarked'].isnull().sum())\ntrain['Embarked'].value_counts()","3cb428b8":"train['Embarked']=train['Embarked'].fillna('S')","6447fcd5":"print(train['Age'].isnull().sum())","59fc9ef0":"Title_list=list(train['Title'].unique())","3edfe343":"for each in Title_list:\n    train.loc[(train['Age'].isnull())&(train['Title']==each), 'Age'] = round(train[['Age', 'Title']].groupby(['Title']).mean().loc[each,'Age'])\n    test.loc[(test['Age'].isnull())&(test['Title']==each), 'Age'] = round(test[['Age', 'Title']].groupby(['Title']).mean().loc[each,'Age'])","c18664fe":"train.describe(include = 'all')","87cf76c8":"train['Age']=pd.cut(train['Age'],5, labels=[0,1,2,3,4])\ntest['Age']=pd.cut(test['Age'], 5, labels=[0,1,2,3,4])","0ed468ed":"train['Fare']=pd.cut(train['Fare'],4, labels=[0,1,2,3])\ntest['Fare']=pd.cut(test['Fare'], 4, labels=[0,1,2,3])","2acc2eda":"train['FamilySize']=train['FamilySize'].map(lambda x: 0 if x == 1 else (1 if x<=4 else (2 if x<=7 else 3)))\ntest['FamilySize']=test['FamilySize'].map(lambda x: 0 if x == 1 else (1 if x<=4 else (2 if x<=7 else 3)))","656e9c5f":"train['Sex']=train['Sex'].map({'male': 1, 'female':0})\ntest['Sex']=test['Sex'].map({'male': 1, 'female':0})","ee53b724":"train=pd.concat([train, pd.get_dummies(train['Age'], prefix='Age')], axis=1)\ntest=pd.concat([test, pd.get_dummies(test['Age'], prefix='Age')], axis=1)\n\ntrain=pd.concat([train, pd.get_dummies(train['FamilySize'],prefix='FamilySIze')], axis=1)\ntest=pd.concat([test, pd.get_dummies(test['FamilySize'],prefix='FamilySIze')], axis=1)\n\ntrain=pd.concat([train, pd.get_dummies(train[['Embarked']])], axis=1)\ntest=pd.concat([test, pd.get_dummies(test[['Embarked']])], axis=1)\n\ntrain=pd.concat([train, pd.get_dummies(train[['Title']])], axis=1)\ntest=pd.concat([test, pd.get_dummies(test[['Title']])], axis=1)\n\ntrain=pd.concat([train, pd.get_dummies(train['Pclass'], prefix='Pclass')], axis=1)\ntest=pd.concat([test, pd.get_dummies(test['Pclass'], prefix='Pclass')], axis=1)\n\ntrain=pd.concat([train, pd.get_dummies(train['Fare'], prefix='Fare')], axis=1)\ntest=pd.concat([test, pd.get_dummies(test['Fare'], prefix='Fare')], axis=1)","bbd63a56":"train.columns","9a44976b":"drop_columns=['PassengerId','Name','Age','SibSp','Parch','Ticket','Cabin', 'Embarked','FamilySize', 'Title','Fare','Pclass']\ntrain.drop(drop_columns, axis=1, inplace=True)\ntest.drop(drop_columns, axis=1, inplace=True)","13f2cde3":"train_X=train[list(train.columns.drop('Survived'))]\ntrain_Y=train['Survived']","dbf7c678":"train_X.dtypes","0d64c111":"# Logistic Regression\nlr=LogisticRegression()\nprint(cross_val_score(lr, train_X, train_Y, cv=5).mean())","e17c14bf":"# Support Vector Machine\nsvc=SVC()\nsvc_param={'kernel': ['linear', 'poly','rbf'], \n           'C': [1,10,20,50,100,200,500,1000], \n          'class_weight':[None, 'balanced']}\nsvc_grid=GridSearchCV(svc, svc_param, n_jobs=4, cv=5)\nsvc_grid.fit(train_X, train_Y)\nprint(svc_grid.best_params_)\nprint(svc_grid.best_score_)","9fc50383":"# SGD Classifier\nsgd=SGDClassifier()\nprint(cross_val_score(sgd, train_X, train_Y, cv=5).mean())","0eb8613c":"# Random Forest\nrf=RandomForestClassifier()\nrf_param={'n_estimators':[10, 50, 100, 500, 1000], \n         'min_samples_split': [2,5,10],\n          'class_weight':[None, 'balanced']\n         }\nrf_grid=GridSearchCV(rf, rf_param, n_jobs=4, cv=5)\nrf_grid.fit(train_X, train_Y)\nprint(rf_grid.best_params_)\nprint(rf_grid.best_score_)","3acc741a":"# Gradient Boosting\ngb=GradientBoostingClassifier()\ngb_param={'n_estimators':[100, 500, 1000],\n          'learning_rate':[0.01, 0.1, 0.2],\n          'max_depth':[3,6,9],\n          'min_samples_split': [2,5],\n          'max_leaf_nodes':[8,16,32]\n         }\ngb_grid=GridSearchCV(gb, gb_param, n_jobs=4, cv=5)\ngb_grid.fit(train_X, train_Y)\nprint(gb_grid.best_params_)\nprint(gb_grid.best_score_)","463e43bb":"# Xgboost classifier\nxgbst=xgb.XGBClassifier()\nxgbst_param={'n_estimators':[100, 500, 1000],\n          'learning_rate':[0.01, 0.1, 0.2],\n          'max_depth':[3,6,9]\n         }\nxgbst_grid=GridSearchCV(xgbst, xgbst_param, n_jobs=4, cv=5)\nxgbst_grid.fit(train_X, train_Y)\nprint(xgbst_grid.best_params_)\nprint(xgbst_grid.best_score_)","51d3722c":"# Adaboost classifier\nada=AdaBoostClassifier()\nada_param={'n_estimators':[50, 100, 500, 1000],\n          'learning_rate':[0.1, 0.5, 1]\n         }\nada_grid=GridSearchCV(ada, ada_param, n_jobs=4, cv=5)\nada_grid.fit(train_X, train_Y)\nprint(ada_grid.best_params_)\nprint(ada_grid.best_score_)","fd233f1e":"submission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","677db8f2":"predict=svc_grid.predict(test)\nsubmission['Survived']=predict\nsubmission.to_csv('final_submission.csv', index=False)","4c8b3b3d":"* Survival rate of passengers with cheap ticket was lower than passengers with expensive ticket.\n* Almost passengers with Fare smaller than 2 died. \n* Meanwhile, most passengers with Fare bigger than 4 survived.\n* It looks like survival rates differ from the intervals. ","26e07766":"Judging from the cross validation score, support vector machine classifier with parameter {'C': 100, 'class_weight': None, 'kernel': 'rbf'} was the best. I will choose this model to predict test dataset. ","d48881c3":"## 5. Model Selection\n\nFor checking model accuracy, we will use 5-fold cross-validation. We can estimate test accuracy using cross-validation checking. \n\nHere are the models we will use for this problem. For each model, hyperparameter tunning will be done to find the best model. \n\n* Logistic Regression\n* Support Vector Machine\n* SGD Classifier\n* Random Forest\n* Gradient Boosting\n* Adaboost\n* XGboost","e6ad3e58":"### 4.3 FamilySize feature to categorical","357e6236":"We can find some interesting facts related to age.\n* Infants, and children had high survival rate.\n* Most passnegers were 15~35 years old.\n* Large number of passengers whose age is over 20 did not survive.\n* It would be better to divide age values into several intervals.","b4b7e346":"* Mlle is french word of Miss. Mme is french word of Mrs. \n* Considering above facts, we will divide Title values into Mr, Miss, Mrs, Master, and Rare(which means etc value).","e7eca576":"### 4.5 One - hot encoding for categorical features","d45f0202":"* Above barplot corresponds with the analysis that female and children passengers were likely to survive more.\n* It is shown that Mr(represents male, adult) title passengers survived less than other titles.","ecdb8a29":"Survival rates of 2~4 familysize passengers were similar. Survival rates of 5~7 familysize passengers were also similar. None of passengers with more than 8 family members survived. \n\nThus, let's divide familysize into 4 categories; Alone, 2~4, 5~7, more than 8.","d8c279ef":"* Around 38% of passengers in the training dataset survived. It seems that there will be no big influence from the class imbalanced problem, since the distribution is quite balanced.","39155b83":"## 1. Checking Data","59bff1aa":"As seen above, there are total 891 observations, and 12 columns. Some values are null values, so we need to deal with null values. ","d8488329":"### 1.1  Categorical vs Numerical features\n\nWe can divide features into two groups; categorical and numerical features.\n\n* Categorical: Pclass, Name, Sex, Ticket, Cabin, Embarked\n   (Pclass is Ordinal)\n* Numerical: Age, Fare, SibSp, Parch\n\n### 1.2  Data Description","3462aa2d":"### 1.3   About Missing Values","18611b56":"* We verified that survival rate among the intervals differed a lot. \n* It would be better to divide Fare values into several intervals. ","897879a4":"Currently, type of sex feature is string. We need to convert these string values into numerical values so that we can use this feature in machine learning method. \n\nI will map (male,female) into numerical value (1,0).","faa47bf0":"### 2.2  Sex","2f63bf5e":"### 1.4  Checking Response(Target) Variable\n\n* Checking target variable is important. In this problem, we need to predict whether the passenger survived or not. \n* Target Variable is 'Survived' in this problem.\n* If target variable has skewed distribution, it can cause class imbalance problem.","20323569":"## 2. Exploratory Data Analysis","4c1c86e2":"# Titanic: Machine Learning from Disaster\n\n## 1. Checking Data\n* 1.1 Categorical vs Numerical features\n\n* 1.2 Data Description\n\n* 1.3 About Missing Values\n\n* 1.4 Checking Response(Target) Variable\n\n## 2. Exploratory Data Analysis\n* 2.1 Pclass\n\n* 2.2 Sex\n\n* 2.3 Embarked\n\n* 2.4 Ticket\n\n* 2.5 Cabin\n\n* 2.6 Age\n\n* 2.7 SibSp and Parch\n\n* 2.8 Fare\n\n* 2.9 Name\n\n## 3. Filling Missing Values\n* 3.1 Embarked\n\n* 3.2 Age\n\n## 4. Feature Engineering\n* 4.1 Age feature to categorical\n\n* 4.2 Fare feature to categorical\n\n* 4.3 FamilySize feature to categorical\n\n* 4.4 Sex feature\n\n* 4.5 One - hot encoding for categorical features\n\n* 4.6 Dropping unnecessary columns\n\n## 5. Model Selection","1a23536b":"### 2.1  Pclass","430f35ee":"### 2.3  Embarked","cd54e952":"### 2.5  Cabin","ab13d9a6":"* There was a big difference between survival rate of female and male.\n* Female survival rate was a lot higher than male survival rate.\n* Sex is an important feature for the target variable.","79f23b40":"### 4.1 Age feature to categorical","5bffc304":"### 2.7  SibSp and Parch\n\n* For SibSp and Parch, both variables are related to the number of family members. It would be better to combine two columns into one column.\n* Our new column name is FamilySize, and it represents the number of family members.\n* It can be derived by SibSp + Parch + 1. The reason we add 1 is to include passenger themselves.","866fa7c0":"During EDA section, we found some discrete patterns with age levels. For example, passengers younger than 16 years old survived a lot, while passengers older than 16 years old died a lot. Dividing continuous age feature into several discrete levels will be helpful for our model accuracy. I will divide age into 5 levels.\n\nWe can use pandas cut method to implement this transformation.","6c3087ca":"#### Training Data\n* There are two features which have quite a lot of missing values; Age and Cabin columns.\n* Especially, it seems that it is hard to fill Cabin column's missing values, since around 80% of values are null. Thus, we will drop Cabin column later.\n* Furthermore, 2 values of Embarked column were missing. Except above columns, non had missing values.\n\n#### Test Data\n* In test data, age and cabin columns had a lot of null values.\n* One missing value in Fare column was detected.","bf47f2f0":"### 2.9 Name","4611821e":"* Fare variable is right-skewed. Skewness can lead to overweight high valued ourliers, causing bad performance. To fix this skewness, I will transform this values with log function.\n\n* Before transformation, there is one missing value in the test data. We will fill this with the median value of the test data.","3dd19541":"It is hard to find specific patterns in ticket variable. Thus, I will drop this column later.","9b8885c5":"Until now, we did exploratory data analysis and found some significant correlation with features and response variable. In this feature engineering section, we modify, combine, drop feature variables to maximize our prediction model accuracy. ","9c08bbe7":"* Passengers from Cherbourg(C) port had higher survival rate than passengers from other port.\n\nLet's get deeper!","661251c2":"* Survival rate differed a lot by FamilySize.\n* Single family and family with more than 5 members had low survival rate.\n* Family with 2~4 members had higher survival rate.\n* Family with 5~7 members had lower survival rate.","73a7be2c":"* It is easy to catch that Name values include passengers' title.\n* For example, Mr., and Mrs. appeared above. \n* Title is significant information and it is even related to passengers' age.\n* I will extract those titles from the original Name column. To extract title, we can utilize the fact that comma is followed by title.","0d824b05":"In titanic dataset, there are two kinds of categorical variables. One is ordinal categorical variables, such as Age, Fare, Pclass, FamilySize (Age, Fare features were categorized above). \n\nThese ordinal cateogorical features can be ordered with specific rules. We can handel those with 2 methods. \n\nOne is label encoding, which transforms values into simple numerical values. Order of each level is preserved, but when similar levels have much different survival rates, this method will not help that much. \n\nThe other is one - hot encoding, which creates dummy variables. When similar levels have much different survival rates, this method will help our model accuracy. However, order of each level will be no longer meaningful. \n\nWe need to check every categorical features whether similar levels have similar survival rates or not. For Age, FamilySize, Fare, Pclass, they had quite different survival rate even though each levels are similar. \n\nTherefore, I will use one - hot encoding for those features. \n\n\nThe other type of categorical variable is non ordinal categorical variables. In this case, I will use one - hot encoding for these variables. Embarked, Title features are non ordinal categorical variables. \n\nWe can make dummy variables using pandas get_dummies method.","e0969d78":"### 3.1 Embarked","8ddbe940":"* We could also verify that higher class tends to have expensive fare. ","76300bb9":"### 4.2 Fare feature to categorical","734f72b6":"* We already know that this variable has about 77% null values.\n* It is hard to derive useful information.\n* Thus, I will exclude this varialbe from my model.","72c97bd7":"* There are only 2 missing values in Embarked feature(training data). \n* We can simply replace missing values with the most frequent value of Embarked (S).","9c84180a":"### 4.6 Dropping unnecessary columns","0f0d3656":"On EDA section, we divided fare values into 4 intervals and found survival rates differ from the intervals. Dividing into several intervals and making it categorical will help our model performance. \n\nWe can use pandas cut method. ","5979f878":"### 4.4 Sex feature","faf8cd08":"### 3.2  Age","9955d18b":"* There was no exception. In all classes, female survival rate was much more higher than male survival rate.","ce7c6609":"After transformation, Fare column became less skewed.","cbd752bb":"From above two plots, we can say\n* For C, Q, gender ratio was about 1, while S had more male passengers.\n* Low survival rate of S port may be related with ratio of male passengers.\n\n\n* 3rd class was the most prevalent for passengers from port S and Q.\n* Passengers from port C were mostly in class 1, 3.\n* Low survival rate of port S and high survival rate of port C may be related with class distribution in each port. ","183edc65":"## 4. Feature Engineering","7ca5d6cc":"* Mean Survival Rate of Passengers by Pclass differed from class 1 to class 3\n* Passengers with higher class survived a lot, while passengers with lower class survived less.\n* Pclass variable plays an significant role on predicting the target variable.","a31b5bab":"### 2.6  Age","a5770b1d":"## 3. Filling Missing Values","032b8989":"There are 177 missing values in Age column. Since it is not a small number, we cannot fill them with just mean value, or median value. Here, I would like to replace missing age values using Title feature. Title feature is related to Age, definitely. Mrs usually implies older women, while Miss implies younger women. Also, Mr usually implies older men, while Master implies younger men. \n\n\nWe can fill missing age values with mean age of the corresponding Title value. ","38519473":"### 2.8  Fare","5158716b":"### 2.4  Ticket"}}