{"cell_type":{"b6015cb1":"code","a1b22928":"code","247798e5":"code","8ef86445":"code","bf14311d":"code","ad593ce1":"code","16dd2e80":"code","94a0e6e2":"code","20c1fd9f":"code","dfdd978f":"code","116fd72d":"code","f00b14da":"code","b71de9a4":"code","ee71692c":"code","d5fd52ab":"code","681d3078":"code","a7829029":"code","cfda71a4":"code","d7217fb9":"code","2a9b6d3a":"code","579abd67":"code","d69e201a":"code","0f595bbd":"code","ac3f60b9":"code","210f85b2":"code","5dcb0d40":"code","ee0437a4":"code","6ab0f1ef":"code","21833dda":"code","13853c9a":"code","2fd0e00f":"code","ab00b038":"code","5192b08b":"code","44a6fbc0":"code","fabc91d6":"code","8ae74b96":"code","f3e39441":"code","6eaa0d10":"code","3436af58":"code","2e5c9985":"code","6d1c30b7":"code","aba779e3":"code","9f1c1c5b":"code","2009b95b":"code","99d9bd6b":"code","37f4209f":"code","a8304732":"code","b7ffcf3e":"code","4f41842f":"code","d7fca017":"code","e1f80b93":"code","d88928bf":"code","c9c39732":"code","8f0b66eb":"code","2256588b":"code","1fc3e3a9":"code","37523166":"code","5ea3815a":"code","570d5ee4":"code","5363998a":"code","454e49cb":"code","321a8c87":"code","cfefb395":"code","e953d0cf":"code","6a4b6aac":"code","fbe8a270":"code","6b2fd3e8":"code","074d891a":"code","fd9bac65":"code","4d8f292a":"code","6f64bb84":"code","7e382f52":"code","9ccbe8e1":"code","8ada6ed7":"code","fa62340f":"code","97742222":"markdown","31c60bf3":"markdown","4805e5b2":"markdown","cb0ad0a0":"markdown","e19b8ac8":"markdown","6c9f04b8":"markdown","3ec180f2":"markdown","1cedc522":"markdown","80402997":"markdown","7092653e":"markdown","462618a6":"markdown","ac1725ef":"markdown","4085b9a3":"markdown","0449b726":"markdown","7d7be048":"markdown","ad08cce8":"markdown","a06c4f9e":"markdown","f6931564":"markdown","45059070":"markdown","4ede5de7":"markdown","79c8567f":"markdown","906a0744":"markdown","81b14bc4":"markdown","2a629bd7":"markdown","413d43b8":"markdown","a848c231":"markdown","ad0da099":"markdown","61ce785f":"markdown","67747287":"markdown","01e4d586":"markdown","44df5476":"markdown","43dec261":"markdown","cb4737a1":"markdown","dbe7870b":"markdown","7a6d21bc":"markdown","653ebbbb":"markdown","a9d0aef1":"markdown","ed8a005a":"markdown","0b665c79":"markdown","73942453":"markdown","84d7b526":"markdown","747b2a8c":"markdown","01b344ea":"markdown","b785988e":"markdown","cc1c18e1":"markdown","d73729fa":"markdown","bd75e065":"markdown","35790fdd":"markdown","d38fc563":"markdown","fa8e5cba":"markdown","b89c5f83":"markdown","32ade73b":"markdown","f0ceb7c4":"markdown","588b6700":"markdown","c57e9937":"markdown","81a2e67c":"markdown","9df93874":"markdown","f3bdb028":"markdown","d33fa3c3":"markdown","e529f6bd":"markdown","418272e7":"markdown","8ab3b2fc":"markdown","a7f0955d":"markdown","0718a4c8":"markdown","2ac5f343":"markdown","35bd81fc":"markdown","7ddc047a":"markdown","8bc5465e":"markdown","5826db77":"markdown","b76563bc":"markdown","0ef00246":"markdown","4f2e16d7":"markdown","2514d655":"markdown","ac3de470":"markdown","2295a961":"markdown","d8f0cb86":"markdown"},"source":{"b6015cb1":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = \"whitegrid\", color_codes = True)\nnp.random.seed(sum(map(ord, \"palettes\")))\n\nfrom sklearn.metrics import roc_auc_score\n\n#Models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.cross_validate import KFold\n","a1b22928":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","247798e5":"train.describe()","8ef86445":"test.describe()","bf14311d":"train.info()\nprint(\"++++++++++++++++++++++++++++++++++++++\")\nprint()\ntest.info()","ad593ce1":"train.head()","16dd2e80":"test.head()","94a0e6e2":"train.describe(include = ['O'])","20c1fd9f":"test.describe(include = ['O'])","dfdd978f":"test_PassengerId = test[\"PassengerId\"]  # save the id for submiting the final results\n\ntrain.drop(['PassengerId', \"Ticket\", 'Cabin'], axis = 1, inplace = True)\ntest.drop(['PassengerId', \"Ticket\", 'Cabin'], axis=1, inplace = True)\ntrain_test_data = [train, test] ","116fd72d":"for dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map({'male': 1, 'female': 0}).astype(int)","f00b14da":"train_test_data[0].head() # train data","b71de9a4":"train_test_data[1].head()  # test data","ee71692c":"train[['Sex', 'Survived']].groupby(['Sex'], \n                                        as_index = False).mean().sort_values(by = 'Survived', ascending = True)","d5fd52ab":"train[['Pclass', 'Survived']].groupby(['Pclass'], \n                                        as_index = False).mean().sort_values(by = 'Survived', ascending = True)","681d3078":"age_fill = np.zeros((2,3)) # 2 for sex and 3 for Pclass\nprint(age_fill)\n","a7829029":"age_fill = np.zeros((2,3)) \nfor dataset in train_test_data:\n    for s in range(0, 2):\n        for p in range(0, 3):\n            age_fill_df = dataset[(dataset['Sex'] == s) &\\\n                               (dataset['Pclass'] == p + 1)]['Age'].dropna()\n            age_to_fill = age_fill_df.median()\n\n            # Convert random age float to nearest .5 age\n            age_fill[s,p] = int( age_to_fill\/0.5 + 0.5 ) * 0.5\n            \n    for s in range(0, 2):\n        for p in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == s) & (dataset.Pclass == p + 1),\\\n                    'Age'] = age_fill[s,p]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain.head()","cfda71a4":"test.head()","d7217fb9":"min(train['Age']), max(train['Age'])","2a9b6d3a":"train['AgeBins'] = pd.cut(train['Age'], 8)","579abd67":"train[['AgeBins', 'Survived']].groupby(['AgeBins'], \n                                       as_index = False).mean().sort_values(by = 'Survived', ascending = True)","d69e201a":"for dataset in train_test_data:    \n    dataset.loc[dataset['Age'] <= 10, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 60) & (dataset['Age'] <= 70), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 70, 'Age'] = 7","0f595bbd":"fig = sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train)\nfig.get_axes().set_xticklabels([\"Female\", \"Male\"])\nfig.get_axes().legend([\"First Class\", \"Second Class\", \"Third Class\"], \n                    loc='upper right');","ac3f60b9":"fig = sns.pointplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train);\n\nfig.get_axes().set_xlabel(\"Sex\")\nfig.get_axes().set_xticklabels([\"Female\", \"Male\"])\nfig.get_axes().set_ylabel(\"Mean(Survived\")\nfig.get_axes().legend([\"First Class\", \"Second Class\", \"Third Class\"], \n                    loc='upper left')","210f85b2":"sns.countplot(x=\"AgeBins\", data = train, palette = \"GnBu_d\");","5dcb0d40":"sns.countplot( x =\"AgeBins\", hue=\"Pclass\", data = train, palette=\"PuBuGn_d\");","ee0437a4":"train.head()","6ab0f1ef":"train = train.drop(['AgeBins'], axis = 1)\ntrain_test_data = [train, test]\ntrain.head()","21833dda":"for dataset in train_test_data:\n    dataset[\"FamilySize\"] = dataset['SibSp'] + dataset['Parch']\ntrain, test = train_test_data[0], train_test_data[1]\ntrain.head()","13853c9a":"train[['FamilySize', 'Survived']].groupby(['FamilySize'], \n                                        as_index = False).mean().sort_values(by = 'Survived', ascending = False)","2fd0e00f":"sns.countplot(x=\"FamilySize\", data = train, palette = \"GnBu_d\");","ab00b038":"train = train.drop(['Parch', 'SibSp'], axis = 1)\ntest = test.drop(['Parch', 'SibSp'], axis = 1)\ntrain_test_data = [train, test]\ntrain.head()","5192b08b":"test.head()","44a6fbc0":"Embarking_freq = train.Embarked.dropna().mode()[0]\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(Embarking_freq)\ntrain, test = train_test_data[0], train_test_data[1]\ntrain.head()   ","fabc91d6":"train[['Embarked', 'Survived']].groupby(['Embarked'], \n                                       as_index = False).mean().sort_values(by = 'Survived', ascending = False)","8ae74b96":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\ntrain.head()","f3e39441":"Fare_freq = test.Fare.dropna().mode()[0]\nfor dataset in train_test_data:\n    dataset['Fare'] = dataset['Fare'].fillna(Fare_freq)","6eaa0d10":"train['FareBins'] = pd.qcut(train['Fare'], 5)\ntrain[['FareBins', 'Survived']].groupby(['FareBins'], \n                                        as_index = False).mean().sort_values(by = 'Survived', ascending = True)","3436af58":"for dataset in train_test_data:    \n    dataset.loc[dataset['Fare']  <=7.854, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.84)   & (dataset['Fare'] <= 10.5), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 10.5)   & (dataset['Fare'] <= 21.679), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 21.679) & (dataset['Fare'] <= 39.688), 'Fare'] = 3\n    dataset.loc[(dataset['Fare'] > 39.688) & (dataset['Fare'] <= 5512.329), 'Fare'] = 4","2e5c9985":"train, test = train_test_data[0], train_test_data[1]\ntrain = train.drop(['FareBins'], axis = 1)\ntrain.head(6)","6d1c30b7":"test.head(6)","aba779e3":"def extract_title(df):\n    # the Name feature includes last name, title, and first name. After splitting \n    # the title is in the second column or at index 1\n    df[\"Title\"] = df.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip()) \n    return df\ntrain = extract_title(train)\ntest = extract_title(test)","9f1c1c5b":"fig = sns.countplot(x = 'Title', data = train, palette = \"GnBu_d\")\nfig = plt.setp(fig.get_xticklabels(), rotation = 45)","2009b95b":"#for dset in train:\ntrain_test_data = [train, test]\nfor dset in train_test_data:\n    dset[\"Title\"] = dset[\"Title\"].replace([\"Melkebeke\", \"Countess\", \"Capt\", \"the Countess\", \"Col\", \"Don\",\n                                         \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"] , \"Lamped\")\n    dset[\"Title\"] = dset[\"Title\"].replace([\"Lady\", \"Mlle\", \"Ms\", \"Mme\"] , \"Miss\")\n","99d9bd6b":"fig2 = sns.countplot(x = 'Title', data = train, palette = \"GnBu_d\")\nfig2 = plt.setp(fig2.get_xticklabels(), rotation = 45)","37f4209f":"train[['Title', 'Survived']].groupby(['Title'], \n                                        as_index = False).mean().sort_values(by = 'Survived', ascending = False)","a8304732":"for dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map({'Mr': 1, 'Miss': 2, 'Mrs': 3, \n                                             'Master': 4, 'Lamped': 5}).astype(int)\ntrain.head()","b7ffcf3e":"train.drop(['Name'], axis = 1, inplace = True)\ntest.drop(['Name'], axis=1, inplace = True)\ntrain.head()","4f41842f":"colormap = plt.cm.viridis\nplt.figure(figsize=(16,16))\nplt.title('Correlation between Features', y=1.05, size = 20)\nsns.heatmap(train.corr(),\n            linewidths=0.1, \n            vmax=1.0, \n            square=True, \n            cmap=colormap, \n            linecolor='white', \n            annot=True)","d7fca017":"y_train = train[\"Survived\"]\nX_train = train.drop([\"Survived\"], axis = 1 )\n\nX_test = test\nX_train.shape, y_train.shape, X_test.shape","e1f80b93":"LR = LogisticRegression(random_state = 0)\nLR.fit(X_train, y_train)\ny_pred_lr = LR.predict(X_test)\nLR_score = LR.score(X_train, y_train)\nprint(\"LR Accuracy  score = {:.2f}\".format(LR_score*100))","d88928bf":"svc = SVC(random_state = 0)\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\nSVC_score = svc.score(X_train, y_train)\nprint(\"SVC Accuracy  score = {:.2f}\".format(SVC_score*100))\n","c9c39732":"KNN = KNeighborsClassifier(n_neighbors = 5)\nKNN.fit(X_train, y_train)\ny_pred_knn = KNN.predict(X_test)\nKNN_score = KNN.score(X_train, y_train)\nprint(\"KNN accuracy score = {:.2f}\".format(KNN_score*100))\n","8f0b66eb":"GNB = GaussianNB()\nGNB.fit(X_train, y_train)\ny_pred_gnb = GNB.predict(X_test)\nGNB_score = GNB.score(X_train, y_train)\nprint(\"GNB accuracy score = {:.2f}\".format(GNB_score*100))","2256588b":"LSVC = LinearSVC()\nLSVC.fit(X_train, y_train)\ny_pred_lsvc = LSVC.predict(X_test)\nLSVC_score = LSVC.score(X_train, y_train)\nprint(\"GNB accuracy score = {:.2f}\".format(LSVC_score*100))","1fc3e3a9":"perceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred_perceptron = perceptron.predict(X_test)\nperceptron_score = perceptron.score(X_train, y_train)\nprint(\"perceptron accuracy score = {:.2f}\".format(perceptron_score*100))\n","37523166":"SGD = SGDClassifier()\nSGD.fit(X_train, y_train)\ny_pred_sgd = SGD.predict(X_test)\nSGD_score = SGD.score(X_train, y_train)\nprint(\"Stochastic Gradient Descent accuracy score = {:.2f}\".format(SGD_score*100))","5ea3815a":"DT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\ny_pred_dt = DT.predict(X_test)\nDT_score = DT.score(X_train, y_train)\nprint(\"Decision Tree accuracy score = {:.2f}\".format(DT_score*100))","570d5ee4":"RF = RandomForestRegressor(n_estimators = 1000)\nRF.fit(X_train, y_train)\ny_pred_rf = RF.predict(X_test)\nRF_score = RF.score(X_train, y_train)\nprint(\"Random forest regressor accuracy score = {:.2f}\".format(RF_score*100))\n","5363998a":"Predictive_models = pd.DataFrame({\n    'Model': ['SVM', 'KNN', 'LR', 'RF', 'GNB', \n              'Perceptron','SGD', 'LSVC', 'DT'],\n    'Score': [SVC_score, KNN_score, LR_score, RF_score, GNB_score, \n              perceptron_score, SGD_score, LSVC_score, DT_score]})\nPredictive_models.sort_values(by ='Score', ascending=True)","454e49cb":"DT = DecisionTreeClassifier()\ny_train = train.loc[:,\"Survived\"]\nX_train = train.drop([\"Survived\"], axis = 1)\n\n\nkfold = KFold(n=len(train), n_folds = 5, shuffle = True, random_state = 0)\nkfold_score = cross_val_score(DT, X_train, y_train, cv = kfold)\n\nkfold_score_mean = np.mean(kfold_score)\n  \nprint(\"Decision Tree accuracy score per fold : \", kfold_score, \"\\n\")\nprint(\"Average accuracy score : {:.4f}\".format(kfold_score_mean))\n","321a8c87":"DT = DecisionTreeClassifier()\ny_train = train.loc[:,\"Survived\"]\nX_train = train.drop([\"Survived\"], axis = 1)\n\nparameters = {'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n             'max_features' : [\"auto\", None, \"sqrt\", \"log2\"],\n             'random_state': [0, 25, 75, 125, 250],\n             'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\nclf = GridSearchCV(DT, parameters)\nclf.fit(X_train, y_train)\n\nDT_Model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)\nprint(DT_Model)\n","cfefb395":"DT = DecisionTreeClassifier(max_depth = 6, \n                            max_features = 'auto', \n                            min_samples_leaf = 2,\n                            random_state = 125)\nX_train = train.drop([\"Survived\"], axis = 1)\ny_train = train.loc[:,\"Survived\"]\n\nkfold = KFold(n=len(train), n_folds = 5, shuffle = True, random_state = 125)\nDT.fit(X_train, y_train)\nkfold_score = cross_val_score(DT, X_train, y_train, cv = kfold)\nkfold_score_mean = np.mean(kfold_score)\n\ny_pred_dt = DT.predict(X_test)\n\n  \nprint(\"Decision Tree accuracy score per fold : \", kfold_score, \"\\n\")\nprint(\"Average accuracy score : {:.4f}\".format(kfold_score_mean)) ","e953d0cf":"DT.feature_importances_","6a4b6aac":"feature_importances = pd.Series(DT.feature_importances_, index = X_train.columns).sort_values()\n#feature_importances.sort()\nfeature_importances.plot(kind = \"barh\", figsize = (7,6));\nplt.title(\" feature ranking\", fontsize = 20)\nplt.show()\n","fbe8a270":"Titanic_submission = pd.DataFrame({\n        \"PassengerId\": test_PassengerId,\n        \"Survived\": y_pred_dt\n    })","6b2fd3e8":"Titanic_submission.to_csv(\"Titanic_compet_submit.csv\", index = False)","074d891a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.pipeline import Pipeline\npipe_svm = Pipeline([('scl', StandardScaler()),\n            ('pca', PCA(n_components=3)),\n            ('clf', SVC(random_state=0))])\nscores = cross_val_score(estimator=pipe_svm, \n                          X=X_train, y=y_train, \n                          cv=10, n_jobs=-1)\nprint('CV accuracy scores: %s' % scores)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores))) ","fd9bac65":"from sklearn.learning_curve import learning_curve\npipe_svm = Pipeline([('scl', StandardScaler()),            \n                     ('pca', PCA(n_components=3)),\n                    ('clf', SVC(random_state = 0))])\ntrain_sizes, train_scores, valid_scores = learning_curve(estimator=pipe_svm, \n                       X=X_train, \n                       y=y_train, \n                       train_sizes=np.linspace(0.1, 1.0, 10), \n                       cv=10,\n                       n_jobs=1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\nvalid_mean = np.mean(valid_scores, axis=1)\nvalid_std = np.std(valid_scores, axis=1)\nplt.plot(train_sizes, train_mean, \n          color='blue', marker='o', \n          markersize=5, \n          label='training accuracy')\nplt.fill_between(train_sizes, \n                  train_mean + train_std,\n                  train_mean - train_std, \n                  alpha=0.15, color='blue')\nplt.plot(train_sizes, valid_mean, \n          color='green', linestyle='--', \n          marker='s', markersize=5, \n          label='validation accuracy')\nplt.fill_between(train_sizes, \n                  valid_mean + valid_std,\n                  valid_mean - valid_std, \n                  alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.65, 0.9])\nplt.show()","4d8f292a":"from sklearn.learning_curve import validation_curve\npipe_svm = Pipeline([('scl', StandardScaler()),            \n#                    ('pca', PCA(n_components=3)),\n                    ('clf', SVC(random_state = 0))])\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 50.0, 100.0, 1000.0, 10000.0]\ntrain_scores, vald_scores = validation_curve(\n                estimator=pipe_svm, \n                 X=X_train, \n                 y=y_train, \n                 param_name='clf__C', \n                 param_range=param_range,\n                 cv=10)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\nvalid_mean = np.mean(valid_scores, axis=1)\nvalid_std = np.std(valid_scores, axis=1)\nplt.plot(param_range, train_mean, \n          color='blue', marker='o', \n          markersize=5, \n          label='training accuracy')\nplt.fill_between(param_range, train_mean + train_std,\n                  train_mean - train_std, alpha=0.15,\n                  color='blue')\nplt.plot(param_range, valid_mean, \n          color='green', linestyle='--', \n          marker='s', markersize=5, \n          label='validation accuracy')\nplt.fill_between(param_range, \n                  valid_mean + valid_std,\n                  valid_mean - valid_std, \n                  alpha=0.15, color='green')\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 0.9])\nplt.show()","6f64bb84":"pipe_svm = Pipeline([('scl', StandardScaler()),\n#                     ('pca', PCA(n_components = 2)),\n                      ('clf', SVC(random_state=1))])\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\nparam_grid = [{'clf__C': param_range, \n                'clf__kernel': ['linear']},\n               {'clf__C': param_range, \n                'clf__gamma': param_range, \n                'clf__kernel': ['rbf']}]\ngs = GridSearchCV(estimator=pipe_svm, \n                   param_grid=param_grid, \n                   scoring='accuracy', \n                   cv=10,\n                   n_jobs=-1)\nclf = gs.fit(X_train, y_train)\nprint(clf.best_score_) \nprint(clf.best_params_)","7e382f52":"y_pred_pip = clf.predict(X_test)\nprint('y_pred_pip: {:.3f}',format(y_pred_pip),\"\\n\")","9ccbe8e1":"gs = GridSearchCV(estimator=pipe_svm, \n                   param_grid=param_grid,\n                   scoring='accuracy', \n                   cv=2, \n                   n_jobs=-1)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\nprint('CV accuracy scores: {:.3f}',format(scores))\nprint('CV accuracy: {:.3f} +\/- {:.3f}',format((np.mean(scores), np.std(scores))))","8ada6ed7":"Titanic_submission = pd.DataFrame({\n        \"PassengerId\": test_PassengerId,\n        \"Survived\": y_pred_pip\n    })","fa62340f":"Titanic_submission.to_csv(\"Titanic_compet_submit_3.csv\", index = False)","97742222":"## 1.3 Import Libraries","31c60bf3":"##  2. 5. Embarked","4805e5b2":"- No missing data\n- Categorical variable\n- Transform the Sex categorical variable into equivalent discrete numerical value (Sex: male being 1 and female = 0). ","cb0ad0a0":"For optimal performance, most learning algorithms need input features on the same scale therefore the first step is that the features need to standardize the columns of the given dataset before we can feed them to an estimator. As this project is part of my learning and excerise, I want to compress the training data from the initial 13 to a lower three-dimensional subspace via principal component analysis (PCA), a feature extraction technique for dimensionality reduction. Instead of going through the fitting and transformation steps for the training and test dataset separately, we can chain the StandardScaler, PCA, and Estimator objects in a pipeline.  The next step is cross validation and model selection. A good approach for model selection is to separate a dataset into three parts: a training set, a validation set, and a test set. The training set is used to fit the different models, and the performance on the validation set is then used for the model selection. The advantage of having a test set that the model hasn't seen before during the training and model selection steps is that we can obtain a less biased estimate of its ability to generalize to new data. In stratified cross-validation, the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset.","e19b8ac8":"##  2. 6. Fare","6c9f04b8":"1. Children are more likely to survive more\n2. Female are more likely to survive \n3. First class passengers are more likely to survive ","3ec180f2":"- FamilySize feature correlation with survival by pivoting on FamilySize feature","1cedc522":"#### Transform the Embarked categorical values into discrete numeric values   (S = 0, C = 1, and Q = 2 )","80402997":"- Fare feature correlation with Survival by pivoting on Fare feature","7092653e":"## 2. 2. Pclass","462618a6":"- Age feature correlation with survival by pivoting on Age feature","ac1725ef":"# 6. Pipeline","4085b9a3":"## 2.8 Heat map for correlation between features","0449b726":"#### Get familiarize with the features information of the dataset","7d7be048":"## 1.1 Goal","ad08cce8":"#### Note: From this the decision tree model has higher accuracy rate and hence this classification method is selected for further KFold cross-validation, parameter tuning, and variable importance","a06c4f9e":"- Categorical or discrete variables (features):\n    - Sex (male or female) the most frequent being male\n    - Embarked: (C, Q or S) the most frequent being S\n    - Cabin (this feature has several duplicates 147 unique and missing data (this could be a candidate to remove from the features)\n    - Ticket with unique 681","f6931564":"#### The train data has 891 instances (rows) and Age, Cabin, Embarked  have missing data. The test data has 418 instances (rows) and Age, Fare, and Cabin have missing data","45059070":"- The train data has no missing data\n- The test data has one missing value and we will replace it with the most frequent\n- This is a continuous feature \n- We will lamp the fare feature into bins to develop the predictive model\n- We use qcut method from pandas to divide the fare into ranges","4ede5de7":"- Embarked feature correlation with survival by pivoting on embarked feature","79c8567f":"One of our assumption was children are more likely to survive and the correlation coefficient for under age 10 is 0.594. Hence, the assumption is validated by the data analysis. ","906a0744":"For the train data, the numerical features are PassengerId, pclass, Age, sibSp, Parch, and Fare.","81b14bc4":"Divide age into groups of bins:  min is 0 and max 80 so let us divide it into 8 and determine the correlation with Survival. I use 10 years of age gap","2a629bd7":"##   3.1. Logistic Regression","413d43b8":"##  2. 4. Family Size (SibSp + Parch)","a848c231":"One of our assumption was the first class passengers are more likely to survive and the correlation coefficient for the first class is shown in the table as 0.63. Hence, our assumption is supported by the data analysis.","ad0da099":"- From the baplot we can see that female of all class survived more than the male.  This can be seen also using the point plot below. ","61ce785f":"- No missing data\n- Discrete variable\n- If we assume the survival is dependent on the family size and to analyze this assumption, we will combine SibSp (# of siblings \/ Spouses aboard ) and Parch (# of parents  \/ children aboard ) features together.","67747287":"##  3.5. Linear SVC","01e4d586":"First let us combine the train and test data for preprocessing. But this combination is not used for scaling or identifying the outliers (data leakage). We will start by removing the data which are not important for data analysis and model prediction. The cabin has more missing data than available data, the PassengerId and Name wouldn't have relationship with survival. Moreover, class of the passengers is relevant to survival but I am assuming that the ticket will not have an effect. Hence, I am not considering the Ticket for further analysis.","44df5476":"The goal of this project:\n\n 1. To study the correlation between the survival rate and the features\n 2. To identify the most correlated and least correlated features\n 3. To propose a predictive model\n 4. To practice machine learning algorithms","43dec261":"##  3.7. Stochastic Gradient Descent","cb4737a1":"# 2. Preparing Data for Model Prediction and Data Analysis","dbe7870b":"####  df.info()  help us to see how many instances does the datasethave and the overall missing data","7a6d21bc":"## 5.1 Variable importance measures","653ebbbb":"The test data has same numerical features: PassengerId, pclass, Age, sibSp, Parch, and Fare.","a9d0aef1":"- Sex feature correlation with survival by pivoting on Sex feature","ed8a005a":"References: Manav Sehgal, Omar El Gabry, Sina, Antonello, Jeff Delaney","0b665c79":"# 3. Machine Learning Algorithms","73942453":"- Map the categorical title feature into numerical values (Mr = 1, Miss = 2, Mrs = 3, Master = 4, Lamped = 5) ","84d7b526":"- Filling missing Data \n- Feature engineering\n- Feature vs Survival correlation\n- Transforming categorical variables into equivalent numerical values","747b2a8c":"## 1.4 Data Retriving and Exploration","01b344ea":"From the .describe() and .info() we can see that there are missing data. If we look the \"Age\" column there are 891-714 = 177 missing age data. One way to replace these missing data is to fill them using the average value, second approach is sampling from a normal distribution using mean and standard deviation of the available data in the training and test data respectively. Age_mean = Age.mean(), Age.std(), Age_add = rnd.uniform(age_mean - age_std, age_mean + age_std), third method that we use here is to use the median based on Sex and Pclass.(From Kaggle computation project)","b785988e":"# 5. Final Model ","cc1c18e1":"##  3.2. Support Vector Machine","d73729fa":"The figure shows the linear relationship between the individual features vs survival and also between each feature. Example, Pclass vs Survival corr_coef is - 0.34 that means they have -ve leaner relationship. Pclass = 1 has higher survival rate than Pclass = 3.  +Ve corr_coef_ mean the two variables have +ve linear relationship and -ve mean they have -ve linear relationship (slope = -ve), close to zero mean they are not correlated, close to +1 or -1 mean they are strongly positively and negatively correlated, respectively. ","bd75e065":"# 1. Introduction","35790fdd":"###### Transforming the Age categorical feature into ordinal numerical values based in the AgeBins","d38fc563":"##  3.6. Perceptron","fa8e5cba":"# 1.2 ASSUMPTIONS","b89c5f83":"## 2. 1. Sex","32ade73b":"- Transforming the Fare feature into ordinal values based in the FareBins","f0ceb7c4":"## 4.2 Parameter tuning","588b6700":"###  Information about the categorical variables","c57e9937":"![](http:\/\/)- Looking at the figure, some of the titles were used only once and we will lamped them ","81a2e67c":"- KFold cros-validation is used ","9df93874":"## 2. 3. Age","f3bdb028":"Because we have transformed the age values into 8 categorical values, we don't need the AgeBins feature that we have created above. so we can drop it","d33fa3c3":"##  2. 7. Title","e529f6bd":"## 4.1. Cross Validation","418272e7":"##  3.8. Decision Tree","8ab3b2fc":"- No missing data\n- Categorical variable\n- Pclass feature correlation with survival by pivoting on Pclass feature\n","a7f0955d":"One of our assumption was the female passengers are more likely to survive than male and the correlation coefficient for the female shown in the table as 0.742. Hence, our assumption is supported by the data analysis.\nFemales are more likely to survive ","0718a4c8":"The main task in this project is to predict if a particular person would have survived in the titanic crush. We will use the given training data to develop a high performance predictive model.In this project, one can learn retrieving csv file, how to drop and add features to the dataset, identifying the numerical and categorical variables, identifying missing data, replacing missing data, transformation (mapping) of categorical variables into equivalent numerical values, transforming variable ranges into discrete bins, correlation between all features and survival, correlation between each features (Heat map), feature engineering, 10 predictive models, model evaluation and selection, parameter tuning for the selected model, KFold cross-validation for the selected model, variable importance for the proposed model, predicting the target value (Survival of the test data), and submitting the final results.","2ac5f343":"##  3.9. Random Forest Regressor ","35bd81fc":"Some notes:\n\n- On the outset some of the features may not have direct correlation to survival such as:  \n    - name of a person, passenger ID, ticket \n- Some of the features may not have full\/major set of data such as Cabin with multiple null \n\nAs Feature Engineering:\n\n- We will consider the correlation between survival and the size of family. Hence, we will create a new FamilySize feature\n- We will consider the correlation between title of the passenger and survival. We will also create a new Title feature\n\nTransforming of categorical variables into their corresponding numeric values:\n- Dividing the age into range of ages.\n- Similarly dividing the fare into range (number of bins)  ","7ddc047a":"# 4. Evaluating Predictive Models","8bc5465e":"##### For our data analysis and developing the predictive model, the full name doesn't have importance but I assume the title does.  So, let us extract the title of each person and add a new  feature  \"Title\"","5826db77":"In the next figure, we will see the correlation among all features using a heat map.","b76563bc":"As we stated above, there is no importance of the Name and PassengerId for the data analysis. however, we need Name to generate the Title feature in the feature engineering. So, we will not drop the Name feature for now. We also need the PassengerId for submitting the final result. ","0ef00246":"- The first step is to divide the training and test dataset into features dataset and target dataset","4f2e16d7":"-We are considering the FamilySize feature. So we don't need the SibSp, and Parch. We will drop them next in favor of FamilySize","2514d655":"##  3.3. K-Nearest Neignbors Classifier","ac3de470":"- 2 missing data\n- Categorical variable\n- The embarking feature takes S, Q and C categorical values for port embarkation. \n- The missing values are filled using the most frequent value","2295a961":"- Title has Mr, Mrs, Miss, Master, and Lamped categorical variables with the most frequent being Mr\n- Title feature correlation with survival by pivoting on title feature","d8f0cb86":"##  3.4. Naive Bayes Classifier"}}