{"cell_type":{"cc1fcd29":"code","3f80af96":"code","2625dce9":"code","c4d676b9":"code","e8692400":"code","0f48ba78":"code","451528b8":"code","3ea1bfe6":"code","b22410c6":"code","f1aaa3b2":"code","23c546fd":"code","3e21b6cd":"code","bb5109ce":"code","a5396b7f":"code","a6a6cd10":"code","13c7616f":"code","6ee40e86":"code","67bd7d36":"code","7ef159ca":"code","f574bf91":"code","34018aae":"code","b1472b5d":"code","0e164632":"code","4b51a60a":"code","de07e2bc":"code","20c5a88d":"code","92604b5e":"code","688e6cd1":"code","f2f071c5":"code","22502edd":"markdown","2c23d079":"markdown","cfec04c3":"markdown","5cf67e7c":"markdown","d81e53a6":"markdown","bfb911ee":"markdown","09689fa4":"markdown","fdc0d97e":"markdown","0146bc7f":"markdown","2ce626ab":"markdown","704df95f":"markdown","ca7f8f6b":"markdown","3c10c523":"markdown","a93b5d45":"markdown","02ca8dce":"markdown","e5ae8c2b":"markdown","7cba37bd":"markdown","e8c39ea4":"markdown","19a557b5":"markdown","5e4f4f9e":"markdown","1aa84351":"markdown","537a143d":"markdown","6f6a4956":"markdown","3d421bff":"markdown","aae8c2fb":"markdown","7cdd40c7":"markdown","d3201ba7":"markdown","d0b22b7e":"markdown","1bbd4eb1":"markdown","f0c235ad":"markdown"},"source":{"cc1fcd29":"import os\nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom fastprogress import master_bar, progress_bar\n\nfrom PIL import Image\n\n# Enable Eager Execution\n#tf.enable_eager_execution() # No need use this, already enable with tf version >=2.0\ntf.executing_eagerly() ","3f80af96":"print(os.listdir(\"..\/input\/vietai-advance-retinal-disease-detection-2020\"))","2625dce9":"ROOT = \"..\/input\/vietai-advance-retinal-disease-detection-2020\"\nTRAIN_DIR = \"..\/input\/vietai-advance-retinal-disease-detection-2020\/train\/train\"\nTEST_DIR = \"..\/input\/vietai-advance-retinal-disease-detection-2020\/test\/test\"","c4d676b9":"data = pd.read_csv(os.path.join(ROOT, 'train.csv'))\ndata.head()","e8692400":"for label in data.columns[1:]:\n    print(\"Distribution of\", label)\n    print(data[label].value_counts())","0f48ba78":"LABELS = data.columns[1:]\ndef build_label(row):\n    return \",\".join([LABELS[idx] for idx, val in enumerate(row[1:]) if val == 1])\n        \ndata.apply(lambda x: build_label(x), axis=1).value_counts()","451528b8":"LABELS","3ea1bfe6":"train_data, val_data = train_test_split(data, test_size=0.2, random_state=2020)","b22410c6":"IMAGE_SIZE = 224                              # Image size (224x224)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]         # Mean of ImageNet dataset (used for normalization)\nIMAGENET_STD = [0.229, 0.224, 0.225]          # Std of ImageNet dataset (used for normalization)\nBATCH_SIZE = 64                             \nLEARNING_RATE = 0.001\nLEARNING_RATE_SCHEDULE_FACTOR = 0.1           # Parameter used for reducing learning rate\nLEARNING_RATE_SCHEDULE_PATIENCE = 5           # Parameter used for reducing learning rate\nMAX_EPOCHS = 100                              # Maximum number of training epochs","f1aaa3b2":"def preprocessing_image(image):\n    \"\"\"\n    Preprocess image after resize and augment data with ImageDataGenerator\n    \n    Parameters\n    ----------\n    image: numpy tensor with rank 3\n        image to preprocessing\n    \n    Returns\n    -------\n    numpy tensor with rank 3\n    \"\"\"\n    # TODO: augment more here\n    \n    return image","23c546fd":"train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255,\n                                                             featurewise_center=True,\n                                                             featurewise_std_normalization=True,\n                                                             preprocessing_function=preprocessing_image)","3e21b6cd":"def build_label_list(row):\n    return [LABELS[idx] for idx, val in enumerate(row[1:]) if val == 1]\n        \ntrain_data[\"label\"] = train_data.apply(lambda x: build_label_list(x), axis=1)\nval_data[\"label\"] = val_data.apply(lambda x: build_label_list(x), axis=1)","bb5109ce":"train_gen = train_datagen.flow_from_dataframe(dataframe=train_data, \n                                        directory=TRAIN_DIR, \n                                        x_col=\"filename\", \n                                        y_col=\"label\",\n                                        class_mode=\"categorical\",\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE), \n                                        batch_size=BATCH_SIZE)","a5396b7f":"val_gen = train_datagen.flow_from_dataframe(dataframe=val_data, \n                                        directory=TRAIN_DIR, \n                                        x_col=\"filename\", \n                                        y_col=\"label\",\n                                        class_mode=\"categorical\",\n                                        shuffle=False,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE), \n                                        batch_size=BATCH_SIZE)","a6a6cd10":"base_model = keras.applications.ResNet50(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n                                    include_top=False,\n                                    weights='imagenet')\nbase_model.trainable = True\n\nmodel = keras.Sequential([\n  base_model,\n  keras.layers.GlobalAveragePooling2D(),\n  keras.layers.Dense(len(LABELS), activation='sigmoid')\n])\n\n# Print out model summary\nmodel.summary()","13c7616f":"import tensorflow.keras.backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","6ee40e86":"mcp = keras.callbacks.ModelCheckpoint(\"resnet50.h5\", monitor=\"val_f1\", save_best_only=True, save_weights_only=True, verbose=1,mode='max')\nrlr = keras.callbacks.ReduceLROnPlateau(monitor='val_f1', factor=LEARNING_RATE_SCHEDULE_FACTOR, mode='max', patience=LEARNING_RATE_SCHEDULE_PATIENCE, min_lr=1e-8, verbose=1)\ncallbacks = [mcp, rlr]","67bd7d36":"device = '\/gpu:0'\n\nwith tf.device(device):\n    steps_per_epoch = train_gen.n \/\/ BATCH_SIZE\n    validation_steps = val_gen.n \/\/ BATCH_SIZE\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=LEARNING_RATE), loss='binary_crossentropy', metrics=[f1])\n\n    # Hu\u1ea5n luy\u1ec7n\n    history = model.fit_generator(train_gen,\n                                  steps_per_epoch=steps_per_epoch,\n                                  epochs=MAX_EPOCHS,\n                                  verbose=1,\n                                  validation_data=val_gen,\n                                  validation_steps=validation_steps,\n                                  callbacks=callbacks)","7ef159ca":"test_df = pd.read_csv(os.path.join(ROOT, 'sample_submission.csv'))\ntest_df.head()","f574bf91":"test_gen = train_datagen.flow_from_dataframe(dataframe=test_df,\n                                             directory=TEST_DIR,\n                                             x_col=\"filename\",\n                                             class_mode=None,\n                                             shuffle=False,\n                                             target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                             batch_size=BATCH_SIZE)","34018aae":"!ls","b1472b5d":"model.load_weights(\"resnet50.h5\")","0e164632":"pred = model.predict_generator(test_gen)","4b51a60a":"labels = (train_gen.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\nlabels\nLABELS = list(LABELS)\n\ndef probs2label(probs):\n    ''' Return real index following LABELS\n    '''\n    global LABELS, labels\n    return \" \".join([str(LABELS.index(labels[idx])) for idx, prob in enumerate(probs) if prob > 0.5])","de07e2bc":"#test_df['predicted'] = np.apply_along_axis(probs2label, 1, pred)\nfor idx, row in test_df.iterrows():\n    test_df.loc[idx]['predicted'] = probs2label(pred[idx])","20c5a88d":"test_df.to_csv(\"submission.csv\", index=False)","92604b5e":"test_df.head()","688e6cd1":"pred","f2f071c5":"train_gen.class_indices","22502edd":"## Set some neccessary hyperparamaters","2c23d079":"# Week01 - Assignment01: Retinal Disease Classificatioin (Tesorflow Vesrion)\n\n### @Class: Advanced Computer Vision\n### @Organize: VietAI\n### @Description: We are proceeding to build a CNN model that allows to diagnosis  retinal diseases.\n\n\n<b>Student Infomation:<\/b>\n- Name: [your fullname]\n- Email: [your email]\n- Phone: [your phone]\n\n<hr>","cfec04c3":"## Import and install all neccessary libraries\n","5cf67e7c":"#Step 03: DEFINE BASELINE MODEL\nIn this notebook, we will use Keras library to implement and train ResNet50 as a baseline model. With initial weights from ImageNet, we will retrain all layers for this problem.","d81e53a6":"# Step 05: TESTING (INFERENCE)","bfb911ee":"## Split the dataset\n*For* the data provided, we will split the dataset to 80% for training and 20% for validation","09689fa4":"# Step 01: SETUP ENVIRONMENT\n","fdc0d97e":"## Implement Dataset loader\nIn Keras, you can use `ImageDataGenerator` to feed image to the model. However, the supported augmentation of `ImageDataGenerator` is not enough, we can add more from another library in this `preprocessing_image` below","0146bc7f":"`ImageDataGenerator` only accepts list of strings as label, we need to convert the label in the dataframe following that way","2ce626ab":"## Write result to submission file","704df95f":"As we can see, **opacity**, **normal** and **glaucoma** are diseases that share largest proportions in label distribution. The other diseases or combinations just account for small pieces.","ca7f8f6b":"## Observations on the dataset\nThe dataset provided is extremely imbalanced. In this baseline model, by simply train the model the original dataset, we will easily get overfitting on the training set and the score on the test set is very low. With the proposed methods below, you will tweak the training process and improve the metric score on the test set:\n- **Image Augmentation**: By augmenting images, we will have more data and make the training set become more regularize. [imgaug](https:\/\/github.com\/aleju\/imgaug) is a very strong augmentation library that you can use in this assignment\n- **Data sampling**: the idea here is to make the distribution between classes in the dataset balance. There are 2 kinds: oversampling and undersampling\n- **Adjust loss function**: the current loss function becomes very small after several epochs. By adding weights, we adjust the loss function to make it suitable for this imbalanced dataset. You can check the [BCEWithLogitsLoss](https:\/\/pytorch.org\/docs\/stable\/nn.html#bcewithlogitsloss) and try applying it.\n- **To simplify the baseline model, the dataset is splited randomly. However, to improve the model, cross-validation techniques can be applied here**","3c10c523":"## Training\nFully training model\n","a93b5d45":"Create training generator","02ca8dce":"# Step 04: TRAINING ","e5ae8c2b":"## Create test data generator","7cba37bd":"## Data analyzing","e8c39ea4":"## Read the test data","19a557b5":"## Read dataset","5e4f4f9e":"## Define F1-score\nKeras recently removed the F1-score metric, we will implement it in the function below:","1aa84351":"Create data generator object","537a143d":"## Define callbacks\nThere are 2 callbacks we need to add to the training:\n- Saving the best model on validation set\n- Reduce learning rate during training","6f6a4956":"We also need to create validation generator. Different from training generator, we don't shuffle the validation set","3d421bff":"## Define model\nIn the baseline, we use ResNet50 pretrained on ImageNet dataset. The classifier of model would be replaced with a new dense layer to make the output suit the problem.","aae8c2fb":"## Load best model weights and switch to evaluation mode","7cdd40c7":"### Analyze distribution of 0 and 1 for each label","d3201ba7":"We need to train about 23 millions parameters","d0b22b7e":"### Analyze combination of classes","1bbd4eb1":"## Predict test images","f0c235ad":"As can be observed, the number of label 0 is much more larger than label 1"}}