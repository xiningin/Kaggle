{"cell_type":{"170441ac":"code","377fab38":"code","affe3095":"code","6a7c78ce":"code","61d7a29e":"code","001d3a6b":"code","6c381147":"code","a98a42c1":"code","89bb6ca1":"code","34d7585e":"code","2bae4655":"code","974eec50":"code","a0d5ab27":"code","19bcae03":"code","063c2ac4":"code","0372ee02":"code","78a6ae94":"code","ca657e01":"code","ef18f268":"code","4f5f29ff":"code","68ee1381":"code","7b550b62":"code","413ad96b":"code","d0661fd1":"code","1901fa7a":"markdown","8e9bac00":"markdown","dc994918":"markdown","65cea6b5":"markdown","1f609c54":"markdown","4d5da90a":"markdown","4c9c0ecf":"markdown","c9506752":"markdown","d626dfa6":"markdown","5a0aaaee":"markdown","97aa7909":"markdown","5f306e6a":"markdown"},"source":{"170441ac":"!pip install luminol\nimport luminol\n\nfrom luminol import anomaly_detector,correlator\n\nfrom luminol.anomaly_detector import AnomalyDetector\nfrom luminol.correlator import Correlator","377fab38":"import matplotlib.pyplot as plt# Standardize\/scale the dataset and apply PCA\nfrom sklearn.decomposition import PCA\ndef Score_data(pred, real):\n    # computing errors\n    errors = np.abs(pred - real).flatten()\n    # estimation\n    mean = sum(errors)\/len(errors)\n    cov = 0\n    for e in errors:\n        cov += (e - mean)**2\n    cov \/= len(errors)\n\n    print('mean : ', mean)\n    print('cov : ', cov)\n    return errors, cov, mean\n\n# calculate Mahalanobis distance\ndef Mahala_distantce(x,mean,cov):\n    return (x - mean)**2 \/ cov\n\n\ndef scale(A):\n    return (A-np.min(A))\/(np.max(A) - np.min(A))\n","affe3095":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n## see this: https:\/\/github.com\/waico\/SKAB\/\ndef evaluating_change_point(true, prediction, metric='nab', numenta_time=None):\n    \"\"\"\n    true - both:\n                list of pandas Series with binary int labels\n                pandas Series with binary int labels\n    prediction - both:\n                      list of pandas Series with binary int labels\n                      pandas Series with binary int labels\n    metric: 'nab', 'binary' (FAR, MAR), 'average_delay'\n                \n    \"\"\"\n    \n    def binary(true, prediction):      \n        \"\"\"\n        true - true binary series with 1 as anomalies\n        prediction - trupredicted binary series with 1 as anomalies\n        \"\"\"\n        def single_binary(true,prediction):\n            true_ = true == 1 \n            prediction_ = prediction == 1\n            TP = (true_ & prediction_).sum()\n            TN = (~true_ & ~prediction_).sum()\n            FP = (~true_ & prediction_).sum()\n            FN = (true_ & ~prediction_).sum()\n            return TP,TN,FP,FN\n            \n        if type(true) != type(list()):\n            TP,TN,FP,FN = single_binary(true,prediction)\n        else:\n            TP,TN,FP,FN = 0,0,0,0\n            for i in range(len(true)):\n                TP_,TN_,FP_,FN_ = single_binary(true[i],prediction[i])\n                TP,TN,FP,FN = TP+TP_,TN+TN_,FP+FP_,FN+FN_       \n    \n        f1 = round(TP\/(TP+(FN+FP)\/2), 2)\n        print(f'False Alarm Rate {round(FP\/(FP+TN)*100,2)} %' )\n        print(f'Missing Alarm Rate {round(FN\/(FN+TP)*100,2)} %')\n        print(f'F1 metric {f1}')\n        return f1\n    \n    def average_delay(detecting_boundaries, prediction):\n        \n        def single_average_delay(detecting_boundaries, prediction):\n            missing = 0\n            detectHistory = []\n            for couple in detecting_boundaries:\n                t1 = couple[0]\n                t2 = couple[1]\n                if prediction[t1:t2].sum()==0:\n                    missing+=1\n                else:\n                    detectHistory.append(prediction[prediction ==1][t1:t2].index[0]-t1)\n            return missing, detectHistory\n            \n        \n        if type(prediction) != type(list()):\n            missing, detectHistory = single_average_delay(detecting_boundaries, prediction)\n        else:\n            missing, detectHistory = 0, []\n            for i in range(len(prediction)):\n                missing_, detectHistory_ = single_average_delay(detecting_boundaries[i], prediction[i])\n                missing, detectHistory = missing+missing_, detectHistory+detectHistory_\n\n        add = pd.Series(detectHistory).mean()\n        print('Average delay', add)\n        print(f'A number of missed CPs = {missing}')\n        return add\n    \n    def evaluate_nab(detecting_boundaries, prediction, table_of_coef=None):\n        \"\"\"\n        Scoring labeled time series by means of\n        Numenta Anomaly Benchmark methodics\n        Parameters\n        ----------\n        detecting_boundaries: list of list of two float values\n            The list of lists of left and right boundary indices\n            for scoring results of labeling\n        prediction: pd.Series with timestamp indices, in which 1 \n            is change point, and 0 in other case. \n        table_of_coef: pandas array (3x4) of float values\n            Table of coefficients for NAB score function\n            indeces: 'Standart','LowFP','LowFN'\n            columns:'A_tp','A_fp','A_tn','A_fn'\n        Returns\n        -------\n        Scores: numpy array, shape of 3, float\n            Score for 'Standart','LowFP','LowFN' profile \n        Scores_null: numpy array, shape 3, float\n            Null score for 'Standart','LowFP','LowFN' profile             \n        Scores_perfect: numpy array, shape 3, float\n            Perfect Score for 'Standart','LowFP','LowFN' profile  \n        \"\"\"\n        def single_evaluate_nab(detecting_boundaries, prediction, table_of_coef=None, name_of_dataset=None):\n            if table_of_coef is None:\n                table_of_coef = pd.DataFrame([[1.0,-0.11,1.0,-1.0],\n                                     [1.0,-0.22,1.0,-1.0],\n                                      [1.0,-0.11,1.0,-2.0]])\n                table_of_coef.index = ['Standart','LowFP','LowFN']\n                table_of_coef.index.name = \"Metric\"\n                table_of_coef.columns = ['A_tp','A_fp','A_tn','A_fn']\n\n            alist = detecting_boundaries.copy()\n            prediction = prediction.copy()\n\n            Scores, Scores_perfect, Scores_null=[], [], []\n            for profile in ['Standart', 'LowFP', 'LowFN']:       \n                A_tp = table_of_coef['A_tp'][profile]\n                A_fp = table_of_coef['A_fp'][profile]\n                A_fn = table_of_coef['A_fn'][profile]\n                def sigm_scale(y, A_tp, A_fp, window=1):\n                    return (A_tp-A_fp)*(1\/(1+np.exp(5*y\/window))) + A_fp\n\n                #First part\n                score = 0\n                if len(alist)>0:\n                    score += prediction[:alist[0][0]].sum()*A_fp\n                else:\n                    score += prediction.sum()*A_fp\n                #second part\n                for i in range(len(alist)):\n                    if i<=len(alist)-2:\n                        win_space = prediction[alist[i][0]:alist[i+1][0]].copy()\n                    else:\n                        win_space = prediction[alist[i][0]:].copy()\n                    win_fault = prediction[alist[i][0]:alist[i][1]]\n                    slow_width = int(len(win_fault)\/4)\n\n                    if len(win_fault) + slow_width >= len(win_space):\n                        print(f'Intersection of the windows of too wide widths for dataset {name_of_dataset}')\n                        win_fault_slow = win_fault.copy()\n                    else:\n                        win_fault_slow= win_space[:len(win_fault)  +  slow_width]\n\n                    win_fp = win_space[-len(win_fault_slow):]\n\n                    if win_fault_slow.sum() == 0:\n                        score+=A_fn\n                    else:\n                        #to get the first index\n                        tr = pd.Series(win_fault_slow.values,index = range(-len(win_fault), len(win_fault_slow)-len(win_fault)))\n                        tr_values= tr[tr==1].index[0]\n                        tr_score = sigm_scale(tr_values, A_tp,A_fp,slow_width)\n                        score += tr_score\n                        score += win_fp.sum()*A_fp\n                Scores.append(score)\n                Scores_perfect.append(len(alist)*A_tp)\n                Scores_null.append(len(alist)*A_fn)\n            return np.array([np.array(Scores),np.array(Scores_null), np.array(Scores_perfect)])\n       #======      \n        if type(prediction) != type(list()):\n            matrix = single_evaluate_nab(detecting_boundaries, prediction, table_of_coef=table_of_coef)\n        else:\n            matrix = np.zeros((3,3))\n            for i in range(len(prediction)):\n                matrix_ = single_evaluate_nab(detecting_boundaries[i], prediction[i], table_of_coef=table_of_coef,name_of_dataset=i)\n                matrix = matrix + matrix_      \n                \n        results = {}\n        desc = ['Standart', 'LowFP', 'LowFN'] \n        for t, profile_name in enumerate(desc):\n            results[profile_name] = round(100*(matrix[0,t]-matrix[1,t])\/(matrix[2,t]-matrix[1,t]), 2)\n            print(profile_name,' - ', results[profile_name])\n        \n        return results\n            \n            \n    #=========================================================================\n    if type(true) != type(list()):\n        true_items = true[true==1].index\n    else:\n        true_items = [true[i][true[i]==1].index for i in range(len(true))]\n        \n\n    if not metric=='binary':\n        def single_detecting_boundaries(true, numenta_time, true_items):\n            detecting_boundaries=[]\n            td = pd.Timedelta(numenta_time) if numenta_time is not None else pd.Timedelta((true.index[-1]-true.index[0])\/len(true_items))  \n            for val in true_items:\n                detecting_boundaries.append([val, val + td])\n            return detecting_boundaries\n        \n        if type(true) != type(list()):\n            detecting_boundaries = single_detecting_boundaries(true=true, numenta_time=numenta_time, true_items=true_items)\n        else:\n            detecting_boundaries=[]\n            for i in range(len(true)):\n                detecting_boundaries.append(single_detecting_boundaries(true=true[i], numenta_time=numenta_time, true_items=true_items[i]))\n\n    if metric== 'nab':\n        return evaluate_nab(detecting_boundaries, prediction)\n    elif metric=='average_delay':\n        return average_delay(detecting_boundaries, prediction)\n    elif metric== 'binary':\n        return binary(true, prediction)","6a7c78ce":"# benchmark files checking\nall_files=[]\nimport os\nfor root, dirs, files in os.walk(\"..\/input\/skoltech-anomaly-benchmark-skab\/SKAB\"):\n    for file in files:\n        if file.endswith(\".csv\"):\n             all_files.append(os.path.join(root, file))","61d7a29e":"# datasets with anomalies loading\nlist_of_df = [pd.read_csv(file, \n                          sep=';', \n                          index_col='datetime', \n                          parse_dates=True) for file in all_files if 'anomaly-free' not in file]\n# anomaly-free df loading\nanomaly_free_df = pd.read_csv([file for file in all_files if 'anomaly-free' in file][0], \n                            sep=';', \n                            index_col='datetime', \n                            parse_dates=True)","001d3a6b":"# dataset characteristics printing\nprint(f'A number of datasets in the SkAB v1.0: {len(list_of_df)}\\n')\nprint(f'Shape of the random dataset: {list_of_df[0].shape}\\n')\nn_cp = sum([len(df[df.changepoint==1.]) for df in list_of_df])\nn_outlier = sum([len(df[df.anomaly==1.]) for df in list_of_df])\nprint(f'A number of changepoints in the SkAB v1.0: {n_cp}\\n')\nprint(f'A number of outliers in the SkAB v1.0: {n_outlier}\\n')\nprint(f'Head of the random dataset:')\ndisplay(list_of_df[0].head())","6c381147":"# random dataset visualizing\nlist_of_df[0].plot(figsize=(12,6))\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Signals')\nplt.show()","a98a42c1":"# plotting the labels both for outlier and changepoint detection problems\nlist_of_df[0].anomaly.plot(figsize=(12,3))\nlist_of_df[0].changepoint.plot()\nplt.legend()\nplt.show()","89bb6ca1":"def scoreLuminolALLData(ts_dict):    \n    data = np.array(ts_dict)\n    ts_s = pd.Series(data)\n    ts_dict = ts_s.to_dict()\n\n\n    detector = anomaly_detector.AnomalyDetector(ts_dict)\n    score = detector.get_all_scores()\n    score_v = []\n    for timestamp, value in score.iteritems():\n        score_v.append(value)\n#         print(timestamp, value)\n    return score_v","34d7585e":"# inference\npredicted_outlier, predicted_cp = [], []\nfor df in list_of_df:\n    X_train = df.drop(['Accelerometer1RMS','Accelerometer2RMS', 'Current', 'Temperature', 'Thermocouple', 'Voltage', 'Pressure', 'anomaly','changepoint'], axis=1)\n#     pca = PCA(n_components=1)\n#     principalComponents = pca.fit_transform(X_train.values.reshape(-1,1))\n#     principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1'])\n    ts_s = scoreLuminolALLData(X_train['Volume Flow RateRMS'].values)\n\n    errors, cov, mean = Score_data(ts_s , X_train['Volume Flow RateRMS'].values)\n    mahala_dist = []\n    for e in errors:\n        mahala_dist.append(Mahala_distantce(e, mean, cov))\n\n    X_train['pca1_value'] = X_train['Volume Flow RateRMS']\n    X_train['pca1_scores'] = mahala_dist\n    X_train['pca1_scores_norm'] = scale(mahala_dist)\n\n    \n    q1_pc1, q3_pc1 = X_train['pca1_scores'].quantile([0.10, 0.75])\n    iqr_pc1 = q3_pc1 - q1_pc1\n    # Calculate upper and lower bounds for outlier for pc1\n    lower_pc1 = q1_pc1 - (1.5*iqr_pc1)\n    upper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n    # Filter out the outliers from the pc1\n    X_train['outlier_pca1'] = ((X_train['pca1_scores']>upper_pc1) | (X_train['pca1_scores']<lower_pc1)).astype('int')\n    \n    \n    # results predicting\n    prediction = pd.Series(X_train['outlier_pca1'], \n                                index=df.index).rolling(3).median().fillna(0).replace(-1,0)\n    \n    # predicted outliers saving\n    predicted_outlier.append(prediction)\n    \n    # predicted CPs saving\n    prediction_cp = abs(prediction.diff())\n    prediction_cp[0] = prediction[0]\n    predicted_cp.append(prediction_cp)\n","2bae4655":"# true changepoint indices selection\ntrue_cp = [df.changepoint for df in list_of_df]\n\npredicted_cp[0].plot(figsize=(12,3), label='predictions', marker='o', markersize=5)\ntrue_cp[0].plot(marker='o', markersize=2)\nplt.legend();","974eec50":"# true outlier indices selection\ntrue_outlier = [df.anomaly for df in list_of_df]\n\npredicted_outlier[0].plot(figsize=(12,3), label='predictions', marker='o', markersize=5)\ntrue_outlier[0].plot(marker='o', markersize=2)\nplt.legend();","a0d5ab27":"# binary classification metrics calculation\nbinary = evaluating_change_point(true_outlier, predicted_outlier, metric='binary', numenta_time='30 sec')","19bcae03":"# average detection delay metric calculation\nadd = evaluating_change_point(true_cp, predicted_cp, metric='average_delay', numenta_time='30 sec')","063c2ac4":"# nab metric calculation\nnab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')","0372ee02":"X_train['anomaly'] = df['anomaly']\nX_train['changepoint'] = df['changepoint']","78a6ae94":"# visualization\na = X_train.loc[X_train['anomaly'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(X_train[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('True Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","ca657e01":"# visualization\na = X_train.loc[X_train['outlier_pca1'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(X_train[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","ef18f268":"N = X_train.shape[0]\nplt.scatter(range(N),X_train['pca1_scores_norm'][:N].cumsum(),marker='1',label='PCA ')\nplt.xlabel('Readings')\nplt.ylabel('anomalies frequency')\nplt.legend()\nplt.show()","4f5f29ff":"#2 -- Distributions of Predicted Probabilities of both classes\nlabels=['Positive','Negative']\nplt.hist(X_train[X_train['outlier_pca1']==1]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='green',  label=labels[0])\nplt.hist(X_train[X_train['outlier_pca1']==0]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='red', label=labels[1])\nplt.axvline(.5, color='blue', linestyle='--', label='decision boundary')\n# plt.xlim([0,1])\nplt.title('Distributions', size=13)\nplt.xlabel('Norm values', size=13)\nplt.ylabel('Readings (norm.)', size=13)\nplt.legend(loc=\"upper right\")","68ee1381":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint(classification_report(X_train['anomaly'], X_train['outlier_pca1']))\nconfusion_matrix(X_train['anomaly'], X_train['outlier_pca1'])","7b550b62":"print(classification_report(X_train['changepoint'], X_train['outlier_pca1']))\nconfusion_matrix(X_train['changepoint'], X_train['outlier_pca1'])","413ad96b":"from sklearn.metrics import roc_auc_score\nroc_auc_score(X_train['outlier_pca1'], X_train['anomaly'])","d0661fd1":"roc_auc_score(X_train['outlier_pca1'], X_train['changepoint'])","1901fa7a":"binary classification metrics calculation","8e9bac00":"average detection delay metric calculation","dc994918":"#### Visualizations","65cea6b5":"true outlier indices selection","1f609c54":"### Method applying","4d5da90a":"###  Labels","4c9c0ecf":"# Metrics calculation","c9506752":"### Data description and visualization","d626dfa6":"# Introduction\nAnomaly detection has applications in many fields, such as system health monitoring, fraud detection, and intrusion detection.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3595464%2F4088133a20318f4e47e1e2d738509d12%2F__results___5_0.png?generation=1590869249365044&alt=media)\n\n## Using Luminol\nDetecting Outliers and Change Points from Time Series","5a0aaaee":"# Results","97aa7909":"nab metric calculation","5f306e6a":"true changepoint indices selection"}}