{"cell_type":{"b26f97c8":"code","953994bc":"code","ef9b9748":"code","ae035f2a":"code","ed16c7af":"code","5b12e3f4":"code","eaf44374":"code","fffe17e3":"code","ebf00ed0":"code","162c057e":"code","1a368db4":"code","83676349":"code","f405706a":"code","4c6057f0":"code","058400a0":"code","17017973":"code","a7f20c22":"code","cafdda91":"code","a2d53f23":"code","cb050f36":"code","fe88e5e7":"code","8cec5b8b":"code","e7cf29ec":"code","f2c0820a":"code","30c0e21a":"code","786a8a7e":"code","d9c4578c":"code","68fe0c4c":"code","176617a8":"code","98ee28a9":"code","e3b30feb":"code","25d84d0b":"code","a1acb791":"code","aa65c49f":"markdown","d453409d":"markdown","2c7a4261":"markdown","7b869e3c":"markdown","82200cdf":"markdown"},"source":{"b26f97c8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM","953994bc":"df = pd.read_csv('\/kaggle\/input\/nyse\/prices-split-adjusted.csv')","ef9b9748":"df","ae035f2a":"df = df[df['symbol']=='AAP']\ndf","ed16c7af":"df_close = df.reset_index()['close']\ndf_close.tail()","5b12e3f4":"df_close.shape","eaf44374":"plt.plot(df_close)\nplt.show()","fffe17e3":"scaler = MinMaxScaler(feature_range=(0,1))\ndf_close = scaler.fit_transform(np.array(df_close).reshape(-1,1))","ebf00ed0":"df_close.shape","162c057e":"plt.plot(df_close)\nplt.show()","1a368db4":"train_size = int(len(df_close)*0.65)\ntest_size = len(df_close) - train_size\ntrain_data = df_close[0:train_size,:]\ntest_data = df_close[train_size:,:]","83676349":"train_size, test_size","f405706a":"plt.figure(figsize=[15,5])\nplt.subplot(121)\nplt.plot(train_data)\nplt.title('Train Data')\nplt.subplot(122)\nplt.plot(test_data)\nplt.title('Test Data')\nplt.show()","4c6057f0":"def create_dataset(dataset, time_step=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-time_step-1):\n        dataX.append(dataset[i:(i+time_step),0])\n        dataY.append(dataset[(i+time_step),0])\n    \n    return np.array(dataX), np.array(dataY)","058400a0":"time_step = 100\ntrain_x, train_y = create_dataset(train_data, time_step)\ntest_x, test_y = create_dataset(test_data, time_step)","17017973":"train_x.shape, train_y.shape, test_x.shape, test_y.shape","a7f20c22":"train_x = train_x.reshape(train_x.shape[0],train_x.shape[1],1)\ntest_x = test_x.reshape(test_x.shape[0],test_x.shape[1],1)","cafdda91":"train_x.shape, train_y.shape, test_x.shape, test_y.shape","a2d53f23":"model = Sequential()\nmodel.add(LSTM(50, return_sequences=True, input_shape=(100,1)))\nmodel.add(LSTM(50, return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))","cb050f36":"model.compile(loss='mean_squared_error', optimizer='adam')","fe88e5e7":"model.summary()","8cec5b8b":"model_history = model.fit(train_x, train_y, \n                          validation_data=(test_x, test_y), \n                          epochs=100, batch_size=64)","e7cf29ec":"model_history.history.keys()","f2c0820a":"plt.figure(figsize=[10,6])\nplt.plot(model_history.history['loss'], label='train loss')\nplt.plot(model_history.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()","30c0e21a":"train_predict = model.predict(train_x)\ntest_predict = model.predict(test_x)","786a8a7e":"trainPredictPlot = np.empty_like(df_close)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[time_step:len(train_predict)+time_step,:] = train_predict\n\ntestPredictPlot = np.empty_like(df_close)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_predict) + (time_step*2) +1:len(df_close)-1, :] = test_predict\n\nplt.figure(figsize=[12,8])\nplt.plot(scaler.inverse_transform(df_close))\nplt.plot(scaler.inverse_transform(trainPredictPlot))\nplt.plot(scaler.inverse_transform(testPredictPlot))\nplt.show()\n\n","d9c4578c":"x_input = test_data[0:100].reshape(1,-1)\nx_input.shape","68fe0c4c":"temp_input = list(x_input)\ntemp_input = temp_input[0].tolist()\ntemp_input","176617a8":"output = []\ndays = 517\nfor i in range(days):\n    print('start')\n    x_input = np.array(temp_input[i:])\n    print(f'{i} day input {x_input}')    \n    x_input = x_input.reshape((1, time_step, 1))\n    yhat = model.predict(x_input, verbose=0)\n    print(yhat)\n    temp_input.extend(yhat[0].tolist())\n    output.extend(yhat[0].tolist())    \n    ","98ee28a9":"type(output)","e3b30feb":"outputnp = np.array(output)","25d84d0b":"outputnp = outputnp.reshape(-1,1)\noutputnp.shape","a1acb791":"testPredictPlot = np.empty_like(df_close)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_predict) + (time_step*2) +1:, :] = outputnp\n\nplt.figure(figsize=[12,8])\nplt.plot(scaler.inverse_transform(df_close))\nplt.plot(scaler.inverse_transform(testPredictPlot))\nplt.show()\n","aa65c49f":"## Forecasting","d453409d":"### MinMax Scaling as LSTM is sensitive to scale of data","2c7a4261":"# Stock Price Forecasting using Stacked LSTM","7b869e3c":"## Model","82200cdf":"## Predict next 30 days"}}