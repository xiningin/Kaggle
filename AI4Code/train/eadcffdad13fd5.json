{"cell_type":{"7616294f":"code","23afe1be":"code","f1a7dba0":"code","8643b2c2":"code","085b95e8":"code","05536bde":"code","39e20d85":"code","974dbac8":"code","df0c057b":"code","350ac6f9":"code","af5dc902":"code","eba01cfd":"code","a4d2b922":"code","eebfd618":"code","8d224320":"code","fe1f89df":"code","7ef9db2c":"code","304db9a8":"code","629ee3ce":"code","1ff11932":"code","e9c70595":"code","1b7a01d0":"code","c21d09e0":"code","0ba77633":"code","6ebea855":"markdown","961ab1b0":"markdown","96d4a51a":"markdown","80712432":"markdown","884f6ee4":"markdown","414261d9":"markdown","d2db649c":"markdown","9c6fa206":"markdown","2274c8c0":"markdown","6eaa0f02":"markdown","495b12e3":"markdown","7da33d8d":"markdown","ea11fd9b":"markdown","c480813d":"markdown","78390a49":"markdown","70c03d29":"markdown","383918f9":"markdown","cbc09403":"markdown","2561672a":"markdown","37787a5a":"markdown","fd55c8b8":"markdown","1bb189cc":"markdown","5a60e9c3":"markdown","b123af44":"markdown","712d82fd":"markdown","433475af":"markdown","3d79bba1":"markdown","6d980b4c":"markdown","b9f00516":"markdown","6df06da9":"markdown","ffe1b5d0":"markdown","6a1bf2eb":"markdown","6fdc539a":"markdown","4f2fd224":"markdown","a863e4c0":"markdown","f095b114":"markdown"},"source":{"7616294f":"import tensorflow as tf","23afe1be":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","f1a7dba0":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt","8643b2c2":"_URL = 'https:\/\/storage.googleapis.com\/mledu-datasets\/cats_and_dogs_filtered.zip'\n\nzip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)","085b95e8":"base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')","05536bde":"train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures","39e20d85":"num_cats_tr = len(os.listdir(train_cats_dir))\nnum_dogs_tr = len(os.listdir(train_dogs_dir))\n\nnum_cats_val = len(os.listdir(validation_cats_dir))\nnum_dogs_val = len(os.listdir(validation_dogs_dir))\n\ntotal_train = num_cats_tr + num_dogs_tr\ntotal_val = num_cats_val + num_dogs_val","974dbac8":"print('total training cat images:', num_cats_tr)\nprint('total training dog images:', num_dogs_tr)\n\nprint('total validation cat images:', num_cats_val)\nprint('total validation dog images:', num_dogs_val)\nprint(\"--\")\nprint(\"Total training images:\", total_train)\nprint(\"Total validation images:\", total_val)","df0c057b":"BATCH_SIZE = 100\nIMG_SHAPE  = 150 # Our training data consists of images with width of 150 pixels and height of 150 pixels","350ac6f9":"# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n    plt.tight_layout()\n    plt.show()","af5dc902":"image_gen = ImageDataGenerator(rescale=1.\/255, horizontal_flip=True)\n\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_SHAPE,IMG_SHAPE))","eba01cfd":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]\nplotImages(augmented_images)","a4d2b922":"image_gen = ImageDataGenerator(rescale=1.\/255, rotation_range=45)\n\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_SHAPE, IMG_SHAPE))","eebfd618":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]\nplotImages(augmented_images)","8d224320":"image_gen = ImageDataGenerator(rescale=1.\/255, zoom_range=0.5)\n\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n","fe1f89df":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]\nplotImages(augmented_images)","7ef9db2c":"image_gen_train = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\ntrain_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,\n                                                     directory=train_dir,\n                                                     shuffle=True,\n                                                     target_size=(IMG_SHAPE,IMG_SHAPE),\n                                                     class_mode='binary')","304db9a8":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]\nplotImages(augmented_images)","629ee3ce":"image_gen_val = ImageDataGenerator(rescale=1.\/255)\n\nval_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE,\n                                                 directory=validation_dir,\n                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n                                                 class_mode='binary')","1ff11932":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(2)\n])","e9c70595":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","1b7a01d0":"model.summary()","c21d09e0":"epochs=100\nhistory = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=int(np.ceil(total_train \/ float(BATCH_SIZE))),\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=int(np.ceil(total_val \/ float(BATCH_SIZE)))\n)","0ba77633":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","6ebea855":"The dataset we have downloaded has following directory structure.\n\n<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n<b>cats_and_dogs_filtered<\/b>\n|__ <b>train<\/b>\n    |______ <b>cats<\/b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]\n    |______ <b>dogs<\/b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n|__ <b>validation<\/b>\n    |______ <b>cats<\/b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]\n    |______ <b>dogs<\/b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n<\/pre>","961ab1b0":"### Visualizing results of the training","96d4a51a":"Let's visualize how a single image would look like five different times, when we pass these augmentations randomly to our dataset. ","80712432":"### Compiling the model\n\nAs usual, we will use the `adam` optimizer. Since we output a softmax categorization, we'll use `sparse_categorical_crossentropy` as the loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so we are passing in the metrics argument.","884f6ee4":"One more time, take a sample image from our training set and repeat it. The augmentation will be randomly applied (or not) to each repetition.","414261d9":"Let's start by importing required packages:\n\n*   os \u2014 to read files and directory structure\n*   numpy \u2014 for some matrix math outside of TensorFlow\n*   matplotlib.pyplot \u2014 to plot the graph and display images in our training and validation data","d2db649c":"We'll now assign variables with the proper file path for the training and validation sets.","9c6fa206":"We can begin by randomly applying horizontal flip augmentation to our dataset and seeing how individual images will look after the transformation. This is achieved by passing `horizontal_flip=True` as an argument to the `ImageDataGenerator` class.","2274c8c0":"# Data Loading","6eaa0f02":"# Data Augmentation","495b12e3":"### Putting it all together","7da33d8d":"# Model Creation","ea11fd9b":"### Understanding our data","c480813d":"### Applying Zoom","78390a49":"For convenience, let us set up variables that will be used later while pre-processing our dataset and training our network.","70c03d29":"To build our image classifier, we begin by downloading the dataset. The dataset we are using is a filtered version of <a href=\"https:\/\/www.kaggle.com\/c\/dogs-vs-cats\/data\" target=\"_blank\">Dogs vs. Cats<\/a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research).\n\nIn previous Colabs, we've used <a href=\"https:\/\/www.tensorflow.org\/datasets\" target=\"_blank\">TensorFlow Datasets<\/a>, which is a very easy and convenient way to use datasets. In this Colab however, we will make use of the class `tf.keras.preprocessing.image.ImageDataGenerator` which will read data from disk. We therefore need to directly download *Dogs vs. Cats* from a URL and unzip it to the Colab filesystem.","383918f9":"### Flipping the image horizontally","cbc09403":"It's time we train our network.\n\nSince our batches are coming from a generator (`ImageDataGenerator`), we'll use `fit_generator` instead of `fit`.","2561672a":"After defining our generators for training and validation images, **flow_from_directory** method will load images from the disk and will apply rescaling and will resize them into required dimensions using single line of code.","37787a5a":"### Creating Validation Data generator","fd55c8b8":"We can also apply Zoom augmentation to our dataset, zooming images up to 50% randomly.","1bb189cc":"## Define the model\n\nThe model consists of four convolution blocks with a max pool layer in each of them.\n\nBefore the final Dense layers, we're also applying a Dropout probability of 0.5. It means that 50% of the values coming into the Dropout layer will be set to zero. This helps to prevent overfitting.\n\nThen we have a fully connected layer with 512 units, with a `relu` activation function. The model will output class probabilities for two classes \u2014 dogs and cats \u2014 using `softmax`. ","5a60e9c3":"We'll now visualize the results we get after training our network.","b123af44":"# Setting Model Parameters","712d82fd":"To see the transformation in action, let's take one sample image from our training set and repeat it five times. The augmentation will be randomly applied (or not) to each repetition.","433475af":"### Model Summary\n\nLet's look at all the layers of our network using **summary** method.","3d79bba1":"To see the transformation in action, let's once again take a sample image from our training set and repeat it. The augmentation will be randomly applied (or not) to each repetition.","6d980b4c":"Let's look at how many cats and dogs images we have in our training and validation directory","b9f00516":"# Dogs vs Cats Image Classification With Image Augmentation","6df06da9":"We can apply all these augmentations, and even others, with just one line of code, by passing the augmentations as arguments with proper values.\n\nHere, we have applied rescale, rotation of 45 degrees, width shift, height shift, horizontal flip, and zoom augmentation to our training images.","ffe1b5d0":"### Rotating the image","6a1bf2eb":"### Train the model","6fdc539a":"Generally, we only apply data augmentation to our training examples, since the original images should be representative of what our model needs to manage. So, in this case we are only rescaling our validation images and converting them into batches using ImageDataGenerator.","4f2fd224":"The rotation augmentation will randomly rotate the image up to a specified number of degrees. Here, we'll set it to 45.","a863e4c0":"Overfitting often occurs when we have a small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number and variety of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples through random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This exposes the model to more aspects of the data, allowing it to generalize better.\n\nIn **tf.keras** we can implement this using the same **ImageDataGenerator** class we used before. We can simply pass different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process.\n\nTo start off, let's define a function that can display an image, so we can see the type of augmentation that has been performed. Then, we'll look at specific augmentations that we'll use during training.","f095b114":"# Importing packages"}}