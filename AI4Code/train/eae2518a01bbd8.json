{"cell_type":{"c0bfe8cc":"code","eca19d76":"code","56e964e5":"code","5216613e":"code","f553518a":"code","8d06876b":"code","3ae536cd":"code","41056ced":"code","ee6ca2fc":"code","28490976":"code","d4315850":"code","d6a1059f":"code","035f3e81":"code","06f41f5a":"code","fbd4619c":"code","08301015":"code","23c4f0d7":"markdown","a2b989d1":"markdown","b4fc4923":"markdown","58b5af37":"markdown","1dbda87e":"markdown"},"source":{"c0bfe8cc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier","eca19d76":"df = pd.read_csv('..\/input\/grid-search\/data.csv',header = None)","56e964e5":"plt.figure()\nplt.scatter(df.iloc[:,0],df.iloc[:,1],c = df.iloc[:,2])\nplt.title('Scatter plot of x and y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()","5216613e":"X = df.iloc[:,0:2]\ny = df.iloc[:,2]","f553518a":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","8d06876b":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_pred_train = clf.predict(X_train)","3ae536cd":"from sklearn import metrics","41056ced":"print(f'accuracy = {metrics.accuracy_score(y_pred, y_test)}')\nprint('\\n')\nprint(f'{metrics.classification_report(y_pred, y_test)}')\nprint('\\n')\nprint(f'{metrics.confusion_matrix(y_pred, y_test)}')\nprint('\\n')\nprint(f'f1_score_train = {metrics.f1_score(y_pred_train, y_train)}')\nprint('\\n')\nprint(f'f1_score = {metrics.f1_score(y_pred, y_test)}')","ee6ca2fc":"from sklearn.model_selection import cross_val_score","28490976":"score = cross_val_score(clf, X, y, cv = 5)\nscore","d4315850":"print(f'accuracy = {score.mean():0.2f} +\/- {score.std()*2:0.2f}')","d6a1059f":"# cross validation using shuffle split\nfrom sklearn.model_selection import ShuffleSplit\ncv = ShuffleSplit(n_splits = 5, test_size = 0.3,random_state = 42)\nscore_shuffle = cross_val_score(clf, X, y, cv = cv)","035f3e81":"print(f'accuracy = {score_shuffle.mean():0.2f} +\/- {score_shuffle.std()*2:0.2f}')","06f41f5a":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import f1_score\n\n#Choose the model\nclf = DecisionTreeClassifier()\n\n#Define the parameters\nparameters = {'max_depth':np.arange(1,10), \n              'min_samples_split':np.arange(2,10),\n               'min_samples_leaf': np.arange(1,10)}\n\n#Make scorer\nscorer = make_scorer(f1_score)\n\n#Make Grid Seach Object\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n\n#Fit\ngrid_obj.fit(X_train, y_train)\n\n#Find the best estimator\nbest_clf = grid_obj.best_estimator_","fbd4619c":"#Predict using the best clf\ny_pred_grid = best_clf.predict(X_test)\ny_pred_grid_train = best_clf.predict(X_train)","08301015":"\nprint(f'training set f1_score without grid search cv ={f1_score(y_pred_train, y_train)}')\nprint(f'testing set f1_score without grid search cv ={f1_score(y_pred, y_test)}')\nprint('\\n')\nprint(f'training set f1_score using grid search cv ={f1_score(y_pred_grid_train, y_train)}')\nprint(f'testing set f1_score using grid search ={f1_score(y_pred_grid, y_test)}')\n\n","23c4f0d7":"# Improving score using GridSearchCV and Cross Validation","a2b989d1":"# GridSearchCV","b4fc4923":"This notebook intends to explain the effects of hyperparametric optimization on model's performance.\n\n## Hyperparametric Optimization\n\nHyperparameters are those parameters of a model, which can't be found out during training of our model. They are not directly learned within the estimator. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include `C`, `kernel` and `gamma` for Support Vector Classifier, `alpha` for Lasso, `max_depth` and `min_samples_leaf` in decision tress etc.\n\nThe choice of hyperparameters can greatly influence model predictions. In scikit-learn, if we simply train a model, without passing the values of these hyperparameters, the model is trained on the default values. But, to improve model accuracy and to make our model robust, we must find the optimal values of the hyperparameters. \n\nOne way to find the optimal hyperparameters is to train our model on different values of these hyperparameters and check accuracy on the validation set. In case of multiple hyperparameters, we need to make a grid having different values at each node as shown in the diagram below.\n\n![grid_seacrh_pic.png](attachment:image.png)\n\nThe validation error is calculated by testing model accuracy on the validation set after training it on training set using each combination of hyperparameters. To understand training, validation and testing errors, [please follow this link](https:\/\/datamaniac.tech\/data_science\/understanding-training-testing-and-validation-errors\/).\n\n## Scikit-Learn Implementation\n\nSklearn provides us a method to perform above grid operations easily using grid_search_cv. We will look at its implementation in this notebook.","58b5af37":"We can see that the f1 score for the training set is 1 and for the testing set it is 0.7. The large difference in training and testing f1 score denotes that we are dealing with the case of high variance (or overfitting). This fact can be understood on the basis of any standard model complexity graph which is as shown below:\n\n![image.png](attachment:image.png)","1dbda87e":"From the above results, if we are using grid search to optimize hyperparameters, our training set error is close to the testing set error and testing set error has increased too. It means that our model is making better prediction as well as it is able to generalize well.\n\n## Conclusion\n\nThat's it for now. In this notebook, we understood the importance of hyperparametric optimization using a simple dataset. Please upvote if you find this notebook helpful, as it motivates me to write more tutorials for beginners. You can visit my website [datamaniac.tech](https:\/\/www.datamaniac.tech) for more such tutorials. Follow me on [Kaggle](https:\/\/www.kaggle.com\/prasun2106), [Github](https:\/\/github.com\/prasun2106) and contact me on [LinkedIn](https:\/\/www.linkedin.com\/in\/prasun-kumar-8250a5119\/).\n"}}