{"cell_type":{"e0c4679a":"code","bd611e83":"code","5d5e61c2":"code","6d5b95f3":"code","4f5e3537":"code","43f70c8f":"code","630f4d01":"code","7bab49de":"code","a5fc54d9":"code","38d1e178":"code","92f54d94":"code","a87066c9":"code","1710b47e":"code","7d484a01":"code","39b106db":"code","ec7f304d":"code","728b99d7":"code","ccf2fee7":"code","5340b3b9":"code","859ccef9":"code","32e1d2e1":"code","1f99c53c":"code","d2f55ec8":"code","206a96b5":"code","2f0dd181":"code","03c63f38":"code","298e9c73":"code","a3d82a1f":"code","792785ac":"code","94be9ea6":"code","8d6d0a4c":"code","9bcccbf5":"code","55382faf":"code","5a69b89c":"code","99223842":"code","4dd20cee":"code","400f4e47":"code","2c16dc0d":"code","9fd0ee5b":"code","85df9218":"code","384c4f57":"markdown","94969dca":"markdown","f36fd278":"markdown","41178770":"markdown","8e882379":"markdown","01980041":"markdown","4ea51b03":"markdown","6d79bce8":"markdown","763c39c6":"markdown","a379e123":"markdown","cc3d9f74":"markdown","c2181ef0":"markdown","2e671a4a":"markdown","ecf43733":"markdown","0f00e5a0":"markdown","90f5a363":"markdown","8c8614cb":"markdown","cb80ae88":"markdown","e69f6161":"markdown","71ce7418":"markdown","c4df0f6d":"markdown"},"source":{"e0c4679a":"from math import *\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport warnings\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import Imputer, StandardScaler\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom tensorflow.python.client import device_lib\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 200)\nwarnings.filterwarnings('ignore')\n\nprint(device_lib.list_local_devices())\n\nconfig = tf.ConfigProto(device_count={\"CPU\": 1, \"GPU\" : 1})\nsession = tf.Session(config=config)\nK.set_session(session)","bd611e83":"#test = pd.read_csv(r\"c:\\work\\dataset\\titanic\\test.csv\", \",\")\n#train = pd.read_csv(r\"c:\\work\\dataset\\titanic\\train.csv\", \",\")\ntest = pd.read_csv(\"..\/input\/test.csv\", \",\")\ntrain = pd.read_csv(\"..\/input\/train.csv\", \",\")\ntest[\"is_test\"] = True\ntrain[\"is_test\"] = False\ncommon = pd.concat([test, train],axis=0).loc[:,[\"PassengerId\", \"Survived\", \"is_test\", \n                                                \"Age\", \"Cabin\", \"Embarked\", \n                                                \"Fare\", \"Name\", \"Parch\", \"Pclass\", \n                                                \"Sex\", \"SibSp\", \"Ticket\"]]","5d5e61c2":"common[\"Ticket\"].count() - len(common[\"Ticket\"].unique())","6d5b95f3":"t = train.groupby(by=\"Ticket\", as_index=False).agg({\"PassengerId\" : 'count', \"Sex\" : lambda x : x[x==\"female\"].count()})\nt.columns = [\"Ticket\", \"SameTicket\", \"FemalesOnTicket\"]\ncommon = pd.merge(common, t, how=\"left\", on=\"Ticket\")\n\ncommon[\"TicketDigits\"] = pd.to_numeric(common[\"Ticket\"].str.split(\" \").str[-1], errors=\"coerce\").astype(np.str).str.len()\ncommon[\"TicketIsNumber\"] = ~common[\"Ticket\"].str.contains(\"[A-Za-z]\", regex = True)\ncommon[\"FemalesPerTicketPart\"] = common[\"FemalesOnTicket\"]\/common[\"SameTicket\"]","4f5e3537":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\nsns.barplot(x=\"TicketDigits\", y=\"Survived\", data=common, ax=ax[0])\nsns.barplot(x=\"TicketIsNumber\", y=\"Survived\", data=common, ax = ax[1])\nsns.regplot(x=\"FemalesPerTicketPart\", y=\"Survived\", data=common, ax = ax[2])\nplt.show();","43f70c8f":"common[\"DoubleName\"] = common[\"Name\"].str.contains(\"\\(\")\ncommon[\"NameLen\"] = common[\"Name\"].str.len()","630f4d01":"fig, ax = plt.subplots(1, 2, figsize=(20, 5))\nsns.barplot(x=\"DoubleName\", y=\"Survived\", data=common, ax=ax[0])\nsns.regplot(x=\"NameLen\", y=\"Survived\", data=common, ax = ax[1])\nplt.show();","7bab49de":"common[\"Title\"] = common[\"Name\"].str.split(\", \").str[1].str.split(\" \").str[0]\ncommon.loc[common[\"Title\"].str[-1]!=\".\", \"Title\"]=\"Bad\"\nrare_title = common[\"Title\"].value_counts()[common[\"Title\"].value_counts() < 5].index\ncommon[\"Title\"] = common[\"Title\"].apply(lambda x: 'Rare' if x in rare_title else x)\n\ntitletarget = common.groupby(by=\"Title\", as_index=False).agg({\"Survived\" : 'mean'})\ntitletarget.columns = [\"Title\", \"TargetByTitle\"]\ncommon = pd.merge(common, titletarget, how=\"left\", on=\"Title\")","a5fc54d9":"sns.barplot(x=\"Title\", y=\"TargetByTitle\", data=common)\nplt.show()","38d1e178":"common[\"Family\"] = common[\"Parch\"] + common[\"SibSp\"] + 1\ncommon[\"Alone\"] = common[\"Family\"] == 1","92f54d94":"cap = train.groupby(by=\"Cabin\", as_index=False).agg({\"PassengerId\" : 'count'})\ncap.columns = [\"Cabin\", \"SameCabin\"]\ncommon = pd.merge(common, cap, how=\"left\", on=\"Cabin\")\ncommon[\"CabinNumber\"] = pd.to_numeric(common[\"Cabin\"].str[1:], errors = \"coerce\")\ncommon[\"CabinEven\"] = common[\"CabinNumber\"] %2\ncommon[\"CabinsPerMan\"] = common[\"Cabin\"].str.split(\" \").str.len()\ncommon[\"Deck\"] = common[\"Cabin\"].str[0].rank().fillna(-1)\n\ndecktarget = common.groupby(by=\"Deck\", as_index=False).agg({\"Survived\" : 'mean'})\ndecktarget.columns = [\"Deck\", \"TargetByDeck\"]\ncommon = pd.merge(common, decktarget, how=\"left\", on=\"Deck\")","a87066c9":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\nsns.barplot(x=\"Deck\", y=\"TargetByDeck\", data=common.sort_values(\"Deck\"), ax=ax[0])\nsns.barplot(x=\"CabinEven\", y=\"Survived\", data=common, ax=ax[1])\nsns.regplot(x=\"CabinNumber\", y=\"Survived\", data=common, ax=ax[2])\n\nplt.show()","1710b47e":"sns.swarmplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", palette=sns.color_palette([\"#20AFCF\",\"#cf4040\"]), data=common)\nplt.show()","7d484a01":"common['AgeGroup'] = pd.qcut(common['Age'].fillna(common['Age'].mean()).astype(int), 6)\nagetarget = common.groupby(by=\"AgeGroup\", as_index=False).agg({\"Survived\" : 'mean'})\nagetarget.columns = [\"AgeGroup\", \"TargetByAgeGroup\"]\ncommon = pd.merge(common, agetarget, how=\"left\", on=\"AgeGroup\")\n\ncommon[\"IsTinyChild\"] = common[\"Age\"]<1\ncommon[\"IsChild\"] = common[\"Age\"]<10\ncommon[\"AverageAge\"] = common[\"Age\"] \/ common[\"Family\"]","39b106db":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\nsns.barplot(x=\"AgeGroup\", y=\"TargetByAgeGroup\", data=common, ax = ax[0])\nsns.barplot(x=\"IsChild\", y=\"Survived\", data=common, ax = ax[1])\nsns.regplot(x=\"AverageAge\", y=\"Survived\", data=common, ax = ax[2])\nplt.show();","ec7f304d":"common['FareGroup'] = pd.qcut(common['Fare'].fillna(common['Fare'].mean()).astype(int), 6)\nfaretarget = common.groupby(by=\"FareGroup\", as_index=False).agg({\"Survived\" : 'mean'})\nfaretarget.columns = [\"FareGroup\", \"TargetByFareGroup\"]\ncommon = pd.merge(common, faretarget, how=\"left\", on=\"FareGroup\")\ncommon[\"AverageFareByFamily\"] = common[\"Fare\"] \/ common[\"Family\"]\ncommon[\"AverageFareByTicket\"] = common[\"Fare\"] \/ common[\"SameTicket\"]\ncommon[\"FareLog\"] = np.log(common[\"Fare\"])","728b99d7":"sns.barplot(x=\"FareGroup\", y=\"TargetByFareGroup\", data=common)\nplt.show()","ccf2fee7":"pclasstarget = common.groupby(by=\"Pclass\", as_index=False).agg({\"Survived\" : 'mean'})\npclasstarget.columns = [\"Pclass\", \"TargetByPclass\"]\ncommon = pd.merge(common, pclasstarget, how=\"left\", on=\"Pclass\")\n\nEmbarkedtarget = common.groupby(by=\"Embarked\", as_index=False).agg({\"Survived\" : 'mean'})\nEmbarkedtarget.columns = [\"Embarked\", \"TargetByEmbarked\"]\ncommon = pd.merge(common, Embarkedtarget, how=\"left\", on=\"Embarked\")\n\nSextarget = common.groupby(by=\"Sex\", as_index=False).agg({\"Survived\" : 'mean'})\nSextarget.columns = [\"Sex\", \"TargetBySex\"]\ncommon = pd.merge(common, Sextarget, how=\"left\", on=\"Sex\")","5340b3b9":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\nsns.barplot(x=\"Pclass\", y=\"TargetByPclass\", data=common, ax=ax[0])\nsns.barplot(x=\"Embarked\", y=\"TargetByEmbarked\", data=common, ax = ax[1])\nsns.barplot(x=\"Sex\", y=\"TargetBySex\", data=common, ax = ax[2])\nplt.show();","859ccef9":"allfeatures = [\n    \"PassengerId\", \n    \"is_test\", \n    \"Survived\", \n    \"Age\", \n    \"Fare\", \n    \"Parch\", \n    \"Pclass\",\n    \"SibSp\", \n    \"Sex\", \n    \"Embarked\", \n    \"SameTicket\", \n    \"FemalesOnTicket\", \n    \"SameCabin\", \n    \"Deck\", \n    \"TargetByDeck\", \n    \"TargetByTitle\", \n    \"TargetByAgeGroup\", \n    \"TargetByFareGroup\",\n    \"TargetByPclass\",\n    \"TargetByEmbarked\",\n    \"TargetBySex\",\n    \"Title\", \n    \"CabinNumber\", \n    \"CabinEven\", \n    \"CabinsPerMan\", \n    \"DoubleName\", \n    \"NameLen\", \n    \"TicketDigits\",\n    \"TicketIsNumber\",\n    \"IsTinyChild\", \n    \"IsChild\", \n    \"Alone\", \n    \"Family\", \n    \"AverageAge\",\n    \"AverageFareByFamily\",\n    \"AverageFareByTicket\",\n    \"FemalesPerTicketPart\"\n]\n\nc = common.loc[:, allfeatures]\nc = pd.get_dummies(c, columns=[\n    \"Title\", \n    \"Embarked\", \n    \"Pclass\",\n    \"Sex\"\n])","32e1d2e1":"c.describe().T.sort_values(\"count\")","1f99c53c":"c.iloc[:,3:] = Imputer(strategy=\"most_frequent\").fit_transform(c.iloc[:,3:])\n\ndep = c[c[\"is_test\"] == False].loc[:, [\"Survived\"]]\nindep = c[c[\"is_test\"] == False].iloc[:, 3:]\nres = c[c[\"is_test\"] == True].iloc[:, 3:]\nres_index = c[c[\"is_test\"] == True].loc[:, \"PassengerId\"]\n\nindep.iloc[:,3:] = StandardScaler().fit_transform(indep.iloc[:,3:])\nres.iloc[:,3:] = StandardScaler().fit_transform(res.iloc[:,3:])\n\nindep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=47)","d2f55ec8":"with tf.device('\/device:CPU:0'):\n    gs1 = Sequential()\n    gs1.add(Dense(45 ,activation='linear', input_dim=45))\n    gs1.add(BatchNormalization())\n\n    gs1.add(Dense(9,activation='linear'))\n    gs1.add(BatchNormalization())\n    gs1.add(Dropout(0.4))\n\n    gs1.add(Dense(5,activation='linear'))\n    gs1.add(BatchNormalization())\n    gs1.add(Dropout(0.2))\n\n    gs1.add(Dense(1,activation='relu', ))\n    gs1.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0), loss='binary_crossentropy', metrics=['accuracy'])\n\n    gs1.fit(indep_train, dep_train, epochs=500, batch_size=30, validation_data=(indep_test,dep_test), verbose=False)\n    g=gs1.predict_classes(res)[:,0]\n    print(accuracy_score(dep_test, gs1.predict_classes(indep_test)), accuracy_score(dep_train, gs1.predict_classes(indep_train)))","206a96b5":"fig, ax = plt.subplots(2, 1, sharex='col', figsize=(20, 10))\nax[0].set_title('Model accuracy history')\nax[0].plot(gs1.history.history['acc'])\nax[0].plot(gs1.history.history['val_acc'])\nax[0].set_ylabel('Accuracy')\nax[0].legend(['train', 'test'], loc='right')\nax[0].grid()\n\nax[1].set_title('Model loss history')\nax[1].plot(gs1.history.history['loss'])\nax[1].plot(gs1.history.history['val_loss'])\nax[1].set_ylabel('Loss')\nax[1].legend(['train', 'test'], loc='right')\nax[1].grid()\nplt.xlabel('Epoch')\nplt.show()","2f0dd181":"cvscores = []\ndata = pd.DataFrame()\ni=1\nwith tf.device('\/device:CPU:0'):\n    for train, test in StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(indep, dep.iloc[:,0]):\n        X = indep.reindex().iloc[train,:]\n        Y = dep.reindex().iloc[train,0]\n        Xv = indep.reindex().iloc[test,:]\n        Yv = dep.reindex().iloc[test,0]\n        \n        gs1 = Sequential()\n        gs1.add(Dense(45 ,activation='linear', input_dim=45))\n        gs1.add(BatchNormalization())\n\n        gs1.add(Dense(9,activation='linear'))\n        gs1.add(BatchNormalization())\n        gs1.add(Dropout(0.4))\n\n        gs1.add(Dense(5,activation='linear'))\n        gs1.add(BatchNormalization())\n        gs1.add(Dropout(0.2))\n\n        gs1.add(Dense(1,activation='relu', ))\n        gs1.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0), loss='binary_crossentropy', metrics=['accuracy'])\n\n        gs1.fit(X, Y, epochs=500, batch_size=30, validation_data=(Xv, Yv), verbose=False)\n        data[i] = gs1.predict_classes(res)[:,0]\n        scores = gs1.evaluate(Xv, Yv, verbose=0)\n        print(gs1.metrics_names[1], scores[1])\n        cvscores.append(scores[1])\n        i+=1\nmlp_mean = np.mean(cvscores)\nmlp_stdev = np.std(cvscores)\nprint(mlp_mean, mlp_stdev)\n\ng = np.round(data.mean(axis=1))","03c63f38":"#result = pd.DataFrame(res_index.astype(np.int), columns=[\"PassengerId\"])\n#result[\"Survived\"] = g.astype(np.int)\n#result.to_csv(r\"c:\\work\\dataset\\titanic\\mlp.csv\", \",\", index=None)\n#result.to_csv(\"mlp.csv\", \",\", index=None)","298e9c73":"# Example of manual parameter tuning\n\"\"\"\nfor i in range(1,10):\n    params = {}\n    params[\"max_depth\"] = i\n    params[\"learning_rate\"] = 0.45\n    params[\"lambda_l1\"] = 0.1\n    params[\"lambda_l2\"] = 0.01\n    params[\"n_estimators\"] = 5000\n    params[\"n_jobs\"]=5 \n    params[\"objective\"] = \"binary\"\n    \n    params[\"boosting\"] = \"dart\"\n    params[\"colsample_bytree\"] = 0.9\n    params[\"subsample\"] =0.9\n\n    train_data = lgb.Dataset(data=indep_train, label=dep_train, free_raw_data=False, feature_name = list(indep_train))\n    cv_result = lgb.cv(params, train_data, nfold=5, stratified=False, metrics=['binary_error'], early_stopping_rounds=50)\n    print(i, 1-np.mean(cv_result[\"binary_error-mean\"]))\n    \"\"\";","a3d82a1f":"indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=47)\ngs1 = lgb.LGBMClassifier(max_depth = 7,\n                         lambda_l1 = 0.1,\n                         lambda_l2 = 0.01,\n                         learning_rate = 0.01, \n                         n_estimators = 500, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9,\n                         n_jobs = 5)\ngs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], eval_metric='accuracy', verbose=False, early_stopping_rounds=50);\n\n\n\ng = gs1.predict(res)\na = accuracy_score(dep_test, gs1.predict(indep_test))\nb = accuracy_score(dep_train, gs1.predict(indep_train))\nprint(a, b)","792785ac":"attr2 = {k: v for k, v in zip(indep.columns, gs1.feature_importances_) if v>0}\nattr2 = sorted(attr2.items(), key=lambda x: x[1], reverse = False)\nx1,y1 = zip(*attr2)\ni1=range(len(x1))\nplt.figure(num=None, figsize=(9, 7), dpi=300, facecolor='w', edgecolor='k')\nplt.barh(i1, y1)\nplt.title(\"LGBM\")\nplt.yticks(i1, x1)\nplt.show();","94be9ea6":"model = []\ncvscores = []\n\nfor i in range(0,90):\n    indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=i)\n    gs1 = lgb.LGBMClassifier(max_depth = 7,\n                             lambda_l1 = 0.1,\n                             lambda_l2 = 0.01,\n                             learning_rate =  0.01, num_iterations=20000,\n                             n_estimators = 5000, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9,\n                             n_jobs = 5, boosting='dart' )\n    gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], eval_metric='accuracy', verbose=False, early_stopping_rounds=50);\n    model.append(gs1)\n    cvscores.append(accuracy_score(dep_test, gs1.predict(indep_test)))\n\ndata = pd.DataFrame()\nte = pd.DataFrame()\nfor i in range(0,90):\n    data[i] = model[i].predict(res)\n    te[i] = model[i].predict(indep_test)\n\ng = np.round(data.mean(axis=1))\nt = np.round(te.mean(axis=1))\n\nlgb_mean = np.mean(cvscores)\nlgb_stdev = np.std(cvscores)\nprint(lgb_mean, lgb_stdev)","8d6d0a4c":"result = pd.DataFrame(res_index.astype(np.int), columns=[\"PassengerId\"])\nresult[\"Survived\"] = g.astype(np.int)\nresult.to_csv(\"lgbm.csv\", \",\", index=None)","9bcccbf5":"gs1 = cb.CatBoostClassifier(depth = 9, reg_lambda=0.1,\n                         learning_rate = 0.09, \n                         iterations = 500)\ngs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)],  verbose=False, early_stopping_rounds=50);\n\ng = gs1.predict(res)\na = accuracy_score(dep_test, gs1.predict(indep_test))\nb = accuracy_score(dep_train, gs1.predict(indep_train))\n#cv = cross_val_score(gs1, indep_train, dep_train, cv=5)\nprint(a, b)","55382faf":"attr2 = {k: v for k, v in zip(indep.columns, gs1.feature_importances_) if v>0}\nattr2 = sorted(attr2.items(), key=lambda x: x[1], reverse = False)\nx1,y1 = zip(*attr2)\ni1=range(len(x1))\nplt.figure(num=None, figsize=(9, 8), dpi=300, facecolor='w', edgecolor='k')\nplt.barh(i1, y1)\nplt.title(\"CatBoost\")\nplt.yticks(i1, x1)\nplt.show();","5a69b89c":"model = []\ncvscores = []\nfor i in range(0, 90):\n    indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=i)\n    gs1 = cb.CatBoostClassifier(depth = 9, reg_lambda=0.1,\n                     learning_rate = 0.09, \n                     iterations = 500)\n    gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)],  verbose=False, early_stopping_rounds=50);\n    model.append(gs1)\n    cvscores.append(accuracy_score(dep_test, gs1.predict(indep_test)))\n    \ndata = pd.DataFrame()\nte = pd.DataFrame()\n\nfor i in range(0, 90):\n    data[i] = model[i].predict(res)\n    te[i] = model[i].predict(indep_test)\n\ng = np.round(data.mean(axis=1))\nt = np.round(te.mean(axis=1))\n\ncb_mean = np.mean(cvscores)\ncb_stdev = np.std(cvscores)\nprint(cb_mean, cb_stdev)\n","99223842":"#result = pd.DataFrame(res_index.astype(np.int), columns=[\"PassengerId\"])\n#result[\"Survived\"] = g.astype(np.int)\n#result.to_csv(r\"c:\\work\\dataset\\titanic\\catboost.csv\", \",\", index=None)\n#result.to_csv(\"catboost.csv\", \",\", index=None)","4dd20cee":"gs1 = xgb.XGBClassifier(max_depth = 9,\n                         learning_rate = 0.01, \n                         n_estimators = 500, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9,\n                         n_jobs = 5)\ngs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)],  verbose=False, early_stopping_rounds=50);\n\ng = gs1.predict(res)\na = accuracy_score(dep_test, gs1.predict(indep_test))\nb = accuracy_score(dep_train, gs1.predict(indep_train))\nprint(a, b)","400f4e47":"attr2 = {k: v for k, v in zip(indep.columns, gs1.feature_importances_) if v>0}\nattr2 = sorted(attr2.items(), key=lambda x: x[1], reverse = False)\nx1,y1 = zip(*attr2)\ni1=range(len(x1))\nplt.figure(num=None, figsize=(9, 7), dpi=300, facecolor='w', edgecolor='k')\nplt.barh(i1, y1)\nplt.title(\"XGBoost\")\nplt.yticks(i1, x1)\nplt.show();","2c16dc0d":"model = []\ncvscores = []\nfor i in range(0, 90):\n    indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=i)\n    gs1 = xgb.XGBClassifier(max_depth = 7, reg_lambda = 0.02,\n                         learning_rate = 0.01, \n                         n_estimators = 5000, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9,\n                         n_jobs = 5)\n    gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)],  verbose=False, early_stopping_rounds=50);\n    model.append(gs1)\n    cvscores.append(accuracy_score(dep_test, gs1.predict(indep_test)))\n    \ndata = pd.DataFrame()\nte = pd.DataFrame()\n\nfor i in range(0, 90):\n    data[i] = model[i].predict(res)\n    te[i] = model[i].predict(indep_test)\n\ng = np.round(data.mean(axis=1))\nt = np.round(te.mean(axis=1))\n\nxgb_mean = np.mean(cvscores)\nxgb_stdev = np.std(cvscores)\nprint(xgb_mean, xgb_stdev)","9fd0ee5b":"#result = pd.DataFrame(res_index.astype(np.int), columns=[\"PassengerId\"])\n#result[\"Survived\"] = g.astype(np.int)\n#result.to_csv(r\"c:\\work\\dataset\\titanic\\xgb.csv\", \",\", index=None)\n#result.to_csv(\"lgbm.csv\", \",\", index=None)","85df9218":"d = {'Model':[\"Keras MLP\", \"LightGBM\", \"CatBoost\", \"XGBoost\"], \n     'Mean accuracy': [mlp_mean, lgb_mean, cb_mean, xgb_mean], \n     'Std. Dev.': [mlp_stdev, lgb_stdev, cb_stdev, xgb_stdev],\n    'Leaderboard': [0.77033, 0.82296, 0.78947, 0.77511]}\npd.DataFrame(data=d, columns=[\"Model\", \"Mean accuracy\", \"Std. Dev.\", \"Leaderboard\"]).sort_values(\"Mean accuracy\", ascending=False).head(10)","384c4f57":"Fare processing. We also can split it on ranges and get survivability per fare range. ","94969dca":"### Model 1. Keras MLP\nMake simple sequental MLP NN. (Parameters selected manually)","f36fd278":"Ticket has repeated values, so use grouping. We can get number of passengers per ticket and number\/percent of females per single ticket. Also ticket can be pure digital or with some text info.","41178770":"Now calculate average survivability for each left categorical fields","8e882379":"### Feature selection and preprocessing","01980041":"### Model 3. CatBoost\nSame approach","4ea51b03":"Lets see how accuracy changes during epochs","6d79bce8":"This got <b>0.82296<\/b> on LB","763c39c6":"### Model 2. LightGBM\nAll parameters selected by brute force, sequentally by 1-2 parameters. No HyperOpt nor GridSearchCV geven such accuracy for me.","a379e123":"### Feature engineering\nTicket based features:","cc3d9f74":"### Model 4. XGBoost","c2181ef0":"Now blend results of models with different seeds (i experimentally got optimal number of models - 90)","2e671a4a":"Unfortunatelly, accuracy on LB too low (0.75119 - 0.79425 with different seeds)","ecf43733":"Make features related to family size","0f00e5a0":"Process cabin number, get deck (vertical coordinate in ship), cabin digit number as distance from ship's nose, and check is number odd\/even as side of ship axis. Also we can get average survivability per deck.","90f5a363":"Several features requires imputation and normalization","8c8614cb":"## Titanic:  MLP (Keras) vs LightGBM vs CatBoost vs XGBoost with same features","cb80ae88":"Get same NN on 5 folds and average result","e69f6161":"Get title from Name, and average survivability per title","71ce7418":"TicketIsNumber seems to be not valued\n\nNow process Name field. Some records have two passengers in single field name. ","c4df0f6d":"Now process age field. We can split it on ranges and get average survivality per one. Also calculate average age of family."}}