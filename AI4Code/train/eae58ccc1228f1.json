{"cell_type":{"5d72b178":"code","a42bb28a":"code","6bf32ca9":"code","f22405dc":"code","04d27909":"code","9648abdf":"code","0ff1f0d5":"code","ba939ec2":"code","1700cca4":"code","1417591b":"code","f8cde9b0":"code","9e2cecec":"code","80d2175d":"code","cfb9f088":"code","0f141491":"code","df9073ab":"code","8894832e":"code","60ce63cb":"code","610666ed":"code","37989cd3":"code","eb95c664":"code","48043c8a":"code","cdc04c17":"code","dd54176b":"code","f5ee9a22":"code","6192120c":"code","b19169cd":"code","8f37074a":"markdown","0004d778":"markdown","54309b9d":"markdown","3b2e79cb":"markdown","e23a132b":"markdown","54f71fcb":"markdown","8ac88b3b":"markdown","87de6408":"markdown","c1cc68fb":"markdown","db5d4b06":"markdown","7e9ddebb":"markdown","fa58a003":"markdown","9378b012":"markdown","4ae3cac5":"markdown","eda00c87":"markdown","2d403cb1":"markdown","6408c65f":"markdown","dd948a80":"markdown","22a707b6":"markdown","54c3c8ed":"markdown"},"source":{"5d72b178":"import numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\nimport math\nfrom torch.optim import lr_scheduler\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport torch\nimport itertools\nfrom torchvision import models\nimport torch.optim as optim\nfrom matplotlib.ticker import MaxNLocator\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom torch.nn import MaxPool2d\nimport chainer.links as L\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.ion()","a42bb28a":"#This data is wrongly matched. Please execute this code to have the correct mapping of X and y values\n\ndata = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\ntarget = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\nY = np.zeros(data.shape[0])\nY[:204] = 9\nY[204:409] = 0\nY[409:615] = 7\nY[615:822] = 6\nY[822:1028] = 1\nY[1028:1236] = 8\nY[1236:1443] = 4\nY[1443:1649] = 3\nY[1649:1855] = 2\nY[1855:] = 5\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = .02, random_state = 2) ## splitting into train and test set","6bf32ca9":"#Shapes of X_train and y_train\nX_train.shape, y_train.shape","f22405dc":"class DatasetProcessing(Dataset):\n    \n    #initialise the class variables - transform, data, target\n    def __init__(self, data, target, transform=None): \n        self.transform = transform\n        self.data = data.reshape((-1,64,64)).astype(np.float32)[:,:,:,None]\n        # converting target to torch.LongTensor dtype\n        self.target = torch.from_numpy(target).long() \n    \n    #retrieve the X and y index value and return it\n    def __getitem__(self, index): \n        return self.transform(self.data[index]), self.target[index]\n    \n    #returns the length of the data\n    def __len__(self): \n        return len(list(self.data))","04d27909":"# preprocessing images and performing operations sequentially\n# Firstly, data is converted to PILImage, Secondly, converted to Tensor\n# Thirdly, data is Normalized\ntransform = transforms.Compose(\n    [transforms.ToPILImage(), transforms.ToTensor()])\n\n\ndset_train = DatasetProcessing(X_train, y_train, transform)\n\n\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size=4,\n                                          shuffle=True, num_workers=4)","9648abdf":"dset_test = DatasetProcessing(X_test, y_test, transform)\ntest_loader = torch.utils.data.DataLoader(dset_test, batch_size=4,\n                                          shuffle=True, num_workers=4)","0ff1f0d5":"plt.figure(figsize = (16, 4))\nfor num, x in enumerate(X_train[0:6]):\n    plt.subplot(1,6,num+1)\n    plt.axis('off')\n    plt.imshow(x)\n    plt.title(y_train[num])","ba939ec2":"class Net(nn.Module):    \n    \n    # This constructor will initialize the model architecture\n    def __init__(self):\n        super(Net, self).__init__()\n          \n        self.cnn_layers = nn.Sequential(\n            # Defining a 2D convolution layer\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            # Putting a 2D Batchnorm after CNN layer\n            nn.BatchNorm2d(32),\n            # Adding Relu Activation\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n          \n        self.linear_layers = nn.Sequential(\n            # Adding Dropout\n            nn.Dropout(p = 0.5),\n            nn.Linear(32 * 32 * 32, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 10),\n        )\n        \n    # Defining the forward pass    \n    def forward(self, x):\n        \n        # Forward Pass through the CNN Layers \n        x = self.cnn_layers(x)\n        x = x.view(x.size(0), -1)\n        # Forwrd pass through Fully Connected Layers\n        x = self.linear_layers(x)\n        return F.log_softmax(x) ","1700cca4":"model = Net()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","1417591b":"########################################\n#       Training the model             #\n########################################\ndef train(epoch):\n    model.train()\n    exp_lr_scheduler.step()\n    tr_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n            \n        # Clearing the Gradients of the model parameters\n        optimizer.zero_grad()\n        output = model(data)\n        pred = torch.max(output.data, 1)[1]\n        correct += (pred == target).sum()\n        total += len(data)\n        \n        # Computing the loss\n        loss = criterion(output, target)\n        \n        # Computing the updated weights of all the model parameters\n        loss.backward()\n        optimizer.step()\n        tr_loss = loss.item()\n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f} \\t Accuracy: {} %'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) \/ len(train_loader), loss.item(),100 * correct \/ total))\n            torch.save(model.state_dict(), '.\/model.pth')\n            torch.save(model.state_dict(), '.\/optimizer.pth')\n    train_loss.append(tr_loss \/ len(train_loader))\n    train_accuracy.append(100 * correct \/ total)","f8cde9b0":"########################################\n#       Evaluating the model           #\n########################################\n\ndef evaluate(data_loader):\n    model.eval()\n    loss = 0\n    correct = 0\n    total = 0\n    for data, target in data_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        output = model(data)\n        loss += F.cross_entropy(output, target, size_average=False).item()\n        pred = torch.max(output.data, 1)[1]\n        total += len(data)\n        correct += (pred == target).sum()\n    loss \/= len(data_loader.dataset)\n    valid_loss.append(loss)    \n    valid_accuracy.append(100 * correct \/ total)\n    print('\\nAverage Validation loss: {:.5f}\\tAccuracy: {} %'.format(loss, 100 * correct \/ total))","9e2cecec":"n_epochs = 50\ntrain_loss = []\ntrain_accuracy = []\nvalid_loss = []\nvalid_accuracy = []\nfor epoch in range(n_epochs):\n    train(epoch)\n    evaluate(test_loader)","80d2175d":"########################################\n#       Plotting the Graph             #\n########################################\n\ndef plot_graph(epochs):\n    fig = plt.figure(figsize=(20,4))\n    ax = fig.add_subplot(1, 2, 1)\n    plt.title(\"Train - Validation Loss\")\n    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train')\n    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='validation')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('loss', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')\n    \n    ax = fig.add_subplot(1, 2, 2)\n    plt.title(\"Train - Validation Accuracy\")\n    plt.plot(list(np.arange(epochs) + 1) , train_accuracy, label='train')\n    plt.plot(list(np.arange(epochs) + 1), valid_accuracy, label='validation')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('accuracy', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')","cfb9f088":"plot_graph(n_epochs)","0f141491":"_, (validation_data, target) = next(enumerate(test_loader))\nwith torch.no_grad():\n    output = model(validation_data.cuda())\nfig = plt.figure()\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.tight_layout()\n    plt.imshow(validation_data[i][0], interpolation='none')\n    pred = output.data.max(1, keepdim=True)[1][i].item()\n    plt.title(\"Prediction: {}\".format(pred))","df9073ab":"with torch.no_grad():\n    output = model(validation_data.cuda())\n\nsoftmax = torch.exp(output).cpu()\nprob = list(softmax.numpy())\n\nfig = plt.figure(figsize = (16, 8))\nfor i in range(0, 4):\n    fig.tight_layout()\n    plt.style.use('classic')\n    plt.subplot(4,2, 2 * i + 1)\n    plt.imshow(validation_data[i][0], interpolation='none')\n    plt.xticks([])\n    plt.yticks([])\n    pred = output.data.max(1, keepdim=True)[1][i].item()\n    plt.title(\"Prediction: {}\".format(pred))\n    plt.subplot(4,2, 2 * i + 2)\n    plt.barh([0], [max(prob[i])])\n    plt.yticks([])\n    plt.title(\"Predicted Probability: {0:.2f}\".format(max(prob[i])))","8894832e":"train_transform= transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.RandomHorizontalFlip(), # Horizontal Flip\n            transforms.RandomCrop(64, padding=2), # Centre Crop\n            transforms.ToTensor(),  #Convereting the input to tensor\n            ])\ndset_train = DatasetProcessing(X_train, y_train, train_transform)\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size=4,\n                                          shuffle=True, num_workers=4)","60ce63cb":"n_epochs = 50\n\ntrain_loss = []\ntrain_accuracy = []\nvalid_loss = []\nvalid_accuracy = []\n\nfor epoch in range(n_epochs):\n    train(epoch)\n    evaluate(test_loader)","610666ed":"# Plotting train and validation loss\nplot_graph(n_epochs)","37989cd3":"_, (validation_data, target) = next(enumerate(test_loader))\nwith torch.no_grad():\n    output = model(validation_data.cuda())\nfig = plt.figure()\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.tight_layout()\n    plt.imshow(validation_data[i][0], interpolation='none')\n    pred = output.data.max(1, keepdim=True)[1][i].item()\n    plt.title(\"Prediction: {}\".format(pred))","eb95c664":"with torch.no_grad():\n    output = model(validation_data.cuda())\n\nsoftmax = torch.exp(output).cpu()\nprob = list(softmax.numpy())\n\nfig = plt.figure(figsize = (16, 8))\nfor i in range(0, 4):\n    fig.tight_layout()\n    plt.style.use('classic')\n    plt.subplot(4,2, 2 * i + 1)\n    plt.imshow(validation_data[i][0], interpolation='none')\n    plt.xticks([])\n    plt.yticks([])\n    pred = output.data.max(1, keepdim=True)[1][i].item()\n    plt.title(\"Prediction: {}\".format(pred))\n    plt.subplot(4,2, 2 * i + 2)\n    plt.barh([0], [max(prob[i])])\n    plt.yticks([])\n    plt.title(\"Predicted Probability: {0:.2f}\".format(max(prob[i])))","48043c8a":"#######################################################################\n#       Defining various model architectures for ensembling           #\n#######################################################################\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n    \n        #1st net\n        self.conv1_1 = nn.Conv2d(1, 4, kernel_size=(4, 4), padding = (3, 3), stride=(2, 2)) #34 * 34\n        self.conv1_2 = nn.Conv2d(4, 8, kernel_size=(4, 4), padding = (2, 2), stride=(2, 2)) #18 * 18 #mp\n        self.fc1_1 = nn.Linear(8 * 9 * 9, 32) #dropout = 0.2\n        self.fc1_1_drop = nn.Dropout(p=0.2)\n        self.fc1_2 = nn.Linear(32, 10)\n\n        #2nd net\n        self.conv2_1 = nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) #64 * 64\n        self.conv2_2 = nn.Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) #64 * 64 #mp\n        self.conv2_2_drop = nn.Dropout2d()\n        self.conv2_3 = nn.Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) #64 * 64\n        self.conv2_4 = nn.Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) #64 * 64 #mp\n        self.conv2_4_drop = nn.Dropout2d()\n        self.fc2_1 = nn.Linear(24 * 16 * 16, 120) #dropout = 0.5\n        self.fc2_1_drop = nn.Dropout(p=0.5)\n        self.fc2_2 = nn.Linear(120, 10)\n\n        #3rd net\n        self.conv3_1 = nn.Conv2d(1, 4, kernel_size=(5, 5), stride=(3, 3), padding=(2, 2)) #22 * 22\n        self.conv3_2 = nn.Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) #22 * 22 #mp\n        self.conv3_3 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) #11 * 11\n        self.fc3_1 = nn.Linear(16 * 11 * 11, 64) #dropout = 0.4\n        self.fc3_1_drop = nn.Dropout(p=0.4)\n        self.fc3_2 = nn.Linear(64, 10)\n\n    def forward(self, x, y, z):\n        x = F.relu(self.conv1_1(x))\n        x = F.relu(self.pool(self.conv1_2(x)))\n        x = x.view(-1, 648) #can also do x.view(-1, 1)\n        x = F.relu(self.fc1_1(x))\n        x = F.dropout(x, training = self.training)\n        x = F.relu(self.fc1_2(x))\n\n        y = F.relu(self.conv2_1(y))\n        y = F.relu(self.pool(self.conv2_2_drop(self.conv2_2(y))))\n        y = F.relu(self.conv2_3(y))\n        y = F.relu(self.pool(self.conv2_4_drop(self.conv2_4(y))))\n        y = y.view(-1, 256 * 24)\n        y = F.relu(self.fc2_1_drop(self.fc2_1(y)))\n        y = F.relu(self.fc2_2(y))\n\n        z = F.relu(self.conv3_1(z))\n        z = F.relu(self.pool(self.conv3_2(z)))\n        z = F.relu(self.conv3_3(z))\n        z = z.view(-1, 16 * 121)\n        z = F.relu(self.fc3_1_drop(self.fc3_1(z)))\n        z = F.relu(self.fc3_2(z))\n\n        x = torch.cat((x, y, z))\n\n        return F.sigmoid(x)","cdc04c17":"########################################\n#       Plotting the Graph             #\n########################################\n\ndef plot_graphs(train_loss, valid_loss, epochs):\n    plt.style.use('ggplot')\n    fig = plt.figure(figsize=(20,4))\n    ax = fig.add_subplot(1, 2, 1)\n    plt.title(\"Train Loss\")\n    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('train_loss', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')\n    ax = fig.add_subplot(1, 2, 2)\n    plt.title(\"Validation Loss\")\n    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='test')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('vaidation _loss', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')","dd54176b":"##################################################\n#       Training the ensembled model             #\n##################################################\n\ndef train_ensemble(epoch):\n    model.train()\n    exp_lr_scheduler.step()\n    tr_loss = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        #print(data.size())\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        optimizer.zero_grad()\n        target = torch.cat((target, target, target))\n        output  = model(data, data, data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item()\n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) \/ len(train_loader), loss.item()))\n    train_loss.append(tr_loss \/ len(train_loader))","f5ee9a22":"####################################################\n#       Evaluating the ensembled model             #\n####################################################\n\n\ndef evaluate_ensemble(data_loader):\n    model.eval()\n    loss = 0\n    \n    for data, target in data_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        target = torch.cat((target, target, target))\n        output  = model(data, data, data)\n        # Using the functional API\n        loss += F.cross_entropy(output, target, size_average=False).item()\n        pred = output.data.max(1, keepdim=True)[1]\n        \n    loss \/= len(data_loader.dataset)\n    valid_loss.append(loss)    \n    print('\\nAverage Validation loss: {:.5f}\\n'.format(loss))","6192120c":"# Initializing the ensembled model\nmodel = Net()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ncriterion = nn.CrossEntropyLoss()\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()\n\nn_epochs = 5\n\ntrain_loss = []\nvalid_loss = []\n\n# Training and Evaluating for n_epochs\nfor epoch in range(n_epochs):\n    train_ensemble(epoch)\n    evaluate_ensemble(test_loader)","b19169cd":"plot_graphs(train_loss, valid_loss, n_epochs)","8f37074a":"### Let us now have a look at our model predictions.","0004d778":"# Ensembling Models","54309b9d":"Some of the important libraries\n### [nn](https:\/\/pytorch.org\/docs\/master\/nn.html)\nThis library contains  function for building conv nets and importing a loss function.\nSome loss functions are:\n* **binary_cross_entropy**: Function that measures the Binary Cross Entropy between the target and the output.\n* **nll_loss**: measures the The negative log likelihood loss.\n* **cross_entropy**: This criterion combines log_softmax and nll_loss in a single function.\n\n### [optim](https:\/\/pytorch.org\/docs\/master\/optim.html)\ntorch.optim is a package implementing various optimization algorithms. To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.Some of the optimizers defined by this library are:\n* **Adam**: It has been proposed in [Adam: A Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/1412.6980)\n* **LBFGS**: Implements [L-BFGS algorithm](https:\/\/en.wikipedia.org\/wiki\/Limited-memory_BFGS).\n* **RMSprop**: Proposed by G. Hinton in his [course](http:\/\/www.cs.toronto.edu\/~tijmen\/csc321\/slides\/lecture_slides_lec6.pdf)\n* **SGD**: Nesterov momentum is based on the formula from [On the importance of initialization and momentum in deep learning](http:\/\/www.cs.toronto.edu\/~hinton\/absps\/momentum.pdf)\n<br\/>\n\n\nLet\u2019s use a **Cross-Entropy loss** and **SGD with momentum**.","3b2e79cb":"## Data Augmentation","e23a132b":"Data loading in PyTorch can be separated in 2 parts:\n<br\/>\n* Data must be wrapped on a Dataset parent class where the methods __getitem__ and __len__ must be overrided. Not that,  the data is not loaded on memory by now.\n* The Dataloader reads the data and puts it into memory.\n\nHere we are working with numpy arrays which can be directly converted to torch variable for mathematical operations.Many datasets contain images which can be imported using PIL library using\n<br\/>\n> *PIL.IMAGE.fromarray(img_name)*\n\n<br\/>\nWe use PIL instead of OpenCV because its Torch default image loader and is compatible with ToTensor() method. The PyTorch website has all the tutorials of this section using their inbuild Dataset class and it can be a toilsome task to load data in case of Custom Dataset.While loading the data, these point should be closely worked upon:\n* Setting a batch Size\n* Shuffling the data\n* Parallelizing the tasks using multiprocecssing workers.\n<br\/>\n\nThe **DataLoader** function  provides all of these features and you can specify them before heading to next task. The label or the target variable can be one an array or one-hot encoded depending on the loss function. For having a look at the different loss functions, please head to [Pytorch loss functions](https:\/\/pytorch.org\/docs\/master\/nn.html#loss-functions)","54f71fcb":"# Model Predicted Probabilities","8ac88b3b":"# Training\n\n![](https:\/\/www.researchgate.net\/publication\/318174473\/figure\/fig2\/AS:614173574701099@1523441799965\/The-topology-of-the-event-driven-Convnet-used-for-this-work-Where-n-is-the-size-of-the.png)\n\n\n\n\n\n\n\n\n## Define a CNN","87de6408":"# Custom Data Loading and Processing","c1cc68fb":"![](https:\/\/saatvikshah1994.github.io\/images\/ensembling_1\/EnsembleDepiction.png)","db5d4b06":"<!-- background: #5A0000  -->\n<!-- color: #fff -->\n\n\n# How to use optimizers?\nTo construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.<br\/>\n*If you want to move the model and variables to cuda, use **model_object.cuda()** and **variable_name.cuda()***\n\n**Follow these steps to build your first model in pytorch**\n\n\n1. Clearing the Gradients of the model parameters \n>       optimizer.zero_grad()\n\n    ** Clear out the gradients accumulated for the parameters of the network before calling loss.backward() and optimizer.step()**\n\n2. Compute the loss\n>       criterion( predicted_target, target)\n\n    **Compute the loss between the predicted value and the target value within the loss function previously defined**\n\n3. Backpropogation\n>       loss.backward()\n\n    **Compute the updated weights of all the model parameters**\n\n4. Taking an optimization step\n>       optimizer.step()\n\n    ** Update the parameters of the network **\n","7e9ddebb":"### You can run this model for more  epochs","fa58a003":"**Hello Everyone**\n<br\/>\n\nThis is a comprehensive tutorial on Developing ensemble models in PyTorch. I will also give a detailed explanation of how to build a model using PyTorch. \n**This notebook has been divided into following sections:**\n\n* Introduction\n    1.   Limitations of Tensorflow and Keras\n    2.  Advantages of PyTorch\n\n\n* Custom Data Loading and Preprocessing\n    1. Illustrating the APIs\n    2. Data Loading and Preprocessing\n\n\n* Model Training and Evaluation\n    1. Defining the model architecture\n    2. Defining and Loss function and Optimizer\n    3. Training the model\n    4. Validating the model\n\n\n* Data Augmentation\n    1. Augmenting more data\n\n\n* Advanced Guide to Ensembling Models\n    1. Defining various models and Training them\n    2. Evaluating the models","9378b012":"Let's have a look at the Training data","4ae3cac5":"**Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model.  Ensemble methods usually produces more accurate solutions than a single model would. This has been the case in a number of machine learning competitions, where the winning solutions used ensemble methods. **\n![](http:\/\/images.slideplayer.com\/37\/10747814\/slides\/slide_2.jpg)\nYou can find more detailed and advanced explanation [here](https:\/\/www.springer.com\/in\/book\/9781441993250)\n<br\/>\nFinally, it's time to get hands dirty on ensemble. I have also mentioned the layer dims after each conv filter is applied.","eda00c87":"# Model Predictions","2d403cb1":"So, we learned how to create custom models using PyTorch & also how to impove the accuracy of models. More to come in next versions. **Please Upvote if you liked it.**","6408c65f":"## Model Evaluation\nNow, it's time to evaluate out model. Here Evaluating means testing the model on the validation set<br\/>\n*Note that I have used Functional API( F.cross_entropy( args )) so as to give an idea how it works.*","dd948a80":"Let's understand the work of the following keywords\n\n### torchvision\nIt is used to load and prepare dataset. Using it you can create *transformations* on the input data.\n<br\/>\n#### transforms\nIt is used for preprocessing images and performing operations sequentially.\n<br\/>\n#### num_workers\nIt is used for multiprocessing.Normally, **num_workers = 4 * (number of gpus)** works well.\n","22a707b6":"### Let us now have a look at the Predicted Probabilities of the predicted words","54c3c8ed":"**One powerful technique to increase the  performance of any deep learning model is through adding more data. So Let's use data augmentation and see if we get some performance improvement.**"}}