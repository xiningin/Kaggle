{"cell_type":{"607f2cb7":"code","5b79ac2f":"code","0c40d159":"code","748d68ad":"code","b7110376":"code","32166ff5":"code","98679188":"code","bfda78ab":"code","bf4c8c87":"code","7e462efd":"code","cfd96f77":"code","4a156960":"code","10872c04":"code","09e03cf5":"code","98f9d693":"code","66802d08":"code","736068b7":"code","a0ddd6cc":"code","0309e521":"code","316b810d":"code","5351ac7d":"code","0e476ae9":"code","8654d5f6":"code","f125a4e9":"code","ceb80ec9":"code","5e83c9c8":"code","f5bba835":"code","d593655e":"code","fdb9b41b":"code","b0fa7766":"code","6d94af20":"code","10abff11":"markdown","b0eb00f6":"markdown","cf3debc8":"markdown","2b4c0756":"markdown","70583952":"markdown","b4abaad4":"markdown","929e5bda":"markdown","1134f56f":"markdown","c0ebacd1":"markdown","cd3ecec1":"markdown","3c1b7eea":"markdown","b698c7dc":"markdown","10d03fef":"markdown","f60d64f8":"markdown","2bb9e468":"markdown","5f267d31":"markdown","ae0ce3c5":"markdown","4df0db59":"markdown","ffd1ee75":"markdown","b4c33ed4":"markdown","2c2920ab":"markdown","3fd6e230":"markdown","bc5009e6":"markdown","b8a2245f":"markdown","e2d8bf1e":"markdown","4c145904":"markdown","8c41a802":"markdown","aa7582fc":"markdown"},"source":{"607f2cb7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5b79ac2f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","0c40d159":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","748d68ad":"pd.set_option('display.max_columns', None)               # To get all the columns\ntrain.head()","b7110376":"def describe_features_with_null_values(df):\n    null_count = df.isnull().sum()\n    null_values = null_count[null_count > 0]\n\n    null_values = null_values.sort_values(ascending = False)\n    \n    null_perc = null_values*100\/len(df)\n\n    null = pd.DataFrame(null_values, columns = ['Null Count'])\n    null['Percentage'] = round(null_perc, 2)\n\n    return null","32166ff5":"df_null = describe_features_with_null_values(train)\ndf_null","98679188":"higher_null_values_list = list(df_null[df_null['Percentage'] > 15].index)\nprint(\"Features having more than 15% Null values are :\", higher_null_values_list)","bfda78ab":"train = train.drop(higher_null_values_list, axis = 1)","bf4c8c87":"corrmat = train.corr()        # Finds correlation between all the columns\nf, ax = plt.subplots(figsize=(12, 9))             # Increases the figure size to (12, 9)\nsns.heatmap(corrmat, vmax = 0.8, square=True);","7e462efd":"train = train.drop(['1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageYrBlt'], 1)","cfd96f77":"df_null = describe_features_with_null_values(train)\ndf_null","4a156960":"def fill_null_values(df):\n    \n    null_df = describe_features_with_null_values(df)\n    \n    # Below 2 lines will give us features with object, float\/int datatype respectively.\n    obj_features = df[null_df.index].dtypes[df[null_df.index].dtypes == object].index\n    float_features = df[null_df.index].dtypes[df[null_df.index].dtypes == float].index\n        \n    for feature in obj_features:\n        df[feature] = df[feature].fillna(df[feature].mode().values[0])\n    \n    for feature in float_features:\n        df[feature] = df[feature].fillna(df[feature].mean())\n        \n    return df","10872c04":"train = fill_null_values(train)","09e03cf5":"print(\"Features which most affects to our SalePrice : \\n\")\nrelated_cols = corrmat.nlargest(10, 'SalePrice')\nprint(related_cols['SalePrice'])","98f9d693":"fig, axes = plt.subplots(2,2, figsize = (8,7))\n\naxes[0][0].scatter(train['OverallQual'], train['SalePrice'])\n\naxes[0][1].scatter(train['GrLivArea'], train['SalePrice'])\n\naxes[1][0].scatter(train['GarageCars'], train['SalePrice'])\n\naxes[1][1].scatter(train['TotalBsmtSF'], train['SalePrice'])\n\nfig.tight_layout()","66802d08":"ind1 = train['TotalBsmtSF'][train['TotalBsmtSF'] > 5000].index.values\nind2 = train['GrLivArea'][train['GrLivArea'] > 4500].index.values\n\nprint('Index of datapoint in TotalBsmtSF different from croud :', ind1)\nprint('Index of datapoint in GrLivArea different from croud :', ind2)","736068b7":"train = train.drop(ind2)","a0ddd6cc":"sns.distplot(train['SalePrice']);","0309e521":"# Applying Normality\ntrain['SalePrice'] = np.log(train['SalePrice'])","316b810d":"sns.distplot(train['SalePrice']);","5351ac7d":"obj_mask = train.dtypes == object\nobj_features = list(obj_mask[obj_mask].index)\n\nle = LabelEncoder()\ntrain[obj_features] = train[obj_features].apply(le.fit_transform)","0e476ae9":"train.head()","8654d5f6":"Y_train = train['SalePrice'].values\nX_train = train.drop(['SalePrice'], 1).values","f125a4e9":"print(\"Shape of X_train :\", X_train.shape)\nprint(\"Shape of Y_train :\", Y_train.shape)","ceb80ec9":"lr = LinearRegression()\nlr.fit(X_train, Y_train)\n\nypred = np.exp(lr.predict(X_train))\n\n# Note : we are applying np.exp(). Thats because our Y_train is Normalized by applying Logarithm\n\nerr = round(mean_absolute_error(np.exp(Y_train), ypred)\/100000, 5)\n\nprint(\"Error Score for Training Set :\", err)","5e83c9c8":"test = test.drop(higher_null_values_list, 1)\ntest = test.drop(['1stFlrSF', 'GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd'], 1)\ntest = fill_null_values(test)","f5bba835":"obj_mask_test = test.dtypes == object\nobj_features_test = list(obj_mask_test[obj_mask_test].index)\n\nle = LabelEncoder()\ntest[obj_features_test] = test[obj_features_test].apply(le.fit_transform)","d593655e":"X_test = test.values","fdb9b41b":"test_pred = np.exp(lr.predict(X_test))","b0fa7766":"test_ind = np.arange(1461, 1461 + len(test))\ntest_series = pd.Series(test_pred, index = test_ind)","6d94af20":"#test_series.to_csv('predictions.csv')","10abff11":"Hooofff... We reached long way. \n\n- We remove the unknown and unwanted features from this universe.\n- We filled the missing values\n- We removed the outliers\n- And we also normalized our SalePrice\n\nNow only one thing is left before we can enter into the battle. And that is an Armor or a shield. \n\nWe will use Captain America Shield. Means we will convert our object datatype or alphabets to float datatype, so that we can train them.\n\nSo lets get onto that.","b0eb00f6":"# Removal","cf3debc8":"# Converting categorical data to numeric","2b4c0756":"### *Converting Pandas Dataframe to Numpy Array*","70583952":"# Outliers :\nOutlier is a rare chace of occurrence within a given data set. It is an observation point that is distint from other observations.\n\nIt is possible that many features have outliers. But what affects our learning curve is the outlier present in the feature which significantly affects the SalePrice.\n\nWe will first find out which features significantly affects our SalePrice. And we will call these our favourite weapons. \n\nNow if our favourite weapons are not perfect, we can lose the battle. So we will find out anything that makes it imperfect and then remove that.","b4abaad4":"# Filling the Missing Data\nWe will create a function which will automatically fill the missing values with their mean\/mode.\n\nLets look at again our missing data.","929e5bda":"## Boom ! We killed the villian and now we have our Infinity Stone Secured with us.\n\nI got 0.12365 error score by submitting the above 'prediction.csv' file. You can still do better score by using other models to train.\n\nThis was just the simplest way I found to get a good score. \n\n### *If you find this notebook helpfull, then do give this an UPVOTE. Or share it with others while sharing your own notebook.*\n\nPeace !","1134f56f":"### Lets see how our universe looks like :","c0ebacd1":"### Removing features with higher null values :\nHaving null values is having unknown item our universe. We don't want that. If the unknown items is more than 15%, we will completely remove the entire feature from our universe.\n\nLets see which features have null values :","cd3ecec1":"- look at the whitist blocks in the above figure. They are more related feature pairs. ","3c1b7eea":"# Normality\n- Now its time to check whether our 'SalePrice' shows Normal distribution or not.\n- If they don't show normal distribution, we will apply Normality which will improve our final score.","b698c7dc":"Now our weapons are perfect. So lets jump to our next thing.","10d03fef":"> ### Everyone is a beginner when they start. \n\nIsn't it frustrating when you see your score to be at the bottom 30%, even when you have spent months studying about the subject and days working on the project ?\n\nWell I know the feeling well. It happens to all of us when we are beginning. But what's most interesting is the everything in this universe is a POWER LAW.\n\nYou ask what is POWER LAW?\n\nThats what Peter Theil ( Co-founder of PayPal and Early invester of Facebook and SpaceX ) say about our old exponential curve.\nIt takes alot of time in the beginning to reach something and then everything rises exponentially fast after a certain point. We will try to reach after than certain point, in this notebook.\n\nThere are only few little things when you learn (which won't even take alot of time) will help you easily achieve top 25%. \n\nTo increase the accuracy (or reduce the error score) data prepreprocessing and selecting model and their parameter, plays a big role. Here, we will only focus on data preprocessing part, which will be good enough to give us a big enough jump in score. Later we will train the model with the simplest LinearRegression Model.\n\nSo, are you ready?","f60d64f8":"You see.. how easy it is to explore our universe with Python.","2bb9e468":"It is clear that our SalePrice is not Normal.\n\nWe can Normalize this by just applying the logarthm to the values. Its that simple trick.","5f267d31":"We can see 6 features having more than 15% of Null values. We will remove those features.\n\nFor the rest, we will fill them with their mean\/mode.","ae0ce3c5":"### *Looking above, I derived the following most related features :*\n- TotalBsmtSF and 1stFlrSF\n- GarageCars and GarageArea\n- TotRmsAbGrd and GrLivArea\n- GarageYrBlt and YearBuilt\n\n### *Now which to keep and which to remove ?*\n- We will keep the feature which are more related to SalePrice.\n- Also we can determine this by our intuition. For eg, TotBsmtSF and 1stFlrSF are equally related. TotBsmtSF represents 'total basement square feet' while 1stFlrSF represents '1st Floor square feet'. As far as my intuition is concerned, I think TotBsmtSF is more likely feature we will consider for buying a house.\n\n### *So clearly we will remove :*\n- 1stFlrSF\n- GarageArea\n- TotRmsAbvGrd\n- GarageYrBlt","4df0db59":"Note : GarageArea and 1stFlrSF is already removed.\n\nFrom above we can say that our favourite weapons are : \n- OverallQual\n- GrLivArea\n- GarageCars\n- TotalBsmtSF\n\nWe will examine our weapons carefully.","ffd1ee75":"# Test Set\n##### Now we will do the same removal and filling the NaN values for Test set.","b4c33ed4":"# Model :\n- For the simplicity. We will just use the simple LinearRegression model from sklearn.\n- You can also try out more models and find out which gives you better score.","2c2920ab":"Bammnn ! Now we also have Captain America's Shield. \n\nWe are now ready to enter the battle.","3fd6e230":"# Test set Predictions","bc5009e6":"## Removing similar Features :\n- There can be many features, which doen't provide any extra information. So we will remove those.\n- The simplest way to look at that is using correlation and its heatmap.\n\n### *Correlation matrix :*\n- Correlation matrix will show how much are the features related to each other. \n- It assigns a value for every two pairs of features. \n- Higher values shows higher relation.\n- More related features means, we can obtain the information from either of that feature and remove the other.\n\n### *Heatmap :*\n- Visualization of Correlation matrix is simple by using heatmap","b8a2245f":"# Data Preprocessing :\n\n## There are 4 simple things we will do for our Data-Preprocessing :\n\n### Removal :\n- Removal of Features having NaN values more than 15%\n- Removal of similar features who do not provide any extra information. Like GarageArea can also be determine by GarageCars.\n\n### Filling the Missing Data :\n- We will fill the null values with their mean\/mode.\n- Features with object datatype has to filled with their mode.\n- While Features with float datatype will be filled with thier mean.\n- (optional) Instead of filling null values with their mean, you can also chose a random value from the range of values near their mean. For eg, if the mean of dataset is X, then instead of filling X at every null values, you can fill a random value from the range let say (X-10, X+10). This range is determined by studying the graph. But for simplicity, we won't do that in this notebook. ( Also the % of null values remaining are not that high to make a lasting difference )\n\n### Outliers :\n- It is also possible that some data points can deviate significantly from the rest for those features. May by due to error or some special features.\n- This can disturb our learning curve.\n- Not to ignore this. You will see how significantly does this affect our final score, even with just the minor change.\n\n### Normalizing our SalePrice:\n- SalePrice, that's what we are after. Our final goal in this project is to predict SalePrice.\n- We will see if our SalePrice is normalize or not.\n- One simple trick to normalize it, if it is not.\n- Again, normalizing the SalePrice will play a significan't role in our final error score.\n\nEnough Talking, time for action! For Marvel fans, this can be like a quest to get the Infinity Stone in a Complex Universe.","e2d8bf1e":"Our Mean Absolute Error comes out to be 0.14031. \n\nNow lets do the same for Test set as well, so that we are ready to upload our final file.","4c145904":"###### From above plots:\n- We can clearly see that SalePrice almost exponentially increases as the feature value increases\n- 2 point in GrLivArea and 1 point in TotalBsmtSF are not following the croud. These are our outliers. It is possible that those house have some drastically different features which gives them lowerprice even if this feature value is good. Lets check if the datapoint in them is the same or not.","8c41a802":"Ohh...  It looks like this universe has lot of elements. There are so many columns to work with. This at first, can seems overwhelming, but we need not worry, because we have python and pandas. This will make things very simple for us to visualize. You can say we have a Thor Hammer to fight with our enemies.","aa7582fc":"See... that outlier property is coming from the same datapoint with index 1298. \n\nWe will remove both the datapoint with index 523 and 1298"}}