{"cell_type":{"7f1b7c4f":"code","c3b51459":"code","afefb714":"code","d5008785":"code","6ef80b23":"code","9f59a284":"code","ddcff750":"code","5e320792":"code","4dba1bbb":"code","8473b03a":"code","38f86d4f":"code","5f81d01f":"code","b62f1811":"code","e4de3413":"code","d088f6c8":"code","a87c27e2":"code","ce8a04dd":"code","6777350b":"code","39d48065":"code","8286aef4":"code","a13ea26f":"code","e2e75b39":"code","6bf9f7a0":"code","b51d0a83":"code","ac5a4dbc":"code","dc15bb48":"code","5bd0da12":"code","3e6ed54f":"code","ffc5742a":"code","2964d119":"code","eb8dc13f":"code","cf613c1f":"code","88ef269f":"code","8a20daea":"code","0976e55f":"code","794d77d6":"code","92079ac8":"code","117c8b79":"code","98df1a4c":"code","a9ade83c":"code","94f9d370":"code","cc1e8012":"code","48330b78":"code","daee4ec5":"code","4a6ad952":"code","a3e620c6":"code","688b756c":"code","947abd51":"code","b2408464":"code","1bc64a14":"code","5ceda96b":"code","381573db":"code","9ed067d1":"code","e5c49ca6":"code","a55b8a3b":"code","02b4fb75":"code","963c9818":"code","7e9d9da9":"code","a02f502b":"code","a5ee0bed":"code","26d34bb9":"code","a57d704d":"code","d24e3e9f":"code","43646a61":"code","702c5778":"code","2e870fba":"code","42837a1f":"code","73f3fea8":"code","53a48e22":"code","fb32120c":"code","1109d265":"code","c77b490f":"code","547f89ad":"code","35009fed":"code","c17cf414":"code","6783810f":"code","941d9fb0":"code","b8a12a83":"markdown","7165e240":"markdown","094b40ae":"markdown","2e04e768":"markdown","8e766f6a":"markdown","608d4ce4":"markdown","d0e4efeb":"markdown","b0b2f6de":"markdown","3b4b361f":"markdown","f8dc91c5":"markdown","6f9a8cf6":"markdown","e63385b4":"markdown","498e2d3a":"markdown","5c26d31a":"markdown","06e60cb7":"markdown","658df431":"markdown","ef31c0b7":"markdown","9ccb16a4":"markdown","f15d5f36":"markdown","58ee69bd":"markdown","b3d5f002":"markdown","b7c0c82f":"markdown"},"source":{"7f1b7c4f":"#This program attempts to visualize media planning spend data by media channel against sales data\n# After visualizing variable pairs, we try to fit a linear regression model for the dataset\n# After fitting the LR model, we predict sales based on past media spend patterns\n# we estimate the accuracy of the prediction\n# we have to use a few external python libraries to complete the prediction\n# for visualisation, both pandas and seaborn were used\n# This dataset has been constructed from scratch as in the real world, it is difficult to assemble channel level sales data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n# This kernel covers starter concepts of Linear Regression Machine Learning\n# Inspiration for this kernel is from Pierian Data","c3b51459":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","afefb714":"# Importing Dtaset to CSV file\nmediamix = pd.read_csv(\"..\/input\/mediamix_sales.csv\")\nmediamix.head()","d5008785":"# Check the Total Rows & Columns (dimensions) of Dataset\nmediamix.shape","6ef80b23":"#Check Basic Information About Data\nmediamix.info()","9f59a284":"# Check the Statastical Aspects of Dataframe\nmediamix.describe()","ddcff750":" # Chek the null values\n\nmediamix.isnull().sum()","5e320792":"# convert into proper date\nmediamix[\"Time\"] = pd.to_datetime(mediamix[\"Time\"])\n#Rename column name\nmediamix = mediamix.rename(columns={'Time': 'Date'})","4dba1bbb":"# check 'tv_sponsorships' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='tv_sponsorships', data=mediamix)\nplt.show()","8473b03a":"# There are outliers in the tv_sponsorships columns and we can cap this variable to 95 percetile\n\nq1 = mediamix[\"tv_sponsorships\"].quantile(0.95)\nmediamix[\"tv_sponsorships\"][mediamix[\"tv_sponsorships\"] >= q1] = q1","38f86d4f":"# check 'tv_cricket' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='tv_cricket', data=mediamix)\nplt.show()","5f81d01f":"# There are outliers in the tv_cricket columns and we can cap this variable to 95 percetile\n\nq1 = mediamix[\"tv_cricket\"].quantile(0.95)\nmediamix[\"tv_cricket\"][mediamix[\"tv_cricket\"] >= q1] = q1","b62f1811":"# check 'tv_RON' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='tv_RON', data=mediamix)\nplt.show()","e4de3413":"# There are lot of outliers in the tv_RON columns and we can cap this variable to 95 percetile\n\nq1 = mediamix[\"tv_RON\"].quantile(0.95)\nmediamix[\"tv_RON\"][mediamix[\"tv_RON\"] >= q1] = q1","d088f6c8":"# check 'NPP' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='NPP', data=mediamix)\nplt.show()","a87c27e2":"# There are lot of outliers in the NPP columns and we can cap this variable to 95 percetile\n\nq1 = mediamix[\"NPP\"].quantile(0.95)\nmediamix[\"NPP\"][mediamix[\"NPP\"] >= q1] = q1","ce8a04dd":"# check 'Magazines' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='Magazines', data=mediamix)\nplt.show()","6777350b":"# There are lot of outliers in the Magazines columns and we can cap this variable to 95 percetile\n\nq1 = mediamix[\"Magazines\"].quantile(0.95)\nmediamix[\"Magazines\"][mediamix[\"Magazines\"] >= q1] = q1","39d48065":"# check 'Social' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='Social', data=mediamix)\nplt.show()","8286aef4":"# There are lot of outliers in the Social columns and we can cap this variable to 95 percetile\n\nq1 = mediamix[\"Social\"].quantile(0.95)\nmediamix[\"Social\"][mediamix[\"Social\"] >= q1] = q1","a13ea26f":"# check 'Programmatic' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='Programmatic', data=mediamix)\nplt.show()","e2e75b39":"# There are lot of outliers in the Programmatic columns and we can cap this variable to 25 percetile\n\nq1 = mediamix[\"Programmatic\"].quantile(0.25)\nmediamix[\"Programmatic\"][mediamix[\"Programmatic\"] <= q1] = q1","6bf9f7a0":"# check 'Native' outliers\nplt.figure(figsize=(4, 4))\nax = sns.boxplot(y='Native', data=mediamix)\nplt.show()","b51d0a83":"# There are lot of outliers in the Native columns and we can cap this variable to 50 percetile\n\nq1 = mediamix[\"Native\"].quantile(0.50)\nmediamix[\"Native\"][mediamix[\"Native\"] >= q1] = q1","ac5a4dbc":"sns.jointplot(data=mediamix, x=\"tv_sponsorships\", y=\"sales\", kind=\"reg\")","dc15bb48":"sns.jointplot(data=mediamix, x=\"tv_cricket\", y=\"sales\", kind=\"reg\")","5bd0da12":"sns.jointplot(data=mediamix, x=\"tv_RON\", y=\"sales\", kind=\"reg\")","3e6ed54f":"sns.jointplot(data=mediamix, x=\"radio\", y=\"sales\", kind=\"reg\")","ffc5742a":"sns.pairplot(mediamix, vars=['tv_sponsorships','tv_cricket','tv_RON','radio','NPP','Magazines','OOH','Social','Programmatic','Display_Rest','Search','Native','sales'])\nplt.show()","2964d119":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(mediamix.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","eb8dc13f":"mediamix = mediamix.drop('Native', axis=1)\n","cf613c1f":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\nmediamix_train, mediamix_test = train_test_split(mediamix, train_size = 0.7, test_size = 0.3, random_state = 100)","88ef269f":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","8a20daea":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['tv_sponsorships','tv_cricket','tv_RON','radio','NPP','Magazines','OOH','Social','Programmatic','Display_Rest','Search','sales']\n\nmediamix_train[num_vars] = scaler.fit_transform(mediamix_train[num_vars])","0976e55f":"mediamix_train.head()","794d77d6":"mediamix_train.describe()","92079ac8":"y_train = mediamix_train.pop('sales')\nX_train = mediamix_train[['tv_sponsorships','tv_cricket','tv_RON','radio','NPP','Magazines','OOH','Social','Display_Rest','Search']]","117c8b79":"import statsmodels.api as sm\n\n# Add a constant\nX_train_lm = sm.add_constant(X_train[['radio']])\n\n# Create a first fitted model\nlr = sm.OLS(y_train, X_train_lm).fit()","98df1a4c":"# Check the parameters obtained\n\nlr.params","a9ade83c":"# Let's visualise the data with a scatter plot and the fitted regression line\nplt.scatter(X_train_lm.iloc[:, 1], y_train)\nplt.plot(X_train_lm.iloc[:, 1], 0.301 + 0.392*X_train_lm.iloc[:, 1], 'r')\nplt.show()","94f9d370":"# Print a summary of the linear regression model obtained\nprint(lr.summary())","cc1e8012":"# Assign all the feature variables to X\nX_train_lm = X_train[['radio', 'tv_sponsorships']]","48330b78":"# Build a linear model\n\nimport statsmodels.api as sm\nX_train_lm = sm.add_constant(X_train_lm)\n\nlr = sm.OLS(y_train, X_train_lm).fit()\n\nlr.params","daee4ec5":"# Check the summary\nprint(lr.summary())","4a6ad952":"#Build a linear model\n\nimport statsmodels.api as sm\nX_train_lm = sm.add_constant(X_train)\n\nlr_1 = sm.OLS(y_train, X_train_lm).fit()\n\nlr_1.params","a3e620c6":"print(lr_1.summary())","688b756c":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","947abd51":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b2408464":"# Dropping highly correlated variables and insignificant variables\n\nX = X_train.drop('Social', 1,)","1bc64a14":"# Build a third fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","5ceda96b":"# Print the summary of the model\nprint(lr_2.summary())","381573db":"# Calculate the VIFs again for the new model\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9ed067d1":"# Dropping highly correlated variables and insignificant variables\nX = X.drop('Search', 1)","e5c49ca6":"# Build a second fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_3 = sm.OLS(y_train, X_train_lm).fit()","a55b8a3b":"# Print the summary of the model\n\nprint(lr_3.summary())","02b4fb75":"# Calculate the VIFs again for the new model\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","963c9818":"X = X.drop('NPP', 1)","7e9d9da9":"# Build a fourth fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_4 = sm.OLS(y_train, X_train_lm).fit()","a02f502b":"print(lr_4.summary())","a5ee0bed":"# Calculate the VIFs again for the new model\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","26d34bb9":"X = X.drop('Display_Rest', 1)","a57d704d":"# Build a fourth fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_4 = sm.OLS(y_train, X_train_lm).fit()","d24e3e9f":"print(lr_4.summary())","43646a61":"# Calculate the VIFs again for the new model\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","702c5778":"X = X.drop('Magazines', 1)","2e870fba":"# Build a fourth fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_4 = sm.OLS(y_train, X_train_lm).fit()\nprint(lr_4.summary())","42837a1f":"# Calculate the VIFs again for the new model\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","73f3fea8":"X = X.drop('OOH', 1)\n\n# Build a fourth fitted model\nX_train_lm = sm.add_constant(X)\n\nlr_4 = sm.OLS(y_train, X_train_lm).fit()\n\nprint(lr_4.summary())","53a48e22":"# Calculate the VIFs again for the new model\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fb32120c":"y_train_sales = lr_4.predict(X_train_lm)\nres = (y_train - y_train_sales)","1109d265":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_sales), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","c77b490f":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['tv_sponsorships','tv_cricket','tv_RON','radio','NPP','Magazines','OOH','Social','Programmatic','Display_Rest','Search','sales']\n\nmediamix_test[num_vars] = scaler.transform(mediamix_test[num_vars])\nmediamix_test.head()","547f89ad":"y_test = mediamix_test.pop('sales')\nX_test = mediamix_test","35009fed":"X_test_sm = sm.add_constant(X_test)\nX_test_sm.head()","c17cf414":"X_test_sm = X_test_sm.drop([\"Date\", \"NPP\", \"Programmatic\", \"Magazines\", \"OOH\", \"Social\", \"Search\", \"Display_Rest\" ], axis=1)","6783810f":"#predict\ny_test_sales = lr_4.predict(X_test_sm)\n","941d9fb0":"#evaluate\nfrom sklearn.metrics import r2_score\n\nr2_score(y_true = y_test, y_pred = y_test_sales)","b8a12a83":"As you might have noticed, radio seems to the correlated to sales the most. Let's see a pairplot for radio vs sales. Also there is multicollinearity between Programmatic and Native column lets drop Native column.","7165e240":"The R-squared number indicates how near the data is to the fitted regression line. The R-squared value is always between 0 and 100 %; a value of 0 % indicates that the model explains no variability in the response data around its mean. When a model describes 100% of the variability in response data around its mean, it is said to be perfect.\n\nRegression analysis is mainly used for Causal analysis, Forecasting the impact of a change, Forecasting trends etc. However, this method does not perform well on large amounts of data as it is sensitive to outliers, multicollinearity, and cross-correlation.\n","094b40ae":"As you might have noticed, VIF of radio as well such that it is now under 5. But from the summary, we can still see some of them have a high p-value. Magazines for instance, has a p-value of 0.937. We should drop this variable as well.","2e04e768":"## Step 2: Visualising the Data\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`.\n\n#### Univariate Analysis","8e766f6a":"Looking at the p-values, it looks like some of the variables aren't really significant (in the presence of other variables).\n\nMaybe we could drop some?\n\nWe could simply drop the variable with the highest, non-significant p value. A better way would be to supplement this with the VIF information.\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating VIF is:","608d4ce4":"## Step 4: Splitting the Data into Training and Testing Sets\n\nAs you know, the first basic step for regression is performing a train-test split.","d0e4efeb":"Dropping the variable and updating the model\n\n\nAs you can see from the summary and the VIF dataframe, some variables are still insignificant. One of these variables is, social as it has a very high VIF of 52.83. Let's go ahead and drop this variables","b0b2f6de":"Univariate analysis between sales and radio shows little correlation. Lets do bivariate analysis.","3b4b361f":"## Step 5: Building a linear model\n\nFit a regression line through the training data using `statsmodels`. Remember that in `statsmodels`, you need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default.","f8dc91c5":"## Step6 : Residual Analysis","6f9a8cf6":"## Step 1: Reading and Understanding the Data\n\nLet us first import NumPy and Pandas and read the housing dataset","e63385b4":"We have improved the adjusted R-squared again. Now let's go ahead and add all the feature variables","498e2d3a":"### Rescaling the Features \n\nAs you saw in the demonstration for Simple Linear Regression, scaling doesn't impact your model. Here we can see that except for `area`, all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nThis time, we will use MinMax scaling.","5c26d31a":"#### Dividing into X and Y sets for the model building","06e60cb7":"##### Conclusion\n\nIncreasing spend in the channel will yield a high return based on the current spending level. Channel contribution is calculated by comparing original sales and predicted sales upon removal of the channel.The outputs of the Marketing Model show the contribution of each marketing channel, which, when combined with marketing spends. It also accounts for temporal decay and declining returns on various media vehicles.","658df431":"As you can notice some of the variable have high VIF values as well as high p-values. Such variables are insignificant and should be dropped.\n\nAs you might have noticed, the variable search has a significantly high VIF (20.84) and a high p-value (0.365) as well. Hence, this variable isn't of much use and should be dropped.","ef31c0b7":"Adding another variable\n\nThe R-squared value obtained is 0.305. Since we have so many variables, we can clearly do better than this. So let's go ahead and add the second most highly correlated variable, i.e. tv_sponsorships.","9ccb16a4":"##### The following are some of the most generally mentioned flaws in media mix modelling:\nReports are few.\n- Data isn't granular enough, and the relationship between channels isn't examined.\n- There is no understanding of the brand or messaging.\n- Doesn't evaluate the customer's experience","f15d5f36":"#### Data Inspection","58ee69bd":"Now as you can see, the VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model only.\nResidual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","b3d5f002":"## Step 2: Data Preparation\n\nLets treat outliers before modeling","b7c0c82f":"#### Data Loading"}}