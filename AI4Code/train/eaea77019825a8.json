{"cell_type":{"82df19ec":"code","f6633998":"code","13416fd9":"code","e1105ce1":"code","82ba113e":"code","56746385":"code","99a87aec":"code","f3435b12":"code","cc9b7374":"code","cf630547":"code","896e7378":"code","a3f06eca":"code","4315dcbd":"code","18959089":"code","1df3c7f3":"code","f14427c3":"code","1aeff9a7":"code","bd46728d":"code","4a95043d":"code","4205185e":"code","2e3fffde":"markdown"},"source":{"82df19ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6633998":"# importing the necessary libraries\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential \nfrom keras.layers import Embedding, Flatten, Dense\nos.environ[\"KMP_SETTINGS\"] = \"false\"\n\nfrom sklearn import preprocessing\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","13416fd9":"# importing the IMDB movie review dataset\ndf = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","e1105ce1":"# preprocessing the textual data by tokenizing it \nmaxlen = 100\ntraining_samples = 25000\nvalidation_samples = 15000\ntesting_samples = 10000\nmax_words = 10000\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df['review'])\nsequences = tokenizer.texts_to_sequences(df['review'])\nword_index = tokenizer.word_index\nprint('Found ', len(word_index), ' unique tokens.')\n\n# encoding the categorical variables \nlabel_encoder = preprocessing.LabelEncoder()\nsentiment = label_encoder.fit_transform(df['sentiment'])","82ba113e":"# padding the tokenized data to make them all of equal length \ndata = pad_sequences(sequences, maxlen=maxlen)\nlabels = np.asarray(sentiment)\nprint('Shape of the tensor containing the reviews:', data.shape)\nprint('Shape of the tensor containing the sentiment labels:', labels.shape)","56746385":"# splitting the dataset into 3 parts for training, validation and testing\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]\nx_test = data[validation_samples: validation_samples + testing_samples]\ny_test = labels[validation_samples: validation_samples + testing_samples]","99a87aec":"# importing the GloVe word embeddings \nglove_dir = '\/kaggle\/input\/glove-global-vectors-for-word-representation\/'\n\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found', len(embeddings_index), 'word index')","f3435b12":"# creating a embedding matrix of size (max_words, embedding_dim) which can be loaded in the embedding layer \nembedding_dim = 100\n\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","cc9b7374":"# creating a model \nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","cf630547":"# loading the embedding matrix into the first layer i.e. Embedding layer of the model\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","896e7378":"# compiling, training and validating the model\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\nhistory_1 = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))","a3f06eca":"# evaluating the performance of the model\nvalues_1 = model.evaluate(x_test,y_test)\nvalues_1","4315dcbd":"df_1 = pd.DataFrame()\ndf_1['Training Accuracy'] = history_1.history['acc']\ndf_1['Validation Accuracy'] = history_1.history['val_acc']\ndf_1['Training Loss'] = history_1.history['loss']\ndf_1['Validation Loss'] = history_1.history['val_loss']\ndf_1['Epochs'] = range(1, len(df_1['Training Accuracy']) + 1)\n\n# comparing the training and validation accuracy \nfig = px.line(df_1, x='Epochs', y=['Training Accuracy', 'Validation Accuracy'], title='Model with Pretrained Word Embeddings')\nfig.show()","18959089":"# comparing the training and validation loss\nfig = px.line(df_1, x='Epochs', y=['Training Loss', 'Validation Loss'], title='Model without Pretrained Word Embeddings')\nfig.show()","1df3c7f3":"# creating a model without pretrained word embeddings \nnetwork = Sequential()\nnetwork.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nnetwork.add(Flatten())\nnetwork.add(Dense(32, activation='relu'))\nnetwork.add(Dense(1, activation='sigmoid'))\nnetwork.summary()","f14427c3":"# compiling, training and validating the model\nnetwork.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory_2 = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val,y_val))","1aeff9a7":"# evaluating the performance of the model\nvalues_2 = network.evaluate(x_test,y_test)\nvalues_2","bd46728d":"df_2 = pd.DataFrame()\ndf_2['Training Accuracy'] = history_2.history['acc']\ndf_2['Validation Accuracy'] = history_2.history['val_acc']\ndf_2['Training Loss'] = history_2.history['loss']\ndf_2['Validation Loss'] = history_2.history['val_loss']\ndf_2['Epochs'] = range(1, len(df_2['Training Accuracy']) + 1)\n\n# comparing the training and validation accuracy\nfig = px.line(df_2, x='Epochs', y=['Training Accuracy', 'Validation Accuracy'], title='Model without Pretrained Word Embeddings')\nfig.show()","4a95043d":"# comparing the training and validation loss\nfig = px.line(df_2, x='Epochs', y=['Training Loss', 'Validation Loss'], title='Model without Pretrained Word Embeddings')\nfig.show()","4205185e":"# comparing the evaluation performance of both the models\nfig = go.Figure()\nfig.add_trace(go.Bar(name='Loss', \n                     x=['Loss with Pretrained Word Embeddings', 'Loss without Pretrained Word Embeddings'], \n                     y=[values_1[0], values_2[0]]))\nfig.add_trace(go.Bar(name='Accuracy', \n                     x=['Accuracy with PreTrained Word Embeddings', 'Accuracy without PreTrained Word Embeddings'], \n                     y=[values_1[1], values_2[1]]))\nfig.show()\n","2e3fffde":"As we can see, the model with Pretrained Word Embeddings performed much better as compared to the model without them. This can be attributed to the fact that in case of small training datasets the model can't fully learn an appropriate task-specific embedding of the vocabulary. Pretrained Word Embeddings give us an advantage as they are already well-structured and systematic in nature and allows the model to capitalize on its capabilities. "}}