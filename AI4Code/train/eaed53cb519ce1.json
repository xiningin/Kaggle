{"cell_type":{"9f2550a2":"code","d20baf58":"code","187b9f34":"code","1524a2b2":"code","cfaa0008":"code","8dd6152c":"code","bea1ed1f":"code","5fcfe113":"code","4d9b202c":"code","d98948dd":"code","52a71db3":"code","df714aa7":"code","ab053773":"code","9e5e5d12":"code","141c6d71":"code","af792753":"code","dfd5b370":"code","ff6287df":"code","724a3891":"code","c0b77fb1":"markdown","bd3ef009":"markdown","55310b6e":"markdown","3faaaab6":"markdown","b47500ee":"markdown","80e35fcc":"markdown","3bb89ee0":"markdown","9f40c7dd":"markdown","cd31c6ff":"markdown","9d3eb3a6":"markdown","4a8932fb":"markdown"},"source":{"9f2550a2":"import numpy as np\nimport pandas as pd","d20baf58":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', encoding='utf-8')\ndf_new = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', encoding='utf-8')\ndf['set'] = 'train'\ndf_new['set'] = 'pred'\ndf = df.drop(columns='id')\ndf_new = df_new.drop(columns='id')\ndata = pd.concat([df, df_new]) # concatenate for text preprocessing\ndata.reset_index(inplace=True)\nprint(df.info())\nprint(df.head())","187b9f34":"print(df.target.value_counts(normalize=True))\ndf.target.value_counts(normalize=True).plot(kind='bar')","1524a2b2":"# Convert text to lowercase\ndata.text = data.text.str.lower()","cfaa0008":"# Remove\/replace unwanted characters\ndata.text = data.text.str.replace('\u0089\u00fb\u00aa', \"'\") # replace with '\ndata.text = data.text.str.replace('\u0089', \" \")\ndata.text = data.text.str.replace('\u00fb', \" \")\ndata.text = data.text.str.replace('\u00f2', \" \")\ndata.text = data.text.str.replace('\u00f3', \" \")\ndata.text = data.text.str.replace('\u00ef', \" \")\ndata.text = data.text.str.replace('\u00ec', \" \")\ndata.text = data.text.str.replace('\u00f7', \" \")\ndata.text = data.text.str.replace('\u00e5\u00ea', \" \")\ndata.text = data.text.str.replace('##', \"#\")","8dd6152c":"# Contractions Expander\nimport re\ncontractions_dict = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"i'd\": \"i had \/ i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i shall \/ i will\",\n\"i'll've\": \"i shall have \/ i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n}\nc_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return c_re.sub(replace, text)","bea1ed1f":"import re\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.casual import TweetTokenizer\n\n# Tweet-tokenizer function\ndef tweet_func(dataframe):\n    \"\"\"Tokenize tweet, stop words removal, and punctuation removal from 'text' column of dataframe\"\"\"\n    tweets = dataframe.text # tweets are in df.text\n    stop_words = set(stopwords.words('english'))\n    tweet_tokenizer = TweetTokenizer() # nltk.tokenize.word_tokenize also works!!\n    tweet_clean = []\n    n_token = []\n    for text in tweets:\n        text = re.sub('#\\w+', '', text) # Remove hashtags\n        text = re.sub('@\\w+', '', text) # Remove mentions\n        text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) # Remove url\n        token = tweet_tokenizer.tokenize(text) # Tweet Tokenizing\n        token = [word for word in token if not word in stop_words] # Stop Words Removal\n        token = [''.join(s) for s in token if not s in string.punctuation] # Punctuation Removal\n        tweet_clean.append(' '.join(token))\n        n_token.append(len(token)) # Word (Token) Count\n    return (tweet_clean, n_token)\n\n# Hashtag finder & counter\ndef hashtag_func(dataframe):\n    \"\"\"Hashtag finder & counter from 'text' column of dataframe\"\"\"\n    tweets = dataframe.text # tweets are in df.text\n    hashtag_list = []\n    hashtag_count = []\n    for text in tweets:\n        hashtag = re.findall('#\\w+', text)\n        hashtag_list.append(' '.join(hashtag))\n        hashtag_count.append(len(hashtag))\n    return (hashtag_list, hashtag_count)\n\n# Mention finder & counter\ndef mention_func(dataframe):\n    \"\"\"Mention finder & counter from 'text' column of dataframe\"\"\"\n    tweets = dataframe.text # tweets are in df.text\n    mention_list = []\n    mention_count = []\n    for text in tweets:\n        mention = re.findall('@\\w+', text)\n        mention_list.append(' '.join(mention))\n        mention_count.append(len(mention))\n    return (mention_list, mention_count)\n\n# Url finder & counter\ndef url_func(dataframe):\n    \"\"\"URL finder & counter from 'text' column of dataframe\"\"\"\n    tweets = dataframe.text # tweets are in df.text\n    url_list = []\n    url_count = []\n    for text in tweets:\n        url = re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n        url_list.append(' '.join(url))\n        url_count.append(len(url))\n    return (url_list, url_count)","5fcfe113":"data.text = data.text.apply(expandContractions)\ntweet_token, n_token = tweet_func(data)\nhashtag, n_hashtag = hashtag_func(data)\nurl, n_url = url_func(data)\nmention, n_mention = mention_func(data)\nprint('Functions applied!')","4d9b202c":"# Text Vectorizing\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Hashtag Vectorizer\ncv_hashtag = CountVectorizer(token_pattern='#\\w+')\nhashtag_vector_train = cv_hashtag.fit_transform(hashtag[0:len(df)])\nhashtag_vector_test = cv_hashtag.transform(hashtag[len(df):])\nprint('hashtag vector:')\nprint(type(hashtag_vector_train))\nprint(hashtag_vector_train.shape, '\\n')\n\n# Mention Vectorizer\ncv_mention = CountVectorizer(token_pattern='@\\w+')\nmention_vector_train = cv_mention.fit_transform(mention[0:len(df)])\nmention_vector_test = cv_mention.transform(mention[len(df):])\nprint('mention vector:')\nprint(type(mention_vector_train))\nprint(mention_vector_train.shape, '\\n')\n\n# Url Vectorizer\ncv_url = CountVectorizer(token_pattern='http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\nurl_vector_train = cv_url.fit_transform(url[0:len(df)])\nurl_vector_test = cv_url.transform(url[len(df):])\nprint('url vector:')\nprint(type(url_vector_train))\nprint(url_vector_train.shape, '\\n')\n\n# Tweet Vectorizer\ncv_tweet = CountVectorizer()\ntweet_vector_train = cv_tweet.fit_transform(tweet_token[0:len(df)])\ntweet_vector_test = cv_tweet.transform(tweet_token[len(df):])\nprint('tweet vector:')\nprint(type(tweet_vector_train))\nprint(tweet_vector_train.shape, '\\n')","d98948dd":"# Concatenate text vectors\nfrom scipy.sparse import hstack\n\nX = hstack([hashtag_vector_train, mention_vector_train, url_vector_train, tweet_vector_train])\nX = X.todense()\nprint('Training features:')\nprint(type(X))\nprint(X.shape)\n\nX_pred = hstack([hashtag_vector_test, mention_vector_test, url_vector_test, tweet_vector_test])\nX_pred = X_pred.todense()\nprint('Prediction features:')\nprint(type(X_pred))\nprint(X_pred.shape)","52a71db3":"from keras.utils.np_utils import to_categorical\ny = to_categorical(df.target)\nprint(type(y))\nprint(y.shape)","df714aa7":"input_shape = (X.shape[1],)","ab053773":"from keras.layers import Dense\nfrom keras.models import Sequential\ndef new_model(input_shape=input_shape):\n    model = Sequential()\n    model.add(Dense(100, activation='relu', input_shape=input_shape))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return(model)","9e5e5d12":"model = new_model(input_shape)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X, y, validation_split=0.25, epochs = 3)","141c6d71":"model = new_model(input_shape)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X, y)","af792753":"predictions = model.predict(X_pred)\nprint('First 5 predictions:')\nprint(predictions[:4])","dfd5b370":"results = []\nfor proba in predictions:\n    if proba[0] > 0.5:\n        results.append(int(0))\n    else:\n        results.append(int(1))\nresults = pd.Series(results)\nprint('Predicted class ratio:')\nprint(results.value_counts(normalize=True))","ff6287df":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nprint(submission.head())\nsubmission.target = results\nsubmission.set_index('id', inplace=True)\nprint(submission.head())","724a3891":"submission.to_csv('submission.csv')\nprint('Submission saved!')","c0b77fb1":"# Text Preprocessing\nSteps:\n0. Convert text to lowercase\n1. Remove unwanted characters\n2. Contractions expander\n3. Word tokenizer (including stop words & punctuation removal).\n4. Hashtag finder & counter\n5. Mention finder & counter\n6. Url finder & counter","bd3ef009":"**NOTES:**\n1. I have tried to optimize the keras model by reducing\/increasing layers & nodes and so far this is the best I got.\n2. I have only used ngram = 1 for the vectorizing.\n\nThank you!","55310b6e":"Train model to the whole training set:","3faaaab6":"Predict class probabilities:","b47500ee":"See model performance with 25% validation set:","80e35fcc":"# Keras Model\n1. n Hidden Layer = 2\n2. n Nodes = 100\n3. activation function = 'relu'","3bb89ee0":"Check target class balance:","9f40c7dd":"# Vectorizing\nSteps:\n1. Text vectorizing with CountVectorizer\n2. Concatenate text vectors\n3. Encode target","cd31c6ff":"# Project Summary\nThe goal is to classify disaster tweets. With this notebook I managed to get 0.79 score. This notebook consists of 3 parts:\n1. Text Preprocessing\n2. Vectorizing (with CountVectorizer)\n3. Keras Model Check\n4. Predictions\n\nI am very new to NLP and deep learning so this notebook is based on my limited knowledge. See also these notebooks for reference:\n1. https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n2. https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert","9d3eb3a6":"Create submission file:","4a8932fb":"# Predictions"}}