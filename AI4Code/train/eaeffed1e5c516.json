{"cell_type":{"e338c370":"code","0f895ce0":"code","ebde81bb":"code","f4328ca7":"code","e8e1e6a0":"code","37a27c68":"code","33f384da":"code","acfcc538":"code","81e160b8":"code","9703d130":"code","be6c2b38":"code","74385240":"code","ab8f75a7":"code","a1ff37bd":"code","6b7aa014":"code","38cbbee0":"code","d698fe4b":"code","df2e4c26":"code","7554baab":"code","60ed1de8":"code","f5625d1a":"markdown","b7f3c7f6":"markdown","a1843c2e":"markdown","0f5205e7":"markdown","d6797bc5":"markdown","556448b5":"markdown","307b84c5":"markdown","d2bc2083":"markdown","88d6442f":"markdown","96bb1199":"markdown","4960c389":"markdown","b044ceb9":"markdown","83600409":"markdown"},"source":{"e338c370":"! pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git\n! wget https:\/\/github.com\/jganzabal\/santander_kaggle_solutions_tests\/raw\/master\/santander_helper.py","0f895ce0":"from santander_helper import auc, DataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten\nfrom keras.optimizers import Adam\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom keras_contrib.callbacks import CyclicLR\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")","ebde81bb":"# GET INDICIES OF REAL TEST DATA FOR FE\n#######################\n# TAKE FROM YAG320'S KERNEL\n# https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split\n\ntest_path = '..\/input\/test.csv'\ntrain_path = '..\/input\/train.csv'\n\ndf_test = pd.read_csv(test_path)\ndf_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in range(df_test.shape[1]):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv('..\/input\/train.csv', dtype=d)\ntest = pd.read_csv('..\/input\/test.csv', dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv(train_path, dtype=d)\ntest = pd.read_csv(test_path, dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')","f4328ca7":"# FREQUENCY ENCODE\ndef encode_FE(df,col,test):\n    cv = df[col].value_counts()\n    nm = col+'_FE'\n    df[nm] = df[col].map(cv)\n    test[nm] = test[col].map(cv)\n    test[nm].fillna(0,inplace=True)\n    if cv.max()<=255:\n        df[nm] = df[nm].astype('uint8')\n        test[nm] = test[nm].astype('uint8')\n    else:\n        df[nm] = df[nm].astype('uint16')\n        test[nm] = test[nm].astype('uint16')        \n    return\n\ntest['target'] = -1\ncomb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\nfor i in range(200): \n    encode_FE(comb,'var_'+str(i),test)\ntrain = comb[:len(train)]; del comb\nprint('Added 200 new magic features!')","e8e1e6a0":"del df_test, real_samples_indexes, synthetic_samples_indexes, unique_count, unique_samples, d\ngc.collect()","37a27c68":"# Load data with counts saved in the previous cells\ndf_train_data = train.drop(columns=['ID_code'])\ny = df_train_data['target'].values\ndf_train_X = df_train_data.drop(columns=['target'])","33f384da":"reverse_columns = True\nif reverse_columns:\n    reverse_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 15, 16, 18, 19, 22, 24, 25, 26,\n                    27, 29, 32, 35, 37, 40, 41, 47, 48, 49, 51, 52, 53, 55, 60, 61,\n                    62, 65, 66, 67, 69, 70, 71, 74, 78, 79, 82, 84, 89, 90, 91, 94,\n                    95, 96, 97, 99, 103, 105, 106, 110, 111, 112, 118, 119, 125, 128,\n                    130, 133, 134, 135, 137, 138, 140, 144, 145, 147, 151, 155, 157,\n                    159, 161, 162, 163, 164, 167, 168, 170, 171, 173, 175, 176, 179,\n                    180, 181, 184, 185, 187, 189, 190, 191, 195, 196, 199,\n                    ]\n\n    for j in reverse_list:\n        df_train_X[f'var_{j}'] *= -1","acfcc538":"# Normalize data\nmeans = df_train_X.mean(axis=0)\nstds = df_train_X.std(axis=0)\ndf_train_X_normalized = (df_train_X - means)\/stds","81e160b8":"# Prepare data for CNN\nX_train_normalized = np.zeros((df_train_X_normalized.shape[0], 400, 1))\nfor i in range(200):\n    X_train_normalized[:, 2*i] = df_train_X_normalized[[f'var_{i}']].values #[indexes]\n    X_train_normalized[:, 2*i+1] = df_train_X_normalized[[f'var_{i}_FE']].values #[indexes]","9703d130":"# Define model\ndef get_model(N_units = 600, kernel_size=2, strides=2):\n    model = Sequential()\n    model.add(Conv1D(N_units, kernel_size=kernel_size, strides=strides, padding='valid', \n                     activation='relu', input_shape=(X_train_normalized.shape[1], 1)))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n    return model","be6c2b38":"get_model().summary()","74385240":"del df_train_data, df_train_X\ngc.collect()","ab8f75a7":"best_model_file_name = 'best_full_model_aux.hdf5'\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\npatience = 18\nepochs = 100\nbs = 512\nN_units = 600\ncommon_rows = 2\nclass_0_aug = 4\nclass_1_aug = 6\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train_X_normalized, y)):\n    print('###############################################')\n    print(f'##################Fold {fold}#######################')\n    print('###############################################')\n    model = get_model(N_units)\n    model.compile(Adam(), loss='binary_crossentropy', metrics=[auc, 'accuracy'])\n    es = EarlyStopping(monitor='val_auc', patience=patience, mode='max', verbose=1)\n    mc = ModelCheckpoint(best_model_file_name, monitor='val_auc', mode='max', verbose=1, save_best_only=True)\n \n    generator = DataGenerator(X_train_normalized[trn_idx], y[trn_idx], \n                              batch_size=bs, shuffle=True, \n                              class_1_aug=class_1_aug, \n                              class_0_aug=class_0_aug,\n                              common_rows = common_rows\n                             )\n    tr_iter_in_epoch = generator.__len__()\n#     gamma = 1 - 6e-05  * 4*312\/tr_iter_in_epoch\n#     clr = CyclicLR(base_lr=0.0001, max_lr=0.005, step_size=4*tr_iter_in_epoch, mode='exp_range', gamma=gamma)\n    clr = CyclicLR(base_lr=0.0001, max_lr=0.005, step_size=4*tr_iter_in_epoch, mode='triangular2')\n    X_val_data, y_val_data = DataGenerator.augment(X_train_normalized[val_idx], \n                                     y[val_idx], class_1_aug=class_1_aug\/\/2, class_0_aug=class_0_aug\/\/2, common_rows = common_rows)\n    indexes_val = np.arange(len(y_val_data))\n    np.random.shuffle(indexes_val)\n    model.fit_generator(generator,\n              epochs=epochs,\n              verbose=2,\n              callbacks = [es, \n                           mc, \n                           clr],\n              validation_data=(X_val_data[indexes_val], y_val_data[indexes_val])\n                )\n    # print(f'Finish training with lr {lr}')\n    model = get_model()\n    # Load weights from ModelCheckpoint\n    model.load_weights(best_model_file_name)\n    # Save them to disk\n    model.save_weights(f'CNN_generator_fold_{fold}_cl1_{class_1_aug}_cl0_{class_0_aug}_{N_units}.hdf5')","a1ff37bd":"df_test = test.drop(columns=['ID_code', 'target'])\nif reverse_columns:\n    for j in reverse_list:\n        df_test[f'var_{j}'] *= -1\ndf_test_X_normalized = (df_test - means)\/stds","6b7aa014":"X_test_normalized = np.zeros((df_test_X_normalized.shape[0], 400, 1))\nfor i in range(200):\n    X_test_normalized[:, 2*i] = df_test_X_normalized[[f'var_{i}']].values\n    X_test_normalized[:, 2*i+1] = df_test_X_normalized[[f'var_{i}_FE']].values","38cbbee0":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfull_val_preds = np.zeros((len(df_train_X_normalized), 1))\nmodel = get_model(N_units)\ntest_predictions = 0\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train_X_normalized, y)):\n    print('###############################################')\n    print(f'##################Fold {fold}#######################')\n    print('###############################################')\n    X_train = X_train_normalized[trn_idx]\n    X_val = X_train_normalized[val_idx]\n    filename = f'CNN_generator_fold_{fold}_cl1_{class_1_aug}_cl0_{class_0_aug}_{N_units}.hdf5'\n    model.load_weights(filename)\n    full_val_preds[val_idx] = model.predict(X_val, verbose=1)\n    print(roc_auc_score(y[val_idx], full_val_preds[val_idx]))\n    test_predictions = test_predictions + model.predict(X_test_normalized, verbose=1)\/5","d698fe4b":"# Validation ROC AUC\nroc_auc_score(y, full_val_preds)","df2e4c26":"from matplotlib import pyplot as plt\n_ = plt.hist(np.log(test_predictions\/(1-test_predictions)), 80)\nplt.title('Histogram of Log Odds in test')\nplt.xlabel('Log odds')\nplt.ylabel('Counts')\nplt.show()","7554baab":"def save_submit_file(predictions, filename, test_filename=test_path, index_column='ID_code', target_column = 'target'):\n    df_test_submit = pd.read_csv(test_filename).set_index(index_column)\n    df_test_submit[target_column] = predictions\n    df_test_submit[[target_column]].to_csv(filename)\n    return ","60ed1de8":"save_submit_file(test_predictions, \n                 f'submit_CNN_generator_cl1_{class_1_aug}_cl0_{class_0_aug}_{N_units}.csv')","f5625d1a":"# Test data - log odds distribution","b7f3c7f6":"# Evaluate Model","a1843c2e":"# Load and prepare data for CNN","0f5205e7":"# CNN model","d6797bc5":"# Solution:\n\n- Implements a really simple CNN arquitecture \n- Train it with 400 columns (Original 200 vars + 200 columns of counts)\n- Data augmentation (Each epoch is feed with different data) \n- Got 0.922 in private LB \n- Using cyclic learning rate to train the CNN","556448b5":"# Create submission file","307b84c5":"## Why CNN?\nThe intuition is to pick var_i and var_i_FE together with each filter, that is why the strides is 2 and the kernel size is 2. Each of the 600 filters will be applied to each pair (var_i, var_i_FE).\n\n## Where is the independence assumption with a CNN?\nThe problem with CNN is that it might try to find correlation between vars. To prevent this we use the **[Datagenerator](https:\/\/github.com\/jganzabal\/santander_kaggle_solutions_tests\/blob\/master\/santander_helper.py)** that in each epoch it will shuffle al rows for each (var_i, var_i_FE) pairs. This idea is taken from this great kernel: https:\/\/www.kaggle.com\/jesucristo\/santander-magic-lgb-0-901\n\n## TODOs\n- Change hyperparameters\n- Add regularization (Dropout, L1, L2)\n- Change arquitecture\n- Add more feature engineering","d2bc2083":"# Train model","88d6442f":"# Discussions","96bb1199":"# Add counts to each of the 200 vars\nThis is taken from https:\/\/www.kaggle.com\/cdeotte\/200-magical-models-santander-0-920\/comments, a must read kernel from @cdeotte","4960c389":"# Download and import packages\nDownload keras-contrib for CyclicLR and santander_helper for auc metric and custom Datagenerator","b044ceb9":"This is taken from https:\/\/www.kaggle.com\/sibmike\/are-vars-mixed-up-time-intervals.\nReally can't justify it from a theoretical point of view but it improoves a bit the private score. Any ideas will be welcome","83600409":"> # Divide fake from real:\n\nThis is taken from https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split"}}