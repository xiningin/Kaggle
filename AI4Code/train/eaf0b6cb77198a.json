{"cell_type":{"bbc43e33":"code","77aa4bfa":"code","ac6f4433":"code","503b3420":"code","33c56e83":"code","36f3aa29":"code","141bb212":"code","03ca7409":"code","60774ead":"code","3b568825":"code","28e56551":"code","c4e2c49d":"code","7b3a8e41":"code","e894afef":"code","27c8d84a":"code","7d9c8671":"code","c1272660":"code","fde05093":"code","d2488e53":"code","681c9bc2":"code","53cca76b":"code","9c2a1c82":"code","3af8f304":"code","9384091f":"code","ca10e294":"code","c568cdd0":"code","78ba9fa7":"markdown","e192f5bf":"markdown","48f04f73":"markdown","8993abda":"markdown","7c8e39d3":"markdown","391290e5":"markdown","0403cd32":"markdown","8c29ceef":"markdown","a2fa6178":"markdown","6d8e8000":"markdown","86a25bb0":"markdown","2bfd25e4":"markdown","cd4206e6":"markdown","88443dea":"markdown","5e37d545":"markdown","e92e53ee":"markdown","9035d00b":"markdown","368ba5c7":"markdown","be2785bc":"markdown","59a8831d":"markdown","a83ed51a":"markdown","125006ec":"markdown","76dab86a":"markdown","0835f388":"markdown","dc5f4248":"markdown","0035f44b":"markdown","a5b5af2b":"markdown","d91b5e8a":"markdown","ca85a41a":"markdown","13240d7f":"markdown","353b2f1c":"markdown","76cbd4d1":"markdown","00797880":"markdown","1548f976":"markdown","f4789772":"markdown","1f674fb4":"markdown","7640b9d0":"markdown","37b7d290":"markdown","be49ce62":"markdown","71fcc2b1":"markdown","cc5bfc0c":"markdown","10782db4":"markdown","7166f86d":"markdown","833b14c3":"markdown","bbef73f8":"markdown","7164c80e":"markdown","a6e853e7":"markdown","3cc0ffae":"markdown","cf3218f5":"markdown","6fedce1e":"markdown","c4735dac":"markdown","c915bc29":"markdown","9d1f3dea":"markdown","31437bb0":"markdown","5b5d04c0":"markdown","a986df71":"markdown","b450d578":"markdown","6b570c33":"markdown","c70ea155":"markdown","5e5cf019":"markdown","21a279e5":"markdown","2ac9e0f4":"markdown","daf9188f":"markdown","76b4699c":"markdown","2a5d74cb":"markdown","83502bab":"markdown","d44c10b1":"markdown","b5d85374":"markdown","64a6838b":"markdown","c1ced136":"markdown","dc544d21":"markdown","59262878":"markdown","89fb6e53":"markdown","fe90a369":"markdown","5ff3f835":"markdown","0ae95e7a":"markdown","e81a9ba9":"markdown","c1b64fed":"markdown","db23563c":"markdown","e1e8c7d1":"markdown","2b65da73":"markdown","d3f2baa1":"markdown"},"source":{"bbc43e33":"from IPython.display import Image\nimport os\nImage(\"..\/input\/week4images\/K-Nearest Neighbor Algorithm 1.jpeg\", width=\"800px\")","77aa4bfa":"Image(\"..\/input\/week4images\/K-Nearest Neighbor Algorithm 2.jpeg\", width=\"800px\")","ac6f4433":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('HVXime0nQeI', width=800, height=300)","503b3420":"Image(\"..\/input\/week4images\/K-Nearest Neighbor Algorithm 3.jpeg\", width=\"1600px\")","33c56e83":"import pandas as pd\nimport numpy as np","36f3aa29":"\"\"\"\nGenerate a set of sample data\n\"\"\"\n\ndef create_data():\n    features = np.array(\n        [[2.88, 3.05], [3.1, 2.45], [3.05, 2.8], [2.9, 2.7], [2.75, 3.4],\n         [3.23, 2.9], [3.2, 3.75], [3.5, 2.9], [3.65, 3.6],[3.35, 3.3]])\n    labels = ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']\n    return features, labels","141bb212":"'''\nPrint the data\n'''\n\nfeatures, labels = create_data()\nprint('features: \\n',features)\nprint('labels: \\n',labels)","03ca7409":"\"\"\"\nDraw the sample data\n\"\"\"\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(5, 5))\nplt.xlim((2.4, 3.8))\nplt.ylim((2.4, 3.8))\nx_feature=list(map(lambda x:x[0],features)) # Return feature x of each data\ny_feature=list(map(lambda y:y[1],features)) \nplt.scatter(x_feature[:5],y_feature[:5],c=\"b\") # Draw the data points of A\nplt.scatter(x_feature[5:],y_feature[5:],c=\"g\") \nplt.scatter([3.18],[3.15],c=\"r\",marker=\"x\") # The coordinates of the testing point: [3.1\uff0c3.2]","60774ead":"Image(\"..\/input\/week4images\/K-Nearest Neighbor Algorithm 4.jpeg\", width=\"1600px\")","3b568825":"\"\"\"\nManhattan Distance\n\"\"\"\n\nimport numpy as np\n\ndef d_man(x, y):\n    d = np.sum(np.abs(x - y))\n    return d\n\nx = np.array([3.1, 3.2])\nprint(\"x:\", x)\ny = np.array([2.5, 2.8])\nprint(\"y:\", y)\nd_man = d_man(x, y)\nprint(d_man)","28e56551":"\"\"\"\nEuclidean distance\n\"\"\"\n\nimport numpy as np\n\ndef d_euc(x, y):\n    d = np.sqrt(np.sum(np.square(x - y)))\n    return d\n\nx = np.random.random(10)  # Randomly generate an array of 10 numbers as the value of x\nprint(\"x:\", x)\ny = np.random.random(10)\nprint(\"y:\", y)\ndistance_euc = d_euc(x, y)\nprint(distance_euc)","c4e2c49d":"\"\"\"\nMajority voting method\n\"\"\"\n\nimport operator\n\ndef majority_voting(class_count):\n    sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_class_count\n\narr = {'A': 3, 'B': 2, \"C\": 6, \"D\": 5}\nmajority_voting(arr)","7b3a8e41":"\"\"\"\nComplete realization of KNN\n\"\"\"\n\ndef knn_classify(test_data, train_data, labels, k):\n    distances = np.array([])  # Create an empty array to save the distance\n    for each_data in train_data:  # Calculate data similarity using Euclidean distance\n        d = d_euc(test_data, each_data)\n        distances = np.append(distances, d)\n    sorted_distance_index = distances.argsort()  # Get the indices sorted by distance\n    sorted_distance = np.sort(distances)\n    r = (sorted_distance[k]+sorted_distance[k-1])\/2  # Calculate the radius\n    class_count = {}\n    for i in range(k):  # Majority vote\n        vote_label = labels[sorted_distance_index[i]]\n        class_count[vote_label] = class_count.get(vote_label, 0) + 1\n    final_label = majority_voting(class_count)\n    return final_label, r","e894afef":"test_data=np.array([3.18,3.15])\nfinal_label,r=knn_classify(test_data, features, labels, 5)\nfinal_label","27c8d84a":"def circle(r, a, b):  # Polar coordinates \uff1ax=r*cos\u03b8\uff0cy=r*sin\u03b8.\n    theta = np.arange(0, 2*np.pi, 0.01)\n    x = a+r * np.cos(theta)\n    y = b+r * np.sin(theta)\n    return x, y\n\nk_circle_x, k_circle_y = circle(r, 3.18, 3.15)\n\nplt.figure(figsize=(5, 5))\nplt.xlim((2.4, 3.8))\nplt.ylim((2.4, 3.8))\nx_feature = list(map(lambda x: x[0], features))  # Return feature x of each data\ny_feature = list(map(lambda y: y[1], features))\nplt.scatter(x_feature[:5],y_feature[:5],c=\"b\") # Draw the data points of A\nplt.scatter(x_feature[5:],y_feature[5:],c=\"g\") \nplt.scatter([3.18],[3.15],c=\"r\",marker=\"x\") # The coordinates of the testing point: [3.1\uff0c3.2]\nplt.plot(k_circle_x, k_circle_y)","7d9c8671":"from ipywidgets import interact, fixed\n\ndef change_k(test_data, features, k):\n    final_label, r = knn_classify(test_data, features, labels, k)\n    k_circle_x, k_circle_y = circle(r, 3.18, 3.15)\n    plt.figure(figsize=(5, 5))\n    plt.xlim((2.4, 3.8))\n    plt.ylim((2.4, 3.8))\n    x_feature = list(map(lambda x: x[0], features))  # Return feature x of each data\n    y_feature = list(map(lambda y: y[1], features))\n    plt.scatter(x_feature[:5],y_feature[:5],c=\"b\") # Draw the data points of A\n    plt.scatter(x_feature[5:],y_feature[5:],c=\"g\") \n    plt.scatter([3.18],[3.15],c=\"r\",marker=\"x\") # The coordinates of the testing point: [3.1\uff0c3.2]\n    plt.plot(k_circle_x, k_circle_y)\n\ninteract(change_k, test_data=fixed(test_data),\n         features=fixed(features), k=[3, 5, 7, 9])","c1272660":"\"\"\"\nLoad dataset\n\"\"\"\n\nimport pandas as pd\n\nlilac_data = pd.read_csv(\"..\/input\/week4data\/syringa.csv\")\nlilac_data.head() # Preview first 5 rows","fde05093":"\"\"\"\nPlot subgraphs of features\n\"\"\"\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))  # Build a 2*3 graph space, 2 rows and 3 columns\nfig.subplots_adjust(hspace=0.3, wspace=0.2)  # Define height space and width space\naxes[0, 0].set_xlabel(\"sepal_length\")  # Define x-axis label\naxes[0, 0].set_ylabel(\"sepal_width\")  # Define y-axis label\naxes[0, 0].scatter(lilac_data.sepal_length[:50],\n                   lilac_data.sepal_width[:50], c=\"b\")\naxes[0, 0].scatter(lilac_data.sepal_length[50:100],\n                   lilac_data.sepal_width[50:100], c=\"g\")\naxes[0, 0].scatter(lilac_data.sepal_length[100:],\n                   lilac_data.sepal_width[100:], c=\"r\")\naxes[0, 0].legend([\"daphne\", \"syringa\", \"willow\"], loc=2)  # Define sample\n\naxes[0, 1].set_xlabel(\"petal_length\")\naxes[0, 1].set_ylabel(\"petal_width\")\naxes[0, 1].scatter(lilac_data.petal_length[:50],\n                   lilac_data.petal_width[:50], c=\"b\")\naxes[0, 1].scatter(lilac_data.petal_length[50:100],\n                   lilac_data.petal_width[50:100], c=\"g\")\naxes[0, 1].scatter(lilac_data.petal_length[100:],\n                   lilac_data.petal_width[100:], c=\"r\")\n\naxes[0, 2].set_xlabel(\"sepal_length\")\naxes[0, 2].set_ylabel(\"petal_length\")\naxes[0, 2].scatter(lilac_data.sepal_length[:50],\n                   lilac_data.petal_length[:50], c=\"b\")\naxes[0, 2].scatter(lilac_data.sepal_length[50:100],\n                   lilac_data.petal_length[50:100], c=\"g\")\naxes[0, 2].scatter(lilac_data.sepal_length[100:],\n                   lilac_data.petal_length[100:], c=\"r\")\n\naxes[1, 0].set_xlabel(\"sepal_width\")\naxes[1, 0].set_ylabel(\"petal_width\")\naxes[1, 0].scatter(lilac_data.sepal_width[:50],\n                   lilac_data.petal_width[:50], c=\"b\")\naxes[1, 0].scatter(lilac_data.sepal_width[50:100],\n                   lilac_data.petal_width[50:100], c=\"g\")\naxes[1, 0].scatter(lilac_data.sepal_width[100:],\n                   lilac_data.petal_width[100:], c=\"r\")\n\naxes[1, 1].set_xlabel(\"sepal_length\")\naxes[1, 1].set_ylabel(\"petal_width\")\naxes[1, 1].scatter(lilac_data.sepal_length[:50],\n                   lilac_data.petal_width[:50], c=\"b\")\naxes[1, 1].scatter(lilac_data.sepal_length[50:100],\n                   lilac_data.petal_width[50:100], c=\"g\")\naxes[1, 1].scatter(lilac_data.sepal_length[100:],\n                   lilac_data.petal_width[100:], c=\"r\")\n\naxes[1, 2].set_xlabel(\"sepal_width\")\naxes[1, 2].set_ylabel(\"petal_length\")\naxes[1, 2].scatter(lilac_data.sepal_width[:50],\n                   lilac_data.petal_length[:50], c=\"b\")\naxes[1, 2].scatter(lilac_data.sepal_width[50:100],\n                   lilac_data.petal_length[50:100], c=\"g\")\naxes[1, 2].scatter(lilac_data.sepal_width[100:],\n                   lilac_data.petal_length[100:], c=\"r\")","d2488e53":"from sklearn.model_selection import train_test_split\n\n#Get the entire features in the lilac dataset\nfeature_data = lilac_data.iloc[:, :-1]\nlabel_data = lilac_data[\"labels\"]  # Get the labels in the lilac dataset\nx_train, x_test, y_train, y_test = train_test_split(\n    feature_data, label_data, test_size=0.3, random_state=2)\n\nx_test  # Output lilac_test and view","681c9bc2":"\"\"\"\nUse scikit-learn to construct KNN model\n\"\"\"\n\nfrom sklearn import neighbors\nimport sklearn\n\ndef sklearn_classify(train_data, label_data, test_data, k_num):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k_num)\n    # Train\n    knn.fit(train_data, label_data)\n    # Predict\n    predict_label = knn.predict(test_data)\n    # Return\n    return predict_label","53cca76b":"\"\"\"\nPredict by dataset\n\"\"\"\n\ny_predict=sklearn_classify(x_train, y_train, x_test, 3)\ny_predict","9c2a1c82":"\"\"\"\nAccuracy calculation\n\"\"\"\n\ndef get_accuracy(test_labels, pred_labels):  \n    correct = np.sum(test_labels == pred_labels) # Calculate the number of correct predictions\n    n = len(test_labels) # Total number of test data\n    accur=correct\/n\n    return accur","3af8f304":"get_accuracy(y_test, y_predict)","9384091f":"normal_accuracy = []  # Create an empty list of accuracy rates\nk_value = range(2, 11)\nfor k in k_value:\n    y_predict = sklearn_classify(x_train, y_train, x_test, k)\n    accuracy = get_accuracy(y_test, y_predict)\n    normal_accuracy.append(accuracy)\n\nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy\")\nnew_ticks = np.linspace(0.6, 0.9, 10)  # Set the y-axis display\nplt.yticks(new_ticks)\nplt.plot(k_value, normal_accuracy, c='r')\nplt.grid(True)  # Add grid","ca10e294":"Image(\"..\/input\/week4images\/K-Nearest Neighbor Algorithm 5.png\", width=\"1000px\")","c568cdd0":"\"\"\"\nIntroduce the time function to calculate the running time of the program\n\"\"\"\n\nimport time\n\n#Without kd tree\ntime_start1 = time.time()\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train, y_train)\npredict_label = knn.predict(x_test)\ntime_end1 = time.time()\nprint(\"normal_time:\", time_end1-time_start1)\n\n#With kd tree\ntime_start2 = time.time()\nkd_knn = neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\nkd_knn.fit(x_train, y_train)\npredict_label = knn.predict(x_test)\ntime_end2 = time.time()\nprint(\"kd_tree_time:\", time_end2-time_start2)","78ba9fa7":"<div style=\"color: #999;font-size: 12px;font-style: italic;\">* Congratulations! You've completed Supervised Learning: Classification: K-Nearest Neighbour Algorithm.<\/div>","e192f5bf":"### Classification Prediction","48f04f73":"To solve classification problems, the K-nearest neighbors (KNN for short) algorithm is a simple and practical method. This chapter will introduce the K-nearest neighbors algorithm in detail. We will give a Python implementation from the aspects of distance calculation and classification decision. Finally, the K-nearest neighbors algorithm will be used to construct a prediction model and solve the lilac flower classification problem.","8993abda":"#### Euclidean Distance","7c8e39d3":"Here\uff1a  <br>\n- `n_neighbors`: Number of neighbors to use by default for K-neighbor queries.<br>\n- `weights`: Weight function used in prediction.<br>\n- `algorithm`: Algorithm used to compute the nearest neighbors.","391290e5":"## K-Nearest Neighbors Classification Prediction","0403cd32":"Before introducing the K-nearest neighbors algorithm, let's first talk about the nearest neighbor (NN) algorithm. It finds the training sample $y$ most similar to $x$ in the training set and uses the category of $y$ as the category of $x$ to achieve the classification aim:","8c29ceef":"#### Brief Introduction To Kd Tree","a2fa6178":"### Visualization","6d8e8000":"_Manhattan distance_, also known as _Mahalanobis distance_ and _taxi distance_, is one of the easiest ways to calculate distance. The formula is as follows:<br>\n<br>\n$$d_{man}=\\sum_{i=1}^{N}\\left | X_{i}-Y_{i} \\right | $$  ","86a25bb0":"After obtaining the prediction results, we need to evaluate the performance of the model, that is, to obtain the accuracy of the model prediction. Calculating the accuracy rate is to compare the difference between the predicted value and the ground truth:","2bfd25e4":"The K-Nearest Neighbors (KNN) algorithm is a generalization of the nearest neighbor (NN) algorithm and one of the simplest methods in the machine learning classification algorithm. The core idea of the KNN algorithm is similar to that of the nearest neighbor algorithm, which is classified by finding categories similar to the unknown samples. However, in the NN algorithm, only one sample is used for decision. When the classification is too absolute, the classification effect is poor. To overcome the defect, the KNN algorithm uses K neighboring samples to jointly decide the categories of unknown samples. In this way, the fault tolerance rate in decision-making is much higher than that of the NN algorithm and the classification effect is better.","cd4206e6":"### Key Points","88443dea":"Next we use the KNN algorithm to classify and predict real datasets.","5e37d545":"- Python 3.6<br>\n- NumPy 1.14.2<br>\n- Matplotlab 2.2.2<br>\n- Pandas 0.22.0<br>\n- scikit-learn 0.19.1","e92e53ee":"### Choice of K","9035d00b":"In the previous explanation, the main purpose of the Kd tree is to improve the speed of data search and reduce the memory and time consumption. Below we use the code to intuitively experience the advantages of the Kd tree. Implementing the Kd tree with the scikit-learn library is quite simple: We just need to pass the `kd_tree` parameter when the function is called:","368ba5c7":"### Data Partition","be2785bc":"Using the above accuracy calculation function, the classification accuracy can be obtained using the following code:","59a8831d":"## Basics of K-Nearest Neighbors Algorithm","a83ed51a":"After obtaining the similarity between the test sample and the training sample, through the ranking of similarity, K adjacent training samples of each of the test samples can be selected. Then how to judge the final category of the test sample with the help of K neighbors? Decision rules based on data characteristics are discussed below. Different decision rules will produce different prediction results. The most common decision rules are:<br>\n<br>\n- **Majority voting method**: The majority voting method is similar to the voting process, that is, the category with the most points among the K neighbors is selected as the test sample category.<br>\n- **Weighted voting method**: According to the distance, the voting of the neighbors is weighted. Shorter the distance is, the larger the weight is. The class that calculates the maximum value by weight is the category of the test sample.","125006ec":"In computer science, a Kd tree (short for K-dimensional tree) is a space-partitioning data structure for organizing points in a k-dimensional space. Kd trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g., range searches and nearest neighbor searches). Kd trees are a special case of binary space-partitioning trees. Every node of the Kd tree corresponds to a K-dimensional super-rectangular region. Kd trees can be used to eliminate the search for most of the data points, thus reducing the amount of calculations:","76dab86a":"Then we try to load and print the data:","0835f388":"To understand the data more intuitively, the dataset is visualized using the **pyplot** package of Matplotlib. For the sake of simplicity, we have used the `map` function and the `lambda` expression to process the data. If you are not familiar with these two methods, you are supposed to earn the corresponding Python knowledge by yourself:","dc5f4248":"### Data Preparation","0035f44b":"As shown in the figure above, by calculating the distance between data $X_{u}$ (unknown sample) and the known categories ${\\omega_{1}, \\omega_{2}, \\omega_{3}}$ (known samples), we judge the similarity between $X_{u}$ and different training sets, and finally determine the category of $X_{u}$. Obviously, it is more appropriate to match the <font color=\"green\">green unknown sample<\/font> and the <font color=\"red\">red known samples<\/font>.","a5b5af2b":"**\u261e Exercise:**","d91b5e8a":"### Accuracy<br>\n","ca85a41a":"## Implementation of KNN","13240d7f":"Here we use the scikit-learn module's `train_test_split` function to complete the dataset segmentation:\n\n```python\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test, y_train, y_test =train_test_split(train_data,train_target,test_size=0.4, random_state=0)\n```\n","353b2f1c":"### Implementation of KNN","76cbd4d1":"The dataset used this time is the lilac flower dataset `syringa.csv`, and the lilac dataset contains `3` categories such as `daphne`, `syringa` and `willow`, i.e., labels. Each of these categories contains `50`  data, each containing `4` feature values: **sepal length**, **sepal width**, **petal length** and **petal width**. Let us import it into DataFrame format using Pandas:","00797880":"---","1548f976":"As shown in the figure above, for the unknown test sample (shown in <font color='red'> ?<\/font>), the KNN algorithm is used for classification. First calculate the similarity between the unknown sample and the training sample, and then find out the nearest K adjacent samples. (`K` value is `3` in the figure, and <font color='red'>?<\/font> is circled by K nearest points.) Finally the category of the unknown sample is judged based on the nearest K samples.","f4789772":"After defining the function above, the next step is to classify the test set that is separated from the lilac dataset, inputting `x_train`, `y_train` and `x_test`, setting K value as `3`, sorting by KNN algorithm, and outputting the test results:","1f674fb4":"## K-Nearest Neighbors Algorithm","7640b9d0":"It can be seen intuitively from the figure that different K values predict different results.","37b7d290":"Here:<br>\n<br>\n- `X_train`, `X_test`, `y_train`, `y_test`: Feature training set, feature test set, label training set, label test set.<br>\n- `train_data`, `train_target`: Feature sets and label sets to be divided.<br>\n- `test_size`: If `float`, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.<br>\n- `random_state`\uff1aIf `int`, `random_state` is the seed used by the random number generator.","be49ce62":"### Introduction","71fcc2b1":"The nearest neighbour (NN) search algorithm aims to find the point in a tree that is nearest to a given input point. This search can be done efficiently by using the tree properties to quickly eliminate large portions of the search space. <br>\n<br>\nSearching for the nearest neighbour in a Kd tree proceeds as follows ([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-d_tree)): <br>\n<br>\n1. Starting with the root node, the algorithm moves down the tree recursively, in the same way that it would if the search point were being inserted (i.e., it goes left or right depending on whether the point is less than or greater than the current node in the split dimension).  <br>\n  <br>\n2. Once the algorithm reaches a leaf node, it saves that node point as the \"current best\".<br>\n  <br>\n3. The algorithm unwinds the recursion of the tree, performing the following steps at every node:<br>\n(a.) If the current node is closer than the current best, then it becomes the current best.<br>\n(b.) The algorithm checks whether there could be any points on the other side of the splitting plane that are closer to the search point than the current best. <br>\n4. When the algorithm finishes this process for the root node, then the search is complete.","cc5bfc0c":"## Summary","10782db4":"#### Manhattan Distance","7166f86d":"After implementing the KNN algorithm, we can start classifying our unknown data `[3.18, 3.15]`. Our K value is initially set to 5. Let's look at the performance of this classification:","833b14c3":"As shown in the figure, when the value of `K` is `5`, five training data closest to the test sample (as indicated by the blue circle) include `3` belonging to `B` class while `2` belonging to `A` class, and the data of the test sample is determined according to the majority voting method to be `B` class.","bbef73f8":"### Decision Rules","7164c80e":"As shown in the figure above, the data with the label `A` (<font color=\"blue\">blue<\/font> dot) is at the lower left corner, and the data with the label `B` (<font color=\"green\">green<\/font> dot) is at the upper right corner. We can see the distribution of differently labeled data. The <font color=\"red\">red x point<\/font> indicates the test data.","a6e853e7":"### Environment","3cc0ffae":"- Principle of KNN algorithm<br>\n- Common distance algorithm<br>\n- Decision rules<br>\n- Building prediction models with KNN<br>\n- Kd tree<br>\n- Implementing KNN with scikit-learn","cf3218f5":"### Load Dataset","6fedce1e":"### Calculate Distance","c4735dac":"In the previous part, we have implemented the KNN algorithm with Python. In real scenarios, we often use the KNN function in the scikit-learn library to classify the data:","c915bc29":"In order to make us more aware of the data, we also use `plt` to draw the characteristics of each data. Since the lilac flower dataset has four features, it cannot be directly represented in the two-dimensional space, so the feature distribution map can only be drawn by the feature combination. In the following code listing, the four features are combined to obtain six cases and their subgraphs are drawn:","9d1f3dea":"The KNN algorithm is very mature in theory. Its simple, easy-to-understand ideas and good classification accuracy make it widely adopted. The specific process of the algorithm mainly contains the following four steps:<br>\n<br>\n1. **Data Preparation**: Through data cleaning, data processing, each piece of data is organized into vectors.<br>\n2. **Calculate Distance**: Calculate the distance between test data and training data.<br>\n3. **Find Neighbors**: Find the K training data samples closest to the test data.<br>\n4. **Decision Classification**: According to the decision rule, the category of test data is obtained from K neighbors.<br>\n<br>\nSee the figure below:","31437bb0":"Since the dataset and features are huge, the data distribution is displayed by means of feature combination. When more features are encountered, data analysis can also be performed by reducing the dimensions of the data features, and the corresponding methods will be explained in subsequent courses.","5b5d04c0":"We denote:<br>\n<br>\n- $X$,$Y$: Two data points<br>\n- $N$: $N$ features per data<br>\n- $X_{i}$: $i$th feature of $X$ <br>\n<br>\nFirst we calculate the absolute values of the differences between the corresponding features of $X$ and $Y$, and then add the absolute values to obtain the Manhattan distance:","a986df71":"In the definition of majority voting, we imported the `operator` calculation module to sort the dictionary type structure. It can be seen from the results that the result returned by the function is `C` with the highest number of votes as `6`.","b450d578":"After learning the above steps, the KNN algorithm has got clear a bit. The following is a complete implementation of the KNN algorithm. The distance calculation for this experiment uses **Euclidean distance**, the classification decision rule is **majority voting method**, and the function `knn_classify()` is defined, where the parameters of the function include:<br>\n<br>\n- `test_data`: The input vector used for classification.<br>\n- `train_data`: The set of training samples.<br>\n- `labels`: The class label vector of the sample data.<br>\n- `k`: The number of selected nearest neighbors.","6b570c33":"```python\nfrom sklearn import neighbors\nneighbors.KNeighborsClassifier((n_neighbors=5, weights='uniform', algorithm='auto')\n```","c70ea155":"## Kd Tree","5e5cf019":"Generally the algorithm uses squared distances for comparison to avoid computing square roots. Additionally, it can save computation by holding the squared current best distance in a variable for comparison. ","21a279e5":"---","2ac9e0f4":"When the value of `K` is selected as `3`, you can see that the accuracy is not high and the classification effect is not ideal. The selection of the value of `K` has always been a hot topic and so far there is no well recognized solution. According to experience, the choice of the value of `K ` should preferably not exceed the square root of the sample size. Actually you can choose an appropriate value for `K` by traversal. Below we draw the accuracy of each of the values of `K` from `2` to `10` to get the best `K` value:","daf9188f":"When calculating the similarity between two samples, it can be expressed by calculating the distance of the feature values. If the distance between the two samples is larger, it means that the similarity between the two samples is lower. On the contrary, if the distance is smaller, the similarity between the two samples is higher.<br>\n<br>\nThere are many ways to calculate the distance. Here we introduce two most commonly used distance formulas: **Manhattan Distance** and **Euclidean Distance**. The calculations of these two distances are as follows:","76b4699c":"Below we try to complete a KNN classification process. First we generate a set of sample data with two categories (`A` and `B`), where each piece of data contains two features (`x` and `y`):","2a5d74cb":"### K-Nearest Neighbors Algorithm","83502bab":"### Model Implementation","d44c10b1":"### Prediction<br>\n","b5d85374":"---","64a6838b":"We recommend the majority voting method here, which is much simpler. You can review the second subsection of this chapter, whose figure is the majority voting method:","c1ced136":"#### Implementation of Kd Tree","dc544d21":"It can be observed that when `K=4` and when `K=6`, the model accuracy is equivalent. But in order to choose the optimal model, we consider the generalization ability of the model. So, here we choose `K=4`, which is a simpler model.","59262878":"In this chapter, we learned the principle and Python implementation of the KNN algorithm, and also completed the KNN algorithm using scikit-learn. Although the KNN algorithm is simple in concept, it performs very well in many classification or regression cases. The following knowledge points are included:<br>\n<br>\n- Principle of KNN algorithm<br>\n- Common distance algorithm<br>\n- Decision rules<br>\n- Building prediction models with KNN<br>\n- Kd tree<br>\n- Implementing KNN with scikit-learn","89fb6e53":"_Euclidean distance_ is derived from the distance formula between two points in $N$ dimensions. The expression is as follows:<br>\n<br>\n$$d_{euc}= \\sqrt{\\sum_{i=1}^{N}(X_{i}-Y_{i})^{2}}$$  <br>\n<br>\nWe denote: <br>\n<br>\n- $X$,$Y$: Two data points<br>\n- $N$: $N$ features per data<br>\n- $X_{i}$: $i$th feature of $X$ <br>\n<br>\nFirst calculate the squares of the differences between corresponding features of $X$ and $Y$, then sum them up, and finally take square root of the sum. The result so obtained is Euclidean distance:","fe90a369":"There is a video about K-Nearest Neighbor. We hope it can deepen your understanding of Supervised Learning: K Nearest Neighbour\n[Video] Logistic Regression | Source: [Statquest.org](https:\/\/statquest.org\/video-index\/)","5ff3f835":"### Nearest Neighbor Algorithm","0ae95e7a":"In the KNN algorithm, the choice of K has a great impact on the final decision of the data. Below we introduce the `ipywidgets` module to more clearly reflect the impact of K's choice on the prediction results. The `ipywidgets` module is an interactive module in `jupyter` that allows you to select different K values from the drop-down menu to predict:","e81a9ba9":"When getting a dataset for a training model, we often divide the data into two parts: one is the **training set** and the other is the **test set**. According to experience, a good segmentation method is **random segmentation** with the segmentation ratio as: `70%` being the training set and `30%` being the test set.","c1b64fed":"#### Nearest Neighbour Search for Kd Tree","db23563c":"In[ ]:","e1e8c7d1":"<h1 style=\"color:brown\">Supervised Learning: Classification","2b65da73":"The KNN algorithm is easy to understand, largely because the method used to classify the inputs is a linear scan, i.e., the input is the distance calculated with each of the training samples. Due to this, such calculations can be very time consuming when the amount of data is particularly large. In order to improve the efficiency of KNN search and to reduce the number of calculation distances, here we introduce a Kd tree.","d3f2baa1":"You may find that when the above code is run, occasionally there is a longer time cost to calculate using the Kd tree than without using the Kd tree. However, in most cases, the time cost using the Kd tree is shorter. The reason for this is that the dataset used here is small, and the division time may be not worthwhile when constructing the Kd tree structure. When the amount of data is larger, the overall search speed improvement effect is very obvious."}}