{"cell_type":{"4ace604b":"code","6310d214":"code","d0a02d1a":"code","e9dd4cf2":"code","b5d05702":"code","977cb41a":"code","87273503":"code","aff3b14d":"code","dd693431":"code","bc84c38b":"code","ca3827f3":"code","ece5dca4":"code","75343d29":"code","7fe4c792":"code","7de23a36":"code","27e0067e":"code","0a97bf78":"code","4578ad1d":"code","88379f68":"code","91987554":"code","9a100b37":"code","4de5dea6":"code","625df3f0":"code","6569d02d":"code","1b49ad07":"markdown","a9b8c035":"markdown","0bcf40a4":"markdown","ca58a4b8":"markdown","621f3eb2":"markdown","1a606d44":"markdown","4b2bca26":"markdown","380e5490":"markdown","c4ab4b1a":"markdown","374f4f9b":"markdown","c9231a15":"markdown","56cf8d8d":"markdown"},"source":{"4ace604b":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color:#eebbcb; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color:#2ca9e1; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n<\/style>","6310d214":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","d0a02d1a":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","e9dd4cf2":"SEED = 42\nNFOLDS = 10\nDATA_DIR = '\/kaggle\/input\/lish-moa\/'\nnp.random.seed(SEED)","b5d05702":"train = pd.read_csv(DATA_DIR + 'train_features.csv')\ntargets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n\ntest = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')","977cb41a":"train.dtypes","87273503":"targets.dtypes","aff3b14d":"GENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]","dd693431":"# GENES\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","bc84c38b":"#CELLS\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","ca3827f3":"train = train.drop(columns = (GENES + CELLS))\ntest = test.drop(columns = (GENES + CELLS))","ece5dca4":"#One-hot encoding for category variables\nall_x = pd.concat([train, test])\nall_x = pd.get_dummies(all_x, columns=[\"cp_type\", \"cp_dose\"])\n\n#Adjust scale\nall_x[\"cp_time\"] = all_x[\"cp_time\"]\/72\n\n#Split into the original dataset.\ntrain = all_x.iloc[:train.shape[0], :].reset_index(drop=True)\ntest = all_x.iloc[train.shape[0]:, :].reset_index(drop=True)","75343d29":"train.head()","7fe4c792":"test.head()","7de23a36":"# drop id col\nX = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].values","27e0067e":"reg = HistGradientBoostingRegressor(learning_rate = 0.013, max_depth=5, l2_regularization=0.02,\n                                    max_iter=175)\nreg = MultiOutputRegressor(reg)","0a97bf78":"oof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\n#kf = KFold(n_splits=NFOLDS)\nkf = MultilabelStratifiedKFold(n_splits=NFOLDS)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    # drop where cp_type==ctl_vehicle (baseline)\n    ctl_mask = X_train[:,-4]== 1#'ctl_vehicle'\n    X_train = X_train[~ctl_mask,:]\n    y_train = y_train[~ctl_mask]\n    \n\n    \n    reg.fit(X_train, y_train)\n    val_preds = reg.predict(X_val) # list of preds per class\n    #val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_losses.append(loss)\n    preds = reg.predict(X_test)\n    #preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds \/ NFOLDS\n    \nprint(oof_losses)\nprint('Mean OOF loss across folds', np.mean(oof_losses))\nprint('STD OOF loss across folds', np.std(oof_losses))","4578ad1d":"# set control train preds to 0\ncontrol_mask = train['cp_type_ctl_vehicle']==1\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","88379f68":"test[sub.columns[1:]] = test_preds","91987554":"test.head()","9a100b37":"sub = sub.drop(columns=sub.columns[1:]).merge(test[list(sub.columns[1:].values) + [\"sig_id\"]], on='sig_id', how='inner')","4de5dea6":"sub[sub.columns[1:]] = sub[sub.columns[1:]].clip(0, 1, axis=1)","625df3f0":"sub","6569d02d":"sub.to_csv('submission.csv', index=False)","1b49ad07":"## Save OOF preds\n","a9b8c035":"# <div class=\"h2\">Baseline<\/div>","0bcf40a4":"### PCA\n\nI refered this notebook https:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa.","ca58a4b8":"**I refered the folloing great notebook for data pipeline:**\n\nhttps:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\n\n","621f3eb2":"### Model","1a606d44":"\n### Train the imprement","4b2bca26":"# <div class=\"h1\">About this notebook<\/div>","380e5490":"## In this notebook, I create HistGradientBoostingRegressor baseline.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.HistGradientBoostingRegressor.html\n\n### <u>Note<\/u>\n\n**Update information**\n\nversion1: First release. It doesn't seem to be performing well enough, so I will be tuning it up.\n\nversion2: Add PCA dimension reduction.\n\nversion3: Tuned learing rate.\n\nversion4: Tuned max_depth.\n\nversion5: Tuned l2_regularization and , max_iter.\n\nversion6: Change kfold 5 -> 10.","c4ab4b1a":"### Scaling and Encording","374f4f9b":"### load data","c9231a15":"### Validation","56cf8d8d":"## If you like, please Upvote\ud83d\ude39"}}