{"cell_type":{"7042578d":"code","d38d1b29":"code","31baeba2":"code","32e00c6b":"code","13e609ad":"code","b3321a16":"code","b44da14b":"code","d756b6a8":"code","08d91ead":"code","71a27f42":"code","d12e50e3":"code","e848d5a9":"code","47f22c80":"code","f394eaab":"code","a4594e5a":"code","c8cfafb9":"code","3ec44075":"markdown","01a0786a":"markdown","ab3d335b":"markdown","6bbf1f28":"markdown","a520a828":"markdown","b10852b1":"markdown","678b6697":"markdown","70016bf1":"markdown","5306cadd":"markdown","aecbbca5":"markdown","c66c992a":"markdown"},"source":{"7042578d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt # import matplotlib\n%matplotlib inline\nimport seaborn as sns # seaborn data visualizer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d38d1b29":"BASE_DIR = Path('..\/input\/pima-indians-diabetes-database')","31baeba2":"diabetes = pd.read_csv(BASE_DIR \/ 'diabetes.csv')\ndiabetes.head(10)","32e00c6b":"diabetes.info","13e609ad":"features_list = list(diabetes.drop(columns='Outcome').columns)\ncolumns = list(diabetes.columns)\nprint(features_list)","b3321a16":"diabetes.dtypes","b44da14b":"diabetes.isnull().sum()","d756b6a8":"# Replace 0 values in certain columns with NumPy NaN values\ndiabetes[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = \\\ndiabetes[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n# Count how many rows have null values now, and print the amount of rows with \n# null values per column\nprint(diabetes.isnull().sum())\n\n# Select null rows\nnull_rows = diabetes[diabetes.isnull().any(axis=1)]\nprint(null_rows.head(10))","08d91ead":"pregnancies = diabetes['Pregnancies'].dropna()\nsns.histplot(x=pregnancies,bins=5)\n\nplt.show()","71a27f42":"age = diabetes['Age'].dropna()\nsns.histplot(x=age, bins=7)\n\nplt.show()","d12e50e3":"bmi = diabetes['BMI'].dropna()\nsns.histplot(x=bmi,bins=5)\n\nplt.show()","e848d5a9":"def make_bins(col_name, num_of_bins):\n    # Define the label names for the new categorical variable we are defining\n    bin_names = []\n    # Create the label names in the format '#'\n    for i in range(1,num_of_bins+1):\n        bin_names.append(i)\n    # Create a new column that puts the numerical variables into buckets\n    cutoff_values = list(plt.hist(diabetes[col_name], bins=num_of_bins)[1])\n    cutoff_values[0] = cutoff_values[0] - 0.5\n    temp_col = pd.cut(diabetes[col_name], cutoff_values, labels=bin_names)\n    # return the pandas series with the column data types turned into numeric\n    return pd.to_numeric(temp_col)\n\ndiabetes['BMI_bin'] = make_bins('BMI', 5)\ndiabetes['Age_bin'] = make_bins('Age', 7)\ndiabetes['Pregnancies_bin'] = make_bins('Pregnancies', 5)\nprint(diabetes.head(10))","47f22c80":"diabetes.dtypes","f394eaab":"from sklearn.model_selection import train_test_split\n\nX = diabetes.drop(columns=[\"BMI\",\"Pregnancies\",\"Age\",\"Outcome\"])\ny = diabetes.Outcome\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\\\n                                        test_size=0.25, random_state=1)","a4594e5a":"from sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a K-Nearest Neighbors imputer, include indicator that row was imputed\nknn_imputer = KNNImputer(add_indicator=True)\n\n# Initiate StandardScaler\nscaler = StandardScaler()\n\n# Create our Logistic model\nlogistic_model = LogisticRegression(random_state=1)\n\n# Create the pipeline\npipeline = Pipeline(steps=[('KNN_imputer', knn_imputer),\\\n                              ('scaler', scaler),('model',logistic_model)])","c8cfafb9":"from sklearn.model_selection import cross_val_score\n\nknn_scores = cross_val_score(pipeline, X_train, y_train,\n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Accuracy scores: \\n\")\nprint(knn_scores)","3ec44075":"Load the filepath that leads to the data csv file into an instantiated Path object.","01a0786a":"I will use K-Nearest Neighbors for imputation of missing values.","ab3d335b":"I have decided to discretize the above features by putting them into buckets. I chose to do this with ordinal encoding, since there is an order to each of the features.","6bbf1f28":"The data here is consists of various diagnostic details of 21 year old female patients, all belonging to the Akimel O'odham people (also known to Americans as the Pima Native Americans). Below I loaded the data from a csv file into a pandas dataframe, and printed the head of the data for examination. The \"Outcome\" column is our target variable, and the rest of the columns form the features of the dataset. ","a520a828":"# Model Training\n\nFor model training and deployment, I will first create a pipeline. I will attempt to model the data using logistic regression.","b10852b1":"# Data Cleaning and Data Wrangling\n\nFirst, I will check to see what the data types are for each column in the dataframe to make sure the data isn't malformed and is a data type that can be used for quantitative analysis.","678b6697":"There are too many observations with null values to justify just dropping the rows, so instead it might make sense to impute the data. For each feature with missing values, I will impute the data with the median for each row.","70016bf1":"# Feature Engineering\n\nI start off by collecting the features columns into a features dataframe. Below I plotted a histogram of the number of pregnancies, the ages of the patients, as well as each patient's BMI. As can be seen from the histograms below, both the Age and Pregnancies features are skewed.","5306cadd":"So we have 768 observations to work with. The code cell below prints the names of the columns that will be our features.","aecbbca5":"So there doesn't appear to be any null values in our dataset, but what is suspicious is that the insulin levels, skin thickness, and blood pressures for some patients is reported as zero, which is of course impossible. Let's examine the rows if we replace the 0 values in the Glucose, BloodPressure, SkinThickness, Insulin, and BMI columns with null values.","c66c992a":"Now to check to see if we have any missing values in the data."}}