{"cell_type":{"e8413c36":"code","a39d5213":"code","6e692a88":"code","eddc2eef":"code","d9cb4935":"code","de684b54":"code","81cf05cd":"code","4070865c":"code","6c6f3b89":"code","b826ab7a":"code","f2e28326":"code","4f07aabd":"code","a4769de3":"code","072dd682":"code","bfbf066c":"code","5f9b5406":"code","2cfb78f0":"code","4cd9f971":"code","600018b7":"code","3accfc8a":"code","6adb56ec":"code","8c4111ae":"code","c406c94b":"code","3e5aa606":"code","e005beb2":"code","b34da6c4":"code","3e682c3e":"code","e49f9c83":"code","7ea89e29":"code","05800608":"code","cc7e349f":"code","8a2d5845":"code","f73d5da0":"code","b4743e0f":"code","b7d44b2f":"code","4d2e417e":"code","02d35364":"code","896de50c":"code","5c19b6d8":"code","a99b81be":"code","3bdfc8c6":"code","455a5371":"code","13a512b3":"code","e3a2c014":"code","f24cae66":"code","f56e8b17":"code","4d1ec2d9":"code","2901d4d3":"code","af68d0d1":"code","265aee4b":"code","5ddc6e46":"code","c050051b":"code","e95c59a9":"code","a5bdff38":"code","bd389004":"code","02bec761":"code","83b03305":"code","32a8a4fa":"code","00fc38d2":"code","3059dc68":"code","cf67832f":"code","b0d0ae79":"code","e99cde6f":"code","a0598fb1":"code","b0a1b8c7":"code","8fba16ab":"code","e694a4e1":"code","21d488c7":"code","16e843cd":"code","0b7767a5":"code","23e02a51":"code","f863cf89":"markdown","678f03c9":"markdown","94ceb079":"markdown","867e1a6d":"markdown","32a483f1":"markdown","438358ca":"markdown","71c2cba9":"markdown","67276f20":"markdown","1d7b1e14":"markdown","318486a6":"markdown","48780212":"markdown","48f0dc3e":"markdown","cb851dfa":"markdown","765efad8":"markdown","f54b144b":"markdown","b0d51ffc":"markdown","42a7e54e":"markdown","25c799c8":"markdown","1c1703b5":"markdown","1664bcff":"markdown","09d84339":"markdown","b2f4bce8":"markdown","f2f78751":"markdown","0fbb9782":"markdown","5dde9c78":"markdown","294d1a68":"markdown","36486b14":"markdown"},"source":{"e8413c36":"!pip install --upgrade seaborn #Upgrade","a39d5213":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nDataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n","6e692a88":"#invite people for the Kaggle party\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","eddc2eef":"for i,col in enumerate(Dataset.columns):\n    print(f'Column {i}: {col}')","d9cb4935":"print(Dataset['SalePrice'].describe())\nprint('-'*26)\nprint(f\"Skewness: {Dataset['SalePrice'].skew()}\")\nprint(f\"Kurtosis: {Dataset['SalePrice'].kurt()}\")\nprint('-'*26)\nsns.distplot(Dataset['SalePrice']) #Where sns.displot is used to plot data distribution\n\n","de684b54":"var = 'GrLivArea' # GrLivArea: Above grade (ground) living area square feet\nDataset.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n","81cf05cd":"var = 'TotalBsmtSF' #TotalBsmtSF: Total square feet of basement area\nDataset.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","4070865c":"var = 'YearBuilt' #TotalBsmtSF: Total square feet of basement area\nDataset.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","6c6f3b89":"var = 'OverallQual' #OverallQual: Overall condition rating\ndata = pd.concat([Dataset['SalePrice'], Dataset[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data, notch = True,)\nfig = sns.swarmplot(x=var, y=\"SalePrice\", data=data, color=\".25\")\nfig.axis(ymin=0, ymax=800000);","b826ab7a":"var = 'YearBuilt' #OverallQual: Overall condition rating\ndata = pd.concat([Dataset['SalePrice'], Dataset[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data, notch = True,)\n\nfig.axis(ymin=0, ymax=800000);","f2e28326":"from __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n\n# def f(x):\n#     return x\n# interact(f, x='Hi there!');\n\n@interact(var=['OverallQual', 'YearBuilt'])\ndef g(var):\n    data = pd.concat([Dataset['SalePrice'], Dataset[var]], axis=1)\n    f, ax = plt.subplots(figsize=(8, 6))\n    fig = sns.boxplot(x=var, y=\"SalePrice\", data=data, notch = True,)\n    fig.axis(ymin=0, ymax=800000);\n    return fig\n","4f07aabd":"_ = plt.figure(0)\n\ncorrmat = Dataset.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.9, square=True)\nplt.title(f'All correlation')\n\n_ = plt.figure(2)\nf, ax = plt.subplots(figsize=(12, 9))\nk = 10 #number of variables for heatmap\nplt.title(f'Focus only {k} largest correlation')\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(Dataset[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","a4769de3":"sns.__version__","072dd682":"#scatterplot\n# cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(Dataset[cols], size = 2.5, hue=\"FullBath\", )\n#plt.show();","bfbf066c":"#missing data\nprint('Missing value')\ntotal = Dataset.isnull().sum().sort_values(ascending=False)\npercent = (Dataset.isnull().sum()\/Dataset.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data[missing_data['Total'] > 0]","5f9b5406":"# Import stat modules\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats","2cfb78f0":"type(np.newaxis)","4cd9f971":"StandardScaler().fit_transform([[1,2,0,1,5,2]])","600018b7":"Dataset['SalePrice']","3accfc8a":"Dataset['SalePrice'][:,None]","6adb56ec":"np.array([Dataset['SalePrice'].values]).T","8c4111ae":"saleprice_scaled = StandardScaler().fit_transform(Dataset['SalePrice'][:,None]);\nSalePrice = Dataset['SalePrice'][:,None]\n'''\nOutput:\nSalePrice = array([[208500],\n                   [181500],\n                   [223500],\n                   ...,\n                   [266500],\n                   [142125],\n                   [147500]]); shape = (row = 1460, col = 1) \n'''\nsaleprice_scaled = StandardScaler().fit_transform(SalePrice)\n'''\nMethod: StandardScaler()\n\nz = (x - u) \/ s\n\nwhere:\n    z is output;\n    x is input;\n    u is mean;\n    s is standard deviation\n\nOutput:\nsaleprice_scaled = array([[ 0.34727322],\n                           [ 0.00728832],\n                           [ 0.53615372],\n                           ...,\n                           [ 1.07761115],\n                           [-0.48852299],\n                           [-0.42084081]]); shape = (row = 1460, col = 1)\n'''\n\nlow_range = saleprice_scaled[\n                saleprice_scaled[:,0].argsort() #Sorts index by values ascendingly\n            ][:10] #Picks 10 lowest index\nhigh_range= saleprice_scaled[\n                saleprice_scaled[:,0].argsort() #Sorts index by values discendingly\n            ][-10:] #Picks 10 highest index\n\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)\n\n","c406c94b":"q75, q25 = np.percentile(saleprice_scaled, [75 ,25])","3e5aa606":"f, (ax_box, ax_hist) = plt.subplots(2,\n                                    sharex=True, #Share X axis\n                                    gridspec_kw={\"height_ratios\": (.15, .85)})\nsns.boxplot(\n            saleprice_scaled,\n            whis = 1.75, #IQR range\n            ax=ax_box #Where to plot\n           )\nsns.distplot(saleprice_scaled, ax=ax_hist)","e005beb2":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ng = sns.jointplot(data=Dataset, x=var, y='SalePrice', size = 7.5, kind=\"reg\")\ng_2 = g.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\ng.plot_marginals(sns.rugplot, color=\"r\", height=-.15, clip_on=False)","b34da6c4":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ng = sns.jointplot(data=Dataset, x=var, y='SalePrice', size = 7.5, kind=\"kde\")","3e682c3e":"Dataset[(Dataset['GrLivArea'] > 4000) & (Dataset['SalePrice'] < 200000)] #These two points seem outly.","e49f9c83":"Dataset = Dataset.drop(Dataset[Dataset['Id'] == 1299].index)\nDataset = Dataset.drop(Dataset[Dataset['Id'] == 524].index)","7ea89e29":"Dataset","05800608":"#histogram and normal probability plot\nfrom scipy.stats import norm\n\nsns.distplot(Dataset['SalePrice'], fit=norm);\nfig = plt.figure()\n# QQ plot\n#Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).0\nres = stats.probplot(Dataset['SalePrice'], plot=plt) ","cc7e349f":"#applying log transformation\nDataset['SalePrice_log'] = np.log(Dataset['SalePrice'])\n#transformed histogram and normal probability plot\nsns.distplot(Dataset['SalePrice_log'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(Dataset['SalePrice_log'], plot=plt)","8a2d5845":"#histogram and normal probability plot\nfrom scipy.stats import norm\n\nsns.distplot(Dataset['GrLivArea'], fit=norm);\nfig = plt.figure()\n# QQ plot\n#Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).0\nres = stats.probplot(Dataset['GrLivArea'], plot=plt) ","f73d5da0":"#applying log transformation\nDataset['GrLivArea_log'] = np.log(Dataset['GrLivArea'])\n#transformed histogram and normal probability plot\nsns.distplot(Dataset['GrLivArea_log'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(Dataset['GrLivArea_log'], plot=plt)","b4743e0f":"sns.distplot(Dataset['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\n# QQ plot\n#Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).0\nres = stats.probplot(Dataset['TotalBsmtSF'], plot=plt) ","b7d44b2f":"pd.Series(len(Dataset['TotalBsmtSF']), index = Dataset.index)","4d2e417e":"# Add a new column\nDataset['HasBsmt'] = pd.Series(0, index = Dataset.index)\nDataset['HasBsmt'][Dataset['TotalBsmtSF'] > 0] = 1 \nDataset['HasBsmt']","02d35364":"Dataset['HasBsmt'][Dataset['TotalBsmtSF'] == 0].head(5)","896de50c":"#Transform data\nDataset['TotalBsmtSF_log'] = np.log(Dataset['TotalBsmtSF'])\n\nDataset['TotalBsmtSF_log'][Dataset['TotalBsmtSF'] == 0] = 0","5c19b6d8":"# Plot only none zero value\n\nsns.distplot(Dataset['TotalBsmtSF_log'][Dataset['TotalBsmtSF_log'] > 0], fit=norm);\nfig = plt.figure()\n# QQ plot\n#Generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default).0\nres = stats.probplot(Dataset['TotalBsmtSF_log'][Dataset['TotalBsmtSF_log'] > 0], plot=plt) ","a99b81be":"f = plt.figure(figsize=(15,5))\nax1= f.add_subplot(121)\nax2 = f.add_subplot(122)\n\nax1.scatter(Dataset['GrLivArea'], Dataset['SalePrice'])\nax1.set_xlabel('SalePrice')\nax1.set_ylabel('GrLivArea')\nax1.set_title('Original Data')\n# plt.subplot(122)\nax2.scatter(Dataset['GrLivArea_log'], Dataset['SalePrice_log'])\nax2.set_xlabel('SalePrice_log')\nax2.set_ylabel('GrLivArea_log')\nax2.set_title('Log Data')","3bdfc8c6":"var = 'TotalBsmtSF'\n\nf = plt.figure(figsize=(15,5))\nax1= f.add_subplot(121)\nax2 = f.add_subplot(122)\n\nquery = Dataset['TotalBsmtSF'] > 0\n\nax1.scatter(Dataset[query][var], Dataset[query]['SalePrice'])\nax1.set_xlabel('TotalBsmtSF')\nax1.set_ylabel('GrLivArea')\nax1.set_title('Original Data')\n# plt.subplot(122)\nax2.scatter(Dataset[query][f'{var}_log'], Dataset[query]['SalePrice_log'])\nax2.set_xlabel('SalePrice_log')\nax2.set_ylabel('GrLivArea_log')\nax2.set_title('Log Data')","455a5371":"import scipy\ndef one_way_ANOVA(Dataset,col, debug = False):\n    #Cal SST\n    overall_mean = Dataset['SalePrice'].mean()\n    Dataset['overall_mean'] = overall_mean\n    SalePrice = Dataset['SalePrice']\n    overall_mean = Dataset['overall_mean']\n    SST = sum((SalePrice - overall_mean)**2)\n    if debug: print(f'SST: {SST}')\n    \n    #Cal SSW\n    group_means = Dataset[[col, 'SalePrice']].groupby(col).mean()\n    group_means = group_means.rename(columns = {'SalePrice':'group_mean'})\n    Dataset = Dataset.merge(group_means, left_on= col, right_index= True)\n    group_mean = Dataset['group_mean']\n    SSW = sum((SalePrice - group_mean)**2)\n    if debug: print(f'SSW: {SSW}')\n    \n    #Cal SSB\n    SSB = sum((Dataset['overall_mean'] - Dataset['group_mean'])**2)\n    if debug: print(f'SSB: {SSB}')\n    \n    #Cal Degree of freedom\n    k = group_means.shape[0]\n    n = Dataset.shape[0]\n    df_between_group = k - 1\n    df_within_group = n - k\n    df_total = n - 1\n    \n    #Cal Mean square\n    MSB = SSB\/df_between_group\n    MSW = SSW\/df_within_group\n    \n    #Cal F ratio\n    F = MSB\/MSW\n    \n    #Cal P value\n    p_value = 1- scipy.stats.f.cdf(F, df_between_group, df_within_group)\n    if debug:\n        print(f'F: {F}')\n    return p_value\none_way_ANOVA(Dataset, 'MSZoning', True)","13a512b3":"test_one_way_df = pd.DataFrame({'x':[1, 1, 1, 1, 1, \n                                     2, 2, 2, 2, 2, \n                                     3, 3, 3, 3, 3],\n                                'SalePrice':[210,240,270,270,300,\n                                             210,240,240,270,270,\n                                             180,210,210,210,240]}) \n#test_one_way_df \none_way_ANOVA(test_one_way_df , 'x', True)","e3a2c014":"val_Dataset = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nval_Dataset = val_Dataset.set_index('Id')\n\n# check contain nullcolumns\nlen_val_Dataset = len(val_Dataset)\n\ndrop_both_cols = []\n\nfor col in val_Dataset:\n    \n    if sum(val_Dataset[col].isnull()):\n        print(f'Columns: {col}')\n        print(f'Drop both {col}')\n        val_Dataset = val_Dataset.drop(col, axis = 1)\n        try:\n            Dataset = Dataset.drop(col, axis = 1)\n        except KeyboardInterrupt:\n            raise\n        except:\n            pass\n        drop_both_cols.append(col)\n        \n        print('=======================================')","f24cae66":"len(Dataset.columns)","f56e8b17":"for log_col in Dataset.columns[Dataset.columns.str.endswith('_log')]:\n    if log_col != 'SalePrice_log':\n        col = log_col.split('_')[0]\n        try:\n            val_Dataset[log_col] = np.log(val_Dataset[col])\n            val_Dataset = val_Dataset.drop(col, axis = 1)\n        except:\n            Dataset = Dataset.drop(log_col, axis = 1)\n","4d1ec2d9":"Dataset = Dataset.drop('SalePrice', axis = 1)","2901d4d3":"Dataset = Dataset.set_index('Id')","af68d0d1":"len(Dataset.columns)","265aee4b":"Dataset = Dataset.drop(['GrLivArea', 'HasBsmt', 'overall_mean'], axis = 1)","5ddc6e46":"Dataset = Dataset.drop(Dataset[Dataset['Electrical'].isnull()].index[0])","c050051b":"val_Dataset = val_Dataset[Dataset.drop('SalePrice_log', axis = 1).columns]","e95c59a9":"Dataset = Dataset[list(Dataset.drop('SalePrice_log', axis = 1).columns) + ['SalePrice_log']]","a5bdff38":"Dataset.head()","bd389004":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\n\nenc.fit(Dataset[['Neighborhood']])\n# for col in Dataset.columns:\n#     if Dataset[col].dtype.type == np.object_ :\n#         print(col)","02bec761":"for col in Dataset.columns:\n    if Dataset[col].dtype.type == np.object_ :\n        print(col)\n        \n        enc = OneHotEncoder(handle_unknown='ignore')\n        enc.fit(Dataset[[col]])\n        \n        new_col = pd.DataFrame(enc.transform(Dataset[[col]]).toarray(), columns=f'{col}_' + enc.categories_[0])\n        ","83b03305":"'Neighborhood_' + enc.categories_[0]","32a8a4fa":"new_cols = pd.DataFrame(enc.transform(Dataset[['Neighborhood']]).toarray(), columns='Neighborhood_' + enc.categories_[0])\n\nnew_cols.index = Dataset.index\n\ntest = pd.concat([Dataset, new_cols], axis = 1)\n\ntest.head()","00fc38d2":"new_val_cols = pd.DataFrame(enc.transform(val_Dataset[['Neighborhood']]).toarray(), columns='Neighborhood_' + enc.categories_[0])\n\nnew_val_cols.index = val_Dataset.index\n\nval_test = pd.concat([val_Dataset, new_val_cols], axis = 1)","3059dc68":"val_test.head()","cf67832f":"sum(sum(test.isnull().values))","b0d0ae79":"import pickle\nfor col in val_Dataset:\n    if Dataset[col].dtype.type == np.object_ :\n        print(col)\n        \n        enc = OneHotEncoder(handle_unknown='ignore')\n\n        enc.fit(Dataset[[col]])\n        \n        new_cols = pd.DataFrame(enc.transform(Dataset[[col]]).toarray(),\n                                columns=f'{col}_' + enc.categories_[0])\n\n        new_cols.index = Dataset.index\n\n        Dataset = pd.concat([Dataset, new_cols], axis = 1)\n        \n        Dataset = Dataset.drop(col, axis = 1)\n        \n        new_val_cols = pd.DataFrame(enc.transform(val_Dataset[[col]]).toarray(),\n                                    columns=f'{col}_' + enc.categories_[0])\n\n        new_val_cols.index = val_Dataset.index\n\n        val_Dataset = pd.concat([val_Dataset, new_val_cols], axis = 1)\n        \n        val_Dataset = val_Dataset.drop(col, axis = 1)\n        \n        pickle.dump(enc, open(f'{col}_one_hot_pickle', 'wb'))","e99cde6f":"Dataset['SalePrice_log']","a0598fb1":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(Dataset.drop('SalePrice_log', axis = 1))\n\nscaled_dataset = pd.DataFrame(scaler.transform(Dataset.drop('SalePrice_log', axis = 1)), columns = Dataset.drop('SalePrice_log', axis = 1).columns)\n\nscaled_dataset.index = Dataset.index\n\nscaled_dataset = pd.concat([scaled_dataset, Dataset[['SalePrice_log']]] ,axis = 1)","b0a1b8c7":"scaled_val = pd.DataFrame(scaler.transform(val_Dataset), columns = val_Dataset.columns)","8fba16ab":"scaled_val.to_csv('test_scaled.csv')\nscaled_dataset.to_csv('train_scaled.csv')","e694a4e1":"pickle.dump(scaler, open(f'scaler', 'wb'))","21d488c7":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression().fit(Dataset.drop('SalePrice_log', axis = 1), Dataset[['SalePrice_log']])\n\npredict = reg.predict(val_Dataset)","16e843cd":"np.e**(np.log(12))","0b7767a5":"predict = pd.DataFrame(predict)\npredict.index = val_Dataset.index\npredict.columns = ['SalePrice_log']\npredict['SalePrice'] = np.e**(predict['SalePrice_log'])\npredict = predict.drop('SalePrice_log', axis = 1)\npredict\n","23e02a51":"predict.to_csv(\"submission.csv\")","f863cf89":"## Statistics analytics\n* Correlation matrix\n* SalePrice correlation matrix\n* Scatter plots between the most correlated variables","678f03c9":"#### GrLivArea","94ceb079":"### Normality\n#### SalePrice","867e1a6d":"* They're slightly linear","32a483f1":"* It looks more normal.","438358ca":"#### Notation\n* GrLivArea (Above grade (ground) living area square feet) and TotalBsmtSF (Total square feet of basement area) seems to be linear border.\n* SalePrice and YearBuilt show the exponential border\n\n## Missing data","71c2cba9":"# Analyse SalePrice","67276f20":"## Import data","1d7b1e14":"## SalePrice statictis analytics\n\n* Normality \n* Homoscedasticity\n* Linearity\n* Absence of correlated errors","318486a6":"### Homoscedasticity\n\n* Homoscedasticity\n\n$$\n\\text{Var}(u_i|x_i) = \\sigma^2\n$$\nwhere $u_i$ is the error at $i$ (remains constand with $\\sigma^2$)\n* Heteroscedasticity\n\n$$\n\\text{Var}(u_i|x_i) = \\text{f}(x_i)\n$$\n\nwhere $u_i$ is the error at $i$ (depends on $x_i$)\n","48780212":"### Scatter plots between the most correlated variables","48f0dc3e":"### SalePrice distribution","cb851dfa":"### One hot encoder","765efad8":"### Correlation matrix","f54b144b":"### Outlier\n#### Univariate analysis","b0d51ffc":"* The regression results suggests GrLivArea and TotalBsmtSF have linear relation to SalePrice\n* The categate analyse results suggests OverallQual and YearBuilt have linear relation to SalePrice","42a7e54e":"### Relationship with numerical variables","25c799c8":"* TotalBsmtSF could be 0 which cannot use log","1c1703b5":"# \u0e17\u0e14\u0e25\u0e2d\u0e07\u0e17\u0e33 EDA \u0e01\u0e31\u0e1a House price :)","1664bcff":"# Test analytics","09d84339":"* As Pmarcelino mentions, we better drop those columns whose data is missing by 15%.\n* Columns start with Garage relate to GarageCars\n* Columns start with Bsmt relate to TotalBsmtSF\n* Columns start with MasVnr relate to  'YearBuilt' and 'OverallQual'","b2f4bce8":"### Relationship with categorical features","f2f78751":"* Test with data from https:\/\/stattrek.com\/anova\/completely-randomized\/one-way-example.aspx","0fbb9782":"### (Special) One way ANOVA","5dde9c78":"### Bivariate analysis","294d1a68":"* It shows they are slightly normal but shewed","36486b14":"* It's less linear than before"}}