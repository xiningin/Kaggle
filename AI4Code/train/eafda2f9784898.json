{"cell_type":{"7206e6d6":"code","637b6b9d":"code","b572c4e2":"code","20049a49":"code","cdd0939e":"code","2bf7f847":"code","4679e587":"code","ca5f199e":"code","3511386a":"code","10187e54":"code","18643e1b":"code","7409434d":"code","38765c33":"code","3a297b96":"markdown","78e8db44":"markdown","f48598d5":"markdown","9f40c669":"markdown","b498715e":"markdown","53c6175e":"markdown"},"source":{"7206e6d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","637b6b9d":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col=0)\ndf.head()","b572c4e2":"#function to clean data based on proportion of missing data in columns\ndef cleaning_data(df):\n    \"\"\"drops the columns if major proportion (>60%) of data is missing \n    and fills the columns if minor proportion of data is missing \n    \n    Args: df - dataframe to impute NaN values or drop columns\n    \n    Returns: cleaned_df - cleaned dataframe either with imputed or dropped columns\"\"\"\n    \n           \n    #record df columns, total non-NaN values, total records in df\n    col_list = df.columns.tolist()\n    val_count = df.notnull().sum().tolist()\n    df_len = len(df)\n    \n    #define threshold limit for NaN values in column\n    threshold = int(df_len*0.6)\n    \n    #iterate the dictionary and drop\/impute columns based on proportion of NaNs\n    for i in val_count:\n        if i < threshold:\n            col = val_count.index(i)\n            cleaned_df = df.drop(col_list[col], axis=1)\n        else:\n            cleaned_df = df.fillna(method = 'ffill')\n    \n\n    return cleaned_df\n\n\ncleaned_data = cleaning_data(df)\ncleaned_data.head()","20049a49":"def noimpact_cols(df):\n    \"\"\"to drop the list of columns stated as non-contributing columns\n    \n    Args: df - cleaned Dataframe\n          cols - list of columns defined by user as non-contributers  \n    \n    Returns: filtered_df - Dataframe without non-contributing columns\n    \"\"\"\n    cols = ['Name','Ticket']\n    filter_df = df.drop(cols, axis=1)\n    \n    return filter_df\n\n\nfiltered_df = noimpact_cols(df)\nfiltered_df.head()","cdd0939e":"def visualize_survival(df):\n    \"\"\"Visualizing survival of boarded passengers by using Pandas grouping \n    and Matplotlib for understanding data and high-impact Features\n    Args: df - cleaned Dataframe\n    \n    Returns: Bargraph suggesting survival rate w.r.t predictors\n    \"\"\"\n    #import relevant plotting package\n    import matplotlib.pyplot as plt\n    \n    #seperate features based on datatype for grouping by value_counts()\n    cat_cols = df.select_dtypes(exclude=['float64']).columns.tolist()\n    float_cols = df.select_dtypes(exclude=['object', 'int64']).columns.tolist()\n    \n    #plotting visualizations for cat_cols\n    for col in cat_cols:\n        #group by Feature and calculate survivors \n        df.groupby(col)['Survived'].sum().plot(kind='bar', color='r', label='Survived')\n        df[col].value_counts().plot(kind='bar', color='g', alpha=0.3, label='Boarded')\n        \n        #labeling the axes\n        plt.ylabel('Survived')\n        plt.xlabel(col)\n        plt.ylim(0, 900)\n        plt.legend()\n        plt.show()\n        \n    #plotting visualizations for float_cols   \n    for col in float_cols:\n        #categorizing and labeling float values based on their quantiles, i.e., [0, 0.25, 0.50, 0.75, 1]\n        label = ['Low', 'Medium', 'High', 'VeryHigh']\n        df['dup'] = pd.qcut(df[col], 4, labels=label)\n        \n        #group by categorized dup_Features and calculate survivors\n        df.groupby('dup')['Survived'].sum().sort_values(ascending=True).plot(kind='bar', color='r', label='Survived')\n        df['dup'].value_counts().sort_values(ascending=True).plot(kind='bar', color='g', alpha=0.3, label='Boarded')\n        df.drop('dup', axis=1, inplace=True)\n        \n        #labeling the axes\n        plt.ylabel('Survived')\n        plt.xlabel(col)\n        plt.ylim(0, 900)\n        plt.legend()\n        plt.show()\n    \n            \n    \n    return df\n\nvisualize_survival(filtered_df)\nfiltered_df.head()","2bf7f847":"# Import models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n#Import other neccesary packages\nfrom sklearn.metrics import precision_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer, OneHotEncoder\nfrom sklearn.impute import SimpleImputer","4679e587":"# Splitting Features and Labels  \ny = df['Survived']\nX = df.drop('Survived', axis=1)\nSEED = 14 #to define random_state of all neccesary functions and reproducibility\n\n#splitting training data and testing data using train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)","ca5f199e":"#Initialize Classifier\nknn = KNeighborsClassifier()\n\nsvc = SVC(C=0.8)\n\nlr = LogisticRegression(penalty='l2', random_state=SEED, \n                        n_jobs=-1,\n                        max_iter=300)\n\ndt = DecisionTreeClassifier(max_depth=5,\n                            max_features='auto',\n                            random_state=SEED)\n\n#list of classifiers\nclassifiers = [('K Nearest Neighbors', knn), ('SVC', svc),\n               ('Logistic Regression', lr), ('Decision Tree', dt)]\n\nvc = VotingClassifier(estimators = classifiers, n_jobs=-1)","3511386a":"def encode_eval(model, feature_df, label_df):\n    \"\"\"For evaluating the model accuracy for two different stages using cross_val_score: \n    1 - to assess the mean_absolute_error with and without category columns\n    2 - to assess the model precision using AUC score\n    \n    Args: model - classifier used to predict the data\n          feature_df - Dataframe consisting of predictor columns\n          label_df - Dataframe consisting of target column\n          \n    Returns: Scoring:\n                1 - mean_absolute_error score with and without category columns\n                2 - AUC score of model after cross-validation\n    \"\"\"\n    \n    #clean and filter data \n    clean_df = cleaning_data(feature_df)\n    filter_df = noimpact_cols(clean_df)\n        \n    #Encoding category data\n    onehot_encoded_data = pd.get_dummies(filter_df)\n    data_without_categories = filter_df.select_dtypes(exclude=['object'])\n\n    #Evaluating MAE of df_with_encoded_categorical_data vs df_without_category_data\n    mae_onehot_encoded = cross_val_score(model, onehot_encoded_data, label_df, scoring='neg_mean_absolute_error', cv=10).mean()*-1\n    mae_without_categories = cross_val_score(model, data_without_categories, label_df, scoring='neg_mean_absolute_error', cv=10).mean()*-1\n    print('MAE for data with one-hot encoding of category columns: ', mae_onehot_encoded)\n    print('MAE for data without category columns: ', mae_without_categories)\n    \n    #ROC-AUC scoring\n    cv_score = cross_val_score(model, onehot_encoded_data, label_df, cv=10, scoring='roc_auc').mean()\n        \n    return cv_score\n\n\nfor clf_name, clf in classifiers:\n    cv_rating = encode_eval(clf, X_train, y_train)\n    print('CV score of {} using AUC: {} \\n'.format(clf, cv_rating))","10187e54":"#clean and filter Training and Validation data\nTrain_new = noimpact_cols(cleaning_data(X_train))\nTest_new = noimpact_cols(cleaning_data(X_test))\n\n\n#Defining the Pipeline\n\n#for clf in classifiers:   \n\npipe = Pipeline([\n                    ('fill_NaN', SimpleImputer(strategy='most_frequent')),\n                    ('encode', OneHotEncoder(handle_unknown='ignore')),\n                    ('scale', Normalizer()),\n                    ('classify', vc)\n        ])\n\n\npipe.fit(Train_new, y_train)\ny_pred = pipe.predict(Test_new)\nscore = precision_score(y_test, y_pred)\nprint('True postitive score of Model: ',score)\n    #print('True postitive Score of {}: {}'.format(clf, score))","18643e1b":"#Test Data\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv', index_col=0)\ntest_data.info()","7409434d":"#Cleaning Test Data\ntest_clean = noimpact_cols(cleaning_data(test_data))\ntest_clean.info()","38765c33":"#Predicting Test data\nprediction = pipe.predict(test_clean)\n\n#Preparing prediction.csv file\nprediction_df = pd.DataFrame(columns=['Survived'], index=test_clean.index, data=prediction)\nprediction_df.to_csv('Predictions_Pipeline_EnsembleModel.csv')","3a297b96":"**NOTES_To_Self**\n* FOR Code Iteration:\n    1. LOOPS, ex. for, while, if \n    2. Anonymous FUNCTIONS, ex. lambda\n    3. Defined FUNCTIONS, ex. def\n    4. Classes","78e8db44":"**Visualization of Features w.r.t Survival**","f48598d5":"# Data Organizing\n* Filling\/Dropping NaN's\n* Dropping no-impact columns\n* Understanding Data","9f40c669":"**Modeling**\n* Split data into train and test for validating model\n* Initialize Classifier\n* Generate dummy variables for categorical\/object columns\n* Cross-validate and calculate scores\n* Pipeline the Model","b498715e":"# PIPELINE the Model\n* define common functions\n* align the defined_functions in Pipeline using FunctionTransformer and FeatureUnion\n* focus on Model reproducibility for any classifier","53c6175e":"# Testing the Model"}}