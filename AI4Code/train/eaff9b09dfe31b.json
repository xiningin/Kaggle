{"cell_type":{"a5f07d20":"code","d6dd07de":"code","25486d79":"code","2299cc7f":"code","3b10277a":"code","a2994ef8":"code","ef8456c2":"code","7f4e15a7":"code","4db2708d":"code","ce813a3c":"code","9fd53aac":"code","c11d07c6":"code","e8c312aa":"code","385c18c9":"code","0ee30f42":"code","4459f9bc":"code","4e6de18a":"code","5ff88ca5":"code","d8efaff5":"code","bf80db88":"code","b6c96dc6":"code","a17164ea":"markdown","95d602e9":"markdown","e24adcc3":"markdown","342d5470":"markdown","a0d7adc7":"markdown","16bd2b99":"markdown","483b0100":"markdown","d52e4c27":"markdown","c713fb8c":"markdown","f78c8fd7":"markdown","dccd57e2":"markdown"},"source":{"a5f07d20":"#Importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pdp\n%matplotlib inline","d6dd07de":"#Importing data\n\ndf_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","25486d79":"df_train.head()","2299cc7f":"sns.countplot(df_train['target'])","3b10277a":"df_train['keyword'].isnull().sum()","a2994ef8":"df_train['location'].isnull().sum()","ef8456c2":"df_train.drop(['id','keyword','location'],axis=1,inplace=True)\ndf_test.drop(['keyword','location'],axis = 1,inplace=True)","7f4e15a7":"Report = pdp.ProfileReport(df_train,title = 'Report',minimal=True)\nReport","4db2708d":"import nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer","ce813a3c":"stemmer = LancasterStemmer()\ncorpus = []","9fd53aac":"for i in range(0,df_train.shape[0]):\n\n    Disaster = re.sub(pattern='[^a-zA-Z]',repl=\" \",string=df_train.text[i])\n\n    Disaster = Disaster.lower()\n\n    Disaster = Disaster.split()\n\n    Disaster = [stemmer.stem(words) for words in Disaster if not words in set(stopwords.words('english'))]\n\n    Disaster = ' '.join(Disaster)\n\n    corpus.append(Disaster)","c11d07c6":"corpus[0:10]","e8c312aa":"from sklearn.feature_extraction.text import CountVectorizer\n\nCV = CountVectorizer()\nX = CV.fit_transform(corpus).toarray()\n\ny = pd.get_dummies(df_train['target'])\ny = y.iloc[:, 1].values","385c18c9":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)","0ee30f42":"from sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train,y_train)","4459f9bc":"y_pred = classifier.predict(X_test)","4e6de18a":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True)","5ff88ca5":"score = accuracy_score(y_test, y_pred)\nprint('The Accuracy of this model is',round(score*100,2))","d8efaff5":"\n\ntest_corpus = []\nfor i in range(0,df_test.shape[0]):\n\n    Disaster = re.sub(pattern='[^a-zA-Z]',repl=\" \",string=df_test.text[i])\n\n    Disaster = Disaster.lower()\n\n    Disaster = Disaster.split()\n\n    Disaster = [stemmer.stem(words) for words in Disaster if not words in set(stopwords.words('english'))]\n\n    Disaster = ' '.join(Disaster)\n\n    test_corpus.append(Disaster)","bf80db88":"X_test = CV.transform(test_corpus).toarray()\ny_pred = classifier.predict(X_test)","b6c96dc6":"Submission = pd.DataFrame(data = y_pred, columns = ['target'])\nSubmission.head()\nSubmission.to_csv('my_submission.csv',header=True,index=False)","a17164ea":"# Submission","95d602e9":"# Predicting the results","e24adcc3":"# Predicting final result","342d5470":"# Creating bag of words","a0d7adc7":"# Spliting the data","16bd2b99":"# Importing NLP","483b0100":"# Model Creation","d52e4c27":"# Cleaning test data","c713fb8c":"# EDA","f78c8fd7":"# Droping null data","dccd57e2":"# Cleaning the data"}}