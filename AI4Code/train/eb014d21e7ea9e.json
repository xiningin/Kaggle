{"cell_type":{"599971fe":"code","4696f546":"code","78b6e499":"code","bdee4c95":"code","92444407":"code","12bfacfd":"code","072e0b97":"code","b12e42e7":"code","54a1192d":"code","5146dfad":"code","3012d1d7":"code","6bd5b2c2":"code","1447af70":"code","d94b2eaa":"code","813a94d8":"code","d4e96b87":"code","d5a9eb50":"markdown","8eff15f0":"markdown","6939f01e":"markdown","7bb9c847":"markdown","d4119577":"markdown","a497af21":"markdown","75cb9c13":"markdown","0c4b4808":"markdown","7f71d88c":"markdown","a513feb4":"markdown","021d1b2f":"markdown","3faf8da6":"markdown","28878e88":"markdown","48276a41":"markdown","d0dcff61":"markdown","d4394bfd":"markdown","65010732":"markdown","b1653185":"markdown","f94ff702":"markdown"},"source":{"599971fe":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nnp.random.seed(500)\nimport warnings\nfrom sklearn.utils.testing import ignore_warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=ConvergenceWarning)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(ngram_range=(1,3))\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.svm import LinearSVC\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","4696f546":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","78b6e499":"df = pd.read_json(os.path.join(dirname, filename),lines=True)\ndf.head()","bdee4c95":"df.info()","92444407":"labels = list(df.category.unique())\nlabels.sort()\nprint(labels)","12bfacfd":"plt.figure(figsize=(14,6))\ndf.category.value_counts().plot(kind='bar')\nplt.show()","072e0b97":"df.category[(df['category']=='ARTS') | (df['category']=='CULTURE & ARTS')]='ARTS & CULTURE'\ndf.category[df['category']=='PARENTS']='PARENTING'\ndf.category[df['category']=='STYLE']='STYLE & BEAUTY'\ndf.category[df['category']=='THE WORLDPOST']='WORLDPOST'","b12e42e7":"labels = list(df.category.unique())\nlabels.sort()\nprint(labels)\nplt.figure(figsize=(14,6))\ndf.category.value_counts().plot(kind='bar')\nplt.show()","54a1192d":"def preprocessing(col,h_pct=1,l_pct=1):\n    '''\n    Cleans the text in the input column\n\n    Parameters\n    ----------\n    col : pandas.core.series.Series\n        The column which needs to be processed\n    h_pct : float (default = 1)\n        The percentage of high frequency words to remove from the corpus\n    l_pct : float (default = 1)\n        The percentage of low frequency words to remove from the corpus\n    \n    Returns\n    -------\n    cleaned text series\n    '''\n    #Lower case\n    lower = col.apply(str.lower)\n    \n    #Removing HTML tags\n    import html\n    rem_html = lower.apply(lambda x: x.replace('#39;', \"'\").replace('amp;', '&')\n                             .replace('#146;', \"'\").replace('nbsp;', ' ').replace('#36;', '$')\n                             .replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br \/>', \" \")\n                             .replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.')\n                             .replace(' @-@ ','-').replace('\\\\', ' \\\\ ').replace('&lt;','<')\n                             .replace('&gt;', '>'))\n    \n    #Lemmatizing\n    from nltk.corpus import wordnet\n    from nltk.stem import WordNetLemmatizer\n    \n    #Stemming\n    from nltk.stem import SnowballStemmer\n    stem = SnowballStemmer('english')\n    stemmed = rem_html.apply(lambda x: ' '.join(stem.stem(word) for word in str(x).split()))\n    \n    #removing punctuation\n    import re\n    rem_punc = stemmed.apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n    \n    #removing stopwords and extra spaces\n    from nltk.corpus import stopwords\n    stop_words = set(stopwords.words('english'))\n    rem_stopwords = rem_punc.apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n    \n    #removing numbers\n    rem_num = rem_stopwords.apply(lambda x: \" \".join(x for x in x.split() if not x.isdigit()))\n    \n    #remove words having length=1\n    rem_lngth1 = rem_num.apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n    \n    if h_pct != 0:\n        #removing the top $h_pct of the most frequent words \n        high_freq = pd.Series(' '.join(rem_lngth1).split()).value_counts()[:int(pd.Series(' '.join(rem_lngth1).split()).count()*h_pct\/100)]\n        rem_high = rem_lngth1.apply(lambda x: \" \".join(x for x in x.split() if x not in high_freq))\n    else:\n        rem_high = rem_lngth1\n    \n    if l_pct != 0:\n        #removing the top $l_pct of the least frequent words\n        low_freq = pd.Series(' '.join(rem_high).split()).value_counts()[:-int(pd.Series(' '.join(rem_high).split()).count()*l_pct\/100):-1]\n        rem_low = rem_high.apply(lambda x: \" \".join(x for x in x.split() if x not in low_freq))\n    else:\n        rem_low = rem_high\n    \n    return rem_low","5146dfad":"counts = pd.Series(' '.join(df.short_description).split()).value_counts()\ncounts","3012d1d7":"high_freq = counts[:int(pd.Series(' '.join(df.short_description).split()).count()*1\/100)]\nhigh_freq","6bd5b2c2":"low_freq = counts[:-int(pd.Series(' '.join(df.short_description).split()).count()*1\/100):-1]\nlow_freq","1447af70":"df.loc[df.short_description.str.len() == df.short_description.str.len().max()]","d94b2eaa":"df.loc[58142]['short_description']","813a94d8":"%matplotlib inline\ndef prep_fit_pred(df, h_pct, l_pct, model, verbose=False):\n    '''\n    Takes the dataframe, and returns asset tag predictions for the stories\n\n    Parameters\n    ----------\n    col : pandas.core.frame.DataFrame\n    h_pct : float\n        The percentage of high frequency words to remove from the corpus\n    l_pct : float\n        The percentage of low frequency words to remove from the corpus\n    model : the model which will be used for predictions\n    verbose : boolean (default: False)\n        Verbosity of the output. True = all outputs, False = no outputs\n            \n    Returns\n    -------\n    preds : pandas.core.series.Series\n        Column with the predicted asset class\n    acc : float\n        Accuracy of the predictions on the test set\n    model : the trained model\n    '''\n    \n    df['short_description_processed'] = preprocessing(df['short_description'],h_pct,l_pct)\n    df['concatenated'] = df['headline'] + '\\n' + df['short_description_processed']\n    #not removing high and low frequency words from headline\n    #this is because the headline carries more significance in determining the classification of the news\n    df['concat_processed'] = preprocessing(df['concatenated'],0,0)\n    \n    if verbose:\n        print('Number of words in corpus before processing: {}'\n              .format(df['short_description'].apply(lambda x: len(x.split(' '))).sum()))\n        print('Number of words in corpus after processing: {} ({}%)'\n              .format(df['short_description_processed'].apply(lambda x: len(x.split(' '))).sum()\n                     , round(df['short_description_processed'].apply(lambda x: len(x.split(' '))).sum()*100\\\n                             \/df['short_description'].apply(lambda x: len(x.split(' '))).sum())))\n        print('Number of words in final corpus: {} ({}%)'\n              .format(df['concat_processed'].apply(lambda x: len(x.split(' '))).sum()\n                     , round(df['concat_processed'].apply(lambda x: len(x.split(' '))).sum()*100\\\n                             \/df['short_description'].apply(lambda x: len(x.split(' '))).sum())))\n\n        print('\\nRaw story:\\n{}'.format(df['short_description'][58142]))\n        print('\\nProcessed story:\\n{}'.format(df['short_description_processed'][58142]))\n        print('\\nAdding additional columns to story:\\n{}'.format(df['concatenated'][58142]))\n        print('\\nFinal story:\\n{}'.format(df['concat_processed'][58142]))\n\n    X = df['concat_processed']\n    y = df['category']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, \n                                                    stratify=y) \n    \n    bow_xtrain = bow.fit_transform(X_train)\n    bow_xtest = bow.transform(X_test)\n\n    model.fit(bow_xtrain,y_train)\n    preds = model.predict(bow_xtest)\n\n    acc = accuracy_score(y_test,preds)*100\n    \n    if verbose:\n        print('\\nPredicted class: {}'.format(preds[58142]))\n        print('Actual class: {}\\n'.format(y_test.iloc[58142]))\n        plt.figure(figsize=(14,14))\n        sns.heatmap(confusion_matrix(y_test,preds),cbar=False,annot=True,square=True\n               ,xticklabels=labels,yticklabels=labels,fmt='d')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.yticks(rotation=0)\n        plt.show()\n        print(classification_report(y_test,preds))\n        print('Accuracy: {0:.2f}%'.format(acc))\n\n    return preds, acc, model","d4e96b87":"preds_abc, acc_abc, abc = prep_fit_pred(df, 0, 1, LinearSVC(), verbose=True)","d5a9eb50":"This cell finds the optimal values of h_pct and l_pct. We start with integer values from 0 to 10, and based on the results, can further tune the percentages to 0.5% steps.\n\nIts takes a while to run (ok, more than a while), so I'll just paste the results I achieved in my local.\n\nThe first iteration returned values of 0.0 and 1.0 for h_pct and l_pct respectivaly, the below are the results when running for vallues between 0.0 to 0.5% for h_pct and 0.5 to 1.5% for l_pct","8eff15f0":"**The below cell has been converted to markdown because of the time it takes to run.\nChange to code for reference**","6939f01e":"These are the top 1% of the least frequent words. All of these words occur only once in the vocabulary, and thus don't have much significance and can be removed","7bb9c847":"#Importing libraries","d4119577":"**This is my first kernel on kaggle. Any feedback will be GREATLY appreciated.\nThanks a lot! :)**","a497af21":"#EDA","75cb9c13":"#Preprocessing","0c4b4808":"There are no NULL entries in the dataset, which is good!","7f71d88c":"*Apologies for the labels (this is my first kernel on Kaggle).*\n\nThe optinal values are still 0.0 and 1.0 for h_pct and l_pct respectively. We'll go ahead with these values","a513feb4":"These are the top 1% of the most frequent words.","021d1b2f":"This one function takes the dataframe, the high and low percet words to remove, the model and a verbosity flag as inputs, and returns the  predictions, model accuracy, and the trained model","3faf8da6":"This is a standard text preprocessing UDF which I use.\nMost of the functions here are self-explanatory, except the h_pct and l_pct parts.\nLet me elaborate a bit on this...","28878e88":"These are the counts of all the words\/tokens in the dataset.\nWe will remove a percentage of the most and least frequent words.\n(This is the last step of the preprocessing function, so stopwords, and punctuation will already be removed by then)","48276a41":"This is the longest story in our dataset. We will use this as reference to see how our preprocessing function works","d0dcff61":"#Checking for optimum h_pct and l_pct combination\n%matplotlib inline\n\nimport warnings\nfrom sklearn.utils.testing import ignore_warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=ConvergenceWarning)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(ngram_range=(1,3))\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.svm import LinearSVC\n\npct=[]\nsvc_max, svc_max_pct, acc_svc = 0, '', []\n\nfor i in tqdm_notebook(np.linspace(0,10,11)):\n    for j in tqdm_notebook(np.linspace(0,10,11)):\n        \n        pct.append(str(i)+'|'+str(j))\n        \n        preds, acc, _ = prep_fit_pred(df, i, j, LinearSVC())\n        acc_svc.append(acc)\n        if acc > svc_max:\n            svc_max = acc\n            svc_max_pct = str(i)+'|'+str(j)\n\nprint('SVC max: {}%, pct:{}'.format(svc_max,svc_max_pct))\n\nplt.figure(figsize=(18,10))\nax = plt.axes()\nplt.plot(pct,acc_svc,label='SVC',marker='v')\nplt.xlabel('pct')\nplt.ylabel('accuracy')\nplt.legend()\nplt.setp(ax.get_xticklabels(), rotation=90, horizontalalignment='right', fontsize='x-small')\nplt.show()","d4394bfd":"These are all the categories in the dataset. \nSome of these seem to be repeated, like ARTS, CULTURE & ARTS, and ARTS & CULTURE. We will merge these together","65010732":"> SVC max: 63.79560061555173%, pct:0.0|1.0\n![image.png](attachment:image.png)![](http:\/\/)","b1653185":"#Model building","f94ff702":"#Putting it together"}}