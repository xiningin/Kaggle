{"cell_type":{"dd964c17":"code","a8519ba8":"code","7fb938eb":"code","4cd0704b":"code","d94b3b7f":"code","854a43a6":"code","b648edf6":"code","b8d8ea95":"code","dcbae30d":"code","a24d25a2":"code","acce02be":"code","e6d01ef0":"code","eefde350":"code","3d978ac2":"code","b43052a1":"code","2bc14517":"code","b2d66260":"code","2731e01a":"code","a881555f":"code","429ec389":"code","bb5c54b6":"code","483ad399":"code","58dfa3d9":"code","8be88620":"code","a168142f":"code","fc456ee4":"code","7da4ef4e":"code","389d097d":"code","12f13c18":"markdown","81595bbe":"markdown","05c87f44":"markdown","610733f9":"markdown","1151003d":"markdown","3209403a":"markdown","02c9f388":"markdown","52f30065":"markdown","8491db0a":"markdown","a998f13a":"markdown","a90cad93":"markdown"},"source":{"dd964c17":"## install dependencies\n# !pip install datasets==1.1.2 pytorch_lightning==1.0.3 wandb==0.10.8 transformers==3.4.0","a8519ba8":"# utils \nimport os\nimport gc\nimport tqdm\nimport torch\nimport pandas as pd\n\n# data\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom torch.utils.data import random_split, Dataset, DataLoader\n\n\n\n# model\nfrom transformers import AutoModel\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\n# training and evaluation \nimport wandb\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ProgressBar, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt","7fb938eb":"# device  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# seed\ntorch.manual_seed(42)\n","4cd0704b":"class NewsDataset(Dataset):\n    \"Custom Dataset class to create the torch dataset\"\n    \n    def __init__(self, root_dir, tokenizer,  max_len=128):\n        \"\"\"\n            root_dir: path where data is residing\n            tokenizer: tokenizer will be used to tokenize the text\n            max_len: max_len for text, padding\/trimming will be applied to follow this rule\n        \"\"\"\n        \n        self.tokenizer = tokenizer\n        \n        self.data = load_dataset(\"csv\", data_files=[os.path.join(root_dir, \"Fake.csv\"), os.path.join(root_dir, \"True.csv\")])['train']\n        \n        \n        self.text = self.data['title']\n        self.label = self.data['label']\n                        \n        self.max_len = max_len\n                                        \n        \n        \n    def __len__(self):\n        \"__len__ function returns the size of the data =\"\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n            idx: index of the data to retrieve\n\n            returns: A dictionary containing input ids based on tokenizer's vocabulary, attention mask and label tensors \n        \"\"\"\n        \n        text = self.text[idx]\n        label = self.label[idx]\n        \n        input_encoding = self.tokenizer.encode_plus(\n            text=text,\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n            padding=\"max_length\",\n        )\n        \n        return {\n            \"input_ids\":input_encoding['input_ids'].squeeze(),\n            \"attention_mask\":input_encoding['attention_mask'].squeeze(),\n            \"label\":torch.tensor([label], dtype=torch.float)\n        }","d94b3b7f":"class Model(nn.Module):\n\n    \"\"\" \n        Fake News Classifier Model\n        A pretrained model is used as for contextualized embedding and a classifier on top of that. \n    \n    \"\"\"\n\n    def __init__(self, model_name, num_classes=2):\n        \"\"\"\n            model_name:  What base model to use from hugginface transformers\n            num_classes: Number of classes to classify. This is simple binary classification hence 2 classes\n        \"\"\"\n        super().__init__()\n        \n\n        # pretrained transformer model as base\n        self.base = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name)\n\n\n        # nn classifier on top of base model\n        self.classfier = nn.Sequential(*[\n            nn.Linear(in_features=768, out_features=256),\n            nn.LeakyReLU(),\n            nn.Linear(in_features=256, out_features=num_classes),\n            nn.Sigmoid()\n        ])\n\n\n    def forward(self, input_ids, attention_mask=None):\n        \"\"\"\n            input_ids: input ids tensors for tokens  shape = [batch_size, max_len]\n            attention_mask: attention for input ids, 0 for pad tokens and 1 for non-pad tokens [batch_size, max_len]\n\n            returns: logits tensors as output, shape = [batch, num_classes]\n\n        \"\"\"\n\n\n        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n        \n        pooler = outputs[1]\n        # pooler.shape = [batch_size, hidden_size]\n\n        logits = self.classfier(pooler)\n\n\n        return logits\n","854a43a6":"class FakeNewsDataModule(pl.LightningDataModule):\n\n    \"\"\"Lightning Data Module to detach data from model\"\"\"\n\n    def __init__(self, config):\n\n        \"\"\"\n            config: a dicitonary containing data configuration such as batch size, split_size etc\n        \"\"\"\n        super().__init__()\n\n        self.config = config\n\n\n        # prepare and setup the dataset\n        self.prepare_data()\n        self.setup()\n\n    def prepare_data(self):\n        \"\"\"prepare datset\"\"\"\n\n        tokenizer = AutoTokenizer.from_pretrained(self.config['model_name'])\n\n        self.dataset = NewsDataset(root_dir=self.config['root_dir'], tokenizer=tokenizer, max_len=self.config['max_len'])\n\n\n\n    def setup(self):\n        \"\"\"make assignments here (val\/train\/test split)\"\"\"\n        \n        train_size = self.config['train_size']\n\n        lengths = [int(len(self.dataset)*train_size), len(self.dataset)-int(len(self.dataset)*train_size)]\n\n        self.train_datset, self.test_dataset =  random_split(dataset=self.dataset, lengths=lengths)\n\n\n    def train_dataloader(self):\n\n        return DataLoader(dataset=self.train_datset, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.config['num_workers'])\n\n    def val_dataloader(self):\n        return DataLoader(dataset=self.test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])\n\n    def test_dataloader(self):\n        # same as validation data\n        return DataLoader(dataset=self.test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])","b648edf6":"class LightningModel(pl.LightningModule):\n\n    \"\"\"\n        LightningModel as trainer model\n    \"\"\"\n    \n    def __init__(self, config):\n        \"\"\"\n            config: training and other conifguration\n        \"\"\"\n\n        super(LightningModel, self).__init__()\n        \n        self.config = config\n        \n        self.model = Model(model_name=self.config['model_name'], num_classes=self.config['num_classes'])\n\n        \n    def forward(self, input_ids, attention_mask=None):\n        logits  = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return logits.squeeze()\n    \n    def configure_optimizers(self):\n        return optim.AdamW(params=self.parameters(), lr=self.config['lr'])\n  \n    \n    def training_step(self, batch, batch_idx):\n        \n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        loss = F.mse_loss(logits, targets)\n        \n        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n        acc = accuracy_score(targets.cpu(), pred_labels)\n        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n        wandb.log({\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1})\n        return {\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1}\n\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        loss = F.mse_loss(logits, targets)\n        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n        acc = accuracy_score(targets.cpu(), pred_labels)\n        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n        precision = precision_score(targets.cpu(), pred_labels, average=self.config['average'])\n        recall = recall_score(targets.cpu(), pred_labels, average=self.config['average'])\n        return {\"val_loss\":loss, \"val_accuracy\":torch.tensor([acc]), \"val_f1\":torch.tensor([f1]), \"val_precision\":torch.tensor([precision]), \"val_recall\":torch.tensor([recall])}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n        avg_precision = torch.stack([x['val_precision'] for x in outputs]).mean()\n        avg_recall = torch.stack([x['val_recall'] for x in outputs]).mean()\n        wandb.log({\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall})\n        return {\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall}\n    \n    \n    def test_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        loss = F.mse_loss(logits, targets)\n        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n        acc = accuracy_score(targets.cpu(), pred_labels)\n        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n        precision = precision_score(targets.cpu(), pred_labels, average=self.config['average'])\n        recall = recall_score(targets.cpu(), pred_labels, average=self.config['average'])\n        return {\"test_loss\":loss, \"test_precision\":torch.tensor([precision]), \"test_recall\":torch.tensor([recall]), \"test_accuracy\":torch.tensor([acc]), \"test_f1\":torch.tensor([f1])}\n    \n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n        avg_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n        avg_precision = torch.stack([x['test_precision'] for x in outputs]).mean()\n        avg_recall = torch.stack([x['test_recall'] for x in outputs]).mean()\n        return {\"test_loss\":avg_loss, \"test_precision\":avg_precision, \"test_recall\":avg_recall, \"test_acc\":avg_acc, \"test_f1\":avg_f1}","b8d8ea95":"root_dir = \"..\/working\/Fake-News\/\"\nos.makedirs(root_dir, exist_ok=True)","dcbae30d":"fake = pd.read_csv(os.path.join('..\/input\/fake-and-real-news-dataset\/', \"Fake.csv\"))\nreal = pd.read_csv(os.path.join('..\/input\/fake-and-real-news-dataset\/', \"True.csv\"))\n\nfake['label'] = [1]*fake.shape[0]\nreal['label'] = [0]*real.shape[0]\n\nfake.to_csv(os.path.join(root_dir, \"Fake.csv\"))\nreal.to_csv(os.path.join(root_dir, \"True.csv\"))","a24d25a2":"config = {\n\n    # data \n    \"root_dir\":root_dir,\n    \"model_name\":\"roberta-base\",\n    \"num_classes\":1,\n    \"max_len\":128,\n    \"train_size\":0.85,\n    \"batch_size\":32,\n    \"num_workers\":4,\n\n\n\n    # training\n    \"average\":\"macro\",\n    \"save_dir\":\".\/\",\n    \"project\":\"fake-news-classification\",\n    \"lr\":2e-5,\n    \"monitor\":\"val_accuracy\",\n    \"min_delta\":0.005,\n    \"patience\":2,\n    \"filepath\":\".\/checkpoints\/{epoch}-{val_f1:4f}\",\n    \"precision\":32,\n    \"epochs\":5,\n    \n}","acce02be":"### Logger, EarlyStopping and Callbacks\nlogger = WandbLogger(\n    name=config['model_name'],\n    save_dir=config[\"save_dir\"],\n    project=config[\"project\"],\n    log_model=True,\n)\nearly_stopping = EarlyStopping(\n    monitor=config[\"monitor\"],\n    min_delta=config[\"min_delta\"],\n    patience=config[\"patience\"],\n)\ncheckpoints = ModelCheckpoint(\n    filepath=config[\"filepath\"],\n    monitor=config[\"monitor\"],\n    save_top_k=1\n)","e6d01ef0":"trainer = pl.Trainer(\n    logger=logger,\n    gpus=[0],\n    checkpoint_callback=checkpoints,\n    callbacks=[early_stopping],\n    default_root_dir=\".\/models\/\",\n    max_epochs=config[\"epochs\"],\n    precision=config[\"precision\"],\n    automatic_optimization=True\n)","eefde350":"dm = FakeNewsDataModule(config=config)\nlm = LightningModel(config=config)","3d978ac2":"trainer.fit(\n    model=lm,\n    datamodule=dm\n)","b43052a1":"trainer.test(\n    model=lm,\n    datamodule=dm\n)","2bc14517":"test_loader = dm.test_dataloader()","b2d66260":"print(os.listdir(\"..\/working\/checkpoints\/\"))","2731e01a":"l  = torch.load(f=\".\/checkpoints\/epoch=2-val_f1=0.999528.ckpt\")\nlm.load_state_dict(l['state_dict'])\n","a881555f":"trainer.test(\n    model=lm,\n    datamodule=dm\n)","429ec389":"actual_label = []\npred_label = []\npred_probs = []\n","bb5c54b6":"# [1, 2, 3] > 1","483ad399":"for batch in tqdm.tqdm(test_loader):\n    \n    y_ = lm(input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device)).detach().cpu()\n    \n    \n    \n    actual_label += batch['label'].squeeze().tolist()\n    \n    pred_probs += y_.tolist()\n    \n    pred_label += y_ > 0.5\n    \n\n    \n    del batch\n    gc.collect()\n","58dfa3d9":"\nprint(classification_report(y_true=actual_label, y_pred=pred_label, digits=5))","8be88620":"print(\"ROBERTa with MSE\")\nprint(classification_report(y_true=actual_label, y_pred=pred_label, digits=5))","a168142f":"ns_probs = [0 for _ in range(len(actual_label))]","fc456ee4":"roc_auc = roc_auc_score(y_true=actual_label, y_score=pred_probs)\nns_auc = roc_auc_score(y_true=actual_label, y_score=ns_probs)\nprint(f'ROC_AUC_Score = {roc_auc:.4f}')","7da4ef4e":"lr_fpr, lr_tpr, _ = roc_curve(y_true=actual_label, y_score=pred_probs)\nns_fpr, ns_tpr, _ = roc_curve(y_true=actual_label, y_score=ns_probs)","389d097d":"# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='RoBERTa')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","12f13c18":"## 2. Model","81595bbe":"## 3. Training and Evaluation","05c87f44":"##### Preprocessing\n- csv file does not have label coloum, adding it to both csv files and save write access wokring\/ directory, will read the data from here now ","610733f9":"#### Trainer Module","1151003d":"#### Data Module","3209403a":"## 1. Custom Dataset ","02c9f388":"## 3. PyTorchLightning Data and Trainer Module ","52f30065":"## AUC ROC Curve","8491db0a":"#### Test from Checkpoint","a998f13a":"## 0. Dependencies","a90cad93":"#### Get the pred probs and predicted labels for test set to compute other metrics"}}