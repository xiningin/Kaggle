{"cell_type":{"1642381b":"code","2bbe5d20":"code","74015c01":"code","ec087f17":"code","4d0577d3":"code","2b7e4b3c":"code","8ecf47f7":"code","bae22d47":"code","fa685e30":"code","d3ee16af":"code","94b43a50":"code","8297ccb0":"code","ace3c5fc":"code","4050fac8":"code","4b9dfd2e":"code","157f1bca":"code","bfe4ace5":"code","4e5e9d4b":"code","67916f1f":"markdown","da1b9b87":"markdown","8a2154cd":"markdown","723cf8ec":"markdown","ab2272bc":"markdown"},"source":{"1642381b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bbe5d20":"import gensim\nimport pandas as pd\nimport json","74015c01":"website_text_df = pd.read_csv('\/kaggle\/input\/hackrx-20-bajaj-fin-serv\/paras-and-lines-website-scraped.csv')\nwebsite_text_df","ec087f17":"#Preprocessing the text obtained from scraping to convert them to tokens\nwebsite_text = website_text_df['lines'].apply(gensim.utils.simple_preprocess)\nwebsite_text","4d0577d3":"#Initializing the model\nmodel = gensim.models.Word2Vec(\n    window=10,\n    min_count=2,\n    workers=4,\n)","2b7e4b3c":"#Building vocabulary for the model\nmodel.build_vocab(website_text, progress_per=1000)","8ecf47f7":"#Training the word2vec model\nmodel.train(website_text, total_examples=model.corpus_count, epochs=model.epochs)","bae22d47":"#model.save(\"\")","fa685e30":"#Checking model performance\nmodel.wv.most_similar('loan')","d3ee16af":"tweets_df = pd.read_csv('\/kaggle\/input\/hackrx-20-bajaj-fin-serv\/tweets-extracted-from-bajaj-finserv-twitter.csv')\ntweets_df","94b43a50":"#We used the tweet-preprocessor library to remove urls, hashtags, emojis from the extracted tweets\n!pip3 install tweet-preprocessor","8297ccb0":"import preprocessor as p\nimport re\ntweet_text_cleaned = tweets_df.Text.apply(p.clean)\ntweet_text_cleaned = tweet_text_cleaned.apply(lambda x: re.sub(r\"www\\S+\", \"\", x))\ntweet_text_preprocessed = tweet_text_cleaned.apply(gensim.utils.simple_preprocess)\ntweet_text_preprocessed","ace3c5fc":"#Training the model on more data \n\n#model = gensim.models.Word2Vec.load('')\nmodel.build_vocab(tweet_text_preprocessed, update=True)\nmodel.train(tweet_text_preprocessed, total_examples=model.corpus_count, epochs=model.epochs)\n#model.save('')","4050fac8":"model.wv.most_similar('loan')","4b9dfd2e":"#Model seems to be performing pretty good \n\nmodel.wv.most_similar('insurance')","157f1bca":"model.wv.most_similar('demat')","bfe4ace5":"urls_df = pd.read_csv('\/kaggle\/input\/hackrx-20-bajaj-fin-serv\/webpage-urls-to-recommend-from.csv')\nurls_df","4e5e9d4b":"query_keyword = 'debt'\n\nfor link in urls_df['links']:\n  heading_in_url = link[37:]\n  words_in_url = heading_in_url.split('-')\n\n  for word in words_in_url:\n    if word.lower() in model.wv.key_to_index:\n      if model.wv.similarity(word.lower(), query_keyword) > 0.3:\n        print(link[37:].replace('-', ' '))\n        print()\n        break\n'''        \nThe model is able to recognize headings with words cibil score, loan, \nemi, credit score as relevant search results, which is pretty cool. \n'''  ","67916f1f":"# Training our Word2Vec Embeddings\n\nWe are training domain specific word embeddings using Gensim Library. \n\nConcept of word embeddings can be understood using this [Youtube Video by Codebasics](https:\/\/www.youtube.com\/watch?v=hQwFeIupNP0&t=2s) \n\nWe used [Jupyter Notebook](https:\/\/github.com\/codebasics\/deep-learning-keras-tf-tutorial\/blob\/master\/42_word2vec_gensim\/42_word2vec_gensim.ipynb) and [Youtube Video by Codebasics](https:\/\/www.youtube.com\/watch?v=Q2NtCcqmIww&t=3s) to understand how to train our own word embeddings","da1b9b87":"## Here comes the magic ","8a2154cd":"# Using the model to recommend relavant articles\n\nWe decided to use the headings of the articles that we smartly got from the url of the article to find out which articles should be suggested when a user searches for some keyword ","723cf8ec":"# Improving the word embeddings model with more data from Twitter\n\nWe also scraped all the tweets from Bajaj Finserv Twitter Handle and improved our word embeddings model using those tweets. ","ab2272bc":"We are checking if there's any word that is very similar to the query keyword using our word embedding model and if that's the case it is being recommended. \n\nWe have kept the similarity score to be more than 0.3 between any word and the query word, for it to be recommended. \n\nWe can easily extend this model for multi-word keyword searches as shown in [this](https:\/\/www.kaggle.com\/umus123\/recommending-urls-based-on-keyword-search) notebook "}}