{"cell_type":{"7be95dbf":"code","d322c350":"code","ec0ef266":"code","fbc431ac":"code","300e3540":"code","2e329f81":"code","a609a0df":"code","80cab8e9":"code","5a58fa5f":"code","cd1715e8":"code","0e48a1e9":"code","d315c02d":"code","9eda0352":"code","57ff01e6":"code","2c7411b3":"code","01b7f9c1":"code","4fea4428":"code","b9b955df":"code","50fe867d":"code","73c2a1c9":"code","54126516":"code","302d95b0":"code","3ec0100a":"code","75842a1c":"code","fc6a222b":"code","d83090a7":"code","e346117e":"code","fd75c6a6":"code","94761ec9":"markdown","0aaa933e":"markdown","431758e8":"markdown","81cb5bfb":"markdown","035497fb":"markdown","3e011e99":"markdown","3f74b84a":"markdown","64ce4be3":"markdown","29580b5e":"markdown","0dc55f79":"markdown","c06eaee7":"markdown","6dc4410c":"markdown","76a4b827":"markdown","79f51f17":"markdown","2445b520":"markdown","9fe63827":"markdown","dea61eeb":"markdown","762cba31":"markdown","e03f9e60":"markdown","d1e9db52":"markdown"},"source":{"7be95dbf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom itertools import cycle\nimport gc\n\nplt.style.use(\"ggplot\")\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","d322c350":"!ls -GFlash ..\/input\/ubiquant-market-prediction\/","ec0ef266":"train = pd.read_parquet('..\/input\/ubiquant-parquet\/train.parquet',\n               columns=['time_id','investment_id','target','f_1','f_2','f_3'])\ntest = pd.read_parquet('..\/input\/ubiquant-parquet\/example_test.parquet')\nss = pd.read_parquet('..\/input\/ubiquant-parquet\/example_sample_submission.parquet')","fbc431ac":"'''Getting an idea of how many observations, assets and time steps'''\n\nobs = train.shape[0]\nprint(f\"Number of observations: {obs}\")","300e3540":"unique_time_ids = train['time_id'].nunique()\nunique_inv_ids = train['investment_id'].nunique()\n\nprint(f'There are {unique_inv_ids} unique investment ids and {unique_time_ids} unique time ids')\n\nprint(f\"Number of assets: {unique_time_ids} (range from {train.investment_id.min()} to {train.investment_id.max()})\")","2e329f81":"example = pd.read_parquet('..\/input\/ubiquant-parquet\/investment_ids\/1.parquet')\nexample.head()","a609a0df":"example_id = train.query('investment_id == 529')\nsns.pairplot(example_id,\n             vars=['f_1','f_2','f_3','target'],\n            hue='time_id')","80cab8e9":"'''The target: investment return rate (IRR)'''\nplt.figure(figsize = (12,5))\nax = sns.distplot(train['target'], bins=1000)\nplt.xlim(-3,3)\nplt.xlabel(\"Histogram of the IRR values\", size=18)\nplt.show();\ngc.collect();","5a58fa5f":"plt.figure(figsize=(20,20))\n\nfor i in range(5):\n    plt.subplot(5,1,i+1)\n    cumReturn = train.loc[train['investment_id']==i,'target'].cumsum()\n    time_id = train.loc[train['investment_id']==i,'time_id']\n    plt.plot(time_id, cumReturn, color='green', lw=2);\n    plt.ylabel (f'investment_id {i}', fontsize=18);\n    plt.title(f'investment_id {i}  time dependency', size=18)\n\nplt.xlabel ('Time_id', fontsize=18)\n\ndel cumReturn, time_id\ngc.collect();","cd1715e8":"for investment_id in range(5):\n    d = train.query('investment_id == @investment_id')\n    d.set_index('time_id')['target'] \\\n        .plot(figsize=(15, 5),\n              title=f'Investment_id {investment_id}',\n              color=next(color_cycle),\n              style='.-')\n    plt.show()","0e48a1e9":"selection = train.groupby(\"investment_id\").time_id.max()\noutlier_inv_ids = selection[selection != 1219].index.values\n\nplt.figure(figsize=(20,5))\nfor n in range(10):\n    plt.plot(train[train.investment_id == outlier_inv_ids[n]].time_id,\n               train[train.investment_id == outlier_inv_ids[n]].target.cumsum(), '.')\n    plt.xlim([0,1220])\n    plt.title(\"Return\/target cumsum for outlier investments\")\n    plt.xlabel(\"time_id\")\n    plt.ylabel(\"cumsum return\");","d315c02d":"print('timestamps in our data', train.time_id.unique())\nprint('the total number of timestamps in our data = ', len(train.time_id.unique()))\n\nmissing_time_ids = []\nfor t in range(1220):\n    if t not in train.time_id.unique():\n        missing_time_ids.append(t)\n        \nprint('Missing time_ids: ', missing_time_ids)","9eda0352":"obs_by_asset = train.groupby(['investment_id'])['target'].count()\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nobs_by_asset.plot.hist(bins=60)\nplt.title(\"target by asset distribution\")\nplt.show()","57ff01e6":"mean_target = train.groupby(['investment_id'])['target'].mean()\nmean_mean_target = np.mean(mean_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nmean_target.plot.hist(bins=60)\nplt.title(\"mean target distribution\")\nplt.show()\n\nprint(f\"Mean of mean target: {mean_mean_target: 0.5f}\")","2c7411b3":"sts_target = train.groupby(['investment_id'])['target'].std()\nmean_std_target = np.mean(sts_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nsts_target.plot.hist(bins=60)\nplt.title(\"standard deviation of target distribution\")\nplt.show()\n\nprint(f\"Mean of std target: {mean_std_target: 0.5f}\")","01b7f9c1":"ax = sns.jointplot(x=obs_by_asset, y=mean_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'blue'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('mean target')\nplt.show()","4fea4428":"qx = sns.jointplot(x=obs_by_asset.values, y=sts_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'blue'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('std target')\nplt.show()","b9b955df":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['investment_id'].nunique().plot()\nplt.title(\"number of unique assets by time\")\nplt.show()","50fe867d":"num_investments_per_time_id = train.groupby(\"time_id\").investment_id.nunique()\n\nplt.figure(figsize=(20,5))\nplt.plot(num_investments_per_time_id.index, num_investments_per_time_id.values, 'o')","73c2a1c9":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['investment_id'].nunique().plot()\nplt.title(\"number of unique assets by time\")\nplt.show()","54126516":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['target'].mean().plot()\nplt.title(\"average target by time\")\nplt.show()","302d95b0":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['target'].std().plot()\nplt.title(\"average target by time\")\nplt.show()","3ec0100a":"r = np.corrcoef(train.groupby('time_id')['investment_id'].nunique(), train.groupby('time_id')['target'].mean())[0][1]\nprint(f\"Correlation of number of assets by target: {r:0.3f}\")","75842a1c":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(train[train.investment_id==4].target.cumsum())\nax[1].plot(train[train.investment_id==4].f_3.cumsum())","fc6a222b":"data_types_dict = {\n    'time_id': 'int32',\n    'investment_id': 'int16',\n    \"target\": 'float16',\n}\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'\n\ntrain_df = pd.read_csv('\/kaggle\/input\/ubiquant-market-prediction\/train.csv', \n                       usecols = data_types_dict.keys(),\n                       dtype=data_types_dict, \n                       index_col = 0)\n\nsample_df = train_df.sample(frac = 0.01)\nsample_df","d83090a7":"correlation = sample_df[[target] + features].corr()","e346117e":"correlation['target'].iloc[1:].hist(bins = 20, figsize = (20,10))","fd75c6a6":"sns.clustermap(correlation, figsize=(20, 20))","94761ec9":"As we have reasoned how the investments with less observations seem more risky, we notice how the number of the assets present at each time step is quite different and also highly oscillating. By the end of the avaliable time, the number of assets has grown by one third. We can see that the number of investments given the time id varies especially around the id 400.","0aaa933e":"## Read in a single invesment_id","431758e8":"Strategy: in training you need to control this effect by expliciting the number of observations because this is predictive of the uncertainty of the predictions. In the test phase, instead, when you are working with an asset that you don't know about, you need to impute an average number of observations, thus expecting an average dispersion of predictions for that asset.","81cb5bfb":"# Train Data Fields\n\ntl;dr - we have time series data but don't know the exact time periods being provided. We also have investment_ids that are not unique. Everything is anonymized so it's not easy to create features.\n\n- `row_id` - A unique identifier for the row.\n- `time_id` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n- `investment_id` - The ID code for an investment. Not all investment have data in all time IDs.\n- `target` - The target.\n- `features` - [f_0:f_299] - Anonymized features generated from market data.","035497fb":"By jointly plotting the distribution of observartions by asset and the mean target value by asset, we may notice that the target value slightly reduces proportionally to the number of observation. The dispersion of values tends to grow with less observations, hence we need to re-plot the scatterplot this time using the standard deviation.","3e011e99":"# The Data\nNote that the training data is roughly 18.55 Gb in size. This is too large to load into memory directly in kaggle notebook.\n\nSome things to note when exploring the entire dataset on a local machine:\n- There are 3579 unique `investment_id`s\n- There are 1211 unique `time_id`s - we are told these are not equally spaced and could be different in the test set.\n- The features columns are mostly normalized with a mean value close to 0 and standard deviation of ~1.\n","3f74b84a":"There are definitely some clusters of highly correlated features that can be later analyzed together.","64ce4be3":"time_id: The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\nYes the IDs are in order from 0-1219 with 8 missing (?) time_ids.\n\nOne time id may belong to 1st Jan 2:00 IST, the next one can be 4th Jan 12:00 IST, the other one 5th Jan 16:00 IST and so on.\n\nClearly the number of data points (rows) in each time_id is not constant.\n\nThe following time_ids are not present. I don't think it should be an issue since we anyway don't have a constant gap between consecutive time_ids.","29580b5e":"The new scatterplot reveals that the less the observations, imply a much more uncertainty in the mean target. ","0dc55f79":"If we plot the number of assets by time alongside the average target by time, it becomes evident that when there are less assets, the target oscillates more with prevalently higher targets. The correlation of assets number and target is negative, in fact. ","c06eaee7":"The average of mean target by asset show a bell-shaped distribution, beware that there are outliers, anyway, because there are some assets with quite negative average target (-0.4 area) and some quite positive ones (+0.8 area). Overall the average mean target by asset is slightly negative (-0.0231)","6dc4410c":"### Features interaction\nWe will do analysis on a smaller random 1% samle of the dataset to speed up the process.","76a4b827":"Also the average of mean standard deviation (std) by asset presents some interesting patterns. First of all, it is skewed toward the right, with some assets having more std (up to 2.5). On the other side there are also some few assets with std almost at zero.","79f51f17":"# Target analysis","2445b520":"Assets are distributed in a different way, there are assets that are actually more frequently observed and others that are not. A good cv and modelling strategy should keep this into account (stratify if you are working with subsamples).","9fe63827":"# Reading the Parquet Version\nReading in csvs can be slow. Instead read from the parquet version here:\n- https:\/\/www.kaggle.com\/robikscube\/ubiquant-parquet","dea61eeb":"The target values look quite normal without any outliers or long tails. We should not have any problems working with it. \n\n\nAssuming an uniform investment (all investment have the same weight), the overall investment is in loss ","762cba31":"Have you noticed that some of the data we have is missing, this can be judged by the long smooth connected areas without hesitation.\n\n- We can clearly see that some investments miss parts of their timeseries or end earlier.\n- Looking back into the competition description, we find: \"The ID code for an investment. Not all investment have data in all time IDs.\"","e03f9e60":"# Example of Features for a Single Investment ID\n- We are only looking at 3 of the features.","d1e9db52":"# Ubiquant Market Prediction\nTwitch Stream EDA.\n\n1. This notebook was create during a live coding session on twitch. follow for past and future broadcasts here: [here](https:\/\/www.twitch.tv\/medallionstallion_) "}}