{"cell_type":{"e2cbce51":"code","1cbea9ad":"code","21c7dc55":"code","84529160":"code","b2fd7e74":"code","91f854a1":"code","c9d003ed":"code","891ecbae":"code","8c78e89e":"code","6e17d55e":"code","3ecd5be9":"code","a0960723":"code","2a400baa":"code","ac2ec18a":"code","e966e474":"code","48ce7ade":"code","f4003c00":"code","2f0e74d6":"code","179f6441":"code","ece56ab2":"code","b9b3d613":"code","325dd2a9":"code","4faced8b":"code","6987e50c":"code","2718805c":"code","e33af0a3":"code","2bd1d1b6":"code","9585d6c4":"code","bf06ed3b":"code","3cd2de7e":"code","7be237fe":"markdown","fa3baa47":"markdown","d1f67289":"markdown","fc84f66d":"markdown","509bb3dd":"markdown","d463483a":"markdown","4f09166e":"markdown","aaf56f91":"markdown","72890423":"markdown","26ecea8a":"markdown"},"source":{"e2cbce51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pylab as pl\nimport seaborn as sns\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","1cbea9ad":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","21c7dc55":"df = pd.read_csv(\"..\/input\/FuelConsumptionCo2.csv\")\n\n# take a look at the dataset\ndf.head()\n","84529160":"# summarize the data\ndf.describe()","b2fd7e74":"#selecting relavent data\ncdf = df[['ENGINESIZE', 'CO2EMISSIONS']]\ncdf.head(5)","91f854a1":"type(cdf)","c9d003ed":"sns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [14, 5]})\nax = sns.countplot(x=\"ENGINESIZE\", data=cdf)","891ecbae":"sns.barplot(x = \"ENGINESIZE\", y = \"CO2EMISSIONS\", data = cdf, ci = False)","8c78e89e":"sns.scatterplot(x = \"ENGINESIZE\", y = \"CO2EMISSIONS\", data = cdf, ci = False)","6e17d55e":"msk = np.random.rand(len(df)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]","3ecd5be9":"train.head(2)","a0960723":"type(train)","2a400baa":"type(train['ENGINESIZE'])","ac2ec18a":"X = train['ENGINESIZE'].values.reshape(-1,1) \n# eshape(-1,1) tells python to convert the array into a matrix with one coloumn. \u201c-1\u201d tells python to figure out the rows by itself. \n# .values: extracts a numpy array with the values of your pandas Series object and then reshapes it to a 2D array.","e966e474":"type(X)","48ce7ade":"noOfTrainEx = X.shape[0] # no of training examples\nprint(\"noOfTrainEx: \",noOfTrainEx)\nnoOfWeights = X.shape[1]+1 # no of features+1 => weights\nprint(\"noOfWeights: \", noOfWeights)","f4003c00":"ones = np.ones([noOfTrainEx, 1]) # create a array containing only ones \nX = np.concatenate([ones, X],1) # cocatenate the ones to X matrix\ntheta = np.ones((1, noOfWeights)) #np.array([[1.0, 1.0]])","2f0e74d6":"y = train['CO2EMISSIONS'].values.reshape(-1,1) # create the y matrix","179f6441":"print(X.shape)\nprint(theta.shape)\nprint(y.shape)","ece56ab2":"# Setting hyper parameter values\n# notice small alpha value\nalpha = 0.01\niters = 3000","b9b3d613":"## Creating cost function\ndef computeCost(X, y, theta):\n    h = X @ theta.T\n    error = h-y\n    loss = np.power(error, 2) \n    J = np.sum(loss)\/(2*noOfTrainEx)\n    return J","325dd2a9":"computeCost(X, y, theta) #Computing cost now produces very high cost","4faced8b":"## Gradient Descent funtion\ndef gradientDescent(X, y, theta, alpha, iters):\n    cost = np.zeros(iters)\n    for i in range(iters):\n        theta = theta - (alpha\/len(X)) * np.sum((X @ theta.T - y) * X, axis=0)\n        cost[i] = computeCost(X, y, theta)\n        if i % 100 == 0: # just look at cost every ten loops for debugging\n            print(i, 'iteration, cost:', cost[i])\n    return (theta, cost)","6987e50c":"g, cost = gradientDescent(X, y, theta, alpha, iters)  ","2718805c":"print(g, cost)","e33af0a3":"#plot the cost\nfig, ax = plt.subplots()  \nax.plot(np.arange(iters), cost, 'r')  \nax.set_xlabel('Iterations')  \nax.set_ylabel('Cost')  \nax.set_title('Error vs. Training Epoch')","2bd1d1b6":"axes = sns.scatterplot(x = \"ENGINESIZE\", y = \"CO2EMISSIONS\", data = cdf, ci = False)\nx_vals = np.array(axes.get_xlim()) \ny_vals = g[0][0] + g[0][1]* x_vals #the line equation\nplt.plot(x_vals, y_vals, '--')","9585d6c4":"from sklearn import linear_model\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['ENGINESIZE']])\ntrain_y = np.asanyarray(train[['CO2EMISSIONS']])\nregr.fit (train_x, train_y)\n# The coefficients\nprint ('Coefficients: ', regr.coef_)\nprint ('Intercept: ',regr.intercept_)","bf06ed3b":"axes = sns.scatterplot(x = \"ENGINESIZE\", y = \"CO2EMISSIONS\", data = cdf, ci = False)\nx_vals = np.array(axes.get_xlim()) \ny_vals = regr.intercept_[0] + regr.coef_[0][0]* x_vals #the line equation\nplt.plot(x_vals, y_vals, '--')","3cd2de7e":"from sklearn.metrics import r2_score\n\ntest_x = np.asanyarray(test[['ENGINESIZE']])\ntest_y = np.asanyarray(test[['CO2EMISSIONS']])\ntest_y_hat = regr.predict(test_x)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_hat - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y_hat , test_y) )","7be237fe":"### Analyzing the data","fa3baa47":"## Using sklearn","d1f67289":"## FuelConsumption.csv:\nWe have downloaded a fuel consumption dataset, **`FuelConsumption.csv`**, which contains model-specific fuel consumption ratings and estimated carbon dioxide emissions for new light-duty vehicles for retail sale in Canada. [Dataset source](http:\/\/open.canada.ca\/data\/en\/dataset\/98f1a129-f628-4ce4-b24d-6f16bf24dd64)\n\n- **MODELYEAR** e.g. 2014\n- **MAKE** e.g. Acura\n- **MODEL** e.g. ILX\n- **VEHICLE CLASS** e.g. SUV\n- **ENGINE SIZE** e.g. 4.7\n- **CYLINDERS** e.g 6\n- **TRANSMISSION** e.g. A6\n- **FUEL CONSUMPTION in CITY(L\/100 km)** e.g. 9.9\n- **FUEL CONSUMPTION in HWY (L\/100 km)** e.g. 8.9\n- **FUEL CONSUMPTION COMB (L\/100 km)** e.g. 9.2\n- **CO2 EMISSIONS (g\/km)** e.g. 182   --> low --> 0\n\n","fc84f66d":"### Linear regression without using sklearn","509bb3dd":"What it means is that we find the difference between predicted values (we use line equation and theta values to predict yhat ) and the original y values (already in the data set i.e the y matrix) and sum them up. Then we find the average and return it. The returned value is the cost.","d463483a":"Linear Regression fits a linear model with coefficients $\\theta = (\\theta_1, ..., \\theta_n)$ to minimize the 'residual sum of squares' between the independent x in the dataset, and the dependent y by the linear approximation. ","4f09166e":"## Linear Regression with one variable\n\nConsider **ENGINESIZE** alone","aaf56f91":"This is based on AndrewNG's course. \n\nAnother example here with detailed discussion:\nhttps:\/\/towardsdatascience.com\/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137","72890423":"### Splitting training and test data\n\nLets split our dataset into train and test sets, 80% of the entire data for training, and the 20% for testing. We create a mask to select random rows using __np.random.rand()__ function: ","26ecea8a":"### Evaluation\nwe compare the actual values and predicted values to calculate the accuracy of a regression model. Evaluation metrics provide a key role in the development of a model, as it provides insight to areas that require improvement.\n\nThere are different model evaluation metrics, lets use MSE here to calculate the accuracy of our model based on the test set: \n<ul>\n    <li> Mean absolute error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it\u2019s just average error.<\/li>\n    <li> Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It\u2019s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.<\/li>\n    <li> Root Mean Squared Error (RMSE): This is the square root of the Mean Square Error. <\/li>\n    <li> R-squared is not error, but is a popular metric for accuracy of your model. It represents how close the data are to the fitted regression line. The higher the R-squared, the better the model fits your data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).<\/li>\n<\/ul>\n"}}