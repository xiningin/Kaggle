{"cell_type":{"2d220063":"code","0679e9fd":"code","72dbeb70":"code","323b1cd5":"code","d18958c5":"code","d988936d":"code","5a9282b2":"code","6ee4cff5":"code","cd712335":"code","48846223":"code","260d8eb5":"code","e5e05e75":"markdown","007a47bf":"markdown","11bf1c40":"markdown","93a6363a":"markdown","9d54acb0":"markdown","7476681d":"markdown","53f7516f":"markdown"},"source":{"2d220063":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0679e9fd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom time import time\nimport os, sys, gc, warnings, random, datetime\nimport math\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import StratifiedKFold , KFold\n# from ngboost import NGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","72dbeb70":"df = pd.read_pickle(\"\/kaggle\/input\/loan-condition-eda-data-cleansing\/df_for_use.pkl\")","323b1cd5":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    auc = roc_auc_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}, ROC_AUC_Score : {4:.4f}'.format(accuracy , precision, recall, f1, auc))","d18958c5":"thresholds = {0.3,0.35, 0.4, 0.45, 0.50, 0.55, 0.60}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)","d988936d":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n","5a9282b2":"import time","6ee4cff5":"### No Fold (Using GPU)\n\nstart = time.time()\n\nlgbm_clf = LGBMClassifier( n_estimators = 3000, random_state = 2020)\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'auc' , eval_set = evals, verbose = 50)\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\nprint( 'ROC_AUC : {0:.4f}'.format(lgbm_roc_score))\n\nprint(\"Runtime :\", time.time() - start)","cd712335":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n\nfrom lightgbm import LGBMClassifier\n\nfrom time import time\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'auc'\n           }\n\nk_fold=5\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=2020)\ntraining_start_time = time()\naucs=[]\ny_preds = np.zeros(X_test.shape[0])\n\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X_train,y_train)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n    aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    y_preds += clf.predict(X_test) \/ 5\n    \n    \n    \nprint('-' * 30)\nprint('Training is completed!.')\nprint(\"\\n## Mean CV_AUC_Score : \", np.mean(aucs))\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print(clf.best_params_)\nprint('-' * 30)\n\n\n# pred_rf = clf.predict(X_test)\nauc = roc_auc_score(y_test,y_preds)\nprint(' ROC_AUC_Score : {0:.4f}'.format (auc))","48846223":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n","260d8eb5":"from time import time\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'auc'}\n\nk_fold=5\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=2020)\ntraining_start_time = time()\naucs=[]\n\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X,y)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n    aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    \nprint('-' * 30)\nprint('Training is completed!.')\nprint(\"\\n## Mean CV_AUC_Score : \", np.mean(aucs))\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print(clf.best_params_)\nprint('-' * 30)\n\n# X_test = test_df.drop('loan_condition_cat', axis=1)\n# y_test = test_df['loan_condition_cat']\n\n# pred_rf = clf.predict(X_test)\n# auc = roc_auc_score(y_test,pred_rf)\n# print(' ROC_AUC_Score : {0:.4f}'.format (auc))","e5e05e75":"### Utilities","007a47bf":"### Import Data","11bf1c40":"### LightGBM with Startified 5 Fold \/ apply to previous train_test_split data","93a6363a":"### LightGBM Without Fold","9d54acb0":"### train_test_split (Stratify)","7476681d":"### Libraries\n","53f7516f":"### LightGBM with Startified 5 Fold \/ Entire Data"}}