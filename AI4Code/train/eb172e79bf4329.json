{"cell_type":{"96c8e411":"code","434d7718":"code","adf3f323":"code","8328a558":"code","f8ac0625":"code","bf35d9c0":"code","8ff6ca64":"code","c547322e":"code","b9fd0751":"code","1454d863":"code","2d533785":"code","68e7cae8":"code","e40ad513":"code","62354c1d":"code","b61aec32":"code","2daa908a":"code","db48c7cd":"code","33844241":"code","1c18280e":"code","88df21e4":"code","f400366b":"code","af85b9a9":"markdown","39398f15":"markdown","f1402c74":"markdown","bfecfa5d":"markdown","9aabb650":"markdown","32715bb5":"markdown","67b89ab2":"markdown"},"source":{"96c8e411":"#Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Split Data Train and Test\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n\n#Modelling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, plot_roc_curve","434d7718":"#Importing Data set\ntrain_data=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","adf3f323":"train_data.head(10)","8328a558":"fig=plt.figure()\nax=fig.add_subplot(1,1,1)\nax.hist(train_data['quality'],bins=10)\nplt.show()","f8ac0625":"#Complete Data exploration\nf = plt.figure()\nf.set_figwidth(20)\nf.set_figheight(10)\nx=train_data['quality']\nplt.plot(x,train_data['fixed acidity'],'r',label='Fixed acidity')\nplt.plot(x,train_data['free sulfur dioxide'],'pink',label='free sulfur dioxide')\nplt.plot(x,train_data['residual sugar'],'maroon',label='residual sugar')\nplt.plot(x,train_data['total sulfur dioxide'],'lightseagreen',label='total sulfur dioxide')\nplt.plot(x,train_data['volatile acidity'],'b',label='Volatile acidity')\nplt.plot(x,train_data['citric acid'],'g',label='citric acid')\nplt.plot(x,train_data['pH'],'y',label='pH')\nplt.plot(x,train_data['alcohol'],'v',label='alcohol')\nplt.plot(x,train_data['chlorides'],'c',label='chlorides')\nplt.plot(x,train_data['sulphates'],'m',label='sulphates')\nplt.plot(x,train_data['density'],'k',label='density')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","bf35d9c0":"#Remove Dulpicates\ntrain_data.drop_duplicates(inplace=True)","8ff6ca64":"#Finding null values\ntrain_data.isna().sum()","c547322e":"#Finding Co-relation between data features attributes\nCorr=train_data.corr()\nCorr_res=[]\nfor i in range(0,len(train_data.dtypes)):\n  for j in range(0,len(train_data.dtypes)):\n    value=Corr.iloc[i:i+1,j:j+1].values\n    if value>0.8 and value!=1 :\n     Corr_res.append(Corr.columns[i])","b9fd0751":"#Standardization\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\ntrain_data.iloc[:,:-1]=std.fit_transform(train_data.iloc[:,:-1]) #Standardize data set except dependent value(Quality feature)","1454d863":"#Assigning dataframe to list of array values\nX=train_data.iloc[:,:-1].values\nY=train_data.iloc[:,-1].values","2d533785":"#Split the data set in the ratio of 80:20 \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state = 42)","68e7cae8":"#K-Nearest Neighbors\nerror_rate = []\nfor i in range(1, 40):\n\t\n\tknn = KNeighborsClassifier(n_neighbors = i)\n\tknn.fit(x_train, y_train)\n\tpred_i = knn.predict(x_test)\n\terror_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize =(10, 6))\nplt.plot(range(1, 40), error_rate, color ='blue',\n\t\t\t\tlinestyle ='dashed', marker ='o',\n\t\tmarkerfacecolor ='red', markersize = 10)\n\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\n","e40ad513":"#K=21 has lowest error rate\n#Model Fit\nclassifier2 = KNeighborsClassifier(n_neighbors= 21, metric = 'manhattan', p = 2,weights='uniform')\nclassifier2.fit(x_train,y_train)","62354c1d":"#Predicting the ouput from input data (x_train) and (y_train) \ny_pred1 = classifier2.predict(x_train)\ny_pred2 = classifier2.predict(x_test)","b61aec32":"#Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy score of train data set:\",accuracy_score(y_train, y_pred1))\nprint(\"Accuracy score of test data set:\",accuracy_score(y_test, y_pred2))","2daa908a":"#Visualization\nplt.figure()\nplt.plot(y_test,'o',color = 'blue',label = 'Actual Values')\nplt.plot(y_pred2,color = 'red',label = 'Predicted values')\nplt.legend()","db48c7cd":"train_data['quality'].value_counts()","33844241":"#If quality value is less than or eqaul to 6 then it will be in class 0\n#If quality value is greater than 6  then it will be in class 1\ntrain_data['quality'] = np.where(train_data['quality'] > 6, 1, 0)\ntrain_data['quality'].value_counts()","1c18280e":"#Assigning dataframe to list of array values\nX = train_data.drop(['quality'], axis = 1).values\ny = train_data['quality'].values","88df21e4":"#Splitting the data in the proportion of 70:30 and 86:14\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   stratify = y,\n                                                   test_size = 0.3,\n                                                   random_state = 1111)","f400366b":"k = range(1,50,2)\ntesting_accuracy = []\ntraining_accuracy = []\nscore = 0\n#Fitting the model\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors = i)\n    pipe_knn = Pipeline([('scale', MinMaxScaler()), ('knn', knn)])\n    pipe_knn.fit(X_train, y_train)\n    \n    y_pred_train = pipe_knn.predict(X_train)\n    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n    \n    y_pred_test = pipe_knn.predict(X_test)\n    acc_score = accuracy_score(y_test,y_pred_test)\n    testing_accuracy.append(acc_score)\n    \n    if score < acc_score:\n        score = acc_score\n        best_k = i\n        \nprint('Best Accuracy Score', score, 'Best K-Score', best_k)","af85b9a9":"# Data Selection","39398f15":"# Attempt through Classification ","f1402c74":"# Quality_of_Red Wine","bfecfa5d":"# Data Preprocessing","9aabb650":"# Splitting the data","32715bb5":"# Exploratory Data Analysis","67b89ab2":"# Model Selection"}}