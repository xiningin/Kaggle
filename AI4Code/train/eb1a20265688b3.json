{"cell_type":{"50eb7d67":"code","2c31aac7":"code","08b2384a":"code","70a68c87":"code","d0451867":"code","bf7e7984":"code","1f7d1c83":"code","ed9d9f10":"code","bda7a776":"code","70d03dda":"code","595691b4":"code","eda85289":"code","b605e04c":"code","c3cb6157":"code","f555f6c7":"code","0078a540":"code","b7f4a3ad":"code","da2016f0":"code","18615bbc":"code","b689fa10":"code","965a5b85":"code","f272af3f":"code","1e3982fd":"code","29809fdb":"code","d3eab67e":"code","b2675707":"code","e85ac6c0":"code","fcd56fff":"code","8f39dfd7":"code","c8aed7a9":"code","725ff2ef":"code","04445ee0":"code","7c876c77":"code","9d50c005":"code","8e57d694":"code","b574e0cc":"code","1b86ed28":"code","9f58b20b":"code","5b294b6b":"code","e03a81b7":"code","218570a5":"code","8c305cea":"code","e9782c4e":"code","880e5518":"code","117588f1":"code","dc8e2346":"code","23834944":"code","450eef62":"code","8dccef7b":"code","97d73f6f":"code","d920d34b":"code","c3b073d6":"code","41d3bfb7":"code","05554cfb":"code","933d9459":"code","bb27046c":"code","0d0935dd":"code","1fa4f1c8":"code","cb9a2cce":"code","70c98161":"code","25d09b1d":"code","220bf999":"code","952f795c":"code","281c8574":"code","a4f466ea":"code","a6e97713":"markdown","0bbb40aa":"markdown","46f2d92d":"markdown","e18073c0":"markdown","ea633c5b":"markdown","58d7c564":"markdown","d2e4352d":"markdown","08ba1d61":"markdown","756bf9a8":"markdown","d4cfc0f1":"markdown","6ecceb8e":"markdown","66fa931f":"markdown","9a4358b6":"markdown","23035c5c":"markdown","5da11356":"markdown","c1d44f8c":"markdown","a45dfadd":"markdown","a2d88dda":"markdown","e7c91429":"markdown","4469ab1c":"markdown","85bdd5a2":"markdown","28230df3":"markdown","402aaeab":"markdown","b5b06c8e":"markdown","1bd7d4ed":"markdown","021f0c8c":"markdown","7352cb2a":"markdown","99035fc4":"markdown","73a2cdd5":"markdown","39fe0ef0":"markdown","0f8586c6":"markdown","16cfe2e0":"markdown","bcb6c487":"markdown","cf6d878b":"markdown","82a8e034":"markdown","3da98598":"markdown","63f9b450":"markdown","50d26e56":"markdown","8979a484":"markdown","5e26be01":"markdown","30f25362":"markdown"},"source":{"50eb7d67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2c31aac7":"# Read the data\nX = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","08b2384a":"print(\"Shape of training set:\",format(X.shape))\nprint(\"Shape of test set:\",format(X_test_full.shape))","70a68c87":"X.describe()","d0451867":"num_X = X.select_dtypes(exclude='object')\nnum_X_cor = num_X.corr()\nf,ax=plt.subplots(figsize=(20,2))\nsns.heatmap(num_X_cor.sort_values(by=['SalePrice'], ascending=False).head(1), cmap='Blues')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold', color='dodgerblue', rotation=0)","bf7e7984":"high_cor_num = num_X_cor['SalePrice'].sort_values(ascending=False).head(10).to_frame()\nhigh_cor_num","1f7d1c83":"f,ax=plt.subplots(figsize=(20,5))\n\nnbr_count = X.Neighborhood.value_counts().to_frame().reset_index() #getting the number of houses for each neighborhood\n\nnbr_plt = ax.barh(nbr_count.iloc[:,0], nbr_count.iloc[:,1], color=sns.color_palette('Blues',len(nbr_count)))\n\nax.invert_yaxis()\nplt.yticks(weight='bold')\nplt.xlabel('Count')\nplt.title('Number of houses by Neighborhood')\nplt.show()","ed9d9f10":"f,ax=plt.subplots(figsize=(20,5))\n\nlotshape_count = X.LotShape.value_counts().to_frame().reset_index() #getting the number of houses for each neighborhood\n\nnbr_plt = ax.barh(lotshape_count.iloc[:,0], lotshape_count.iloc[:,1], color=sns.color_palette('Reds',len(nbr_count)))\n\nax.invert_yaxis()\nplt.yticks(weight='bold')\nplt.xlabel('Count')\nplt.title('Number of houses by Lot Shape')\nplt.show()","bda7a776":"X.Utilities.value_counts()","70d03dda":"nX = X.shape[0]\nnX_test = X_test_full.shape[0]\ny_train = X['SalePrice'].to_frame()\n#Combine train and test sets\ncombined_df = pd.concat((X,X_test_full), sort=False).reset_index(drop=True)\n\n#Drop the target \"SalePrice\" and Id columns\ncombined_df.drop(['SalePrice'], axis=1, inplace=True)\nprint(f\"Total size is {combined_df.shape}\")\n","595691b4":"nullpercentage = (combined_df.isnull().mean())*100\nnullpercentage = nullpercentage.sort_values(ascending=False).to_frame()\nnullpercentage.head()","eda85289":"def msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    \n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, f'Columns with less than {thresh} missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()","b605e04c":"msv1(combined_df, color=sns.color_palette('Blues'))","c3cb6157":"combined_df1 = combined_df.dropna(thresh=len(combined_df)*0.8, axis=1) #dropping columns with more than 20% null values\nprint(f\"We dropped {combined_df.shape[1]-combined_df1.shape[1]} features in the combined set\")","f555f6c7":"combined_df1.select_dtypes(exclude='object').isnull().sum().sort_values(ascending=False).head(11)","0078a540":"def msv2(data, width=12, height=8, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(width, height))\n\n    allna = (data.isnull().sum() \/ len(data))*100\n    tightout= 0.008*max(allna)\n    allna = allna.drop(allna[allna == 0].index).sort_values().reset_index()\n    mn= ax.barh(allna.iloc[:,0], allna.iloc[:,1], color=color, edgecolor=edgecolor)\n    ax.set_title('Missing values percentage per column', fontsize=15, weight='bold' )\n    ax.set_xlabel('Percentage', weight='bold', size=15)\n    ax.set_ylabel('Features with missing values', weight='bold')\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    for i in ax.patches:\n        ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2))+'%',\n            fontsize=10, fontweight='bold', color='grey')\n    return plt.show()","b7f4a3ad":"msv2(combined_df1)","da2016f0":"#LotFrontage has 16% missing values. Filling with median.\ncombined_df1['LotFrontage'] = combined_df1.LotFrontage.fillna(combined_df1.LotFrontage.median())\n#Masonry Veneer Area having missing values would mean that there is no veneer. Hence we fill with 0.\ncombined_df1['MasVnrArea'] = combined_df1.MasVnrArea.fillna(0)\n#No value for GarageYrBlt would mean no garage exists. But since this is a year value, we cannot fill it with 0s. Hence we fill with median.\ncombined_df1['GarageYrBlt']=combined_df1[\"GarageYrBlt\"].fillna(1980)\n","18615bbc":"combined_df1.shape","b689fa10":"df_Cat = combined_df1.select_dtypes(include='object')\nnan_cols = df_Cat.columns[df_Cat.isnull().any()]\nNA_Cat= df_Cat[nan_cols]\nNA_Cat.isnull().sum().sort_values(ascending=False)","965a5b85":"cols_to_fill = ['SaleType','Exterior1st','Exterior2nd','KitchenQual',\n                'Electrical','Utilities','Functional','MSZoning']\n\ncombined_df1[cols_to_fill] = combined_df1[cols_to_fill].fillna(method='ffill')","f272af3f":"cols = combined_df1.columns\nfor col in cols:\n    if combined_df1[col].dtype == 'object':\n        combined_df1[col] = combined_df1[col].fillna('None')\n    elif combined_df1[col].dtype != 'object':\n        combined_df1[col] = combined_df1[col].fillna(0)","1e3982fd":"combined_df1.isnull().sum().sort_values(ascending=False).head() #checking whether all the null values have been dealt with","29809fdb":"combined_df1.shape","d3eab67e":"combined_df1['TotalArea'] = combined_df1['TotalBsmtSF'] + combined_df1['1stFlrSF'] + combined_df1['2ndFlrSF'] + combined_df1['GrLivArea'] + combined_df1['GarageArea']\n# Creating a new feature 'TotalArea' by adding up the area of all the floors and basement","b2675707":"combined_df1['MSSubClass'] = combined_df1['MSSubClass'].apply(str)\ncombined_df1['YrSold'] = combined_df1['YrSold'].astype(str)","e85ac6c0":"combined_df_onehot = pd.get_dummies(combined_df1)\nprint(f\"the shape of the original dataset {combined_df1.shape}\")\nprint(f\"the shape of the encoded dataset {combined_df_onehot.shape}\")\nprint(f\"We have {combined_df_onehot.shape[1]- combined_df1.shape[1]} new encoded features\")","fcd56fff":"X_train = combined_df_onehot[:nX]   #nX is the number of rows in the original training set\ntest = combined_df_onehot[nX:]\n\nprint(X_train.shape, test.shape)","8f39dfd7":"fig = plt.figure(figsize=(15,15))\n\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=X['GrLivArea'], y=y_train['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Greater living Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=X['TotalBsmtSF'], y=y_train['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=X['MasVnrArea'], y=y_train['SalePrice'], color=('blue'),alpha=0.5)\nplt.axvline(x=1200, color='r', linestyle='-')\nplt.title('Masonry Veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=X['1stFlrSF'], y=y_train['SalePrice'], color=('green'),alpha=0.5)\nplt.axvline(x=4500, color='r', linestyle='-')\nplt.title('1st Floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=X['GarageArea'], y=y_train['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1300, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=X['TotRmsAbvGrd'], y=y_train['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('Total Rooms - Price scatter plot', fontsize=15, weight='bold' )","c8aed7a9":"print(X['TotRmsAbvGrd'].sort_values(ascending=False).head(1))\nprint(X['GarageArea'].sort_values(ascending=False).head(3))\nprint(X['1stFlrSF'].sort_values(ascending=False).head(1))\nprint(X['MasVnrArea'].sort_values(ascending=False).head(2))\nprint(X['TotalBsmtSF'].sort_values(ascending=False).head(1))\nprint(X['GrLivArea'].sort_values(ascending=False).head(2))\n","725ff2ef":"x_train = X_train[(X_train['TotRmsAbvGrd'] < 13) & (X_train['MasVnrArea']<1200) & (X_train['1stFlrSF']<4000) & (X_train['TotalBsmtSF']<5000)\n                 & (X_train['GarageArea']<1300) & (X_train['GrLivArea']<4600)]\n\nprint(f'We removed {X_train.shape[0]- x_train.shape[0]} outliers')","04445ee0":"target = X[['SalePrice']]\ntarget.shape","7c876c77":"pos = [635,1298,581,1190,297,1169,523]\ntarget.drop(target.index[pos], inplace=True)","9d50c005":"print('We make sure that both train and target sets have the same row number after removing the outliers:')\nprint( 'Train: ',x_train.shape[0], 'rows')\nprint('Target:', target.shape[0],'rows')","8e57d694":"print(\"Skewness before log transform: \", X['LotFrontage'].skew())\nprint(\"Kurtosis before log transform: \", X['LotFrontage'].kurt())","b574e0cc":"from scipy.stats import skew\n\n#num_feats = combined_df1.dtypes[combined_df1.dtypes != \"object\"].index\n\n\n#skewed_feats = x_train[num_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n\n#skewed_feats = skewed_feats[skewed_feats > 0.55]\n#skewed_feats = skewed_feats.index\n\n#x_train[skewed_feats] = np.log1p(x_train[skewed_feats])","1b86ed28":"print(f\"Skewness after log transform: {x_train['LotFrontage'].skew()}\")\nprint(f\"Kurtosis after log transform: {x_train['LotFrontage'].kurt()}\")","9f58b20b":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,10))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((2,2),(0,0))\nsns.distplot(X.LotFrontage, color='plum')\nplt.title('Before: Distribution of Lot Frontage',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(0,1))\nsns.distplot(X['GrLivArea'], color='tan')\nplt.title('Before: Distribution of GrLivArea',weight='bold', fontsize=18)\n\n\nax1 = plt.subplot2grid((2,2),(1,0))\nsns.distplot(x_train.LotFrontage, color='plum')\nplt.title('After: Distribution of Lot Frontage',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(1,1))\nsns.distplot(x_train['GrLivArea'], color='tan')\nplt.title('After: Distribution of GrLivArea',weight='bold', fontsize=18)\nplt.show()","5b294b6b":"target[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","e03a81b7":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.hist(X.SalePrice, bins=10, color='red',alpha=0.5)\nplt.title('Sale price distribution before log transform',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.hist(target.SalePrice, bins=10, color='darkgreen',alpha=0.5)\nplt.title('Sale price distribution after log transform',weight='bold', fontsize=18)\nplt.show()","218570a5":"print(f\"Skewness after log transform: {target['SalePrice'].skew()}\")\nprint(f\"Kurtosis after log transform: {target['SalePrice'].kurt()}\")","8c305cea":"from sklearn.model_selection import train_test_split\n#train-test split\ntarget = np.array(target)\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(x_train, target, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint(X_train_full.shape,y_train.shape)","e9782c4e":"X_train","880e5518":"from sklearn.preprocessing import RobustScaler\nscaler= RobustScaler()\n# transform \"x_train\"\nX_train_full = scaler.fit_transform(X_train_full)\n# transform \"x_test\"\nX_valid_full = scaler.transform(X_valid_full)\n#Transform the test set\nX_test= scaler.transform(test)","117588f1":"X_test","dc8e2346":"from sklearn.metrics import mean_squared_error\ndef score(prediction): #creating a function to get RMSE for predictions\n    return str(math.sqrt(mean_squared_error(y_valid, prediction)))","23834944":"import sklearn.model_selection as ms\nfrom sklearn.linear_model import Ridge\nimport math\n\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]} \n\nridge_reg= ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nridge_reg.fit(X_train_full,y_train)\nprint(f\"The best value of Alpha is: {ridge_reg.best_params_}\")\nprint(f\"The best score achieved with Alpha=10 is: {math.sqrt(-ridge_reg.best_score_)}\")\nridge_pred=math.sqrt(-ridge_reg.best_score_)\n","450eef62":"\nridge_mod=Ridge(alpha=15)\nridge_mod.fit(X_train_full,y_train)\ny_pred_train=ridge_mod.predict(X_train_full)\ny_pred_test=ridge_mod.predict(X_valid_full)\n\nprint(f'Root Mean Square Error train =  {str(math.sqrt(mean_squared_error(y_train, y_pred_train)))}')\nprint(f'Root Mean Square Error test =  {score(y_pred_test)}')   ","8dccef7b":"from sklearn.model_selection import cross_val_score\n\n#Ridge regression\nRidge_CV=Ridge(alpha=15)\nMSEs=cross_val_score(Ridge_CV, x_train, target, scoring='neg_mean_squared_error', cv=5)\n\n#RMSE score of the 5 folds\nprint(\"RMSE scores of the 5 folds:\")\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n\n#Final RMSE score with Lasso\nprint(f'Mean RMSE with Ridge: {round(math.sqrt(np.mean(-MSEs)),4)}')\n","97d73f6f":"from sklearn.linear_model import Lasso\n\nparams= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\nlasso=Lasso(tol=0.01)\nlasso_reg=ms.GridSearchCV(lasso, param_grid=params, scoring='neg_mean_squared_error', cv=15)\nlasso_reg.fit(X_train_full,y_train)\n\nprint(f'The best value of Alpha is: {lasso_reg.best_params_}')","d920d34b":"lasso_mod=Lasso(alpha=0.0009)\nlasso_mod.fit(X_train_full,y_train)\ny_lasso_train=lasso_mod.predict(X_train_full)\ny_lasso_test_pred=lasso_mod.predict(X_valid_full)\n\nprint(f'Root Mean Square Error train  {str(math.sqrt(mean_squared_error(y_train, y_lasso_train)))}')\nprint(f'Root Mean Square Error test  {score(y_lasso_test_pred)}')","c3b073d6":"coefs = pd.Series(lasso_mod.coef_, index = x_train.columns)\n\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\", color='darkcyan')\nplt.xlabel(\"Lasso coefficient\", weight='bold')\nplt.title(\"Feature importance in the Lasso Model\", weight='bold')\nplt.show()","41d3bfb7":"print(f\"Lasso kept {sum(coefs != 0)} important features and dropped the other  {sum(coefs == 0)} features\")","05554cfb":"#Lasso regression\nLasso_CV=Lasso(alpha=0.0009, tol=0.001)\nMSEs=ms.cross_val_score(Lasso_CV, x_train, target, scoring='neg_mean_squared_error', cv=5)\n\n#RMSE score of the 5 folds\nprint(\"RMSE scores of the 5 folds:\")\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n\n#Final RMSE score with Lasso\nprint(f'Mean RMSE with Lasso: {round(math.sqrt(np.mean(-MSEs)),4)}')","933d9459":"from sklearn.linear_model import ElasticNetCV\n\nalphas = [10,1,0.1,0.01,0.001,0.002,0.003,0.004,0.005,0.00056]\nl1ratio = [0.1, 0.3,0.5, 0.9, 0.95]\n\nelastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\nelasticmod = elastic_cv.fit(X_train_full, y_train.ravel())\nela_pred=elasticmod.predict(X_valid_full)\nprint('Root Mean Square Error test = ' + str(math.sqrt(mean_squared_error(y_valid, ela_pred))))\nprint(elastic_cv.alpha_)\nprint(elastic_cv.l1_ratio_)","bb27046c":"from sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\n%matplotlib inline\n\ndef get_score(n_estimators):\n    \n    \n    xgb = XGBRegressor(n_estimators=n_estimators, learning_rate = 0.02)\n    \n    scores_new = -1 * cross_val_score(xgb, X_train_full, y_train,\n                              cv=5,\n                              scoring='neg_mean_squared_error')\n\n    print(\"Average RMSE score for n_estimators : {} is :\".format(n_estimators), round(math.sqrt(np.mean(scores_new)),4))\n    return math.sqrt(np.mean(scores_new))\n\nresults = {} #dict to store results\nfor i in range(100,800,100):  #checking different values of n_estimators\n        results[i] = get_score(i)\n\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()","0d0935dd":"from sklearn.model_selection import GridSearchCV\n\n#xg_reg = XGBRegressor()\n#xgparam_grid= {'learning_rate' : [0.01],'n_estimators':[2000,3000,4000],\n#                                     'reg_alpha':[0.0001,0.01],\n#                                    'reg_lambda':[1,0.01]}\n#\n#xg_grid=GridSearchCV(xg_reg, param_grid=xgparam_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n#xg_grid.fit(X_train_full,y_train)\n#print(xg_grid.best_estimator_)\n#print(xg_grid.best_score_)","1fa4f1c8":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n             importance_type='gain',\n             learning_rate=0.01, max_delta_step=0, max_depth=3,\n            objective='reg:squarederror',\n             min_child_weight=1, missing=None, monotone_constraints='()',\n             n_estimators=4000, n_jobs=1, random_state=0,\n             reg_alpha=0.0001, reg_lambda=0.01, scale_pos_weight=1, subsample=1,\n              validate_parameters=1, verbosity=1)\nxgmod=xgb.fit(X_train_full,y_train)\nxg_pred=xgmod.predict(X_valid_full)\nprint(f'Root Mean Square Error test = {score(xg_pred)}')","cb9a2cce":"from sklearn.ensemble import VotingRegressor\n\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), \n                            ('XGBRegressor', xgb)])\nvote= vote_mod.fit(X_train_full, y_train.ravel())\nvote_pred=vote.predict(X_valid_full)\n\nprint(f'Root Mean Square Error test = {score(vote_pred)}')","70c98161":"from mlxtend.regressor import StackingRegressor\n\n\nstackreg = StackingRegressor(regressors=[elastic_cv,ridge_mod, lasso_mod, vote_mod], \n                           meta_regressor=xgb, use_features_in_secondary=True\n                          )\n\nstack_mod=stackreg.fit(X_train_full, y_train.ravel())\nstacking_pred=stack_mod.predict(X_valid_full)\n\nprint(f'Root Mean Square Error test = {score(stacking_pred)}')","25d09b1d":"final_test=(0.4*vote_pred+0.4*stacking_pred+ 0.2*y_lasso_test_pred)\nprint(f'Root Mean Square Error test=  {score(final_test)}')","220bf999":"#VotingRegressor to predict the final Test\nvote_test = vote_mod.predict(X_test)\nfinal1=np.expm1(vote_test)\n\n#StackingRegressor to predict the final Test\nstack_test = stackreg.predict(X_test)\nfinal2=np.expm1(stack_test)\n\n#LassoRegressor to predict the final Test\nlasso_test = lasso_mod.predict(X_test)\nfinal3=np.expm1(lasso_test)\n","952f795c":"# averaging the predictions before submitting\nfinal=(0.4*final1+0.4*final2+0.2*final3)","281c8574":"final","a4f466ea":"# creating submission file for submitting to the competition\noutput = pd.DataFrame({'Id': X_test_full.index,\n                       'SalePrice': final})\noutput.to_csv('submission.csv', index=False)\noutput.head()","a6e97713":"### Combining the training and testing sets before cleaning the data:","0bbb40aa":"The overall quality, the living area, basement area, garage cars(no. of cars that can be accomodated in the garage) and garage area have the highest correlation values with the sale price. That makes sense, as greater the area, higher the price. Also, better quality of the house yields a higher price.\n","46f2d92d":"### **Ensemble methods:**\nThe goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability \/ robustness over a single estimator.\n\nHaving run 4 different regression methods so far, we shall now combine them using ensemble methods.\n\n### Voting Regressor:\nA voting regressor is an ensemble meta-estimator that fits base regressors each on the whole dataset. It, then, averages the individual predictions to form a final prediction.","e18073c0":"We can see that the distributions for both features were skewed earlier to the right , i.e. positively skewed.\nAfter the application of log transform, the distributions are now less skewed and close to the normal distribution.\n\nNext, we will apply log transform to the target variable as well.","ea633c5b":"We use RobustScaler to scale our data because it's powerful against outliers.","58d7c564":"We will get rid of the features with more than 80% missing values . For example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 80%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 80% missing values.\nThe threshold for percentage of missing values can be changed as required depending on the dataset, and number of columns that would get dropped.","d2e4352d":"## Exploratory Data Analysis:\n\nWe shall look at the data and see what can be changed and what needs to be removed, so that a regression algorithm can better predict the final outcome, i.e. the price.\nThe dataset has 81 features and 1460 houses. Our target feature is the \"SalePrice\". The features are a mix of numerical and categorical features.","08ba1d61":"## Data Cleaning\n\nFirst, we take a look at the columns with the highest percentage of null values.","756bf9a8":"The rest of the features contain a large amount of null values. We will fill these with 'None' for the categorical and '0' for the numerical columns, assuming that a missing value indicates that, that particular feature is not present in the house - e.g. No value for 'GarageQual' would mean that the house does not have a garage.","d4cfc0f1":"# Housing Prices Prediction using Regression\n---\n\nThis code aims to predict housing prices in the city of Ames, Iowa using regression. The data is taken from the Kaggle competition for Housing prices prediction. We will analyze the data and clean it, and then run regression algorithms to achieve the lowest prediction error. The error metric used in this code is RMSE.","6ecceb8e":"We see the top 10 features with high correlation to Sale Price.","66fa931f":"Now that we know the index values of the outliers, we can remove them from the training data","9a4358b6":"### Skewness and Kurtosis of the numerical data\n\n**Skewness**: It is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\nIt differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n\nThere are 2 types of skewness, positive and negative. \n\n***Positive skewness*** indicates that the data is skewed towards the right side of the graph. The mean and median will be greater than the mode.\n***Negative skewness*** indicates that the data is skewed towards the left side of the graph. The mean and median will be less than the mode.\n\nA skewness value close to 0 indicates that the data is fairly symmetrical. So we need to bring the skewness value close to 0.\n\n**Kurtosis**: It is a measure of outliers in the distribution. It defines how heavily the tails of a distribution differ from the tails of a normal distribution.\n\nHigh kurtosis in a data set is an indicator that data has heavy tails or outliers.\nLow kurtosis in a data set is an indicator that data has light tails or lack of outliers.\n\nWe need the kurtosis of our data to be less than 3. This indicates that the extreme values are less than that of the normal distribution.\n","23035c5c":"As it turns out, almost all the houses have all utilities. Hence, this column can be dropped.","5da11356":"The outliers are the points in the right that have a larger area or value but a very low sale price. We localize those points by sorting their respective columns in descending order. The head() function uses the number of data points visible beyond the threshold.","c1d44f8c":"### Function to visualize columns with null values","a45dfadd":"As with ridge, we can observe some variation with lasso as well. The mean RMSE value is only slightly lower, not by much.","a2d88dda":"Below is an example of a highly skewed feature - Lot Frontage:","e7c91429":"We do the same thing with \"SalePrice\" column, we localize those outliers and make sure they are the right outliers to remove.","4469ab1c":"## Encoding Categorical Data\n\nWe use one-hot encoding to encode all the categorical features.","85bdd5a2":"We will use log transformation on the skewed variables with the help of the **np.log1p** function.","28230df3":"Out of the remaining columns, there are a few columns still having some missing values. We fill the numerical ones with the median of the data or 0s depending on the type of data it represents.","402aaeab":"Some of the columns have only a few missing values. We will use the forward fill method to fill these.","b5b06c8e":"### 3. ElasticNet:\nNext we use ElasticNet, which is a regressor that uses both L1 and L2 regularization. \nWe use cross-validation to find :\n1. Alpha: Constant that multiplies the penalty terms.\n2. L1-ratio: Ratio between the regularizers.","1bd7d4ed":"The RMSE score after cross-validation was slightly worse than the score earlier. We can see that there is some variation between the 5 folds. This score gives a better idea about the performance of the model on new data.\n\n### 2. Lasso\nNext, we shall apply L1 regularization, using Lasso. L1 regularization also penalizes the weights by forcing them to be close to zero, but it also makes some weights for uninformative features to be zero. This acts as feature selection when we have a large number of features, and thus also helps against overfitting.","021f0c8c":"## Machine Learning :\n\nWe split the data into training and validation sets.","7352cb2a":"### 4. XGBoost regressor:\n\nNext we shall try the XGBoost regressor and see if there is any improvement in the score.","99035fc4":"### Stacking Regressor:\nStacked generalization consists of stacking the output of individual estimator and using a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n\nWe stack all the previous models, including the Voting Regressor with XGBoost as the meta regressor\/ final regressor.","73a2cdd5":"## Detecting and eliminating Outliers\n\nOutliers are points in the dataset that seem to buck the trend, usually without a clear reason. An example would be an extremely large house selling for a very low price. We need to remove these from the dataset before training our regression model because they can negatively impact the weights the model will learn.","39fe0ef0":"### Cross-Validation:\nWe shall use cross validation to check if the error we got earlier holds true throughout the dataset. \nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter that refers to the number of groups that a given data sample is to be split into.\n\nWe will split our data into 5 folds. This runs the model 5 times, each time testing on a different chunk of data.\n","0f8586c6":"Most of the features do not have any null values and some have very few. We drop those columns from the dataset which have more than 20% missing values.","16cfe2e0":"### Categorical columns with NaNs :\n\nWe look at the categorical columns in the data and check which columns have null values.","bcb6c487":"In the above graph, we can see the most important features as observed by the lasso model. Total Area is one of the most significant, as well as a few Neighborhoods, just as we had thought earlier.","cf6d878b":"After Data cleaning, feature engineering and encoding, we now split the dataset back into training and test sets.","82a8e034":"We shall now look at the categorical features, as some categorical features like neighborhood tend to have a big impact on the price of a house. Houses in posh neighborhoods will be costlier than others.\nBelow, we check the number of houses for each neighborhood in our dataset.","3da98598":"## Feature Engineering\n\nWe will add a new feature called **'Total Area'** that sums up the areas of all the floors and the basement. This is because we know that area has a significant impact on the price of a house, and summing it up could help our regression algorithm predict better.","63f9b450":"We run GridSearchCV to get the best hyperparameters, which we will then apply and fit the model to the training set.","50d26e56":"### Regularization :\nRegularizations are techniques used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting. It basically adds a penalty as model complexity increases. The regularization parameter penalizes all the parameters except the intercept so that the model generalizes the data and won\u2019t overfit.\n\nWe shall first apply L2 regularizatio. This is done using Ridge regression. L2 regularization forces the weights to be close to zero, but does not make them zero. We need to find the optimum value of alpha(regularization parameter) so as to avoid underfitting. If alpha is too low, the model will generalize too much and end up underfitting the data.\n\n### 1. Ridge Regression:","8979a484":"Finally, we average out the results(predictions) of 3 models with the best RMSE values. The coefficients assigned were tested manually, and the selected combination yielded the best results.","5e26be01":"### Fit the models on test data:\nWe fit the models on the test data, and apply np.expm1 to convert the numbers back, since we log transformed the data earlier.","30f25362":"We take a look at the correlation between numerical features and the target \"SalePrice\", in order to have a first idea of the connections between features. Darker colors indicate a strong correlation and vice versa. As we can see, there are quite a few features in the dataset having a strong correlation with the Sale Price. This means that, changes in these features lead to changes in the target variable."}}