{"cell_type":{"e9762f5f":"code","54e689cd":"code","eb075761":"code","96cc79a0":"code","dbca29d1":"code","d644543e":"code","a45d5ee5":"code","4405d1fa":"code","24ae46b5":"code","0dce761c":"code","2c8bc63e":"code","8451cbbf":"code","fafc2c70":"code","6cec3dfc":"code","0c913acb":"code","a3326237":"code","0216b9e4":"code","efbc830a":"code","79eea75f":"code","84dd91bb":"code","13c6a883":"code","00ef13bf":"code","f4b8cdfd":"code","ecc85028":"code","40e6bf34":"code","e7c55dc2":"code","993d4295":"code","73da6416":"code","08ccabf7":"code","930daafe":"code","a29947fb":"code","aa360ead":"code","bf72251b":"code","8e9a5955":"code","b9e90dcb":"code","f61704d2":"code","3b67d4f0":"code","b2e0aa74":"code","ea0da156":"code","a20b316a":"code","ff5d13cb":"code","041f453b":"code","e9f7b66e":"code","caafe787":"code","9cfb4fb6":"code","64e522fe":"code","fcdb3441":"code","2ace36f7":"code","7135a9d7":"code","de192a69":"code","d579e45e":"code","7e014dfd":"code","5e7650a5":"code","e2b900ce":"code","57b24082":"code","448c25ab":"code","d391879c":"code","db1f62ec":"code","0a5b0c0c":"code","7866caaa":"code","35b6fa22":"code","44c6ce04":"code","4c7c5ed4":"code","0e76a538":"code","eef98c72":"code","20b7183b":"markdown","8b73f581":"markdown","acd66a97":"markdown","e6a556e7":"markdown","9c9ae8e1":"markdown","47fccffd":"markdown","365a62d5":"markdown","695076a4":"markdown","e7de1ad2":"markdown","52125df4":"markdown","fac4a786":"markdown","6481d21a":"markdown","c4a6f95d":"markdown","6d544a88":"markdown","6556738e":"markdown","6cc867e2":"markdown"},"source":{"e9762f5f":"import pandas as pd\nimport numpy as np ","54e689cd":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","eb075761":"train_id = train['id']\ntest_id = test['id']","96cc79a0":"train.drop(columns = ['id'], inplace = True)\ntest.drop(columns = ['id'], inplace = True)","dbca29d1":"train.isnull().sum()","d644543e":"test.isnull().sum()","a45d5ee5":"train.drop(columns = ['keyword','location'], inplace = True)\ntest.drop(columns = ['keyword','location'], inplace = True)","4405d1fa":"train.head()","24ae46b5":"# Converting all text to lowercase\ntrain['text'] = [t.lower() for t in train['text']]\ntest['text'] = [t.lower() for t in test['text']]","0dce761c":"# Removing punctuations\nimport re\nimport string\ntrain['text'] = [re.sub('[%s]' % re.escape(string.punctuation), '', i) for i in train['text']]\ntest['text'] = [re.sub('[%s]' % re.escape(string.punctuation), '', i) for i in test['text']]","2c8bc63e":"# Removing numeric characters\ntrain['text'] = [re.sub('\\d','',n) for n in train['text']]\ntest['text'] = [re.sub('\\d','',n) for n in test['text']]","8451cbbf":"import nltk\nfrom nltk.tokenize import word_tokenize","fafc2c70":"# Word Tokenization\n\ntrain['text'] = [word_tokenize(i) for i in train['text']]\ntest['text'] = [word_tokenize(i) for i in test['text']]","6cec3dfc":"train['text'].head()","0c913acb":"# Stop Words Removal\n\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\ntrain['text'] = [[i for i in j if not i in stop_words] for j in train['text']]\ntest['text'] = [[i for i in j if not i in stop_words] for j in test['text']]","a3326237":"train.head()","0216b9e4":"from collections import defaultdict\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import wordnet as wn\n\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\n\ntag_map","efbc830a":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ntrain['text'] = [[lemmatizer.lemmatize(word, tag_map[tag[0]]) for word, tag in pos_tag(i)] for i in train['text']]\ntest['text'] = [[lemmatizer.lemmatize(word, tag_map[tag[0]]) for word, tag in pos_tag(i)] for i in test['text']]","79eea75f":"train.head()","84dd91bb":"train['lemmatized_text'] = train['text'].apply(lambda x : ' '.join(x))\ntest['lemmatized_text'] = test['text'].apply(lambda x : ' '.join(x))","13c6a883":"train.head()","00ef13bf":"train.drop(columns = ['text'], inplace = True)\ntest.drop(columns = ['text'], inplace = True)","f4b8cdfd":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features = 5000)\n\ntrain_emb = tfidf.fit_transform(train['lemmatized_text']).toarray()\ntest_emb = tfidf.fit_transform(test['lemmatized_text']).toarray()","ecc85028":"train_emb.shape[1:]","40e6bf34":"y = train['target']","e7c55dc2":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score","993d4295":"MNB = MultinomialNB()","73da6416":"x_train,x_valid,y_train,y_valid = train_test_split(train_emb,y,test_size = 0.3, random_state = 100) ","08ccabf7":"MNB.fit(x_train,y_train)\npred_MNB = MNB.predict(x_valid)","930daafe":"print(\"Accuracy score : {:.2f}\".format(accuracy_score(y_valid, pred_MNB)))","a29947fb":"print(\"ROC-AUC score : {:.2f}\".format(roc_auc_score(y_valid, pred_MNB)))","aa360ead":"print(classification_report(y_valid, pred_MNB))","bf72251b":"MNB.fit(train_emb,y)","8e9a5955":"MNB_predictions = MNB.predict(test_emb)","b9e90dcb":"Prediction_results = pd.DataFrame({\"target\": MNB_predictions}, index = test_id)","f61704d2":"#submission_file = Prediction_results.to_csv('submission.csv')","3b67d4f0":"from sklearn import svm\nSVC = svm.SVC()\n#SVC.fit(x_train,y_train)\n#pred_SVC = SVC.predict(x_valid)","b2e0aa74":"#print(\"Accuracy score : {:.2f}\".format(accuracy_score(y_valid, pred_SVC)))","ea0da156":"#print(\"ROC-AUC score : {:.2f}\".format(roc_auc_score(y_valid, pred_SVC)))","a20b316a":"from collections import Counter\n\n# Finding the number of unique word in the corpus\ndef word_counter(text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","ff5d13cb":"train.head()","041f453b":"train_text = train.lemmatized_text\ncounter = word_counter(train_text)\ncounter","e9f7b66e":"print(\"Number of unique words in the corpus : {:.2f}\".format(len(counter)))","caafe787":"words = len(counter)\n# maximum number of words in a sequence\nmax_length = 20","9cfb4fb6":"train_sent = train['lemmatized_text']\ntrain_labels = train['target']\ntest_sent = test['lemmatized_text']","64e522fe":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=words)\ntokenizer.fit_on_texts(train_sent)","fcdb3441":"word_index = tokenizer.word_index\nword_index","2ace36f7":"train_sequence = tokenizer.texts_to_sequences(train_sent)","7135a9d7":"train_sequence[0]","de192a69":"test_sequence = tokenizer.texts_to_sequences(test_sent)","d579e45e":"test_sequence","7e014dfd":"from keras.preprocessing.sequence import pad_sequences\n\ntrain_padded = pad_sequences(train_sequence, maxlen = max_length, padding = \"post\", truncating = \"post\")","5e7650a5":"train_padded","e2b900ce":"test_padded = pad_sequences(test_sequence, maxlen = max_length, padding = \"post\", truncating = \"post\")","57b24082":"test_padded","448c25ab":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nimport tensorflow as tf\n\ndef leaky_relu(z, name = None):\n    return tf.maximum(0.01*z,z, name = name)\n\nmodel = Sequential()\n\nmodel.add(Embedding(words,32,input_length = max_length))\n#model.add(LSTM(128, return_sequences = True, dropout = 0.1))\nmodel.add(LSTM(64, dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = leaky_relu))\nmodel.add(Dense(1, activation = tf.nn.elu))\n\noptimizer = Adam(learning_rate = 3e-4)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])","d391879c":"model.summary()","db1f62ec":"model.fit(train_padded, train_labels, epochs = 40)","0a5b0c0c":"import h5py\n#model.save('baseline_lstm_model.h5')","7866caaa":"from keras.models import load_model\n#model = load_model('baseline_lstm_model.h5')","35b6fa22":"lstm_base_pred = model.predict_classes(test_padded, verbose = 0)","44c6ce04":"lstm_base_pred = lstm_base_pred.reshape(-1,1).ravel()","4c7c5ed4":"len(lstm_base_pred)","0e76a538":"Prediction_results_lstm = pd.DataFrame({\"target\":lstm_base_pred}, index = test_id)\nPrediction_results_lstm","eef98c72":"#submission_lstm_elu_leaky_relu = Prediction_results_lstm.to_csv('submission_lstm_elu_leaky_relu.csv')","20b7183b":"Splitting the train set in train and validation set to see how good is Naive Bayes for our data","8b73f581":"### Support Vector Machines","acd66a97":"### Sequencing and Sentence Padding","e6a556e7":"Location column has a lot of missing values and keyword column contains information that is present in the text column and we will be exracting that information down the line from the text column","9c9ae8e1":"## Alternative Approach using Sequencing, Padding, and LSTM","47fccffd":"## Model Training","365a62d5":"### Tokenization and Stop Words","695076a4":"### Model Training","e7de1ad2":"### Model Building using LSTM","52125df4":"### Word Embedding using TF_IDF Vectorizer","fac4a786":"#### Sequence Padding","6481d21a":"### Naive Bayes","c4a6f95d":"## Data Cleaning","6d544a88":"### Lemmatization","6556738e":"## Preprocessing Text Data","6cc867e2":"#### Multinomial Naive Bayes got a 0.515 score which is decent but can be significantly improved."}}