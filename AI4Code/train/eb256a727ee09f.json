{"cell_type":{"faa415ae":"code","91e149f3":"code","2c444750":"code","30dc1b6b":"code","79142235":"code","7065971f":"code","e9889693":"code","d847c27b":"code","cf011cad":"code","8fb80dbb":"code","7e38b1f4":"code","ce1648a4":"code","cc343e14":"code","ca7bd822":"code","7a2c525e":"code","20af6de1":"code","e5c29057":"code","57ca1d75":"code","28e07484":"code","a6ae3a38":"code","1cacffe4":"code","8ac9e33a":"code","93e15590":"code","7c067464":"code","d98be31b":"markdown","bd536703":"markdown","2a8d9045":"markdown","d4ca3012":"markdown","b788c05e":"markdown","6cc04051":"markdown","6fcd57dc":"markdown","aff967e6":"markdown","0649d7d5":"markdown"},"source":{"faa415ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# run this first cell\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# this will load all data from the competition\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91e149f3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2c444750":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_df.head()","30dc1b6b":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\ntest_df.head()","79142235":"train_df.info()","7065971f":"test_df.info()","e9889693":"train_df.describe().T","d847c27b":"total = np.product(train_df.shape)\nmissing = (train_df.isna().sum()).sum()\nprint(\"Percentage of missing values in train data: \", (missing\/total)*100)","cf011cad":"total = np.product(test_df.shape)\nmissing = (test_df.isna().sum()).sum()\nprint(\"Percentage of missing values in test data: \", (missing\/total)*100)","8fb80dbb":"train_df.drop('id',axis=1)\ntest_df.drop('id', axis=1)","7e38b1f4":"train_df[:5]","ce1648a4":"# collecting all the features\nfeatures = train_df.columns.drop(['id','claim'])\nprint(features)","cc343e14":"train_df['n_missing'] = train_df[features].isna().sum(axis=1)\ntest_df['n_missing'] = test_df[features].isna().sum(axis=1)","ca7bd822":"# replacing the NULL\/NA values with median values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = \"median\")\nfor col in features:\n    train_df[col] = imputer.fit_transform(np.array(train_df[col]).reshape(-1,1))\n    test_df[col] = imputer.transform(np.array(test_df[col]).reshape(-1,1))","7a2c525e":"# here we don't have any missing values present in both train and test\nprint((train_df.isna().sum()).sum())\nprint((test_df.isna().sum()).sum())","20af6de1":"# Scaling the values from 0 to 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfor col in features:\n    train_df[col] = scaler.fit_transform(np.array(train_df[col]).reshape(-1,1))\n    test_df[col] = scaler.transform(np.array(test_df[col]).reshape(-1,1))","e5c29057":"X = train_df.drop(['id','claim'], axis=1)\nX_test = test_df.drop('id', axis=1)\ny = train_df[\"claim\"]","57ca1d75":"print(X.shape)\nprint(X_test.shape)\nprint(y.shape)","28e07484":"X[:5]","a6ae3a38":"xgb_params = {\n    'n_estimators' : 3800,\n    'reg_lambda' : 3,\n    'reg_alpha' : 26,\n    'subsample' : 0.6000000000000001,\n    'colsample_bytree' : 0.6000000000000001,\n    'max_depth' : 9,\n    'min_child_weight' : 5,\n    'gamma' : 13.054739572819486,\n    'learning_rate': 0.01,\n    'tree_method': 'gpu_hist',\n    'booster': 'gbtree'\n}","1cacffe4":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier(**xgb_params)\nxgb_model.fit(X,y)","8ac9e33a":"predict = xgb_model.predict_proba(X_test)[:, 1]","93e15590":"predict[:5]","7c067464":"predictions = pd.DataFrame()\npredictions[\"id\"] = test_df[\"id\"]\npredictions[\"claim\"] = predict\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","d98be31b":"# pie char representing the amount of train and test data\nfig, ax = plt.subplots(figsize=(5,5))\npie = ax.pie([len(train_df), len(test_df)], \n             labels=['Trian dataset','Test dataset'],\n             colors=['cyan','violet'],\n             autopct='%1.1f%%')\nfig.set_facecolor('white')\nplt.title(\"Pie chart representing the amount of train and test data\")\nplt.show()","bd536703":"folds = 3\nparam_comb = 5\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n\n# Here we go\n# start_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X, y)\n# timer(start_time)","2a8d9045":"from xgboost import XGBClassifier\nxgb = XGBClassifier(objective='binary:logistic',silent=True)","d4ca3012":"**Exploratory Data Analysis**","b788c05e":"# countplot of claim value distribution\nsns.countplot(y= train_df['claim'], palette = 'Set1')\nplt.title(\"Count plot showing the claim value distribution\")\nprint(train_df['claim'].value_counts())","6cc04051":"**importing the data**","6fcd57dc":"# checking the missing values\nplt.figure(figsize=(10,10))\nsns.heatmap(train_df.isna(), cmap = 'viridis', yticklabels=False, cbar=False)","aff967e6":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5],\n        'learning_rate':[0.01,0.001,0.0001],\n        'tree_method':'gpu_hist',\n        'booster': 'gbtree',\n        'n_estimators' : [3000,3600],\n        'reg_alpha' : [25,26],\n        }","0649d7d5":"**importing the modules**"}}