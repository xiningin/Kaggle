{"cell_type":{"9f68b068":"code","014aa037":"code","a2767a80":"code","ad45f9b1":"code","8fe43b09":"code","ae9ce2f1":"code","f3967ed5":"code","3f5fa07c":"markdown","6d7af259":"markdown","bf8e6aa4":"markdown","1221ea88":"markdown"},"source":{"9f68b068":"# -*- coding: utf-8 -*-\nvocab_json_path  =\"..\/input\/dakshina-bnt-to-bn\/config.json\"\nmodel_weights_dir=\"..\/input\/dakshina-inspection\/\"\nfont_path        =\"..\/input\/dakshina-inspection\/Bangla.ttf\"\n#----------------------------------------\n# imports\n#----------------------------------------\nimport pandas as pd\nimport string\nimport os \nimport numpy as np\nimport json\nimport tensorflow as tf\nimport random\nfrom tqdm.auto import tqdm\nfrom scipy.special import softmax\nimport math\nimport matplotlib.pyplot as plt \nimport matplotlib.font_manager as fm\ntqdm.pandas()\nprop = fm.FontProperties(fname=font_path,size=20)\nplt.rcParams.update({'font.size': 20})\n# --------------------------------\n# plot utils\n#--------------------------------\n\ndef plot_attn_head(inp,tgt,attn,max_idx):\n    \n    attn=attn[:max_idx,:max_idx]\n    ax=plt.gca()\n    ax.matshow(attn)\n    ax.set_xticks(range(len(inp)))\n    ax.set_yticks(range(len(tgt)))\n    ax.set_yticklabels(tgt,fontproperties=prop)\n    ax.set_xticklabels(inp)\n    \ndef plot_attn_weights(inp,tgt,attn_heads):\n    fig=plt.figure(figsize=(30,15))\n    \n    for idx,val in enumerate(inp):\n        if val in [\"START\",\"END\"]:\n            inp[idx]=''\n    for idx,val in enumerate(tgt):\n        if val in [\"START\",\"END\"]:\n            tgt[idx]=''\n    inp=inp[1:]\n    tgt=tgt[1:]\n    inp_idx=inp.index('')\n    tgt_idx=tgt.index('')\n    max_idx=max(inp_idx,tgt_idx)\n    \n    inp=inp[:max_idx]\n    tgt=tgt[:max_idx]\n    for h,head in enumerate(attn_heads):\n        ax=fig.add_subplot(2,4,h+1)\n        plot_attn_head(inp,tgt,head,max_idx)\n        ax.set_xlabel(f\"Head:{h+1}\")\n    plt.tight_layout()\n    plt.show()\n#----------------------------------------\n# Model Config\n#----------------------------------------\nclass ModelConfig(object):\n    def __init__(self,\n                vocab_type,\n                vocab_json_path,\n                model_weights_dir,\n                num_layers       = 4,\n                num_heads        = 8,\n                d_model          = 128,\n                dff              = 512,\n                rate             = 0.1,\n                pe_max           = 50,\n                d_len            = 30,\n                pad_value        = 0,\n                start_value      = 1,\n                end_value        = 2,\n                inp_shape        = (30,),\n                tar_shape        = (30,)):\n        '''\n            initialize a model config\n            args:\n                vocab_type     : unicode or grapheme\n                vocab_json_path: path of the json file that contains vocabulary data\n            OPTIONAL:\n                num_layers :# number of encoder decoder layers\n                num_head   :# number of attention heads in MHA\n                d_model    :# Embedding Dimension \n                dff        :# Feed Forward netwrok Dimension\n                rate       :# dropout rate \n                pe_max     :# max positonal endocing \n                d_len      :# data length\n                pad_value  :# padding value in data encoding\n                start_value:# START token index\n                end_value  :# END token index\n                inp_shape  :# shape of model input\n                tar_shape  :# shape of model target\n        '''\n        assert os.path.exists(vocab_json_path),\"vocab json missing\/wrong file path\"\n        assert os.path.exists(f\"{model_weights_dir}model_{vocab_type}.h5\"),\"model weights missing\"\n        \n        with open(vocab_json_path) as f:\n            vocab_json_data = json.load(f)\n        assert \"source_vocab\" in vocab_json_data.keys(),\"source vocab missing\"\n        assert \"g_target_vocab\" in vocab_json_data.keys(),\"target grapheme vocab missing\"\n        assert \"u_target_vocab\" in vocab_json_data.keys(),\"target unicode vocab missing\"\n        \n        self.vocab_type       = vocab_type\n        self.vocab_json_data  = vocab_json_data\n        self.num_layers       = num_layers               \n        self.num_heads        = num_heads               \n        self.d_model          = d_model            \n        self.dff              = dff             \n        self.rate             = rate             \n        self.pe_max           = pe_max              \n        self.d_len            = d_len             \n        self.pad_value        = pad_value\n        self.start_value      = start_value\n        self.end_value        = end_value\n        self.inp_shape        = inp_shape\n        self.tar_shape        = tar_shape\n        self.inp_vocab        = vocab_json_data[\"source_vocab\"]\n        self.tgt_vocab        = vocab_json_data[f\"{vocab_type[0]}_target_vocab\"]\n        self.inp_voclen       = len(self.inp_vocab)\n        self.tgt_voclen       = len(self.tgt_vocab)\n        self.weight_path      = f\"{model_weights_dir}model_{vocab_type}.h5\"\n\ncfg_g=ModelConfig(\"grapheme\",vocab_json_path,model_weights_dir)\ncfg_u=ModelConfig(\"unicode\",vocab_json_path,model_weights_dir)\ndf= pd.read_csv(\"..\/input\/dakshina-inspection\/data.csv\")\ndf","014aa037":"class Masking(tf.keras.layers.Layer):\n    def __init__(self,pad_value,size,**kwargs,):  \n        super().__init__(**kwargs)\n        self.pad_value = pad_value\n        self.size      = size\n        \n    def create_padding_mask(self,seq):\n        '''\n            creates padding mask: fixed pad value 0\n        '''\n        seq = tf.cast(tf.math.equal(seq,self.pad_value), tf.float32)\n        # add extra dimensions to add the padding to the attention logits.\n        return seq[:,tf.newaxis, tf.newaxis, :] \n\n    def create_look_ahead_mask(self,size):\n        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n        # (seq_len, seq_len)\n        return mask  \n\n\n    def call(self,inp,tar):\n        '''\n            create relevent masks:\n            args:\n                inp : source encoded\n                tar : target endcoded\n            returns:\n                mask     : Encoder padding mask\n                          * Used in the 2nd attention block in the decoder,\n                          * This padding mask is used to mask the encoder outputs.\n                comb_mask: look ahead mask\n                          * Used in the 1st attention block in the decoder.\n                          * It is used to pad and mask future tokens in the input received by the decoder.\n        '''\n        #mask\n        mask            = self.create_padding_mask(inp)\n        # lmask\n        look_ahead_mask = self.create_look_ahead_mask(self.size)\n        dec_mask        = self.create_padding_mask(tar)\n        combined_mask   = tf.maximum(dec_mask, look_ahead_mask)\n\n        return mask,combined_mask\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({'pad_value'   : self.pad_value,\n                       'size'        : self.size})\n        return config\n\nclass PositionalEncoding(tf.keras.layers.Layer):\n    '''\n    tensorflow wrapper for positional encoding layer\n    args:\n      position  :   incoming sequence length\n      d_model   :   required embedding dim\n    '''\n    def __init__(self,position,d_model,use_scale,**kwargs,):  \n        super().__init__(**kwargs)\n        self.use_scale = use_scale\n        self.position  = position\n        self.d_model   = d_model\n        \n        \n    def call(self,x):\n        # pos encoding\n        pos=self.positional_encoding()\n        # input processing\n        seq_len = tf.shape(x)[1]\n        if self.use_scale:\n            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += pos[:, :seq_len, :]\n        return x \n    \n    def get_angles(self,pos, i):\n        angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(self.d_model))\n        return pos * angle_rates\n\n    def positional_encoding(self):\n        angle_rads = self.get_angles(np.arange(self.position)[:, np.newaxis],np.arange(self.d_model)[np.newaxis, :])\n        # apply sin to even indices in the array; 2i\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n        # apply cos to odd indices in the array; 2i+1\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n        pos_encoding = angle_rads[np.newaxis, ...]\n        return tf.cast(pos_encoding, dtype=tf.float32)\n\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'position'   : self.position,\n            'd_model'    : self.d_model,\n            'use_scale'  : self.use_scale\n        })\n        return config\n\n    \nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads,**kwargs,):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        assert d_model % num_heads == 0,\"Model Dimension Must be divideable by number of head provided\"\n        # attrs\n        self.num_heads   = num_heads\n        self.d_model     = d_model\n        self.depth       = self.d_model \/\/ self.num_heads\n        # ops\n        self.wq    = tf.keras.layers.Dense(d_model)\n        self.wk    = tf.keras.layers.Dense(d_model)\n        self.wv    = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        '''\n            * Split the last dimension into (num_heads, depth).\n            * Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        '''\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  ## (batch_size, seq_len, d_model)\n        k = self.wk(k)  ## (batch_size, seq_len, d_model)\n        v = self.wv(v)  ## (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  ## (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  ## (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  ## (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n        # (batch_size, seq_len_q, num_heads, depth)\n        scaled_attention                    = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n        # (batch_size, seq_len_q, d_model)\n        concat_attention                    = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  \n        # (batch_size, seq_len_q, d_model)\n        output                              = self.dense(concat_attention)  \n        return output, attention_weights\n    \n    def scaled_dot_product_attention(self,q, k, v, mask):\n        '''\n            Calculate the attention weights.\n\n            args:\n                q   : query shape == (..., seq_len_q, depth)\n                k   : key shape == (..., seq_len_k, depth)\n                v   : value shape == (..., seq_len_v, depth_v)\n                mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n            returns:\n                output, attention_weights\n            NOTES:\n            * q, k, v must have matching leading dimensions.\n            * k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n            * The mask has different shapes depending on its type(padding or look ahead) but it must be broadcastable for addition.\n\n        '''\n        ## (..., seq_len_q, seq_len_k)\n        matmul_qk = tf.matmul(q, k, transpose_b=True)  \n        # scale matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n        # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n        ## (..., seq_len_q, seq_len_k)\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n        ## (..., seq_len_q, depth_v)\n        output = tf.matmul(attention_weights, v)  \n        return output, attention_weights\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'd_model'  : self.d_model,\n            'num_heads': self.num_heads,\n        })\n        return config\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  ## (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)                  ## (batch_size, seq_len, d_model)\n    ])\n\nclass EncoderBaseLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1,**kwargs,):\n        super(EncoderBaseLayer, self).__init__(**kwargs)\n        # attrs\n        self.num_heads = num_heads\n        self.d_model   = d_model\n        self.dff       = dff\n        self.rate      = rate\n        # ops\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x,mask,training=True):\n        ## op outs:(batch_size, input_seq_len, d_model)\n        attn_output, _ = self.mha(x, x, x, mask)  \n        attn_output    = self.dropout1(attn_output, training=training)\n        out1           = self.layernorm1(x + attn_output)  \n        \n        ffn_output     = self.ffn(out1)  \n        ffn_output     = self.dropout2(ffn_output, training=training)\n        out2           = self.layernorm2(out1 + ffn_output) \n        return out2\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'd_model'  : self.d_model,\n            'num_heads': self.num_heads,\n            'dff'      : self.dff,\n            'rate'     : self.rate\n        })\n        return config\n\n\nclass DecoderBaseLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1,**kwargs,):\n        super(DecoderBaseLayer, self).__init__(**kwargs,)\n        # attrs\n        self.num_heads = num_heads\n        self.d_model   = d_model\n        self.dff       = dff\n        self.rate      = rate\n        # ops\n        self.mha1 = MultiHeadAttention(self.d_model, self.num_heads)\n        self.mha2 = MultiHeadAttention(self.d_model, self.num_heads)\n        \n        self.ffn  = point_wise_feed_forward_network(self.d_model, self.dff)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(self.rate)\n        self.dropout2 = tf.keras.layers.Dropout(self.rate)\n        self.dropout3 = tf.keras.layers.Dropout(self.rate)\n\n    def call(self, x, enc,comb_mask,mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        ##op outs:(batch_size, target_seq_len, d_model)\n        attn1, attn_weights_block1 = self.mha1(x, x, x, comb_mask)  \n        attn1                      = self.dropout1(attn1)\n        out1                       = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc, enc, out1,mask)  \n        attn2                      = self.dropout2(attn2)\n        out2                       = self.layernorm2(attn2 + out1)  \n\n        ffn_output                 = self.ffn(out2)  \n        ffn_output                 = self.dropout3(ffn_output)\n        out3                       = self.layernorm3(ffn_output + out2)  \n\n        return out3, attn_weights_block1, attn_weights_block2\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'd_model'  : self.d_model,\n            'num_heads': self.num_heads,\n            'dff'      : self.dff,\n            'rate'     : self.rate\n        })\n        return config\ndef Encoder(inp,mask,cfg):\n    x   = tf.keras.layers.Embedding(cfg.inp_voclen,cfg.d_model,name=\"EncoderInputEncoding\")(inp)\n    x   = PositionalEncoding(cfg.pe_max,cfg.d_model,True,name=\"EncoderPositionalEncoding\")(x)\n    for i in range(cfg.num_layers):\n        x=EncoderBaseLayer(cfg.d_model, cfg.num_heads, cfg.dff, cfg.rate,name=f\"EncoderLayer_{i}\")(x,mask)\n    return x\n\ndef Decoder(tar,enc,mask,comb_mask,cfg):\n    x   = tf.keras.layers.Embedding(cfg.tgt_voclen,cfg.d_model,name=\"DecoderInputEncoding\")(tar)\n    x   = PositionalEncoding(cfg.pe_max,cfg.d_model,True,name=\"DecoderPositionalEncoding\")(x)\n    x   = tf.keras.layers.Dropout(cfg.rate)(x)\n    w_attn={}\n    for i in range(cfg.num_layers):\n        x,awb1,awb2=DecoderBaseLayer(cfg.d_model, cfg.num_heads, cfg.dff, cfg.rate,name=f\"DecoderLayer_{i}\")(x,enc,comb_mask,mask)\n        w_attn[f'decoder_layer{i+1}_block1'] = awb1\n        w_attn[f'decoder_layer{i+1}_block2'] = awb2\n    x=tf.keras.layers.Dense(cfg.tgt_voclen,name=\"logits\")(x)\n    return x,w_attn    \n\n\ndef net(cfg):\n    inp           = tf.keras.layers.Input(shape=cfg.inp_shape,name=\"input\")\n    tar           = tf.keras.layers.Input(shape=cfg.tar_shape,name=\"target\")\n    mask,comb_mask=Masking(pad_value=cfg.pad_value,size=cfg.d_len,name=\"masks\")(inp,tar)\n    enc= Encoder(inp,mask,cfg)\n    x,w_attn=Decoder(tar,enc,mask,comb_mask,cfg)\n    model=tf.keras.Model(inputs=[inp,tar],outputs=[x,w_attn],name=\"TransformerBaseNet\")\n    return model\n","a2767a80":"class Tralsliterator(object):\n    def __init__(self,cfg):\n        self.cfg=cfg\n        self.transformer=net(cfg)\n        self.transformer.load_weights(cfg.weight_path)\n        print(f\"Loaded Transliterator Weights:{cfg.vocab_type}\")\n    def process_sentence(self,sentence,return_processed_sentence=False):\n        '''\n            process a sentence with model tokens\n        '''\n        words=[]\n        # clean the sentence\n        sentence=sentence.lower()\n        for word in sentence.split():\n            word=word.translate(str.maketrans('', '', string.punctuation))\n            for ch in word:\n                if ch not in self.cfg.inp_vocab:\n                    word=word.replace(ch,\"\")\n            if word.strip():\n                words.append(word)\n        if return_processed_sentence:\n            return \" \".join(words)\n        tokens=[]\n        # encode\n        for word in words:\n            word=[ch for ch in word]\n            word=[self.cfg.inp_vocab[self.cfg.start_value]]+word+[self.cfg.inp_vocab[self.cfg.end_value]]\n            token=[self.cfg.inp_vocab.index(ch) for ch in word]\n            token+=[self.cfg.pad_value for _ in range(self.cfg.d_len-len(token))]\n            tokens.append(token)\n        return np.array(tokens)\n    def transliterate(self,sentence,process_attention_visualization=False):\n        preds=[]\n        inp=self.process_sentence(sentence)\n        label=np.ones_like(inp,dtype=\"int64\")*self.cfg.start_value\n        # sequential decoding\n        for i in tqdm(range(self.cfg.d_len)):\n            pred,_=self.transformer.predict({\"input\":inp,\"target\":label})\n            pred  =pred[:,i,:]\n            preds.append(pred)\n            char_out=softmax(pred,axis=-1)\n            max_idx =np.argmax(char_out,axis=-1)\n            if i < self.cfg.d_len - 1:\n                label[:, i + 1] = max_idx\n\n            words=[]\n            for w_label in label:\n                _label=[]\n                for v in w_label[1:]:\n                    if v==self.cfg.end_value:\n                        break\n                    _label.append(v)\n                words.append(\"\".join([self.cfg.tgt_vocab[l] for l in _label]))\n        \n        # attention visualization data \n        if process_attention_visualization:\n            _,w_attn=self.transformer.predict({\"input\":inp,\"target\":label})\n            viz_data=[]\n            for idx,word_inp in enumerate(inp):\n                word_tgt=label[idx]\n                word_inp=[self.cfg.inp_vocab[i] for i in word_inp]\n                word_tgt=[self.cfg.tgt_vocab[i] for i in word_tgt]\n                word_attn ={}\n                for k,v in w_attn.items():\n                    word_attn[k]=v[idx]\n                viz_data.append({\"inp\":word_inp,\"tgt\":word_tgt,\"attn\":word_attn})\n            return viz_data,\" \".join(words)\n        # prediction\n        else:\n            return \" \".join(words)\n        \n","ad45f9b1":"model_g=Tralsliterator(cfg_g)# grapheme based transliterator\nmodel_u=Tralsliterator(cfg_u)# unicode based transliterator","8fe43b09":"def get_random_prediction():\n    idx=random.randint(0,len(df))\n    bangla  =df.iloc[idx,0]\n    sentence=df.iloc[idx,1]\n    g_pred=model_g.transliterate(sentence)\n    u_pred=model_u.transliterate(sentence)\n    print(\"=======================TAKLA=========================\")\n    print()\n    print(sentence)\n    print()\n    print(\"=======================GROUND TRUTH==================\")\n    print()\n    print(bangla)\n    print()\n    print(\"================GRAPHEME BASED TRANSLITERATION======\")\n    print()\n    print(g_pred)\n    print()\n    print(\"================UNICODE BASED TRANSLITERATION=======\")\n    print(u_pred)\n    print()\n    \nget_random_prediction()","ae9ce2f1":"idx=random.randint(0,len(df))\nbangla  =df.iloc[idx,0]\nsentence=df.iloc[idx,1]\nviz_data,g_pred=model_g.transliterate(sentence,process_attention_visualization=True)\nprint(\"=======================TAKLA=========================\")\nprint()\nprint(sentence)\nprint()\nprint(\"=======================BANGLA=======================\")\nprint()\nprint(bangla)\nprint()\nprint(\"================GRAPHEME BASED TRANSLITERATION======\")\nprint()\nprint(g_pred)\nprint()\nprint(\"Word Present:\",len(viz_data))","f3967ed5":"word_index=random.randint(0,len(viz_data)-1)\nblock_name='decoder_layer4_block2'\ndata=viz_data[word_index]\ninp=data[\"inp\"]\ntgt=data[\"tgt\"]\nattn=data[\"attn\"][block_name]\nplot_attn_weights(inp,tgt,attn)","3f5fa07c":"# Attention Scores","6d7af259":"# Prediction","bf8e6aa4":"# Modeling","1221ea88":"# Tralsliterator"}}