{"cell_type":{"33e3350d":"code","39d0dd41":"code","b580b111":"code","d65d9e04":"code","26cbd321":"code","50e80b72":"code","6561c57a":"code","7d5a3d50":"code","a316045a":"code","65d9ae24":"code","1965a7f3":"code","b5afbc96":"code","e230f9a0":"code","8cb9a5d2":"code","bdb260d7":"code","9951cd34":"code","7a2fed45":"code","588874f8":"code","1de4872a":"code","5431190a":"code","35f7c264":"code","7353303d":"code","88f991f8":"code","0785de60":"code","d43136e0":"code","c81ad02c":"code","5cab1b65":"code","2de6c0fb":"code","f2fa4fd7":"code","2ce80460":"code","67c4fc2f":"code","0da7d0e7":"markdown","95974ef3":"markdown","5e7b6a2d":"markdown","9eafb1a9":"markdown","2c4a0de0":"markdown","7087f5ae":"markdown","3f35c053":"markdown","52879ae4":"markdown"},"source":{"33e3350d":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings  \nwarnings.filterwarnings('ignore')","39d0dd41":"pchembl_data = pd.read_csv('\/kaggle\/input\/complete70037\/complete_data.csv')\ndescriptors = pd.read_csv('\/kaggle\/input\/all-data-desc\/comp_data_desc.csv')\ndescriptors.drop('Name', 1, inplace=True)\ndescriptors","b580b111":"col_add = ['molecule_chembl_id','pchembl_value','target_pref_name','bao_label']\nfor each in col_add:\n    descriptors[each] = pchembl_data[each]\n    \ndesc = descriptors.copy()","d65d9e04":"desc = desc[desc.pchembl_value.notnull()]","26cbd321":"desc.target_pref_name.value_counts().plot.bar(figsize=(25,4), color='#00BFC4', ec='black')","50e80b72":"desc.bao_label.value_counts().plot.bar(figsize=(8,4), color='#00BFC4', ec='black')","6561c57a":"desc1 = desc[(desc.bao_label ==  'assay format') & (desc.target_pref_name == 'Beta-lactamase AmpC')]","7d5a3d50":"desc1.dropna(inplace = True)\ndesc1.isnull().sum().sum()","a316045a":"pchembl_std = desc1.groupby('molecule_chembl_id').agg({'pchembl_value': 'std'})\npchembl_std[pchembl_std.pchembl_value > 2]","65d9ae24":"desc1.drop_duplicates(subset = ['molecule_chembl_id'],inplace=True)","1965a7f3":"conditions = [(desc1['pchembl_value'] < 5) , (desc1['pchembl_value'] >=5) & (desc1['pchembl_value'] <6 ) , (desc1['pchembl_value'] >= 6)]\nvalues = [-1, 0, 1]\ndesc1['activity_class'] = np.select(conditions, values)","b5afbc96":"desc1.activity_class.value_counts().plot.bar(figsize=(8,4), color='#00BFC4', ec='black')","e230f9a0":"desc1.drop(['molecule_chembl_id','target_pref_name','bao_label','pchembl_value'],1, inplace = True)","8cb9a5d2":"# removing low variance features\n\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0.1)\nvar_thres.fit(desc1)","bdb260d7":"desc1.columns[var_thres.get_support()]\nconstant_columns = [column for column in desc1.columns\n                    if column not in desc1.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","9951cd34":"desc1 = desc1.drop(constant_columns,axis=1)","7a2fed45":"def remove_collinear_features(df_model, target_var, threshold, verbose):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold and which have the least correlation with the target (dependent) variable. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        df_model: features dataframe\n        target_var: target (dependent) variable\n        threshold: features with correlations greater than this value are removed\n        verbose: set to \"True\" for the log printing\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = df_model.drop(target_var, 1).corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n    dropped_feature = \"\"\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1): \n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                if verbose:\n                    print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                col_value_corr = df_model[col.values[0]].corr(df_model[target_var])\n                row_value_corr = df_model[row.values[0]].corr(df_model[target_var])\n                if verbose:\n                    print(\"{}: {}\".format(col.values[0], np.round(col_value_corr, 3)))\n                    print(\"{}: {}\".format(row.values[0], np.round(row_value_corr, 3)))\n                if col_value_corr < row_value_corr:\n                    drop_cols.append(col.values[0])\n                    dropped_feature = \"dropped: \" + col.values[0]\n                else:\n                    drop_cols.append(row.values[0])\n                    dropped_feature = \"dropped: \" + row.values[0]\n                if verbose:\n                    print(dropped_feature)\n                    print(\"-----------------------------------------------------------------------------\")\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    df_model = df_model.drop(columns=drops)\n\n    print(\"dropped columns: \")\n    print(list(drops))\n    print(\"-----------------------------------------------------------------------------\")\n    print(\"used columns: \")\n    print(df_model.columns.tolist())\n\n    return df_model","588874f8":"desc2 = remove_collinear_features(desc1,'activity_class',0.75,True)\ndesc2","1de4872a":"X = desc2.drop('activity_class', axis=1)\ny = desc2.activity_class","5431190a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =72)","35f7c264":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\nrf = RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_leaf=8,\n                       min_samples_split=3, n_estimators=15)\n\nrf.fit(X_train,y_train)\n\ny_pred_rf = rf.predict(X_test)\nacc_rf = round( metrics.accuracy_score(y_test, y_pred_rf) * 100 , 2 )\nprint( 'Accuracy of Random Forest model : ', acc_rf )\n","7353303d":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(y_test, y_pred_rf, labels=rf.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\ndisp.plot()","88f991f8":"# feature importance \nplt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')\n\nfeat_importances = pd.Series(rf.feature_importances_, index= X_train.columns)\n\nfeat_importances.nlargest(50).plot(kind='barh')","0785de60":"cols = feat_importances.nlargest(10).index\ncols","d43136e0":"sel_cols = ['ATSC3e',  'GATS8s', 'CrippenLogP', 'BCUTp-1l',\n        'MLFER_BO',  'AATSC8s' ]","c81ad02c":"from pandas.plotting import parallel_coordinates\n\nplt.figure(figsize=(20,10), dpi= 80)\n    \nfor i in range(2) :\n    plt.figure(figsize=(20,10), dpi= 80)\n    \n    x = 3*i\n    parallel_coordinates(desc2, 'activity_class' , cols =sel_cols[0+x:3+x],colormap='brg')\n    plt.gca().spines[\"top\"].set_alpha(0)\n    plt.gca().spines[\"bottom\"].set_alpha(.3)\n    plt.gca().spines[\"right\"].set_alpha(0)\n    plt.gca().spines[\"left\"].set_alpha(.3)\n    plt.grid(alpha=0.3)\n\n    #plt.title(, fontsize=22)\n    plt.xticks(fontsize=12, rotation = 45)\n    plt.yticks(fontsize=12)\n    plt.show()","5cab1b65":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(criterion='entropy', max_depth=3, max_features='sqrt',\n                       min_samples_leaf=10, min_samples_split=3)\n\n\n\n\n# Train the model using the training sets \nclf.fit(X_train, y_train)\ny_preddt = clf.predict(X_test)\nacc_dt = round( metrics.accuracy_score(y_test, y_preddt) * 100, 2 )\nprint( 'Accuracy of Decision Tree model : ', acc_dt )","2de6c0fb":"cm_dt = confusion_matrix(y_test, y_preddt, labels=clf.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=clf.classes_)\ndisp.plot()","f2fa4fd7":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_t = sc.fit_transform(X_train)\nX_test_t = sc.transform(X_test)","2ce80460":"from sklearn.svm import SVC\n\nsvc = SVC(C = 1 , kernel = 'rbf' , gamma = 0.001)\nsvc.fit(X_train_t,y_train)\n\ny_pred_svm = svc.predict(X_test_t)\n\nacc_svm = round( metrics.accuracy_score(y_test, y_pred_svm) * 100, 2 )\nprint( 'Accuracy of SVM model : ', acc_svm  )","67c4fc2f":"cm_svm = confusion_matrix(y_test, y_pred_svm, labels=svc.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=svc.classes_)\ndisp.plot()","0da7d0e7":"### Using those rows whose target_pref_name == 'Beta-lactamase AmpC' and bao_label ==  'assay format'","95974ef3":"### **Parallel coordinates Plot**\n\n#### Parallel coordinates is a visualization technique used to plot individual data points across multiple features. Each of the features corresponds to a vertical axis and each data element is displayed as a series of connected points along the measure\/axes.\n\n#### Parallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group.\n\n#### Following plots displayes some of the top features and the grouping of the data points based on the 3 activity class selected through random forest feat_importance ","5e7b6a2d":"#### The hyperparameters of the following classification models were calculated using GridSearchCV","9eafb1a9":"### The descriptor values of the compounds were calculated using **PaDEL Descriptors Software**. A total of **2325** descriptors were calculated that includes **1D** and **2D** descriptors and **Pubchem Fingerprints**.","2c4a0de0":"### Checking for duplicates","7087f5ae":"#### No value has STD > 2\n\n#### Removing the duplicates","3f35c053":"#### Activity-class of compounds based on their pchembl values:\n##### pchembl_value < 5 = Inactive(-1)\n##### pchembl_value between 5 and 6 = Intermediate(0)\n##### pchembl_value >= 6 = Active(1)","52879ae4":"#### **Note :**\n\n* *The Parallel Coordinate plot was constructed using the top features predicted by random forest feat_importance. But there is not much distinction or grouping of the 3 classes seen in the plot. This indicates that the activity classification based the pchembl_value ( which is the log tranformed value of different drug potency values) of different compound may not be producing a clear distinction among the classes.*\n\n* *It is presumed that using the Standard_value column as the target variable and using those values to classify the activity of the molecules may produce better model.*\n\n* *Different models for different standard_type (potency, IC50, inhibition, Kcat\/km etc.) of the molecules may produce better models.*\n\n\nMy next notebook submission would be of both regression and classification model based on the potency measures of the drug molecule. \n\n\n\n"}}