{"cell_type":{"dc8ad15f":"code","5da9c914":"code","76eb275c":"code","29edf1e6":"code","0cd59699":"code","39e3a6e7":"code","7679c256":"code","6115a319":"code","610d0e42":"code","96bb3ecb":"code","cf0a0492":"code","0348fe45":"code","05c28488":"code","02833eb4":"code","098af8db":"code","7d3525e0":"code","ae1f3ece":"code","798094d2":"code","6149fcce":"code","319b607b":"code","5891019f":"code","817df9e9":"code","dbb24950":"code","773f234c":"code","4d9004cf":"code","2d4b0400":"code","30d35ec0":"code","c6ba67d0":"code","672afd64":"code","cd425f02":"code","778bb552":"code","a7d306cd":"code","805a3dbe":"code","62a36a58":"code","ab0f9b1c":"code","1c24d9b2":"code","90f36312":"code","2d5cab60":"code","1b28c3f3":"code","22905049":"code","98511c11":"code","df101acd":"code","42f4c80c":"code","5273b93d":"code","fcedbc76":"code","cd588008":"code","5100b804":"code","7d9274b5":"code","321baa64":"code","28416187":"code","3f470bfb":"code","2abe63e0":"code","cb6c92e2":"code","8a57e8f9":"code","3b8f32b4":"code","9d72d843":"markdown","6939c434":"markdown","ed55dd1a":"markdown","3d1b6632":"markdown","6e2e5758":"markdown","49fff3f3":"markdown","1491cf1f":"markdown","2b314204":"markdown","719e9f04":"markdown","690d6253":"markdown","a0edaf52":"markdown","78e19220":"markdown","8b480101":"markdown","2df895fc":"markdown","4c87beef":"markdown","cc547f3a":"markdown","c84f0fd6":"markdown","176855aa":"markdown","7b6da986":"markdown","ff9407de":"markdown","ab166712":"markdown","2459e680":"markdown","e242e091":"markdown","459fa075":"markdown","6d83f2c1":"markdown"},"source":{"dc8ad15f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\nwarnings.filterwarnings('ignore')","5da9c914":"df = pd.read_csv(\"..\/input\/hrworks-dataset\/hr_data.csv\")","76eb275c":"df.head()","29edf1e6":"df.dtypes","0cd59699":"df.shape","39e3a6e7":"df.describe()","7679c256":"df.describe(include=\"object\")","6115a319":"df.isnull().sum()","610d0e42":"df['Offered band'].fillna(method='ffill',inplace=True)\ndf['Age'].fillna(df.groupby('Gender')['Age'].transform('mean'), inplace = True) ","96bb3ecb":"df.isnull().sum()","cf0a0492":"df.drop('SLNO', axis=1, inplace=True)\ndf.drop(columns=['Offered band','Candidate relocate actual'])","0348fe45":"df['Status'] = df['Status'].map(lambda x:1 if x=='Joined' else 0)","05c28488":"df['DOJ Extended'].value_counts()","02833eb4":"df['LOB'].value_counts()","098af8db":"df['Gender'].value_counts()","7d3525e0":"df['Candidate Source'].value_counts()","ae1f3ece":"df['Location'].value_counts()","798094d2":"df.head()","6149fcce":"Y = df[\"Status\"]\ntotal = len(Y)*1.\nax = sns.countplot(x=\"Status\", data=df)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","319b607b":"def plot_withY(label, dataset):\n    plt.figure(figsize=(10,10))\n    sns.countplot(x=label, data=df, hue=\"Status\")\n    plt.xticks(rotation=45)\n    plt.show()","5891019f":"plot_withY('Gender', df)","817df9e9":"plot_withY('Candidate Source', df)","dbb24950":"plot_withY('Joining Bonus', df)","773f234c":"def plot_num(feature):\n    sns.boxplot(data=df, x='Status', y = feature)\n    plt.show()","4d9004cf":"plot_num('Duration to accept offer')","2d4b0400":"plot_num('Notice period')","30d35ec0":"plot_num('Pecent hike expected in CTC')","c6ba67d0":"plot_num('Percent hike offered in CTC')","672afd64":"plot_num('Percent difference CTC')","cd425f02":"plot_num('Rex in Yrs')","778bb552":"plot_num('Age')","a7d306cd":"plt.figure(figsize=(20,10))\nnorm = plt.Normalize(vmin=0,vmax=10)\nsns.scatterplot(x= 'Percent hike offered in CTC', y= 'Pecent hike expected in CTC', hue='Status', data=df)\n#sns.relplot(, hue='Status', data=df)\nplt.show()","805a3dbe":"plt.figure(figsize = (7,7))\n\nsns.heatmap(df.corr(), annot = True)","62a36a58":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn import metrics","ab0f9b1c":"x_feature = list(df.columns)\nx_feature.remove('Status')\nencoded_data = pd.get_dummies(df[x_feature], drop_first = True) \ny = df['Status']\nx = encoded_data","1c24d9b2":"x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.7, random_state = 42)","90f36312":"log_reg=LogisticRegression(max_iter=55,solver= \"newton-cg\")\nlog_reg.fit(x_train,y_train)\ny_pred = log_reg.predict(x_test)\nlog_reg.predict_proba(x_test)","2d5cab60":"print(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.accuracy_score(y_test,y_pred))","1b28c3f3":"from sklearn.model_selection import GridSearchCV\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\nclf = GridSearchCV(LogisticRegression(),grid,cv=10,scoring = 'roc_auc')\nclf.fit(x_train, y_train)\ntrain_predictions = clf.predict(x_test)","22905049":"print(metrics.classification_report(y_test, train_predictions))","98511c11":"from sklearn.ensemble import RandomForestClassifier","df101acd":"clf=RandomForestClassifier()\nclf.fit(x_train,y_train)","42f4c80c":"y_pred_clf=clf.predict(x_test)\nprint(metrics.confusion_matrix(y_test,y_pred_clf))\nprint(metrics.accuracy_score(y_test,y_pred_clf))\nprint(metrics.classification_report(y_test,y_pred_clf))","5273b93d":"from sklearn.utils import resample","fcedbc76":"x_train_u, y_train_u = resample(x_train[y_train==1],\n                               y_train[y_train==1],\n                               n_samples = x_train[y_train==0].shape[0],\n                               random_state = 1)\n\nx_train_u = np.concatenate((x_train[y_train==0],x_train_u))\ny_train_u = np.concatenate((y_train[y_train==0],y_train_u))\nprint(x_train_u.shape)\nprint(y_train_u.shape)","cd588008":"log_reg_up = LogisticRegression()\nlog_reg_up.fit(x_train_u, y_train_u)\ny_pred_up = log_reg_up.predict(x_test)\nprint(metrics.classification_report(y_test, y_pred_up))","5100b804":"log_reg_clf = RandomForestClassifier()\nlog_reg_clf.fit(x_train_u, y_train_u)\ny_pred_clfu = log_reg_up.predict(x_test)\nprint(metrics.classification_report(y_test, y_pred_clfu))","7d9274b5":"from sklearn.model_selection import GridSearchCV","321baa64":"# Logistic Regression Cross Validation\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\nLR = GridSearchCV(LogisticRegression(),grid,cv=10,scoring = 'roc_auc')\nLR.fit(x_train_u, y_train_u)\ntrain_predictions = LR.predict(x_test)","28416187":"print(metrics.classification_report(y_test,train_predictions))","3f470bfb":"# Random Forest Cross Validation\ntunned_parameters =  [{'n_estimators': [200, 500],\n                       'max_features': ['auto', 'sqrt', 'log2'],\n                       'max_depth' : [4,5,6,7,8],\n                       'criterion' :['gini', 'entropy']}]\nRF = GridSearchCV(RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True),\n                  tunned_parameters,\n                  cv=5,\n                  scoring = 'roc_auc')\nRF.fit(x_train_u,y_train_u)\ny_pred_rf = RF.predict(x_test)\nprint(metrics.classification_report(y_test,y_pred_rf))","2abe63e0":"from imblearn.ensemble import EasyEnsembleClassifier","cb6c92e2":"ensem=EasyEnsembleClassifier()","8a57e8f9":"ensem.fit(x_train_u, y_train_u)\ny_pred_en = ensem.predict(x_test)\nprint(metrics.confusion_matrix(y_test, y_pred_en))\nprint(metrics.accuracy_score(y_test,y_pred_en))\nprint(metrics.classification_report(y_test,y_pred_en))","3b8f32b4":"importance = log_reg.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(20,10))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","9d72d843":"This shows the percent of people who did not join versus the percent who joined. We can see that there is data imbalance. We will deal with this during the model Building.","6939c434":"We can see that there are very minor number of null values in two of the features. We can either drop this or fill it up. I am replacing the null values. ALso, I am considering Gender as a factor to influence Age","ed55dd1a":"###### Problem Statement:\n- HRWorks supports several information technology (IT) companies in India with their talent acquisition. One of the challenges they face is about 30% of the candidates who accept the jobs offer do not join the company. This leads to huge loss of revenue and time as the companies initiate the recruitment process again to fill the workforce demand. HRWorks wants to find if a model can be built to predict the likelihood of a candidate joining the company.","3d1b6632":"After plotting for each and everyone of the categorical variable, there is nothing worthwhile that can be sumamrzed from the analysis because of the huge data imbalance. So, next let us look into the Numerical variables.","6e2e5758":"After the analysis, we can see the following conclusions:\n\n- None of the categorical variable had any effect on our target variable\n- Among the numerical variables, we have some observations.\n- When we plot age we can see that there are some outliers. But among the candidates who did not choose to join, we can see that there are some extreme outlier towards the age 60. So, there are chances that age affects our target variable.\n- When we consider years of experience, we can see some extreme outliers among the candidates who joined. We can assume that there is slight effect by years of experience on target variable.\n- When we take the CTC offered, we can see that there are a large number of outliers to much extreme values. This shows that more candidates join when offered higher ctc. When we analysis the hike expected that the outliers are higher among teh candidates who joined. So, we can conclude that these two features are related. That is, when the candidates expect higher hike and if it is satisfied, the candidates tend to join.\n- We can conclude that lesser the notice period duration(peaking at 40 to 45 days), higher are the chances for the candidates to join. More the notice period, the longer the candidates will procrastinate and may decide not to join.\n- Also, we can see that there are many outliers in the feature \"Duration to accept the offer\" in the candidates who did not join. Reducing that duration can make sure that candidates do not drop out.","49fff3f3":"We had observed above that our data is imbalanced. SO before coming to any conclusion, we need to apply some sampling technique overcome this issue","1491cf1f":"#### 2. Define any hypothesis if possible\n\nHypothesis:\n- Null Hypothesis: All the candidates will join the firms\n- Alternate Hypothesis : Not all candidates join the firms","2b314204":"Next, let us convert our target variable from categorical to numerical value for ease of analysis and model building.","719e9f04":"### Analysis with categorical variables","690d6253":"We can see that there is an increasingly linear relation between the expected and offered ctc among the joined candidates. ","a0edaf52":"## Analysis\n### Univariate Analysis","78e19220":"#### Ensemble Technique\n\nLet us now try ensemble technique to see if the model can be further improved.","8b480101":"#### Q4. Develop a machine learning algorithms and compare different models.","2df895fc":"#### 1.Identify and define the problem statement clearly also mention why it is necessary for an organisation to solve the problem.\u00b6\n\nHRworks is a Workforce supply company which supplies people to differnet companies throughouht India, in the IT field. One of the major problem the company faces is that the people back out after accepting the offer letter. This creates a huge lose of the company in term sof money, resources and time. So here, the business problem is to creater a model to resolve this issue. We need to find the factors which affects this and hence will be easier to resolve.","4c87beef":"Let us now drop the unwanted columns that adds no values to our data.","cc547f3a":"## Machine Learning models\n\nNow we will create models using different algorithms and find out which is best among all.","c84f0fd6":"#### Random Forest","176855aa":"##### Let us now try to fine tune the parameters and do crossvalidation for both the algorithms","7b6da986":"## Data Preprocessing","ff9407de":"#### Logistic Regression","ab166712":"## Importing all the necessary libraries","2459e680":"First let us check for missing values in our data set","e242e091":"#### 3. Do the EDA of dataset and write the observation you got form the dataset?","459fa075":"After the Cross validation, we can see that the Logistic Regression model has the best scores.","6d83f2c1":"### Analysis of Numerical variables"}}