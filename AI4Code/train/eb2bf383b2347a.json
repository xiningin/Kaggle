{"cell_type":{"01f563f6":"code","918bbf71":"code","d4ac048a":"code","201b16fa":"code","1fc5a167":"code","b21ed080":"code","d6979af9":"code","98d3701e":"code","55aad991":"code","c83ddae8":"code","4aacdfab":"code","1cbad366":"code","3f21b98d":"code","42ce7e22":"code","bb7e22c3":"code","b0e5f9da":"code","7e4822fc":"code","bd24458a":"code","4d3dfff5":"code","ab7e8fc2":"code","3de55764":"code","12285314":"code","4942bbcd":"code","d2e0c213":"code","d262d5b8":"code","e3f4ca6c":"code","58d747ab":"code","be089a63":"code","8661174b":"code","205f48f0":"code","40655e49":"code","5b8206fc":"code","724ceb94":"code","6e50183e":"code","ff949dc4":"code","8b497dd5":"code","95841ff8":"code","f48c3414":"code","c3f523da":"code","21cd0baf":"code","13ea5c36":"code","9326fbbd":"code","753f10ae":"code","e6a8af5a":"code","07f9fcbc":"code","c6e18bd2":"code","2ad20b33":"code","234d3faf":"code","fe3e44d8":"code","f85d421e":"code","29af24bb":"code","24e509a3":"code","e98c5c79":"code","ba0302fb":"code","daf7caa5":"code","01664694":"code","09eadeeb":"code","c3d44b67":"code","3483b5ee":"code","10f97993":"code","86ac81f9":"code","80e8acaf":"code","87b6213c":"code","f71ddda0":"code","4521bd7d":"code","7b20e342":"code","595566b4":"code","41313dc7":"code","a223e68a":"code","d7104447":"code","584886e0":"code","68b23cd2":"code","2d89da5a":"code","a63d8d86":"code","018d3144":"code","7e7ad79b":"code","d15d23f5":"code","0aba2ed6":"code","74eaafe6":"code","bcd36346":"code","87db9966":"code","d881624c":"code","0600b4d1":"code","6f4f00d1":"code","d9a21cf3":"code","98a32a38":"code","067e3318":"code","6e20cd8e":"code","78c1a50d":"code","7dbab15c":"code","ac7f98c8":"markdown","3bd54230":"markdown","6b292dd9":"markdown","ce169297":"markdown","2d0d30f2":"markdown","4a77598e":"markdown","23e9e053":"markdown","17c8ec3d":"markdown","a5f7439f":"markdown","6dfd27b1":"markdown","bd54a760":"markdown","9258277e":"markdown","c68a17cd":"markdown","44023c3e":"markdown","acae644a":"markdown","d2010f40":"markdown","c84a14e1":"markdown","9e2d7ceb":"markdown","f5147e7f":"markdown","db0771be":"markdown","2baa69e1":"markdown","21b8c15f":"markdown","00c4bb61":"markdown","cb972671":"markdown","a7da0c36":"markdown","be80cfaf":"markdown","d7f176b4":"markdown","5b86aae0":"markdown","b008542b":"markdown","38095135":"markdown","1d198429":"markdown","8ebcdd5d":"markdown","96df9043":"markdown","fdaed607":"markdown","2fc2ad15":"markdown","f2bef1ac":"markdown","c85b61ff":"markdown","4f3b7ad5":"markdown","8a35fb7a":"markdown","883c51ed":"markdown","05073f05":"markdown","b760b041":"markdown","fdea6001":"markdown"},"source":{"01f563f6":"from __future__ import absolute_import, division, print_function, unicode_literals\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nsns.set(rc={'figure.figsize' : (12, 6)})\nsns.set_style(\"darkgrid\", {'axes.grid' : True})\n# plt.style.use('seaborn-whitegrid')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport skimage\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nfrom statsmodels.iolib.table import SimpleTable\n\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook","918bbf71":"air_res = pd.read_csv('..\/input\/recruit-restaurant-visitor-forecasting\/air_reserve.csv')\nair_store = pd.read_csv('..\/input\/recruit-restaurant-visitor-forecasting\/air_store_info.csv')\nhpg_res = pd.read_csv('..\/input\/recruit-restaurant-visitor-forecasting\/hpg_reserve.csv')\nhpg_store = pd.read_csv('..\/input\/recruit-restaurant-visitor-forecasting\/hpg_store_info.csv')\nair_visit = pd.read_csv('..\/input\/recruit-restaurant-visitor-forecasting\/air_visit_data.csv')\nid_rel = pd.read_csv('..\/input\/recruit-restaurant-visitor-forecasting\/store_id_relation.csv')","d4ac048a":"air_res.head()","201b16fa":"air_res.info()","1fc5a167":"air_store.head()","b21ed080":"air_store.info()","d6979af9":"air_visit.head()","98d3701e":"air_visit.info()","55aad991":"hpg_res.info()","c83ddae8":"id_rel.head()","4aacdfab":"air_res.air_store_id.nunique() # number of unique restaurants in air system","1cbad366":"hpg_res.hpg_store_id.nunique() # number of unique restaurants in hpg system","3f21b98d":"id_rel.air_store_id.nunique() # number of unique restaurants that are in both systems at once","42ce7e22":"# Rename some columns before merging\n# air_store.rename(columns={\"air_genre_name\": \"genre_name\", \" B\": \"c\"})","bb7e22c3":"# Merge tables\nair = pd.merge(air_res, air_store, on='air_store_id')\nhpg = pd.merge(hpg_res, hpg_store, on='hpg_store_id')","b0e5f9da":"air_rel = pd.merge(air, id_rel, how='left', on='air_store_id')\nhpg_rel = pd.merge(hpg, id_rel, how='left', on='hpg_store_id')\nfull = pd.merge(air_rel, hpg_rel, how='outer')","7e4822fc":"print(\"In air reservations are: %d \\nIn hpg reservations are: %d \\nIn both systems at once there are: %d\" % \\\n      (air.shape[0], hpg.shape[0], id_rel.shape[0]))\nprint(\"So, totally must be: %d reservations.\" \\\n      % (full.shape[0]))","bd24458a":"# Then we need to convert columns 'visit_datetime' and 'reserve_datetime' from object type -> to data\/time type\nfull['visit_datetime'] = pd.to_datetime(full['visit_datetime'])\nfull['reserve_datetime'] = pd.to_datetime(full['reserve_datetime'])","4d3dfff5":"full.info()","ab7e8fc2":"# Split converted date-time columns to year, month, date, day of week and time separate coluns of dataset\nfull['visit_year'] = pd.Series(full.visit_datetime.dt.year)\nfull['visit_month']  = pd.Series(full.visit_datetime.dt.month)\nfull['visit_date'] = pd.Series(full.visit_datetime.dt.day)\nfull['visit_weekday'] = pd.Series(full.visit_datetime.dt.weekday)\nfull['visit_time'] = pd.Series(full.visit_datetime.dt.time)","3de55764":"full['reserve_year'] = pd.Series(full.reserve_datetime.dt.year)\nfull['reserve_month']  = pd.Series(full.reserve_datetime.dt.month)\nfull['reserve_date'] = pd.Series(full.reserve_datetime.dt.day)\nfull['reserve_weekday'] = pd.Series(full.reserve_datetime.dt.weekday)\nfull['reserve_time'] = pd.Series(full.reserve_datetime.dt.time)","12285314":"full.head()","4942bbcd":"# Fill NaNs to ease operations with ids and creation new columns\nfull['air_store_id'] = full['air_store_id'].fillna('0')\nfull['hpg_store_id'] = full['hpg_store_id'].fillna('0')\nfull['air_genre_name'] = full['air_genre_name'].fillna('0')\nfull['hpg_genre_name'] = full['hpg_genre_name'].fillna('0')","d2e0c213":"# Now lets put our data in order\n# Create column 'store_id', where all ids from two sources will be collected together\nfull.loc[(full['air_genre_name'] != '0'), 'store_id'] = full['air_store_id']\nfull.loc[(full['air_genre_name'] == '0'), 'store_id'] = full['hpg_store_id']\n\n# Create column 'store_genre_name', where all genres of restaurants will be collected together\nfull.loc[(full['air_genre_name'] != '0'), 'store_genre_name'] = full['air_genre_name']\nfull.loc[(full['air_genre_name'] == '0'), 'store_genre_name'] = full['hpg_genre_name']\n\n# Create column 'air_hpg_link', to save connection between restaurants that are in both sources\nfull.loc[(full['air_store_id'] != '0') & (full['hpg_genre_name'] != '0'), 'air_hpg_link'] = full['air_store_id']\nfull.loc[(full['air_store_id'] != '0') & (full['hpg_genre_name'] == '0'), 'air_hpg_link'] = full['hpg_store_id']\n\n# Create column 'store_genre_name', where all genres of restaurants will be collected together\nfull.loc[(full['air_genre_name'] != '0'), 'area_name'] = full['air_area_name']\nfull.loc[(full['air_genre_name'] == '0'), 'area_name'] = full['hpg_area_name']\n\nfull['air_hpg_link'] = full['air_hpg_link'].fillna('0')","d262d5b8":"full.head()","e3f4ca6c":"fullhist = full.groupby(['visit_datetime'],as_index=False).count().sort_values(by=['visit_datetime'])\nfullhist_mnth = fullhist.loc[fullhist['visit_datetime'] <= pd.to_datetime('2016-02-01 23:59:00')]\nfullhist_week = fullhist.loc[(fullhist['visit_datetime'] >= pd.to_datetime('2016-01-04 00:00:00')) \\\n                            & (fullhist['visit_datetime'] <= pd.to_datetime('2016-01-10 23:59:00'))]\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(22,25))\nax1.plot(fullhist.visit_datetime, fullhist.store_id)\nax1.set_title(\"Visits during full period\")\nplt.ylabel(\"Number of visits\")\nplt.grid(True)\n\nax2.plot(fullhist_mnth.visit_datetime, fullhist_mnth.store_id)\nax2.set_title(\"Visits during one month\")\nplt.ylabel(\"Number of visits\")\nplt.grid(True)\n\nax3.plot(fullhist_week.visit_datetime, fullhist_week.store_id)\nax3.set_title(\"Visits during one week\")\nplt.ylabel(\"Number of visits\")\n\nplt.xlabel(\"Period\")\nplt.grid(True)\nplt.show()","58d747ab":"monthshist = full.groupby(['visit_month'],as_index=False).count().sort_values(by=['visit_month'])\nweekshist = full.groupby(['visit_weekday'],as_index=False).count().sort_values(by=['visit_weekday'])\ndayhist = full.groupby(['visit_time'],as_index=False).count().sort_values(by=['visit_time'])\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,10))\n\nplt.ylabel(\"Number of visits\")\nax1.bar(monthshist.visit_month, monthshist.store_id)\nax1.set_title(\"Visits by months\")\nplt.xlabel(\"Months\")\n\nax2.bar(weekshist.visit_weekday, weekshist.store_id, color='C2')\nax2.set_title(\"Visits by one week\")\nplt.xlabel(\"Week days\")\n\nax3.plot(dayhist.visit_time, dayhist.store_id, color='C1')\nax3.set_title(\"Visits during one day\")\nplt.xlabel(\"Time\")\n\nplt.show()","be089a63":"full.head(1)","8661174b":"# Create dataset combined by visit date from full data\n# Note: that in air data the last datetime of visit is near 05.2017 and in hpg data is ended at 04.2017\n# So on last month of period (t.e. May 2017) joined data for both sited is not full!\ndatehist = full.loc[full['visit_datetime'] < pd.to_datetime('2017-05-01')]\ndatehist['visit_date_full'] = pd.Series(datehist.visit_datetime.dt.date)\ndatehist = datehist.groupby(['visit_date_full'],as_index=False).count().sort_values(by=['visit_date_full'])\ndatehist.tail()","205f48f0":"# Create dataset combined by visit date from months data\ndate_mnth_hist = datehist.loc[datehist['visit_date_full'] < pd.to_datetime('2016-02-01')]\ndate_mnth_hist.tail()","40655e49":"# Importing everything from above\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error","5b8206fc":"# Calculate average of last n observations\ndef moving_average(data, n):\n    \"\"\"\n    data - series type, which need to be smoothed\n    n - size of the moving window\n    \"\"\"\n    return np.average(data[-n:])\n\nmoving_average(datehist.store_id, 10)","724ceb94":"moving_average(date_mnth_hist.store_id, 10)","6e50183e":"# Or use Pandas implementation - DataFrame.rolling(window).mean(), that provides rolling window calculations.\n# As main parameters to this function you should pass: \n# * window - Size of the moving window. This is the number of observations used for calculating the statistic.\n# * win_type - Provide a window type.\n# Also, note that by default, the result is set to the right edge of the window. \n# This can be changed to the center of the window by setting center=True.\ndef plotMovingAverage(data, window_size, plot_intervals=False, scale=1.96, plot_anomalies=False):\n    \"\"\"\n        data - dataframe with timeseries\n        window_size - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    rolling_mean = data.rolling(window=window_size).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window_size))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(data[window_size:], rolling_mean[window_size:])\n        deviation = np.std(data[window_size:] - rolling_mean[window_size:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond \/ Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:           # Fix it!!!\n            anomalies = pd.DataFrame(index=data.index, columns=[data.name])\n            anomalies[data<lower_bond] = data[data<lower_bond]\n            anomalies[data>upper_bond] = data[data>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(data[window_size:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","ff949dc4":"rolling_mean = date_mnth_hist.rolling(window=4).mean()\nrolling_mean.tail()","8b497dd5":"# Let's smooth by the previous 7 days so we get weekly trend more clearly.\nplotMovingAverage(date_mnth_hist.store_id, 7)","95841ff8":"# Let's smooth by the previous 2 days so we get trend more clearly without loosing extra high values of visitors on weekends.\nplotMovingAverage(date_mnth_hist.store_id, 2)","f48c3414":"# Plot confidence intervals for our smoothed values for a full period\nplotMovingAverage(datehist.store_id, 7, plot_intervals=True)","c3f523da":"def weighted_average(data, weights):\n    \"\"\"\n        Calculate weighter average on series\n        data - series type, which need to be smoothed\n        weights - weights of samples\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += data.iloc[-n-1] * weights[n]\n    return float(result)\n\nweighted_average(date_mnth_hist.store_id, [0.6, 0.4, 0.2, 0.1])","21cd0baf":"def exponential_smoothing(data, alpha):\n    \"\"\"\n        data - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [data[0]] # first value is same as series\n    for n in range(1, len(data)):\n        result.append(alpha * data[n] + (1 - alpha) * result[n-1])\n    return result","13ea5c36":"def plotExponentialSmoothing(data, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        data - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(28, 10))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(data, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(data.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","9326fbbd":"plotExponentialSmoothing(date_mnth_hist.store_id, [0.5, 0.3, 0.05])","753f10ae":"def double_exponential_smoothing(data, alpha, beta):\n    \"\"\"\n        data - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [data[0]]\n    for n in range(1, len(data)+1):\n        if n == 1:\n            level, trend = data[0], data[1] - data[0]\n        if n >= len(data): # forecasting\n            value = result[-1]\n        else:\n            value = data[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result","e6a8af5a":"def plotDoubleExponentialSmoothing(data, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        data - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(data, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(data.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","07f9fcbc":"# Let's smooth full data with different settings of parameters (alpha and beta) so we cas see which of them performs the best.\nplotDoubleExponentialSmoothing(datehist.store_id, alphas=[0.9, 0.02], betas=[0.9, 0.02])","c6e18bd2":"# Then try to smooth weekly data with different settings of parameters (alpha and beta).\nplotDoubleExponentialSmoothing(fullhist_week.reset_index().store_id, alphas=[0.9, 0.02], betas=[0.9, 0.02])","2ad20b33":"def initial_trend(series, season_len):\n    \"\"\"\n    series - initial time series\n    season_len - length of a season\n    \"\"\"\n    summ = 0.0\n    for i in range(season_len):\n        summ += float(series[i + season_len] - series[i]) \/ season_len\n    return summ \/ season_len  ","234d3faf":"def initial_seasonal_components(series, season_len):\n    \"\"\"\n    series - initial time series\n    season_len - length of a season\n    \"\"\"\n    seasonals = {}\n    season_averages = []\n    n_seasons = int(len(series) \/ season_len)\n#     print(\"n_seasons=%d, len(series)=%d, season_len=%d\" % (n_seasons, len(series), season_len))\n    # let's calculate season averages\n    for j in range(n_seasons):\n        season_averages.append(sum(series[season_len*j : season_len*j+season_len]) \/ float(season_len))\n    # let's calculate initial values\n    for i in range(season_len):\n        sum_of_vals_over_avg = 0.0\n        for j in range(n_seasons):\n            sum_of_vals_over_avg += series[season_len*j+i] - season_averages[j]\n        seasonals[i] = sum_of_vals_over_avg \/ n_seasons\n    return seasonals   ","fe3e44d8":"def triple_exponential_smoothing(series, season_len, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    series - initial time series\n    season_len - length of a season\n    alpha, beta, gamma - Holt-Winters model coefficients\n    n_preds - predictions horizon\n    scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)   \n    \"\"\"\n    result = []\n    Smooth = []\n    Season = []\n    Trend = []\n    PredictedDeviation = []\n    UpperBond = []\n    LowerBond = []  \n    seasonals = initial_seasonal_components(series, season_len)\n    \n    for i in range(len(series) + n_preds):\n        if i == 0: # components initialization\n            smooth = series[0]\n            trend = initial_trend(series, season_len)\n            result.append(series[0])\n            Smooth.append(smooth)\n            Trend.append(trend)\n            Season.append(seasonals[i%season_len])\n            \n            PredictedDeviation.append(0)\n            UpperBond.append(result[0] + scaling_factor * PredictedDeviation[0])\n            LowerBond.append(result[0] - scaling_factor * PredictedDeviation[0])                   \n            continue\n                \n        if i >= len(series): # predicting\n            m = i - len(series) + 1\n            result.append((smooth + m*trend) + seasonals[i%season_len])\n            # when predicting we increase uncertainty on each step\n            PredictedDeviation.append(PredictedDeviation[-1]*1.01)  \n        else:\n            val = series[i]\n            last_smooth, smooth = smooth, alpha * (val-seasonals[i%season_len]) + (1-alpha) * (smooth+trend)\n            trend = beta * (smooth-last_smooth) + (1-beta)*trend\n            seasonals[i%season_len] = gamma*(val-smooth) + (1-gamma)*seasonals[i%season_len]\n            result.append(smooth+trend+seasonals[i%season_len])\n            # Deviation is calculated according to Brutlag algorithm.\n            PredictedDeviation.append(gamma * np.abs(series[i] - result[i]) + (1-gamma)*PredictedDeviation[-1])         \n            UpperBond.append(result[-1] + scaling_factor * PredictedDeviation[-1])\n            LowerBond.append(result[-1] - scaling_factor * PredictedDeviation[-1])\n            Smooth.append(smooth)\n            Trend.append(trend)\n            Season.append(seasonals[i%season_len])\n    return[result, LowerBond, UpperBond]","f85d421e":"with plt.style.context('seaborn-white'): \n        plt.figure(figsize=(20, 8))\n        plt.plot(triple_exponential_smoothing(date_mnth_hist.store_id, 7, 0.4, 0.5, 0.5, 7)[0], label = \"Smoothed\")\n        plt.plot(date_mnth_hist.store_id, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Holt-Winters model Smoothing\")\n        plt.grid(True)","29af24bb":"from sklearn.model_selection import TimeSeriesSplit # you have time seriaes splitting already done for you\n\ndef timeseriesCVscore(params, series, loss_function=mean_squared_error, season_len=7):\n    \"\"\"\n        Returns error on CV  \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        season_len - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=5) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n        res = triple_exponential_smoothing(series=values[train], season_len=season_len, alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))[0]\n        predictions = res[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","24e509a3":"# leave some data for testing = one month\ntest_period_len = 30","e98c5c79":"from scipy.optimize import minimize              # for function minimization\n\n%time\ndata = datehist.store_id[:-test_period_len] # leave some data for testing = one month\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimization of loss function of one or more variables.\nopt = minimize(timeseriesCVscore, x0=x, args=(data, median_absolute_error), method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1)))\n\n# Take optimal values\nalpha_final, beta_final, gamma_final = opt.x\nprint(\"Best alpha=%f, best beta=%f, best gamma=%f\" % (alpha_final, beta_final, gamma_final))\n\n# and train the model with them, forecasting for the next 7 days\nret = triple_exponential_smoothing(data, season_len = 7, alpha = alpha_final, beta = beta_final, gamma = gamma_final, n_preds = 7, scaling_factor = 3) ","ba0302fb":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","daf7caa5":"def plotHoltWinters(series, returned, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        returned - list, returned from triple_exponential_smoothing func: (result, LowerBond, UpperBond)\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    [result, LowerBond, UpperBond] = returned\n    plt.figure(figsize=(28, 10))\n    plt.plot(result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    error = mean_absolute_percentage_error(series.values, result[:len(series)])\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values < LowerBond[:len(series)]] = \\\n            series.values[series.values < LowerBond[:len(series)]]\n        anomalies[series.values > UpperBond[:len(series)]] = \\\n            series.values[series.values > UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(UpperBond, \"r--\", alpha=0.5, label = \"Up\/Low confidence\")\n        plt.plot(LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(result[:-7])), y1=UpperBond, y2=LowerBond, alpha=0.2, color = \"grey\") \n        \n    plt.vlines(len(series), ymin=min(LowerBond), ymax=max(UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-test_period_len, len(result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);","01664694":"plotHoltWinters(datehist.store_id[:-test_period_len], ret, plot_intervals=True, plot_anomalies=True)","09eadeeb":"def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n\n        y.plot(ax=ts_ax)\n        ts_ax.set_title('Time Series Analysis Plots')\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n\n        print(\"Dickey-Fuller test criterion: p=%f\" % sm.tsa.stattools.adfuller(y)[1])\n\n        plt.tight_layout()\n        \n    test = sm.tsa.adfuller(y)\n    print('adf: ', test[0])  \n    print ('p-value: ', test[1])\n    print('Critical values: ', test[4])\n    if test[0]> test[4]['5%']: \n        print('A unit root is present, series are not stationary') \n    else:\n        print('No unit roots are present, series are stationary') \n    return \n\ntsplot(datehist.store_id, lags=30)","c3d44b67":"data_diff1 = datehist.store_id.diff(periods=1).dropna()\ntsplot(data_diff1, lags=30)","3483b5ee":"m = data_diff1.index[len(data_diff1.index)\/\/2+1]\nr1 = sm.stats.DescrStatsW(data_diff1[m:])\nr2 = sm.stats.DescrStatsW(data_diff1[:m])\nprint('p-value: ', sm.stats.CompareMeans(r1,r2).ttest_ind()[1]) ","10f97993":"tsplot(data_diff1, lags=30)","86ac81f9":"# setting initial values and some bounds for them\nps = range(3, 6)\nd=1 \nqs = range(3, 6)\nPs = range(3, 5)\nD=1 \nQs = range(3, 5)\ns = 7 # season length is still 7\n\n# creating list with all the possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)","80e8acaf":"def optimizeSARIMA(parameters_list, d, D, s):\n    \"\"\"\n        Return dataframe with parameters and corresponding AIC\n        parameters_list - list with (p, q, P, Q) tuples\n        d - integration order in ARIMA model\n        D - seasonal integration order \n        s - length of season\n    \"\"\"\n    \n    results = []\n    best_aic = float(\"inf\")\n\n    for param in tqdm_notebook(parameters_list):\n        # we need try-except because on some combinations model fails to converge\n        try:\n            model=sm.tsa.statespace.SARIMAX(data_diff1, order=(param[0], d, param[1]), \n                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n        aic = model.aic\n        # saving best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n\n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    # sorting in ascending order, the lower AIC is - the better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table","87b6213c":"# %time\n# result_table = optimizeSARIMA(parameters_list, d, D, s)","f71ddda0":"# set the parameters that give the lowest AIC\n# p, q, P, Q = result_table.parameters[0]\np = 5\nq = 5\nP = 3\nQ = 3\nprint(\"p = %f, q = %f, P = %f, Q = %f\" % (p, q, P, Q))","4521bd7d":"best_model=sm.tsa.statespace.SARIMAX(data_diff1, order=(p, d, q), \n                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprint(best_model.summary())","7b20e342":"# Firstly let's inspect the residuals of the model.\ntsplot(best_model.resid[7+1:], lags=30)","595566b4":"#  resid, \u0445\u0440\u0430\u043d\u0438\u0442 \u043e\u0441\u0442\u0430\u0442\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438, qstat=True, \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u0447\u0442\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0443\u043a\u0430\u0437\u044b\u043d\u043d\u044b\u0439 \u0442\u0435\u0441\u0442 \u043a \u043a\u043e\u044d\u0444-\u0430\u043c\n# Calculate the autocorrelation function.\nq_test = sm.tsa.stattools.acf(best_model.resid[7+1:], qstat=True) \nacf_scores = pd.DataFrame({'Q-stat':q_test[1], 'p-value':q_test[2]})\nacf_scores","41313dc7":"data = pd.DataFrame.from_dict({'actual':data_diff1})\ndata['sarima_model'] = best_model.fittedvalues\nforecast = best_model.predict(start = data.shape[0], end = data.shape[0]+ 14)\nforecast = data.sarima_model.append(forecast)\n# forecast.tail()\ndata.sarima_model.shape","a223e68a":"def plotSARIMA(series, model, n_steps):\n    \"\"\"\n        Plots model vs predicted values\n        series - dataset with timeseries\n        model - fitted SARIMA model\n        n_steps - number of steps to predict in the future\n        \n    \"\"\"\n    # adding model values\n    data = pd.DataFrame.from_dict({'actual':data_diff1})\n    data['sarima_model'] = model.fittedvalues\n    # making a shift on s+d steps, because these values were unobserved by the model\n    # due to the differentiating\n    data['sarima_model'][:s+d] = np.NaN\n    \n    # forecasting on n_steps forward \n    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n    forecast = data.sarima_model.append(forecast)\n    \n    # calculate estimations of the last 2 weeks of predictions (which are still registered in actual data) \n    true = data.actual[data.shape[0]-n_steps:data.shape[0]]\n    pred = data.sarima_model[data.shape[0]-n_steps:data.shape[0]]\n    r2 = r2_score(true, pred)\n    mae = mean_absolute_error(true, pred)\n    mse = mean_squared_error(true, pred)\n    print(\"R2 = %f, MSE = %f, MAE = %f\" % (r2, mse, mae))\n\n    plt.figure(figsize=(22, 7))\n    plt.title(\"SARIMA Model Predictions Plot\")\n    plt.plot(forecast, color='r', label=\"model\")\n    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n    plt.plot(data.actual, label=\"actual\")\n    plt.legend()\n    plt.grid(True);\n\nplotSARIMA(data_diff1, best_model, 14)","d7104447":"# Create dataset combined by visit date from full data\n# Note: that in air data the last datetime of visit is near 05.2017 and in hpg data is ended at 04.2017\n# So on last month of period (t.e. May 2017) joined data for both sited is not full!\n# dthist = full.loc[full['visit_datetime'] < pd.to_datetime('2017-05-01')]\n# dthist['visit_date_full'] = pd.Series(datehist.visit_datetime.dt.date)\n# dthist = dthist.groupby(['visit_date_full'],as_index=False).count().sort_values(by=['visit_date_full'])","584886e0":"# Creating a copy of the initial datagrame to make various transformations \ndatehist_timeidx = datehist.set_index('visit_date_full')\ndata = pd.DataFrame(datehist_timeidx.store_id.copy())\ndata.columns = [\"y\"]\n# Adding the lag of the target variable from 7 steps back up to 21\nfor i in range(7, 22):\n    data[\"lag_{}\".format(i)] = data.y.shift(i)\ndata.tail()","68b23cd2":"def timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","2d89da5a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# for time-series cross-validation set 5 folds \ntscv = TimeSeriesSplit(n_splits=5)\n\ny = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\n# reserve 30% of data for testing\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n\n# Create and train linear regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","a63d8d86":"def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        Plots modelled vs actual values, prediction intervals and anomalies\n    \"\"\"\n#     Make prediction\n    prediction = model.predict(X_test)\n    with plt.style.context('seaborn-white'): \n        plt.figure(figsize=(22, 7))\n        plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n        plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    \n        if plot_intervals:\n            cv = cross_val_score(model, X_train, y_train, cv=tscv, scoring=\"neg_mean_absolute_error\")                         \n            mae = cv.mean() * (-1)\n            deviation = cv.std()\n        \n            scale = 1.96\n            lower = prediction - (mae + scale * deviation)\n            upper = prediction + (mae + scale * deviation)\n        \n            plt.plot(lower, \"r--\", label=\"upper bond \/ lower bond\", alpha=0.5)\n            plt.plot(upper, \"r--\", alpha=0.5)\n        \n            if plot_anomalies:\n                anomalies = np.array([np.NaN]*len(y_test))\n                anomalies[y_test<lower] = y_test[y_test<lower]\n                anomalies[y_test>upper] = y_test[y_test>upper]\n                plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n        error = mean_absolute_percentage_error(prediction, y_test)\n        plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n        plt.legend(loc=\"best\")\n        plt.tight_layout()\n        plt.grid(True)","018d3144":"def plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    with plt.style.context('seaborn-white'): \n        coefs = pd.DataFrame(model.coef_, X_train.columns)\n        coefs.columns = [\"coef\"]\n        coefs[\"abs\"] = coefs.coef.apply(np.abs)\n        coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n        plt.figure(figsize=(22, 7))\n        coefs.coef.plot(kind='bar')\n        plt.grid(True, axis='y')\n        plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed')","7e7ad79b":"plotModelResults(lr, plot_intervals=True)\nplotCoefficients(lr)","d15d23f5":"data.index = pd.to_datetime(data.index)\ndata[\"weekday\"] = data.index.weekday\ndata['is_weekend'] = data.weekday.isin([5,6])*1\n# List of public and common local holidays in Japan\nholidays = pd.to_datetime(pd.Series(['01.01.2016', '02.01.2016', '03.01.2016', '11.01.2016', '11.02.2016', '03.03.2016', '20.03.2016', '21.03.2016', \\\n           '29.04.2016', '03.05.2016', '04.05.2016', '05.05.2016', '07.07.2016', '18.07.2016', '11.08.2016', '19.09.2016', \\\n           '22.09.2016', '10.10.2016', '03.11.2016', '15.11.2016', '23.11.2016', '23.12.2016', '25.12.2016', '31.12.2016', \\\n           '01.01.2017', '02.01.2017', '03.01.2017', '09.01.2017', '11.02.2017', '03.03.2017', '20.03.2017', '29.04.2017']))\ndata['is_holiday'] = data.index.isin(holidays)*1\ndata.tail()","0aba2ed6":"def plotFeatures(df, features):\n    \"\"\"\n    Visualizing features of data\n    df - dataframe,from which featuresare taken\n    features - list of features\n    \"\"\"\n    with plt.style.context('seaborn-white'):\n        plt.figure(figsize=(22, 7))\n        plt.title(\"Encoded features\")\n        for i in range(0, len(features)):\n            plt.plot(data[features[i]], label=features[i], linewidth=2.0)\n        plt.grid(True)\n        plt.legend(loc=\"best\")","74eaafe6":"ftrs = ['weekday', 'is_weekend', 'is_holiday']\nplotFeatures(data, ftrs)","bcd36346":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Create and train linear regression again\ny = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n\n# But this time perform normalization on the data \nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","87db9966":"def code_mean(data, cat_feature, real_feature):\n    \"\"\"\n    Returns a dictionary where keys are unique categories of the cat_feature,\n    and values are means over real_feature\n    \"\"\"\n    return dict(data.groupby(cat_feature)[real_feature].mean())","d881624c":"average_wd = code_mean(data, 'weekday', \"y\")\nplt.figure(figsize=(7, 5))\nplt.title(\"Weekday averages\")\npd.DataFrame.from_dict(average_wd, orient='index')[0].plot()\nplt.grid(True);","0600b4d1":"# Finally, let's put all the transformations together in a single function .\ndef prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n    \"\"\"\n        series: pd.DataFrame\n            dataframe with timeseries\n\n        lag_start: int\n            initial step back in time to slice target variable \n            example - lag_start = 1 means that the model \n                      will see yesterday's values to predict today\n\n        lag_end: int\n            final step back in time to slice target variable\n            example - lag_end = 4 means that the model \n                      will see up to 4 days back in time to predict today\n\n        test_size: float\n            size of the test dataset after train\/test split as percentage of dataset\n\n        target_encoding: boolean\n            if True - add target averages to the dataset\n        \n    \"\"\"\n    \n    # copy of the initial dataset\n#     data_timeidx = datehist.set_index('visit_date_full')    \n    data = pd.DataFrame(series.copy())\n    data.columns = [\"y\"]\n    \n    # lags of series\n    for i in range(lag_start, lag_end):\n        data[\"lag_{}\".format(i)] = data.y.shift(i)\n    \n    # datetime features\n    data.index = pd.to_datetime(data.index)\n    data[\"weekday\"] = data.index.weekday\n    data['is_weekend'] = data.weekday.isin([5,6])*1\n    # List of public and common local holidays in Japan\n    holidays = pd.to_datetime(pd.Series(['01.01.2016', '02.01.2016', '03.01.2016', '11.01.2016', '11.02.2016', '03.03.2016', '20.03.2016', '21.03.2016', \\\n               '29.04.2016', '03.05.2016', '04.05.2016', '05.05.2016', '07.07.2016', '18.07.2016', '11.08.2016', '19.09.2016', \\\n               '22.09.2016', '10.10.2016', '03.11.2016', '15.11.2016', '23.11.2016', '23.12.2016', '25.12.2016', '31.12.2016', \\\n               '01.01.2017', '02.01.2017', '03.01.2017', '09.01.2017', '11.02.2017', '03.03.2017', '20.03.2017', '29.04.2017']))\n    data['is_holiday'] = data.index.isin(holidays)*1\n    \n    if target_encoding:\n        # calculate averages on train set only\n        test_index = int(len(data.dropna())*(1-test_size))\n        data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n\n        # drop encoded variables \n        data.drop([\"weekday\"], axis=1, inplace=True)\n    \n    # train-test split\n    y = data.dropna().y\n    X = data.dropna().drop(['y'], axis=1)\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n\n    return X_train, X_test, y_train, y_test","6f4f00d1":"datehist_timeidx = datehist.set_index('visit_date_full')\nX_train, X_test, y_train, y_test = prepareData(datehist_timeidx.store_id, lag_start=7, lag_end=22, test_size=0.3, target_encoding=True)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lr)","d9a21cf3":"plt.figure(figsize=(15, 10))\nsns.heatmap(X_train.corr());","98a32a38":"from sklearn.linear_model import LassoCV, RidgeCV\n\n# Perform Ridge (L2) regression\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(ridge)","067e3318":"ridge.alpha_","6e20cd8e":"lasso = LassoCV(cv=tscv)\nlasso.fit(X_train_scaled, y_train)\n\nplotModelResults(lasso,  X_train=X_train_scaled, X_test=X_test_scaled,plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lasso)","78c1a50d":"lasso.alpha_ ","7dbab15c":"from xgboost import XGBRegressor \nxgb = XGBRegressor(booster='gblinear', alpha=11, updater='coord_descent', feature_selector='greedy', top_k=7)\nxgb.fit(X_train_scaled, y_train)\nplotModelResults(xgb,  X_train=X_train_scaled, X_test=X_test_scaled,plot_intervals=True, plot_anomalies=True)","ac7f98c8":"* **Double exponential smoothing**          \n\nUp to now, the methods that we've discussed have been for a single future point prediction (with some nice smoothing). That is cool, but it is also not enough. Let's extend exponential smoothing so that we can predict two future points (of course, we will also include more smoothing).            \n\nSeries decomposition will help us - we obtain two components: **intercept (i.e. level)  \u2113  and slope (i.e. trend)  b** . We have tried to predict intercept (or expected series value) with previous methods; now, we will apply the same exponential smoothing to the trend by assuming that the future direction of the time series changes depends on the previous weighted changes. As a result, we get the following set of functions:\n<img src=\"https:\/\/miro.medium.com\/max\/1783\/1*ws6cwxbpczBgTGYEZep79Q.png\" alt=\"exp smoothing 2\" style=\"width: 400px;\"\/>\n\nAs a result we get a set of functions. The first one describes intercept, as before it depends on the current value of the series, and the second term is now split into previous values of the level and of the trend. The second function describes trend \u2014 it depends on the level changes at the current step and on the previous value of the trend. In this case \u03b2 coefficient is a weight in the exponential smoothing. The final prediction is the sum of the model values of the intercept and trend.","3bd54230":"## Imports","6b292dd9":"Simple lags and linear regression gave us predictions that are so bad in terms of quality and up to it's simplicity. There are many unnecessary features, so we'll do feature selection in a little while.\n\nAs it was done before, we'll add day of week, a boolean for is_weekend and boolean for is_holiday, which represents the days of public and common local [holidays in Japan](https:\/\/www.timeanddate.com\/calendar\/?year=2017&country=26).                \nTo do so, we need to transform the current dataframe index into the datetime format and extract hour and weekday.","ce169297":"In order not to search for optimal parameters again and not to consider all combinations again, lets take from the grid of parameters the set that gives the lowest AIC and fix them (thus saving time):\n* p  = 5\n* d  = 1\n* q  = 5\n* P  = 3\n* D  = 1\n* Q  = 3\n","2d0d30f2":"A high p-value lets us to assume that the null hypothesis of equality of means is correct, which indicates the stationarity of the series.","4a77598e":"## XGboost\nIn the end, lets try XGboost to our data:","23e9e053":"## Data review\nIn this competition, you are provided a time-series forecasting problem centered around restaurant visitors. The data comes from two separate sites:\n* **Hot Pepper Gourmet (hpg)**: similar to Yelp, here users can search restaurants and also make a reservation online\n* **AirREGI \/ Restaurant Board (air)**: similar to Square, a reservation control and cash register system\n\nFrom this sites you can use the reservations, visits, and other information to forecast future restaurant visitor totals on a given date. The training data covers the dates from *January 2016 until April 2017*. The test set covers the *last week of April and May of 2017*. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the \"Golden Week.\" There are days in the test set where the restaurant were closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants were closed.","17c8ec3d":"## Fighting Overfitting\nWhat we can see on the lower plot is some **overfitting**! Lag_7 feature was so great in the training dataset that the model decided to concentrate all of its forces on it. As a result, the quality of prediction, still not too high, dropped. This problem can be solved in a variety of ways; for example, we can calculate the target encoding not for the whole train set, but for some window instead. That way, encodings from the last observed window will most likely better describe the current series state. Alternatively, we can just drop it manually since we are sure that it makes things only worse in this case.","a5f7439f":"* p  is most probably 6 since it is the last significant lag on the PACF, after which, most others are not significant.\n* d  equals 1 because we had first differences\n* q  should be somewhere around 6 as well as seen on the ACF\n* P  might be 3, since 7-th and 15-th lags are somewhat significant on the PACF (lets the season be 7)\n* D  again equals 1 because we performed seasonal differentiation\n* Q  is probably 5. The 28-th lag on ACF is significant.\n\nLet's test various models and see which one is better.","6dfd27b1":"And here we will try to show statistics of visits to restaurants in distributed by month, by day of the week and time of day:","bd54a760":"## Explore the data\n### File Descriptions\nThis is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin. Each restaurant has a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast. Latitudes and Longitudes are not exact to discourage de-identification of restaurants.\n\n**air_reserve.csv** - This file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur.\n\n**hpg_reserve.csv** - This file contains reservations made in the hpg system.\n\n**air_store_info.csv** - This file contains information about select air restaurants. Column names and contents are self-explanatory.\n\n**hpg_store_info.csv** - This file contains information about select hpg restaurants. Column names and contents are self-explanatory.\n\n**store_id_relation.csv** - This file allows you to join select restaurants that have both the air and hpg system.\n\n**air_visit_data.csv** - This file contains historical visit data for the air restaurants.\n\n**sample_submission.csv** - This file shows a submission in the correct format, including the days for which you must forecast.\n\n**date_info.csv** - This file gives basic information about the calendar dates in the dataset.","9258277e":"The test confirmed the assumptions about the unstationary of the series. In many cases, taking the difference of the series allows to bring series to stationariry. If, for example, the first differences of a series are stationary, it is called an *integrated series of the first order*.\nSo let's define the order of the integrated series for our series:","c68a17cd":"## SARIMA model\n\nFor predicting we will use model **SARIMA**(p,d,q)(P,D,Q,s), which stands for **Seasonal Autoregression Moving Average model**.\n\nThere are seasonal and Non-seasonal ARIMA models that can be used for forecasting. Both of them base on two of the following simple models:\n* **AR(p)** \u2014 autoregression model, i.e., regression of the time series onto itself. Basic assumption \u2014 current series values depend on its previous values with some lag (or several lags). The maximum lag in the model is referred to as p. To determine the initial p you need to have a look at PACF plot \u2014 find the biggest significant lag, after which most other lags are becoming not significant.\n* **MA(q)** \u2014 moving average model. Without going into detail it models the error of the time series, again the assumption is \u2014 current error depends on the previous with some lag, which is referred to as q. Initial value can be found on ACF plot with the same logic.\n\nIf we combine AR and MA models we'll get **ARMA(p,q)** model - **Autoregressive\u2013moving-average model**.              \n\nThen, adding new components to ARMA model we comes to **ARIMA(p,d,q)** - **Auto Regressive Integrated Moving Average**, which knows how to handle non-stationary data with the help of nonseasonal differences. In an ARIMA model we transform a time series into stationary one(series without trend or seasonality) using differencing. \n\n* **I(d)** \u2014 order of integration. It is simply the number of nonseasonal differences needed for making the series stationary. In our case it\u2019s just 1, because we used first differences.\n\n**Seasonal ARIMA (SARIMA) models**:\nAs the name suggests, this model is used when the time series exhibits seasonality. This model is similar to ARIMA models, we just have to add in a few parameters to account for the seasons.\n\n* **S(s)** \u2014 this letter is responsible for seasonality and equals the season period length of the series\n\nSo we write SARIMA as ARIMA(p,d,q)(P, D, Q)m:                 \np \u2014 the number of autoregressive                  \nd \u2014 degree of differencing                     \nq \u2014 the number of moving average terms                    \nm \u2014 refers to the number of periods in each season                           \n(P, D, Q )\u2014 represents the (p,d,q) for the seasonal part of the time series\n\nNow that we know how to set the initial parameters, let's have a look at the final plot once again and set the parameters:","44023c3e":"Firstly, lets look a bit at tables data examples.          \nThen try to discover info about it's structure and data types, as they can be not quite correctly represented by the Pandas.","acae644a":"The value of these statistics and p-values indicate that the hypothesis of the randomness of residues is not rejected, and most likely this process represents \"white noise\".                       \nNow let's calculate the *coefficient of determination R\u00b2* to understand what percentage of observations this model describes. For this  let's make predictions using our model.","d2010f40":"As can be seen from the plots above, the resulting series of first differences approached the stationary one. To be sure, we will break it into several intervals and make sure the mean value is equal on the different intervals:","c84a14e1":"**Implementation of Brutlag's algorithm in Anomaly Detection**         \n(You can read more [here](https:\/\/annals-csis.org\/proceedings\/2012\/pliks\/118.pdf))","9e2d7ceb":"* **Exponential smoothing**            \nAnd now let\u2019s take a look at what happens if instead of weighting the last nn values of the time series we start weighting all available observations while exponentially decreasing weights as we move further back in historical data. There\u2019s a formula of the simple exponential smoothing that will help us in that:\n<img src=\"https:\/\/miro.medium.com\/max\/1655\/1*8IBEmsFCywApUE_joR0aqA.png\" alt=\"exp smoothing\" style=\"width: 500px;\"\/>\n\nHere the model value is a weighted average between the current true value and the previous model values. The \u03b1 weight is called a smoothing factor. It defines how quickly we will \u201cforget\u201d the last available true observation. The less \u03b1 is the more influence previous model values have, and the smoother the series is.                \nExponentiality is hiding in the recursivity of the function \u2014 we multiply each time (1\u2212\u03b1) by the previous model value which, in its turn, also containes (1\u2212\u03b1) and so forth until the very beginning.","f5147e7f":"### Regularization and feature selection\nAs we already know, not all features are equally healthy - some may lead to overfitting while others should be removed. Besides manual inspection, we can apply regularization. Two of the most popular regression models with regularization are Ridge and Lasso regressions. They both add some more constrains to our loss function.\n\nIn the case of Ridge regression, those constraints are the sum of squares of the coefficients multiplied by the regularization coefficient. The bigger the coefficient a feature has, the bigger our loss will be. Hence, we will try to optimize the model while keeping the coefficients fairly low.\n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/d92ca2429275bfdc0474523babbafe014ca8b580\" alt=\"L2\" style=\"width: 250px;\"\/>\n\nIn L2 regularization, regularization term is the sum of square of all feature weights as shown above in the equation. As a result of this  L2  regularization, we will have higher bias and lower variance, so the model will generalize better (at least that's what we hope will happen).\n\nThe second regression model, Lasso regression, adds to the loss function, not squares, but absolute values of the coefficients. As a result, during the optimization process, coefficients of unimportant features may become zeroes, which allows for automated feature selection. This regularization type is called  L1.\n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/dfef9f9ff9b5a18268107c5b9f2f1e9640cfa750\" alt=\"L1\" style=\"width: 250px;\"\/>\n\nFirst, let's make sure that we have features to drop and that the data has highly correlated features.","db0771be":"In the Holt-Winters model, as well as in the other models of exponential smoothing, there's a constraint on how large the smoothing parameters can be, each of them ranging from 0 to 1. Therefore, in order to minimize our loss function, we have to choose an algorithm that supports constraints on model parameters. In our case, we will use the **truncated Newton conjugate gradient**, also known as Hessian-free optimization.\n\nA truncated Newton method consists of repeated application of an iterative optimization algorithm to approximately solve Newton's equations, to determine an update to the function's parameters. The inner solver is truncated, i.e., run for only a limited number of iterations. It follows that, for truncated Newton methods to work, the inner solver needs to produce a good approximation in a finite number of iterations; conjugate gradient has been suggested and evaluated as a candidate inner loop.\n\nRead more about truncated Newton conjugate gradient [here](http:\/\/www.cs.toronto.edu\/~jmartens\/docs\/Deep_HessianFree.pdf) ( and for Russian speakers [here](https:\/\/habr.com\/ru\/post\/350794\/)).","2baa69e1":"So we have generated a whole dataset here. Now we can build and train a model:","21b8c15f":"## Time series cross validation\nBefore we start building model let\u2019s talk first about how to estimate model parameters automatically.\nThere\u2019s nothing unusual here, as always we have to choose a loss function suitable for the task, that will tell us how close the model approximates data. Then using cross-validation we will evaluate our chosen loss function for given model parameters, calculate gradient, adjust model parameters and so forth, bravely descending to the global minimum of error.\n\nThe question is how to do cross-validation on time series, because, you know, time series do have time structure and one just can\u2019t randomly mix values in a fold without preserving this structure, otherwise all time dependencies between observations will be lost. That\u2019s why we will have to use a bit more tricky approach to optimization of the model parameters: **\u201ccross-validation on a rolling basis\u201d**.\n\nThe idea is rather simple \u2014 we train our model on a small segment of the time series, from the beginning until some t, make predictions for the next t+n steps and calculate an error. Then we expand our training sample until t+n value and make predictions from t+n until t+2\u2217n, and we continue moving our test segment of the time series until we hit the last available observation. As a result we have as many folds as many n will fit between the initial training sample and the last observation. Read more [here](https:\/\/stats.stackexchange.com\/questions\/14099\/using-k-fold-cross-validation-for-time-series-model-selection).\n\nTo make things intuitive, here is an image for same:\n\n<img src=\"https:\/\/i.stack.imgur.com\/fXZ6k.png\" alt=\"cross-validation on a rolling basis\" style=\"width: 600px;\"\/>\n\nNow, knowing how to set cross-validation, we will find optimal parameters for the Holt-Winters model, recall that we have daily seasonality in moths data, hence the season_len=7 parameter.","00c4bb61":"# Other linear and not models on time series\nOften, it is needed to build models fast, good, cheap. That means that some of these models will never be considered \"fast\" to buid as they demand too much time for data preparation (as in SARIMA) or require frequent re-training on new data (again, SARIMA) or are difficult to tune (good example - SARIMA). Therefore, it's very often much easier to select a few features from the existing time series and build a simple linear regression model or, say, a random forest.               \nThus, lets start from **Feature Extraction**.\n\n## Lags of time series\nShifting the series  n  steps back, we get a feature column where the current value of time series is aligned with its value at time  t\u2212n . If we make a 1 lag shift and train a model on that feature, the model will be able to forecast 1 step ahead from having observed the current state of the series. Increasing the lag, say, up to 6, will allow the model to make predictions 6 steps ahead; however it will use data observed 6 steps back. If something fundamentally changes the series during that unobserved period, the model will not catch these changes and will return forecasts with a large error. Therefore, during the initial lag selection, one has to find a balance between the optimal prediction quality and the length of the forecasting horizon.","cb972671":"It is clear that the residuals are stationary, and there are no apparent autocorrelations.","a7da0c36":"As we can see from the time series smoothing plot, random selection of parameters for the model does not bring the expected results: the smoothed values of the series are larger than the actual values, which denies the idea of exponential smoothing. That is why we'd better should think about the automatic selection of Holt-Winters model coefficients (alpha, beta and gamma).","be80cfaf":"## Feature visualisations\nHere we have a first look at the distributions of the feature in our data files before taking them for a more detailed analysis. This inital visualisation will be the foundation on which we build our analysis.\n\nWe start visualisation with plotting the number of visits to the restaurants. Here we plot the total number of visitors during the full training time (for given period from Jan.2016 to May.2017), then visitors per month (Jan.2016) and per week (04.01.2016-10.01.2016, which also dont't contain holidays):","d7f176b4":"*  **Triple exponential smoothing a.k.a. Holt-Winters**\n\nWe've looked at exponential smoothing and double exponential smoothing. This time, we're going into triple exponential smoothing.\n\nAs you could have guessed, the idea is to add a third component - **seasonality**. This means that we should not use this method if our time series is not expected to have seasonality. Seasonal components in the model will explain repeated variations around intercept and trend, and it will be specified by the length of the season, in other words by the period after which the variations repeat. For each observation in the season, there is a separate component: for example, if the length of the season is 7 days (a weekly seasonality), we will have 7 seasonal components, one for each day of the week.\n\nWith this, let's write out a new system of equations:\n<img src=\"https:\/\/miro.medium.com\/max\/2153\/1*tQUjJKDKmqjGPeQ9YqV_dQ.png\" alt=\"exp smoothing 2\" style=\"width: 400px;\"\/>\n\nThe intercept now depends on the current value of the series minus any corresponding seasonal component. Trend remains unchanged, and the seasonal component depends on the current value of the series minus the intercept and on the previous value of the component. Take into account that the component is smoothed through all the available seasons; for example, if we have a Monday component, then it will only be averaged with other Mondays. Now that we have the seasonal component, we can predict not just one or two steps ahead but an arbitrary  m  future steps ahead, which is very encouraging.\n\nBelow is the code for a triple exponential smoothing model, which is also known by the last names of its creators, Charles Holt and his student Peter Winters. Additionally, the Brutlag method was included in the model to produce confidence intervals:\n<img src=\"https:\/\/miro.medium.com\/max\/2053\/1*6Cu5nv-COdclmdfXnAU02Q.png\" alt=\"exp smoothing 2\" style=\"width: 400px;\"\/>\n\nwhere  T  is the length of the season,  d  is the predicted deviation. Other parameters were taken from triple exponential smoothing. \n\nYou you can read more how to present it in the form of mathematic and calculate [here](https:\/\/en.wikipedia.org\/wiki\/Exponential_smoothing). ","5b86aae0":"As you can see from this information in our model all the coefficients are significant and you can proceed to the evaluation of the model.\n\n## Model analysis and evaluation\nLet's check the residuals of this model for compliance with \"white noise\", and also analyze the residuals' correlogram, as it can help us to identify important elements of regression for inclusion and prediction.\n\n### Autocorellation\nAutocorrelation of residues is observed when the values of previous residues overestimate (**positive**) or underestimate (**negative**) values of following ones.\n\n* **Positive autocorrelation** on the residuals graph is occur in alternating zones of positive and negative residuals:\n<img src=\"https:\/\/konspekta.net\/lektsiiorgimg\/baza3\/1413867055430.files\/image529.gif\" alt=\"Positive autocorrelation\" style=\"width: 400px;\"\/>\n\n* **Negative autocorrelation** on the chart is showed in the fact that the residuals \"too often\" change sign:\n<img src=\"https:\/\/pandia.ru\/text\/78\/207\/images\/image052_0.gif\" alt=\"Negative autocorrelation\" style=\"width: 400px;\"\/>\n\nAlso, note, that the presence of autocorrelation of random errors of the regression model on it's  residuals' correlogram leads to a deterioration in the quality of estimations of regression parameters, as well as to overestimation of test statistics, which are used to check the quality of the model (that is, an fake improvement in the quality of the model relative to its actual level of accuracy). \nTherefore, testing the autocorrelation of random errors is a necessary procedure for constructing a regression model.\n\n\n### Ljung\u2013Box Q-test\n\nSo the first thing we do is we do a Ljung\u2013Box Q-test to try the hypothesis that the residuals are random, i.e. are \" white noise.\" This test is carried out on the remains of the ARIMA model. Thus, we need to first obtain the residuals of the model and construct an ACF for them, and then apply the test to the resulting coefficient. \n\n**The Ljung\u2013Box test** (named for Greta M. Ljung and George E. P. Box) is a type of statistical test of whether any of a group of autocorrelations of a time series are different from zero and instead of testing randomness at each distinct lag, it tests the \"overall\" randomness based on a number of lags, and is therefore a portmanteau test.\n\nThe Ljung\u2013Box test may be defined as:\n\n* H0: The data are independently distributed (i.e. the correlations in the population from which the sample is taken are 0, so that any observed correlations in the data result from randomness of the sampling process).\n* H1: The data are not independently distributed; they exhibit serial correlation.              \nThe test statistic is:\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/bb68b5d73ab4813712778e246303961ae48d6190\" alt=\"The Ljung\u2013Box stats\" style=\"width: 200px;\"\/>\nFor significance level \u03b1, the critical region for rejection of the hypothesis of randomness is: <img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2aa39af3127113dd435272f4117a64bdd431593d\" alt=\"The Ljung\u2013Box stats\" style=\"width: 120px;\"\/>\n\nWith statsmodels this can be done as follows:","b008542b":"## Move, smoothe, evaluate: Rolling window estimations\nYou are conducting an exploratory analysis of this dataset as time-series data. To make sure you have the best picture of your data, you'll want to separate long-trends and seasonal changes from the random fluctuations. In this part, we'll describe some of the time smoothers commonly used to help you do this.\n\n* **Moving averages**       \n\nThe easiest local smoother to grasp intuitively is the moving average (or running mean) smoother. It consists of taking the mean of a fixed number of nearby points. As we only use nearby points, adding new data to the end of the time series does not change estimated values of historical results.              \nLet\u2019s start with a naive hypothesis \u2014 \u201ctomorrow will be the same as today\u201d, but instead of a model like *y\u0302(t)=y(t\u22121)* (which is actually a great baseline for any time series prediction problems and sometimes it\u2019s impossible to beat it with any model) we\u2019ll assume that the future value of the variable depends on the average *n* of its previous values and therefore we\u2019ll use moving average.\n![image.png](attachment:image.png)\nUnfortunately we can\u2019t make this prediction long-term \u2014 to get one for the next step we need the previous value to be actually observed. But moving average has another use case \u2014 smoothing of the original time series to indicate trends. Pandas has an implementation available *DataFrame.rolling(window).mean()*. The wider the window - the smoother will be the trend. In the case of the very noisy data, which can be very often encountered in finance, this procedure can help to detect common patterns.","38095135":"Since we now have different scales in our variables, thousands for the lag features and tens for categorical, we need to transform them into same scale for exploring feature importance and, later, regularization.               \nIt needed because regularization assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nThe simplest transformation is Standard Scaling (or Z-score normalization):\n<img src=\"https:\/\/miro.medium.com\/max\/178\/1*l3GMKe44JVe8YKQi8ZdWgQ.png\" alt=\"The Ljung\u2013Box stats\" style=\"width: 150px;\"\/>\nThis type of normalization is implemented in Sklearn as *StandardScaler()*,\nthat standardize features by removing the mean and scaling to unit variance.\n","1d198429":"## Forecast quality metrics\nBefore actually forecasting, let\u2019s understand how to measure the quality of predictions and have a look at the most common and widely used metrics:\n\n* **R squared**: coefficient of determination,  (\u2212\u221e,1]          \nIt represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.                         \nAs such variance is dataset dependent, R\u00b2 may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R\u00b2 score of 0.0.\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/2568\/1*pKx5cF_Fx6nco2KNowSuvA.png\" alt=\"MAE\" style=\"width: 400px;\"\/>\n\nIn Sklearn implemented function: *sklearn.metrics.r2_score*\n\n* **Mean Absolute Error** - a risk metric corresponding to the expected value of the absolute error loss or l1-norm loss. It is an interpretable metric because it has the same unit of measurement as the initial series, [0, +inf)\n<img src=\"http:\/\/mund-consulting.com\/wp-content\/uploads\/2014\/08\/mae1.png\" alt=\"MAE\" style=\"width: 200px;\"\/>\n\nIn Sklearn implemented function: *sklearn.metrics.mean_absolute_error*\n\n* **Mean Squared Error** - a risk metric corresponding to the expected value of the squared (quadratic) error or loss. It is the most commonly used metric that gives a higher penalty to large errors and vice versa, [0,+\u221e)\n<img src=\"https:\/\/s3.amazonaws.com\/media-p.slid.es\/uploads\/759308\/images\/5478762\/pasted-from-clipboard.png\" alt=\"MSE\" style=\"width: 200px;\"\/>\n\nIn Sklearn implemented function: *sklearn.metrics.mean_squared_error*\n\n* **Mean Squared Logarithmic Error** - a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss. Practically, this is the same as MSE, but we take the logarithm of the series. As a result, we give more weight to small mistakes as well. This is usually used when the data has exponential trends, [0,+\u221e)\n<img src=\"https:\/\/raw.githubusercontent.com\/DevStarSJ\/Study\/master\/Blog\/Kaggle\/Coursera.competition\/image\/coursera.competition.03.07.png\" alt=\"MSLE\" style=\"width: 400px;\"\/>\n\nIn Sklearn implemented function: *sklearn.metrics.mean_squared_log_error*","8ebcdd5d":"Surprisingly, but the features of data, we have created from date-time indexes, from the point of view of the model, were not very useful. Also from the plot it is notisable, that we've chosen the period of seasonally right: 7-th lag in data features is the most significant factor in model, then 14-th, 21-st and so on. Thus, we clearly see seasonality in our dataset.                \nThen pay your attention that the test error goes down a little bit at this time. \n\n### Target encoding\n\nI'd like to add another variant for encoding categorical variables: *encoding by mean value*. If it is undesirable to explode a dataset by using many dummy variables that can lead to the loss of information and if they cannot be used as real values because of the conflicts like \"0 hours < 23 hours\", then it's possible to encode a variable with slightly more interpretable values. The natural idea is to encode with the mean value of the target variable. In our example, every day of the week and every hour of the day can be encoded by the corresponding average number of ads watched during that day or hour. It's very important to make sure that the mean value is calculated over the training set only (or over the current cross-validation fold only) so that the model is not aware of the future.","96df9043":"Lasso regression turned out to be more conservative; it removed weekday_average from the most important features and dropped 9 features completely, which only made the quality of prediction better.","fdaed607":"Judging by the plots, model was able to approximate quite well the initial time series, capturing the weekly seasonality, overall  trend, and even some anomalies. If you look at the model deviations, you can clearly see that the model reacts quite sharply to changes in the structure of the series but then quickly returns the deviation to the normal values, essentially \"forgetting\" the past. This feature of the model allows us to quickly build anomaly detection systems, even for noisy series data, without spending too much time and money on preparing the data and training the model.","2fc2ad15":"## Importing Data","f2bef1ac":"So, when we applied 4 days smoothing on our data, we could clearly see the dynamics of visitors coming in restaurants. During the weekends, the values are higher (more time and occasions to have special dinner on the weekends, or no desire to cook otherwise) while fewer times peaple go there on weekdays.","c85b61ff":"## Conclusion\n\nAs you can see from the plots and scores, our model builds a quite good forecast for some, not far, future. But, anyway, there are much things to fix for later:\n\n* there are still left outliers in the actual data, which we have not completely removed, \n* as well as the SARIMA module of statsmodels package, because it is quite new. \n\nThe notebook is more aimed at showing how you can analyze time series in python.","4f3b7ad5":"## Econometric approach\n\n### Stationarity\n\nBefore we start modeling we should mention such an important property of time series as stationarity.\nIf the process is stationary that means it doesn\u2019t change its statistical properties over time, namely mean and variance do not change over time (constancy of variance is also called *homoscedasticity*), also covariance function does not depend on the time (should only depend on the distance between observations). You can see this visually on the pictures from the post of  [Sean Abu](http:\/\/www.seanabu.com\/2016\/03\/22\/time-series-seasonal-ARIMA-model-in-python\/):\n\n* The mean of the series should not be a function of time. The red graph below is not stationary because the mean increases over time. alt text\n\n<img src=\"http:\/\/www.seanabu.com\/img\/Mean_nonstationary.png\" alt=\"1\" style=\"width: 300px;\"\/>\n\n* The variance of the series should not be a function of time. This property is known as homoscedasticity. Notice in the red graph the varying spread of data over time. alt text\n\n<img src=\"http:\/\/www.seanabu.com\/img\/Var_nonstationary.png\" alt=\"cross-validation on a rolling basis\" style=\"width: 300px;\"\/>\n\n* Finally, the covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the \u2018red series\u2019.\n\n<img src=\"http:\/\/www.seanabu.com\/img\/Cov_nonstationary.png\" alt=\"cross-validation on a rolling basis\" style=\"width: 300px;\"\/>\n\nSo why stationarity is so important? Because it\u2019s easy to make predictions on the stationary series as we assume that the future statistical properties will not be different from the currently observed. Most of the time series models in one way or the other model and predict those properties (mean or variance, for example), that\u2019s why predictions would be wrong if the original series were not stationary. Unfortunately most of the time series we see outside of textbooks are non-stationary but we can (and should) change this.\nSo, to fight non-stationarity we have to know our enemy so to say. Let\u2019s see how to detect it. \n\nThere are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the **Dickey-Fuller test**.\n\n### Dickey-Fuller test\n\nThe main idea of the Dickey-Fuller test for the stationarity of time series is fact of presence of a unit root. If we can get stationary series from non-stationary using the first difference we call those series integrated of order 1. Null hypothesis of the test \u2014 time series is non-stationary, was rejected on the first three charts and was accepted on the last one. We\u2019ve got to say that the first difference is not always enough to get stationary series as the process might be integrated of order d, d > 1 (and have multiple unit roots), in such cases the augmented Dickey-Fuller test is used that checks multiple lags at once.\n\nWe can fight non-stationarity using different approaches \u2014 various order differences, trend and seasonality removal, smoothing, also using transformations like Box-Cox or logarithmic.","8a35fb7a":"As we see, coefficient of determination R2 is quite high, something around 0.7428.                    \nR2 is a statistic that can give some information about the goodness of fit of a model. In AR models, the R2 coefficient of determination is a statistical measure of how well the regression predictions approximate the real data points. An R2 of 1 indicates that the regression predictions perfectly fit the data.                           \nSo then, our model proved to be good at predicting last 2 weeks of time series.","883c51ed":"Alse we can compute and plot *confidence intervals* for our smoothed values.            \n\nA **confidence interval (CI)** is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter. The interval has an associated confidence level, or coverage that, loosely speaking, quantifies the level of confidence that the deterministic parameter is captured by the interval. More strictly speaking, the confidence level represents the frequency (i.e. the proportion) of possible confidence intervals that contain the true value of the unknown population parameter. In other words, if confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics, the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level.\n\nThe confidence level is designated prior to examining the data. Most commonly, the 95% confidence level is used. However, other confidence levels can be used, for example, 90% and 99%.                       \nWe will compute the 95% confidence level for our data.\nIn other words, the lower endpoint of the 95% confidence interval is:\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/6514f50cf9973fa91a7350e949e4dfa78ec202b8\" alt=\"lower\" style=\"width: 300px;\"\/>\nand the upper endpoint of the 95% confidence interval is:\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/ae05ccddc5e73c2293965a2cdc7c82c8a80a38eb\" alt=\"upper\" style=\"width: 300px;\"\/>","05073f05":"Now we have to tune two parameters:  \u03b1  and  \u03b2 . The former is responsible for the series smoothing around the trend, the latter for the smoothing of the trend itself. The larger the values, the more weight the most recent observations will have and the less smoothed the model series will be.\n\nSo, when we applied 4 different sets of params for exponetial smoothing on our data, we could see, that small parameter values (t.e.alpha=0.2, beta=0.2) lead to incorrect smoothing of the time series, loss of the level line and poor trend display in the data.\nAlso, the bigger value for alpha is selected, the better trend and level are visible in the data plot.\n\nCertain combinations of the parameters may produce strange results, especially if set manually. We'll look into choosing parameters automatically in a bit; before that, let's discuss triple exponential smoothing.","b760b041":"* **Weighted average**            \nWeighted average is a simple modification of the moving average, inside of which observations have different weights summing up to one, usually more recent observations have greater weight.\n<img src=\"https:\/\/miro.medium.com\/max\/1655\/1*8IBEmsFCywApUE_joR0aqA.png\" alt=\"weighted average\" style=\"width: 500px;\"\/>\n","fdea6001":"Lets try to make features visible:"}}