{"cell_type":{"02e14d4d":"code","354c5e5c":"code","12401259":"code","73b8c613":"code","0c80c509":"code","646a7b19":"code","5ff7dd99":"code","563928ad":"code","96b2fea0":"code","b03d9b2c":"code","b6577d1f":"code","dfe74dc9":"code","16a59de9":"code","f814c2c3":"code","b7f28210":"code","860760c5":"code","72634100":"code","6cea3387":"code","5e7c4b77":"code","0948165b":"code","ff85cac5":"code","eed77582":"code","5e153ac3":"markdown","9b7184b1":"markdown"},"source":{"02e14d4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Uncomment to see where your variables get placed (see below)\n# tf.debugging.set_log_device_placement(True)","354c5e5c":"tensor = tf.constant([[0,1,2],[3,4,5]])\nvar = tf.Variable(tensor)\n\n# tf variables can be all types such as boolean or complex numbers\ntf.Variable([False,False,True,True])\n\nprint(\"var = \", var)\nprint(\"shape = \", var.shape)\nprint(\"dtype = \", var.dtype)\nprint(\"numpi = \", var.numpy, \"\\n\\n\",np.array(var))\n","12401259":"print(\"var = \", var)\nprint(\"\\ntensor (constant) = \", tf.convert_to_tensor(var))\n\n# This creates a new tensor; it does not reshape the variable.\nprint(\"\\nCopying and reshaping: \", tf.reshape(var, ([1,6])))","73b8c613":"# assignment is psossible with veriables as inline operation like list.append\nvar.assign([[18,1,2],[3,4,5]])\nprint(var)","0c80c509":"# a and b are on the different locations of memory\na = tf.Variable([2.0, 3.0])\n# Create b based on the value of a\nb = tf.Variable(a)\na.assign([5, 6])\n\n# a and b are different\nprint(a.numpy())\nprint(b.numpy())\n\n# There are other versions of assign\nprint(a.assign_add([2,3]).numpy())  # [7. 9.]\nprint(a.assign_sub([7,9]).numpy())  # [0. 0.]\n","646a7b19":"new_var = tf.Variable(a, name = \"tekmen0\")\nprint(new_var)","5ff7dd99":"# this variable will not be differentiated nor updated\nstep_counter = tf.Variable(1, trainable=False)","563928ad":"#to create variables on cpu\nwith tf.device('CPU:0'):\n\n  # Create some tensors\n  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n  c = tf.matmul(a, b)\n\nprint(c)\n","96b2fea0":"#store variables in cpu but do the computation on the gpu\n#however this is slower than doing all of the things on gpu\nwith tf.device('CPU:0'):\n  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n  b = tf.Variable([[1.0, 2.0, 3.0]])\n\nwith tf.device('GPU:0'):\n  # Element-wise multiply\n  k = a * b\n\nprint(k)","b03d9b2c":"x = tf.Variable(3.0)\n\n# record operation to compute gradients\nwith tf.GradientTape() as tape:\n  y = x**2\n\n#get the gradient of y with respect to x\ndy_dx = tape.gradient(y,x)\n\nprint(dy_dx)\nprint(dy_dx.numpy())","b6577d1f":"w = tf.Variable(tf.random.normal((3,2), name=\"w\"))\nb = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\"b\")\nx = [[1.,2.,3.]]\n\n# persistent opsiyonu default oldu\u011fu gibi False olunca\n# tape.gradient() methodu \u00e7a\u011fr\u0131l\u0131r \u00e7a\u011f\u0131r\u0131lmaz gradientler \n# kayboluyor memoryden, multiple output \u00e7a\u011f\u0131r\u0131rken \u00f6nemli\nwith tf.GradientTape(persistent=True) as tape:\n    #y tensor olarak kal\u0131yor\n    y = tf.matmul(x,w) + b\n    loss = tf.reduce_mean(y)\n\n[dl_dw, dl_db] = tape.gradient(loss, [w, b])\nprint(dl_dw, \"\\n\\n\",dl_db)\n\nmy_vars = { \"w\" : w,\n              \"b\" : b}\n\n#dictionaries are also possible \ngrads = tape.gradient(loss, my_vars)\nprint(grads[\"w\"])\n\n# will release tape (it is released as defult from one \n# call after of gradient)\ndel tape","dfe74dc9":"layer = tf.keras.layers.Dense(2, activation = \"relu\")\nx = tf.constant([[1.,2.,3.]])\n\nwith tf.GradientTape() as tape:\n    y = layer(x)\n    loss = tf.reduce_mean(y)\n\ngrads = tape.gradient(loss, layer.trainable_variables)\n\n# do not forget that layer.trainable_variables are just tensorflow variables\n# (n_input, n_next_features) is the dimensions of the weights in tensorflow\nfor var, g in zip(layer.trainable_variables, grads):\n    print(var.name, g.shape)","16a59de9":"#bir modeldeki featurelere nas\u0131l ula\u015f\u0131r\u0131z? ???\n# bir modeldeki weights'e ve biase layer.trainable_variables ya da model.trainable_variables gibi bir \u00f6zellikle eri\u015febiliriz\n\n# Trainable variable\nx0 = tf.Variable(3.0, name=\"x0\")\n# Non-trainable variable \nx1 = tf.Variable(3.0, name=\"x1\", trainable=False)\n# Not a variable because variable + tensor returns tensor\nx2 = tf.Variable(2.0, name=\"x2\") + 1.0 #var + tensor\n# Not a variable\nx3 = tf.constant(3.0, name=\"x3\")\n\n\n# gradient tape sadece computationlar\u0131 tutar, gradients() methodu tutulan computation grapha g\u00f6re hesaplama yapar\n# ve persisten = False'de 1 kere hesaplama yapt\u0131 m\u0131 gradientler kaybolur.\nwith tf.GradientTape(persistent = True) as tape:\n    y = (x0**2) + (x1**2) + (x2**2)\n    \ngrad = tape.gradient(y,[x0,x1,x2,x3])\n\nfor g in grad:\n    print(g)\n    \n# only will compute gradient for x0","f814c2c3":"# list the watched variables by tape\nvariables = [var for var in tape.watched_variables()]\nfor var in variables:\n    print(var.name)","b7f28210":"# to watch tensors and other constants we can use GradientTape().watch(tensor) utility\nx = tf.constant(3.0)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = x**2\n\ndy_dy = tape.gradient(y,x)\nprint(dy_dx.numpy())","860760c5":"x0 = tf.Variable(0.0)\nx1 = tf.Variable(1.)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(x1)\n    y = tf.nn.softplus(x1 + x2)\n    \ngrads = tape.gradient(y,[x1,x0])\n\n#since x0 is not activated by default, no ggradient calculation run on it\nfor i in grads:\n    print(i)","72634100":"x = tf.Variable(3., name=\"x\")\n\nwith tf.GradientTape() as tape:\n    y = x**2\n    z = 3*y\n\n#it is possible to get intermediate value as grad, namely variable y, even if they are tensors\n#they are not listed in the watched variables\n    \nfor g in tape.watched_variables():\n    print(g.name)\nprint()    \n\ngrad = tape.gradient(z, [y,x])\n\nfor g in grad:\n    print(g)","6cea3387":"# if gradient targets (y's) are multiple or not a scalar, \n# sum of gradients or gradient of sums are calculated\n\nx = tf.Variable(2.0)\nwith tf.GradientTape() as tape:\n  y0 = x**2\n  y1 = 1 \/ x\n\nprint(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())\n","5e7c4b77":"# it makes to get cost (sum of losses) easier\nx = tf.Variable(2.)\n\nwith tf.GradientTape() as tape:\n  y = x * [3., 4.]\n\nprint(tape.gradient(y, x).numpy())\n\n# to get every values gradient seperate, there is jacobian option","0948165b":"# linear aral\u0131kl\u0131 tensor olu\u015ftur\nx = tf.linspace(-10, 10 , 200+1)\n\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    # burada y'de her elemente g\u00f6re y vekt\u00f6r\u00fcn\u00fcn gradientler toplam\u0131n\u0131 alsa bile elementwise gradient alm\u0131\u015f oluyor\n    # jacobian hesaplamay\u0131 skip edebiliriz bu y\u00fczden her element ba\u011f\u0131ms\u0131z oldu\u011fu i\u00e7in\n    y = tf.nn.sigmoid(x)\n\ndy_dx = tape.gradient(y,x)\n\nplt.plot(x, y, label='y')\nplt.plot(x, dy_dx, label=\"dy_dx\")\nplt.legend()\n_ = plt.xlabel(\"x\")","ff85cac5":"# if while gibi controlf-flow durumlar\u0131 do\u011fal olarak handle eder tensorflow\n# \u00e7\u00fcnk\u00fc hesapland\u0131k\u00e7a kaydediyor i\u015flemi\n\nx = tf.constant(1.0)\n\nv0 = tf.Variable(2.)\nv1 = tf.Variable(2.)\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n    \n    if x > 0.0:\n        result = v0\n    else:\n        result = v1**2\n    \ndv0, dv1 = tape.gradient(result, [v0,v1])\n\nprint(dv0)\nprint(dv1)\n\n#control flow operations are invisible to gradient based optimizers","eed77582":"# gradient returns None if variable and function is not connected\nx = tf.Variable(1.0)\ny = tf.Variable(2.0)\n\nwith tf.GradientTape() as tape:\n    z = y**2\n\ndy, dx = tape.gradient(z, [y,x])\n\nprint(\"gradient returns None if variable and function is not connected:\\n\")\nprint(dy)\nprint(dx)\n\n# gradient returns None if variable operation is unintentionally returned tensor\n# since tensors aren't watched by tape default\n\nwith tf.GradientTape() as tape:\n    z = y + 1 #var + tensor\n\nprint(\"\\n\", \"#\"*100, \"\\n\")\nprint(\"gradient returns None if variable operation is unintentionally returned tensor\")\nprint(tape.gradient(z,y))\n\n# gradient returns None when calculations are did outside tensorflow\n\nx = tf.Variable(([[0,1],[2,3]]))\nwith tf.GradientTape() as tape:\n    x2 = x**2\n\n    #not a tensorflow operation\n    y = np.mean(x2)\n\n    y = tf.reduce_mean(y)\n\nprint(\"\\n\", \"#\"*100, \"\\n\")\nprint(\"gradient returns None when calculations are did outside tensorflow\")\nprint(tape.gradient(y,x))\n\n# dtype = integer olan \u015feylerin graidentini alma genelde\n# baz\u0131 gradient sonland\u0131r\u0131c\u0131 i\u015flemlerde None d\u00f6n\u00fcyor mesela x1.assign_add(x0), x0'\u0131 hesaplamayacak\n# baz\u0131 float i\u015flemleri de gradient hesaplama implementasyonu yap\u0131lmam\u0131\u015f i\u015flemler, contrast ayarlama gibi","5e153ac3":"## Understanding tensorflow automatic differentiation","9b7184b1":"### all tf.keras.layers.some_layer.trainable_variables are just tf variables"}}