{"cell_type":{"1f100dbf":"code","8e5be468":"code","6e0c915b":"code","c8b933a9":"code","70073f2a":"code","a2eb9be0":"code","f0ae1717":"code","cad9f344":"code","9f11e648":"code","6d1894a3":"code","dfe8ed65":"code","d242b67c":"code","105089d5":"code","f2eff62c":"markdown","988dffae":"markdown","8abfa18b":"markdown","84919227":"markdown","06541cb8":"markdown"},"source":{"1f100dbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8e5be468":"telco_original = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\ntelco_original['TotalCharges'] = telco_original.TotalCharges.replace({' ': 0})\ntelco_original['TotalCharges'] = pd.to_numeric(telco_original.TotalCharges, errors='coerce')\n# remove the 9 rows with missing values\nprint(telco_original.info())\n\ntelco_original = telco_original.drop('customerID', axis=1)\n\ntelco_original['Churn'] = telco_original.Churn.replace({'No': 0, 'Yes':1})\n\nX, y = telco_original.drop('Churn', axis=1), telco_original.Churn","6e0c915b":"telco_original.nunique()","c8b933a9":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# generate the list of categorical and numerical variables\ncategorical_variables = X.nunique()[X.nunique() < 5].keys().to_list()\n\nnumerical_variables=list(set(X.columns) - set(categorical_variables))\n\nohe = OneHotEncoder(drop='first', sparse=False)\n\nX_ohe = ohe.fit_transform(X[categorical_variables])\nX_ohe_df = pd.DataFrame(X_ohe, columns=ohe.get_feature_names(categorical_variables))\n\n# Merging the transformed dataframe togheter\nX = pd.merge(X[numerical_variables], X_ohe_df, left_index=True, right_index=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, \n                                                    test_size=0.2, random_state=123)","70073f2a":"import xgboost as xgb\nfrom hyperopt import STATUS_OK\n\ntrain_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n\nN_FOLDS = 5\n\n# define objective to minimize\ndef objective(params, n_folds = N_FOLDS):\n    params['objective'] = 'binary:logistic'\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=train_dmatrix, params=params,\n                  nfold=n_folds, num_boost_round=10000, early_stopping_rounds=100, \n                  metrics=\"auc\", as_pandas=True, seed=123)\n\n    # Print the accuracy\n    loss = 1 - cv_results[\"test-auc-mean\"].iloc[-1]\n    n_estimators = cv_results[\"test-auc-mean\"].idxmax() + 1\n    return {'loss': loss, 'params': params, 'n_estimators': n_estimators, 'status': STATUS_OK}","a2eb9be0":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\n\nhyperparameter_space = {\n    'n_jobs': -1,\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n    'subsample': hp.uniform('subsample', 0.6, 1.0),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 7, 2),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'max_depth': hp.randint('max_depth', 1,16),\n    'gamma': hp.uniform('gamma', 0.1,0.4),\n    'max_delta_step': hp.randint('max_delta_step',0,10),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2))\n}","f0ae1717":"from hyperopt import Trials\n\nbayes_trials = Trials()","cad9f344":"from hyperopt import fmin\nfrom hyperopt import tpe\n\nMAX_EVALS = 50\n\nbest = fmin(fn = objective, space = hyperparameter_space, algo = tpe.suggest, max_evals = MAX_EVALS,\n           trials = bayes_trials, rstate = np.random.RandomState(50))","9f11e648":"best","6d1894a3":"best['num_boost_round']=10000\nbest['early_stopping_rounds']=100\n\nxgb_best = xgb.XGBClassifier(**best)\n\nxgb_best.fit(X_train, y_train)","dfe8ed65":"# Import roc_curve\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\ndef add_roc_plot(model, test_x, test_y, legend_text):\n    y_pred_prob = model.predict_proba(test_x)[:, 1]\n    # Calculate the roc metrics\n    fpr, tpr, thresholds = roc_curve(test_y, y_pred_prob)\n    # Plot the ROC curve\n    plt.plot(fpr, tpr, label=legend_text)\n    plt.legend()\n\n    \nmodels_list = [xgb_best]\nmodel_names = ['Extreme Gradient Boosting']\n\nplt.figure(figsize=(6, 6))\n[add_roc_plot(model, X_test, y_test, legend_text) for model, legend_text in zip(models_list, model_names)]\n\n# Add labels and diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim((0,1))\nplt.ylim((0,1))\nplt.plot([0, 1], [0, 1], \"k-\")\nplt.show()","d242b67c":"from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score\n\nlist_scores = [roc_auc_score, recall_score, precision_score, accuracy_score]\ncalc_scores = []\ndef compute_scores(model, x_test, y_test, scores):\n    return [round(score(y_test, model.predict(x_test)), 2) for score in scores]\n    \n[calc_scores.append(compute_scores(model, X_test, y_test, list_scores)) for model in models_list] \n\nscore_names = ['roc_auc', 'recall', 'precision', 'accuracy']\nscores_df = pd.DataFrame(calc_scores, columns=score_names, index=model_names)\n\nscores_df","105089d5":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1,1,figsize=(12,10))\nxgb.plot_importance(xgb_best, ax = ax)\nplt.show()","f2eff62c":"## Models fitting\n","988dffae":"The hyperparameters tuning of the XGBoost model will be accomplished with a bayesian optimization using Tree Parzen Estimator implemented in the hyperopt package.","8abfa18b":"# Churn Prediction with XGBoost and hyperopt\n\nExtreme Gradient Boosting is used to predict customer churn. To select the best hyperparameters a 5-Fold Cross Validation with bayesian search is used. \nSince the dataset is unbalanced with respect to Churn the area under the ROC curve (roc_auc) was used as criterium for the selection.","84919227":"All the object columns contain only few categories (<5) and can be OneHot encoded (this step could have been done before splitting too, but for consistency with the standardization procedure it will be done after).","06541cb8":"## Preparing the data for machine learning\n\n1. Set empty TotalCharges to 0 and convert it to numeric\n2. Encode Churn as 0 for 'No' and 1 for 'Yes'\n3. One-Hot encoding of categorical data\n4. Drop customerID"}}