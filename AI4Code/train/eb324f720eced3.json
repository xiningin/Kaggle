{"cell_type":{"b1d8ab90":"code","d02d78ff":"code","5b5b2500":"code","bf606d75":"code","b7977708":"code","517c8817":"code","4f59e390":"code","17cebb27":"code","aee4f41e":"code","7ae6f37a":"code","52c3c1c0":"code","9fe9b822":"code","3430b74c":"code","ca7cd145":"code","8935e2e0":"code","bbdc066b":"code","683b78f3":"code","b5208d89":"code","57248f36":"code","522e4022":"code","ab212267":"code","40bb4c86":"code","f588ecea":"code","5b7f4264":"code","5d7b5625":"code","14c68b73":"code","f0fc613f":"code","11aa1b06":"code","c8732922":"code","9b93448c":"code","e572140f":"code","e8058784":"code","5583374f":"code","9c06907b":"code","a6c175ca":"code","dd4ed9b1":"code","8e2103bf":"code","38b5e46f":"code","eb19ff89":"code","0407fcce":"code","06664e1b":"code","c3055ca4":"code","3b1c9f9c":"code","0e5db502":"code","784d51c6":"code","1e4b9c4f":"code","7a7b5c6c":"code","11b1a72a":"code","1e23105b":"code","9fc8bb33":"code","3fa30c25":"code","878cb96d":"code","cc170827":"code","d4be0ef7":"code","a227ce49":"code","8d508745":"markdown","bfad02fa":"markdown","2665c9f3":"markdown","1c6b65a4":"markdown","6712bd56":"markdown","36a3a7ed":"markdown","87e3d4f1":"markdown","5ea99960":"markdown","1ee0980c":"markdown","3888b86f":"markdown","f5926782":"markdown"},"source":{"b1d8ab90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d02d78ff":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\nimport math\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","5b5b2500":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\nplt.rcParams['axes.facecolor'] = primary_bgcolor\n\ncolors = [primary_blue, primary_blue2, primary_blue3, primary_grey, primary_black, primary_bgcolor, primary_green]\nsns.palplot(sns.color_palette(colors))","bf606d75":"plt.rcParams['figure.dpi'] = 120\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['font.family'] = 'serif'","b7977708":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain_df.columns = [column.lower() for column in train_df.columns]\n# train_df = train_df.drop(columns=['passengerid'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest_df.columns = [column.lower() for column in test_df.columns]\n\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\nsubmission.head()\n\ntrain_df.head()","517c8817":"feature_cols = train_df.drop(['survived', 'passengerid'], axis=1).columns\ntarget_column = 'survived'\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = ['age', 'fare']\ncategorical_columns = train_df[feature_cols].drop(columns=numerical_columns).columns\n\npure_num_cols = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\npure_cat_cols = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\nprint(len(numerical_columns), len(categorical_columns))","4f59e390":"!pip install -U lightautoml","17cebb27":"# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\n\nimport torch","aee4f41e":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","7ae6f37a":"def acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ndef f1_metric(y_true, y_pred, **kwargs):\n    return f1_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ntask = Task('binary', metric = f1_metric)\n\nroles = {\n    'target': 'survived',\n    'drop': ['passengerid', 'name', 'ticket'],\n}","52c3c1c0":"%%time \nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(train_df, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","9fe9b822":"%%time\ntest_pred = automl.predict(test_df)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(acc_score(train_df['survived'].values, oof_pred.data[:, 0])))","3430b74c":"submission['Survived'] = (test_pred.data[:, 0] > 0.5).astype(int)\nsubmission.to_csv('lightautoml_utilized_300s_f1_metric.csv', index = False)\nsubmission.head()","ca7cd145":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ntrain_df.columns = [column.lower() for column in train_df.columns]\n# train_df = train_df.drop(columns=['passengerid'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')\ntest_df.columns = [column.lower() for column in test_df.columns]\n\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\nsubmission.head()\n\ntrain_df.head()","8935e2e0":"feature_columns = train_df.iloc[:, 1:-1].columns.values\ntarget_column = 'target'\nfeature_columns","bbdc066b":"import h2o\nfrom h2o.automl import H2OAutoML","683b78f3":"h2o.init()","b5208d89":"train_hf = h2o.H2OFrame(train_df.copy())\ntest_hf = h2o.H2OFrame(test_df.copy())","57248f36":"train_hf[target_column] = train_hf[target_column].asfactor()","522e4022":"%%time\naml = H2OAutoML(\n    seed=2021, \n    max_runtime_secs=100,\n    nfolds = 3,\n    exclude_algos = [\"DeepLearning\"]\n)\n\naml.train(\n    x=list(feature_columns), \n    y=target_column, \n    training_frame=train_hf\n)","ab212267":"lb = aml.leaderboard \nlb.head(rows = lb.nrows)","40bb4c86":"%%time\n\npreds = aml.predict(h2o.H2OFrame(test_df[feature_columns].copy()))\npreds_df = h2o.as_list(preds)\npreds_df\n\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = preds_df[['Class_1', 'Class_2', 'Class_3', 'Class_4']]\nsubmission.to_csv('h2o_automl_300s.csv', index=False)\nsubmission.head()","f588ecea":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_df.columns = [column.lower() for column in train_df.columns]\n# train_df = train_df.drop(columns=['passengerid'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_df.columns = [column.lower() for column in test_df.columns]\n\nsubmission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\nfeature_columns = train_df.drop(['survived', 'passengerid'], axis=1).columns\ntarget_column = 'survived'\n\ntrain_hf = h2o.H2OFrame(train_df.copy())\ntest_hf = h2o.H2OFrame(test_df.copy())\n\ntrain_df.head()","5b7f4264":"%%time\naml = H2OAutoML(\n    seed=2021, \n    max_runtime_secs=100,\n    nfolds = 3,\n    exclude_algos = [\"DeepLearning\"]\n)\n\naml.train(\n    x=list(feature_columns), \n    y=target_column, \n    training_frame=train_hf\n)","5d7b5625":"lb = aml.leaderboard \nlb.head(rows = lb.nrows)","14c68b73":"%%script false --no-raise-error\n!apt-get install -y build-essential python3-dev","f0fc613f":"%%script false --no-raise-error\n!pip -q install pip --upgrade\n!pip install graphviz --upgrade\n!pip install dtreeviz\n!pip install mljar-supervised","11aa1b06":"%%script false --no-raise-error\nfrom supervised.automl import AutoML # mljar-supervised","c8732922":"%%script false --no-raise-error\n%%time\nautoml = AutoML(\n    mode=\"Compete\", \n    eval_metric=\"f1\",\n    total_time_limit=300,\n    features_selection=False # switch off feature selection\n)\nautoml.fit(\n    train[feature_cols], \n    train[target_column]\n)","9b93448c":"%%script false --no-raise-error\n%%time\npreds = automl.predict(test[feature_cols])\n\nsubmission['Survived'] = preds\nsubmission.to_csv('mljar_automl_300s_f1_metric.csv', index=False)\nsubmission.head()","e572140f":"%%script false --no-raise-error\n!pip install pycaret","e8058784":"%%script false --no-raise-error\nfrom pycaret.classification import *","5583374f":"%%script false --no-raise-error\nfrom category_encoders.cat_boost import CatBoostEncoder\n\ncat_train_df = train_df.copy()\ncat_test_df = test_df.copy()\n\nce = CatBoostEncoder()\n\ncols_to_encode = ['name', 'sex', 'ticket', 'cabin', 'embarked']\ncat_train_df[pure_cat_cols] = ce.fit_transform(cat_train_df[pure_cat_cols], cat_train_df[target_column])\ncat_test_df[pure_cat_cols] = ce.transform(cat_test_df[pure_cat_cols])","9c06907b":"%%script false --no-raise-error\nsetup(\n    data = cat_train_df[feature_cols.to_list() + [target_column]], \n    target = target_column,\n    fold = 3,\n    silent = True,\n)","a6c175ca":"%%script false --no-raise-error\n%%time\nbest_models = compare_models(\n    sort='F1', \n    n_select=3, \n    budget_time=300,\n) # we will use it later","dd4ed9b1":"%%script false --no-raise-error\n# select best model \nbest = automl(optimize = 'F1')","8e2103bf":"%%script false --no-raise-error\nplot_model(best, plot = 'confusion_matrix')\n\nplot_model(best, plot = 'feature_all')","38b5e46f":"%%script false --no-raise-error\n!pip install evalml","eb19ff89":"%%script false --no-raise-error\nfrom evalml.automl import AutoMLSearch","0407fcce":"%%script false --no-raise-error\nX = train_df.drop(columns=[target_column, 'passengerid'])\ny = train_df[target_column]\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2)","06664e1b":"%%script false --no-raise-error\n%%time\nautoml = AutoMLSearch(\n    X_train=X_train, \n    y_train=y_train, \n    problem_type='binary',\n    random_seed=2021,\n    max_time=300,\n)","c3055ca4":"%%script false --no-raise-error\nautoml.search()","3b1c9f9c":"%%script false --no-raise-error\nautoml.rankings","0e5db502":"%%script false --no-raise-error\n%%time\npipeline = automl.best_pipeline\npipeline.fit(X, y)","784d51c6":"%%script false --no-raise-error\npreds = pipeline.predict(test_df.drop([target_column, 'passengerid'], axis=1))\n\nsubmission['Survived'] = preds.to_series().astype(int)\nsubmission.to_csv('evalml_automl_300s_f1_metric.csv', index=False)\nsubmission.head()","1e4b9c4f":"%%script false --no-raise-error\n!pip install tpot","7a7b5c6c":"%%script false --no-raise-error\nfrom category_encoders.cat_boost import CatBoostEncoder\n\ncat_train_df = train_df.copy()\ncat_test_df = test_df.copy()\n\nce = CatBoostEncoder()\n\ncols_to_encode = ['name', 'sex', 'ticket', 'cabin', 'embarked']\ncat_train_df[pure_cat_cols] = ce.fit_transform(cat_train_df[pure_cat_cols], cat_train_df[target_column])\ncat_test_df[pure_cat_cols] = ce.transform(cat_test_df[pure_cat_cols])","11b1a72a":"%%script false --no-raise-error\nX = cat_train_df.drop(columns=[target_column, 'passengerid'])\ny = cat_train_df[target_column]\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=2021)","1e23105b":"%%script false --no-raise-error\n\nfrom tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')","9fc8bb33":"%%script false --no-raise-error\n!pip install flaml","3fa30c25":"%%script false --no-raise-error\nfrom flaml import AutoML\nfrom sklearn.datasets import load_boston","878cb96d":"%%script false --no-raise-error\n# Initialize an AutoML instance\nautoml = AutoML()\n\n# Specify automl goal and constraint\nautoml_settings = {\n    \"time_budget\": 300,  # in seconds\n    \"metric\": 'accuracy',\n    \"task\": 'classification',\n}","cc170827":"%%script false --no-raise-error\n# Train with labeled input data\nautoml.fit(\n    X_train=train_df[feature_cols], \n    y_train=train_df[target_column],\n    **automl_settings\n)","d4be0ef7":"%%script false --no-raise-error\n# Predict\nprint(automl.predict_proba(train_df[feature_cols]))\n# Export the best model\nprint(automl.model)","a227ce49":"%%script false --no-raise-error\n\n# Initialize an AutoML instance\nautoml = AutoML()\n\n# Specify automl goal and constraint\nautoml_settings = {\n    \"time_budget\": 10,  # in seconds\n    \"metric\": 'r2',\n    \"task\": 'regression',\n    \"log_file_name\": \"test\/boston.log\",\n}\n\nX_train, y_train = load_boston(return_X_y=True)\n# Train with labeled input data\nautoml.fit(X_train=X_train, y_train=y_train,\n                        **automl_settings)\n\n# Predict\nprint(automl.predict(X_train))\n# Export the best model\nprint(automl.model)","8d508745":"<a id='1'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. LightAutoML<\/p>\n\n![automl-generic-logo.png](attachment:5532bf4a-417b-4f19-8871-e05b3e1d17e7.png)\n\nLightAutoML is open-source Python library aimed at automated machine learning. It is designed to be lightweight and efficient for various tasks with tabular, text data. LightAutoML provides easy-to-use pipeline creation, that enables:\n\n* Automatic hyperparameter tuning, data processing.\n* Automatic typing, feature selection.\n* Automatic time utilization.\n* Automatic report creation.\n* Graphical profiling system.\n* Easy-to-use modular scheme to create your own pipelines.\n\nRef: https:\/\/github.com\/sberbank-ai-lab\/LightAutoML","bfad02fa":"<a id='2'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. H2O AutoML<\/p>\n\n![h2oia-logo.png](attachment:5edfcd80-f2fc-4e2b-8bfb-638813e8b4dd.png)\n\nThe H2O AutoML interface is designed to have as few parameters as possible so that all the user needs to do is point to their dataset, identify the response column and optionally specify a time constraint or limit on the number of total models trained.\n\nIn both the R and Python API, AutoML uses the same data-related arguments, x, y, training_frame, validation_frame, as the other H2O algorithms. Most of the time, all you\u2019ll need to do is specify the data arguments. You can then configure values for max_runtime_secs and\/or max_models to set explicit time or number-of-model limits on your run.\n\nRef: https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html","2665c9f3":"### Simple regression problem","1c6b65a4":"## Titanic","6712bd56":"<a id='5'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. EvalML: AutoML<\/p>\n\n![evalml-logo.png](attachment:d0680f7c-05a5-4681-b9e5-c2aebf4b06f0.png)\n\nEvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-specific objective functions.\n\n**Key Functionality**\n* **Automation** - Makes machine learning easier. Avoid training and tuning models by hand. Includes data quality checks, cross-validation and more.\n* **Data Checks** - Catches and warns of problems with your data and problem setup before modeling.\n* **End-to-end** - Constructs and optimizes pipelines that include state-of-the-art preprocessing, feature engineering, feature selection, and a variety of modeling techniques.\n* **Model Understanding** - Provides tools to understand and introspect on models, to learn how they'll behave in your problem domain.\n* **Domain-specific** - Includes repository of domain-specific objective functions and an interface to define your own.\n\nRef: https:\/\/evalml.alteryx.com\/en\/latest\/_modules\/evalml\/automl\/automl_search.html","36a3a7ed":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">\ud83c\udfc4\u200d\u2642\ufe0f AutoML Libraries Comparison \ud83d\udcc8<\/p>\n\n![kaggle-python.png](attachment:73eea8b7-cb73-4d3c-8bf8-81ee6d793519.png)\n\nThe main idea of this kernel is to compare the most popular AutoMl algorithems in terms of setup and competition performance (f1-score).\n\nIn all the cases I will just limit the time to 5 minutes (it could be better to use more time, but you can always fork and try yourselfs). The idea is to give a baseline so you can use the one is easier for you or the one the better performs.\n\nAs the data is synthetic, I will do just some simple feature engineering. With this method, we can also compare how all the libraries handle categorical data etc.\n\nIn recent years, the demand for machine learning experts has outpaced the supply, despite the surge of people entering the field. To address this gap, there have been big strides in the development of user-friendly machine learning software that can be used by non-experts. The first steps toward simplifying machine learning involved developing simple, unified interfaces to a variety of machine learning algorithms.\n\nAs you are going to see, many of the libraries are not going to be run because of machine resources and some packages issues with Kaggle Docker environment. Anyway, the idea is to show how to use it, which functions to use, and how simple it is. Remember that you can fork and try your self on your self computer.","87e3d4f1":"<a id='7'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">7. FLAML: Fast and Lightweight AutoML<\/p>\n\n![FLAML.png](attachment:bd1a7c87-ee2f-4a87-874d-9966024c6024.png)\n\nFLAML is a lightweight Python library that finds accurate machine learning models automatically, efficiently and economically. It frees users from selecting learners and hyperparameters for each learner. It is fast and economical. The simple and lightweight design makes it easy to extend, such as adding customized learners or metrics. FLAML is powered by a new, [cost-effective hyperparameter optimization and learner selection](https:\/\/github.com\/microsoft\/FLAML\/tree\/main\/flaml\/tune) method invented by Microsoft Research. FLAML leverages the structure of the search space to choose a search order optimized for both cost and error. For example, the system tends to propose cheap configurations at the beginning stage of the search, but quickly moves to configurations with high model complexity and large sample size when needed in the later stage of the search. For another example, it favors cheap learners in the beginning but penalizes them later if the error improvement is slow. The cost-bounded search and cost-based prioritization make a big difference in the search efficiency under budget constraints.\n\nRef: https:\/\/github.com\/microsoft\/FLAML","5ea99960":"<a id='6'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">6. TPOT: Genetic Approach<\/p>\n\n![tpot-logo.jpeg](attachment:d7ee6694-5a87-448f-9386-07edc866ecfc.jpeg)","1ee0980c":"<a id='3'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. MLJAR AutoML<\/p>\n\n![mljar-logo.png](attachment:53c7fa2c-719d-4001-b380-592cb5df008d.png)\n\nMLJAR is an Automated Machine Learning framework. It is available as Python package with code at GitHub: https:\/\/github.com\/mljar\/mljar-supervised\n\nThe MLJAR AutoML can work in several modes:\n\n* **Explain** - ideal for initial data exploration\n* **Perform** - perfect for production-level ML systems\n* **Compete** - mode for ML competitions under restricted time budget. By the default, it performs advanced feature engineering like golden features search, kmeans features, feature selection. It does model stacking.\n* **Optuna** - uses Optuna to highly tune algorithms: `Random Forest`, `Extra Trees`, `Xgboost`, `LightGBM`, `CatBoost`, `Neural Network`. Each algorithm is tuned with Optuna hperparameters framework with selected time budget (controlled with optuna_time_budget). By the default feature engineering is not enabled (you need to manually swtich it on, in AutoML() parameter).\n\nRef: https:\/\/www.kaggle.com\/mt77pp\/mljar-automl-tps-apr-21\n\n*These execution cells are not going to be executed as MLJAR is getting troubles to be installed in Kaggle kernels environment*","3888b86f":"<a id='4'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. PyCaret<\/p>\n\n![pycaret.png](attachment:1f521119-fee2-4755-99ac-e2e72d0be7e9.png)\n\n*These execution cells are not going to be executed as Pycaret spent so much resources and time and cant be executed in Kaggle kernels environment. You can always fork\/clone and execute locally*","f5926782":"<a id='table-of-contents'><\/a>\n<h2 style=\"font-family: Serif; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\">TABLE OF CONTENTS<\/h2>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#1\">I&nbsp;&nbsp;&nbsp;&nbsp;LightAutoML<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#2\">II&nbsp;&nbsp;&nbsp;&nbsp;H2O AutoML<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#3\">III&nbsp;&nbsp;&nbsp;&nbsp;MLJAR AutoML<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#4\">IV&nbsp;&nbsp;&nbsp;&nbsp;PyCaret<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#5\">V&nbsp;&nbsp;&nbsp;&nbsp;EvalML alteryx<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#6\">VI&nbsp;&nbsp;&nbsp;&nbsp;TPOT: AutoML Genetic Appoach <\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Serif; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#7\">VII&nbsp;&nbsp;&nbsp;&nbsp;FLAML: Microsoft AutoML Framework<\/a><\/h3>\n\n---"}}