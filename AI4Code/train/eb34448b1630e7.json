{"cell_type":{"a1684ef6":"code","7df40b59":"code","4d292c32":"code","5a9a2b40":"code","51acdaab":"code","41c8572a":"code","eacbd8f9":"code","847fa046":"code","35526f1f":"code","0203e1d6":"code","681076bf":"code","6f3fbdc7":"code","85c4e7d2":"code","92b93a6f":"code","04fd1a3c":"code","667885c9":"code","5176fc03":"code","63b978f4":"code","dcfddca5":"code","85cf730d":"code","000b2d40":"code","1c523f4f":"code","9a6b8432":"code","a63cd815":"code","9c46d880":"markdown","bce9805b":"markdown","9c620875":"markdown","38ac4fbe":"markdown","7d4f7396":"markdown","e04abbed":"markdown","a7565a48":"markdown","58632cc5":"markdown","243939da":"markdown","11c934a6":"markdown","2dc7fc5a":"markdown","6cbd7036":"markdown","80fa746f":"markdown","a7cdf45b":"markdown","9726dd7a":"markdown","06e49246":"markdown","9ad20677":"markdown","cec96309":"markdown","5f5f21aa":"markdown","c62596c6":"markdown"},"source":{"a1684ef6":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt                  # plots\n\nfrom scipy.optimize import minimize  \n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","7df40b59":"ads = pd.read_csv('..\/input\/mlcourse\/ads.csv', index_col=['Time'], parse_dates=['Time'])\ncurrency = pd.read_csv('..\/input\/mlcourse\/currency.csv', index_col=['Time'], parse_dates=['Time'])\n","4d292c32":"plt.figure(figsize=(15, 7))\nplt.plot(ads.Ads)\nplt.title('Ads watched (hourly data)')\nplt.grid(True)\nplt.show()  ","5a9a2b40":"plt.figure(figsize=(15, 7))\nplt.plot(currency.GEMS_GEMS_SPENT)\nplt.title('In-game currency spent (daily data)')\nplt.grid(True)\nplt.show()","51acdaab":"from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import  mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","41c8572a":"def moving_average(series, k):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-k:])\n\nmoving_average(ads, 24) # prediction for the last observed day (past 24 hours)","eacbd8f9":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond \/ Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","847fa046":"plotMovingAverage(ads, 4) ","35526f1f":"plotMovingAverage(ads, 12) ","0203e1d6":"plotMovingAverage(ads, 4, plot_intervals=True)","681076bf":"plotMovingAverage(currency, 4, plot_intervals=True)","6f3fbdc7":"ads_anomaly = ads.copy()\nads_anomaly.iloc[-20] = ads_anomaly.iloc[-20] * 0.2 # say we have 80% drop of ads ","85c4e7d2":"plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)","92b93a6f":"plotMovingAverage(currency, 7, plot_intervals=True, plot_anomalies=True) # weekly smoothing","04fd1a3c":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)","667885c9":"weighted_average(ads, [0.6, 0.3, 0.1])","5176fc03":"class HoltWinters:    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n         \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) \/ self.slen\n        return sum \/ self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)\/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])\/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg\/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","63b978f4":"from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n\ndef timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=3) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], slen=slen, \n                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","dcfddca5":"%%time\ndata = ads.Ads[:-20] # leave some data for testing\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimizing the loss function \nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_squared_log_error), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n# Take optimal values...\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\n# ...and train the model with them, forecasting for the next 50 hours\nmodel = HoltWinters(data, slen = 24, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 50, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","85cf730d":"Add some code for plotting the results:","000b2d40":"def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    plt.figure(figsize=(20, 10))\n    plt.plot(model.result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n            series.values[series.values<model.LowerBond[:len(series)]]\n        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n            series.values[series.values>model.UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up\/Low confidence\")\n        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n        \n    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);","1c523f4f":"plotHoltWinters(ads.Ads)","9a6b8432":"plotHoltWinters(ads.Ads, plot_intervals=True, plot_anomalies=True)","a63cd815":"%%time\ndata = currency.GEMS_GEMS_SPENT[:-50] \nslen = 30 # 30-day seasonality\n\nx = [0, 0, 0] \n\nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_absolute_percentage_error, slen), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\nmodel = HoltWinters(data, slen = slen, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 100, scaling_factor = 3)\nmodel.triple_exponential_smoothing()\n\nplotHoltWinters(currency.GEMS_GEMS_SPENT, plot_intervals=True, plot_anomalies=True)","9c46d880":"* Simplest moving average for 1-step ahead prediction:","bce9805b":"Actually, here we have no idea how the weights should be constructed (apart from being non-negative and summing to 1).\n\n","9c620875":"The model was able to successfully approximate the initial time series, capturing the daily seasonality, overall downwards trend, and even some anomalies.","38ac4fbe":"And in currency series as well\/\n## Prepare forecast quality metrics","7d4f7396":"Theoretically, I would like to see of there are any outliers \/anomalies. \"Unfortunately\", there is no of the in the original **ads** series, so I will introduce them artificially to show how the procedure could work.\n","e04abbed":"## Preparation\nFirst, I import the libraries and look at the data folder to see the exact names  of the files which I gonna work with.","a7565a48":"There is clear seasonality in ads watched.","58632cc5":"And the same picture for original(!) **currency** series:","243939da":"Source: https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python","11c934a6":"I will estimate the parameters automatically, using CV.","2dc7fc5a":"In the Holt-Winters model, as well as in the other models of exponential smoothing, there's a constraint on how large the smoothing parameters can be, each of them ranging from 0 to 1. Therefore, in order to minimize our loss function, we have to choose an algorithm that supports constraints on model parameters. In our case, we will use the truncated Newton conjugate gradient.","6cbd7036":"## Reading the data\n","80fa746f":"## 1-B Weighted Moving Average model\nWeighted average is a simple modification to the moving average. The weights sum up to 1 with larger weights assigned to more recent observations.","a7cdf45b":"* Pandas has an implementation available with DataFrame.rolling(window).mean(). The wider the window, the smoother the trend (especially useful with noisy financial data)","9726dd7a":"Ok, here the periodic component looks more regular, but the model reflects all changes in the real data  with certain delay.\n\nLet's plot","06e49246":"And the same picture for currency.","9ad20677":"The same algorithm applied to the second series:","cec96309":"## 1. Statistical approach\n## 1-A Simple Moving Average model\n(the future value of our variable depends on the average of its  k  previous values)","5f5f21aa":"I will work with **ads.csv** and **currency.csv** files (ads watched per hour and in-game currency spend per day). The first column is Time for both.","c62596c6":"## 1-C Triple exponential smoothing a.k.a. Holt-Winters"}}