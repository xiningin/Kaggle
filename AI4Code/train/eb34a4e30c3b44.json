{"cell_type":{"46015710":"code","558d6b31":"code","b4be7032":"code","3211642e":"code","61574f33":"code","dbc4c0c9":"code","ddaf2d65":"code","9f67818e":"code","50f4ac6b":"code","2e2b3af9":"code","6f219e1e":"code","c359da51":"code","0dc0c5bc":"code","092c7bda":"code","3ffc6c1a":"code","421ef610":"code","4dcd087f":"code","4e14c821":"code","d87d30c8":"code","9ba9a0d9":"code","3ca96e5b":"code","cc69f6a4":"markdown","7adc5e58":"markdown","6a95de7e":"markdown","de543bff":"markdown","91468af8":"markdown","36df61c3":"markdown","3d4e7305":"markdown","8ed13b0f":"markdown","8039ddb3":"markdown","8f6120b1":"markdown","0815c34a":"markdown","a439d567":"markdown","57829a7f":"markdown","9f8cefd3":"markdown","a5f1cf07":"markdown"},"source":{"46015710":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_format = 'retina'","558d6b31":"PATH_TO_DATA = Path('..\/input\/hierarchical-text-classification\/')","b4be7032":"train_df = pd.read_csv(PATH_TO_DATA \/ 'train_40k.csv').fillna(' ')\nvalid_df = pd.read_csv(PATH_TO_DATA \/ 'val_10k.csv').fillna(' ')","3211642e":"train_df.head()","61574f33":"train_df.info()","dbc4c0c9":"train_df.loc[0, 'Text']","ddaf2d65":"train_df.loc[0, 'Cat1'], train_df.loc[0, 'Cat2'], train_df.loc[0, 'Cat3']","9f67818e":"train_df['Cat1'].value_counts()","50f4ac6b":"train_df['Cat1_Cat2'] = train_df['Cat1'] + '\/' + train_df['Cat2']\nvalid_df['Cat1_Cat2'] = valid_df['Cat1'] + '\/' + valid_df['Cat2']","2e2b3af9":"train_df['Cat1_Cat2'].nunique()","6f219e1e":"train_df['Cat1_Cat2'].value_counts().head()","c359da51":"# put a limit on maximal number of features and minimal word frequency\ntf_idf = TfidfVectorizer(max_features=50000, min_df=2)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1e2, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=0, \n                           multi_class='multinomial',\n                           fit_intercept=True)\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","0dc0c5bc":"%%time\ntfidf_logit_pipeline.fit(train_df['Title'], train_df['Cat1_Cat2'])","092c7bda":"%%time\nvalid_pred_level_2 = tfidf_logit_pipeline.predict(valid_df['Title'])","3ffc6c1a":"valid_pred_level_1 = [el.split('\/')[0] for el in valid_pred_level_2]","421ef610":"print(\"Level 1:\\n\\tF1 micro (=accuracy): {}\\n\\tF1 weighted:\\t      {}\".format(\n    f1_score(y_true=valid_df['Cat1'], y_pred=valid_pred_level_1, average='micro').round(3),\n    f1_score(y_true=valid_df['Cat1'], y_pred=valid_pred_level_1, average='weighted').round(3)\n    )\n)","4dcd087f":"print(\"Level 2:\\n\\tF1 micro (=accuracy): {}\\n\\tF1 weighted:\\t      {}\".format(\n    f1_score(y_true=valid_df['Cat1_Cat2'], y_pred=valid_pred_level_2, average='micro').round(3),\n    f1_score(y_true=valid_df['Cat1_Cat2'], y_pred=valid_pred_level_2, average='weighted').round(3)\n    )\n)","4e14c821":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title='Confusion matrix', figsize=(7,7),\n                          cmap=plt.cm.Blues, path_to_save_fig=None):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    cm = confusion_matrix(y_true, y_pred).T\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    \n    if path_to_save_fig:\n        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')","d87d30c8":"plot_confusion_matrix(\n    y_true=valid_df['Cat1'],\n    y_pred=valid_pred_level_1, \n    classes=sorted(train_df['Cat1'].unique()),\n    figsize=(8, 8)\n)","9ba9a0d9":"%%capture\nimport eli5","3ca96e5b":"eli5.show_weights(\n    estimator=tfidf_logit_pipeline.named_steps['logit'],\n    vec=tfidf_logit_pipeline.named_steps['tf_idf'])","cc69f6a4":"We concatenate level 1 and level 2 classes, the model will be trained with these targets. It's very important that the model satisfies the class taxonomy. This way it never predicts contradicting level 1 and level 2 classes (eg. 'pet supplies' as L1 and 'meat poultry' as L2 when actually 'meat poultry' is a sub-level of 'grocery gourmet food')","7adc5e58":"Example of a review","6a95de7e":"# <center> Classifying amazon product reviews with logistic regression\n## <center> Two levels of structured classes\n    \nWe are faced to a simple NLP problem \u2013 Amazon product reviews classification. But classes are structured, like in this picture. \n\n<img src=\"https:\/\/habrastorage.org\/webt\/nf\/en\/j7\/nfenj7gktep6dtbrtzgijcsdzwy.png\" width=40%\/>\n\nThat poses a question, what's the best way to approach this hierarchical text classification problem. \n\nHere we present a basic tf-idf + logreg baseline. There're 3 levels of this taxonomy in our data, but here we disregard the 3rd one.\n\n**Idea**\n\nEach review has 3 labels which are elements of a taxonomy, eg. \n\n> 'The description and photo on this product needs to be changed to indicate this product is the BuffalOs version of this beef jerky.'\n\n> Category 1: `grocery gourmet food` \n\n> Category 2: `meat poultry`\n\n> Category 3: `jerky`\n\nFirst, we concatenate Category 1 and Category 2 classes for each sample, eg. `grocery gourmet food\/meat poultry`. Then we train the model and measure F1 score for Category 2.\n\nThen we split the prediction string and thus get predictions for Category 1:\n\n-  Category 3 prediction is `grocery gourmet food\/meat poultry\/jerky` --> Category 1 prediction is `grocery gourmet food`\n\nAfter that we measure F1 scores for Category 1.\n\n**Results:**\n\nF1 micro (=accuracy):\n- Category 1: **0.948**\n- Category 2: **0.889**\n\nPS. using \"level\" and \"category\" interchangeably here.","de543bff":"## Explaining model predictions","91468af8":"Distribution of level 1 classes","36df61c3":"Now we have 64 classes","3d4e7305":"That was a level 2 model. Now to predict level 1 as well we simple take the first part of level1\/level2 prediction. Eg. if 'health personal care\/health care' is predicted, then the level 1 prediction is 'health personal care'","8ed13b0f":"For evaluation, let's take a look at F1 score (micro and weigthed) at Level 1 and Level 2 separately. Note that in a multiclass setting F1 score with micro averaging is the same as accuracy.","8039ddb3":"We'll be training the model with concatenations of review titles and texts","8f6120b1":"Fields:\n\n* productId \u2013 the review is given about this product\n* Title - title of a review as given by the author\n* user - Iduser ID of the author of the review\n* Helpfulness - whether the review is found helpful by other users\n* Score - score of a review as rated by other users\n* Time - timestamp of the review\n* Text - text of a review","0815c34a":"## Reading and analyzing the data","a439d567":"Confusion matrix is quite balanced.","57829a7f":"We can explore words\/ngrams, which a most indicative of different classes. With 64 classes it might me a bit overwhelming though.","9f8cefd3":"## Training the model\n\nWe are training our model only with review titles, a ciuple of experiments show that it works better than with review text. ","a5f1cf07":"Most popular ones (at level 2) are:"}}