{"cell_type":{"e053289f":"code","493f053c":"code","77a6f63b":"code","83538fc2":"code","9046c06d":"code","c5fe7d6b":"code","5ec7a671":"code","f3f7fd94":"code","11d31e83":"code","e84c9780":"code","5f6192ae":"code","e0732c06":"code","d6da5347":"code","d1d21e23":"code","3cb37b08":"code","8ebd5b14":"code","3c08001d":"code","d223e97f":"code","2a98803c":"code","79ecd739":"code","f5cdfa5e":"code","a20d9146":"code","3e2d978e":"code","fb92fefc":"code","81e53218":"code","7ea69853":"code","e0514a70":"code","a55307e0":"code","640f874d":"code","43ed4acc":"code","6b0922fb":"code","f64fc5b1":"code","4dd52dbe":"code","3fa41696":"code","56932f67":"code","4f84cf1f":"code","2d0665a2":"code","e7eb02e4":"code","23479f1a":"code","4a3515a5":"code","cef1875d":"code","195654b2":"code","2311a671":"code","c17d82fc":"code","edb6b2ba":"code","18f32d44":"code","a35c13e1":"code","188f1944":"code","b2239fea":"code","64478c8d":"code","ebc0c645":"code","6e1e1f0a":"code","e159d75f":"code","90aa16f5":"code","995597d8":"code","482c5c3d":"code","22c6f8e2":"code","bce75972":"markdown","9a1bfbbf":"markdown","b40952d5":"markdown","fe52b1bb":"markdown","17b6b43e":"markdown","f1b58a6b":"markdown","6c21b5a4":"markdown","80cc2a1f":"markdown","0e2b360f":"markdown","bbed419d":"markdown","a16278fb":"markdown","5c965fd0":"markdown","67fd410e":"markdown","18710168":"markdown","6defbb82":"markdown","cd3183fd":"markdown","841ced5c":"markdown","d2b73cfc":"markdown","047a5de5":"markdown","d56bd21a":"markdown","0e99415c":"markdown","0cd15bfb":"markdown","9c3fd71f":"markdown","c5aaef39":"markdown","cfa3d385":"markdown","b0e0556f":"markdown","21ed81c4":"markdown","fd329d0e":"markdown","0f26b5c7":"markdown","42025b6c":"markdown","690cfb3e":"markdown","c22386ac":"markdown","86a12ebc":"markdown","cdbbfa11":"markdown","a2884c31":"markdown","f6de450c":"markdown","fdfef859":"markdown","81e3d751":"markdown","8b36cadb":"markdown","a7d3a4a2":"markdown","753d6426":"markdown","b39da3c7":"markdown","fcf56622":"markdown","53f2629d":"markdown","64f28b28":"markdown","94bad591":"markdown","beb1f4ae":"markdown","ca4c3685":"markdown"},"source":{"e053289f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","493f053c":"import os\nimport string\nimport pickle\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport nltk\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nstop_words = set(nltk.corpus.stopwords.words('english')) \n\nsns.set()","77a6f63b":"filepath_train = os.path.join('..', 'input', 'train.csv')\nfilepath_test = os.path.join('..', 'input', 'test.csv')","83538fc2":"df_train = pd.read_csv(filepath_train)\ndf_test = pd.read_csv(filepath_test)","9046c06d":"df_train.shape, df_test.shape","c5fe7d6b":"df_train.head()","5ec7a671":"df_train.info()","f3f7fd94":"# Separating the targets from the feature we will work on\nX = df_train.drop(['qid', 'target'], axis=1)\ny = df_train['target']\nX.shape, y.shape","11d31e83":"n_0 = y.value_counts()[0]\nn_1 = y.value_counts()[1]\nprint('{}% of the questions in the train set are tagged as insincere.'.format((n_1*100\/(n_1 + n_0)).round(2)))","e84c9780":"# Visualizing some insincere questions randomly chosen\n\nnp.array(X[y==1])[np.random.choice(len(np.array(X[y==1])), size=15, replace=False)]\n","5f6192ae":"# Custom function to create the meta-features we want from X and add them in a new DataFrame\n\ndef add_metafeatures(dataframe):\n    new_dataframe = dataframe.copy()\n    questions = df_train['question_text']\n    n_charac = pd.Series([len(t) for t in questions])\n    n_punctuation = pd.Series([sum([1 for x in text if x in set(string.punctuation)]) for text in questions])\n    n_upper = pd.Series([sum([1 for c in text if c.isupper()]) for text in questions])\n    new_dataframe['n_charac'] = n_charac\n    new_dataframe['n_punctuation'] = n_punctuation\n    new_dataframe['n_upper'] = n_upper\n    return new_dataframe","e0732c06":"X_meta = add_metafeatures(X)","d6da5347":"X_meta.head()","d1d21e23":"print('Number of characters description : \\n\\n {} \\n\\n Number of punctuations description : \\n\\n {} \\n\\n Number of uppercase characters description : \\n\\n {}'.format(\n    X_meta['n_charac'].describe(),\n    X_meta['n_punctuation'].describe(), \n    X_meta['n_upper'].describe()))","3cb37b08":"# Separating X_meta with our targets in y\n\nX_meta_sincere = X_meta[y==0]\nX_meta_insincere = X_meta[y==1]","8ebd5b14":"_, axes = plt.subplots(2, 3, sharey=True, figsize=(18, 8))\nsns.boxplot(x=X_meta['n_charac'], y=y, orient='h', ax=axes.flat[0]);\nsns.boxplot(x=X_meta['n_punctuation'], y=y, orient='h', ax=axes.flat[1]);\nsns.boxplot(x=X_meta['n_upper'], y=y, orient='h', ax=axes.flat[2]);\n\nX_meta_charac = X_meta[X_meta['n_charac']<400]\nX_meta_punctuation = X_meta[X_meta['n_punctuation']<10]\nX_meta_upper = X_meta[X_meta['n_upper']<15]\n\nsns.boxplot(x=X_meta_charac['n_charac'], y=y, orient='h', ax=axes.flat[3]);\nsns.boxplot(x=X_meta_punctuation['n_punctuation'], y=y, orient='h', ax=axes.flat[4]);\nsns.boxplot(x=X_meta_upper['n_upper'], y=y, orient='h', ax=axes.flat[5]);","3c08001d":"pd.concat([X_meta[X_meta['n_charac']>400], y], axis=1, join='inner')","d223e97f":"print(np.array(X_meta[X_meta['n_charac']>400]['question_text']))","2a98803c":"punctuation_ratio = 100*X_meta['n_punctuation'] \/ X_meta['n_charac']","79ecd739":"plt.figure(figsize=(18, 8))\nsns.boxplot(punctuation_ratio, y, orient='h');","f5cdfa5e":"pd.concat([X_meta[punctuation_ratio>50], y, punctuation_ratio], axis=1, join='inner')","a20d9146":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline","3e2d978e":"vectorizer = CountVectorizer(stop_words='english')\nsvd = TruncatedSVD(n_components=1, random_state=42)","fb92fefc":"preprocessing_pipe = Pipeline([('vectorizer', vectorizer), ('svd', svd)])","81e53218":"# Building the latent semantic analysis dataframe for sincere and insincere questions\n\nlsa_insincere = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\ntopics_insincere = pd.DataFrame(svd.components_)\ntopics_insincere.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n\nlsa_sincere = preprocessing_pipe.fit_transform(X[y==0]['question_text'])\ntopics_sincere = pd.DataFrame(svd.components_)\ntopics_sincere.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n\ntopics_insincere.shape, topics_sincere.shape","7ea69853":"fig, axes = plt.subplots(1, 2, figsize=(22,10));\n\ntopics_sincere.iloc[0].sort_values(ascending=False)[:30].sort_values().plot.barh(ax=axes[0]);\ntopics_insincere.iloc[0].sort_values(ascending=False)[:30].sort_values().plot.barh(ax=axes[1]);","e0514a70":"vectorizer = CountVectorizer(stop_words='english')\nsvd = TruncatedSVD(n_components=2, random_state=42)\n\npreprocessing_pipe = Pipeline([('vectorizer', vectorizer), ('svd', svd)])\n\n# Building the latent semantic analysis dataframe for sincere and insincere questions\n\nlsa_insincere_2 = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\ntopics_insincere_2 = pd.DataFrame(svd.components_)\ntopics_insincere_2.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n\nlsa_sincere_2 = preprocessing_pipe.fit_transform(X[y==0]['question_text'])\ntopics_sincere_2 = pd.DataFrame(svd.components_)\ntopics_sincere_2.columns = preprocessing_pipe.named_steps['vectorizer'].get_feature_names()\n\n\nfig_1, axes_1 = plt.subplots(1, 2, figsize=(18, 8))\nfor i, ax in enumerate(axes_1.flat):\n    topics_insincere_2.iloc[i].sort_values(ascending=False)[:30].sort_values().plot.barh(ax=ax)\n    \nfig_2, axes_2 = plt.subplots(1, 2, figsize=(18, 8))\nfor i, ax in enumerate(axes_2.flat):\n    topics_sincere_2.iloc[i].sort_values(ascending=False)[:30].sort_values().plot.barh(ax=ax)","a55307e0":"vectorizer_22 = CountVectorizer(stop_words='english', ngram_range=(2, 2))\nsvd_10c = TruncatedSVD(n_components=9, random_state=42)\n\npreprocessing_pipe = Pipeline([('vectorizer_22', vectorizer_22), ('svd_10c', svd_10c)])\n\n# Building the latent semantic analysis dataframe for insincere questions\n\nlsa_insincere_10c = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\ntopics_insincere_10c = pd.DataFrame(svd_10c.components_)\ntopics_insincere_10c.columns = preprocessing_pipe.named_steps['vectorizer_22'].get_feature_names()\n","640f874d":"fig, axes = plt.subplots(3, 3, figsize=(20, 12))\nfor i, ax in enumerate(axes.flat):\n    topics_insincere_10c.iloc[i].sort_values(ascending=False)[:10].sort_values().plot.barh(ax=ax)","43ed4acc":"vectorizer_23 = TfidfVectorizer(stop_words='english', ngram_range=(2, 3))\nsvd_9c = TruncatedSVD(n_components=9, random_state=42)\n\npreprocessing_pipe = Pipeline([('vectorizer_23', vectorizer_23), ('svd_9c', svd_9c)])\n\n# Building the latent semantic analysis dataframe for insincere questions\n\nlsa_insincere_9c = preprocessing_pipe.fit_transform(X[y==1]['question_text'])\ntopics_insincere_9c = pd.DataFrame(svd_9c.components_)\ntopics_insincere_9c.columns = preprocessing_pipe.named_steps['vectorizer_23'].get_feature_names()\n\nfig, axes = plt.subplots(3, 3, figsize=(20, 12))\nfor i, ax in enumerate(axes.flat):\n    topics_insincere_9c.iloc[i].sort_values(ascending=False)[:10].sort_values().plot.barh(ax=ax)","6b0922fb":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])","f64fc5b1":"#Custom function to preprocess the questions\n\ndef preprocess(X):\n    docs = nlp.pipe(X)\n    lemmas_as_string = []\n    for doc in docs:\n        doc_of_lemmas = []\n        for t in doc:\n            if t.text.lower() not in stop_words and t.text.isalpha() == True:\n                if t.lemma_ !='-PRON-':\n                    doc_of_lemmas.append(t.lemma_)\n                else:\n                    doc_of_lemmas.append(t.text)\n        lemmas_as_string.append(' '.join(doc_of_lemmas))\n    return pd.DataFrame(lemmas_as_string)","4dd52dbe":"%%time\nX_prep = preprocess(X['question_text'])\nX_prep.to_pickle('X_preprocessed.pkl')","3fa41696":"X_prep = pd.read_pickle('X_preprocessed.pkl')\nX_prep.columns = ['question_text']\nX_prep.head()","56932f67":"vectorizer_23 = TfidfVectorizer(stop_words='english', ngram_range=(2, 3))\nsvd_9c = TruncatedSVD(n_components=9, random_state=42)\n\npreprocessing_pipe = Pipeline([('vectorizer_23', vectorizer_23), ('svd_9c', svd_9c)])\n\n# Building the latent semantic analysis dataframe for insincere questions\n\nlsa_insincere_9c = preprocessing_pipe.fit_transform(X_prep[y==1]['question_text'])\ntopics_insincere_9c = pd.DataFrame(svd_9c.components_)\ntopics_insincere_9c.columns = preprocessing_pipe.named_steps['vectorizer_23'].get_feature_names()\n\nfig, axes = plt.subplots(3, 3, figsize=(20, 12))\nfor i, ax in enumerate(axes.flat):\n    topics_insincere_9c.iloc[i].sort_values(ascending=False)[:10].sort_values().plot.barh(ax=ax)","4f84cf1f":"df_train_qid = df_train.copy()","2d0665a2":"df_train_qid['qid_base_ten'] = df_train_qid['qid'].apply(lambda x : int(x, 16))","e7eb02e4":"df_train_qid.head()","23479f1a":"min_qid = df_train_qid['qid_base_ten'].min()\nmax_qid = df_train_qid['qid_base_ten'].max()\ndf_train_qid['qid_base_ten_normalized'] = df_train_qid['qid_base_ten'].apply(lambda x : (x - min_qid)\/min_qid)","4a3515a5":"plt.figure(figsize=(18, 8));\nplt.scatter(x=df_train_qid['qid_base_ten_normalized'][:100], y=df_train_qid.index[:100]);\nplt.xlabel('qid_base_ten_normalized');\nplt.ylabel('Question index in df_train_qid');","cef1875d":"df_test_qid = df_test.copy()\n\ndf_test_qid['qid_base_ten'] = df_test_qid['qid'].apply(lambda x : int(x, 16))\n\ndf_test_qid['qid_base_ten_normalized'] = df_test_qid['qid_base_ten'].apply(lambda x : (x - min_qid)\/min_qid)\n\nplt.figure(figsize=(18, 8));\nplt.scatter(x=df_test_qid['qid_base_ten_normalized'][:100], y=df_test_qid.index[:100]);\nplt.xlabel('qid_base_ten_normalized');\nplt.ylabel('Question index in df_test_qid');","195654b2":"df_train_qid.drop('target', axis=1, inplace=True)\ndf_train_qid['test_or_train'] = 'train'\ndf_test_qid['test_or_train'] = 'test'","2311a671":"df_qid = pd.concat([df_train_qid, df_test_qid]).sort_values('qid_base_ten_normalized').reset_index()\ndf_qid.drop('index', axis=1, inplace=True)\ndf_qid.head()","c17d82fc":"df_qid_train = df_qid[df_qid['test_or_train']=='train']\ndf_qid_test = df_qid[df_qid['test_or_train']=='test']\n\nplt.figure(figsize=(18, 8));\nplt.scatter(x=df_qid_train['qid_base_ten_normalized'], y=df_qid_train.index, label='Train');\nplt.scatter(x=df_qid_test['qid_base_ten_normalized'], y=df_qid_test.index, label='Test',s=5);\nplt.xlabel('qid_base_ten_normalized');\nplt.ylabel('Question index');\nplt.title('qid_base_ten_normalized for train and test datasets')\nplt.legend();","edb6b2ba":"plt.figure(figsize=(18, 8));\nplt.scatter(x=df_qid_train['qid_base_ten_normalized'][:1500], y=df_qid_train.index[:1500], label='Train');\nplt.scatter(x=df_qid_test['qid_base_ten_normalized'][:50], y=df_qid_test.index[:50], label='Test',s=150, marker='d');\nplt.xlabel('qid_base_ten_normalized');\nplt.ylabel('Question index');\nplt.title('qid_base_ten_normalized for the first 1500 train points and 50 test points')\nplt.legend();","18f32d44":"from sklearn.model_selection import train_test_split","a35c13e1":"X_train, X_test, y_train, y_test = train_test_split(X['question_text'], y, test_size=.2, random_state=42, stratify=y)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","188f1944":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score","b2239fea":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline","64478c8d":"tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\nlr = LogisticRegression()","ebc0c645":"pipe_baseline = Pipeline([('tfidf', tfidf), ('lr', lr)])","6e1e1f0a":"pipe_baseline.fit(X_train, y_train)","e159d75f":"y_pred = pipe_baseline.predict(X_test)","90aa16f5":"cm = confusion_matrix(y_test, y_pred)\n\nax = plt.gca()\nsns.heatmap(cm, cmap='Blues', cbar=False, annot=True, xticklabels=y_test.unique(), yticklabels=y_test.unique(), ax=ax);\nax.set_xlabel('y_pred');\nax.set_ylabel('y_true');\nax.set_title('Confusion Matrix');","995597d8":"cr = classification_report(y_test, y_pred)\nprint(cr)","482c5c3d":"y_prob = pipe_baseline.predict_proba(X_test)","22c6f8e2":"best_threshold = 0\nf1=0\nfor i in np.arange(.1, .51, 0.01):\n    y_pred = [1 if proba>i else 0 for proba in y_prob[:, 1]]\n    f1score = f1_score(y_pred, y_test)\n    if f1score>f1:\n        best_threshold = i\n        f1=f1score\n        \ny_pred = [1 if proba>best_threshold else 0 for proba in y_prob[:, 1]]\nf1 = f1_score(y_pred, y_test)\nprint('The best threshold is {}, with an f1_score of {}'.format(best_threshold, f1))","bce75972":"## Importing utils from sklearn","9a1bfbbf":"First I create the filepaths to the csv files, load the dataframes and check if everything is ok.**","b40952d5":"Let's visualize our meta-features!","fe52b1bb":"![ReadyURL](https:\/\/media.giphy.com\/media\/12WPxqBJAwOuIM\/giphy.gif \"AreYouReady\")","17b6b43e":"After having preprocessed X one time, I saved the result as a pickle file and saved it, to avoid having to wait 15min each time I run this notebook. I therefore commented the cell code above and created the one below to load X preprocessed from the pickle file.","f1b58a6b":"### Insincere questions topic modeling with bi-grams","6c21b5a4":"Now let's use our topic modeling code on this preprocessed `DataFrame` !","80cc2a1f":"We can see emerging topics, about Donald Trump or racism for instance. Let's do the same with bi-grams and tri-grams to see what happens.","0e2b360f":"## First insights","bbed419d":"As we can see, 3 of these questions are about math problems, and one about Star Trek.","a16278fb":"## Splitting into train and test with `sklearn.model_selection.train_test_split`","5c965fd0":"I'll use spacy to preprocess the questions.","67fd410e":"I was wondering wether the test dataset was an extract from the train one, or completely different. To answer this existential question, I've been working on the 'qid' column.\n\nThe question id is an hexadecimal number. The first step here is to extract this value, and see if the questions are ordered by id.","18710168":"Here I will also use a `CountVectorizer` and a `TruncatedSVD` with 9 components to identify the nine main topics of insincere questions, but with the parameter ngram_range set at (2, 2)  for the `CountVectorizer`","6defbb82":"![LetsgoURL](https:\/\/media.giphy.com\/media\/3o7TKUM3IgJBX2as9O\/giphy.gif \"LetsGo\")","cd3183fd":"We can suppose that the math question classified as sincere might have been missclassified. \nMayber other questions have also been missclassified, leading to inaccuracy for our models.","841ced5c":"![SpockUrl](https:\/\/media.giphy.com\/media\/vp122eOzO0Hxm\/giphy.gif \"fascinating\")","d2b73cfc":"# Exploratory Data Analysis","047a5de5":"![MoveURL](https:\/\/media.giphy.com\/media\/l0MYsTuL1N15t4FiM\/giphy.gif \"MoveAlong\")","d56bd21a":"Here, we still can't figure out if the train\/test plit has been done in a stratified way!\n\nThat's all for this bonus part on questions id, it is not that useful, but it was working on it was fun!\n\n![FolksURL](https:\/\/media.giphy.com\/media\/upg0i1m4DLe5q\/giphy.gif \"Folks\")","0e99415c":"![ExploURL](https:\/\/media.giphy.com\/media\/VEakclh6bmV4A\/giphy.gif \"Exploration\")\n\nThe train dataframe is loaded, we can start our exploratory data analysis!","0cd15bfb":"## A little bit of topic modeling","9c3fd71f":"Ok that's a beginning! Lets work with predict_proba to find the best threshold that optimizes our f1_score for insincere questions (target = 1)","c5aaef39":"Let's create a first, simple model, that will be my baseline model. I have to chosen to use a TfidFVectorizer and a LogisticRegression on my raw data (ie: no preprocessing)","cfa3d385":"Here we'll try to find wich topics appear more often in sincere and insincere questions. To do so, I'll use a `CountVectorizer` and a `TruncatedSVD`in a pipeline (yes the pipeline is only here to show off).","b0e0556f":"![SocietyURL](https:\/\/media.giphy.com\/media\/10E3mQGzAxWFZm\/giphy.gif \"Society\")","21ed81c4":"We can see that the distribution here is slightly the same for sincere and insincere questions, so it will not be very usefull to keep this ratio as a feature.  Nevertheless, let's again take a look at these outliers over 50.","fd329d0e":"The second line of graphs is just a zoom in the interesting parts of the grpahs on the first line\n\nWe can see there is a slight difference between the distribution of the number of characters, of punctuations and of uppercase characters for sincere and insincere questions.\n\nLets take a look at the outliers for the number of characters (ie : n_charac > 400)","0f26b5c7":"To conclude this EDA : \n\n- Insincere questions main topics are really interesting from a social point of view. Someone with societal analysis skills would probably be interested by taking a look at these questions!\n\n- Some of the insincere questions might have been missclassified, which could lead do a decreased accuracy of our ML models.\n\n- Some people posting questions on Quora really are insane!","42025b6c":"## Baseline model","690cfb3e":"So the question ids range of the test dataset is sthe same as the question ids range for the train one. The test and train datasets come as expected from a random train\/test split on a single dataset.\nThe figure below confirms the 'random' choice of the elemnts for the test dataset.","c22386ac":"As I suspected, questions are indeed sorted by ascending question id in our train dataset. Let's see if it is the same in the test one.","86a12ebc":"## Bonus : questions id in train and test datasets","cdbbfa11":"With a simple `LogisticRegression` and a `TfidfVectorizer` we already have an f1 score of 0.59!\n\nNext step is to try to improve this model, tuning the LogisticRegression C parameter for instance.\n\n![StartURL](https:\/\/media.giphy.com\/media\/3oAt1TznOzEcx3MssU\/giphy.gif \"GettingStarted\")","a2884c31":"## Working on meta-features","f6de450c":"We can see there are no missing values, which is great so far!\n\nLet's see how our targets are distributed.","fdfef859":"Based on this cleaner version of the histograms we plotted before, we can list a few main topics in insincere questions:\n\n- Donald Trump\n- Racism\n- Sex with a family member\n- People asking stupid questions on quora\n\nIt seems like we lost information, compared to the previous LSA on non-preprocessed data (less tdifferent topics in the 9 main components). I'll keep this in mind when i'll build machine learning models, to decide wether i'll train them on the raw or the preprocessed dataset.\n","81e3d751":"Here we'll try to create meta features to understand better the structure of the questions.","8b36cadb":"Over the three math questions, 1 has been classified as sincere, 2 as insincere. Lets take a closer look at the full text for these questions:","a7d3a4a2":"Maybe it is interesting to calculate the punctuation ratio:","753d6426":"Now is the most difficult part for me. As I said, i've only been learning python and machine learning for 6 weeks. So if you have already read until this point, thank you, and do not hesitate to give me advices on how I could improve my kernel! ","b39da3c7":"   # Machine Learning","fcf56622":"## Preprocessing with Spacy","53f2629d":"Again, math formulas as sincere questions, nothing to see here, go next.","64f28b28":"Here again, questions are sorted by ascending question id ! Now I wonder if I can know how Quora has made its train an test datasets. Is it with a random (and stratified?) train.test split, or a simple split based on the id?\n\nTo get the answer, I have merged the train and test dataframes, with the 'qid_base_ten_normalized' column, sorted by ascending 'qid_base_ten_normalized' and reset the index.","94bad591":"Here we have some issues due to our non-preprocessed data. Indeed, for instance, 'year old girl' and 'year old girls' are two different components for our Vectorizer.\nSo it is maybe time to preprocess ou raw data to make it more explicit!","beb1f4ae":"**Hi!** \n\nI've been learning python and machine learning for 6 weeks now, and I wanted to try working on a Kaggle competition. So this is my very first kernel on Kaggle!\n\nI hope you will enjoy this kernel and find interesting highlights on this dataset from Quora.\n\nThis is still a work in progress, the next steps of the ML part will arrive soon!\n\nHere is the current architecture of this notebook :\n\n**Exploratory Data Analysis:**\n\n    - First insights\n    - Working on meta-features\n    - A little bit of topic modeling\n    - Insincere questions topic modeling with bi-grams\n    - Preprocessing with Spacy\n    - BONUS : questions id in train and test datasets\n\n**Machine Learning**\n\n    - Baseline model\n    ","ca4c3685":"Soooo here we are. Some words are obviously more common in insincere questions, like 'white' and 'black', but other words of importance in our LSA are shared by both sincere and insincere questions at the same level, like 'people'. Maybe working on bi-grams or tri-grams will help us define more precislely what an insincere question looks like.\nBut before this, I wanted to try a Truncated SVD with 2 components, to see if I'll be able to link these two components to the results of my bi-grams study later."}}