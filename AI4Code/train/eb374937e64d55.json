{"cell_type":{"de8d10fd":"code","e78fa5cd":"code","a08ed840":"code","3fe488e0":"code","39ab8906":"code","82be9d32":"code","a494b672":"code","e490bcc2":"code","7da45566":"code","78517805":"code","d3a4ff79":"code","7d554cee":"code","aedd9b89":"code","67957143":"code","15a37b35":"code","19762806":"code","c3b2b10c":"code","6efe2df5":"code","57b1c4af":"code","4bdda202":"code","702c6d89":"code","e148cc73":"code","86dcf5d6":"code","610bf85b":"markdown","5511f090":"markdown","36e82337":"markdown","f202d70f":"markdown","1bd19225":"markdown","9ca521e3":"markdown","6f0cdc7a":"markdown","f1ee2d7c":"markdown","80134429":"markdown","735e36df":"markdown","c45388e9":"markdown","7ffa87e6":"markdown","fccdf934":"markdown","1608e8c9":"markdown"},"source":{"de8d10fd":"import pandas as pd, numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option(\"display.max_columns\", 300)\npd.set_option(\"display.max_rows\", 300)","e78fa5cd":"df=pd.read_csv('..\/input\/adult-for-class-imbalance-data\/adult.csv')","a08ed840":"df.head()","3fe488e0":"df=df.replace('?', np.nan)","39ab8906":"df.head()","82be9d32":"df.dropna(inplace=True)","a494b672":"df.head()","e490bcc2":"df['income'].value_counts()","7da45566":"df.info()","78517805":"from sklearn.preprocessing import LabelEncoder\n\nLabelenc_workclass = LabelEncoder()\ndf['workclass'] = Labelenc_workclass.fit_transform(df['workclass'])\n\nLabelenc_education = LabelEncoder()\ndf['education'] = Labelenc_education.fit_transform(df['education'])\n\nLabelenc_marital_status = LabelEncoder()\ndf['marital-status'] = Labelenc_marital_status.fit_transform(df['marital-status'])\n\nLabelenc_occupation = LabelEncoder()\ndf['occupation'] = Labelenc_occupation.fit_transform(df['occupation'])\n\nLabelenc_relationship = LabelEncoder()\ndf['relationship'] = Labelenc_relationship.fit_transform(df['relationship'])\n\nLabelenc_race = LabelEncoder()\ndf['race'] = Labelenc_race.fit_transform(df['race'])\n\nLabelenc_gender = LabelEncoder()\ndf['gender'] = Labelenc_gender.fit_transform(df['gender'])\n\nLabelenc_native_country = LabelEncoder()\ndf['native-country'] = Labelenc_native_country.fit_transform(df['native-country'])\n\nLabelenc_income = LabelEncoder()\ndf['income'] = Labelenc_income.fit_transform(df['income'])\n","d3a4ff79":"df.head()","7d554cee":"df['income'].value_counts()","aedd9b89":"X = df.drop(['income'],axis=1)\ny = df['income']","67957143":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=123)","15a37b35":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","19762806":"clf_random = RandomForestClassifier(random_state=0)\nclf_random.fit(X_train,y_train)","c3b2b10c":"y_pred=clf_random.predict(X_test)\nprint(classification_report(y_test, y_pred))","6efe2df5":"cm = confusion_matrix(y_test, y_pred) \ncm_df = pd.DataFrame(cm,\\\n                     index = ['<=50K', '>50K'], \\\n                     columns = ['<=50K', '>50K'])\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_df, annot=True,fmt='g',cmap='Greys_r')\nplt.title('Random Forest \\nAccuracy:{0:.3f}'\\\n .format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True Values')\nplt.xlabel('Predicted Values')\nplt.show()","57b1c4af":"clf_random = RandomForestClassifier(n_estimators=20, max_depth=None, min_samples_split=7, random_state=0, \n                                    class_weight='balanced')\nclf_random.fit(X_train,y_train)\ny_pred=clf_random.predict(X_test)\nprint(classification_report(y_test, y_pred))","4bdda202":"import imblearn\nfrom imblearn.over_sampling import SMOTE","702c6d89":"X_resampled, y_resampled = SMOTE().fit_resample(X_train,y_train)\n\n## Resampled dataset can then be used to train models","e148cc73":"clf_random.fit(X_resampled,y_resampled)\ny_pred=clf_random.predict(X_test)\nprint(classification_report(y_test, y_pred))","86dcf5d6":"cm = confusion_matrix(y_test, y_pred) \ncm_df = pd.DataFrame(cm,\\\n                     index = ['<=50K', '>50K'],\\\n                     columns = ['<=50K', '>50K'])\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_df, annot=True,fmt='g',cmap='Greys_r')\nplt.title('Random Forest \\nAccuracy:{0:.3f}'\\\n .format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True Values')\nplt.xlabel('Predicted Values')\nplt.show()","610bf85b":"The number of people earning more than 50K is 11208 and less than or equal to 34014.\nHowever there is a big issue with dataset. Around 74% of the dataset has people earning less than 50,000 USD; hence, it is a highly imbalanced dataset.","5511f090":"With the above report, infer that the model was able to classify class 0 (less than or equal \nto 50,000) with 88% precision whereas class 1 (greater than 50,000) had a \nprecision of 74%.\nclass 1 (greater than 50,000) has a lower score in terms of both precision and \nrecall. This can be attributed to the fact that the dataset was highly imbalanced, \nand this has led to the poor performance of the model.","36e82337":"## Fit a RandomForestClassifer","f202d70f":"without using class imbalance (SMOTE Technique), your classifier was able \nto identify only 1401 people who were earning more than 50,000 USD, whereas by \nusing sampling techniques (SMOTE), the classifier identified 1584 people who were \nearning more than 50,000 USD.\n","1bd19225":"## Using Label Encoders From Scikit Learn to convert the categorical to numerical variables","9ca521e3":"### CLASS-IMBALANCED DATA\n\nOut of the four shortlisted sellers, one is a very well known company. In such a situation, there is a high chance of this company getting most of the orders as compared to the rest of the other three sellers.\nIf the online shopping company decided to divert all the customers to this seller, for a large number of customers, it would acutally end up matching their preference.\n\nAbove is a classic scenario of Class-Imbalance, since one class is dominating the rest of the classes in terms of data points. \n\nClass-Imbalance is also seen in fraud detection, anti-money laundering, spam detection, cancer detection and many other situation.\n","6f0cdc7a":"## Imblearn SMOTE Function\n\nthis should be able to see an improvement in model performance for the annual income of more than 50,000 USD class. This should \nhappen since by using SMOTE technique, the number of samples in the minority class \n(greater than 50,000) would increase, which would fix the issue of overfitting.  This in turn should increase the number of correctly classified samples for this class (greater than 50,000). we will see this information with the help of the confusion matrix.\n","f1ee2d7c":"## Split train-test data","80134429":"## Dealing with Class-Imbalanced\n\nOne way of dealing with an imbalanced dataset is to assign a penalty to every \nwrong prediction on the minority class. This can be done using the class_weight\nparameter available in scikit-learn, which assigns a penalty for every wrong prediction \nof the minority class. As an example, let's see how to use this class_weight\nparameter in RandomForestClassifier:","735e36df":"There are other strategies to deal with imbalanced data as well. Some of them are \nas follows:\n\n\u2022 Random undersampling: In the case of random undersampling, the majority \nclass samples are randomly eliminated to maintain class balance. The advantage \nof using this method is that it reduces the number of training samples, and \nhence the training time decreases; however, it may lead to underfitted models.\n\n\u2022 Random oversampling: In the case of random oversampling, the minority class \nsamples are replicated randomly to represent a higher representation of the \nminority class in the training sample. The advantage of using this method is that \nthere is no information loss; however, it may lead to overfitting of the data.\n\n\u2022 Synthetic Minority Oversampling Technique (SMOTE): This technique is used \nto mitigate the problems you faced in random oversampling. In this method, \na subset of the minority class data is taken, and a similar replica of the data \nis created, which is added to the main datasets. The advantage of using this \nmethod is that it reduces overfitting the data and does not lead to any loss of \ninformation. However, it is not very effective for high-dimensional data.","c45388e9":"Excercise:\n    \nwe will be working with an online store company to help classify their customers based on their annual income, specifically, whether it exceeds 50,000 USD or not. \n    \nIn this exercise, you will observe how imbalanced data effects the performance of a model, and why it is so important to modify your process while working on an imbalanced dataset. We will also have to drop the missing values that are stored in the dataset as \"?\" before you start using it for the model training step:\n\nNote\nDataset source: Ronny Kohavi and Barry Becker (1996). UCI Machine \nLearning Repository [https:\/\/archive.ics.uci.edu\/ml\/datasets\/adult]. Data \nMining and Visualization. Silicon Graphics","7ffa87e6":"Note: less than or equal to 50,000 is encoded as 0 and greater than 50,000 is encoded as 1 when you use the label encoder on the income column. ","fccdf934":"There are lot of categorical variables, to perform classifications, need to convert the categorical values (workclass, education, marital-status, occupation, relationshop, race, gender, native-country and  income) into numerical values.\n","1608e8c9":"From the preceding confusion matrix, we can say that the model classified 836\npeople as earning less than or equal to 50,000 USD; however, they were actually \nearning more than 50,000 USD. Similarly, the model classified 490 people as \nearning more than 50,000 USD when they were actually earning less than or \nequal to 50,000 USD"}}