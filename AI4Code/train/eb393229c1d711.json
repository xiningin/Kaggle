{"cell_type":{"f79b80df":"code","76b3bbdf":"code","ecd0f4df":"code","34842002":"code","30d1c5c3":"code","86961d4b":"code","63893483":"code","a778df89":"code","23788bb9":"code","82e34e10":"code","17f67e3f":"code","21d68449":"code","5b1e2ff0":"code","a25c7be4":"code","b506476d":"code","f81b6097":"code","ba267149":"code","db8a9da1":"code","b7d882a6":"code","ff2e7d80":"code","70adbf7d":"code","4f1f2fcf":"code","a56aa99f":"code","dbe1c469":"code","e5fae9d4":"code","a463fbf5":"code","3ec6f944":"code","1c00b95e":"code","fe21f175":"code","febd61e0":"code","80b37102":"code","a1641ce4":"code","55fad85a":"code","0e4d1afb":"code","04f84ef5":"code","e500fa1a":"code","b097b9dc":"code","d0445983":"code","73280b49":"code","f35b4d8a":"code","cfb4c83e":"code","1e0d820a":"code","a8a05801":"code","a9914717":"code","03d8486d":"code","30b71be4":"code","8c869e2a":"code","9ab9ece9":"code","d5b71179":"code","5660a73c":"code","c26dd72c":"code","4ebce7f3":"code","6b96a45d":"code","969583bd":"code","cf3821a9":"code","a2692f0c":"code","981fd7ed":"code","ee9dc618":"code","60280a69":"code","e74c9aec":"code","b6aeb76a":"code","0e19ae25":"code","d5ea82cd":"code","beb7cbdb":"code","de3c9054":"code","73f49092":"code","a1de8d13":"code","a2b0612b":"code","747d70fd":"code","7c2d99af":"code","0fe4879b":"code","319e6b09":"markdown","6d66f26c":"markdown","8bd0e609":"markdown","87968215":"markdown","444bf27c":"markdown","93e9e3c0":"markdown","aef7a887":"markdown","1d32a375":"markdown","4b6755ac":"markdown","05aaf1e0":"markdown","6bc7eb15":"markdown","afbd7979":"markdown","e2f9cc9d":"markdown","3ef6e7c4":"markdown","7fdaa686":"markdown","c145a713":"markdown","205f98ca":"markdown","a72e9dec":"markdown","917128d2":"markdown","4ff36ba6":"markdown","33fc73be":"markdown","cc36039b":"markdown","3e5fa247":"markdown","4a694cd9":"markdown","6c7722fe":"markdown"},"source":{"f79b80df":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","76b3bbdf":"train = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/train.csv\")\n","ecd0f4df":"train.head()","34842002":"label = train['income']\n\ndel train['income']\n\ntest = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/test.csv\")","30d1c5c3":"label.head()","86961d4b":"# \ub77c\ubca8 \uac12 \uc778\ucf54\ub529\nlabel = label.map(lambda x: 1 if x == '>50K' else 0)","63893483":"label.head()","a778df89":"del train['id']\ndel test['id']","23788bb9":"tmp_train = train.copy()\ntmp_test  = test.copy()","82e34e10":"tmp_train.head()","17f67e3f":"tmp_train.info()","21d68449":"tmp_train.describe()","5b1e2ff0":"tmp_test.head()","a25c7be4":"has_na_columns = ['workclass', 'occupation', 'native_country']","b506476d":"(tmp_train[has_na_columns]=='?').sum()","f81b6097":"for c in has_na_columns:\n    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n    tmp_test.loc[test[c]   == '?', c] = test[c].mode()[0]","ba267149":"(tmp_train[has_na_columns]=='?').sum()","db8a9da1":"tmp_train['capital_gain'].plot.hist()","b7d882a6":"tmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_gain']  = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_gain'].plot.hist()\n","ff2e7d80":"train['capital_loss'].plot.hist()","70adbf7d":"tmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_loss'].plot.hist()","4f1f2fcf":"tmp_train = tmp_train.drop(columns=['capital_loss', 'capital_gain'])\ntmp_test  = tmp_test.drop(columns=['capital_loss', 'capital_gain'])","a56aa99f":"from sklearn.model_selection import train_test_split\n\ntmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label, \n                                                          test_size=0.3,\n                                                          random_state=2020,\n                                                          shuffle=True,\n                                                          stratify=label)","dbe1c469":"tmp_train.head()","e5fae9d4":"# \uc778\ub371\uc2a4 \ucd08\uae30\ud654\ntmp_train = tmp_train.reset_index(drop=True)\ntmp_valid = tmp_valid.reset_index(drop=True)\ntmp_test  = tmp_test.reset_index(drop=True)","a463fbf5":"tmp_train.reset_index?","3ec6f944":"tmp_train.head()","1c00b95e":"tmp_train.dtypes.index, tmp_train.dtypes[1]","fe21f175":"cat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] \nnum_columns = [c for c in tmp_train.dtypes.index if c not in cat_columns]\n\nprint('\ubc94\uc8fc\ud615 \ubcc0\uc218: \\n{}\\n\\n \uc218\uce58\ud615 \ubcc0\uc218: \\n{}\\n'.format(cat_columns, num_columns))","febd61e0":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\ntmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\ntmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])","80b37102":"tmp_train.describe()","a1641ce4":"tmp_valid.describe()","55fad85a":"tmp_test.describe()","0e4d1afb":"from sklearn.preprocessing import OneHotEncoder\n\ntmp_all = pd.concat([tmp_train, tmp_valid, tmp_test])\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(tmp_all[cat_columns])","04f84ef5":"ohe.categories_","e500fa1a":"ohe_columns = list()\nfor lst in ohe.categories_:\n    ohe_columns += lst.tolist()","b097b9dc":"ohe_columns","d0445983":"new_train_cat = pd.DataFrame(ohe.transform(tmp_train[cat_columns]), columns=ohe_columns)\nnew_valid_cat = pd.DataFrame(ohe.transform(tmp_valid[cat_columns]), columns=ohe_columns)\nnew_test_cat  = pd.DataFrame(ohe.transform(tmp_test[cat_columns]), columns=ohe_columns)","73280b49":"new_train_cat.head()","f35b4d8a":"cat_columns","cfb4c83e":"tmp_train.shape","1e0d820a":"tmp_train = pd.concat([tmp_train, new_train_cat], axis=1)\ntmp_valid = pd.concat([tmp_valid, new_valid_cat], axis=1)\ntmp_test = pd.concat([tmp_test, new_test_cat], axis=1)\n\n# \uae30\uc874 \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc81c\uac70\ntmp_train = tmp_train.drop(columns=cat_columns)\ntmp_valid = tmp_valid.drop(columns=cat_columns)\ntmp_test = tmp_test.drop(columns=cat_columns)","a8a05801":"tmp_train.head()","a9914717":"tmp_y_train = y_train\ntmp_y_valid = y_valid","03d8486d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import f1_score","30b71be4":"lr = LogisticRegression()\n\nlr.fit(tmp_train, tmp_y_train)\n\ny_pred = lr.predict(tmp_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","8c869e2a":"SVC?","9ab9ece9":"svc = SVC()\n\nsvc.fit(tmp_train, tmp_y_train)\n\ny_pred = svc.predict(tmp_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","d5b71179":"rf = RandomForestClassifier()\n\nrf.fit(tmp_train, tmp_y_train)\n\ny_pred = rf.predict(tmp_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","5660a73c":"xgb = XGBClassifier(tree_method='gpu_hist')\n\nxgb.fit(tmp_train, tmp_y_train)\n\ny_pred = xgb.predict(tmp_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","c26dd72c":"lgb.fit?","4ebce7f3":"lgb = LGBMClassifier(tree_method='gpu_hist')\n\nlgb.fit(tmp_train, tmp_y_train)\n\ny_pred = lgb.predict(tmp_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","6b96a45d":"def preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    for c in has_na_columns:\n        tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n        tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n        tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n    \n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    \n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","969583bd":"def xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = (y > threshold).astype(int) \n    return 'f1',f1_score(t, y_bin, average='micro')","cf3821a9":"from sklearn.model_selection import StratifiedKFold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)","a2692f0c":"skf.split?","981fd7ed":"val_scores = list()\noof_pred = np.zeros((test.shape[0],))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5 (metrics\uc5d0'logloss'\ub4f1 \uac00\ub2a5-XGBOOST \uc0ac\uc774\ud2b8\uac80\uc0c9\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","ee9dc618":"test.shape","60280a69":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","e74c9aec":"val_scores2=list()\noof_pre=np.zeros((test.shape[0],))","b6aeb76a":"for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]","0e19ae25":"#clf = LGBMClassifier\/ LogisticRegression + 'logloss'","d5ea82cd":"val_scores = list()\n\nnew_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(4)] #\ubaa8\ub3784\uac1c\ub77c\uc11c 4\nnew_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(4)]\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    print(f\"Fold {i} Start\")\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clfs = [LogisticRegression(), \n            RandomForestClassifier(), \n            XGBClassifier(tree_method='gpu_hist'), \n            LGBMClassifier(tree_method='gpu_hist')]\n    \n    for model_idx, clf in enumerate(clfs):\n        clf.fit(x_train, y_train)\n        \n        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) \/ n_splits","beb7cbdb":"new_x_train_list","de3c9054":"new_x_test_list","73f49092":"new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None) #\uc5ec\uae30\uc11c axis=1\ud558\ub824\uace0 \uc704 for \uc5d0\uc11c \ub9e4\ud2b8\ub9ad\uc2a4\ub85c \ub9cc\ub4ec\n#new_label = np.concatenate([tmp_y_train, tmp_y_valid])\nnew_label = label\nnew_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n\nnew_train.shape, new_label.shape, new_test.shape","a1de8d13":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    # oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","a2b0612b":"submit = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv\")","747d70fd":"submit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)","7c2d99af":"submit.head()","0fe4879b":"submit.to_csv('submission.csv', index=False)","319e6b09":"#### 1) CSV \ud30c\uc77c \ubd88\ub7ec\uc624\uae30","6d66f26c":"#### 4) XGBoost","8bd0e609":"### 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac","87968215":"#### 2) \ub370\uc774\ud130 \ud655\uc778\n.head(), .describe(), .info() \ub4f1\uc758 \ud568\uc218\ub85c \ub370\uc774\ud130\ub97c \ud655\uc778\ud569\ub2c8\ub2e4. ","444bf27c":"### 2. Scikit-Learn \ubd84\ub958 \ubaa8\ub378 \uc0ac\uc6a9\ud574\ubcf4\uae30\nScikit-Learn\uc758 \uae30\ubcf8 \ubd84\ub958 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\uac01 \ubaa8\ub378\uc758 \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc740 \ub300\ud68c \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc778 f1_score\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","93e9e3c0":"#### 6) \uc2a4\ucf00\uc77c\ub9c1\nScikit-learn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \uc788\ub294 Standard Scaler\ub97c \uc0ac\uc6a9\ud574\uc11c \uc218\uce58\ud615 \ubcc0\uc218\ub4e4\uc758 \ud45c\uc900\ud654\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.","aef7a887":"#### 3) \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8","1d32a375":"\uc778\ub125\uc2a4 \uc90c","4b6755ac":"\uc8fc\uc758:train\uc5d4 \uc788\ub294\ub370  valid \uc5d0\ub294 \uc5c6\ub294 \ubc94\uc8fc\uac00 \uc788\uc744\uc218 \uc788\uc74c","05aaf1e0":"#### 1) \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378","6bc7eb15":"#### 5) \ub370\uc774\ud130 \ucabc\uac1c\uae30\n##### 1. Train, Valid, Test Set\n* Train Data : \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294\ub370 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \uc54c\uace0 \uc788\ub294 \ud559\uc2b5\ud560 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Valid Data : \ud559\uc2b5\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \ud559\uc2b5\ud558\uc9c0 \uc54a\uc744 \ub370\uc774\ud130, \ubaa8\ub378 \uac80\uc99d\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Test Data : \ud559\uc2b5\ud55c \ubaa8\ub378\ub85c \uc608\uce21\ud560 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uc608\uce21\ud560 \ub370\uc774\ud130, \ubbf8\ub798 \ub370\uc774\ud130)","afbd7979":"#### 3) \uacb0\uce21\uce58 \ucc98\ub9ac\n\uc774\uc804 \ud0dc\uc9c4\ub2d8 \uac15\uc758\uc5d0\uc11c 'workclass', 'occupation', 'native_country' \uceec\ub7fc\uc5d0 \uacb0\uce21\uce58\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. <br>\n\uc77c\ubc18\uc801\uc778 \uacb0\uce21\uce58\uc640 \ub2e4\ub974\uac8c '?'\ub85c \ud45c\ud604\ub418\uc5b4\uc788\ub294 \uac12\ub4e4\uc740 \ud574\ub2f9 \uceec\ub7fc\uc758 \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\n##### \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uacbd\uc6b0 \uac00\uc7a5 \uac04\ub2e8\ud558\uac8c \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc9c0\ub9cc, \ub2e4\ub978 \uceec\ub7fc\uc744 \ud544\ud130\ub9c1\ud574\uc11c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ex) education_num \ub4f1","e2f9cc9d":"#### 4) Log \ubcc0\ud658\ncapital_gain \ubcc0\uc218\uc640 capital_loss \ubcc0\uc218\uc758 \ubd84\ud3ec\uac00 \ud55c\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \ud615\ud0dc\uc774\ubbc0\ub85c Log \ubcc0\ud658\uc744 \ud1b5\ud574 \ubd84\ud3ec\uc758 \ud615\ud0dc\ub97c \uc870\uc815\ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.","3ef6e7c4":"#### 6) \uc778\ucf54\ub529\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc218\uce58\ud615 \ubcc0\uc218\ub85c \uc778\ucf54\ub529 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubc94\uc8fc\ud615 \ubcc0\uc218\uc5d0\ub294 Onehot Encoding\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.","7fdaa686":"value: numpy","c145a713":"#### 2) 2 Stage Meta Model \ud559\uc2b5\nnew_train, new_test\uc5d0 \ub4e4\uc5b4\uc788\ub294 \ubcc0\uc218\ub294 \ubaa8\ub450 \uc218\uce58\ud615 \ubcc0\uc218\uc774\ubbc0\ub85c Standard Scaling\ub9cc \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.<br>\n\uc0c8\ub85c \uc0dd\uc131\ud55c \ub370\uc774\ud130 new_train, new_test \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 2 Stage Meta Model\uc744 \ud559\uc2b5\ud558\uace0 \uacb0\uacfc\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.","205f98ca":"# ### 3. k-Fold Cross Validation\n\uba3c\uc800 1. \uc5d0\uc11c \uc815\ub9ac\ud55c \uc804\ucc98\ub9ac \ud504\ub85c\uc138\uc2a4\ub97c \ud558\ub098\uc758 \ud568\uc218\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.","a72e9dec":"### 4. OOF(Out-Of-Fold) \uc559\uc0c1\ube14\nk-Fold\ub97c \ud65c\uc6a9\ud574\uc11c \ubaa8\ub378 \uac80\uc99d \ubc0f \uac01 \ud3f4\ub4dc\uc758 \uacb0\uacfc\ub97c \uc559\uc0c1\ube14\ud558\ub294 OOF \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4.","917128d2":"### 5. Stacking \uc559\uc0c1\ube14\n2 stage \uc559\uc0c1\ube14\uc778 Stacking \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4. Stacking \uc559\uc0c1\ube14\uc740 \uc218\uc2ed\uac1c\uc758 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubaa8\uc544 2 stage \ubaa8\ub378\ub85c \ud559\uc2b5 \ud6c4 \uacb0\uacfc\ub97c \ub0b4\ub294 \uc559\uc0c1\ube14 \ubc29\uc2dd\uc785\ub2c8\ub2e4.\n\n#### 1) 1 stage \uacb0\uacfc \ubaa8\uc73c\uae30\nStacking \uc559\uc0c1\ube14\uc744 \uc9c4\ud589\ud560 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc(train, test)\ub97c \ubaa8\uc74d\ub2c8\ub2e4. ","4ff36ba6":"\uc313\uc740 \ubaa8\ub378\uc774 \uc801\uc5b4\uc11c \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\uc73c\ub2c8 OOF \uc559\uc0c1\ube14\ub85c \uc608\uce21\ud55c \uac12\uc744 \uacb0\uacfc\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ","33fc73be":"#### 5) LightGBM","cc36039b":"### 6. \uacb0\uacfc \ub9cc\ub4e4\uae30","3e5fa247":"#### 2) \uc11c\ud3ec\ud2b8 \ubca1\ud130 \uba38\uc2e0(rbf \ucee4\ub110)","4a694cd9":"ID \uceec\ub7fc\uc740 \ud589\uc758 \uc2dd\ubcc4\uc790\ub85c \ud544\uc694 \uc5c6\ub294 \uceec\ub7fc\uc774\ubbc0\ub85c \uc0ad\uc81c\ud558\uaca0\uc2b5\ub2c8\ub2e4. ","6c7722fe":"##### \ub370\uc774\ud130 \ucabc\uac1c\uae30, Train -> (Train, Valid)\n- train_test_split \ud30c\ub77c\ubbf8\ud130 \n    - test_size  (float): Valid(test)\uc758 \ud06c\uae30\uc758 \ube44\uc728\uc744 \uc9c0\uc815\n    - random_state (int): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \ub0b4\ubd80\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ub09c\uc218 \uac12 (\ud574\ub2f9 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub9e4\ubc88 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4.)\n    - shuffle     (bool): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \uc11e\uc744\uc9c0 \uc720\ubb34\n    - stratify   (array): Stratify\ub780, \ucabc\uac1c\uae30 \uc774\uc804\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc744 \ucabc\uac1c\uace0 \ub098\uc11c\ub3c4 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc124\uc815\ud574\uc57c\ud558\ub294 \uac12\uc785\ub2c8\ub2e4. \ud074\ub798\uc2a4 \ub77c\ubca8\uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.\n        - ex) \uc6d0\ubcf8 Train \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc774 (7:3) \uc774\uc5c8\ub2e4\uba74, \ucabc\uac1c\uc5b4\uc9c4 Train, Valid(test) \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\ub3c4 (7:3)\uc774 \ub429\ub2c8\ub2e4. \ub2f9\uc5f0\ud788 \ubd84\ub958 \ub370\uc774\ud130\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4."}}