{"cell_type":{"bbe385e6":"code","5709f5f5":"code","3f2156ce":"code","94abe64a":"code","5274fc99":"code","82217cd9":"code","9d8c4a59":"code","79d2cefe":"code","50e2439c":"code","6a4e1be9":"code","b88f04b4":"code","42292b34":"code","39f9c804":"code","4d02b764":"code","4dd081ef":"code","267bd624":"code","f700b2c5":"code","0d0d50c1":"code","490b4427":"code","d31ecec4":"code","fdeaff09":"code","165ab1b7":"code","ff712255":"code","23d6bc67":"code","30537958":"code","7569b0b7":"code","21acb1ca":"code","b91d64f8":"code","4e80b964":"code","63ee0193":"code","042fe5e9":"code","074d3e99":"code","8e577041":"code","4d4734a4":"code","b2ef16ec":"code","048b744d":"code","b39f8eec":"code","2128f6bc":"code","33aba1ab":"code","7b96a114":"code","c4276f07":"code","42fe7fc2":"markdown","f3508eab":"markdown","6d5f3251":"markdown","27379106":"markdown","d24c639f":"markdown","21955b40":"markdown","8cfb6b7b":"markdown","7369b377":"markdown","4f01fbd7":"markdown","a3c6f8c9":"markdown","00b8bcc6":"markdown","5e98b88b":"markdown"},"source":{"bbe385e6":"from __future__ import print_function, division\n!pip install torchsummary pywick captum \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nfrom tqdm import tqdm\nfrom torchsummary import summary\n\nplt.ion()   # interactive mode","5709f5f5":"# %%time\n# !pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ..\/input\/nvidiaapex","3f2156ce":"# from apex import amp, optimizers\n# torch.backends.cudnn.benchmark = True\n!pip install torch-lr-finder \n# -v --global-option=\"apex\"","94abe64a":"from torch import Tensor\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\nfrom typing import Type, Any, Callable, Union, List, Optional\n\n\n__all__ = ['ResNet',\n           'resnet152']\n\n\nmodel_urls = {\n    'resnet152': 'https:\/\/download.pytorch.org\/models\/resnet152-394f9c45.pth'\n}\n\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n        # Added another relu here\n        self.relu2 = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        # Modified to use relu2\n        out = self.relu2(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https:\/\/arxiv.org\/abs\/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https:\/\/ngc.nvidia.com\/catalog\/model-scripts\/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width \/ 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        \n        # Added another relu here\n        self.relu2 = nn.ReLU(inplace=True)\n        self.relu3 = nn.ReLU(inplace=True)\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        # Modified to use relu2\n        out = self.relu3(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        zero_init_residual: bool = False,\n        groups: int = 1,\n        width_per_group: int = 64,\n        replace_stride_with_dilation: Optional[List[bool]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https:\/\/arxiv.org\/abs\/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n\n    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n\ndef _resnet(\n    arch: str,\n    block: Type[Union[BasicBlock, Bottleneck]],\n    layers: List[int],\n    pretrained: bool,\n    progress: bool,\n    **kwargs: Any\n) -> ResNet:\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n    r\"\"\"ResNet-152 model from\n    `\"Deep Residual Learning for Image Recognition\" <https:\/\/arxiv.org\/pdf\/1512.03385.pdf>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)","5274fc99":"class EarlyStopping():\n    \"\"\"\n    Early stopping to stop the training when the loss does not improve after\n    certain epochs.\n    \"\"\"\n    def __init__(self, patience=5, min_delta=0):\n        \"\"\"\n        :param patience: how many epochs to wait before stopping when loss is\n               not improving\n        :param min_delta: minimum difference between new loss and old loss for\n               new loss to be considered as an improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n    def __call__(self, val_loss):\n        if self.best_loss == None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                print('INFO: Early stopping')\n                self.early_stop = True","82217cd9":"class EarlyStoppingAcc():\n    \"\"\"\n    Early stopping to stop the training when the loss does not improve after\n    certain epochs.\n    \"\"\"\n    def __init__(self, patience=5, min_delta=0):\n        \"\"\"\n        :param patience: how many epochs to wait before stopping when loss is\n               not improving\n        :param min_delta: minimum difference between new loss and old loss for\n               new loss to be considered as an improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_acc = None\n        self.early_stop = False\n    def __call__(self, val_acc):\n        if self.best_acc == None:\n            self.best_acc = val_acc\n        elif self.best_acc - val_acc < self.min_delta:\n            self.best_acc = val_acc\n        elif self.best_acc - val_acc >= self.min_delta:\n            self.counter += 1\n            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                print('INFO: Early stopping')\n                self.early_stop = True","9d8c4a59":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n    ]),\n}","79d2cefe":"%%time\nbatch_size = 32\ndata_dir = '..\/input\/dataset140000\/dataverse-split'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                             shuffle=True, num_workers=26)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\ntrain_targets = image_datasets['train'].targets\nval_targets = image_datasets['val'].targets\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nprint(class_names)","50e2439c":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","6a4e1be9":"val_loss_plt = []\ntrain_loss_plt = []\nval_acc_plt = []\ntrain_acc_plt = []\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    early_stopping_loss = EarlyStopping(patience=3)\n    early_stopping_acc = EarlyStoppingAcc(patience=5)\n    \n    for epoch in tqdm(range(num_epochs)):\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n        # print(torch.cuda.memory_stats(device)['active.all.allocated'])\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n#                         with amp.scale_loss(loss, optimizer) as scaled_loss:\n#                             scaled_loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            print('\\n{} set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n                phase,\n                epoch_loss, epoch_acc, len(dataloaders[str(phase)]),\n                100. * epoch_acc \/ len(dataloaders[str(phase)])))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n            if phase == 'val':\n                val_loss_plt.append(epoch_loss)\n                val_acc_plt.append(epoch_acc)\n                early_stopping_loss(epoch_loss)\n                early_stopping_acc(epoch_acc)\n            else:\n                train_loss_plt.append(epoch_loss)\n                train_acc_plt.append(epoch_acc)\n            \n        if early_stopping_loss.early_stop or early_stopping_acc.early_stop:\n            break\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","b88f04b4":"def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images\/\/2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","42292b34":"model_ft = resnet152(progress=True, pretrained=True)\nnum_ftrs = model_ft.fc.in_features\n\n# for param in model_ft.parameters():\n#     param.requires_grad = False\n\nfor i, (name, param) in enumerate(model_ft.named_parameters()):\n#     print(i+1, name)\n    if (i >= 3):\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\n# for i, (name, param) in enumerate(model_ft.named_parameters()):\n#     print(i+1, name)\n#     if (i >= 435):\n#         param.requires_grad = True\n#     else:\n#         param.requires_grad = False\n\nmodel_ft.avgpool = nn.AvgPool2d(7, stride=1)\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names))\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-05)\n\n# model_ft, optimizer_ft = amp.initialize(model_ft, optimizer_ft, opt_level='O2')\n\n# optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=0.0309, momentum=0.9, weight_decay=1e-05)","39f9c804":"# %%time\n# from torch_lr_finder import LRFinder\n# lr_finder = LRFinder(model_ft, optimizer_ft, criterion, device=\"cuda\")\n# lr_finder.range_test(dataloaders['train'], val_loader=dataloaders['val'], end_lr=1, num_iter=50, step_mode=\"exp\")\n# lr_finder.plot()","4d02b764":"exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.1)","4dd081ef":"summary(model_ft, (3,224,224))","267bd624":"%%time\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)","f700b2c5":"torch.save(model_ft.state_dict(), '.\/resnet_lesions_pytorch_final.pt')","0d0d50c1":"visualize_model(model_ft)","490b4427":"def plot_graph(train, val, graph_type):\n    x_train = [x+1 for x in range(len(train))]\n    x_val = [x+1 for x in range(len(val))]\n    plt.plot(x_train, train, label='train')\n    plt.plot(x_val, val, label='val')\n    plt.xlabel('epochs')\n    plt.ylabel(graph_type)\n    plt.legend(title='Parameter where:')\n    plt.show()","d31ecec4":"plot_graph(train_loss_plt, val_loss_plt, 'loss')","fdeaff09":"plot_graph(train_acc_plt, val_acc_plt, 'accuracy')","165ab1b7":"import itertools\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.cpu().numpy().astype('float') \/ cm.cpu().numpy().sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","ff712255":"%%time\nconfusion_matrix = torch.zeros(len(class_names), len(class_names))\nwith torch.no_grad():\n    for i, (inputs, classes) in enumerate(dataloaders['train']):\n        inputs = inputs.to(device)\n        classes = classes.to(device)\n        outputs = model_ft(inputs)\n        _, preds = torch.max(outputs, 1)\n        for t, p in zip(classes.view(-1), preds.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n\nprint(confusion_matrix)","23d6bc67":"plt.figure(figsize=(10,10))\nplot_confusion_matrix(confusion_matrix, class_names, True)","30537958":"@torch.no_grad()\ndef get_all_preds(model, loader):\n    all_preds = torch.tensor([]).to(device)\n    for batch in loader:\n        images, labels = batch\n\n        preds = model(images.to(device))\n        all_preds = torch.cat(\n            (all_preds, preds)\n            ,dim=0\n        )\n    return (all_preds, labels)","7569b0b7":"# %%time\n# val_preds, val_labels = get_all_preds(model_ft.to(device), dataloaders['val'])","21acb1ca":"from sklearn.metrics import confusion_matrix, roc_curve, auc\n\ndef test_class_probabilities(model, device, test_loader, which_class):\n    model.eval()\n    actuals = []\n    probabilities = []\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            prediction = output.argmax(dim=1, keepdim=True)\n            actuals.extend(target.view_as(prediction) == which_class)\n            probabilities.extend(np.exp(output[:, which_class].cpu().numpy()))\n    return [i.item() for i in actuals], [i.item() for i in probabilities]","b91d64f8":"# list out keys and values separately\nkey_list = list(image_datasets['val'].class_to_idx.keys())\nval_list = list(image_datasets['val'].class_to_idx.values())\n\nfor i in range(7):\n    which_class = i\n    actuals, class_probabilities = test_class_probabilities(model_ft.to(device), device, dataloaders['val'], which_class)\n\n    fpr, tpr, _ = roc_curve(actuals, class_probabilities)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (AUC = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    position = val_list.index(which_class)\n    plt.title('ROC for {} class'.format(key_list[position]))\n    plt.legend(loc=\"lower right\")\n    plt.show()","4e80b964":"# for i, (name, param) in enumerate(model_ft.named_parameters()):\n#     print(i+1, name)\n#     if (i >= 435):\n#         param.requires_grad = True\n#     else:\n#         param.requires_grad = False","63ee0193":"# summary(model_ft, (3,224,224))","042fe5e9":"# val_loss_plt = []\n# train_loss_plt = []\n# val_acc_plt = []\n# train_acc_plt = []","074d3e99":"# changing optmizer and scheduler\n# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.00954, momentum=0.9, weight_decay=1e-05)\n# model_ft, optimizer_ft = amp.initialize(model_ft, optimizer_ft, opt_level='O1')","8e577041":"# lr_finder.reset()\n# lr_finder = LRFinder(model_ft, optimizer_ft, criterion, device=\"cuda\")\n# lr_finder.range_test(dataloaders['train'], val_loader=dataloaders['val'], end_lr=1, num_iter=50, step_mode=\"exp\")\n# lr_finder.plot()","4d4734a4":"# exp_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.1)","b2ef16ec":"# %%time \n# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)","048b744d":"# torch.save(model_ft.state_dict(), '.\/resnet_lesions_pytorch2_v4.pt')","b39f8eec":"# visualize_model(model_ft)","2128f6bc":"# plot_graph(train_loss_plt, val_loss_plt, 'loss')","33aba1ab":"# plot_graph(train_acc_plt, val_acc_plt, 'accuracy')","7b96a114":"# confusion_matrix = torch.zeros(len(class_names), len(class_names))\n# with torch.no_grad():\n#     for i, (inputs, classes) in enumerate(dataloaders['val']):\n#         inputs = inputs.to(device)\n#         classes = classes.to(device)\n#         outputs = model_ft(inputs)\n#         _, preds = torch.max(outputs, 1)\n#         for t, p in zip(classes.view(-1), preds.view(-1)):\n#                 confusion_matrix[t.long(), p.long()] += 1\n\n# print(confusion_matrix)","c4276f07":"# print(confusion_matrix.diag()\/confusion_matrix.sum(1))","42fe7fc2":"## Get dataset","f3508eab":"## Add EarlyStopping classe","6d5f3251":"## Confusion Matrix","27379106":"## Install Apex","d24c639f":"## Show loss and accuracy graph","21955b40":"## Confusion Matrix Per Class\n","8cfb6b7b":"## Create train model","7369b377":"## Train again with more trainable params","4f01fbd7":"## Create Confusion Matrix","a3c6f8c9":"\n## ROC Curve","00b8bcc6":"## Show loss and accuracy graph","5e98b88b":"## Add ResNet 152 model from torchvision"}}