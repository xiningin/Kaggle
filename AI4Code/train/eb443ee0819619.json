{"cell_type":{"893f0faa":"code","7e3f9a6f":"code","54a75968":"code","3a98c34f":"code","7f28d66a":"code","81faa888":"code","c8281248":"code","475b5cfb":"markdown","4cfab38e":"markdown","23abdbd2":"markdown","d5951ca9":"markdown","4c7d996e":"markdown","3f92e325":"markdown","aae6799f":"markdown","acc379ce":"markdown","d0307f75":"markdown","6223391f":"markdown","8f1ab74a":"markdown","b409d04b":"markdown","cc3636c0":"markdown","f316dc06":"markdown"},"source":{"893f0faa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e3f9a6f":"# install tesorflow bert package\n!pip install bert-for-tf2\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert\n\n#Loding pretrained bert layer\nBertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\n\n\n# Loading tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocab_file, do_lower_case)\n\nprint(\"done!\")","54a75968":"text = 'Encoding will be clear with this example'\n# tokenize\ntokens_list = tokenizer.tokenize(text)\nprint('Text after tokenization')\nprint(tokens_list)\n\n# initilize dimension\nmax_len =34\ntext = tokens_list[:max_len-2]\ninput_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\nprint(\"After adding  flasges -[CLS] and [SEP]: \")\nprint(input_sequence)\n\n\ntokens = tokenizer.convert_tokens_to_ids(input_sequence )\nprint(\"tokens to id \")\nprint(tokens)\n\npad_len = max_len -len(input_sequence)\ntokens += [0] * pad_len\nprint(\"tokens: \")\nprint(tokens)\n\nprint(pad_len)\npad_masks = [1] * len(input_sequence) + [0] * pad_len\nprint(\"Pad Masking: \")\nprint(pad_masks)\n\nsegment_ids = [0] * max_len\nprint(\"Segment Ids: \")\nprint(segment_ids)","3a98c34f":"import numpy as np\ndef bert_encode(texts, tokenizer, max_len=520):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\nMAX_LEN = 20\n\n# encode train set \ntrain_input = bert_encode([\n    'love is more powerful than hate',\n    'hope is more powerful than fear',\n    'light is more powerful than darkness',\n    'this is our moment',\n    'this is our mission',\n    'we cant take four more years of donald trump', \n    'i have always believed you can define america in one word possibilities', \n    'together, we can and will rebuild our economy', \n    'donald trump has failed to protect the american people and that is unforgivable',\n    'donald trump continues to cozy up to russia',\n    'i am grateful for your support and honored to be the nominee',\n    'character is on the ballot',\n    'compassion is on the ballot',\n    'decency science democracy',\n    'this campaign is about winning the heart and the soul of america', \n    'our president has failed in his most basic duty to this nation',\n    'you wont hear me racebaiting',\n    'you wont hear me dividing', \n    'i will ensure the rights of lgbtq people', \n    'we have to heal this nation', \n    'i am running as a proud democrat',\n    'i am going to be an american president',\n    'we are a diverse country',\n    'improving the criminal justice system',\n    'putting black americans in a position to gain generational wealth',\n    'this moment demands moral leadership',\n    'we can choose a different path one of hope unity and light', \n    'lets overcome this era of darkness together',\n    'to lead america you have to understand america', \n    'the cruelty of this president truly knows no bounds', \n    'immigrants are not your political props',\n    'president trump has failed our nation', \n    'leadership requires foresight',\n    'its time we bring integrity back to the white house',\n    'we have to beat donald trump',\n    'times of crisis often bring out our best as americans',\n    'donald trump put our nation on the sidelines',\n    'trump still does not have a plan to get this virus under control',\n    'we can choose four more years of fear division and hate', \n    'you will not pay a penny more in taxes under my administration',\n    'the importance of wearing masks and keeping a safe social distance', \n    'president obama and i left donald trump a booming economy',\n    'donald trump can lie about the economy all he wants',\n    'we must stand up to hate and intolerance', \n    'its time we ensured all people are treated equally as well',\n    'the only thing this president has done alone is fail america',\n    'no lies fear-mongering or malarkey',\n    'we run this campaign led by the values that guide my life',\n    'we must not become a country at war with ourselves',\n    'we are facing so many crises under donald trump',\n    'the simple truth is donald trump failed to protect america',\n    'donald trump looks at this violence and sees a political lifeline',\n    'he failed to protect america so now hes trying to scare america',\n    'fear never builds the future but hope does',\n    'together we are going to build back better than before',\n    'donald trump is determined to instill fear and divide us',\n    'president trump has refused to stand up to russia',\n    'this is our moment to root out systemic racism',\n    'be a patriot and wear a mask',\n    'it is time we reward work and not just wealth',\n    'we need to restore honor and decency to the white house',\n    'unlike donald trump i will listen to the experts',\n    'donald trump is incapable of providing the leadership this moment requires',\n    'we have got to flip the senate folks',\n    'nature knows',\n    'if president trump has his way in the us supreme court its unconscionable',\n    'There is only one way to end this horror vote',\n    'i made a promise to his family that i will not let him become just another hashtag',\n    'donald trump wants to destroy obamacare',\n    'vote like your health care depends on it', \n    'i will be a president for all americans',\n    'the right to vote is the most sacred american right there is',\n    'i will always tell you the truth',\n    'i will listen to the experts',\n    'you deserve a president who tells you the truth', \n    'how many more people have to suffer because of president trumps lies', \n    'do your part and wear a mask', \n    'i know we can beat trump and build this nation back better', \n    'weapons of war have no place in our communities',\n    'science knows',\n    'i believe that every american deserves a fair shot to get ahead',\n    'science will win',\n    'donald trump wants to give his rich friends another tax cut',\n    'we need a president who cares about more than the wealthy',\n    'you lost your freedom because president trump did not act',\n    'unlike president trump i will be a president for all americans',\n    'honor and decency are on the ballot this november', \n    'i will actually listen to advice and expertise not attack him for telling the truth', \n    'we all know president trump has a tendency to stray from the truth', \n    'donald trump the greatest failure of presidential leadership in our nations history', \n    'donald trump has been trying to throw out obamacare for years', \n    'we are going to make donald trump a one-term president',\n    'i am ready to fight for you and for our nation', \n    'we need a president who will unite our country', \n    'we honor the hardworking men and women who drive our economy', \n    'the longer donald trump is president the more reckless he gets', \n    'i know americans are not looking for a handout',\n    'let me be clear lgbtq rights are human rights',\n    'hope over fear',\n    'truth over lies',\n    'there is a nasty rumor out there',\n    'massive red wave coming',\n    'biden lied',\n    'polls numbers are looking very strong',\n    'big crowds great enthusiasm',\n    'thank you libertarians',\n    'corrupt politician',\n    'he will never let you down',\n    'vote trump',\n    'sleepy joe biden had a very bad showing last night',\n    'the radical left',\n    'negative biden news',\n    'the debate was rigged',\n    'the trump campaign was not treated fairly',\n    'the radical left will destroy our country',\n    'joe biden and the democrat socialists will kill your jobs',\n    'the debate was rigged',\n    'joe biden and the democrat socialists will  dismantle your police departments'\n    'there is a nasty rumor out there',\n    'massive red wave coming',\n    'biden lied',\n    'polls numbers are looking very strong',\n    'big crowds great enthusiasm',\n    'thank you libertarians',\n    'corrupt politician',\n    'he will never let you down',\n    'vote trump',\n    'sleepy joe biden had a very bad showing last night',\n    'the radical left',\n    'negative biden news',\n    'the debate was rigged',\n    'the trump campaign was not treated fairly',\n    'the radical left will destroy our country',\n    'joe biden and the democrat socialists will kill your jobs',\n    'the debate was rigged',\n    'joe biden and the democrat socialists will  dismantle your police departments',\n    'if biden wins china wins',\n    'when we win you win north carolina wins and america wins',\n    'if biden wins china will own the united states',\n    'nobody has ever done as much for iowa as i have done for iowa',\n    'think of where we would be now without fake and fraudulent stories',\n    'He has always been a corrupt politician',\n    'joe biden must immediately release all emails',\n    'there is nothing worse than a corrupt politician',\n    'this is your chance to make america great again',\n    'fight hard republicans',\n    'they have been taking advantage of the system for years',\n    'one of the most important issues for pennsylvania is the survival of your fracking industry', \n    'joe biden has repeatedly pledged to abolish fracking',\n    'with me you are going to frack',\n    'see you in court',\n    'fight hard republicans',\n    'fake news is devastated',\n    'protecting people with preexisting conditions', \n    'he has done nothing on healthcare cost or otherwise or virtually anything else',\n    'guy is a total loser',\n    'early voting is underway in the great states of georgia and texas',\n    'find out where to early vote by clicking below',\n    'volunteer to be a trump election poll watcher',\n    'rigged election',\n    'the radical left is trying hard to undermine the christopher columbus legacy',\n    'california hired a pure sleepy joe democrat firm to count and harvest votes',\n    'no way republicans get a fair shake',\n    'vote trump and watch the greatest comeback of them all',\n    'biden disrespects voters with courtpacking dodge',\n    'people want the truth',\n    'vote trump',\n    'save your 2nd amendment',\n    'vote for trump what the hell do you have to lose',\n    'nancy pelosi could not care less about the american people',\n    'taxes too high',\n    'crime too high',\n    'lockdowns too severe',\n    'i have gone through years of a fake illegal witchhunt',\n    'it was a hoax',\n    'massive corruption surrounding sleepy joe biden',\n    'protect people with preexisting conditions',\n    'make america great again',\n    'fight hard republicans',\n    'we are taking back our country',\n    'totally negative china virus reports',\n    'joe has never been a nice or kind guy',\n    'fake news',\n    'joe biden spoke fondly of this racist today',\n    'almost nobody showed up to the sleepy joe biden rally',\n    'the reporting and polls are a media con job fake news',\n    'we have far more support and enthusiasm than even in 2016',\n    'biden is coughing and hacking and playing fingers with his mask all over the place',\n    'journalism has reached the all time low in history',\n    'joe has never been a nice or kind guy',\n    'there joe goes again',\n    'we are winning',\n    'economy is starting to boom',\n    'i keep reading fake news stories that my campaign is running low on money',\n    'not true and if it were so i would put up money myself',\n    'the fact is that we have much more money than we had four years ago',\n    'china virus',\n    'i keep reading fake news stories',\n    'crooked hillary',\n    'leftwing radicals',\n    'radical left justices',\n    \n    \n\n\n    \n], tokenizer, max_len=MAX_LEN)\ntrain_labels = np.array([\n    [1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],\n    [1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],\n    [1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],\n    [1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],\n    [1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],[1,0],\n    [0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],\n    [0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],\n    [0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],\n    [0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],\n    [0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],\n   \n])\n\nprint(\"number of test samples:\", len(train_input[0]), \"labels:\", len(train_labels))\n","7f28d66a":"# first define input for token, mask and segment id  \nfrom tensorflow.keras.layers import  Input\ninput_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")\n\n#  output  \nfrom tensorflow.keras.layers import Dense\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])  \nclf_output = sequence_output[:, 0, :]\nout = Dense(2, activation='softmax')(clf_output)\n\n# intilize model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nmodel = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\nmodel.compile(Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","81faa888":"# train\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.05,\n    epochs=4,\n    batch_size=1\n)\n\nmodel.save('model.h5')\nprint(\"done and saved!\")","c8281248":"test_input = bert_encode(['donald trump is a bigot', 'this is fake news'], tokenizer, max_len= MAX_LEN )\ntest_pred = model.predict(test_input)\npreds = np.around(test_pred, 3)\npreds","475b5cfb":"# 4) Train the network","4cfab38e":"In this lecture we will use an advanced deep neural network to analyze large amounts of text.\n\nSpecifically we will use BERT by google, which was google's response to an earlier network called ELMo.\n\nThis huge deep neural network can transform text into a vector, representing the meaning of the text.\n\nIf you would want to train a network the size of BERT you will need an expensive computer with a couple of state of the art GPUs, a big chunk of text, sometimes they use all the text on the internet for example. And then you need to wait a long time.\n\nThe latest Natural Language Processing network, GPT-3, cost in total 16 million dollars to train. Just an example of why here, we just download a pretrained BERT.\nWe can then cut of the top part of BERT, and with a headless network, as they say, or here that would mean a headless BERT, we can use the high level neurons BERT uses to make a prediction, without having to train a deep neural network from scratch.\n\n![high level features](https:\/\/informatics.sydney.edu.au\/blogs\/lecun.png)\n\nIn this image you can easily visualize what is happening, showing the features learned by a deep convolutional neural network used for vision. In the first layers of the network we can see very basic features. As we go deeper into the network, features start to resemble whole objects more in a vision network. In a language processing network the same happens, but that is more difficult to visualize.\n\nDeep neural networks need to be trained on millions of images or terabytes of text to work properly. What we can do after that however, is take a pretrained network, remove the top (classifier) layers, and attach our own classifier, to the 'high-level features'. We will then get a deep network with state of the art performance, without the need to train the whole network from scratch.\n\nIn this video some different layers of BERT are visualized.\n[https:\/\/vimeo.com\/358488181](https:\/\/vimeo.com\/358488181)\n\nIf you want to know even more, here is an article explaining how BERT sees the world in more detail.\n[https:\/\/towardsdatascience.com\/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1](https:\/\/towardsdatascience.com\/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)","23abdbd2":"## 4.1) Does the network learn to detect Depression?\n\n## 4.2) One thenth of the dataset is used as a validation set. Why do you think we need a validation dataset?","d5951ca9":"## 3.1) What is the maximum number of input words in this network? Where is this defined?\n\n## 3.2) How many neurons does the output of BERT have?\n\n## 3.3) How many output neurons does our added 'dense' layer have?","4c7d996e":"Use a neural network (BERT) to analyse huge amounts of data automatically\n===","3f92e325":"# 5) Test the network","aae6799f":"# load in BERT (enable INTERNET in SETTINGS for this to work)\n### (requires phone verification)","acc379ce":"## 1.1) Why can't neural networks read words directly?\n\n## 1.2) Which neuron is activated by the word 'Encoding'? And by 'example'?\n\n## 1.3) How many neurons are activated in total?\n\n## 1.4) What is the maximum number of words that can be encoded in this example?\n\n## 1.5) Change the maximum number of encoded words to 32.","d0307f75":"# 6) Create and train your own dataset\n\n## 6.1) What is something you would like to train BERT to recognize?\n\n## 6.2) Name 5 positive examples and 5 negative examples.\n\n## 6.3) Try to find 100 or more positive and 100 or more negative examples online (more is better) and create a new labeled dataset.\n\n## 6.4) Train and test the network.","6223391f":"## 2.1) What is the network's output for depressed text? And for happy text?\n\n## 2.2) Why is this actually not a good dataset to train the network on?","8f1ab74a":"# 1) Tokanization example\n\nHere we will transform words into tokens that a Neural Network can read.\n\nA neural network cannot read words, only values of input 'neurons'.\nHere we will map words to input neurons.\n","b409d04b":"## 5.1) Does our network actually work?\n\n## 5.2) Are there examples the network is confused about?\n\n## 5.3) Does the network generalize to data it has never seen before?","cc3636c0":"# 3) Create a new small (one layer) network on top of BERT base","f316dc06":"# 2) Create a (fake) dataset"}}