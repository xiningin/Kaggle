{"cell_type":{"8c0060dd":"code","929aa567":"code","2755418f":"code","df4b0d26":"code","3739072a":"code","cf270d88":"code","48c59b1e":"code","ad944608":"code","2256ea57":"code","f0a9984b":"code","fb9afb8b":"code","591a815b":"code","c1b5ec9e":"code","f3cacfc1":"code","318b353a":"code","810cd18a":"code","ec8d9178":"code","ad91d1c9":"code","df58a363":"code","c31865e6":"code","cc8f827d":"code","089d67aa":"markdown","96dc4d7b":"markdown","e8b07294":"markdown","f02d3e11":"markdown","fc6286e3":"markdown","9add1500":"markdown","20268ec8":"markdown","dc52df97":"markdown","fbfdc795":"markdown"},"source":{"8c0060dd":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","929aa567":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2755418f":"dataset = [train, test]\nfor data in dataset:\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    ms = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms = ms[ms[\"Percent\"] > 0]\n    print(ms)\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)","df4b0d26":"## Data Manipulations\n## NaN\ndata = [train, test]\nfor dataset in data:\n    dataset[\"Age\"] = dataset[\"Age\"].fillna(dataset[\"Age\"].median())\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(0)\n    ## Binarize Male \/ Female\n    dataset[\"Sex\"].loc[dataset[\"Sex\"] == \"male\"] = 0\n    dataset[\"Sex\"].loc[dataset[\"Sex\"] == \"female\"] = 1\n    #Create 'Child' feature\n    dataset[\"IsChild\"] = 0\n    dataset[\"IsChild\"].loc[dataset[\"Age\"] < 18] = 1\n    dataset[\"IsChild\"].loc[dataset[\"Age\"] > 18] = 0\n    #Numerize 'Embarked feature'\n    dataset[\"Embarked\"].loc[dataset[\"Embarked\"] == \"S\"] = 1\n    dataset[\"Embarked\"].loc[dataset[\"Embarked\"] == \"C\"] = 2\n    dataset[\"Embarked\"].loc[dataset[\"Embarked\"] == \"Q\"] = 3\n\n    ## Create Title Class\n    titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\n    ## Create Age Group class\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","3739072a":"test['Fare'].fillna(test['Fare'].median(), inplace = True)","cf270d88":"for dataset in data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare','Average_fare','high_fare'])","48c59b1e":"train = train.drop(['Name', 'Cabin', 'Fare', 'Ticket', 'PassengerId'], axis=1)\ntest2 = test\ntest = test.drop(['Name', 'Cabin', 'Fare', 'Ticket', 'PassengerId'], axis=1)","ad944608":"train = pd.get_dummies(train, columns = [\"Fare_bin\"], prefix=[\"Fare_type\"])\ntest = pd.get_dummies(test, columns = [\"Fare_bin\"], prefix=[\"Fare_type\"])","2256ea57":"print(\"Head Train\")\ntrain.head()","f0a9984b":"print(\"Head test\")\ntest.head()","fb9afb8b":"print('check the nan value in train data')\nprint(train.isnull().sum())\nprint('___'*30)\nprint('check the nan value in test data')\nprint(test.isnull().sum())","591a815b":"sns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","c1b5ec9e":"## SPLITTING THE DATA\nfrom sklearn.model_selection import train_test_split\n\nall_X = train.drop('Survived', axis=1)\nall_y = train['Survived']\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.2,random_state=0)\nprint(\"Data correctly splitted\")","f3cacfc1":"## CHECK FOR 'NaN'\nprint('check the nan value in train data')\nprint(train.isnull().sum())\nprint('___'*30)\nprint('check the nan value in test data')\nprint(test.isnull().sum())","318b353a":"from sklearn import tree\n\ntree_clf = tree.DecisionTreeClassifier(min_samples_split=15)\nt0 = time()\nprint(\"Training...\")\ntree_clf.fit(train_X, train_y)\nprint(\"Training Completed in:\", round(time()-t0, 2), \"s\")\n\npred = (tree_clf.predict(test_X))\nacc = accuracy_score(test_y, pred)\nprint(\"ACCURACY:\", round(acc*100, 2), \"%\")","810cd18a":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=6500, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nt0 = time()\nprint(\"Training...\")\nforest_clf.fit(train_X, train_y)\nprint(\"Training completed in:\", round(time()-t0, 2), \"s\")\n\npred = (forest_clf.predict(test_X))\nacc = accuracy_score(test_y, pred)\nprint(\"ACCURACY:\", round(acc*100, 2), \"%\")\n\nprint(\"Important features\")\npd.Series(forest_clf.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\nprint('__'*30)\nprint(acc)","ec8d9178":"from sklearn.neural_network import MLPClassifier\n\nnnc_clf = MLPClassifier(\n    hidden_layer_sizes=(100, ),\n    activation='relu',\n    solver='adam',\n    alpha=0.0001,\n    batch_size='auto',\n    learning_rate='constant',\n    learning_rate_init=0.001,\n    power_t=0.5, max_iter=500,\n    shuffle=True, random_state=None,\n    tol=0.0001,\n    verbose=False,\n    warm_start=False,\n    momentum=0.9,\n    nesterovs_momentum=True,\n    early_stopping=False,\n    validation_fraction=0.1,\n    beta_1=0.9,\n    beta_2=0.999, \n    epsilon=1e-08,\n    n_iter_no_change=10)\n\nt0 = time()\nprint(\"Training...\")\nnnc_clf.fit(train_X, train_y)\nprint(\"Training completed in:\", round(time()-t0, 2), \"s\")\n\nnnc_clf.predict(test_X)\nacc = nnc_clf.score(test_X, test_y)\nprint(\"ACCURACY:\", round(acc*100, 2), \"%\")","ad91d1c9":"from sklearn.model_selection import cross_val_score\nclf = [forest_clf, nnc_clf, tree_clf]\nfor clf in clf:\n    t0 = time()\n    print(clf)\n    scores = cross_val_score(clf, all_X, all_y, cv=10)\n    print(\"Validation completed in:\", round(time()-t0, 2), \"s\")\n    print(scores)\n    print(\"MOYENNE:\", np.mean(scores)*100, \"%\")\n    print(\"---\"*30, \"\\n\")","df58a363":"from sklearn.model_selection import GridSearchCV\n# Random Forest Classifier Parameters tunning \nmodel = RandomForestClassifier()\nn_estim=range(100,1000,100)\n\n## Search grid for optimal parameters\nparam_grid = {\"n_estimators\" :n_estim}\n\n\nmodel_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodel_rf.fit(train_X,train_y)\n\n\n\n# Best score\nprint(model_rf.best_score_)\n\n#best estimator\nmodel_rf.best_estimator_","c31865e6":"selectedClf = forest_clf\n\nselectedClf.fit(all_X, all_y)\npred = selectedClf.predict(test)\npred","cc8f827d":"## Export results\nholdout_ids = test2[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": pred}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv('titanic_submission.csv', index=False)","089d67aa":"## HEATMAP","96dc4d7b":"# Neural Network Classifier","e8b07294":"# NULL FEATURE","f02d3e11":"# Random Forest","fc6286e3":"# Decision Tree","9add1500":"# Hyper Parameters tuning","20268ec8":"# Machine Learning","dc52df97":"# Final Training","fbfdc795":"# Cross Validation Score"}}