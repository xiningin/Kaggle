{"cell_type":{"90f89f3f":"code","0d7fdab7":"code","dbebb6ab":"code","4d639e59":"code","8a685b14":"code","367234b7":"code","17ee2f40":"code","e9b42d1a":"code","cd5bcc1f":"code","ae2024fb":"code","377aa0f8":"code","f2938d88":"code","f51e88f7":"code","af97e785":"code","3afac133":"code","b97a923b":"code","1a941385":"code","7aa272e7":"code","1f52ec5e":"code","b54e4250":"code","8fe0c584":"code","fffc784e":"code","4a9d3f9e":"code","8f89a6bd":"code","e7eb6d1d":"code","e14b486f":"code","d5453615":"code","89490dbe":"code","ec1e91e0":"code","499f66e5":"code","82633b7f":"code","fc55fe37":"code","747eb4c8":"code","3c1adc64":"code","81168c40":"code","2f8dd77a":"code","f6ef91cc":"code","adaa725a":"code","31fd2ea7":"markdown"},"source":{"90f89f3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\nimport warnings\nimport datetime\nimport time\nimport gc\nfrom tqdm import tqdm\nfrom scipy.stats import describe\n%matplotlib inline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0d7fdab7":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","dbebb6ab":"#Loading Train and Test Data\ndf_train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ndf_test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(df_train.shape[0],df_train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(df_test.shape[0],df_test.shape[1]))","4d639e59":"df_train.head()","8a685b14":"df_test.head()","367234b7":"df_train.target.describe()","17ee2f40":"df_train[\"month\"] = df_train[\"first_active_month\"].dt.month\ndf_test[\"month\"] = df_test[\"first_active_month\"].dt.month\ndf_train[\"year\"] = df_train[\"first_active_month\"].dt.year\ndf_test[\"year\"] = df_test[\"first_active_month\"].dt.year\ndf_train['elapsed_time'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days\ndf_test['elapsed_time'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days\ndf_train.head()","e9b42d1a":"# df_train = pd.get_dummies(df_train, columns=['feature_1', 'feature_2'])\n# df_test = pd.get_dummies(df_test, columns=['feature_1', 'feature_2'])\n# df_train.head()","cd5bcc1f":"# df_test.head()","ae2024fb":"df_hist_trans = pd.read_csv('..\/input\/historical_transactions.csv')\ndf_new_merchant_trans = pd.read_csv('..\/input\/new_merchant_transactions.csv')","377aa0f8":"# df_hist_trans.head()","f2938d88":"# df_new_merchant_trans.head()","f51e88f7":"def binarize(df):\n    for col in ['authorized_flag', 'category_1']:\n        df[col] = df[col].map({'Y':1, 'N':0})\n    return df","af97e785":"df_hist_trans = binarize(df_hist_trans)\ndf_new_merchant_trans = binarize(df_new_merchant_trans)","3afac133":"df_hist_trans['purchase_date'] = pd.to_datetime(df_hist_trans['purchase_date'])\ndf_hist_trans['month_diff'] = ((datetime.datetime.today() - df_hist_trans['purchase_date']).dt.days)\/\/30\ndf_hist_trans['month_diff'] += df_hist_trans['month_lag']\n\ndf_new_merchant_trans['purchase_date'] = pd.to_datetime(df_new_merchant_trans['purchase_date'])\ndf_new_merchant_trans['month_diff'] = ((datetime.datetime.today() - df_new_merchant_trans['purchase_date']).dt.days)\/\/30\ndf_new_merchant_trans['month_diff'] += df_new_merchant_trans['month_lag']","b97a923b":"df_hist_trans[:5]","1a941385":"df_hist_trans = pd.get_dummies(df_hist_trans, columns=['category_2', 'category_3'])\ndf_new_merchant_trans = pd.get_dummies(df_new_merchant_trans, columns=['category_2', 'category_3'])","7aa272e7":"df_hist_trans = reduce_mem_usage(df_hist_trans)\ndf_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)\n\nagg_fun = {'authorized_flag': ['mean']}\nauth_mean = df_hist_trans.groupby(['card_id']).agg(agg_fun)\nauth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\nauth_mean.reset_index(inplace=True)\n\nauthorized_transactions = df_hist_trans[df_hist_trans['authorized_flag'] == 1]\nhistorical_transactions = df_hist_trans[df_hist_trans['authorized_flag'] == 0]","1f52ec5e":"historical_transactions.head()","b54e4250":"historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\nauthorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\ndf_new_merchant_trans['purchase_month'] = df_new_merchant_trans['purchase_date'].dt.month","8fe0c584":"def aggregate_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n    'category_1': ['sum', 'mean'],\n    'category_2_1.0': ['mean'],\n    'category_2_2.0': ['mean'],\n    'category_2_3.0': ['mean'],\n    'category_2_4.0': ['mean'],\n    'category_2_5.0': ['mean'],\n    'category_3_A': ['mean'],\n    'category_3_B': ['mean'],\n    'category_3_C': ['mean'],\n    'merchant_id': ['nunique'],\n    'merchant_category_id': ['nunique'],\n    'state_id': ['nunique'],\n    'city_id': ['nunique'],\n    'subsector_id': ['nunique'],\n    'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n    'installments': ['sum', 'mean', 'max', 'min', 'std'],\n    'purchase_month': ['mean', 'max', 'min', 'std'],\n    'purchase_date': [np.ptp, 'min', 'max'],\n    'month_lag': ['mean', 'max', 'min', 'std'],\n    'month_diff': ['mean']\n    }\n    \n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history","fffc784e":"history = aggregate_transactions(historical_transactions)\nhistory.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\nhistory[:5]","4a9d3f9e":"authorized = aggregate_transactions(authorized_transactions)\nauthorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\nauthorized[:5]","8f89a6bd":"new = aggregate_transactions(df_new_merchant_trans)\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\nnew[:5]","e7eb6d1d":"def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group\n#___________________________________________________________\nfinal_group =  aggregate_per_month(authorized_transactions) \nfinal_group[:10]","e14b486f":"def successive_aggregates(df, field1, field2):\n    t = df.groupby(['card_id', field1])[field2].mean()\n    u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\n    u.columns = [field1 + '_' + field2 + '_' + col for col in u.columns.values]\n    u.reset_index(inplace=True)\n    return u","d5453615":"additional_fields = successive_aggregates(df_new_merchant_trans, 'category_1', 'purchase_amount')\nadditional_fields = additional_fields.merge(successive_aggregates(df_new_merchant_trans, 'installments', 'purchase_amount'),\n                                            on = 'card_id', how='left')\nadditional_fields = additional_fields.merge(successive_aggregates(df_new_merchant_trans, 'city_id', 'purchase_amount'),\n                                            on = 'card_id', how='left')\nadditional_fields = additional_fields.merge(successive_aggregates(df_new_merchant_trans, 'category_1', 'installments'),\n                                            on = 'card_id', how='left')\n","89490dbe":"# We now train the model with the features we previously defined. A first step consists in merging all the dataframes:","ec1e91e0":"df_train = pd.merge(df_train, history, on='card_id', how='left')\ndf_test = pd.merge(df_test, history, on='card_id', how='left')\n\ndf_train = pd.merge(df_train, authorized, on='card_id', how='left')\ndf_test = pd.merge(df_test, authorized, on='card_id', how='left')\n\ndf_train = pd.merge(df_train, new, on='card_id', how='left')\ndf_test = pd.merge(df_test, new, on='card_id', how='left')\n\ndf_train = pd.merge(df_train, final_group, on='card_id', how='left')\ndf_test = pd.merge(df_test, final_group, on='card_id', how='left')\n\ndf_train = pd.merge(df_train, auth_mean, on='card_id', how='left')\ndf_test = pd.merge(df_test, auth_mean, on='card_id', how='left')\n\ndf_train = pd.merge(df_train, additional_fields, on='card_id', how='left')\ndf_test = pd.merge(df_test, additional_fields, on='card_id', how='left')","499f66e5":"target = df_train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in df_train.columns if c not in drops]\nfeatures = list(df_train[use_cols].columns)\ndf_train[features].head()","82633b7f":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits","fc55fe37":"np.save('oof', oof)\nnp.save('predictions', predictions)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","747eb4c8":"from sklearn.metrics import mean_squared_error\nval_score = np.sqrt(mean_squared_error(target, oof))\nval_score","3c1adc64":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(clf, max_num_features=30, height=0.5, ax=ax, title='Feature importance', xlabel='Feature importance', ylabel='Features')\nplt.show()","81168c40":"sorted(list(zip(clf.feature_importance(), features)), reverse=True)","2f8dd77a":"df_sub = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\ndf_sub[\"target\"] = predictions\ndf_sub.to_csv(\"sub_val_{}.csv\".format(val_score), index=False)","f6ef91cc":"df_sub.head()","adaa725a":"df_sub.to_csv(\"submission.csv\", index=False)","31fd2ea7":" Packages\n First, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA."}}