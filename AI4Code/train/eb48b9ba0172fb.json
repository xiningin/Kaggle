{"cell_type":{"dcaf8119":"code","836d312e":"code","21cfd11b":"code","8d1ef925":"code","45669168":"code","920bcb61":"code","1067136f":"code","09e62da3":"code","38135089":"code","a1135a78":"code","51af0590":"code","f9148e25":"code","29d735f6":"code","9679aa42":"code","dbb413af":"code","ea31b87d":"code","cb62c7ea":"code","b6f7382f":"code","75311457":"code","e9623da6":"code","a4052530":"code","b3aa22d1":"code","457a650d":"code","2a029f84":"code","681b6451":"code","9c63589a":"code","6a57425e":"code","8b6be2f8":"code","b122f9a1":"code","d6240647":"code","9e8a8be9":"code","bd7c84a1":"code","fc0d5ec7":"code","cbab96ce":"code","fecef80c":"code","3bd15483":"code","6cc83238":"code","c0e43930":"code","5168e80b":"code","b621b815":"code","6651cb0b":"code","33c255e8":"code","edda046c":"code","a617df84":"code","58b84f40":"code","3fdb765d":"code","cd793f0e":"code","7d940fc4":"code","7c67ec55":"code","2fceaea1":"code","3927cfcd":"code","56c0ef41":"code","bee42fe4":"code","88e90f2d":"code","cedfdefd":"code","1493095e":"code","84577283":"code","613f6bd3":"code","85ff8b5c":"code","9dc5d630":"code","0b063974":"code","0df82434":"code","be4c9305":"code","8e67cc56":"code","c900100c":"code","61c5e587":"code","a5811bd1":"code","c54a973c":"code","ebab7ef6":"code","e852b148":"markdown","f21b302a":"markdown","2c6ffbc1":"markdown","8aec0fdd":"markdown","1c5e1654":"markdown","40bc6eac":"markdown","29effc18":"markdown","15173a6e":"markdown","66a85564":"markdown","65b07bc3":"markdown","d294d3a2":"markdown","389ef90b":"markdown","45256c5e":"markdown","197b7ac7":"markdown","81779f6a":"markdown","bd639d9a":"markdown","6651d9ca":"markdown","67956e85":"markdown","c9f32b5d":"markdown","5f9603da":"markdown","b905a6db":"markdown","9674471d":"markdown","2e581c24":"markdown","9f601b58":"markdown","d074b2ee":"markdown","1c02bcb0":"markdown","77a299dd":"markdown","78eff478":"markdown","31ba82e5":"markdown","26b05734":"markdown","c0442c11":"markdown","5d630c3f":"markdown","db64caf9":"markdown","dc41fd69":"markdown","0348a898":"markdown","f05efe98":"markdown","d3681ceb":"markdown","099095f6":"markdown","46d48a46":"markdown","b27f8bd6":"markdown","8496a367":"markdown","c403bc06":"markdown","efe7c3ea":"markdown","346f09f8":"markdown","c11ba80f":"markdown","0e2a330f":"markdown","18d241ad":"markdown"},"source":{"dcaf8119":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","836d312e":"df=pd.read_csv('..\/input\/concretecsv\/concrete.csv')","21cfd11b":"df.shape # Check number of columns and rows in data frame","8d1ef925":"df.info()","45669168":"df.head()","920bcb61":"df.describe().T","1067136f":"# Total Number of zeores in an attriibute.\ncols = list(df.columns)\nfor i in np.arange(len(cols)):\n     print('number of 0s in ', cols[i],' is ',df[cols[i]].isin([0]).sum())","09e62da3":"# studying the distribution of continuous attributes mean, median, mode defining the central tendency, \n# standard deviation refecting the spread and skewness reflecting the tail \ncols = list(df.columns)\nfor i in np.arange(len(cols)):\n    sns.distplot(df[cols[i]], color='blue')\n    #plt.xlabel('Experience')\n    plt.show()\n    print('Distribution of ',cols[i])\n    print('Mean is:',df[cols[i]].mean())\n    print('Median is:',df[cols[i]].median())\n    print('Mode is:',df[cols[i]].mode())\n    print('Standard deviation is:',df[cols[i]].std())\n    print('Skewness is:',df[cols[i]].skew())\n    print('Maximum is:',df[cols[i]].max())\n    print('Minimum is:',df[cols[i]].min())","38135089":"# missing values\ndf.isnull().values.any() # If there are any null values in data set","a1135a78":"# Check the presence of outliers\ncol = list(df.columns)\nl = len(df)\nfor i in np.arange(len(col)):\n    sns.boxplot(x= df[col[i]], color='cyan')\n    plt.show()\n    print('Boxplot of ',col[i])\n    #calculating the outiers in attribute \n    Q1 = df[col[i]].quantile(0.25)\n    Q2 = df[col[i]].quantile(0.50)\n    Q3 = df[col[i]].quantile(0.75) \n    IQR = Q3 - Q1\n    L_W = (Q1 - 1.5 *IQR)\n    U_W = (Q3 + 1.5 *IQR)    \n    print('Q1 is : ',Q1)\n    print('Q2 is : ',Q2)\n    print('Q3 is : ',Q3)\n    print('IQR is:',IQR)\n    print('Lower Whisker, Upper Whisker : ',L_W,',',U_W)\n    bools = (df[col[i]] < (Q1 - 1.5 *IQR)) |(df[col[i]] > (Q3 + 1.5 * IQR))\n    print('Out of ',l,' rows in data, number of outliers are:',bools.sum())   #calculating the number of outliers","51af0590":"sns.pairplot(df,hue_order=df['strength'],diag_kind='kde');","f9148e25":"cor=df.corr()\nsns.heatmap(cor,annot=True);","29d735f6":"# print columns that are highly correlated\ncor_mat=df.corr().abs()\ncor_mat\nupper_tri = cor_mat.where(np.triu(np.ones(cor_mat.shape),k=1).astype(np.bool))\n#print(upper_tri)\ncol_h =[column for column in upper_tri.columns if any(upper_tri[column] > 0.80)]\n\nprint(\"The columns with more than 0.8 correlation :\",col_h[0:6])","9679aa42":"#  function to treat outliers\ndef detect_treate_outliers(df,operation):\n    cols=[]\n    IQR_list=[]\n    lower_boundary_list=[]\n    upper_boundary_list=[]\n    outliers_count=[]\n    for col in df.columns:\n        #print('col',col)\n        if((df[col].dtype =='int64' or df[col].dtype =='float64') and (col != 'HR')):\n            #print('Inside if')\n            IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n            lower_boundary = df[col].quantile(0.25) - (1.5 * IQR)\n            upper_boundary = df[col].quantile(0.75) + (1.5 * IQR)\n            up_cnt = df[df[col]>upper_boundary][col].shape[0]\n            #print('Upper count=',up_cnt)\n            lw_cnt = df[df[col]<lower_boundary][col].shape[0]\n            #print('lower count=',lw_cnt)\n            if(up_cnt+lw_cnt) > 0:\n                cols.append(col)\n                IQR_list.append(IQR)\n                lower_boundary_list.append(lower_boundary)\n                upper_boundary_list.append(upper_boundary)\n                outliers_count.append(up_cnt+lw_cnt)\n                if operation == 'update':\n                    df.loc[df[col] > upper_boundary,col] = upper_boundary\n                    df.loc[df[col] < lower_boundary,col] = lower_boundary\n                else:\n                    pass\n            else:\n                pass\n   #print('cols=',cols)\n   # print('IQR_list=',IQR_list)\n   # print('lower_boundary_list=',lower_boundary_list)\n   # print('upper_boundary_list=',upper_boundary_list)\n   # print('outliers_count=',outliers_count)\n    ndf = pd.DataFrame(list(zip(cols,IQR_list,lower_boundary_list,upper_boundary_list,outliers_count)),columns=['Features','IQR','Lower Boundary','Upper Boundary','Outlier Count'])\n    #print('Data=',ndf)\n    #print('Columns having outliers=',cols)\n    if operation == 'update':\n        return (len(cols),df)\n    else:\n        return (len(cols),ndf)","dbb413af":"#Removing outliers by replacing the data below lower whisker with it and above upper whisker with it respectively.\ncount,df1=detect_treate_outliers(df,'update')\nif count>0:\n    print('Updating dataset')\n    df=df1\ndataf=df","ea31b87d":"dataf.info()","cb62c7ea":"dataf.shape","b6f7382f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import zscore","75311457":"pca_data=dataf.copy()","e9623da6":"pca_data=pca_data.apply(zscore)","a4052530":"Scaler=StandardScaler()\nX_PCA=pca_data.drop(['strength'],axis=1)\nY_PCA=pca_data['strength']\npc=PCA(n_components=8,random_state=12)\ncomp_features=pc.fit(X_PCA)","b3aa22d1":"covmatrix=np.cov(X_PCA,rowvar=False)\nplt.figure(figsize=(8,4))\nsns.heatmap(covmatrix,annot=True)","457a650d":"print(\"####################The Eigen Values#########################\")\nprint(pc.explained_variance_)\nprint(\"####################The Eigen Vectors#########################\")\nprint(pc.components_)","2a029f84":"plt.bar(list(range(1,9)),pc.explained_variance_ratio_, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","681b6451":"plt.step(list(range(1,9)),np.cumsum(pc.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","9c63589a":"print(\"Varience Ratio covered by each componenet:{}\".format(pc.explained_variance_ratio_ * 100))\nP_Components=pc.explained_variance_ratio_\nprint(\"The Ideal number of components that could explain:{}% of variance in data is 6\".format(np.sum(P_Components[0:6])*100))","6a57425e":"pca6=PCA(n_components=6)\npca6.fit(X_PCA)\nxpca_6=pca6.transform(X_PCA)\nypca_6=Y_PCA","8b6be2f8":"pca6.explained_variance_","b122f9a1":"pca6.components_","d6240647":"#KMeans Clustering\nfrom sklearn.cluster import KMeans\n\ncluster_range = range( 2, 6)   # expect 3 to four clusters from the pair panel visual inspection hence restricting from 2 to 6\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(df)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:14]","9e8a8be9":"# Elbow plot\n\nplt.figure(figsize=(10,5))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","bd7c84a1":"from sklearn.cluster import KMeans\nfrom scipy.stats import zscore\n\nX1 = df.loc[:, 'cement':'strength']\nXScaled1 = X1.apply(zscore)\n\ncluster = KMeans( n_clusters = 3, random_state = 2354 )\ncluster.fit(XScaled1)\n\nprediction=cluster.predict(XScaled1)\nXScaled1[\"GROUP\"] = prediction     # Creating a new column \"GROUP\" which will hold the cluster id of each record\n\nXScaled1_copy = XScaled1.copy(deep = True)  # Creating a mirror copy for later re-use instead of building repeatedly","fc0d5ec7":"centroids = cluster.cluster_centers_\ncentroids","cbab96ce":"centroid_df = pd.DataFrame(centroids, columns = list(X1) )\ncentroid_df","fecef80c":"concat_data = XScaled1","3bd15483":"col = list(df.columns)\ncol.remove('strength')\nfor i in np.arange(len(col)):\n    with sns.axes_style(\"white\"):\n        plot = sns.lmplot(col[i],'strength',data=concat_data,hue='GROUP')\n    plot.set(ylim = (-3,3))\n    ","6cc83238":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor\nX_df = df.drop(['strength'], axis=1)\nX = df.drop(['strength'], axis=1)\ny = df['strength']","c0e43930":"XScaled = X.apply(zscore)","5168e80b":"X_train, X_test, y_train, y_test = train_test_split(XScaled, y, train_size=0.7, test_size=0.3, random_state=12)","b621b815":"##fitting training,test data, predictions on raw scaled data and calculating the training and test scores\n\n#linear model\nlm=LinearRegression()\nlm.fit(X_train,y_train)\ny_pred_lm= lm.predict(X_test)\ntr_lm = lm.score(X_train,y_train)* 100\nte_lm= lm.score(X_test,y_test)* 100\nrmse_lm = np.sqrt(mean_squared_error(y_test,y_pred_lm))\n\n#polynomial model\nlm_Poly=Pipeline([('Poly',PolynomialFeatures(degree=2)),\n               ('Model2',LinearRegression())\n               ])\nlm_Poly.fit(X_train,y_train)\ny_pred_poly= lm_Poly.predict(X_test)\ntr_poly= lm_Poly.score(X_train,y_train)* 100\nte_poly= lm_Poly.score(X_test,y_test)* 100\nrmse_poly = np.sqrt(mean_squared_error(y_test,y_pred_poly))\n\n# svr model\nsvrm=SVR()\nsvrm.fit(X_train,y_train)\ny_pred_svr= svrm.predict(X_test)\ntr_sv= svrm.score(X_train,y_train)* 100 \nte_sv= svrm.score(X_test,y_test)* 100\nrmse_sv = np.sqrt(mean_squared_error(y_test,y_pred_svr))\n\n# decision tree model\ndtree=DecisionTreeRegressor()\ndtree.fit(X_train,y_train)\ny_pred_dt= dtree.predict(X_test)\ntr_dt= dtree.score(X_train,y_train)* 100\nte_dt= dtree.score(X_test,y_test)* 100\nrmse_dt = np.sqrt(mean_squared_error(y_test,y_pred_dt))\n\n#random forest model\nrf=RandomForestRegressor()\nrf.fit(X_train,y_train)\ny_pred_rf= rf.predict(X_test)\ntr_rf= rf.score(X_train,y_train)* 100\nte_rf= rf.score(X_test,y_test)* 100 \nrmse_rf = np.sqrt(mean_squared_error(y_test,y_pred_rf))\n\n# lasso model\nls=Lasso(alpha=0.2)\nls.fit(X_train,y_train)\ny_pred_ls= ls.predict(X_test)\ntr_ls= ls.score(X_train,y_train)* 100 \nte_ls= ls.score(X_test,y_test)* 100\nrmse_ls = np.sqrt(mean_squared_error(y_test,y_pred_ls))\n\n# ridge model\nrd=Ridge()\nrd.fit(X_train,y_train)\ny_pred_rd= rd.predict(X_test)\ntr_rd= rd.score(X_train,y_train)* 100\nte_rd= rd.score(X_test,y_test)* 100\nrmse_rd = np.sqrt(mean_squared_error(y_test,y_pred_rd))\n\n#Ada booster\nad=AdaBoostRegressor()\nad.fit(X_train,y_train)\ny_pred_ad= ad.predict(X_test)\ntr_ad=ad.score(X_train,y_train)* 100\nte_ad= ad.score(X_test,y_test)* 100\nrmse_ad = np.sqrt(mean_squared_error(y_test,y_pred_ad))\n    \n#Bagging Regressor\nba=BaggingRegressor()\nba.fit(X_train,y_train)\ny_pred_ba= ba.predict(X_test)\ntr_ba=ba.score(X_train,y_train)* 100\nte_ba= ba.score(X_test,y_test)* 100\nrmse_ba = np.sqrt(mean_squared_error(y_test,y_pred_ba))\n\n# Gradient Boost\ngb=GradientBoostingRegressor()\ngb.fit(X_train,y_train)\ny_pred_gb= gb.predict(X_test)\ntr_gb=gb.score(X_train,y_train)* 100\nte_gb= gb.score(X_test,y_test)* 100\nrmse_gb = np.sqrt(mean_squared_error(y_test,y_pred_gb))","6651cb0b":"#Feature Importance from Decision Tree, RF, Lasso and Ridge\nFeatures=(['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg','fineagg', 'age'])\nlm_f = lm.coef_\ndt_f= dtree.feature_importances_\nrf_f= rf.feature_importances_\nls_f= ls.coef_\nrd_f= rd.coef_\nad_f=ad.feature_importances_\ngb_f=gb.feature_importances_","33c255e8":"summary={'FEATURES':Features,\"Linear\":lm_f,\"Dtree\":dt_f,'Random Forest':rf_f,'Lasso':ls_f ,'Ridge':rd_f,\n         'Adaptive Boost':ad_f,'Gradient Boost':gb_f}","edda046c":"# Feature Analysis Summmary raw data\nfeature_frame=pd.DataFrame(summary)\nfeature_frame","a617df84":"#Root mean suare error of models with raw data\nRMSE_raw={\"Linear\":rmse_lm,\"Dtree\":rmse_dt,'Random Forest':rmse_rf,'Lasso':rmse_rf,'Ridge':rmse_rd,'Ada':rmse_ad,\n          'Bag':rmse_ba,'Gradient B':rmse_gb}\nRMSE_raw","58b84f40":"X_pca_train,X_pca_test,Y_pca_train,Y_pca_test=train_test_split(xpca_6,ypca_6,test_size=0.3,random_state=12)","3fdb765d":"##fitting training,test data, predictions on PCA scaled data and calculating the training and test scores\n# PCA linear model\nlm.fit(X_pca_train,Y_pca_train)\ny_pred_lmPCA= lm.predict(X_pca_test)\ntr_PCA_lm=lm.score(X_pca_train,Y_pca_train)* 100\nte_PCA_lm =lm.score(X_pca_test,Y_pca_test)* 100\nrmse_lmPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_lmPCA))\n\n# PCA polynomial model\nlm_Poly.fit(X_pca_train,Y_pca_train)\ny_pred_POLYPCA=lm_Poly.predict(X_pca_test)\ntr_PCA_Poly=lm_Poly.score(X_pca_train,Y_pca_train)* 100\nte_PCA_Poly= lm_Poly.score(X_pca_test,Y_pca_test)* 100\nrmse_POLYPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_POLYPCA))\n\n# PCA svr\nsvrm.fit(X_pca_train,Y_pca_train)\ny_pred_svrPCA= svrm.predict(X_pca_test)\ntr_PCA_sv= svrm.score(X_pca_train,Y_pca_train)* 100\nte_PCA_sv= svrm.score(X_pca_test,Y_pca_test)* 100\nrmse_svrPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_svrPCA))\n\n# PCA decision tree\ndtree.fit(X_pca_train,Y_pca_train)\ny_pred_dtPCA= dtree.predict(X_pca_test)\ntr_PCA_dt= dtree.score(X_pca_train,Y_pca_train)* 100\nte_PCA_dt= dtree.score(X_pca_test,Y_pca_test)* 100\nrmse_dtPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_dtPCA))\n\n#PCA random forest\nrf.fit(X_pca_train,Y_pca_train)\ny_pred_rfPCA= rf.predict(X_pca_test)\ntr_PCA_rf= rf.score(X_pca_train,Y_pca_train)* 100\nte_PCA_rf= rf.score(X_pca_test,Y_pca_test)* 100\nrmse_rfPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_rfPCA))\n\n#PCA lasso\nls.fit(X_pca_train,Y_pca_train)\ny_pred_lsPCA= ls.predict(X_pca_test)\ntr_PCA_ls= ls.score(X_pca_train,Y_pca_train)* 100\nte_PCA_ls= ls.score(X_pca_test,Y_pca_test)* 100\nrmse_lsPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_lsPCA))\n\n# PCA ridge\nrd.fit(X_pca_train,Y_pca_train)\ny_pred_rdPCA= rd.predict(X_pca_test)\ntr_PCA_rd= rd.score(X_pca_train,Y_pca_train)* 100\nte_PCA_rd= rd.score(X_pca_test,Y_pca_test)* 100\nrmse_rdPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_rdPCA))\n\n# PCA Ada booster\nad.fit(X_pca_train,Y_pca_train)\ny_pred_adPCA= ad.predict(X_pca_test)\ntr_PCA_ad=ad.score(X_pca_train,Y_pca_train)* 100\nte_PCA_ad= ad.score(X_pca_test,Y_pca_test)* 100\nrmse_adPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_adPCA))\n    \n#  PCA Bagging Regressor\nba.fit(X_pca_train,Y_pca_train)\ny_pred_baPCA= ba.predict(X_pca_test)\ntr_PCA_ba=ba.score(X_pca_train,Y_pca_train)* 100\nte_PCA_ba= ba.score(X_pca_test,Y_pca_test)* 100\nrmse_baPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_baPCA))\n\n#  PCA Gradient Boost\ngb.fit(X_pca_train,Y_pca_train)\ny_pred_gbPCA= gb.predict(X_pca_test)\ntr_PCA_gb=gb.score(X_pca_train,Y_pca_train)* 100\nte_PCA_gb= gb.score(X_pca_test,Y_pca_test)* 100\nrmse_gbPCA = np.sqrt(mean_squared_error(Y_pca_test,y_pred_gbPCA))","cd793f0e":"# coeficients and feature importance of PCA components\nlm_coff_PCA = lm.coef_\ndt_coe_PCA= dtree.feature_importances_\nrf_coe_PCA= rf.feature_importances_\nls_coe_PCA= ls.coef_\nrd_coe_PCA= rd.coef_\nad_coe_PCA=ad.feature_importances_\ngb_coe_PCA=gb.feature_importances_","7d940fc4":"#Feature Importance of PCA with Linear model,Decision Tree, RF, Lasso and Ridge\nsummary_PCA={'PCA_linear':lm_coff_PCA,'PCA_Dtree':dt_coe_PCA,'PCA_Random Forest':rf_coe_PCA,'PCA_Lasso':ls_coe_PCA, 'PCA_Ridge':rd_coe_PCA,\n             'PCA_Ada':ad_coe_PCA,'PCA_Gradient Boost':gb_coe_PCA}","7c67ec55":"# Feature Analysis Summmary of PCA components\nfeature_frame_PCA=pd.DataFrame(summary_PCA)\nfeature_frame_PCA","2fceaea1":"#Root mean suare error of models with principal components\nRMSE_PCA={\"Linear_PCA\":rmse_lmPCA,\"Dtree_PCA\":rmse_dtPCA,'Random Forest_PCA':rmse_rfPCA,'Lasso_PCA':rmse_rfPCA,'Ridge_PCA':rmse_rdPCA,\n          'Ada_PCA':rmse_adPCA,'Bag_PCA':rmse_baPCA,'GradientB_PCA':rmse_gbPCA}\nRMSE_PCA","3927cfcd":"TABL=pd.DataFrame({'Model_Names':['Linear Regression','Polynomial_regresison','Support Vector Regressor','Decision Tree Regressor',\n                                 'Random Forest Regressor','Lasso Regressor','Ridge Regressor','Adaptive Boost Regressor',\n                                 'Bagging Regressor','Gradient Boost'],\n'Training_Score_Scaled_Raw':[tr_lm,tr_poly,tr_sv,tr_dt,tr_rf,tr_ls,tr_rd,tr_ad,tr_ba,tr_gb],\n'Testing_Score_Scaled_Raw':[te_lm,te_poly,te_sv,te_dt,te_rf,te_rf,te_rd,te_ad,te_ba,te_gb],\n'Training_Score_PCA':[tr_PCA_lm,tr_PCA_Poly,tr_PCA_sv,tr_PCA_dt,tr_PCA_rf,tr_PCA_ls,tr_PCA_rd,tr_PCA_ad,tr_PCA_ba,tr_PCA_gb],\n'Testing_Score_PCA':[te_PCA_lm,te_PCA_Poly,te_PCA_sv,te_PCA_dt,te_PCA_rf,te_PCA_ls,te_PCA_rd,te_PCA_ad,te_PCA_ba,te_PCA_gb]})\n\n","56c0ef41":"TABL","bee42fe4":"print(\"Best Test score we have achived on raw Data:\",TABL['Testing_Score_Scaled_Raw'].max())\nprint(\"Best Test score we have achived on PCA components:\",TABL['Testing_Score_PCA'].max())","88e90f2d":"#Regularization using GridsearchCV\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint","cedfdefd":"# Create the parameter grid \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","1493095e":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n","84577283":"grid_search.best_params_","613f6bd3":"#training with the best parameters obtained by grid search\ngrid_s = RandomForestRegressor(bootstrap= True, max_depth= 90,max_features= 3,min_samples_leaf= 3,\n                                     min_samples_split= 8,n_estimators= 200)","85ff8b5c":"grid_s.fit(X_train,y_train)\ngrid_s.score(X_train,y_train) * 100","9dc5d630":"grid_s.score(X_test,y_test) * 100","0b063974":"# Number of trees in random forest\nn_estimators = [int(x) for x in range(200,2000,200)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","0df82434":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","be4c9305":"rf_random.best_params_","8e67cc56":"#training with the best parameters obtained by randomised search\nrf_s = RandomForestRegressor(n_estimators= 1400,min_samples_split= 2,min_samples_leaf= 1,max_features= 'sqrt',\n                             max_depth= 80,bootstrap= False)","c900100c":"rf_s.fit(X_train,y_train)\nrf_s.score(X_train,y_train) * 100","61c5e587":"rf_s.score(X_test,y_test) * 100","a5811bd1":"#4c. Model performance range at 95% confidence level ","c54a973c":"from sklearn.model_selection import cross_val_score,KFold","ebab7ef6":"K=10\nseed=12\nkfold_Linear=KFold(shuffle=True,n_splits=K,random_state=seed)\naccuracies = cross_val_score(estimator = rf_s, X = XScaled, y = y, cv = kfold_Linear) \naccuracies\nprint(\"K Fold score mean:{}\".format(accuracies.mean()*100))\nprint(\"K Fold score standard deviation:{}\".format(accuracies.std()*100))","e852b148":"<l>age has a range from 1 to 365.<\\l>\n<l>standard deviation of cement is the highest and of superplastic is lowest.<\\l>\n<l>Range of slag,ash and superplastic is 0 to 359 , 0 to 200 and 0 to 32.2 respectively \n<l>50% of the data in ash seems to be zero.<\\l>\n<l>In slag, ash and age the mean is away from the 50% .<\\l>","f21b302a":"After outlier removal there are 1030 records in the dataset.There are some outliers again in the final data, but these are not real noise, as the distribution squeezes a bit after removal of ouliers.","2c6ffbc1":"Though we have done PCA to identify the features and their corresponding varience ,let us see the feature importances captured by each of the models.","8aec0fdd":"Rmse is lowest for Random Forest and the scores are high with Principal Components too. It is the best model in this case, since it does well in both training and testing scores.","1c5e1654":"#4b. Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit","40bc6eac":"Rmse is lowest for Random Forest.","29effc18":"EDA","15173a6e":"<l>1a. Univariate analysis<\/l>\n\n<l>i. Univariate analysis \u2013 data types and description of the independent attributes which should include (name, meaning, range of values observed, central values (mean and median), standard deviation and quartiles, analysis of the body of distributions \/ tails, missing values, outliers.<\/l>","66a85564":"Model performance range at 95% confidence level\nTo further analyse final evaluation on unseen data, let's do K fold cross validation","65b07bc3":"GridSearchCV","d294d3a2":"# Feature Engineering techniques","389ef90b":"# Model Creation","45256c5e":"#1c. Pick one strategy to address the presence outliers and missing values and perform necessary imputation","197b7ac7":"Using PCA for dimensionality reduction","81779f6a":"There is no Null values in the dataset.","bd639d9a":"From the graphs above it is evident that 6 components capture just over 97% of data, rather than using all the features we can use just 6 major components on our models to train and predict.","6651d9ca":"cement, coarseagg,fineagg, strength and water are almost normally distributed.\nAge , Ash, superplastic are slightly skewed.\nslag, ash, water, superplastic, coarseagg, fineagg , age are all multi gaussian, (multiple peaks) . \nslag, water, superplastic and age are right skewed.","67956e85":"2c. Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings in terms of the independent attributes and their suitability to predict strength ","c9f32b5d":"Rmse is low for Random Forest and the scores are high with raw data.","5f9603da":"Let's  display the model scores as a dataframe having scores of training and testing data on scaled raw and PCA features.","b905a6db":"#4a. Algorithms that you think will be suitable for this project ","9674471d":"# Multivariate analysis\n\n<l>#1b. Bi-variate analysis between the predictor variables and between the predictor variables and target column. <\/l>\n<l>Comment on your findings in terms of their relationship and degree of relation if any. Presence of leverage points.<\/l> \n<l>Visualize the analysis using boxplots and pair plots, histograms or density curves. Select the most appropriate \nattributes <\/l>","2e581c24":"RandomizedSearchCV","9f601b58":"Split the data using train_test_split","d074b2ee":"<l>Linear model did not perform that great on both raw as well as PCA components.<\/l>\n<l>Polynomial Regression on a degree of 2 has been implemented ,which means the number of independent variables have been increased.(This actually turns the linear equation of \"y=mx + B\" into quadratic(degree=2) ). Since the scores are better with\nrandom forest than for polynomial(deg=2), using higher polynomial function might overfit the data and also we may run into curse of dimensionality with small dataset.<\/l>\n<l>Decision tree is a overfit, as it prodces 99% accuracy with training and 78% and 69% with raw and PCA components respectively in test, hence it is not the best model.<\/l>\n<l>Random forest  seems to be slightly overfit, but looks well on both training and test data on both raw and PCA.<\/l>","1c02bcb0":"There are 1030 rows and 9 columns","77a299dd":"<l>#Applying the selected PCA to check model performances. <\/l>\n<l>Split the PCA components using train_test_split with 6 features(n_components=6) to see how our models perfrom on the PCA components. <\/l>","78eff478":"46% of the data in slag is zero,55% in ash and 37% in superplastic","31ba82e5":"Hyper parameter tuning on Random forest by Grid search to squeeze that extra performance out of the model without making it overfit or underfit","26b05734":"The hyperparameter tuned model produces a accuracy score of 92.48 % on 10 fold cross validation \nwith a standard deviation of 2.30. So , it would give an accuracy of range 90.18% to 94.78%, which is pretty good at 95% confidence Interval.","c0442c11":"Exploring the mix up of Gaussians in the data, Cluster Analysis using Kfold(Centroid based) \nSince there are 3 peaks in some of the features as seen from the gaussian curv in pair plot, there are 3 to 9 clusters in this dataset, using KMeans Clustering to analyse them.","5d630c3f":"Traget Column is 'strength' ","db64caf9":"The training and the test scores of Random Forest has improved with random Search.\nTuning helps to build better model,training score without tuning 98%, testing score without tuning 91.\nTraining score with hyperparameter tuning 99%, Testing score with hyperparameter tuning 92%","dc41fd69":"The scatter plot for all the variables with respect to strength we could see that the groupings formed through clustering evidently prove that similar group of data have similar values","0348a898":"#Instead of interpreting the neumerical values of the centroids, let us do a visual analysis by converting the \ncentroids and the data in the cluster into box plots.","f05efe98":"For this problem statement, Linear models seems to be a good fit, We are not just going to limit ourselves with \nlinear regression, we are going to explore all the linear models , polynomial models to see which performs best \nand going to select one.\n\n#2b. Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help\n#3a. Obtain feature importance for the individual features and present your findings ","d3681ceb":"<l>Pair plot analysis:<\/l>\n<l>Gaussians for many of the features. Cluster analysis would help to understand the grouping and hidden pattern in data.<\/l>\n<l>As the value of 'cement' increases so does 'strength'.<\/l>\n<l>There is a weak correlation of 'ash', 'water', 'coarseagg' and 'fineagg' with 'strength'.<\/l>\n<l>From the correlation matrix we could infer that , features have less correraltion between each other. <\/l>","099095f6":"<l>From the above table we infer -  <\/l>\n\n<l>Linear and Ridge(alpha=0.2) alomst has same coeficients for all features.<\/l>\n<l>Decision Tree and Random forest's coefficients are similar.<\/l>\n<l>Lasso regularization stands out and it made less important features to be zero,since lasso penalizes the error with high value.<\/l>\n<l>Lasso can be used to select feature importances , but here it is done with PCA below.<\/l>","46d48a46":"Let us analyze the 'strength' column vs other columns group wise","b27f8bd6":"Data Description:  \nThe actual concrete compressive strength (MPa) for a given mixture under a  specific age (days) was determined from laboratory. Data is in raw form (not scaled). The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations). Domain:  \nCement manufacturing \nContext: \nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate. \n\nObjective:  \nModeling of strength of high performance concrete using Machine Learning \n\nAttribute Information: \uf0b7 \nCement    : measured in kg in a m3 mixture \uf0b7 \nBlast     : measured in kg in a m3 mixture \uf0b7 \nFly ash    : measured in kg in a m3 mixture \uf0b7 \nWater     : measured in kg in a m3 mixture \uf0b7 \nSuperplasticizer   : measured in kg in a m3 mixture \uf0b7 \nCoarse Aggregate   : measured in kg in a m3 mixture \uf0b7 \nFine Aggregate   : measured in kg in a m3 mixture \uf0b7 \nAge     : day (1~365) \uf0b7 Concrete compressive strength measured in MPa ","8496a367":"2a. Identify opportunities (if any) to create a composite feature, drop a feature etc","c403bc06":"# Case Study: Concrete compressive strength","efe7c3ea":"Data types and Names of the independent attributes","346f09f8":"The above are the best estimators for our random search model and we shall use these values for on our final model \nto check how our scores are improved.","c11ba80f":"All 9 columns are non null and numeric","0e2a330f":"Cluster Analysis and their relationship with Strength\n\n1. From the Above plots of Age VS Strength it is very evident and convincing that Age can be strong preditor in the strength of the concrete mix, As you can see for all the groups of clusters in age we see a strong positive linear relationship between age and strength. We can also infer that as the mixture ages the strength of the concrete increases. 1.1. The Line of best fit is also around the mean and the residuals or error is also minimal\n2. Cement seems to have little positive relationsip with strength , But may not be a strong predictor.\n3. Water vs strength (Group 2 has some linear relation ship) whereas, group 0 and group 1 have a slight linear relationship.    Hence, water also may not be a strong predictor of strength.\n4. Fineagg, for group 0 and group 1 the line is almost horizontal, which means for value change in fineagg there is no considerable change in strength, But for group 3 there is some relationship for fineagg vs strength. Hence Fineagg may also not be good predictor of concrete strength.\n5.Slag is almost horizontal for group 0 and group 2, group 1 has a slight relationship which makes slag not a great predictpr of strength.\n6. Ash , suplerplastic and coarseagg all have horizontal data distribution on atleast one or more groups with strength also making them weak predictprs of strength.\nSo from our cluster analysis , we could infer that age has a strong relationship with strength for data in all clusters","18d241ad":"#The elbow plot confirms our visual analysis that there are likely 3 or 4 good clusters. Let us start with 3 clusters"}}