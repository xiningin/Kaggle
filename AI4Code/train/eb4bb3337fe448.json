{"cell_type":{"1af085b6":"code","34254220":"code","de36b1a4":"code","326ea38f":"code","4f5370cc":"code","a5c52e11":"code","e838312c":"code","02642124":"code","ef790a72":"code","9f2733b3":"code","132d2dcf":"code","eeee4d9f":"code","3b5aab54":"code","c3edc5ad":"code","3a4c7a59":"code","a70de097":"code","ca95465e":"code","ede5642c":"code","1a6df91e":"code","40d57d15":"code","44bc0db1":"code","1cfcfd2f":"code","5e918df5":"code","4cecb13f":"code","8cd7062e":"code","3d459e08":"code","406c44c4":"markdown","88d72839":"markdown","04df5f12":"markdown","8aec2829":"markdown","c35da9c6":"markdown","b519bdb4":"markdown","af286372":"markdown","551481e7":"markdown","a4feebbc":"markdown","15586081":"markdown","869f39f1":"markdown","a7049698":"markdown","ac916bc2":"markdown","d189446a":"markdown","d6c0817c":"markdown","471eff70":"markdown","e67c04ba":"markdown","21797e60":"markdown"},"source":{"1af085b6":"import pandas as pd \nimport numpy as np \nimport random \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n#from imblearn.pipeline import Pipeline","34254220":"def show_history(h):\n    epochs_trained = len(h.history['loss'])\n    plt.figure(figsize=(16, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n    plt.ylim([0., 1.])\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \ndef print_counts(y):\n    labels, counts = np.unique(y, return_counts=True)\n    total = len(y)\n    for label, count in zip(labels, counts):\n        prtg = (count * 100) \/ total\n        print(\"Label: {}, Count: {} ({:.2f}%)\".format(label, count, prtg))\n        \ndef create_pipeline(Scaler, Model):\n    pipe = Pipeline([\n        ('scaler', Scaler),\n        ('model', Model)\n    ])\n    return pipe\n\ndef iterate_models(models, scaler,  X, y, X_t, y_t):\n    for model in models:\n        pline = create_pipeline(scaler, model)\n        pline.fit(X, y)\n        score = pline.score(X_t, y_t)\n        print(\"Classifier: {}, accuracy: {:.5f}\".format(model.__class__.__name__, score))","de36b1a4":"path = \"..\/input\/wine-quality-dataset\/WineQT.csv\"\ndf = pd.read_csv(path)\ndf.info()","326ea38f":"df.describe()","4f5370cc":"df.head(7)","a5c52e11":"df.isna().sum()","e838312c":"df = df.drop(\"Id\", axis=1)","02642124":"df[\"quality\"].hist()","ef790a72":"printcounts(df[\"quality\"].values)","9f2733b3":"df_train = df.sample(frac=0.7, random_state=7).copy()\ny_train = df_train.pop(\"quality\").values - 3\ndf_test = df.drop(df_train.index).copy()\ny_test = df_test.pop(\"quality\").values - 3\nprint(\"Train lenght:\", len(df_train))\nprint(\"Test lenght:\", len(df_test))","132d2dcf":"print_counts(y_train)","eeee4d9f":"oversampler = SMOTE(random_state=42, k_neighbors=3)\nX1, y1 = oversampler.fit_resample(df_train.values, y_train)","3b5aab54":"print_counts(y1)","c3edc5ad":"ros = RandomOverSampler(random_state=42)\nX_res, y_res = ros.fit_resample(df_train.values, y_train)","3a4c7a59":"print_counts(y_res)","a70de097":"oversample = SMOTE(random_state=42)\nX, y = oversample.fit_resample(df.drop(\"quality\", axis=1).values, df[\"quality\"].values)","ca95465e":"print_counts(y)","ede5642c":"scaler = MinMaxScaler()\nscaler2 = StandardScaler()\n\nLogis = LogisticRegression(solver='liblinear')\nsvc = SVC()\nrf = RandomForestClassifier()\nada = AdaBoostClassifier(n_estimators=100)\nknn = KNeighborsClassifier(n_neighbors=6)\n\nmodels = [Logis, svc, rf, ada, knn]","1a6df91e":"iterate_models(models, scaler, df_train.values, y_train, df_test.values, y_test)","40d57d15":"iterate_models(models, scaler2, df_train.values, y_train, df_test.values, y_test)","44bc0db1":"iterate_models(models, scaler, X1, y1, df_test.values, y_test)","1cfcfd2f":"iterate_models(models, scaler2, X1, y1, df_test.values, y_test)","5e918df5":"iterate_models(models, scaler, X_res, y_res, df_test.values, y_test)","4cecb13f":"iterate_models(models, scaler2, X_res, y_res, df_test.values, y_test)","8cd7062e":"X_tra, X_tst, y_tra, y_tst = train_test_split(X, y, test_size=0.3, random_state=42)\niterate_models(models, scaler, X_tra, y_tra, X_tst, y_tst)","3d459e08":"iterate_models(models, scaler2, X_tra, y_tra, X_tst, y_tst)","406c44c4":"We can see a higher accuracy without data augmentation using SVC, data augmentation seems to affect negatively to the algorihms so that accuracy dropped. On the other hand, if we use data augmentation on the hole dataset we reach better results, the random forest classifier got an accuracy of 85%, however, this kind of preprocesing of the data should be fitted only on the training set.","88d72839":"We can observe the there is a class imbalance, most of the quality note are in 5 and 6. Quality 3, 4, and 8 have few data.","04df5f12":"### Now we will train four models and use a min-max scaler","8aec2829":"### Using all data in augmentation","c35da9c6":"## Read Data","b519bdb4":"### On the other hand, we will see how the data augmentation affect if we use it on the entire dataset","af286372":"## Create functions","551481e7":"Read the data and explore the variables","a4feebbc":"## Data augmentation","15586081":"I have defined some useful functions for this task and avoid to repeat code","869f39f1":"### We start by spliting the dataset before go to any data transformation.","a7049698":"### As we could see before, some classes have few data compared to the others. This is when data augmentation techniques is necesary. Gere, we explore two techniques, SMOTE and RandomOverSampler.","ac916bc2":"## Import libraries","d189446a":"## Train the models!","d6c0817c":"## Train and Test data","471eff70":"### Random Sampler","e67c04ba":"### Using Data without augmentation","21797e60":"### SMOTE"}}