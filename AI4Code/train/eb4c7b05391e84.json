{"cell_type":{"230e0e02":"code","8b647077":"code","df4ab80d":"code","fa9a08c9":"code","be29c858":"code","e33ac651":"code","116e2043":"code","b3194ca9":"code","ca965462":"code","d5d68f3e":"code","6dc5eb5c":"code","43938e85":"code","4df3e15b":"code","60cadcff":"code","b380ebe8":"code","2385e709":"code","fce9a1f5":"code","457e4b79":"code","4b4fec66":"code","cef287f4":"code","cb52f1a6":"code","3340f711":"code","b3ee8a28":"code","db99837e":"code","5f185594":"code","a75d9a54":"markdown","8d179a44":"markdown","444cc97e":"markdown","c2ccf87a":"markdown","7b8b46db":"markdown","99bf2cb9":"markdown","9dd4cb44":"markdown"},"source":{"230e0e02":"# import packages\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport os \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom pylab import rcParams\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\n#set random seed\nRANDOM_SEED = 314 \n\nrcParams['figure.figsize'] = 14, 8.7 # Golden Mean\nLABELS = [\"Normal\",\"Fraud\"]\nsns.set()","8b647077":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","df4ab80d":"path = '..\/input'\nname=[]\n\nfor i in os.listdir(path):\n        name.append(i)\nprint(name)    \nfor j in name:\n        new_j = j.replace('.csv','')\n        print('Creating {} DataFrame'.format(new_j))\n        new_j=exec('{} = pd.read_csv(os.path.join(path,j))'.format(new_j))\n        print('Done creating.') #format(new_j)","fa9a08c9":"train_transaction = reduce_mem_usage(train_transaction)","be29c858":"train_transaction.info()","e33ac651":"train_transaction.shape","116e2043":"train_transaction.head()","b3194ca9":"obj_cols = train_transaction.dtypes\nobj_cols[obj_cols=='object']","ca965462":"train_transaction.isna().sum()#check to see if any values are null, which there are not","d5d68f3e":"pd.set_option('precision', 3)\ntrain_transaction.describe()","6dc5eb5c":"#visualizations of time and amount\nplt.figure(figsize=(15,7))\nplt.title('Distribution of Time Feature')\nsns.distplot(train_transaction.TransactionDT)","43938e85":"plt.figure(figsize=(15,7))\nplt.title('Distribution of Monetary Value Feature')\nsns.distplot(train_transaction.TransactionAmt)","4df3e15b":"count = train_transaction.isFraud.value_counts()\nregular = count[0]\nfrauds = count[1]\ntotal= frauds + regular \nperc_reg = (regular\/total)*100\nperc_frauds = (frauds\/total)*100\nprint('There were {} non-fraudulent transactions ({:.3f}%) and {} fraudulent transactions ({:.3f}%).'.format(regular, perc_reg, frauds, perc_frauds))","60cadcff":"plt.figure(figsize=(15,7))\nsns.countplot(x='isFraud',data=train_transaction)\nplt.title('CountPlot Frauds 1 = Positive , 0 = Negative')","b380ebe8":"#Macro on correlations \ncorr = train_transaction.corr()\ncorr","2385e709":"#heatmap\nplt.figure(figsize=(15,7))\nsns.heatmap(corr)\nplt.title('Heatmap correlations Train_data')","fce9a1f5":"frauds = train_transaction[train_transaction['isFraud']==1]","457e4b79":"notfrauds= train_transaction[train_transaction['isFraud']==0]","4b4fec66":"frauds.TransactionAmt.describe()","cef287f4":"notfrauds.TransactionAmt.describe()","cb52f1a6":"#plot of high value transactions\nplt.figure(figsize=(15,7))\nbins = np.linspace(200, 2500, 100)\nplt.hist(notfrauds.TransactionAmt, bins, alpha=1, normed=True, label='Normal')\nplt.hist(frauds.TransactionAmt, bins, alpha=0.6, normed=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Amount by percentage of transactions (transactions \\$200+)\")\nplt.xlabel(\"Transaction amount (USD)\")\nplt.ylabel(\"Percentage of transactions (%)\");\nplt.show()","3340f711":"#train_x, test_x = train_test_split(XX, test_size=0.2, random_state=RANDOM_SEED) xx == Merged dataframe\n#train_x = train_x[train_x.isFraud == 0] #where normal transactions\n#train_x = train_x.drop(['isFraud'], axis=1) #drop the class column\n\n\n#test_y = test_x['isFraud'] #save the class column for the test set\n#test_x = test_x.drop(['isFraud'], axis=1) #drop the class column\n\n#train_x = train_x.values #transform to ndarray\n#test_x = test_x.values","b3ee8a28":"#The magics\nfeats= ['TransactionID',\n 'C14',\n 'C13',\n 'C12',\n 'C11',\n 'C10',\n 'C8',\n 'C7',\n 'C6',\n 'C5',\n 'C4',\n 'C3',\n 'C2',\n 'C1',\n 'C9',\n 'isFraud',\n 'TransactionDT',\n 'TransactionAmt',\n 'ProductCD',\n 'card1']","db99837e":"sns.set()\nplt.figure(figsize=(15,7))\ntrain_transaction[feats].isna().sum().sort_values(ascending=False).plot(kind='barh')","5f185594":"corr_matrix = train_transaction[feats].corr()\ncorr_matrix.isFraud.sort_values(ascending=False)","a75d9a54":"##  ** Lazy way to load data that u can use everywhere:( **","8d179a44":"# Leave your questions in the comments section, and suggest how can I improve this kernel ! ","444cc97e":"![](https:\/\/miro.medium.com\/max\/2400\/1*uke4skoetttEBQDJwnae8g.png)","c2ccf87a":"# This is a starter kernel , that i'll update soon !\n## Please if you found it helpful upvote it !","7b8b46db":"# Presentation:\n> For years hackers would just pick your informations then create their own cards with you informations and their pictures on it ,here's a good movie for curious peeps : https:\/\/www.imdb.com\/title\/tt3173594\/ so **Visa and Mastercard** mandated that banks and merchants introduce EMV \u2014 chip card technology, which made it possible for merchants to start requesting a PIN for each transaction.\nIn addition to the implementation of chip card technology, companies have been investing massive amounts in other technologies for detecting fraudulent transactions.\nWould Machine Learning & AI constitute great allies in this wouldn't say battle , but a war !\n","99bf2cb9":"> Since the fraud cases are relatively few in number compared to bin size, we see the data looks predictably more variable. In the long tail, especially, we are likely observing only a single fraud transaction. It would be hard to differentiate fraud from normal transactions by transaction amount alone.","9dd4cb44":"![z](https:\/\/miro.medium.com\/max\/946\/1*C_UZyG_AeX7h4Zk_9qGQ3w.gif)"}}