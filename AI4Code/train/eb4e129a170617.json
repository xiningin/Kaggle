{"cell_type":{"33e80bf3":"code","db08cd50":"code","c67db811":"code","5bb043e5":"code","0ee77db0":"code","94c895b8":"code","1a1b2c4d":"code","72730e42":"code","a35e6b4b":"code","ddbb9e0d":"code","edd19769":"code","55b67c42":"code","c01ed924":"code","6a56b6ee":"code","b1f88a02":"code","cdb681e8":"code","f2244a5e":"code","97b9e5ef":"code","9560b9dc":"code","2512f4c1":"code","0c647122":"code","63484c64":"code","0544d34b":"code","14e24f24":"code","fc8cba5d":"code","223b3a40":"code","18cdcc8a":"code","247f39cc":"code","ce5d544c":"code","043628f2":"code","8014c728":"code","74f02699":"code","fed3a88d":"code","414977b2":"code","0f016543":"code","a9cf29be":"code","38a9c04e":"code","70feda7f":"code","563e7001":"code","7168b24a":"code","97a16d5f":"code","85ac7289":"code","0749df7e":"code","b94e1ac1":"code","c449858d":"code","893c41fc":"code","b5223698":"code","72f11ac4":"code","5a4dccd6":"code","28090dbe":"code","0bbc89f1":"code","113bd652":"code","26c2ecaf":"code","ac461855":"code","c1fe2727":"code","3ce74025":"code","25b1c199":"code","90afca4c":"code","705196aa":"markdown","8ee09bc2":"markdown","7d10fc55":"markdown","1135f2e3":"markdown","b124ac77":"markdown","87fb6274":"markdown","86f75876":"markdown","edc2f36d":"markdown","50c98350":"markdown","8f2882a8":"markdown","e938d8ed":"markdown","57f18918":"markdown","f9fb1e1c":"markdown","039bf907":"markdown","4acfae56":"markdown","4b96f563":"markdown","b684bb51":"markdown","dfa028eb":"markdown","e3bf1589":"markdown","d0750cc3":"markdown","cf6d1084":"markdown","679793c7":"markdown","a0231266":"markdown","8e55b4d6":"markdown","7cc836f1":"markdown","0f029264":"markdown","57e15919":"markdown","2a379efd":"markdown","4560de32":"markdown","4599e352":"markdown","a5bdb9a0":"markdown","f84bd825":"markdown","c6812765":"markdown","c7abdbc1":"markdown","d6f00a62":"markdown","01fa5473":"markdown","167e4eac":"markdown","95a9ebc9":"markdown","e8458faf":"markdown","75e517f2":"markdown","33a817e3":"markdown","ac545f37":"markdown","4e8c5b3e":"markdown","75404366":"markdown","89420d7d":"markdown","856e1ee4":"markdown","ea8dcb0a":"markdown","369787d2":"markdown","98e046f8":"markdown","3e74dc8f":"markdown","1cec9548":"markdown","4e7a7714":"markdown","9e541f05":"markdown","ab54d5bb":"markdown","fd8ac9ea":"markdown","be3346c6":"markdown","8e3ae1a1":"markdown","a8e2c264":"markdown","27ac8e4c":"markdown","d28b5f9a":"markdown","a8f70f58":"markdown","07e73035":"markdown","3ef86751":"markdown"},"source":{"33e80bf3":"%matplotlib inline","db08cd50":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c67db811":"import imblearn\nprint(imblearn.__version__)","5bb043e5":"# Needed to load Excel source file\n!pip install openpyxl","0ee77db0":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer","94c895b8":"import tensorflow as tf\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout","1a1b2c4d":"dry_beans_dataset = pd.read_excel(\"..\/input\/dry-bean-dataset\/Dry_Bean_Dataset.xlsx\")","72730e42":"dry_beans_dataset.shape","a35e6b4b":"dry_beans_dataset.head()","ddbb9e0d":"dry_beans_dataset.rename(columns = {\"Area\": \"area\", \"Perimeter\": \"perimeter\", \"MajorAxisLength\": \"major_axis_length\", \"MinorAxisLength\": \"minor_axis_length\", \"AspectRation\": \"aspect_ratio\", \"Eccentricity\": \"eccentricity\", \"ConvexArea\": \"convex_area\", \"EquivDiameter\": \"equiv_diameter\", \"Extent\": \"extent\", \"Solidity\": \"solidity\", \"roundness\": \"roundness\", \"Compactness\": \"compactness\", \"ShapeFactor1\": \"shape_factor_1\", \"ShapeFactor2\": \"shape_factor_2\", \"ShapeFactor3\": \"shape_factor_3\", \"ShapeFactor4\": \"shape_factor_4\", \"Class\": \"class\"}, inplace = True)","edd19769":"dry_beans_dataset.head()","55b67c42":"dry_beans_dataset.info()","c01ed924":"# Display correlation matrix\nplt.figure(figsize = (20, 20))\nsns.heatmap(dry_beans_dataset.corr(), \n            annot = True, \n            cmap = \"Blues\",\n            fmt = \".2f\",\n            vmin = -1.00, vmax = 1.00)\nplt.show()","6a56b6ee":"plt.figure(figsize = (8, 5))\nsns.countplot(x = dry_beans_dataset[\"class\"], palette=\"Set2\")\nplt.title(\"Number of beans per type\")\nplt.show()","b1f88a02":"dry_beans_features = dry_beans_dataset.drop([\"class\"], axis = 1)","cdb681e8":"# Plot features value distribution\nfig, axs = plt.subplots(ncols = 4, nrows = 4, figsize = (20, 20))\nindex = 0\n\naxs = axs.flatten()\nfor k, v in dry_beans_features.items():\n    sns.histplot(v, ax = axs[index])\n    index += 1\nplt.tight_layout(pad = 0.4, w_pad = 0.5, h_pad = 5.0)","f2244a5e":"# Display boxplots per feature\nfig, axs = plt.subplots(ncols = 4, nrows = 4, figsize = (20, 20))\nidx = 0\naxs = axs.flatten()\nfor k, v in dry_beans_features.items():\n    sns.boxplot(y = k, data = dry_beans_features, ax = axs[idx])\n    idx += 1\nplt.tight_layout(pad = 0.4, w_pad = 0.5, h_pad = 5.0)","97b9e5ef":"def separate_features_from_labels(dataset):\n    \"\"\"\n    Function: separates features from labels (classes).\n    \n    Parameters: dataset.\n    \n    Returns: tuple of features and labels as np.array.\n    \"\"\"\n    return (dataset.drop(\"class\", axis = 1).values, dataset[\"class\"].values)","9560b9dc":"(features, labels) = separate_features_from_labels(dry_beans_dataset)","2512f4c1":"trval_features, test_features, trval_labels, test_labels = train_test_split(features, labels, test_size = 0.10, stratify = labels, random_state = 42)\n\ntrain_features, val_features, train_labels, val_labels = train_test_split(trval_features, trval_labels, test_size = len(test_labels), stratify = trval_labels, random_state = 42)","0c647122":"train_features.shape, train_labels.shape, val_features.shape, val_labels.shape, test_features.shape, test_labels.shape","63484c64":"def display_class_distribution(indexed_data, dataset, title = None):\n    \"\"\"\n    Function: displays number of items per class.\n    \n    Parameters: indexed data (X.index) and dataset (X).\n    \n    Returns: barchart of number of items per class.\n    \"\"\"\n    plt.figure(figsize = (7, 5))\n    plt.bar(indexed_data, dataset)\n    plt.xlabel(\"Bean type\")\n    plt.ylabel(\"Count\")\n    if title is not None:\n        plt.title(title)\n    plt.show()","0544d34b":"display_class_distribution((pd.DataFrame(train_labels).groupby(0).size()).index, (pd.DataFrame(train_labels).groupby(0).size()), \"Number of beans per type in training data\")","14e24f24":"display_class_distribution((pd.DataFrame(val_labels).groupby(0).size()).index, (pd.DataFrame(val_labels).groupby(0).size()), \"Number of beans per type in validation data\")","fc8cba5d":"display_class_distribution((pd.DataFrame(test_labels).groupby(0).size()).index, (pd.DataFrame(test_labels).groupby(0).size()), \"Number of beans per type in testing data\")","223b3a40":"def min_max_scaler(features):\n    \"\"\"\n    Function: scales values betweeb 0 and 1.\n    \n    Parameters: dataset as an NumPy array.\n    \n    Returns: scaled NumPy array in float32 dtype.\n    \"\"\"\n    max_n = np.max(features)\n    min_n = np.min(features)\n    features_scaled = np.array([(x - min_n) \/ (max_n - min_n) for x in features])\n    return features_scaled.astype(\"float32\")","18cdcc8a":"train_features = min_max_scaler(train_features)\nval_features = min_max_scaler(val_features)\ntest_features = min_max_scaler(test_features)","247f39cc":"train_features.min(), train_features.max(), train_features.dtype","ce5d544c":"train_labels_np = pd.get_dummies(train_labels).to_numpy()\nval_labels_np = pd.get_dummies(val_labels).to_numpy()\ntest_labels_np = pd.get_dummies(test_labels).to_numpy()","043628f2":"print(train_labels_np)","8014c728":"le = LabelEncoder()","74f02699":"train_labels_enc = le.fit_transform(train_labels)\nval_labels_enc = le.fit_transform(val_labels)\ntest_labels_enc = le.fit_transform(test_labels)","fed3a88d":"print(train_labels_enc)","414977b2":"tf.keras.backend.clear_session()","0f016543":"input_shape = train_features.shape[1]","a9cf29be":"model = Sequential([\n    Input(input_shape),\n    Dense(64, activation = \"relu\"),\n    Dropout(0.1),\n    Dense(128, activation = \"relu\"),\n    Dropout(0.1),\n    Dense(256, activation = \"relu\"),\n    Dropout(0.1),\n    Dense(512, activation = \"relu\"),\n    Dropout(0.1),\n    Dense(256, activation = \"relu\"),\n    Dropout(0.1),\n    Dense(128, activation = \"relu\"),\n    Dropout(0.1),\n    Dense(64, activation = \"relu\"),\n    Dense(7, activation = \"softmax\")\n])","38a9c04e":"model.summary()","70feda7f":"model.compile(loss = \"categorical_crossentropy\",\n             optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001),\n             metrics = [\"accuracy\"])","563e7001":"history = model.fit(train_features, train_labels_np,\n         epochs = 50,\n         validation_data = (val_features, val_labels_np), verbose = 0)","7168b24a":"# Plot loss dynamics \nplt.plot(history.history[\"loss\"], label = \"loss\")\nplt.plot(history.history[\"val_loss\"], label = \"validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Change of training and validation loss over time\")\nplt.show()","97a16d5f":"# Plot accuracy dynamics\nplt.plot(history.history[\"accuracy\"], label = \"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Change of training and validation accuracy over time\")\nplt.show()","85ac7289":"model_performance = model.evaluate(test_features, test_labels_np)","0749df7e":"print(f\"Model loss on testing data is {model_performance[0]}\")\nprint(f\"Model accuracy on testing data is {model_performance[1]*100}%\")","b94e1ac1":"f1 = make_scorer(f1_score , average = \"weighted\")","c449858d":"params_ada = {\n    \"n_estimators\": [5, 10, 15, 20],\n    \"learning_rate\": [0.4, 0.6, 0.8, 1.0]\n}","893c41fc":"gs_ada = GridSearchCV(AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 10, n_estimators = 20, random_state = 42)), param_grid = params_ada, scoring = f1, cv = 4, n_jobs = -1)","b5223698":"gs_ada.fit(train_features, train_labels_enc)","72f11ac4":"gs_ada.cv_results_","5a4dccd6":"gs_ada.best_params_","28090dbe":"ada = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 10, n_estimators = 20, random_state = 42), n_estimators = 20, learning_rate = 0.6, random_state = 42)","0bbc89f1":"def classification_task(estimator, attributes, labels):\n    \"\"\"\n    Function: \"fit\", \"predict\" and \"score\" values of an estimator.\n    \n    Parameters: estimator, attributes (X) and labels (y).\n    \n    Returns: model's performance measured as accuracy and f1_score.\n    \"\"\"\n    estimator.fit(attributes, labels)\n    predictions = estimator.predict(attributes)\n    \n    print(f\"Accuracy: {accuracy_score(labels, predictions)}\")\n    print(f\"F1 score: {f1_score(labels, predictions, average = 'weighted')}\")","113bd652":"# Model performance on training data\nclassification_task(ada, train_features, train_labels_enc)","26c2ecaf":"# Model performance on validation data\nclassification_task(ada, val_features, val_labels_enc)","ac461855":"# Model performance on testing data\nclassification_task(ada, test_features, test_labels_enc)","c1fe2727":"predicted = ada.predict(test_features)","3ce74025":"print(classification_report(test_labels_enc, predicted))","25b1c199":"# Plot confusion matrix\nplt.figure(figsize = (8, 7))\nsns.heatmap(confusion_matrix(test_labels_enc, predicted),\n            annot = True,\n            fmt = \".0f\",\n            cmap = \"coolwarm\",\n            linewidths = 2, \n            linecolor = \"white\",\n            xticklabels = ada.classes_,\n            yticklabels = ada.classes_)\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Confusion matrix on the testing data\")\nplt.show()","90afca4c":"le.classes_","705196aa":"Model's performance is evaluated on all three sets. To avoid repeating one and the same computations, the relevant code is organised into function.","8ee09bc2":"#### 4.2. Train the model","7d10fc55":"Classification report (see below) shows all metrics per class. On the testing set, all 7 bean types got \"f1_score\" between 99% and 100%, which is much better than the dataset's authors achived with Support Vector Classifier. ","1135f2e3":"#### 5.1. Build AdaBoost classifier","b124ac77":"The plots below show that the model couldn't converge. Both training and validation loss stopped to fall after the 30th epoch. Accuracy barely reached 50% on the validation set.","87fb6274":"#### 4.3. Evaluate the model","86f75876":"#### 2.4. Display beans per type","edc2f36d":"A brief check confirms the scaling was successful.","50c98350":"Grid Search is performed with the training data. Calling `cv_results_` on it returns full report for all folders.","8f2882a8":"It could be seen from the DataFrame above that all but the last (\"class\") features hold numeric values. It is explained that **area (A)** is the area of a bean zone and the number of pixels within its boundaries; **perimeter (P)** is bean circumference defined as the length of its border; **major_axis_length (L)** shows the distance between the ends of the longest line that can be drawn from a bean; **minor_axis_length (l)** shows the longest line that can be drawn from the bean while standing perpendicular to the main axis; **aspect_ratio (K)** defines the relationship between L and l. **eccentricity (Ec)** is the egccentricity of the ellipse having the same moments as the region; **convex_area (C)** shows the number of pixels in the smallest convex polygon that can contain the area of a bean seed; **equivalent_diameter (Ed)** is the diameter of a circle having the same area as a bean seed area; **extent (Ex)** is the ratio of the pixels in the bounding box to the bean area; **solidity (S)**, also known as convexity, shows the ratio of the pixels in the convex shell to those found in beans; **roundness (R)** is calculated with the following formula: (4piA)\/(P^2); **compactness (CO)** measures the roundness of an object: Ed\/L; **shape_factor_1 (SF1)**; **shape_factor_2 (SF2)**; **shape_factor_3 (SF3)**; **shape_factor_4 (SF4)**; and **class**: Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira. ","e938d8ed":"### Imports","57f18918":"It is interesting to note that a strong (linear) correlation is observed between some features. For example, between \"convex_area\" and \"area\", and between \"compactness\" and \"shape_factor_3\". This is not a surprise, since the \"area\" is tightly related to the \"convex_area\". It is not quite clear how \"shape_factor_3\" (as well as the other \"shape factors\") is calculated but it might be assumed that beans' \"compactness\" might played a role.\n\nAs a whole, there is a strong (both positive and negative) correlation between features. An attempt was made for removing the most correlated ones but less features didn't affect the Neural Network's performance. Therefore, the final model works with all features.","f9fb1e1c":"#### 3.3. Normalize feature values","039bf907":"#### 2.5. Display distribution of values in each feature","4acfae56":"Almost 350 000 trainable parameters for a dataset of 13000 samples with almost straightforward data seems too much. Nonetheless, it is trained and evaluated.\nMulticlass problems require \"categorical_crossentropy\" loss function; model's performance is evaluated for its \"accuracy\". Gradient Descent is computed with \"Adam\" optimizer with reduced learning rate (for better results).","4b96f563":"Labels are removed from the dataset before displaying values distribution. The features are stored in a new variable `dry_beans_features`.","b684bb51":"Also, it is important to check if all three sets hold similar number of samples per category. The function below displays distribution of labels in each dataset.","dfa028eb":"#### 2.6. Check for outliers","e3bf1589":"## 2. Data Cleaning and Exploratory Data Analysis","d0750cc3":"## 4. Build, train, and evaluate a classifier with a Neural Network","cf6d1084":"Labels are encoded with `sklearn`'s class `LabelEncoder()` for training with the Ensemble algorithm. It returns the labels in different shape, which is not appropriate for Neural Networks.","679793c7":"#### 3.5. Encode labels (for Ensemble algorithm)","a0231266":"## Conclusion","8e55b4d6":"In conclusion, it could be said that the explored Neural Network configurations couldn't learn the relationships between Dry Bean features. Nonetheless, AdaBoost is capable to distinguish and classify all 7 sorts much better than the algorithms employed by the authors of Dry Bean Dataset.","7cc836f1":"Misclassified objects are displayed on the confusion matrix below. All beans of type \"1\" (\"Bombay\") were properly classified, as well as most of the others. Only 2 \"Sira\" beans (class \"6\") were wrongly declared \"Dermason\".","0f029264":"The Dry Bean Dataset is loaded and stored in `dry_beans_dataset`. A brief check confirms that it has 13611 rows and 17 features","57e15919":"# Comparing classifiation of Dry Bean dataset with a Neural Network and with an Ensemble Algorithm","2a379efd":"The plots above show that values in some features have similar distribution (e.g., \"area\", \"perimeter\", \"major\" and \"minor\" axis length), whereas others are completely different. For example, neither \"shape factor\" looks similar to another. Some distributions are, more or less, bi- or trimodal, whereas others have only one mode. Distribution shapes of some features suggest existence of outliers. ","4560de32":"It is very important to check datasets' shape; model training is impossible if they are not in the proper form. ","4599e352":"[1] KOKLU, M. and OZKAN, I.A., (2020), Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques. Computers and Electronics in Agriculture, 174, 105507. web link: https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0168169919311573?via%3Dihub","a5bdb9a0":"Both Neural Networks and Machine Learning algorithms expect to get scaled features. Thus, the function below makes all feature values in the range between 0 and 1 (it does the same as `Min_Max_Scaler` of `sklearn`).","f84bd825":"The code lines below show that the model's accuracy on the testing set is 55%, which is quite unsatisfactory.","c6812765":"Both \"f1_score\" and \"accuracy\" reach more than 95% on the training set. They are even higher on the validation and testing data.","c7abdbc1":"The net gets training and validation data, which are passed through it for 50 epochs.","d6f00a62":"A brief check confirms that all features hold numeric values (both *int* and *float* type), and only those in the last column are \"object\" type. There are not missing values in the dataset.","01fa5473":"#### 5.3. Evaluate AdaBoost Classifier","167e4eac":"The best combination is: \"learning rate of 0.6\" with \"maximum number of estimators\". These are fed to the algorithm.","95a9ebc9":"#### 2.3. Check correlation between features","e8458faf":"Imbalanced data impeded \"manual\" stratified splitting into training, validation, and training sets. For this reason, although highly undesirable for Deep Learning models, `train_test_split` of `sklearn` was used instead. Prior to that, however, features and labels (\"class\") were separated.","75e517f2":"`SMOTE` module of `imblearn.over_sampling` was used to address imbalanced data. It \"added\" more samples to the under-sampled classes so as the dataset had equal number of bean categories at the end. The number of samples reached more than 20000 but this endeavour didn't improve the Neural Network's performance. Therefore, it was abandoned.","33a817e3":"#### 5.2. Train AdaBoost Classifier","ac545f37":"Column titles, however, do not meet Python convention (see below). All words should be with small letters, and space should be replaced with underscore. Therefore, the first task is to make row headings Pythonic style.","4e8c5b3e":"The Neural Network would expect numeric values; so far, \"labels\" hold *strings* of bean types. Therefore, they are converted into a NumPy matrix.","75404366":"The plot below shows that the dataset is quite imbalanced. Some beans are over-sampled (e.g., \"Dermason\"), whereas others - under-sampled (e.g., \"Bombay\").","89420d7d":"## Introduction","856e1ee4":"#### 3.2. Display beans class distribution","ea8dcb0a":"#### 3.4. Convert labels to NumPy matrix (for NN)","369787d2":"## 3. Data pre-processing","98e046f8":"The dots outside the boxplots below indicate a lot of outliers in the data. Those were removed (from all features) but the Neural Network didn't performed better with \"cleaned\" data. Also, it turned out that one of the bean types (Bombay) is significantly larger that the others, and the whole class was removed as an \"outlier\" (which negatively affected the whole classification task). Therefore, the final model works with the whole dataset, as created by its authors. ","3e74dc8f":"Various architectures were tested but neither returned an accuracy higher than 62%-64%. Number of neurons were increased, thereafter decreased. More layers were added, then removed. This is a relatively complex model for this small dataset. The final version has Dropout layers, which help for avoiding overfitting on the training data.","1cec9548":"To avoid clutter from existing models and layers (when model is being fune-tuned several times), especially when memory is limited, `clear_session()` resets all prior state generated by Keras.","4e7a7714":"The main AdaBoost hyper-parameters are the number of estimators (i.e., classifiers), and the learning rate. These are placed in a dictionary, which is passed to the GridSearch function. The latter will compute \"f1_score\" and other metrics for all possible combinations and will show the best performing one. Number of estimators (i.e., trees) and their maximum depth are RandomForest's hyper-parameters. Search space would increase dramatically if these should be computed, too. For this reason, Grid Search is executed only for AdaBoost. It is assumed that a depth of 10 \"levels\" for 20 trees will return sufficiently high \"f1_score\".","9e541f05":"#### 4.1. Build the model","ab54d5bb":"\"Adaptive Boosting\", known as \"AdaBoost\", \"is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases\".\n\nIt recieves three main parameters, which are used for grid search. The first one is \"base_estimator\" - a type of Classifier. I chose `RandomForestClassifier()` since it is one of the strongest classifiers. \"n_estimators\" indicate the \"maximum number of estimators at which boosting is terminated\". The higher the \"learning_rate\", the faster the training. There is a trade-off between learning_rate and n_estimators.\"\n\nModel's performance is evaluated for \"accuracy\" and \"f1_score\" (weighted average of \"precision\" and \"recall\"). The latter is instantiated for the GridSearch.","fd8ac9ea":"First, the dataset is split into \"train and validation\" and \"testing\" sets; thereafter, the same operation is applied for splitting \"training\" and \"validation\" data. Test size is set to 10%; it is better to have more samples for training. Size of validation data is the same as that of the testing one. Labels are used for stratifying the samples into training, validation, and testing ones.","be3346c6":"## 5. Build, train, and evaluate an AdaBoost classifier","8e3ae1a1":"#### 3.1. Train - validation - test  split","a8e2c264":"#### 2.1. Rename features","27ac8e4c":"## 1. Load the Dataset","d28b5f9a":"## References","a8f70f58":"#### 2.2. Check data types and for missing values","07e73035":"This Notebook explores if and to what extent beans in the Dry Bean dataset could be classified with a Deep Neural Network and\/or with an Ensemble ML Algorithm. \n\nDry Bean dataset could be downloaded from UC Irvine Machine Learning Repository at [this link](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Dry+Bean+Dataset). It is an Excel file with image data of 13611 beans categorised in 7 different types. The authors[1] took into account features such as form, shape, type, and structure by the market situation. It is explained that a computer vision system was developed to distinguish the seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. Their classification model is based on all 13611 bean images. The dataset has 16 features: 12 describing dimensions and 4 - their shape form.\n\nAccording to paper's abstract, the authors explored various classification algorithms. They achived 91.73%, 93.13%, 87.92% and 92.52% correct classification rates for Multilayer perceptron, Support Vector Machine (SVM), k-Nearest Neighbors and Decision Tree, respectively. The SVM classification model, which got the highest accuracy results, has classified the Barbunya, Bombay, Cali, Dermason, Horoz, Seker and Sira beans with 92.36%, 100.00%, 95.03%, 94.36%, 94.92%, 94.67% and 86.84% accuracy, respectively. \n\nAn attempt was made here for reproducing these results and\/or for obtaining higher scores. The Deep Neural Network barely reached more than 60% accuracy. The Adaptive Boosting classifier, however, scored 98%-100% accuracy both on the validation and testing sets, for all bean types.\n\nThe work is organised in chapters. Unsuccessful operations and models are described but their implementation code is omitted.","3ef86751":"Everything seems alright: all three sets have similar samples distribution (in terms of the bean type they belong to)."}}