{"cell_type":{"309fed5a":"code","a984c927":"code","80f43445":"code","d25946bd":"code","09f2ff92":"code","4eaed67e":"code","3cbf8846":"code","98b83d7e":"code","f1619274":"code","06355eed":"code","ddf107cb":"code","b746d8e0":"code","b014f541":"code","489e7047":"code","c5416602":"code","c118d75a":"code","da97919c":"code","659daa62":"code","d268d52b":"code","ef9e10e3":"code","78e5c954":"code","702ce89f":"code","3ef46d59":"code","84d15754":"code","8068db01":"code","beaea3c0":"code","2432eb55":"code","6712ff44":"code","0e40211b":"code","7cf9c621":"code","5f2d0223":"code","f413a92b":"code","14094e1e":"code","db80c39e":"code","3f3a7f12":"code","e63f24e0":"code","db0933f6":"code","2d75473d":"code","ecd9b9a1":"markdown","3644503b":"markdown","da9021e5":"markdown","012b1946":"markdown"},"source":{"309fed5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a984c927":"!pip install pyspark","80f43445":"import pyspark","d25946bd":"from pyspark.sql import SparkSession","09f2ff92":"spark= SparkSession.builder.appName('dataframe').getOrCreate()","4eaed67e":"spark","3cbf8846":"# Reading the files by pyspark\ndf_pyspark=spark.read.csv('\/kaggle\/input\/guess-the-product\/test_set.csv' , header=True, inferSchema=True)","98b83d7e":"# Another way of Reading the files by pyspark\ndf_pyspark1=spark.read.option('header', True).csv('\/kaggle\/input\/guess-the-product\/test_set.csv')","f1619274":"type(df_pyspark1)","06355eed":"df_pyspark.printSchema()","ddf107cb":"df_pyspark.show()","b746d8e0":"df_pyspark.head(5)","b014f541":"# Select the column name \ndf_pyspark.select(\"Vendor_Code\").show()","489e7047":"# Select the two column from dataframe  \n\ndf_pyspark.select([\"Vendor_Code\",\"GL_Code\"]).show()","c5416602":"df_pyspark.dtypes","c118d75a":"df_pyspark.describe().show()","da97919c":"# Adding Columns name in data Frame\ndf_pyspark.withColumn(\"invoice with adding 100\", df_pyspark[\"Inv_Amt\"]+100).show()","659daa62":"# Drop the columns\ndf_pyspark.drop(\"invoice with adding 100\").show()\n","d268d52b":"## Rename Columns\ndf_pyspark.withColumnRenamed(\"Item_Description\",\"new_Item_Description|\").show()","ef9e10e3":"# importing pandas as pd\nimport pandas as pd\n  \n# importing numpy as np\nimport numpy as np\n  \n# dictionary of lists\ndict = {'First Score':[100, 90, np.nan, 95],\n        'Second Score': [30, 45, 56, np.nan],\n        'Third Score':[np.nan, 40, 80, np.nan]}\n  \n# creating a dataframe from list\ndf = pd.DataFrame(dict)","78e5c954":"# Converting Pandas datafrmae to spark\ndf = spark.createDataFrame(df)","702ce89f":"df.show(5)","3ef46d59":"df.na.drop().show()","84d15754":"# any = How \ndf.show(5)","8068db01":"# any one na value in that column then it will drop\ndf.na.drop(how=\"any\").show()","beaea3c0":"# Threshold is 2 any row with maximum two null values will get deleted \ndf.na.drop(how=\"any\", thresh=2).show()","2432eb55":"#Subset ie only from specfic column in this we have selected as the column as  Third Score\ndf.na.drop(how=\"any\",subset=[\"Third Score\"]).show()\n","6712ff44":"# Filling the missing values\ndf.na.fill(\"Missing Values\").show()","0e40211b":"df_pyspark.filter(\"Inv_Amt<4\").show()","7cf9c621":"df_pyspark.filter(\"Inv_Amt<4\").select([\"Inv_Id\",\"Vendor_Code\"]).show()","5f2d0223":"#Another way to write the filter\ndf_pyspark.filter(df_pyspark[\"Inv_Amt\"]<4).show()","f413a92b":"#Another way to write the filter with not condition (inverse operation)\ndf_pyspark.filter(~(df_pyspark[\"Inv_Amt\"]<98)).show()","14094e1e":"#Multiple condition\ndf_pyspark.filter((df_pyspark[\"Inv_Amt\"]<4) & (df_pyspark[\"Inv_Id\"]<16000)).show()","db80c39e":"## Groupby sum of invoice amounta\ndf_pyspark.groupBy(\"GL_Code\").sum(\"Inv_Amt\").show()","3f3a7f12":"## Groupby mean value\ndf_pyspark.groupBy(\"GL_Code\").mean(\"Inv_Amt\").show()","e63f24e0":"## Groupby max value\ndf_pyspark.groupBy(\"GL_Code\").max(\"Inv_Amt\").show()","db0933f6":"## Groupby count\ndf_pyspark.groupBy(\"GL_Code\").count().show()","2d75473d":"# aggregate sum of invoice amount\ndf_pyspark.agg({\"Inv_Amt\":\"sum\"}).show()","ecd9b9a1":"# Pyspark\n* Pyspark DataFrame \n* Reading Dataset \n* Checking the datatypes of the column\n* Slecting the column and Indexing \n* Checking describe option similar to pandas\n* Adding columns\n* Droping columns\n* Renaming the columns\n","3644503b":"# Pyspark GroupBy And Aggregate Function ","da9021e5":"# Filter operation","012b1946":"# Pyspark Handling Missing Values\n*  Dropping Columns\n*  Dropping Rows\n* Various Parameter in Dropping Functionality \n* Handling missing value by mean"}}