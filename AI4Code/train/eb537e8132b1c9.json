{"cell_type":{"0decbae5":"code","5922854a":"code","62ca3f02":"code","29a67887":"code","ef02ff24":"code","bf0de9e1":"code","2cacdc6b":"code","2a23af20":"code","87088dc2":"code","f075cf9c":"code","79230a7c":"code","4abc6486":"code","489245c2":"code","cf2321e5":"markdown","8eb457db":"markdown","82b58fff":"markdown","d42f63cf":"markdown","bfa23917":"markdown","49d7397e":"markdown","65e82d41":"markdown","5bde172d":"markdown","b8e50e18":"markdown"},"source":{"0decbae5":"import numpy as np\nimport pandas as pd\nimport re\nimport random\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve\nfrom scipy.sparse import csr_matrix\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport matplotlib\nimport matplotlib.colors as mc","5922854a":"# fictitious food reviews\ndocs = ['The pizza is great!',\n       'This is amazingly great pizza.',\n       'This is amazingly good pizza.',\n       'This pizza is terribly good.',\n       'These are terrible olives.',\n       'These olives are terrible.',\n       'These are amazingly gross olives.',\n       'Olives are amazingly terrible!']\ny = [1,1,1,1,0,0,0,0]","62ca3f02":"# basic text processing\n# omit stop words and apply stemming\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english') + ['this', 'that', 'these'])\ndef process_text(text):\n    # Lower case\n    text = [ t.lower() for t in text.split(' ') ]\n    # Drop punctuation and numerics\n    text = [ re.sub(r'[^A-Za-z]', '', t) for t in text ]\n    # Drop stopwords\n    text = [ t for t in text if t not in stop_words ]\n    # Apply porter stemming\n    text = ' '.join([ stemmer.stem(t) for t in text ])\n    return text\n    \ndocs_processed = [ process_text(t) for t in docs ]\n\nctv = CountVectorizer()\ndoc_term_matrix = ctv.fit_transform(docs_processed)\n\nx = pd.DataFrame(doc_term_matrix.todense(),\n                 columns = ctv.get_feature_names())","29a67887":"# Some adjustements for the sake of visualization\ndf = x.copy()\ndf = df.apply(lambda x: x\/3)\ndf['target'] = [1 if x == 0 else 0.66 for x in y]\ndf.index = docs\n\ncols = [\"#ffffff\", \"#000000\", \"#e74c3c\", \"#2ecc71\"]\ncm = sns.color_palette(cols)\n\nfig, axarr = plt.subplots(1, 1, figsize=(10, 4))\nsns.heatmap(df[['target'] + list(df.columns[:-1])],\n            cmap=cm,\n            linecolor='white',\n            linewidths=2,\n            cbar=False)\n_ = plt.title('Bag of Words')","ef02ff24":"class NBTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Implementation of Wang, S. and Manning C.D. Baselines and Bigrams: \n    Simple, Good Sentiment and Topic Classification (2012)\n    https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf\n    \n    Adapted from: \n        Jeremy Howard Machine Learning for Coders Lesson 11\n        Kaggle @jhoward https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline\n        Kaggle @Ren https:\/\/www.kaggle.com\/ryanzhang\/tfidf-naivebayes-logreg-baseline\n    \"\"\"    \n    def __init__(self, alpha=1):\n        self.r = None\n        self.alpha = alpha\n\n    def fit(self, X, y):\n        # Convert X matrix to 1's and 0's\n        X = csr_matrix(X)\n        y = csr_matrix(y).T\n        y_not = csr_matrix(np.ones((y).shape[0])).T - y\n        # compute smoothed log prob ratio\n        # use sparse pointwise multiplication, more memory efficient\n        # than slicing\n        p = self.alpha + X.multiply(y).sum(0)\n        q = self.alpha + X.multiply(y_not).sum(0)\n        # computed using L1 norm per Wang and Manning\n        # (difference is accounted for by logistic regression bias term in otherwise)\n        # adjusted smoothing to be equivalent to sklearn MultinomialNB using vocab size\n        # (minor deviation from Wang and Manning)\n        self.r = csr_matrix(np.log(\n            (p \/ (self.alpha * X.shape[1] + X.multiply(y).sum())) \/\n            (q \/ (self.alpha * X.shape[1] + X.multiply(y_not).sum()))\n        ))\n        return self\n\n    def transform(self, X, y=None):\n        return X.multiply(self.r)\n    \n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return X.multiply(self.r)","bf0de9e1":"doc_term_matrix_nb = NBTransformer().fit_transform(doc_term_matrix,y)\ndf = pd.DataFrame(doc_term_matrix_nb.todense(),\n                 columns = ctv.get_feature_names())\ndf['target'] = [df.min().min() if x == 0 else df.max().max() for x in y]\ndf.index = docs\n\ncols = sns.diverging_palette(145, 280, s=85, l=25, n=10000)\ncm = sns.color_palette(cols)\n\nfig, axarr = plt.subplots(1, 1, figsize=(10, 4))\nsns.heatmap(df[['target'] + list(df.columns[:-1])],\n            cmap=cm,\n            linecolor='white',\n            linewidths=2)\n_ = plt.title('Naive Bayes Scaled Bag of Words')","2cacdc6b":"df = pd.read_csv('..\/input\/comments.csv', header=None, names=['comment', 'rating'])\n\n# At least a 7 sounds like a good place to be.\nhotness_threshold = 7\ndf['hot'] = df.rating.apply(lambda x: 1 if x >= hotness_threshold else 0)\n\n# find and remove comments that contain only a score (e.g. \"8\/10\" or \"8\/10.\")\ndf['score_only'] = df.comment.apply(lambda x: re.match(r\"\\d*[\\.]?\\d\\\/\\d{1,2}[\\.]?$\",x) is not None)\ndf = df.loc[np.logical_not(df['score_only']),:].reset_index()\ndf = df.drop('score_only', axis=1)\n\nprint('{}% rated attractive (threshold score of {})'.format(int(100*df.hot.mean()), hotness_threshold))\n\n_ = sns.distplot(df.rating,\n             bins=range(0,11),\n             kde=False,\n             hist_kws=dict(edgecolor=\"w\", linewidth=2))\nplt.title('Distribution of Ratings')\n_ = plt.axvline(7, 0,100000, color='r')","2a23af20":"# Again applying just some basic preprocessing\ndef preprocessing(doc):\n    # if the entirety of the comment is a numeric rating (e.g. 3\/10),\n    # replace with identifier token for use as feature\n    if re.match(r\"\\d*[\\.]?\\d\\\/\\d{1,2}[\\.]?$\", doc) is not None:\n        return 'NO_COMMENT'\n    # space non-alpha characters\n    doc = re.sub('([^a-zA-Z]+)', r' ', doc)\n    # lowercase + drop punc \/ numbers \/ special characters\n    tokens = [token.lower() for token in doc.split(' ') if token.isalpha()]\n    # some comments are only a rating score, lets identify those\n    return ' '.join(tokens)\n\n# Create bag of words including bigrams\ncv = CountVectorizer(preprocessor=preprocessing,\n                     stop_words='english',\n                     analyzer='word',\n                     ngram_range=(1,2),\n                     max_df=0.9999,\n                     min_df=0.0001)\n\nx = cv.fit_transform(df.comment)\ny = df.hot","87088dc2":"%%time \nnb = MultinomialNB()\nkf = KFold(n_splits = 4)\n\nscores_nb = []\nfor it, iv in kf.split(x):\n    nb.fit(x[it,:], y[it])\n    probs = nb.predict_proba(x[iv,:])\n    scores_nb.append(average_precision_score(y[iv], probs[:,1]))\n    \nfpr_nb, tpr_nb, thresholds = roc_curve(y[iv], probs[:,1])\npre_nb, rec_nb, thresholds = precision_recall_curve(y[iv], probs[:,1])","f075cf9c":"%%time \nlgr = LogisticRegression(solver='liblinear', \n                         class_weight='balanced',\n                         C=0.5)\n\nscores_lgr = []\nfor it, iv in kf.split(x):\n    lgr.fit(x[it,:], y[it])\n    probs = lgr.predict_proba(x[iv,:])\n    scores_lgr.append(average_precision_score(y[iv], probs[:,1]))\n    \nfpr_lgr, tpr_lgr, thresholds = roc_curve(y[iv], probs[:,1])\npre_lgr, rec_lgr, thresholds = precision_recall_curve(y[iv], probs[:,1])","79230a7c":"%%time \n# Hybrid model\npipeline = Pipeline([('NB', NBTransformer()),\n                     ('Logistic Regression', LogisticRegression(solver='liblinear', \n                                                                class_weight='balanced',\n                                                                C=0.5))])\n\nscores_nblgr = []\nfor it, iv in kf.split(x):\n    pipeline.fit(x[it,:], y[it])\n    probs = pipeline.predict_proba(x[iv,:])\n    scores_nblgr.append(average_precision_score(y[iv], probs[:,1]))\n    \nfpr_nblgr, tpr_nblgr, thresholds = roc_curve(y[iv], probs[:,1])\npre_nblgr, rec_nblgr, thresholds = precision_recall_curve(y[iv], probs[:,1])","4abc6486":"print(\"Naive-Bayes: \\t\\t\\t\\t{}\".format(round(np.mean(scores_nb),4)))\nprint(\"Logistic Regression: \\t\\t\\t{}\".format(round(np.mean(scores_lgr),4)))\nprint(\"Naive-Bayes + Logistic Regression: \\t{}\".format(round(np.mean(scores_nblgr),4)))","489245c2":"fig, axarr = plt.subplots(1, 2, figsize=(16, 4))\nplt.subplot(121)\nplt.plot(fpr_nb, tpr_nb, label='NB')\nplt.plot(fpr_lgr, tpr_lgr, label='LGR')\nplt.plot(fpr_nblgr, tpr_nblgr, label='NB+LGR')\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.title('ROC Curve')\n\nplt.subplot(122)\nplt.plot(rec_nb, pre_nb, label='NB')\nplt.plot(rec_lgr, pre_lgr, label='LGR')\nplt.plot(rec_nblgr, pre_nblgr, label='NB+LGR')\nplt.xlabel('recall')\nplt.ylabel('precision')\nplt.title('Precision-Recall Curve')\n_ = plt.legend(loc='best')","cf2321e5":"This NBTransformer class takes a document term matrix (X) and labels (y), and scales X by the log of the ratio of the probability of the word given class 1 versus the probability of the word given class 0. For each word (feature), $p$ is count of the word in class 1, $\\lVert p \\lVert_1$ is the total of all words in class 1, and $q$ is the count of the word in class 0. The transformer class includes a smoothing parameter $\\alpha$, for details see the [MultinomialNB User Guide](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html#multinomial-naive-bayes).\n$$ r = log \\left( \\frac{\\frac{p}{\\lVert p \\lVert_1 }}{\\frac{q}{\\lVert q \\lVert_1}}\\right) = log \\left(\\frac{P(X|C=1)}{P(X|C=0)}\\right) $$","8eb457db":"After applying some very basic preprocessing, a document term matrix including bigrams was created using CountVectorizer. We then fit Naive-Bayes and Logistic Regression for comparison purposes with 4-fold cross-validation. Finally, we fit the hybrid Naive-Bayes using a pipeline.","82b58fff":"After applying some basic text preprocessing (stemming, stop words, lowercase, drop punctuation, drop numeric), the corpus is converted into a bag of words document term matrix. What we can see is that  'pizza', 'good' and 'great' are only found in positive reviews, while 'olives' (which are revolting) and 'gross' are only found in negative reviews. 'Terrible' is found in both good and bad reviews, but most often in bad. 'Amazingly' is evenly distributed amongst good and bad reviews. Intuitively 'olive' and 'pizza' are the strongest signals for classification, while 'amazingly' doesn't really impart much useful information at all.","d42f63cf":"# Hot or Not? NB+LGR Hybrid NLP Baseline\n\nWhen embarking on a new modeling project it is always wise to start out with a nice, reliable baseline. A good baseline model should be quick to fit, have just a few hyperparameters, and be fairly stable. The idea  is to set a lower-bound on your performance metrics before you start in on the heavy-compute time models. That way if the accuracy of your RNN is dramatically worse than your baseline, you'll know that you need to investigate what's going on before you invest a lot of time on parameter tuning.\n\nFor NLP problems [Jeremy Howard](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline), former President and Chief Scientist at Kaggle, suggests using a technique proposed by [Wang & Manning (2012)](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf) combining Naive-Bayes with a linear classifier. The method is pretty neat and while it typically doesn't stand up to the latest-and-greatest deep learners, it has at times shown a significant improvement over Naive-Bayes or Logistic Regression alone.\n\nForming generative and discriminative hybrid models is an interesting approach. Generative classifiers assume some functional form for $P(X|Y)$ and $P(Y)$ then use Bayes rule to calculate $P(Y|X)$ and assign labels. Discriminative classifiers assume a functional form for $P(Y|X)$ and then estimate parameters directly from the data. Discriminant models are generally expected to yield better results, but are prone to overfitting, Generative models can outperform discriminant ones if the assumed generative function is correct. Feeding the results of a generative model into a discriminative one can smooth errors, providing a form of regularization by scaling discriminant inputs by a prior distribution. \n\nIn this kernel we'll take a closer look at how this technique works on a toy example and then apply it to a real Kaggle dataset to see it in action.","bfa23917":"## Conclusion\nAlas, no big winner here, the hybrid model only _very slightly_ outperformed Logistic Regression as measured by average precision. We can also see that the ROC and Precision-Recall curves are almost exactly overlapping.","49d7397e":"That said, the hybrid model took negligibly longer to fit and technically _did_ perform better. It certainly meets the essential criteria for a baseline model, it is a fast, easy and reliable addition to the tool kit. Definitely worth trying out next time you start a new NLP competition.\n\nDo you have an example dataset with more dramatic results? I'd love to know about it!","65e82d41":"## How it works with a toy example\n\nLets take a look at how using Naive-Bayes to pre-transform the data works using a toy example before applying this technique to a real data set. Here are eight food reviews and associated labels. Looks like our reviewers were focused on eating pizza (yum!) and olives (ew!).","5bde172d":"## Applied to Real Data\n\nNext we'll apply the hybrid model to some data scraped by [@milesh1](https:\/\/www.kaggle.com\/milesh1) from the [Reddit Rateme](https:\/\/www.reddit.com\/r\/Rateme\/). On this subreddit users can submit pictures other users rate their attractiveness out of 10 ([they're real good humans, Brent](https:\/\/twitter.com\/dog_rates)). \nFor the purposes of this experiment we'll try to predict scores of 7 or up, because 7 sounds like a very respectable hotness score.","b8e50e18":"We end up with the scaled document term matrix below. Here we can see that 'amazingli' is has disappeared, the counts were reduced to nearly zero (not quite zero because of the slight word imbalance between classes). 'Pizza' and 'olive' are now the (absolute) largest values, as expected. And 'terrible' suggests a negative review, but is a weaker signal than the word 'olive'. \n\nThis fits exactly with the intuition discussed above. We have effectively created a prior that we can feed directly into the linear classifier of our choosing, such as logistic regression or SVC. The final results will be in line with the Naive-Bayes classifier _unless_ the linear classifer has a strong reason to override Naive-Bayes because due to error. Voila! Now lets try this out on some real data!"}}