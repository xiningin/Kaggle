{"cell_type":{"ace318ef":"code","c27eea56":"code","bcf6b128":"code","0ac62c78":"code","a104e200":"code","57130e37":"code","3d7cfa83":"code","af23605b":"code","6a918e91":"code","9d8eac1a":"code","4559975a":"code","b28d036f":"code","b3d19968":"code","952f4492":"code","1525e66c":"code","292296e8":"code","5d83ee47":"code","8c92e2b7":"code","3815d619":"code","cbf40172":"code","7e37a720":"code","907a546c":"code","44ca7e95":"code","c2a1f85b":"code","63124ff9":"code","5f5cba78":"code","51dd2242":"code","40a69e98":"code","2d21e859":"markdown","ab1e9597":"markdown","73cf52c4":"markdown","3f59bd0c":"markdown","d4b92468":"markdown","b1e85103":"markdown","57d3aacd":"markdown","622189bd":"markdown","be60dad9":"markdown","e8930b83":"markdown","66853290":"markdown","aa279af3":"markdown","276da108":"markdown","9e076b56":"markdown","895eaff3":"markdown"},"source":{"ace318ef":"%matplotlib inline","c27eea56":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","bcf6b128":"df_train = pd.read_json('..\/input\/train.json')\ndf_test = pd.read_json('..\/input\/test.json')","0ac62c78":"df_train.head()","a104e200":"df_test.head()","57130e37":"#Train set\ningredients_train = df_train.ingredients\nwords_train = [' '.join(x) for x in ingredients_train]\nprint(len(words_train), words_train[0])\n\n#Test set\ningredients_test = df_test.ingredients\nwords_test = [' '.join(x) for x in ingredients_test]","3d7cfa83":"vectorizer = CountVectorizer(max_features = 1000)\nbag_of_words = vectorizer.fit(words_train)\nbag_of_words","af23605b":"ing_array_train = bag_of_words.transform(words_train).toarray()\ning_array_test = bag_of_words.transform(words_test).toarray()\ning_array_train","6a918e91":"df_ing_train = pd.DataFrame(ing_array_train, columns=vectorizer.vocabulary_)\ndf_ing_test = pd.DataFrame(ing_array_test, columns=vectorizer.vocabulary_)\ndf_ing_train.head()","9d8eac1a":"df_train_new = df_train.merge(df_ing_train, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\ndf_train_new.head()","4559975a":"df_test_new = df_test.merge(df_ing_test, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\ndf_test_new.head()","b28d036f":"X = df_train_new.drop(['id', 'cuisine'], axis=1)\ny = df_train_new.cuisine\nX.shape, y.shape","b3d19968":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.85)\nX_train.shape, X_val.shape","952f4492":"m = RandomForestClassifier(oob_score=True)\nm.fit(X_train, y_train)","1525e66c":"m.oob_score_, m.score(X_val, y_val)","292296e8":"def create_model(n_words, n_trees, train, test, words=None):\n    #create vectorized df's\n    df_train, df_test = vect_train_test(train, test, n_words, words)\n    \n    X = df_train.drop(['id', 'cuisine'], axis=1)\n    y = df_train.cuisine\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.85)\n    \n    m = RandomForestClassifier(n_estimators=n_trees, oob_score=True)\n    m.fit(X_train, y_train)\n    \n    moob_score = m.oob_score_\n    score = m.score(X_val, y_val)\n    model = m\n    \n    return moob_score, score, model\n\ndef vect_train_test(dftrain, dftest, n_words=1000, words=None):\n    vectorizer = CountVectorizer(max_features = n_words)\n    ingredients_train = dftrain.ingredients\n    words_train = [' '.join(x) for x in ingredients_train]\n    ingredients_test = dftest.ingredients\n    words_test = [' '.join(x) for x in ingredients_test]\n    if isinstance(words, pd.Series):\n        bag_of_words = vectorizer.fit(words)\n    else:\n        bag_of_words = vectorizer.fit(words_train)\n\n    ing_array_train = bag_of_words.transform(words_train).toarray()\n    ing_array_test = bag_of_words.transform(words_test).toarray()\n\n    df_ing_train = pd.DataFrame(ing_array_train, columns=vectorizer.vocabulary_)\n    df_ing_test = pd.DataFrame(ing_array_test, columns=vectorizer.vocabulary_)\n\n    df_train = dftrain.merge(df_ing_train, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\n    df_test= dftest.merge(df_ing_test, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\n    return df_train, df_test\n\ndef run_variations(variations, target):\n    models = []\n    for var in variations:\n        moob_score, score, model = create_model(var[0], var[1], df_train, df_test)\n        models.append({'n_vectors': var[0],\n                       'n_trees': var[1],\n                       'moob_score': moob_score,\n                      'score': score,\n                      'model': model})\n        print(var, moob_score, score)\n    if target == 'vector':\n        plot_vector_score(models)\n    elif target == 'trees':\n        plot_ntree_score(models)\n    return models\n\ndef plot_vector_score(models):\n    plt.plot([x['n_vectors'] for x in models], [y['moob_score'] for y in models])\n    plt.title('Score increase from Vector increase')\n    plt.xlabel('Word vector size')\n    plt.ylabel('Score')\n    plt.show()\n    return\n\ndef plot_ntree_score(models):\n    plt.plot([x['n_trees'] for x in models], [y['moob_score'] for y in models])\n    plt.title('Score increase from number of estimators (trees) increase')\n    plt.xlabel('Number of Estimators')\n    plt.ylabel('Score (350 word vector)')\n    plt.show()\n    return","5d83ee47":"top = sorted(list(zip(X_val.columns, \n                      m.feature_importances_)), key=lambda x: x[1], reverse=True)[:200]","8c92e2b7":"df_imp = pd.DataFrame(top, columns=['feat', 'imp'])\ndf_imp.imp = df_imp.imp.astype(float)\ndf_imp[df_imp.imp > 0.002].plot('feat', 'imp', kind='barh', figsize=(12,12))","3815d619":"df_keep = df_imp[df_imp.imp > 0.004]\nnew_ing = df_keep.feat","cbf40172":"#Reducing features to the most significants did not improve results...\nmoob_score, score, m = create_model(350, 30, df_train, df_test, new_ing)\nmoob_score, score","7e37a720":"variations_1 = [(100, 30),\n             (150, 30),\n             (200, 30),\n             (300, 30),\n             (350, 30),\n             (500, 30),\n             (700, 30),\n             (1000, 30),\n             (1300, 30)]\n\nmodels_1 = run_variations(variations_1, 'vector')","907a546c":"# variations_2 = [(350, 10),\n#                (350, 20),\n#                (350, 25),\n#                (350, 30),\n#                (350, 35),\n#                (350, 50),\n#                (350, 75),\n#                (350, 100),\n#                (350, 150),\n#                (350, 200),\n#                (350, 300)]\n\n# models_2 = run_variations(variations_2, 'trees')","44ca7e95":"# variations_3 = [(1000, 50),\n#                 (1000, 100),\n#                (1000, 150)]\n\n# models_3 = run_variations(variations_3, 'trees')","c2a1f85b":"moob_score, score, m = create_model(1000, 300, df_train, df_test) #after running variations these values seemed the best right now..","63124ff9":"moob_score, score","5f5cba78":"X_test = df_test_new.drop('id', axis=1)\ny_test = m.predict(X_test)\ny_test","51dd2242":"df_sub = pd.DataFrame(np.array([df_test.id, y_test]).T, \n                      columns=['id', 'cuisine']).set_index('id')\n\ndf_sub.head()","40a69e98":"df_sub.to_csv(f'submission_{m.n_estimators}_V4Kernel.csv')","2d21e859":"## Load data","ab1e9597":"#### Create list of words in each recipe row","73cf52c4":"## Optimal model (current)","3f59bd0c":"## Create sets","d4b92468":"## Handle ingredients column","b1e85103":"## Create test set file","57d3aacd":"#### Incorporate the word vectors into the train and test dataframes","622189bd":"How about number of trees? How does the score improve with changes to that?","be60dad9":"## Structuring the above into helper functions","e8930b83":"#### Transform the word lists into vectors using the vectorizer trained on the training data","66853290":"We can see that size 1000 for word vectors is ideal.","aa279af3":"#### Create a word vector based on the training set","276da108":"### Check feature importance","9e076b56":"## Create Random Forest","895eaff3":"## Create variations"}}