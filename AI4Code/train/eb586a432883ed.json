{"cell_type":{"0ab688f2":"code","9c8778cb":"code","8768c11a":"code","6cbb458f":"code","56b9cf43":"code","67e49c9e":"code","d3ecaa27":"code","a1f687dc":"code","5d4ba2af":"code","951bbd5d":"code","89d809fe":"code","e4c54916":"code","8acb34c0":"code","4a878da1":"code","7474bb1c":"code","5fb5b834":"code","22c46591":"code","a0765180":"code","c9c560a1":"code","ce4e7fcb":"code","12fab37c":"code","e07ae279":"code","21283ff2":"code","1be89498":"code","3d1d4935":"code","9f5af1c4":"code","db102eb8":"code","787a22be":"code","1f6441c6":"code","37ff42f2":"markdown","dbae0a58":"markdown","84945f1e":"markdown","d9c5cd9c":"markdown","0f54d409":"markdown","1a513d32":"markdown","8968fd22":"markdown","f70141d9":"markdown","c0ab6038":"markdown","0ec09162":"markdown","caffc6c1":"markdown","fdab4d6f":"markdown","5c970ef8":"markdown","69f7dca6":"markdown","49b0eb68":"markdown","dbce34c0":"markdown","f67f308d":"markdown","397b5884":"markdown","bfffbae9":"markdown","c27c9efa":"markdown","fb15e1c0":"markdown","fdd32eec":"markdown"},"source":{"0ab688f2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9c8778cb":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","8768c11a":"train.dependency = train.dependency.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\ntrain.edjefa = train.edjefa.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\ntrain.edjefe = train.edjefe.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\ntest.dependency = test.dependency.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\ntest.edjefa = test.edjefa.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\ntest.edjefe = test.edjefe.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)","6cbb458f":"train.rez_esc = train.rez_esc.replace({99.0 : 5.0}).astype(np.float64)\ntest.rez_esc = test.rez_esc.replace({99.0 : 5.0}).astype(np.float64)","56b9cf43":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_non_bool = ['rez_esc', 'escolari', 'age','SQBescolari','SQBage','agesq']\n\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n\nhh_non_bool = ['v2a1', 'v18q1', 'meaneduc', 'SQBovercrowding', 'SQBdependency',\n               'SQBmeaned', 'overcrowding', 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1',\n               'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3', 'tamhog', 'tamviv', 'hhsize',\n               'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',  'bedrooms',\n               'qmobilephone', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin']\n\nhh_cont = [ 'dependency', 'edjefe', 'edjefa']\n\n\nids = ['Id', 'idhogar', 'Target']","67e49c9e":"test['Target'] = np.nan\ndata = train.append(test)\ndata.info()","d3ecaa27":"train.idhogar.nunique(), test.idhogar.nunique(), data.idhogar.nunique()","a1f687dc":"# data miss value count > 0\nmiss_count = data.isnull().sum() > 0\n# data miss value count\nmisvalue_counts = data.isnull().sum()[miss_count]\n# miss value percent\nmisvalue_percent = misvalue_counts\/data.shape[0]*100\n\nmisvalue_percent","5d4ba2af":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')","951bbd5d":"hh_train = train.loc[train.parentesco1 == 1, ids+hh_bool+hh_non_bool+hh_cont].reset_index()\n\ntarget = hh_train[['Target']]\nhh_train_ids = hh_train[['idhogar']]\n# before filling miss value\nhh_train = hh_train.drop(['Id','idhogar','Target','index'], axis=1)\n# after filling miss value\nhh_train_df = pd.DataFrame(imputer.fit_transform(hh_train),columns=list(hh_train.columns))\n\n# add idhogar and Target columns\nhh_train['idhogar'] = hh_train_ids\nhh_train_df['idhogar'] = hh_train_ids\nhh_train['Target'] = target\nhh_train_df['Target'] = target","89d809fe":"# indiviual level data on train set\nind_train = train.loc[ :, ids+ind_bool+ind_non_bool].reset_index()\n\nind_train_ids = ind_train[['idhogar']]\nind_target = ind_train[['Target']]\n\n# before filling miss value, drop old index\nind_train = ind_train.drop(['Id','idhogar','Target','index'], axis=1)\n\n# after filling miss value\nind_train_df=pd.DataFrame(imputer.fit_transform(ind_train),columns=list(ind_train.columns))\n\n# add idhogar, Target\nind_train['idhogar'] = ind_train_ids\nind_train['Target'] = ind_target\nind_train_df['idhogar'] = ind_train_ids\nind_train_df['Target'] = ind_target","e4c54916":"from collections import OrderedDict\n\nmis_cols = ['v2a1','v2a1','v18q1','v18q1','meaneduc','meaneduc','SQBmeaned','SQBmeaned']\n\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\nlabel_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', \n                               4: 'non vulnerable'})\n#----------------------------------------------------------------------------\n\nplt.figure(figsize = (12, 7))\nfor i, col in enumerate(mis_cols):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # \u6838\u5bc6\u5ea6\u4f30\u8ba1\n        if (i%2 == 0):\n            sns.kdeplot(hh_train_df.loc[hh_train_df.Target == poverty_level,col].dropna(), \n                        ax = ax, color = color, label = label_mapping[poverty_level])\n            plt.title('%s before filling KDE'%(col.capitalize()))\n            plt.xlabel('%s'%col)\n            plt.ylabel('Density')\n        else :\n            sns.kdeplot(hh_train.loc[hh_train.Target == poverty_level, col].dropna(),\n                        ax = ax, color = color, label = label_mapping[poverty_level])\n            plt.title('%s after filling KDE'%(col.capitalize()))\n            plt.xlabel('%s'%col)\n            plt.ylabel('Density')\nplt.subplots_adjust(top = 2.5)","8acb34c0":"cols = ['rez_esc','rez_esc']\nplt.figure(figsize=(14, 2.5))\nfor i, col in enumerate(cols):\n    ax = plt.subplot(1, 2, i + 1)\n    for poverty_level, color in colors.items():\n        if (i%2 == 0):\n            sns.kdeplot(ind_train_df.loc[ind_train_df.Target == poverty_level,col].dropna(), \n                        ax = ax, color = color, label = label_mapping[poverty_level])\n            plt.title('%s KDE'%(col.capitalize()))\n            plt.xlabel('%s'%col)\n            plt.ylabel('Density')\n        else :\n            sns.kdeplot(ind_train.loc[ind_train.Target == poverty_level, col].dropna(),\n                        ax = ax, color = color, label = label_mapping[poverty_level])\n            plt.title('%s filled miss KDE'%(col.capitalize()))\n            plt.xlabel('%s'%col)\n            plt.ylabel('Density')\nplt.subplots_adjust(top = 2)","4a878da1":"# test set\nmis_hh = test.groupby(by='idhogar').parentesco1.agg('sum')==0\n\n# idhogar,miss head of the household\nmis_idhogar = test.groupby(by='idhogar').parentesco1.agg('sum')[mis_hh].index","7474bb1c":"pd.options.display.max_columns = 10\ntest.loc[test.idhogar.isin(mis_idhogar),:][['Id','idhogar','parentesco1']].sort_values(by='idhogar')","5fb5b834":"test.loc[test.Id == 'ID_99d27ab2f','parentesco1'] = 1\ntest.loc[test.Id == 'ID_49d05f9e6','parentesco1'] = 1\ntest.loc[test.Id == 'ID_b0874f522','parentesco1'] = 1\ntest.loc[test.Id == 'ID_ceeb5dfe2','parentesco1'] = 1\ntest.loc[test.Id == 'ID_aa8f26c06','parentesco1'] = 1\ntest.loc[test.Id == 'ID_e42c1dde2','parentesco1'] = 1\ntest.loc[test.Id == 'ID_9c12f6ebc','parentesco1'] = 1\ntest.loc[test.Id == 'ID_26d95edff','parentesco1'] = 1\ntest.loc[test.Id == 'ID_93fa2f7cc','parentesco1'] = 1\ntest.loc[test.Id == 'ID_bca8a1dde','parentesco1'] = 1\ntest.loc[test.Id == 'ID_4036d87e3','parentesco1'] = 1\ntest.loc[test.Id == 'ID_9f025fde6','parentesco1'] = 1\ntest.loc[test.Id == 'ID_6094ce990','parentesco1'] = 1\ntest.loc[test.Id == 'ID_00e8a868f','parentesco1'] = 1\ntest.loc[test.Id == 'ID_d0beee31f','parentesco1'] = 1\ntest.loc[test.Id == 'ID_894de66bc','parentesco1'] = 1\ntest.loc[test.Id == 'ID_aa650fb4a','parentesco1'] = 1\ntest.loc[test.Id == 'ID_139a474f3','parentesco1'] = 1","22c46591":"# household level test data\nhh_test = test.loc[test.parentesco1 == 1, ids+hh_bool+hh_non_bool+hh_cont].reset_index()\n\nhh_test_ids = hh_test[['idhogar']]\n\nhh_test = hh_test.drop(['Id','idhogar','Target','index'], axis = 1)\n\n# filling miss values\nhh_test_df = pd.DataFrame(imputer.fit_transform(hh_test),columns=list(hh_test.columns))\n\n# add idhogar columns\nhh_test_df['idhogar'] = hh_test_ids\nhh_test['idhogar'] = hh_test_ids","a0765180":"# indiviual level test data\nind_test = test.loc[:, ids+ind_bool+ind_non_bool].reset_index()\n\nind_test_ids = ind_test[['idhogar']]\nind_test = ind_test.drop(['Id','idhogar','Target','index'], axis = 1)\nind_test_df = pd.DataFrame(imputer.fit_transform(ind_test),columns=list(ind_test.columns))\n\n# add idhogar columns\nind_test['idhogar'] = ind_test_ids\nind_test_df['idhogar'] = ind_test_ids","c9c560a1":"ind_train_groupobj = ind_train_df.groupby(by='idhogar')\n\nind_train_data = pd.DataFrame({'idhogar':ind_train_df.idhogar.unique()})","ce4e7fcb":"def AddFeatures(feature_df, cols, funcs, groupobj):\n    for func in funcs:\n        for col in cols:\n            group_object = groupobj[col].agg(func).reset_index()\n            group_object.rename(index=str, columns={col:col+'_'+func}, inplace=True)\n            feature_df = feature_df.merge(group_object, on='idhogar', how='left')\n    return feature_df","12fab37c":"# indiviual bool features\nind_train_data = AddFeatures(ind_train_data, ind_bool, ['mean','sum'], ind_train_groupobj)\n\n# indiviual non bool features\nfuncs = ['mean','min','max','median','sum','nunique']\nind_train_data = AddFeatures(ind_train_data, ind_non_bool, funcs, ind_train_groupobj)","e07ae279":"ind_test_groupobj = ind_test_df.groupby(by='idhogar')\nind_test_data = pd.DataFrame({'idhogar':ind_test_df.idhogar.unique()})\n\nind_test_data = AddFeatures(ind_test_data, ind_bool, ['mean','sum'], ind_test_groupobj)\n\nind_test_data = AddFeatures(ind_test_data, ind_non_bool, funcs, ind_test_groupobj)","21283ff2":"train_data = hh_train_df.merge(ind_train_data, on = 'idhogar', how='left')\ntest_data = hh_test_df.merge(ind_test_data, on = 'idhogar', how='left')","1be89498":"import gc\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score","3d1d4935":"def model(train_data, test_data, n_folds = 10):\n    # household id\n    train_ids = train_data[['idhogar']]\n    test_ids = test_data[['idhogar']]\n    # Target\/label\n    labels = train_data[['Target']].astype(int)\n    # drop idhogar, Target\n    train_data = train_data.drop(['idhogar','Target'],axis = 1)\n    test_data = test_data.drop(['idhogar'], axis = 1)\n    # feature columns name\n    feature_names = list(train_data.columns)\n    # 10 folds cross validation\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 2018)\n    # test predictions\n    test_predictions = list()\n    # validation predictions\n    out_of_fold = np.zeros(train_data.shape[0])\n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    # record scores : means f1_macro\n    Valid_F1 = []\n    Train_F1 = []\n    # lightgbm not support f1_macro, so map\n    Valid_Score = []\n    Train_Score = []\n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(train_data):\n        # Training data for the fold\n        train_features = train_data.loc[train_indices, :]\n        train_labels = labels.loc[train_indices, :]\n        # Validation data for the fold\n        valid_features = train_data.loc[valid_indices, :]\n        valid_labels = labels.loc[valid_indices, :]\n        # Create the model\n        model = lgb.LGBMClassifier(boosting_type='gbdt',n_estimators=2000, \n                                   objective = 'multiclass', class_weight = 'balanced',\n                                   learning_rate = 0.03,  num_leaves = 31,\n                                   reg_alpha = 0.1, reg_lambda = 0.3, num_class = 4,\n                                   subsample = 0.8, n_jobs = -1, random_state = 2018)\n\n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'multi_error',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = 'auto',\n                  early_stopping_rounds = 100, verbose = 200)\n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        # \n        test_predictions.append(model.predict(test_data, num_iteration = best_iteration))\n        # feature importance\n        feature_importance_values += model.feature_importances_ \/n_folds\n        # Record the best multi error\n        valid_score = model.best_score_['valid']['multi_error']\n        train_score = model.best_score_['train']['multi_error']\n        Valid_Score.append(valid_score)\n        Train_Score.append(train_score)\n        # Record F1_macro score\n        pred_valid = model.predict(valid_features, num_iteration = best_iteration)\n        pred_train = model.predict(train_features, num_iteration = best_iteration)\n        valid_f1 = f1_score(valid_labels, pred_valid, average='macro')\n        train_f1 = f1_score(train_labels, pred_train, average='macro')\n        Valid_F1.append(valid_f1)\n        Train_F1.append(train_f1)\n\n        # validation set result\n        out_of_fold[valid_indices] = pred_valid\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        print('................................................')\n        \n    # feature importance\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    # overall valida\n    Valid_F1.append(f1_score(labels, out_of_fold, average='macro'))\n    Train_F1.append(np.mean(Train_F1))\n    Valid_Score.append(np.mean(Valid_Score))\n    Train_Score.append(np.mean(Train_Score))\n    # dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train error': Train_Score,\n                            'valid error': Valid_Score,\n                            'train f1' : Train_F1,\n                            'valid f1' : Valid_F1}) \n\n    # make submission.csv\n    predict_df = pd.DataFrame(np.array(test_predictions).T)\n    voting_result = [predict_df.iloc[x,:].value_counts().argmax() for x in range(predict_df.shape[0])]\n    submission = test_ids.copy()\n    submission['Target'] = voting_result\n    # metric, fetaure importance , househodl target\n    return metrics, feature_importances,submission","9f5af1c4":"metric, feature_importance, submission = model(train_data, test_data, 10)","db102eb8":"# filling mean, round 100\nmetric ","787a22be":"submit = test[['Id','idhogar']]\n\nsubmit = submit.merge(submission, on = 'idhogar')\n\nsubmit = submit.drop(['idhogar'],axis = 1)\n\nsubmit.to_csv('submit.csv',index = False)","1f6441c6":"feature_importance = feature_importance.sort_values(by = 'importance')\n\nfeature_importance.set_index('feature').plot(kind='barh', figsize=(10, 40))\nplt.title('Feature Importances')","37ff42f2":"# Filling miss value on testset","dbae0a58":"<font size=4 color='orange'>Indiviual Test data<\/font>","84945f1e":"## Outlier processing\nrez_esc :\tYears behind in school","d9c5cd9c":"## Let us assign one member to the 18 families as the head of the household.","0f54d409":"# Now Training Model with lightGBM","1a513d32":"# create new indiviual feature","8968fd22":"# make submission.csv","f70141d9":"### The following 26 rows of data come from families with missing households in the test data.","c0ab6038":"<font size=5 color='orange'> before\/after filling miss value (KDE)<\/font>\n\n- hh_train : before filling miss value\n- hh_train_df : after filling miss value\n    - v2a1  \n    - v18q1  \n    - meaneduc    \n    - SQBmeaned","0ec09162":"# fill miss value : mean","caffc6c1":"<font size = 5 color='orange'>Feature Importances<\/font>","fdab4d6f":"<font size=6 color='orange'>indiviual data\u586b\u5145\u524d\u540eKDE\u53d8\u5316\u60c5\u51b5<\/font>\n\n- ind_train : before filling miss value\n- ind_train_df : after filling miss value\n    - rez_esc","5c970ef8":"## Feature description\n<br>dependency:\u3000\tDependency rate calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household     between 19 and 64)\n<br>edjefa\t: \u3000  years of education of female head of household based on the interaction of escolari (years of education) head of household and gender yes=1 and no=0\n<br>edjefe\t:\u3000\u3000years of education of male head of household based on the interaction of escolari (years of education) head of household and gender yes=1 and no=0","69f7dca6":"# <font size=6 color='orange'>Train data<\/font>\n\n- hh_ : household level data\n- ind_ : indiviual level data\n- hh_train : before filling miss value\n- ind_train : before filling miss value\n- hh_train_df : after filling miss value\n- ind_train_df : after filling miss value","49b0eb68":"# Merge household and indiviual data","dbce34c0":"# Feature Conversion","f67f308d":"# Feature classification","397b5884":"<font size=6 color='orange'>Test data<\/font>\n<pr>There are 18 families in the test data that are not specified by the head of the household. In order to align hh_test and ind_test, you need to specify the head of the household.\n    \n- hh_test :  before filling miss value\n- hh_test_df :  after filling miss value\n- ind_test : before filling\n- ind_test_df : after filling","bfffbae9":"# Merge train test data","c27c9efa":"# Miss value count","fb15e1c0":"<font size=4 color='orange'>Indiviual Train data<\/font>","fdd32eec":"hh : household features\n\nind : individual features\n\nids : Id, idhogar, Target"}}