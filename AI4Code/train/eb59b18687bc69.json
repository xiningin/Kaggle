{"cell_type":{"fab2d196":"code","ace3bd11":"code","1597d824":"code","53c41813":"code","03b2145f":"code","2a4dfc8e":"code","a22a767f":"code","f470747c":"code","deb0a61d":"code","c7c215ad":"code","0b115c19":"code","767dabdd":"code","ff4c0a0f":"code","84138d0c":"code","6ce2f774":"code","e03b3789":"code","fa8212a0":"code","9a4db4d7":"code","2e6a8319":"code","133472a1":"code","df8c38c6":"code","88897312":"markdown","ed855800":"markdown","e5799eb4":"markdown","27a2dc26":"markdown"},"source":{"fab2d196":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ace3bd11":"#load training data\ntraindf=pd.read_csv('..\/input\/semeval-2018-task-ec\/2018-E-c-En-train.txt',encoding='utf-8',sep=\"\\t\")\n\n#load testing data\ndevdf=pd.read_csv('..\/input\/semeval-2018-task-ec\/2018-E-c-En-dev.txt',encoding='utf-8',sep=\"\\t\")\n\n#load testing data\ntestdf=pd.read_csv('..\/input\/semeval-2018-task-ec\/2018-E-c-En-test.txt',encoding='utf-8',sep=\"\\t\")","1597d824":"#show first 10 rows in training\ntraindf.head(10)","53c41813":"#show first 10 rows in testing\ntestdf.head(10)","03b2145f":"#import regular expression library\nimport re\n\n#extract hashtags from training data and put them in new column named hashtag \ntraindf[\"hashtags\"]=traindf[\"Tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x))\n\n#extract hashtags from develop data and put them in new column named hashtag \ndevdf[\"hashtags\"]=devdf[\"Tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x))\n\n#extract hashtags from testing data and put them in new column named hashtag \ntestdf[\"hashtags\"]=testdf[\"Tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x))","2a4dfc8e":"#display first rows in training data\ntraindf.head(10)","a22a767f":"#display first rows in testing data\ntestdf.head(10)","f470747c":"#preprocessing tweets training and testing\nfrom emoji import UNICODE_EMOJI\nimport emoji\n\n#translate emojis in training\ntraindf[\"clean\"]=traindf[\"Tweet\"].apply(lambda x: emoji.demojize(x))\n\n#translate emojis in develop\ndevdf[\"clean\"]=devdf[\"Tweet\"].apply(lambda x: emoji.demojize(x))\n\n#translate emojis in testing\ntestdf[\"clean\"]=testdf[\"Tweet\"].apply(lambda x: emoji.demojize(x))","deb0a61d":"#remove urls in training\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))\n\n#remove urls in develop\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))\n\n#remove urls in testing\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))","c7c215ad":"import nltk\n#tokenize training tweet\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n\n#tokenize develop tweet\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n\n#tokenize testing tweet\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))","0b115c19":"#import stopwords corpus from nltk\nfrom nltk.corpus import stopwords\nimport string #load punctuation charachers\n\n#remove stopwords and punctuations\nstopwrds = set(stopwords.words('english'))\n\n#training data\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: [y for y in x if (y not in stopwrds)])\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y) for y in x])\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: [re.sub('\\\\n','',y) for y in x])\n\n#develop data\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: [y for y in x if (y not in stopwrds)])\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y) for y in x])\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: [re.sub('\\\\n','',y) for y in x])\n\n#testing data\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: [y for y in x if (y not in stopwrds)])\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y) for y in x])\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: [re.sub('\\\\n','',y) for y in x])","767dabdd":"#clean unneeded spaces or empty columns or non sense words\n\n#training data\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: [y for y in x if y.strip() != '' and len(y.strip())>2])\n\n#develop data\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: [y for y in x if y.strip() != '' and len(y.strip())>2])\n\n#testing data\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: [y for y in x if y.strip() != '' and len(y.strip())>2])","ff4c0a0f":"traindf.head(10)","84138d0c":"#save Cleaned tweets\n\n#training data\ntraindf=traindf[[\"clean\",\"hashtags\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]]\n\n#develop data\ndevdf=devdf[[\"clean\",\"hashtags\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]]\n\n#testing data\ntestdf=testdf[[\"clean\",\"hashtags\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]]","6ce2f774":"#Total tweets records\nlen(traindf)","e03b3789":"#counting each emotion \nval_df=pd.DataFrame([{\"anger\":len(traindf[traindf[\"anger\"]==1]),\n       \"anticipation\":len(traindf[traindf[\"anticipation\"]==1]),\n       \"disgust\":len(traindf[traindf[\"disgust\"]==1]),\n       \"fear\":len(traindf[traindf[\"fear\"]==1]),\n       \"joy\":len(traindf[traindf[\"joy\"]==1]),\n       \"love\":len(traindf[traindf[\"love\"]==1]),\n       \"optimism\":len(traindf[traindf[\"optimism\"]==1]),\n       \"pessimism\":len(traindf[traindf[\"pessimism\"]==1]),\n       \"sadness\":len(traindf[traindf[\"sadness\"]==1]),\n       \"surprise\":len(traindf[traindf[\"surprise\"]==1]),\n       \"trust\":len(traindf[traindf[\"trust\"]==1])}])\nval_df\n","fa8212a0":"val_df.iloc[0].plot(kind=\"bar\")","9a4db4d7":"#convert tokenize tweets to sentences\ntraindf[\"clean\"]=traindf[\"clean\"].apply(lambda x: ' '.join(x).replace('\\\\n',''))\ndevdf[\"clean\"]=devdf[\"clean\"].apply(lambda x: ' '.join(x).replace('\\\\n',''))\ntestdf[\"clean\"]=testdf[\"clean\"].apply(lambda x: ' '.join(x).replace('\\\\n',''))","2e6a8319":"traindf.head(10)","133472a1":"testdf.head(10)","df8c38c6":"#save clean tweets\n\n#training data\ntraindf.to_csv(\"cleaned_training_tweets.csv\",index=False,encoding=\"utf-8\")\n\n#develop data\ndevdf.to_csv(\"cleaned_develop_tweets.csv\",index=False,encoding=\"utf-8\")\n\n#testing data\ntestdf.to_csv(\"cleaned_testing_tweets.csv\",index=False,encoding=\"utf-8\")","88897312":"This notebook outlines the preprocessing Phase:\n* split hashtags\n* translate emojis\n* remove urls\n* tokenize tweets\n* remove stopwords & punctuations\n* remove unneeded spaces","ed855800":"Tweets are multi label classified into eleven emotions:\n(anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust)","e5799eb4":"# SemEval 2018 task E-c Tweets multi label classification","27a2dc26":"**Preprocessing**"}}