{"cell_type":{"d758dc5c":"code","cce452e5":"code","eb34dfdc":"code","db3aaf5d":"code","2b3cf497":"code","a7089973":"code","e09c091b":"code","76e541be":"code","0b5eba1e":"code","ab1a84b4":"code","f498b5cc":"code","1dc3f8df":"code","6758257d":"code","0f2e5022":"code","3a5a003c":"code","0b87d62f":"code","2baa90f3":"code","16bb4bab":"code","2b1a81c1":"code","662891e1":"code","12d8de2e":"code","96f31912":"code","45dc2b5a":"code","137c9866":"code","a55213a2":"code","e85d804f":"code","22a86138":"code","cd3abde2":"markdown","a7e14aed":"markdown","9da4b0b2":"markdown","6f74cb9b":"markdown","e1442ed6":"markdown","69a57baa":"markdown","37197321":"markdown","7f02181a":"markdown","0ae3b528":"markdown","8e23425c":"markdown","56b24553":"markdown","1c1847ca":"markdown","20622b38":"markdown","8cea5025":"markdown"},"source":{"d758dc5c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")","cce452e5":"def plot_metric(clf, testX, testY, name):\n    \"\"\"\n    Small function to plot ROC-AUC values and confusion matrix\n    \"\"\"\n    styles = ['bmh', 'classic', 'fivethirtyeight', 'ggplot']\n\n    plt.style.use(random.choice(styles))\n    plot_confusion_matrix(clf, testX, testY)\n    plt.title(f\"Confusion Matrix [{name}]\")","eb34dfdc":"data = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv\")\ndata2 = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/valid.csv\")\ndata.head()","db3aaf5d":"data = data.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\n\ndata2 = data2.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata2['Y'] = data2['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\n\ndata.head()","2b3cf497":"labels = ['Open Questions', 'Low Quality Question - Close', 'Low Quality Question - Edit']\nvalues = [len(data[data['Y'] == 2]), len(data[data['Y'] == 0]), len(data[data['Y'] == 1])]\nplt.style.use('classic')\nplt.figure(figsize=(16, 9))\nplt.pie(x=values, labels=labels, autopct=\"%1.1f%%\")\nplt.title(\"Target Value Distribution\")\nplt.show()","a7089973":"data['text'] = data['Title'] + ' ' + data['Body']\ndata = data.drop(['Title', 'Body'], axis=1)\n\ndata2['text'] = data2['Title'] + ' ' + data2['Body']\ndata2 = data2.drop(['Title', 'Body'], axis=1)\n\n\ndata.head()","e09c091b":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndata['text'] = data['text'].apply(clean_text)\ndata2['text'] = data2['text'].apply(clean_text)","76e541be":"# Training Sets\ntrain = data\ntrainX = train['text']\ntrainY = train['Y'].values\n\n# Validation Sets\nvalid = data2\nvalidX = valid['text']\nvalidY = valid['Y'].values\n\nassert trainX.shape == trainY.shape\nassert validX.shape == validY.shape\n\nprint(f\"Training Data Shape: {trainX.shape}\\nValidation Data Shape: {validX.shape}\")","0b5eba1e":"# Load the vectorizer, fit on training set, transform on validation set\nvectorizer = TfidfVectorizer()\ntrainX = vectorizer.fit_transform(trainX)\nvalidX = vectorizer.transform(validX)","ab1a84b4":"# Define and fit the classifier on the data\nlr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(trainX, trainY)","f498b5cc":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(validX, validY))*100:.2f}%\")","1dc3f8df":"# Also plot the metric\nplot_metric(lr_classifier, validX, validY, \"Logistic Regression\")","6758257d":"# Define and fit the classifier on the data\nnb_classifier = MultinomialNB()\nnb_classifier.fit(trainX, trainY)","0f2e5022":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(validX, validY))*100:.2f}%\")","3a5a003c":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Naive Bayes\")","0b87d62f":"# Define and fit the classifier on the data\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(trainX, trainY)","2baa90f3":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Random Forest Classifier is: {(rf_classifier.score(validX, validY))*100:.2f}%\")","16bb4bab":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Random Forest\")","2b1a81c1":"# Define and fit the classifier on the data\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(trainX, trainY)","662891e1":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Decision Tree Clf. is: {(dt_classifier.score(validX, validY))*100:.2f}%\")","12d8de2e":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","96f31912":"# Define and fit the classifier on the data\nkn_classifier = KNeighborsClassifier()\nkn_classifier.fit(trainX, trainY)","45dc2b5a":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of KNN Clf. is: {(kn_classifier.score(validX, validY))*100:.2f}%\")","137c9866":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","a55213a2":"# Define and fit the classifier on the data\nxg_classifier = XGBClassifier()\nxg_classifier.fit(trainX, trainY)","e85d804f":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(validX, validY))*100:.2f}%\")","22a86138":"# Also plot the metric\nplot_metric(xg_classifier, validX, validY, \"XGBoost Classifier\")","cd3abde2":"## 3. Random Forest Classifier\nLet's now enter the forest with the Random Forest Classifier and see where it takes us!","a7e14aed":"## 5. KNN Classifier\nWe now are going to use KNN Classifier for this task.","9da4b0b2":"## Vectorizing the Data\nLet's vectorize the data so it's in the numerical format","6f74cb9b":"All the open questions are grouped under a single class (1), while the closed one is grouped under (0)","e1442ed6":"Let's join the title and the body of the text data so that we can use both of them in our classification","69a57baa":"# Modelling\nLet's start with different non-deep learning approaches for this task.","37197321":"# Data Preprocessing and Some EDA","7f02181a":"Read the data and don't use the low quality edit data","0ae3b528":"## 6. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far","8e23425c":"## 2. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!","56b24553":"## 4. Decision Tree Classifier\nLet's now take some decisions using the Decision Tree Classifer","1c1847ca":"## Splitting the Data\nLet's now split the dataset into training and validation sets","20622b38":"## 1. Logistic Regression\nLet's first start with our good old, Logistic Regression!","8cea5025":"In the latest version of the dataset, I have included two separate files:\n\n1. train.csv containing 45000 rows for training\n2. valid.csv containing 15000 rows for validation\n\nI did this for a more clear comparison between notebooks and their models.\n\nThis notebook is an example of conforming to this change. It is completely based on the following notebook:\n\nhttps:\/\/www.kaggle.com\/heyytanay\/stack-overflow-qa-classification-87-acc"}}