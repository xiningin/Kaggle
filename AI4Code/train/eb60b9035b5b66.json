{"cell_type":{"8da68c6b":"code","57c7be7a":"code","e517890f":"code","a7f9c635":"code","016d7f61":"code","ada6bca0":"code","e1b3537a":"code","089803ef":"code","47a6ed00":"code","024f115b":"code","7df5849e":"code","bf13d983":"code","cd29bbf4":"code","ed66d989":"code","890327ea":"code","179b533f":"code","c323af1c":"code","ac83efbb":"code","ea3ac847":"code","e77c9df7":"code","fd6e0683":"code","58e4dd35":"code","4f734761":"code","b83b008b":"code","72dc95f9":"code","242cdd5c":"code","e109e2d7":"code","88a04efc":"code","c03100f4":"code","36affd26":"code","9473589c":"code","3982f1cc":"code","eca734a6":"code","eb2a294b":"code","38ea2152":"code","5d4b3129":"code","3d672e07":"code","512368e3":"markdown","e36b2e3a":"markdown","78cea46e":"markdown","a48234c7":"markdown","35a6d33e":"markdown","4e1c36d7":"markdown","a06016a2":"markdown","466a4f17":"markdown","679545c1":"markdown","8c196c68":"markdown","aacd07f7":"markdown","aaf4fad0":"markdown","5248f80e":"markdown","75f79fb3":"markdown","d10bd862":"markdown","a8c63707":"markdown","804d44b4":"markdown","f51eb887":"markdown","698ec099":"markdown","a8543aca":"markdown","fd4b1247":"markdown","7f5c6f76":"markdown","a51e1a3b":"markdown","f2aca57c":"markdown","5fc09242":"markdown","5d89faec":"markdown","c49b590f":"markdown","ac365e61":"markdown","96694d66":"markdown","8e53a489":"markdown","35ae1c56":"markdown","218fb78f":"markdown","3c96e29c":"markdown","9eb87d45":"markdown","fe5eb6db":"markdown","1ec06733":"markdown","49f8946e":"markdown","4c77814b":"markdown","6ffb9fb0":"markdown","9618f0a9":"markdown","d52bb315":"markdown","60d1c300":"markdown","ee3f4a7b":"markdown"},"source":{"8da68c6b":"\nimport pandas as pd\ndf = pd.read_csv('..\/input\/adult-income\/adult11.csv', sep=',')\ndf.head()","57c7be7a":"df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n              'occupation', 'relationship','race', 'gender', 'capital-gain', 'capital-loss', \n              'hours-per-week', 'native-country', 'salary']\ndf.head()","e517890f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline ","a7f9c635":"df.info()","016d7f61":"df.isnull().sum()","ada6bca0":"df.corr()","e1b3537a":"ax=sns.heatmap(df.corr(),annot=True, cmap=\"spring\")\nbottom, top = ax.get_ylim()","089803ef":"#Let's have a look at histograms.\ndf.hist(figsize=(18,8), color = \"yellow\")\nplt.show()","47a6ed00":"fig,axarr = plt.subplots(nrows=1,ncols=2,figsize=(11,6))\nsns.countplot(x=\"salary\", data=df, palette='spring',ax=axarr[0])\ndf.salary.value_counts().plot.pie(autopct =\"%1.1f%%\",ax=axarr[1])\nplt.title('Salary Kar\u015f\u0131la\u015ft\u0131rmas\u0131')\nplt.tight_layout()\nplt.show()","024f115b":"plt.figure(figsize=(16,7))\nsns.set(style=\"whitegrid\")\nsns.countplot(df['education'], order = df['education'].value_counts().index, palette='spring')\nplt.xticks(rotation=70)\ndf['education'].value_counts()","7df5849e":"plt.figure(figsize=(12,6))\nsns.barplot(x=\"education\", y=\"hours-per-week\", data=df, hue=\"salary\", palette='spring')\nplt.xticks(rotation=70)","bf13d983":"plt.figure(figsize=(12,8))\nsns.pointplot(x=\"education\", y=\"hours-per-week\", hue='gender', palette=\"spring\", linestyles=[\"-\", \"-.\"], data=df)\nplt.xticks(rotation=80)","cd29bbf4":"df.select_dtypes(exclude = 'category').plot(kind = 'box', figsize = (20,8))","ed66d989":"df=df.drop(\"fnlwgt\",axis=1)","890327ea":"plt.figure(figsize=(19,12))\nnum_feat = df.select_dtypes(include=['int64']).columns\nfor i in range(5):\n    plt.subplot(2,3,i+1)\n    plt.boxplot(df[num_feat[i]])\n    plt.title(num_feat[i],color=\"g\",fontsize=20)\n    plt.yticks(fontsize=14)\n    plt.xticks(fontsize=14)\nplt.show()","179b533f":"#I wanted to look at the data against Capital Gain.\ndf.describe()","c323af1c":"df[['education', 'education-num']].groupby(['education'], as_index=False).mean().sort_values(by='education-num', ascending=False)","ac83efbb":"df.drop(columns=['education'], inplace=True)","ea3ac847":"df['marital-status'] = df['marital-status'].replace([' Divorced',' Married-spouse-absent',' Never-married',' Separated',' Widowed'],'Single')\ndf['marital-status'] = df['marital-status'].replace([' Married-AF-spouse',' Married-civ-spouse'],'Couple')\n\ndf.head(10)","e77c9df7":"X = df.drop(['salary'], axis=1)\ny = df['salary']\nX.select_dtypes(include='object').tail(20)","fd6e0683":"categorical_columns = [c for c in X.columns  if X[c].dtype.name == 'object']\nfor c in categorical_columns:\n  X[c] = np.where(X[c] == ' ?', X[c].mode(), df[c])\nX.select_dtypes(include='object').tail(20)","58e4dd35":"X = pd.concat([X, pd.get_dummies(X.select_dtypes(include='object'))], axis=1)\nX = X.drop(['workclass', 'marital-status', 'occupation',\n       'relationship', 'race', 'gender', 'native-country'], axis=1)\nX.head()","4f734761":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix as cm\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\nd_tree1 = DecisionTreeClassifier(max_depth = 3, random_state=42)\nd_tree1.fit(X_train, y_train)\nprint (\"Train data set size : \", X_train.shape)\nprint (\"Test data set size : \", X_test.shape)","b83b008b":"predictions = d_tree1.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nax=sns.heatmap(cm1, annot=True, fmt=\".0f\", center=0, cmap=\"spring\", linewidths=\"0.5\"  )\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 16)\nplt.show()","72dc95f9":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions, target_names=['50Kdan d\u00fc\u015f\u00fck', '50Kdan y\u00fcksek']))","242cdd5c":"from sklearn.tree import export_graphviz\n# resim g\u00f6stermesi i\u00e7in\nfrom IPython.display import Image\n# bu \u00e7\u0131kt\u0131y\u0131 yazd\u0131rmak yerine string olarak kaydetmek i\u00e7in \nfrom sklearn.externals.six import StringIO\n# buras\u0131n\u0131n konuyla alakas\u0131 yok, \u00e7\u0131kt\u0131 i\u00e7in bir nesne yarat\u0131l\u0131yor\ndt_data =StringIO()\n# a\u011fac\u0131 d\u0131\u015far\u0131ya aktar\u0131yoruz\nexport_graphviz(d_tree1, out_file=dt_data,filled=True,rounded=True,impurity=False,\n               feature_names=X.columns, class_names=['<=50K','>50K'])\n# pydotplus k\u00fct\u00fcphanesini \u00e7a\u011f\u0131ral\u0131m ve grafik yaratal\u0131m\nimport pydotplus\ngraph = pydotplus.graph_from_dot_data(dt_data.getvalue())\n# bu grafi\u011fi g\u00f6sterelim\nImage(graph.create_png())","e109e2d7":"plt.figure(figsize=(16, 9))\n\nfrom sklearn import ensemble\n\nd_tree2 = DecisionTreeClassifier(max_depth = 8, random_state=42)\nd_tree2.fit(X_train, y_train)\nranking = d_tree2.feature_importances_\nfeatures = np.argsort(ranking)[::-1][:10]\ncolumns = X.columns\n\nplt.title(\"Attribute Importance Ranking by Decision Tree\", y = 1.03, size = 18)\nplt.bar(range(len(features)), ranking[features], color=\"yellow\", align=\"center\")\nplt.xticks(range(len(features)), columns[features], rotation=80)\nplt.show()","88a04efc":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix as cm\nlr = LogisticRegression(random_state=42)\nlr.fit(X_train, y_train)","c03100f4":"predictions = lr.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nax=sns.heatmap(cm1, annot=True, fmt=\".0f\", center=0, cmap=\"spring\", linewidths=\"0.5\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","36affd26":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions, target_names=['<=50K', '>50K']))","9473589c":"from sklearn.ensemble import GradientBoostingClassifier\nGB = GradientBoostingClassifier(loss='exponential',learning_rate=0.01,n_estimators=200)\nGB.fit(X_train,y_train)","3982f1cc":"predictions = GB.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nax=sns.heatmap(cm1, annot=True, fmt=\".0f\", center=0, cmap=\"spring\", linewidths=\"0.5\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","eca734a6":"y_pred = GB.predict(X_test)\nprint('Classification Report:')\nprint('\\n')\nprint(classification_report(y_test,y_pred))","eb2a294b":"import xgboost as xgb\nXgb = xgb.XGBClassifier(learning_rate=0.1,n_estimators=500,max_depth=5,min_child_weight=4,random_state=42 )\nXgb.fit(X_train, y_train)\npredictions = Xgb.predict(X_test)\nXGBA = accuracy_score(y_test, predictions)","38ea2152":"score = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nax=sns.heatmap(cm1, annot=True, fmt=\".0f\", center=0, cmap=\"spring\", linewidths=\"0.5\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","5d4b3129":"\nComparison = pd.DataFrame({d_tree1.score(X_test, y_test),\n                              lr.score(X_test, y_test),\n                       Xgb.score(X_test, y_test),\n                       GB.score(X_test, y_test)})\n\nComparison.index = ['Decission Tree Score','Logistik Regression Score',\n                       'XGBoosting Classification Score',\n                       'Gradient Boosting Score']\n\nComparison","3d672e07":"lastTable = pd.DataFrame({ \"Real Salary\": y_test[0:10],\n                        \"Decission Tree Score\": d_tree1.predict(X_test)[0:10],\n                        \"Logistik Regression Score\": lr.predict(X_test)[0:10],\n                          \"Gradient Boosting Score\": GB.predict(X_test)[0:10],\n                        \"XGBoosting Classification Score\": Xgb.predict(X_test)[0:10]}); lastTable","512368e3":"\n1. Separating the data into training and test sub-data groups.\n2. Creating a decision tree model.\n3. 'Fit' the model to the training data.","e36b2e3a":"## <font color=lightgreen> 1. Decission Tree Classifier","78cea46e":"<font color=magenta> We see that high school, some-college and bachelor deegrees are in the majority.","a48234c7":"<font color=magenta> Separating 'Object' type variables into 0 and 1 using one-hot-encoding method:","35a6d33e":"<font color=magenta>\nWe see the highest weekly working hours in those who have graduated from professors and doctorates. Those with more than 50 K income have always worked harder.\n<font color=magenta>\nGroup by gender and examine average weekly working hours of people: \n\n1. With plt.figure (), we determine the width and length of the image.\n2. We write the x and y values \u200b\u200bof the variable we want to visualize in the sns.pointplot () function.\n3. With hue = 'sex' option, we divide the data into two according to the sex 'column.\n4. We visualize with palette = 'dark' instead of the default color option.\n5. We rotate the x-axis 70 degrees so that the values \u200b\u200bdo not mix.","4e1c36d7":"0.8-1.0 Very high corelation\n\n0.6-0.8 High corelation\n\n0.4-0.6 Moderate corelation\n\n0.2-0.4 Weak corelation\n\n<font color=magenta>0.0-0.2 \nToo weak or no correlation. The highest correlation was observed between education number and hours per week. (0.14)","a06016a2":"#      <font color=lightgreen> Adult Income Prediction\n    \n    \nAdult Income data set was studied. The data set description is as follows.\n\n\n<font color=blue>i. Categorical Variables\n    \n\nWorkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. Individual work category\n\nEducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. Individual's highest education degree\n\nMarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. Individual marital status\n\nOccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. Individual's occupation\n\nRelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. Individual's relation in a family\n\nRace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. Race of Individual\n\nGender: Female, Male.\n\nNative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands. Individual's native country\n\n\n<font color=darkblue>ii.Continuous Variables\n\n\nAge: continuous. Age of an individual\n\nFnlwgt: final weight, continuous. The weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau.\n\nCapital-gain: continuous.\n\nCapital-loss: continuous.\n\nHours-per-week: continuous.\n\nIndividual's working hour per week\n\n\n<font color=green>Please note the format of sep = (whether the seperator is a semicolon or a comma).\n","466a4f17":"<font color=magenta> Since the target variable in our model is 'salary', we separate it from the rest of the data set.\n\nI'm looking at the data of type object.\n","679545c1":"<font color=magenta> I categorized all marital status details as married and single.","8c196c68":"<font color=magenta>We see that the weekly working hours of men are generally higher, except for women who have made a doctorate.\n\n","aacd07f7":"<font color=magenta>4. To give the test data to the model and predict it.\n<font color=magenta>5. Finding the confusion matrix according to the similarity between the real value and the estimate.","aaf4fad0":"## <font color=yellow> Editing Pre-Model Variables and EDA\n","5248f80e":"<font color=magenta>6. Finding precision, recall, f1-score values.","75f79fb3":"## <font color=yellow> As you see our accuracy score reached 88% with XGBoost. XGBoost is a decison-tree (decision-tree) based and gradient-boosting Machine Learning system. If your data, unstructured data such as pictures, text or sound will be the right choice for deep learning with artificial neural networks.","d10bd862":"[](http:\/\/) <font color=magenta> \nExamining the correlations","a8c63707":"# <font color=yellow> \nSetting up the model & applying standard modeling processes:","804d44b4":"<font color=magenta> It turned out as I thought, we are deleting the education column.","f51eb887":"<font color=magenta>4. Giving the test data to the model and predicting it.\n    \n<font color=magenta>5. Finding a confusion matrix based on the similarity between real value and estimate.","698ec099":"<font color=magenta> \nAlthough some data do not appear to be missing, '?' filled with question mark. I have completed this missing data.\n\n1. Columns with data type 'object' are selected.\n2. These columns are filled with the mode () value of that column.","a8543aca":"## <font color=yellow>2.Logistic Regression","fd4b1247":"<font color=magenta> Setting up the model, applying standard modeling processes:","7f5c6f76":"<font color=magenta>6. See classification success metrics.","a51e1a3b":"<font color=blue> Importing libraries. <\/font> ","f2aca57c":"<font color=magenta>I understand that Final Weight has contradictory observations on the Box plot, I exclude it from my dataset as I think it will not add much meaningful value to my forecast.","5fc09242":"<font color=magenta>\nGroup and distribution of people who receive weekly average working hours of less than 50K and more, depending on their educational status: \n    \nHere, I use barplot because my variables are categorical.\n\n\n1. With plt.figure () we determine the width and length of the image.\n2. We write the x and y values \u200b\u200bof the variable we want to visualize in the sns.barplot () function.\n3. By hue = 'salary' option, we divide the data into two according to the column of 'salary'.\n4. We visualize with palette = 'spring' instead of the default color option.\n5. We rotate the x-axis 70 degrees so that the values \u200b\u200bdo not mix.","5d89faec":" <font color=magenta> As we see above,there are 48.842 and no 'non-null' data.   ","c49b590f":"<font color=magenta> Education and education_num may mean the same thing, let's check.","ac365e61":"## <font color=yellow>4.XGBoost Classifier","96694d66":"## <font color=yellow>3.Gradient Boosting Classifier","8e53a489":"<font color=magenta>The distribution of educational status of the people in the data set:\n\nIn addition to the image above, since we want to sort our variables on the x axis from large to small, we determine the order and rotate the values \u200b\u200b70 degrees so that our x axis does not nest.\n\n1. With plt.figure () we determine the width and length of the image.\n2. We write the variable we want to visualize to the sns.countplot () function and sort the target variable from large to small.\n3. We rotate the x-axis 70 degrees so that the values \u200b\u200bdo not mix.\n4. We find the distribution of the target variable with the value_counts () function.","35ae1c56":"<font color=magenta>Setting up the model, applying standard modeling processes:","218fb78f":"<font color=magenta> Train the model and make predictions on test data","3c96e29c":"<font color=magenta>7. \nVisualizing the decision tree (Here my code is working but there is a library problem about importing, I recommend you to try it on your own notebook, that's why i did not delete it.)","9eb87d45":"<font color=magenta> Setting up the model, applying standard modeling processes:","fe5eb6db":"<font color=magenta>\nI create it separately for contradictory observations.","1ec06733":"<font color=magenta>6. Finding Precision, recall, f1-score values.","49f8946e":"## <font color=yellow>Examining categorical variables:\n    \n <font color=magenta>   \nSince the target variable is salary, its distribution was viewed with the value_counts () function and visualized with the seaborn library. \n\n1. We determine the width and length of the image with plt.figure ().\n2. We write the variable we want to visualize to the sns.countplot () function.\n3. We find the distribution of the target variable with the value_counts () function.","4c77814b":"<font color=blue> I firstly add column names.\n","6ffb9fb0":"4. To give the test data to the model and predict it.\n5. Finding the confusion matrix according to the similarity between the real value and the estimate.","9618f0a9":"<font color=blue> Checking whether my data set include null data or not. <\/font> ","d52bb315":"<font color=magenta>8. Visualization of the attribute importance order made while the classification model is being established.","60d1c300":"<font color=magenta>The distribution of Hours-per-week data was observed to be close to normal. Since the age variable is skewed to the right, education_num is skewed to the left, and the rate of capital_gain and capital_loss is 0, a structure gathered on the left stands out. The hour_per_week variable appears to have a bell curve shape that tends to the left but is closer to the relative normal distribution compared to other variables.","ee3f4a7b":"<font color=magenta>We see that those with low annual income are 3\/4 of the entire data set."}}