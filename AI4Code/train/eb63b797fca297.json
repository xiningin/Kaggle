{"cell_type":{"483958ef":"code","8db73cd6":"code","76bf8fb7":"code","e0e3c2e3":"code","34525ba1":"code","09c8509f":"markdown"},"source":{"483958ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom multiprocessing import cpu_count\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom warnings import catch_warnings\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport pandas_datareader.data as web\nimport warnings\nwarnings.simplefilter('ignore')\n%matplotlib inline","8db73cd6":"def get_stock_data(ticker, start_date, end_date):\n    df = web.DataReader(ticker, 'yahoo', start_date, end_date)\n    df.reset_index(inplace=True)\n    df = df[['Date', 'Adj Close']]\n    df.columns = ['ds', 'y']\n    return df","76bf8fb7":"import datetime as dt\n# We would like all available data from 01\/01\/2000 until 12\/31\/2016.\n# can only go back to 2019-04-29 with Bloomberg\nstart_date = dt.datetime(2019,1,1)\nend_date = dt.datetime(2019,4,29)\n\ndata = get_stock_data('TSLA', start_date, end_date)\n#data = get_stock_data('AAPL', start_date, end_date)\n#data = get_stock_data('AMZN', start_date, end_date)\n#data = get_stock_data('GE', start_date, end_date)\n#data = get_stock_data('BA', start_date, end_date)","e0e3c2e3":"# grid search sarima hyperparameters\nfrom math import sqrt\nfrom multiprocessing import cpu_count\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom warnings import catch_warnings\nfrom warnings import filterwarnings\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error\n \n# one-step sarima forecast\ndef sarima_forecast(history, config):\n    order, sorder, trend = config\n    # define model\n    model = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)\n    # fit model\n    model_fit = model.fit(disp=False)\n    # make one step forecast\n    yhat = model_fit.predict(len(history), len(history))\n    return yhat[0]\n\n# root mean squared error or rmse\ndef measure_rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n\n# split a univariate dataset into train\/test sets\ndef train_test_split(data, n_test):\n    return data[:-n_test], data[-n_test:]\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test, cfg):\n    predictions = list()\n    # split dataset\n    train, test = train_test_split(data, n_test)\n    # seed history with training dataset\n    history = [x for x in train]\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # fit model and make forecast for history\n        yhat = sarima_forecast(history, cfg)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n    # estimate prediction error\n    error = measure_rmse(test, predictions)\n    return error\n\n# score a model, return None on failure\ndef score_model(data, n_test, cfg, debug=False):\n    result = None\n    # convert config to a key\n    key = str(cfg)\n    # show all warnings and fail on exception if debugging\n    if debug:\n        result = walk_forward_validation(data, n_test, cfg)\n    else:\n        # one failure during model validation suggests an unstable config\n        try:\n            # never show warnings when grid searching, too noisy\n            with catch_warnings():\n                filterwarnings(\"ignore\")\n                result = walk_forward_validation(data, n_test, cfg)\n        except:\n            error = None\n    # check for an interesting result\n#    if result is not None:\n#        print(' > Model[%s] %.3f' % (key, result))\n    return (key, result)\n\n# grid search configs\ndef grid_search(data, cfg_list, n_test, parallel=True):\n    scores = None\n    if parallel:\n        # execute configs in parallel\n        executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n        tasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)\n        scores = executor(tasks)\n    else:\n        scores = [score_model(data, n_test, cfg) for cfg in cfg_list]\n    # remove empty results\n    scores = [r for r in scores if r[1] != None]\n    # sort configs by error, asc\n    scores.sort(key=lambda tup: tup[1])\n    return scores\n\n# create a set of sarima configs to try\ndef sarima_configs(seasonal=[0]):\n    models = list()\n    # define config lists\n    p_params = range(2)\n    d_params = range(2)\n    q_params = range(5)\n    t_params = ['n','c','t','ct']\n    P_params = range(5)\n    D_params = range(2)\n    Q_params = range(2)\n    m_params = range(2)\n    # create config instances\n    for p in p_params:\n        for d in d_params:\n            for q in q_params:\n                for t in t_params:\n                    for P in P_params:\n                        for D in D_params:\n                            for Q in Q_params:\n                                for m in m_params:\n                                    cfg = [(p, d, q), (P, D, Q, m), t]\n                                    models.append(cfg)\n    return models","34525ba1":"# define dataset\n#data = data['y'].values\nprint(data.shape)\n# data split\nn_test = 10\n# model configs\ncfg_list = sarima_configs()\nprint('Configurations: ', len(cfg_list))\n# grid search\nscores = grid_search(data['y'].values, cfg_list, n_test)\nprint('done')\n# list top 3 configs\nfor cfg, error in scores[:3]:\n    print(cfg, error)","09c8509f":"# SARIMA & Grid Search"}}