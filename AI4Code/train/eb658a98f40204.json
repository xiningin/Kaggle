{"cell_type":{"df44bfed":"code","7f9f44d5":"code","2dbfaa6d":"code","d19d774d":"code","ea76a55e":"code","33b90902":"code","432e84ae":"markdown"},"source":{"df44bfed":"import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Linear, LayerNorm, ReLU, Dropout\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nimport os\nimport copy\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom torch.utils.data import Dataset,TensorDataset, DataLoader,RandomSampler\nimport time,datetime\n","7f9f44d5":"def Get_nowtime(fmat='%Y-%m-%d %H:%M:%S'):\n    return datetime.datetime.strftime(datetime.datetime.now(),fmat)\n\ndef Metric(target,pred):\n    metric = 0\n    for i in range(target.shape[-1]):\n        metric += (np.sqrt(np.mean((target[:,:,i]-pred[:,:,i])**2))\/target.shape[-1])\n    return metric\n\ndef Write_log(logFile,text,isPrint=True):\n    if isPrint:\n        print(text)\n    logFile.write(text)\n    logFile.write('\\n')\n    return None\n\ndef Seed_everything(seed=1017):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSeed_everything()\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n\n    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n        print(classname)\n        try:\n            nn.init.xavier_uniform_(m.weight)\n        except:\n            pass\n        try:\n            nn.init.zeros_(m.bias)\n        except:\n            pass\n    if classname.find('GRU') != -1 or classname.find('LSTM') != -1:\n        print(classname)\n        for name, param in m.named_parameters():\n            print(name)\n            if 'bias_ih' in name:\n                 torch.nn.init.zeros_(param)\n            elif 'bias_hh' in name:\n                torch.nn.init.zeros_(param)\n            elif 'weight_ih' in name:\n                 nn.init.xavier_uniform_(param)\n            elif 'weight_hh' in name:\n                 nn.init.orthogonal_(param)","2dbfaa6d":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\ndef mcrmse(y_actual, y_pred, weight=None, num_scored=5):\n    score = 0\n    for i in range(5):\n        if weight is not None:\n            score += torch.sqrt(torch.mean((y_actual[:,:,i]-y_pred[:,:,i])**2*weight)) \/ num_scored\n        else:\n            score += torch.sqrt(torch.mean((y_actual[:,:,i]-y_pred[:,:,i])**2)) \/ num_scored\n    return score\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)\n","d19d774d":"\ntrain = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn\n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr\n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        num_target=5\n        self.cate_emb = nn.Embedding(14,100)\n        self.gru = nn.GRU(100*3+3, 256, num_layers=1, batch_first=True, dropout=0.5, bidirectional=True)\n        self.gru1 = nn.GRU(512, 256, num_layers=1, batch_first=True, dropout=0.5, bidirectional=True)\n        self.predict = nn.Linear(512,num_target)\n\n    #https:\/\/discuss.pytorch.org\/t\/clarification-regarding-the-return-of-nn-gru\/47363\/2\n    def forward(self, cateX,contX):\n        cate_x = self.cate_emb(cateX).view(cateX.shape[0],cateX.shape[1],-1)\n        sequence = torch.cat([cate_x,contX],-1)\n        x, h  = self.gru(sequence)\n        x, h = self.gru1(x)\n        x = F.dropout(x,0.5,training=self.training)\n        predict = self.predict(x)\n        return predict\n\ndef train_and_predict(type = 0, FOLD_N = 5):\n\n    gkf = GroupKFold(n_splits=FOLD_N)\n    device = torch.device('cuda:%s'%0 if torch.cuda.is_available() else 'cpu')\n\n    log = open('.\/train.log','w',1)\n    all_best_model = []\n    oof = []\n    for fold, (train_index, valid_index) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n        Write_log(log,'fold %s train start:%s'%(fold,Get_nowtime()))\n        t_train = train.iloc[train_index]\n        #t_train = t_train[t_train['SN_filter'] == 1]\n        train_x = preprocess_inputs(t_train)\n        train_cate_x = torch.LongTensor(train_x[:,:,:3])\n        train_cont_x = torch.Tensor(train_x[:,:,3:])\n        train_y = torch.Tensor(np.array(t_train[pred_cols].values.tolist()).transpose((0, 2, 1)))\n        w_train = torch.Tensor(np.log(t_train['signal_to_noise'].values.reshape(-1,1)+1.1)\/2)\n\n        t_valid = train.iloc[valid_index]\n        t_valid = t_valid[t_valid['SN_filter'] == 1]\n        valid_x = preprocess_inputs(t_valid)\n        valid_count = valid_x.shape[0]\n        valid_cate_x = torch.LongTensor(valid_x[:,:,:3])\n        valid_cont_x = torch.Tensor(valid_x[:,:,3:])\n        valid_y = torch.Tensor(np.array(t_valid[pred_cols].values.tolist()).transpose((0, 2, 1)))\n\n\n        train_data = TensorDataset(train_cate_x,train_cont_x,train_y,w_train)\n        train_data_loader = DataLoader(dataset=train_data,shuffle=True,batch_size=64,num_workers=1)\n        valid_data = TensorDataset(valid_cate_x,valid_cont_x,valid_y)\n        valid_data_loader = DataLoader(dataset=valid_data,shuffle=False,batch_size=32,num_workers=1)\n        \n        valid_y = valid_y.numpy()\n\n        model = Net()\n        model.apply(weights_init)\n        model = model.to(device)\n        #model_params = [p for p in model.parameters() if p.requires_grad]\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        all_valid_metric = []\n        all_epoch_valid_metric = []\n        not_improve_epochs = 0\n        best_valid_metric = 1e9\n        for epoch in range(60):\n            running_loss = 0.0\n            t0 = datetime.datetime.now()\n            model.train()\n            for n,data in enumerate(train_data_loader):\n                cate_x,cont_x,y,weight = [x.to(device) for x in data]\n                outputs = model(cate_x,cont_x)\n                optimizer.zero_grad()\n                loss = mcrmse(y,outputs[:,:68,:],weight)\n                loss.backward()\n                #torch.nn.utils.clip_grad_norm(model.parameters(),1.0)\n                optimizer.step()\n                running_loss += loss.item()\n            running_loss = running_loss \/ n\n\n            valid_loss = 0.0\n            all_pred = []\n            model.eval()\n            for data in valid_data_loader:\n                cate_x,cont_x,y = [x.to(device) for x in data]\n                outputs = model(cate_x,cont_x)\n                all_pred.append(outputs.detach().cpu().numpy())\n                loss = mcrmse(y,outputs[:,:68,:])\n                valid_loss += (loss.item() * cate_x.shape[0])\n            valid_loss = valid_loss \/ valid_count\n            all_pred = np.concatenate(all_pred,0)\n            valid_metric =  Metric(valid_y[:,:,[0,1,3]],all_pred[:,:68,[0,1,3]])\n            all_epoch_valid_metric.append(valid_metric)\n            t1 = datetime.datetime.now()\n            Write_log(log,'epoch %s | train mean loss:%.6f | valid loss:%.6f | valid metric:%.6f | \u23f0:%ss'%(str(epoch).rjust(3),running_loss,valid_loss,valid_metric,(t1-t0).seconds))\n            if valid_metric < best_valid_metric:\n                Write_log(log,'[epoch %s] save better model'%(epoch))\n                torch.save(model.state_dict(),'.\/gru-cate-emb-100-fold-%s.cpkt'%(fold))\n                best_valid_metric = valid_metric\n                best_model = copy.deepcopy(model.state_dict())\n                not_improve_epochs = 0\n            else:\n                not_improve_epochs += 1\n                Write_log(log,'Not improve epoch +1 ---> %s'%not_improve_epochs)\n\n        all_best_model.append(best_model)\n        model.load_state_dict(best_model)\n        model.eval()\n        all_id = []\n        all_y_id = []\n        for i,row in t_valid.iterrows():\n            for j in range(row['seq_length']):\n                all_id.append(row['id']+'_%s'%j)\n            for k in range(len(row['reactivity'])):\n                all_y_id.append(row['id']+'_%s'%k)\n\n        all_id = np.array(all_id).reshape(-1,1)\n        all_y_id = np.array(all_y_id).reshape(-1,1)\n        all_pred = []\n\n        for data in valid_data_loader:\n            cate_x,cont_x,y = [x.to(device) for x in data]\n            outputs = model(cate_x,cont_x)\n            all_pred.append(outputs.detach().cpu().numpy())\n        all_pred = np.concatenate(all_pred,0)\n        t_valid_metric = Metric(valid_y[:,:,[0,1,3]],all_pred[:,:68,[0,1,3]])\n        t_oof = pd.DataFrame(all_pred.reshape(-1,5),columns=['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C'])\n        t_oof['id_seqpos'] = all_id\n        t_target_df = pd.DataFrame(valid_y.reshape(-1,5),columns=['label_reactivity','label_deg_Mg_pH10','label_deg_pH10','label_deg_Mg_50C','label_deg_50C'])\n        t_target_df['id_seqpos'] = all_y_id\n        t_oof = t_oof.merge(t_target_df,how='left',on='id_seqpos')\n        all_valid_metric.append(t_valid_metric)\n        Write_log(log,'fold %s valid metric:%.6f'%(fold,t_valid_metric))\n        oof.append(t_oof)\n    oof = pd.concat(oof)\n    oof_metirc = 0\n    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n        oof_metirc += (np.sqrt(np.mean((oof.loc[~oof['label_%s'%col].isna(),'label_%s'%col].values-oof.loc[~oof['label_%s'%col].isna(),col].values)**2)) \/ 3.0)\n    log.close()\n    os.rename('.\/train.log','gru-cate-emb-100-0.6%f-%.6f.log'%(np.mean(all_valid_metric),oof_metirc))\n\n    # predict test\n    def Pred(df):\n        test_x = preprocess_inputs(df)\n        test_cate_x = torch.LongTensor(test_x[:,:,:3])\n        test_cont_x = torch.Tensor(test_x[:,:,3:])\n        test_data = TensorDataset(test_cate_x,test_cont_x)\n        test_data_loader = DataLoader(dataset=test_data,shuffle=False,batch_size=64,num_workers=1)\n        all_id = []\n        for i,row in df.iterrows():\n            for j in range(row['seq_length']):\n                all_id.append(row['id']+'_%s'%j)\n\n        all_id = np.array(all_id).reshape(-1,1)\n        all_pred = np.zeros(len(all_id)*5).reshape(len(all_id),5)\n        for fold in range(FOLD_N):\n            model.load_state_dict(all_best_model[fold])\n            model.eval()\n            t_all_pred = []\n            for data in test_data_loader:\n                cate_x,cont_x = [x.to(device) for x in data]\n                outputs = model(cate_x,cont_x)\n                t_all_pred.append(outputs.detach().cpu().numpy())\n            t_all_pred = np.concatenate(t_all_pred,0)\n            all_pred += t_all_pred.reshape(-1,5)\n        all_pred \/= FOLD_N\n        sub = pd.DataFrame(all_pred,columns=['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C'])\n        sub['id_seqpos'] = all_id\n        return sub\n    public_sub = Pred(test.loc[test['seq_length']==107])\n    private_sub = Pred(test.loc[test['seq_length']==130])\n    sub = pd.concat([public_sub,private_sub]).reset_index(drop=True)\n    return oof[['id_seqpos']+['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C']+['label_reactivity','label_deg_Mg_pH10','label_deg_pH10','label_deg_Mg_50C','label_deg_50C']],\\\n           sub[['id_seqpos']+['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C']]\n","ea76a55e":"from sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[:,:,0])\ntrain['cluster_id'] = kmeans_model.labels_\n","33b90902":"oof,sub = train_and_predict()\noof.to_csv('.\/oof.csv',index=False)\nsub.to_csv('.\/submission.csv',index=False)","432e84ae":"**I converted [GRU+LSTM with feature engineering and augmentation](https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation) to PyTorch(not augment). But the result is worse than Keras. Anyone knows why?**"}}