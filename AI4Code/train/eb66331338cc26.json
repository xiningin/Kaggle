{"cell_type":{"5da7db08":"code","332d54ed":"code","4626baa5":"code","c38085c7":"code","3e7fa8c3":"code","4c872176":"code","acebd278":"code","db5ea390":"code","8d69ec8b":"code","26a1b637":"code","2c995aa8":"code","2975d93d":"code","8e45df06":"code","f676581a":"code","81a5e949":"code","9f07def4":"code","22c1ac75":"markdown","ff83932e":"markdown","c05c2ab4":"markdown","467d548d":"markdown","27d94cf9":"markdown","c3147195":"markdown","35dcbe4b":"markdown","8a0f7b22":"markdown","bc861dbd":"markdown"},"source":{"5da7db08":"import numpy as np\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt\nimport random\n\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nimport warnings\n\nwarnings.simplefilter('ignore')","332d54ed":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm","4626baa5":"SEED = 824\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","c38085c7":"# Hyper-Paremeters\nNUM_FOLDS = 5\nNUM_EPOCHS = 20\nMAX_LEN = 314\nBATCH_SIZE = 16\nLR = 1e-5\n\nFILE_PATH = {\n    'train': '..\/input\/commonlitreadabilityprize\/train.csv',\n    'test': '..\/input\/commonlitreadabilityprize\/test.csv',\n    'submit': '..\/input\/commonlitreadabilityprize\/sample_submission.csv'\n}\nTOKEN_PATH = '..\/input\/huggingface-bert\/bert-base-cased'\nDEVICE = 'cuda' if torch.cuda.is_available else 'cpu'\nprint('Using ',DEVICE)","3e7fa8c3":"# train: id, url_legal, license, excerpt, target, std_error\n# test: id, url_legal, license, excerpt\ntrain_df = pd.read_csv(FILE_PATH['train'])\ntest_df = pd.read_csv(FILE_PATH['test'])\nprint(train_df.nunique(), '\\n')\ntrain_df.head()","4c872176":"tokenizer = transformers.BertTokenizer.from_pretrained(TOKEN_PATH)\nsample_excerpt = train_df['excerpt'].iloc[1]\n\nsample_token = tokenizer.encode_plus(\n    sample_excerpt,\n    add_special_tokens=True,\n    max_length = MAX_LEN,\n    pad_to_max_length = True,\n    truncation=True) # decode by 'tokenizer.decode(sample_token['input_ids'])'\n#sample_token","acebd278":"def create_folds(df, num_folds):\n    df['fold'] = -1\n    \n    # shuffle rows with inplacement and reset index, \n    # where drop=True prevent create a new column to store the old index\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    # Sturge's rule to determine the number of bins\n    nums_bin = int(1 + 3.22 * np.log10(len(df)))\n    \n    df.loc[:, 'bins'] = pd.cut(df['target'], bins=nums_bin, labels=False)\n    kf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n    for fold, (train_id, val_id) in enumerate(kf.split(X=df, y=df.bins.values)):\n        df.loc[val_id, 'fold'] = fold\n    \n    df = df.drop('bins', axis=1)\n    return df\ntrain_df = create_folds(train_df, NUM_FOLDS)\nprint(train_df.fold.value_counts())\ntrain_df.head(8)","db5ea390":"class MyDataset(Dataset):\n    def __init__(self, sentences, targets, is_train=True):\n        self.sentences = sentences\n        self.targets = targets if is_train else None\n        self.is_train = is_train\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self, index):\n        s = self.sentences[index]\n        if self.is_train:\n            t = self.targets[index]\n        \n        # tokenization\n        token_s = tokenizer.encode_plus(\n            s, # the sentence\n            add_special_tokens = True,\n            max_length = MAX_LEN,\n            pad_to_max_length = True,\n            return_attention_mask = True,\n            truncation=True)\n        if self.is_train:\n            target_tensor = torch.tensor(t, dtype=torch.float)\n        ids = torch.tensor(token_s['input_ids'], dtype = torch.long)\n        mask = torch.tensor(token_s['attention_mask'], dtype = torch.long)\n        \n        if self.is_train:\n            return {'ids' : ids, 'mask' : mask,'targets' : target_tensor}\n        else:\n            return {'ids' : ids, 'mask' : mask}","8d69ec8b":"model = transformers.BertForSequenceClassification.from_pretrained(TOKEN_PATH, num_labels=1)\nmodel.to(DEVICE)\nmodel_original_stat_dict = model.state_dict()","26a1b637":"from collections import defaultdict\ndef train(model, dataloaders, optimizer, scheduler):\n    scores = defaultdict(list)\n    losses = defaultdict(list)\n    best_model_wts = None\n    best_score = float('inf')\n    \n    for epoch in tqdm(range(NUM_EPOCHS)):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train(True)\n            else:\n                model.train(False)\n        \n            preds, targets, epoch_losses = [], [], [] # store info at each epoch\n        \n            for data in dataloaders[phase]:\n                optimizer.zero_grad()\n\n                ids = data['ids'].to(DEVICE)\n                mask = data['mask'].to(DEVICE)\n                target = data['targets'].to(DEVICE)\n\n                output = model(ids, mask)\n                output = output['logits'].squeeze(-1)\n                loss = torch.sqrt(nn.MSELoss()(output, target))\n\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                epoch_losses.append(loss.item())\n                preds.append(output.detach().cpu().numpy())\n                targets.append(target.detach().cpu().numpy())\n\n            preds = np.concatenate(preds)\n            targets = np.concatenate(targets)\n            \n            losses[phase].append(np.mean(epoch_losses))\n            \n            score = np.sqrt(mean_squared_error(preds, targets))\n            scores[phase].append(score)\n            \n            if phase == 'val' and score < best_score:\n                best_score = score\n                best_model_wts = model.state_dict()\n            \n    print('Best score:',best_score)\n    return best_model_wts, best_score, [scores, losses]","2c995aa8":"infos = {} # to store losses and scores of each model\nfor cur_fold in range(NUM_FOLDS):\n    print('-'*5, 'start {}'.format(cur_fold), '-'*5)\n    \n    model.load_state_dict(model_original_stat_dict)\n    # dataset \n    p_train = train_df[train_df['fold'] != cur_fold].reset_index(drop=True)\n    p_valid = train_df[train_df['fold'] == cur_fold].reset_index(drop=True)\n\n    train_dataset = MyDataset(p_train['excerpt'], p_train['target'])\n    val_dataset = MyDataset(p_valid['excerpt'], p_valid['target'])\n\n    train_dataLoader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    val_dataLoader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    dataLoaders = {'train': train_dataLoader, 'val': val_dataLoader}\n\n    # setup optimizer and scheduler\n    optimizer = AdamW(model.parameters(), LR, betas=(0.9, 0.99), weight_decay=1e-2)\n    train_steps = len(p_train) \/\/ BATCH_SIZE * NUM_EPOCHS\n    num_steps = int(train_steps\/10) # decay at each 10% steps\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\n    model_stat_dict, score, info = train(model, dataLoaders, optimizer, scheduler)\n    infos[cur_fold] = info\n    \n    save_path = f'model_{cur_fold}.pth'\n    torch.save(model_stat_dict, save_path)","2975d93d":"def get_preds(dataloader, model):\n    preds = []\n    with torch.no_grad():\n        for data in dataloader:\n            ids = data[\"ids\"].to(DEVICE)\n            mask = data[\"mask\"].to(DEVICE)\n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n            preds.append(output.detach().cpu().numpy())\n    preds = np.concatenate(preds)  \n    return preds            ","8e45df06":"test_df = pd.read_csv(FILE_PATH['test'])\nmodel_path = [f'model_{i}.pth' for i in range(5)]\ntest_dataset = MyDataset(test_df['excerpt'], None, False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\nall_preds = []\nfor fold in range(5):\n    model_path = f'model_{fold}.pth'\n    model.load_state_dict(torch.load(model_path))   \n    all_preds.append(get_preds(test_loader, model))\nall_preds","f676581a":"score = pd.DataFrame(all_preds).T.mean(axis=1)\nscore","81a5e949":"sample = pd.read_csv(FILE_PATH['submit'])\nsample['target'] = score\nsample","9f07def4":"sample.to_csv(\"submission.csv\",index = False)","22c1ac75":"# About this notebook\n\u8fd9\u4e2anotebook\u66f4\u591a\u7684\u662f\u5bf9\u6bd4\u8d5b\u548cBert\u6a21\u578b\u7684**\u719f\u6089\u3001\u5b66\u4e60**\uff0c\u5728\u76ee\u524d\u9636\u6bb5\u66f4\u591a\u7684\u662f\u53c2\u8003\u522b\u4eba\u5199\u7684\u4ee3\u7801\uff0c\u719f\u6089\u6574\u4e2a\u6bd4\u8d5b\u548c\u5efa\u6a21\u6d41\u7a0b\uff0c\u540e\u7eed\u53ef\u4ee5\u53c2\u8003\u5176\u4ed6paper\u6216\u8005\u5176\u4ed6\u6280\u5de7\u63d0\u5347\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u6027\u80fd\n\n## Idea:\nThe main idea of this model, it uses the huggingFace pretrain model as the tokenizer and the regression model as well\n## Goal: \n| \u6b64\u9636\u6bb5\u53ea\u8ffd\u6c42\u5b8c\u6210\uff0cstart small, step by step\u5efa\u7acb\u6a21\u578b\n- [x] \u7b80\u6613EDA\n- [x] \u5efa\u7acbbaseline model\n- [x] Submit\n\n# Reference:\n- [BERT Biginner](https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\/notebook)\n- [LightWeight RoBerta](https:\/\/www.kaggle.com\/andretugan\/lightweight-roberta-solution-in-pytorch\/notebook#Dataset)","ff83932e":"# 0.Setup","c05c2ab4":"# 1.Simple EDA of data","467d548d":"# 4. Build BERT Model","27d94cf9":"## 3.2 setup dataset and dataLoader","c3147195":"The output of tokenizer:\n- input_ids: Words id. Special: 101[CLS], 102[SEP]: begin and end of sentence\n    - 102[SEP]: seperate the sentence, also can represent start of next sentence\n- token_type_ids:  Binary mask to grasp sentences: This time it is a regression problem, all 0. When looking at the connection between sentences, change it by inserting [SEP] etc. in the middle.\n    - token_type_ids \u53ef\u9009\u3002\u5c31\u662f token \u5bf9\u5e94\u7684\u53e5\u5b50id\uff0c\u503c\u4e3a0\u62161\uff080\u8868\u793a\u5bf9\u5e94\u7684token\u5c5e\u4e8e\u7b2c\u4e00\u53e5\uff0c1\u8868\u793a\u5c5e\u4e8e\u7b2c\u4e8c\u53e5\uff09\u3002\u5f62\u72b6\u4e3a(batch_size, sequence_length)\u3002\n- Attention_mask: 0 if element == [PAD] else 1, for mask modeling","35dcbe4b":"# 3. Design model\n## 3.1 Divide data into KFold\n### Reference:\n- [Create Folds](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds)\n- [Lightweight Roberta](https:\/\/www.kaggle.com\/andretugan\/lightweight-roberta-solution-in-pytorch\/notebook#Model)\n- [BERT beginner](https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\/notebook#2.-BERT:-Deepen-your-understanding-of-Tokenizer)","8a0f7b22":"# 6. Submittion","bc861dbd":"# 5. Train Function\n**Load dataset**\n> Note: currently only using one fold for testing\n- val: fold 0, train: remaining folds\n\n**Input Variables of train_function**\n- dataLoader\n- model\n- optimizer\n- scheduler"}}