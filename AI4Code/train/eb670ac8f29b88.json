{"cell_type":{"15546853":"code","99b2bfde":"code","24dff9f4":"code","9f654fd8":"code","e4bceee3":"code","6d6302e1":"code","53ae064a":"code","6f298abc":"code","abc79eea":"code","e39b2e9c":"markdown"},"source":{"15546853":"import sys\npackage_dir = \"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","99b2bfde":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","24dff9f4":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","9f654fd8":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nbert_config = BertConfig('..\/input\/bert-inference\/bert\/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)","e4bceee3":"test_df = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","6d6302e1":"model = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"..\/input\/bertnew\/bert_pytorch.bin\"))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","53ae064a":"test_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","6f298abc":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})\nsubmission.to_csv('submission.csv', index=False)","abc79eea":"submission.head()","e39b2e9c":"This is the inference kernel for BERT pytorch. Check out the amazing Kernel for finetuning BERT by Yuval: https:\/\/www.kaggle.com\/yuval6967\/toxic-bert-plain-vanila"}}