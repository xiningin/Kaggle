{"cell_type":{"8d453d0d":"code","77caacd6":"code","b6093c46":"code","1c21bd13":"code","a794e635":"code","cf49df3c":"code","fe38f59c":"code","eb3ecd21":"code","ca83e347":"code","be3ca1ef":"code","df925eaa":"code","528ea05a":"code","8d97446b":"code","85890525":"code","2bb56533":"code","63d585d0":"code","f6a005f4":"code","d3700a2a":"code","0de47675":"code","a240077f":"code","6abc129f":"code","4e448d72":"code","0734e551":"code","89a55f60":"code","da7e6289":"code","31825eb7":"code","775c3c38":"code","5eca1c33":"code","109ed6bf":"code","74701710":"code","609c09e9":"code","93b30f0f":"code","31897a19":"code","3de8c6a8":"code","50ccb49b":"code","5b494826":"code","b06ff520":"code","9c6cd71d":"code","16aad3d6":"code","8b96ee38":"code","1d6f7d1d":"code","1e893dbc":"code","d1520f35":"code","7f69efe0":"code","cb7845d6":"code","bed1557e":"code","f43a7450":"code","587441bd":"code","5966e17b":"code","3ae683ae":"code","1d1f12cf":"code","43641f7a":"code","cfe7f32c":"code","5ac90845":"code","64c08a06":"code","d57e9554":"code","a3c13fc2":"code","24babc17":"code","33522d4d":"code","a4164319":"code","09b12405":"code","36a48b42":"code","3f8a41d4":"code","2a2921be":"code","72108f12":"code","e3ab3940":"code","3b26d7e1":"code","61e6d752":"code","8b3ba629":"code","f43b99b0":"code","8006f830":"code","4f978f47":"code","3ba41b21":"code","dd0e043a":"code","7d821d30":"code","797bff0f":"code","afd01e60":"code","ae4d817d":"code","cebea40e":"code","69d70a96":"code","29d37e3a":"code","35e17726":"code","6e20568b":"code","dc8fc2fd":"code","044bfb78":"code","d12fc915":"code","9e32600b":"code","5c7e15de":"code","b98316f1":"code","2229b7d1":"code","0854906b":"code","f7afe507":"code","da94eb40":"code","2869abd1":"code","181b7b5d":"code","64a14097":"code","b0b9eb91":"markdown","dad4d857":"markdown","03babc70":"markdown","d5951ee1":"markdown","dda5df56":"markdown","12b52156":"markdown","71d9bc24":"markdown","a9e0104d":"markdown","d2422788":"markdown","24175063":"markdown","b0221429":"markdown","033e7123":"markdown","b0a613b7":"markdown","50123c2d":"markdown","5634e883":"markdown","08b986c2":"markdown","e372f9b8":"markdown","a861c455":"markdown","472d7b9b":"markdown","394f7831":"markdown","a2902c05":"markdown","96e0e0fd":"markdown","dc842fdd":"markdown","5c316ccc":"markdown","0b713707":"markdown","fa3a3ab4":"markdown","9294fd42":"markdown","6b6982b5":"markdown","3ece2599":"markdown","dd58e3f5":"markdown","1fa529cb":"markdown","e8a38c99":"markdown","ec7138fc":"markdown","8053ea5f":"markdown","53c13efb":"markdown","3de325a2":"markdown","ceb1dc17":"markdown","6f3e0114":"markdown","865a2eb2":"markdown","845f63c3":"markdown","a8ab089b":"markdown","2818ebb4":"markdown","4e8af120":"markdown","e27356f3":"markdown","08a98916":"markdown","08b28abe":"markdown","12dd4c50":"markdown","ed6a75c1":"markdown","44021f8e":"markdown","de9697c2":"markdown","47c6d010":"markdown","4610c1e0":"markdown","41d4076c":"markdown","691d7e5c":"markdown","5ab7da80":"markdown","507c9455":"markdown","363ffac9":"markdown","55f88b38":"markdown","31bd5522":"markdown","7e4d0f69":"markdown","ee78fecb":"markdown","d98b69a8":"markdown","5413899c":"markdown","02914ef6":"markdown","15eef8fe":"markdown","827d8b39":"markdown","5842bfbb":"markdown","290dfd18":"markdown","c53b03fb":"markdown","a2b3b537":"markdown","b029d9b8":"markdown","d26dad4d":"markdown","ef8f422d":"markdown","51ea3080":"markdown","0d8120e4":"markdown","f47b742c":"markdown","3bdeb503":"markdown","4d09677a":"markdown","2bf65b68":"markdown","a0b6439f":"markdown","5220c580":"markdown","8939d7b2":"markdown","bef99300":"markdown","257e31a4":"markdown","b5f71975":"markdown","0da134cd":"markdown","195d67e7":"markdown"},"source":{"8d453d0d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","77caacd6":"import warnings\nwarnings.simplefilter('ignore')","b6093c46":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","1c21bd13":"!pip install pandas-profiling==2.11.0","a794e635":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix","cf49df3c":"data_titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_titanic.head(3)","fe38f59c":"data_health = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata_health.tail(3)","eb3ecd21":"data_water = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/PB_1996_2019_NH4.csv', sep=';')\ndata_water.tail(3)","ca83e347":"data_water['date'] = pd.to_datetime(data_water['Date'], format='%d.%m.%Y', errors='coerce').dt.to_period('m')\ndata_water = data_water[['ID_Station','date','NH4']]\ndisplay(data_water)","be3ca1ef":"data_nlp = pd.read_csv('..\/input\/nlp-reports-news-classification\/water_problem_nlp_ua_for_Kaggle_100.csv', delimiter=';', \n                 header=0, encoding='cp1251')\ndata_nlp.tail(3)","df925eaa":"df = pd.read_csv('..\/input\/nlp-reports-news-classification\/water_problem_nlp_ua_for_Kaggle_100.csv', delimiter=';', \n                 header=0, encoding='cp1251', \n                 dtype = {'text': str, \n                          'env_problems': 'Int64',\n                          'pollution': 'Int64', \n                          'treatment': 'Int64',\n                          'climate': 'Int64',\n                          'biomonitoring': 'Int64'})\ndf.head(10)","528ea05a":"df = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/PB_1996_2019_NH4.csv', sep=';', \n                 skiprows = lambda x: x>0 and np.random.rand() > 0.01)\nprint(\"The shape of the df is {}. It has been reduced 100 times!\".format(df.shape))\n\n\n'''\nHow it works:\nskiprows accepts a function that is evaluated against the integer index.\nx > 0 makes sure that the headers is not skipped\nnp.random.rand() > 0.01 returns True 99% of the tie, thus skipping 99% of the time.\nNote that we are using skiprows\n'''\ndf.head(3)","8d97446b":"df = pd.read_csv('..\/input\/nlp-reports-news-classification\/water_problem_nlp_ua_for_Kaggle_100.csv', delimiter=';', \n                 header=0, encoding='cp1251', low_memory=True,\n                 dtype = {'text': str, \n                          'env_problems': 'Int64',\n                          'pollution': 'Int64', \n                          'treatment': 'Int64',\n                          'climate': 'Int64',\n                          'biomonitoring': 'Int64'})\ndf.head(3)","85890525":"# Download one file\nimport requests\nprint(f'Download confirmed daily data from RNBO of Ukraine')\nmyfile = requests.get('https:\/\/api-covid19.rnbo.gov.ua\/charts\/main-data?mode=ukraine')\nopen('filename', 'wb').write(myfile.content)\ndata_covid = pd.read_json('filename')\ndata_covid.tail(5)","2bb56533":"# Download some files\nprint('Download daily data for some regions of Ukraine (Kyiv - 4909, Lviv.reg. - 4895, Vinn.reg - 4907)')\nfor filename in ['country=4909', 'country=4895', 'country=4907']:\n    myfile = requests.get(f'https:\/\/api-covid19.rnbo.gov.ua\/charts\/main-data?mode=ukraine&{filename}')\n    open('filename', 'wb').write(myfile.content)\n    data_covid_region = pd.read_json('filename')\n    display(data_covid_region.tail(3))","63d585d0":"df = data_titanic.copy()\ndf.info()","f6a005f4":"# df.loc[], df.iloc\ndf.iloc[2:5, :].loc[:, \"Name\":\"Fare\"]","d3700a2a":"df.head(3)","0de47675":"df.iloc[2, 5]","a240077f":"df.loc[2, 'Age']","6abc129f":"# data.at\npd.set_option('max_colwidth', 1000)\ndf_all = pd.DataFrame(columns=['data'], index=[0, 1])\ndf_all.at[0,'data'] = data_titanic.copy()\ndf_all.at[1,'data'] = data_covid.copy()\ndf_all.at[2,'data'] = df.Name.tolist()\ndf_all.at[3,'data'] = df.Age.values\ndf_all","4e448d72":"df_all.at[0, 'data'].head(3)","0734e551":"# Select multiple slices of columns from a df\ncols_str = list(map(str, list(df.columns))) # so that we can do df[\"0\"] as string for the example\ndf.columns = cols_str\n\n# Using pandas concatenation\n# if you are ever confused about axis = 1 or axis = 0, just put axis = \"columns\" or axis = \"rows\"\ndisplay(pd.concat([df.loc[:, \"PassengerId\":\"Pclass\"], df.loc[:, \"Sex\":\"SibSp\"]], axis = \"columns\").head(3))\n\n# Using lists\n# please ntoe that df.columns is a series with index, so we are using index to filter #\ndisplay(df[list(df.columns[0:3]) + list(df.columns[4:7])].head(3))\n\n# Using numpy\ndisplay(df.iloc[:, np.r_[0:3, 4:7]].head(3)) # probably the most beautiful solution","89a55f60":"# Sets the value of the specified option.\npd.set_option('max_columns',100)\npd.set_option('max_rows',900)","da7e6289":"data_titanic.head(8)","31825eb7":"pd.set_option('max_colwidth',200)\ndata_nlp.head(1)","775c3c38":"# Reset all of them options to default\npd.reset_option('all')","5eca1c33":"del df_all","109ed6bf":"import gc\ngc.collect()","74701710":"%%time\n# Time execution of a Python statement or expression in the cell","609c09e9":"df = data_titanic.copy()\ndf.head()\ndf.shape\n\n# absolute values\n(df[\"Age\"] < 18).sum()\nprint(\"In the columns Age we have {} of rows that are below 18\".format((df[\"Age\"] < 18).sum()))\n\n# mean value\n(df[\"Age\"] < 18).mean()\nprint(\"In the columns Age the values that are below 18 represent {}%\".format((df[\"Age\"] < 18).mean()))","93b30f0f":"d = {\"class\": [\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"D\", \"E\", \"F\"]}\ndf = pd.DataFrame(d)\ndf\n\n# Step 1: count the frequencies\nfrequencies = df[\"class\"].value_counts(normalize = True)\nprint(frequencies)\n\n# Step 2: establish your threshold and filter the smaller categories\nthreshold = 0.1\nsmall_categories = frequencies[frequencies < threshold].index\nprint(small_categories)\n\n# Step 3: replace the values\ndf[\"class\"] = df[\"class\"].replace(small_categories, \"Other\")\ndf[\"class\"].value_counts(normalize = True)","31897a19":"df = data_titanic.copy()\n# Solution 1\nprint('df.isnull().sum().sum()')\nprint(df.isnull().sum().sum(), \"\\n\\n\")\n\n# Solution 2\nprint('df.isna().sum()\\n')\nprint(df.isna().sum(), \"\\n\\n\")\n\n# Solution 3\nprint('df.isna().any()\\n')\nprint(df.isna().any(), \"\\n\\n\")\n\n# Solution 4:\ndf.isna().any(axis = None)","3de8c6a8":"# Do some fast feature eng on the DF\nd = {\"gender\":[\"male\", \"female\", \"male\"], \"color\":[\"red\", \"green\", \"blue\"], \"age\":[25, 30, 15]}\ndf = pd.DataFrame(d)\ndf\n\n# Solution\ndf[\"gender_mapped\"] = df[\"gender\"].map({\"male\":\"M\", \"female\":\"F\"}) # using dictionaries to map values\ndf[\"gender_mapped\"] = df[\"gender\"].map({\"female\":0, \"male\":1}) # using dictionaries to map values\ndf[\"color_factorized\"] = df[\"color\"].factorize()[0] # using factorize: returns a tuple of arrays (array([0, 1, 2]), Index(['red', 'green', 'blue'], dtype='object')) that's why we select [0]\ndf[\"age_compared_boolean\"] = df[\"age\"] < 18 # return a True False boolean value\ndf[\"age_str\"] = df[\"age\"].astype('str')\n\ndf","50ccb49b":"# Create DataFrame with types as in other\n#df2 = df2.astype(df1.dtypes.to_dict())","5b494826":"data_titanic_num = data_titanic[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Embarked']].copy()\ndata_titanic_num[\"Sex\"] = data_titanic_num[\"Sex\"].map({\"female\":0, \"male\":1})\ndata_titanic_num[\"Embarked\"] = data_titanic_num[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\": 2})\ndata_titanic_num = data_titanic_num.dropna()  # without NAN \ndata_titanic_num","b06ff520":"data_titanic['Age'].replace([np.inf, -np.inf], np.nan).fillna(-1)","9c6cd71d":"data_titanic.shape","16aad3d6":"data_titanic.dropna().reset_index(drop=True)","8b96ee38":"data_titanic[['Name', 'Age']].dropna().reset_index(drop=True)","1d6f7d1d":"data_titanic.dropna(subset = ['Name', 'Age']).reset_index(drop=True)","1e893dbc":"data_health.describe()","d1520f35":"from pandas.plotting import scatter_matrix\nscatter_matrix(data_titanic, alpha=0.4, figsize=(10, 10), diagonal=\"kde\");","7f69efe0":"from pandas.plotting import andrews_curves\nplt.figure(figsize=(10, 10));\ndata_titanic_num_cleaning = data_titanic[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch']].fillna(-1)\ndata_titanic_num_cleaning['Age10'] = data_titanic_num_cleaning['Age'] \/\/ 10\nandrews_curves(data_titanic_num_cleaning, 'Age10');","cb7845d6":"data_water","bed1557e":"data_water = pd.pivot_table(data_water, values='NH4', index=['date'], columns='ID_Station')\ndata_water","f43a7450":"data_water = data_water[[27, 28, 29]]\ndata_water.columns = ['target', 'Kl', \"Khm\"]\ndata_water = data_water.dropna().reset_index(drop=False)\ndata_water","587441bd":"data_titanic.info()","5966e17b":"def get_numeric_features(df):\n    # Get numeric features from df\n    return df.select_dtypes(include=np.number).columns.tolist()","3ae683ae":"get_numeric_features(data_titanic)","1d1f12cf":"def get_categorical_features(df):\n    # Get categorical features from df\n    # see get_numeric_features in Tip 5.1\n    return list(set(df.columns.tolist()) - set(get_numeric_features(df)))","43641f7a":"get_categorical_features(data_titanic)","cfe7f32c":"data_titanic.head(3)","5ac90845":"from sklearn.preprocessing import LabelEncoder\ndef df_encoding(df):\n    #  Search and encoding categorical columns\n    categorical_columns = get_categorical_features(df) # see Tip 5.2\n    \n    for col in categorical_columns:\n        if col in df.columns:\n            le = LabelEncoder()\n            le.fit(list(df[col].astype(str).values))\n            df[col] = le.transform(list(df[col].astype(str).values))\n    \n    return df","64c08a06":"data_titanic = df_encoding(data_titanic)\ndata_titanic","d57e9554":"data_covid['confirmed'].plot()","a3c13fc2":"# Daily gain\ndata_covid['n_confirmed'] = data_covid['confirmed'].diff()\ndata_covid","24babc17":"data_covid['n_confirmed'].plot()","33522d4d":"# Sliding total for a week\ndata_covid['n_confirmed_week'] = data_covid['n_confirmed'].rolling(7).sum()\ndata_covid['n_confirmed_week']","a4164319":"data_covid['n_confirmed_week'].plot()","09b12405":"# Plot with all these features\n(data_covid['confirmed']\/data_covid['confirmed'].max()).plot()\n(data_covid['n_confirmed']\/data_covid['n_confirmed'].max()).plot()\n(data_covid['n_confirmed_week']\/data_covid['n_confirmed_week'].max()).plot()","36a48b42":"data_titanic.drop(columns = ['Ticket', 'Cabin'])","3f8a41d4":"# Dublicates removing\nprint(data_health.shape)\ndisplay(data_health.drop_duplicates())","2a2921be":"data_covid['date'] = pd.to_datetime(data_covid['dates'])\n# Forced conversion\npd.to_datetime(data_covid[\"dates\"], format = '%Y-%m-%d', errors='coerce')\ndata_covid.head(3)","72108f12":"data_covid.info()","e3ab3940":"# MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef df_minmax_scaler(df):\n    # Data Scalling\n    scaler = MinMaxScaler().fit(df)\n    df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n    return scaler, df","3b26d7e1":"scaler, df = df_minmax_scaler(data_health.copy())\ndf","61e6d752":"data_health.info()","8b3ba629":"# Data preparation for data_health (very simple)\ndata_h = data_health.copy()\ntarget_data_h = data_h.pop('target')\ndisplay(data_h.head(3))\ndata_h.info()","f43b99b0":"data_titanic.info()","8006f830":"# Data preparation for data_titanic (real - classification task)\nfrom sklearn.preprocessing import StandardScaler\n\ndata = data_titanic.copy()\ndata.index = data['PassengerId']  # it's need removed all id-features with unique values\ndata = data.drop(columns=['PassengerId'])\ndata = data.dropna()  # it's very simple but not good approach\ntarget_data = data.pop('Survived')\nscaler = StandardScaler()\ndata = pd.DataFrame(scaler.fit_transform(data), columns = data.columns)\ndisplay(data.head(3))\ndata.info()","4f978f47":"data_water.head(3)","3ba41b21":"# Data preparation for data_water (real - regression task)\ndata_w = data_water.copy()\ndata_w = data_w.drop(columns=['date'])\ntarget_data_w = data_w.pop('target')\nscaler_w = StandardScaler()\ndata_w = pd.DataFrame(scaler_w.fit_transform(data_w), columns = data_w.columns)\ndisplay(data_w.head(3))\ndata_w.info()","dd0e043a":"from sklearn.model_selection import train_test_split\n# For data_health\ntrain, test, target, target_test = train_test_split(data, target_data, test_size=0.2, random_state=0)\nprint(train.shape, test.shape)","7d821d30":"# For data_water\ntrain_w, test_w, target_w, target_test_w = train_test_split(data_w, target_data_w, test_size=0.2, random_state=0)\nprint(train_w.shape, test_w.shape)","797bff0f":"from sklearn.metrics import r2_score, accuracy_score\n\ndef acc(model, train, test, target, target_test, is_round=True):\n    # Calculation accuracy score for train and test prediction\n    # is_round=True - for classification task only, for regression task is_round=False\n    \n    if is_round:\n        # Classification task\n        ytrain = model.predict(train).astype(int)\n        ytest = model.predict(test).astype(int)\n        acc_train = round(accuracy_score(target, ytrain), 2)\n        acc_test = round(accuracy_score(target_test, ytest), 2)\n    else:\n        # Regression task\n        ytrain = model.predict(train)\n        ytest = model.predict(test)\n        acc_train = round(r2_score(target, ytrain), 2)\n        acc_test = round(r2_score(target_test, ytest), 2)\n        \n    print('Accuracy for train prediction =', acc_train)\n    print('Accuracy for test prediction =', acc_test,'\\n')\n    \n    return ytrain, ytest\n\n# See examples below","afd01e60":"from sklearn.metrics import classification_report\ndef classification_report_print(y_true, y_pred, title, target_names=['0', '1']):\n    print(f'Classification report {title}:')\n    print(classification_report(y_true, y_pred, target_names=target_names))\n\n# See examples below","ae4d817d":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(train, target)\n\nytrain, ytest = acc(linreg, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","cebea40e":"%%time\n# Support Vector Machines\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nsvr = SVC()\nsvr_CV = GridSearchCV(svr, param_grid={'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                                       'tol': [1e-3]}, verbose=False)\nsvr_CV.fit(train, target)\nprint(svr_CV.best_params_,'\\n')\n\nytrain, ytest = acc(svr_CV, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","69d70a96":"# Linear SVR\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\n\nlinear_svc = LinearSVC()\nparam_grid = {'dual':[False],\n              'C': np.linspace(1, 15, 15)}\nlinear_svc_CV = GridSearchCV(linear_svc, param_grid=param_grid, verbose=False)\nlinear_svc_CV.fit(train, target)\nprint(linear_svc_CV.best_params_,'\\n')\n\nytrain, ytest = acc(linear_svc_CV, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","29d37e3a":"# Decision Tree Classifier for data_health\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndecision_tree = DecisionTreeClassifier()\nparam_grid = {'min_samples_leaf': [i for i in range(2,10)]}\ndecision_tree_CV = GridSearchCV(decision_tree, param_grid=param_grid, verbose=False)\ndecision_tree_CV.fit(train, target)\nprint(decision_tree_CV.best_params_, '\\n')\n\nytrain, ytest = acc(decision_tree_CV, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","35e17726":"# DecisionTreeRegressor for data_water\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Parameters of model (param_grid) taken from the notebook https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models\ndecision_tree = DecisionTreeRegressor()\nparam_grid = {'min_samples_leaf': [i for i in range(2,10)]}\ndecision_tree_CV_w = GridSearchCV(decision_tree, param_grid=param_grid, verbose=False)\ndecision_tree_CV_w.fit(train_w, target_w)\nprint(decision_tree_CV_w.best_params_, '\\n')\n\nytrain_dt_w, ytest_dt_w = acc(decision_tree_CV_w, train_w, test_w, target_w, target_test_w, is_round=False)","6e20568b":"%%time\n# Random Forest for data_health\n# Parameters of model (param_grid) taken from the notebook https:\/\/www.kaggle.com\/morenovanton\/titanic-random-forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_forest = RandomForestClassifier()\nparam_grid = {'n_estimators': [40, 50, 60], 'min_samples_split': [40, 50, 60, 70], 'min_samples_leaf': [12, 13, 14, 15, 16, 17], \n              'max_features': ['auto'], 'max_depth': [3, 4, 5, 6], 'criterion': ['gini'], 'bootstrap': [False]}\nrandom_forest_CV = GridSearchCV(estimator=random_forest, param_grid=param_grid, verbose=False)\nrandom_forest_CV.fit(train, target)\nprint(random_forest_CV.best_params_, '\\n')\n\nytrain, ytest = acc(random_forest_CV, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","dc8fc2fd":"%%time\n# Random Forest for data_water\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Parameters of model (param_grid) taken from the notebook https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models\nrandom_forest = RandomForestRegressor()\nparam_grid = {'n_estimators': [10, 100, 500], 'min_samples_leaf': [i for i in range(5,10)], \n              'max_features': ['auto'], 'max_depth': [i for i in range(4,6)], \n              'criterion': ['mse'], 'bootstrap': [False]}\nrandom_forest_CV_w = GridSearchCV(estimator=random_forest, param_grid=param_grid, verbose=False)\nrandom_forest_CV_w.fit(train_w, target_w)\nprint(random_forest_CV_w.best_params_, '\\n')\n\nytrain_rf_w, ytest_rf_w = acc(random_forest_CV_w, train_w, test_w, target_w, target_test_w, is_round=False)","044bfb78":"%%time\n# XGBoost Classifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nxgb_clf = xgb.XGBClassifier(objective='reg:logistic') \nparameters = {'n_estimators': [50, 60, 70, 80, 90], \n              'learning_rate': [0.09, 0.1, 0.15, 0.2],\n              'max_depth': [3, 4, 5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters).fit(train, target)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_, '\\n')\n\nytrain, ytest = acc(xgb_reg, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","d12fc915":"%%time\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.2, random_state=0)\nmodelL = lgb.LGBMClassifier(n_estimators=1000, num_leaves=40)\nmodelL.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], early_stopping_rounds=50, verbose=True)\n\nytrain, ytest = acc(modelL, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","9e32600b":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nlogreg = LogisticRegression()\nlogreg_CV = GridSearchCV(estimator=logreg, param_grid={'C' : [.2, .3, .4]}, verbose=False)\nlogreg_CV.fit(train, target)\nprint(logreg_CV.best_params_, '\\n')\n\nytrain, ytest = acc(logreg_CV, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","5c7e15de":"# KNN - k-Nearest Neighbors algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\nknn_CV = GridSearchCV(estimator=knn, param_grid={'n_neighbors': range(2, 7)}, \n                      verbose=False).fit(train, target)\nprint(knn_CV.best_params_, '\\n')\n\nytrain, ytest = acc(knn_CV, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","b98316f1":"%%time\n# MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmlp = MLPClassifier()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,5)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [1000]\n              }\nmlp_GS = GridSearchCV(mlp, param_grid=param_grid, verbose=False)\nmlp_GS.fit(train, target)\nprint(mlp_GS.best_params_, '\\n')\n\nytrain, ytest = acc(mlp_GS, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","2229b7d1":"# See start at \"Tip 6.12. Logistic Regression\" (logreg_CV), \n# \"Tip 6.14. MLP Classifier\" (mlp_GS) and \"Tip 6.7. Linear SVC\" (linear_svc_CV)\n# Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nVoting_ens = VotingClassifier(estimators=[('log', logreg_CV), ('mlp', mlp_GS ), ('svc', linear_svc_CV)])\nVoting_ens.fit(train, target)\n\nytrain, ytest = acc(Voting_ens, train, test, target, target_test, is_round=True)\nclassification_report_print(target, ytrain, 'for training data')\nclassification_report_print(target_test, ytest, 'for test data')","0854906b":"# See start at \"Tip 6.11. LGBM Classifier\"\n# Feature importance diagram\nfig =  plt.figure(figsize = (10,10))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()","f7afe507":"def plot_prediction(target, y_list, label_list, data_name, MAV=0.5):\n    # Thanks to https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l3at-nh4-nn-models\n    # Building plot with target, Maximum allowable value (MAV) and \n    # prediction for the data_name (training, validation or test) data by 3 models\n    \n    x = np.arange(len(y_list[0]))\n    plt.figure(figsize=(16,10))\n    if target is not None:\n        plt.scatter(x, target, label = \"Target data\")\n    for i in range(len(y_list)):\n        plt.scatter(x, y_list[i], label = label_list[i])\n    plt.plot(x, np.full(len(y_list[0]), MAV), label = \"Maximum allowable value\", color = 'r')\n    plt.title(f'Prediction for the {data_name} data')\n    plt.legend(loc='best')\n    plt.grid(True)","da94eb40":"models_name = ['Decision Tree prediction', 'Random Forest prediction']\nplot_prediction(target_w, [ytrain_dt_w, ytrain_rf_w], models_name, 'training', MAV=0.5)","2869abd1":"# In progress...\n#df.plot(kind = \"scatter\", x = \"spirit_servings\", y = \"wine_servings\")","181b7b5d":"# In progress...\n# Interactive plots out of the box in pandas\n# run !pip install hvplot\n#pd.options.plotting.backend = \"hvplot\"\n#df.plot(kind = \"scatter\", x = \"spirit_servings\", y = \"wine_servings\", c = \"continent\")","64a14097":"# From https:\/\/www.kaggle.com\/vbmokin\/titanic-top-score-one-line-of-the-prediction\ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\ndf = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)\ntest_x = df.loc[testdf.index]\ntest_x['Survived'] = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex > 0.5) & (test_x.Alone > 0.5)) | \\\n          ((test_x.WomanOrBoySurvived > 0.238) & \\\n           ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633))))\npd.DataFrame({'Survived': test_x['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('submission.csv', index=False)","b0b9eb91":"### Tip 4.8. EDA<a class=\"anchor\" id=\"4.8\"><\/a>","dad4d857":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","03babc70":"## 8. BONUS<a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","d5951ee1":"### Tip 6.2. Splitting data with train_test_split<a class=\"anchor\" id=\"6.2\"><\/a>","dda5df56":"### Tip 6.13. k-Nearest Neighbors (KNN) <a class=\"anchor\" id=\"6.13\"><\/a>","12b52156":"### Tip 4.2. Combine the small categories into a single category named \"Other\" <a class=\"anchor\" id=\"4.2\"><\/a>","71d9bc24":"### Tip 6.3. Accuracy score for train and test prediction<a class=\"anchor\" id=\"6.3\"><\/a>","a9e0104d":"### Tip 6.8. Decision Tree Classifier & Regressor<a class=\"anchor\" id=\"6.8\"><\/a>","d2422788":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import main libraries](#1)\n    - [Tip 1.1. Import the most popular and useful main Python libraries](#1.1)\n    - [Tip 1.2. Warnings - ignore all](#1.2)\n    - [Tip 1.3. Ignore all warnings about later execution](#1.3)    \n    - [Tip 1.4. Install new libraries or packages with the given version](#1.4)\n    - [Tip 1.5. Import module or subpackage](#1.5)\n1. [Data download](#2)\n    - [Tip 2.1. Download typical csv-file to DataFrame](#2.1)\n    - [Tip 2.2. Download csv-file saved from MS Excel-file to DataFrame](#2.2)\n    - [Tip 2.3. Download csv-file with Cyrillic text to DataFrame](#2.3)\n    - [Tip 2.4. Download csv-file with given data types and NAN values](#2.4)\n    - [Tip 2.5. Download 1% data with random rows from big csv-file](#2.5)\n    - [Tip 2.6. Internally process the file in chunks (low_memory)](#2.6)    \n    - [Tip 2.7. Download json-data via API in Kaggle](#2.7)\n    - [Tip 2.8. Selection data from DataFrame (Pandas Tips)](#2.8)\n1. [Auxiliary functions](#3)\n    - [Tip 3.1. Pandas option for output data](#3.1)\n    - [Tip 3.2. The garbage collector](#3.2)\n    - [Tip 3.3. Time execution of a Python code in the cell](#3.3)\n1. [EDA & Data cleaning](#4)\n    - [Tip 4.1. Count of rows that match a condition](#4.1)\n    - [Tip 4.2. Combine the small categories into a single category named \"Other\"](#4.2)\n    - [Tip 4.3. Count the missing values](#4.3)\n    - [Tip 4.4. Convert one type of values to others](#4.4)\n    - [Tip 4.5. Replaced inf, -inf, nan to given value](#4.5)\n    - [Tip 4.6. Filtering the missing data in DataFrame](#4.6)\n    - [Tip 4.7. Generate descriptive statistics](#4.7)\n    - [Tip 4.8. EDA](#4.8)\n    - [Tip 4.9. Create a spreadsheet-style pivot table as a DataFrame](#4.9)    \n1. [FE](#5)\n    - [Tip 5.1. Get numeric features from the DataFrame](#5.1)\n    - [Tip 5.2. Get categorical features from the DataFrame](#5.2)\n    - [Tip 5.3. Search and encoding categorical columns](#5.3)\n    - [Tip 5.4. Difference or rolling of values in DataFrame](#5.4)\n    - [Tip 5.5. Data dropping (rows or columns removing) in DataFrame](#5.5)\n    - [Tip 5.6. Date in str format to date in datetime format in DataFrame](#5.6)\n    - [Tip 5.7. MinMaxScaling data in DataFrame](#5.7)\n1. [Modeling](#6)\n    - [Tip 6.1. Data preparation with standardization for modeling](#6.1)\n    - [Tip 6.2. Splitting data with train_test_split](#6.2)\n    - [Tip 6.3. Accuracy score for train and test prediction](#6.3)\n    - [Tip 6.4. Classification report](#6.4)\n    - [Tip 6.5. Linear Regression](#6.5)\n    - [Tip 6.6. Support Vector Machines](#6.6)\n    - [Tip 6.7. Linear SVC](#6.7)\n    - [Tip 6.8. Decision Tree Classifier & Regressor](#6.8)\n    - [Tip 6.9. Random Forest Classifier & Regressor](#6.9)\n    - [Tip 6.10. XGB Classifier](#6.10)\n    - [Tip 6.11. LGBM Classifier](#6.11)\n    - [Tip 6.12. Logistic Regression](#6.12)\n    - [Tip 6.13. k-Nearest Neighbors (KNN)](#6.13)\n    - [Tip 6.14. MLP Classifier](#6.14)\n    - [Tip 6.15. Voting Classifier](#6.15)\n    - [Tip 6.16. Feature importance diagram](#6.16)\n1. [Analysis and visualization of modeling results](#7)\n    - [Tip 7.1. Drawing plot data with modeling results](#7.1)\n    - [Tip 7.2. Drawing plot data of DataFrame (Pandas)](#7.2)\n1. [BONUS](#8)\n    - [Tip 8.1. Submission data from DataFrame to Kaggle competition](#8.1)","24175063":"### Competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)","b0221429":"### Tip 1.1. Import the most popular and useful main Python libraries<a class=\"anchor\" id=\"1.1\"><\/a>","033e7123":"\"\\n\" in \"print\" - line skip in the text output ","b0a613b7":"#### See more in the notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)","50123c2d":"#### See more in the notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)","5634e883":"### Tip 6.12. Logistic Regression <a class=\"anchor\" id=\"6.12\"><\/a>","08b986c2":"### Tip 1.3. Ignore all warnings about later execution <a class=\"anchor\" id=\"1.3\"><\/a>\n","e372f9b8":"## 4. EDA & Data cleaning<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","a861c455":"### Tip 3.1. Pandas option for output data<a class=\"anchor\" id=\"3.1\"><\/a>","472d7b9b":"**Logistic Regression** is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression).","394f7831":"### Tip 5.1. Get numeric features from the DataFrame<a class=\"anchor\" id=\"5.1\"><\/a>","a2902c05":"### Tip 8.1. Submission data from DataFrame to Kaggle competition<a class=\"anchor\" id=\"8.1\"><\/a>","96e0e0fd":"### Tip 7.1. Drawing plot data with modeling results<a class=\"anchor\" id=\"7.1\"><\/a>","dc842fdd":"### Tip 2.4. Download csv-file with given data types and NAN values<a class=\"anchor\" id=\"2.4\"><\/a>\nThe data type \"Int64\" allows the presence of NAN values during import, in contrast to the data type \"int\"","5c316ccc":"**XGBoost** is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements. Reference [Towards Data Science.](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","0b713707":"### Tip 6.16. Feature importance diagram <a class=\"anchor\" id=\"6.16\"><\/a>","fa3a3ab4":"### Tip 2.3. Download csv-file with Cyrillic text to DataFrame <a class=\"anchor\" id=\"2.3\"><\/a>","9294fd42":"#### See more in the notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)","6b6982b5":"### Tip 1.5. Import module or subpackage <a class=\"anchor\" id=\"1.5\"><\/a>","3ece2599":"### Tip 5.4. Difference and rolling of values in DataFrame<a class=\"anchor\" id=\"5.4\"><\/a>","dd58e3f5":"#### See more in the notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)","1fa529cb":"### Tip 4.5. Replaced inf, -inf, nan to given value <a class=\"anchor\" id=\"4.5\"><\/a>","e8a38c99":"### Tip 5.3. Search and encoding categorical columns <a class=\"anchor\" id=\"5.3\"><\/a>","ec7138fc":"**Light GBM** is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019. Reference [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/).","8053ea5f":"In pattern recognition, the **k-Nearest Neighbors algorithm** (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm).","53c13efb":"This model uses a **Decision Tree** as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).","3de325a2":"## 7. Analysis and visualization of modeling results<a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","ceb1dc17":"### Tip 6.14. MLP Classifier <a class=\"anchor\" id=\"6.14\"><\/a>","6f3e0114":"### Tip 3.3. Time execution of a Python code in the cell<a class=\"anchor\" id=\"3.3\"><\/a>","865a2eb2":"#### See more in the notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)","845f63c3":"### Tip 4.3. Count the missing values <a class=\"anchor\" id=\"4.3\"><\/a>","a8ab089b":"### Tip 5.6. Date in str format to date in datetime format in DataFrame<a class=\"anchor\" id=\"5.6\"><\/a>","2818ebb4":"### Tip 2.1. Download typical csv-file to DataFrame <a class=\"anchor\" id=\"2.1\"><\/a>","4e8af120":"### Tip 5.5. Data dropping (rows or columns removing) in DataFrame<a class=\"anchor\" id=\"5.5\"><\/a>","e27356f3":"### Tip 6.10. XGB Classifier <a class=\"anchor\" id=\"6.10\"><\/a>","08a98916":"### Tip 6.9. Random Forest Classifier & Regressor <a class=\"anchor\" id=\"6.9\"><\/a>","08b28abe":"## 3. Auxiliary functions<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","12dd4c50":"### Tip 1.2. Warnings - ignore all<a class=\"anchor\" id=\"1.2\"><\/a>","ed6a75c1":"### Tip 4.7. Generate descriptive statistics<a class=\"anchor\" id=\"4.7\"><\/a>","44021f8e":"### Tip 6.11. LGBM Classifier <a class=\"anchor\" id=\"6.11\"><\/a>","de9697c2":"### Tip 4.4. Convert one type of values to others <a class=\"anchor\" id=\"4.4\"><\/a>","47c6d010":"### Tip 5.2. Get categorical features from the DataFrame<a class=\"anchor\" id=\"5.2\"><\/a>","4610c1e0":"Available options:\n> display.[chop_threshold, colheader_justify, column_space, date_dayfirst,\n>          date_yearfirst, encoding, expand_frame_repr, float_format, height,\n>          line_width, max_columns, max_colwidth, max_info_columns, max_info_rows,\n>          max_rows, max_seq_items, mpl_style, multi_sparse, notebook_repr_html,\n>          pprint_nest_depth, precision, width]","41d4076c":"## 1. Import main libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","691d7e5c":"### Tip 2.2. Download csv-file saved from MS Excel-file to DataFrame <a class=\"anchor\" id=\"2.2\"><\/a>\nMS Excel with saves csv-files with the default settings with the delimiter \";\"","5ab7da80":"### Tip 6.6. Support Vector Machines <a class=\"anchor\" id=\"6.6\"><\/a>","507c9455":"### Tip 4.6. Filtering the missing data in DataFrame<a class=\"anchor\" id=\"4.6\"><\/a>","363ffac9":"In progress...","55f88b38":"## Acknowledgements\n\n### Datasets:\n* for Classification task solutions - competition's dataset [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)\n* for Classification task solutions - [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)\n* for Regression task solutions - my dataset [Ammonium prediction in river water](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)\n* from API for Regression task solutions - official data of COVID-19 in Ukraine (https:\/\/covid19.rnbo.gov.ua\/)\n* for NLP task - [NLP : Reports & News Classification](https:\/\/www.kaggle.com\/vbmokin\/nlp-reports-news-classification)\n\n### Notebooks:\n* [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [COVID in UA: Prophet with 4, Nd seasonality](https:\/\/www.kaggle.com\/vbmokin\/covid-in-ua-prophet-with-4-nd-seasonality)\n* [Top score : one line of the prediction](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-score-one-line-of-the-prediction)\n* [AI-ML-DS Training. L3AT: NH4 - NN models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l3at-nh4-nn-models)\n* https:\/\/www.dataschool.io\/python-pandas-tips-and-tricks\/\n* https:\/\/github.com\/rougier\/numpy-100","31bd5522":"### Tip 6.4. Classification report<a class=\"anchor\" id=\"6.4\"><\/a>","7e4d0f69":"There is **VotingClassifier**. The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#voting-classifier).","ee78fecb":"### Tip 2.7. Download json-data via API in Kaggle<a class=\"anchor\" id=\"2.7\"><\/a>","d98b69a8":"**Random Forest** is one of the most popular model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators= [100, 300]) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Random_forest).","5413899c":"The **MLPClassifier** optimizes the squared-loss using LBFGS or stochastic gradient descent by the Multi-layer Perceptron regressor. Reference [Sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor).","02914ef6":"### Tip 3.2. The garbage collector<a class=\"anchor\" id=\"3.2\"><\/a>","15eef8fe":"In progress...","827d8b39":"**Support Vector Machines** are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine).","5842bfbb":"### Tip 4.9. Create a spreadsheet-style pivot table as a DataFrame<a class=\"anchor\" id=\"4.9\"><\/a>","290dfd18":"### Tip 5.7. MinMaxScaling data in DataFrame<a class=\"anchor\" id=\"5.7\"><\/a>","c53b03fb":"### Tip 2.5. Download 1% data with random rows from big csv-file<a class=\"anchor\" id=\"2.5\"><\/a>","a2b3b537":"### Tip 6.1. Data preparation with standardization for modeling<a class=\"anchor\" id=\"6.1\"><\/a>","b029d9b8":"**Linear Regression** is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Linear_regression).\n\nNote the confidence score generated by the model based on our training dataset.","d26dad4d":"## 2. Data download<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","ef8f422d":"### Tip 2.6. Internally process the file in chunks (low_memory)<a class=\"anchor\" id=\"2.6\"><\/a>\nInternally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types specify the type with the dtype parameter.","51ea3080":"### Tip 6.5. Linear Regression <a class=\"anchor\" id=\"6.5\"><\/a>","0d8120e4":"### Tip 7.2. Drawing plot data of DataFrame (Pandas)<a class=\"anchor\" id=\"7.2\"><\/a>","f47b742c":"Thanks to [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n\nThere are 60+ predictive modeling algorithms to choose from. Consider the classification problem with the next models (with hyperparameters tuning by GridSearchCV):\n\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Neural network with Keras\n- Support Vector Machines and Linear SVC\n- Stochastic Gradient Descent, Gradient Boosting Classifier, RidgeCV, Bagging Classifier\n- Decision Tree Classifier, Random Forest Classifier, AdaBoost Classifier, XGB Classifier, LGBM Classifier, ExtraTrees Classifier \n- Gaussian Process Classification\n- MLP Classifier (Deep Learning)\n- Voting Classifier\n\nApplication and tuning of these models using cross-validation with [learning_curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) see in the notebook [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models).","3bdeb503":"## It's done 50 tips: 1.1-1.5, 2.1-2.8, 3.1-3.3, 4.1-4.9, 5.1-5.7, 6.1-6.16, 7.1, 8\n### I improved \"Tip 2.7. Download json-data via API in Kaggle\"\n### I added: \n* \"Tip 4.8. EDA\"\n* \"Tip 4.9. Create a spreadsheet-style pivot table as a DataFrame\"\n*  Limited output in the \"Tip 3.1. Pandas option for output data\"\n*  new \"Tip 5.1. Get numeric features from the DataFrame\"\n*  new \"Tip 5.2. Get categorical features from the DataFrame\"","4d09677a":"### Tip 6.7. Linear SVC <a class=\"anchor\" id=\"6.7\"><\/a>","2bf65b68":"### Tip 6.15. Voting Classifier <a class=\"anchor\" id=\"6.15\"><\/a>","a0b6439f":"#### See more in the notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)","5220c580":"### Tip 2.8. Selection data from DataFrame (Pandas Tips)<a class=\"anchor\" id=\"2.8\"><\/a>","8939d7b2":"### Tip 1.4. Install new libraries or packages with the given version<a class=\"anchor\" id=\"1.4\"><\/a>","bef99300":"## 6. Modeling<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","257e31a4":"<a class=\"anchor\" id=\"0\"><\/a>\n# 50 Tips for Data Science for tabular data for beginners\n## Frequently used useful code for:\n* Import libraries\n* Data download\n* Data cleaning\n* FE\n* Modeling\n* Analysing, and visualization of modeling results\n* Prediction and submitting of modeling results\netc.\n\n### With BONUS - the short solution for Competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) with LB=0.80382 (Top 4%)\n\n### Part of tips from the first versions of the notebook see in a new notebook [50 Advanced Tips: Data Science for tabular data](https:\/\/www.kaggle.com\/vbmokin\/50-advanced-tips-data-science-for-tabular-data)\n\nLater I will publish another notebook for EDA.","b5f71975":"## 5. FE<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","0da134cd":"### Result: the file \"submission.csv\" gives LB = 0.80382 (Top 4%)","195d67e7":"### Tip 4.1. Count of rows that match a condition <a class=\"anchor\" id=\"4.1\"><\/a>"}}