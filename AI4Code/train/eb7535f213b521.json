{"cell_type":{"dcbb9f97":"code","145859a5":"code","32f7ff39":"code","4aa2cd42":"code","6e7cd1d3":"code","f8acf303":"code","6b71cce1":"code","f881774f":"code","82d3b40e":"code","25a8444a":"code","18bbd3c8":"code","05900019":"code","d06140b4":"code","b139da85":"code","f4900628":"code","b20168ce":"code","bd595e9f":"code","ffaff909":"code","f863e5e2":"code","217263c9":"code","7640ded0":"code","132a557d":"code","a5f545a6":"code","26fde8f7":"code","d017cd54":"code","c29053cb":"code","bd8b2e7c":"code","b3dc94f7":"markdown","3789f9a0":"markdown","31016749":"markdown"},"source":{"dcbb9f97":"import torch\nimport pandas as pd\nimport numpy as np\n#torch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim import Adam, lr_scheduler\n\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModel\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\nimport warnings\nwarnings.simplefilter('ignore')","145859a5":"class testDataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","32f7ff39":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.W.weight)\n        nn.init.constant_(self.W.bias, 0)\n        nn.init.xavier_normal_(self.V.weight)\n        nn.init.constant_(self.V.bias, 0)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass dModel(nn.Module):\n    def __init__(self,modelpath, leng):\n        super(dModel, self).__init__()\n        self.roberta = transformers.AutoModel.from_pretrained(modelpath)#,num_labels=1)\n        self.head = AttentionHead(leng,leng,1)\n        self.fc = nn.Linear(leng, 1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.dropouts = nn.ModuleList([\n                nn.Dropout(p=0.1) for _ in range(5)\n            ])\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n    def forward(self, **x):\n        x = self.roberta(**x)[0]\n        x = self.head(x)\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.fc(dropout(x))\n            else:\n                logits += self.fc(dropout(x))\n        \n        logits \/= len(self.dropouts)\n        return logits","4aa2cd42":"def generate_embeddings(model_path,weight_path, max_len,leng):\n    model = dModel(model_path,leng)\n    model.load_state_dict(torch.load(weight_path,map_location=device))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = model.to(device)\n    model.eval()\n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n    dataset = testDataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=16, num_workers=2, pin_memory=True, shuffle=False, drop_last=False\n    )\n    final_output = np.empty((0,leng))\n\n    for b_idx, (data) in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(device)\n            x = model.roberta(**data)[0]\n            output = model.head(x)\n            final_output = np.append(final_output,output.clone().detach().cpu().numpy(),axis=0)\n    \n    torch.cuda.empty_cache()\n    return final_output\n\ndef generate_predictions_md(model_path,weight_path, max_len,leng):\n    #model = AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=1)\n    model = dModel(model_path,leng)\n    model.load_state_dict(torch.load(weight_path,map_location=device))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model = model.to(device)\n    model.eval()\n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n    dataset = testDataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=8, num_workers=2, pin_memory=True, shuffle=False, drop_last=False\n    )\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(device)\n            output = model(**data)\n            #output = output.logits.squeeze(-1).detach().cpu().numpy()\n            output = output.squeeze(-1).detach().cpu().numpy()\n            final_output = np.append(final_output,output)\n    \n    torch.cuda.empty_cache()\n    return final_output","6e7cd1d3":"#0:LB 0.483 to 0.487 to 0.486\n\n#1:LB 0.487 to 0.489 to 0.488\n\n#2:LB 0.489 to 0.489\u3000to 0.489\n\n#3:LB 0.498 to 0.492 to 0.485\n\n#4:LB 0.498 to 0.492 to 0.491\n\n#5:LB 0.497 to 0.504 \n\n#6:LB 0.500 to 0.509 \n\nmodel_paths = [\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/distilroberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\"\n              ]\nweight_paths = [\"..\/input\/commonlitreadability-weight\/simplelarge_0.469_fold0_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/simplelarge_0.455_fold1_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/simplelarge_0.464_fold3_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/bestmodel_0.469_fold1_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/bestbase_0.443_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertalarge_LB0.500_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/basebestmodel_0.428_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertabase_LB0.504_mlen250.pt\"\n               ]\nlengs = [1024,1024]#,1024]#,768,768,1024,768]#,768]\n\n\nmodel_paths_emb = [#\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/distilroberta-base\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",]\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",]\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\"]\n                  ]\nweight_paths_emb = [#\"..\/input\/commonlitreadability-weight\/simplelarge_0.469_fold0_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/simplelarge_0.455_fold1_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/simplelarge_0.464_fold3_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/bestmodel_0.469_fold1_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/bestbase_0.443_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertalarge_LB0.500_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/basebestmodel_0.428_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertabase_LB0.504_mlen250.pt\"\n                   ]\nlengs = [1024,768,768]#[1024,1024,1024,768,768,1024,768]#,768]","f8acf303":"#single LB 0.483\npreds0 = generate_predictions_md(model_path=\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                              weight_path=\"..\/input\/commonlitreadability-weight\/simplelarge_0.469_fold0_mlen250.pt\",\n                              max_len=250,leng=1024)\n\n#single LB 0.487\npreds1 = generate_predictions_md(model_path=\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                              weight_path=\"..\/input\/commonlitreadability-weight\/simplelarge_0.455_fold1_mlen250.pt\",\n                              max_len=250,leng=1024)\n\n#single LB 0.489\npreds2 = generate_predictions_md(model_path=\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                              weight_path=\"..\/input\/commonlitreadability-weight\/simplelarge_0.464_fold3_mlen250.pt\",\n                              max_len=250,leng=1024)","6b71cce1":"test_embeddings = []\n\nfor mpath,wpath, leng in zip(model_paths_emb,weight_paths_emb, lengs):\n    emb = generate_embeddings(mpath,wpath, max_len=250,leng=leng)\n    test_embeddings.append(emb)","f881774f":"test_embeddings[0].shape","82d3b40e":"import lightgbm as lgb\n\nparams = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    \n    'eta': 0.05,\n    'max_depth': 2,\n    \n    'gamma': 1,\n    'subsample': 0.9,\n    \n    'nthread': 2,\n    'tree_method' : 'gpu_hist'\n}\n\nparams_l = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 2048,\n    'learning_rate': 0.05,\n    \"n_jobs\": -1,\n    'max_depth': 4,\n    \"verbose\": -1\n}\n\nnfolds = 5\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=2048)","25a8444a":"def get_pred(train_embeddings,test_emb, targets, train_df):\n    best_iterations = []\n    oof_rmses = []\n    preds = np.zeros(test.shape[0])\n    pred_list = []\n\n    for k, (train_idx, valid_idx) in enumerate(kf.split(train_df)):    \n\n        dtrain = xgb.DMatrix(train_embeddings[train_idx], targets[train_idx])\n        dvalid = xgb.DMatrix(train_embeddings[valid_idx], targets[valid_idx])\n        evals_result = dict()\n        booster = xgb.train(params,\n                            dtrain,\n                            evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                            num_boost_round=300,\n                            early_stopping_rounds=20,\n                            evals_result=evals_result,\n                            verbose_eval=False)\n\n        best_iteration = np.argmin(evals_result['valid']['rmse'])\n        best_iterations.append(best_iteration)\n        oof_rmse = evals_result['valid']['rmse'][best_iteration]\n        oof_rmses.append(oof_rmse)\n        \n        preds += booster.predict(xgb.DMatrix(test_emb), ntree_limit=int(best_iteration+1)) \/ nfolds\n        pred_list.append(booster.predict(xgb.DMatrix(test_emb), ntree_limit=int(best_iteration+1)))\n    pred_list = np.array(pred_list)[np.argpartition(oof_rmses, 3)[:2]]\n    preds = pred_list.mean(axis=0)\n    evals_df = pd.DataFrame()\n    evals_df['fold'] = range(1, nfolds+1)\n    evals_df['best_iteration'] = best_iterations\n    evals_df['oof_rmse'] = oof_rmses\n\n    display(evals_df)\n    print('mean oof rmse = {}'.format(np.mean(oof_rmses)))\n    \n    torch.cuda.empty_cache()\n    return preds\n\ndef get_pred_lgb(train_embeddings,test_emb, targets, train_df):\n\n    pred = np.zeros(test.shape[0])\n    pred_list = []\n    rmses = []\n\n    for k, (train_idx, valid_idx) in enumerate(kf.split(train_df)):    \n\n        dtrain = lgb.Dataset(train_embeddings[train_idx], targets[train_idx])\n        dvalid = lgb.Dataset(train_embeddings[valid_idx], targets[valid_idx], reference=dtrain)\n\n        model = lgb.train(\n            params_l,\n            dtrain, \n            num_boost_round=300,\n            early_stopping_rounds=20,\n            valid_sets=[dtrain, dvalid], \n            verbose_eval=-1\n        )\n\n        y_pred = model.predict(train_embeddings[valid_idx])\n        rmse = rmse_score(targets[valid_idx], y_pred)\n        rmses.append(rmse)\n\n        tmp_pred = model.predict(test_emb)\n        pred += tmp_pred \/ 5\n    \n    print(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))\n    return pred","18bbd3c8":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef get_pred_svm(X,y,X_test,bins,nfolds=5,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2048)\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds","05900019":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntarget = np.load(\"..\/input\/embedding-featurevector\/target.npy\")\ntrain_df = pd.read_csv(\"..\/input\/step1-exclude-anomaly\/train.csv\")","d06140b4":"num_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df.loc[:,'bins'] = pd.cut(train_df['target'],bins=num_bins,labels=False)\nbins = train_df.bins.to_numpy()","b139da85":"pred_comb_svm = np.empty((0,len(test)))\npred_comb = np.empty((0,len(test)))\npred_comb_lgb = np.empty((0,len(test)))\nfor i in range(len(test_embeddings)):\n    embeddings = np.load(\"..\/input\/embedding-featurevector\/embeddings\" + str(i+2) + \".npy\")\n    print(embeddings.shape)\n    preds = get_pred(embeddings,test_embeddings[i], target, train_df)\n    preds_lgb = get_pred_lgb(embeddings,test_embeddings[i], target, train_df)\n    pred_comb = np.append(pred_comb,preds[None,:],axis=0)\n    pred_comb_lgb = np.append(pred_comb_lgb,preds_lgb[None,:],axis=0)\n    #preds_svm = get_pred_svm(embeddings, target, test_embeddings[i], bins = bins)\n    #pred_comb_svm = np.append(pred_comb_svm,preds_svm[None,:],axis=0)","f4900628":"pred_ori = (preds0 + preds1 + preds2)\/3\n#pred_ori = (preds0 + preds1)\/2\npred_svm = pred_comb_svm.sum(axis=0)\/len(pred_comb_svm)\npred_xgb = pred_comb.sum(axis=0)\/len(pred_comb)\npred_lgb = pred_comb_lgb.sum(axis=0)\/len(pred_comb_lgb)\npredf = (pred_ori*3 + pred_xgb*3)\/6 # +pred_comb_svm[1])\/6","b20168ce":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\nTOKENIZER_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","bd595e9f":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","ffaff909":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","f863e5e2":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","217263c9":"from transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nimport gc\ngc.enable()\n\ntest_dataset = LitDataset(test_df, inference_only=True)\n\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","7640ded0":"model_predictions = (all_predictions[0] + all_predictions[2] + all_predictions[3])\/3\nmodel_predictions5 = all_predictions.mean(axis=0)","132a557d":"pred_2sigma = (pred_ori*3 + model_predictions5*5)\/8","a5f545a6":"#predfinal = (predf + model_predictions)\/2","26fde8f7":"predfinal_x = (pred_ori*3 + pred_xgb*2 + model_predictions*5)\/10\npredfinal_l = (pred_ori*3 + pred_lgb*2 + model_predictions*5)\/10","d017cd54":"train_ex = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_ex = train_ex[train_ex.target!=0]\nupper = train_ex.target.mean()+1.0*train_ex.target.std()\nlower = train_ex.target.mean()-1.0*train_ex.target.std()\n\n#plt.scatter(train_ex.target.values,train_ex.standard_error.values)\n#print(lower,upper)","c29053cb":"test[\"best_pred\"] = (pred_ori*3 + model_predictions*3)\/6\ntest['pred2sigma'] =pred_2sigma\ntest['predfull'] = predfinal_l\ntest = test.sort_values(\"best_pred\")\nsigma2 = int(len(test)*0.05)\nheadindex = test.head(sigma2).index\ntailindex = test.tail(sigma2).index\nbestpredindex = test[(test.best_pred > lower) & (test.best_pred < upper)].index\nfullindex = test[~((test.best_pred > lower) & (test.best_pred < upper))].index\nmeansindex = test[(test.best_pred > train_ex.target.max()) | (test.best_pred < train_ex.target.min())].index\ntest[\"prediction\"] = 0\ntest.loc[fullindex,\"prediction\"] = test.loc[fullindex,\"predfull\"]\ntest.loc[bestpredindex,\"prediction\"] = test.loc[bestpredindex,\"best_pred\"]\ntest.loc[meansindex,\"prediction\"] = train_ex.target.mean()\ntest.loc[headindex,\"prediction\"] = test.loc[headindex,'pred2sigma']\ntest.loc[tailindex,\"prediction\"] = test.loc[tailindex,'pred2sigma']\ntest","bd8b2e7c":"submission = pd.DataFrame()\nsubmission['id'] = test['id'].copy()\nsubmission['target'] = test['prediction'].copy()\nsubmission.to_csv('submission.csv', index=False)\nsubmission","b3dc94f7":"# SVM prediction","3789f9a0":"# Pytorch inference ensemble notebook\n\nthanks to https:\/\/www.kaggle.com\/andretugan\/pre-trained-roberta-solution-in-pytorch\n\nThis comptition summary.\n\n\n### preprocess\n\n[make NLP](https:\/\/www.kaggle.com\/kunihikofurugori\/make-nlpdataset)\n\n\u2192 not effective (score is not improved.)\n\n[exclude anomaly data](https:\/\/www.kaggle.com\/kunihikofurugori\/step1-exclude-anomaly)\n\n\u2192 score slightly is improved....?\n\n### transformer training\n\n[transformer tpu8fold](https:\/\/www.kaggle.com\/kunihikofurugori\/tpu8fold-transformer)\n\n\u2192 The score is improved because I tried to many seed pattern.\n\n(Please see Section 4.2 in [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https:\/\/arxiv.org\/pdf\/2002.06305.pdf))\n\n\n[Add attention and early stopping](https:\/\/www.kaggle.com\/kunihikofurugori\/tpu8fold-transformer-attention)\n\u2192 attention and early stopping is powerful method. these method is effective.\n\n(Please see Section 5 in [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https:\/\/arxiv.org\/pdf\/2002.06305.pdf))\n\n[Add multi dropout](https:\/\/www.kaggle.com\/kunihikofurugori\/step2-1-tpu8foldtraining-rbase)\n\n\u2192 score slightly is improved....?\n\n### inference\n\nxgb,lgb inference model is slightly imporved. \n\nsvm inference model is not imporved.\n\nLB score in my model ensembleing is 0.461 and andretugan model is model 0.467 and then 2 pair ensemble is then LB 0.457.","31016749":"# model2"}}