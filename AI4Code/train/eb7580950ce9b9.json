{"cell_type":{"6d1e32d2":"code","f5b15b88":"code","0086095b":"code","80efba91":"code","0348d8ee":"code","38e91f4f":"code","183937ff":"code","655dd50b":"code","0bd8a6de":"code","0120bf9d":"code","2d51ac39":"code","9ec046bc":"code","ba85f6c6":"code","37e77d45":"code","7c9e8ca6":"code","f5c6a6d1":"markdown","f9ab2862":"markdown","203d8d31":"markdown","3764678a":"markdown","e193c47e":"markdown"},"source":{"6d1e32d2":"import os \nprint(os.listdir(\"..\/input\/competitive-data-science-predict-future-sales\/\"))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pylab as plt\nimport matplotlib.dates as mdates\n\nimport seaborn as sns","f5b15b88":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})","0086095b":"# remove outliers\nsales = sales[(sales.item_price < 300000 ) & (sales.item_cnt_day < 750)]\n# sales = sales[(sales.item_price < 300000 ) & (sales.item_cnt_day < 1000)]\ntrain = sales.drop(labels = ['date', 'item_price'], axis = 1)\ntrain.head()","80efba91":"train_clean = pd.merge(train, items.drop(columns=['item_name']), on=['item_id'])\ntrain_clean = train_clean.groupby([\"item_id\",\"shop_id\",\"date_block_num\",'item_category_id']).sum().reset_index()\ntrain_clean = train_clean.rename(index=str, columns = {\"item_cnt_day\":\"item_cnt_month\"})\ntrain_clean = train_clean[[\"item_id\",\"shop_id\",\"date_block_num\",'item_category_id',\"item_cnt_month\"]]\ntrain_clean.sample(10)","0348d8ee":"train_data = train_clean.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_month'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')\ntrain_data.reset_index(inplace = True)\ntrain_data.head(10)","38e91f4f":"all_data = pd.merge(test,train_data,on = ['item_id','shop_id'],how = 'left')\nall_data.fillna(0,inplace = True)\nall_data.head()","183937ff":"def split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        end_ix = i + n_steps\n        if end_ix > len(sequences):\n            break\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","655dd50b":"dummy_samples = [[0] * 34] * 2\nall_data.drop(['ID','shop_id','item_id'],inplace = True, axis = 1)\nall_data.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33]\n# create dummy_samples dataframe and concatenate with all_data\nall_data = pd.concat([pd.DataFrame(dummy_samples), all_data], ignore_index=True)","0bd8a6de":"train_data = np.expand_dims(all_data.values[:,:-2],axis = 2)\nvalidation_data = np.expand_dims(all_data.values[:,1:-1],axis = 2)\ntest_data = np.expand_dims(all_data.values[:,2:],axis = 2)","0120bf9d":"from numpy import array\nn_steps = 5\nX_train, y = split_sequences(train_data, n_steps)\nX_val, y_val = split_sequences(validation_data, n_steps)\nX_test, y_test = split_sequences(test_data, n_steps)\nprint(X_train.shape, y.shape)","2d51ac39":"n_input = X_train.shape[1] * X_train.shape[2]\n# reshape to 2 dimensional input\nX = X_train.reshape((X_train.shape[0], n_input))\nX_val = X_val.reshape((X_val.shape[0], n_input))\nX_test = X_test.reshape((X_test.shape[0], n_input))","9ec046bc":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport  tensorflow.keras.optimizers as optimizers\n# define model\nmodel = Sequential()\nmodel.add(Dense(128, activation= 'relu', input_dim=n_input))\nmodel.add(Dense(64, activation= 'relu' ))\nmodel.add(Dense(10))\nmodel.compile(optimizer=optimizers.Adam(lr=.0001), loss= 'mse', metrics = ['mean_squared_error'])\nmodel.summary()","ba85f6c6":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallbacks = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(factor=0.25, patience=2, min_lr=0.000001, verbose=1),\n    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","37e77d45":"model.fit(X, y, epochs=15, callbacks=callbacks, validation_data=(X_val, y_val))","7c9e8ca6":"ALPHA = 0.5\nBETA = 0.0\nN_SESSIONS = 33 \n\ndef create_filtered_prediction(train_ts, alpha, beta):\n    train_time_filtered_ts = np.zeros((train_ts.shape[0], N_SESSIONS), dtype=np.float)\n    train_time_filtered_ts[0, :] = train_ts[0, :]\n    train_memontum_ts = np.zeros((train_ts.shape[0], N_SESSIONS), dtype=np.float)\n    prediction_ts = np.zeros((train_ts.shape[0], N_SESSIONS+1), dtype=np.float)\n    for i in range(1, N_SESSIONS):\n        train_time_filtered_ts[:, i] = (1-alpha) * (train_time_filtered_ts[:, i-1] + \\\n                                                    train_memontum_ts[:, i-1]) + alpha * train_ts[:, i]\n        train_memontum_ts[:, i] = (1-beta) * train_memontum_ts[:, i-1] + \\\n                                  beta * (train_time_filtered_ts[:, i] - train_time_filtered_ts[:, i-1])\n        prediction_ts[:, i+1] = train_time_filtered_ts[:, i] + train_memontum_ts[:, i]\n    return prediction_ts\n\nall_data_x = all_data.iloc[:,:-1]\nall_data_y = all_data.iloc[:,-1:]\n\ntrain_time_series_df = np.clip(all_data_x.values, 0, 20).astype(float)\npredictions = create_filtered_prediction(train_time_series_df, ALPHA, BETA)\n# validate prediction by predicting 34th month \n# all_data_x.size()\n# predictions\n# rmse(predictions, all_data_y)\nprint(\"rmse = \",np.sqrt(np.mean((predictions-all_data_y.to_numpy())**2)))","f5c6a6d1":"### \u5efa\u7acb\u57fa\u672c\u76843\u5c64MLP","f9ab2862":"### \u548c\u4e0a\u4e00\u4efd\u9810\u6e2c\u8d70\u76f8\u53cd\u65b9\u5411\uff0c\u50c5\u4f7f\u7528\u6b77\u53f2\u6d88\u8cbb\u6578\u91cf\u9032\u884c\u9810\u6e2c","203d8d31":"### \u7d50\u8ad6\n\n\u9019\u88e1\u50c5\u5229\u7528\u9023\u7e8c\u6708\u4efd\u7684\u92b7\u552e\u6578\u91cf\u6e2c\u8a66\u4e86\u5169\u7a2e\u65b9\u5f0f\uff1a\n1. \u57fa\u672c\u7684NN (MLP)\u6e2c\u8a66\n2. \u5229\u7528time series memontum\u9032\u884c\u9810\u6e2c\n\u4ee5\u7d50\u679c\u4f86\u8aaarmse\u597d\u50cf\u4e0d\u662f\u4ee4\u4eba\u6eff\u610f\u7684\u7d50\u679c\uff0c\u672a\u4f86\u5e0c\u671b\u5c07\u6bcf\u4e00\u500bitem\u7684\u985e\u5225\u7b49\u8a73\u7d30\u8cc7\u8a0a\u5e36\u5165\u4ee5\u512a\u5316\u8868\u73fe\u3002\n\u4ee5\u8cc7\u6599\u5206\u5e03\u4f86\u8aaa\uff0c\u91dd\u5c0d\u7279\u5b9a\u985e\u5225\u7684\u92b7\u552e\u6578\u91cf\u6a19\u6e96\u5316\u6216\u8a31\u6709\u6a5f\u6703\u5f97\u5230\u66f4\u597d\u7684\u7d50\u679c\n\n","3764678a":"### MLP & TSM\n\u9019\u6b21\u7684\u76ee\u7684\u662f\u57fa\u65bc\u4e0a\u6b21\u7684EDA\u4ee5\u53caxgboost\u7684\u7d50\u679c\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u65b9\u5f0f\u9032\u884c\u9810\u6e2c\u3002\n\u9996\u5148\u5728\u4e0a\u6b21\u7684\u6bcf\u65e5750\u6d88\u8cbb\u7b46\u6578\u7684\u9650\u5236\u518d\u52a0\u4e0a\u91d1\u984d\u4e0d\u5f97\u8d85\u904e300000\u7684\u9650\u5236\uff08\u6709\u56fa\u5b9a\u9ad8\u55ae\u50f9\u54c1\u9805\u5e72\u64fe\uff09\n","e193c47e":"### \u8a08\u7b97\u6bcf\u65e5\u6d88\u8cbb\u91cf"}}