{"cell_type":{"81e0156c":"code","b04ad711":"code","bc9d93bc":"code","6da9c3e8":"code","ea6b2702":"code","0f69370a":"code","b86dbb0c":"code","4a5cfe76":"code","e34b1c02":"code","f7ecec71":"code","93e8ab1c":"code","ad07d6af":"code","c73567d0":"code","d2f31d17":"code","3b4f983f":"code","792266b0":"code","229cd146":"code","77622a1f":"code","c0dcc608":"code","35e567bb":"code","b71da292":"code","dfb5b2b5":"code","a696e099":"code","b456680c":"code","dd332a08":"code","9e3cfb66":"code","2e7dc2df":"code","791b487d":"code","caf4032c":"code","240ff96b":"code","291aad2d":"markdown","0536dad7":"markdown","1222ed3a":"markdown","f8a877b4":"markdown","5b30ce87":"markdown","28c2c533":"markdown","04d3c921":"markdown"},"source":{"81e0156c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b04ad711":"data = pd.read_csv(\"..\/input\/customer-segmentation\/Train.csv\")\n","bc9d93bc":"data.head()","6da9c3e8":"data.info()","ea6b2702":"# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n\nsegmentation_order = ['A', 'B', 'C', 'D']\ndef countplot(x, hue, data, order, title, fig_size):\n  fig, ax = plt.subplots(figsize= fig_size)\n  sns.countplot(x=x, hue=hue, ax=ax, data=data, order=order, palette= 'bright')\n  ax.set_title(title)\n  plt.show()","0f69370a":"countplot('Segmentation', None, data, segmentation_order, 'Customer Segmentation', (10,6))","b86dbb0c":"countplot('Gender', None, data, None, 'Number of Male and Female Customers', (6,4))","4a5cfe76":"countplot('Segmentation', 'Gender', data, segmentation_order, 'Customer Segmentation based on Gender of the Customer', (10,7))","e34b1c02":"countplot('Segmentation', 'Spending_Score', data, segmentation_order, 'Customer Segmentation based on the Spending Score', (10,7))","f7ecec71":"countplot('Segmentation', 'Graduated', data, segmentation_order, 'Customer Segmentation based on the Status of Graduation', (10,7))","93e8ab1c":"countplot('Segmentation', 'Ever_Married', data, segmentation_order, 'Customer Segmentation based on the Marital Status', (10,7))","ad07d6af":"# Making a copy of the original dataframe before making any manipulations for later reference \ndata_copy = data.copy()\ndata_copy.head()","c73567d0":"# Dealing with Missing values \ndata_copy.isnull().sum()*100\/len(data)","d2f31d17":"data_copy.dropna(inplace=True)","3b4f983f":"data_copy.reset_index(drop=True, inplace=True)\ndata_copy.head(2)","792266b0":"data_copy.drop(\"ID\", axis=1, inplace=True)","229cd146":"# Since, the data have categorical values, changing them into numericals with Label encoding \n\nfrom sklearn.preprocessing import LabelEncoder\n\ncat = [\"Ever_Married\", \"Graduated\",\"Gender\", \"Profession\", \"Spending_Score\", \"Var_1\", 'Segmentation']\nfor i in cat:\n  le = LabelEncoder()\n  labels = le.fit_transform(data_copy[i])\n  data_copy.drop(i, axis=1, inplace=True)\n  data_copy[i]= labels\ndata_copy.head()\n\n\n","77622a1f":"fig,ax = plt.subplots(figsize=(10,7))\nsns.heatmap(data_copy.corr(), annot=True, ax=ax)\nplt.show()","c0dcc608":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","35e567bb":"x = data_copy.drop('Segmentation', axis=1)\ny = data_copy.Segmentation.values","b71da292":"x.head()","dfb5b2b5":"y","a696e099":"np.random.seed(42)\n\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y,\n                                                    test_size=0.2)","b456680c":"x_train.shape, y_train.shape","dd332a08":"x_test.shape, y_test.shape","9e3cfb66":"models = {\"KNN\": KNeighborsClassifier(),\n          \"Random_Forest\": RandomForestClassifier(),\n          \"Logistic_Regression\": LogisticRegression(solver='liblinear')}\n\ndef fit_and_score_models(model, x_train, x_test, y_train, y_test):\n\n  np.random.seed(42)\n  model_scores = {}\n\n  for name, model in models.items():\n    model.fit(x_train, y_train)\n    model_scores[name] = model.score(x_test, y_test)\n  return model_scores","2e7dc2df":"model_scores = fit_and_score_models(models, x_train, x_test, y_train, y_test)\nmodel_scores","791b487d":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot(kind='bar')\nplt.xticks(rotation=0);","caf4032c":"model = LogisticRegression(solver='liblinear')\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)","240ff96b":"y_preds = model.predict(x_test)\nprint(classification_report(y_test, y_preds))","291aad2d":"# EDA","0536dad7":"Now, let us see some evaluation metrics for our best performed model","1222ed3a":"# Modelling","f8a877b4":"Since, there's not much of a correlation between features, so we need not remove any features...","5b30ce87":"With the available data, this the best our model can do, if we change the way of encoding, it may perform a little bit better, but knowing that the data is not suuficient, I'm not gonna make anymore experiments on this, \n\n\n**Thanks for viewing this and kindly, Upvote my kernel if you liked it...**\n\n**Cheers :))**","28c2c533":"# Now, lets try fitting three different models and see the results","04d3c921":"As, we can see, our Logistic Regression model is performing relatively well than the other models..."}}