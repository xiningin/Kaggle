{"cell_type":{"5e219285":"code","f8cd68e8":"code","be302b95":"code","a39bf72c":"code","67632416":"code","92a7fc97":"code","b41f86cd":"code","a76f82bb":"code","6f83dbf6":"code","3edf4b49":"code","2e48bc8b":"markdown","d5e8da20":"markdown","a792c15b":"markdown","1930cb92":"markdown","292b208c":"markdown","f880c649":"markdown","6e16db94":"markdown","37935a32":"markdown","729523ca":"markdown","ab4b185b":"markdown"},"source":{"5e219285":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll use only numerical predictors\nX = X_full.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","f8cd68e8":"X_train.head()","be302b95":"# Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","a39bf72c":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = XGBRegressor(n_estimators=100, random_state=0, learning_rate=0.1)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","67632416":"cols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Fill in the lines below: drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\nprint(\"MAE (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","92a7fc97":"categorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train_prep = X_train[my_cols].copy()\nX_valid_prep = X_valid[my_cols].copy()\nX_test_prep = X_test[my_cols].copy()","b41f86cd":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\nnumerical_transformer = SimpleImputer(strategy='median')\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))\n                                         ])\npreprocessor = ColumnTransformer(transformers = [\n                                    ('num', numerical_transformer, numerical_cols),\n                                    ('cat', categorical_transformer, categorical_cols)\n])","a76f82bb":"model_XGB = XGBRegressor(n_estimators=200, learning_rate=0.07)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model_XGB)\n                     ])\nclf.fit(X_train_prep, y_train)\npreds = clf.predict(X_valid_prep)\nprint('MAE:', mean_absolute_error(y_valid, preds))","6f83dbf6":"preds_test = clf.predict(X_test_prep)","3edf4b49":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_prep.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","2e48bc8b":"Data from the [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course). \n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)\n\n","d5e8da20":"> # Step 2: Prepare Preprocessing\n\n\n\n","a792c15b":"Print the first five rows of the data.","1930cb92":"# Step 4: Generate test predictions\n\n","292b208c":"Divide the variables into categorical and numerical:","f880c649":"> # Step 3. Fit the model (XGBRegression)","6e16db94":"So there are 1168 rows in data in total, 3 columns in the training data have missing values. Overall, there are 276 missing entries in the training data.","37935a32":"### Score the dataset with dropped columns","729523ca":"We can already see a few missing values in the first several rows.  \n\n# Step 1: Preliminary investigation\n","ab4b185b":"To compare different approaches to dealing with missing values, I'll use  `score_dataset()` function.  This function reports the [mean absolute error](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) (MAE) from a XGBRegressor model."}}