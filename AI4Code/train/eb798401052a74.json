{"cell_type":{"1f190f5f":"code","bae81eaa":"code","09f08c15":"code","d0efcedb":"code","af74096d":"code","4dbc25bf":"code","4f658c8a":"code","00ca8ece":"code","42ddcf96":"code","5355f5d8":"code","9d6be424":"code","b3a51e9a":"code","8b7e8363":"code","0d229e3d":"code","a4a7c73a":"code","d289cb20":"code","b8864a5c":"code","6fb8f2dc":"code","a5d3169f":"code","9cd28547":"code","33d7c678":"code","926c28fb":"code","fc9d142b":"code","88c561d7":"code","2cf2afc8":"code","6044c498":"code","56cc63d9":"code","64f1a465":"code","ab017b5a":"code","54879168":"code","c2c0739d":"code","0070b3db":"code","249418d1":"code","dfa85c54":"code","6e8ad7f2":"code","fab54fbe":"code","3c74d397":"code","4a35a2c8":"code","5b18fa51":"code","97a6a1dc":"code","71413977":"code","42cf4071":"markdown","b3143df6":"markdown","8ea39a33":"markdown","d1926543":"markdown","131f1a42":"markdown","650e2b8b":"markdown","f10bc530":"markdown","caee9256":"markdown","5e928e9e":"markdown","ffbab257":"markdown","438fe96c":"markdown","ce77ae08":"markdown","0cd90d8f":"markdown","32ecf4f7":"markdown","a7aa884d":"markdown","9e221940":"markdown","17e00db0":"markdown","0957ac91":"markdown","7d5f130d":"markdown","36500bf9":"markdown","dff707e0":"markdown","e4ebb7b2":"markdown","5f718348":"markdown","89d5688f":"markdown","4021625f":"markdown","42910f7e":"markdown","981ae3db":"markdown","4455c0b8":"markdown","a04310b3":"markdown","fe04be6a":"markdown","b1b5a992":"markdown","34c86f22":"markdown","f0f9e9b6":"markdown","8369f2e9":"markdown","06fea48e":"markdown","2750b317":"markdown","6ff74244":"markdown","5ae55f83":"markdown"},"source":{"1f190f5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bae81eaa":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","09f08c15":"df.info()","d0efcedb":"duplicates = df.duplicated(subset='id')\ndf[duplicates]","af74096d":"df = df.drop(['id'],axis=1)","4dbc25bf":"date= list(df.select_dtypes(include=['datetime64[ns]']))\ncats= list(df.select_dtypes(include=['object','bool']) )\nnums= list(df.select_dtypes(include=['int64','float64']))\nprint(date)\nprint(cats)\nprint(nums)","4f658c8a":"df.isna().sum()","00ca8ece":"df.describe()","42ddcf96":"df['bmi'].fillna(df['bmi'].mean(), inplace=True)","5355f5d8":"df.duplicated().sum()","9d6be424":"plt.figure(figsize=(15, 7))\nfor i in range(0, len(nums)):\n    plt.subplot(2, 3, i+1)\n    sns.boxplot(y=df[nums[i]],color='green',orient='v')\n    plt.tight_layout()","b3a51e9a":"plt.figure(figsize=(15, 7))\nfor i in range(0, len(nums)):\n    plt.subplot(2, 3, i+1)\n    sns.distplot(df[nums[i]], color='gray')\n    plt.tight_layout()","8b7e8363":"outlier = ['avg_glucose_level','bmi']\nskewed = ['avg_glucose_level','bmi']","0d229e3d":"for col in skewed:\n    df[col] = np.log(df[col])","a4a7c73a":"plt.figure(figsize=(12, 5))\nfor i in range(0, len(nums)):\n    plt.subplot(2, len(nums)\/2, i+1)\n    sns.distplot(df[nums[i]], color='gray')\n    plt.tight_layout()","d289cb20":"from scipy import stats\nprint(f'Length of the data before filtering outlier: {len(df)}')\n\nfiltered_entries = np.array([True] * len(df))\n\nfor col in outlier:\n    zscore = abs(stats.zscore(df[col]))\n    filtered_entries = (zscore < 3) & filtered_entries\n    \ndf = df[filtered_entries]\n\nprint(f'Length of the data after filtering outlier: {len(df)}')","b8864a5c":"for i in range(0, len(cats)):\n    plt.subplot(3, len(cats)\/2, i+1)\n    sns.countplot(df[cats[i]], color='gray', orient='v')\n    plt.tight_layout()","6fb8f2dc":"df['stroke'].value_counts().plot(kind='bar')","a5d3169f":"for col in cats:\n    print(str(col))\n    print(df[col].unique())","9cd28547":"labenco = []\nonehot = []\nfor col in cats:\n    if len(df[col].unique()) == 2:\n        labenco.append(col)\n    else:\n        onehot.append(col)\nprint(labenco)\nprint(onehot)","33d7c678":"df_labencoded = df.copy()\nfor col in labenco:\n    df_labencoded[col] = df_labencoded[col].astype('category').cat.codes\ndf_labencoded.head()","926c28fb":"plt.figure(figsize=(15, 8))\nsns.heatmap(df_labencoded.corr(), cmap='Blues', annot=True, fmt='.2f')","fc9d142b":"for col in nums:\n    plt.figure(figsize=(15, 8))\n    print(sns.barplot(x='stroke',y=col,data=df_labencoded))","88c561d7":"for col in onehot:\n    df_loop = df_labencoded[[col,'stroke']].copy()\n    onehots = pd.get_dummies(df_loop[col], prefix=col)\n    df_loop = df_loop.join(onehots)\n    plt.figure(figsize=(15, 8))\n    print(sns.heatmap(df_loop.corr(), cmap='Blues', annot=True, fmt='.2f'))","2cf2afc8":"for col in onehot:\n    df_loop = df_labencoded[[col,'stroke']].copy()\n    onehots = pd.get_dummies(df_loop[col], prefix=col)\n    df_loop = df_loop.join(onehots)\n    plt.figure(figsize=(15, 8))\n    print(sns.barplot(x=col,y='stroke',data=df_labencoded))","6044c498":"selected_feat=['age','hypertension','heart_disease','work_type','smoking_status','stroke']\ndf_pre_model = df[selected_feat].copy()\ndf_pre_model.head()","56cc63d9":"onehot_pre_model = ['work_type','smoking_status']\nfor col in onehot_pre_model:\n    onehots = pd.get_dummies(df_pre_model[col], prefix=col)\n    df_pre_model = df_pre_model.join(onehots)\n    df_pre_model = df_pre_model.drop([col],axis=1)\ndf_pre_model.head()","64f1a465":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\ndf_model = df_pre_model.copy()\ndf_model['age'] = StandardScaler().fit_transform(df_pre_model['age'].values.reshape(len(df), 1))\ndf_model.head()","ab017b5a":"X = df_model.drop(['stroke'],axis=1)\ny = df_model['stroke']","54879168":"from imblearn import under_sampling, over_sampling\nX_under, y_under = under_sampling.RandomUnderSampler(0.5).fit_resample(X, y)\nX_over, y_over = over_sampling.RandomOverSampler(0.5).fit_resample(X, y)\nX_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE(0.5).fit_resample(X, y)","c2c0739d":"print('Original')\nprint(pd.Series(y).value_counts())\nprint('UNDERSAMPLING')\nprint(pd.Series(y_under).value_counts())\nprint('OVERSAMPLING')\nprint(pd.Series(y_over).value_counts())\nprint('SMOTE')\nprint(pd.Series(y_over_SMOTE).value_counts())","0070b3db":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\ndef eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n    \n    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive\n    print(\"AUC: %.2f\" % auc(fpr, tpr))\n\ndef show_feature_importance(model):\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\n    ax.invert_yaxis()\n\n    plt.xlabel('score')\n    plt.ylabel('feature')\n    plt.title('feature importance score')\n\ndef show_best_hyperparameter(model, hyperparameters):\n    for key, value in hyperparameters.items() :\n        print('Best '+key+':', model.get_params()[key])","249418d1":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X_over_SMOTE, y_over_SMOTE, test_size = 0.3, random_state = 42)","dfa85c54":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","6e8ad7f2":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train,y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","fab54fbe":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","3c74d397":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier(random_state=42)\nmodel.fit(X_train, y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","4a35a2c8":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train,y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","5b18fa51":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier(random_state=42)\nmodel.fit(X_train,y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","97a6a1dc":"from xgboost import XGBClassifier\nmodel = XGBClassifier(random_state=42)\nmodel.fit(X_train, y_train)\nprint(str(model)+' '+'EVALUATION')\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","71413977":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport numpy as np\n\nhyperparameters = {\n                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],\n                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],\n                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],\n\n                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],\n\n                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]\n                    }\n\nxg = XGBClassifier(random_state=42)\nxg_tuned = RandomizedSearchCV(xg, hyperparameters,random_state=42, cv=5, scoring='recall')\nxg_tuned.fit(X_train,y_train)\n\ny_pred = xg_tuned.predict(X_test)\neval_classification(xg_tuned, y_pred, X_train, y_train, X_test, y_test)\nprint('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","42cf4071":"**Age column's values are relatively larger compared to the other column's values in the data, so the age column's are standardized.**","b3143df6":"**Mainly, heatmap and barplot will be used hence its representative enough to see whether there are a correlation between the data.**","8ea39a33":"**Since hyperparameter tuning doesnt provide a better recall score, we use the original xgboost model instead.**","d1926543":"**In order to import the data, pandas function read_csv is used**","131f1a42":"**The distribution of avg_glucose_level and bmi columns are skewed to the left. Other than that, avg_glucose_level and bmi columns also contains outlier.**","650e2b8b":"# MODELING AND EVALUATION","f10bc530":"# IMPORTING DATA","caee9256":"**.info() function is used to check the columns data types so it can be processed further**","5e928e9e":"**Search for null values in the data.**","ffbab257":"**Remove the outlier from the data using z-score method because the data are already close to normal distribution thanks to the log transformation!**","438fe96c":"**Create a new dataframe containing only the selected columns from the original dataframe.**","ce77ae08":"**Since the target values are imbalanced, SMOTE method for imbalanced data are used.**","0cd90d8f":"**Based on the heatmap and barplot above, it can be said that smoking_status and work_type can be used to predict stroke.**","32ecf4f7":"**Since the mean and the median values are pretty close in the \"bmi\" column of the dataframe, the mean values can be used to fill the null.**","a7aa884d":"**Predict the target using various model to see which one has the most satisfying result.**","9e221940":"**We can be predict a stroke with a recall score of 83% and accuracy score of 92% that a patient with the disease can be predicted.**","17e00db0":"**The distribution of avg_glucose_level and bmi columns are a lot better than before, its closer to normal distribution.**","0957ac91":"**NUMERICAL VARIABLES EDA**","7d5f130d":"**Created a function so it will be easier to evaluate a model.**","36500bf9":"**Used countplot just to see a brief summary of the categorical data.**","dff707e0":"# DATA STANDARDIZATION","e4ebb7b2":"**Search for the duplicate values in the dataframe, got 0 value.**","5f718348":"**Based from the heatmap and barplot above, it can be said that age, hypertension, and heart_disease columns can be used to predict stroke.**","89d5688f":"**The XGBoost model has the biggest accuracy and recall score, so it can be said that decision tree has the most satisfying result of the bunch. Other than that, the train score and the test score arent too far apart, theres only 2% of a difference in accuracy, so it can be said that the model are not overfitted.**","4021625f":"**Used .value_counts() function on the stroke\/target columns to see whether the target are imbalanced, and it is!**","42910f7e":"# DATA CLEANING","981ae3db":"**Used hyperparameter tuning method to improve the model quality. Scoring method based on the recall value is used because we dont want to mispredict the patient who got stroke but we predict otherwise.**","4455c0b8":"**Boxplot are used to check whether there are outlier contained in the data. age_glucose_level dan bmi columns contains outlier based on the boxplot.**","a04310b3":"**Distplot are used to check the distribution of the data. If the data are skewed we can list it to process it further.**","fe04be6a":"# CONCLUSION","b1b5a992":"**Checked the duplicate values based on ID column, if theres none, the column can be dropped since its no longer serves a purpose in the analysis**","34c86f22":"**Created a list containing columns based on their data types.**","f0f9e9b6":"**As we can see above, the data are balanced!**","8369f2e9":"**Log transformation is used to remove the skewness from the data.**","06fea48e":"**Encode the data based on their unique values, labelencoding and one-hot encoding are used.**","2750b317":"# DATA ENCODING AND EXPLORATORY DATA ANALYSIS ","6ff74244":"**Figured that there are 201 missing values in the data, .describe() function is used to check whether the \"mean\" values are close enough to the median. If they are close enough, it can be assumed that the mean is robust enough and the data doesnt really contain a lot of extreme outliers**","5ae55f83":"**ONEHOT ENCODING EDA**"}}