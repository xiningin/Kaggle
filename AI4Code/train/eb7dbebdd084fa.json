{"cell_type":{"1202f0c9":"code","2963c425":"code","155d2992":"code","e7a38472":"code","e577a426":"code","00a53c20":"code","e87a4084":"code","4735f277":"code","c9931948":"code","9fa446fe":"code","1cd44ff0":"code","17157a83":"code","b8a760e8":"code","ce49ab9a":"code","fa2ce006":"code","76994927":"code","ea68f390":"code","3a2d1020":"code","75f4bdf7":"code","3a5e867f":"code","a5cdb376":"code","d8858d2f":"code","81b3805a":"code","b7953c69":"code","292c18c7":"code","75acf2b4":"code","667620ac":"code","104abfe6":"code","77d97db0":"code","13bdb1b1":"code","c5c3f87c":"code","b8423c70":"code","55e9458f":"code","93ed1d63":"code","6e40008a":"code","f95f2ee3":"code","b6ad919c":"code","12cda914":"code","420d302f":"code","60d7ed5f":"code","76f6e1c5":"code","23ad252d":"code","7133b671":"code","412990a7":"code","b141c321":"code","c706a44d":"code","27869d0e":"code","fc23b67d":"code","5f0a9c9d":"markdown","18193f28":"markdown","5d9ed28b":"markdown","f29c717d":"markdown","a2a7cec3":"markdown","697de123":"markdown","9ecfbc11":"markdown","8a879b87":"markdown","9f29b027":"markdown","8592084b":"markdown","3ba2d6b5":"markdown","b2b90f57":"markdown","136add4d":"markdown","6720f93a":"markdown","c4da55b1":"markdown","b4937154":"markdown","e07965dd":"markdown","5162f342":"markdown","b00eef44":"markdown","03d32cd5":"markdown","e14ba946":"markdown","debca212":"markdown","750996a9":"markdown"},"source":{"1202f0c9":"import numpy as np \nimport pandas as pd","2963c425":"import seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","155d2992":"#The next step is to load the data\n\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","e7a38472":"#Statistic (now, we need to describe the data)\ntrain.describe(include = 'all'). transpose()","e577a426":"train.isnull().sum()","00a53c20":"#now we need to check how many passengers survived \ntrain['Survived'].value_counts(normalize=True)","e87a4084":"graph = sns.countplot(train['Survived'], palette = \"Paired\").set_title('Survived and Non-Survived')","4735f277":"FacetGrid = sns.FacetGrid(train, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=\"Paired\",  order=None, hue_order=None )\nFacetGrid.add_legend()","c9931948":"fig, axarr = plt.subplots(1, 2, figsize=(12,8))\na = sns.countplot(train['Sex'], ax=axarr[0], palette = \"Paired\").set_title('Passengers number by sex')\naxarr[1].set_title('Survival rate by sex')\nb = sns.barplot(x='Sex', y='Survived', data=train, ax=axarr[1], palette = \"Paired\").set_ylabel('Survival rate')","9fa446fe":"train.groupby('Pclass').mean()['Survived']*100","1cd44ff0":"fig, axarr = plt.subplots(1,2,figsize=(12,5))\na = sns.countplot(x='Pclass', hue='Survived', data=train, ax=axarr[0], palette = \"Paired\").set_title('Survivors and Non-Survivors count by class')\naxarr[1].set_title('Survival rate by class')\nb = sns.barplot(x='Pclass', y='Survived', data=train, ax=axarr[1], palette = \"Paired\").set_ylabel('Survival rate')","17157a83":"train.groupby(['Pclass', 'Sex']).Survived.mean()","b8a760e8":"plt.title('Survival rate by sex and class')\ngraph = sns.barplot(x='Pclass', y='Survived', hue='Sex', data=train, palette = \"Paired\").set_ylabel('Survival rate')","ce49ab9a":"data = [train, test]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain['not_alone'].value_counts()","fa2ce006":"fig, axarr = plt.subplots(1,2,figsize=(12,5))\naxarr[0].set_title('Age Category')\nf = sns.distplot(train['Age'], color='b', bins=45, ax=axarr[0])\naxarr[1].set_title('Age category of survivors')\ngraph = sns.kdeplot(train['Age'].loc[train['Survived'] == 1], \n                shade= True, ax=axarr[1], label='Survived').set_xlabel('Age')\ngraph = sns.kdeplot(train['Age'].loc[train['Survived'] == 0], \n                shade=True, ax=axarr[1], label='Not Survived')","76994927":"plt.figure(figsize=(7,6))\ngraph = sns.swarmplot(y='Age', x='Sex', hue='Survived', data=train, palette = \"Paired\").set_title(' Age and Sex Survivel')","ea68f390":"plt.figure(figsize=(7,6))\nh = sns.swarmplot(x='Pclass', y='Age',hue='Survived', data=train, palette = \"Paired\").set_title('Age and Class Survival') \n             ","3a2d1020":"train_df = train.drop(['PassengerId'], axis=1)","75f4bdf7":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train, test]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain_df = train.drop(['Cabin'], axis=1)\ntest_df = test.drop(['Cabin'], axis=1)","3a5e867f":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train[\"Age\"].mean()\n    std = test[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\ntrain_df[\"Age\"].isnull().sum()","a5cdb376":"train_df['Embarked'].describe()","d8858d2f":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","81b3805a":"train_df.info()","b7953c69":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","292c18c7":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","75acf2b4":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","667620ac":"train_df['Ticket'].describe()","104abfe6":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","77d97db0":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","13bdb1b1":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \ntrain_df['Age'].value_counts()","c5c3f87c":"train_df['Age'].describe()","b8423c70":"train_df.head(10)","55e9458f":"train_df[\"Fare\"].describe()","93ed1d63":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain_df['Fare'].value_counts() ","6e40008a":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","f95f2ee3":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(10)","b6ad919c":"train_df = train_df.drop(\"PassengerId\", axis=1)\ntrain_df","12cda914":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","420d302f":"#Apply DecisionTreeClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n\ndecision_tree.score(X_train, Y_train)","60d7ed5f":"#Apply GradientBoostingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                 max_depth=1, random_state=0).fit(X_train, Y_train)\ny_prediction= clf.predict(X_test)\nclf.score(X_train, Y_train)\nacc_clf = round(clf.score(X_train, Y_train) * 100, 2)\nprint(round(acc_clf,2,), \"%\")","76f6e1c5":"#Apply LGBMClassifier\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier().fit(X_train, Y_train)\ny_predict= model.predict(X_test)\nmodel.score(X_train, Y_train)\nacc_model = round(model.score(X_train, Y_train) * 100, 2)\nprint(round(acc_model,2,), \"%\")","23ad252d":"#Apply Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","7133b671":"#Apply RandomForestClassifier\nrandom_forest= RandomForestClassifier(n_estimators=100,\n                             max_features='auto',\n                             criterion='entropy',\n                             max_depth=10)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","412990a7":"#Apply XGboost\n\nfrom xgboost import XGBClassifier\n\nparams_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nprint(round(acc_xgb,2,), \"%\")","b141c321":"results = pd.DataFrame({\n    'Model': ['LGBMClassifier', 'Logistic Regression', \n              'Random Forest', 'Boosting', \n              'Decision Tree','xgb'],\n    'Score': [ acc_model,acc_log,\n              acc_random_forest, acc_clf,\n              acc_decision_tree,acc_xgb]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)","c706a44d":"from sklearn.model_selection import cross_val_score\nrf = DecisionTreeClassifier()\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","27869d0e":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(decision_tree.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)\n","fc23b67d":"importances.plot.bar(color=\"g\", align=\"center\", width=0.5, alpha=0.5, figsize=(10,6))","5f0a9c9d":"# 3. Pclass","18193f28":"# ML models","5d9ed28b":"The tragic disaster of 1912. The TItanic a ship that sank leading to the deaths of more than 1500 passengers and crew. \"Of the 2,240 passengers and crew on board, more than 1,500 lost their lives\"","f29c717d":"# Libraries","a2a7cec3":"# 4. Age","697de123":"# Exploratory Data Analysis ","9ecfbc11":"# 2. Sex","8a879b87":"# 1. Survived","9f29b027":"<img src='https:\/\/i1.wp.com\/www.elenacuoco.com\/wp-content\/uploads\/2014\/12\/RMS-titanic-ship-1.jpg?fit=1024%2C700&ssl=1' \/>","8592084b":"Survival rate grows with the higher class of service.","3ba2d6b5":"It is now becoming clearer that a significant number of male survivors were less than 12 years old.","b2b90f57":"**Made the data ready for any model**","136add4d":"**Data Types**\n\nPassenger Id\n\nSurvived\n\nPclass - class of service (third- economy, second-business, first-class)\n\nName\n\nSex\n\nAge\n\nSibsp (the number of the passenger's relatives)\n\nParch (the number of children and parents)\n\nTicket\n\nCabin (#)\n\nEmbarked (S - Southampton, Q - Queenstown, C - Cherbourg)","6720f93a":"# Data","c4da55b1":"**Feature Importance**","b4937154":"# Feature Engineering","e07965dd":"On graph above, we maintain that the largest number of children are in the third class and, accordingly, the smallest in the first class.The survival rate between classes varies as well.","5162f342":"The survival rate of the females from first and second class 97% (first class) and 92%(second class) where the third-class is only about 50%. As for males, about 38%(first-class) 17%(second-class) and around 15%(third class).","b00eef44":" 38.4% of passengers managed to survive and 61.6% of the passengers perished when the Titanic sank","03d32cd5":"# Titanic: Machine Learning from Disaster","e14ba946":"The first chart shows that initially there were 65% male and 35% female on board. In the second graph, we can see that in the end, the number of survivors is 0.76 female and 0.24 male.","debca212":"**Title was the most important feature in survive!**","750996a9":"# The Best Model....."}}