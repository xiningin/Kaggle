{"cell_type":{"97da836f":"code","3ba6f0ac":"code","de2c2989":"code","d855560f":"code","a9d1ab5e":"code","9818b66d":"code","404d5a95":"code","b0f346f7":"code","9e9bbe4f":"code","98db4acc":"code","196bfd7f":"code","51cddb3e":"code","fd41a29e":"code","eede7c0e":"code","9cba0fa7":"code","602f069d":"code","9c0a3f65":"code","360a3c6e":"code","3817fd6c":"code","3507d15e":"code","3ea04fea":"markdown","998e5cb6":"markdown","05c8bddb":"markdown","e3befad6":"markdown","391e5f7c":"markdown","024e8b5c":"markdown"},"source":{"97da836f":"! pip install entropy","3ba6f0ac":"!pip install EntroPy-Package","de2c2989":"''' This module has essential functions supporting\nfast and effective computation of permutation entropy and\nits different variations.'''\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial.distance import euclidean","d855560f":"def s_entropy(freq_list):\n    ''' This function computes the shannon entropy of a given frequency distribution.\n    USAGE: shannon_entropy(freq_list)\n    ARGS: freq_list = Numeric vector representing the frequency distribution\n    OUTPUT: A numeric value representing shannon's entropy'''\n    freq_list = [element for element in freq_list if element != 0]\n    sh_entropy = 0.0\n    for freq in freq_list:\n        sh_entropy += freq * np.log(freq)\n    sh_entropy = -sh_entropy\n    return(sh_entropy)","a9d1ab5e":"def ordinal_patterns(ts, embdim, embdelay):\n    ''' This function computes the ordinal patterns of a time series for a given embedding dimension and embedding delay.\n    USAGE: ordinal_patterns(ts, embdim, embdelay)\n    ARGS: ts = Numeric vector representing the time series, embdim = embedding dimension (3<=embdim<=7 prefered range), embdelay =  embdding delay\n    OUPTUT: A numeric vector representing frequencies of ordinal patterns'''\n    time_series = ts\n    possible_permutations = list(itertools.permutations(range(embdim)))\n    lst = list()\n    for i in range(len(time_series) - embdelay * (embdim - 1)):\n        sorted_index_array = list(np.argsort(time_series[i:(embdim+i)]))\n        lst.append(sorted_index_array)\n    lst = np.array(lst)\n    element, freq = np.unique(lst, return_counts = True, axis = 0)\n    freq = list(freq)\n    if len(freq) != len(possible_permutations):\n        for i in range(len(possible_permutations)-len(freq)):\n            freq.append(0)\n        return(freq)\n    else:\n        return(freq)","9818b66d":"def p_entropy(op):\n    ordinal_pat = op\n    max_entropy = np.log(len(ordinal_pat))\n    p = np.divide(np.array(ordinal_pat), float(sum(ordinal_pat)))\n    return(s_entropy(p)\/max_entropy)","404d5a95":"def complexity(op):\n    ''' This function computes the complexity of a time series defined as: Comp_JS = Q_o * JSdivergence * pe\n    Q_o = Normalizing constant\n    JSdivergence = Jensen-Shannon divergence\n    pe = permutation entopry\n    ARGS: ordinal pattern'''\n    pe = p_entropy(op)\n    constant1 = (0.5+((1 - 0.5)\/len(op)))* np.log(0.5+((1 - 0.5)\/len(op)))\n    constant2 = ((1 - 0.5)\/len(op))*np.log((1 - 0.5)\/len(op))*(len(op) - 1)\n    constant3 = 0.5*np.log(len(op))\n    Q_o = -1\/(constant1+constant2+constant3)\n\n    temp_op_prob = np.divide(op, sum(op))\n    temp_op_prob2 = (0.5*temp_op_prob)+(0.5*(1\/len(op)))\n    JSdivergence = (s_entropy(temp_op_prob2) - 0.5 * s_entropy(temp_op_prob) - 0.5 * np.log(len(op)))\n    Comp_JS = Q_o * JSdivergence * pe\n    return(Comp_JS)","b0f346f7":"def weighted_ordinal_patterns(ts, embdim, embdelay):\n    time_series = ts\n    possible_permutations = list(itertools.permutations(range(embdim)))\n    temp_list = list()\n    wop = list()\n    for i in range(len(time_series) - embdelay * (embdim - 1)):\n        Xi = time_series[i:(embdim+i)]\n        Xn = time_series[(i+embdim-1): (i+embdim+embdim-1)]\n        Xi_mean = np.mean(Xi)\n        Xi_var = (Xi-Xi_mean)**2\n        weight = np.mean(Xi_var)\n        sorted_index_array = list(np.argsort(Xi))\n        temp_list.append([''.join(map(str, sorted_index_array)), weight])\n    result = pd.DataFrame(temp_list,columns=['pattern','weights'])\n    freqlst = dict(result['pattern'].value_counts())\n    for pat in (result['pattern'].unique()):\n        wop.append(np.sum(result.loc[result['pattern']==pat,'weights'].values))\n    return(wop)","9e9bbe4f":"from entropy import *\nimport numpy as np\nnp.random.seed(1234567)\nx = np.random.rand(3000)\n#print(perm_entropy(x, order=3, normalize=True))                 # Permutation entropy\n#print(spectral_entropy(x, 100, method='welch', normalize=True)) # Spectral entropy\n#print(svd_entropy(x, order=3, delay=1, normalize=True))         # Singular value decomposition entropy\n#print(app_entropy(x, order=2, metric='chebyshev'))              # Approximate entropy\n#print(sample_entropy(x, order=2, metric='chebyshev'))           # Sample entropy\n#print(lziv_complexity('01111000011001', normalize=True))        # Lempel-Ziv complexity","98db4acc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","196bfd7f":"import torch\nimport fastai\nfrom fastai.tabular.all import *\nfrom fastai.text.all import *\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *\nfrom fastai import *\n\nimport time\nfrom datetime import datetime\n\nprint(f'Notebook last run on {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\nprint('Using fastai version ',fastai.__version__)\nprint('And torch version ',torch.__version__)","51cddb3e":"from PIL import Image\n\nimg = Image.open(\"..\/input\/entropy\/entropy.png\")\nimg","fd41a29e":"TensorTypes = (TensorImage,TensorMask,TensorPoint,TensorBBox)","eede7c0e":"def _add1(x): return x+1\ndumb_tfm = RandTransform(enc=_add1, p=0.5)\nstart,d1,d2 = 2,False,False\nfor _ in range(40):\n    t = dumb_tfm(start, split_idx=0)\n    if dumb_tfm.do: test_eq(t, start+1); d1=True\n    else:           test_eq(t, start)  ; d2=True\nassert d1 and d2\ndumb_tfm","9cba0fa7":"_,axs = subplots(1,2)\nshow_image(img, ctx=axs[0], title='original')\nshow_image(img.flip_lr(), ctx=axs[1], title='flipped');","602f069d":"img.resize((64,64))","9c0a3f65":"timg = TensorImage(image2tensor(img))\ntpil = PILImage.create(timg)","360a3c6e":"TensorTypes = (TensorImage,TensorMask,TensorPoint,TensorBBox)\n\ndef flip_lr(x:Image.Image): return x.transpose(Image.FLIP_LEFT_RIGHT)\ndef flip_lr(x:TensorImageBase): return x.flip(-1)\ndef flip_lr(x:TensorPoint): return TensorPoint(_neg_axis(x.clone(), 0))\ndef flip_lr(x:TensorBBox):  return TensorBBox(TensorPoint(x.view(-1,2)).flip_lr().view(-1,4))","3817fd6c":"img = PILImage(PILImage.create(timg).resize((600,400)))\nimg","3507d15e":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Nur ein kleiner, tiefer Atemzug. Das war s.' )","3ea04fea":"Important\n\nPlease note that EntroPy cannot be installed using pip or conda. There is already a package called entropy on the pip repository, which should not be mistaken with the current package.","998e5cb6":"#Code by Raphael Vallat https:\/\/raphaelvallat.com\/entropy\/build\/html\/index.html","05c8bddb":"#Codes by srk-srinivasan https:\/\/github.com\/srk-srinivasan\/Permutation-Entropy\/blob\/master\/p_entropy.py","e3befad6":"ENTROPY\n\nEntropy is a scientific concept, as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.\nhttps:\/\/en.wikipedia.org\/wiki\/Entropy","391e5f7c":"#EntroPy\n\nEntroPy is a Python 3 package providing several time-efficient algorithms for computing the complexity of one-dimensional time-series. It can be used for example to extract features from EEG signals.\n\nhttps:\/\/raphaelvallat.com\/entropy\/build\/html\/index.html","024e8b5c":"#That's how I'm feeling writing this Kaggle Notebook. No Clue at all!\n\n![](http:\/\/pit-claudel.fr\/clement\/blog\/wp-content\/uploads\/2013\/12\/entropy-of-english-featured.png)pit-claudel.fr"}}