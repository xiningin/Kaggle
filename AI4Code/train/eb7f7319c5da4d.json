{"cell_type":{"5cc91e13":"code","c3a10ebd":"code","ade4cce3":"code","00c56e07":"code","899efcd8":"code","2cb2aea3":"code","81de759b":"code","9e5f1055":"code","d9ced5bc":"code","41ecc926":"code","cbe8b8d6":"code","98199c72":"code","43717832":"code","a52c3137":"code","c45e4f33":"code","1247fca5":"code","bcc1b6c8":"code","5aaae568":"code","ace42348":"code","91b7ff8c":"code","36e39424":"code","c29ec559":"code","972b9c45":"code","23c845dc":"code","8acdd23d":"code","394f29f5":"code","835da06b":"code","219c1e1f":"code","adf82462":"code","fcd10987":"code","b9aeba99":"code","cf7f1aff":"code","fe05ce9c":"code","04ba5c31":"code","875cd803":"code","bcb4f9dd":"code","b8cf2dc3":"code","08c85de9":"code","33599743":"code","b6b4e8fa":"code","c3981485":"code","26d19a5d":"code","3cf5c275":"code","38e0ed71":"code","e38dac94":"code","27f96dc8":"code","ed71deea":"code","44b5c215":"code","fdc36139":"code","079955d2":"code","c4b4e83f":"code","7c161ab8":"markdown","b1ea9841":"markdown","1269ab36":"markdown","a44a89df":"markdown","1ef70392":"markdown","2b8f1138":"markdown","3121e571":"markdown","dcabc796":"markdown","3892d540":"markdown","9deb9771":"markdown","530ea995":"markdown","e69affcf":"markdown","58ca2472":"markdown","1d24582b":"markdown","af6c5d26":"markdown","1bc9d4b4":"markdown","7904d91c":"markdown","7db5eed4":"markdown","f58033e6":"markdown","3f43e165":"markdown","7ea13d0d":"markdown","d2c57dda":"markdown","aa1dad45":"markdown","da3502e8":"markdown","446ef3d3":"markdown","dfa86743":"markdown","1a37b68b":"markdown","c82c62f4":"markdown","fe5f843a":"markdown","950a3783":"markdown","90e1cd15":"markdown","93170d26":"markdown","b69fb206":"markdown","53cb2f35":"markdown","e6fbb79d":"markdown","7c7c844a":"markdown","a1ac2422":"markdown","1ae9e970":"markdown","56ccd65b":"markdown","929010ea":"markdown"},"source":{"5cc91e13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c3a10ebd":"#!pip install lazypredict","ade4cce3":"# Visualization \nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')","00c56e07":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.info()\n","899efcd8":"train_data.head()","2cb2aea3":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.info()","81de759b":"test_data.head()","9e5f1055":"data = pd.concat([train_data['Survived'], train_data['Fare']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=train_data['Survived'], y=train_data['Fare'], data=data)\n#fig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","d9ced5bc":"fig, ax = plt.subplots(figsize=(5,5))\nax.scatter(train_data['Survived'],train_data['Fare'])\nax.set_xlabel('Survived')\nax.set_ylabel('Fare')\nplt.show()","41ecc926":"train_data[train_data['Fare'] > 500]","cbe8b8d6":"test_data[test_data['Fare'] > 500]","98199c72":"#do it for both sets\ntrain_data[\"Fare\"].replace({ 512.3292 : 7.25}, inplace=True)\ntest_data[\"Fare\"].replace({ 512.3292 : 7.25}, inplace=True)","43717832":"train_data = train_data[(train_data.PassengerId != 259) & (train_data.PassengerId != 680) & (train_data.PassengerId != 738)]","a52c3137":"#Check for na's\ntrain_data.isna().sum()\n","c45e4f33":"test_data.isna().sum()","1247fca5":"import missingno as msno \nmsno.bar(train_data) ","bcc1b6c8":"#train data has 177 age nans\nm = train_data['Age'].mean()\nm","5aaae568":"#test data has 86 age nans\nm = test_data['Age'].mean()\nm","ace42348":"train_data['Age'] = train_data['Age'].replace(np.nan, 29)\ntest_data['Age'] = test_data['Age'].replace(np.nan, 30)","91b7ff8c":"#next is cabin \n#Shows there is more than 60% of NA's for the column so we should drop it. Judging by .info()\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)","36e39424":"#embarked on train data is only 2 nans\n#embarked on test data is 0\ntrain_data['Embarked'] = train_data['Embarked'].replace(np.nan, 'Q')\n\n#Fare on train data is 0 nans\n#Fare on test data is 1\n#m = test_data['Fare'].mean()\n#The mean obtained was 36 for the column 'Fare'\ntest_data['Fare'] = test_data['Fare'].replace(np.nan, 36.0)","c29ec559":"#Check for na's\ntrain_data.isna().sum()","972b9c45":"#Check for na's\ntest_data.isna().sum()","23c845dc":"# create a new feature to extract title names from the Name column\n#We can see that there are alot of extra names we dont need.\ntrain_data['Title'] = train_data.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntrain_data.Title.value_counts()","8acdd23d":"test_data['Title'] = test_data.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\ntest_data.Title.value_counts()","394f29f5":"# normalize the titles\nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royal\",\n    \"Don\":        \"Royal\",\n    \"Sir\" :       \"Royal\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royal\",\n    \"Dona\":       \"Royal\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royal\"\n}\n# map the normalized titles to the current titles for train and test data.\ntrain_data.Title = train_data.Title.map(normalized_titles)\ntest_data.Title = test_data.Title.map(normalized_titles)","835da06b":"# view value counts for the normalized titles\ntest_data.Title.value_counts()","219c1e1f":"# view value counts for the normalized titles\ntrain_data.Title.value_counts()","adf82462":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#initialize label endoce as first step.\nle = LabelEncoder()\n\ntrain_data[\"Sex\"] = le.fit_transform(train_data[\"Sex\"].values)\ntest_data[\"Sex\"] = le.fit_transform(test_data[\"Sex\"].values)","fcd10987":"# for now we drop ticket column\ntrain_data = train_data.drop(['Ticket'], axis=1)\ntest_data = test_data.drop(['Ticket'], axis=1)","b9aeba99":"train_data[\"Embarked\"].value_counts()","cf7f1aff":"#label encode them \ntrain_data[\"Embarked\"] = le.fit_transform(train_data[\"Embarked\"].values)\ntest_data[\"Embarked\"] = le.fit_transform(test_data[\"Embarked\"].values)","fe05ce9c":"train_data[\"Name\"] = le.fit_transform(train_data[\"Name\"].values)\ntest_data[\"Name\"] = le.fit_transform(test_data[\"Name\"].values)","04ba5c31":"train_data.info()","875cd803":"test_data.info()","bcb4f9dd":"train_data.head()","b8cf2dc3":"test_data.head()","08c85de9":"#With the power of google, many people say logistic regression is the best for titanic datasets.\n#although lightgbm is more suited \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport lightgbm as gbm\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score","33599743":"#\"PassengerId\",\"Pclass\",'Name',\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"\n#But due to feature importance  Passengerid,Name,Parch,Embarked have been removed\n\nfeatures = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Fare\"]\nX = train_data[features]\ny = train_data['Survived']\n\nX_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.15)\n\n#Last bit - encode labels\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","b6b4e8fa":"pipelineone = Pipeline([('RandomForest', RandomForestClassifier())])\n\nparam_grid = {'RandomForest__max_features': ['auto'],\n 'RandomForest__min_samples_leaf': [1, 2, 4],\n 'RandomForest__min_samples_split': [2,3,4],\n 'RandomForest__n_estimators': [100,200,300]}\n\n\n#Gridsearch takes in param_grid, and pipeline.\nmodel = GridSearchCV(pipelineone, param_grid, cv =None)\nmodel.fit(X_train, y_train)\n\n#submission\ny_pred_sub = model.predict(test_data[features])  \n\ny_pred_random = model.predict(X_test)\nforest = accuracy_score(y_test, y_pred_random)\nprint(forest)","c3981485":"#[0.001, 0.01, 0.1, 1, 10, 100, 1000]\npipelinetwo = Pipeline([('logisticregression', LogisticRegression(max_iter=100))])\nparam_grid = {'logisticregression__penalty' : ['l2'],\n              'logisticregression__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n              'logisticregression__solver' : ['liblinear']}\n\n#Gridsearch takes in param_grid, and pipeline.\nmodel = GridSearchCV(pipelinetwo, param_grid, cv =None)\nmodel.fit(X_train, y_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)  \nlogistic = accuracy_score(y_test, y_pred)\nprint(logistic)","26d19a5d":"# get importance\nimportance = model.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","3cf5c275":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\ny_pred = xg.predict(X_test)  \nXGB = accuracy_score(y_test, y_pred)\nprint(XGB)","38e0ed71":"KN = KNeighborsClassifier(n_neighbors=3)\nKN.fit(X_train, y_train)\ny_pred = KN.predict(X_test)  \nknn = accuracy_score(y_test, y_pred)\nprint(knn)","e38dac94":"svc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)  \nSVC = accuracy_score(y_test, y_pred)\nprint(SVC)","27f96dc8":"lsvc = LinearSVC(random_state=0)\nlsvc.fit(X_train, y_train)\ny_pred = lsvc.predict(X_test)  \nLSVC = accuracy_score(y_test, y_pred)\nprint(LSVC)","ed71deea":"finalleaderboard = {\n  \"Random Forest\": forest,\n  \"logistic regression\": logistic,  \n  \"XGB\": XGB,\n  \"KNeighborsClassifier\": knn,\n  \"SVC\": SVC,\n  \"Linear SVC\": LSVC  \n}\n\nfinalleaderboard = pd.DataFrame.from_dict(finalleaderboard, orient='index', columns=['Accuracy'])\nprint(finalleaderboard)","44b5c215":"len(y_pred_random)","fdc36139":"\n#Code required to submit for competition\n#There must be 418  test rows - test_data must be 418\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_sub})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","079955d2":"test_data.PassengerId.shape, y_pred_sub.shape","c4b4e83f":"print(np.count_nonzero(y_pred))\n#conclusion:\n#This many survived.","7c161ab8":"We can already see that there is one outlier with person that survived with an overwhelming fare that is around 500.","b1ea9841":"# logistic regression feature importance\ud83d\udea2\n\n\"PassengerId\",\"Pclass\",'Name',\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"\n\nLarge positive values of w_j signify higher importance of the jth feature in the prediction of positive class. Large negative values signify higher importance in the prediction of negative class.\n\nWe can see that the less significant columns are \"PassengerId\", \"Name\", \"Parch\",\"Fare\",\"Embarked\"","1269ab36":"# KNeighborsClassifier\ud83d\udea2","a44a89df":"# Start off with sex which has either male or female.\ud83d\udea2\n\nDepending on the data we have, we might run into situations where, after label encoding, we might confuse our model into thinking that a column has data with some kind of order or hierarchy, when we clearly don\u2019t have it. To avoid this, we \u2018OneHotEncode\u2019 that column. https:\/\/machinelearningmastery.com\/how-to-one-hot-encode-sequence-data-in-python\/\n\nIn order to one hot encode we must take in the label encode and use it as a parameter.","1ef70392":"# The goal for this project was to intensely clean as much of the two datasets given(test and train set), whilst fitting it with multiple models for analysis. Missing data that was cleaned,scaled and imputed. The explanations are done straight to the point with clear and explanatory text.\ud83d\udea2","2b8f1138":"# before cleaning, we shall do data exploration \ud83d\udea2\n\nrelationship with categorical variables","3121e571":"# major lessons learned:\ud83d\udea2\n\nThe overall score went down when removing columns, so it was better to impute them with the means of each column to get a higher score. If i did that then the score would skyrocket probably past my highest score ever of 78%.\nBut it was the right call to disable the rows that had major Nans like above 60% of their columns.\n\nFeature importance narrowed the columns \"PassengerId\",\"Pclass\",'Name',\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\" down to just [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Fare\"]. This was because the other columns were removed due to their smaller significance in each model. This was done through the random forest and logistic regression feature importance code.\n\nRegression analysis wise, there was a huge KNeighborsClassifier, random forest and XBG outperformed the rest. It was interesting as logistic regression was considered to be the top model for this data set.\n","dcabc796":"# ENCODE CATEGORICAL VARIABLES\ud83d\udea2\nsex, ticket and embarked and name.","3892d540":"# XGBOOST\ud83d\udea2","9deb9771":"# Linear SVC\ud83d\udea2","530ea995":"# Import libraries\ud83d\udea2","e69affcf":"# logistic regression(with gridsearch)\ud83d\udea2","58ca2472":"We know that the obvious names such as Mr,Miss,Mrs etc. can be their own categories.","1d24582b":"As we can see there is an uneven amount of Nans for each data set. ","af6c5d26":"The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.","1bc9d4b4":"# MODEL FITTING(with gridsearch) and finding useful variables using FEATURE IMPORTANCE\ud83d\udea2","7904d91c":"# random forest(with gridsearch)\ud83d\udea2","7db5eed4":"# Outlier detected as fare > 500 \ud83d\udea2\nboth test and train data have fare outliers","f58033e6":"# CLEAN DATA next \ud83d\udea2\nBoth train and test both have same columns (except survived as thats the predictor)","3f43e165":"ticket type : A\/5, PC, etc. Not every ticket has associated type\nticket number: starting from 0 to 3101317. \nFor most of the tickets first number is associated with Pclass, except tickets that are less than 5 digits long or tickets than have ticket type associated with them. There is not much additional information in ticket number, so it can be ommited\n\nWe are not too fussed about the ticket number so we can simply just get the ticket type such as a\/5 and PC","7ea13d0d":"# Lets check again for nan's\ud83d\udea2","d2c57dda":"# train_data = Age, cabin and embarked are important to fix the na's\ud83d\udea2\n# test_data = Age, cabin and Fare are important to fix the na's\ud83d\udea2","aa1dad45":"![lazy%20c.jpg](attachment:lazy%20c.jpg)","da3502e8":"# Second we focus on ticket\ud83d\udea2\nticket is an interesting one, as its an object with several nuances. Several categories, some with random conjunctions of letters and numbers\n","446ef3d3":"We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.","dfa86743":"# SCALE after split: next we use a scaler such as min max. Max-Min Normalization will be used as we will obtain smaller standard deviations.\ud83d\udea2","1a37b68b":"# Use msno to get a sense of what categories are missing from training and test set.NOTE: png's sometimes will be disabled because of competition submitting. \ud83d\udea2","c82c62f4":"![titanic.jpg](attachment:titanic.jpg)","fe5f843a":"# feature engineer the 'name' column(not implemented in final, but a good way to learn of anything important)\ud83d\udea2\n\nas its useful, also because we want to keep around the 10 variable mark.\n\nStatistically we want a 1 predictor to 10 ratio (1 to ten)\n\nWe will keep the name column - progress this is because if you can group the families, that can help determine if they survive or not.","950a3783":"We can see below that the outlier names are all gone now.","90e1cd15":"# random forest feature importance\ud83d\udea2\n\nhigher means better and more important\n\n\"PassengerId\",\"Pclass\",'Name',\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"\n\nthe weakest\/less important columns are Passengerid,Name,SibSp,Parch,Embarked. They were removed as a result.","93170d26":"# Extra step: Adding a lazy classifier to better understand other models\ud83d\udea2\n\nhttps:\/\/pypi.org\/project\/lazypredict\/","b69fb206":"# SVC\ud83d\udea2","53cb2f35":"this will make it easier if all the fares that are 512.3292 are replaced by 7.25 and removed. ","e6fbb79d":"check if the passengers 259, 680, 738\tand test data passenger 1235 are all nans in their fares and remove them","7c7c844a":"Furthermore we can map it like so to.. reduce the number of categories of names.\n\n# thanks to https:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9 for the extra help on the normalization of the name column.\ud83d\udea2","a1ac2422":"# Third we focus on embarked\ud83d\udea2\nSimply its all categorical objects.","1ae9e970":"# lastly is Name\ud83d\udea2","56ccd65b":"The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.","929010ea":"its true to say and confirms that there is one extreme outlier that needs to be fixed."}}