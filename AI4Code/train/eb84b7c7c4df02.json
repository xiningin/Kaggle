{"cell_type":{"5ba1ac15":"code","67ae4c15":"code","9ad836c0":"code","8f53d426":"code","bd09b9b2":"code","e1edadac":"code","b8915657":"code","4fd73cec":"code","34187005":"code","4231b21f":"code","c7d85640":"code","a58a98c2":"code","27ed2430":"code","d5095331":"code","60fce57b":"code","3ac8f839":"code","107b52d4":"code","38469b08":"code","9fe651a0":"markdown","c2a7e45a":"markdown","2d1a2fbb":"markdown","0442b115":"markdown","230d0e04":"markdown","e73179bd":"markdown","ee5c9472":"markdown","79816388":"markdown","7c0d7de9":"markdown","89c99331":"markdown","11130435":"markdown","a7e8ce3a":"markdown","d47b9648":"markdown","a378f24a":"markdown","6065b976":"markdown","928e65de":"markdown"},"source":{"5ba1ac15":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","67ae4c15":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","9ad836c0":"print (\"Num Cols: \", len(df.columns))\nprint (\"Num Rows: \",len(df.index))","8f53d426":"df.head()","bd09b9b2":"df.info(verbose=True, null_counts=True)","e1edadac":"df.describe()","b8915657":"ax = sns.heatmap(df.isnull(),yticklabels=False,cbar=False)\nax.set(xlabel='columns', ylabel='rows (white if null)')\nplt.show()","4fd73cec":"fig=plt.figure(figsize=(25, 5))\nunique = df.select_dtypes(include=['object','category']).nunique().sort_values()\nplt.bar(unique.index, unique)\nplt.xticks(rotation=90)\nplt.show()\nprint(\"min: \", unique.min())\nprint(\"max: \", unique.max())","34187005":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Define feature column categories by column type\ncategorical_cols = df.select_dtypes(include=['object','category']).columns.to_list()\nnumeric_cols = df.select_dtypes(include='number').columns.to_list()\n# Remove the target column (SalePrice) from our feature list\nnumeric_cols.remove('SalePrice')\n\nprint (\"Categorical columns: \", categorical_cols)\nprint (\"Numeric columns: \", numeric_cols)","4231b21f":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')","c7d85640":"# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","a58a98c2":"# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', numerical_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])","27ed2430":"from sklearn.model_selection import train_test_split\n\n# Grab target as y, remove target from X\ntrain_test = df.copy()\ny = train_test.SalePrice\nX = train_test.drop(columns=['SalePrice'])\n\n# Split into train, test\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state = 17)","d5095331":"train_X.head()","60fce57b":"train_y.head()","3ac8f839":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\n\n# Time to tune params!\ndef display_validation(pipeline):\n    # Preprocessing of training data, fit model \n    pipeline.fit(train_X,train_y)\n    # Preprocessing of validation data, get predictions\n    preds = pipeline.predict(val_X)\n\n    # Evaluate the model\n    score = mean_absolute_error(val_y, preds)\n    print('MAE:', score)","107b52d4":"from sklearn.ensemble import RandomForestRegressor\nimport random\n\nfor n in [50,100, 500]:\n    model = RandomForestRegressor(n_estimators=n, random_state = 17)\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n    print(\"n_estimators: \", n)\n    display_validation(pipeline)","38469b08":"# First, train again on that best n_estimators value\nfinal_model = RandomForestRegressor(n_estimators=100, random_state = 17)\nfinal_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', final_model)\n                             ])\n\n# Preprocessing of validation data, get predictions\nfinal_pipeline.fit(train_X,train_y)\ntest_data_labels = final_pipeline.predict(test)\n\n# Create predictions to be submitted!\npd.DataFrame({'Id': test.Id, 'SalePrice': test_data_labels}).to_csv('RFC_100.csv', index =False)  \nprint(\"Done :D\")","9fe651a0":"## Step 3: Clean the Data\n\nWhile the data from Kaggle competitions is already pretty clean, it still requires some manipulation before a model will accept the data as input. Specifically, we need to:\n\n  1. **Select the features** we want to use in the model. This may be columns in their original state or features created using additional logic and\/or combinations of column information.\n  2. Decide how to deal with null values in the dataset. This process is called **imputation**. \n  3. **Encode** data that is in a string or other categorical form into numerical features that can be read by a model. The most common way to encode non-numeric features is called **one-hot encoding**, which we'll use below.\n\n","c2a7e45a":"As a baseline simplistic model, we'll use all columns in the dataset as features. That means we'll want to split the columns into three groups:\n - **numeric columns**, which will be the easiest to process\n - **categorical columns**, mostly columns that contain strings like \"Neighbourhood\" or \"Building Style\". We'll process these columns slightly differently than the numeric columns.\n - **the target column**, that we're trying to predict. In this case, we're trying to predict the house price, so we'll keep that column separate from the others.\n\n From the df.info() results in step 2, we know that the categorical columns are all of type \"object\", a fact we can use to filter them from the numeric columns.","2d1a2fbb":"## Step 5: Train a Model\n\nNow that we have training and test sets that are well formatted, we can fit a model! There are two main choices to make here:\n - What **model type** are we going to use? (Linear regression, decision tree, neural net, etc.)\n - What **error metric** are we going to use? (Mean absolute error, mean squared error, etc.)\n\nOnce you pick a model type and a metric, you can tune the model's individual parameters to optimize for the metric output. While these choices are critical to model success, for now, we'll keep it simple: We'll train using a [random forest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html), and use [mean absolute error (MAE)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_absolute_error.html) as our success metric.","0442b115":"Since the MAE is lowest for n_estimators=100, we'll use that in our submission below. (Normally, you'd do a lot more tuning and validation than this -- this example is just a baseline to get started with!)","230d0e04":"As an example of what parameter tuning looks like, here we'll tune the number of estimators by calculating the MAE for each of three n_estimator values. Whichever has the smallest MAE value will likely give the best results!","e73179bd":"This heatmap show us where the null values in the dataset are located. Each column in the data is its own column in the heatmap. If the column is mostly black, it has very few null values, but if it's mostly white (like PoolQU or Alley) it contains mostly null values.\n\nFrom the heatmap and info() call above, we can see that while most of the columns in this dataset will be useful, we have a few that have so many null datapoints that the signal from those columns may be less useful, and we might consider dropping those columns in the data cleaning stage.\n\nSpending more time digging into the data can vastly improve feature selection. Given the time, one might look into creating additional features based off of these columns (for example, adding up all of the square foot features to get a total indoor square feet or adding up all of the bathroom columns to get a total bathroom count). For the sake of simplicity, we'll skip that step right now.","ee5c9472":"## Step 1: Import the Data","79816388":"While the numeric columns are almost in the correct format, there are still some NaN values we need to deal with. While there are many ways we could decide to impute (fill in) those missing values, we'll keep it simple here: we'll fill in missing values of a column with the average of all the current values in the column.\n\nFor example, if we had a column that before imputation looked like\n\n[1, 3, None, 1, 3]\n\nthe average of the non-missing columns is 2, so after imputation the column would look like\n\n[1, 3, 2, 1, 3].\n\nTo do the imputation for us, we'll use a [simple imputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html).","7c0d7de9":"Finally, we combine the numeric and categoric data encoding and imputing into one final preprocessor we can use on our data. Note how this column transformer uses the numeric transforms on our numeric columns and our categoric tranformns on our categoric columns.","89c99331":"The data below is from the [Kaggle House Prices Competition Dataset](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data). It's already split into train and test sets, so we can load both directly.","11130435":"## Step 6: Submit\/Store a Final Model\n\nTo submit predictions in Kaggle, you nead to create a csv file with the correct column names. Here, we do that by running the test data through our same training pipeline to process the data and predict the values, then save that result as a csv.","a7e8ce3a":"All done, congrats on getting to the end! ","d47b9648":"## Step 4: Create a Train Test Split\n\nWe're finally ready to get back to our data! While we've already been given both a train set (which contains final house sale prices that we can train on) and a test set (which doesn't contain final house sale prices, we need to predict those with our model!) we still need to further split our train set into a **training set** and a **validation set**.\n\nWe'll use the training set to train various models, and the validation set to test how well those models do, allowing us to tune parameters.\n\nLuckily, there's a great function called [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) that will split up the data into two groups for us. We'll put 80% of the data in the test set and 20% in the validation set.","a378f24a":"The categorical columns require a bit more work. First, we'll have to impute them as well, this time using the mode rather than the mean since we can't take the mean of a string. This takes the column\n\n['a', None, 'b', 'a']\n\nto\n\n['a', 'a', 'b', 'a'].\n\nNext, we'll need to transform those strings into numeric values using [one-hot encoding](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/). That takes a single column with options 'a' and 'b' and turns it into two columns: one that indicates (via 0\/1) if a row contains an 'a', and one that indicates if a row contains a 'b'. Thus\n\n'a' now becomes [1, 0] and\n\n'b' now becomes [0, 1].\n\nTo do these transformations, we'll use both a [simple imputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html) and a [one-hot encoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html). The [pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) is a way of keeping track of what transformations you do on the data in what order that makes those transformations easily repeatable on new data.","6065b976":"## What's in This Notebook\n\nThis is meant as a guide to basic model creation, from importing the data through submitting a competition file using a random forest. The goal here is to get a big picture sense of the ML coding\/thought process rather than specific deep dives into any particular topic.\n\nI've used clean coding practices with an emphasis on pipelines in this notebook, which hopefully makes the code extra easy to read!","928e65de":"## Step 2: Look at the Data\n\nIt's hard to do data analysis on a dataset if you don't know what the data looks like. Knowing how many columns there are per row and what kind of information is in those columns informs what kinds of features you might train on. Knowing how many rows (datapoints) helps chose what model to use - for example, a neural net is unlikely to work well on a dataset that only has 100 datapoints. \n\nWhile you could spend eons on this step alone, a good first start to looking at your dataset is:\n\n - [df.head()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.head.html) to see what the first few datapoints look like.\n - [df.describe()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html) to look at numeric data ranges (count, mean, percentiles, etc.).\n - [df.info(verbose=True, null_counts=True)](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html) to learn the type (float, int, etc.) and number of null values in each column.\n - [sns.heatmap(df.isnull(),yticklabels=False, cbar=False)](https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html) to get a visual representation of how null values are distributed within the dataset."}}