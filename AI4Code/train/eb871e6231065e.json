{"cell_type":{"91ac0ffb":"code","715d362d":"code","2e0471c4":"code","94a1140f":"code","3892cb8d":"code","10f37fbb":"code","95166fb4":"code","48b9e26e":"code","0d3b4abf":"code","44a7cb65":"markdown","1386964b":"markdown","b800f8a7":"markdown","6cc4222d":"markdown","b10c5ef8":"markdown","a9fe659f":"markdown","121d59e1":"markdown","9d1e3d00":"markdown"},"source":{"91ac0ffb":"!npm install jupnode\nimport jupnode","715d362d":"sh('npm install @tensorflow\/tfjs-node')\nsh('npm install sharp')\nsh('npm install csv')","2e0471c4":"var tf = require('@tensorflow\/tfjs-node');\nvar csv = require('csv\/lib\/sync')\nvar sharp = require('sharp\/lib\/index')\nlet data = csv.parse(fs.readFileSync('..\/input\/digit-recognizer\/train.csv')).slice(1);\n\nvar _dataset = data.map(d => {\n    let out = new Float32Array(10)\n    out[parseInt(d[0])] = 1;\n\n    let t1 = tf.tensor(out, [1, 10])\n    \n    let inp = new Float32Array(d.slice(1).map(v=>parseInt(v)))\n    let t2 = tf.tensor4d(inp,[1,28,28,1]).div(255)\n    return [t1,t2]\n  })\n\nlet test = csv.parse(fs.readFileSync('..\/input\/digit-recognizer\/test.csv')).slice(1);\nvar _datasetTest = test.map(d => {\n    let inp = new Float32Array(d.map(v => parseInt(v)))\n    return tf.tensor4d(inp, [1, 28, 28, 1]).div(255)\n})\n  \nimage(\n    Buffer.from(\n      await sharp(\n        Buffer.from(_dataset[0][1].mul(255).dataSync()),\n        { raw: { width: 28, height: 28, channels: 1 } }).png().toBuffer()\n    ).toString(\"base64\"))","94a1140f":"var model = tf.sequential();\nmodel.add(tf.layers.dropout({rate:0.1, inputShape: [28,28, 1]}));\nmodel.add(tf.layers.conv2d({ filters: 16, kernelSize: [7, 7], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true}));\nmodel.add(tf.layers.leakyReLU());\nmodel.add(tf.layers.conv2d({ filters: 20, kernelSize: [7, 7], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\nmodel.add(tf.layers.leakyReLU());\nmodel.add(tf.layers.conv2d({ filters: 24, kernelSize: [7, 7], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\nmodel.add(tf.layers.leakyReLU());\nmodel.add(tf.layers.conv2d({ filters: 3, kernelSize: [5, 5], kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\nmodel.add(tf.layers.leakyReLU());\nmodel.add(tf.layers.reshape({ targetShape: [6 * 6 * 3] }));\nmodel.add(tf.layers.dense({ units: 10, kernelInitializer: 'glorotUniform', activation: 'linear', padding: 'valid', useBias: true }));\nmodel.add(tf.layers.softmax());\n\nmodel.summary()","3892cb8d":"try{\nfs.readdirSync('.\/models').forEach(fileName => {\n  fs.rmdirSync('.\/models\/'+fileName, { recursive: true })\n})\n}catch(err){\n    console.log(err)\n}","10f37fbb":"var tf = require('@tensorflow\/tfjs-node');\n\nvar validK = 0.99\nvar trainFiles = _dataset.slice(0, Math.floor(_dataset.length * validK))\nvar validFiles = _dataset.slice(Math.ceil(_dataset.length * validK))\n\nvar getImages = (files, index, batchSize) => {\n \n    let input = [];\n    let output = [];\n    for (let i = 0; i !== batchSize; i++) {\n        input.push(files[index + i][1]);\n        output.push(files[index + i][0]);\n    }\n    return { input: tf.concat(input), output: tf.concat(output)}\n}\n \nvar getValidation = async (model) => {\n  let errorL = tf.scalar(0);\n  let hit = 0;\n    for (let i = 0; i !== validFiles.length; i++) {\n        errorL = errorL.add(tf.tidy(() => {\n        let { input, output } = getImages(validFiles, i, 1)\n        \n          let result = model.predict(input);\n             let data = result.dataSync();\n      let maxV = -Infinity;\n      let maxI = 0;\n      data.forEach((v,i) => {\n        if (v > maxV) {\n          maxV = v;\n          maxI = i;\n        }\n      })\n          if (output.dataSync()[maxI]||0===1) {\n            hit++;\n      }\n            return loss(result, output)\n        }))\n    }\n    return [errorL.div(validFiles.length).dataSync()[0],hit\/validFiles.length];\n}\n \nvar minimize = (input, output) => {\n        optimizer.minimize(() => {\n          let result = model.predict(input);\n            \n            return loss(result, output)\n        })\n}\nvar optimizer = tf.train.adam(3.0e-4);\nvar loss = tf.losses.meanSquaredError;\n \nlet epochs = 50;\nlet batchSize = 512;\n    \nfor (let e = 0; e !== epochs; e++) {\n     \n    let time = new Date().valueOf();\n    for (let i = 0; i !== Math.floor(trainFiles.length \/ batchSize) * batchSize; i += batchSize) {\n        tf.tidy(() => {\n            \/\/console.log(i);\n            let { input, output } = getImages(trainFiles, i, batchSize);\n            minimize(input, output);\n        })\n    }\n    console.log(e, Date.now()- time)\n    let shuffle = (array) => array.sort(() => Math.random() - 0.5);\n    trainFiles = shuffle(trainFiles)\n    let valError = await getValidation(model)\n    console.log('valError: ', valError);\n    try {\n    await fs.promises.mkdir('models\/');\n    }catch(err){}\n    model.save(`file:\/\/.\/models\/${e}_${valError[0]}`)\n}\n ","95166fb4":"\/\/get best model\nlet testModel = await tf.loadLayersModel('file:\/\/.\/models\/' + fs.readdirSync('.\/models').sort((l, r) => parseFloat(l.split('_')[1]) - parseFloat(r.split('_')[1]))[0] + '\/model.json');\nawait getValidation(testModel)\n","48b9e26e":"\/\/get best model\nlet testModel = await tf.loadLayersModel('file:\/\/.\/models\/' + fs.readdirSync('.\/models').sort((l, r) => parseFloat(l.split('_')[1]) - parseFloat(r.split('_')[1]))[0] + '\/model.json');\n\nlet out =[['ImageId','Label']]\n_datasetTest.forEach((v, ImageId) => {\n    tf.tidy(() => {\n        let data = testModel.predict(v).dataSync();\n        let maxV = -Infinity;\n        let maxI = 0;\n        data.forEach((v,i) => {\n            if (v > maxV) {\n              maxV = v;\n              maxI = i;\n            }\n        })\n        out.push([String(ImageId+1),String(maxI)])\n\n    })\n})\nfs.writeFileSync('out.csv',csv.stringify(out))","0d3b4abf":"sh(`kaggle competitions submit -c digit-recognizer -f out.csv -m \"tfjs\"`)","44a7cb65":"# prepare dataset","1386964b":"Generate csv file for competition","b800f8a7":"Test best model","6cc4222d":"# train","b10c5ef8":"model architecture","a9fe659f":"# Init node.js","121d59e1":"remove old models","9d1e3d00":"train and save models to \"models\" folder"}}