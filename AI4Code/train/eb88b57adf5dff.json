{"cell_type":{"920f1873":"code","71255bf6":"code","09bab859":"code","bca37c57":"code","062b9757":"code","8692cc93":"code","6cd690a4":"code","fc7f8673":"code","bc760c46":"code","496e1fbf":"code","c18d1d2a":"code","935344b2":"code","91d1eac1":"code","3e69adcb":"code","66b23d1d":"code","a60b8b40":"code","0b1bd61f":"code","34e410b9":"code","103237da":"code","66e91e9b":"code","95556777":"code","f6aec77f":"code","09b73d05":"code","0259faf6":"code","90c32127":"code","6eba487a":"code","8f27fcbb":"code","3a9b6fca":"code","f50a0d7f":"code","3ae05982":"code","4af3d38b":"code","32d086e7":"code","d999dd70":"code","49aee76a":"code","7648ba36":"code","01cabca8":"code","8852bb1b":"code","50cbd372":"code","9cd72eac":"code","59cc6bbf":"code","197e6bfa":"code","a745142b":"code","53c8ee16":"markdown","5ec73a2b":"markdown","db5dfbe7":"markdown","0ad2f343":"markdown","ceb9474a":"markdown","61a8f90b":"markdown","c7634c17":"markdown","eb63190a":"markdown","aebc3384":"markdown","475370dd":"markdown","28990669":"markdown","2668dc35":"markdown","b53561b9":"markdown","9720bce6":"markdown","64d2039e":"markdown","404f351c":"markdown","d50584ac":"markdown","2f061099":"markdown","ea3e6a53":"markdown","c1c91333":"markdown","9cf4baf2":"markdown","bccd787b":"markdown","2c79ac92":"markdown","528fcb91":"markdown","b4165e56":"markdown","9b10ddc4":"markdown","9f26e4fb":"markdown","2011e6f4":"markdown","8271660b":"markdown","6f503e2d":"markdown","002ebc4e":"markdown","a34c0c8b":"markdown","a03fcacc":"markdown","1065fe4e":"markdown","6d0efaed":"markdown","4727ee68":"markdown","3b87c475":"markdown","9c65eab8":"markdown"},"source":{"920f1873":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\npd.set_option('display.max_columns', None)\n\n# machine learning\n#Trees    \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\n\n#Ensemble Methods\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting  # explicitly require this experimental feature\nfrom sklearn.ensemble import HistGradientBoostingClassifier # now you can import normally from ensemble\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\n\n#Gaussian Processes\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n    \n#GLM\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn.linear_model import Perceptron   \n    \n#Nearest Neighbor\nfrom sklearn.neighbors import KNeighborsClassifier\n    \n#SVM\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import NuSVC\n\n#Discriminant Analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n #Navies Bayes\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\n\n# metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# PCA\nfrom sklearn import decomposition\n\nprint(\"Setup Complete\")","71255bf6":"# Specify the path of the CSV file to read\ntrain_df_final = pd.read_csv(\"..\/input\/pumpitup-challenge-dataset\/train_df_final.csv\")\nX_test_final = pd.read_csv(\"..\/input\/pumpitup-challenge-dataset\/X_test_final.csv\")","09bab859":"X_test_final.shape","bca37c57":"train_df_final.shape","062b9757":"X = train_df_final.drop(\"label\",axis=1)\ny = train_df_final[\"label\"]","8692cc93":"# Create training and test sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)","6cd690a4":"X.isnull().values.any()","fc7f8673":"sc = ss()\nX_train = sc.fit_transform(X_train)\nX_valid = sc.transform(X_valid)\nX_test = sc.transform(X_test_final)","bc760c46":"# Make an instance of the Model\npca = decomposition.PCA(.95)","496e1fbf":"pca.fit(X_train)","c18d1d2a":"pca.n_components_","935344b2":"X_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nX_valid_pca = pca.transform(X_valid)","91d1eac1":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_valid)\n\nacc_decision_tree = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_decision_tree","3e69adcb":"# Extra Tree\n\nextra_tree = DecisionTreeClassifier()\nextra_tree.fit(X_train, y_train)\ny_pred = extra_tree.predict(X_valid)\n\nacc_extra_tree = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_extra_tree","66b23d1d":"# Random Forest\n\nrfc = RandomForestClassifier(criterion='entropy', n_estimators = 1000,min_samples_split=8,random_state=42,verbose=5)\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_valid)\n\nacc_rfc = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_rfc","a60b8b40":"# GradientBoostingClassifier\n\nGB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.075, \n                                max_depth=13,max_features=0.5,\n                                min_samples_leaf=14, verbose=5)\n\nGB.fit(X_train, y_train)     \ny_pred = GB.predict(X_valid)\n\nacc_GB = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_GB","0b1bd61f":"# Histogram-based Gradient Boosting Classification Tree.\n\n#This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).\n\n\nHGB = HistGradientBoostingClassifier(learning_rate=0.075, loss='categorical_crossentropy', \n                                               max_depth=8, min_samples_leaf=15)\n\nHGB = HGB.fit(X_train_pca, y_train)\n\ny_pred = HGB.predict(X_valid_pca)\n\nacc_HGB = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_HGB","34e410b9":"# LightGBM \n\n#is another fast tree based gradient boosting algorithm, which supports GPU, and parallel learning.\n\n\nLGB = LGBMClassifier(objective='multiclass', learning_rate=0.75, num_iterations=100, \n                     num_leaves=50, random_state=123, max_depth=8)\n\nLGB.fit(X_train, y_train)\ny_pred = LGB.predict(X_valid)\n\nacc_LGB = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_LGB","103237da":"# AdaBoost classifier\n\nAB = AdaBoostClassifier(n_estimators=100, learning_rate=0.075)\nAB.fit(X_train, y_train)     \ny_pred = AB.predict(X_valid)\n\nacc_AB = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_AB","66e91e9b":"# BaggingClassifier\n\nBC = BaggingClassifier(n_estimators=100)\nBC.fit(X_train_pca, y_train)     \ny_pred = BC.predict(X_valid_pca)\n\nacc_BC = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_BC","95556777":"# XGBoost\n\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=5)\nxgb.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\ny_pred = xgb.predict(X_valid)\nacc_xgb = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_xgb","f6aec77f":"# ExtraTreesClassifier\n\nETC = ExtraTreesClassifier(n_estimators=100)\nETC.fit(X_train, y_train)     \ny_pred = ETC.predict(X_valid)\n\nacc_ETC = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_ETC","09b73d05":"# Logistic Regression for multilabel classification\n\n# https:\/\/acadgild.com\/blog\/logistic-regression-multiclass-classification\n# https:\/\/medium.com\/@jjw92abhi\/is-logistic-regression-a-good-multi-class-classifier-ad20fecf1309\n\nLG = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\nLG.fit(X_train, y_train)     \ny_pred = LG.predict(X_valid)\n\nacc_LG = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_LG","0259faf6":"coeff_df = pd.DataFrame(train_df_final.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(LG.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","90c32127":"# PassiveAggressiveClassifier\n\nPAC = PassiveAggressiveClassifier()\nPAC.fit(X_train, y_train)\ny_pred = PAC.predict(X_valid)\n\nacc_PAC = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_PAC","6eba487a":"# RidgeClassifierCV\n\nRC = RidgeClassifierCV()\nRC.fit(X_train, y_train)\ny_pred = RC.predict(X_valid)\n\nacc_RC = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_RC","8f27fcbb":"# Perceptron\n\nP = Perceptron()\nP.fit(X_train, y_train)\ny_pred = P.predict(X_valid)\n\nacc_P = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_P","3a9b6fca":"# Stochastic Gradient Descent\n# https:\/\/scikit-learn.org\/stable\/modules\/sgd.html\n\nSGD = SGDClassifier(shuffle=True,average=True)\nSGD.fit(X_train, y_train)\ny_pred = SGD.predict(X_valid)\n\nacc_SGD = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_SGD","f50a0d7f":"# KNN\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_valid)\n\nacc_knn = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_knn","3ae05982":"# Support Vector Classifier\n\nSVC = SVC(probability=True)\nSVC.fit(X_train, y_train)\ny_pred = SVC.predict(X_valid)\n\nacc_SVC = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_SVC","4af3d38b":"# Linear SVC\n\nlinear_SVC = LinearSVC()\nlinear_SVC.fit(X_train,y_train)\nlinear_SVC.predict(X_valid)\n\nacc_linear_SVC = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_linear_SVC","32d086e7":"# LinearDiscriminantAnalysis\n\nLDA = LinearDiscriminantAnalysis()\nLDA.fit(X_train,y_train)\nLDA.predict(X_valid)\n\nacc_LDA = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_LDA","d999dd70":"# QuadraticDiscriminantAnalysis\n\nQDA = QuadraticDiscriminantAnalysis()\nQDA.fit(X_train,y_train)\nQDA.predict(X_valid)\n\nacc_QDA = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_QDA","49aee76a":"# BernoulliNB\n\nbernoulliNB = BernoulliNB()\nbernoulliNB.fit(X_train,y_train)\nbernoulliNB.predict(X_valid)\n\nacc_bernoulliNB = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_bernoulliNB","7648ba36":"# GaussianNB\n\ngaussianNB = GaussianNB()\ngaussianNB.fit(X_train,y_train)\ngaussianNB.predict(X_valid)\n\nacc_gaussianNB = round(accuracy_score(y_valid,y_pred) * 100, 2)\nacc_gaussianNB","01cabca8":"models = pd.DataFrame({\n    'Model': ['LightGBM','Decision Tree',\"Extra Tree\",'Random Forest','Support Vector', 'KNN', 'Logistic Regression', \n              'Stochastic Gradient Decent', 'Linear SVC',\"XGBoost\", \"Ada Boost Classifier\", \n              \"Bagging Classifier\", \"Passive Agressive Cl\", \"Ridge\",\"Perceptron\",\n              'Gradient Boosting Classifier','Extra Trees',\n              \"LinearDA\",\"QuadraticDA\",\"BernoulliNB\",\"GaussianNB\"],\n    'Score': [acc_LGB,acc_decision_tree,acc_extra_tree,acc_rfc, acc_SVC, acc_knn, acc_LG,\n              acc_SGD, acc_linear_SVC, acc_xgb, acc_AB, \n              acc_BC, acc_PAC, acc_RC, acc_P,\n              acc_GB, acc_ETC,\n             acc_LDA, acc_QDA, acc_bernoulliNB, acc_gaussianNB]})\nsorted_by_score = models.sort_values(by='Score', ascending=False)","8852bb1b":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='Score', y = 'Model', data = sorted_by_score, color = 'g')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score on validation data (%)')\nplt.ylabel('Model')","50cbd372":"\"\"\"\nYou can combine your best predictors as a VotingClassifier, which can enhance the performance.\n\n\"\"\"\n\nestimators = [('RFC', rfc), ('LGB', LGB), ('GB', GB)]\n\nensemble = VotingClassifier(estimators, voting='soft')\n\nensemble.fit(X, y)","9cd72eac":"submission_df = pd.read_csv(\"..\/input\/pumpitup-challenge-dataset\/SubmissionFormat.csv\")","59cc6bbf":"X_test = sc.transform(X_test_final)\nsubmission_df['status_group']=rfc.predict(X_test)","197e6bfa":"vals_to_replace = {2:'functional', 1:'functional needs repair', 0:'non functional'}\n\nsubmission_df.status_group = submission_df.status_group.replace(vals_to_replace)","a745142b":"submission_df.to_csv(\"submission_TatianaSwrt_rfc_noretrain_80.csv\",sep=',', index=False)","53c8ee16":"Result with pca and 30 vars: 77.21\nResult without pca and with 30 vars: 78.95\nResult without pca and with 80 vars: 79.19","5ec73a2b":"This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.","db5dfbe7":"sc = ss()\nX = sc.fit_transform(X)\nX_test = sc.transform(X_test_final)\n\n# RandomForest\nrfc = RandomForestClassifier(criterion='entropy',min_samples_split=8, n_estimators=1000)\n\nrfc.fit(X, y)     ","0ad2f343":"We need to find the following best parameters for our Gradient Boosting model:\n- learning rate\n- max_depth\n- min_samples_leaf\n- max_featres\n- n_estimators\nThe full GridSearchCV takes very long (it ran for more than 12h and didn't yet finish, I interrupted manually), so we'll perform a randomized search instead.\n\nReference: https:\/\/zlatankr.github.io\/posts\/2017\/01\/23\/pump-it-up","ceb9474a":"## 3. Compare model results","61a8f90b":"# Tuning for LGB\nsc = ss()\nX = sc.fit_transform(X)\n\nLGB = LGBMClassifier(objective='multiclass', num_threads=2, verbose=2, random_state=123)\n\nparams = {'num_iterations ': [100, 150, 200],\n          'max_depth': [5, 8, 15],\n          'learning_rate': [0.01, 0.75, 0.1, 0.2],\n          'num_leaves' : [25, 40, 50]\n         }\n\ngrid_search = GridSearchCV(estimator=LGB, cv=4, param_grid=params, n_jobs=-1, verbose=5) # n_jobs=-1 = use all the CPU cores\n\ngrid_search.fit(X, y.values.ravel())\n\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","c7634c17":"0.7822558922558923\n{'learning_rate': 0.75, 'max_depth': 8, 'num_iterations ': 100, 'num_leaves': 50}","eb63190a":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the odds of the response (and thus increase the probability), and negative coefficients decrease the odds of the response (and thus decrease the probability).","aebc3384":"### 2.4 KNN","475370dd":"# GradientBoostingClassifier\n\nGB = GradientBoostingClassifier(n_estimators=150, learning_rate=0.05, max_depth=14,max_features=0.5,min_samples_leaf=14,verbose=5)\n\nGB.fit(X, y)     ","28990669":"## 8. Conclusion and possible future improvements\n\nThe goal of this project was to predict if a pump is functional, non-functional or needs repair based on some data describing the pump and its surroundings.\n\nIn my research I've first performed an exploratory data analysis. In the beginning I calculated a preliminary\/baseline accuracy score which means that a model predicting with the accuracy less than 54.31% is not adding any value, so it would not be better than an uneducated guess. I then splited the data into numerical and categorical columns, identified missing values to deal with in the preprocessing phase, searched for outliers in the data and \nassessed correlations among attributes.\n\nIn the next step I've performed data cleaning and preprocessing. First of all I dropped features containing similar information to avoid multicollinearity. Then I filled missing values, reduced cardinality of several categorical features that had many types of values to be able to encode them. I performed ordinal encoding for those variables where it made sense and one-hot encoding for the rest of variables. Finally, I used feature engineering to create new predictors (including LDA, binning, binary variables, turning a date-time variable into a continious numerical variable).\n\nAfter all the preprocessing I performed feature selection based on L1 regularization with logistic regression that yielded 80 most important variables out of 90 total columns.\n\nIn the final step I tested multiple models with standard parameters and plotted the results on a graph.\n\nThe top 3 models for this project are: \n- Random Forest\n- Gradient Boosting Classifier\n- Light Gradient Boosting Classifier\n\nHyperparameter tuning has only slightly improved the scores.\nSurprisingly retraining didn't improve the scores, this needs to be further investigated.\nPrinciple Component Analysis didn't improve the scores either as well as  a Voting classifier I created using all 3 top models listed above.\n\n**I achieved the maximum of 79.71% accuracy on the validation data and 79.6% accuracy on the test data submitted to the competition on DrivenData with the Random Forest model.**\n\nIdeas for future improvements:\n\n- Create a 'for' loop to automate the process of model selection\n- More feature ingeneering\n- Try to remove amount_tsh\n- 2 binary variables -> replace unknown with false\n- Log transform to reduce skew: population, amount_tsh\n- Don't do ordinal but other type of encoding\n- Deal with imbalanced classes (keras to balance classes?)\n- Xgboost -> feature importance (not Random Forest)\n- Try a different scaler\n- Remove outliers in the population variable\n- Fill missing values with median\/mean","2668dc35":"### 2.5 SVC","b53561b9":"## 7. Submission","9720bce6":"# Lightgbm\nLGB = LGBMClassifier(objective='multiclass', learning_rate=0.75, num_iterations=100, \n                     num_leaves=40, random_state=123,max_depth=15)\n\nLGB.fit(X, y)","64d2039e":"### 2.7 Naive Bayes","404f351c":"### 1.3 Principle component analysis (PCA)\np.s. It didn't improve the score, so I don't use it in the final model","d50584ac":"Result with pca and 30 vars: 77.49\nResult without pca and with 30 vars: 79.21\nResult with pca and with 80 vars: 77.53\nResult without pca and with 80 vars: 79.71","2f061099":"### 2.6 Discriminant Analysis","ea3e6a53":"# Tuning for RF\nsc = ss()\nX = sc.fit_transform(X)\n\nrfc = RandomForestClassifier(criterion='entropy', n_estimators = 50,random_state=42)\n\nparams = {\"min_samples_split\" : [4, 6, 8],\n             \"n_estimators\" : [500, 700, 1000]}\n\n\ngrid_search = GridSearchCV(estimator=rfc, cv=4, param_grid=params, n_jobs=-1, verbose=5) # n_jobs=-1 = use all the CPU cores\n\ngrid_search.fit(X, y.values.ravel())\n\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","c1c91333":"# randomized search min_samples_split\n# The result is much lower with min_split, so I don't use it.\n\nGB = GradientBoostingClassifier(n_estimators=100, \n                                learning_rate=0.075,\n                                max_depth=14,\n                                max_features=1.0,\n                                min_samples_leaf=14)\n\n\nparam_dist = {\"min_samples_split\":[0.1,0.5,0.3,0.7,1.0]}\n\nrs = RandomizedSearchCV(estimator=GB,\n                  param_distributions=param_dist,\n                  scoring='accuracy',\n                  cv=10, n_iter=10, n_jobs=-1)\n\nrs.fit(X, y)\n\nprint(rs.best_score_)\nprint(rs.best_params_)","9cf4baf2":"Result with pca and with 30 vars: 76.52\nResult without pca and with 30 vars: 77.88\nResult without pca and with 80 vars: 78.47","bccd787b":"Result with pca: 75.98\nResult without pca: 77.33","2c79ac92":"The top 3 models are: \n- Random Forest\n- Gradient Boosting Classifier\n- Light GB\n\nOut of them, the Gradient Boosting Classifier is the fastest one but Random Forest gives a little better score so far (79.71 compared to 79.24 of GB).\nWe are now going to find the best parameters for these 3 models using GridSearch.","528fcb91":"RandomizedSearchCV result:\n\nparam_dist \n- \"n_estimators\" : [50,100, 150],\n- \"learning_rate\":[0.05, 0.025, 0.075, 0.01],\n- \"max_depth\" : [12,13,14], \n- \"min_samples_leaf\":[14,15,16,17],\n- \"max_features\" : [0.5,0.3,0.7,1.0]\n\n0.7958922558922559\n{'n_estimators': 100, 'min_samples_leaf': 14, 'max_features': 0.5, 'max_depth': 13, 'learning_rate': 0.075}","b4165e56":"## 2. Model selection\n### Test different models with standard parameters on validation set \n\n\n**TO DO**: combine all models in a loop","9b10ddc4":"### 2.2 Ensembles","9f26e4fb":"### 2.1 Trees","2011e6f4":"# *Pump-it-up project*\n\n### Can you predict which water pumps are faulty?\n\n## Goal\nUsing data from Taarifa and the Tanzanian Ministry of Water, predict which pumps are functional, which need some repairs, and which don't work at all based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. \n\nA smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n\n# III. Model selection and submission\n## 1. Data preparation\n### 1.1 Libraries and input data","8271660b":"### 1.2 Train\/test splitting","6f503e2d":"### 2.3 Generalized Logistic Models","002ebc4e":"### 1.3 Standard Scaling\n\nThe idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n\nIn case of multivariate data, this is done feature-wise (in other words independently for each column of the data).\n\nGiven the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case).\n\nhttps:\/\/stackoverflow.com\/questions\/40758562\/can-anyone-explain-me-standardscaler","a34c0c8b":"0.7912626262626262\n{'min_samples_split': 8, 'n_estimators': 1000}","a03fcacc":"## 6. Voting classifier","1065fe4e":"## 5. Retrain the tuned model on the whole train set\n\nFor some reason retraining the model on the whole train set (train + validation) gives much worse results on the test set. The reason is not quite clear for me (overfitting?...) and it needs further research. For the time being I will omit this step. Instead I have adjusted above the parameters of the top 3 models based on the tuning.","6d0efaed":"Result with pca and 30 vars: 74.3\nResult without pca and with 30 vars: 75.45\nResult without pca and with 80 vars: 75.13","4727ee68":"# randomized search full\nGB = GradientBoostingClassifier(n_estimators=100, \n                                learning_rate=0.075,\n                                max_depth=14,\n                                max_features=1.0,\n                                min_samples_leaf=16)\n\n\nparam_dist = {\"n_estimators\" : [50,100, 150],\n              \"learning_rate\":[0.05, 0.025, 0.075, 0.01],\n             \"max_depth\" : [12,13,14], \n              \"min_samples_leaf\":[14,15,16,17],\n             \"max_features\" : [0.5,0.3,0.7,1.0]}\n\nrs = RandomizedSearchCV(estimator=GB,\n                  param_distributions=param_dist,\n                  scoring='accuracy',\n                  cv=10, n_iter=10, n_jobs=-1)\n\nrs.fit(X, y)\n\nprint(rs.best_score_)\nprint(rs.best_params_)","3b87c475":"## 4. Hyperparameter tuning\n\nWe will be using the Grid Search for hyperparameter tuning for 3 best models. ","9c65eab8":"0.7462794612794612\n{'min_samples_split': 0.1}"}}