{"cell_type":{"554d9bb1":"code","3f9a7cf0":"code","6cb4f90f":"code","e017b53e":"code","4924db72":"code","5ca26b47":"code","0178ec62":"code","9de45ac3":"code","215025d2":"code","6993f2ac":"code","a1f3eec5":"code","f0e43f71":"code","b61a8f2b":"code","935c3e32":"code","2a010ffe":"code","7f5ceafb":"code","aa042229":"code","eadcaca6":"code","c870a2f1":"code","a2a03a27":"code","6e6f412f":"code","ddbd2b10":"code","e601187e":"code","7327b2eb":"code","38a16b67":"code","4961f71f":"code","faeeb1c8":"code","c96ab2e7":"code","5f720fc7":"code","ccba7045":"code","906ca201":"code","1fbe2e61":"code","8a96f08d":"code","f26b92f3":"markdown","982633a0":"markdown","522da177":"markdown","b6705b82":"markdown","65114f92":"markdown","90e7c6b5":"markdown","c4bbdd2e":"markdown","e52a92f5":"markdown","ec621449":"markdown","aa662a75":"markdown","7d2d8406":"markdown","01913afc":"markdown","4d23ab90":"markdown","a3b3d45d":"markdown","15d2a06a":"markdown","f1d3b861":"markdown","23ae4644":"markdown","dc96f398":"markdown","fc76ce24":"markdown","f6705d54":"markdown","901043e2":"markdown","dc5ff702":"markdown"},"source":{"554d9bb1":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nimport os\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dropout\nfrom keras.models import model_from_json\nimport pickle","3f9a7cf0":"#Open csv & add header\ndata = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', engine = 'python', header = None)\ndata.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']","6cb4f90f":"set(data.target)","e017b53e":"data.target = (data.target).replace(4,1)","4924db72":"set(data.target)","5ca26b47":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.text, data.target, test_size=0.10, random_state=48)","0178ec62":"training_sentences = []\ntraining_labels = []\ntesting_sentences = []\ntesting_labels = []\nprint('prepearing train and test...')\n# Train\nfor s in X_train:\n  training_sentences.append(s)\nfor l in y_train:  \n  training_labels.append(l)\n#Test\nfor s in X_test:\n  testing_sentences.append(s)\nfor l in y_test:  \n  testing_labels.append(l)\ntraining_labels = np.array(training_labels)\ntesting_labels = np.array(testing_labels)","9de45ac3":"vocab_size = 30000\nembedding_dim = 200\nmax_length = 120\ntrunc_type = 'post'\noov_tok =  '<OOV>'","215025d2":"tokenizer = Tokenizer(num_words = vocab_size, oov_token= oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length)","6993f2ac":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    model = tf.keras.Sequential([\n                                 tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True),\n                                 tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n                                 tf.keras.layers.Dense(128, activation='relu'),\n                                 tf.keras.layers.Dropout((0.2)),\n                                 tf.keras.layers.Dense(64, activation='relu'),\n                                 tf.keras.layers.Dense(1, activation= 'sigmoid')\n    ])\n\n    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(\n        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n        name='Adam'),\n                  metrics=['accuracy'])","a1f3eec5":"num_epochs = 5\nhistory = model.fit(\n    padded,\n    training_labels,\n    epochs=num_epochs,\n    validation_data=(testing_padded, testing_labels),\n    batch_size = 16 * tpu_strategy.num_replicas_in_sync\n)","f0e43f71":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","b61a8f2b":"preds = model.predict(testing_padded)\nprint(classification_report(testing_labels, preds.round()))","935c3e32":"num_epochs = 2\nhistory = model.fit(\n    padded,\n    training_labels,\n    epochs=num_epochs,\n    validation_data=(testing_padded, testing_labels),\n    batch_size = 16 * tpu_strategy.num_replicas_in_sync\n)","2a010ffe":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","7f5ceafb":"preds = model.predict(testing_padded)\nprint(classification_report(testing_labels, preds.round()))","aa042229":"filepath = 'model.json'\nprint('saving model...')\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n\noutfile = open('tokenizer.pkl','wb')\npickle.dump(tokenizer,outfile)\noutfile.close()\nprint('Tokenizer saved!')\nprint('model saved!')","eadcaca6":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(testing_labels, preds, pos_label=None)\nthresholds","c870a2f1":"from sklearn.metrics import roc_curve\nfrom numpy import sqrt\nfrom numpy import argmax\nfpr, tpr, thresholds = roc_curve(testing_labels, preds)\ngmeans = sqrt(tpr * (1-fpr))\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))","a2a03a27":"from matplotlib import pyplot\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, label='Logistic')\npyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n# show the plot\npyplot.show()","6e6f412f":"NEUTRAL='neutral'\nNEGATIVE='negative'\nPOSITIVE='positive'\nSENTIMENT_THRESHOLD = thresholds[ix]\ntrunc_type = 'post'","ddbd2b10":"import time\ndef decode_sentiment(score):\n    return NEGATIVE if score <= SENTIMENT_THRESHOLD else POSITIVE","e601187e":"def predict(text):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(\n        tokenizer.texts_to_sequences([text]),\n        maxlen=max_length,\n        truncating=trunc_type)\n    # Predict\n    score = model.predict([x_test])[0]\n    # Decode sentiment\n    label = decode_sentiment(score)\n\n    return {\"label\": label, \"score\": float(score),\n       \"elapsed_time\": time.time()-start_at} ","7327b2eb":"res = predict(list(\"I suck at Mortal Kombat\"))\n\nprint(res)","38a16b67":"res = predict(list(\"Your code is without doubt the worst code I've ever seen\"))\n\nprint(res)","4961f71f":"res = predict(list(\"I love this song!\"))\nprint(res)","faeeb1c8":"res = predict(list(\"I love cats\"))\nprint(res)","c96ab2e7":"res = predict(list(\"I feel lucky today\"))\nprint(res)","5f720fc7":"res = predict(list(\"I hate storms\"))\nprint(res)","ccba7045":"res = predict(list(\"this is a terrible moment\"))\nprint(res)","906ca201":"res = predict(list(\"I've failed my German exam\"))\nprint(res)","1fbe2e61":"res = predict(list(\"The code looks clean\"))\nprint(res)","8a96f08d":"res = predict(list(\"you won't get that degree\"))\nprint(res)","f26b92f3":"# Plotting","982633a0":"# Now, we can test our model. For instance, I will try to recognize the sentiment of 10 sentences","522da177":"## As we can see, there is a noticeable difference between both results. With 5 epochs, the model achieved an accuracy score of %82.58, while the 2 epoch model managed to get %83.68. Almost 1% more with less epochs, thus less training time.","b6705b82":"# Taking in count the analysis results, now I restart the model and train the model with only 2 epochs","65114f92":"# Define model","90e7c6b5":"# #Replace 4's by 1's in order to use a Sigmoid","c4bbdd2e":"# Possible improvements:\n* Playing with the vocabulary size or the embedding dimension could lead to better (or worst) results\n* Combining the use of threshold and the G-mean in order to add the possibility of neutral results\n* Changing the loss function (see Huber loss) or tuning optimizer values\n* The use of Transfer Learning techniques could improve the accuracy\n","e52a92f5":"# plot of the roc curve for the model","ec621449":"# Testing results with naive bayes","aa662a75":"# Save the 2 epoch model","7d2d8406":"### Now, we shall define the threshold value for our test","01913afc":"# Plot the 2 epochs training","4d23ab90":"# Open csv & add header","a3b3d45d":"## The model managed to get 7 out 10 sentences correctly. As we can see, it is not a perfect model, but it's still good. If we look at the 10 predictions, it is clear that the model is mostly failing at predicting negative sentences.","15d2a06a":"## This code is a modification of the version you can find at [Paolo Ripamonti's notebook](https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis)","f1d3b861":"# Prepare sentences","23ae4644":"# Tokenize & process data","dc96f398":"# It looks like there are only 0 and 4 values, no neutral values","fc76ce24":"# Analysis after training:\n### Looking at the results, one could notice that this model achieves it's best result at the second epoch. From the third epoch on wards, the model starts to overfit the training data, so it would be useless to train the model with more than 2 epochs.","f6705d54":"# Fit model with 5 epochs, in order to see the plot's evolution","901043e2":"# Split data into train & test","dc5ff702":"# Predict & show report"}}