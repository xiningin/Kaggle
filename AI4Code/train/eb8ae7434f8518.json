{"cell_type":{"64181b71":"code","5d7e9314":"code","28d310ad":"code","e59e6910":"markdown","daf56952":"markdown"},"source":{"64181b71":"from transformers import BertTokenizer, TFBertForMaskedLM\nimport numpy as np\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = TFBertForMaskedLM.from_pretrained('bert-base-uncased') ","5d7e9314":"# text = input('Write your text here: ') # Just start and type your phrase # Uncomment this line\ntext = 'Some text' # Comment this line!!! It only for committing notebook","28d310ad":"num_of_words = np.random.randint(3,8) # How many new words generate\n\nall_text = [text]\nsentence = ('[CLS] {} [MASK] . [SEP]'.format(text))\nfor i in range(num_of_words):\n    \n    indices = tokenizer.encode(sentence, add_special_tokens=False, return_tensors='tf')\n    prediction = bert_model(indices)\n    masked_indices = np.where(indices==103)[1]\n\n    output = np.argmax( np.asarray(prediction[0][0])[masked_indices,:] ,axis=1)\n    new_word = tokenizer.decode(output)\n    all_text.append(' ' + new_word)\n    new_text = ''.join(all_text)\n    sentence = ('[CLS] {} [MASK] . [SEP]'.format(new_text))\n\nprint(''.join(all_text))","e59e6910":"# Hello!\nHere is a BERT toy to play with text. It uses a ready-made model and it can not be considered something serious. Nevertheless, sometimes interesting phrases are obtained.\nIf hands reach, I will try to make a generator of texts with Markov Chains or improve this model. In the meantime, you can play around with the text below. Have a good time! ","daf56952":"## Some generated phrases: \"input -> generated\"\n* ***My python skill is*** -> excellent too good too bad training skills\n* ***Kaggle is a very*** -> popular game show show host\n* ***This quarantine will*** -> work perfectly well now\n* ***I like big*** -> things too much anyway\n* ***Now say my name*** -> again please please please\n* ***ps4 is much*** -> faster however slower\n* ***Toss a coin to your*** -> left hand side up\n* ***Please upvote*** -> it again please do\n> But I think once will be enough ;)"}}