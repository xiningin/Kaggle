{"cell_type":{"911349f3":"code","8c5f3522":"code","11e8c1d5":"code","1cea6ba7":"code","b641e7c6":"code","5287a434":"code","23ca7b6f":"code","db2fc813":"code","27d76466":"code","43f6443d":"code","c4b01617":"code","a4f0f6d0":"code","9f82c320":"code","2620a2d0":"code","9cfdb7ab":"code","84ecbfd1":"code","f13a500b":"code","bdc01671":"code","3fd226ee":"code","5cc02fb0":"code","01295901":"code","02d03e1e":"code","094dbed3":"code","193e665f":"code","8f6b90cd":"code","60ba632b":"code","add201bb":"code","5ce05b29":"code","93c27e74":"code","88b3e8f4":"code","8b5c46c6":"code","0c6e92a5":"code","f6836444":"code","882fb255":"code","fc77f4bd":"code","75c41d52":"code","c4f68abb":"code","bcb0e609":"code","6b8309d0":"code","cd8a446e":"code","3e2d7dc4":"code","66d8eff5":"code","3de53f0c":"markdown","81ee319a":"markdown","868ab47f":"markdown","fe20bab2":"markdown","9c669241":"markdown","401debf0":"markdown","dbdc5a1c":"markdown","3b1e599d":"markdown","70dc32a6":"markdown","c61365c0":"markdown","45d41ffd":"markdown","4a3cf834":"markdown","1b7a67e4":"markdown","82db28c4":"markdown","d1ee92bf":"markdown","d13ee960":"markdown","02837bcb":"markdown","3e5a1ef2":"markdown","3dd11234":"markdown","961112eb":"markdown"},"source":{"911349f3":"# Count the time span of the notebook\nimport datetime\nstart_time = datetime.datetime.now()","8c5f3522":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfrom IPython.display import display\nfrom pandas import set_option\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow import keras\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.layers import Input,Dense,Dropout\nfrom tensorflow.keras import Model\nfrom  tensorflow.keras.regularizers import l2\n\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport tensorflow as tf\n\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n# Show all the columns\npd.set_option('display.max_columns', None) ","11e8c1d5":"# Set seed to make the codebase reproducible\nSEED = 42\nnp.random.seed(SEED)\nbatch_size = 128\nepochs = 100\nK = 15\nnum_folds = K","1cea6ba7":"# Consider first column as index\ntrain = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv', index_col=0)\ntrain.head()","b641e7c6":"print(\"This dataset has {:.1f}\".format(100*train.isna().to_numpy().sum()\/(train.shape[0]*train.shape[1])) + \"% missing values\")","5287a434":"# Row and Column size of the training dataset\ntrain.shape","23ca7b6f":"# Show the number of missing values\nmissing = train.isnull().sum()\nprint(missing)","db2fc813":"def remove_outliers_using_quantiles(qu_dataset, qu_field, qu_fence):\n    a = qu_dataset[qu_field].describe()\n    \n    iqr = a[\"75%\"] - a[\"25%\"]\n    print(\"interquartile range:\", iqr)\n    \n    upper_inner_fence = a[\"75%\"] + 1.5 * iqr\n    lower_inner_fence = a[\"25%\"] - 1.5 * iqr\n    print(\"upper_inner_fence:\", upper_inner_fence)\n    print(\"lower_inner_fence:\", lower_inner_fence)\n    \n    upper_outer_fence = a[\"75%\"] + 3 * iqr\n    lower_outer_fence = a[\"25%\"] - 3 * iqr\n    print(\"upper_outer_fence:\", upper_outer_fence)\n    print(\"lower_outer_fence:\", lower_outer_fence)\n    \n    count_over_upper = len(qu_dataset[qu_dataset[qu_field]>upper_inner_fence])\n    count_under_lower = len(qu_dataset[qu_dataset[qu_field]<lower_inner_fence])\n    percentage = 100 * (count_under_lower + count_over_upper) \/ a[\"count\"]\n    print(\"percentage of records out of inner fences: %.2f\"% (percentage))\n    \n    count_over_upper = len(qu_dataset[qu_dataset[qu_field]>upper_outer_fence])\n    count_under_lower = len(qu_dataset[qu_dataset[qu_field]<lower_outer_fence])\n    percentage = 100 * (count_under_lower + count_over_upper) \/ a[\"count\"]\n    print(\"percentage of records out of outer fences: %.2f\"% (percentage))\n    \n    if qu_fence == \"inner\":\n        output_dataset = qu_dataset[qu_dataset[qu_field]<=upper_inner_fence]\n        output_dataset = output_dataset[output_dataset[qu_field]>=lower_inner_fence]\n    elif qu_fence == \"outer\":\n        output_dataset = qu_dataset[qu_dataset[qu_field]<=upper_outer_fence]\n        output_dataset = output_dataset[output_dataset[qu_field]>=lower_outer_fence]\n    else:\n        output_dataset = qu_dataset\n    \n    print(\"length of input dataframe:\", len(qu_dataset))\n    print(\"length of new dataframe after outlier removal:\", len(output_dataset))\n    \n    return output_dataset\n\n# Drop the outliers rows\ntrain.dropna(inplace=True)\nnew_dataset = remove_outliers_using_quantiles(train, \"target\", \"inner\")\n\n# the dataset is actually already cleaned, hence no outliers","27d76466":"# Show is there any imbalance in the target value, actually there's a huge imbalance,\n# so stratification is needed\ntrain['target'].value_counts()","43f6443d":"# Create separate variables for both categorical and continous data\ncategorical_cols = ['cat'+str(i) for i in range(19)]\ncontinous_cols = ['cont'+str(i) for i in range(11)]","c4b01617":"cols=categorical_cols+continous_cols\ntrain_objs_num = len(train)\n\n# Just combining both train and test sets columns so that encoding categorical data becomes easy \ndataset = pd.concat(objs=[train[cols], test[cols]], axis=0)\n\n# Encode only the categories, though LGBM doesn't need that conversion\n# As I'll compare with other models, I'm converting them anyway\ndataset_preprocessed = pd.get_dummies(dataset,columns=categorical_cols)\n\n# Separate train and test set again\ntrain_preprocessed = dataset_preprocessed[:train_objs_num]\ntest_preprocessed = dataset_preprocessed[train_objs_num:]\n\ntrain_preprocessed.head()","a4f0f6d0":"def get_DAE():\n    # The dataset contains 11 independent variables\n    inputs = Input((11,))\n    x = Dense(1500, activation='relu')(inputs) # 1500 original\n    x = Dense(1500, activation='relu', name=\"feature\")(x) # 1500 original\n    x = Dense(1500, activation='relu')(x) # 1500 original\n    outputs = Dense(11, activation='relu')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='mse')\n\n    return model","9f82c320":"# Here we don't need any target value, as DAE slightly corrupts the input data but still, \n# maintain the uncorrupted data as our target output.\nalldata = pd.concat([train[continous_cols],test[continous_cols]],axis=0)\nprint(alldata.shape)\nautoencoder = get_DAE()\nautoencoder.fit(alldata[continous_cols], alldata[continous_cols],\n                    epochs=20,\n                    batch_size=256,\n                    shuffle=True\n                    )","2620a2d0":"# Create corrupted data for the corresponding input\ntest_denoised = test_preprocessed.copy()\ntest_denoised[continous_cols] = autoencoder.predict(test_denoised[continous_cols])\ntrain_denoised = train_preprocessed.copy()\ntrain_denoised[continous_cols] = autoencoder.predict(train_denoised[continous_cols])","9cfdb7ab":"train_denoised['target'] = train.target","84ecbfd1":"X = train_denoised.drop(['target'], axis=1)\nY = train_denoised.target\nX_TEST = test_denoised","f13a500b":"train.head()","bdc01671":"X.head()","3fd226ee":"# As the labels are not equal, stratification is employed\nX_train, X_test, y_train, y_test =train_test_split(X,train['target'],\n                                                   test_size=0.20,\n                                                   random_state=42,\n                                                   stratify=train['target'])","5cc02fb0":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n# Early stopping should be based on loss, not on accuracy\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=0, save_best_only=True)","01295901":"model = keras.Sequential([\n    ## reshaping the input entries\n    # there're 30 independent variables \n    keras.layers.Dense(50, input_shape=(X.shape[1],), activation='relu'),  \n    keras.layers.Dropout(0.50),    ## to avoid overfitting and underfiting\n\n    ## creating the hidden layer\n    keras.layers.Dense(100,activation='relu'),\n    keras.layers.Dropout(0.70),    ##  to avoid overfitting and underfiting\n    \n    keras.layers.Dense(150,activation='relu'),\n    keras.layers.Dropout(0.70),     ## to avoid overfitting and underfiting\n \n    # sigmoid as this is a binary classification problem\n    ## final neural layer\n    keras.layers.Dense(1,activation='sigmoid')\n    \n])\n\n\n# as an optimizer, adams give promising performance\nmodel.compile(optimizer='adam',\n             loss='binary_crossentropy',  ## since output in 0 or 1\n             metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])\n\nmodel.fit(X,Y,epochs=epochs, \n          batch_size=batch_size,\n          callbacks=[annealer,es,mc],\n          validation_split=0.1,\n          shuffle=True,\n         )\n\ny_pred_cnn = model.predict(X_TEST)\n\ncnn_score = model.evaluate(X, Y)[1]\ncnn_score","02d03e1e":"# Spot-Check Algorithms\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('XGB'   , XGBClassifier(verbosity = 0)))\n    basedModels.append(('LGB'   , LGBMClassifier()))\n#     basedModels.append(('LR'   , LogisticRegression()))\n#     basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n#     basedModels.append(('KNN'  , KNeighborsClassifier()))\n#     basedModels.append(('CART' , DecisionTreeClassifier()))\n#     basedModels.append(('SVM'  , SVC(probability=True)))\n#     basedModels.append(('AB'   , AdaBoostClassifier()))\n#     basedModels.append(('GBM'  , GradientBoostingClassifier()))\n#     basedModels.append(('RF'   , RandomForestClassifier()))\n#     basedModels.append(('ET'   , ExtraTreesClassifier()))\n#     basedModels.append(('CBC'   , CatBoostClassifier()))\n\n\n    return basedModels","094dbed3":"def BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    scoring = 'roc_auc'\n\n    results = []\n    names = []\n    for name, model in models:\n        # stratification is needed for imbalanced target of dataframe\n        kfold = StratifiedKFold(n_splits=num_folds)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results","193e665f":"models = GetBasedModel()\n\n# Analyze all the base models\nnames,results = BasedLine2(X_train, y_train,models)","8f6b90cd":"# Compare all the models according to the baseline score\n\ndef ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame","60ba632b":"basedLineScore = ScoreDataFrame(names,results)\nbasedLineScore","add201bb":"# Standard and MinMax scalers are considered\n# Let's see which one performs better\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'XGB'  , Pipeline([('Scaler', scaler),('XGB'  , XGBClassifier())])))\n    pipelines.append((nameOfScaler+'LGB'  , Pipeline([('Scaler', scaler),('LGB'  , LGBMClassifier())])))\n#     pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n#     pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n#     pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n#     pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n#     pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n#     pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n#     pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n#     pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n#     pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n\n\n    return pipelines ","5ce05b29":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, y_train,models)\nscaledScoreStandard = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard], axis=1)\ncompareModels","93c27e74":"names,results = BasedLine2(X_train, y_train,models)\nscaledScoreMinMax = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax], axis=1)\ncompareModels","88b3e8f4":"# Functions for KFold evaluation\ndef create(hyperparams):\n    \"\"\"Create LGBM Classifier for a given set of hyper-parameters.\"\"\"\n    model = LGBMClassifier(**hyperparams)\n    return model\n\ndef fit(model, X, y):\n    \"\"\"Simple training of a given model.\"\"\"\n    model.fit(X, y)\n    return model\n\ndef fit_with_stop(model, X, y, X_val, y_val, esr):\n    \"\"\"Advanced training with early stopping.\"\"\"\n    model.fit(X, y,\n              eval_set=(X_val, y_val),\n              early_stopping_rounds=esr, \n              verbose=200)\n    return model\n\ndef evaluate(model, X, y):\n    \"\"\"Compute AUC for a given model.\"\"\"\n    yp = model.predict_proba(X)[:, 1]\n    auc_score = roc_auc_score(y, yp)\n    return auc_score\n\ndef kfold_evaluation(X, y, k, hyperparams, esr=100):\n    \"\"\"Run a KFlod evaluation.\"\"\"\n    scores = []\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = StratifiedKFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X,y)):\n        print(f\"\\n----- FOLD {i} -----\")\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val, esr)\n        train_score = evaluate(model, X_train, y_train)\n        val_score = evaluate(model, X_val, y_val)\n        scores.append((train_score, val_score))\n        \n        # Using AUC, as per competition's rules\n        print(f\"Fold {i} | Eval AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns=['train score', 'validation score'])\n    \n    return scores\n\n\ndef kfold_prediction(X, y, X_test, k, hyperparams, esr=100):\n    \"\"\"Make predictions with a bagged model based on KFold.\"\"\"\n    yp = np.zeros(len(X_test))\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = KFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n----- FOLD {i} -----\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val, esr)\n        yp += model.predict_proba(X_test)[:, 1] \/ k\n       \n    \n    return yp","8b5c46c6":"# Set default parameters\nBEST_PARAMS = {\n    'n_estimators': 10000, # Number of boosted trees to fit\n    'learning_rate': 0.05, # Me\n    'metric': 'auc', # Me\n    'device_type': 'gpu'\n}","0c6e92a5":"# Objective function\n# Scrutinize the best hyperparameter using this function\ndef objective(trial):\n    # Search spaces\n    hyperparams = {\n    'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n    'max_depth': trial.suggest_int('max_depth', 6, 127),\n    'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n    'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n    'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n    'random_state': 2021,\n    'metric': 'auc',\n    'n_estimators': 20000,\n    'n_jobs': -1,\n    'bagging_seed': 2021,\n    'feature_fraction_seed': 2021,\n    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n    'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n    'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n    'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n    'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n    'cat_smooth': trial.suggest_int('cat_smooth', 10, 100),\n    'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n    }\n    \n    # Add BEST_PARAMS\n    hyperparams.update(BEST_PARAMS)\n    \n    # Evaluation\n    scores = kfold_evaluation(X, Y, K, hyperparams, 100)\n    \n    return scores['validation score'].mean()","f6836444":"# Optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)\n# study.optimize(objective, timeout=3600*7)","882fb255":"# Best score\nstudy.best_value","fc77f4bd":"# Historic\nplot_optimization_history(study)","75c41d52":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","c4f68abb":"plot_param_importances(study)","bcb0e609":"# plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search\n# went and which parts of the space were explored more.\noptuna.visualization.plot_slice(study)","6b8309d0":"# Best parameters\nBEST_PARAMS.update(study.best_params)\nBEST_PARAMS","cd8a446e":"# Update hyperparams for prediction\nBEST_PARAMS['learning_rate'] = 0.03238848685934311","3e2d7dc4":"# Finally predictions on test set and submission using all the hypertuned parameters and relevant dataframe\ntest['target'] = kfold_prediction(X, Y, X_TEST, K, BEST_PARAMS, 500)\ntest['target'].to_csv('submission.csv')","66d8eff5":"end_time = datetime.datetime.now()\nprint(end_time - start_time)","3de53f0c":"### Set Constants","81ee319a":"### Deep Neural Network","868ab47f":"#  <center> Workflow <\/center>","fe20bab2":"# 4. Scaling the Dataframe","9c669241":"### Standard Scaler","401debf0":"# Fetch Data","dbdc5a1c":"# 1. Import Libraries","3b1e599d":"### dropna()","70dc32a6":"# 2. Feature Engineering","c61365c0":"## Functions for Kfold-Cross validaiton, training, evaluation and prediction","45d41ffd":"# Denoising AutoEncoder (DAE)","4a3cf834":"![img](https:\/\/i.imgur.com\/1IsBI7H.png)","1b7a67e4":"# Try different Machine Learning Base Models","82db28c4":"# 6. Inference","d1ee92bf":"# MinMax Scaler","d13ee960":"# 5. Hyperparameter tuning (OPTUNA)","02837bcb":"# 3. Models Implementation","3e5a1ef2":"# Remove Outliers","3dd11234":"# Train Test Split","961112eb":"N.B. \n* You need to run this notebook after turning on the GPU\n* The database is cleaned and there's no missing values, that's why there's no scope for imputation\n* Scaling didn't improve the score, that's why this step is eliminated\n* LGBM performs the best out of all other algorithms, I removed low scored models as they were killing times\n* As the dataset is imbalanced, I've used stratification "}}