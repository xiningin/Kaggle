{"cell_type":{"04c07e58":"code","df63fccf":"code","227fca90":"code","287313d6":"code","8f431e70":"code","9f4671b0":"code","bb13b8a6":"code","501ac809":"code","40ce8a64":"code","c1943b0d":"code","c3ac340b":"code","f2efb570":"code","db21dd71":"code","607200d1":"code","c549f289":"code","a85c6b7d":"code","5099eb14":"code","e1850831":"code","bc32c9ea":"code","34fb25b9":"code","e67d9237":"code","7be4722c":"code","d1000d70":"code","7ad60196":"code","e88a479e":"code","41796dd4":"code","959f3b2c":"code","d8dfbf8e":"code","e216d873":"code","d90dfd7c":"code","8804b5c3":"code","b6274932":"code","3e24ecde":"code","930f375b":"code","1e7dc17b":"code","72b22404":"markdown","ca183e77":"markdown","f85cfbc0":"markdown","931ce05e":"markdown","91a2b71a":"markdown","c76e9238":"markdown"},"source":{"04c07e58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","df63fccf":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\ntrain.shape, test.shape","227fca90":"train.head()","287313d6":"test.head()","8f431e70":"# Checking null values in training dataset\ntrain.isnull().sum().sum()","9f4671b0":"# Checking null values in test dataset\ntest.isnull().sum().sum()","bb13b8a6":"train.describe()","501ac809":"test.describe()","40ce8a64":"# Count the target\nsns.countplot(train['target'])","c1943b0d":"# Check how many positive and negative number are there in training data\ntrain_pos_count=[]\ntrain_neg_count=[]\nfor i in range(200):\n    pos=0\n    neg=0\n    for j in train[str(i)]:\n        if j>0:\n            pos+=1\n        else:\n            neg+=1\n    train_pos_count.append(pos)\n    train_neg_count.append(neg)","c3ac340b":"# Plotting the distplot of positive train count\nmyarray = np.asarray(train_pos_count)\nsns.distplot(myarray)","f2efb570":"# Plotting the distplot of negative test count\nmyarray = np.asarray(train_neg_count)\nsns.distplot(myarray)","db21dd71":"# Check how many positive and negative number are there in test data\n\ntest_pos_count=[]\ntest_neg_count=[]\nfor i in range(200):\n    pos=0\n    neg=0\n    for j in test[str(i)]:\n        if j>0:\n            pos+=1\n        else:\n            neg+=1\n    test_pos_count.append(pos)\n    test_neg_count.append(neg)","607200d1":"# Plotting the distplot of positive test count\nmyarray = np.asarray(test_pos_count)\nsns.distplot(myarray)","c549f289":"# Plotting the distplot of negative test count\nmyarray = np.asarray(test_neg_count)\nsns.distplot(myarray)","a85c6b7d":"# Preparing the data\nX=train.drop(['id','target'],axis=1)\ny=train['target']","5099eb14":"# KMeans Clustering \nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\ntrain_labels=kmeans.labels_","e1850831":"# Labels Count in cluster of training data \none=0\nzero=0\nfor i in train_labels:\n    if i==1:\n        one+=1\n    else:\n        zero+=1\nprint(\"1: \",one)\nprint(\"0: \",zero)","bc32c9ea":"y_train.value_counts()","34fb25b9":"X_test=test.drop(['id'],axis=1)","e67d9237":"kmeans_test = KMeans(n_clusters=2, random_state=0).fit(X_test)\ntest_labels=kmeans_test.labels_","7be4722c":"# Labels Count in cluster of test data\ntest_one=0\ntest_zero=0\nfor i in test_labels:\n    if i==1:\n        test_one+=1\n    else:\n        test_zero+=1\nprint(\"1: \",test_one)\nprint(\"0: \",test_zero)","d1000d70":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\ntrain_pca_result = pca.fit_transform(X)\ntest_pca_result = pca.fit_transform(X_test)","7ad60196":"%matplotlib notebook\nfig = plt.figure()\n#plt.figure(figsize=(20,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(train_pca_result[:,0], train_pca_result[:,1], train_pca_result[:,2], marker='o')\nplt.show()","e88a479e":"%matplotlib notebook\nfig = plt.figure()\n# plt.figure(figsize=(20,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(test_pca_result[:,0], test_pca_result[:,1], test_pca_result[:,2], marker='o')\nplt.show()","41796dd4":"# Splitting the dataset in training and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)","959f3b2c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_ = scaler.fit_transform(X_train)\nval_= scaler.fit_transform(X_val)\ntest_ = scaler.transform(X_test)","d8dfbf8e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nModel_rf=RandomForestClassifier(max_depth=2)\nModel_rf.fit(train_,y_train)\ny_pred=Model_rf.predict(val_)\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","e216d873":"from sklearn.ensemble import AdaBoostClassifier\nModel_ada=AdaBoostClassifier()\nModel_ada.fit(train_,y_train)\ny_pred=Model_ada.predict(val_)\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","d90dfd7c":"from sklearn.ensemble import GradientBoostingClassifier\nModel_gb=GradientBoostingClassifier()\nModel_gb.fit(train_,y_train)\ny_pred=Model_gb.predict(val_)\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_val))","8804b5c3":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nbest_score = 0\nfor penalty in ['l1', 'l2']:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:       \n        logreg = LogisticRegression(class_weight='balanced',  penalty=penalty, C=C, solver='liblinear')\n        logreg.fit(train_, y_train)\n        score = logreg.score(val_, y_val)\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'penalty': penalty}       ","b6274932":"logreg = LogisticRegression(**best_parameters)\nlogreg.fit(train_, y_train)\ntest_score = logreg.score(val_, y_val)\ntest_score","3e24ecde":"# from sklearn.linear_model import LogisticRegression\n# logreg = LogisticRegression(class_weight='balanced', solver='liblinear', penalty ='l1', C= 0.1, max_iter=10000)\n# logreg.fit(train_, y_train)\n# test_score = logreg.score(val_, y_val)\n# test_score","930f375b":"# Logistic Regression model\ny_pred_final=logreg.predict_proba(test_)[:,1]","1e7dc17b":"submission = pd.DataFrame({\"id\": test[\"id\"],\"target\": y_pred_final})\nsubmission.to_csv('submission.csv', index=False)","72b22404":"* Test Data Visualization","ca183e77":"* Training Data Visualization","f85cfbc0":"* AdaBoost Classifier","931ce05e":"* Gradient Boost Classifier","91a2b71a":"* Random Forest Classifier","c76e9238":"Here we can see there may be some clustering possible based on negative and positive number in dataset"}}