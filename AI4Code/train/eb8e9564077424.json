{"cell_type":{"945a9bba":"code","26718afd":"code","0468c4a1":"code","a7b1fc7f":"code","e19db38e":"code","4da27285":"code","cc37792c":"code","7275311d":"code","023dd001":"code","e6c4d2d1":"code","e40ab0d1":"code","35a5767e":"code","aca1248d":"code","cbda3402":"code","d0b22b70":"code","0514503d":"code","4069ec57":"code","ded3dd9d":"code","aa1c8c29":"code","7079c322":"code","965bbefe":"code","e1487a38":"code","87507af6":"code","fe82a990":"code","0e861980":"code","68d10213":"markdown","81b31a89":"markdown","7abd428f":"markdown","a6096212":"markdown","b9afe6e3":"markdown","f8655103":"markdown","4f4b5891":"markdown","17f41694":"markdown","9c2b2fbb":"markdown"},"source":{"945a9bba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn as sk # data analysis \nimport seaborn as sn #data visualization library\nimport matplotlib.pyplot as plt #interactive visualization library\n\n#importing models from SciKit Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import feature_selection\n\n#Error Metrics from SciKit Learn\nfrom sklearn.metrics import accuracy_score,precision_score, classification_report,precision_recall_curve\nfrom sklearn.metrics import auc,roc_auc_score,roc_curve, average_precision_score, plot_precision_recall_curve\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26718afd":"# importing data\ndf_cc=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf_cc.shape","0468c4a1":"#returns the first n rows for the object\ndf_cc.head(2)","a7b1fc7f":"#verify the null values\ndf_cc.isna().sum()","e19db38e":"#classification of data into features(X) and labels(y)\n# Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\nX,y =df_cc.drop([\"Class\"],axis=1),df_cc.Class","4da27285":"#row and column classification of feature and label\nX.shape, y.shape","cc37792c":"#splitting the data into training and test,classify the 33% data for test and 67% for training\nX_train, X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)","7275311d":"#row and column classification of feature (train, test) and label (train, test)\nX_train.shape,X_test.shape, y_train.shape,y_test.shape","023dd001":"%%time\n#Applying Logistic Regression \nmodel_lrm=LogisticRegression(solver='liblinear',random_state = 0)\n\n# fitting on training data\nmodel_lrm.fit(X_train,y_train)","e6c4d2d1":"# Logistic Regression performance on training and test data\nprint('Accuracy on training set (Logistic Regression):',model_lrm.score(X_train,y_train))\nprint('Accuracy on test set (Logistic Regression):',model_lrm.score(X_test,y_test))","e40ab0d1":"%%time\n#Implementing Naive Bayes on creditcard dataset\nmodel_nvm=GaussianNB()\n\n# fitting on training data\nmodel_nvm.fit(X_train,y_train)","35a5767e":"# Naive Bayes performance on training and test data\nprint ('Accuracy on training date set (Naive Bayes) :',model_nvm.score(X_train,y_train))\nprint ('Accuracy of test data set (Naive Bayes) :', model_nvm.score(X_test,y_test))","aca1248d":"# Precicted valye based on LogisticRegression\ny_pred_lrm=model_lrm.predict(X_test)","cbda3402":"# predict probabilities\ny_pred_prob = model_lrm.predict_proba(X_test)\n# retrieve just the probabilities for the positive class\ny_score = y_pred_prob[:, 1]","d0b22b70":"#accuracy score\nprint('Accuracy score of predicted data :',accuracy_score(y_test,y_pred_lrm))","0514503d":"#precision score\nprint('Precision Score of predicted data :',precision_score(y_test,y_pred_lrm))","4069ec57":"# 492 frauds out of 284,807 transactions\/positive class (frauds) account for 0.172% of all transactions.\nOther_Class=len(df_cc.Amount[df_cc.Class == 0])\nFraud_Class=len(df_cc.Amount[df_cc.Class == 1])\nOther_Class, Fraud_Class","ded3dd9d":"#plotting of feature 'Class', that is response variable and it takes value 1 in case of fraud and 0 otherwise.\nsn.kdeplot(df_cc.Amount[df_cc.Class == 0],label = 'Other(NonFraud) Trans', shade=True,color='yellow')\nsn.kdeplot(df_cc.Amount[df_cc.Class == 1], label = 'Fraud Trans', shade=True,color='red')\nplt.xlabel('Amount');\nplt.show()","aa1c8c29":"# The average_precision_score function computes the average precision (AP) from prediction scores. \n# The value is between 0 and 1 and higher is better\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision_score(y_test, y_score)))","7079c322":"# Plot Precision Recall Curve for binary classifiers\nplot_precision_recall_curve(model_lrm, X_test,y_test,color='lightseagreen',\n                            response_method='predict_proba')\nplt.title('Precision Recall Curve');","965bbefe":"# Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\nroc_auc_score(y_test,y_score)","e1487a38":"# calculate Receiver operating characteristic (ROC) curve for model\nfpr, tpr,threshhold = roc_curve(y_test,y_score)\n# plot model roc curve\nplt.plot(fpr, tpr, label='Logistic',linewidth=2,color='orange')\n# axis labels\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","87507af6":"# Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced\n# It computes a precision-recall curve from the ground truth label for different probability thresholds.\nfpr, tpr, thresholds=precision_recall_curve(y_test,y_pred_lrm,pos_label=1)\n\n#Area Under the Curve (AUC) using the trapezoidal rule.\nprint('Area Under the Precision-Recall Curve (AUPRC):',auc(fpr, tpr))","fe82a990":"#creating dictory to map the column with optimal feature rating\nfeature_dict=dict(zip((df_cc.columns),list(model_lrm.coef_[0])))\nfeature_dict","0e861980":"#Visulaization of Important features\nfeature_df=pd.DataFrame(feature_dict,index=[0])\nfeature_df.T.plot.bar(title=\"Credit Card Dataset - Feature Importance\",legend=False,color='salmon');\nplt.xlabel('Features');","68d10213":"# Splitting the Data ~ features and labels","81b31a89":"# AUC - ROC Curve\n  * AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model\u2019s performance.\n  * AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability.\n  * TPR - True Positive Rate \/ Recall \/ Sensitivity\n  * FPR - False Positive Rate ","7abd428f":"# Data Pre-processing Procedure","a6096212":"# Choosing Right Estimator\n    * Choosing the right estimator \n        - Logistic Regression \n        - Naive Bayes  ","b9afe6e3":"# Accuracy Metrics","f8655103":"# Credit Card Fraud Detection\nAnonymized credit card transactions labeled as fraudulent or genuine\n\n### Kaggle\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n    \n### Context\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n\n### Success Matrix\nIdentify fraudulent credit card transactions.\n    * Measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). \n    * Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n\n### Data Source\n    *Credit Card data\n  \n### Execution Strategy  - An end-to-end Scikit-Learn worfklow\\n\",\n         1. Getting the data ready-libraries and data loading\n         2. Data pre-processing into features and labels\n         3. Splitting into Test and Training data\n         3. Choosing the right maching learning estimator\/Algorithm\/Machine Learning Model for this problem\n         4. Fitting your chosen Machine Learning Model to data and using it to make a prediction\n         5. Evaluting the Machine Learning Model\n         6. Improving predictions through experimentation (hyperparameter tuning), if required < 95%\n         7. Feature Importance Evaluations","4f4b5891":"# Feature Importance Evaluation","17f41694":"###  Finalizing a classification model based on performance\n    LogisticRegression\n    * 99.87% of accuracy on test data\n    * 99.86% of accuracy on training data","9c2b2fbb":"# Area Under the Precision-Recall Curve (AUPRC)\n    * Precision Recall Curve \n    * Area Under the Curve (AUC)"}}