{"cell_type":{"e2dc4d1d":"code","29f3b594":"code","e30002b4":"code","77fd8d2b":"code","55f92600":"code","69afa0e7":"code","5d31d6e4":"code","1c051191":"markdown","44ac9340":"markdown","a2064d24":"markdown","cf788ef2":"markdown"},"source":{"e2dc4d1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ndef Graph_Confusion_Matrix(CM, labels):\n    #np.fill_diagonal(CM,0)\n\n    plt.figure(figsize = (5,5))\n    sn.set(font_scale=1.4)#for label size\n    sn.heatmap(CM, annot=True,annot_kws={\"size\": 16}, fmt='g'\n               ,xticklabels = labels\n               ,yticklabels = labels)# font size\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    \nclass anomaly_detection():\n    \n    def __init__(self, df_X):\n         self.df_X = df_X\n        \n         m,n = df_X.values.shape\n\n         self.sigma = np.zeros((n,n))\n         self.mu = np.zeros((n))\n\n    def Guassion_Parameters(self):\n        ##\n        #Obtain the mean and covariance matrix need for the Guassian curve\n        ##\n        X = self.df_X.values\n        X_mean = X.mean(axis=0)\n        \n        m, n = X.shape # number of training examples, number of features\n        \n        #Creates the covariance Matrix\n        Sigma = np.dot((X - X_mean).T, (X - X_mean))\n        \n        Sigma = Sigma * (1.0\/m)\n        \n        self.mu = X_mean\n        self.sigma = Sigma\n\n\n    def multivariateGaussian(self, df_X):\n        ##\n        #Calculates the P-Value based on the parameters found above.\n        #This is the vectorised form of the equation.\n        ##\n        \n        X = df_X\n        \n        m, n = X.shape # number of training examples, number of features\n    \n        X = X.values - self.mu.reshape(1,n) # (X - mu)\n    \n        # vectorized implementation of calculating p(x) for each m examples: p is m length array\n        p = (1.0 \/ (math.pow((2 * math.pi), n \/ 2.0) * math.pow(np.linalg.det(self.sigma),0.5))) * np.exp(-0.5 * np.sum(X.dot(np.linalg.pinv(self.sigma)) * X, axis=1))\n    \n        return p\n\n    def Best_Epsilon(self, Yval ,Pred):\n        ##\n        #Obtains the best epsilon when the F1 score is maxamised.\n        ##\n        \n        \n        bestF1 = 0\n        bestEpsilon = 0\n    \n        stepsize = (max(Pred) - min(Pred)) \/ 1000\n        for epsilon in np.arange(min(Pred), max(Pred), stepsize):\n            \n            predictions = (Pred < epsilon).astype(int)\n            \n            F1 = f1_score(Yval, predictions)\n            \n            if F1 > bestF1:\n                bestF1 = F1\n                bestEpsilon = epsilon\n                \n        return(bestEpsilon, bestF1)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","29f3b594":"df = pd.read_csv(\"..\/input\/creditcard.csv\")\ndf.head(5)","e30002b4":"%matplotlib inline\n\nfor i, col in enumerate(df.columns):\n    plt.figure(i, figsize=(5,5))\n    sn.distplot(df[col].loc[df['Class'] == 1], color=\"red\" ,label=\"Fraud 1\")\n    sn.distplot(df[col].loc[df['Class'] == 0], color=\"blue\" ,label=\"Authorised 0\")\n    plt.legend(fontsize=12)\n    plt.title(col, fontsize= 12)\n    #print(i,col)","77fd8d2b":"df_base = df[[\"Class\", \"V3\", \"V4\", \"V10\", \"V11\", \"V12\" , \"V14\" , \"V16\", \"V17\", \"V18\"]] ","55f92600":"#Seporates the two classes\ndf_fraud = df_base.loc[df[\"Class\"] == 1]\ndf_authorised = df_base.loc[df[\"Class\"] == 0]\n\n#Resets the index\ndf_fraud = df_fraud.reset_index(drop=True)\ndf_authorised = df_authorised.reset_index(drop=True)\n\n#splits into three groups\ntrain, test, validation = np.split(df_authorised, [int(0.7* len(df_authorised)),int(0.85* len(df_authorised)) ])\npost_V, post_T = np.split(df_fraud ,2)\n\ntest = test.append(post_T).sample(frac=1).reset_index(drop=True)\nvalidation = validation.append(post_V).sample(frac=1).reset_index(drop=True)\n\nfor i in range (1,len(df_base.columns)):\n        \n    model_1 = anomaly_detection(train.iloc[:,[0,i]].drop([\"Class\"], axis=1))\n    \n    #Fits the model\n    model_1.Guassion_Parameters()\n\n    pred_Val = model_1.multivariateGaussian(validation.iloc[:,[0,i]].drop([\"Class\"], axis=1))\n\n    #Use validation data to find the best threshold for P Value\n    best_E, Best_F1 = model_1.Best_Epsilon(validation[\"Class\"], pred_Val)\n\n    yval = validation[\"Class\"]\n\n\n    CM_Val = confusion_matrix(yval,  (pred_Val < best_E).astype(int), [1,0])\n    Graph_Confusion_Matrix(CM_Val, [1,0])\n    print(\"Columns Name: \", train.iloc[:,[0,i]].columns)\n    print(\"F1 Score: \",Best_F1)","69afa0e7":"model_1 = anomaly_detection(train[[\"V12\", \"V17\"]])\n\n#Fits the model\nmodel_1.Guassion_Parameters()\n\npred_Val = model_1.multivariateGaussian(validation[[\"V12\", \"V17\"]])\n\n#Use validation data to find the best threshold for P Value\nbest_E, Best_F1 = model_1.Best_Epsilon(validation[\"Class\"], pred_Val)\n\nyval = validation[\"Class\"]\n\n\nCM_Val = confusion_matrix(yval,  (pred_Val < best_E).astype(int), [1,0])\nGraph_Confusion_Matrix(CM_Val, [1,0])\nprint(\"Columns Name: \", train.iloc[:,[0,i]].columns)\nprint(\"F1 Score: \",Best_F1)","5d31d6e4":"pred_Test = model_1.multivariateGaussian(test[[\"V12\", \"V17\"]])\n\nytest = test[\"Class\"]\nCM_Test = confusion_matrix(ytest,  (pred_Test < best_E).astype(int),  [1,0])\nGraph_Confusion_Matrix(CM_Test,  [1,0])\n","1c051191":"Features V12 and V17 have the best and 2nd best F1 score respectivily.\n\nLets try creating a model with both these features:","44ac9340":"We want to select features that have different PDF's such as V16 and V17 but there are other. The code below shows all the features I have selected.","a2064d24":"Below the loop creates a model for each feature then outputs the corresponding confusion matrix and F1 score.","cf788ef2":"Although the F1 score is worse this model does detect more fraud transactions."}}