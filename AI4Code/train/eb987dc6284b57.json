{"cell_type":{"7b7038bb":"code","c00164c6":"code","e86967ab":"code","7b443f28":"code","654a10db":"code","c59bb766":"code","461d77a7":"code","e784f05d":"code","f44d457c":"code","d4522feb":"code","50288000":"code","d665324c":"code","e4a8f872":"code","4c1fb04d":"code","86aabca2":"code","b6decd18":"code","b7d1b6b8":"code","45b42882":"code","fb539a35":"code","fcd2a2d4":"code","8612500a":"code","bdedbebf":"code","f370a486":"code","7b10e908":"code","99daada0":"code","a92ffdad":"code","72e6d9d9":"code","0b3979ac":"code","755e3947":"code","f3f44813":"markdown","2b7a4e4d":"markdown","733baf9b":"markdown","ddb6a542":"markdown","fb5ef38f":"markdown","cf389823":"markdown","6fd872c3":"markdown","9273692c":"markdown","8573f713":"markdown","0f3d4087":"markdown","4a1b05a5":"markdown","60e3e997":"markdown","5851abea":"markdown","7b909b6b":"markdown","d11acc53":"markdown","697aec41":"markdown","92cea5f6":"markdown","8c0e4ca0":"markdown","794b51c0":"markdown","b48a188c":"markdown","4414346d":"markdown","128c0ca0":"markdown","7f7c2746":"markdown","06bfcca6":"markdown","31fdf81d":"markdown","026eb36c":"markdown","cebf2c9e":"markdown","e4aeddb7":"markdown","b05fdab5":"markdown","b828a9e7":"markdown","035ae134":"markdown","6b8587ef":"markdown","6776c009":"markdown","848c6468":"markdown","2a03cc18":"markdown","60bbbc97":"markdown","18b58f61":"markdown","f84f9c93":"markdown","28813035":"markdown","a80e94bf":"markdown","10ef7a87":"markdown","9d633b68":"markdown","a47d48ae":"markdown","447f1839":"markdown","8fbab7e9":"markdown","b85eb382":"markdown","3a10ed4b":"markdown","80ea9a36":"markdown","7b7b0c72":"markdown","d41eaf1a":"markdown"},"source":{"7b7038bb":"import numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LassoCV, ElasticNet, enet_path, ElasticNetCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom math import sqrt\nimport matplotlib.pyplot as plot\nimport pandas as pd\nimport csv","c00164c6":"# Load the data using Pandas ---------------------------------------------------\n# Output: Pandas Series Object\n\nschool_explorer = pd.read_csv(\n    '..\/input\/2016 School Explorer.csv')\nd5_shsat = pd.read_csv(\n    '..\/input\/D5 SHSAT Registrations and Testers.csv')","e86967ab":"school_explorer.loc[[427,1023,712,908],'School Name'] = ['P.S. 212 D12',\n                                                         'P.S. 212 D30',\n                                                         'P.S. 253 D21',\n                                                         'P.S. 253 D27']","7b443f28":"# List all the columns with numeric values\nnumeric_list = list(school_explorer.columns[[7,8]+list(range(16,27))+\n                                            [28,30,32,34,36]+\n                                            list(range(42,161))])\n# List all the columns with percent values\npercent_list = list(school_explorer.columns[list(range(18,27))+(list([28,30,32,34,36]))])\n# List all the categorical data columns\ncategory_list = list(school_explorer.columns[[27,29,31,33,35,37,38]])","654a10db":"# Drop the first 3 columns becaue they are mostly empty and do not have a \n# lot of predictive value\nschool_explorer = school_explorer.drop(['Adjusted Grade',\n                                        'New?',\n                                        'Other Location Code in LCGMS'], axis=1)","c59bb766":"# Remove % signs and N\/As from the percent data.  Format as a floating point numbers\nfor elm in percent_list:\n\tschool_explorer[elm] = school_explorer[elm].astype('str')\t\t\t# Force it to be a string\n\tschool_explorer[elm] = school_explorer[elm].str.replace(\"%\",\"\")\t\t# Replace % with blanks\n\tschool_explorer[elm] = school_explorer[elm].str.replace(\"nan\",\"0\")\t# Replace nan with zeros\n\tschool_explorer[elm] = school_explorer[elm].astype('float')\t\t\t# Force it to be a float\n\tschool_explorer[elm].replace(0, np.NaN, inplace = True)\t\t\t\t# Replace zeros with Numpy NaN\n\tschool_explorer[elm] = school_explorer[elm].interpolate()\t\t\t# Interpolate missing values (linear)","461d77a7":"# Remove $ signs and currency formating from the School Income Estimate.  Formate as a floating point number\nschool_explorer['School Income Estimate'] = school_explorer['School Income Estimate'].astype('str')\t\t\t\t\t\t\t# Cast as a string\nfor elm in [\",\", \"$\", \" \"]:\n\t\tschool_explorer['School Income Estimate'] = school_explorer['School Income Estimate'].str.replace(elm, \"\")\t\t\t# Remove special char\n\nschool_explorer['School Income Estimate'] = school_explorer['School Income Estimate'].str.replace(\"nan\", \"0\")\t\t\t\t# Remove nans\t\nschool_explorer['School Income Estimate'] = school_explorer['School Income Estimate'].astype('float')\t\t\t\t\t\t# Cast as a float\n#school_explorer['School Income Estimate'] = school_explorer['School Income Estimate'].replace(0.0, np.NaN, inplace = True)\t# Fill with Numpy NaN\t","e784f05d":"# Fill in missing data with the pandas interpolation function over columns with missing data.\nfor elm in ['Economic Need Index', 'Average ELA Proficiency', 'Average Math Proficiency', 'School Income Estimate']:\n\tschool_explorer[elm] = school_explorer[elm].interpolate()","f44d457c":"# Quantizing the categorical data.  Assign numberic values to categories. -----------------------------------\nfor elm in category_list:\n\tschool_explorer[elm] = school_explorer[elm].astype('str')\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Cast as a string\n\t'''\n\tReplace all the missing data points with a '1', meaning 'Meeting Target'.  Is this a fair assumption to make?\n\t'''\n\tschool_explorer[elm] = school_explorer[elm].str.replace(\"nan\", \"1\")\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\tattribute_map = {'Not Meeting Target':'0', 'Meeting Target':'1', 'Approaching Target':'2', 'Exceeding Target':'3'}\t\t# Map to integer\n\tschool_explorer[elm].replace(attribute_map, inplace = True)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Value Swap\n\tschool_explorer[elm] = school_explorer[elm].astype(int)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Cast as an integer","d4522feb":"# Read the data from the source file and only present the data for 2016 (same year as the school explorer data)\nd5_shsat_2016 = d5_shsat[['DBN', 'Enrollment on 10\/31','Number of students who registered for the SHSAT', 'Number of students who took the SHSAT']][d5_shsat['Year of SHST']==2016].groupby(['DBN'], as_index = False).agg(np.sum)\nd5_shsat_2016.rename(columns={'DBN':'Location Code'}, inplace = True)","50288000":"# Calculate the ratio of students who actually took the test\/registerd compared to the number of students enrolled.  This is the target for prediciton.\nfor row in d5_shsat_2016:\n\td5_shsat_2016['% Test Takers'] = (d5_shsat_2016['Number of students who took the SHSAT'] \/ d5_shsat_2016['Enrollment on 10\/31'])\n\td5_shsat_2016['% Test Registration'] = (d5_shsat_2016['Number of students who registered for the SHSAT'] \/ d5_shsat_2016['Enrollment on 10\/31'])","d665324c":"# Create a list of the two target values for predition\ntarget_list = [d5_shsat_2016['% Test Takers'], d5_shsat_2016['% Test Registration']]","e4a8f872":"# Merge with the attribute data based on the location code.\nmerged_data = pd.merge(school_explorer, d5_shsat_2016, on = 'Location Code')","4c1fb04d":"print('Size of the School Explorer Data = ', school_explorer.shape)\nprint('Size of the D5 SHSAT Data = ', d5_shsat_2016.shape)\nprint('Size of the finished merged data = ', merged_data.shape)","86aabca2":"# Column Names from datasets\nnames = merged_data.columns.get_values()\nnames.tolist()","b6decd18":"# This standardized function applies to all the data, including the target values that were merged together.\nnorm_data = merged_data.iloc[:,13:].apply(lambda x: (x - np.mean(x)) \/ (np.std(x)))","b7d1b6b8":"# Create the attribute matrix by slicing the normalized data up to the number of students who registered for the exam.\n# This grabs everything in the normalized data except the last two columns (the target values) and converts to a list.\nxNormalized = norm_data.iloc[:,:-2].values.tolist()\n#print (xNormalized)\n# The normalization function tries to divide by zero for rows with only zeros (std = 0) this corrects the issue.\nxNormalized = [[0 if np.isnan(elm) else elm for elm in row] for row in xNormalized]\n\n# Create the first target variable, % of enrolled students who took the exam\n# This grabs the last column and converts to a list.\ntarget_take = norm_data.iloc[:,-1].values.tolist()\n#print type(target_take)\n\n# Create the second target variable, % of enrolled students who registered the exam\n# This grabs the second to last column and converts to a list.\ntarget_reg = norm_data.iloc[:,-2].values.tolist()\n#print type(target_reg)\n\n# Convert to Numpy Array\nX = np.array(xNormalized)\nY1 = np.array(target_take)\nY2 = np.array(target_reg)","45b42882":"schoolModel = LassoCV(cv = 2, max_iter = 10000, selection = 'random').fit(X, Y1)","fb539a35":"alphas, coefs, _ = linear_model.lasso_path(X, Y1, return_models=False)","fcd2a2d4":"nattr, nalpha = coefs.shape\n#find coefficient ordering\nnzList = []\nfor iAlpha in range(1,nalpha):\n    coefList = list(coefs[: ,iAlpha])\n    nzCoef = [index for index in range(nattr) if coefList[index] != 0.0]\n    for q in nzCoef:\n        if not(q in nzList):\n            nzList.append(q)","8612500a":"nameList = [names[nzList[i]] for i in range(len(nzList))]\nprint(\"Attributes Ordered by How Early They Enter the Model\", nameList)","bdedbebf":"plot.plot(alphas, coefs.T)\nplot.title(\"Lasso Coefficient Curves\")\nplot.xlabel('alpha')\nplot.ylabel('Coefficients')\nplot.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\nplot.legend(nameList, loc='upper center', bbox_to_anchor=(0.5, -0.25), fancybox=True, shadow=True, ncol=3)\nplot.axis('tight')\nplot.semilogx()\nax = plot.gca()\nax.invert_xaxis()\nplot.show()","f370a486":"alphaStar = schoolModel.alpha_  #0.04263474216648463\nindexLTalphaStar = [index for index in range(100) if alphas[index] > alphaStar]\nindexStar = max(indexLTalphaStar)","7b10e908":"#here's the set of coefficients to deploy\ncoefStar = list(coefs[:,indexStar])\nprint(\"Best Coefficient Values \", coefStar)","99daada0":"#names = names.tolist()\nfor i in range(len(coefStar)):\n    if coefStar[i] != 0.0 or -0.0:\n        print('Attribute = ', names[i])\n        print('Coefficient = ', coefStar[i])\n    else:\n        pass","a92ffdad":"absCoef = [abs(a) for a in coefStar]\n\n#sort by magnitude\ncoefSorted = sorted(absCoef, reverse=True)\n\nidxCoefSize = [absCoef.index(a) for a in coefSorted if not(a == 0.0)]\n\nnamesList2 = [names[idxCoefSize[i]] for i in range(len(idxCoefSize))]\n\nprint(\"Attributes Ordered by Coef Size at Optimum alpha\", namesList2)","72e6d9d9":"# Plot the MSE for each curve ------------------------------------------\n\nplot.figure()\nplot.plot(schoolModel.alphas_, schoolModel.mse_path_, ':')\nplot.plot(schoolModel.alphas_, schoolModel.mse_path_.mean(axis=-1),\nlabel='Average MSE Across Folds', linewidth=2)\nplot.axvline(schoolModel.alpha_, linestyle='--',\nlabel='CV Estimate of Best alpha')\nplot.semilogx()\nplot.legend()\nax = plot.gca()\nax.invert_xaxis()\nplot.xlabel('alpha')\nplot.ylabel('Mean Square Error')\nplot.title('Lasso Model, Cross Validations = 2')\nplot.axis('tight')\nplot.show()\n#print out the value of alpha that minimizes the Cv-error\nprint(\"alpha Value that Minimizes CV Error \",schoolModel.alpha_)\nprint(\"Minimum MSE \", min(schoolModel.mse_path_.mean(axis=-1)))","0b3979ac":"l1_ratio = [.1, .5, .7, .9, .95, .99, 1] #alpha","755e3947":"l1_ratio_MSE = []\nfor i in l1_ratio:\n    schoolModel3 = ElasticNetCV(l1_ratio = i, cv = 2, selection = 'random').fit(X, Y1)\n    alphas, coefs, _ = linear_model.enet_path(X, Y1)\n    print(\"alpha Value that Minimizes CV Error \",schoolModel3.alpha_)\n    print(\"Minimum MSE \", min(schoolModel3.mse_path_.mean(axis=-1)))\n    l1_ratio_MSE.append(min(schoolModel3.mse_path_.mean(axis=-1)))","f3f44813":"We identified above that Lasso or ElasticNet would be a good first choice and have completed the analysis with Lasso, but how about ElasticNet?  The ElasticNet algorithm uses and adjustable blend of ridge penalty and Lasso penalty.  This is done via alpha which parameterizes the fraction of the total penalty that is ridge and the fraction that is Lasso.  When alpha (called l1_ratio in the Scikit-Learn) is equal to 0 ElasticNet equals the ridge algorithm and when alpha equals 1 ElasticNet equals the Lasso algorithm.   ","2b7a4e4d":"This shows us that 'Grade 8 ELA 4s - Limited English Proficient' is the strongest predictor of taking the SHSAT.  Followed closely by 'Grade 8 ELA 4s - White' and 'Grade 8 ELA 4s - Multiracial'.  The fact that these attributes have the largest impact on the target result agree with previous results.  We can order all the coefficients by magnitude for the best solution found:","733baf9b":"## PASSNYC: Datascience for Good","ddb6a542":"If we follow the flowchart we will see that we are attempting to predict a quantity (% of students who take or registar for the SHSAT) which points us in the direction of a regression analysis.  Next we see that we have less than 100K samples (way less...) and we hope that a few features will be important.  This arrives at the conclusion that either Lasso or ElasticNet would be good first choices for our problem.  ","fb5ef38f":"#### Plot the Coefficient Curves","cf389823":"#### Results Discussion","6fd872c3":"The data columns that contain a percent sign need to be reformated to remove it.\nAdditionally there are a few empty or 'N\/A' values that need to be replaced with\nzeros.  Finally the columns are all transformed into floating point numbers\nfor math purposes later.","9273692c":"This list orders the attributes in terms of importance.","8573f713":"My initial run will utilize Lasso (Least Absolute Shrinkage and Selection Operator) for a couple of reasons:\n    - It attempts to force attribute coefficients  to zero to reduce complexity.\n    - In doing so, we can get an estimate of which features are most  influential.\n    - Reducing the number of coefficients reduces the overfitting data problem.","0f3d4087":"The final thing we do before moving on to prediction is to write the column names to a list for later use and write it to a list.","4a1b05a5":"#### Cleaning up the Data","60e3e997":"Another nice feature of selection the LASSO algorithm is that the Scikit-Learn API support the use of cross validation.  This way we can train and test on the whole data without having to reserve any for testing.  This is extreamly import of this particular problem since we only have 21 rows of data, we don't have a lot to go around.","5851abea":"One of the reasons we choose Lasso was to identify which attributes would be the most valuable in making predictions.  Lasso does this by changing the coeffients of attributes that are most closely associated with impacting the prediction.  We can then say the move influential attributes are in order of how the alogrithm increases them; i.e. the order in which the 'come off' of zero.","7b909b6b":"#### Model Implimentation","d11acc53":"Looking at the rest of the data, there are a few additional columns with zero values that will impact the standardization that need to be fixed.  Again we do this by interpolation.","697aec41":"Here is were we identify the first problem with this task.  We started with a School Explorer 2016 dataset with 1272 rows of data.  But we were only given SHSAT data for 26 individual schools in 2016.  \n\nThen once we merge the two files, we only have school explorer data AND SHSAT data for 21 schools in 2016.","92cea5f6":"The data provided contains multiple issue that need to be resolved in oder to process the data.  For example, there are mulitple 'N\/A' entries that need to be removed.  Additionally the data needs to be transformed into the correct data type (float, int, etc) and all formating must be removed ($ signs and % signs) ","8c0e4ca0":"#### Model Selection","794b51c0":"Next we use the Scikit-Learn function 'lasso_path' for ploting the values of alpha (penalization parameter) and the subsequent coeffients.  This helps us visualize the solutions and identify the best one.","b48a188c":"From the output above, we can see that as alpha is increased from 0.1 to 1.0 the solution continues to get better.  The best solution is found for alpha equal to 1.0, or a perfect Lasso algorithm.  We also see that the ElasticNet algorithm comes to the same value (within 3 decimal places) for alpha and MSE for the best solution.\n\nTherefore, there is no need to continue evaluating the problem with the ElasticNet algorithm as the Lasso algorithm is equal to the best it can do and is already complete. ","4414346d":"Interestingly, (and somewhat uplifting) is that race and economic need index did not rate highly in the Lasso model.","128c0ca0":"Next we slice the standardized data into a xNormalized and two targets and transform them into NumPy arrays.  Note that the standardization process hits a small hiccup for the columns with only zero values (standard deviation = 0.0).  We need to overwrite these back to zero.","7f7c2746":"The D5 data contains individual schools enrollment and number of students who regrister for and took the SHSAT.  Obviously schools with larger enrollments may have a larger number of students registar and take the exam.  In order to evenly evaluate schools, we need to create two new columns that measure the percentage of students who registered for the test and the precentage of students who took the test.","06bfcca6":"Then we want to know what coeffients correspond that the best value of alpha.  Note the ordering of the coefs is not sorted by appearance in the model but the orignal ordering.","31fdf81d":"Source: https:\/\/www.kaggle.com\/passnyc\/data-science-for-good\n\nFiles have been downloaded and placed into a Project folder within the Python \nworking directory for offline processing.  The files are .csv type files and\nare read via the Pandas read_csv method.","026eb36c":"Next, we merget the cleaned School Explorer Data with the D5 SHSAT data for 2016 based the school location code.","cebf2c9e":"For now we will primarily be focused on the % of students who actually took the SHSAT as that is the primary objective.  \n\nWe also set a couple of parameters for the Lasso algorithm; number of cross validation folds (cv), number of maximum allowable iterations (max_iter) and selection.  \n\nThe number of cross validation folds dictates how the data is split for each solution.  With a cv = 2 we reserve half the data for training and test on the other half for the first fold then we flip the data and train again.  \n\nThe number of max_iterations is only intended to prevent the computer from diverging and running away uncontrolled.\n\nThe selection parameter makes a random coefficient to be updated every iteration rather than looping over features sequentially by default. This (setting to \u2018random\u2019) often leads to significantly faster convergence.","e4aeddb7":"For all of the categorical data, we need to quantize it based on the number of possible categories.  Lucky for us, they all use the same categories and we simply map each column appropriatly.","b05fdab5":"#### Setting up the Python environment","b828a9e7":"There is a clear pattern to the first few influential (good or bad) parameters.  English language skills occupy 4 of the first 7 parameters including the top two and math skills occupies 2 of the 7.  \n\nThe conclusion that we can draw from this is elevated profiency in the English language is key to successfuly taking the SHSAT.  PASSNYC should focus resources on increasing these skills in the underserved communities.","035ae134":"As you can see above, the final line of code uses the interpolate function to fill in any missing values.  This is a linear interpolation based on the position of value in the data, assuming evenly spaced data.  By doing this we will not negatively impact our standaization later.","6b8587ef":"As with any standard Python project, we start by importing the necessary packages.","6776c009":"The next obvious question would be, \"So which solution is the best?\"  We identify the best solution as \"alphaStar\" and cycle through the values for alpha until be have the best one.","848c6468":"With only 21 rows of data, we were able to achieve a model with an MSE of approximatly 24%.  Certainly not an ideal solution but a good result given the amount of data used. Also of note, increasing the number of cross validation folds has a dramatically negative impact on the MSE of the model.  This is another symptom of having a poorly conditioned problem with limited data.  Clearly adding more infomation to the model would greatly reduce the error.\n\nWith that said, the overall pattern of the solution still holds true.  Focusing on improving English language skills of all students, particularly in Grade 8, would be the most influential factor in increasing successful completion of the SHSAT exam.","2a03cc18":"PASSNYC is a data science\/machine learning competition aimed at utilizing publically available data to identify schools or students that would most benefit from the services provided by the PASSNYC organization.  The primary objective of this competition is to identify ways to increase registration and completion of the Specialized High School Admissions Test (SHSAT).  Successfully passing this test allows students to attend some of the best high schools that New York has to offer.\n\nIn order to help facilitate this objective, machine learning algorithms will be used to identify critical school\/student parameters that impact registration and completion of the SHSAT.  The completed model will then be used to guide PASSNYC in actionable steps to take to increase registration and completion of the SHSAT based on the model.  ","60bbbc97":"This provides for a poorly conditioned problem with minimal data.  We have substantially more columns thand rows and not enough rows all together.  \n\nWhy is it that the D5 SHSAT data is so sparse?  Do all the other schools have zero registration and participation in the SHSAT?  If so, then I think the first conclusion that we can draw is that PASSNYC would be well served by spreading the word on the importance of this exam.  ","18b58f61":"Now that we have a model picked, we want to standardize the data; i.e. transform it such that it has a mean of 0.0 and a standard deviation of 1.0.","f84f9c93":"Let's split the columns into the type of data they contain for easier reformatting.  We'll make a list of all the columns that contain percent data, numeric data, and categorical data.","28813035":"At this point the data is formatted appropriately but it was given in two seperate files.  The first, School Explorer Data for 2016 and the D5 SHSAT for 2016.  The School Explorer Data gives us lots of information about lots of differenct schools in New York.  Unfortunely the D5 SHSAT data only gives us information on a handful of schools and for multiple years.\n\nSince we have School Explorer Data for only 2016 it may not be reasonable to make predictions of SHSAT testing for previous years.  To facilitate the library of algorithms we need to merge the two files into one and base it on the SHSAT test for 2016 only.","a80e94bf":"Next, we reformate the column for School Income Estimate.","10ef7a87":"But first, a couple of duplicate school names show up in the school_explorer data.  We need to rename them to distinct names which will be based on their district.  The schools are identifiedin the data by their index location and renamed to their original school name with an added district designation.","9d633b68":"Let's begin our prediction by identifying the specific algorithm that we want to impliment.  Luckly Scikit-Learn has a handy flowchart to help guide us:\n\nhttp:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html","a47d48ae":"These two columns will become our targets for prediction.  If we can determine the attributes of schools that get substantial numbers of students to register and take the SHSAT we may be able to link those attributes to actions that PASSNYC can take.","447f1839":"Next, we need to discard the first three columns from the school_explorer data because they do not contain any data that will help us.","8fbab7e9":"We start by setting a couple of values for alpha to try between 0 and 1, then loop through the algorithm and see which one produces the best answer.","b85eb382":"The Lasso implimentation yields curves for various values of alpha (the penilization).  We then plot those curves to visualize the amout of each parameters influce on a particular solution.  One could draw a vertical line through this plot at any point and have one solution.","3a10ed4b":"Finally, how well did the model do?  We use the Mean Squared Error on each cross validation fold to estimate the error for each alpha.  The minimum value for MSE corresponse to the best solution and value for alpha.","80ea9a36":"You will notice that many of the coeffients are zero, meaning that they don't have much influence on the final prediction.  In effect, there parameters could be dropped from the model and simplify it further.\n\nSo lets look at the attributes that do matter and by how much.  ","7b7b0c72":"Here we sort the coeffients by size at the optimum alpah.  Again, we see that the English language skills prove to be the most influential.","d41eaf1a":"### What about Elastic Net?"}}