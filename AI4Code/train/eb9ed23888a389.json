{"cell_type":{"a2f7ec79":"code","03dd7877":"code","0e915ba5":"code","8501e965":"code","fc131a38":"code","931a1172":"code","23a61b02":"code","50a23768":"code","eb56f17b":"code","2bbf26ea":"code","55692447":"code","ace3bf92":"code","68c83994":"code","dd4c7768":"code","43142c2f":"code","36bd0710":"code","e6cda506":"code","f4e52b22":"code","0524a02f":"code","c3cedbf7":"code","9d90607d":"code","237ae313":"code","358a870d":"code","115e697c":"code","d3919a04":"code","eed7a06a":"code","8ac9bb66":"code","fd713608":"code","cdab74ef":"code","6db344d9":"code","98ca40cb":"code","0539b978":"code","487aebbb":"code","c1a1a8ce":"code","162b32de":"code","582cad6d":"code","d1752f6d":"code","e84a7841":"code","e8e179b1":"code","d57b6340":"code","48372dcc":"code","f29eda10":"code","1adcf8b4":"code","2633934b":"code","d6211bbf":"code","3fe0cc2e":"code","f4e36a3c":"code","d7a6adf7":"code","7b72cd36":"code","c0c49248":"code","6a07a064":"code","430e9f5b":"markdown","03617b6b":"markdown","78ec68bd":"markdown","29cb1791":"markdown","d8e00add":"markdown","de5bc633":"markdown","ab0ecd71":"markdown","7099d0a5":"markdown","ee999eb5":"markdown","a17d6dda":"markdown"},"source":{"a2f7ec79":"# \u4e09\u884c\u9b54\u6cd5\u4ee3\u7801\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.vision import *","03dd7877":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","0e915ba5":"bs = 8 # 64 # \u6279\u91cf\u8bbe\u7f6e, \u8fc7\u5927\u4f1a\u8d85\u51faKaggle GPU Disk \u5bb9\u91cf ","8501e965":"# path = untar_data(URLs.PETS)\/'images' # \u4ece\u4e91\u7aef\u4e0b\u8f7d\u6570\u636e\u96c6\uff0c\u56fe\u7247\u5168\u90e8\u5728\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\npath = Path('\/kaggle\/input\/'); path.ls()\npath_data = path\/'the-oxfordiiit-pet-dataset'\/'images'\/'images'; path_data.ls()[:5]\npath_model12 = path\/'v3lesson6models'; path_model12.ls()\npath_model3 = path\/'v3lesson6modelsmore'; path_model3.ls()\npath_img = path\/'catdogtogether'; path_img.ls()","fc131a38":"# \u56fe\u7247\u53d8\u5f62\u8bbe\u8ba1\ntfms = get_transforms(max_rotate=20, # \u4ee5\u540e\u9010\u4e00\u5c1d\u8bd5\n                      max_zoom=1.3, \n                      max_lighting=0.4, \n                      max_warp=0.4,\n                      p_affine=1., \n                      p_lighting=1.)","931a1172":"# \u5c06\u56fe\u7247\u5939\u8f6c\u5316\u6210ImageList\nsrc = ImageList.from_folder(path_data).split_by_rand_pct(0.2, seed=2) # \u65e0\u9700\u5355\u72ec\u505anp.random.seed(2)\nsrc\nsrc.train[0:2] # \u67e5\u770b\u8bad\u7ec3\u96c6\u4e2d\u56fe\u7247\nsrc.valid[0] # \u76f4\u63a5\u770b\u56fe\nsrc.train.__class__ # fastai.vision.data.ImageList\nsrc.__class__ # fastai.data_block.ItemLists","23a61b02":"# \u5feb\u6377\u751f\u6210DataBunch\ndef get_data(size, bs, padding_mode='reflection'): # \u63d0\u4f9b\u56fe\u7247\u5c3a\u5bf8\uff0c\u6279\u91cf\u548c padding\u6a21\u5f0f\n    return (src.label_from_re(r'([^\/]+)_\\d+.jpg$') # \u4ece\u56fe\u7247\u540d\u79f0\u4e2d\u63d0\u53d6label\u6807\u6ce8\n           .transform(tfms, size=size, padding_mode=padding_mode) # \u5bf9\u56fe\u7247\u505a\u53d8\u5f62\n           .databunch(bs=bs).normalize(imagenet_stats))","50a23768":"data = get_data(224, bs, 'zeros') # \u56fe\u7247\u7edf\u4e00\u6210224\u7684\u5c3a\u5bf8\ndata.train_ds.__class__ # fastai.data_block.LabelList \u6240\u4ee5\u53ef\u4ee5\u50cflist\u4e00\u6837\u63d0\u53d6\u6570\u636e\ndata.train_ds[0]\ndata.train_ds[0][0] # \u63d0\u53d6\u56fe\u7247\uff0c\u4e14\u5df2\u7ecf\u53d8\u5f62\uff0cImage class\ndata.train_ds[0][1] # \u63d0\u53d6label\uff0c Category class\ndata.train_ds[0][1].__class__\ndata.train_ds[0][0].__class__","eb56f17b":"def _plot(i,j,ax):\n    x,y = data.train_ds[3]\n    x.show(ax, y=y)\n\nplot_multi(_plot, 3, 3, figsize=(8,8))","2bbf26ea":"data = get_data(224,bs) # padding mode = reflection \u6548\u679c\u66f4\u52a0\uff0c\u65e0\u8fb9\u6846\u9ed1\u533a","55692447":"plot_multi(_plot, 3, 3, figsize=(8,8))","ace3bf92":"gc.collect() # \u91ca\u653eGPU\u5185\u5b58\uff0c\u4f46\u662f\u6570\u636e\u65e0\u4ece\u67e5\u770b\uff1f\uff1f\uff1f\nlearn = cnn_learner(data, \n                    models.resnet34, \n                    metrics=error_rate, \n                    bn_final=True, # bn_final=True\u4ec0\u4e48\u610f\u601d\uff0c\u51fa\u5904\u5728\u54ea\u91cc\uff1f\u770b\u4e0b\u9762\u7684\u7ed3\u679c\u5bf9\u6bd4\n                    model_dir='\/kaggle\/working') # \u786e\u4fdd\u6a21\u578b\u53ef\u88ab\u5199\u5165\uff0c\u4e14\u65b9\u4fbf\u4e0b\u8f7d","68c83994":"learn.summary()","dd4c7768":"learn.load(path_model12\/'3_1e-2_0.8')","43142c2f":"learn.load(path_model12\/'2_1e-6_1e-3_0.8')","36bd0710":"data = get_data(352,bs) # \u653e\u5927\u56fe\u7247\u5c3a\u5bf8\nlearn.data = data","e6cda506":"data = get_data(352,16)","f4e52b22":"learn = cnn_learner(data, \n                    models.resnet34, \n                    metrics=error_rate, \n                    bn_final=True,\n                    model_dir='\/kaggle\/working\/').load(path_model3\/'2_1e-6_1e-4')","0524a02f":"idx=150\nx,y = data.valid_ds[idx] # \u9a8c\u8bc1\u96c6\u56fe\u7247\u4fdd\u6301\u4e0d\u53d8\uff08\u4e0d\u8bba\u8fd0\u884c\u591a\u5c11\u6b21\uff09\ny\ny.data\ndata.valid_ds.y[idx] # \u6253\u5370label\ndata.classes[25] # \u8bf4\u660e25\u662fleonberger\u7684\u5e8f\u53f7\nx.show()","c3cedbf7":"# \u521b\u9020\u4e00\u4e2a3x3\u7684matrix\u4f5c\u4e3akernel\nk = tensor([\n    [0.  ,-5\/3,1],\n    [-5\/3,-5\/3,1],\n    [1.  ,1   ,1],\n]).expand(1,3,3,3)\/6 # \u7136\u540e\u5728\u8f6c\u5316\u4e3a\u4e00\u4e2a4D\uff0crank4 tensor\uff0c\u5728\u7f29\u5c0f6\u500d","9d90607d":"k","237ae313":"k.shape # \u67e5\u770b\u5c3a\u5bf8","358a870d":"t = data.valid_ds[idx][0].data # \u4ece\u56fe\u7247\u4e2d\u63d0\u53d6\u6570\u636etensor\nt.shape # \u5c55\u793atensor\u5c3a\u5bf8","115e697c":"t[None].shape # \u5c06\u56fe\u7247tensor\u8f6c\u5316\u4e3a\u4e00\u4e2arank 4 tensor","d3919a04":"# F.conv2d??\n# \u5bf9\u56fe\u7247tensor\u505afilter\u5904\u7406\nedge = F.conv2d(t[None], k)","eed7a06a":"show_image(edge[0], figsize=(5,5)) # \u5c55\u793a\u88abkernel\u5904\u7406\u8fc7\u7684\u56fe\u7247\u7684\u6837\u5b50","8ac9bb66":"data.c # \u53ef\u4ee5\u7406\u89e3\u6210\u7c7b\u522b\u6570\u91cf","fd713608":"learn.model # \u67e5\u770b\u6a21\u578b\u7ed3\u6784","cdab74ef":"print(learn.summary()) # \u67e5\u770blayer tensor\u5c3a\u5bf8\u548c\u8bad\u7ec3\u53c2\u6570\u6570\u91cf","6db344d9":"# learn.model.eval?\nm = learn.model.eval(); # \u8fdb\u5165 evaluation \u6a21\u5f0f","98ca40cb":"xb,_ = data.one_item(x); xb.shape; # \u83b7\u53d6\u4e00\u4e2a\u56fe\u7247tensor, \u5e94\u8be5\u662f\u53d8\u5f62\u8fc7\u540e\u7684\uff0c\nxb # xb tensor\u957f\u4ec0\u4e48\u6837\u5b50\n# Image(xb) # \u662frank 4 tensor, dim \u8fc7\u591a\uff0c\u65e0\u6cd5\u4f5c\u56fe\n# data.denorm? \ndata.denorm(xb) # \u7ed9\u4e88\u4e00\u4e2a\u65b0\u7684mean, std\u8f6c\u5316xb\uff0c\u5c55\u793a\u65b0tensor\ndata.denorm(xb)[0].shape # 4D \u8f6c\u5316\u4e3a 3D\n\n\nxb_im = Image(data.denorm(xb)[0]); xb_im # denorm\u4e4b\u540e\u5c31\u80fd\u4f5c\u56fe\u4e86\nxb = xb.cuda(); xb # tensor \u540e\u9762\u5e26\u4e0a\u4e86cuda","0539b978":"from fastai.callbacks.hooks import * # import hooks functions","487aebbb":"def hooked_backward(cat=y): # y = leonberger label\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(xb) # xb  = leonberger tensor\n            print(preds.shape)\n            print(int(cat))\n            print(preds[0, int(cat)])\n            print(preds)\n            preds[0,int(cat)].backward() # \u8fd4\u56de leonberger\u5bf9\u5e94\u7684grad\u7ed9\u5230hook_g\n    return hook_a,hook_g","c1a1a8ce":"y\nint(y) # \u83b7\u53d6\u7c7b\u522b\u5bf9\u5e94\u7684\u5e8f\u53f7\nhook_a,hook_g = hooked_backward()","162b32de":"# hook_a -> <fastai.callbacks.hooks.Hook at 0x7f8b78205278>\n# hook_g -> <fastai.callbacks.hooks.Hook at 0x7f8b78205208>\n# hook_a.stored.shape # 4D tensor, torch.Size([1, 512, 11, 11])\n# hook_a.stored[0].shape # from 4D to 3D \nacts  = hook_a.stored[0].cpu() # \u4ecegpu\u6a21\u5f0f\u5230cpu\u6a21\u5f0f\nacts.shape","582cad6d":"avg_acts = acts.mean(0) # \u538b\u7f29512\u4e2a\u503c\uff0c\u6765\u83b7\u53d6\u4ed6\u4eec\u7684\u5747\u503c\navg_acts.shape","d1752f6d":"def show_heatmap(hm): # \u7528kernel\u6765\u505a\u70ed\u529b\u56fe\n    _,ax = plt.subplots(1,3)\n    xb_im.show(ax[0]) # \u753b\u51fa\u539f\u56fe\n    ax[1].imshow(hm, alpha=0.6, extent=(0,352,352,0),\n              interpolation='bilinear', cmap='magma');\n    xb_im.show(ax[2]) # \u4e24\u56fe\u5408\u5e76\n    ax[2].imshow(hm, alpha=0.6, extent=(0,352,352,0),\n              interpolation='bilinear', cmap='magma');","e84a7841":"show_heatmap(avg_acts)","e8e179b1":"# hook_g.stored.__class__ # is a list\n# len(hook_g.stored) # just 1\n# hook_g.stored[0].__class__ # is a tensor\n# hook_g.stored[0].shape # 4D tensor\n# hook_g.stored[0][0].shape # 3D tensor\ngrad = hook_g.stored[0][0].cpu()\n# grad.mean(1).shape # \u5bf9\u4e2d\u95f4\u768411\u53d6\u5747\u503c\n# grad.mean(1).mean(1).shape # \u5bf9\u4e2d\u95f4\u7684\u4e24\u4e2a11\u53d6\u5747\u503c\ngrad_chan = grad.mean(1).mean(1)\ngrad.shape,grad_chan.shape","d57b6340":"# grad_chan[...,None,None].shape # \u5c06\u538b\u7f29\u540e\u7684grad\u4ece1D\u53d83D\nmult = (acts*grad_chan[...,None,None]).mean(0) # activation \u4e0e grad \u7684\u76f8\u4e58\uff0c\u518d\u53d6\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5747\u503c\uff0c\u53d8\u6210\u4e00\u4e2akernel\n# \u6700\u540e\u4e00\u5c42\u7684activation * \u6700\u540e\u4e00\u5c42\u538b\u7f29\u7684grad \u518d\u6c42\u548c\uff0c\u5e76\u538b\u7f29512\u5c42\u53d6\u5747\u503c\nmult.shape","48372dcc":"show_heatmap(mult)","f29eda10":"# fn = get_image_files(path_img); fn\npath_img\/'catdogTogether.png'","1adcf8b4":"# x = open_image(fn[0]); x\nx = open_image(path_img\/'catdogTogether.png'); x","2633934b":"# data.one_item?? # \u5c06\u4e00\u5f20\u56fe\u4f5c\u4e3a\u4e00\u6574\u4e2abatch\n\nxb,_ = data.one_item(x)\nxb_im = Image(data.denorm(xb)[0]) # \u751f\u6210\u56fe\u7247\nxb = xb.cuda()\nxb_im","d6211bbf":"hook_a,hook_g = hooked_backward() # y\u4f9d\u65e7\u662f\u5e8f\u53f7\u4e3a25\u7684leonberger","3fe0cc2e":"acts = hook_a.stored[0].cpu() # \u672c\u56fe\u7247 \u6700\u540e\u4e00\u5c42activation \ngrad = hook_g.stored[0][0].cpu() # \u672c\u56fe\u7247 \u6700\u540e\u4e00\u5c42 grad, \u5e76\u4e14\u662f\u57fa\u4e8eleonberger\u7c7b\u522b\u53bb\u63d0\u53d6\u7684grad\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\n\ngrad_chan = grad.mean(1).mean(1) # \u5bf9 11x11 \u53d6\u5747\u503c\uff0c 512 \u957f\u7684vector\nmult = (acts*grad_chan[...,None,None]).mean(0); mult.shape # \u751f\u621011x11 tensor","f4e36a3c":"show_heatmap(mult)","d7a6adf7":"data.classes[0]","7b72cd36":"hook_a,hook_g = hooked_backward(0)","c0c49248":"acts = hook_a.stored[0].cpu()\ngrad = hook_g.stored[0][0].cpu()\n\ngrad_chan = grad.mean(1).mean(1)\nmult = (acts*grad_chan[...,None,None]).mean(0)","6a07a064":"show_heatmap(mult)","430e9f5b":"```python\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4)) # \u7f29\u5c0f\u5b66\u4e60\u7387\uff0c\u4ee5\u53ca\u641c\u7d22\u8303\u56f4\uff0c \u4f46pct_start = 0.3\u9ed8\u8ba4\u503c\n# Total time: 06:53\n# epoch\ttrain_loss\tvalid_loss\terror_rate\ttime\n# 0\t1.273031\t0.375372\t0.092693\t03:27\n# 1\t1.203877\t0.460149\t0.088633\t03:25\n\nlearn.model_dir = '\/kaggle\/working\/'\nlearn.save('2_1e-6_1e-4')\n\n\nfrom IPython.display import FileLinks\nFileLinks('.') # \u70b9\u51fb\u94fe\u63a5\u4e0b\u8f7dmodels\n\n# .\/\n#   2_1e-6_1e-4.pth\n#   __notebook_source__.ipynb\n# .\/.ipynb_checkpoints\/\n#   __notebook_source__-checkpoint.ipynb\n```","03617b6b":"```python\ndef fit_one_cycle(learn:Learner, \n                  cyc_len:int, \n                  max_lr:Union[Floats,slice]=defaults.lr,\n                  moms:Tuple[float,float]=(0.95,0.85), \n                  div_factor:float=25., \n                  pct_start:float=0.3, \n                  final_div:float=None,\n                  wd:float=None, \n                  callbacks:Optional[CallbackList]=None, \n                  tot_epochs:int=None, \n                  start_epoch:int=None)->None:\n    \n    \"Fit a model following the 1cycle policy.\"\n    max_lr = learn.lr_range(max_lr)\n    callbacks = listify(callbacks)\n    callbacks.append(OneCycleScheduler(learn, \n                                       max_lr, \n                                       moms=moms, \n                                       div_factor=div_factor, \n                                       pct_start=pct_start,\n                                       final_div=final_div, \n                                       tot_epochs=tot_epochs, \n                                       start_epoch=start_epoch))\n    \n    learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks)\n```","78ec68bd":"## Heatmap","29cb1791":"## Grad-CAM","d8e00add":"**\u751f\u6210\u94fe\u63a5\uff0c\u4e0b\u8f7d\u6a21\u578b\u5230\u672c\u5730**\n```python\nfrom IPython.display import FileLinks\nFileLinks('.')\n```","de5bc633":"```python\nlearn.fit_one_cycle(3, slice(1e-2), pct_start=0.8) \n# slice(1e-2), max_lr=slice(1e-6,1e-3) \n# \u5177\u4f53\u4ec0\u4e48\u7528\u9014\u89c1 https:\/\/docs.fast.ai\/basic_train.html#Learner.lr_range\nlearn.model_dir = '\/kaggle\/working\/'\nlearn.save('3_1e-2_0.8')\n\n# Total time: 06:19\n# epoch\ttrain_loss\tvalid_loss\terror_rate\ttime\n# 0\t2.406209\t1.178268\t0.188769\t02:04\n# 1\t1.676663\t0.509336\t0.140054\t02:06\n# 2\t1.438834\t0.590069\t0.139378\t02:07\n```","ab0ecd71":"```python\nlearn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-3), pct_start=0.8)\n# \u7406\u89e3pct_start\u7528\u9014\u89c1 https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/callbacks\/one_cycle.py#L30\n# \u9ed8\u8ba4\u503c=0.3\uff0c\u8fd9\u91cc\u8bbe\u7f6e0.8\uff0c \u4f5c\u4e3aannealing\u7684\u5206\u6c34\u5cad\nlearn.save('2_1e-6_1e-3_0.8')\n\n# Total time: 04:21\n# epoch\ttrain_loss\tvalid_loss\terror_rate\ttime\n# 0\t1.283900\t0.470629\t0.104195\t02:09\n# 1\t1.200091\t0.379310\t0.103518\t02:11\n```","7099d0a5":"```python\nlearn.summary() # bn_final=True\n\n# Total params: 21,831,599\n# Total trainable params: 563,951\n# Total non-trainable params: 21,267,648\n\nlearn = cnn_learner(data, \n                    models.resnet34, \n                    metrics=error_rate, \n                    bn_final=False, # bn_final=True\u4ec0\u4e48\u610f\u601d\uff1f\n                    model_dir='\/kaggle\/working') # \u786e\u4fdd\u6a21\u578b\u53ef\u88ab\u5199\u5165\uff0c\u4e14\u65b9\u4fbf\u4e0b\u8f7d\n\nlearn.summary() # bn_final=False, \u5c11\u4e86\u4e0d\u5230100\u4e2a\u53c2\u6570weights, \u56e0\u4e3a\u6ca1\u6709\u4e0b\u9762\u6700\u540e\u4e00\u5c42BN\n\n# Linear               [1, 37]              18,981     True      \n# ______________________________________________________________________\n# BatchNorm1d          [1, 37]              74         True      \n# ______________________________________________________________________\n\n```","ee999eb5":"# Lesson 6: pets revisited","a17d6dda":"Paper: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https:\/\/arxiv.org\/abs\/1610.02391)"}}