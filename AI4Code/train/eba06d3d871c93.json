{"cell_type":{"d0bbe22b":"code","d9c5bf91":"code","acd28559":"code","d3ceaf32":"code","f5639176":"code","310c33a0":"code","50cdab72":"code","415cf959":"code","ddf3b7dc":"code","5a31a333":"code","4f09e438":"code","3b9a81ba":"markdown","9309d014":"markdown","7dc66d20":"markdown","59fcf7c2":"markdown","43f447d7":"markdown","dbde1d25":"markdown","67b42a1c":"markdown","6e90e539":"markdown","a57b0473":"markdown","35bb4bb8":"markdown","83570065":"markdown","d2fba9ec":"markdown","0c54fcb7":"markdown"},"source":{"d0bbe22b":"import numpy as np\r\nimport pandas as pd\r\nimport plotly.express as px\r\nimport plotly.graph_objects as go\r\nimport plotly.figure_factory as ff\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.model_selection import train_test_split","d9c5bf91":"mean_01=np.array([1,1])\r\ncov_01=np.array([[1,-0.2],[-0.2,1]])\r\nmean_02=np.array([3,4])\r\ncov_02=np.array([[1,0.1],[0.1,1]])\r\nnp.random.seed(42)\r\ndata_01=np.random.multivariate_normal(mean_01,cov_01,500, check_valid= \"warn\")\r\ndata_02=np.random.multivariate_normal(mean_02,cov_02,500, check_valid= \"warn\")\r\ndata = np.vstack((data_01,data_02))\r\ndf_train = pd.DataFrame(data, columns = [\"Feature_1\", \"Feature_2\"])\r\ndf_train[\"class\"] = [0]*500 + [1]*500 ","acd28559":"## For Contributing, refer to expectedoutput1.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Feature_1\")\r\nplt.ylabel(\"Feature_2\")\r\nplt.scatter(df_train[\"Feature_1\"][:500],df_train[\"Feature_2\"][:500], label=\"Class A\", edgecolor = \"b\", s = 60, alpha = 0.8)\r\nplt.scatter(df_train[\"Feature_1\"][500:],df_train[\"Feature_2\"][500:], label=\"Class B\", marker=\"^\", edgecolor = \"r\", s =60, alpha = 0.8)\r\nplt.legend()\r\nplt.show()","d3ceaf32":"X = df_train[[\"Feature_1\",\"Feature_2\"]]\r\nY = df_train[[\"class\"]]\r\nX = np.hstack((np.ones((1000,1)),X.to_numpy()))\r\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y.to_numpy(), test_size=0.2, random_state=42)\r\nprint(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)","f5639176":"def hypothesis(x,theta):\n    sigmoid=(1.0\/(1.0 + np.exp(-1.0*np.dot(x,theta))))\n    return(sigmoid)\n\ndef error(X,Y,theta):\n    m=X.shape[0]\n    err=0\n    for i in range(m):\n        hx=hypothesis(X[i],theta)\n        err+=Y[i]*np.log2(hx) + (1-Y[i])*np.log2(1-hx)\n    err \/=m\n    return(-err)\n\ndef gradient(X,Y,theta):\n    grad=np.zeros((X.shape[1]))\n    m=X.shape[0]\n    fea=X.shape[1]\n    for i in range(m):\n        hx=hypothesis(X[i],theta)\n        for j in range(fea):\n            grad[j]+=(hx-Y[i])*X[i,j]\n    grad=grad\/m\n    return(grad)\n    \ndef gradient_ascent(X,Y,learning_rate=0.5):\n    theta=2*np.random.random(X.shape[1])\n    theta[0]=0\n    error_list=[]\n    acc_list=[]\n    theta_list = []\n    for i in range(100):\n        grad=gradient(X,Y,theta)\n        err=error(X,Y,theta)\n        error_list.append(err)\n        acc_list.append(accuracy(X,Y,theta))\n        theta_list.append(theta.copy())\n        for j in range(X.shape[1]):\n            theta[j]-=learning_rate*grad[j]\n    probabilty_list = predic_proba(X, theta)\n    return(theta, theta_list, error_list, acc_list, probabilty_list)\n\ndef predict(x,theta):\n    p=hypothesis(x,theta)\n    if p<0.5:\n        return 0\n    else:\n        return 1\n\ndef predic_proba(x,theta):\n    probabilty_list = []\n    for i in range(X.shape[0]):\n        probability = hypothesis(X[i],theta)\n        probabilty_list.append(probability)\n    return probabilty_list\n\n\ndef accuracy(X,Y,theta):\n    y_pred=[]\n    for i in range(X.shape[0]):\n        p=predict(X[i],theta)\n        y_pred.append(p)\n    y_pred=np.array(y_pred)\n    y_pred=y_pred.reshape((-1,1))\n    return(Y==y_pred).sum()\/X.shape[0]","310c33a0":"theta, theta_list, error_list, acc_list, probabilty_list=gradient_ascent(X_train,Y_train)","50cdab72":"## For Contributing, refer to expectedoutput2.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.plot(error_list)\r\nplt.title(\"Error ,i.e Negative of maximum likelihood\")\r\nplt.show()","415cf959":"## For Contributing, refer to expectedoutput3.html in the expected outputs folder.\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x= list(range(1, 101)), \n    y=acc_list, \n    hovertemplate = '<br \/>Iteration: %{x} <br \/>Accuracy: %{y}<extra><\/extra>',\n    mode='lines+markers',\n    marker_color='rgba(68, 154, 191, 0.3)',\n    marker_line_color='rgba(17, 5, 247, 0.82)',\n    marker_size = 6,\n    marker_line_width = 1,\n    \n))\n\n\nfig.update_layout(\n    title=\"Visualising the Accuracy\",\n    xaxis_title = \"Iteration\",\n    yaxis_title = \"Accuracy\",\n    width=800,\n    height=800\n)\nfig.show()","ddf3b7dc":"print(\"The accuracy for the algorithm is:\",acc)\r\nprint(\"The final theta parameters calculated are:\",theta)","5a31a333":"## For Contributing, refer to expectedoutput4.html in the expected outputs folder.\r\n","4f09e438":"## For Contributing, refer to expectedoutput5.html in the expected outputs folder.\r\nx_val = np.linspace(-3,7,10)\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_train[\"Feature_1\"][:500],\n    y=df_train[\"Feature_2\"][:500],\n    name=\"Class A\",\n    mode=\"markers\",\n    marker_color='rgba(39, 175, 250, 0.25)',\n    marker_line_color='rgba(14, 61, 250, 1)',\n    marker_size = 9,\n    marker_line_width = 1,\n    marker_symbol='circle',\n    hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra> Class A<extra><\/extra>',\n))\nfig.add_trace(go.Scatter(\n    x=df_train[\"Feature_1\"][500:],\n    y=df_train[\"Feature_2\"][500:],\n    name=\"Class B\",\n    mode=\"markers\",\n    marker_color='rgba(255, 148, 38, 0.8)',\n    marker_line_color='rgba(255, 43, 0, 0.8)',\n    marker_size = 9,\n    marker_line_width = 1,\n    marker_symbol='star-triangle-up',\n    hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra> Class B<extra><\/extra>'\n))\nfig.add_trace(go.Scatter(x=x_val, y=-1*(theta[0]+x_val*theta[1])\/theta[2],\n                    mode='lines',\n                    name='Decision Boundry',\n                    hoverinfo='skip',\n                    \n                    ))\nfig.update_layout(\n    title=\"Visualization of the decision boundary\",\n    xaxis_title=\"Feature_1\",\n    yaxis_title=\"Feature_2\",\n    width=800,\n    height=800,\n    xaxis_range = [-4,8],\n)\n\n\nfig.show()\nplt.show()","3b9a81ba":"## Visalising accuracy over test set","9309d014":"## Preparing training and test sets","7dc66d20":"## Visualising Error over training set","59fcf7c2":"For demonstration purposes, let us take a 2 dimensional dataset with tow features (Feature_1 and Feature_2) and consisting of two classes (Class A and Class B) having a distribution specifications as follows:\r\n\r\n**Class A:** The Class A is centred around the mean of (1,1) and has the covariance matrix [[1,-0.2],[-0.2,1]]\r\n\r\n**Class B:** The Class B is centred around the mean of (3,4) and has the covariance matrix [[1,0.1],[0.1,1]]\r\n\r\nDefintions: \r\n\r\n**Mean:** A Class with centre (x1, x2) as mean denotes that the average value along \"Feature_1\" is x1 and the average value along \"Feature_2\" is x2\r\n\r\nP.S: Since we would like to ensure that the outputs corrosponds to the desired output, we will also add the seed value of 42 while generating these distributions.\r\n\r\n","43f447d7":"## Training the model","dbde1d25":"## Visualising the decision boundry over iterations","67b42a1c":"# Importing important libraries","6e90e539":"# Plotting the decision boundry","a57b0473":"# Loading the training dataset","35bb4bb8":"This notebook is a part of the OneML_ContriHub the link to which can be found [here](https:\/\/github.com\/ContriHUB\/OneML_ContriHub)","83570065":"## Defining the model","d2fba9ec":"# Visualising the dataset","0c54fcb7":"# Machine Learning Model"}}