{"cell_type":{"ebf46fb8":"code","2178f516":"code","232334e6":"code","40f57c69":"code","b5cbed6f":"code","7fd38726":"code","5b32285f":"code","8c8023aa":"code","29ceed4f":"code","d05d14de":"code","45ff1053":"code","ca17d576":"code","8897b788":"code","2c25b0e3":"code","8931f416":"code","e1e8ed70":"code","2c870511":"code","e48a2760":"code","fa9d9dc4":"code","ce8844df":"code","5fca62e0":"code","d3e27359":"code","fa5e5fbc":"code","7a476082":"code","796d8e49":"code","3ef58240":"code","c1d4ea10":"code","ad3af13a":"code","f061dcdc":"code","e66f56ab":"code","4a802490":"code","cd060413":"code","9db89efb":"code","6b873f75":"code","c1038629":"code","ab1c4e84":"code","3ae3ffab":"code","cbd75a5a":"code","3175a172":"code","5506a6f5":"code","1f2b7817":"code","a88912b6":"code","d9caab8f":"code","3a298e7b":"markdown","76294bfa":"markdown","ac212844":"markdown","7d9b34b0":"markdown","803889c0":"markdown","bd89a2e0":"markdown","96ffb4c6":"markdown","35530bd9":"markdown","315016ca":"markdown","7c46535d":"markdown","adc77424":"markdown","7e718c61":"markdown","370501dc":"markdown","0afec7b0":"markdown","b0f69b08":"markdown","9c4134f8":"markdown","4f315371":"markdown","698262b8":"markdown","eaf5c54b":"markdown","c77b1e53":"markdown"},"source":{"ebf46fb8":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\n#scaling \nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n\n#models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nimport lightgbm as lgb","2178f516":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\", index_col=0)","232334e6":"train.shape","40f57c69":"train.sample(10)","b5cbed6f":"train.info()","7fd38726":"train[\"Cover_Type\"].value_counts()","5b32285f":"plt.figure(figsize = (10,7))\nsns.countplot(x = train[\"Cover_Type\"])\nplt.title(\"Count of Target (Cover_Type)\")","8c8023aa":"pca = PCA(n_components=2,whiten=True)\ntrain_pca = pca.fit_transform(train.drop(\"Cover_Type\",axis =1))","29ceed4f":"df_pca = pd.DataFrame(train_pca, columns = [\"pca_1\",\"pca_2\"])\ndf_pca[\"Target\"] = train[\"Cover_Type\"]","d05d14de":"df_pca.info()","45ff1053":"\"\"\"plt.figure(figsize = (15,10))\nsns.lmplot(data = df_pca, x = \"pca_1\", y = \"pca_2\", hue= \"Target\")\"\"\"","ca17d576":"\"\"\"sns.boxplot(data = df_pca)\"\"\"","8897b788":"cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Cover_Type']","2c25b0e3":"plt.figure(figsize = (15,10))\nsns.heatmap(train[cols].corr(), vmin = -1, vmax=1, annot = True, cmap=\"Spectral\") ","8931f416":"X = train.drop(\"Cover_Type\",axis =1)\ny = train[\"Cover_Type\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e1e8ed70":"def model_scoring(model, scaling):\n    pipe = Pipeline(steps = [ \n        (\"scaler\", scaling),\n        (\"model\",model)\n    ])\n    \n    pipe.fit(X_train,y_train)\n    print(\"model score:\",pipe.score(X_test,y_test))\n\n    y_pred= pipe.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    \n    return pipe","2c870511":"#reg_model = model_scoring(LogisticRegression(solver = \"sag\") ,RobustScaler() )","e48a2760":"#reg_model = model_scoring(LogisticRegression(solver = \"sag\") ,StandardScaler() )","fa9d9dc4":"# Aspect changes as this in in degrees - negative isnt neccesary\ntrain[\"Aspect\"][train[\"Aspect\"] < 0] += 360\ntrain[\"Aspect\"][train[\"Aspect\"] > 359] -= 360\n\ntest[\"Aspect\"][test[\"Aspect\"] < 0] += 360\ntest[\"Aspect\"][test[\"Aspect\"] > 359] -= 360","ce8844df":"# Distance \n\n# Manhhattan distance to Hydrology\ntrain[\"manhattan_dist_hydrology\"] = np.abs(train[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(train[\"Vertical_Distance_To_Hydrology\"])\ntest[\"manhattan_dist_hydrology\"] = np.abs(test[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(test[\"Vertical_Distance_To_Hydrology\"])\n\n# Euclidean distance to Hydrology\ntrain[\"euclidean_dist_hydrology\"] = (train[\"Horizontal_Distance_To_Hydrology\"]**2 + train[\"Vertical_Distance_To_Hydrology\"]**2)**0.5\ntest[\"euclidean_dist_hydrology\"] = (test[\"Horizontal_Distance_To_Hydrology\"]**2 + test[\"Vertical_Distance_To_Hydrology\"]**2)**0.5","5fca62e0":"## create column of count of soil types, and wilderness types\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)","d3e27359":"for x in train.columns:\n    if \"Soil\" in x:\n        if train[x].nunique() ==1:\n            print(x,train[x].nunique())\n    else:\n        pass","fa5e5fbc":"#drop Soil_Type7 and Soil_Type7 as they only have 1 value\n\ntrain.drop([\"Soil_Type7\",\"Soil_Type15\"],axis = 1, inplace= True )\ntest.drop([\"Soil_Type7\",\"Soil_Type15\"],axis = 1, inplace= True )","7a476082":"train.tail(5)","796d8e49":"#memory before\ntrain.info(memory_usage=\"deep\", max_cols=1)","3ef58240":"#memory before\ntest.info(memory_usage=\"deep\", max_cols=1)","c1d4ea10":"for col in train.columns:\n    print(f\"{col} min = {min(train[col])}, max = {max(train[col])}\")","ad3af13a":"for col in train.columns:\n    if train[col].dtype == \"float64\":\n        train[col]=pd.to_numeric(train[col], downcast=\"float\")\n    if train[col].dtype == \"int64\":\n        train[col]=pd.to_numeric(train[col], downcast=\"integer\")\n        \nfor col in test.columns:\n    if test[col].dtype ==\"float64\":\n        test[col] = pd.to_numeric(test[col], downcast=\"float\")\n    if test[col].dtype ==\"int64\":\n        test[col] = pd.to_numeric(test[col], downcast=\"integer\")","f061dcdc":"train.info(memory_usage=\"deep\", max_cols=1)","e66f56ab":"test.info(memory_usage=\"deep\", max_cols=1)","4a802490":"X = train.drop(\"Cover_Type\",axis =1)\ny = train[\"Cover_Type\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","cd060413":"std_scaler = StandardScaler()\nmin_scaler = MinMaxScaler()   # for Multinomial bayes - cant use negative values ","9db89efb":"std_scaler.fit(X_train)\nX_train= std_scaler.transform(X_train)\nX_test = std_scaler.transform(X_test)","6b873f75":"min_scaler.fit(X_train)\nX_train_mm= min_scaler.transform(X_train)\nX_test_mm = min_scaler.transform(X_test)","c1038629":"\"\"\"\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(solver = \"sag\", max_iter= 300 )))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('LightXGB', lgb.LGBMClassifier()))\nmodels.append(('RF', RandomForestClassifier(max_depth= 10)))\nmodels.append(('Ber', BernoulliNB()))\n#models.append(('SVM', LinearSVC(max_iter = 3000)))\n\n# evaluate models\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=3)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n\"\"\"","ab1c4e84":"\"\"\"\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()\"\"\"","3ae3ffab":"lgb_params = {\n    'model__boosting_type': [\"dart\"],\n    'model__n_estimators': [400],\n    \"model__learning_rate\": [0.01],\n    'model__num_leaves': [200],\n    'model__max_depth':[11]\n                   }","cbd75a5a":"kfold = StratifiedKFold(n_splits= 5, shuffle=True , random_state= 42)","3175a172":"def grid_scoring(model, params ):\n    \n    pipe = Pipeline(steps = [ \n        (\"scaler\", RobustScaler()),\n        (\"model\",model)\n    ])\n    \n    grid = GridSearchCV(estimator=pipe,\n                param_grid=params,\n                scoring = \"accuracy\",\n                cv = kfold)\n    \n    grid.fit(X_train,y_train)\n    \n    print(\"Best score: \", grid.best_score_)\n    print(\"Best params: \",grid.best_params_)\n    \n    y_pred= grid.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    \n    return grid","5506a6f5":"lgb_model = grid_scoring(lgb.LGBMClassifier(), lgb_params)","1f2b7817":"lgb_model.plot_importance()","a88912b6":"s_test = std_scaler.transform(test)","d9caab8f":"test_pred = lgb_model.predict(s_test)\n\nsub = pd.DataFrame(test_pred, columns=[\"Cover_Type\"])\nsub.set_index(test.index,inplace=True)\n\nsub.to_csv(\"submission.csv\")","3a298e7b":"### Correlation analaysis \nWe will exclude soil types as these are boolean values ","76294bfa":"# GridSearchCV\nUsing the best model above we will try difference parameters","ac212844":"# Downcasting \nWe note from train.info above that all columns are int64 \\\nWe should look at the min and max of the columns to check if they need full 64bit to store the integers ","7d9b34b0":"## Re-Split\nWith new features","803889c0":"### PCA analysis \nDue to the large number of features, creating pairplots or jointplots would not be feasible to analyse  \n\nWe however can decompose our features into 2 features using principal Component Analysis","bd89a2e0":"**Note**: Large number of outliers, scaling might improve our scoring ","96ffb4c6":"**Note**: Elevation seems to have the highest negative correlation to Cover_type although relativelty weak correlation ","35530bd9":"# Class Imbalance \n### Class 5 is missing from the classification report above! \nClass 5 only has one observation, which is not ideal. In a normal circumstances we would need to add more samples of Class 5 \\\nWhich is available in the [original data ](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction) \\\nHowever Kaggle doesnt allow us to submit with another dataset \n\nIm adverse to drop data so we shall keep it \n\nAlso due to imbalanced dataset, **accuracy** is usually a poort metric so we will use another (f1_weighted?) ","315016ca":"![image.png](attachment:97f75336-b8a0-4e8b-a220-719f126bac38.png)","7c46535d":"# Scaling","adc77424":"# Project Description and goals \nIn this months Tabular Playground we have a synthetic [forest cover dataset ](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data). \n\nMy initial process should be as follows: \n* Quick EDA - null values, duplicates, correlation, redundant feature analysis \n* Feature Engineering - can we create futher features?\n\nIn this notebook we will try to create a full pipeline as opposed to [last months notbook ](https:\/\/www.kaggle.com\/slythe\/tabular-playground-competition-nov-2021) where we seperated each step \\\n**Pipeline process:**\n* Scaling & splitting \n* Base model testing (trees, linear, ANN) \n* Model optimization \n\nScoring = accuracy","7e718c61":"# Feature Engineering and splitting \nI took alot of points from the below notebook all credit to GULSHAN MISHRA \n\nhttps:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering\/notebook","370501dc":"## Split","0afec7b0":"**Note**  Standardscaler and Robust scaler dont seem to make a difference","b0f69b08":"As per discussion \nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823","9c4134f8":"# Libraries ","4f315371":"## Quick Model - logistic Regression\nWe will use a linear model as our baseline and test the two scaler types: \n* Standard Scaler \n* Robust Scaler ","698262b8":"# Submission","eaf5c54b":"# EDA ","c77b1e53":"## Evaluation of Base models (cross validation scoring)"}}