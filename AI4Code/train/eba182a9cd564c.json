{"cell_type":{"dae4948e":"code","273464a2":"code","a45cd5d6":"code","1d70aaf1":"code","c99045f8":"code","c6db199b":"code","2ed8cffe":"code","ab5387d8":"markdown","15670263":"markdown","0c54ab03":"markdown","3df05fd5":"markdown"},"source":{"dae4948e":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\n\n##we use step \n\ndf = pd.read_csv(\"..\/input\/jena-climate-2009-2016\/jena_climate_2009_2016.csv\")\nprint(df.shape)\ndf.head()","273464a2":"all_cols = df.columns\nsns.set_style('darkgrid')\nfig, axs = plt.subplots(7,2 , figsize=(20,10))\n\ntitles = [\n    \"Pressure\",\n    \"Temperature\",\n    \"Temperature in Kelvin\",\n    \"Temperature (dew point)\",\n    \"Relative Humidity\",\n    \"Saturation vapor pressure\",\n    \"Vapor pressure\",\n    \"Vapor pressure deficit\",\n    \"Specific humidity\",\n    \"Water vapor concentration\",\n    \"Airtight\",\n    \"Wind speed\",\n    \"Maximum wind speed\",\n    \"Wind direction in degrees\",\n]\n\ncolors = [\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"red\",\n    \"purple\",\n    \"brown\",\n    \"pink\",\n    \"gray\",\n    \"olive\",\n    \"cyan\",\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"brown\",\n]\n\nfor i in range(len(titles)):\n    \n    if i<= 6:\n        axs[i,0].set_title(f'{titles[i]} vs Time');\n        axs[i,0].plot(range(len(df[all_cols[0]])), df[all_cols[i+1]],c = colors[i]);\n    if i > 6:\n        axs[i-7,1].set_title(f'{titles[i]} vs Time');\n        axs[i-7,1].plot(range(len(df[all_cols[0]])), df[all_cols[i+1]],c = colors[i]);\n        \n\nplt.tight_layout(pad=2)\nplt.show()\n\nfig, ax = plt.subplots(1,3, figsize = (26,10))\ndf_corr = df.corr()\nax[0].set_title('Correlation - Matrix')\nsns.heatmap(df_corr, square=True, annot=True, ax=ax[0]);\n\nax[1].set_title('T_(in DegC)_Distibution')\nsns.histplot(df[all_cols[2]], ax = ax[1], kde =True);\n\nax[2].set_title('Bivariate-Hist(T(in degC) \/ Pressure(in bar) )')\nsns.histplot(\n    df, x=all_cols[1], y=all_cols[2],\n    discrete=(False, False), log_scale=(False, False), ax= ax[2]\n);\n\nplt.show()\n","a45cd5d6":"df_train = df[['p (mbar)','T (degC)', 'VPmax (mbar)', 'VPdef (mbar)', 'sh (g\/kg)' ,'rho (g\/m**3)',\t'wv (m\/s)']] ##Selected Features\n\ndata = df_train.to_numpy()\nprint(data.shape)\n\n\ndef norm(data):\n    mean = data[:200000].mean(axis=0)\n    data -= mean\n    std = data[:200000].std(axis=0)\n    data \/= std\n    \n    return data\n\ndata = norm(data)\nprint(data.shape)\n    \ndef gen(data, past, delay, minn, maxx, shuffle=False, batch_size=128, rate=6):\n    if maxx is None:\n        maxx = len(data) - delay -1\n    i = minn + past\n    while True:\n        if shuffle:\n            rows = np.random.randint(minn + past, maxx, size = batch_size)\n        else:\n            if i + batch_size > maxx:\n                i = minn + past\n            rows=np.arange(i, min(i + batch_size, maxx))\n            i+=len(rows)\n            \n        samples = np.zeros((len(rows), past \/\/ rate, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - past, rows[j], rate) ## sample apoint every hour intead of every 10 mins\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets","1d70aaf1":"past = 720 ## ==>> looks 5 days back \nrate = 6 ## ==>> sample 1 point every hour\ndelay = 144 ## ==>> predict after 1 day\/24 hrs\n\ntrain_split, test_split = 0.60, 0.25\ntrain_max = round(train_split * data.shape[0])\nval_max = (1-train_split)*data.shape[0]\ntest_max = round(test_split*val_max)\nval_max = round(val_max - test_max)\n\nval_steps =  (train_max+val_max) - (train_max+1) - past ## (-past) because no target values for last  720 points\ntest_steps = (train_max+val_max+test_max)-(train_max+val_max+1) - past\n\nprint('Train_Max: ',round(train_max),', Val_Max: ',round(val_max),', Test_Max: ',round(test_max))\n\ntrain_gen = gen(data, past,delay, minn = 0, maxx = train_max, batch_size=128, shuffle=True)\nval_gen = gen(data, past, delay, minn = train_max+1, maxx = train_max + val_max, batch_size=128)\ntest_gen = gen(data, past, delay, minn = train_max+val_max+1, maxx = train_max+val_max+test_max, batch_size=128)\n\n\n","c99045f8":"def baseline(val_gen):\n    maes = []\n    for step, exe in enumerate(val_gen):\n        samples, targets = exe\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds-targets))\n        maes.append(mae)\n        if step == 2000:\n            break\n    return np.mean(maes)\n\ndef visual_loss(hist, base_mae):\n    loss = hist.history[\"loss\"]\n    val_loss = hist.history[\"val_loss\"]\n    epochs = range(len(loss))\n    sns.set_style('darkgrid')\n    plt.figure()\n    plt.plot(epochs, loss, \"1-b\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"1-r\", label=\"Validation loss\")\n    plt.plot(epochs, base_mae*np.ones(len(loss)), 'k-', label = \"CommonSense_Baseline_loss\")\n    plt.title('Loss_Curve')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n","c6db199b":"#'''\ninp = tf.keras.layers.Input(shape = (past \/\/ rate, data.shape[-1]))\ngru_1 = tf.keras.layers.GRU(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.3)(inp)\ngru_2 = tf.keras.layers.GRU(64, activation='relu',dropout=0.1, recurrent_dropout=0.3)(gru_1)\n\ndense = tf.keras.layers.Dense(1, activation='linear')(gru_2)\nmodel = tf.keras.models.Model(inputs = inp, outputs = dense)\n\nmodel.compile(loss=\"mae\", optimizer=\"Adam\")\nmodel.summary()\n\nse = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=5)\nhist = model.fit_generator(train_gen, steps_per_epoch=500, epochs=5, validation_data = val_gen, validation_steps= val_steps, callbacks=[se]) ## steps_per_epoch =\n#'''                                                                                                          ## number of batches passed during each epoch\nbase_mae = baseline(val_gen)\nprint('CommonSense_Baseline_MAE_normalized: ', base_mae)\nprint('CommonSense_Absoulte_MAE_(in degC)_baseline: ', base_mae*df[all_cols[2]].std(axis=0))\n#visual_loss(hist, base_mae)\nprint()\nprint('--'*50)\nprint()\nx_t, y_t = next(test_gen)\nprint(model.evaluate(x_t, y_t))\nprint('-')","2ed8cffe":"import random\ndef predict(seed, x_test, yreal):\n    c = seed\n    kx = x_test[c,:,1].tolist() ## every 120\/seq_length time-steps \/ point is going to have a single future point (here T in C on of the the given features)\n    pred = model.predict(x_test)\n    ky = yreal[c]\n    x_lin = range(len(kx))\n    plt.plot(x_lin, kx, '.-b', label = 'Test_data_past' )\n    plt.plot(x_lin[-1]+1, ky, 'kx', label = 'Real_point')\n    plt.plot(x_lin[-1]+1, pred[c], 'ro', label ='Predicted_point' )\n    plt.legend()\n    plt.xlabel('time')\n    plt.ylabel('Norm_Temp_(in DegC)')\n    plt.title('Prediction_on_Test_Data')\n    plt.show()\n\nbatch_size =128\nseed = 10\nnum = 5\n\nx_test, yreal = next(test_gen)\nprint('x-test: ', x_test.shape, 'yreal: ', yreal.shape)\n\nfor _ in range(num):\n    seed = random.randint(0, batch_size-2)\n    predict(seed, x_test, yreal)","ab5387d8":"<img style=\"width:100%; height:70%;\" src=\"https:\/\/www.newshub.co.nz\/dam\/form-uploaded-images-ordered\/2021\/04\/11\/heavy-rain-storm-clouds-warning-CREDIT-GETTY-520773327-1120.jpg\" \/>\n\n<img style=\"width:90px; height:90px; float:left;\" src=\"https:\/\/acegif.com\/wp-content\/gifs\/globe-17.gif\"><img style=\"width:90px; height:90px; float:right;\" src=\"https:\/\/acegif.com\/wp-content\/gifs\/globe-17.gif\"><h1 style=\"color:white; background-color:black; text-align:center;\"><b> Weather <\/b> <i> Forecasting <\/i><b> using <\/b><i> RNN <\/i><\/h1>","15670263":"<img style=\"width:100px; height:90px; float:left;\" src=\"https:\/\/thumbs.gfycat.com\/GoldenPleasantAmericancreamdraft-max-1mb.gif\"><img style=\"width:100px; height:90px; float:right;\" src=\"https:\/\/thumbs.gfycat.com\/GoldenPleasantAmericancreamdraft-max-1mb.gif\"><h2 style=\"color:white; background-color:black; text-align:center;\"><b><i> Inference <\/i><\/b><\/h2>","0c54ab03":"<img style=\"width:120px; height:90px; float:left;\" src=\"http:\/\/ibaseelectrosoft.com\/iBaseTech\/images\/training\/mlgf.gif\"><img style=\"width:120px; height:90px; float:right;\" src=\"http:\/\/ibaseelectrosoft.com\/iBaseTech\/images\/training\/mlgf.gif\"><h2 style=\"color:white; background-color:black; text-align:center;\"><b><i> Model - Creation <\/i><\/b><\/h2>","3df05fd5":"<img style=\"width:90px; height:90px; float:left;\" src=\"https:\/\/static.wixstatic.com\/media\/b008fb_8f7ff58ce8444721a4292503c9d2e054~mv2.gif\"><img style=\"width:90px; height:90px; float:right;\" src=\"https:\/\/static.wixstatic.com\/media\/b008fb_8f7ff58ce8444721a4292503c9d2e054~mv2.gif\"><h2 style=\"color:white; background-color:black; text-align:center;\"><b><i> Data-Visualization <\/i><\/b><\/h2>"}}