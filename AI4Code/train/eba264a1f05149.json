{"cell_type":{"e597bd6b":"code","c86d8749":"code","9b9acc00":"code","6c64a786":"code","3d614f8d":"code","e46d310a":"code","2cacc0d2":"code","143724fa":"code","cd1468cb":"code","9e93b322":"code","030035fa":"code","b486bc81":"code","35cc0d21":"code","321018f4":"code","c05ff87f":"code","cd79425d":"code","442a7a91":"code","0f7ba3f5":"code","906d0c00":"code","a96295a7":"code","ef219a76":"code","04817eb8":"code","f10a4c11":"code","747347e5":"code","cb6cca02":"code","2f2a6dd9":"code","da47c6f7":"code","35f3a71c":"code","04d55078":"code","eadbbeda":"code","443adcb2":"code","56cfd686":"code","d283814f":"code","8d613d53":"code","4f41f9f9":"code","ec9e4d86":"code","7d1a4e6d":"code","4f899583":"code","c72447e7":"code","ff18d7dd":"code","a6a00d5a":"code","78db5d25":"code","007fbf15":"code","47bca696":"code","fe812a76":"code","7993240e":"code","c619a9a3":"code","b784e945":"code","4c77891c":"code","2725b523":"code","c119d355":"code","28c0545e":"code","f97e4f67":"code","82b2237e":"code","39908329":"code","db00332c":"code","2f8a6cd8":"code","443c84cc":"code","b66b783d":"code","54a63f47":"code","0fb318fd":"code","78af2f83":"code","d96c6c66":"code","21350492":"code","4b13dfa3":"code","66f140f6":"code","879a7004":"code","d8e708c1":"code","2e603f45":"code","85aa37f0":"code","bdaf78e7":"code","a99787c7":"code","46c3cb6f":"code","492fbc81":"code","d81a5411":"code","22512689":"code","3b96dd4e":"code","1b6a97b3":"markdown","ee9cf4bf":"markdown","9e856fc1":"markdown","1318f8c7":"markdown","36ae7eeb":"markdown","0cbb0b94":"markdown","9ce4f4c3":"markdown","247046c9":"markdown","fcf3e309":"markdown","d7cc555b":"markdown","a64427b4":"markdown","a2a1086f":"markdown","88a1328a":"markdown","3240b576":"markdown","4b875c2f":"markdown","d249c401":"markdown","d7239632":"markdown","6d5affab":"markdown","13e148d2":"markdown","8cdbd8b6":"markdown","9e599bb5":"markdown","9f442e0f":"markdown","3872bb10":"markdown","678a52ef":"markdown","044b9655":"markdown","c8ba4be2":"markdown","c84098d5":"markdown","ef8ad179":"markdown","38c30170":"markdown","7a2c9b45":"markdown","4f3d8f88":"markdown","ebea735f":"markdown","1c5ec493":"markdown","534bdfcd":"markdown","826115ef":"markdown","20110c8d":"markdown","7887072e":"markdown","e99ef83e":"markdown","387ea6ce":"markdown","6a2d8bea":"markdown","702fdb8e":"markdown","24fcbab6":"markdown","7e8c258e":"markdown","778379fb":"markdown","912d99d6":"markdown","0e3eaae8":"markdown","c666b744":"markdown","0e572647":"markdown","fd5e76a0":"markdown","a16befd0":"markdown","6ebc0a74":"markdown","4b186135":"markdown","ae11879c":"markdown","ee4429b4":"markdown","340a750d":"markdown","4d5d6a6e":"markdown","88facb33":"markdown","f2bec7a7":"markdown","6e8b19ee":"markdown","c5427823":"markdown","195a8766":"markdown","7eac8ba9":"markdown","bd30b158":"markdown","c8703875":"markdown"},"source":{"e597bd6b":"debug = False","c86d8749":"debug2 = False","9b9acc00":"import numpy as np \nimport pandas as pd \nimport os\n       \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","6c64a786":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \n\nimport transformers\nimport random\n\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nscaler = torch.cuda.amp.GradScaler() # Acceleration on GPU.\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Automatically determine if cpu is gpu\ndevice","3d614f8d":"SEED = 533\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","e46d310a":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain.head(3)","2cacc0d2":"# 0th sentence example of excerpt\ntrain.excerpt[0]","143724fa":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest","cd1468cb":"sample = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsample","9e93b322":"# kaggle offline mode: submit is offline because internet is offline. It comes from the published dataset.\ntokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")\n\n# Please use this when using online such as local PC.\n# tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","030035fa":"test_s = train[\"excerpt\"].iloc[0]\ntest_s","b486bc81":"test_s = test_s.replace(\"\\n\",\"\")\ntest_s","35cc0d21":"result1 = tokenizer.encode_plus(test_s)\nresult1","321018f4":"tokenizer.decode(result1[\"input_ids\"])","c05ff87f":"if debug2:\n    sen_length = []\n\n    for sentence in tqdm(train[\"excerpt\"]):\n\n        token_words = tokenizer.encode_plus(sentence)[\"input_ids\"]\n        sen_length.append(len(token_words))\n\n    print('maxlenth of all sentences are  ', max(sen_length))","cd79425d":"test_s","442a7a91":"len(test_s.split(\" \"))","0f7ba3f5":"result2 = tokenizer.encode_plus(\n    test_s,\n    add_special_tokens = True, # Whether to insert [CLS], [SEP]\n    max_length = 314, # Align the number of words using padding and transcription\n    pad_to_max_length = True, # Put [PAD] in the blank area\n    \n    truncation = True # Cutout function. For example, max_length10 is a function that makes only the first 10 characters. I got an alert if I didn't put it in, so I'll put it in\n)","906d0c00":"result2","a96295a7":"tokenizer.decode(result2[\"input_ids\"])","ef219a76":"result3 = tokenizer.encode_plus(\n    test_s,\n    add_special_tokens = True, # Whether to insert [CLS], [SEP]\n    max_length = 10, # Align the number of words using padding and transcription\n    pad_to_max_length = True, # Put [PAD] in the blank area\n    \n    truncation = True # Cutout function. For example, max_length10 is a function that makes only the first 10 characters. I got an alert if I didn't put it in, so I'll put it in\n)","04817eb8":"result3","f10a4c11":"max_sens = 314","747347e5":"train = train.sort_values(\"target\").reset_index(drop=True)\ntrain","cb6cca02":"train[\"kfold\"] = train.index % 5","2f2a6dd9":"train","da47c6f7":"p_train = train[train[\"kfold\"]!=0].reset_index(drop=True)\np_valid = train[train[\"kfold\"]==0].reset_index(drop=True)","35f3a71c":"class BERTDataSet(Dataset):\n    \n    def __init__(self,sentences,targets):\n        \n        self.sentences = sentences\n        self.targets = targets\n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        sentence = sentence.replace(\"\\n\",\"\")\n        \n        bert_sens = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, \n                                max_length = max_sens, # set 314 from above code\n                                pad_to_max_length = True, \n                                return_attention_mask = True,\n                                truncation = True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n            \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': target\n            }","04d55078":"train_dataset = BERTDataSet(p_train[\"excerpt\"],p_train[\"target\"])\nvalid_dataset = BERTDataSet(p_valid[\"excerpt\"],p_valid[\"target\"])","eadbbeda":"train_dataset[0]","443adcb2":"train_batch = 16\nvalid_batch = 32","56cfd686":"train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=4,pin_memory=True)","d283814f":"for a in train_dataloader:\n    print(a)\n    break","8d613d53":"model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)","4f41f9f9":"model.to(device)\nmodel.train()","ec9e4d86":"for a in train_dataloader:\n    ids = a[\"ids\"].to(device)\n    mask = a[\"mask\"].to(device)\n    tokentype = a[\"token_type_ids\"].to(device)\n    \n    output = model(ids,mask)\n    break","7d1a4e6d":"output","4f899583":"output[\"logits\"]","c72447e7":"output[\"logits\"].shape","ff18d7dd":"output[\"logits\"].squeeze(-1)","a6a00d5a":"output[\"logits\"].squeeze(-1).shape","78db5d25":"output = output[\"logits\"].squeeze(-1)","007fbf15":"from transformers import AdamW\nLR=2e-5\noptimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) ","47bca696":"from transformers import get_linear_schedule_with_warmup\n\n\nepochs = 20\n\nif debug:\n    epochs = 1\n\ntrain_steps = int(len(p_train)\/train_batch*epochs)\nprint(train_steps)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","fe812a76":"if debug2:\n\n    le=[]\n    train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\n\n    for b in tqdm(range(epochs)):\n\n        for a in train_dataloader:\n            break\n            le.append(scheduler.get_last_lr())\n            scheduler.step()\n\n    x = np.arange(len(le))\n    plt.plot(x,le)","7993240e":"def loss_fn(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))","c619a9a3":"def training(\n    train_dataloader,\n    model,\n    optimizer,\n    scheduler\n):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n\n    allpreds = []\n    alltargets = []\n\n    for a in train_dataloader:\n\n        losses = []\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n            tokentype = a[\"token_type_ids\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward() # backwards of loss\n        scaler.step(optimizer) # Update optimizer\n        scaler.update() # scaler update\n\n        scheduler.step() # Update learning rate schedule\n        \n        del loss # effective for memory\n\n        # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return losses,train_rme_loss","b784e945":"if debug2:\n    train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\n    losses,train_rme_loss = training(train_dataloader,model,optimizer,scheduler)\n    \n    print(losses,train_rme_loss)","4c77891c":"def validating(\n    valid_dataloader,\n    model\n):\n    \n    model.eval()\n\n    allpreds = []\n    alltargets = []\n\n    for a in valid_dataloader:\n\n        losses = []\n\n        with torch.no_grad():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n            tokentype = a[\"token_type_ids\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss\n\n\n    # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return allpreds,losses,valid_rme_loss","2725b523":"if debug2:\n    allpreds,losses,valid_rme_loss = validating(valid_dataloader,model)\n    print(allpreds[:3])\n    print(losses)\n    print(valid_rme_loss)","c119d355":"def initialize(fold):\n    \n\n    # initializing the data\n\n    random_seed(SEED)\n\n    p_train = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n    p_valid = train[train[\"kfold\"]==fold].reset_index(drop=True)\n\n\n    train_dataset = BERTDataSet(p_train[\"excerpt\"],p_train[\"target\"])\n    valid_dataset = BERTDataSet(p_valid[\"excerpt\"],p_valid[\"target\"])\n\n    train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=4,pin_memory=True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\n    model.to(device)\n    LR=2e-5\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\n    train_steps = int(len(p_train)\/train_batch*epochs)\n\n    num_steps = int(train_steps*0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    scaler = torch.cuda.amp.GradScaler() \n    \n    return train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler\n","28c0545e":"train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler = initialize(0)","f97e4f67":"trainlosses = []\nvallosses = []\nbestscore = None\n\ntrainscores = []\nvalidscores = []\n\nfor epoch in tqdm(range(epochs)):\n    \n    print(\"---------------\" + str(epoch) + \"start-------------\")\n    \n    trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n    \n    trainlosses.append(trainloss)\n    trainscores.append(trainscore)\n    \n    print(\"trainscore is \" + str(trainscore))\n    \n    preds,validloss,valscore=validating(valid_dataloader,model)\n    \n    vallosses.append(validloss)\n    validscores.append(valscore)\n\n    \n    print(\"valscore is \" + str(valscore))\n    \n    if bestscore is None:\n        bestscore = valscore\n        \n        print(\"Save first model\")\n        \n        state = {\n                        'state_dict': model.state_dict(),\n                        'optimizer_dict': optimizer.state_dict(),\n                        \"bestscore\":bestscore\n                    }\n            \n\n        torch.save(state, \"model0.pth\")\n        \n    elif bestscore > valscore:\n        \n        bestscore = valscore\n        \n        print(\"found better point\")\n        \n        state = {\n                        'state_dict': model.state_dict(),\n                        'optimizer_dict': optimizer.state_dict(),\n                        \"bestscore\":bestscore\n                    }\n            \n\n        torch.save(state, \"model0.pth\")\n        \n    else:\n        pass\n    ","82b2237e":"plt.scatter(p_valid[\"target\"],preds)","39908329":"x = np.arange(epochs)\nplt.plot(x,trainlosses)\nplt.plot(x,vallosses)","db00332c":"x = np.arange(epochs)\nplt.plot(x,trainscores)\nplt.plot(x,validscores)","2f8a6cd8":"# For other k-fold\nbestscores = []\nbestscores.append(bestscore)","443c84cc":"for fold in range(1,5):\n    \n\n    \n    train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler = initialize(fold)\n\n\n    trainlosses = []\n    vallosses = []\n    bestscore = None\n\n    trainscores = []\n    validscores = []\n\n    for epoch in tqdm(range(epochs)):\n\n        print(\"---------------\" + str(epoch) + \"start-------------\")\n\n        trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n\n        trainlosses.append(trainloss)\n        trainscores.append(trainscore)\n\n        print(\"trainscore is \" + str(trainscore))\n\n        preds,validloss,valscore=validating(valid_dataloader,model)\n\n        vallosses.append(validloss)\n        validscores.append(valscore)\n\n\n        print(\"valscore is \" + str(valscore))\n\n        if bestscore is None:\n            bestscore = valscore\n\n            print(\"Save first model\")\n\n            state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestscore\n                        }\n\n\n            torch.save(state, \"model\" + str(fold) + \".pth\")\n\n        elif bestscore > valscore:\n\n            bestscore = valscore\n\n            print(\"found better point\")\n\n            state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestscore\n                        }\n\n\n            torch.save(state, \"model\"+ str(fold) + \".pth\")\n\n        else:\n            pass\n\n\n    bestscores.append(bestscore)\n","b66b783d":"bestscores","54a63f47":"np.mean(bestscores)\nprint(\"my cv is \" + str(np.mean(bestscores)))","0fb318fd":"import gc\ndel train_dataset,valid_dataset,train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler\n_ = gc.collect()","78af2f83":"test","d96c6c66":"class BERTinfDataSet(Dataset):\n    \n    def __init__(self,sentences):\n        \n        self.sentences = sentences\n       \n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        \n        sentence = sentence.replace(\"\\n\",\"\")\n        \n        \n        bert_sens = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, # [CLS],[SEP]\n                                max_length = 314,\n                                pad_to_max_length = True, # add padding to blank\n                                truncation=True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n        \n    \n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                \n            }","21350492":"test_dataset = BERTinfDataSet(test[\"excerpt\"])","4b13dfa3":"test_batch = 32","66f140f6":"test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=4,pin_memory=True)","879a7004":"model = transformers.BertForSequenceClassification.from_pretrained('..\/input\/bert-base-uncased',num_labels=1)","d8e708c1":"pthes = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if \".pth\" in s]\npthes","2e603f45":"def predicting(\n    test_dataloader,\n    model,\n    pthes\n    \n):\n\n    allpreds = []\n    \n    for pth in pthes:\n        \n        state = torch.load(pth)\n        \n        model.load_state_dict(state[\"state_dict\"])\n        model.to(device)\n        model.eval()\n    \n    \n        preds = []\n        allvalloss=0\n\n        with torch.no_grad():\n\n\n            for a in test_dataloader:\n\n\n\n                ids = a[\"ids\"].to(device)\n                mask = a[\"mask\"].to(device)\n                tokentype = a[\"token_type_ids\"].to(device)\n\n               # output = model(ids,mask,tokentype)\n                output = model(ids,mask)\n\n                output = output[\"logits\"].squeeze(-1)\n\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            \n            allpreds.append(preds)\n\n    return allpreds","85aa37f0":"allpreds = predicting(test_dataloader,model,pthes)","bdaf78e7":"findf = pd.DataFrame(allpreds)\nfindf = findf.T","a99787c7":"findf","46c3cb6f":"finpred = findf.mean(axis=1)\nfinpred","492fbc81":"sample","d81a5411":"sample[\"target\"] = finpred","22512689":"sample","3b96dd4e":"sample.to_csv(\"submission.csv\",index = False)","1b6a97b3":"# This notebook has a sequel.\n## please refer and I'm happy if you upvote.\n\n1) why my score got worse from 0.528(ver6) to 0.546(ver15) and how I could improve it back to 0.528(ver19) in this notebook.\n\nHow to initialize the code correctly (English&\u65e5\u672c\u8a9e): https:\/\/www.kaggle.com\/chumajin\/how-to-initialize-the-code-correctly-english\n\n2) BERT and RoBERTa were compared with various random seeds in this model.\n\nBERT v.s. RoBERTa (English & \u65e5\u672c\u8a9e) : https:\/\/www.kaggle.com\/chumajin\/bert-v-s-roberta-english\/notebook#About-this-notebook","ee9cf4bf":"#### Therefore, output is used in this form.","9e856fc1":"* BertModel\n* BertForPreTraining\n* BertForMaskedLM\n* BertForNextSentencePrediction\n* BertForSequenceClassification\n* BertForMultipleChoice\n* BertForTokenClassification\n* BertForQuestionAnswering\n\nThis page explains it.\n\nhttps:\/\/kento1109.hatenablog.com\/entry\/2019\/08\/20\/161936 @ Japanese\n\nFrequently, you can hide a part of a sentence to make a prediction or judge whether there is a connection between sentences.\n\n\nThis time, I used BertForSequenceClassification.\n\n* At first I was using BertModel, but I couldn't improve the score. .. ..\nBy default, there are two outputs so that they are classified into two, such as 0 and 1, but this time it is a regression problem, so I will set the output (num_labels) to 1.\nIf you want to have a classification problem, you should adjust here.","1318f8c7":"#### The following is the learning rate scheduler built into the transformer (the learning rate changes with each step).\n#### In the recommendation of the site above, at 20 epoch, at the first 10%, after reaching the target learning rate, it was said that it attenuates to 0, so I made it.","36ae7eeb":"#### From ver 8, if you want to submit only this notebook, set debug2 to false. I'll omit some code. ","0cbb0b94":"# 1. EDA for this competition","9ce4f4c3":"As the output of Tokenizer, it comes out in dictionary type, and there are the following three\n* input_ids: Word id (the one in BERT's pretrained model) * In detail, 101 [CLS] and 102 [SEP] are added at the beginning and end.\n* token_type_ids: Binary mask to grasp sentences: This time it is a regression problem, all 0. When looking at the connection between sentences, change it by inserting [SEP] etc. in the middle.\n* attention_mask: Binary mask to judge embedding: For example, this time, we will substitute [PAD] to match the number of characters later, but we will judge it.\nYou may understand if you can see this person. https:\/\/qiita.com\/omiita\/items\/72998858efc19a368e50  @ Japanese","247046c9":"#### It is necessary to fill in less than 314 sentences with [PAD].","fcf3e309":"#### Since the loss function is RMSE this time, I defined it as follows.","d7cc555b":"#### Put the above optimizer and loss and make training a function.\n#### I'm using AMD's autocast (), so I'm writing it that way.\n\n\nhttps:\/\/qiita.com\/Sosuke115\/items\/40265e6aaf2e414e2fea  @ Japanese\n\nThe image is easier to do here\n\nhttps:\/\/qiita.com\/sugulu_Ogawa_ISID\/items\/62f5f7adee083d96a587 @ Japanese","a64427b4":"#### The number of epochs is set to 20 epoch when debug = False and 1 when True. If you want to know only the flow, please set it to True.","a2a1086f":"#### Predict the target of test data and submit it as a submission file.","88a1328a":"## [Important] In BERT, it is necessary to have the same number of words in each sentence.\n\n\n## (Max of pretrained model is 512)","3240b576":"# 8. inference\n#### For commercials, I will score on this notebook, but it will take time to submit, so it is better to do it on another notebook.\n#### \u203b\u3000Because this notebook includes training, so it takes much more than inference only.","4b875c2f":"#### Model Loading","d249c401":"#### Make sure it's done properly","d7239632":"# 4. BERT modeling","6d5affab":"#### Avarage the 5 model and making submission.","13e148d2":"## 3.2 Build DataSet and DataLoader (same as Neural network that handles numbers)","8cdbd8b6":"  ### I'm looking forward to helping you even a little. please upvote, thank you!\n  ### Also, thank you for those who always upvote.\n  \n  ### The reference url in this notebook is mostly Japanese. If you are interested, please translate in your browser.  ","9e599bb5":"#### Make sure it's done properly","9f442e0f":"# 2. BERT: Deepen your understanding of Tokenizer\n##      It makes English words id and various things","3872bb10":"### The model input was output by tokenizer (word id (input_ids), attention_mask (mask))\nIn this case, token_type_id seems to be OK with or without it.\n\nTake a look at the contents of output","678a52ef":"#### First, let's count the number of words in the longest sentence in this train data. \n#### In ver19, in order to submit this notebook with avoiding time exceeding, I omit this part by debug2 = False. It will take some time.","044b9655":"#### This time, we will use the maximum length of 314. (Hereafter, converted in Dataset)","c8ba4be2":"https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n\n\nThis person shares how to make it, but here we simply sort it in target order and divide it into 5 folds in order.","c84098d5":"#### When shortening the sentence (I may adjust this area if it takes a lot of calculation time. For example, try 10; some people use 256 for other notebooks.)","ef8ad179":"#### Let's return to the sentence from the word id of input_ids.","38c30170":"#### If you feed the tokenizer encode_plus without thinking about anything","7a2c9b45":"#### It can be seen that [PAD] has been added to make the length of the sentences uniform.\n#### 0 in Attention mask means to hide it and ignore it","4f3d8f88":"#### prediction function","ebea735f":"#### Google translate is amazing ... thank you for google.\n#### And Thanks to the staff of this competition.","1c5ec493":"#### 314 words is the maximum number of words in this train data. However, it includes [CLS] and [SEP].","534bdfcd":"# 0. Preparation","826115ef":"#### If you adjust the argument of tokenizer, it will be adjusted automatically.","20110c8d":"#### ver 21 : remove \\n in order to get better score.","7887072e":"#### It will take some time, but if you look at the transition of the learning rate for each iteration, it looks like this.\n#### In ver19, in order to submit this notebook with avoiding time exceeding, I omit this part by debug2 = False. It will take some time.","e99ef83e":"# 3. Preparation of Pytorch neural network\n## 3.1 k-fold","387ea6ce":"#### Also create validation version.","6a2d8bea":"As a reference, the results show like this.\n\n![image.png](attachment:aa166e24-2d53-4755-8ee9-1206eb1a2bd3.png)","702fdb8e":"If debug2 is set to True, the following result will be displayed.\n\n![image.png](attachment:157229b9-8cbf-4ae6-936e-8703e53a3a41.png)","24fcbab6":"#### You can use it as it is","7e8c258e":"### I've done a lot for explanation, so I'll initialize the parameter.\n### From ver 20, this method is using how to make repeatable results from my experiment. \n### Please see the this shared notebook https:\/\/www.kaggle.com\/chumajin\/how-to-initialize-the-code-correctly-english","778379fb":"#### Since it is an inference, there is no target, so I omitted it","912d99d6":"# 5. Creating a function to train\n\n#### Define optimizer\n* This time, I tried various things and learned the importance of the learning rate (at first, the learning rate was too high, and all the results predicted by regression were the same value, or I got stuck for more than a day ... .)\n\n* For the time being, this notebook has adopted some parameters for Bert's fine-tuning instability, with reference to https:\/\/ai-scholar.tech\/articles\/bert\/bert-fine-tuning. English page of this paper is https:\/\/arxiv.org\/abs\/2006.04884.\n\n* However, in the above literature, it is a story of bert-large-uncased. This time, it took too long to turn 5 k-fold 20 epoch, so I chose bert-base-uncased.\n\n* It's fine, so if you want to use it for the time being, you can ignore it and just let it flow.","0e3eaae8":"#### 1epoch test \n#### In ver19, in order to submit this notebook with avoiding time exceeding, I omit this part by debug2 = False. It will take some time.","c666b744":"#### BERT has various pretrained models depending on the application","0e572647":"#### Try to return to the sentence from the word id","fd5e76a0":"#### There are various models of bert, but this time I used bert-base-uncased.\n#### There is also bert-large-uncased, but the number of dimensions of the embedded vector in BERT is different.\n#### There is a bert-large-uncased on https:\/\/www.kaggle.com\/xhlulu\/huggingface-bert, so you can use it if you change the input there.","a16befd0":"#### The difficulty level of the readability of excerpt sentences is entered in target, and it is a competition to predict it (very simple)","6ebc0a74":"--------That said, I couldn't publish it without getting a certain score, so here's a memo of the points I devised. --------\n\n  (Although it is said to be for beginners, it is a maniac. I will explain it later.)\n* I got a score using BertForSequenceClassification rather than using BertModel, so I chose it.\n* Regarding the instability of Bert's fine-tuning, we adopted some parameters by referring to https:\/\/ai-scholar.tech\/articles\/bert\/bert-fine-tuning.\n* English page of this paper is https:\/\/arxiv.org\/abs\/2006.04884.\n\n     \n    \u203b\u3000Since it takes time for 20 epoch and 5 k-folds in the Large-model, I adopted the base uncased model only there.","4b186135":"#### First, let's try and set kfold = 0 as validation and the others as train data. Note that it will be stuck without reset_index ().","ae11879c":"#### Guessed values are coming out properly. Where you want to use logits","ee4429b4":"# 6. Do training","340a750d":"#### Random seed fixation","4d5d6a6e":"#### A lot of 0s are added after 102 of input_ids. Attention_mask also has 0 added, although it was all 1 earlier.","88facb33":"# About this notebook \n ### This is beginner's room of pytorch BERT\n \n \n ### Rather than aiming for a score, I aim to:\n \n*   Understanding deeply.\n*   Natinal Language processing is performed in the same way as a neural network of numbers.\n        \n       If you have never built a pytorch neural network, this is recommended first. \n       \n       https:\/\/www.kaggle.com\/chumajin\/pytorch-neural-network-starter-detail\n       \n       \n\n      \n        \n\n\n*   Create a BERT template.\n  ","f2bec7a7":"#### Looking at the shape, 1 is superfluous, so if you use squeeze, it will cut 1","6e8b19ee":"# 7. I will continue to turn other K-folds.","c5427823":"#### I will play around with id 0 using tokenizer.","195a8766":"#### The test has no target and standard error","7eac8ba9":"# Thank you for watching so far!\n\n# If you find it useful, I would appreciate it if you could upvote it.\n## Also, I learned a lot from the people at each reference URL. thank you very much!","bd30b158":"#### How to make a dataloader (pin_memory, num_worker, AMP that will come later) is explained here in an easy-to-understand manner.\n\nhttps:\/\/qiita.com\/sugulu_Ogawa_ISID\/items\/62f5f7adee083d96a587\u3000 @ Japanese page\n","c8703875":"#### You can see that [CLS] indicating the beginning and [SEP] indicating the end have been added.\n#### These two are called Special tokens (default, which has the option of including or not including the tokenizer argument, is included)."}}