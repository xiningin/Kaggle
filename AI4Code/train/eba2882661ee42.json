{"cell_type":{"f63e6f0b":"code","0adb5fa1":"code","4e85e880":"code","62ff5004":"code","18ac802f":"code","62b8a70b":"code","76710b03":"code","9a568370":"code","44377f3a":"code","6fbc7060":"code","510c9862":"code","958e0c61":"code","95a8811a":"code","df9e93b3":"code","dded4c00":"code","41ed54f0":"code","03ba1993":"code","51a6bbdc":"code","4da3bdac":"code","1e2b1087":"code","aa4423cb":"code","c157c962":"code","f3337f93":"code","baf30f9f":"code","d44fa5e7":"code","f4980110":"code","92c102a1":"code","0f566187":"code","fa19c3fc":"code","570a896a":"code","5fecc2ef":"code","264eab07":"code","48ca18aa":"markdown","8cf64265":"markdown","f2bd5f62":"markdown","432bf0b0":"markdown","66a99791":"markdown","c6aa83bc":"markdown","065f7be0":"markdown","4a77b6be":"markdown","f17b2c80":"markdown","86fb9a4f":"markdown","87252901":"markdown","8330d939":"markdown","39a8f96f":"markdown","bc860cf0":"markdown","4dc25fa9":"markdown","26f60e86":"markdown","0ea5e5ea":"markdown","39ac43d5":"markdown","e22f260e":"markdown"},"source":{"f63e6f0b":"%config Completer.use_jedi = False","0adb5fa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n","4e85e880":"train_data.head(10)","62ff5004":"train_data['Sex'] = (train_data['Sex'] == 'female').astype('int')","18ac802f":"print(train_data['Embarked'].unique())\ntrain_data['Embarked'] = train_data['Embarked'].replace(to_replace=['S', 'C', 'Q', np.nan], value=['0','1','2','-1']).astype('int')","62b8a70b":"train_data.head()","76710b03":"train_data.hist(figsize=(10,10))","9a568370":"train_data.corr()","44377f3a":"train_data.corr()['Survived'].sort_values(ascending=False)","6fbc7060":"x  = scatter_matrix(train_data, figsize=(10,10))","510c9862":"pd.pivot_table(train_data, index='Survived')","958e0c61":"total = len(train_data)\ns = sum(train_data['Survived'])\nprint(f\"out of {total} passengers, {s} survived.\")\nprint(f\"survival rate: {s\/total * 100:.2f}%\")","95a8811a":"train_data.head(10)[['Pclass', 'Survived']]","df9e93b3":"fig, axs = plt.subplots(1,2)\nsurvived = train_data['Survived'] == 1\ntrain_data.loc[survived]['Pclass'].hist(ax=axs[0])\naxs[0].title.set_text(\"survived\")\ntrain_data.loc[~survived]['Pclass'].hist(ax=axs[1])\naxs[1].title.set_text(\"dead\")\nfor i in range(1,4):\n    s = train_data.loc[train_data['Pclass'] == i]['Survived']\n    print(f\"from {len(s)} passengers of class {i}, {sum(s)} survived. Rate: {sum(s)\/len(s) * 100: .2f} %\")","dded4c00":"fig, axs = plt.subplots(1,2)\nsurvived = train_data['Survived'] == 1\ntrain_data.loc[survived]['SibSp'].hist(ax=axs[0])\naxs[0].title.set_text(\"survived\")\ntrain_data.loc[~survived]['SibSp'].hist(ax=axs[1])\naxs[1].title.set_text(\"dead\")\nfor i in range(max(train_data['SibSp'])):\n    s = train_data.loc[train_data['SibSp'] == i]['Survived']\n    print(f\"from {len(s)} passengers with {i} siblings, {sum(s)} survived. \") \n    if len(s) > 0:\n        print(f\"Rate: {sum(s)\/len(s) * 100: .2f} %\")","41ed54f0":"fig, axs = plt.subplots(1,2)\nsurvived = train_data['Survived'] == 1\ntrain_data.loc[survived]['Parch'].hist(ax=axs[0])\naxs[0].title.set_text(\"survived\")\ntrain_data.loc[~survived]['Parch'].hist(ax=axs[1])\naxs[1].title.set_text(\"dead\")\nfor i in range(max(train_data['Parch'])):\n    s = train_data.loc[train_data['Parch'] == i]['Survived']\n    print(f\"from {len(s)} passengers with {i} children\/parents, {sum(s)} survived. \") \n    if len(s) > 0:\n        print(f\"Rate: {sum(s)\/len(s) * 100: .2f} %\")","03ba1993":"fig, axs = plt.subplots(1,3)\nfor i in range(1,4):\n    ci_fare = train_data.loc[train_data['Pclass'] == i]['Fare']\n    print(\"class\",i)\n    print(\"\\tmean:\", ci_fare.mean())\n    print(\"\\tstd:\", ci_fare.std())\n    ci_fare.hist(ax = axs[i-1])\n    axs[i-1].title.set_text(\"class \" + str(i))","51a6bbdc":"for i in range(1,4):\n    ci = train_data.loc[train_data['Pclass'] == i]\n    print('class',i)\n    print('\\tcorr', ci.corr()['Survived']['Fare'])","4da3bdac":"cl1 = train_data.loc[train_data['Pclass'] == 1]\npd.pivot_table(cl1, index='Survived')","1e2b1087":"fig, axs = plt.subplots(1,2)\nsurvived = train_data['Survived'] == 1\ntrain_data.loc[survived]['Embarked'].hist(ax=axs[0])\naxs[0].title.set_text(\"survived\")\ntrain_data.loc[~survived]['Embarked'].hist(ax=axs[1])\naxs[1].title.set_text(\"dead\")\nfor i in range(3):\n    s = train_data.loc[train_data['Embarked'] == i]['Survived']\n    print(f\"from {len(s)} passengers that embarked in  {i}, {sum(s)} survived. \") \n    if len(s) > 0:\n        print(f\"Rate: {sum(s)\/len(s) * 100: .2f} %\")","aa4423cb":"train_data.head()","c157c962":"print('pclass:\\t',len(train_data[train_data['Pclass'].isna()]))\nprint('sex:\\t',len(train_data[train_data['Sex'].isna()]))\nprint('Age:\\t',len(train_data[train_data['Age'].isna()]))\nprint('SibSp:\\t',len(train_data[train_data['SibSp'].isna()]))\nprint('Parch:\\t',len(train_data[train_data['Parch'].isna()]))\nprint('Fare:\\t',len(train_data[train_data['Fare'].isna()]))\nprint('Embarked:',len(train_data[train_data['Embarked'] == -1]))\nprint('All',len(train_data))","f3337f93":"age = train_data['Age']\nmean = age.mean()\nstd = age.std()\nis_nan = age.isna()\nn_nan = sum(is_nan)\nrand_age = np.random.randint(mean - std, mean + std, size = n_nan)\ntrain_data['Age'].loc[is_nan] = rand_age\n\nprint('Age:\\t',len(train_data[train_data['Age'].isna()]))","baf30f9f":"train_data['Embarked'].loc[train_data['Embarked'] == -1] = np.random.randint(0,3, size = 2)\nprint('Embarked:',len(train_data[train_data['Embarked'] == -1]))","d44fa5e7":"train_data.head()","f4980110":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","92c102a1":"for data in [train_data, test_data]:\n    age = data['Age']\n    mean = age.mean()\n    std = age.std()\n    is_nan = age.isna()\n    n_nan = sum(is_nan)\n    if n_nan > 0:\n        rand_age = np.random.randint(mean - std, mean + std, size = n_nan)\n        data['Age'].loc[is_nan] = rand_age\n\n    print('Age:\\t',len(data[data['Age'].isna()]))\n\n    fare = data['Fare']\n    mean = fare.mean()\n    std = fare.std()\n    is_nan = fare.isna()\n    n_nan = sum(is_nan)\n    if n_nan > 0:\n        rand_fare = np.random.randint(mean - std, mean + std, size = n_nan)\n        data['Fare'].loc[is_nan] = rand_fare\n\n    print('Fare:\\t',len(data[data['Fare'].isna()]))\n","0f566187":"print('pclass:\\t',len(test_data[test_data['Pclass'].isna()]))\nprint('sex:\\t',len(test_data[test_data['Sex'].isna()]))\nprint('Age:\\t',len(test_data[test_data['Age'].isna()]))\nprint('SibSp:\\t',len(test_data[test_data['SibSp'].isna()]))\nprint('Parch:\\t',len(test_data[test_data['Parch'].isna()]))\nprint('Fare:\\t',len(test_data[test_data['Fare'].isna()]))\nprint('Embarked:',len(test_data[test_data['Embarked'] == -1]))\nprint('All',len(test_data))","fa19c3fc":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=53)\n# model = MLPClassifier(hidden_layer_sizes=(20),max_iter=10000)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\nout = pd.DataFrame(columns=['PassengerId','Survived'])\nout['PassengerId'] = test_data['PassengerId']\nout['Survived'] = predictions.astype('int')\nout.to_csv('submission_titanic_RF.csv', index=False)\n\nprint(\"done\")","570a896a":"(np.sum((model.predict(X) == y).astype(np.int))\/y.shape[0])","5fecc2ef":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\n\n\nclasifiers = [\n    (lambda : tree.DecisionTreeClassifier(), 'decision tree'),\n    (lambda : KNeighborsClassifier(n_neighbors=3), 'knn'),\n    (lambda : GaussianNB(), 'naive bayes'),\n    (lambda : svm.SVC(), 'SVM'),\n    (lambda : svm.NuSVC(gamma='auto'), 'non linear SVM'),\n    (lambda : MLPClassifier(hidden_layer_sizes=(20),max_iter=10000), 'NN'),\n    (lambda : RandomForestClassifier(n_estimators=100, max_depth=4, random_state=53), 'RandomForest')\n]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]\nX = pd.get_dummies(train_data[features])\n\nfor get_classifier,name in clasifiers:\n    acc = []\n    for i in range(5):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i*111)\n        c = get_classifier()\n        c.fit(X_train, y_train)\n        preds = c.predict(X_test)\n        acc.append(np.sum((preds == y_test).astype(np.int))\/y_test.shape[0])\n    print(\"accuracy\",name , \"best:\", max(acc) , \"avg:\", sum(acc)\/len(acc))\n\nX.head()\n","264eab07":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nfor get_classifier,name in clasifiers:\n    acc = []\n    for i in range(5):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i*111)\n        c = get_classifier()\n        c.fit(X_train, y_train)\n        preds = c.predict(X_test)\n        acc.append(np.sum((preds == y_test).astype(np.int))\/y_test.shape[0])\n    print(\"accuracy\",name , \"best:\", max(acc) , \"avg:\", sum(acc)\/len(acc))\n    \nX.head()","48ca18aa":"passengers that traveled with one or two spouses were more likely to survive","8cf64265":"## Ticket class","f2bd5f62":"It actually performs slightly better without fare on the train data.\nFrom the training performance, we can see that information hidden in fare, is probably also expressed somewhere else in the data.\nThanks to this, when we train a model without it, we get the same, or even better results. But if our train data sample is not descriptive enough, \nwhen we use our model to predict the outcome in the real world, this feature can help with the predictions that would normaly be wrong, and it is in fact the case with test data.","432bf0b0":"## Cabin\nIt is possible to unwrap the cabin number to coordinates on the ship, and check if there were specific areas where passengers were for instance trapped, but it's too much work for possibly low gain.","66a99791":"If you were from 3rd class, tough break...","c6aa83bc":"## Do all the prep for data.","065f7be0":"### Can we say anything about the fare?\nFare is definately correlated with Pclass, since the first class tickets tend to naturally be more pricy.\nBut is it a good indicator of anything else? If we include the information about Pclass, have we already included all information possible to get from fare?\nLet's check.","4a77b6be":"It's the same for those traveling with parents\/children.","f17b2c80":"# Histograms","86fb9a4f":"## Passenger Id\nUniformly distributed, so nothing to see here.","87252901":"# Imports and reading data","8330d939":"# Individual columns","39a8f96f":"# Data prep\nWe are going to disregard PassengerId, Name, Ticket number and Cabin. The rest will be taken into account. \n\nThere are 177 rows without information on Age. I will substitute the NaN values with normal distribution.\nThere are 2 rows without information on Embarking Place, I will input two random values there. The impact of these on the data is minimal.\n\n\nThen train the **random forest**.","bc860cf0":"# changing data to numeric values","4dc25fa9":"## Survived","26f60e86":"While for class 3 the effect is almost neglegable, for the 2nd and 1st class, people who paid more for the ticket were more likely to survive.\nIt does not mean that you should buy overpriced tickets, but it may mean that a person buying pricy tickets has a quality that also\nmade them more likely to survive. Maybe they felt more entitled to get to the life rafts, since they paid more for their tickets.\nThat is only a guess, and it does not matter, but the fare might be an important feature.","0ea5e5ea":"There is a definately a correlation between survival rates and sex, fare, and Pclass ","39ac43d5":"## Embarked","e22f260e":"RandomForest and RandomTree performed the best."}}