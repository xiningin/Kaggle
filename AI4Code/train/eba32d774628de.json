{"cell_type":{"5245cccc":"code","eeea3b1d":"code","22674972":"code","bab43276":"code","a38fdfad":"code","f8b2aef1":"code","c155f955":"code","3da3ff44":"code","5c4f2d91":"code","2eae69d8":"code","61bb43b2":"code","2e1873d0":"code","a07777ae":"code","59658dcd":"code","197a7bd7":"code","326affbf":"code","92b57fb0":"code","0ca36729":"code","373860a5":"code","88028f4a":"code","c0ecf1c3":"code","6ee4f320":"code","be829fb4":"code","6e0dca24":"code","67717c3c":"code","443bdde6":"code","f0539888":"code","f0f693fc":"code","5180b964":"code","b417dd1d":"code","aba5e8bc":"code","d4eafb31":"markdown","51452d2a":"markdown","221a40db":"markdown","853e1925":"markdown","0aa6d746":"markdown","ee8ebd01":"markdown","973efd1d":"markdown","0f58b3b5":"markdown","951bb5a7":"markdown","ed300f17":"markdown","5e0ed2aa":"markdown","eea989ba":"markdown","89283128":"markdown","fc34292c":"markdown","a6747ae9":"markdown","a21b6869":"markdown"},"source":{"5245cccc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split,\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eeea3b1d":"#### Import Data","22674972":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","bab43276":"df.head()","a38fdfad":"df.info()","f8b2aef1":"plt.figure(figsize=(10,6))\nsns.kdeplot(df['fixed acidity'], hue=df['quality'])\n#Fixed Activity data follows the normal distribution","c155f955":"plt.figure(figsize=(10,6))\nsns.distplot(df['volatile acidity'])\n#Volatile Acidity data follows the normal distribution.","3da3ff44":"df['citric acid'].value_counts()","5c4f2d91":"plt.figure(figsize=(10,6))\nsns.distplot(df['citric acid'])","2eae69d8":"df['residual sugar'].value_counts()","61bb43b2":"df.groupby('quality').mean()","2e1873d0":"df.describe()","a07777ae":"corr = df.corr()\ncorr","59658dcd":"plt.figure(figsize=(16,10))\nsns.heatmap(corr, linewidths=3, annot=True)","197a7bd7":"sc = StandardScaler()\n\ndf_scaled = pd.DataFrame(sc.fit_transform(df.drop('quality',axis=1)),columns=df.columns[:-1])\n\ndf_scaled.head()","326affbf":"X_train, X_test, y_train, y_test = train_test_split(df_scaled, df['quality'], test_size=0.3, random_state=100)","92b57fb0":"y_train.value_counts()","0ca36729":"def metrics(y_true, y_pred):\n    print('Confusion Matrix:\\n', confusion_matrix(y_true, y_pred))\n    print('\\n\\nAccuracy Score:\\n', accuracy_score(y_true, y_pred))\n    print('\\n\\nClassification Report: \\n', classification_report(y_true, y_pred))\n\n\ndef predictions(model,X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n        model.fit(X_train, y_train)\n        #predictions\n        train_pred = model.predict(X_train)\n        test_pred = model.predict(X_test)\n        actual = [y_train, y_test]\n        pred = [train_pred, test_pred]\n        for i in range(0,2):\n            if i==0:\n                print('----Train Metrics----')\n            else:\n                print('----Test Metrics----')\n            metrics(actual[i], pred[i])","373860a5":"lg = LogisticRegression(multi_class='ovr')\n\n","88028f4a":"predictions(lg)","c0ecf1c3":"knn = KNeighborsClassifier()","6ee4f320":"predictions(knn)","be829fb4":"nb = GaussianNB()\n\npredictions(nb)","6e0dca24":"dtree = DecisionTreeClassifier()\n\npredictions(dtree)","67717c3c":"bag = BaggingClassifier()","443bdde6":"predictions(bag)","f0539888":"rf = RandomForestClassifier()\n\npredictions(rf)","f0f693fc":"gb = GradientBoostingClassifier()\n\npredictions(gb)","5180b964":"xgb = XGBClassifier()\n\npredictions(xgb)","b417dd1d":"lgbm = LGBMClassifier()\npredictions(lgbm)","aba5e8bc":"cat = CatBoostClassifier()\n\npredictions(cat)","d4eafb31":"#### CAT Boost","51452d2a":"### Train Test Split","221a40db":"### Logistic Regression","853e1925":"#### Gradient Boosting","0aa6d746":"**Observations:**\n* Fixed acidity falls in same range for all quality level.\n* Lower the volatile acidity level, higher the quality of wine.\n* Higher the citric acid level, higher the quality of wine\n* There is no significant difference in residual sugar value between different quality levels.\n* Lower the chlorides level, higher the quality level of wine.\n* There is relationship between free sulfur dioxide and total sulfur dioxide.\n* Lower the pH value, higher the quality level of wine.\n* Higher the sulphates level, quality level of wine increases.\n* Alochol values falls in same range for all alcohol quality level.","ee8ebd01":"### Scaling","973efd1d":"#### Random Forest","0f58b3b5":"#### Naive Bayes","951bb5a7":"#### Decision Tree","ed300f17":"#### XG Boost","5e0ed2aa":"### Modelling","eea989ba":"**Observation:**\n* There is collinearity between multiple independent variable","89283128":"#### Bagging","fc34292c":"#### EDA","a6747ae9":"#### Light GBM","a21b6869":"### KNN"}}