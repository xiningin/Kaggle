{"cell_type":{"822c48db":"code","8b0b20a7":"code","95514f56":"code","e584823e":"code","65a43929":"code","e7234256":"code","9abc10c9":"code","d4c19f66":"code","e1ee32fc":"code","ca979eb5":"code","38703ffb":"code","59b34f81":"code","326c9b84":"code","78cf7b36":"code","212991d7":"code","10251c48":"code","15a0fc39":"code","c5c34a55":"code","13ec67d4":"code","8e74cc38":"code","c3860785":"code","d2d234e6":"code","8694186e":"code","90ddafe0":"code","c3bfeb34":"code","950ae51b":"code","144136ca":"code","71d563f1":"code","f46b9242":"code","cee2b617":"code","113cb9d8":"code","5c9dfa95":"code","494b3aa9":"code","941385dd":"code","9d304e56":"code","32f9e1b1":"code","11569e85":"code","6ea2187d":"code","53cba875":"code","07776ed8":"markdown"},"source":{"822c48db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","8b0b20a7":"import matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.base import get_data_home\n%matplotlib inline\n\ndata_home = get_data_home()\ntwenty_home = os.path.join(data_home, \"20news_home\")\n\nif not os.path.exists(data_home):\n    os.makedirs(data_home)\n    \nif not os.path.exists(twenty_home):\n    os.makedirs(twenty_home)\n    \n!cp ..\/input\/20-newsgroup-sklearn\/20news-bydate_py3* \/tmp\/scikit_learn_data","95514f56":"news_groups_train = fetch_20newsgroups(subset='train', shuffle=True, download_if_missing=False)\nnews_groups_test = fetch_20newsgroups(subset='test', shuffle=True, download_if_missing=False)\nx_train, y_train = news_groups_train.data, news_groups_train.target\nx_sp_train, x_sp_val, y_sp_train, y_sp_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\nx_test, y_test = news_groups_test.data, news_groups_test.target","e584823e":"print(news_groups_train.target_names)","65a43929":"print(\"count for data in 20newsgroups\", len(x_sp_train), len(x_sp_val), len(x_test))\nprint(\"count for train and validation data in 20newsgroups\", len(x_sp_train) + len(x_sp_val), \" and for test data\", len(x_test))\nprint(\"count for train data in 20newsgroups\", len(x_sp_train))\nprint(\"count for validation data in 20newsgroups\", len(x_sp_val))\nprint(\"count for test data in 20newsgroups\", len(x_test))","e7234256":"print(x_sp_train[10])","9abc10c9":"print(y_sp_train[10])","d4c19f66":"def show_distributation(data):\n    dict = {}\n    for index, name in enumerate(news_groups_train.target_names):\n        dict.setdefault(name, np.sum(data==index))\n    print(dict)\n    print(dict.keys())\n    print(dict.values())\n    \n    index = np.arange(len(news_groups_train.target_names))\n    plt.figure(figsize=(10,5))\n    plt.bar(index, dict.values())\n    plt.xticks(index, dict.keys(), rotation=90)\n    plt.title(\"category distributation\")\n    plt.xlabel(\"data count\")\n    plt.ylabel(\"data category\")\n    plt.show()","e1ee32fc":"show_distributation(y_sp_train)","ca979eb5":"show_distributation(y_sp_val)","38703ffb":"show_distributation(y_test)","59b34f81":"def show_words(data):\n    count = []\n    for f in data:\n        count.append(len(f.split()))\n    plt.figure(figsize=(10,5))\n    plt.hist(count, bins=20)\n    plt.title(\"words distributation\")\n    plt.xlabel(\"words count\")\n    plt.ylabel(\"words weight\")\n    plt.show()\n    \ndef show_chars(data):\n    count = []\n    for f in data:\n        count.append(len(f))\n    plt.figure(figsize=(10,5))\n    plt.hist(count, bins=20)\n    plt.title(\"chars distributation\")\n    plt.xlabel(\"chars count\")\n    plt.ylabel(\"chars count\")\n    plt.show()","326c9b84":"show_words(x_sp_train)","78cf7b36":"show_chars(x_sp_train)","212991d7":"from sklearn.feature_extraction.text import CountVectorizer\nfrom time import time\n# refer: http:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n# max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n# max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n# min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n# min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n    \nvectorizer = CountVectorizer(max_df=0.97, min_df=3,\n                                max_features=None,\n                                stop_words='english')\nt0 = time()\nvec_x_train = vectorizer.fit_transform(x_sp_train)\nprint(\"done in %0.3fs.\" % (time() - t0))","10251c48":"print(vec_x_train)","15a0fc39":"from sklearn.feature_extraction.text import TfidfVectorizer\n# refer: http:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\ntfidf_vectorizer = TfidfVectorizer(max_df=0.97, min_df=2,\n                                max_features=None,\n                                stop_words='english')\nt0 = time()\ntfidf_x_sp_train = tfidf_vectorizer.fit_transform(x_sp_train)\ntfidf_x_sp_val = tfidf_vectorizer.transform(x_sp_val)\n\ntfidf_x_train = tfidf_vectorizer.transform(x_train)\ntfidf_x_test = tfidf_vectorizer.transform(x_test)\nprint(\"done in %0.3fs.\" % (time() - t0))","c5c34a55":"print(tfidf_x_sp_train.shape)\nprint(tfidf_x_sp_val.shape)\nprint(tfidf_x_train.shape)\nprint(tfidf_x_test.shape)","13ec67d4":"print(tfidf_x_sp_train)","8e74cc38":"# refer: https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html\n# https:\/\/www.programcreek.com\/python\/example\/98848\/gensim.models.word2vec.Word2Vec\n# http:\/\/mattmahoney.net\/dc\/text8.zip\nimport os\nimport gensim\nfrom gensim.models import word2vec\nEMBEDDING_DIM = 100\n# if not os.path.isfile('data\/w2c\/word2vec_model.model'):\n#     sentences = word2vec.Text8Corpus('data\/w2c\/text8')\n#     word2vec_model = word2vec.Word2Vec(sentences, size=EMBEDDING_DIM, min_count=1, sg=0)\n#     word2vec_model.save('data\/w2c\/word2vec_model.model')\n#     print(\"word2vec_model is saved\")\nword2vec_model = word2vec.Word2Vec.load('..\/input\/word2vec-modelmodel\/word2vec_model.model')\nprint(\"word2vec_model is loaded\")","c3860785":"from sklearn.neural_network import MLPClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\ndef show_performance(model, x_train, y_train, x_val, y_val):    \n    results = {}\n    results['model_name'] = model.__class__.__name__\n    t0 = time()\n    model.fit(x_train, y_train)\n    results['train_time'] = time() - t0\n    t1 = time()\n    predicts = model.predict(x_train)\n    results['val_time'] = time() - t1\n    train_score = model.score(x_train, y_train)\n    val_score = model.score(x_val, y_val)\n    results['train_score'] = train_score\n    results['val_score'] = val_score\n    print(results)\n    ","d2d234e6":"lr = LogisticRegression(C=1.0, penalty='l2')\nshow_performance(lr, tfidf_x_sp_train, y_sp_train, tfidf_x_sp_val, y_sp_val)","8694186e":"svc = SVC(kernel='linear', C=0.5, gamma=0.9, random_state=0)\nshow_performance(svc, tfidf_x_sp_train, y_sp_train, tfidf_x_sp_val, y_sp_val)","90ddafe0":"gnb = MultinomialNB(alpha=0.5)\nshow_performance(gnb, tfidf_x_sp_train, y_sp_train, tfidf_x_sp_val, y_sp_val)","c3bfeb34":"#refer: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom multiprocessing import cpu_count\nfrom sklearn import feature_selection\n\nfs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=40)\ndef show_performance_with_gscv(name, model, x_train, y_train, params):\n    results = {}\n    results['model_name'] = name\n    gscv = GridSearchCV(model, params, cv=3, n_jobs=cpu_count()-1, return_train_score=True)\n#     x_train_fs = fs.fit_transform(x_train, y_train)\n    \n    gscv.fit(x_train, y_train)\n    \n    results['params'] = gscv.best_params_\n    results['train_time'] = np.mean(gscv.cv_results_['mean_fit_time'])\n    results['val_time'] = np.mean(gscv.cv_results_['mean_score_time'])\n    results['train_score'] = gscv.cv_results_['mean_train_score'][gscv.best_index_]\n    \n    # it is get fro train data set, could be taken as a val result\n    results['val_score'] = gscv.cv_results_['mean_test_score'][gscv.best_index_]\n    \n    results['best_model'] = gscv.best_estimator_\n    \n    return results","950ae51b":"params = {'C': [0.01, 1, 3]}\nrs_lr = show_performance_with_gscv('LogisticRegression', LogisticRegression(penalty='l2'), tfidf_x_train, y_train, params)\nprint(rs_lr)","144136ca":"params = {'C': [0.1, 1, 3], 'gamma':[0.5, 0.9]}\nrs_svc = show_performance_with_gscv('SVC', SVC(kernel='linear'), tfidf_x_train, y_train, params)\nprint(rs_svc)","71d563f1":"params = {'alpha': [0.0001, 0.01, 0.5, 0.95]}\nrs_nb = show_performance_with_gscv('NaiveBayes', MultinomialNB(), tfidf_x_train, y_train, params)\nprint(rs_nb)","f46b9242":"def autolabel(ax, rects, xpos='center'):\n        \"\"\"\n        Attach a text label above each bar in *rects*, displaying its height.\n\n        *xpos* indicates which side to place the text w.r.t. the center of\n        the bar. It can be one of the following {'center', 'right', 'left'}.\n        \"\"\"\n\n        xpos = xpos.lower()  # normalize the case of the parameter\n        ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n        offset = {'center': 0.5, 'right': 0.57, 'left': 0.43}  # x_txt = x + w*off\n\n        for rect in rects:\n            height = rect.get_height()\n            ax.text(rect.get_x() + rect.get_width()*offset[xpos], 1.01*height,\n                    '{}'.format(height), ha=ha[xpos], va='bottom')\n            \ndef show_metrics(rs_lr, rs_svc, rs_nb):\n#     https:\/\/matplotlib.org\/gallery\/lines_bars_and_markers\/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n\n    train_time = (rs_lr['train_time'], rs_svc['train_time'], rs_nb['train_time'])\n    val_time = (rs_lr['val_time'], rs_svc['val_time'], rs_nb['val_time'])\n    \n    train_score = (rs_lr['train_score'], rs_svc['train_score'], rs_nb['train_score'])\n    val_score = (rs_lr['val_score'], rs_svc['val_score'], rs_nb['val_score'])\n\n    ind = np.arange(len(train_time))  # the x locations for the groups\n    width = 0.35  # the width of the bars\n\n    fig, ax0 = plt.subplots(1, 1, figsize = (16,5))\n    \n    rects1 = ax0.bar(ind - width\/2, train_time, width,\n                    color='SkyBlue', label='train time')\n    rects2 = ax0.bar(ind + width\/2, val_time, width,\n                    color='IndianRed', label='val time')\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax0.set_ylabel('time')\n    ax0.set_title('time by train and val')\n    ax0.set_xticks(ind)\n    ax0.set_xticklabels(('LogisticRegression', 'SVC', 'NaiveBayes'))\n    ax0.legend()\n    autolabel(ax0, rects1, \"left\")\n    autolabel(ax0, rects2, \"right\")\n    plt.show()\n    \n    fig, ax1 = plt.subplots(1, 1, figsize = (16,5))\n    \n    rects3 = ax1.bar(ind - width\/2, train_score, width,\n                    color='SkyBlue', label='train score')\n    rects4 = ax1.bar(ind + width\/2, val_score, width,\n                    color='IndianRed', label='val score')\n    ax1.set_ylabel('Scores')\n    ax1.set_title('Scores by train and val')\n    ax1.set_xticks(ind)\n    ax1.set_xticklabels(('LogisticRegression', 'SVC', 'NaiveBayes'))\n    ax1.legend()\n    autolabel(ax1, rects3, \"left\")\n    autolabel(ax1, rects4, \"right\")\n    plt.show()\n    \nshow_metrics(rs_lr, rs_svc, rs_nb)","cee2b617":"# tfidf_x_test_fs = fs.transform(tfidf_x_test)\nrs_lr_acc = rs_lr['best_model'].score(tfidf_x_test, y_test)\nrs_svc_acc = rs_svc['best_model'].score(tfidf_x_test, y_test)\nrs_nb_acc = rs_nb['best_model'].score(tfidf_x_test, y_test)\nprint(\"LogisticRegression best score\", rs_lr_acc)\nprint(\"svc best score\", rs_svc_acc)\nprint(\"NaiveBayes best score\", rs_nb_acc)","113cb9d8":"#https:\/\/keras.io\/getting-started\/sequential-model-guide\/\n#https:\/\/github.com\/dennybritz\/cnn-text-classification-tf\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\nfrom keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000","5c9dfa95":"words_count = []\nfor i in x_train:\n    words_count.append(len(text_to_word_sequence(i, split=' ')))\nprint(np.max(words_count))\nprint(np.min(words_count))\n# 18~ 16333\nNUM_WORDS = 20000\nMAX_LEN= 1000\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=NUM_WORDS)\ntokenizer.fit_on_texts(x_sp_train)\nword_index = tokenizer.word_index\nx_sp_train_dl = pad_sequences(tokenizer.texts_to_sequences(x_sp_train), maxlen=MAX_LEN)\ny_sp_train_dl = to_categorical(np.asarray(y_sp_train), num_classes=20)\n\nx_sp_val_dl = pad_sequences(tokenizer.texts_to_sequences(x_sp_val), maxlen=MAX_LEN)\ny_sp_val_dl = to_categorical(np.asarray(y_sp_val), num_classes=20)\n\nx_test_dl = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=MAX_LEN)\ny_test_dl = to_categorical(np.asarray(y_test), num_classes=20)","494b3aa9":"def text_CNN(embedding_layer):\n    sequence_input = Input(shape=(MAX_LEN,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n   \n    # Yoon Kim model (https:\/\/arxiv.org\/abs\/1408.5882)\n    \n    embedded_sequences = Reshape((MAX_LEN, EMBEDDING_DIM, 1))(embedded_sequences)\n    x = Conv2D(100, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n    x = MaxPooling2D((MAX_LEN - 5 + 1, 1))(x)\n\n    y = Conv2D(100, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n    y = MaxPooling2D((MAX_LEN - 4 + 1, 1))(y)\n\n    z = Conv2D(100, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n    z = MaxPooling2D((MAX_LEN - 3 + 1, 1))(z)\n\n    alpha = concatenate([x,y,z])\n    alpha = Flatten()(alpha)\n    alpha = Dropout(0.5)(alpha)\n    preds = Dense(len(news_groups_train.target_names), activation='softmax')(alpha)\n    model = Model(sequence_input, preds)\n    adadelta = optimizers.Adadelta()\n        \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=adadelta,\n                  metrics=['acc'])\n    return model","941385dd":"def show_history(history):\n    plt.plot(history['acc'])\n    plt.plot(history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","9d304e56":"min_num_words = min(NUM_WORDS, len(word_index))\nembedding_matrix = np.zeros((min_num_words+1, EMBEDDING_DIM))\n\nfor word, index in word_index.items():\n    if index > min_num_words:\n        continue\n    elif word in word2vec_model:\n            embedding_matrix[index] = word2vec_model[word]\n\nprint('embedding matrix shape: {}'.format(embedding_matrix.shape))   \nembedding_layer = Embedding(min_num_words+1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, \n                            trainable=False)\ntext_cnn = text_CNN(embedding_layer)\ntext_cnn.summary()","32f9e1b1":"# https:\/\/keras.io\/getting-started\/sequential-model-guide\/#training\nearly_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n\nhistory = text_cnn.fit(x_sp_train_dl, y_sp_train_dl, validation_data=(x_sp_val_dl, y_sp_val_dl), epochs=60, batch_size=50,callbacks=[early_stopping])","11569e85":"show_history(history.history)","6ea2187d":"rs_text_cnn = text_cnn.evaluate(x_test_dl, y_test_dl, batch_size=50)\nrs_text_cnn_acc = rs_text_cnn[1]","53cba875":"train_accs = (rs_lr['train_score'], rs_svc['train_score'], rs_nb['train_score'], history.history['acc'][-1])\nval_accs = (rs_lr['val_score'], rs_svc['val_score'], rs_nb['val_score'], history.history['val_acc'][-1])\ntest_accs = (rs_lr_acc, rs_svc_acc, rs_nb_acc, rs_text_cnn_acc)\nind = np.arange(len(train_accs))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(1, 1, figsize = (16,5))\n\nrects0 = ax.bar(ind-width\/3, train_accs, width\/3,\n                color='Blue', label='train acc')\nrects1 = ax.bar(ind, val_accs, width\/3,\n                color='SkyBlue', label='val acc')\nrects2 = ax.bar(ind+width\/3, test_accs, width\/3,\n                color='Green', label='test acc')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('acc')\nax.set_xlabel('model')\nax.set_title('acc for each model')\nax.set_xticks(ind)\nax.set_xticklabels(('LogisticRegression', 'SVC', 'NaiveBayes', 'text_cnn'))\nax.legend()\nautolabel(ax, rects0, 'left')\nautolabel(ax, rects1, 'center')\nautolabel(ax, rects2, 'right')\nplt.show()\n\nplt.show()","07776ed8":"**Text-CNN**"}}