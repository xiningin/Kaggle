{"cell_type":{"b36db7f7":"code","829bcce6":"code","0f4a940b":"code","9ce90626":"code","df825d6b":"code","6d8b98f2":"code","c4925d97":"code","fa94e9ac":"code","8e9d5c4a":"code","14a86cf5":"code","6325d105":"code","c6d1ae73":"code","51902431":"code","4ea058e8":"code","05167c3e":"code","dc7d6b1e":"code","57af4594":"code","54ac7c6d":"code","8420357c":"code","627762b4":"code","f12db4c4":"code","a29be9c5":"code","d59f84ce":"code","b747c8df":"code","9a03769c":"code","2d68e74b":"code","8b47636e":"code","47a30269":"code","aacd670e":"code","32cab241":"code","8735de17":"code","ae74c88d":"code","337f9a35":"code","5888a095":"markdown","40f7df59":"markdown","774dac36":"markdown","bf8419be":"markdown","5808f499":"markdown","b1ff1746":"markdown","1e821a3e":"markdown","6049f30b":"markdown","c3a6574c":"markdown","2e3158b0":"markdown","41366677":"markdown","abccc735":"markdown","d6ba212d":"markdown","5c55e1d5":"markdown"},"source":{"b36db7f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","829bcce6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns","0f4a940b":"# import warnings\n# warnings.filterwarnings(\"error\")","9ce90626":"data = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines = True)","df825d6b":"data.head() #Display the first 5 rows","6d8b98f2":"data = data.drop('article_link',axis=1) #drop the article_link we consider headline as independent variable","c4925d97":"data","fa94e9ac":"data.isnull().sum() # check is null value is there or not","8e9d5c4a":"sns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x ='is_sarcastic',data=data,palette=\"BrBG\")","14a86cf5":"from wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nfrom wordcloud import ImageColorGenerator\ntext = \" \".join(i for i in data.headline)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\nplt.figure( figsize=(15,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6325d105":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, len(data)):\n  review = re.sub('[^a-zA-Z]', ' ', data['headline'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)","c6d1ae73":"corpus[5]","51902431":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500,ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values","4ea058e8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","05167c3e":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","dc7d6b1e":"y_pred = classifier.predict(X_test)","57af4594":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nacc = accuracy_score(y_test, y_pred)","54ac7c6d":"print(acc)","8420357c":"## TFidf Vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3))\nX=tfidf_v.fit_transform(corpus).toarray()","627762b4":"y=data['is_sarcastic']","f12db4c4":"X.shape","a29be9c5":"from sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","d59f84ce":"print(X1_train.shape)\nprint(X1_test.shape)\nprint(y1_train.shape)\nprint(y1_test.shape)","b747c8df":"count_df = pd.DataFrame(X1_train, columns=tfidf_v.get_feature_names())","9a03769c":"count_df.head()","2d68e74b":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X1_train, y1_train)","8b47636e":"y1_pred = classifier.predict(X1_test)","47a30269":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y1_test, y1_pred)\nprint(cm)\nacc = accuracy_score(y1_test, y1_pred)","aacd670e":"acc","32cab241":"from sklearn.naive_bayes import MultinomialNB\nclassifier=MultinomialNB()","8735de17":"classifier.fit(X1_train, y1_train)\npred = classifier.predict(X1_test)\npred = classifier.predict(X1_test)\nscore = accuracy_score(y1_test, pred)\nscore","ae74c88d":"classifier=MultinomialNB(alpha=0.1)","337f9a35":"previous_score=0\nfor alpha in np.arange(0,1,0.1):\n    sub_classifier=MultinomialNB(alpha=alpha)\n    sub_classifier.fit(X1_train,y1_train)\n    y_pred=sub_classifier.predict(X1_test)\n    score = accuracy_score(y1_test, pred)\n    if score>previous_score:\n        classifier=sub_classifier\n    print(\"Alpha: {}, Score : {}\".format(alpha,score))","5888a095":"# Creating the Bag of Words model","40f7df59":"# Splitting the dataset into the Training set and Test set","774dac36":"# Count of Sarcastic and non sarcastic words","bf8419be":"# MultinomialNB Algorithm","5808f499":"# Confusion matrix","b1ff1746":"# Creating the TF - IDF model","1e821a3e":"\n# Splitting the dataset into the Training set and Test set","6049f30b":"# Most  common words","c3a6574c":"# import libraries","2e3158b0":"# Training the Naive Bayes model on the Training set","41366677":"# Multinomial Classifier with Hyperparameter","abccc735":"# Cleaning the texts","d6ba212d":"# Naive Bayes","5c55e1d5":"# Load the dataset"}}