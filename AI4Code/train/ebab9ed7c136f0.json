{"cell_type":{"cfe1a048":"code","9f4d84a5":"code","05698b52":"code","79ac3a66":"code","4b860589":"code","6ddf5904":"code","b3b79d8e":"code","c14a94cc":"code","dd8f718b":"code","829ae64d":"code","5bc1a139":"code","35d443f4":"code","d8fc1abb":"code","31050ba8":"code","86c105e7":"code","590fe211":"code","bda120d4":"code","dfbccc1f":"code","172ce12b":"code","419ee84a":"code","122e296b":"markdown","b1cc07ff":"markdown","a7ff79fb":"markdown"},"source":{"cfe1a048":"import numpy as np\nimport torch\nimport pandas as pd\nfrom torchvision import datasets, transforms\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn, optim\nimport torch.nn.functional as F","9f4d84a5":"# Load data\ndf_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","05698b52":"# View top of training data\ndf_train.head()","79ac3a66":"df_test.head()","4b860589":"# Create the training, validation and test sets\nall_train_images = df_train.drop('label', axis=1)\nall_train_labels = df_train['label']\ntest_images = df_test\n\n# Split training data so we have a small validation set and training set\ntrain_images, val_images, train_labels, val_labels = train_test_split(all_train_images, all_train_labels, test_size=0.2)\n\n# Reindex so we can refer to first element using index 0\ntrain_images.reset_index(drop=True, inplace=True)\nval_images.reset_index(drop=True, inplace=True)\ntrain_labels.reset_index(drop=True, inplace=True)\nval_labels.reset_index(drop=True, inplace=True)\n","6ddf5904":"# Size of input images - 28 x 28 pixels\nIMG_SIZE = 28\n# Create a Dataset to hold the input data and make it available to dataloader\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels, transforms=None):\n        self.X = images\n        self.y = labels\n        self.transforms = transforms\n        \n    def __len__(self):\n        return (len(self.X))\n    \n    def __getitem__(self, i):\n        data = self.X.iloc[i, :]\n        data = np.array(data).astype(np.uint8).reshape(IMG_SIZE, IMG_SIZE, 1)\n        \n        if self.transforms:\n            data = self.transforms(data)\n            \n            # Also return label if we have it\n            if self.y is not None:\n                return (data, self.y[i])\n            else:\n                return data    ","b3b79d8e":"# Training Image transformations\ntrain_trans = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor()\n]\n)\ntrain_trans","c14a94cc":"# Validation Image transformations\nval_trans = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor()\n]\n)\nval_trans","dd8f718b":"# Create data loaders\nbatch_size = 64\n\ntrain_dataset = MyDataset(train_images, train_labels, train_trans)\nval_dataset = MyDataset(val_images, val_labels, val_trans)\ntest_dataset = MyDataset(test_images, None, val_trans)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","829ae64d":"# Create classifier module\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(IMG_SIZE * IMG_SIZE, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.log_softmax(self.fc4(x), dim=1)\n        return x","5bc1a139":"# Check that the model is setup correctly\nmodel = Classifier()\nmodel","35d443f4":"# Check that the output has the expected size [64, 10] and that the model does not error out when sending input through it\nimages, labels = next(iter(train_loader))\nlog_ps = model(images)\nassert (log_ps.shape[0] == 64) & (log_ps.shape[1] == 10) # check the size is as expected","d8fc1abb":"# Configure criterion and optimizer\nlearning_rate = 0.003\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","31050ba8":"# Train the model\n\nepochs = 30\n\ntrain_losses, val_losses = [], []\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in train_loader:\n        # Clear last image's results\n        optimizer.zero_grad()\n        log_ps = model(images)\n        loss = criterion(log_ps, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    else:\n        val_loss = 0\n        accuracy = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                log_ps = model(images)\n                val_loss += criterion(log_ps, labels)\n                ps = torch.exp(log_ps)\n                top_p, top_class = ps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n        train_losses.append(running_loss\/len(train_loader))\n        val_losses.append(val_loss\/len(val_loader))\n        \n        print(\"Epoch: {}\/{}.. \".format(e+1, epochs),\n             \"Training Loss: {:.3f}.. \".format(running_loss\/len(train_loader)),\n             \"Validation Loss: {:.3f}.. \".format(val_loss\/len(val_loader)),\n             \"Accuracy: {:.3f}\".format(accuracy\/len(val_loader)))\n        ","86c105e7":"# Plot the losses\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.legend(frameon=False);","590fe211":"# Function to view image and prediction\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.set_title(title)\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n    \n    return ax","bda120d4":"# Do a manual comparison of images and the predicted labels\nimages = next(iter(test_loader))\nfor i in range(20):\n    log_ps = model(images)\n    ps = torch.exp(log_ps)\n    top_p, top_class = ps.topk(1, dim=1)\n    imshow(images[i], title=top_class[i].item());","dfbccc1f":" # Create submission\ntest_labels = []\nwith torch.no_grad():\n    for images in test_loader:\n        log_ps = model(images)\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        for p in top_class:\n            test_labels.append(p.item())\n\n#Look at top 10 labels\ntest_labels[:10]","172ce12b":"submission_df = pd.DataFrame()\nsubmission_df['ImageId'] = range(1, len(test_labels) + 1)\nsubmission_df['Label'] = test_labels\nsubmission_df.head()","419ee84a":"submission_df.to_csv('\/kaggle\/submission2.csv', index=False)","122e296b":"No submit from the contest page","b1cc07ff":"Overfitting right away?","a7ff79fb":"In the training data the first column contains the label and the rest of the columns the pixels for the image file.\nThe test data is used for submission and we do not know the label so there is no label column."}}