{"cell_type":{"35402c79":"code","db8b6906":"code","99986726":"code","f047281d":"code","d2c270dd":"code","f08d638d":"code","476bca70":"code","9d50d9b3":"code","0bd76ed8":"code","9a04c5cf":"code","673a329c":"code","e5f354f0":"code","65f86cfe":"code","aee66979":"code","fd4b93a2":"code","3b975aec":"code","0509a041":"code","4e3ee31e":"code","6898e0bd":"code","10df1620":"code","667c74e6":"code","9bde3ce1":"code","43e2bd81":"markdown","05e23a33":"markdown","6e25735c":"markdown","4cb027a6":"markdown","b6beeb74":"markdown","ae05cc13":"markdown","0b0d3f45":"markdown","ed6c2a71":"markdown","745f0ec7":"markdown","e816fd37":"markdown","91ec5e00":"markdown","c607f3c8":"markdown","8e6117b8":"markdown"},"source":{"35402c79":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as mt\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n \ndf1 = pd.read_csv('..\/input\/Womens Clothing E-Commerce Reviews.csv')\ndf = df1[['Review Text','Rating','Class Name','Age']]\n#df1.info()\n#df1.describe()\ndf1.head()","db8b6906":"# fill NA values by space\ndf['Review Text'] = df['Review Text'].fillna('')\n\n# CountVectorizer() converts a collection \n# of text documents to a matrix of token counts\nvectorizer = CountVectorizer()\n# assign a shorter name for the analyze\n# which tokenizes the string\nanalyzer = vectorizer.build_analyzer()\n\ndef wordcounts(s):\n    c = {}\n    # tokenize the string and continue, if it is not empty\n    if analyzer(s):\n        d = {}\n        # find counts of the vocabularies and transform to array \n        w = vectorizer.fit_transform([s]).toarray()\n        # vocabulary and index (index of w)\n        vc = vectorizer.vocabulary_\n        # items() transforms the dictionary's (word, index) tuple pairs\n        for k,v in vc.items():\n            d[v]=k # d -> index:word \n        for index,i in enumerate(w[0]):\n            c[d[index]] = i # c -> word:count\n    return  c\n\n# add new column to the dataframe\ndf['Word Counts'] = df['Review Text'].apply(wordcounts)\ndf.head()","99986726":"review_length = {'100':0,'200':0,'300':0,'400':0,'500':0,'600':0,'700':0}\nfor review in df['Review Text']:\n    if len(review)<100:\n        review_length['100'] +=1\n    if 100<len(review)<200:\n        review_length['200'] +=1\n    if 200<len(review)<300:\n        review_length['300'] +=1\n    if 300<len(review)<400:\n        review_length['400'] +=1\n    if 400<len(review)<500:\n        review_length['500'] +=1\n    if 500<len(review)<600:\n        review_length['600'] +=1\n    if 600<len(review):\n        review_length['700'] +=1\n        \n","f047281d":"review_length","d2c270dd":"plt.xlabel('length of reviews')  \nplt.ylabel('number of reviews')  \n  \n\nplt.title(\"num vs len of reviews\")\nplt.bar(*zip(*review_length.items()))\nplt.show()","f08d638d":"# selecting some words to examine detailed \nselectedwords = ['awesome','great','fantastic','extraordinary','amazing','super',\n                 'magnificent','stunning','impressive','wonderful','breathtaking',\n                 'love','content','pleased','happy','glad','satisfied','lucky',\n                 'shocking','cheerful','wow','sad','unhappy','horrible','regret',\n                 'bad','terrible','annoyed','disappointed','upset','awful','hate']\n\ndef selectedcount(dic,word):\n    if word in dic:\n        return dic[word]\n    else:\n        return 0\n    \ndfwc = df.copy()  \nfor word in selectedwords:\n    dfwc[word] = dfwc['Word Counts'].apply(selectedcount,args=(word,))\n    \nword_sum = dfwc[selectedwords].sum()\nprint('Selected Words')\nprint(word_sum.sort_values(ascending=False).iloc[:5])\n\nprint('\\nClass Names')\nprint(df['Class Name'].fillna(\"Empty\").value_counts().iloc[:5])\n\n\n\ncn = df['Class Name'].fillna(\" \").value_counts()\n\n\nrt = df['Review Text']\n","476bca70":"df1=df['Rating'].value_counts().to_frame()\navgdf1 = df.groupby('Class Name').agg({'Rating': np.average})\navgdf2 = df.groupby('Class Name').agg({'Age': np.average})\navgdf3 = df.groupby('Rating').agg({'Age': np.average})\n\ntrace1 = go.Bar(\n    x=avgdf1.index,\n    y=round(avgdf1['Rating'],2),\n    marker=dict(\n        color=avgdf1['Rating'],\n        colorscale = 'RdBu')\n)\n\ntrace2 = go.Bar(\n    x=df1.index,\n    y=df1.Rating,\n    marker=dict(\n        color=df1['Rating'],\n        colorscale = 'RdBu')\n)\n\ntrace3 = go.Bar(\n    x=avgdf2.index,\n    y=round(avgdf2['Age'],2),\n    marker=dict(\n        color=avgdf2['Age'],\n        colorscale = 'RdBu')\n)\n\ntrace4 = go.Bar(\n    x=avgdf3.index,\n    y=round(avgdf3['Age'],2),\n    marker=dict(\n        color=avgdf3['Age'],\n        colorscale = 'Reds')\n)\n\nfig = tools.make_subplots(rows=2, cols=2, print_grid=False)\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\n\nfig['layout']['xaxis1'].update(title='Class')\nfig['layout']['yaxis1'].update(title='Average Rating')\nfig['layout']['xaxis2'].update(title='Rating')\nfig['layout']['yaxis2'].update(title='Count')\nfig['layout']['xaxis3'].update(title='Class')\nfig['layout']['yaxis3'].update(title='Average Age of the Reviewers')\nfig['layout']['xaxis4'].update(title='Rating')\nfig['layout']['yaxis4'].update(title='Average Age of the Reviewers')\n\nfig['layout'].update(height=800, width=900,showlegend=False)\nfig.update_layout({'plot_bgcolor':'rgba(0,0,0,0)',\n                   'paper_bgcolor':'rgba(0,0,0,0)'})\n#fig['layout'].update(plot_bgcolor='rgba(0,0,0,0)')\n#fig['layout'].update(paper_bgcolor='rgba(0,0,0,0)')\npy.iplot(fig)","9d50d9b3":"cv = df['Class Name'].value_counts()\n\ntrace = go.Scatter3d( x = avgdf1.index,\n                      y = avgdf1['Rating'],\n                      z = cv[avgdf1.index],\n                      mode = 'markers',\n                      marker = dict(size=10,color=avgdf1['Rating']),\n                      hoverinfo =\"text\",\n                      text=\"Class: \"+avgdf1.index+\" \\ Average Rating: \"+avgdf1['Rating'].map(' {:,.2f}'.format).apply(str)+\" \\ Number of Reviewers: \"+cv[avgdf1.index].apply(str)\n                      )\n\ndata = [trace]\nlayout = go.Layout(title=\"Average Rating & Class & Number of Reviewers\",\n                   scene = dict(\n                    xaxis = dict(title='Class'),\n                    yaxis = dict(title='Average Rating'),\n                    zaxis = dict(title='Number of Sales'),),\n                   margin = dict(l=30, r=30, b=30, t=30))\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)\nplt.savefig('3D_Scatter.png')","0bd76ed8":"# Rating of 4 or higher -> positive, while the ones with \n# Rating of 2 or lower -> negative \n# Rating of 3 -> neutral\ndf = df[df['Rating'] != 3]\ndf['Sentiment'] = df['Rating'] >=4\ndf.head()\n\n# split data\ntrain_data,test_data = train_test_split(df,train_size=0.8,random_state=0)\n# select the columns and \n# prepare data for the models \nX_train = vectorizer.fit_transform(train_data['Review Text'])\ny_train = train_data['Sentiment']\nX_test = vectorizer.transform(test_data['Review Text'])\ny_test = test_data['Sentiment']","9a04c5cf":"train_data['Sentiment']","673a329c":"start=dt.datetime.now()\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","e5f354f0":"start=dt.datetime.now()\nnb = MultinomialNB()\nnb.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","65f86cfe":"start=dt.datetime.now()\nsvm = SVC()\nsvm.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","aee66979":"start=dt.datetime.now()\nnn = MLPClassifier()\nnn.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","fd4b93a2":"# define a dataframe for the prediction probablities of the models\n#df1 = train_data.copy()\n#df1['Logistic Regression'] = lr.predict_proba(X_train)[:,1]\n#df1['Naive Bayes'] = nb.predict_proba(X_train)[:,1]\n#df1['SVM'] = svm.decision_function(X_train)\n#df1['Neural Network'] = nn.predict_proba(X_train)[:,1]\n#df1=df1.round(2)\n#df1.head()\n\n# define a dataframe for the predictions\ndf2 = train_data.copy()\ndf2['Logistic Regression'] = lr.predict(X_train)\ndf2['Naive Bayes'] = nb.predict(X_train)\ndf2['SVM'] = svm.predict(X_train)\ndf2['Neural Network'] = nn.predict(X_train)\ndf2.head()","3b975aec":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(df2['Logistic Regression'],df2['Sentiment']))\nprint(accuracy_score(df2['Naive Bayes'],df2['Sentiment']))\nprint(accuracy_score(df2['SVM'],df2['Sentiment']))\nprint(accuracy_score(df2['Neural Network'],df2['Sentiment']))","0509a041":"from sklearn.tree import DecisionTreeClassifier\ndtr = DecisionTreeClassifier()\ndtr.fit(X_train,y_train)","4e3ee31e":"pred_dtr = dtr.predict_proba(X_test)[:,1]\nfpr_dtr,tpr_dtr,_ = roc_curve(y_test,pred_dtr)\nroc_auc_dtr = auc(fpr_dtr,tpr_dtr)","6898e0bd":"f, axes = plt.subplots(2,2,figsize=(15,10))\naxes[0,1].plot(fpr_dtr, tpr_dtr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_dtr))\naxes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Decision Tree')\naxes[0,1].legend(loc='lower right', fontsize=13)","10df1620":"pred_lr = lr.predict_proba(X_test)[:,1]\nfpr_lr,tpr_lr,_ = roc_curve(y_test,pred_lr)\nroc_auc_lr = auc(fpr_lr,tpr_lr)\n\npred_nb = nb.predict_proba(X_test)[:,1]\nfpr_nb,tpr_nb,_ = roc_curve(y_test.values,pred_nb)\nroc_auc_nb = auc(fpr_nb,tpr_nb)\n\npred_svm = svm.decision_function(X_test)\nfpr_svm,tpr_svm,_ = roc_curve(y_test.values,pred_svm)\nroc_auc_svm = auc(fpr_svm,tpr_svm)\n\npred_nn = nn.predict_proba(X_test)[:,1]\nfpr_nn,tpr_nn,_ = roc_curve(y_test.values,pred_nn)\nroc_auc_nn = auc(fpr_nn,tpr_nn)\n\nf, axes = plt.subplots(2, 2,figsize=(15,10))\naxes[0,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\naxes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\naxes[0,0].legend(loc='lower right', fontsize=13)\n\naxes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\naxes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\naxes[0,1].legend(loc='lower right', fontsize=13)\n\naxes[1,0].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\naxes[1,0].legend(loc='lower right', fontsize=13)\n\naxes[1,1].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\naxes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Neural Network')\naxes[1,1].legend(loc='lower right', fontsize=13);","667c74e6":"# preparation for the confusion matrix\nlr_cm=confusion_matrix(y_test.values, lr.predict(X_test))\nnb_cm=confusion_matrix(y_test.values, nb.predict(X_test))\nsvm_cm=confusion_matrix(y_test.values, svm.predict(X_test))\nnn_cm=confusion_matrix(y_test.values, nn.predict(X_test))\n\nplt.figure(figsize=(15,12))\nplt.suptitle(\"Confusion Matrices\",fontsize=24)\n\nplt.subplot(2,2,1)\nplt.title(\"Logistic Regression\")\nsns.heatmap(lr_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,2)\nplt.title(\"Naive Bayes\")\nsns.heatmap(nb_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,3)\nplt.title(\"Support Vector Machine (SVM)\")\nsns.heatmap(svm_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,4)\nplt.title(\"Neural Network\")\nsns.heatmap(nn_cm, annot = True, cmap=\"Greens\",cbar=False);","9bde3ce1":"print(\"Logistic Regression\")\nprint(mt.classification_report(y_test, lr.predict(X_test)))\nprint(\"\\n Naive Bayes\")\nprint(mt.classification_report(y_test, nb.predict(X_test)))\nprint(\"\\n Support Vector Machine (SVM)\")\nprint(mt.classification_report(y_test, svm.predict(X_test)))\nprint(\"\\n Neural Network\")\nprint(mt.classification_report(y_test, nn.predict(X_test)))","43e2bd81":" # <span id=\"6\"><\/span> Building a Sentiment Classifier\n#### [Return Contents](#0)\n<hr\/>","05e23a33":"## <span id=\"13\"><\/span> ROC Curves and AUC","6e25735c":"At first, I added the prediction results to my training data. However, if you want to observe the prediction probabilies, you might use the commented out code.","4cb027a6":"## <span id=\"12\"><\/span> Adding Results to the Dataframe","b6beeb74":"## <span id=\"9\"><\/span> Support Vector Machine (SVM)","ae05cc13":"Since we do not have a column which shows the sentiment as positive or negative in the dataset, I defined a new sentiment column. To do this, I assumed the reviews which has **4 or higher ** rating as **positive (True in the new dataframe)** and **2 or lower** rating as **negative (False in the new dataframe)**. Also, I did not include the lines that has **neutral** ratings which are equal to **3**. Following that, I splitted the data as training and test sets.","0b0d3f45":"## <span id=\"15\"><\/span> Precision - Recall - F1-Score","ed6c2a71":"## <span id=\"8\"><\/span> Naive Bayes","745f0ec7":"## <span id=\"14\"><\/span> Confusion Matrices","e816fd37":"## <span id=\"7\"><\/span> Logistic Regression","91ec5e00":"## <span id=\"10\"><\/span> Neural Network","c607f3c8":"It seems that most of the ratings are positive and the average rating between the classes looks close. On the other hand, when we look at ages, average age does not change significantly according to the rating. Also, average age changes slightly between class names except casual bottoms. We can disregard casual bottoms because the below chart shows that there are just two reviews and making an inference will not be right.","8e6117b8":"Then, I fitted the models one by one. Since, some of them take too much time, running each of them in different cells is a better choice.   "}}