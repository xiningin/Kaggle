{"cell_type":{"b127fec5":"code","d1468187":"code","36419121":"code","11f3121d":"code","b1d8a1de":"code","0a65b730":"code","5bb41098":"code","0b1b67f7":"code","221044ad":"code","96b10141":"code","2d3f483d":"code","b72d683d":"code","54ff5620":"code","21c21155":"code","40ab1898":"code","c483fead":"code","503ff670":"code","a6e32e1d":"code","509f96bb":"code","2f33f57d":"code","e5f5e816":"code","16661e06":"code","7bfc9d08":"code","13cfd9fb":"code","082cbacd":"code","a02d2f58":"code","834347bd":"code","ffbf3a22":"code","26ece204":"code","b04ac9f9":"code","c149f29b":"code","f8f876d1":"code","4d194ecf":"code","313ea70b":"code","d6e71fbb":"code","a33cd3e9":"code","667f612f":"code","3a7df6c6":"code","df32e8c5":"code","eb204c92":"code","3aa162ba":"code","d3ff6469":"code","fec503d1":"code","624a658b":"code","5776dfab":"code","56cb8f8b":"code","9014cb27":"code","0580e5f9":"code","47eb7ec0":"code","ffe6be69":"code","e15e78a1":"code","4609fb96":"code","bf0639f4":"code","909a7c84":"code","fb45d473":"code","368737ff":"code","df2feb76":"code","8939fba0":"code","d058762b":"code","84ce0100":"code","10f6412c":"code","f7a442fb":"code","0c341c81":"code","71dcd297":"code","c5c74088":"code","a7f643f3":"code","f6facacd":"code","21f47525":"code","7cfb7260":"code","a8bdb9b9":"code","f6f626f3":"code","7bdecc04":"code","7321f0e2":"code","84f266fe":"code","f84be810":"code","cf6a6518":"code","d3e8300e":"code","1aef5b15":"code","830dadae":"code","335807e6":"code","0789871f":"code","f3279e41":"code","8b967bb6":"code","1ad2f380":"code","ceae3469":"code","16f85568":"code","6abb4838":"code","ad6a92a9":"code","47145d7c":"markdown","29e67bef":"markdown","7e01f98d":"markdown","1376b3e0":"markdown","d9c5b5e6":"markdown","df78dd41":"markdown","96fc6cf9":"markdown","1b5bc2f9":"markdown","85cbffc0":"markdown"},"source":{"b127fec5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1468187":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","36419121":"!pip install bs4\n!pip install contractions","11f3121d":"import seaborn as sns\nimport matplotlib.pyplot as plt","b1d8a1de":"reviews = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')","0a65b730":"# reviews = reviews.iloc[:20000]","5bb41098":"reviews.head()","0b1b67f7":"reviews.iloc[10].Text","221044ad":"reviews.iloc[10].Summary","96b10141":"reviews.info()","2d3f483d":"reviews.nunique()","b72d683d":"reviews.isnull().sum()\n# ProfileName doesnt matter since UserId and ProfileName is same. So If UserId is present then we should not care about ProfileName","54ff5620":"# users who didnt provide Summary\nreviews[reviews.Summary.isnull()].UserId.value_counts()","21c21155":"# How many users provided, respective ratings\nreviews.Score.value_counts()","40ab1898":"reviews.Score.value_counts().plot.bar()","c483fead":"# Most of the reviews are positive, So ppl do like food , Average of the Score is 4.1\nreviews.Score.mean()","503ff670":"import datetime\nlol = lambda x: datetime.datetime.fromtimestamp(x)\nreviews['Date'] = reviews.Time.apply(lol)","a6e32e1d":"reviews['weekDay_Time'] = reviews.Date.dt.weekday\nreviews['month'] = reviews.Date.dt.month\nreviews['quarter'] = reviews.Date.dt.quarter","509f96bb":"reviews.head()","2f33f57d":"sns.countplot(data=reviews, x='weekDay_Time')","e5f5e816":"sns.countplot(data=reviews, x='month')","16661e06":"sns.countplot(data=reviews, x='quarter')","7bfc9d08":"reviews = reviews[~reviews.Summary.isnull()]\nreviews=reviews.reset_index(drop=True)","13cfd9fb":"reviews.Text = reviews.Text.str.lower()\nreviews.Summary = reviews.Summary.str.lower()","082cbacd":"reviews.head()","a02d2f58":"Text_data = reviews.Text.values\nSummary = reviews.Summary.values","834347bd":"reviews[reviews.Text.duplicated()]","ffbf3a22":"\nTotal_len = len(Summary)\nTotal_distinct_Summary = len(set(Summary))\nprint(Total_len)\nprint(Total_distinct_Summary)","26ece204":"\nTotal_len = len(Text_data)\nTotal_distinct_Text_data = len(set(Text_data))\nprint(Total_len)\nprint(Total_distinct_Text_data)","b04ac9f9":"reviews = reviews.drop_duplicates('Text')\nreviews = reviews.reset_index(drop=True)","c149f29b":"Text_data = reviews.Text.values\nSummary = reviews.Summary.values","f8f876d1":"from wordcloud import WordCloud, STOPWORDS\nsummaryWordCloud = ' '.join(Summary).lower()\nwordcloud2 = WordCloud().generate(summaryWordCloud)\nplt.imshow(wordcloud2)","4d194ecf":"# TextWordCloud = ' '.join(Text_data).lower()\n# wordcloud2 = WordCloud().generate(TextWordCloud)\n# plt.imshow(wordcloud2)","313ea70b":"sampleReview = reviews.iloc[:20000].copy()\n\nfrom bs4 import BeautifulSoup\nsoup = lambda text:BeautifulSoup(text)\npassText = lambda text:soup(text).get_text()\n\nsampleReview.Text = sampleReview.Text.apply(passText)","d6e71fbb":"reviews.Summary.apply(lambda x:len(x.split(' '))).plot(kind='hist')","a33cd3e9":"reviews.Text.apply(lambda x:len(x.split(' '))).plot(kind='hist')","667f612f":"reviews.Text.apply(lambda x:len(x.split(' '))).quantile(0.95)","3a7df6c6":"input_characters = set()\ntarget_characters = set()","df32e8c5":"import re","eb204c92":"from nltk.tokenize import TweetTokenizer\ntweet = TweetTokenizer()","3aa162ba":"import contractions","d3ff6469":"\n#Fix Contradiction\nsampleReview.Text = sampleReview.Text.apply(lambda x: contractions.fix(x))\nsampleReview.Summary = sampleReview.Summary.apply(lambda x: contractions.fix(x))\n\nsampleReview.Text = sampleReview.Text.apply(lambda x: tweet.tokenize(x))\nsampleReview.Summary = sampleReview.Summary.apply(lambda x: tweet.tokenize(x))","fec503d1":"sampleReview.head()","624a658b":"sampleReview.Text = sampleReview.Text.apply(lambda tokens: [word for word in tokens if word.isalpha()])\nsampleReview.Summary = sampleReview.Summary.apply(lambda tokens: [word for word in tokens if word.isalpha()])","5776dfab":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nsampleReview.Text = sampleReview.Text.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])","56cb8f8b":"from nltk.corpus import stopwords\nstop_words = stopwords.words()\nstop_words = set(stop_words)","9014cb27":"len(sampleReview.Text)","0580e5f9":"%%time\nsampleReview.Text = sampleReview.Text.apply(lambda tokens: [word for word in tokens if word not in stop_words][:100])","47eb7ec0":"sampleReview.head()","ffe6be69":"# from nltk.corpus import wordnet\n# sampleReview.Text = sampleReview.Text.apply(lambda tokens: [word for word in tokens if wordnet.synsets(word)])\n# sampleReview.Summary = sampleReview.Summary.apply(lambda tokens: [word for word in tokens if wordnet.synsets(word)])","e15e78a1":"sampleReview.Summary = sampleReview.Summary.apply(lambda x:['<BOS>']+x+['<EOS>'])","4609fb96":"sampleReview.Text = sampleReview.Text.apply(lambda x:' '.join(x))\nsampleReview.Summary = sampleReview.Summary.apply(lambda x:' '.join(x))","bf0639f4":"sampleReview.Summary","909a7c84":"input_texts = list(sampleReview.Text.values)\ntarget_texts = list(sampleReview.Summary.values)","fb45d473":"sampleReview.to_csv('filtered_data.csv', index=False)","368737ff":"from keras.preprocessing.text import Tokenizer\nVOCAB_SIZE = 50000\n\ntokenizerText = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\ntokenizerText.fit_on_texts(input_texts)\n\ntokenizerSummary = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\ntokenizerSummary.fit_on_texts(target_texts)\n\n\ndef text2seq(encoder_text, decoder_text, VOCAB_SIZE):\n  encoder_sequences = tokenizerText.texts_to_sequences(encoder_text)\n  decoder_sequences = tokenizerSummary.texts_to_sequences(decoder_text)\n  \n  return encoder_sequences, decoder_sequences\n\nencoder_sequences, decoder_sequences = text2seq(input_texts, target_texts, VOCAB_SIZE)","df2feb76":"textVocabSize = len(tokenizerText.word_index)\nsummaryVocabSize = len(tokenizerSummary.word_index)\ntextVocabSize, summaryVocabSize","8939fba0":"def vocab_creater(text_lists, VOCAB_SIZE, tokenizer):\n\n  dictionary = tokenizer.word_index\n  \n  word2idx = {}\n  idx2word = {}\n  for k, v in dictionary.items():\n      if v < VOCAB_SIZE:\n          word2idx[k] = v\n          idx2word[v] = k\n      if v >= VOCAB_SIZE-1:\n          continue\n          \n  return word2idx, idx2word\n\nword2idxText, idx2wordText = vocab_creater(input_texts, textVocabSize, tokenizerText)\nword2idxSummary, idx2wordSummary = vocab_creater(target_texts, summaryVocabSize, tokenizerSummary)","d058762b":"EMBEDDING_DIM=100\nmaxLenText=100\nmaxLenSummary=40","84ce0100":"from keras.preprocessing.sequence import pad_sequences\n\ndef padding(encoder_sequences, decoder_sequences, maxLenText, maxLenSummary):\n  \n  encoder_input_data = pad_sequences(encoder_sequences, maxlen=maxLenText, dtype='int32', padding='post', truncating='post')\n  decoder_input_data = pad_sequences(decoder_sequences, maxlen=maxLenSummary, dtype='int32', padding='post', truncating='post')\n  \n  return encoder_input_data, decoder_input_data\n\nencoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences,maxLenText, maxLenSummary)","10f6412c":"def glove_100d_dictionary():\n  embeddings_index = {}\n  f = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\n  for line in f:\n      values = line.split()\n      word = values[0]\n      coefs = np.asarray(values[1:], dtype='float32')\n      embeddings_index[word] = coefs\n  f.close()\n  return embeddings_index","f7a442fb":"embeddings_index = glove_100d_dictionary()","0c341c81":"def embedding_matrix_creater(embedding_dimention, tokenizer):\n  embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dimention))\n  for word, i in tokenizer.word_index.items():\n      embedding_vector = embeddings_index.get(word)\n      if embedding_vector is not None:\n          # words not found in embedding index will be all-zeros.\n          embedding_matrix[i] = embedding_vector\n  return embedding_matrix","71dcd297":"embeddingMatrixText = embedding_matrix_creater(100, tokenizerText)\nembeddingMatrixSummary = embedding_matrix_creater(100, tokenizerSummary)","c5c74088":"from keras.layers import Embedding\nencoder_embedding_layer = Embedding(input_dim = textVocabSize+1, \n                                    output_dim = 100,\n                                    input_length = maxLenText,\n                                    mask_zero=True,\n#                                     weights = [embeddingMatrixText],\n                                    trainable = True)\ndecoder_embedding_layer = Embedding(input_dim = summaryVocabSize+1, \n                                    output_dim = 100,\n                                    input_length = maxLenSummary,\n                                    mask_zero=True,\n#                                     weights = [embeddingMatrixSummary],\n                                    trainable = True)","a7f643f3":"# sampleReview.Text = sampleReview.Text.apply(lambda tokens: [contractions[word] for word in tokens if word.isalpha()])\n# sampleReview.Summary = sampleReview.Summary.apply(lambda tokens: [contractions[word] for word in tokens if word.isalpha()])","f6facacd":"\nfrom numpy.random import seed\nseed(1)\n\nfrom sklearn.model_selection import train_test_split\nimport logging\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pydot\n\n\nimport keras\nfrom keras import backend as k\nk.set_learning_phase(1)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras import initializers\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense,LSTM,Dropout,Input,Activation,Add,concatenate, Embedding, RepeatVector\nfrom keras.layers.advanced_activations import LeakyReLU,PReLU\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\n\nfrom keras.layers import TimeDistributed","21f47525":"MAX_LEN = 100\nEMBEDDING_DIM = 100\nHIDDEN_UNITS = 300\ntextVocabSize = textVocabSize+1\nsummaryVocabSize = summaryVocabSize+1 \nLEARNING_RATE = 0.002\nBATCH_SIZE = 8\nEPOCHS = 5","7cfb7260":"# input_characters = sorted(list(input_characters))+[' ']\n# target_characters = sorted(list(target_characters))+[' ']\n# num_encoder_tokens = len(encoder_input_data)\n# num_decoder_tokens = len(decoder_input_data)\nnum_encoder_tokens = textVocabSize\nnum_decoder_tokens = summaryVocabSize\n\nmax_encoder_seq_length = max([len(txt) for txt in encoder_input_data])\nmax_decoder_seq_length = max([len(txt) for txt in decoder_input_data])\n\nprint(\"Number of samples:\", len(input_texts))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","a8bdb9b9":"# encoder_input_data = np.zeros(\n#     (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n# )\n# decoder_input_data = np.zeros(\n#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n# )\n# decoder_target_data = np.zeros(\n#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n# )","f6f626f3":"# for i, seqs in enumerate(encoder_input_data):\n#     for j, seq in enumerate(seqs):\n#         decoder_target_data[i, j, seq] = 1.0","7bdecc04":"from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional","7321f0e2":"\n\"\"\"\nChatbot Inspired Encoder-Decoder-seq2seq\n\"\"\"\nencoder_inputs = Input(shape=(maxLenText, ), dtype='int32',)\nencoder_embedding = encoder_embedding_layer(encoder_inputs)\nencoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True,return_sequences=True)\nencoder_outputs1, state_h, state_c = encoder_LSTM(encoder_embedding)\n\nencoder_lstm2 = LSTM(HIDDEN_UNITS,return_sequences=True,return_state=True) \nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_outputs1) \n\nencoder_lstm3= LSTM(HIDDEN_UNITS, return_state=True) \nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n\ndecoder_inputs = Input(shape=(maxLenSummary, ), dtype='int32',)\ndecoder_embedding = decoder_embedding_layer(decoder_inputs)\n\ndecoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\ndecoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n\n# attn_out, attn_states = tf.keras.layers.Attention()([encoder_outputs, decoder_outputs]) \n\n# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n\n\n# dense_layer = Dense(VOCAB_SIZE, activation='softmax')\ndecoder_time = TimeDistributed(Dense(summaryVocabSize, activation='softmax'))\noutputs = decoder_time(decoder_outputs)\nmodel = Model([encoder_inputs, decoder_inputs], outputs)","84f266fe":"rmsprop = Adam(lr=0.01, clipnorm=1.)\nmodel.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=[\"accuracy\"])","f84be810":"model.summary()","cf6a6518":"import numpy as np1\nnum_samples = len(decoder_sequences)\ndecoder_output_data = np.zeros((num_samples, maxLenSummary, summaryVocabSize), dtype=\"int32\")","d3e8300e":"for i, seqs in enumerate(decoder_sequences):\n    for j, seq in enumerate(seqs):\n        if j > 0:\n            decoder_output_data[i][j-1][seq] = 1","1aef5b15":"art_train, art_test, sum_train, sum_test = train_test_split(encoder_input_data, decoder_input_data, test_size=0.2)\ntrain_num = art_train.shape[0]\ntarget_train = decoder_output_data[:train_num]\ntarget_test = decoder_output_data[train_num:]","830dadae":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","335807e6":"class My_Custom_Generator(keras.utils.Sequence) :\n    def __init__(self, art_train, sum_train, decoder_output, batch_size) :\n        self.art = art_train\n        self.sum = sum_train\n        self.decoder = decoder_output\n        self.batch_size = batch_size\n\n    def __len__(self) :\n        return (np.ceil(len(self.art) \/ float(self.batch_size))).astype(np.int)\n  \n    def __getitem__(self, idx) :\n        batch_x1 = self.art[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_x2 = self.sum[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_y = self.decoder[idx * self.batch_size : (idx+1) * self.batch_size]\n        return [np.array(batch_x1),np.array(batch_x2)],np.array(batch_y)","0789871f":"batch_size = 64\n\nmy_training_batch_generator = My_Custom_Generator(art_train, sum_train, target_train, batch_size)\nmy_validation_batch_generator = My_Custom_Generator(art_test, sum_test, target_test,batch_size)","f3279e41":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=10, verbose=1, epsilon=1e-4, mode='min')\nmodel.fit_generator(generator=my_training_batch_generator,\n                   steps_per_epoch = int(16000 \/\/ batch_size),\n                   epochs = 200,\n                   verbose = 1,callbacks=[reduce_lr_loss])","8b967bb6":"model.save_weights('nmt_weights_100epochs.h5')","1ad2f380":"encoder_states = [state_h, state_c]\nencoder_model = Model(encoder_inputs, encoder_states)\n\nthought_input = [Input(shape=(HIDDEN_UNITS, )), Input(shape=(HIDDEN_UNITS, ))]\n\ndecoder_embedding = decoder_embedding_layer(decoder_inputs)\n\ndecoder_outputss, state_h, state_c = decoder_LSTM(decoder_embedding, initial_state=thought_input)\ndecoder_states = [state_h, state_c]\n\ndecoder_outputs = decoder_time(decoder_outputss)\n\ndecoder_model = Model(inputs=[decoder_inputs]+thought_input, outputs=[decoder_outputs]+decoder_states)","ceae3469":"# encoder_states = [encoder_outputs, state_h, state_c]\n# encoder_model = Model(encoder_inputs, encoder_states)\n\n\n# decoder_embedding = decoder_embedding_layer(decoder_inputs)\n# decoder_outputss, state_h, state_c = decoder_LSTM(decoder_embedding, initial_state=thought_input)\n# decoder_states = [state_h, state_c]\n\n# decoder_outputs = decoder_time(decoder_outputss)\n\n# decoder_model = Model(inputs=[decoder_inputs]+thought_input, outputs=[decoder_outputs]+decoder_states)","16f85568":"def decode_sequence(input_seq):\n    states_value = encoder_model.predict(input_seq)\n    target_seq = np.zeros((100,1))\n    target_seq[0, 0] = word2idxSummary['bos']\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_word =idx2wordSummary[sampled_token_index]\n        decoded_sentence += ' '+ sampled_word\n\n        if (sampled_word == 'eos' or\n           len(decoded_sentence) > 40):\n            stop_condition = True\n\n        target_seq = np.zeros((100,1))\n        target_seq[0, 0] = sampled_token_index\n\n        states_value = [h, c]\n    return decoded_sentence","6abb4838":"# art_train[10].shape","ad6a92a9":"print(\"Text->\",tokenizerText.sequences_to_texts([art_train[10]]))\nprint(\"\\n\\n\\n\")\nprint(\"Summary->\",tokenizerSummary.sequences_to_texts([sum_train[10]]))\nprint(\"\\n\\n\\n\")\nprint(\"using model->\",decode_sequence([art_train[10]]))","47145d7c":"## Word Cloud for our Data. Looks COOL","29e67bef":"### Lets preprocess convert everything to Lower Case","7e01f98d":"## Get Total unique Products","1376b3e0":"## Remove HTML Tags","d9c5b5e6":"## ok lets drop the similar Text Data","df78dd41":"## do we have similar Text Data?","96fc6cf9":"## do we have similar Summariaes?","1b5bc2f9":"## Histogram for the length of reviews and summary","85cbffc0":"## Lets do Some EDA on the Text Data"}}