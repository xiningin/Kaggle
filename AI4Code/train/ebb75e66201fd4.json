{"cell_type":{"d34face0":"code","3575e954":"code","ea3d611f":"code","c264dee8":"code","850e9f79":"code","849146e8":"code","693f999a":"code","14f197ee":"code","b8305e36":"code","073dab89":"code","1d5f2897":"code","3c5a8ed2":"code","68587e98":"code","cea48a7b":"code","7b144152":"code","a9d81238":"code","f30ca2fa":"code","231f3064":"code","9fc12f9f":"code","f7baee39":"code","7a19274f":"code","e33bba9f":"code","9341245a":"code","fd2056f3":"code","a3128b07":"code","07613d5b":"code","49d896fc":"markdown","df913289":"markdown","6e0b4aac":"markdown","98e49121":"markdown","437a3983":"markdown"},"source":{"d34face0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold,KFold\nfrom sklearn.metrics import mean_squared_log_error\nimport lightgbm as lgb\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3575e954":"plt.style.use('ggplot')","ea3d611f":"train=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nsample_submission=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","c264dee8":"train.head()","850e9f79":"train['date_time'].min(),train['date_time'].max()","849146e8":"test['date_time'].min(),test['date_time'].max()","693f999a":"train['date_time']=pd.to_datetime(train['date_time'],format='%Y-%m-%d %H:%M:%S')","14f197ee":"test['date_time']=pd.to_datetime(test['date_time'],format='%Y-%m-%d %H:%M:%S')","b8305e36":"train.shape","073dab89":"train=train.loc[~(train['date_time']=='2011-01-01')].reset_index(drop=True)","1d5f2897":"train.shape","3c5a8ed2":"fig,ax=plt.subplots(4,2,figsize=(20,15))\nfor i,col in enumerate(train.columns[1:9]):\n    ax[i%4][i\/\/4].hist(train[col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%4][i\/\/4].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%4][i\/\/4].set_xlabel(f'{col}')\n    ax[i%4][i\/\/4].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","68587e98":"fig,ax=plt.subplots(3,1,figsize=(10,15))\nfor i,col in enumerate(train.columns[9:12]):\n    ax[i%3].hist(train[col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%3].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%3].set_xlabel(f'{col}')\n    ax[i%3].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","cea48a7b":"train[['date_time','deg_C']].set_index('date_time').resample('D').mean()","7b144152":"# Following code is inspired from - https:\/\/www.kaggle.com\/nroman\/eda-for-ashrae\nfig,ax=plt.subplots(1,1,figsize=(15,10))\ntrain[['date_time','deg_C']].set_index('date_time').resample('D').mean()['deg_C'].plot(ax=ax,label='by hour(train)',alpha=1,color='blue').set_ylabel('deg C',fontsize=10)\nax.set_title('Trend of Mean deg_C by Day',fontsize=12)\nax.set_xlabel('')","a9d81238":"fig,ax=plt.subplots(1,1,figsize=(15,10))\ntest[['date_time','deg_C']].set_index('date_time').resample('D').mean()['deg_C'].plot(ax=ax,label='by hour(train)',alpha=1,color='blue').set_ylabel('deg C',fontsize=10)\nax.set_title('Trend of Mean deg_C by Day',fontsize=12)\nax.set_xlabel('')","f30ca2fa":"## Quick modelling:\nn_folds=5\n#Using random gbm params,\nparams={\"objective\":'regression',\n       'learning_rate':0.06,\n       'num_leaves':2**7-1,\n        'n_estimators':30,\n        'min_child_samples':8,\n       'n_jobs':-1,\n       'max_depth':-1,\n       'metric':'l2',\n       'tree_learner':'serial',\n        'bagging_fraction':0.8,\n        'bagging_freq':5,\n       'seed':42}\npred_cols=train.columns.drop(['date_time','target_carbon_monoxide','target_benzene','target_nitrogen_oxides'])\ntarget=[c for c in train.columns if c.startswith('target')]","231f3064":"for t in target:\n    train[t]=np.log1p(train[t])","9fc12f9f":"for t in target:\n    temp[t]=","f7baee39":"folds=KFold(n_splits=n_folds,shuffle=False)\noof_preds=np.zeros((train.shape[0],3))\nsubs=np.zeros((test.shape[0],3))\nfeature_importance=pd.DataFrame()\nfeature_importance_df=pd.DataFrame()\nfor g,t in enumerate(target):\n    print(f'********Starting training for target {t}***********')\n    losses=[0]*n_folds\n    val_scores=[0]*n_folds\n    loss=0\n    for i,(trn_idx,val_idx) in enumerate(folds.split(train)):\n        trn_X,trn_Y=train[pred_cols].loc[trn_idx],train[t].loc[trn_idx]\n        val_X,val_Y=train[pred_cols].loc[val_idx],train[t].loc[val_idx]\n        \n        trn_df=lgb.Dataset(trn_X,label=trn_Y)\n        val_df=lgb.Dataset(val_X,label=val_Y)\n        \n        watchlist=[trn_df,val_df]\n        \n        model=lgb.train(params,\n                        train_set=trn_df,\n                        num_boost_round=3000,\n                        early_stopping_rounds=30,\n                        valid_sets=watchlist,\n                        verbose_eval=30)\n        \n        feature_importance['target']=t\n        feature_importance['columns']=pred_cols\n        feature_importance['folds']=i\n        feature_importance['imp']=model.feature_importance()\n        preds=model.predict(val_X,num_iteration=model.best_iteration)\n        val_scores[i]=np.sqrt(model.best_score['valid_1']['l2'])\n        oof_preds[val_idx][:,g]=model.predict(val_X,num_iteration=model.best_iteration)\n        subs[:,g]+=model.predict(test[pred_cols],num_iteration=model.best_iteration)\n        rmsle=np.sqrt(mean_squared_log_error(val_Y,preds))\n        print(f'Fold {i+1} Loss {rmsle} Best Score {val_scores[i]}')\n    feature_importance_df=pd.concat([feature_importance_df,feature_importance],axis=0)\n    subs[:,g]\/=n_folds\n    subs[:,g]=np.expm1(subs[:,g])\n    \n    print(f'Mean RMSLE {np.mean(val_scores)} std {np.std(val_scores)}')\n    print('**************')\n        ","7a19274f":"feature_importance_df","e33bba9f":"feature_importance.describe()","9341245a":"subs.shape","fd2056f3":"sample_submission.iloc[:,1:]=subs","a3128b07":"sample_submission.head()","07613d5b":"sample_submission.to_csv('sample_submission.csv',index=False)","49d896fc":"Looking at the day wise trend,we see that there has been sudden peak and dips for certain days over the month.While the temperatures have been above 20 deg after late may, there is a dip in temperature less than 15 dec after Nov but there is a sudden increase in mid december.","df913289":"For the test set, if we try to compare between March month of train, we could see the temperatures have started from approx 3 deg and increased above 15 deg.\n\nLets try to build a quick baseline model and then come back to the EDA part to lookinto something useful.","6e0b4aac":"* The distribution of deg_C shows peaks between 20 to 30 deg.\n* There is a dip in relative humidity at 40% and there are two peaks at 30% and 45% approx.\n* The absolute humidity value shows peaks at 0.25g\/m3(i have assumed it to be g\/m3.Data info did not explicitly mention any units).\n* The distribution of sensor_1,2,3 & 5 appears to be left skewed whereas sensor-4 is normal with outliers at 500.","98e49121":"* Train is available from Mar 2010 to Jan 2011. Only 1 day is available in Jan 2011 and a closer look tells us that the same data point is repeated in test set. Lets remove that row in train data and use 2010 data for training.\n* Test data is for Jan 2011 to Apr 2011.","437a3983":"**Work in progress**"}}