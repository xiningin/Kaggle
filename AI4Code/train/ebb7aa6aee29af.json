{"cell_type":{"071e127f":"code","74dc08ec":"code","e3e188b6":"code","1f2fa6dc":"code","7c6a3aca":"code","e5ed0e46":"code","aae29619":"code","819b44ed":"code","60e3e210":"code","db8f0304":"code","7901f231":"code","f5cec882":"code","b677fdd9":"code","471beda5":"code","0dc81a8d":"code","de31dbb5":"code","577e844b":"code","e993419b":"code","d77fa97d":"code","2b229f3b":"code","aafc3672":"code","b3abb555":"code","dc3e3172":"code","fdd08a9c":"markdown","f86ff63b":"markdown","53ea3a96":"markdown","5d89f69b":"markdown","28d9c048":"markdown","a9e30e4e":"markdown","1139dfef":"markdown","a37c7f5b":"markdown"},"source":{"071e127f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport copy\n\nsns.set(rc = {'figure.figsize':(15,10)})\nplt.rcParams[\"figure.figsize\"] = (20,10)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","74dc08ec":"dataset = datasets.load_iris()\nX, y = dataset.data, dataset.target","e3e188b6":"df_iris = pd.DataFrame(data= np.c_[dataset['data'], dataset['target']],\n                     columns= dataset['feature_names'] + ['target'])\nsns.pairplot(df_iris, hue='target')","1f2fa6dc":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.7, random_state=42\n)\nacc = np.zeros(25)\nfor k in range(1,25):\n        # fit the knn algorithm\n        neigh = KNeighborsClassifier(n_neighbors=k)\n        neigh.fit(X_train, y_train)\n        # apply knn to test data\n        y_predict = neigh.predict(X_test)\n        acc[k] = acc[k] + np.sum(y_predict==y_test)\/len(y_predict)\nacc","7c6a3aca":"N=100\nacc = np.zeros(25)\nacc_train = np.zeros(25)\nfor r in range(N):\n    if np.mod(r,20)==0: print(r)\n    X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.7)\n    for k in range(1,25):\n        # fit the knn algorithm\n        neigh = KNeighborsClassifier(n_neighbors=k)\n        neigh.fit(X_train, y_train)\n        # apply knn to test data\n        y_predict = neigh.predict(X_test)\n        acc[k] = acc[k] + np.sum(y_predict==y_test)\/len(y_predict)\n        # apply knnn to train data\n        y_train_predict = neigh.predict(X_train)\n        acc_train[k] = acc_train[k] + np.sum(y_train_predict==y_train)\/len(y_train_predict)\n        #print('k='+str(k)+' acc='+str(acc[k]))\nacc = acc\/N\nacc_train = acc_train\/N","e5ed0e46":"markers_on = np.where(acc==np.max(acc))\nplt.plot(acc, '-gD', markevery=markers_on[0], label='Test Data')\nplt.plot(acc_train, '--r', label='Training Data')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nplt.ylim([0.85,1])\nplt.xlim([1,25])\nplt.legend()","aae29619":"fname = '\/kaggle\/input\/pixel-values-from-images-over-haiti\/HaitiPixels.csv'\ndf = pd.read_csv(fname)","819b44ed":"sns.pairplot(df, hue = \"Class\")","60e3e210":"data_Hati = df.to_numpy()\nX = data_Hati[:,1:4]\ny = data_Hati[:,0]","db8f0304":"np.unique(y)","7901f231":"idx_veg = np.where(y=='Vegetation')\nplt.scatter(X[idx_veg,0], X[idx_veg,2], c='g', alpha=0.5, label='Vegetation')\nidx_Rooftop = np.where(y=='Rooftop')\nplt.scatter(X[idx_Rooftop,0], X[idx_Rooftop,2], c='r', alpha=0.5, label='Rooftop')\nidx_Soil = np.where(y=='Soil')\nplt.scatter(X[idx_Soil,0], X[idx_Soil,2], c='saddlebrown', alpha=0.5, label='Soil')\nidx_Blue_Tarp = np.where(y=='Blue Tarp')\nplt.scatter(X[idx_Blue_Tarp,0], X[idx_Blue_Tarp,2], c='b', alpha=0.5, label='Blue Tarp')\nplt.ylabel('Blue')\nplt.xlabel('Red')\nplt.title('Haiti Pixels by Class')\nplt.legend()","f5cec882":"N=1\nacc = np.zeros(25)\nacc_train = np.zeros(25)\nfor r in range(N):\n    if np.mod(r,20)==0: print(r)\n    X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.7)\n    for k in range(1,25):\n        # fit the knn algorithm\n        neigh = KNeighborsClassifier(n_neighbors=k)\n        neigh.fit(X_train, y_train)\n        # apply knn to test data\n        y_predict = neigh.predict(X_test)\n        acc[k] = acc[k] + np.sum(y_predict==y_test)\/len(y_predict)\n        # apply knnn to train data\n        y_train_predict = neigh.predict(X_train)\n        acc_train[k] = acc_train[k] + np.sum(y_train_predict==y_train)\/len(y_train_predict)\n        #print('k='+str(k)+' acc='+str(acc[k]))\nacc = acc\/N\nacc_train = acc_train\/N","b677fdd9":"markers_on = np.where(acc==np.max(acc))\nplt.plot(acc, '-gD', markevery=markers_on[0], label='Test Data')\nplt.plot(acc_train, '--r', label='Training Data')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nplt.ylim([0.9,0.96])\nplt.xlim([1,25])\nplt.legend()","471beda5":"N=5\nTPR = np.zeros(25) # True Positive Rate = percent of targets that were found\nFAR = np.zeros(25) #False Alarm Rate = percent of non-targets that were predicted as target\nfor r in range(N):\n    if np.mod(r,20)==0: print(r)\n    X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.7)\n    idx_tarp = np.where(y_test=='Blue Tarp')\n    idx_notarp = np.where(y_test!='Blue Tarp')\n    for k in range(1,25):\n        # fit the knn algorithm\n        neigh = KNeighborsClassifier(n_neighbors=k)\n        neigh.fit(X_train, y_train)\n        # apply knn to test data\n        y_predict = neigh.predict(X_test)\n        TPR[k] = TPR[k] + np.sum(y_predict[idx_tarp]=='Blue Tarp')\/len(idx_tarp[0])\n        FAR[k] = FAR[k] + np.sum(y_predict[idx_notarp]=='Blue Tarp')\/len(idx_notarp[0])\nTPR = TPR\/N\nFAR = FAR\/N","0dc81a8d":"fig, (ax1, ax2) = plt.subplots(1,2)\n\nmarkers_on = np.where(TPR==np.max(TPR))\nax1.plot(TPR, '-gD', markevery=markers_on[0], label='True Positive Rate')\nax1.set_xlabel('K')\nax1.set_ylabel('True Positive Rate')\nax1.set_ylim([0.9,1])\nax1.set_xlim([1,25])\nax1.legend()\nax1.set_title('KNN Best TPR ='+str(round(np.max(TPR),3))+' at K ='+str(markers_on[0]))\n\nmarkers_on = np.where(FAR==np.min(FAR[1:]))\nax2.plot(FAR, '-rD', markevery=markers_on[0], label='False Alarm Rate')\nax2.set_xlabel('K')\nax2.set_ylabel('False Alarm Rate')\nax2.set_ylim([0,0.01])\nax2.set_xlim([1,25])\nax2.legend()\nax2.set_title('KNN Best FAR ='+str(round(np.min(FAR[1:]),5))+' at K ='+str(markers_on[0]))","de31dbb5":"k=10\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.7)\nidx_tarp = np.where(y_test=='Blue Tarp')\nidx_notarp = np.where(y_test!='Blue Tarp')\nneigh = KNeighborsClassifier(n_neighbors=k)\nneigh.fit(X_train, y_train)\n# apply knn to test data\ny_predict = neigh.predict(X_test)\ncm=confusion_matrix(y_test, y_predict)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=neigh.classes_)\ndisp.plot()\nTPR_5class = np.sum(y_predict[idx_tarp]=='Blue Tarp')\/len(idx_tarp[0])\nFAR_5class = np.sum(y_predict[idx_notarp]=='Blue Tarp')\/len(idx_notarp[0])\nprint('True Positive Rate = '+str(np.sum(y_predict[idx_tarp]=='Blue Tarp')\/len(idx_tarp[0])))\nprint('False Alarm Rate = '+str(np.sum(y_predict[idx_notarp]=='Blue Tarp')\/len(idx_notarp[0])))","577e844b":"k=10\nN = 50\nTPR_5class = 0\nFAR_5class = 0\nfor r in range(N):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, stratify=y, test_size=0.7)\n    idx_tarp = np.where(y_test=='Blue Tarp')\n    idx_notarp = np.where(y_test!='Blue Tarp')\n    neigh = KNeighborsClassifier(n_neighbors=k)\n    neigh.fit(X_train, y_train)\n    # apply knn to test data\n    y_predict = neigh.predict(X_test)\n    TPR_5class = TPR_5class + np.sum(y_predict[idx_tarp]=='Blue Tarp')\/len(idx_tarp[0])\n    FAR_5class = FAR_5class + np.sum(y_predict[idx_notarp]=='Blue Tarp')\/len(idx_notarp[0])\nTPR_5class = TPR_5class\/N\nFAR_5class = FAR_5class\/N\nprint('True Positive Rate = '+str(TPR_5class))\nprint('False Alarm Rate = '+str(FAR_5class))","e993419b":"idx_notarp = np.where(y!='Blue Tarp')\nyb = copy.deepcopy(y)\nyb[idx_notarp] = 'Not Tarp'","d77fa97d":"N=5\nTPR = np.zeros(25) # True Positive Rate = percent of targets that were found\nFAR = np.zeros(25) #False Alarm Rate = percent of non-targets that were predicted as target\nfor r in range(N):\n    if np.mod(r,20)==0: print(r)\n    X_train, X_test, y_train, y_test = train_test_split(\n    X, yb, stratify=yb, test_size=0.7)\n    idx_tarp = np.where(y_test=='Blue Tarp')\n    idx_notarp = np.where(y_test!='Blue Tarp')\n    for k in range(1,25):\n        # fit the knn algorithm\n        neigh = KNeighborsClassifier(n_neighbors=k)\n        neigh.fit(X_train, y_train)\n        # apply knn to test data\n        y_predict = neigh.predict(X_test)\n        TPR[k] = TPR[k] + np.sum(y_predict[idx_tarp]=='Blue Tarp')\/len(idx_tarp[0])\n        FAR[k] = FAR[k] + np.sum(y_predict[idx_notarp]=='Blue Tarp')\/len(idx_notarp[0])\nTPR = TPR\/N\nFAR = FAR\/N","2b229f3b":"fig, (ax1, ax2) = plt.subplots(1,2)\n\nmarkers_on = np.where(TPR==np.max(TPR))\nax1.plot(TPR, '-gD', markevery=markers_on[0], label='True Positive Rate')\nax1.set_xlabel('K')\nax1.set_ylabel('True Positive Rate')\nax1.set_ylim([0.9,1])\nax1.set_xlim([1,25])\nax1.legend()\nax1.set_title('KNN Best TPR ='+str(round(np.max(TPR),3))+' at K ='+str(markers_on[0]))\n\nmarkers_on = np.where(FAR==np.min(FAR[1:]))\nax2.plot(FAR, '-rD', markevery=markers_on[0], label='False Alarm Rate')\nax2.set_xlabel('K')\nax2.set_ylabel('False Alarm Rate')\nax2.set_ylim([0,0.01])\nax2.set_xlim([1,25])\nax2.legend()\nax2.set_title('KNN Best FAR ='+str(round(np.min(FAR[1:]),5))+' at K ='+str(markers_on[0]))","aafc3672":"k=4\nX_train, X_test, y_train, y_test = train_test_split(\n    X, yb, stratify=yb, test_size=0.7)\nidx_tarp = np.where(y_test=='Blue Tarp')\nidx_notarp = np.where(y_test!='Blue Tarp')\nneigh = KNeighborsClassifier(n_neighbors=k)\nneigh.fit(X_train, y_train)\n# apply knn to test data\ny_predict = neigh.predict(X_test)\ncm=confusion_matrix(y_test, y_predict)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=neigh.classes_)\ndisp.plot()","b3abb555":"k=4\nN = 50\nTPR_2class = 0\nFAR_2class = 0\nfor r in range(N):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, yb, stratify=yb, test_size=0.7)\n    idx_tarp = np.where(y_test=='Blue Tarp')\n    idx_notarp = np.where(y_test!='Blue Tarp')\n    neigh = KNeighborsClassifier(n_neighbors=k)\n    neigh.fit(X_train, y_train)\n    # apply knn to test data\n    y_predict = neigh.predict(X_test)\n    TPR_2class = TPR_2class + np.sum(y_predict[idx_tarp]=='Blue Tarp')\/len(idx_tarp[0])\n    FAR_2class = FAR_2class + np.sum(y_predict[idx_notarp]=='Blue Tarp')\/len(idx_notarp[0])\nTPR_2class = TPR_2class\/N\nFAR_2class = FAR_2class\/N\nprint('True Positive Rate = '+str(TPR_2class))\nprint('False Alarm Rate = '+str(FAR_2class))","dc3e3172":"print('True Positive Rate, 5 class = '+str(TPR_5class))\nprint('False Alarm Rate, 5 class = '+str(FAR_5class))\nprint('True Positive Rate, 2 class = '+str(TPR_2class))\nprint('False Alarm Rate, 2 class = '+str(FAR_2class))","fdd08a9c":"# Fisher's Iris Data:","f86ff63b":"Compute acccuracy and find k for best accuracy","53ea3a96":"# Haiti Blue Tarp Pixels: 2-Class Problem","5d89f69b":"k=10 looks pretty good","28d9c048":"# Summary","a9e30e4e":"Question; what K is best?","1139dfef":"# Haiti Pixel Data","a37c7f5b":"compute FAR and TPR and find a good k"}}