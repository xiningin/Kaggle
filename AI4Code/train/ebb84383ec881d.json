{"cell_type":{"1b13cd7b":"code","f3dddd1d":"code","0b1ce845":"code","b49b8c48":"code","eb959211":"code","704cd771":"code","5c82905c":"code","702c35a5":"code","084e7e54":"code","0c009ceb":"code","a8e6d7e2":"code","dccf54cb":"code","e9ed788a":"code","c7ab850e":"code","6f53e4b4":"code","ae3edd57":"code","addd3caa":"code","dea35ba4":"code","9b8aab44":"code","c3d76517":"code","17f36566":"code","481b1698":"code","cb2a8f29":"code","bc3c632a":"code","cdb92a71":"code","28f0d911":"code","59b8f750":"code","51f3f5ea":"code","a820140d":"code","c370d57a":"code","6f3dae61":"code","e49c68d8":"code","fdaa571c":"code","da53585e":"code","f6ef91d2":"code","360ab7ba":"code","866f3a12":"code","fb6e777c":"code","3332d8ea":"code","cdfcfe1c":"code","9e909669":"code","499f8472":"code","acc5cbeb":"code","f74616d5":"code","522f02c4":"code","ae5aedfe":"code","a5fd9f01":"code","54080b1f":"code","be018f74":"code","78defeca":"code","9a871d8c":"code","a8cc2a8e":"code","6e90b071":"code","88260bf7":"code","bf774b46":"code","c7436669":"code","d3840d38":"code","04d41778":"code","da3612ce":"code","bae72638":"code","538b7dee":"code","47598b57":"code","7aa92a2d":"code","5a80aca9":"code","066d2712":"code","04f356f5":"code","e11470f8":"code","a550e78c":"code","60f6b97f":"code","f999fa67":"code","92db7557":"code","042d6d79":"code","2bdacf22":"code","2125ae9f":"code","a313d8f2":"code","55840155":"code","381fe2b0":"markdown","517a1db0":"markdown","642a99d7":"markdown","ce7e2fde":"markdown","bd46a000":"markdown","5ea8af46":"markdown","4df05629":"markdown","d8843126":"markdown","3888dd0b":"markdown","06966a8e":"markdown","68771482":"markdown","c24ae49d":"markdown","9d1f169e":"markdown","abe66e16":"markdown","4ca7aab2":"markdown","972506c6":"markdown","10092caf":"markdown","ad10eb76":"markdown","5921f596":"markdown","01e20fb0":"markdown","8822959c":"markdown","54497be1":"markdown","bd91153d":"markdown","0b293296":"markdown","0a6371b3":"markdown","dccb3347":"markdown","86e9eac5":"markdown","d4fdd370":"markdown","5e1d8629":"markdown"},"source":{"1b13cd7b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max.columns', None)\n%matplotlib inline","f3dddd1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b1ce845":"# Import du dataset\n# from google.colab import files\n# uploaded = files.upload()\n\n# Local\n# This data is composed of multiple datasets for each brand of car\n# We firstly need to get all those datsets in one plca befor deciding of merging them all together\ndata_path= '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/'\ndatasets_dict= {}\n\nfor data in os.listdir(data_path):\n    if 'unclean' not in data:\n        datasets_dict[data.replace(\".csv\", \"\")] = pd.read_csv(f'{data_path}\/{data}')\n\nprint(datasets_dict.keys())","b49b8c48":"# We will then check if all the datatsets have the sames columns\ndata_set_columns= [dataset.columns for dataset in datasets_dict.values()]\n[print(sorted(set(columns))) for columns in data_set_columns]","eb959211":"# We need to add a column \"manufacturer\" on each dataset to keep track of it once they're all merged\nmanufacturers_dict={\n    \"focus\": \"ford\",\n    \"cclass\": \"mercedes\",\n    \"hyundi\": \"hyundai\",\n    \"merc\": \"mercedes\",\n    \"vw\": \"Volkswagen\"\n}\nfor manufacturer, dataset in datasets_dict.items():\n    if manufacturer in manufacturers_dict.keys():\n      dataset[\"Manufacturer\"]= manufacturers_dict[manufacturer]\n    else:\n      dataset[\"Manufacturer\"]= manufacturer","704cd771":"# We merge all datasets together\nfull_df= pd.concat(\n    list(datasets_dict.values()),\n    ignore_index= True)","5c82905c":"full_df.head()","702c35a5":"# Datset columns\nfull_df.columns","084e7e54":"# Infos\nfull_df.info()","0c009ceb":"# We will work on a copy of this data set\ndf= full_df.copy()\nprint(f\"full_df: {full_df.shape}, Copy: {df.shape}\")","a8e6d7e2":"df.describe().T","dccf54cb":"skew = df.describe().T\nskew['coef']=skew['std']\/skew['mean']\nskew","e9ed788a":"# Heatmap to visualize the empty columns\nplt.figure(figsize=(15,10))\nsns.heatmap(df.isnull())","c7ab850e":"# Percentage of missing values per column\n(full_df.isnull().sum()\/full_df.shape[0] * 100).sort_values(ascending= False)","6f53e4b4":"# Analysis of columns with more than 90% missing values\nmissing_cols= df.columns[full_df.isnull().sum()\/full_df.shape[0] > 0.90]\nmissing_cols","ae3edd57":"#\u00a0We fillna() the tax column withe the tax(\u00a3) columns\ndf[\"tax\"]= df[\"tax\"].fillna(df[\"tax(\u00a3)\"])\n\n# We then drop it\ndf.drop(\"tax(\u00a3)\", inplace=True, axis='columns')","addd3caa":"df.isnull().sum()\/df.shape[0] *100","dea35ba4":"df.shape","9b8aab44":"df.isnull().sum()\/df.shape[0]","c3d76517":"# We can drop the Nan values as they represent only 8% of the datframe\nprint(df.shape)\ndf= df.dropna()\nprint(df.shape)\n","17f36566":"df.dtypes","481b1698":"# List of numerical columns\ndf_numerical= [col for col in df.columns if df[col].dtype != 'object']\ndf_numerical","cb2a8f29":"\"\"\"Distribution of numerical values\"\"\"\n\nfig, ax = plt.subplots(3, 2, \n                       figsize=(10, 14))\ncol= 1\nfor i in df[df_numerical].columns:\n    plt.subplot(4, 2, col)\n    sns.distplot(df[i], color='blue')\n    col=col+1\n    plt.xlabel(i, fontsize=12)\n    plt.legend()\nplt.show()","bc3c632a":"# We will transform ou data frame to take values only where price is not null\ndf= df.dropna(subset=['price'])\ndf.shape","cdb92a71":"# We convert price column \nprice_col= df.price.astype(str)\nprice_col= price_col.str.replace('[\u00a3\\,]','').astype(float)","28f0d911":"# Target variable distribution\nsns.distplot(price_col)","59b8f750":"# Stastical informations abour target\nprint(f\"Median: {price_col.median()}\\nMean: {price_col.mean()}\\nMode: {price_col.mode()}\")","51f3f5ea":"# Clearer view of our target\nsns.boxplot(price_col)","a820140d":"# finally we replace the column in our dataset\ndf.price= price_col","c370d57a":"# Check of the type once again\ndf.dtypes","6f3dae61":"# Price by transmission type\nsns.barplot(x = df[\"transmission\"], y = df[\"price\"])","e49c68d8":"# Price by manufacturer\nsns.barplot(x = df[\"Manufacturer\"], y = df[\"price\"])","fdaa571c":"# Price by year\nplt.figure(figsize=(15,5),facecolor='w') \nsns.barplot(x = df[\"year\"], y = df[\"price\"])","da53585e":"df.mileage.isnull().sum()","f6ef91d2":"sns.pairplot(df[['mileage', 'price']])","360ab7ba":"df.engineSize.isnull().sum()","866f3a12":"sns.distplot(df.engineSize)","fb6e777c":"df.year.isnull().sum()","3332d8ea":"sns.boxplot(df.year)","cdfcfe1c":"# Value at 2060 outlier\ndf.year.sort_values()","9e909669":"# Deleting this row\ndf= df[df.year<=2021]","499f8472":"df.year.max()","acc5cbeb":"# null values\ndf.model.isnull().sum()","f74616d5":"# Number of models\nlen(df.model.unique())","522f02c4":"# Population of each model for a given manufacturer\nplt.figure(figsize=(7,11))\ndf[\"model\"].hist(by= df.Manufacturer, figsize= (15,11))","ae5aedfe":"# Most represented model in dataset\ndf.model.value_counts(normalize= True)*100","a5fd9f01":"df.transmission.isnull().sum()","54080b1f":"df.transmission.value_counts(normalize=True)*100","be018f74":"sns.countplot(df.transmission)","78defeca":"df[\"fuelType\"].isnull().sum()","9a871d8c":"plt.figure(figsize=(11,7))\nplt.xticks(rotation=90)\nsns.countplot(df['fuelType'])","a8cc2a8e":"df.columns","6e90b071":"# Correlation heatmap\nsns.heatmap(df.corr(),\n            annot= True,\n            square= True,\n            linewidth=1, linecolor='w')","88260bf7":"# Pair plots\nsns.pairplot(df.sample(frac= 0.8))","bf774b46":"# Country column\nmap_country={\n    'bmw':'germany', \n    'mercedes':'germany', \n    'audi':'germany', \n    'vauxhall': 'USA', \n    'ford': 'USA', \n    'toyota':'japan', \n    'hyundai':'south_korea',\n    'Volkswagen':'germany', \n    'skoda': 'czech'\n}\n\ndf[\"country\"]= df[\"Manufacturer\"].map(map_country)\ndf[[\"Manufacturer\", \"country\"]].head(3)","c7436669":"# Age column\ndf[\"age\"]= abs(df[\"year\"]-2021)\ndf[[\"year\", \"age\"]].head(3)","d3840d38":"# Droping the year column\ndf.drop(\"year\", axis=1, inplace=True)\ndf.columns","04d41778":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler","da3612ce":"#\u00a0Categorical columns\ndf_categorical = [col for col in df.columns if df[col].dtype =='object']\nprint(df_categorical)\n\n# Numerical columns\ndf_numerical = [col for col in df.columns if df[col].dtype !='object']\nprint(df_numerical)","bae72638":"#\u00a0One Hot encoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse= False, drop='first')\ntest = ohe.fit_transform(df[df_categorical])\ntest","538b7dee":"# Pandas dummies\ndf_expended= pd.get_dummies(df)\ndf_expended.shape","47598b57":"df_expended.head()","7aa92a2d":"# Standard scaler whithout standardisation\nfrom sklearn.preprocessing import RobustScaler\nstder= StandardScaler(with_std= False)\n\ndf_expended_std = stder.fit_transform(df_expended)\ndf_expended_std = pd.DataFrame(df_expended_std, columns = df_expended.columns)\nprint(df_expended_std.shape)\ndf_expended_std.head()","5a80aca9":"# Features\nX= df_expended_std.drop(\"price\", axis=1)\n\n# Target\ny= df_expended_std[\"price\"]\n\nprint(f\"X: {X.shape}\\ny: {y.shape}\")","066d2712":"# We creat our test and train sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=42)","04f356f5":"size={\n    \"x_test\": X_test.shape,\n    \"x_train\": X_train.shape,\n    \"y_test\": y_test.shape,\n    \"y_train\": y_train.shape\n}\nprint(size)","e11470f8":"column_names = df_expended_std.drop('price', axis= 1).columns\n\nno_of_features = []\nr_squared_train = []\nr_squared_test = []\n\n# We iterate over a range of 4, 223 for the number of best features\nfor k in range(4, 224, 2):\n    selector = SelectKBest(f_regression, \n                           k = k)\n    \n    # Our transformed sets with the k-best features\n    X_train_transformed = selector.fit_transform(X_train, y_train)\n    X_test_transformed = selector.transform(X_test)\n\n    # We train a basic regression model on those transformed sets\n    regressor = LinearRegression()\n    regressor.fit(X_train_transformed, y_train)\n\n\n    no_of_features.append(k)\n    r_squared_train.append(regressor.score(X_train_transformed, y_train))\n    r_squared_test.append(regressor.score(X_test_transformed, y_test))\n    \nsns.lineplot(x = no_of_features, y = r_squared_train, legend = 'full')\nsns.lineplot(x = no_of_features, y = r_squared_test, legend = 'full')","a550e78c":"# We can see that the curve stabilizes around ~160 variables\n# We will inspect more closely\n\nmax_test_score= max(r_squared_test)\nindex_max= r_squared_test.index(max_test_score)\nprint(f\"Best score is obtained for {no_of_features[index_max]} features --> score: {max_test_score}\") ","60f6b97f":"# We will see with less variables\nfor n_features in range(50, 224,2):\n  index_reasonable= no_of_features.index(n_features)\n  score= r_squared_test[index_reasonable]\n  print(\"--\",n_features, score)","f999fa67":"sns.lineplot(x = no_of_features[20:], y = r_squared_train[20:], legend = 'full')\nsns.lineplot(x = no_of_features[20:], y = r_squared_test[20:], legend = 'full')","92db7557":"# We selected 158 columns, let see them\nselector = SelectKBest(f_regression, k = 158)\n\n# Transformed sets\nX_train_transformed = selector.fit_transform(X_train, y_train)\nX_test_transformed = selector.transform(X_test)\n\n# Names of best features\nkbest_features = list(column_names[selector.get_support()])","042d6d79":"# Function to try our different models later\ndef test_regressor_model(models_list, X_train_transformed, X_test_transformed, y_train, y_test):\n  \"\"\"\n  - models_list: list of tuple, \n    - tuple[0] = model to test, \n    - tuple[1] = model_name\n  \"\"\"\n  for model in models_list:\n    model[0].fit(X_train_transformed, y_train)\n\n    y_pred= model[0].predict(X_test_transformed)\n    score= model[0].score(X_test_transformed, y_test)\n    print(f\"{model[1]:-<50}{score}\")\n","2bdacf22":"# Creation of the list of models we want to test\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge, ElasticNet, Lasso\n\nmodels_list= [(DecisionTreeRegressor(), \"DecisionTreeRegressor\"),\n              (Ridge(), \"Ridge regression\"),\n              (ElasticNet(), \"Elastic Net\"),\n              (Lasso(), \"Lasso\")]\n\n# Scoring              \ntest_regressor_model(models_list, X_train_transformed, X_test_transformed, y_train, y_test)","2125ae9f":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nparams= {'min_samples_split': range(2, 15)}\ngrid= GridSearchCV(DecisionTreeRegressor(), params, n_jobs=15)\n\ngrid.fit(X_train_transformed, y_train)","a313d8f2":"print(grid.best_params_)","55840155":"from sklearn.metrics import r2_score\n\ny_pred= grid.predict(X_test_transformed)\n\nr2_score(y_test, y_pred)","381fe2b0":"##\u00a0Splitting sets","517a1db0":"# Model optimisation\n- We will try to optimise as much as possible our selected model","642a99d7":"# Data pre-processing","ce7e2fde":"## Missing values","bd46a000":"#### Transmission\n- No NaN values\n- Distribution of each kind of transmission\n- Most of the transmissions are manual\n","5ea8af46":"#### Fuel type\n- No NaN values\n- Different kind of fuel type in the dataset\n- Most present fuel type is Petrol","4df05629":"#### Models\n- No NaN values in this column\n- We can observe the most present cars model for each manufacturer\n- The most present model in the dataset","d8843126":"## Numerical Values","3888dd0b":"- Mercedes\/audi\/BMW are the manufacturer which worth the most money","06966a8e":"# **Model**\n\n- For this regression study, we need to predic the price of a car based on all the different given parameters, in order to do so we will use a linear regression model.\n- We have:\n  - Feature engineered (kind of) by creating two variables, age and country of the manufacturer.\n  - Pre processed our data: One Hot Encoder, Standardisation\n- We will:\n  - Select the best features with SelectKBest: Trying multiple sets of features and choose a suitable number of features.","68771482":"- We do not see any high coeficient of variance","c24ae49d":"### Year\n- No NaN values\n- Value with year =2060\n  - Delete this row","9d1f169e":"### Drop NaN values","abe66e16":"##\u00a0Features creation\n- We will create:\n  - \"Country\" feature based on the manufacturer's country\n  - \"Age\" feature based on the age of the car, easily computable\n- We will remove the year column to avoid correlation","4ca7aab2":"### Engine Size\n\n","972506c6":"### Filling useful columns, deleting the useless ones\n","10092caf":"- 158 features seems a good choice as it's shows the first score of at least 0.85","ad10eb76":"### Target: price\n- Our target 'price' is an object column, we will transform it to int\n- **Distribution study**:\n  - *Positive skewness factor*: verified by mean > median > mode\n    -  We therefore know that we have some outliers with high weight, we keep it in mind just in case\n    ","5921f596":"- Automatic and semi-auto worth more money than manual transmission","01e20fb0":"##\u00a0Correlations\n- Important correlations:\n  - year\/mileage: -0.74\n  - year\/price: +0.49\n  - price\/engineSize: +0.64\n  - tax\/mpg: -0.45\n  - price\/mileage: -0.42","8822959c":"### Mileage\n- No NaN values\n- Obvious negative corrrelation between price and mileage\n  - will show a correlation heatmap later in the study\n","54497be1":"## Selecting best features\n- We have 223 features after the OHE with pd.get_dummies(), **I will therefore use the SelectKbest()** from sklearn in order to select the best features to apply regression.\n  - **SelectKBest** will do for us an univariate feature selection with a scoring based on f_regression. It will select the k best features based of the scoring of each one against the data set\n- I will **select from 4 to 223 features (k) on f_regression** in order to see the most revelant features in the dataset.\n- **We will select 158 features**","bd91153d":"# EDA\n- **Target variable**: \"price\"\n- **Shape**: (118150, 17)\n- **Missing values**:\n  - Clearly the tax(\u00a3) with **more than 90% of missing values**  \n- **Values types**","0b293296":"## Categorical values\n","0a6371b3":"##\u00a0Standardisation\n- We will then standardize all the variables in the data set\n- The standard score of a sample x is calculated as:\n\n```\n    z = (x - u) \/ s\n```\n\n  - u --> is the mean of the training samples or zero if with_mean=False\n  - s --> is the standard deviation of the training samples or one if with_std=False.","dccb3347":"# Merging all datasets\n- We will use the append() method to do so\n    - Some columns describe the same variables but are not named the same way\n    - We will therefore have to complete one of the column with the values of the other column","86e9eac5":"## One Hot encoder\n- In order to have only numerical values, we need to encode our categorical data\n- We will choose pandas.get_dummies over OHE from sklearn since it will keep column names more recognizable","d4fdd370":"- The most recent cars worth the most money\n- We can also see that cars old enought to be considered as **collection cars (here 1970) worth also good money**","5e1d8629":"## Trying different models\n- We will choose the DecisionTreeRegressor as it gives us a R^2 of 0.92"}}