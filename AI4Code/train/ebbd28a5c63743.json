{"cell_type":{"4d8f5948":"code","f4c8a10e":"code","2fd8a517":"code","fbdbae54":"code","c5c2e1b1":"code","a59685e0":"code","e84b1426":"code","97ed25e9":"code","1ea70750":"code","21b305c6":"code","2a20509e":"code","04a93888":"code","eaca5e9f":"code","66591548":"code","3bccc02b":"code","9f5a8923":"code","4d2fe5ab":"code","1fba4133":"code","e9b6102b":"code","45b44648":"code","f4e7f9e2":"code","0008fff0":"code","3894e532":"code","62a616fe":"code","5585bc44":"code","801acdc4":"code","852428ae":"markdown","cdcf47e7":"markdown","4852f44f":"markdown","7af95ac2":"markdown","ea704d34":"markdown","c96b9dd3":"markdown","5a31ab4d":"markdown"},"source":{"4d8f5948":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n#import plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC","f4c8a10e":"# Load data\n##### Load train and Test set\n\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n\n\ntrain.head()","2fd8a517":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","fbdbae54":"# Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Check for Null values\ndataset.isnull().sum()","c5c2e1b1":"# Infos\ntrain.info()\ntrain.isnull().sum()","a59685e0":"train.dtypes","e84b1426":"### Summarize data\n# Summarie and statistics\ntrain.describe()","97ed25e9":"full_data = [train, test]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \nfor dataset in full_data:\n   \n    dataset['Embarked'] = dataset['Embarked'].replace('S', 1)\n    dataset['Embarked'] = dataset['Embarked'].replace('C', 2)\n    dataset['Embarked'] = dataset['Embarked'].replace('Q', 3)\n\ntrain.head(2)","1ea70750":"# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nfor dataset in full_data:\n\n       \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n     # Mapping Age\n\ntrain.head()","21b305c6":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger name\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\ntrain.head(50)\n\n\n\n# Group all non-common titles into one single grouping \"Rare\"  .........\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n\nfor dataset in full_data:\n  \n # Mapping titles   .................\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n\n\n\ntrain = train.drop('Name', axis = 1)\ntest  = test.drop('Name', axis = 1)\ntrain.head()","2a20509e":"for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \nfor dataset in full_data:\n   \n    dataset['Embarked'] = dataset['Embarked'].replace('S', 1)\n    dataset['Embarked'] = dataset['Embarked'].replace('C', 2)\n    dataset['Embarked'] = dataset['Embarked'].replace('Q', 3)\n\ntrain.head()","04a93888":"full_data = [train, test]\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n# age\nfor dataset in full_data:\n    mean = train[\"Age\"].mean()\n    std = test[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train[\"Age\"].astype(int)\n    \n\nfor dataset in full_data: \n\n     dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0 \n\n     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1 \n\n     dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2 \n\n     dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3 \n\n     dataset.loc[ dataset['Age'] > 62, 'Age'] = 4\n\n# Create new feature FamilySize as a combination of SibSp and Parch\n\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain = train.drop(['SibSp','Parch'], axis = 1)\ntest  = test.drop(['SibSp','Parch'], axis = 1)\n\n\n\ntrain = train.drop('Ticket', axis = 1)\ntest = test.drop('Ticket', axis = 1)\n\n#Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntrain = train.drop('Cabin', axis = 1)\ntest = test.drop('Cabin', axis = 1)\n\ntrain = train.drop('CategoricalFare', axis = 1)\n\ntrain = train.drop('PassengerId', axis = 1)\n\n \ntrain.head()","eaca5e9f":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","66591548":"y_train = train['Survived'].ravel() # creat an array  : targat\n\ntrain = train.drop(['Survived'], axis=1) # :train_data\nx_train = train.values # Creates an array of the train data\nx_test = test.values ","3bccc02b":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n","9f5a8923":"#Cross Validation (K-fold)\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","4d2fe5ab":"# KNN\n\nclf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","1fba4133":"round(np.mean(score)*100, 2)","e9b6102b":"#Decision Tree\n\nclf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","45b44648":"# decision tree Score\nround(np.mean(score)*100, 2)","f4e7f9e2":"#Random Forest\nclf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","0008fff0":"# Random Forest Score\nround(np.mean(score)*100, 2)","3894e532":"#Naive Bayes\nclf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","62a616fe":"# Random Forest Score\nround(np.mean(score)*100, 2)","5585bc44":"#Testing\n\nclf = SVC()\nclf.fit(train, y_train)\n\ntest_data = test.drop(\"PassengerId\", axis=1).copy()\nprediction = clf.predict(test_data)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)","801acdc4":"submission = pd.read_csv('submission.csv')\n","852428ae":"Plots\nPearson Correlation Heatmap\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows","cdcf47e7":" 2. Load and check data \n\n\n  Load data","4852f44f":"Age and Cabin features have an important part of missing values.\n\nSurvived missing values correspond to the join testing dataset (Survived column doesn't exist in test set and has been replace by NaN values when concatenating the train and test set)","7af95ac2":"1. Introduction\n\nI have carefully analyzed a lot of notebooks and I have been working on this competition for some time. I have synthesized the results from various notebooks and experimented on my own on the data and reached 82.49 score, I hope this notebook is useful for someone","ea704d34":"3. Feature analysis\n","c96b9dd3":"check for null and missing values","5a31ab4d":"Takeaway from the Plots\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information."}}