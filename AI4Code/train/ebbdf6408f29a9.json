{"cell_type":{"693be2ae":"code","0f2458f7":"code","c6c6abaa":"code","9b2cea8b":"code","c19153e9":"code","a50d6e8f":"code","9cd0de66":"code","a2bbedbe":"code","f0cb617d":"code","d6729952":"code","d9f13bea":"code","7f27e2f6":"code","3f24e0ab":"code","481e7aa4":"code","f21dc2b4":"code","52feaa60":"code","1d9d52c8":"code","b0187fef":"code","051d099e":"code","492db86f":"code","103b680b":"code","600b7fec":"code","b743b521":"code","78e800a7":"code","d16ca5a5":"code","eb50980d":"code","f579669a":"code","3896168f":"code","d4f7ed64":"code","6085e6f6":"code","33709fae":"code","46d7da8a":"code","e44b87f1":"code","c3e5e3be":"code","f449de59":"code","3429fd8b":"code","5bea830a":"code","5c964802":"code","d2216760":"code","501dc158":"code","3ade1101":"code","016498fc":"code","464ccddd":"code","7372a7eb":"code","70ae8f3b":"code","3abf60a0":"code","74d1aec5":"code","7bf874b7":"code","e8c9e531":"code","51877795":"code","cc2327d4":"code","11a47494":"code","9359e88d":"code","dc4788e3":"code","42c432bb":"code","9cbc9575":"code","16d6ef06":"code","c24eec62":"code","98e894eb":"code","2ccc96bf":"code","ccc8f8df":"code","d34984a9":"code","9424398d":"code","950fca09":"code","ecdbf2b8":"markdown","c360bd05":"markdown","2ee318dd":"markdown","d4a5a5ff":"markdown","b0d69eed":"markdown","72245057":"markdown","8b693f37":"markdown","49403c90":"markdown","b04e6ae3":"markdown","bd96d98d":"markdown","6c45490d":"markdown","19bb88cf":"markdown","87e60734":"markdown","b02b5c22":"markdown","4b461515":"markdown","68fea414":"markdown","94bfb53f":"markdown","3b8462e7":"markdown","5b972eab":"markdown","55ce7a54":"markdown","9ec3c987":"markdown","1c925274":"markdown","0f2840bc":"markdown","5eac25d6":"markdown","8b5a8f6b":"markdown","a2f27d16":"markdown","eac78c0c":"markdown","dde4cfa2":"markdown","12dd2407":"markdown","414ca609":"markdown","da90bced":"markdown","ea1230d4":"markdown","1f39e72a":"markdown","8986f30d":"markdown","ac338405":"markdown","2d754c75":"markdown","0096b121":"markdown","ad079f3b":"markdown","739e040c":"markdown","dd8bedee":"markdown","6de86af3":"markdown","ae0451d8":"markdown","0da0b6d2":"markdown","905d778b":"markdown","5e31d961":"markdown","9eee78c2":"markdown","3eabda64":"markdown","0e9f4ab2":"markdown"},"source":{"693be2ae":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f2458f7":"# importing useful packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\npd.options.display.float_format = '{:,}'.format","c6c6abaa":"train = pd.read_csv(r'..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-jan-2022\/test.csv')","9b2cea8b":"#holidays = pd.read_csv(r'..\/input\/gdp-of-finland-norway-and-sweden-2015-2019\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv', parse_dates=['date'])\n#holidays.head()","c19153e9":"print(f'train shape: {train.shape}', f'test shape: {test.shape}', f'train rows \/ (train + test rows): {train.shape[0]\/(train.shape[0]+test.shape[0])}', sep='\\n')","a50d6e8f":"print(f'train columns: {train.columns}', f'test columns: {test.columns}', sep='\\n')","9cd0de66":"train.head()","a2bbedbe":"train['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])","f0cb617d":"train.isna().sum()","d6729952":"test.isna().sum()","d9f13bea":"train['country'].unique()","7f27e2f6":"train['store'].unique()","3f24e0ab":"train['product'].unique()","481e7aa4":"country_sold = train.groupby('country')['num_sold'].sum().sort_values(ascending=False)\ncountry_sold.map('{:,}'.format)","f21dc2b4":"plt.pie(country_sold.values, labels=country_sold.index, autopct='%0.1f%%')\nplt.title('Sales share per country')\nplt.show()","52feaa60":"products_mean = train.groupby(['country', 'store', 'product']).agg(\n    {'product': 'count', 'num_sold': 'mean'})\nproducts_mean['product'].map('{:,}'.format)\nproducts_mean['num_sold'].map('{:,}'.format)\nproducts_mean","1d9d52c8":"store_sold_average = train.groupby(['country', 'store']).agg(\n    {'num_sold': 'mean'})\nstore_sold_average","b0187fef":"store_sold_total = train.groupby(['store'])['num_sold'].sum()\nplt.pie(store_sold_total.values, labels=store_sold_total.index, autopct='%0.1f%%')\nplt.show()","051d099e":"sns.kdeplot(x=train['num_sold'], hue=train['country'])\nplt.title('Sales distribution')\nplt.show()","492db86f":"sns.kdeplot(x=train['num_sold'], hue=train['store'])\nplt.title('Sales distribution')\nplt.show()","103b680b":"train.set_index(train['date'], inplace=True)\ntest.set_index(test['date'], inplace=True)\ntrain.head()","600b7fec":"train['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Weekday'] = train.index.day_name()\ntrain.head()","b743b521":"plt.figure(figsize=[12, 6])\ntrain['num_sold'].plot(linewidth=0.5)","78e800a7":"for country in train['country'].unique():\n    temp_df = train.copy()\n    temp_df.loc[temp_df['country'] == country, 'num_sold'].plot(linewidth=0.5)\n    plt.title(country)\n    plt.show()","d16ca5a5":"for country in train['country'].unique():\n    temp_df = train.copy()\n    for store in temp_df['store'].unique():\n        temp_df.loc[temp_df['store'] == store, 'num_sold'].plot(linewidth=0.5)\n        plt.title([country, store])\n        plt.show()","eb50980d":"train['num_sold'].resample('Y').sum().map('{:,}'.format)","f579669a":"year_sales = train['num_sold'].resample('Y').sum()\nyear_sales.pct_change()","3896168f":"year_month_group = train.groupby(['Year', 'Month']).agg(\n    {'num_sold': 'sum'}).sort_values(by=['Year', 'num_sold'], ascending=False)\nyear_month_group['num_sold'] = year_month_group['num_sold'].map('{:,}'.format)\nyear_month_group","d4f7ed64":"month_group = train.groupby(['Month']).agg(\n    {'num_sold': 'sum'}).sort_values(by=['num_sold'], ascending=False)\nmonth_group['num_sold'] = month_group['num_sold'].map('{:,}'.format)\nmonth_group","6085e6f6":"weekday_sales = train.groupby(['Year', 'Weekday']).agg(\n    {'num_sold': 'sum'}).sort_values(by=['Year', 'num_sold'], ascending=False)\nweekday_sales['num_sold'] = weekday_sales['num_sold'].map('{:,}'.format)\nweekday_sales","33709fae":"sns.boxplot(data=train, x='Year', y='num_sold')\nplt.ylabel('num_sold')\nplt.title('Sales by Year')","46d7da8a":"sns.boxplot(data=train, x='Month', y='num_sold')\nplt.ylabel('num_sold')\nplt.title('Sales by Year')","e44b87f1":"df = train.groupby(['country','Year','Month']).num_sold.mean().reset_index()\nfig = plt.figure(figsize  = (20,13)) \nyear = 2015\nfor i in range(4):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.plot(df[(df['Year']==year) & (df['country']=='Norway')]['Month'], df[(df['Year']==year) & (df['country']=='Norway')]['num_sold'], label = 'Norway')\n    ax.plot(df[(df['Year']==year) & (df['country']=='Sweden')]['Month'], df[(df['Year']==year) & (df['country']=='Sweden')]['num_sold'], label = 'Sweden')\n    ax.plot(df[(df['Year']==year) & (df['country']=='Finland')]['Month'], df[(df['Year']==year) & (df['country']=='Finland')]['num_sold'], label = 'Finland')\n    ax.title.set_text(f'Avg Monthly Sales Trend in {year}')\n    ax.set_ylabel('Average Sales')\n    ax.set_xlabel('Month')\n    ax.legend()\n    year+=1","c3e5e3be":"df = train.groupby(['store','Year','Month']).num_sold.mean().reset_index()\nfig = plt.figure(figsize  = (20,13)) \nyear = 2015\nfor i in range(4):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.plot(df[(df['Year']==year) & (df['store']=='KaggleRama')]['Month'], df[(df['Year']==year) & (df['store']=='KaggleRama')]['num_sold'], label = 'KaggleRama')\n    ax.plot(df[(df['Year']==year) & (df['store']=='KaggleMart')]['Month'], df[(df['Year']==year) & (df['store']=='KaggleMart')]['num_sold'], label = 'KaggleMart')\n    ax.title.set_text(f'Avg Monthly Sales Trend in {year}')\n    ax.set_ylabel('Average Sales')\n    ax.set_xlabel('Month')\n    ax.legend()\n    year+=1","f449de59":"train['product'].unique()","3429fd8b":"df = train.groupby(['product','Year','Month']).num_sold.mean().reset_index()\nfig = plt.figure(figsize  = (20,13)) \nyear = 2015\nfor i in range(4):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.plot(df[(df['Year']==year) & (df['product']=='Kaggle Mug')]['Month'], df[(df['Year']==year) & (df['product']=='Kaggle Mug')]['num_sold'], label = 'Kaggle Mug')\n    ax.plot(df[(df['Year']==year) & (df['product']=='Kaggle Hat')]['Month'], df[(df['Year']==year) & (df['product']=='Kaggle Hat')]['num_sold'], label = 'Kaggle Hat')\n    ax.plot(df[(df['Year']==year) & (df['product']=='Kaggle Sticker')]['Month'], df[(df['Year']==year) & (df['product']=='Kaggle Sticker')]['num_sold'], label = 'Kaggle Sticker')\n    ax.title.set_text(f'Avg Monthly Sales Trend in {year}')\n    ax.set_ylabel('Average Sales')\n    ax.set_xlabel('Month')\n    ax.legend()\n    year+=1","5bea830a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport math","5c964802":"train_encoded = pd.concat([train, pd.get_dummies(train[['country', 'store', 'product']])], axis=1).drop(\n    columns=['row_id', 'date', 'country', 'store', 'product', 'country_Finland', 'store_KaggleMart', 'product_Kaggle Sticker'])\nweekday_dict = {\n    'Monday': 1,\n    'Tuesday': 2,\n    'Wednesday': 3,\n    'Thursday': 4,\n    'Friday': 5,\n    'Saturday': 6,\n    'Sunday': 7,\n}\ntrain_encoded['Weekday'] = train_encoded['Weekday'].map(weekday_dict)\ntrain_encoded.head()","d2216760":"X = train_encoded.iloc[:, 1:]\ny = train_encoded['num_sold']","501dc158":"linear_regression = LinearRegression()\nlinear_regression_fit = linear_regression.fit(X, y)","3ade1101":"y_pred = linear_regression_fit.predict(X)\nprint(f'Linear regression RMSE: {math.sqrt(mean_squared_error(y, y_pred))}')","016498fc":"pd.DataFrame(linear_regression_fit.coef_, X.columns, columns=['Coefficients'])","464ccddd":"month_dict = {\n    1: 3,\n    2: 2,\n    3: 3,\n    4: 3,\n    5: 3,\n    6: 2,\n    7: 1,\n    8: 1,\n    9: 1,\n    10: 1,\n    11: 1,\n    12: 3,\n}\n\nweekend_dict = {\n    1: 0,\n    2: 0,\n    3: 0,\n    4: 0,\n    5: 0,\n    6: 1,\n    7: 1}\ntrain_encoded['IsWeekend'] = train_encoded['Weekday'].map(weekend_dict)\ntrain_encoded['month_class'] = train_encoded['Month'].map(month_dict)\ntrain_encoded.head()","7372a7eb":"plt.rcParams['figure.figsize'] = (20, 10)\nsns.heatmap(train_encoded.corr(), annot=True, cmap=\"coolwarm\")","70ae8f3b":"X2 = train_encoded.iloc[:,1:]\nX2.drop(columns=['Month', 'Weekday'], inplace=True)\nlinear_regression_fit = linear_regression.fit(X2, y)\ny_pred2 = linear_regression_fit.predict(X2)\nprint(f'Linear regression RMSE: {math.sqrt(mean_squared_error(y, y_pred2))}')","3abf60a0":"pd.DataFrame(linear_regression_fit.coef_, X2.columns, columns=['Coefficients'])","74d1aec5":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","7bf874b7":"X_train, X_test, y_train, y_test = train_test_split(\n     X2, y, test_size=0.25, random_state=123)","e8c9e531":"random_forest = RandomForestRegressor()\nrandom_forest_fit = random_forest.fit(X_train, y_train)\ntest_rf_pred = random_forest_fit.predict(X_test)\nprint(f'Random forest RMSE: {math.sqrt(mean_squared_error(y_test, test_rf_pred))}')","51877795":"importances = random_forest.feature_importances_\nfeature_importance = pd.DataFrame(importances, X2.columns, columns=['Feature importance'])\nfeature_importance.sort_values(by='Feature importance', ascending=False).plot.bar(legend=None, title='Feature importance')","cc2327d4":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\nxgb_fit = xgb_reg.fit(X_train, y_train)\ntest_xgb_pred = xgb_fit.predict(X_test)\nprint(f'XGBoost RMSE: {math.sqrt(mean_squared_error(y_test, test_xgb_pred))}')","11a47494":"train.head()","9359e88d":"train.columns","dc4788e3":"avg_sales_2018 = train[train['Year'] == 2018].groupby(['country', 'store', 'product', 'Month', 'Weekday'], as_index=False)['num_sold'].mean()\navg_sales_2018['num_sold_2019'] = avg_sales_2018['num_sold'] + avg_sales_2018['num_sold'] * 0.08 \navg_sales_2018.head()","42c432bb":"test.head()","9cbc9575":"# test['Year'].unique() \ntest2 = test.copy()\ntest2['Year'] = pd.to_datetime(test.index).year\ntest2['Month'] = pd.to_datetime(test.index).month\ntest2['Weekday'] = pd.to_datetime(test.index).day_name()\ntest_pred = test2.merge(avg_sales_2018, \n                       how='inner', \n                       left_on=['country', 'store', 'product', 'Month', 'Weekday'],\n                       right_on=['country', 'store', 'product', 'Month', 'Weekday'])\ntest_pred.head()","16d6ef06":"#submission_df = pd.DataFrame({'row_id': test_pred['row_id'],'num_sold': test_pred['num_sold_2019']})\n#submission_df.to_csv('avg_predictions.csv', index = False)","c24eec62":"median_sales_2018 = train[train['Year'] == 2018].groupby(['country', 'store', 'product', 'Month', 'Weekday'], as_index=False)['num_sold'].median()\nmedian_sales_2018['num_sold_2019'] = median_sales_2018['num_sold'] + median_sales_2018['num_sold'] * 0.08\nmedian_pred = test2.merge(median_sales_2018, \n                       how='inner', \n                       left_on=['country', 'store', 'product', 'Month', 'Weekday'],\n                       right_on=['country', 'store', 'product', 'Month', 'Weekday'])\nsubmission_df = pd.DataFrame({'row_id': median_pred['row_id'],'num_sold': median_pred['num_sold_2019']})\nsubmission_df.to_csv('median_predictions.csv', index = False)","98e894eb":"test.head()","2ccc96bf":"test['Year'] = test.index.year\ntest['Month'] = test.index.month\ntest['Weekday'] = test.index.day_name()","ccc8f8df":"test.head()","d34984a9":"test_encoded = pd.concat([test, pd.get_dummies(test[['country', 'store', 'product']])], axis=1).drop(\n    columns=['row_id', 'date', 'country', 'store', 'product', 'country_Finland', 'store_KaggleMart', 'product_Kaggle Sticker'])\ntest_encoded['Weekday'] = test_encoded['Weekday'].map(weekday_dict)\ntest_encoded.head()","9424398d":"test_encoded['IsWeekend'] = test_encoded['Weekday'].map(weekend_dict)\ntest_encoded['month_class'] = test_encoded['Month'].map(month_dict)\ntest_encoded.drop(columns=['Month', 'Weekday'], inplace=True)\ntest_encoded.head()","950fca09":"# preds = random_forest_fit.predict(test_encoded)\n# submission_df = pd.DataFrame({'row_id':test['row_id'],'num_sold':preds})\n# submission_df.to_csv('submit_rf.csv', index = False)","ecdbf2b8":"- How many \/ Which countries contains a Kaggle store? \n- How many Kaggle stores are there in the dataset? H\n- How many \/ Which products are there? \n- Which countries\/stores sell more?","c360bd05":"Train correlations:","2ee318dd":"The same for the stores.","d4a5a5ff":"- The trend is exactly the same for each country, and what about the stores?\n","b0d69eed":"- Like for the countries and stores, the trend for the products is similar for every year.\n- Hats peak in April and December and has minmum sales in September-October.\n- Mugs peak in December and the sales dips in July-August.\n- Stickers follow almost same sales through out the Year irrespective of country, store etc.","72245057":"- It seems that features importance values are aligned with what we have seen earlier","8b693f37":"- Most of sales are concentrated in the weekend.\n\n\nLet's see the variability in each year and month.","49403c90":"Kaggle stores are situated only in northern Europe.","b04e6ae3":"Last plots didn't tell much more then what we saw earlier.","bd96d98d":"`num_sold` column is on another scale in comparison with other variables, but it is the target variable so we don't have to scale it.","6c45490d":"As We can expected, it seems that stores contain only nerd products :)","19bb88cf":"Sales increased for about 7% from 2017 to 2018!","87e60734":"Random Forest works much better, but I think tha the hey to improve more is working on feature engineering.","b02b5c22":"How many missing values are there in training and test sets?","4b461515":"Let's see if kaggle sales are going up through the years as it seems and which are the best months and days of the week to sell:","68fea414":"Kaggle sales are going up through last years.","94bfb53f":"**1. Preparing data for training**","3b8462e7":"Let's explore Sales average per month trend","5b972eab":"**Credits to other notebooks in the competition**","55ce7a54":"- Sales distribution is right skewed in all countries and stores","9ec3c987":"- Month has a negative coefficient but we say earlier that december is the best month every year. Maybe it's a good idea to classify eache month by past results\n- Kaggle Hat has a much higher coefficient than Kaggle Mug, and it's a good sign that they are both positive. In fact Kaggle Sticker is the worst product\n- Same insights for countries field coefficients\n- Weekday has a positive coefficient and it's correct, in fact we say tha sales go up from Monday to Friday","1c925274":"Since there are many outliers, I could try to use the median for the predictions","0f2840bc":"- KaggleRama seems to bee the best store, on average and on total sales, counting for the 63.5% of total amount\n\nLet's see the sales distribution for each country and store.","5eac25d6":"There are two different stores.","8b5a8f6b":"## EDA","a2f27d16":"I'll do a one hot encoding for categorical variables, except for weekdays. For weekdays I will distinguish between days, Fridays and other days of the week I will keep only month and weekday for the time fields, as we saw that the year seems to not influence the sales.","eac78c0c":"**I think that a linear model is not a good solution for this problem.**","dde4cfa2":"**First overview of the datasets:**","12dd2407":"We are very far from having a good result.\nIn any case, let's see model coefficients:","414ca609":"As we can see, we have to predict the sales occured in each store in order to predict are best ones going forward.","da90bced":"Which columns compose the datasets?","ea1230d4":"**3. Test preprocessing and predictions**","1f39e72a":"Sales are too variables, but we can instantly see some patterns:\n- there is always a peak in sales around December\/January then they go immediately down\n- after sales increase until May\/June\/July and they go down until October\/November\n- the cycle is repeated\n\nWe can say that there are regular seasonality and cyclical trends.\n\nLet's see if the trend is the same for each country and store:","8986f30d":"Baseline predictions with Random forest regressor:","ac338405":"- Trends are pretty much the same, lines are almost the same line translated, especially for Sweden and Finland\n\nWhat about the stores?","2d754c75":"- Variability through each year is pretty much the same.\n\n- There are many outliers. We'll to take care of that in training model section","0096b121":"The df is a timeseries, so I'll transform date column into the index and we can then add year, month and weekday name columns:","ad079f3b":"The root mean square error is a little bit lower. It seems that new features worked!\n\nLet's see new coefficients.","739e040c":"**Importing Datasets:**","dd8bedee":"There are no NA's, great news.","6de86af3":"Let's visualize something possible time patterns:","ae0451d8":"- Same insights from the stores\n\nAnd what about the products?","0da0b6d2":"- Countries, stores and products sold are equally distributed in train set, but some places sell more in quantity. I would say that every product and every store are registered for every day between 2015 and 2018\n- Norway through the years sold more than 4 millions of pieces, for about 43% of all sales\n- Finland in the country with the worst results\n- It seems that the Kaggle Hat is the best product everywhere, followed by the Kaggle Mug.\n\nLet's see which store sells more between KaggleRama and KaggleMart in each country: \n","905d778b":"## Model building","5e31d961":"**Target column deductions:**\n- It seems to be negatively correlated with Month column, but as we saw the best month for sales is december. I think I will only keep month class column in model training\n- Other columns interact with `num_sold` columns like we saw in EDA section: the day of the week influence the number of sales, the hat is the most sold product, Norway is the country with more sales and KaggleRama store sells more than KaggleMart\n\n**Interactions between predictors:**\n- `Weekday` column is highly correlated with `IsWeekend` column. I can think about excluding one of those.","9eee78c2":"Dividing the predictors from the target column:","3eabda64":"- December is the month with most sales, probably for Christmas\n- The The first part of the year from January to May is a good time for kaggle pockets","0e9f4ab2":"Now let's try some ensembles to boost our predictions, let's begin with a Random Forest"}}