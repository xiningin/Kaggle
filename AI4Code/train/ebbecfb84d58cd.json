{"cell_type":{"cbe38b3d":"code","ddbaa783":"code","968c8892":"code","0019fa63":"code","b9d604e2":"code","96d5f6d6":"code","23a2f59e":"code","e6736c32":"code","4b9708f9":"code","08b71978":"code","521e109c":"code","a1c566fe":"code","cf5f4bb4":"code","9a28a29f":"code","bad47bc0":"code","202d0554":"code","153b342d":"code","de1e6a92":"code","b3e6522e":"code","2a41eacc":"code","c8d2fea6":"code","a2ec3455":"code","4e9cf651":"code","8d9dedbc":"code","70f271df":"code","f56ee7db":"code","25f7c43a":"code","827ba5e1":"code","bfc0e74b":"code","e3b7efbd":"code","afaeb973":"markdown","9bf9df1f":"markdown","ebce3555":"markdown","9e9a3847":"markdown","8eeb1952":"markdown","22dccd0c":"markdown","3a848ff3":"markdown","24fbc13d":"markdown","46a8a64e":"markdown","56e95c24":"markdown","4ed58f2f":"markdown","0856a343":"markdown","1328839c":"markdown","a7c9842a":"markdown","5ae17039":"markdown","aa80df7d":"markdown","84b0a48d":"markdown","f6c147a3":"markdown","8e7736d4":"markdown","573a1658":"markdown","36e63f42":"markdown","53817c06":"markdown","f8003ffc":"markdown","2749b967":"markdown","8134f991":"markdown","665d4421":"markdown","8289ba5b":"markdown","6181443b":"markdown","b003af36":"markdown"},"source":{"cbe38b3d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","ddbaa783":"test = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")","968c8892":"fig,ax = plt.subplots(1,2, figsize=(20,6))\nsns.distplot(train.target, ax=ax[0])\nsns.boxplot(train.target, ax=ax[1], color=\"maroon\", saturation=4.6)\nplt.show()","0019fa63":"outlier_band = (np.quantile(train.target,0.75) - np.quantile(train.target,0.25))*1.5\nlow, high = np.quantile(train.target,0.25) - outlier_band, np.quantile(train.target,0.75) + outlier_band \ntrain = train[ (train.target>low) & (train.target<high)]","b9d604e2":"fig,ax = plt.subplots(1,2, figsize=(20,6))\nsns.distplot(train.target, ax=ax[0], color=\"green\")\nsns.boxplot(train.target, ax=ax[1], color=\"gold\", saturation=0.6)\nplt.show()","96d5f6d6":"corr=train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nfig, ax = plt.subplots(figsize=(18,12)) \nsns.heatmap(corr,cmap=\"RdPu\", mask=mask, annot=True)","23a2f59e":"from sklearn.feature_selection import SelectKBest, mutual_info_regression\nfs = SelectKBest(score_func=mutual_info_regression, k=\"all\")\nfs.fit(train.drop(['target'],axis=1), train.target)\nX_n = fs.transform(train.drop(['target'],axis=1))","e6736c32":"score = pd.concat([pd.DataFrame(train.columns),pd.DataFrame(fs.scores_)],axis=1)\nscore.columns = [\"feature\",\"scores\"]\nscore = score.sort_values(\"scores\", ascending=False)\nscore = score[score.feature != \"target\"]\nsns.barplot(x=score.scores, y=score.feature)\nplt.title(\"Importance of features\")","4b9708f9":"fig,axes = plt.subplots(5,3, figsize=(30,30))\nsns.boxplot(train.id, ax=axes[0,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont1, ax=axes[0,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont2, ax=axes[0,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont3, ax=axes[1,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont4, ax=axes[1,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont5, ax=axes[1,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont6, ax=axes[2,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont7, ax=axes[2,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont8, ax=axes[2,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont9, ax=axes[3,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont10, ax=axes[3,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont11, ax=axes[3,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont12, ax=axes[4,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont13, ax=axes[4,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont14, ax=axes[4,2], color=\"yellow\", saturation=0.75)","08b71978":"fig,axes = plt.subplots(5,3, figsize=(30,30))\nsns.regplot(train.id, train.target, ax=axes[0,0], color=\"purple\")\nsns.regplot(train.cont1, train.target,ax=axes[0,1], color=\"purple\")\nsns.regplot(train.cont2, train.target,ax=axes[0,2], color=\"purple\")\nsns.regplot(train.cont3, train.target,ax=axes[1,0], color=\"purple\")\nsns.regplot(train.cont4, train.target,ax=axes[1,1], color=\"purple\")\nsns.regplot(train.cont5, train.target,ax=axes[1,2], color=\"purple\")\nsns.regplot(train.cont6, train.target,ax=axes[2,0], color=\"purple\")\nsns.regplot(train.cont7, train.target,ax=axes[2,1], color=\"purple\")\nsns.regplot(train.cont8, train.target,ax=axes[2,2], color=\"purple\")\nsns.regplot(train.cont9, train.target,ax=axes[3,0], color=\"purple\")\nsns.regplot(train.cont10, train.target,ax=axes[3,1], color=\"purple\")\nsns.regplot(train.cont11, train.target,ax=axes[3,2], color=\"purple\")\nsns.regplot(train.cont12, train.target,ax=axes[4,0], color=\"purple\")\nsns.regplot(train.cont13, train.target,ax=axes[4,1], color=\"purple\")\nsns.regplot(train.cont14, train.target,ax=axes[4,2], color=\"purple\")","521e109c":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont2, train.target, color=\"black\")\nplt.axvline(0.22, color='purple')\nplt.axvline(0.35, color='violet')\nplt.axvline(0.415, color='blue')\nplt.axvline(0.481, color='green')\nplt.axvline(0.549, color='yellow')\nplt.axvline(0.612, color='orange')\nplt.axvline(0.673, color='red')\nplt.axvline(0.727, color='pink')\nplt.axvline(0.747, color='black')\nplt.title(\"Binning cont2\")","a1c566fe":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont3, train.target)\nplt.axvline(0.386, color=\"black\")\nplt.axvline(0.78, color=\"white\")\nplt.title(\"Binning cont3\")","cf5f4bb4":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont9, train.target, color=\"green\")\nplt.axvline(0.108, color=\"red\")\nplt.axvline(0.1165, color=\"pink\")\nplt.axvline(0.559, color=\"magenta\")\nplt.axvline(0.84, color=\"orange\")\nplt.title(\"Binning cont9\")","9a28a29f":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont14, train.target, color=\"blue\")\nplt.axvline(0.34, color=\"yellow\")\nplt.axvline(0.525, color=\"orange\")\nplt.axvline(0.66, color=\"red\")\nplt.axvline(0.78, color=\"pink\")\nplt.title(\"Binning cont14\")","bad47bc0":"lims2 = [-0.1,0.22, 0.35, 0.415, 0.489, 0.549, 0.612, 0.673, 0.727, 0.747,0.9]\nlims3 = [0,0.386, 0.78,1.5]\nlims9 =  [-0.2,0.108, 0.1165, 0.559, 0.84,5]\nlims14 = [0,0.34, 0.525, 0.66, 0.78,0.9]\n\ntrain[\"c14\"] = pd.cut(train.cont14, bins=lims14, labels=np.arange(0,5), include_lowest=True)\ntrain[\"c2\"] = pd.cut(train.cont2, bins=lims2, labels=np.arange(0,10),  include_lowest=True)\ntrain[\"c3\"] = pd.cut(train.cont3, bins=lims3, labels=np.arange(0,3),include_lowest=True)\ntrain[\"c9\"] = pd.cut(train.cont9, bins=lims9, labels=np.arange(0,5), include_lowest=True)","202d0554":"train[[\"c14-0\",\"c14-1\",\"c14-2\",\"c14-3\",\"c14-4\"]] = pd.DataFrame(pd.get_dummies(train.c14))\ntrain[[\"c2-0\",\"c2-1\",\"c2-2\",\"c2-3\",\"c2-4\",\"c2-5\",\"c2-6\",\"c2-7\",\"c2-8\",\"c2-9\"]] = pd.DataFrame(pd.get_dummies(train.c2))\ntrain[[\"c3-0\",\"c3-1\",\"c3-2\"]] = pd.DataFrame(pd.get_dummies(train.c3))\ntrain[[\"c9-0\",\"c9-1\",\"c9-2\",\"c9-3\",\"c9-4\"]] = pd.DataFrame(pd.get_dummies(train.c9))","153b342d":"train.drop([\"c2\",\"c3\",\"c9\",\"c14\"],axis=1,inplace=True)","de1e6a92":"train.drop([\"target\"], axis=1).corrwith(train.target).to_frame().sort_values(0, ascending=False).style.background_gradient(cmap=\"RdPu\")","b3e6522e":"x = train.drop([\"target\",\"id\"],axis=1)\ny=train.target","2a41eacc":"from sklearn.model_selection import train_test_split\nx_tr, x_te, y_tr, y_te = train_test_split(x,y, test_size=0.33)","c8d2fea6":"from sklearn.model_selection import GridSearchCV","a2ec3455":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor(objective='reg:squarederror', verbose=0)\nXG = GridSearchCV(xg_reg, scoring='neg_mean_squared_error', cv=5,param_grid={'colsample_bytree':[0.1], 'learning_rate':[0.1, 0.01], 'max_depth':[16], 'alpha':[5], 'n_estimators':[50]})\nXG.fit(x_tr,y_tr)\nprint(XG.score(x_te,y_te))","4e9cf651":"import catboost as ctb\nctb_reg = ctb.CatBoostRegressor(verbose=0)\nCB = GridSearchCV(ctb_reg, scoring='neg_mean_squared_error', cv=5,param_grid={'learning_rate':[0.1], 'max_depth':[12]})\nCB.fit(x_tr,y_tr)\nprint(CB.score(x_te,y_te))","8d9dedbc":"import sklearn.neighbors as knn\nknn_reg = knn.KNeighborsRegressor(n_neighbors=5, algorithm='kd_tree')\nKNN = GridSearchCV(knn_reg, scoring='neg_mean_squared_error', cv=5,param_grid={'n_neighbors':[5], 'weights':['uniform','distance']})\nKNN.fit(x_tr,y_tr)\nprint(KNN.score(x_te,y_te))","70f271df":"import sklearn.tree as t\ndt_reg = t.DecisionTreeRegressor(criterion='mse')\nDT = GridSearchCV(dt_reg, scoring='neg_mean_squared_error', cv=5, param_grid={'max_depth': [16]})\nDT.fit(x_tr,y_tr)\nprint(DT.score(x_te,y_te))","f56ee7db":"import sklearn.tree as t\next_reg = t.ExtraTreeRegressor(criterion='mse')\nET = GridSearchCV(ext_reg, scoring='neg_mean_squared_error', cv=5, param_grid={'max_depth': [16]})\nET.fit(x_tr, y_tr)\nprint(ET.score(x_te,y_te))","25f7c43a":"import lightgbm as lgb\nlgb = lgb.LGBMRegressor()\nL = GridSearchCV(lgb, scoring='neg_mean_squared_error', cv=5, param_grid={'max_depth': [8,9,10,11,12,13,14,15,16], 'n_estimators':[1000,5000], 'learning_rate':[0.01]})\nL.fit(x_tr,y_tr)\nL.score(x_te,y_te)","827ba5e1":"lims2 = [-0.1,0.22, 0.35, 0.415, 0.489, 0.549, 0.612, 0.673, 0.727, 0.747,0.9]\nlims3 = [0,0.386, 0.78,1.5]\nlims9 =  [-0.2,0.108, 0.1165, 0.559, 0.84,5]\nlims14 = [0,0.34, 0.525, 0.66, 0.78,0.9]\n\n\ntest[\"c14\"] = pd.cut(test.cont14, bins=lims14, labels=np.arange(0,5), include_lowest=True)\ntest[\"c2\"] = pd.cut(test.cont2, bins=lims2, labels=np.arange(0,10),  include_lowest=True)\ntest[\"c3\"] = pd.cut(test.cont3, bins=lims3, labels=np.arange(0,3),include_lowest=True)\ntest[\"c9\"] = pd.cut(test.cont9, bins=lims9, labels=np.arange(0,5), include_lowest=True)\n\ntest[[\"c14-0\",\"c14-1\",\"c14-2\",\"c14-3\",\"c14-4\"]] = pd.DataFrame(pd.get_dummies(test.c14))\ntest[[\"c2-0\",\"c2-1\",\"c2-2\",\"c2-3\",\"c2-4\",\"c2-5\",\"c2-6\",\"c2-7\",\"c2-8\",\"c2-9\"]] = pd.DataFrame(pd.get_dummies(test.c2))\ntest[[\"c3-0\",\"c3-1\",\"c3-2\"]] = pd.DataFrame(pd.get_dummies(test.c3))\ntest[[\"c9-0\",\"c9-1\",\"c9-2\",\"c9-3\",\"c9-4\"]] = pd.DataFrame(pd.get_dummies(test.c9))\nidzz=test.id\ntest.drop([\"c2\",\"c3\",\"c9\",\"c14\",\"id\"],axis=1,inplace=True)","bfc0e74b":"model = L\nmodel.fit(x,y)\nyhat = model.predict(test)","e3b7efbd":"output = pd.DataFrame({\"Id\":idzz, \"target\":yhat})\noutput.to_csv('submission.csv', index=False)","afaeb973":"**Let's have a look under the hood of our ML predictors, shall we?**","9bf9df1f":"### CatBoost","ebce3555":"# Trial models","9e9a3847":"### Decision trees","8eeb1952":"### Regression plots:","22dccd0c":"## Correlation of all features with target","3a848ff3":"### Creating the predictor and target sets","24fbc13d":"# Feature engg","46a8a64e":"# Final model","56e95c24":"### XGBoost","4ed58f2f":"As we can see from the correlation matrix some of the features are correlated. We need to something about them :(","0856a343":"Removiing outliers which are more than 1.5* 3rd quantile and less than 1.5* 1st quartile","1328839c":"### LightGBM","a7c9842a":"### KNeighboursRegressor","5ae17039":"## Correlation checking between features and target variable","aa80df7d":"## LightGBM offers the best result","84b0a48d":"Using mutual_info_regression to capture even non linear-relation between target and predictor variables","f6c147a3":"Now converting this categorical data into binomial distribution for each feature state","8e7736d4":"Binning the data for features:\n1. cont2 - [0.22, 0.35, 0.415, 0.489, 0.549, 0.612, 0.673, 0.727, 0.747]\n2. cont3 - [0.386, 0.78]\n3. cont9 - [0.108, 0.1165, 0.559, 0.84]\n4. cont14 - [0.34, 0.525, 0.66, 0.78]","573a1658":"### Splitting the data into train and test sets","36e63f42":"### ExtratreesRegressor","53817c06":"## Outlier removal","f8003ffc":"# Instantiation","2749b967":"### Let's have a look at the variance of each feature","8134f991":"### Let's have a closer look at more interesting features\n1. cont2\n2. cont3\n3. cony9\n4. cont14","665d4421":"Applying all the transformations to test set:","8289ba5b":"We will try with the following algorithms and use GridSearchCV to choose the parameters and the model. Please note that I have generally only mentioned the hyperparameter that I finally used for the model because my laptop is slow :(\n1. XGBoost\n2. CatBoost\n3. KNeighborsRegressor\n4. DecisionTreesRegressor\n5. ExtraTreesRegressor\n6. LightGBM\n","6181443b":"Dropping the categorical bins which we had created","b003af36":"# Data input and exploration"}}