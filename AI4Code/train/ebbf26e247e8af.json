{"cell_type":{"6c10691b":"code","194800f8":"code","1499632b":"code","45bc4ea4":"code","e9548862":"code","71be205c":"code","8b58be37":"code","85fa0e22":"code","46307f7b":"code","c397ae35":"code","5fa7c522":"code","d9d27993":"code","52facb27":"code","51ed22b8":"code","511f7caa":"code","eb194aa1":"code","2a86ec89":"code","52e97647":"code","e7e3621f":"code","65c7e107":"code","65e671e9":"code","985f5c38":"code","8c775663":"code","ce4f7cb6":"code","52777edf":"code","00b93228":"code","c6ff8b49":"code","5a147463":"code","940b1991":"code","281dcb6f":"code","77860546":"markdown","a7dad0e2":"markdown","ff870f73":"markdown","6f50ee77":"markdown","52d062d3":"markdown","5b341a08":"markdown","ff989d04":"markdown","1e4eba43":"markdown","2230d635":"markdown","fea69557":"markdown","7f1ef05e":"markdown","3030a698":"markdown","848ef5a4":"markdown","cd8d0a98":"markdown","fd4fe81c":"markdown","9c6bcdb5":"markdown","1caaa013":"markdown","45fa0a16":"markdown","d04e8b71":"markdown","d5e6735c":"markdown"},"source":{"6c10691b":"import numpy as np\nimport pandas as pd\n\n\nfrom bokeh.io import output_notebook\n\nfrom bokeh.models import ColumnDataSource, ColorBar, LinearColorMapper, BasicTicker, HoverTool\nfrom bokeh.layouts import row, column, grid\n\nfrom bokeh.plotting import figure, show, output_file\nfrom bokeh.palettes import Viridis256, Spectral7, Inferno\nfrom bokeh.transform import factor_cmap\n\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\nimport sklearn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\n\nfrom scipy.stats import skew\n\noutput_notebook()","194800f8":"files = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n        \nraw_data = pd.read_csv(files[1]).set_index('Id')\n\ntest_df = pd.read_csv(files[2]).set_index('Id')\n","1499632b":"print(f'Number of samples in train.csv : {len(raw_data)}\\n')\n\nprint(f'Number of rows in test.csv : {len(test_df)}\\n')\n\nprint(f'Number of features for both training and testing : {len(raw_data.columns)}\\n ')\nraw_data.head(5)","45bc4ea4":"print(raw_data.isna().sum()[0:10])\n\n# we don't have missing values ! otherwise we would have to work on (taking average\/removing\/...)","e9548862":"raw_data.describe().transpose()","71be205c":"clean_df = raw_data.copy()\n\ndel raw_data\n\n\nclean_df.drop(columns = ['Soil_Type7', 'Soil_Type15'], inplace = True)\n\ntest_df.drop(columns = ['Soil_Type7', 'Soil_Type15'], inplace = True)\n\n\ntrain_target = clean_df.pop('Cover_Type')\n\nclean_df['Hillshade_9am'] = np.clip(clean_df['Hillshade_9am'].values, 0,255)\ntest_df['Hillshade_9am'] = np.clip(test_df['Hillshade_9am'].values, 0,255)\n\nclean_df['Hillshade_Noon'] = np.clip(clean_df['Hillshade_Noon'].values, 0,255)\ntest_df['Hillshade_Noon'] = np.clip(test_df['Hillshade_Noon'].values, 0,255)\n\n\nclean_df['Hillshade_3pm'] = np.clip(clean_df['Hillshade_3pm'].values, 0,255)\ntest_df['Hillshade_3pm'] = np.clip(test_df['Hillshade_3pm'].values, 0,255)\n\n\n\n\n\nclean_df['Aspect'] = np.mod(clean_df['Aspect'].values, 360)\ntest_df['Aspect'] = np.mod(test_df['Aspect'].values, 360)\n\n","8b58be37":"clean_df['HighWater'] = (clean_df['Vertical_Distance_To_Hydrology']< 0).astype(np.int16)\n\ntest_df['HighWater'] = (test_df['Vertical_Distance_To_Hydrology']< 0).astype(np.int16)\n\n\n\nclean_df['EVDtH'] = clean_df['Elevation'] -clean_df['Vertical_Distance_To_Hydrology']\ntest_df['EVDtH'] = test_df['Elevation'] -test_df['Vertical_Distance_To_Hydrology']\n\n\n#clean_df['EHDtH'] = clean_df['Elevation'] -clean_df['Horizontal_Distance_To_Hydrology']*0.2\n#test_df['EHDtH'] = test_df['Elevation'] -test_df['Horizontal_Distance_To_Hydrology']*0.2","85fa0e22":"forest_type = ['Spruce\/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'CottonWood\/willow', 'Aspen', 'Douglas-fir', 'Krummholz']\n\nclass_ratio = train_target.value_counts().sort_index().to_list()\n\n\n\nsource = ColumnDataSource(data = dict(x_values = forest_type, y_values = class_ratio, color = Spectral7))\n\nTOOLTIPS = [\n    (\"Type of forest\", \"@x_values\"),\n    ('value', \"@y_values\")\n]\n\n\n\np = figure(x_range = forest_type,\n           y_axis_type=\"log\",\n           y_range = [0.1,10**7],\n           width = 800,\n           height = 600,\n           title = 'Target feature proportion by class', tooltips = TOOLTIPS)\n\np.vbar(x = 'x_values',\n       top = 'y_values',\n       source = source, \n       alpha = 0.8,\n       line_color = 'white',\n       color = 'color',\n       bottom=0.1,\n      width = 0.9)\n\np.xgrid.grid_line_color = None\np.background_fill_color = '#fafafa'\n\nshow(p)","46307f7b":"number_sample = len(train_target)\n\nfor idx,forest in enumerate(forest_type):\n    \n    print(f'Class {idx+1} corresponding to {forest} forest represents {class_ratio[idx]\/number_sample:.5%} of the total dataset')","c397ae35":"cat_features = [ 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n       'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n       'Soil_Type39', 'Soil_Type40','HighWater']\n\n\n\nnumber_cat_features = len(cat_features)\n\nfigures = []\n\npalette = [Viridis256[63], Viridis256[191]]\n\nfor num, cat in enumerate(cat_features):\n        \n    cat_serie = clean_df[cat].value_counts()\n    \n    source = ColumnDataSource(data = \n                              dict(x_values = cat_serie.index.astype('str').to_list(),\n                                   y_values = cat_serie.to_list()))\n    \n    p = figure(x_range = source.data['x_values'],\n               width = 200, height = 200,\n               title = 'Proportion of '+cat,\n               toolbar_location=None,\n               tools=\"\")\n    \n    p.vbar(x = 'x_values',\n           top ='y_values',\n           source = source,\n           line_color='white',\n           fill_color=factor_cmap('x_values', palette=palette, factors = source.data['x_values']),\n           width = 0.9)\n    \n    p.title.text_font_size = '8pt'\n    p.xgrid.grid_line_color = None\n    p.yaxis.visible = False\n    \n    figures.append(p)\n    \n\nshow(grid(figures, ncols = 6))\n    \n    ","5fa7c522":"num_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n                'Horizontal_Distance_To_Fire_Points']\n\n\ndef plot_violin(dataframe, features, sample_size):\n    \n    distrib_df = dataframe[features].sample(sample_size).melt(var_name = 'column_name', value_name = 'values')\n    \n    \n    plt.figure(figsize = (20,8))\n    \n    sns.violinplot(x = 'column_name', y = 'values', data = distrib_df)\n    \n    plt.title('Distribution of numerical features without normalisation')\n    plt.xlabel(\" \")\n    plt.xticks(rotation=45)\n    plt.show()\n    \nplot_violin(clean_df,num_features, 100000)\n    \n    ","d9d27993":"def make_histogram(dataframe, features):\n    \n    figures = []\n    \n    for feature in features : \n    \n        hist_val, edges_val = np.histogram(dataframe[feature], density=True, bins=100)\n\n        p = figure(width = 500, height = 300, title = 'Histogram of raw '+ feature, tools='', background_fill_color=\"#fafafa\")\n\n        p.quad(top=hist_val, bottom=0, left=edges_val[:-1], right=edges_val[1:],\n               fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n        p.y_range.start = 0\n        p.xaxis.axis_label = 'x'\n        p.yaxis.axis_label = 'Pr(x)'\n        p.grid.grid_line_color=\"white\"\n        \n        figures.append(p)\n        \n    return figures\n\nhistos = make_histogram(clean_df,num_features)\n\nshow(grid(histos, ncols = 3))\n    \n    ","52facb27":"print('Training data ----------- \\n')\n\nfor feat in num_features : \n    print('Skewness of the ' +feat+ f' distribution is : {skew(clean_df[feat])}')\n    \n\nprint('Testing data ----------- \\n')\n\nfor feat in num_features : \n    print('Skewness of the ' +feat+ f' distribution is : {skew(test_df[feat])}')\n","51ed22b8":"\ndef handle_neg_skew(dataframe, feature):\n    \n    find_max = (dataframe[feature]+1).max()\n        \n\n    return  np.sqrt(find_max - dataframe.pop(feature))\n\n\n\n\nclean_df['Sqrt_Hillshade_Noon'] = handle_neg_skew(clean_df,'Hillshade_Noon')\ntest_df['Sqrt_Hillshade_Noon'] = handle_neg_skew(test_df,'Hillshade_Noon')\n\n\n\nclean_df['Sqrt_Hillshade_9am'] = handle_neg_skew(clean_df,'Hillshade_9am')\ntest_df['Sqrt_Hillshade_9am'] = handle_neg_skew(test_df,'Hillshade_9am')\n\n\n\n\nprint('Training data')\nprint('Skewness of Sqrt_Hillshade_Noon distribution is : ',skew(clean_df['Sqrt_Hillshade_Noon']))\nprint('Skewness of Sqrt_Hillshade_9am distribution is : ',skew(clean_df['Sqrt_Hillshade_9am']))\n\nprint('Testing data')\nprint('Skewness of Sqrt_Hillshade_Noon distribution is : ',skew(test_df['Sqrt_Hillshade_Noon']))\nprint('Skewness of Sqrt_Hillshade_9am distribution is : ',skew(test_df['Sqrt_Hillshade_9am']))\n\n\n","511f7caa":"histos = make_histogram(clean_df,['Sqrt_Hillshade_9am','Sqrt_Hillshade_Noon'])\n\nshow(grid(histos, ncols = 2))","eb194aa1":"\n\"\"\"\nclean_df['Sqrt_Horizontal_Distance_To_Hydrology'] = np.sqrt(np.clip(clean_df['Horizontal_Distance_To_Hydrology'],0,None))\n\nhistos = make_histogram(clean_df,['Horizontal_Distance_To_Hydrology','Sqrt_Horizontal_Distance_To_Hydrology'])\n\n\n\n\nshow(grid(histos, ncols = 2))\n\nclean_df['Sqrt_Horizontal_Distance_To_Roadways'] = np.sqrt(np.clip(clean_df['Horizontal_Distance_To_Roadways'],0,None))\n\nhistos = make_histogram(clean_df,['Horizontal_Distance_To_Roadways','Sqrt_Horizontal_Distance_To_Roadways'])\n\n\nshow(grid(histos, ncols = 2))\n\nclean_df['Sqrt_Horizontal_Distance_To_Fire_Points'] = np.sqrt(np.clip(clean_df['Horizontal_Distance_To_Fire_Points'],0,None))\n\nhistos = make_histogram(clean_df,['Horizontal_Distance_To_Fire_Points','Sqrt_Horizontal_Distance_To_Fire_Points'])\n\n\nshow(grid(histos, ncols = 2))\n\n#The problem with clipping is clearly visible across histograms, the distribution form is sligthly modified for $0$.\n\nclean_df.drop(columns = ['Horizontal_Distance_To_Hydrology',\n                         'Horizontal_Distance_To_Roadways',\n                         'Horizontal_Distance_To_Fire_Points'], inplace = True)\n\n\ntest_df['Sqrt_Horizontal_Distance_To_Fire_Points'] = np.sqrt(np.clip(test_df.pop('Horizontal_Distance_To_Fire_Points'),0,None))\ntest_df['Sqrt_Horizontal_Distance_To_Roadways'] = np.sqrt(np.clip(test_df.pop('Horizontal_Distance_To_Roadways'),0,None))\ntest_df['Sqrt_Horizontal_Distance_To_Hydrology'] = np.sqrt(np.clip(test_df.pop('Horizontal_Distance_To_Hydrology'),0,None))\n\n\n\n\"\"\"\nprint()","2a86ec89":"clean_df['target'] = train_target\n","52e97647":"\n\ndef plot_correlation_heatmap(dataframe_corr, width = 900, height = 800):\n    \n    dataframe_corr.index.name = 'Features1'\n    dataframe_corr.columns.name = 'Features2'\n\n    corr_matrix = pd.DataFrame(dataframe_corr.stack(), columns = ['correlation']).reset_index()\n\n\n\n    mapper = LinearColorMapper(palette = Viridis256,\n                              low = corr_matrix['correlation'].min(),\n                              high = corr_matrix['correlation'].max())\n\n\n    TOOLS = \"hover,save,pan, box_zoom,reset\"\n\n    p = figure(title = 'Correlation Matrix',\n              x_range = corr_matrix['Features1'].drop_duplicates().to_list(),\n              y_range = corr_matrix['Features2'].drop_duplicates().to_list(),\n              x_axis_location = 'below',\n              width = width,\n              height = height,\n              tools = TOOLS,\n              toolbar_location = 'left')\n\n\n    p.grid.grid_line_color = None\n    p.axis.axis_line_color = None\n    p.axis.major_tick_line_color = None\n    p.axis.major_label_text_font_size = \"7px\"\n    p.axis.major_label_standoff = 0\n    p.xaxis.major_label_orientation = np.pi \/ 3\n\n    p.rect(x='Features1', y=\"Features2\", width=1, height=1,\n           source=corr_matrix,\n           fill_color={'field': 'correlation', 'transform': mapper},\n           line_color=None)\n\n    color_bar = ColorBar(color_mapper=mapper,\n                         major_label_text_font_size=\"7px\",\n                         ticker=BasicTicker(desired_num_ticks=256),\n                         border_line_color=None)\n    p.add_layout(color_bar, 'right')\n\n    return p\n\n\n\n#corr_df = clean_df.corr()\n\n#p = plot_correlation_heatmap(corr_df)\n\n#show(p)","e7e3621f":"soil_features = [x for x in clean_df.columns if x.startswith('Soil_Type')]\n\nclean_df['Soil_type_count'] = clean_df[soil_features].sum(axis = 1)\ntest_df['Soil_typ_count'] = test_df[soil_features].sum(axis = 1)\n\n\nwilderness_features = [x for x in clean_df.columns if x.startswith('Wilderness_Area')]\n\nclean_df['Wilderness_area_count'] = clean_df[wilderness_features].sum(axis=1)\ntest_df['Wilderness_area_count'] = test_df[wilderness_features].sum(axis=1)","65c7e107":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n\n\n\nclean_df = reduce_mem_usage(clean_df)\n\ntest_df = reduce_mem_usage(test_df)","65e671e9":"new_num_features = ['Elevation', 'Aspect', 'Slope',\n                    'Horizontal_Distance_To_Hydrology',\n                    'Vertical_Distance_To_Hydrology',\n                    'Horizontal_Distance_To_Roadways',\n                    'Sqrt_Hillshade_9am', 'Sqrt_Hillshade_Noon', 'Hillshade_3pm',\n                    'Horizontal_Distance_To_Fire_Points', 'EVDtH']\n\n\nmean_val = clean_df[new_num_features].mean()\nstandard_dev_val = clean_df[new_num_features].std()   \n\n\nclean_df[new_num_features] = (clean_df[new_num_features] - mean_val )\/ standard_dev_val\n\n\n\ntest_df[new_num_features] = (test_df[new_num_features] - mean_val )\/ standard_dev_val  \n","985f5c38":"clean_df = clean_df.loc[clean_df['target'] !=5]\n\nmapping = {1: 0,\n           2: 1,\n           3: 2,\n           4: 3,\n           6: 4,\n           7: 5}\n\n\nclean_df['target'].replace(mapping, inplace=True)\n","8c775663":"my_dict = clean_df['target'].value_counts().to_dict()\n\nclass_weight = {}\n\ntotal = len(clean_df)\n\nfor key, value in my_dict.items():\n    class_weight[key] = (1\/value) * (total\/len(my_dict))\n    \nprint(class_weight)\n\n\n","ce4f7cb6":"train_target = np.array(clean_df.pop('target'))\n\ntrain_values = np.asarray(clean_df)\n","52777edf":"BATCH_SIZE = 2048\n\nEPOCHS = 30\n\nFOLDS = 10\n\nNUMBER_FEATURE = 56\n\n\npredictions = np.zeros((1,1))\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n                                              patience = 2,\n                                              factor = 0.5,\n                                              verbose = 0)\n\n\nearly_stop = keras.callbacks.EarlyStopping(monitor ='val_accuracy',\n                                           patience = 4,\n                                           restore_best_weights = True, \n                                           verbose = 1)","00b93228":"def make_model(input_size):\n    \n    model = tf.keras.Sequential([\n        keras.layers.InputLayer(input_shape = (input_size,)),\n        keras.layers.Dense(units = 256 , activation = 'relu'),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(units = 128, activation = 'relu'),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(units = 128,  activation = 'relu'),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(units = 64, activation = 'relu'),\n        keras.layers.Dense(units = 6, activation = 'softmax'),\n        \n        \n    ])\n    \n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = 4e-4),\n                 loss =  tf.keras.losses.SparseCategoricalCrossentropy() ,\n                 metrics = ['accuracy'])\n    \n    return model\n\n    \n    \ndef build_dataset(features_matrix, target):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((features_matrix,target))\n    #dataset = dataset.batch(BATCH_SIZE, drop_remainder = True).prefetch(1)\n    \n    return dataset","c6ff8b49":"complete_hist = []\n\ncross_val = StratifiedKFold(n_splits= FOLDS, random_state = 1337, shuffle = True)\n\nfor fold_idx, (train_idx, val_idx) in enumerate(cross_val.split(train_values, train_target)):\n    \n    X_train, X_val = train_values[train_idx], train_values[val_idx]\n    y_train, y_val = train_target[train_idx], train_target[val_idx]\n    \n    \n    \n    mymodel = make_model(input_size = NUMBER_FEATURE)\n    \n    history = mymodel.fit(X_train,y_train,\n                   batch_size = BATCH_SIZE,\n                   epochs = EPOCHS,\n                   validation_data = (X_val,y_val),\n                   callbacks = [reduce_lr,early_stop],\n                   verbose = 0)\n    print(f\"----Fold number {fold_idx}-----\")\n    #print(f\"Training Loss : {history.history['loss'][-1]} , Training Accuracy : {history.history['accuracy'][-1]} \")\n    print(f\"Validation Loss : {history.history['val_loss'][-1]} Validation Accuracy : {history.history['val_accuracy'][-1]}\")\n    \n    \n    \n    predictions = predictions + mymodel.predict(np.asarray(test_df))\n    \n    complete_hist.append(history)\n    \n    \n    \n#x_train, x_val, y_train, y_val = train_test_split(np.asarray(clean_df),train_target , test_size = 0.15, stratify =train_target )\n\n\n#Stratify option allows to split the dataset in order to keep proportion regarding train_target,\n#So classes proportions will remain the same accross training and validation set \n\n\n","5a147463":"def plot_metrics(history,custom_title=\"\"):\n    \n    titles = ['Loss', 'Accuracy']\n    metrics = ['loss', 'val_loss', 'accuracy','val_accuracy']\n    \n    palette = Inferno[4]\n    figures = []\n    \n    epochs = len(history.history['loss'])\n    \n    for k in range(0,4,2):\n        \n        p = figure(width = 600, height = 400, title = custom_title)\n        \n        \n        \n        p.line(np.arange(epochs), history.history[metrics[k]], legend_label = 'training', line_width = 4, color = palette[1])\n        \n        p.line(np.arange(epochs), history.history[metrics[k+1]], line_width = 4, legend_label = 'validation', color = palette[2])\n        \n        figures.append(p)\n        \n    return figures\n\nshow(row(plot_metrics(complete_hist[-1], custom_title= 'Last Fold results ')))","940b1991":"pred_df = pd.DataFrame({'pred_val' :  np.argmax(predictions, axis = 1)})\n\n\nmapping = {0: 1,\n           1: 2,\n           2: 3,\n           3: 4,\n           4: 6,\n           5: 7}\n\n\npred_df['pred_val'].replace(mapping, inplace=True)\n","281dcb6f":"submission_df = pd.DataFrame(data = {'Id' : test_df.index, 'Cover_Type' : pred_df.values.reshape(-1,)}).set_index('Cover_Type')\n\nsubmission_df.to_csv('submission.csv', header=True)","77860546":"### Class Weights\n____\n\nWe have to remember that we are dealing with imbalanced dataset, so we don't have many positive samples for few classes,\n\nSo the idea is to build a classifier that heavily weight the few examples that are available. It is possible to do so, by passing Keras weights for each class through a parameter `best_weight` in `model.fit`\n\n\n**Unfortunately sometimes, this does not improve convergence\/accuracy. On this dataset using CLASS WEIGHTS DOES NOT IMPROVE ACCURACY**\n___","a7dad0e2":"---\nDealing with Skeness with other variable like `Horizontal_Distance_To_Hydrology` or `Horizontal_Distance_To_Hydrology` is more complex.\n\nBecause they have a **Positive Skew** but also **negative values**\n\nHowever when dealing with positive skew, you mostly use to type of transformation :\n\n- $\\log(x)+ eps$\n\n- $\\sqrt{x}$\n\n\nBoth function that cannot handle negative values. **Thus two solution are possible :**\n\n* **Adding a constant** (not great when facing new values that are possibly worse)\n\n* **Removing\/Clipping the negative values** (not great, because we are losing information on the real distribution)\n\n\nSince we have a lot of data here, we will clip the negative values to $0$\n\n---\n\n> **After trying to fix Skewed distributions, results on this dataset are WORSE. Sometimes data transformation isn't useful !! We won't use it !!**\n\n","ff870f73":"<a id=\"sec5\"><\/a>\n## Building and Training a NN model\n\n___\n\n**Many approaches might work, one can try for example :**\n\n1. Should we normalised one-hot encoded features? (thus *mean* and *standard deviation* are the same across all variables)\n3. Modifying more feature distributions \n\n\n> But first let's remove class 5 from our training dataset","6f50ee77":"---\nMost of Categorical features are imbalanced so the features are sparse. \n\nBy analyzing their impact on target features, we might be able to remove few of them.\n\n**But before doing so, let's explore Numerical and Ordinal features**\n\n---","52d062d3":"---\n**After analysing each features' histogram, some  features clearly got skewed distributions.**\n\n- *Negative Skew* for example `Hillshade_Noon`\n\n- *Positive Skew* for example `Horizontal_Distance_To_Roadways`\n\n**A solution would to apply *transformations* in order to obtain more Bell-like distribution (Gaussian Distribution)**\n\n- Why ?\n\n\n- Having distributions close to the normal distribution might help to model to learn relationships between variables ! (i think?)\n\n---","5b341a08":"# Table of Content\n\n\n* [Introduction](#sec1)\n* [Librairies](#sec2)\n* [Loading Data\/Rapid Overview](#sec3)\n* [Feature Engineering\/Data Cleansing](#sec4)\n   - [Quick Data Transformation](#subsec1)\n   - [Data Visualisation](#subsec2)\n   - [Dealing With Skewness](#subsec3)\n   - [Correlation between features](#subsec4)\n   - [Reducing Memory Usage](#subsec5)\n* [Building and Training an NN model](#sec5)\n","ff989d04":"<a id=\"sec3\"><\/a>\n## Loading Data \/ Rapid Overview\n\n___\n\nThe dataset was generated by CTGAN using a real dataset, [Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview).\n\nAfter going through discussions on this dataset, few things can be retrieve :\n\n1. Several features can be created to improve accuracy\n2. Understanding features \n\n\nFrom all these information, let's build a solid classifier achieve strong results.\n___","1e4eba43":"---\n1. `ReduceLROnPlateau` is a function that once our model is *stuck* will help by decreasing the learning rate and help the model to reach more accuratly the local minima\n\n2. `EarlyStopping` just helps by stopping the training when no improvement is seen after a consecutive number of epoch\n---","2230d635":"> Note : Few more features could have been added, but we will keep it intuitive\n<a id=\"subsec5\"><\/a>\n### Reducing memory usage\n___\n\nWhen loading large dataset in pandas, adjusting ***types*** can drastically reduce memory usage!","fea69557":"<a id=\"subsec2\"><\/a>\n### Data Analysis (Visualisation)\n\n___\nWe clearly have two main types of features :\n\n- ***Numerical***  : Elevation, Aspect , ...\n- ***Categorical*** : Soil_TypeX, Wilderness_AreaX\n\n\nFirst, let's analyse closely all Categorical Features, how balanced they are\n\n___\n\n","7f1ef05e":"___\n**Many information can be deduced from the correlation heatmap**\n\n1. Because `Wilderness Areas` are exclusive, they are highly correlated between each other\n2. `Soil Type` features have more or less the same impact\n3. `Elevation` is highly correlated to target\n\n\n\nLet's first normalise inputs, then through the process of creating models we will find the optimal solution (more or less)\n___","3030a698":"<a id=\"subsec4\"><\/a>\n### Correlation between features\n___\n\n\nSometimes a simplest way to find relationship between variables is through correlation matrix.\n\nThis matrix can help creating new features or even dropping ones\n\n___","848ef5a4":"<a id=\"sec2\"><\/a>\n## Librairies","cd8d0a98":"<a id=\"sec4\"><\/a>\n## Feature Engineering \/ Data Cleansing\n___\n<a id=\"subsec1\"><\/a>\n### Quick Data Transformation\n\n\n**Some features are *static*, their values are always the same :**\n\n**-`Soil_Type7`**\n\n**-`Soil_Type15`**\n\n**Thus we can rapidly remove them**\n\n\n**Moreover few features are going *beyond* their supposed range**\n\nThanks to the information gave here : [fixing features](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373)\n\n\nWe know that :\n\n1. **The following variables should have values that lie between $[0-255]$**\n\n    - `Hillshade_Noon` \n    - `Hillshade_9am` \n    - `Hillshade_3pm`\n\n**2. `Aspect` is an angle variable and its values must lie between $[0 -360]$**\n","fd4fe81c":"---\n**First, let's focus on `Hillshade_Noon` and `Hillshade_9am`.** \n\n**Moreover other variables have negative values, most transformation do not handle this type of data**\n\n**Skew values that lie in between $[-1,1]$ won't be analysed, because this skewness value isn't important**\n\n*(Also because, i still have everything to learn in terms of statistics)*\n\n\nThey both have negative values, so two transformation are possible to make them more like Normal Distribution :\n\n- $\\sqrt{max(x+1) -x}$\n\n- $\\log({max(x+1) - 1}) +eps$\n\n\nAfter some analysis using `square_root` provide bettern results ( should i quickly add a function to show it?)\n\n---","9c6bcdb5":"<a id=\"subsec3\"><\/a>\n### Dealing with Skewness","1caaa013":">**Normally we should use the max found in training set for the test set however after checking both split have the same max. So it changes nothing**","45fa0a16":"**Thanks to a [discussion](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/discussion\/10693) from the first Competition, we know that adding features to indicate when values are negative or positive might be good**","d04e8b71":" ___\n **Classes are clearly imbalanced**: \n \n - ***Aspen type* has only one example ! Even if we keep it, our model won't be able to predict it properly.**\n\n - **Class 1 and 2 are overly presents and overshadow other classes**\n \n We must keep the same ratio when splitting the data to training and validation set !\n \n Plus removing Class 5 might be an idea\n \n ___","d5e6735c":"<a id=\"sec1\"><\/a>\n## Introduction\n\n\nThis month, more than building a complete pipeline easy to follow along, we are going to focus a bit on Data Transformation and **how handling skewness can benefits a model (or not)**.\n\nObviously, as always this notebook mostly uses **basic Bokeh visualisation**.\n\nThis month data is based on a previous Competition [Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview). \n\nFrom multiple variables the main goal is to predict a Forest Type. Because this competition is close to a previous one, many insights found for the previous competition can also be applied. Thanks to many posts, one can improve his model.([Fixing features](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373))\n\nHowever here, we are not going to focus on that aspect."}}