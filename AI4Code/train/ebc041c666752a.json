{"cell_type":{"228c9b78":"code","145c6854":"code","fe92777c":"code","aff97c73":"code","90c4719a":"code","407d3bc1":"code","3577fb08":"code","0451ebc9":"code","60e98402":"code","e2033e8b":"code","5cc7c2dc":"code","f8e6014d":"code","1c3eff82":"code","12a9c89d":"code","65db2744":"code","48c3b1ce":"code","2a1eddea":"code","6099cd52":"code","62180fea":"code","87299363":"code","2f9332e0":"code","5b21518a":"code","61fda7c7":"code","c5693a69":"code","67ce8cb2":"code","e992cf5d":"code","282f1560":"code","68606be9":"code","936cf65f":"markdown","2520063b":"markdown","7dab47d5":"markdown","d1de0768":"markdown","b27312a2":"markdown","65e3dd65":"markdown","75bef214":"markdown","48d4b05e":"markdown","6531e0bd":"markdown","61cf12c6":"markdown","6402f269":"markdown","835b0051":"markdown","c1784fd9":"markdown","1e6af803":"markdown","0476606c":"markdown","68c7fddb":"markdown","b5280b4f":"markdown","a152936c":"markdown","bb281df4":"markdown","e8540032":"markdown"},"source":{"228c9b78":"!pip install advertools","145c6854":"import advertools as adv\nimport pandas as pd\npd.options.display.max_columns = None\nimport plotly.graph_objects as go\nimport plotly\nprint('advertools v' + str(adv.__version__))\nprint('pandas v' + str(pd.__version__))\nprint('plotly v' + str(plotly.__version__))\n","fe92777c":"flights = pd.read_csv('\/kaggle\/input\/flights-serps-and-landing-pages\/flights_serp_scrape.csv')\nflights.head(2)","aff97c73":"serps_to_plot = flights.copy()\nserps_to_plot.columns = flights.columns.str.replace('serp_', '').str.replace('scrape_', '')\ndef plot_data(serps_to_plot, num_domains=10, select_domain=None):\n#     df = pd.DataFrame(serp_results, columns=serp_results[0].keys())\n    df = serps_to_plot\n    if select_domain:\n        df = df[df['displayLink'].isin(select_domain)]\n    top_domains = df['displayLink'].value_counts()[:num_domains].index.tolist()\n    top_df = df[df['displayLink'].isin(top_domains)]\n    top_df_counts_means = (top_df\n                           .groupby('displayLink', as_index=False)\n                           .agg({'rank': ['count', 'mean']}))\n    top_df_counts_means.columns = ['displayLink', 'rank_count', 'rank_mean']\n    top_df = (pd.merge(top_df, top_df_counts_means)\n              .sort_values(['rank_count', 'rank_mean'],\n                           ascending=[False, True]))\n    rank_counts = (top_df\n                   .groupby(['displayLink', 'rank'])\n                   .agg({'rank': ['count']})\n                   .reset_index())\n    rank_counts.columns = ['displayLink', 'rank', 'count']\n    summary = (df\n               .groupby(['displayLink'], as_index=False)\n               .agg({'rank': ['count', 'mean']})\n               .sort_values(('rank', 'count'), ascending=False)\n               .assign(coverage=lambda df: (df[('rank', 'count')]\n                                            .div(df[('rank', 'count')]\n                                                 .sum()))))\n    summary.columns = ['displayLink', 'count', 'avg_rank', 'coverage']\n    summary['displayLink'] = summary['displayLink'].str.replace('www.', '')\n    summary['avg_rank'] = summary['avg_rank'].round(1)\n    summary['coverage'] = (summary['coverage'].mul(100)\n                           .round(1).astype(str).add('%'))\n    num_queries = df['queryTime'].nunique()\n\n    fig = go.Figure()\n    fig.add_scatter(x=top_df['displayLink'].str.replace('www.', ''),\n                    y=top_df['rank'], mode='markers',\n                    marker={'size': 30, 'opacity': 1\/rank_counts['count'].max()})\n\n    fig.add_scatter(x=rank_counts['displayLink'].str.replace('www.', ''),\n                    y=rank_counts['rank'], mode='text',\n                    text=rank_counts['count'])\n\n    for domain in rank_counts['displayLink'].unique():\n        rank_counts_subset = rank_counts[rank_counts['displayLink'] == domain]\n        fig.add_scatter(x=[domain.replace('www.', '')],\n                        y=[0], mode='text',\n                        marker={'size': 50},\n                        text=str(rank_counts_subset['count'].sum()))\n\n        fig.add_scatter(x=[domain.replace('www.', '')],\n                        y=[-1], mode='text',\n                        text=format(rank_counts_subset['count'].sum() \/ num_queries, '.1%'))\n        fig.add_scatter(x=[domain.replace('www.', '')],\n                        y=[-2], mode='text',\n                        marker={'size': 50},\n                        text=str(round(rank_counts_subset['rank']\n                                       .mul(rank_counts_subset['count'])\n                                       .sum() \/ rank_counts_subset['count']\n                                       .sum(), 2)))\n\n    minrank, maxrank = min(top_df['rank'].unique()), max(top_df['rank'].unique())\n    fig.layout.yaxis.tickvals = [-2, -1, 0] + list(range(minrank, maxrank+1))\n    fig.layout.yaxis.ticktext = ['Avg. Pos.', 'Coverage', 'Total<br>appearances'] + list(range(minrank, maxrank+1))\n\n#     fig.layout.height = max([600, 100 + ((maxrank - minrank) * 50)])\n    fig.layout.height = 600\n    fig.layout.yaxis.title = 'SERP Rank (number of appearances)'\n    fig.layout.showlegend = False\n    fig.layout.paper_bgcolor = '#eeeeee'\n    fig.layout.plot_bgcolor = '#eeeeee'\n    fig.layout.autosize = True\n    fig.layout.margin.r = 2\n    fig.layout.margin.l = 120\n    fig.layout.margin.pad = 0\n    fig.layout.hovermode = False\n    fig.layout.yaxis.autorange = 'reversed'\n    fig.layout.yaxis.zeroline = False\n    return fig\n\nfig = plot_data(serps_to_plot.query('gl == \"us\"'))\nfig.layout.title = 'SERP Postions for USA'\nfig","90c4719a":"fig = plot_data(serps_to_plot.query('gl == \"uk\"'))\nfig.layout.title = 'SERP Postions for UK'\nfig","407d3bc1":"(flights\n .drop_duplicates(subset=['serp_searchTerms', 'serp_gl'])\n [['serp_searchTerms', 'serp_gl', 'serp_totalResults']]\n .sort_values('serp_totalResults', ascending=False)[:20]\n .reset_index(drop=True)\n .style.format({'serp_totalResults': '{:,}'})\n .set_caption('Queries by number of results'))","3577fb08":"flights['scrape_body_text'].str.contains('[kc]orona', regex=True, case=False).mean()","0451ebc9":"top_h1_tags = flights['scrape_h1'].value_counts()\ntop_h1_tags[2:12]","60e98402":"(flights[flights['scrape_h1']=='Flights to Vienna']\n [['serp_searchTerms', 'serp_link', 'scrape_h1']]\n .sort_values('serp_link')\n .style.set_caption('Pages with \"Flights to Vienna\" as their H1 tag'))","e2033e8b":"flights[flights['serp_searchTerms'].str.contains('vienna')][['serp_searchTerms', 'serp_link', 'scrape_h1']].sort_values('serp_link')","5cc7c2dc":"(flights['scrape_h1'].dropna()\n .str.split('@@').str.len()\n .value_counts(normalize=False)\n .to_frame().reset_index()\n .rename(columns={'index': 'h1_tags_per_page',\n                  'scrape_h1': 'count'})\n .assign(perc=lambda df: df['count'].div(df['count'].sum()))\n .style.format({'perc': '{:.1%}'})\n.hide_index())","f8e6014d":"flights[flights['scrape_h1'].str.split('@@').str.len() > 2].sort_values(['serp_displayLink', 'serp_link'])['scrape_h1'].str.split('@@')[:15]","1c3eff82":"[f for f in dir(adv) if f.startswith('extract')]","12a9c89d":"serp_title_currency = adv.extract_currency(flights['serp_title'])\nserp_title_currency.keys()","65db2744":"serp_title_currency['overview']","48c3b1ce":"serp_title_currency['top_currency_symbols']","2a1eddea":"{sym[0] for sym in serp_title_currency['currency_symbol_names'] if sym}","6099cd52":"[x for x in serp_title_currency['surrounding_text'] if x][:15]","62180fea":"snippet_emoji = adv.extract_emoji(flights['serp_snippet'])\nsnippet_emoji.keys()","87299363":"snippet_emoji['overview']","2f9332e0":"snippet_emoji['top_emoji']","5b21518a":"snippet_questions = adv.extract_questions(flights['serp_snippet'])\nsnippet_questions.keys()","61fda7c7":"snippet_questions['overview']","c5693a69":"pd.Series(' '.join(q) for q in snippet_questions['question_text'] if q).value_counts()[:10]","67ce8cb2":"adv.extract_hashtags(flights['scrape_body_text'].dropna())['overview']","e992cf5d":"body_text_length = flights['scrape_body_text'].dropna().str.split().str.len()","282f1560":"fig = go.Figure()\nfig.add_histogram(x=body_text_length.values)\nfig.layout.title = 'Distribution of word count of body text'\nfig.layout.bargap = 0.1\nfig.layout.xaxis.title = 'Number of words per page'\nfig.layout.yaxis.title = 'Count'\nfig","68606be9":"fig = go.Figure()\nfig.add_histogram(x=body_text_length[body_text_length < 1700].values)\nfig.layout.title = 'Distribution of word count of body text (for pages having less than 1,700 words)'\nfig.layout.bargap = 0.1\nfig.layout.xaxis.title = 'Number of words per page'\nfig.layout.yaxis.title = 'Count'\nfig","936cf65f":"## Column names\n\nAs you can see above, columns are prepended with `serp_` or `scrape_` to indicate the source of the data. The reason is that there is an overalp of elements from both sources. There is a title in the SERP page, and a title scraped from the landing page for example. \n\n### Columns with multiple results\nIn some cases, like header tags and images, pages can contain multiple items. In these cases the elements have been merged into one string, where the items are separated by two @ signs e.g. `first h2 tag@@second tag@@third tag`. You simply need to split by \"@@\" to get them as lists.","2520063b":"## Questions\nIt might interesting to see if websites are using questions to encourage people to click. Let's see how many snippets contain questions. ","7dab47d5":"Not so many hashtags in the body text. \n\n## Body text\nLet's see how long the body text is distributed, by checking the word counts. ","d1de0768":"2,425 or around 60% of the titles seem to contain a currency symbol.","b27312a2":"## Extract structured entities from text\nThe entities referred to are simply patterns that can tell us about the content.  \n`advertools` provides a few functions to extract some entities that might provide some good insights. ","65e3dd65":"### Duplicated H1 Tags\nAre the sites using unique H1s in their pages? Or is there a lot of overlap\/duplication?","75bef214":"As you can see above, there are a few are that duplicated exactly across different landing pages.  \nWe can check more to see if they are from the same sites, different site, etc.","48d4b05e":"Surrounding text: when calling the function, you can specify how many characters you want extracted to the right and left of the currency symbol. This is to give you context on where it appears. If you are sure that all prices are displayed properly you can specify `left_chars` to be one, and `right_chars` to be three or four for this case, and you will get clean prices extracted. Watch out for different number formatting and currency usage conventions, as they differ from country to country. ","6531e0bd":"## Content Quantity (Inventory) per Query\nOf course it's about quality and not quantity. But the higher the number of results for a certain query, the more crowded and competitive it is going to be.  \nThese are the queries sorted by the number of results (coupled with the `gl` (geo location) of the query)","61cf12c6":"The numbers are clearly extremely high. A better approach might be to search for the queries quoted, to force Google to search for the phrase, and not any word in the query, \"flights to singapore\" for example. \n\n## Header tags\nThese pages were scraped on March 11, 2020, with the Coronavirus a major news story disrupting the travel business. We can quickly check how many pages are talking about this topic. As you can see below, it seems that about 4% of the landing pages contain the word \"corona\" in their body text.","6402f269":"## Emoji","835b0051":"There is a lot more that can be analyzed, this was just a quick exploration of the dataset. ","c1784fd9":"# Airline Tickets Search Engine Results Pages and Landing Pages \nThis is a quick exploration of the [tickets and airlines dataset](https:\/\/www.kaggle.com\/eliasdabbas\/flights-serps-and-landing-pages) that contains landing page data in addition to Google search results data.  \n\nFor more details on the first part of the dataset, you can check this [tutorial on SEMrush](https:\/\/www.semrush.com\/blog\/analyzing-search-engine-results-pages\/) detailing how the keywords were developed, and how the data was imported and analyzed.  ","1e6af803":"23.75% of landing page snippets contain questions. That's a significant portion.  \nLet's see if there are duplicates across sites. ","0476606c":"To explore further, the following are all URLs and H1 tags for all queries containing \"vienna\".  \nYou probably want to have content that is unique and stands out, so it's good to explore on a large scale, what the competition is doing. ","68c7fddb":"## Visual summary of SERP positions","b5280b4f":"## Hashtags","a152936c":"### Number of H1 tags per page\nLet's see if there is an overuse of H1 tags or not. ","bb281df4":"The majority have one, but it might be interesting to see what people are trying to achieve by having multiple H1's. The following is a sample of pages having more than two tags. This is just an overview, and obviously you would have to dig deeper to figure out how to tackle this. ","e8540032":"## Extract currency\nCurrency symbols signify prices, and these are very important for understanding the market. In this case we can easily compare prices of tickets for example.  \n`extract_currency` returns a dictionary with several values.  \nThe best approach is to assign it to a name, and explore the keys available in the dictionary. "}}