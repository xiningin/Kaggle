{"cell_type":{"b24f116f":"code","491fcf41":"code","19398b3a":"code","ac97c257":"code","4e9c03c4":"code","8106b391":"code","d77ff945":"code","c3774a65":"code","a9134b1e":"code","7290c591":"code","f674c3ce":"code","7e7952d6":"code","b8a57842":"code","53cc7df5":"code","8a1035c6":"code","63ded88c":"code","08daf41e":"code","2c69f10f":"code","a5520a9c":"code","8f6de955":"code","19ced76e":"code","a54775cb":"code","8b90bd4e":"code","e44dd2c5":"code","59ec874a":"code","bacd2421":"code","d4feb8cc":"code","e6f448c3":"code","fc92dc5a":"code","ad921b5a":"code","8a46fb6a":"markdown","4232d63b":"markdown","01d5dea5":"markdown","9381f757":"markdown","287ccfd6":"markdown","249c17c2":"markdown","d4105ced":"markdown","1ed0120b":"markdown","e3d82f82":"markdown","6e3c75ed":"markdown","360e76cb":"markdown","47543496":"markdown","00c8c591":"markdown","14c21ac3":"markdown","3f36dc9a":"markdown","77247b39":"markdown","e54c8fb8":"markdown","fbb74696":"markdown","56f75829":"markdown","7710ec5b":"markdown","0427f190":"markdown"},"source":{"b24f116f":"import numpy as np\nimport pandas as pd\n\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)","491fcf41":"\"\"\"\nAUTHOR'S WORDS:\n\n\n[Machine Predictive Maintenance Classification Dataset]\n\nSince real predictive maintenance datasets are generally difficult to obtain and in particular difficult to publish, we present and provide a synthetic \ndataset that reflects real predictive maintenance encountered in the industry to the best of our knowledge.\n\nThe dataset consists of 10 000 data points stored as rows with 14 features in columns\n\n\nUID                     : unique identifier ranging from 1 to 10000\n\nproductID               : consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants \n                          and a variant-specific serial number\n\nair temperature [K]     : generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\n\nprocess temperature [K] : generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\n\nrotational speed [rpm]  : calculated from powepower of 2860 W, overlaid with a normally distributed noise\n\ntorque [Nm]             : torque values are normally distributed around 40 Nm with an \u00cf\u0192 = 10 Nm and no negative values.\n\ntool wear [min]         : The quality variants H\/M\/L add 5\/3\/2 minutes of tool wear to the used tool in the process. and a\n                          'machine failure' label that indicates, whether the machine has failed in this particular data point for any of the \n                          following failure modes are true.\n\n\nImportant : There are two Targets - Do not make the mistake of using one of them as feature, as it will lead to leakage.\n\nTarget       : Failure or Not\nFailure Type : Type of Failure\n\"\"\"\n\n#MIMO??","19398b3a":"# Dataset Source: https:\/\/www.kaggle.com\/shivamb\/machine-predictive-maintenance-classification\n\ndf = pd.read_csv(\"..\/input\/machine-predictive-maintenance-classification\/predictive_maintenance.csv\")","ac97c257":"df.head()","4e9c03c4":"df.info()","8106b391":"df_numeric = df.loc[:,['Air temperature [K]','Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']]\ndf_cat    = df.loc[:,['Type']]","d77ff945":"df.describe()","c3774a65":"import matplotlib.pyplot as plt\nimport plotly.express as px\n\nfig = plt.figure(figsize = (15,15))\nax  = fig.gca()\n\ndf_numeric.loc[:,['Air temperature [K]','Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']].hist(ax = ax)","a9134b1e":"df_numeric.skew()","7290c591":"# Observe distrubution of failures\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfig = px.pie(df,\n             title  = 'Failure Types',\n             values = 'UDI',\n             names  = 'Failure Type')\nfig.show()","f674c3ce":"\"\"\"\n[BOX PLOT FOR NUMERIC FEATURES]\n\"\"\"\n\n# Air Temperature relation with Target\/Failure Type\nfig = px.box(df,\n             y      =  \"Air temperature [K]\",\n             x      =  \"Target\",\n             title  =  \"Air Temperature relation with Target and Failure Type\",\n             color  =  \"Failure Type\",\n             width  =  800,\n             height =  400)\nfig.show()\n\n# Process Tempearture relation with Target\/Failure Type\nfig = px.box(df,\n             y      =  \"Process temperature [K]\",\n             x      =  \"Target\",\n             title  =  \"Process Tempearture relation with Target and Failure Type\",\n             color  =  \"Failure Type\",\n             width  =  800,\n             height =  400)\nfig.show()\n\n# Rotational speed [rpm] relation with Target\/Failure Type\nfig = px.box(df,\n             y      =  \"Air temperature [K]\",\n             x      =  \"Target\",\n             title  =  \"Rotational speed [rpm] relation with Target and Failure Type\",\n             color  =  \"Failure Type\",\n             width  =  800,\n             height =  400)\nfig.show()\n\n# Torque [Nm] relation with Target\/Failure Type\nfig = px.box(df,\n             y      =  \"Torque [Nm]\",\n             x      =  \"Target\",\n             title  =  \"Torque [Nm] relation with Target and Failure Type\",\n             color  =  \"Failure Type\",\n             width  =  800,\n             height =  400)\nfig.show()\n\n# Tool wear [min] relation with Target\/Failure Type\nfig = px.box(df,\n             y      =  \"Tool wear [min]\",\n             x      =  \"Target\",\n             title  =  \"Tool wear [min] relation with Target and Failure Type\",\n             color  =  \"Failure Type\",\n             width  =  800,\n             height =  400)\nfig.show()","7e7952d6":"# Correlation with Product ID with Failure\npd.pivot_table(df,\n               index   = 'Failure Type', \n               columns = 'Type', \n               values  = 'UDI',\n               aggfunc ='count')\n","b8a57842":"pd.pivot_table(df,\n               index   = 'Target', \n               columns = 'Type', \n               values  = 'UDI',\n               aggfunc ='count')","53cc7df5":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['Type']         = le.fit_transform(df.loc[:,[\"Type\"]].values)\ndf['Failure Type'] = le.fit_transform(df.loc[:,[\"Failure Type\"]].values)","8a1035c6":"df = df.drop([\"UDI\",\"Product ID\"],axis = 1)","63ded88c":"from sklearn.model_selection import train_test_split\n\nX  = df.iloc[:, :-2].values\ny  = df.loc[:,['Target','Failure Type']].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","08daf41e":"from sklearn.preprocessing import StandardScaler\n\nscaler       = StandardScaler()\nX_train_sc   = scaler.fit_transform(X_train)                # Fit and transform the training set \nX_test_sc    = scaler.transform(X_test)                     # DO NOT CHEAT! Only transform the test set","2c69f10f":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier","a5520a9c":"svc     = SVC()\nsvc_clf = MultiOutputClassifier(estimator=svc)\n\nsvc_clf.fit(X_train, y_train)\n\nprint(\"Multi-Output Training Accuracy: \", svc_clf.score(X_train, y_train)*100, \"%\")","8f6de955":"from sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Test the Model \ny_pred_svc   = svc_clf.predict(X_test)","19ced76e":"# Performance Metrics\nprint(\"Test Accuracy (Target)       : \",accuracy_score(y_test[:,0], y_pred_svc[:,0])*100,\"%\")\nprint(\"Test Precision (Target)      : \",precision_score(y_test[:,0], y_pred_svc[:,0])*100,\"%\")\nprint(\"Test Recall (Target)         : \",recall_score(y_test[:,0], y_pred_svc[:,0])*100,\"%\")\nprint(\"Test Accuracy (Failure Type) : \",accuracy_score(y_test[:,1], y_pred_svc[:,1])*100,\"%\")","a54775cb":"# Confusion Matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\n\ncm             = confusion_matrix(y_test[:,0],\n                                  y_pred_svc[:,0])\n\ndisp           = ConfusionMatrixDisplay(confusion_matrix=cm,)\n\nfig, ax        = plt.subplots(figsize = (5,5))\n\ndisp.plot(cmap = plt.cm.Blues,\n          ax   = ax)","8b90bd4e":"cm      = confusion_matrix(y_test[:,1],\n                           y_pred_svc[:,1])\n\ndisp    = ConfusionMatrixDisplay(confusion_matrix = cm,\n                                 display_labels   = ['Heat Dissipation Failure','No Failure','Overstrain Failure','Power Failure','Random Failures','Tool Wear Failure'])\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndisp.plot(cmap = plt.cm.Blues,\n          ax   = ax)","e44dd2c5":"!pip install xgboost","59ec874a":"import xgboost as xgb\nfrom xgboost import XGBClassifier","bacd2421":"xgb_clf = MultiOutputClassifier(XGBClassifier())\nxgb_clf.fit(X_train, y_train)\nprint(\"Multi-Output Training Accuracy: \", xgb_clf.score(X_train, y_train)*100, \"%\")","d4feb8cc":"# Test the Model \ny_pred_xgb   = xgb_clf.predict(X_test)","e6f448c3":"# Performance Metrics\nprint(\"Test Accuracy (Target)       : \",accuracy_score(y_test[:,0], y_pred_xgb[:,0])*100,\"%\")\nprint(\"Test Precision (Target)      : \",precision_score(y_test[:,0], y_pred_xgb[:,0])*100,\"%\")\nprint(\"Test Recall (Target)         : \",recall_score(y_test[:,0], y_pred_xgb[:,0])*100,\"%\")\nprint(\"Test Accuracy (Failure Type) : \",accuracy_score(y_test[:,1], y_pred_xgb[:,1])*100,\"%\")","fc92dc5a":"# Confusion Matrix\ncm             = confusion_matrix(y_test[:,0],\n                                  y_pred_xgb[:,0])\n\ndisp           = ConfusionMatrixDisplay(confusion_matrix=cm,)\n\nfig, ax        = plt.subplots(figsize = (5,5))\n\ndisp.plot(cmap = plt.cm.Blues,\n          ax   = ax)","ad921b5a":"cm      = confusion_matrix(y_test[:,1],\n                           y_pred_xgb[:,1])\n\ndisp    = ConfusionMatrixDisplay(confusion_matrix = cm,\n                                 display_labels   = ['Heat Dissipation Failure','No Failure','Overstrain Failure','Power Failure','Random Failures','Tool Wear Failure'])\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndisp.plot(cmap = plt.cm.Blues,\n          ax   = ax)","8a46fb6a":"Apparently there are no missing data. Next we will perform Exploratory Data Analysis. But before that, I will segregate numerical features and categorical features to ease the process","4232d63b":"# Data Preprocessing 2: Drop Unwanted Features","01d5dea5":"As expected, the dataset is highly imbalanced where the machine failure consist only 3.2% of the whole dataset. There are few methods to deal with it such as downsampling the majority class, SMOTEC, ADASYN and etc. But let's leave it as it is","9381f757":"Next, Box plot are generated to observe the relationship between categorical features with the Target and Failure Type","287ccfd6":"A pie chart below shows the distribution of the Failure Types","249c17c2":"# Developing Model 2: XGBoost","d4105ced":"Let's see how the data looks like","1ed0120b":"Let's see the histogram for numerical features. From these chart, we will observe how the value is distributed and check whether if there are any anomalies","e3d82f82":"# Exploratory Data Analysis - Categorical Features","6e3c75ed":"# Developing Model 1: Support Vector Classification","360e76cb":"Apparently it is hard to come up with any conclusion from the box plot. So we will try observing correlation of certain features with Failure Type using pandas Pivot Table","47543496":"Let's summarises these model performance\n\n\n\n---\n\n\n\n\nSupport Vector Classifier:\n```\n- Training Accuracy            : \u274c\n- Test Accuracy (Target)       : \u274c \n- Test Precision (Target)      : \u2705\n- Test Recall (Target)         : \u274c\n- Test Accuracy (Failure Type) : \u274c\n```\n\nXGBoost Classifier:\n```\n- Training Accuracy            : \u2705\n- Test Accuracy (Target)       : \u2705\n- Test Precision (Target)      : \u274c\n- Test Recall (Target)         : \u2705\n- Test Accuracy (Failure Type) : \u2705\n```\n\nLooks like XGBoost is an absolute win based on numbers. Based on confusion matrix, SVC is definitely a no-go. It is observed that there are many Failures were misclassified as No Failure\n","00c8c591":"# Data Preprocessing 1: Encoding Categorical Features","14c21ac3":"# Data Preprocessing 3: Split Training and Testing","3f36dc9a":"Apparently from my observation, there are no abnormalities on the pattern. Next, we will perform skewness analysis to see whether the numerical features are severely skewed or not.\n\n- If the skewness is between -0.5 & 0.5, the data are nearly symmetrical.\n- If the skewness is between -1 & -0.5 (negative skewed) or between 0.5 & 1(positive skewed), the data are slightly skewed.\n- If the skewness is lower than -1 (negative skewed) or greater than 1 (positive skewed), the data are extremely skewed\n\n([Source](www.analyticsvidhya.com\/blog\/2021\/05\/shape-of-data-skewness-and-kurtosis\/))","77247b39":"# Importing Dataset","e54c8fb8":"From these pivot table, we can see that machine Type L has higher tendency to fail","fbb74696":"# Predictive Maintenance","56f75829":"# Exploratory Data Analysis - Numerical Features","7710ec5b":"# Data Preprocessing 4: Feature Scaling","0427f190":"To conclude the EDA process, the dataset has no missing data and anomalies to deal with. However, columns like UDI and Product ID may not contribute to the prediction performance. Thus, these two columns will be removed in Data Preprocessing Step"}}