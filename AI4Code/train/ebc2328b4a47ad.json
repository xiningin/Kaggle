{"cell_type":{"1877a29e":"code","2fb67730":"code","9c28fce5":"code","bc5c8e7d":"code","8cf4547d":"code","6aead2f4":"code","6f08a62a":"code","32bd8986":"code","3c8134ae":"code","8c7d56ac":"code","e0b2a733":"code","994f58ed":"code","1ba686c0":"code","b2664e32":"code","30ec6f21":"code","88adb247":"code","1f315e10":"code","c3b817df":"code","a7ef5092":"code","92c739d8":"code","21bc6ba0":"code","af68893b":"code","10be0629":"markdown","7ca3b74f":"markdown","fb3282cd":"markdown","de413bde":"markdown","abd15a69":"markdown","2828c829":"markdown"},"source":{"1877a29e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fb67730":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\ntrain","9c28fce5":"train.drop(['row_id'], axis = 1, inplace = True)\ntest.drop(['row_id'], axis = 1, inplace = True)","bc5c8e7d":"# Check missing values\ntrain.info()","8cf4547d":"train['country'].value_counts()","6aead2f4":"train['product'].value_counts()","6f08a62a":"train['store'].value_counts()","32bd8986":"train['year'] = train['date'].apply(lambda x: x.split('-')[0])\ntrain['month'] = train['date'].apply(lambda x: x.split('-')[1])\ntrain['date'] = train['date'].apply(lambda x: x.split('-')[2])\ntrain","3c8134ae":"test['year'] = test['date'].apply(lambda x: x.split('-')[0])\ntest['month'] = test['date'].apply(lambda x: x.split('-')[1])\ntest['date'] = test['date'].apply(lambda x: x.split('-')[2])","8c7d56ac":"train.info()","e0b2a733":"train['num_sold'].max()","994f58ed":"train[['date', 'month']] = train[['date', 'month']].astype(np.int8)\ntest[['date', 'month']] = test[['date', 'month']].astype(np.int8)\n\n# As train['num_sold'].max() = 2884, it can be encoded in int16\ntrain[['year', 'num_sold']] = train[['year', 'num_sold']].astype(np.int16)\ntest['year'] = test['year'].astype(np.int16)\n\ntrain.info()","1ba686c0":"target = ['num_sold']\nnum_features = [col for col in train.columns if train[col].dtype in ['int8', 'int16']\n               and col not in target]\ncat_features = [col for col in train.columns if train[col].dtype == 'object']","b2664e32":"num_features","30ec6f21":"cat_features","88adb247":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnum_pipeline = Pipeline([\n    ('num_scaler', StandardScaler()), \n])\n\ncat_pipeline = Pipeline([\n    ('cat_encoder', OneHotEncoder(sparse = False, handle_unknown = 'ignore')), \n    ('cat_scaler', StandardScaler()), \n])","1f315e10":"from sklearn.compose import ColumnTransformer\n\npreprocess_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_features), \n    ('cat', cat_pipeline, cat_features), \n])","c3b817df":"X_train = preprocess_pipeline.fit_transform(train[num_features + cat_features])\nX_test = preprocess_pipeline.transform(test[num_features + cat_features])\ny_train = train[target]","a7ef5092":"# from sklearn.model_selection import KFold\n# from sklearn.metrics import mean_squared_error\n# from lightgbm import LGBMRegressor\n\n# split = KFold(n_splits = 5)\n# for train_index, test_index in split.split(X_train):\n#     X_tr, X_te = X_train[train_index], X_train[test_index]\n#     y_tr, y_te = y_train[train_index], y_train[test_index]\n    \n#     params = {\n#         'boosting_type': 'gbdt', \n#         'objective': 'regression',\n#         'learning_rate': '0.05',\n#         'n_jobs': -1, \n#     }\n    \n#     lgbm_reg = LGBMRegressor(**params)\n#     lgbm_reg.fit(X_tr, y_tr)\n#     y_pred = lgbm_reg.predict(X_te)\n\n#     score = mean_squared_error(y_te, y_pred, squared = False)\n    \n# score.mean()","92c739d8":"from lightgbm import LGBMRegressor\n\nparams = {\n    'boosting_type': 'gbdt', \n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'n_jobs': -1, \n}\n\nlgbm_reg = LGBMRegressor(**params)\nlgbm_reg.fit(X_train, y_train)","21bc6ba0":"submission","af68893b":"y_pred = lgbm_reg.predict(X_test)\nsubmission['num_sold'] = y_pred\nsubmission.to_csv('my_submission.csv', index = False)","10be0629":"## EDA","7ca3b74f":"## LightGBM Regressor","fb3282cd":"## Submission","de413bde":"## Data type conversion","abd15a69":"## Data Preprocessing","2828c829":"## Feature Engineering"}}