{"cell_type":{"176cc7be":"code","f8c7a7f5":"code","ff067366":"code","911ce767":"code","9cef54d0":"code","3a4aa9e9":"code","434d367d":"code","988a71c0":"code","761a9af0":"code","7602640f":"code","67dfb41d":"code","ac096840":"code","a0bdbce4":"code","4fd9a820":"code","10b59ccc":"code","ff094a2c":"code","bad34446":"code","755f1c67":"code","67146acc":"code","f37185ef":"code","3eef5ae9":"code","88e489f6":"code","1f06ed66":"code","07f49c7b":"code","bed86a19":"code","5de3f33d":"code","cee9caa9":"code","80eeeb91":"markdown","f3256c28":"markdown","3fbfc138":"markdown","9f986723":"markdown","d78e7cc7":"markdown","c72527c1":"markdown","67fbd6ab":"markdown"},"source":{"176cc7be":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nwarnings.simplefilter('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport nltk\nimport re\nimport string\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","f8c7a7f5":"set(stopwords.words('english'))","ff067366":"files=['..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv',\n       '..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv',\n       '..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/all_data.csv',\n       '..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv'\n      ]\n\ndef load_data(file):\n    return pd.read_csv(file)\nwith multiprocessing.Pool() as pool:\n    test,train,all_data,sub=pool.map(load_data,files)","911ce767":"# for col in all_data.columns:\n#     print(\"{} -----------> {}\".format(col,all_data[col].dtypes))\n#     print(\"{} ===========> {}\".format(col,train[col].dtypes))\ntrain.info()","9cef54d0":"train.target.value_counts(dropna=True).head()","3a4aa9e9":"train.shape","434d367d":"train['target'].isnull().sum()","988a71c0":"X=train[['comment_text','target']]\ntrain.columns.values","761a9af0":"del train","7602640f":"tox=0\nneut=0\nno_of_rows=X.shape[0]\nfor row in range(no_of_rows):\n    if X['target'][row]>0.7:\n        tox+=1\n    else:\n        neut+=1","67dfb41d":"print(f'{round((tox*100)\/no_of_rows,3)}% data contains toxic comments')\nprint(f'{round((neut*100\/no_of_rows),3)}% data contains neutral comments')","ac096840":"# remove all numbers with letters attached to them\nalphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n\n# '[%s]' % re.escape(string.punctuation),' ' - replace punctuation with white space\n# .lower() - convert all strings to lowercase \npunc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n\n# Remove all '\\n' in the string and replace it with a space\nremove_n = lambda x: re.sub(\"\\n\", \" \", x)\n\n# Remove all non-ascii characters \nremove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n\n# Apply all the lambda functions wrote previously through .map on the comments column\nX['comment_text'] = X['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)","a0bdbce4":"import wordcloud\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ndef wordcloud(df, label):\n    \n    \n    subset=df[df[label]>0.7]\n    text=subset.comment_text.values\n    wc= WordCloud(background_color=\"white\",max_words=4000)\n\n    wc.generate(\" \".join(text))\n\n    plt.figure(figsize=(20,20))\n    plt.subplot(221)\n    plt.axis(\"off\")\n    plt.title(\"Words frequented in {}\".format(label), fontsize=20)\n    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)","4fd9a820":"wordcloud(X,'target')","10b59ccc":"toxic_train=X[X['target']>0.7].iloc[0:45451,:]\ntoxic_train.shape","ff094a2c":"neutral_train=X[X['target']<=0.7].iloc[0:150000,:]\nneutral_train.shape","bad34446":"balanced_train=pd.concat([toxic_train,neutral_train],axis=0)\nbalanced_train.shape\n\n# balanced_train=X","755f1c67":"del toxic_train, neutral_train","67146acc":"# Import packages for pre-processing\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectFromModel\n\n# Import tools to split data and evaluate model performance\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n# Import ML algos\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","f37185ef":"'''\nvectorizer values: CountVectorizer, TfidfVectorizer\ngram_range values: (1,1) for unigram, (2,2) for bigram\n'''\ndef cv_tf_train_test(df_done,label,vectorizer,ngram):\n\n    ''' Train\/Test split'''\n    # Split the data into X and y data sets\n    X = df_done.comment_text\n    y = df_done[label]\n\n    # Split our data into training and test data \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    ''' Count Vectorizer\/TF-IDF '''\n\n    # Create a Vectorizer object and remove stopwords from the table\n    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')\n    \n    X_train_cv1 = cv1.fit_transform(X_train) # Learn the vocabulary dictionary and return term-document matrix\n    X_test_cv1  = cv1.transform(X_test)      # Learn a vocabulary dictionary of all tokens in the raw documents.\n    \n        \n    ''' Initialize all model objects and fit the models on the training data '''\n    lr = LogisticRegression()\n    lr.fit(X_train_cv1, y_train)\n    print('lr done')\n\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_train_cv1, y_train)\n\n\n    xgb=XGBClassifier()\n    xgb.fit(X_train_cv1,y_train)\n    \n    svm_model = LinearSVC()\n    svm_model.fit(X_train_cv1, y_train)\n\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_cv1, y_train)\n    print('rdf done')\n    \n    # Create a list of F1 score of all models \n    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_cv1), y_test), f1_score(knn.predict(X_test_cv1), y_test), \n                                f1_score(xgb.predict(X_test_cv1),y_test),\n                                f1_score(svm_model.predict(X_test_cv1), y_test), f1_score(randomforest.predict(X_test_cv1), y_test)]} \n                          \n    # Create DataFrame with the model names as column labels \n    df_f1 = pd.DataFrame(f1_score_data, index=['Log Regression','KNN', 'XGB', 'SVM', 'Random Forest'])\n\n#     accuracy_data = {'Accuracy Score':[accuracy_score(lr.predict(X_test_cv1), y_test), accuracy_score(knn.predict(X_test_cv1), y_test), \n#                                 accuracy_score(xgb.predict(X_test_cv1),y_test),\n#                                 accuracy_score(svm_model.predict(X_test_cv1), y_test), accuracy_score(randomforest.predict(X_test_cv1), y_test)]} \n                          \n    # Create DataFrame with the model names as column labels \n#     df_acc = pd.DataFrame(accuracy_data, index=['Log Regression','KNN', 'XGB', 'SVM', 'Random Forest'])\n    return df_f1\n","3eef5ae9":"balanced_train['target']=np.where(balanced_train['target']>0.7,1.0,0.0)\nbalanced_train.head()","88e489f6":"import time\n\nt0 = time.time()\n\ndf_tox_cv = cv_tf_train_test(balanced_train, 'target', TfidfVectorizer, (1,1))\ndf_tox_cv.rename(columns={'F1 Score': 'F1 Score(target)'}, inplace=True)\n\nt1 = time.time()\n\ntotal = 'Time taken: {} seconds'.format(t1-t0)\nprint(total)\n\ndf_tox_cv","1f06ed66":"X = balanced_train.comment_text\ny = balanced_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initiate a Tfidf vectorizer\ntfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n\nX_train_fit = tfv.fit_transform(X_train)  # Convert the X data into a document term matrix dataframe\nX_test_fit = tfv.transform(X_test)  # Converts the X_test comments into Vectorized format\n","07f49c7b":"# from sklearn.calibration import CalibratedClassifierCV\n# svm_model = LinearSVC()\n# clf = CalibratedClassifierCV(svm_model) \n# clf.fit(X_train_fit, y_train)\n    \n\n# my_ans=[]\n# for row in range(test.shape[0]):\n#     comment=[test['comment_text'][row]]\n#     cmt=tfv.transform(comment)\n#     my_ans.append(clf.predict_proba(cmt)[:,1])\n\n# data={'id':[],\n#       'prediction':[]\n#      }\n# df=pd.DataFrame(data)\n\n# df['id']=test['id']\n\n# df['prediction']=pd.DataFrame(my_ans)","bed86a19":"# lr=LogisticRegression()\n# lr.fit(X_train_fit,y_train)\n\n# my_ans=[]\n# for row in range(test.shape[0]):\n#     comment=[test['comment_text'][row]]\n#     cmt=tfv.transform(comment)\n#     my_ans.append(lr.predict_proba(cmt)[:,1])\n\n# data={'id':[],\n#       'prediction':[]\n#      }\n# df=pd.DataFrame(data)\n\n# df['id']=test['id']\n\n# df['prediction']=pd.DataFrame(my_ans)","5de3f33d":"randomforest = RandomForestClassifier(n_estimators=100,random_state=42)\nrandomforest.fit(X_train_fit,y_train)\n\nmy_ans=[]\nfor row in range(test.shape[0]):\n    comment=[test['comment_text'][row]]\n    cmt=tfv.transform(comment)\n    my_ans.append(randomforest.predict_proba(cmt)[:,1])\n\ndata={'id':[],\n      'prediction':[]\n     }\ndf=pd.DataFrame(data)\n\ndf['id']=test['id']\n\ndf['prediction']=pd.DataFrame(my_ans)","cee9caa9":"df.to_csv('submission.csv',index=False)","80eeeb91":"# SVM CLassifier","f3256c28":"# Logistic Regressor CLassifier","3fbfc138":"# Random Forest CLassifier","9f986723":"# Handling Class Imbalance","d78e7cc7":"# Assigning Binary Value to Labels","c72527c1":"# Model F1 Score Comparison ","67fbd6ab":"# **Preprocessing comment_text for training**"}}