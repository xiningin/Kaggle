{"cell_type":{"08a056ee":"code","ca330af8":"code","ffbd382b":"code","0dc6a5a4":"code","f08c7cf5":"code","8398422e":"code","25102113":"code","1ea67dd6":"code","46918349":"code","61af89b2":"code","ccd79d2a":"code","313592ce":"code","b484273e":"code","1aee727c":"code","5f0738b6":"code","8aec9c89":"code","390596cc":"code","d71de81e":"code","038c1247":"code","6b5ece28":"code","a26810c4":"code","93e5f68c":"code","604a8c10":"code","60fd3623":"code","065ed331":"code","38c52bcf":"code","e292af5c":"code","67d4fcdf":"code","e965d5b7":"code","49006a62":"code","afead43d":"code","7ba9afdf":"code","8299b046":"code","13cb94d5":"code","2cd07c72":"code","849611cd":"code","87f91823":"code","eee15577":"code","2e9e4af9":"code","ae3861c3":"code","0f61fc94":"code","80b6182e":"markdown","3bfbe739":"markdown","3c534c6b":"markdown","9037274b":"markdown"},"source":{"08a056ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install pdpipe\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca330af8":"#imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ffbd382b":"df= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col = 'Id')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col = 'Id')\nexample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\npd.read_csv\ndf.head()","0dc6a5a4":"df_test.shape","f08c7cf5":"example.shape","8398422e":"df.shape","25102113":"df.info()","1ea67dd6":"df.describe()","46918349":"# count of missing values\ndf.isna().sum()[df.isna().sum() > 0]","61af89b2":"#sns.pairplot(df)\n#df.head()\n#sns.set(font_scale=1.5)\n#plt.tight_layout()\n#plt.show()","ccd79d2a":"sns.histplot(data=df, x='SalePrice')","313592ce":"df.corr()[['SalePrice']].sort_values(by='SalePrice', ascending=False)","b484273e":"fig, ax = plt.subplots(figsize=(16,8))\nax= sns.heatmap(df.corr(), cmap ='bwr', linewidths=.5)","1aee727c":"df.corr()","5f0738b6":"sns.pairplot(df.loc[:, ['OverallQual', 'GrLivArea', 'SalePrice', 'YearBuilt','YearRemodAdd']])\ndf.head()\nsns.set(font_scale=1.5)\nplt.tight_layout()\nplt.show()","8aec9c89":"# baseline model - linear regression with two main numerical features\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n","390596cc":"model = LinearRegression()\ntarget = df.loc[:, 'SalePrice']\nfeatures = df.loc[:, ['OverallQual', 'GrLivArea' ]]\nmodel.fit(features, target)\ncv_results = cross_val_score(\nestimator = model,\n    X=features,\n    y= target,\n    cv=10,\n    scoring = 'neg_root_mean_squared_error'\n)\nRMSE_train = np.round(cv_results.mean(), 0)\nprint('mean of the cross validation neg RMSE', RMSE_train )\n\nfeatures_test = df_test.loc[:,['OverallQual', 'GrLivArea' ] ]\nfc= model.predict(features_test)","d71de81e":"# is it a good RMSE? Scatter index. SI= (RMSE\/average observed value)*100%\nsi_train = RMSE_train\/df.loc[:, 'SalePrice'].mean()\nr2_train = cross_val_score(\nestimator = model,\n    X=features,\n    y= target,\n    cv=10,\n    scoring = 'r2'\n).mean()\nprint('scatter index training' , np.round(si_train,2))\nprint('r2 cross validation training', np.round(r2_train, 2))","038c1247":"df_forecast = pd.DataFrame(fc, columns=['SalePrice'])\ndf_forecast['Id'] = df_test.index\ndf_forecast.to_csv('fc_linear_1.csv', index=False)\nprint(df_forecast.shape)\ndf_forecast","6b5ece28":"model = LinearRegression()\ntarget = df.loc[:, 'SalePrice']\nfeatures = df.loc[:, ['OverallQual', 'GrLivArea', 'YearBuilt','YearRemodAdd']]\nmodel.fit(features, target)\ncv_results = cross_val_score(\nestimator = model,\n    X=features,\n    y= target,\n    cv=10,\n    scoring = 'neg_root_mean_squared_error'\n)\nRMSE_train = np.round(cv_results.mean(), 0)\nprint('mean of the cross validation neg RMSE', RMSE_train )\n\n\n# is it a good RMSE? Scatter index. SI= (RMSE\/average observed value)*100%\nsi_train = RMSE_train\/df.loc[:, 'SalePrice'].mean()\nr2_train = cross_val_score(\nestimator = model,\n    X=features,\n    y= target,\n    cv=10,\n    scoring = 'r2'\n).mean()\nprint('scatter index training' , np.round(si_train,2))\nprint('r2 cross validation training', np.round(r2_train, 2))","a26810c4":"features_test = df_test.loc[:,['OverallQual', 'GrLivArea', 'YearBuilt','YearRemodAdd' ] ]\nfc= model.predict(features_test)\ndf_forecast = pd.DataFrame(fc, columns=['SalePrice'])\ndf_forecast['Id'] = df_test.index\ndf_forecast.to_csv('fc_linear_2.csv', index=False)\nprint(df_forecast.shape)\ndf_forecast","93e5f68c":"df.head()\n\nfeatures_cat = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n               'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n               'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',  'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional',\n              'FireplaceQu', 'GarageType',  'GarageFinish','GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n\nfeatures_binary = []\nfor col_name in features_cat:\n    unique_values = df.loc[:, col_name].unique()\n    print('\\nColumn name: {}\\nUnique values: {}\\nNumber of values:{}'.format(col_name, unique_values, len(unique_values))) #print unique values in the categorical cols\n    if len(unique_values) ==2:\n        features_binary.append(col_name) #create a list that will use for label encoding\n        features_cat.remove(col_name) #remove from the categorical list that will be used for onehotencoding\nprint(features_cat, '\\n', features_binary)","604a8c10":"# import for feature engineering\nfrom sklearn.preprocessing import LabelEncoder\nimport pdpipe as pdp #One Hot Encoding\ndf.head()","60fd3623":"\nlabelenconder = LabelEncoder()\n\nonehot = pdp.OneHotEncode(features_cat, drop_first = False)\nenc_df = onehot.fit_transform(df)\nfor i in features_binary:\n    enc_df.loc[:, i] = labelenconder.fit_transform(df.loc[:, i])\nenc_df.head()\n","065ed331":"print(enc_df.shape, df.shape)","38c52bcf":"enc_df.isna().sum()[enc_df.isna().sum() > 0]","e292af5c":"enc_df.loc[ enc_df.loc[:, 'MasVnrArea'].isna(), 'MasVnrArea'] = 0 # if there's no Masonry there's no area for Masonry\nenc_df.loc[:, 'MasVnrArea'].isna().sum()\nenc_df.loc[ enc_df.loc[:, 'GarageYrBlt'].isna(), 'GarageYrBlt'] = 0 #if there's no garage... check if doesn't break the prediction, later on\nenc_df.loc[ enc_df.loc[:, 'LotFrontage'].isna(), 'LotFrontage'] = 0 #if there's no garage... check if doesn't break the prediction, later on\n\n#df.loc[df.loc[:, 'GarageYrBlt'].isna(), ['GarageYrBlt', 'GarageType']]","67d4fcdf":"#  design Pipeline (OneHotEncoder, Lasso, Gridsearch(alpha) - set the order)\n#Lasso with gridsearch for alpha\n#['Landslope_'+df.loc[:, 'LandSlope'].unique()]\nenc_df.columns != ('Id' and 'SalePrice')","e965d5b7":"features = enc_df.loc[:, enc_df.columns != 'SalePrice' ]\ntarget = enc_df.loc[:, 'SalePrice']","49006a62":"model.fit(features, target)\ncv_results = cross_val_score(\nestimator = model,\n    X=features,\n    y= target,\n    cv=10,\n    scoring = 'neg_root_mean_squared_error'\n)\nRMSE_train = np.round(cv_results.mean(), 0)\nprint('mean of the cross validation neg RMSE', RMSE_train )\n\n# is it a good RMSE? Scatter index. SI= (RMSE\/average observed value)*100%\nsi_train = RMSE_train\/df.loc[:, 'SalePrice'].mean()\nr2_train = cross_val_score(\nestimator = model,\n    X=features,\n    y= target,\n    cv=10,\n    scoring = 'r2'\n).mean()\nprint('scatter index training' , np.round(si_train,2))\nprint('r2 cross validation training', np.round(r2_train, 2))","afead43d":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","7ba9afdf":"model_ridge = Ridge(alpha = 1500)\nscaler = StandardScaler()\nfeatures_standardized = scaler.fit_transform(features)\npd.options.display.float_format = '{:.2f}'.format # avoid scientific notation using exp\npd.DataFrame(features_standardized).describe() # eight value summary","8299b046":"model_ridge.fit(features_standardized, target)\ncv_results = cross_val_score(\nestimator = model_ridge,\n    X=features_standardized,\n    y= target,\n    cv=10,\n    scoring = 'neg_root_mean_squared_error'\n)\nRMSE_train = np.round(cv_results.mean(), 0)\nprint('mean of the cross validation neg RMSE', RMSE_train )\n\n# is it a good RMSE? Scatter index. SI= (RMSE\/average observed value)*100%\nsi_train = RMSE_train\/df.loc[:, 'SalePrice'].mean()\nr2_train = cross_val_score(\nestimator = model,\n    X=features_standardized,\n    y= target,\n    cv=10,\n    scoring = 'r2'\n).mean()\nprint('scatter index training' , np.round(si_train,2))\nprint('r2 cross validation training', np.round(r2_train, 2))","13cb94d5":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nfeatures_scaled = scaler.fit_transform(features)\n\nmodel.fit(features_scaled, target)\ncv_results = cross_val_score(\nestimator = model,\n    X=features_scaled,\n    y= target,\n    cv=10,\n    scoring = 'neg_root_mean_squared_error'\n)\nRMSE_train = np.round(cv_results.mean(), 0)\nprint('mean of the cross validation neg RMSE', RMSE_train )\n\n# is it a good RMSE? Scatter index. SI= (RMSE\/average observed value)*100%\nsi_train = RMSE_train\/df.loc[:, 'SalePrice'].mean()\ncv_r2 = cross_val_score(\nestimator = model,\n    X=features_scaled,\n    y= target,\n    cv=10,\n    scoring = 'r2'\n)\n\nprint('scatter index training' , np.round(si_train,2))\nprint('r2 cross validation training', np.round(cv_r2.mean(), 2))","2cd07c72":"pd.options.display.float_format = '{:.2f}'.format # avoid scientific notation using exp\npd.DataFrame(features_scaled, columns = features.columns)","849611cd":"features_binary = ['Street', 'CentralAir']\nprint(df_test.loc[:, 'Utilities'].unique()) # this thing has NaN so breaks down after, gonna do it manually then\n\ndf_test.loc[ df_test.loc[:, 'Utilities'].isna(), 'Utilities'] = 0\ndf_test.loc[ df_test.loc[:, 'Utilities'] == 'AllPub', 'Utilities'] = 1","87f91823":"print(df_test['MSZoning'].unique(), '\\n', df['MSZoning'].unique())\nlabelenconder.get_params()\n\ndf_test.loc[:,'Street'].unique()","eee15577":"# test with all features\n\n\nenc_df_test = onehot.transform(df_test)\n#display(enc_df_test.loc[:, 'Street'])\nfor i in features_binary:\n    \n    print(i)\n    labelenconder.fit(df_test.loc[:, i])\n    enc_df_test.loc[:, i] = labelenconder.transform(df_test.loc[:, i])\n    \nenc_df_test.fillna(0, inplace = True)\n\nprint(enc_df_test.isna().sum()[enc_df_test.isna().sum() > 0])\nfeatures_test = enc_df_test.loc[:, enc_df_test.columns != 'SalePrice']\nfeatures_test_scaled = scaler.transform(features_test) # se faccio questo incasino tutto, perche'?\nprint(features_test.shape, features.shape)\nfc= model.predict(features_test)\ndf_forecast = pd.DataFrame(fc, columns=['SalePrice'])\ndf_forecast['Id'] = df_test.loc[:, 'Id']\ndf_forecast.to_csv('fc_linear_all_features_scaled.csv', index=False)","2e9e4af9":"scaler = StandardScaler()\n\npipeline_lin =\nsearch_space\n\n\nalpha #highest, more regularization (more slopes to 0 etc., more bias), low +> variance)","ae3861c3":"# REMOVE \"ID\" FROM FEATURES!!!\n# Check why scaling broke the model\n#elastic net l1,l2 gridsearch","0f61fc94":"#restart: ridge and lasson BEFORE onehotEncoding (ignore categorical)","80b6182e":"# EDA","3bfbe739":"**TODOS**\n**Data Cleaning**\n**Feature engineering**\n1. One Hot Encoding for categorical variables https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n2. Polynomials?\n**Modeling**\n* Choose model type\n* Instantiate a model with certain settings known as hyperparameters\n* Organize data into a feature matrix and target vector\n* Model fitting\n* Make predictions with the trained model\n\n**Steps Models**\ncross validation\nfeature selection - lasso, ridge - gridsearch for features and alpha\nOutliers analysis RANSAC regression\n\n","3c534c6b":"Previous best result\nmean of the cross validation neg RMSE -40053.0\nscatter index training -0.22\nr2 cross validation training 0.74","9037274b":"**General notes**\n\n\nYrSold seems to not be too relevant (low corr to SalePrice) might be because it spans over 4 yr only.\n"}}