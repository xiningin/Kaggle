{"cell_type":{"89d0ee29":"code","0ff5b71c":"code","3cd9e918":"code","3dd8618f":"code","a3ad8d05":"code","0d5563d3":"code","5b75bfad":"code","9035ec4e":"code","94f66dc6":"code","5bcd9e4d":"code","c737d212":"code","37f81cc5":"code","f7aebaf9":"code","b12ec9b3":"code","f3b5bbe5":"code","0256fe57":"code","466ce2d3":"code","263ac53b":"code","eeb7b737":"code","06d15511":"code","b5fd9844":"code","28c492b5":"markdown","e029db6a":"markdown"},"source":{"89d0ee29":"!pip install tensorflow_datasets jax jaxlib flax","0ff5b71c":"import jax\nfrom jax import random\nimport jax.numpy as jnp\n\nimport flax\nfrom flax import nn\nfrom flax import optim\n\nimport tensorflow_datasets as tfds\nimport tensorflow.compat.v2 as tf\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\ntfds.disable_progress_bar()\ntf.enable_v2_behavior()\n\nimport time","3cd9e918":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU not found\")\nelse:\n    print('Found GPU at: {}'.format(device_name))\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(gpus[0], True)","3dd8618f":"train_ds = tfds.load('cifar10', split=tfds.Split.TRAIN)\ntrain_ds = train_ds.map(lambda x: {'image': tf.cast(x['image'], tf.float32) \/ 255.,\n                                     'label': tf.cast(x['label'], tf.int32)})\ntrain_ds = train_ds.cache().shuffle(1000)\ntmp_train = train_ds.batch(16)\ntrain_ds = train_ds.batch(128)\ntest_ds = tfds.as_numpy(tfds.load(\n      'cifar10', split=tfds.Split.TEST, batch_size=-1))\ntest_ds = {'image': test_ds['image'].astype(jnp.float32) \/ 255.,\n             'label': test_ds['label'].astype(jnp.int32)}","a3ad8d05":"mini_batch = next(tfds.as_numpy(tmp_train))","0d5563d3":"#I genuinely have no idea how exactly dropout must be implemented\n#in this framework as devs themselves admit the documentation on\n#stochastic context manager is 'a bit sparse'\nclass CNN(nn.Module):\n    def apply(self, x):\n        x = nn.Conv(x, features=96, kernel_size=(3, 3))\n        x = nn.relu(x)\n        x = nn.Conv(x, features=96, kernel_size=(3, 3), strides = (2,2))\n        x = nn.relu(x)\n        with nn.stochastic(random.PRNGKey(0)): \n            x = nn.dropout(x, rate = 0.2)\n        x = nn.Conv(x, features=192, kernel_size=(3, 3))\n        x = nn.relu(x)\n        x = nn.Conv(x, features=96, kernel_size=(3, 3), strides = (2,2))\n        x = nn.relu(x)\n        x = x.reshape((x.shape[0], -1))\n        x = nn.BatchNorm(x, use_running_average=not train_step, momentum = 0.99, epsilon=1e-3, name='init_bn')\n        x = nn.Dense(x, features=256)\n        x = nn.relu(x)\n        x = nn.Dense(x, features=10)\n        x = nn.log_softmax(x)\n        return x","5b75bfad":"def onehot(labels, num_classes=10):\n    return (labels[..., None] == jnp.arange(num_classes)[None]).astype(jnp.float32)\n\ndef cross_entropy_loss(preds, labels):\n    return -jnp.mean(jnp.sum(onehot(labels) * preds, axis=-1))\n#We could also implement it for single element and vectorize later with vmap\n\ndef compute_metrics(preds, labels):\n    return {'loss': cross_entropy_loss(preds, labels),\n            'accuracy': jnp.mean(jnp.argmax(preds, -1) == labels)}","9035ec4e":"#Using jit decorator for GPU acceleration for entire function\n@jax.jit\ndef train_step(optimizer, batch):\n    def loss_fn(model):\n        preds = model(batch['image'])\n        loss = cross_entropy_loss(preds, batch['label'])\n        return loss, preds\n    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    (_, preds), grad = grad_fn(optimizer.target)\n    optimizer = optimizer.apply_gradient(grad)\n    return optimizer\n\n@jax.jit\ndef eval_step(model, batch):\n    preds = model(batch['image'])\n    return compute_metrics(preds, batch['label'])\n\ndef eval_model(model, test_ds):\n    metrics = eval_step(model, test_ds)\n    metrics = jax.device_get(metrics)\n    summary = jax.tree_map(lambda x: x.item(), metrics)\n    return summary['loss'], summary['accuracy']","94f66dc6":"learning_rate = 0.001\nbeta = 0.9\nbeta_2 = 0.999","5bcd9e4d":"_, init_params = CNN.init_by_shape(random.PRNGKey(0), [((1, 32, 32, 3), jnp.float32)])\nmodel = nn.Model(CNN, init_params)\noptimizer = optim.Adam(learning_rate=learning_rate, beta1=beta, beta2 = beta_2).create(model)","c737d212":"#jit pre-compilation    \nstart_mini = time.monotonic()\ntrain_step(optimizer, mini_batch)\nmini_time = time.monotonic() - start_mini\nprint('mini_batch training: %.2fs' % mini_time)\n    \nstart_mini_2 = time.monotonic()\neval_model(optimizer.target, test_ds)\nmini_val_time = time.monotonic() - start_mini_2\nprint('mini_batch validation: %.2fs' % mini_val_time)","37f81cc5":"#Mini-batch after compilation\nstart_mini = time.monotonic()\ntrain_step(optimizer, mini_batch)\nmini_time = time.monotonic() - start_mini\nprint('mini_batch training: %.2fs' % mini_time)\n    \nstart_mini_2 = time.monotonic()\neval_model(optimizer.target, test_ds)\nmini_val_time = time.monotonic() - start_mini_2\nprint('mini_batch validation: %.2fs' % mini_val_time)","f7aebaf9":"for epoch in range(1, 2):\n    batch_gen = tfds.as_numpy(train_ds)\n    for batch in batch_gen:\n        optimizer = train_step(optimizer, batch)","b12ec9b3":"def train(train_ds, test_ds, model, optimizer):\n\n    batch_size = 128\n    num_epochs = 10\n    learning_rate = 0.001\n    beta = 0.9\n    beta_2 = 0.999\n    loss = 0\n    accuracy = 0\n    \n    start_time = time.monotonic()\n    \n    for epoch in range(1, num_epochs + 1):\n        train_time = 0\n        start_time_3 = time.monotonic()\n        batch_gen = tfds.as_numpy(train_ds)\n        for batch in batch_gen:\n            start_time_step = time.monotonic()\n            optimizer = train_step(optimizer, batch)\n            train_time += time.monotonic() - start_time_step\n            \n        flax_step = time.monotonic() - start_time_3\n        \n        start_time_2 = time.monotonic()\n        loss, accuracy = eval_model(optimizer.target, test_ds)\n        flax_inf = time.monotonic() - start_time_2\n        \n        print('eval epoch: %d, epoch: %.2fs, actual_training: %.2fs, validation: %.2fs, loss: %.4f, accuracy: %.2f' % \n              (epoch, flax_step, train_time, flax_inf, loss, accuracy * 100))\n        \n    flax_time = time.monotonic() - start_time\n    return optimizer, flax_time, accuracy, flax_inf","f3b5bbe5":"_, flax_time, flax_acc, flax_inf = train(train_ds, test_ds, model, optimizer)","0256fe57":"#Slightly different data loading pipeline\n(ds_train, ds_test), ds_info = tfds.load('cifar10', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True,)\n\ndef normalize_img(image, label):\n    return tf.cast(image, tf.float32) \/ 255., label\n\nds_train = ds_train.map(normalize_img)\nds_train = ds_train.cache()\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\nds_train = ds_train.batch(128)\nds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n\nds_test = ds_test.map(normalize_img)\nds_test = ds_test.batch(128)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)","466ce2d3":"model = Sequential([\n    Conv2D(input_shape=(32,32,3), filters=96, kernel_size=(3,3), activation='relu'),\n    Conv2D(filters=96, kernel_size=(3,3), strides=2, activation='relu'),\n    Dropout(0.2),\n    Conv2D(filters=192, kernel_size=(3,3), activation='relu'),\n    Conv2D(filters=192, kernel_size=(3,3), strides=2, activation='relu'),\n    Flatten(),\n    BatchNormalization(),\n    Dense(256, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=Adam(0.001),\n    metrics=['accuracy'],\n)","263ac53b":"start_time = time.monotonic()\n\nmodel.fit(\n    ds_train,\n    epochs=10,\n    validation_data=ds_test)\n\ntf_time = time.monotonic() - start_time","eeb7b737":"start_time = time.monotonic()\n\nloss, tf_acc = model.evaluate(ds_test)\n\ntf_inf = time.monotonic() - start_time","06d15511":"from keras import backend as K \nK.clear_session()\n\n!pip install numba\nfrom numba import cuda\ncuda.select_device(0)\ncuda.close()","b5fd9844":"from prettytable import PrettyTable\nt = PrettyTable(['', 'Flax', 'TensorFlow'])\nt.add_row(['Train time', '%.2fs'%(flax_time), '%.2fs'%(tf_time)])\nt.add_row(['Inference time', '%.2fs'%(flax_inf), '%.2fs'%(tf_inf)])\nt.add_row(['Accuracy', '%.2f%%'%(flax_acc * 100), '%.2f%%'%(tf_acc * 100)])\nprint(t)","28c492b5":"# Flax model","e029db6a":"# TensorFlow model"}}