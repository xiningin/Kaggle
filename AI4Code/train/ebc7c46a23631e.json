{"cell_type":{"ccafa842":"code","fb646f28":"code","790aa460":"code","b98b06b5":"code","ef1156e7":"code","e41a242d":"code","80285f62":"code","4676ddc8":"code","35f9f9e1":"code","b9e09b21":"code","b946bd39":"code","a80b1be2":"code","f2031e5f":"code","e256f40a":"code","9a45e51e":"code","8296cd69":"code","0a0711d1":"code","10089bbc":"code","ef1cfedb":"code","341ffb48":"code","39778176":"code","08704d8d":"code","dca40572":"code","af2e30ae":"code","b902a55e":"code","08e56303":"code","9805f457":"code","a4a2ed97":"code","4c2a2e56":"code","bc7fd5c7":"code","a85415de":"code","95306988":"code","003e000f":"code","6623b66a":"code","325827ee":"code","eae66796":"code","ccee7675":"code","fe3520af":"code","c9048bd5":"code","6119fb6e":"code","d8f25532":"code","31a8952d":"code","eb97348a":"code","41541b8d":"code","ac06e35c":"code","cde69fa3":"code","fad8fec6":"code","1723109b":"code","2a6b00a8":"code","e6eadad0":"code","10193b08":"code","daf73020":"code","84133c55":"markdown","a1191683":"markdown","1468615a":"markdown","8e3c557e":"markdown","8da8092f":"markdown","4d238f7f":"markdown","71320ce8":"markdown","f4dd795f":"markdown","f8e4b3b7":"markdown","bb49ac01":"markdown","fe9427fb":"markdown","78cd2084":"markdown","749b1378":"markdown","0c897f53":"markdown","9873359b":"markdown","9d9b2ef7":"markdown","b434a8e9":"markdown","0faa5af5":"markdown","03eeda14":"markdown","b9f01ddb":"markdown"},"source":{"ccafa842":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb646f28":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","790aa460":"data = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","b98b06b5":"data.head()","ef1156e7":"data = data.drop('id',axis=1)","e41a242d":"data.info()","80285f62":"data.nunique()","4676ddc8":"data.shape","35f9f9e1":"data.duplicated().sum()","b9e09b21":"data.isnull().sum()","b946bd39":"import missingno as msno","a80b1be2":"msno.bar(data)\nplt.show()","f2031e5f":"data.isnull().mean() * 100 # Check in percentages of missing values.","e256f40a":"data = data.dropna(subset=['bmi'])","9a45e51e":"msno.bar(data)\nplt.show()","8296cd69":"data.head(2)","0a0711d1":"fig, axes = plt.subplots(2, 4, sharex=True, figsize=(24,10))\nfig.suptitle('Count of all categorical variables')\nsns.countplot(ax=axes[0, 0], data=data, x='stroke',hue = 'gender',palette='crest')\nsns.countplot(ax=axes[0, 1], data=data, x='stroke',hue='hypertension',palette='RdPu')\nsns.countplot(ax=axes[0, 2], data=data, x='stroke',hue = 'heart_disease',palette='magma')\nsns.countplot(ax=axes[0, 3], data=data, x='stroke',hue = 'ever_married',palette=\"ch:s=-.2,r=.6\")\nsns.countplot(ax=axes[1,0],data = data, x ='stroke' ,hue = 'work_type',palette=\"dark:salmon_r\")\nsns.countplot(ax=axes[1,1],data = data, x ='stroke',hue='Residence_type',palette=\"coolwarm\" )\nsns.countplot(ax=axes[1,2],data = data, x ='stroke',hue='smoking_status',palette='viridis' )\nsns.countplot(ax=axes[1,3],data = data, x ='stroke',palette='RdPu' )\nplt.show()","10089bbc":"sns.distplot(data['age'])\nplt.show()","ef1cfedb":"def LABEL_ENCODING(c1):\n    from sklearn import preprocessing\n    label_encoder = preprocessing.LabelEncoder()\n    data[c1]= label_encoder.fit_transform(data[c1])\n    data[c1].unique()","341ffb48":"data.head()","39778176":"LABEL_ENCODING(\"gender\")\nLABEL_ENCODING(\"ever_married\")\nLABEL_ENCODING(\"work_type\")\nLABEL_ENCODING(\"Residence_type\")\nLABEL_ENCODING(\"smoking_status\")\ndata","08704d8d":"x = data.drop('stroke',axis=1)\ny = data['stroke']","dca40572":"print(x.shape)\nprint(y.shape)","af2e30ae":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","b902a55e":"ordered_rank_features = SelectKBest(score_func=chi2,k=10)\nordered_feature = ordered_rank_features.fit(x,y)","08e56303":"dfscores = pd.DataFrame(ordered_feature.scores_,columns=[\"Score\"])\ndfcolumns = pd.DataFrame(x.columns)","9805f457":"features_rank=pd.concat([dfcolumns,dfscores],axis=1)\nfeatures_rank.columns=['Features','Score']\nfeatures_rank","a4a2ed97":"features_rank.nlargest(10,'Score')","4c2a2e56":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","bc7fd5c7":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a85415de":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","95306988":"X_train","003e000f":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","6623b66a":"y_pred = classifier.predict(X_test)","325827ee":"y_pred","eae66796":"from sklearn.metrics import accuracy_score\nacc1 = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy score: {acc1}\")","ccee7675":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","fe3520af":"y_pred = classifier.predict(X_test)","c9048bd5":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nacc2 = accuracy_score(y_test, y_pred)","6119fb6e":"print(f\"Accuracy score: {acc2}\")","d8f25532":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","31a8952d":"acc3 = accuracy_score(y_test, y_pred)","eb97348a":"print(f\"Accuracy score: {acc3}\")","41541b8d":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","ac06e35c":"y_pred = classifier.predict(X_test)","cde69fa3":"acc4 = accuracy_score(y_test, y_pred)","fad8fec6":"print(f\"Accuracy score : {acc4}\")","1723109b":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","2a6b00a8":"y_pred = classifier.predict(X_test)","e6eadad0":"acc5 = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy score: {acc5}\")","10193b08":"mylist=[]\nmylist2=[]\nmylist.append(acc1)\nmylist2.append(\"Logistic Regression\")\nmylist.append(acc2)\nmylist2.append(\"SVM\")\nmylist.append(acc3)\nmylist2.append(\"KNN\")\nmylist.append(acc4)\nmylist2.append(\"Naive Bayes\")\nmylist.append(acc5)\nmylist2.append(\"DTR\")\nplt.rcParams['figure.figsize']=8,6\nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classification Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classification Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","daf73020":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","84133c55":"1. id: unique identifier\n1. gender: \"Male\", \"Female\" or \"Other\"\n1. age: age of the patient\n1. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n1. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n1. ever_married: \"No\" or \"Yes\"\n1. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n1. Residence_type: \"Rural\" or \"Urban\"\n1. avg_glucose_level: average glucose level in blood\n1.  bmi: body mass index\n1.  smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n1.  stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","a1191683":"# Training the Naive Bayes on the Training set","1468615a":"# SVM","8e3c557e":"# Feature Selection","8da8092f":"# Feature Scaling","4d238f7f":"# Training Decision Tree Classification on Train set","71320ce8":"# Split the dataset into train and test","f4dd795f":"# Applying k-Fold Cross Validation","f8e4b3b7":"# EDA","bb49ac01":"**Here is features are only 10 so take it all for model, Feature selection is not use full for this model**","fe9427fb":"# Import the independent and dependent variable.","78cd2084":"# Training the K-NN model on the Training set","749b1378":"# Take top 10 features variables.","0c897f53":"# Create visualization for all model with their Accuracy","9873359b":"# Apply SelectKBest Algorithm","9d9b2ef7":"# Apply the label encoding for categorical columns","b434a8e9":"![](https:\/\/media.istockphoto.com\/illustrations\/brain-with-stroke-illustration-id167588136?k=20&m=167588136&s=612x612&w=0&h=EF3_xj0BKuMOKTexjwJCDXr_kWYwU2dULs-OkeUcyg8=)","0faa5af5":"# Logistic Regression","03eeda14":"# Predict the tset set result","b9f01ddb":"![](https:\/\/static.wingify.com\/gcp\/uploads\/sites\/3\/2021\/01\/Feature-image_7-Thank-You-Page-Examples-That-Can-Boost-Visitor-Experience.png)"}}