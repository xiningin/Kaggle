{"cell_type":{"65e9bf2c":"code","36c71fae":"code","cb7d4c21":"code","54acb21c":"code","a19fe8e1":"code","2bb3ac5c":"code","756363ae":"code","4f419d22":"code","b963d369":"code","f8fd78b1":"code","6d4f7734":"code","31896e5c":"code","c233d240":"code","1fd88f33":"code","6a612c79":"code","a9e37aaa":"code","3c585e96":"markdown"},"source":{"65e9bf2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36c71fae":"customer_df = pd.read_csv('\/kaggle\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv', delimiter=',')\ncustomer_df.describe()","cb7d4c21":"customer_df.info()","54acb21c":"#checking for null values\ncustomer_df.loc[customer_df.isna().any(axis=1)]\n","a19fe8e1":"Male_dummies=pd.get_dummies(customer_df.Gender,prefix=None,prefix_sep='_') \nregion_dummies=pd.get_dummies(customer_df.Geography,prefix=None)\nprint(region_dummies)","2bb3ac5c":"customer_df=customer_df.merge(Male_dummies,left_index=True, right_index=True).merge(region_dummies,right_index=True,left_index=True)\ncustomer_df.head()","756363ae":"customer_df=customer_df[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n       'IsActiveMember', 'EstimatedSalary', 'Exited', 'Germany', 'Spain',\n       'Male']]\ncustomer_df.Male=customer_df.Male.astype('int64')\ncustomer_df.Spain=customer_df.Spain.astype('int64')\ncustomer_df.Germany=customer_df.Germany.astype('int64')\ncustomer_df.rename(columns={'Exited': 'Churn'},inplace=True)","4f419d22":" customer_df.info()","b963d369":"X = customer_df.drop(['Churn'], axis=1)\ny = customer_df['Churn']\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(scaled_features, y, test_size = 0.2, random_state = 0)\n","f8fd78b1":"\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='lbfgs')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='scale')))\nmodels.append(('RF',RandomForestClassifier(n_estimators=200, random_state=0) ))\n\n\n","6d4f7734":"seed = 7\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)","31896e5c":"# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c233d240":"# fit a random forest classifier on the data and predict the test set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=200, random_state=0)  \nclassifier.fit(X_train, y_train)  \npredictions = classifier.predict(X_test)\nclassifier.predict_proba(X_test)\n","1fd88f33":"# return accuracy metrics. this model has accuracy of 86%, not bad for the start\nfrom sklearn.metrics import classification_report, accuracy_score\n#print(classification_report(y_test,predictions ))  \nprint(accuracy_score(y_test, predictions ))\n","6a612c79":"# confusion metrics can be calculated using cross validation\nfrom sklearn.model_selection import cross_val_predict\ny_train_pred=cross_val_predict(classifier,X_train,y_train,cv=3)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train,y_train_pred)\n","a9e37aaa":"\n# for feature selection the top important ones can be selected\nfeature_importances = pd.Series(classifier.feature_importances_, index=X.columns)\nfeature_importances.nlargest(10).plot(kind='barh')\n\n# this model can be used to predict if some costumers are about to abandan the bank and actions can be taken for those customers.\n ","3c585e96":"The Graph shows that the Random Forest Classifier is the best among all of those models."}}