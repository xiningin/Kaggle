{"cell_type":{"631948dd":"code","0725b22a":"code","595ae48c":"code","0fac8b0c":"code","583ba4f6":"code","0792c943":"code","94c1e322":"code","4217f9af":"code","6697efad":"code","1b84f17e":"code","370a2138":"code","ff6dbac2":"code","c5dcbe33":"code","56ea8401":"code","c97e7fe6":"code","8b9a8434":"code","c953e454":"code","3fc2f176":"code","b98930a4":"code","759e19b3":"code","ec55e9bb":"code","bda0bc51":"code","94106504":"code","b387578f":"code","a5f449dc":"markdown","e6ee3198":"markdown","7e9f0a16":"markdown","50c21e04":"markdown","b83cc57c":"markdown","ecf5cda5":"markdown","22980702":"markdown","951e4688":"markdown","45b17490":"markdown","5ac4a5de":"markdown","54008d75":"markdown","1b42715b":"markdown","e1be1e09":"markdown"},"source":{"631948dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0725b22a":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Dense , Dropout , BatchNormalization , concatenate\nfrom tensorflow.keras.layers import Activation , Input , GlobalAveragePooling2D  \nfrom tensorflow.keras.models import Model , Sequential , load_model\nfrom tensorflow.keras.applications import InceptionV3 , MobileNetV2\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nimport re\nfrom PIL import Image","595ae48c":"IMG_DIMS = 64 # inception net has a minimum input size of 75x75 thus 150 is good\nCHANNELS = 3\nBATCH_SIZE = 32\nSEED = 42\nSPLITS = 5\n\nAUTO  = tf.data.experimental.AUTOTUNE","0fac8b0c":"# getting training data gcs path\nGCS_PATH = KaggleDatasets().get_gcs_path('melanoma-256x256')\ntrain_datasets = tf.io.gfile.glob(GCS_PATH + '\/train*.tfrec')\n\nprint('number of TFRecords in train : ',len(train_datasets))\n\n# getting testing data\nGCS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')\ntest_datasets = tf.io.gfile.glob(GCS_PATH + '\/tfrecords\/test*.tfrec')\n\nprint('number of TFRecords in test : ' ,len(test_datasets))","583ba4f6":"# parse data from TF Records\ndef parse_TFR_data_labelled(sample):\n    features = {\n      'image': tf.io.FixedLenFeature([] , tf.string , default_value = ''),\n      'image_name': tf.io.FixedLenFeature([] , tf.string , default_value=''),\n      'patient_id': tf.io.FixedLenFeature([] , tf.int64 , default_value=0),\n      'sex': tf.io.FixedLenFeature([] , tf.int64 , default_value=0),\n      'age_approx': tf.io.FixedLenFeature([] , tf.int64 , default_value=0),\n      'anatom_site_general_challenge':tf.io.FixedLenFeature([] ,tf.int64 , default_value=0 ),\n      'diagnosis': tf.io.FixedLenFeature([] ,tf.int64 , default_value=0 ),\n      'target': tf.io.FixedLenFeature([] ,tf.int64 , default_value=0 ),\n      'width': tf.io.FixedLenFeature([] ,tf.int64 , default_value=0 ),\n      'height': tf.io.FixedLenFeature([] ,tf.int64 , default_value=0 )\n    }\n    \n    p = tf.io.parse_single_example(sample , features)\n    \n    img = p['image']\n    target = p['target']\n    \n    return img , target","0792c943":"# decode img\ndef decode_image(img , IMG_DIMS):\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img , [IMG_DIMS , IMG_DIMS])\n    return img","94c1e322":"# load data set for training and validation\ndef _get_ds(files , train=True , repeat=True , img_dims=64 , batch_size=32):\n    ds = tf.data.TFRecordDataset(files , num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    ds = ds.map(parse_TFR_data_labelled , num_parallel_calls=AUTO)     \n    ds = ds.map(lambda img ,label: (decode_image(img, img_dims),label) , num_parallel_calls=AUTO)\n    if train:\n        ds = ds.shuffle(buffer_size=1000)\n       \n    ds = ds.batch(batch_size*REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds","4217f9af":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","6697efad":"# preparing the testing data\ndef parsed_TFR_unlabelled_2(sample):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string, default_value=''),\n        'image_name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n        'target': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    }\n    p = tf.io.parse_single_example(sample , feature_description)\n    img = p['image']\n    name = p['image_name']\n    return name , img\n\ntest_data= tf.data.TFRecordDataset(test_datasets)\ntest_data = test_data.map(parsed_TFR_unlabelled_2 , num_parallel_calls=AUTO)\ntest_data = test_data.map(lambda name , img: (name , decode_image(img , 64)))\n\nsub_df = pd.read_csv('..\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\n\nx_dict = {}\nfor p in test_data:\n    temp = {p[0].numpy().decode() : p[1].numpy()}\n    x_dict.update(temp)\n    \nprint(f'number of samples in testing data : {len(x_dict)}')\n\ntest = []\nfor i in sub_df['image_name']:\n    test.append(x_dict[i])\n    del(x_dict[i])\n    \ntest = np.array(test)\nprint(f'sahpe of testing set sorted according to the submission file : {test.shape}')","1b84f17e":"def create_Inf_model(IMG_DIMS , CHANNELS):\n    in_put = Input(shape=(IMG_DIMS , IMG_DIMS , 3))\n    # applying auggumentations\n    #pre = aug(in_put)\n    # pre process layer\n    pre_process_layer = tf.keras.applications.mobilenet_v2.preprocess_input\n    pre = pre_process_layer(in_put)\n    # base model non trainable\n    base_model = MobileNetV2(input_shape=(IMG_DIMS , IMG_DIMS , 3) , include_top=False , weights='imagenet')\n    x = base_model(pre , training = False)\n    # top trainable model layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128 , activation = 'relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(1 , activation = 'sigmoid')(x)\n    model = Model(inputs=in_put , outputs=x)\n    # optimizer\n    opt = tf.keras.optimizers.Adam(0.0001)\n    model.compile(optimizer=opt , loss='binary_crossentropy' , metrics=['accuracy' , 'AUC'])\n    return model","370a2138":"TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(TPU)\ntf.tpu.experimental.initialize_tpu_system(TPU)\nstrategy = tf.distribute.experimental.TPUStrategy(TPU)\n\nREPLICAS = strategy.num_replicas_in_sync","ff6dbac2":"kf = KFold(n_splits=SPLITS)\noof_hist = []\noof_val = []\nfor f , (idxT , idxV) in enumerate(kf.split(train_datasets)):\n    train = []\n    val =[]\n    for idx in idxT:\n        train.append(train_datasets[idx])\n    for idx in idxV:\n        val.append(train_datasets[idx])\n\n    # instantiate model\n    with strategy.scope():\n         # cretae model check points\n        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model_fold_'+str(f)+'_weights.hdf5' , \n                                                         monitor='val_auc',\n                                                         mode='max',\n                                                         save_best_only =True,\n                                                         verbose = 1 )\n        # early stopping\n        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc' , patience=5 , mode='max' )\n        model = create_Inf_model(IMG_DIMS , CHANNELS)\n        \n        history = model.fit(_get_ds(train) ,\n                        epochs = 20 ,\n                        steps_per_epoch = count_data_items(train)\/BATCH_SIZE\/\/REPLICAS,\n                        validation_data=_get_ds(val , train=False , repeat=False),\n                        callbacks = [cp_callback , es_callback],\n                        verbose = 0)\n        model.save('model_'+str(f)+'_.hdf5')\n    oof_hist.append(history)\n    oof_val.append(_get_ds(val, train=False))","c5dcbe33":"\ntrain_data= tf.data.TFRecordDataset(train_datasets)\ntrain_data = train_data.map(parse_TFR_data_labelled , num_parallel_calls=AUTO)\ntrain_data = train_data.map(lambda img , target: (decode_image(img , 64) , target))\n\nimg = []\nlabel = []\nfor sample in train_data:\n    i = sample[0].numpy()\n    t = sample[1].numpy()\n    img.append(i)\n    label.append(t)\n    \nimg = np.array(img)\nlabel = np.array(label)\n\nprint(f'shape of images : {img.shape}')\nprint(f'shape of labels : {label.shape}')","56ea8401":"file_names = ['model_'+str(f)+'_.hdf5' for f in range(5)]\nmember_models = [load_model(m) for m in file_names]\nmember_models","c97e7fe6":"# level0_predict function\ndef level0_predict(m_models , train_x):\n    predictions = []\n    for model in m_models:\n        p = model.predict(train_x)\n        predictions.append(p)\n    return predictions","8b9a8434":"def stacked_set(predictions):\n    X = None\n    for p in predictions:\n        if X is None:\n            X = p\n        else:\n            X = np.dstack((X , p))\n    X = X.reshape(X.shape[0] , X.shape[1]*X.shape[2])\n    return X","c953e454":"def fit_agg(X , Y):\n    model = Sequential([\n        Dense(3 , activation='relu'),\n        Dense(1 , activation ='sigmoid')\n    ])\n    model.compile(optimizer='adam' ,loss='mean_squared_error' , metrics=['accuracy'])\n    model.fit(X,Y , epochs = 10)\n    return model","3fc2f176":"def ensemble_fit(X_train , Y , X_test , member_models):\n    # get prediction of each sub model\n    preds = level0_predict(member_models , X_train)\n    # prepare the stacked set\n    X = stacked_set(preds)\n    # fit aggregator\n    model = fit_agg(X,Y)\n    return model\n    ","b98930a4":"def ensemble_predict(agg, member_models , data ):\n    # get the sub model predictions\n    preds = level0_predict(member_models , data)\n    # prepare the stacked set\n    X = stacked_set(preds)\n    # final prediction\n    result = agg.predict(X)\n    return result","759e19b3":"final_model = ensemble_fit(img , label , test , member_models)","ec55e9bb":"result = ensemble_predict(final_model , member_models , test)","bda0bc51":"result","94106504":"sub_df['target'] = result\nsub_df.set_index('image_name' , inplace=True)\nsub_df.head()","b387578f":"sub_df.to_csv('submission.csv')","a5f449dc":"now lets initialize our TPU","e6ee3198":"to train these ensemble model I will be using the same KFold technique that I used in KFold with efficientnet notebook and transfer learning notebook","7e9f0a16":"now lets make a submission","50c21e04":"now lets created the stacked dataset function","b83cc57c":"the final ensemble predict function","ecf5cda5":"now lets create level0_predict function<br>\nbut first lets create a list of all our memeber models","22980702":"# utility Functions\nagain first lets have some of the utility functions","951e4688":"now our Level 1 is ready lets use it\n\nensemble fit","45b17490":"# Level 0\n\nnow we will use MobileNetV2 as our base model and create an inference model over it that create different instances of this model and save those modesls.. we will be using KFold cross validation technique\n\nso first lets create our model","5ac4a5de":"# Introduction\nensembles models are multiple models grouped together with their result generalised at the end. RandomForest is one of the most popular example of ensemble. the general architecture of the ensemble is shown in the following figure\n\n![general architecture ensemble model](https:\/\/www.researchgate.net\/publication\/324552457\/figure\/fig3\/AS:616245728645121@1523935839872\/An-example-scheme-of-stacking-ensemble-learning.png)\n\nnow you can see their are two levels in the ensemble model\n1. Level 0 -- this level is a stack of all the models which have been trained on the training                   data. \n2. Level 1 -- at this the prediction of each model are taken and then generalized. there are two               ways of generalizing.<br>\na) by aggregating all the prediction like average , max etc.<br>\nb) by training another linear model on these predictions with respect to actual labels <br>\nwe will be using approach b. in this notebook\n              \nbased on the stacked models there are two categories on ensemble models\n1. homogenous models -- in these models all the modesl in level 0 have the same architecture and                         they will of same type but they will be trained on different samples of                           training data.\n2. heterogenous models -- in these models all the models in level 0 are of types or have a                                 different architecture as shown in the figure below\n\n![hetrogenous model](https:\/\/blogs.sas.com\/content\/subconsciousmusings\/files\/2017\/05\/modelstacking.png)\n\nHowever in this notebook we will only look at the homogenous model. if there's time I will publish a seperate notebook explaining the heterogenous models","54008d75":"final prediction","1b42715b":"the fit_aggregator function","e1be1e09":"# Level 1 -- Generalization\nnow what we are going to do is to first prepare the training set for these models to predict on it so than we will create the following function \n* level0_predict\n* create_stacked_dataset\n* ensemsble_predict\n\n\nwe will use the stacked dataset to create a dataset which will be use to by the aggregator to fit and generalize and to make a prediction\n\nnow prepare the training data for prediction"}}