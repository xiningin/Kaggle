{"cell_type":{"5eb11a32":"code","cfef0550":"code","d60d9534":"code","e3bd213e":"code","a5e2169f":"code","e6967210":"code","ef018f3e":"code","5f18810f":"code","a1f5c502":"code","7f50bfeb":"code","4f311252":"code","be288571":"markdown","faf84083":"markdown","589ca8da":"markdown","41084f7c":"markdown","dd273130":"markdown"},"source":{"5eb11a32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfef0550":"train_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","d60d9534":"y = train_data.pop('label')\nx = train_data\n# normalization\nx = x \/ 255.0\ntest_data = test_data \/ 255.0\n# Converting dataframe into arrays\nx = np.array(x)\ny = np.array(y)\n# shaping\nx = x.reshape(-1,28,28,1)\n# enci=oding\nfrom keras.utils.np_utils import to_categorical\ny = to_categorical(y, 10)","e3bd213e":"test_data=test_data.values","a5e2169f":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ncnn = keras.models.Sequential()\ncnn.add(keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \ncnn.add(keras.layers.MaxPool2D(pool_size = 2, strides=2))\ncnn.add(keras.layers.Flatten())\ncnn.add(keras.layers.Dense(128, activation='relu'))\ncnn.add(keras.layers.Dense(10,activation='softmax'))","e6967210":"cnn.compile(  optimizer= 'Adam', loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","ef018f3e":"history = cnn.fit(x, y, shuffle= True, epochs=20, validation_split=0.2)","5f18810f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['accuracy'],label = 'ACCURACY')\nplt.plot(history.history['val_accuracy'],label = 'VALIDATION ACCURACY')\nplt.legend()","a1f5c502":"plt.plot(history.history['loss'],label = 'TRAINING LOSS')\nplt.plot(history.history['val_loss'],label = 'VALIDATION LOSS')\nplt.legend()","7f50bfeb":"test_data.shape","4f311252":"test_data = test_data.reshape(-1, 28, 28 , 1).astype('float32')\nimport numpy\nres = cnn.predict(test_data)\nres = numpy.argmax(res,axis = 1)\nres = pd.Series(res, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   res],axis = 1)\nsubmission.to_csv(\"My_submission.csv\",index=False)","be288571":"# Part 3: Training and validation the model","faf84083":"# Building Model","589ca8da":"# Model Performance Plots","41084f7c":"# Part 4: Testing, predicting and submission","dd273130":"# Part 1: Data preprocessing"}}