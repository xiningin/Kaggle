{"cell_type":{"ebc40408":"code","70618e30":"code","14e24b18":"code","0e0b3828":"code","e93a5882":"code","661bda16":"code","af0c215a":"code","8957c601":"code","3ab752bb":"code","7360fca1":"code","d548c5c3":"code","41b778ae":"code","948c60db":"code","f1c4c42e":"code","41c2593c":"code","dfababd0":"code","cd5e8909":"code","a7a97d86":"code","62479dfa":"code","428d69e0":"code","f91216b3":"code","ab5007d1":"code","7a1d8700":"code","7df13150":"code","86e0a888":"code","7d90dd0e":"code","234de1ef":"code","e3ba73ad":"code","5d585ac7":"code","ac3d9814":"code","e0410ee7":"markdown","741ddf2e":"markdown","30448a10":"markdown","e80818c2":"markdown","4b78a331":"markdown","4dd487ff":"markdown","b3d5d79d":"markdown","3d67fc26":"markdown","67c988bd":"markdown","e8a4bc49":"markdown","ef591e66":"markdown","7fb3678f":"markdown","a0277e61":"markdown","cc111010":"markdown","58908d8a":"markdown","33a0c76b":"markdown","8f09f0b1":"markdown"},"source":{"ebc40408":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70618e30":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datatable as dt\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold\n\n# Importing modelling packages\nfrom lightgbm import LGBMClassifier\n\n\n# Removes warning\nimport warnings\nwarnings.filterwarnings('ignore')","14e24b18":"# Using datatable for faster loading\n\ntrain_df = dt.fread(r'\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest_df = dt.fread(r'\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\n\nprint(\"Data is loaded\")","0e0b3828":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e93a5882":"# Reduce Memory Usage\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","661bda16":"train_df.head()","af0c215a":"train_df.dtypes","8957c601":"train_df.describe()","3ab752bb":"train_df.shape","7360fca1":"test_df.shape","d548c5c3":"missing_train = train_df.isnull().sum().sum()\nmissing_test = test_df.isnull().sum().sum()\nprint('Total missing value in train dataset is:', missing_train)\nprint('Total missing value in test dataset is:', missing_test)","41b778ae":"train_df.info()","948c60db":"test_df.info()","f1c4c42e":"print(train_df.dtypes.value_counts())","41c2593c":"train_df['target'].value_counts()","dfababd0":"X = train_df.drop('target', axis=1)\ny = train_df['target']\n\n# freeing up some memory\ndel train_df","cd5e8909":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","a7a97d86":"# Define the hyperparameters.\nfit_params = dict(early_stopping_rounds = 200,\n                  eval_set = [(X_train, y_train), (X_test, y_test)], \n                  eval_metric = 'auc', \n                  verbose = 200)\n\nrs_params = dict(learning_rate = [0.05],\n                 reg_lambda = [0, 20],\n                 n_estimators = [5000],\n                 max_depth = [7, 10],\n                 subsample = [0.8, 0.9],\n                 colsample_bytree = [0.8, 0.9],\n                 reg_alpha = [20, 40])","62479dfa":"lgb = LGBMClassifier(random_state = 42)","428d69e0":"rs_lgb = RandomizedSearchCV(estimator = lgb, \n                            param_distributions = rs_params,\n                            scoring = 'roc_auc', \n                            cv = 2,\n                            n_iter = 1,\n                            random_state = 34)","f91216b3":"# Train the model with given hyperparameters and train data\nrs_lgb.fit(X_train, y_train, **fit_params)","ab5007d1":"best_accuracy = rs_lgb.best_score_\nprint('Best AUC score in train data: {:.2f} %'.format(best_accuracy*100))","7a1d8700":"best_params = rs_lgb.best_params_\nprint('Best params for the model are:', best_params)","7df13150":"final_model = rs_lgb.best_estimator_\nfinal_model","86e0a888":"#Predict_proba will give the only probability of 1.\npred_lgbm = final_model.predict_proba(X_test)[:, -1]","7d90dd0e":"# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, pred_lgbm)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","234de1ef":"test_score = roc_auc_score(y_test,pred_lgbm)\nprint('AUC score for test data: {:.2f} %'.format(test_score*100))","e3ba73ad":"y_pred = final_model.predict_proba(test_df)[:, -1]\ny_pred","5d585ac7":"# Submit\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'target': y_pred\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"predictions successfully submitted\")","ac3d9814":"submission","e0410ee7":"<a id=\"eda\"><\/a>\n## 2. EDA","741ddf2e":"<h1 style='color:white;background-color:black' > Table of Contents <\/h1>\n\n* [Introduction](#introduction)\n* [Exploratory Data Analysis (EDA)](#eda)\n    - [Data Acquisition](#data_acquisition)\n    - [Data Understanding](#data_understanding)\n    - [Data Correlation](#data_correlation)\n* [Data Splitting](#data_splitting)\n* [Model](#model)\n    - [Hyperparameters Tuning](#hyper-parameters_tuning)\n    - [LGBM Model](#lgbm_model)\n    - [RandomSearchCV](#randomsearchcv)\n* [Evaluation](#evaluation)\n* [Make Submission](#make_submission)","30448a10":"<a id=\"lgbm_model\"><\/a>\n### 4.2 LGBM Model","e80818c2":"<a id=\"data_splitting\"><\/a>\n## 3. Data Splitting","4b78a331":"<a id=\"data_acquisition\"><\/a>\n### 2.1 Data Acquisition","4dd487ff":"The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated on area under the **ROC Curve** between the predicted probability and target.","b3d5d79d":"## Memory Reduction","3d67fc26":"<a id=\"randomsearchcv\"><\/a>\n### 4.3 RandomSearchCV","67c988bd":"#### Importing Libraries","e8a4bc49":"<a id=\"data_understanding\"><\/a>\n### 2.2 Data Understanding","ef591e66":"<a id=\"evaluation\"><\/a>\n## 5. Evaluation","7fb3678f":"<a id=\"hyper-parameters_tuning\"><\/a>\n### 4.1 Hyper-Parameters Tuning","a0277e61":"<a id=\"introduction\"><\/a>\n## Introduction","cc111010":"#### Load the Dataset","58908d8a":"**Data Size**\n* Train dataset has 1000000 rows and 286 featurse which include target variable.\n* Test dataset has 5000000 rows and 286 featurse which is **not** include target variable.\n\n**Missing Values**\n* There is no missing value found in train and test dataset.\n\n**Total Features**\n* The number total of features for data type float is 240 and integer 46.\n\n**Target Variable**\n* The target output is represent in binary with (1 or 0)\n* Target distribution is consider to be balanced so no need to apply oversampling or undersampling method.","33a0c76b":"<a id=\"model\"><\/a>\n## 4. Model","8f09f0b1":"<a id=\"make_submission\"><\/a>\n## 6. Make Submission"}}