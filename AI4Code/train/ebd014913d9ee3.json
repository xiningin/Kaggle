{"cell_type":{"9a0a4eab":"code","6f27ae3d":"code","e9bb1f31":"code","949ae572":"code","a840ee5a":"code","4e2c7762":"code","9b52b25a":"code","1d191117":"code","df89a6ce":"code","83573147":"code","85034637":"code","09326fb3":"code","91d84b81":"code","4fea58fa":"code","d90b335b":"code","82fefa93":"code","cec276a5":"code","bd3d260e":"code","90c9a0d9":"code","248c3e14":"code","6efa2e72":"code","f2fea41d":"code","89073606":"code","76fe929f":"code","5b352455":"code","3d226432":"markdown","6beeff9f":"markdown","b7723f9d":"markdown","6fa9ebc4":"markdown","d2c267d2":"markdown","689e5825":"markdown","0b2d9e45":"markdown","ac52f0ca":"markdown","0e0561b1":"markdown","0edc6334":"markdown","7d24d23c":"markdown","c9f0c4c1":"markdown"},"source":{"9a0a4eab":"'''\nImportando as bbiotecas necess\u00e1rias\n'''\nimport sklearn\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as graphic\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score as acs\nfrom sklearn import preprocessing\nfrom time import time\n","6f27ae3d":"'''\nGuardando as bases de teste e treino\n'''\n\nadultTrain = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")\n\nadultTest = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\")\n","e9bb1f31":"adultTest.shape","949ae572":"adultTrain.shape","a840ee5a":"adultTrain.head()","4e2c7762":"'''\nDevemos eliminar os dados faltantes das bases de teste e de treinamento\n'''\nnTrain = adultTrain.dropna()\nadultTest.set_index('Id',inplace=True)\nnTest = adultTest.dropna()\nnTest.shape","9b52b25a":"nTrain.shape\nnTrain.head()","1d191117":"nTest.shape\nnTest.head()","df89a6ce":"'''\nTransformando os valores em n\u00famericos\n'''\nnumTrain = nTrain.apply(preprocessing.LabelEncoder().fit_transform)\nnumTest = adultTest.apply(preprocessing.LabelEncoder().fit_transform)\nnumTrain.head()","83573147":"adultTrain[\"native.country\"].value_counts()","85034637":"adultTrain[\"age\"].value_counts().plot(kind=\"bar\")","09326fb3":"adultTrain[\"sex\"].value_counts().plot(kind=\"bar\")","91d84b81":"adultTrain[\"education\"].value_counts().plot(kind=\"bar\")","4fea58fa":"adultTrain[\"occupation\"].value_counts().plot(kind=\"bar\")","d90b335b":"adultTrain[\"income\"].value_counts().plot(kind=\"bar\")","82fefa93":"adultTrain[\"income\"].value_counts().plot(kind=\"bar\")","cec276a5":"adultTrain[\"race\"].value_counts().plot(kind=\"pie\")","bd3d260e":"numTrain2 = nTrain.apply(preprocessing.LabelEncoder().fit_transform)\nnumTrain2.corr()","90c9a0d9":"'''\nPegando os dados com os maiores m\u00f3dulos entre as correla\u00e7\u00f5es (maior que 0.09)\n(\"age\",\"education.num\",\"marital.status\",\"relationship\",\"sex\",\"capital.gain\",\"capital.loss\",\"hours.per.week\")\n'''\nxTrain = numTrain[[\"age\",\"education.num\",\"marital.status\",\"relationship\",\"sex\",\"capital.gain\",\"capital.loss\",\"hours.per.week\"]]\nyTrain = nTrain.income\n\nxTest = numTest[[\"age\",\"education.num\",\"marital.status\",\"relationship\",\"sex\",\"capital.gain\",\"capital.loss\",\"hours.per.week\"]]\n","248c3e14":"def mediaClass (Class,CV,Xadult, Yadult):\n    '''\n    Essa fun\u00e7\u00e3o calcula a m\u00e9dia dos scores para CV foldes eilizando k neighbors \n    '''\n    scores = cross_val_score(Class, Xadult, Yadult, cv=CV)\n    average = 0\n    for i in scores:\n        average += i\n    average = average\/len(scores)\n    return average , scores ","6efa2e72":"from sklearn.ensemble import RandomForestClassifier\n\n# fazendo uma floresta aleat\u00f3ria\n#Testando alguns valores para achar melhor profundidade e n\u00famero de estimadores\n'''\nmediaMax=0\nscoreMax=0\ntempMax=0\n\nfor i in range (50,101,10):\n    for j in range (5,10):\n        randomForst=RandomForestClassifier(n_estimators=i,max_depth = j) \n\n        # Testando por valida\u00e7\u00e3o cruzada e o tempo de processamento:\n        \n        tempo0 = time()\n        media,scores = mediaClass (randomForst,10,xTrain, yTrain)\n        tempo1 = time()\n        tempoRndFrst = tempo1 - tempo0\n        if media>mediaMax:\n            mediaMax = media\n            scoreMax=scores\n            tempMax=tempoRndFrst\n            a=i\n            b=j\n'''","f2fea41d":"from sklearn.tree import DecisionTreeClassifier \n\n#Fazendo uma \u00e1rvore de decis\u00e3o\n#Testando alguns valores para achar melhor profundidade\n'''\nmediaMax=0\nscoreMax=0\ntempMax=0\n\nfor i in range (1,21):\n    Tree = DecisionTreeClassifier(max_depth=i)\n    \n    tempo0 = time()\n    media,scores = mediaClass (Tree,10,xTrain, yTrain)\n    tempo1 = time()\n    tempoTree = tempo1 - tempo0\n    if media>mediaMax:\n        mediaMax = media\n        scoreMax=scores\n        tempMax=tempoTree\n        a=i\n'''","89073606":"from sklearn import svm\n'''\nsvm = svm.SVC(gamma='auto')\ntempo0 = time()\nmedia,scores = mediaClass (svm,10,xTrain, yTrain)\ntempo1 = time()\ntemposvm = tempo1 - tempo0\n'''","76fe929f":"Tree = DecisionTreeClassifier(max_depth=9)\nTree.fit(xTrain,yTrain)\nyTest=Tree.predict(xTest)","5b352455":"savepath = \"predictions82.csv\"\nprev = pd.DataFrame(yTest, columns = [\"income\"])\nprev.to_csv(savepath, index_label=\"Id\")\nprev","3d226432":"**Assim chegamos nos melhores valores para a Suport Vector Machine de:**\n\n**Scores:** [0.82863772, 0.82499171, 0.83659264, 0.82493369, 0.83488064,\n       0.83554377, 0.84051724, 0.8295756 , 0.8358209 , 0.8331675 ]\n       \n**M\u00e9dia:** 0.8324661394667263\n\n**Tempo:** 680.6293885707855 segundos","6beeff9f":"**Parte 1 :** An\u00e1lise dos Dados","b7723f9d":"**PMR-3508 Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es (2019)**\n\nTrabalho 2 \n\nAn\u00e1lise: Base Adult 3 m\u00e9todos\n\nAutor: Thiago Lam Brawerman\n\nNUSP: 10770502","6fa9ebc4":"**\u00c1rvore de decis\u00e3o**","d2c267d2":"**Assim chegamos nos melhores valores para a \u00e1rvore de decis\u00e3o de:**\n\n**Profundidade M\u00e1xima:** 9\n\n**Scores:** [0.85747431, 0.85515413, 0.84885648, 0.85676393, 0.85112732,\n       0.85311671, 0.85046419, 0.85311671, 0.85008292, 0.85870647]\n       \n**M\u00e9dia:** 0.8534863164611277\n\n**Tempo:** 1.243316411972046 segundos","689e5825":"**Floresta Aleat\u00f3ria**","0b2d9e45":"**Assim chegamos nos melhores valores:**\n\n**N\u00famero de estimadores:** 60\n\n**Profundidade M\u00e1xima:** 9\n\n**Scores:** [0.85946304, 0.8568114 , 0.84819357, 0.85676393, 0.8551061 ,\n       0.8551061 , 0.84980106, 0.84980106, 0.85174129, 0.85804312]\n       \n**M\u00e9dia:** 0.8540830675197842\n\n**Tempo:** 16.81484627723694 segundos","ac52f0ca":"   A partir dos resultados de tempo e m\u00e9dia de acur\u00e1cias temos que o classificados de arvore de decis\u00e3o obteve os melhores resultados. Isso pois, os resultados obtidos atrav\u00e9s de valida\u00e7\u00f5es cruzadas dos 3 tipos diferente de classificadores mostraram que a acur\u00e1cia obtida pelo m\u00e9todo \"Suport Vector Machine\" era inferior aos outros dois e seu tempo de processamento era muito inferior, logo esse foi prontamente descartado. Comparando as outras acur\u00e1cias tivemos resultados muito poximos com uma diferen\u00e7a de +\/- 0.0006, sendo que a floresta aleat\u00f3ria teve resultados melhores. Contudo an\u00e1lisando o tempo de processamento os resultados da \u00e1rvore de decis\u00e3o foram por volta de 10 vezes menor, logoesse foi escolhido como o classificador a ser usado. ","0e0561b1":"**Parte 4 : ** Resultados","0edc6334":"**Parte 2 :** Preparo dos dados de Teste e de Treino","7d24d23c":"**Parte 3 : **Teste dos 3 modelos de aprendizado","c9f0c4c1":"**Suport Vector Machine**"}}