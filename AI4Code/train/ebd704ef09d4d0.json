{"cell_type":{"5bd5d937":"code","705e8a1c":"code","6b6546ed":"code","1a9dac33":"code","9028f442":"code","19a6e52c":"code","595b9c74":"code","347cf5e0":"code","53c02d47":"code","f9aa5128":"code","7948ce1d":"code","46e4254e":"code","d51bdd59":"code","a6ec30b2":"code","ed487d7e":"code","622d0ff1":"code","3b33d95b":"code","7824e09e":"code","a3e360c1":"code","bef92327":"code","1dd502dd":"code","8dcf3053":"code","7d19127c":"code","d24e78c6":"code","48c9622d":"code","55e269d0":"code","026fda20":"code","8fed5ab3":"code","85519289":"code","5e6dbe38":"code","87bb8eca":"code","8dd7b6db":"code","886122df":"code","40714bc3":"code","043b73be":"code","22b5ec43":"code","f12e3fb8":"code","e0a2c04f":"code","5449f569":"code","90bc88ca":"code","015940de":"code","79dee016":"code","276b9ac6":"code","085c57c2":"code","747f96e3":"code","313e1a0d":"code","d52839cf":"code","e5507a39":"code","b3b0f506":"code","a673e268":"code","743cfe3f":"markdown","daf88094":"markdown","3074dbfa":"markdown","59681737":"markdown","f2a9abf9":"markdown","5fd06ff1":"markdown","e0146e89":"markdown","14661af3":"markdown","3cb42323":"markdown","f186ad90":"markdown","b16d02c1":"markdown","cf86b641":"markdown","c2dd74c3":"markdown","a8148a8c":"markdown","a8814e03":"markdown","0cd11527":"markdown","8d812947":"markdown","31acb128":"markdown","4e509e3b":"markdown","62672fb1":"markdown","296d2ce9":"markdown","4f7a9b35":"markdown","cd1a2dcf":"markdown","85c249b5":"markdown","4a35f98a":"markdown"},"source":{"5bd5d937":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","705e8a1c":"titanic_train = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/titanic\/test.csv')","6b6546ed":"# Finding out where there is missing data\nprint(\"Training Dataset:\")\nprint(titanic_train.isnull().sum())\nprint()\nprint(\"Test Dataset:\")\nprint(titanic_test.isnull().sum())","1a9dac33":"# Add in the mean age for the null entries in age for both the training and test datasets\ntitanic_train['Age'] = titanic_train['Age'].fillna(titanic_train['Age'].mean())\ntitanic_test['Age'] = titanic_test['Age'].fillna(titanic_test['Age'].mean())\n\n# Add the mean inplace of the null entry for fare into the test set\ntitanic_test['Fare'] = titanic_test['Fare'].fillna(titanic_test['Fare'].mean())\n\n# Add the mode inplace of embarked null entries into to the training dataset\ntitanic_train['Embarked'] = titanic_train['Embarked'].fillna(titanic_train['Embarked'].mode()[0])\n\n# Drop the column, cabin, from both training and test datasets\ntitanic_train.drop(columns = ['Cabin'], inplace = True)\ntitanic_test.drop(columns = ['Cabin'], inplace = True)\n\n# Drop the column, name, from both the training and test datasets\ntitanic_train.drop(columns = ['Name'], inplace = True)\ntitanic_test.drop(columns = ['Name'], inplace = True)\n\n# Drop the column, ticket, from both the training and test datasets\ntitanic_train.drop(columns = ['Ticket'], inplace = True)\ntitanic_test.drop(columns = ['Ticket'], inplace = True)","9028f442":"# Confirm all missing data has been dealt with\n\nprint(\"Training Dataset:\")\nprint(titanic_train.isnull().sum())\nprint()\nprint(\"Test Dataset:\")\nprint(titanic_test.isnull().sum())","19a6e52c":"# Create the matrix of feature (X) and dependent variable vector (y) for the training set\nX = titanic_train.iloc[:, [2,3,4,5,6,7,8]].values\ny = titanic_train.iloc[:, 1].values","595b9c74":"print(X)","347cf5e0":"print(y)","53c02d47":"np.shape(X)","f9aa5128":"# Encoding the independent variables of the training set\n# Encode the Passenger class column\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder= 'passthrough')\nX = np.array(ct.fit_transform(X))\n# Get rid of one of the dummy variables\nX = X[:, 1:]","7948ce1d":"print(X)","46e4254e":"np.shape(X)","d51bdd59":"# Encode the Passenger sex class\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [2])], remainder= 'passthrough')\nX = np.array(ct.fit_transform(X))\n# Get rid of one of the dummy variables\nX = X[:, 1:]","a6ec30b2":"print(X)","ed487d7e":"np.shape(X)","622d0ff1":"# Encode the Passenger embarked column\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [7])], remainder= 'passthrough')\nX = np.array(ct.fit_transform(X))\n# Get rid of one of the dummy variables\nX = X[:, 1:]","3b33d95b":"print(X)","7824e09e":"np.shape(X)","a3e360c1":"# Splitting the data in the training set into its own training and test set.\n# This is so the accuracy of the models can be determined, before we make predictions on the test set.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","bef92327":"print(X_train)","1dd502dd":"print(X_test)","8dcf3053":"print(y_test)","7d19127c":"print(y_test)","d24e78c6":"# Standisation \nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","48c9622d":"print(X_train)","55e269d0":"print(X_test)","026fda20":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)","8fed5ab3":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","85519289":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","5e6dbe38":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","87bb8eca":"from sklearn.model_selection import GridSearchCV\nparameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},\n              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","8dd7b6db":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n# parameters: 100 trees, spliting quality based on entropy\n# RFC will not overfit, the as this data is assumed to be non-noisy as features have been selected and scaled\nrf_classifier.fit(X_train, y_train)","886122df":"y_pred = rf_classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","40714bc3":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","043b73be":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = rf_classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","22b5ec43":"X2_test = titanic_test.iloc[:, [1,2,3,4,5,6,7]].values","f12e3fb8":"print(X2_test)","e0a2c04f":"ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder= 'passthrough')\nX2_test = np.array(ct.fit_transform(X2_test))\n# Get rid of one of the dummy variables\nX2_test = X2_test[:, 1:]","5449f569":"print(X2_test)","90bc88ca":"ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [2])], remainder= 'passthrough')\nX2_test = np.array(ct.fit_transform(X2_test))\n# Get rid of one of the dummy variables\nX2_test = X2_test[:, 1:]","015940de":"print(X2_test)","79dee016":"np.shape(X2_test)","276b9ac6":"ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [7])], remainder= 'passthrough')\nX2_test = np.array(ct.fit_transform(X2_test))\n# Get rid of one of the dummy variables\nX2_test = X2_test[:, 1:]","085c57c2":"print(X2_test)","747f96e3":"X2_test = sc.transform(X2_test)","313e1a0d":"print(X2_test)","d52839cf":"y_hat = classifier.predict(X2_test)","e5507a39":"print(y_hat)","b3b0f506":"np.shape(y_hat)","a673e268":"for prediction in y_hat:\n    print(prediction)","743cfe3f":"**The sinking of the Titanic**\n\nOn April 15, 1912, on her maiden voyage, \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg as the hull was breached. There was a shortage of lifeboats on board, due to the \"unsinkable\" notion the Titanic was given. This resulted in the death of 1502 out of 2224 passengers and crew. Random chance played a role in the survival of some, but was there any other factors which were involved?\n\nHerein, a predictive model will be created to try a predict whether a passanger was likely to survive or not on the titanic based on various factors. The model will be trained on a dataset which contains the ground truth i.e. whether the passanger survived or not. Followed by the model making predictions for a test set which does not contain the ground truth.\n\nCompetition Page: https:\/\/www.kaggle.com\/c\/titanic","daf88094":"Creating a **confusion matrix** to test the accuracy.","3074dbfa":"Applying **Grid Search** to find the best model and the best parameters.","59681737":"Now apply feature scaling to the X2_test data.","f2a9abf9":"There is now no more missing data in either dataset.","5fd06ff1":"The Random Forest Classification model already has already shown that is not as robust as the Kernel SVM model in this case, but there it is still a powerful model. When the k-fold validation was proformed the accuracy decreased by approximately 3%. So even though a higher accuracy was achieved on this model, I will opt to use the Kernel SVM in my prediction of the test set.","e0146e89":"Training the **Random Forest Classification** model the training set","14661af3":"Encode the relevent data of X2_test.","3cb42323":"**Data preprocessing** The datasets will have their respective missing data and categorical data dealt with.","f186ad90":"The Kernel SVM model has proven to be a robust model as the accuracies in the accuracy socre, k-fold cross validation, and grid search have were only 0.6% appart. ","b16d02c1":"Applying **k-fold cross validation**.","cf86b641":"Moment of truth... Time to make the predictions using the Kernel SVM model on the test set. ","c2dd74c3":"Training the **Kernel SVM** model on the training set.","a8148a8c":"**Splitting the data**","a8814e03":"Creating a **confusion matrix** to test the accuracy.","0cd11527":"**Encode** the data within the training and test sets and don't fall for the dummy variable trap. If the are N dummy variables they are to be reduced to (N - 1) dummy variables. Both the training and test independent variables started with seven column. The number of columns will increase as dummy variables are created. Note the dummy variables are placed in the front of the columns which were there prior to the encoding.","8d812947":"**Feature Scaling** If not scaled, the features with high values will start dominating when calculating distances and will therefore become the prodominate features in the model.","31acb128":"**Import some libraries** Import libraries numpy, matplotlib, and pandas, then assign them respective shortcuts.","4e509e3b":"Applying **k-fold cross validation**.","62672fb1":"**Model Creation** Now that the data has been processed, models can be trained on the training set and then the best performing model on can be chosen to make predictions on the test set. ","296d2ce9":"**Import both the datasets**","4f7a9b35":"Now to apply the same encoding and feature scaling to the independent variables of the actual test dataset (titanic_test). Once this has been completed a predictions will be made on whether the passengers (who do not contain ground truth) survived the titanics or not.","cd1a2dcf":"Create an matrix of features (X) and dependent variable vector (y) for the training set. The dependent variable vector in this case is whether the passenger survived or not. ","85c249b5":"In both the training and test datasets there is a large amount of missing data in \"Cabin\", \"Age\", and \"Fare\" columns. Seeing as the \"Cabin\" feature has a large amount of missing information this column will be excluded from both the data sets. For the age feature the mean age will replace the null entries. The categorical feature \"Embarked\" will have its null entries replaced by the its mode. The \"Name\" column will be removed as the name of passenger is assumed not to have an impact on survival, rather their socio-economic standing would have better predictive capabilities and this is represented in the \"Fare\" and \"Pclass\" (passenger class) columns. The ticket number will be treated in the same way as the name.","4a35f98a":"Applying Grid Search to find the best hyperparameters for model after training the random forest classification proved to be to computationally intense. Therefore, only the k-fold cross validation was run on this model. "}}