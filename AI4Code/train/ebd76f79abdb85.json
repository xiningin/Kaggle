{"cell_type":{"9edfca68":"code","6eaf9f10":"code","cae19cf7":"code","06219f1b":"code","b40bbda2":"code","a0c1ebc3":"code","9fbd4b70":"code","c9282699":"code","6c48eabf":"code","b00b9172":"code","56a2d520":"code","8745c082":"code","5fe6707b":"code","edab61f0":"code","08abc8e9":"code","354fe11d":"code","81944e9e":"code","6c7effad":"code","82d354b4":"code","96aaa236":"code","59ab34ab":"code","9062a684":"code","67ddb717":"markdown","5ffc6c60":"markdown"},"source":{"9edfca68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6eaf9f10":"# To enable plotting graphs in Jupyter notebook\n%matplotlib inline ","cae19cf7":"import pandas as pd\nimport numpy as np\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\ncar_df = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/autos\/imports-85.data\",names = ['symboling','normalized_losses','make','fuel_type','aspiration','num_of_doors','body_style','drive_wheels','engine_location','wheel_base','length','width','height','curb_weight','engine_type','num_of_cylinders','engine_size','fuel_system','bore','stroke','compression_ratio','horsepower','peak_rpm','city_mpg','highway_mpg','price'])","06219f1b":"car_df.head(2).transpose()","b40bbda2":"car_df.dtypes","a0c1ebc3":"car_df = car_df.drop('make', axis=1) \n\n# dropping following columns due to low variance filter. i.e an attribute which is mostly one type of data is not a good dimension\ncar_df = car_df.drop('fuel_type', axis=1)\ncar_df = car_df.drop('engine_location', axis=1)\ncar_df = car_df.drop('num_of_doors', axis=1) \ncar_df = car_df.drop('body_style' , axis=1)\ncar_df = car_df.drop('drive_wheels', axis=1)\ncar_df = car_df.drop('engine_type', axis=1) # need more info on this column\ncar_df = car_df.drop('fuel_system', axis=1)\ncar_df = car_df.drop('aspiration', axis=1)\ncar_df = car_df.drop('normalized_losses', axis=1)\n","9fbd4b70":"# Check the structure of the dataframe after dropping the unwanted columns\ncar_df.dtypes","c9282699":"# Replace the string numbers into numerical values for number of cylinders\ncar_df['cylinder'] = car_df['num_of_cylinders'].replace({'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five':5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10, 'eleven': 11, 'twelve': 12})","6c48eabf":"car_df = car_df.replace('?', np.nan)  #replace ? with NA which is equivalent of NULL\n#car_df[car_df.isnull().any(axis=1)]  # display records with 'NA' \n\n# Change the attribute types from object to float type (generic numeric types) \ncar_df['bore'] = car_df['bore'].astype('float64')\ncar_df['stroke']= car_df['stroke'].astype('float64')\ncar_df['horsepower']= car_df['horsepower'].astype('float64')\ncar_df['peak_rpm']= car_df['peak_rpm'].astype('float64')\ncar_df['price'] = car_df['price'].astype('float64')\n#car_df['cylinder']= car_df['cylinder'].astype('int64')  # not required \n#car_df['normalized_losses']= car_df['normalized_losses'].astype('float64')\n\n# fill up NaN in numeric columns with median values of those columns respectively\ncar_df['price'] = car_df['price'].fillna(car_df['price'].median())\ncar_df['bore']= car_df['bore'].fillna(car_df['bore'].median())\ncar_df['horsepower'] = car_df['horsepower'].fillna(car_df['horsepower'].median())\ncar_df['peak_rpm'] = car_df['peak_rpm'].fillna(car_df['peak_rpm'].median())\ncar_df['stroke'] = car_df['stroke'].fillna(car_df['stroke'].median())\ncar_df['cylinder'] = car_df['cylinder'].fillna(car_df['cylinder'].median())\n","b00b9172":"# Look at the distribution of data on the various attributes. Look for outliers.... \n\ncar_df.describe().transpose()","56a2d520":"#importing seaborn for statistical plots\nimport seaborn as sns\ncar_df_attr = car_df.iloc[:,1:16]\n\nsns.pairplot(car_df_attr , diag_kind = 'kde')","8745c082":"# Copy all the predictor variables into X dataframe. Since 'mpg' is dependent variable drop it\nX = car_df.drop('price', axis=1)\nX = X.drop('num_of_cylinders', axis=1)# Removing this column as we have created another column \"cylinder\" out of this\n\n# Copy the 'mpg' column alone into the y dataframe. This is the dependent variable\ny = car_df[['price']]","5fe6707b":"#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\n\nfrom sklearn.model_selection import train_test_split","edab61f0":"# Split X and y into training and test set in 75:25 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)","08abc8e9":"# invoke the LinearRegression function and find the bestfit model on training data\nregression_model = LinearRegression()\nregression_model.fit(X_train, y_train)","354fe11d":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))","81944e9e":"# Let us check the intercept for the model\n\nintercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))","6c7effad":"# Model score - R2 or coeff of determinant\n# R^2=1\u2013RSS \/ TSS\n\nregression_model.score(X_test, y_test)","82d354b4":"import statsmodels.formula.api as smf\n\n# put the price column and the predictors together in on single array for OLS to work\n\n#cars = pd.concat([y_train, X_train], axis=1, join='inner')\ncars = pd.concat([y_train, X_train], axis=1)\ncars.head()","96aaa236":"lmcars = smf.ols(formula= 'price ~ symboling + wheel_base + length + width + height + curb_weight +  engine_size + bore + stroke + compression_ratio + horsepower + peak_rpm + city_mpg + highway_mpg  + cylinder', data=cars).fit()\n#lmcars = smf.ols(formula= 'price ~ length+ engine_size + bore+ stroke + compression_ratio + curb_weight+ peak_rpm + horsepower + cylinder', data=cars).fit()","59ab34ab":"lmcars.params","9062a684":"print(lmcars.summary())","67ddb717":"conclusions from the above scatter plot\n\n#did not consider other attributes such as number of doors, engine location etc due to low variance filter concept\n\n#pair plot shows clearly some relation between price and height, curb weight, engine size, number of cylinders, horsepower and inversely related to mpg\n\nThe data seems to be a mix of cars of high end and economy leading to a cluster of points at lower end in many of the attributes, leading to a long tail in multiple attributes\n\nIn this context a decision tree and linear regression could be a good start.","5ffc6c60":"How can the model peformance be improved?\n\nAlways address outliers first. Linear models get easily impacted by outliers\nWe noticed that on many attributes, there was a clump of data points on lower values and scattered on high values\nThat is an indication of mixing data from two or more different segments. Maybe if we segregate the data we will get better results\nNot all attributes show a linear relation to price. Convert these attributes to quadratic form which may give a better fit on linear model"}}