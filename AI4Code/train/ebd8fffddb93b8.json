{"cell_type":{"05ee3c20":"code","9eceb30d":"code","4bf8a2f4":"code","ea838f1b":"code","645279a9":"code","82e40d7b":"code","28d7aa9f":"code","87bc76a6":"code","49145e43":"code","c7d384fd":"code","ede0b823":"code","073dd6c2":"code","7d34fb00":"code","7fab79a6":"code","eda11a45":"code","aa6c0576":"code","f001a4c4":"code","d9791721":"code","77af5a43":"code","6e35468f":"code","4845a989":"code","09eac3a3":"code","f896cd2f":"code","1c977e0a":"code","8ef70387":"code","2b118024":"code","40188248":"code","603d7721":"code","34169abf":"markdown","b52f0baf":"markdown","57a1c644":"markdown","56333cec":"markdown","9730d084":"markdown"},"source":{"05ee3c20":"nrows = None","9eceb30d":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n%matplotlib inline\n\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","4bf8a2f4":"%%time\ntrain = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\", nrows = nrows)\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\", nrows = nrows)\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")","ea838f1b":"train.head()","645279a9":"test.head()","82e40d7b":"sub.head()","28d7aa9f":"df = pd.concat([train, test])\ndf = df.reset_index(drop = True)\ndf.shape","87bc76a6":"def clear(x, punct, rep = \"\"):\n    for p in punct:\n        x = x.replace(p, rep)\n    return x\npunct = [\"\u00f1\",\"\\\\\",\"\\\"\",\"\u00e8\",\"!\",\"\u2122\",\"\u00ae\",\"(\",\")\",\"'s\"]\nsplit_word = [\",\",\"&\",\"\/\",\"-\",\"\u2011\",\"+\"]\n\n## \u6b63\u898f\u5316\ndf[\"product_name\"] = df[\"product_name\"].apply(lambda words : clear(clear(words.lower(), punct, \"\"), split_word, \" \").split(\" \"))","49145e43":"plt.hist(df[\"product_name\"].apply(len))","c7d384fd":"from collections import defaultdict\nunuse_words = defaultdict(int)\n\ndef get_vec(x):\n    vs = []\n    for xx in x:\n        if len(vs) >= n_length:\n            break\n        try:\n            vs.append(model.wv[xx])\n        except:\n            flg = False\n            for i in range(1, len(xx)):\n                try:\n                    v1 = model.wv[xx[:i]]\n                    v2 = model.wv[xx[i:]]\n    #                     print(xx[:i], xx[i:])\n                    vs.append(v1)\n                    vs.append(v2)\n                    flg = True\n                    break\n                except:\n                    pass\n            if flg == False:\n                for i in range(1, len(xx)):\n                    try:\n                        v1 = model.wv[xx[:i]]\n    #                         print(xx[:i])\n                        vs.append(v1)\n                        break\n                    except:\n                        pass\n\n                if len(vs) >= n_length:\n                    break\n                for i in range(1, len(xx)):\n                    try:\n                        v2 = model.wv[xx[i:]]\n    #                         print(xx[i:])\n                        vs.append(v2)\n                        break\n                    except:\n                        pass\n                unuse_words[xx] += 1\n    if len(vs) < n_length:\n        for i in range(n_length - len(vs)):\n            vs.append(np.zeros(model.vector_size))\n    if len(vs) > n_length:\n        vs = vs[:n_length]\n    vs = np.array(vs)\n    if len(vs) == 0:\n        vs = np.zeros([1, model.vector_size])\n    return vs","ede0b823":"%%time\nimport gensim\nmodel = pd.read_pickle(\"..\/input\/ykc-cup-2nd-save-fasttext\/fasttext_gensim_model.pkl\")","073dd6c2":"\ndef to_vec(x):\n    vs = get_vec(x)\n    return vs\n\nn_length = 15\nvecs = df[\"product_name\"].apply(lambda x : to_vec(x))\nvecs = np.stack(vecs)\ndel model","7d34fb00":"sorted(unuse_words.items(), key=lambda x: x[1], reverse = True)[:100]","7fab79a6":"## word vector\u4ee5\u5916\u306e\u7279\u5fb4\u91cf\nadditional_feats = []\n\n## vector\u306b\u3067\u304d\u306a\u304b\u3063\u305f\u5358\u8a9e\nadditional_words = [k for k, v in unuse_words.items() if v >= 5 and len(k) > 2]\nprint(additional_words)\nadditional_word_cols = []\nfor w in additional_words:\n    c = f\"is_{w}\"\n    df[c] = df[\"product_name\"].apply(lambda x : w in x)\n    additional_word_cols.append(c)\n\nadditional_feats.append(df[additional_word_cols])\n\n## \nadditional_feats.append(pd.get_dummies(df[\"order_dow_mode\"]))\n\n##\nadditional_feats.append(pd.get_dummies(df[\"order_hour_of_day_mode\"]))\n\n##\nfrom sklearn.preprocessing import StandardScaler\nadditional_feats.append(StandardScaler().fit_transform(df[[\"order_rate\"]]))\n\nadditional_feats = np.hstack(additional_feats)\nadditional_feats.shape","eda11a45":"target = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\n\ntrain_wv = vecs[~df[target].isna()]\ntest_wv = vecs[df[target].isna()]\ntrain_x = additional_feats[~df[target].isna()]\ntest_x = additional_feats[df[target].isna()]\n\ny_train = df[~df[target].isna()][target].values\nfrom keras.utils.np_utils import to_categorical\ny_train_ohe = to_categorical(y_train, num_classes=None)\n\ndel vecs, additional_feats, df\ntrain_wv.shape, test_wv.shape, train_x.shape, test_x.shape, y_train.shape, y_train_ohe.shape","aa6c0576":"from tensorflow import keras\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam, SGD, Nadam\nfrom tensorflow.keras import backend as K\n","f001a4c4":"class F1(keras.callbacks.Callback):\n    def __init__(self, model, inputs, targets, epoch_max):\n        self.model = model\n        self.inputs = inputs\n        self.targets = targets\n        self.epoch = 0\n        self.epoch_max = epoch_max\n        self.eval_freq = 5\n        \n    def on_epoch_end(self, epoch, logs):\n        if (self.epoch % self.eval_freq == 0) or (self.epoch_max - self.epoch < 3):\n            p = self.model.predict(self.inputs)\n            score = get_score(self.targets, p)\n            print(f\"{self.epoch}, : \")\n            print(score)            \n        self.epoch += 1\n        \ndef get_score(t, p):\n    score = {\n        \"logloss\"  : log_loss(t, p),\n        \"f1_micro\" : f1_score(t, np.argmax(p, axis = 1), average = \"micro\")}\n    return score","d9791721":"def get_model(param):\n    inp_wv = Input((n_length, 300))\n    inp = Input((train_x.shape[1], ))\n    \n    mask = Lambda(lambda x : 1 \/ (1 + K.exp(-100 * K.std(x, axis = 2, keepdims = True))))(inp_wv)\n        \n    def calc_mean(x, mask, axis = 1, keepdims = False):\n        return K.sum(x * mask, axis = axis, keepdims = keepdims) \/ K.sum(mask, axis = axis, keepdims = keepdims)\n    \n    aggs = []\n    mean = Lambda(lambda x, mask : calc_mean(x, mask, axis = 1))(inp_wv, mask)\n    maximum = Lambda(lambda x : K.max(x, axis = 1))(inp_wv)\n    aggs.append(BatchNormalization()(mean))\n    aggs.append(BatchNormalization()(maximum))\n    \n    h = inp_wv\n    for f in param[\"n_units\"]:\n        h = keras.layers.Dense(f, \"relu\")(h)\n        h = Dropout(param[\"p_drop\"])(h)\n        mean = Lambda(lambda x, mask : calc_mean(x, mask, axis = 1))(h, mask)\n        maximum = Lambda(lambda x : K.max(x, axis = 1))(h)\n        aggs.append(BatchNormalization()(mean))\n        aggs.append(BatchNormalization()(maximum))\n    \n    h = Concatenate()(aggs)\n    h = Dropout(param[\"p_drop_fc\"])(h)\n    h = Concatenate()([h, inp])\n    for f in param[\"n_units_fc\"]:\n        h = Dense(f, \"relu\")(h)\n        h = Dropout(param[\"p_drop_fc\"])(h)\n    out = Dense(21, \"softmax\")(h)\n    model = keras.models.Model(inputs = [inp_wv, inp], outputs = out)\n    return model\n","77af5a43":"def trainNN(param, x_tr, y_tr, x_va, y_va, y_va_label, verbose = 0):\n    \n    param_base = {\n    \"lr\" : 1e-3,\n    \"lr_min\" : 1e-5,\n    \"lr_reduce_factor\" : 0.5,\n    \"lr_reduce_patience\" : 5,\n    \"epochs\" : 100,\n    }\n    param.update(param_base)\n    \n#     param = {\n#         \"lr\" : 1e-3,\n#         \"lr_min\" : 1e-5,\n#         \"lr_reduce_factor\" : 0.5,\n#         \"lr_reduce_patience\" : 5,\n#         \"decay\" : 1e-4,\n#         \"batch_size\" : 128,\n#         \"epochs\" : 2,\n#         \"n_unit\" : 512,\n#         \"n_layer\" : 1,\n#         \"n_unit_scale\" : 0.5,\n#         \"n_unit_fc\" : 512,\n#         \"n_layer_fc\" : 1,\n#         \"n_unit_fc_scale\" : 0.5,\n#         \"p_drop\" : 0.5\n#     }\n    \n    param[\"n_units\"] = [int(param[\"n_unit\"] * (param[\"n_unit_scale\"] ** k)) for k in range(param[\"n_layer\"])]\n    param[\"n_units_fc\"] = [int(param[\"n_unit_fc\"] * (param[\"n_unit_fc_scale\"] ** k)) for k in range(param[\"n_layer_fc\"])]\n    param[\"n_units_fc\"] = [n for n in param[\"n_units_fc\"] if n >=64]\n    print(param)\n    \n    model = get_model(param)\n    model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam(learning_rate=param[\"lr\"], decay = param[\"decay\"]))\n    cb_schedule = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=param[\"lr_reduce_factor\"], patience=param[\"lr_reduce_patience\"],\n                                                    verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=param[\"lr_min\"])\n    cb_earlystop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=10, restore_best_weights=True)\n    cb_save = keras.callbacks.ModelCheckpoint(f'model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n    callbacks = [cb_schedule, cb_earlystop, cb_save, F1(model, x_va, y_va_label, param[\"epochs\"])]\n\n    model.fit(x_tr, y_tr, \n              batch_size = param[\"batch_size\"],\n              epochs=param[\"epochs\"],\n              callbacks = callbacks,\n              validation_data=(x_va, y_va), verbose = verbose)\n    loss_history = model.history.history[\"val_loss\"]\n    return model","6e35468f":"def run_cv(param, n_split = 3, verbose = 1):\n    preds_test = []\n    scores = []\n    oof = np.zeros([len(train), 21])\n    kfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=42)\n    for i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, y_train)):\n        print(f\"--------fold {i_fold}-------\")\n\n        ## train data\n        x_tr_wv = train_wv[train_idx]\n        x_tr = train_x[train_idx]\n        y_tr = y_train_ohe[train_idx]\n\n        ## valid data\n        x_va_wv = train_wv[valid_idx]\n        x_va = train_x[valid_idx]\n        y_va = y_train_ohe[valid_idx]\n        y_va_label = y_train[valid_idx]\n\n        ## train LGBM model\n        model = trainNN(param, [x_tr_wv, x_tr], y_tr, [x_va_wv, x_va], y_va, y_va_label, verbose = verbose)\n\n        ## evaluate on valid\n        pred_val = model.predict([x_va_wv, x_va])\n        oof[valid_idx] = pred_val\n\n        score = get_score(y_va_label, pred_val)\n        print(score)\n        scores.append(score)\n\n        ## pred on test\n        pred_test = model.predict([test_wv, test_x])\n        preds_test.append(pred_test)\n        \n        del model\n\n    score_df = pd.DataFrame(scores)\n    return score_df, oof, preds_test","4845a989":"# import optuna\n# def objective(trial):\n#     param = {\n#         \"decay\" : trial.suggest_loguniform('decay', 1e-8, 1e-2),\n#         \"batch_size\" : trial.suggest_int('batch_size', 32, 512),\n#         \"n_unit\" : trial.suggest_int('n_unit', 128, 1024),\n#         \"n_layer\" : trial.suggest_int('n_layer', 1, 1),\n#         \"n_unit_scale\" : trial.suggest_uniform('n_unit_scale', 0.2, 0.5),\n#         \"n_unit_fc\" : trial.suggest_int('n_unit_fc', 128, 512),\n#         \"n_layer_fc\" : trial.suggest_int('n_layer_fc', 1, 3),\n#         \"n_unit_fc_scale\" : trial.suggest_uniform('n_unit_fc_scale', 0.2, 0.8),\n#         \"p_drop\" : trial.suggest_uniform('p_drop', 0.1, 0.8),\n#         \"p_drop_fc\" : trial.suggest_uniform('p_drop_fc', 0.2, 0.8),\n#     }\n    \n    \n#     score_df, oof, preds_test = run_cv(param, n_split = 3, verbose = False)\n#     score = score_df.mean()[\"logloss\"]\n#     return score\n \n    \n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=100, timeout = 26000)\n \n# print('Best trial:', study.best_trial.params)","09eac3a3":"# print('Best trial:', study.best_trial.params)","f896cd2f":"params = {'decay': 0.0002561250212980425, 'batch_size': 276, 'n_unit': 2048, 'n_layer': 1, 'n_unit_scale': 0.6165090828302822, 'n_unit_fc': 225, 'n_layer_fc': 1, 'n_unit_fc_scale': 0.5835791877627263, 'p_drop': 0.57113050869184264, 'p_drop_fc': 0.3911374259982809}\nscore_df, oof, preds_test = run_cv(params, n_split = 10, verbose = False)","1c977e0a":"score_df","8ef70387":"score_df.mean()","2b118024":"pred_test_mean = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_mean, axis = 1)","40188248":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","603d7721":"pd.to_pickle(\n    {\"train\" : train, \"test\" : test, \"oof\" : oof, \"pred_test\" : pred_test_mean, \"sub\" : sub}, \"data.pkl\")","34169abf":"## fasttext pretrain","b52f0baf":"\n## feature engineering","57a1c644":"## submission","56333cec":"## read data ","9730d084":"## NN"}}