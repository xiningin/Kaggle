{"cell_type":{"cb7045b8":"code","d73d678f":"code","054c787b":"code","1a40120d":"code","dbe4366e":"code","15e1f594":"code","bfbb1bee":"code","0f34b47a":"code","60753cfa":"code","a8bb8b04":"code","6fb4314c":"code","91208931":"code","bc6a9eae":"code","5d458b44":"code","c11764b8":"code","9596d58f":"code","63142d18":"code","d5d405a1":"code","17d1c786":"code","ee721f1f":"code","f8becf9a":"code","b1c05214":"code","aba28230":"code","4640edc1":"code","2462ad08":"code","82197240":"code","7a86a18d":"markdown","b7c22be4":"markdown","12b7719a":"markdown","14b2f3b2":"markdown","ddd4c535":"markdown","e2a155e9":"markdown","63b2c347":"markdown","cab8fb21":"markdown","c06eabeb":"markdown","fc727155":"markdown","404fc705":"markdown","dcf3f133":"markdown","f926db52":"markdown","e65b2871":"markdown"},"source":{"cb7045b8":"!pip install xmltodict\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport xmltodict\nimport random\nfrom os import listdir\nfrom os.path import isfile, join\nimport torchvision \nimport torch \nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n","d73d678f":"def getImageNames():\n    image_names = []\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            extension = fullpath[len(fullpath) - 4:]\n            if extension != '.xml':\n                image_names.append(filename)\n    return image_names\n\n\ndef get_path(image_name):\n    \n    #CREDIT: kaggle.com\/dohunkim\n    \n    home_path = '\/kaggle\/input\/face-mask-detection\/'\n    image_path = home_path + 'images\/' + image_name\n    \n    if image_name[-4:] == 'jpeg':\n        label_name = image_name[:-5] + '.xml'\n    else:\n        label_name = image_name[:-4] + '.xml'\n    \n    label_path = home_path + 'annotations\/' + label_name\n        \n    return  image_path, label_path","054c787b":"\ndef parse_xml(label_path):\n    \n    #CREDIT: kaggle.com\/dohunkim\n    \n    x = xmltodict.parse(open(label_path , 'rb'))\n    item_list = x['annotation']['object']\n    \n    # when image has only one bounding box\n    if not isinstance(item_list, list):\n        item_list = [item_list]\n        \n    result = []\n    \n    for item in item_list:\n        name = item['name']\n        bndbox = [(int(item['bndbox']['xmin']), int(item['bndbox']['ymin'])),\n                  (int(item['bndbox']['xmax']), int(item['bndbox']['ymax']))]       \n        result.append((name, bndbox))\n    \n    size = [int(x['annotation']['size']['width']), \n            int(x['annotation']['size']['height'])]\n    \n    return result, size\n\n\ndef visualize_image(image_name, bndbox=True):\n    \n    #CREDIT: kaggle.com\/dohunkim\n    \n    \n    image_path, label_path = get_path(image_name)\n    \n    image = cv2.imread(image_path)\n    print(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    if bndbox:\n        labels, size = parse_xml(label_path)\n        thickness = int(sum(size)\/400.)\n        \n        for label in labels:\n            name, bndbox = label\n            \n            if name == 'with_mask':\n                cv2.rectangle(image, bndbox[0], bndbox[1], (0, 255, 0), thickness)\n            elif name == 'without_mask':\n                cv2.rectangle(image, bndbox[0], bndbox[1], (255, 0, 0), thickness)\n            else: # name == 'none'\n                cv2.rectangle(image, bndbox[0], bndbox[1], (0, 0, 255), thickness)\n    \n    plt.figure(figsize=(20, 20))\n    plt.subplot(1, 2, 1)\n    plt.axis('off')\n    plt.title(image_name)\n    plt.imshow(image)\n    plt.show()","1a40120d":"image_names = getImageNames()\nimage_names[:4]","dbe4366e":"image_path, label_path = get_path(image_names[0])\nprint(image_path, label_path)\nvisualize_image(image_names[0])","15e1f594":"NUM_OF_IMGS_TO_VISUALIZE = 1\n%matplotlib inline\nfor i in range(5, NUM_OF_IMGS_TO_VISUALIZE):\n    visualize_image(image_names[i])","bfbb1bee":"def cropImage(image_name):\n    image_path, label_path = get_path(image_name)\n    \n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    \n    labels, size = parse_xml(label_path)\n    \n    cropedImgLabels = []\n\n    for label in labels:\n        name, bndbox = label\n        \n        \n        croped_image = image[bndbox[0][1]:bndbox[1][1], bndbox[0][0]:bndbox[1][0]]\n        \n        label_num = 0\n        \n        if name == \"without_mask\":\n            label_num = 1\n        else:\n            label_num = 0\n        \n        cropedImgLabel = [croped_image, label_num]\n        \n        cropedImgLabels.append(cropedImgLabel)\n        \n    return cropedImgLabels\n        \n        \n        \n    ","0f34b47a":"def createDirectory(dirname):\n    try:\n        os.mkdir(dirname)\n    except FileExistsError:\n        print(\"Directory \" + dirname + \" already exists.\")","60753cfa":"dir_name = 'train\/'\nlabel_0_dir = dir_name + \"0\/\"\nlabel_1_dir = dir_name + \"1\/\"\n#label_2_dir = dir_name + \"2\/\"\nmodels_dir = \"models\/\"\n\n\ncreateDirectory(dir_name)\ncreateDirectory(label_0_dir)\ncreateDirectory(label_1_dir)\n#createDirectory(label_2_dir)\ncreateDirectory(models_dir)","a8bb8b04":"label_0_counter = 0\nlabel_1_counter = 0\n#label_2_counter = 0\n\nfor image_name in image_names:\n    cropedImgLabels = cropImage(image_name)\n    \n    for cropedImgLabel in cropedImgLabels:\n        \n        label = cropedImgLabel[1]\n        img = cropedImgLabel[0]\n        \n        if label == 0:\n            croped_img_name = str(label_0_counter) + \".jpg\"\n            cv2.imwrite(label_0_dir + croped_img_name, img)\n            label_0_counter += 1\n        elif label == 1:\n            croped_img_name = str(label_1_counter) + \".jpg\"\n            cv2.imwrite(label_1_dir + croped_img_name, img)\n            label_1_counter += 1\n        #else:\n            #croped_img_name = str(label_2_counter) + \".jpg\"\n            #cv2.imwrite(label_2_dir + croped_img_name, img)\n            #label_2_counter += 1","6fb4314c":"filenames_label_0 = [f for f in listdir(label_0_dir) if isfile(join(label_0_dir, f))]\nfilenames_label_1 = [f for f in listdir(label_1_dir) if isfile(join(label_1_dir, f))]\n#onlyfiles_2 = [f for f in listdir(label_2_dir) if isfile(join(label_2_dir, f))]","91208931":"print(\"Total number of images: \" + str(len(filenames_label_0) + len(filenames_label_1)))\nprint(\"Number of images labeled 0: \" + str(len(filenames_label_0)))\nprint(\"Number of images labeled 1: \" + str(len(filenames_label_1)))\n#print(\"Number of images labeled 2: \" + str(len(onlyfiles_2)))","bc6a9eae":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","5d458b44":"model = models.resnet50(pretrained=True)","c11764b8":"for layer, param in model.named_parameters():\n    \n    if 'layer4' not in layer:\n        param.requires_grad = False\n\nmodel.fc = torch.nn.Sequential(torch.nn.Linear(2048, 512),\n                                 torch.nn.ReLU(),\n                                 torch.nn.Dropout(0.2),\n                                 torch.nn.Linear(512, 2),\n                                 torch.nn.LogSoftmax(dim=1))","9596d58f":"train_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n    ])","63142d18":"dataset = datasets.ImageFolder(dir_name, transform = train_transforms)\n\ndataset_size = len(dataset)\ntrain_size = int(dataset_size * 0.6)\nval_size = int(dataset_size * 0.2)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n\nprint('Dataset size: ', len(dataset))\nprint('Train set size: ', len(train_dataset))\nprint('Validation set size: ', len(val_dataset))\nprint('Test set size: ', len(test_dataset))","d5d405a1":"BATCH_SIZE = 20\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=True)\n\nval_loader = torch.utils.data.DataLoader(val_dataset,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=True)","17d1c786":"LEARNING_RATE = 0.001\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)","ee721f1f":"#model.cuda()\nmodel.to(device)","f8becf9a":"total_epoch = 20\n\nbest_epoch = 0\ntraining_losses = []\nval_losses = []\n\n\nfor epoch in range(total_epoch):\n    \n    epoch_train_loss = 0\n    \n    for X, y in train_loader:\n        \n        X, y = X.cuda(), y.cuda()\n        \n        optimizer.zero_grad()\n        result = model(X)\n        loss = criterion(result, y)\n        epoch_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n      \n    training_losses.append(epoch_train_loss)\n    \n    \n    epoch_val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for X, y in val_loader:\n            \n            X, y = X.cuda(), y.cuda()\n             \n            result = model(X)\n            loss = criterion(result, y)\n            epoch_val_loss += loss.item()\n            _, maximum = torch.max(result.data, 1)\n            total += y.size(0)\n            correct += (maximum == y).sum().item()\n            \n    val_losses.append(epoch_val_loss)\n    accuracy = correct\/total\n    print(\"EPOCH:\", epoch, \", Training Loss:\", epoch_train_loss, \", Validation Loss:\", epoch_val_loss, \", Accuracy: \", accuracy)\n    \n    \n    if min(val_losses) == val_losses[-1]:\n        best_epoch = epoch\n        checkpoint = {'model': model,\n                            'state_dict': model.state_dict(),\n                            'optimizer' : optimizer.state_dict()}\n\n        torch.save(checkpoint, models_dir + '{}.pth'.format(epoch))\n        print(\"Model saved\")","b1c05214":"plt.plot(range(total_epoch), training_losses, label='Training')\nplt.plot(range(total_epoch), val_losses, label='Validation')\nplt.legend()","aba28230":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    return model.eval()\n\n\nfilepath = models_dir + str(best_epoch) + \".pth\"\nloaded_model = load_checkpoint(filepath)\nprint(filepath)\n\ntrain_transforms = transforms.Compose([\n                                       transforms.Resize((224,224)),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n                                       ])","4640edc1":"correct = 0\ntotal = 0\n    \nwith torch.no_grad():\n    for X, y in test_loader:\n\n        X, y = X.cuda(), y.cuda()\n\n        result = loaded_model(X)\n        _, maximum = torch.max(result.data, 1)\n        total += y.size(0)\n        correct += (maximum == y).sum().item()\n\naccuracy = correct\/total\n\nprint(\"\\n\")\nprint(\"------------\")\nprint(\"Accuracy: \" + str(accuracy))\nprint(\"------------\")\nprint(\"\\n\")","2462ad08":"!pip install cvlib\n!wget \nimport cvlib as cv\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\n\nfont_scale=1\nthickness = 2\nred = (0,0,255)\ngreen = (0,255,0)\nblue = (255,0,0)\nfont=cv2.FONT_HERSHEY_SIMPLEX\n\n#File must be downloaded\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n\nwhile(cap.isOpened()):\n    ret, frame = cap.read()\n    if ret == True:\n\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.4, 4)\n        \n        for (x, y, w, h) in faces:\n            \n            cv2.rectangle(frame, (x, y), (x+w, y+h), blue, 2)\n            \n            croped_img = frame[y:y+h, x:x+w]\n            pil_image = Image.fromarray(croped_img, mode = \"RGB\")\n            pil_image = train_transforms(pil_image)\n            image = pil_image.unsqueeze(0)\n            \n            \n            result = loaded_model(image)\n            _, maximum = torch.max(result.data, 1)\n            prediction = maximum.item()\n\n            \n            if prediction == 0:\n                cv2.putText(frame, \"Masked\", (x,y - 10), font, font_scale, green, thickness)\n                cv2.rectangle(frame, (x, y), (x+w, y+h), green, 2)\n            elif prediction == 1:\n                cv2.putText(frame, \"No Mask\", (x,y - 10), font, font_scale, red, thickness)\n                cv2.rectangle(frame, (x, y), (x+w, y+h), red, 2)\n        \n        cv2.imshow('frame',frame)\n        \n        if (cv2.waitKey(1) & 0xFF) == ord('q'):\n            break\n    else:\n        break\n\ncap.release()\ncv2.destroyAllWindows()","82197240":"# !wget https:\/\/ichef.bbci.co.uk\/news\/1024\/cpsprodpb\/D9C8\/production\/_111125755_facemask.jpg -O test.jpg\n\ncroped_img = cv2.imread(\"test.jpg\")\n    \npil_image = Image.fromarray(croped_img, mode = \"RGB\")\npil_image = train_transforms(pil_image)\nimage = pil_image.unsqueeze(0)\n\n\nresult = loaded_model(image.to(\"cuda:0\"))\nm, maximum = torch.max(result.data, 1)\nprediction = maximum.item()\nprint(\"Maximum: \", result.data, \" \", prediction, m)","7a86a18d":"# Initialize Fully Connected Layers","b7c22be4":"# Create Necessary Directories","12b7719a":"# Detection\nRun the following code block on an actual computer","14b2f3b2":"# Test","ddd4c535":"# Loss and Optimizer Functions","e2a155e9":"# Shuffle and Split the Data","63b2c347":"# Real Time Medical Mask Detection","cab8fb21":"# Write Cropped Images","c06eabeb":"# Visualization of Training and Validation Losses","fc727155":"# Initialize Loaders","404fc705":"# Pretrained Resnet50 Model","dcf3f133":"# Training and Validation","f926db52":"# Loading the Existing Model","e65b2871":"# Demonstration\n\n### https:\/\/gifyu.com\/image\/lCJ0"}}