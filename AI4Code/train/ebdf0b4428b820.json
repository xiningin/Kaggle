{"cell_type":{"822524c4":"code","82f78980":"code","d6313517":"code","2bc61c70":"code","413829b2":"code","4a739cc7":"code","7e760e17":"code","76fb5506":"code","448f9a73":"code","458e56da":"code","4fa400a8":"code","2cac1e81":"code","7ff4c8aa":"code","91ad2c76":"code","b4e5e79d":"code","bae2f69f":"code","5af002ec":"code","9ffd7253":"code","c0aa0d30":"code","5b8a1611":"code","f72baa39":"code","1d387c8b":"code","76397ecf":"code","4c4b5e0f":"code","5e7fb5dc":"code","54a9e0a4":"code","ef182060":"code","10be7161":"code","7b97f5f7":"code","7c4f877d":"code","188d2f42":"code","3f5fda89":"code","4e1347b0":"code","c34e143c":"code","d500c9cc":"code","6a383357":"code","2758d383":"code","c69991e4":"code","6f2f0f18":"code","7e13c64c":"code","2bea8202":"code","8edf601d":"code","7f06f7e3":"code","4158e99c":"code","2eb8073e":"code","334b7ce7":"code","e3c3d4d4":"code","98eb7179":"code","3bf83424":"code","df2c19c9":"code","65a7702f":"code","eb14ce7b":"code","9c5aecbe":"code","ff13555c":"code","da0d3ee1":"code","e4aecfea":"code","fda4307f":"code","e9945548":"code","3f145e46":"code","e750cead":"code","e05dc6a3":"code","1a474504":"code","bc932d1d":"code","3fef7fcc":"code","d4c1fea3":"markdown","bb19f236":"markdown","f5bc264a":"markdown","0497bd64":"markdown","35de6e33":"markdown","d6bea0e2":"markdown","40b49a3a":"markdown","e3280462":"markdown","26c0537e":"markdown","f297f26a":"markdown","d4b7338a":"markdown","2c256ce5":"markdown","cbfa9b31":"markdown","28207e74":"markdown","7275f3ec":"markdown","d32f1dc6":"markdown","9b89a1c1":"markdown","6b8aabc9":"markdown","a8013a49":"markdown","ef643981":"markdown","685401ea":"markdown","9eeff0ae":"markdown","cffe525d":"markdown","083d5b32":"markdown","88efb1d1":"markdown","42c6bd9e":"markdown","a786a885":"markdown"},"source":{"822524c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82f78980":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","d6313517":"print(\"shape of training data:\",train.shape)\nprint(\"shape of testing data:\",test.shape)","2bc61c70":"train.head()","413829b2":"test.head()","4a739cc7":"# train.isnull().sum()","7e760e17":"numerical_feature = [feature for feature in train.columns if train[feature].dtype != 'O']\n\nprint(\"len of numerical_feature\",len(numerical_feature))","76fb5506":"train[numerical_feature].head()","448f9a73":"train[numerical_feature].dtypes","458e56da":"test_numeric = [feature for feature in test.columns if test[feature].dtype != 'O']\nprint(\"len of numeric feature in test data\",len(test_numeric))","4fa400a8":"test[test_numeric].isnull().sum()","2cac1e81":"# impute the nan in test data with median\nfor feature in test_numeric:\n    test[feature] = test[feature].fillna(test[feature].median())","7ff4c8aa":"# test[test_numeric].isnull().sum()","91ad2c76":"# we will check the relation of datetime variables with saleprice\nyear_feature = [feature for feature in numerical_feature if 'Yr' in feature or 'Year' in feature]\n\nprint(\"len of year feature\",len(year_feature))","b4e5e79d":"train[year_feature].head()","bae2f69f":"import matplotlib.pyplot as plt\nimport seaborn as sns","5af002ec":"# we will see the yearSold corresponding to saleprice\nplt.bar(train['YrSold'],train['SalePrice'])\nplt.show()","9ffd7253":"train.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel(\"year sold\")\nplt.ylabel(\"saleprice\")\nplt.title(\"YearSold Vs SalePrice\")\nplt.xticks(rotation=45)\nplt.show()","c0aa0d30":"for feature in year_feature:\n    if feature != 'YrSold':\n        data = train.copy()\n        \n        data[feature] = data['YrSold'] - data[feature]\n        \n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel(\"SalePrice\")\n        plt.title(feature + \" Vs SalePrice\")\n        plt.show()","5b8a1611":"train[year_feature].isna().sum()","f72baa39":"train['GarageYrBlt'].fillna(train['GarageYrBlt'].median(),inplace=True)","1d387c8b":"numerics_with_nan = [feature for feature in train.columns if train[feature].isna().sum()>1 and train[feature].dtype != 'O']\n\nfor feature in numerics_with_nan:\n    print(feature,\" perc_miss:\",np.round(train[feature].isna().mean(),3))","76397ecf":"for feature in numerics_with_nan:\n    median_val = train[feature].median()\n    \n    train[feature].fillna(median_val,inplace=True)","4c4b5e0f":"train[numerics_with_nan].isna().sum()","5e7fb5dc":"discrete_feature = [feature for feature in numerical_feature if len(train[feature].unique())<25 and feature not in year_feature + ['Id'] ]\n\nprint(\"len of discrete_feature\",len(discrete_feature))","54a9e0a4":"train[discrete_feature].head()","ef182060":"# let's see the relationshop of discrete feature with our target saleprice\nfor feature in discrete_feature:\n    train.groupby(feature)['SalePrice'].median().plot(kind='bar')\n    plt.xlabel(feature)\n    plt.ylabel(\"SalePrice\")\n    plt.title(feature)\n    plt.show()","10be7161":"continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature + year_feature + ['Id']]\n\nprint(\"len of continuous feature\",len(continuous_feature))","7b97f5f7":"train[continuous_feature].head()","7c4f877d":"# train[continuous_feature].isna().sum()","188d2f42":"for feature in continuous_feature:\n    data = train.copy()\n    data.hist(column=feature,bins=40)\n    plt.title(feature + ' Vs SalePrice')\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","3f5fda89":"for feature in continuous_feature:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        sns.boxplot(data[feature])\n        plt.title(feature)\n        plt.show()\n        ","4e1347b0":"# applying log-Normal transformation\nfor feature in continuous_feature:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data['SalePrice'] = np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.title(feature + ' Vs SalePrice')\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","c34e143c":"train[numerical_feature].corr()['SalePrice'][:-1].sort_values(ascending=False).to_frame()","d500c9cc":"categorical_feature = [feature for feature in train.columns if train[feature].dtype == 'O']\n\nprint(\"len of categorical_feature\",len(categorical_feature))","6a383357":"train[categorical_feature].head()","2758d383":"categories_with_nan = [feature for feature in categorical_feature if train[feature].isna().sum() > 1]\nprint(len(categories_with_nan))","c69991e4":"for feature in categories_with_nan:\n    print(feature,' count of null:',train[feature].isna().sum())","6f2f0f18":"feature_todrop = [feature for feature in categorical_feature if train[feature].isnull().mean() > 0.3]\nfeature_todrop","7e13c64c":"train.drop(feature_todrop,axis=1,inplace=True)\ntest.drop(feature_todrop,axis=1,inplace=True)","2bea8202":"# for example we will impute this by TA as it's most occuring compare to otheres.\ntrain['GarageQual'].value_counts()","8edf601d":"feature_toimpute = [feature for feature in train.columns if train[feature].isnull().mean() < 0.30 and train[feature].dtype=='O']\nlen(feature_toimpute)","7f06f7e3":"def impute_nan(train,variable):\n    most_frequent_category = train[variable].value_counts().index[0]\n    train[variable].fillna(most_frequent_category,inplace=True)","4158e99c":"# Now, lets impute the NAN values\nfor feature in feature_toimpute:\n    impute_nan(train,feature)","2eb8073e":"plt.figure(figsize=(20,20))\ncorr = train.corr()\nsns.heatmap(corr,vmax=0.9,annot=True)\nplt.show()","334b7ce7":"train.drop('Id',axis=1,inplace=True)","e3c3d4d4":"train.head()","98eb7179":"categorical_feature = [feature for feature in train.columns if train[feature].dtype == 'O']\n\nprint(\"len of categorical_feature\",len(categorical_feature))","3bf83424":"from sklearn.preprocessing import LabelEncoder","df2c19c9":"for c in categorical_feature:\n    lb = LabelEncoder()\n    lb.fit(list(train[c].values))\n    train[c] = lb.transform(list(train[c].values))\n   ","65a7702f":"test_category = [feature for feature in test.columns if test[feature].dtype == 'O']\nprint(\"len of category_feature in test data\",len(test_category))","eb14ce7b":"for c in test_category:\n    lb = LabelEncoder()\n    lb.fit(list(test[c].values))\n    test[c] = lb.transform(list(test[c].values))","9c5aecbe":"train[categorical_feature].head()","ff13555c":"test[test_category].head()","da0d3ee1":"print(train.shape)\nprint(test.shape)","e4aecfea":"x = train.iloc[:,:-1].values\ny = train['SalePrice']","fda4307f":"test_x = test.drop('Id',axis=1).values\ntest_x.shape","e9945548":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor","3f145e46":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\nmodel = RandomForestRegressor()\nmodel.fit(x_train,y_train)","e750cead":"y_pred = model.predict(x_test)","e05dc6a3":"from sklearn.metrics import mean_squared_error\nprint(model.score(x_test,y_test))\nmse = mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nprint(rmse)","1a474504":"model = RandomForestRegressor()\nmodel.fit(x,y)\n\n","bc932d1d":"predict = model.predict(test_x)","3fef7fcc":"submission = pd.DataFrame({'Id':test['Id'],'SalePrice':predict})\nsubmission.to_csv('submission.csv', index=False)","d4c1fea3":"* There are not any missing values in any one of numerical feature, we have impute all the missing value so, we can move ahead with the visualization part of continuous feature*","bb19f236":"**Now, All the features(Numerical & Categorical) with the Nan are best imputed and data is cleaned and ready for further analysis and feature selection part**","f5bc264a":"**Let's have a look at a discrete features**","0497bd64":"### Outliers**","35de6e33":"### Temporical variables","d6bea0e2":"**Imputing Missing values for categorical Features**","40b49a3a":"* OverQual can be seen as a strong-positive relationship with SalePrice","e3280462":"* The price of the property is decreasing here,","26c0537e":"* Id variable is of no need so we will drop the Id variable, as its only the counting index ","f297f26a":"** Temporary variable are the variables for which data changes after an interval of time for which a datetime. like every like wise the price of houses are increasing or not. **","d4b7338a":"**Discrete_Feature**","2c256ce5":"* There is not so difference between yearBuilt and saleprice, as salePrice was constant for many years thus we did not see the increase in yearSold also","cbfa9b31":"**Handling Missing values in numerical features**","28207e74":"## Numerical_features**","7275f3ec":"## Categorical_Features**","d32f1dc6":"**Observations**\n* OverallQual & GrLivArea has strong positive relationship with salePrice\n* GarageCars & GarageArea also has a good positive relation as the house with parking area has much more price.\n* KitchenAbvGr has negative relation\n* and the feature such as poolarea does not much more affect the salePrice.","9b89a1c1":"**If you find something interesting and useful please upvote these notebook. it will be very much motivating and appeciating for me to move ahead with the data science journey**\n**Thank You**","6b8aabc9":"**Continuous Features**\n* features which present in numerical feature and are not present in discrete feature, temporary variables.","a8013a49":"**Handling Categorical Features**","ef643981":"**The relationship seems quite interesting, we can remove these ouliers by bringing then in some particular range. we can do this by help of IQR, and by calculating the lower and upper boundary**","685401ea":"**There are 38 numerical feature which also include some Temporary variable such as yearSold for which I am going to have a separate analysis because this can have a kind of relationship that as the records per year are different and changing according to time**","9eeff0ae":"**Please take the informations on this notebook with a grain of salt. Please go thoroughly through every part of analysis, I have tried so much of things to find some better insights. I'm open to all improvements (even rewording), don't hesitate to leave me a comment or upvote if you found it useful. If I'm completely wrong somewhere or if my findings makes no sense don't hesitate to leave me a comment.**","cffe525d":"**Applying Random Forest Regressor**","083d5b32":"**This is my first data analysis on kaggle. I am a begineer with the Kaggle and this is my first notebook shared publically on kaggle.**","88efb1d1":"* we have drop the unwanted features and ready to impute remaining features","42c6bd9e":"**the feature having missing value greater then 30% will drop them and try to impute the outher missing values by using the simple technique known as  frequent occuring category imputation inplace of missing value as there are 1460 rows and nan less then 100 we can impute it with this.**","a786a885":"**The purpose of this EDA is to find insights which will serve us later in another notebook for Data cleaning\/preparation\/transformation which will ultimately be used into a machine learning algorithm.**"}}