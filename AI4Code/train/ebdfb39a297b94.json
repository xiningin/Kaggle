{"cell_type":{"eca87769":"code","44a48959":"code","2ddedfc0":"code","76b96c21":"code","5340bee4":"code","883c225d":"code","ccdb318d":"code","073a0d1e":"code","105f580f":"code","00c3fcad":"code","ab1bceb4":"code","c779fdac":"code","adef6d06":"markdown","f22562d7":"markdown","05f2b216":"markdown","10430f60":"markdown","5aa8ff46":"markdown","efcfe476":"markdown","07fbeef9":"markdown","6077d7e3":"markdown","0922c74e":"markdown"},"source":{"eca87769":"#DATA LOADING \nimport pandas as pd\nimport numpy as np \nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.feature_selection import chi2\nfrom matplotlib import pyplot\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom catboost import Pool, CatBoostClassifier, cv\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n#import pickle---(Check on if to save the model for training again !)\n#Loading The DataSet as Train Data and Test Data \nX = pd.read_csv(\"..\/input\/animalstate-awc\/train.csv\")\nX_test_full = pd.read_csv(\"..\/input\/animalstate-awc\/test.csv\")\nprint(\"A GLIMPSE OF THE DATA COLUMN TYPES--\")\ndisplay(X.info())\ny = X.outcome_type\nX.drop(['outcome_type'], axis=1, inplace=True)\n\n#FEATURE SELECTION OF NUMERIC AND CATEGORICAL COLUMNS -----\n\nNumerical_cols = [col for col in X.columns if X[col].dtype == \"int64\" or X[col].dtype ==\"float64\"]\nX_numeric = X[Numerical_cols]\nprint(\"The Following are Numerical Columns : \\n\",Numerical_cols,sep=\"\\n\")\n\n#Columns to be dropped are:- \n     #Dropped                                    -  Categorical Equivalent Columns\n#1. age_upon_intake(days),age_upon_intake(years) - [age_upon_intake_age_group]\n#2. age_upon_outcome(days), age_upon_outcome(years) -[age_upon_outcome_age_group]\n\n#3. intake_number - outcome_number, Both are Equal, very obvious. Hence dropping one of them\n\nX_numeric.drop(['age_upon_intake_(days)','age_upon_intake_(years)','age_upon_outcome_(days)',\n                'age_upon_outcome_(years)','intake_number'],axis=1,inplace=True)\n\n# Print number of unique entries by column, in ascending order\nobject_nunique = list(map(lambda col: X[col].nunique(), Numerical_cols))\nd1 = dict(zip(Numerical_cols, object_nunique))\n\n#print(\"For Numerical Variables, Unique Values in each Column : \", sorted(d1.items(), key=lambda x: x[1]), sep=\"\\n\")\nlist1 = ['age_upon_intake_(days)','age_upon_intake_(years)','age_upon_outcome_(days)',\n                'age_upon_outcome_(years)','intake_number']\nprint(\"Numerical Columns to Be dropped are: \\n\", list1)\nprint(\"Numerical Columns Considered for Correlation are : \\n\")\ndisplay( X_numeric.head())\n\n#CATEGORICAL COLUMNS FILETERING\nCategorical_cols = [col for col in X.columns if X[col].dtype == \"object\"]\nprint(\"The Following are Categorical Columns : \\n\",Categorical_cols)\n\n#GETTING THE CATEGORICAL DF READY\nX_categorical = X[Categorical_cols]\n\n#CHECKING FOR UNIQUE VALUES IN THE COLUMNS TO CONSIDER CARDINALITY AND HENCE DROP SUCH COLUMNS--\nobject_nunique = list(map(lambda col: X[col].nunique(), Categorical_cols))\nd = dict(zip(Categorical_cols, object_nunique))\n# Print number of unique entries by column, in ascending order\n\n#print(\"For Categorical Variables Unique Value in each column: \", sorted(d.items(), key=lambda x: x[1]), sep =\"\\n\")\n\n\n#REMOVING AND REPLACING HIGH CARDINALITY CATEGORICAL COLUMNS ---\n#Points to be kept in mind :- \n#We will Replace Most of the Categorical Columns with Numeric Ones to Enhace our Label encoding.\n#After Dropping the Columns from Here, we will Still Check for correlation, to extract proper Features\n#Dropped Columns are categorical data which was just increasing the cardinality. \n\n\n#Columns to be Dropped are :-\n    #Dropped        Numeric Columns Equivalent\n#1. date_of_birth - [dob_year,dob_month]\n#2. intake_monthyear, intake_datetime  - [intake_month, intake_year, intake_ hour]\n#3. outcome_monthyear, outcome_datetime - [outcome_month, outcome_year, outcome_hour]\n#4. time_in-shelter - [time_in_shelter_days]\n#5. animal_id_outcome - not considerate in training and correlation\n#6. age_upon_intake, age_upon_outcome\n\nX_categorical.drop([\"animal_id_outcome\",\"outcome_datetime\",\"intake_datetime\",\n                    'date_of_birth','intake_monthyear','intake_datetime',\n                    'outcome_monthyear','outcome_datetime','time_in_shelter',\n                   'age_upon_intake','age_upon_outcome'],axis=1,inplace=True)\n\nlist2 = [\"animal_id_outcome\",\"outcome_datetime\",\"intake_datetime\",\n                    'date_of_birth','intake_monthyear','intake_datetime',\n                    'outcome_monthyear','outcome_datetime','time_in_shelter',\n                   'age_upon_intake','age_upon_outcome']\nprint(\"Categorical Columns to Be dropped are : \\n\", list2)\nprint(\"Categorical Columns Considered for Correlation are : \\n\")\ndisplay(X_categorical.head())\n\n#DEFINING LABEL ENCODER -\nle = LabelEncoder()\n\n#USING LABEL ENCODER TO ENCODE ALL THE CATEGORICAL COLUMNS NOW--\nX_cCodes = X_categorical.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n\n#Remark : While label encoding Categorical Columns we will have float like string values,\n#hence handle such columns by converting Df columns astype(str) explicitly.\n\n#USING LABEL ENCODER TO ENCODE ALL THE NUMERICAL COLUMNS NOW--\nX_nCodes = X_numeric.apply(LabelEncoder().fit_transform)\n\n#TARGET VARIABLE COLUMN TRANSFORM - \n#Now Apply Label Encoding on The Target Variable Column - \n\nle.fit(y)\ny_codes = le.transform(y)\n\n#Displaying Label Encoded Columns, Now we proceed to Extract the Imp Features.\nprint(\"Displaying Categorical Label Encoded Columns : \")\ndisplay(X_cCodes.head())\nprint(\"Displaying Numerical Label Encoded Columns : \")\ndisplay(X_nCodes.head())\n\n\n\n\n\n\n\n\n","44a48959":"#CHI^2 TEST RUN !\n#Selecting Categorcial Features\nbestfeatures = SelectKBest(score_func=chi2, k=\"all\")\nfit = bestfeatures.fit(X_cCodes,y_codes)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_cCodes.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns\nfeatureScores[\"Score\"]=featureScores.Score.apply(np.round)\n#print(featureScores.nlargest(10,'Score'))  #print 10 best features\nfeatureScores.sort_values(by=['Score'], ascending=False,inplace=True)\nfeatureScores","2ddedfc0":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X_cCodes,y_codes)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X_cCodes.columns)\nfeat_importances.nlargest(11).plot(kind='barh')\nplt.show()\n","76b96c21":"#Hence Extracting Final Categorical Features.\nX_drop_categorical = X_categorical.drop([\"age_upon_intake_age_group\",\"age_upon_outcome_age_group\"], axis = 1)\n#X_drop_Numeric.head()\nfinalCatCol = list(X_drop_categorical.columns) #Use These Categorical Columns For Training.\n\n\n\nprint(\"Hence, Looking at the above Visualization we infer the list of final Categorical Columns :\",finalCatCol )","5340bee4":"#CHI^2 TEST RUN !\n#Selecting NUMERICAL Features\nbestfeatures = SelectKBest(score_func=chi2, k=\"all\")\nfit1 = bestfeatures.fit(X_nCodes,y_codes)\ndfscores1 = pd.DataFrame(fit1.scores_)\ndfcolumns1 = pd.DataFrame(X_nCodes.columns)\n#concat two dataframes for better visualization \nfeatureScores1 = pd.concat([dfcolumns1,dfscores1],axis=1)\nfeatureScores1.columns = ['Features','Score']  #naming the dataframe columns\nfeatureScores1[\"Score\"]=featureScores.Score.apply(np.round)\n#print(featureScores.nlargest(10,'Score'))  #print 10 best features\nfeatureScores1.sort_values(by=['Score'], ascending=False,inplace=True)\nfeatureScores1","883c225d":"model1 = ExtraTreesClassifier()\nmodel1.fit(X_nCodes,y_codes)\nprint(model1.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances1 = pd.Series(model1.feature_importances_, index=X_nCodes.columns)\nfeat_importances1.nlargest(11).plot(kind='barh')\nplt.show()\n\n","ccdb318d":"#Hence Extracting Final Numerical Features.\nX_drop_Numeric = X_numeric.drop([\"count\",\"outcome_year\",\"intake_year\"], axis = 1)\n#X_drop_Numeric.head()\nfinalNumericCol = list(X_drop_Numeric.columns) #Use These Numerical Columns For Training.\n\n\nprint(\"Hence, Looking at the above Visualization we infer the list of final numerical Columns :\",finalNumericCol)","073a0d1e":"SelectedFeatures = finalCatCol+finalNumericCol\nSelectedFeatures # List of final Selected Features Considered For training the data --","105f580f":"#Printing the Original Data with Selected Columns. \nTraindf = X[SelectedFeatures] #The Training DataFrame !! \ndisplay(Traindf.head())\nTraindf.isnull().sum() ","00c3fcad":"#HANDLING NULL VALUES AND CATEGORICAL COLUMNS\nTraindf.fillna(-999,inplace=True)\nX_test_full.fillna(-999,inplace=True)\ncate_features_index = np.where(Traindf.dtypes != float)[0]\n#SPLITTING THE DATASET FOR TRAINING AND EVALUATION OF THE MODEL\nTraindf[\"outcome_number\"] = Traindf[\"outcome_number\"].astype(np.int64)\nTraindf[\"time_in_shelter_days\"] = Traindf[\"time_in_shelter_days\"].astype(np.int64)\nX1_train, X1_test, y1_train, y1_test = train_test_split(Traindf,y, train_size=0.85,random_state=1234)\n\n#Preparing target variable column\nle = LabelEncoder()\nle.fit(y1_train)\ny1_train_enc = le.transform(y1_train)\ny1_test_enc = le.transform(y1_test)\n\n#TRAINING STAGE AND EVALUATION \n\n#TRAINING THE CATBOOSTCLASSIFIER MODEL ON TRAIN DATA\ncat = CatBoostClassifier(one_hot_max_size=7,eval_metric='Accuracy',\n                         use_best_model=True,random_seed=42,loss_function='MultiClass')\ncat.fit(X1_train,y1_train_enc,cat_features=cate_features_index,eval_set=(X1_test,y1_test_enc))\n \n#Checking the Accuracy of the Test Score\n#pool = Pool(X1_train, y1_train_enc, cat_features=cate_features_index)\n#cv_scores = cv(pool, cat.get_params(), fold_count=10, plot=True)\n#print('CV score: {:.5f}'.format(cv_scores['test-Accuracy-mean'].values[-1]))\nprint('the test accuracy is :{:.6f}'.format(accuracy_score(y1_test_enc,cat.predict(X1_test))))\n\n#PREDICTION AND SAVING RESULTS STAGE \n\n\n#PREDICTING VALUES BY USING TEST DATA\nX_ready = X_test_full[SelectedFeatures]\nX_ready[\"time_in_shelter_days\"] = X_ready[\"time_in_shelter_days\"].astype(np.int64)\npred = cat.predict(X_ready)\n\n#INVERSE LABELENCODING TRANSFORM TO PASS THE ORIGINAL LABELS PREDICTED BY THE MODEL AS A NUMPY ARRAY.\nresult = list(le.inverse_transform(pred))\nout_arr = np.asarray(result)\n\n#Checkpoint to Handle the PREDICTED VALUES passed to .csv file\n#print(type(result))\n#print(type(out_arr)) \n#print(result)  #You Can Print Predictions to the Console ! \n#print('Train', X1_train.shape, y1_train.shape)\n#print('Test', X1_test.shape, y1_test.shape)\n\n#WRITING THE OUTPUT, CONTAINING PREDICTED VALUES BACK TO THE SUBMISSION.CSV FILE.\noutput = pd.DataFrame({'animal_id_outcome': X_test_full.animal_id_outcome,'outcome_type': out_arr})\noutput.to_csv('submission.csv', index=False)\n\n\n\n","ab1bceb4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndf1 = pd.read_csv(\"..\/input\/forplot\/Final_Submission.csv\")\ndf1.isnull().sum()\ndf1[\"outcome_type\"].value_counts() # Predicted Values\n#pred_Values = df1[\"outcome_type\"].value_counts(normalize=True) #For relative Frequencies\npred_Values = df1[\"outcome_type\"].value_counts()\nplt.figure(figsize=(12,8))\nsns.barplot(pred_Values.index, pred_Values.values, alpha=1.0, dodge=False)\nplt.title('PREDICTED VALUES BY THE MODEL, ABOUT ANIMAL STATE ON 31,809 VALUES OF TEST DATA')\nplt.ylabel('Number of Animals', fontsize=12)\nplt.xlabel('Animal State', fontsize=12)\nplt.show()","c779fdac":"X2 = pd.read_csv(\"..\/input\/animalstate-awc\/train.csv\")\nX2.head()\nX2[\"outcome_type\"].value_counts() #Original DataSet Target Values\n#X2[\"outcome_type\"].value_counts(dropna=False)\noriginal_Values = X2[\"outcome_type\"].value_counts()\nplt.figure(figsize=(12,8))\nsns.barplot(original_Values.index, original_Values.values, alpha=1.0, dodge=False)\nplt.title('VALUES GIVEN IN THE TRAIN DATASET , ABOUT ANIMAL STATE ON 47,809 VALUES OF TRAIN DATA')\nplt.ylabel('Number of Animals', fontsize=12)\nplt.xlabel('Animal State', fontsize=12)\nplt.show()\n","adef6d06":"We Can See from Above there is 1 NAN in Sex_upon_intake and Sex_upon_outcome Columns.","f22562d7":"**EXTRACTING CATEGORICAL COLUMNS**","05f2b216":"![Imgur](https:\/\/i.imgur.com\/X7uFllL.png)","10430f60":"![Imgur](https:\/\/i.imgur.com\/DRVG7F3.png)","5aa8ff46":"**EXTRACTING NUMERICAL FEATURES**","efcfe476":"![Imgur](https:\/\/i.imgur.com\/F2FeDI8.png)\n","07fbeef9":"Animal Welfare Center - \n\nPrediction of Animal Outcome State, from a Multiclass target column. \nThe dataset comprises of Categorical and Numerical Features. Apply Your Furnished feature selection techniques and Modelling of classifiers to predict the state.","6077d7e3":"**DEFINING MODEL CATBOOSTCLASSIFIER AND EVALUATION,\nPREDICTING THE VALUES AND SAVING THE RESULTS**","0922c74e":"**ANALYSIS AND DISCUSSION ON RESULTS**\n\n1. On Seeing the above two visualization, you can clearly form out a picture how much the trained model is aligned to the given data. \n\n2. Variation in Plots is beacuse of less values in test data(31,809 approx.) in comparison to train data(47,809 approx.) \n\n3. About Data set you can clearly say how unbalanced the data is for few values like( \"missing\", \"Died\", \"Relocate\", \"RTO-Adopt\", \"Disposal\"). \n\n4. Rest all preidcted values are likely to be aligned with training data. Clearly ! Though we can say many animals went for Adoption after their period in the Welfare Center. \n\n5. Well Because of missing data in test dataset, few values cannot be predicted by the model clearly because of no data given for such scenarios. which limits the accuracy of model to 65%. Moreover efficiency over 75% is very difficult to achieve in such unbalanced data.\n\n6. But still the Model let's you classify the category of animal state, provided you pass the required params. \n\n7. I see this as a advantage and an application in field of Medical Healthcare for animals and will let the other veterinary practitioners to comment on the outcome_type of animal state, so as to provide better medical facilities and extra care to specific category of animals."}}