{"cell_type":{"76621f22":"code","5ec7fe49":"code","646a82e1":"code","e1dbb216":"code","8a6aee89":"code","f0ee6604":"code","68b88b32":"code","a915e7cb":"code","5bb39722":"code","deb3c7df":"code","d4ac91d5":"code","cdd4f68d":"code","510e72bb":"code","0b9dd12f":"code","14f8417c":"code","d66c4de9":"code","547e9426":"code","87a5d3e7":"code","74d5e07b":"code","afa48cc7":"code","2f3fa6b2":"code","e7bef123":"code","60cd1db4":"code","73e056ca":"code","c2238e10":"code","ab8f9652":"code","fe5a8193":"code","7a915c39":"code","8861fc86":"code","86278cb2":"code","b5f3c939":"code","d7c6b0f7":"code","911216e4":"code","0755f657":"code","4ddc8e4a":"code","9d957e4a":"code","927551e3":"code","417e55c5":"code","c86cb807":"code","e3260c3f":"code","50a8a44a":"code","0d14440e":"code","313b75f2":"code","0c41473a":"code","33ef067c":"code","cd039644":"code","af6e84d6":"code","139030be":"code","70c64d67":"code","b2a82bcd":"markdown","99c85207":"markdown","23728ac1":"markdown","d95b8b78":"markdown","6e40dd08":"markdown","2472a14e":"markdown","0f52455d":"markdown","f9ce6d11":"markdown","02f1ee45":"markdown","9a11ac76":"markdown","eb4b7bda":"markdown","28838846":"markdown","1c2d8103":"markdown","58f7b51e":"markdown","03e21d26":"markdown","bc3522f5":"markdown","68d353f6":"markdown","73de47cd":"markdown","600a41f4":"markdown","03778c24":"markdown","b69a73c8":"markdown","42e95e56":"markdown","ed4c68fa":"markdown","d95809f1":"markdown","b7d58136":"markdown","82d8d352":"markdown","2f55e6ab":"markdown","69eb7d45":"markdown","c39ba5f6":"markdown"},"source":{"76621f22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error , r2_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import OneHotEncoder\nimport time","5ec7fe49":"def Explore(df):\n        print('DataFrame Shape : \\n')\n        display(df.shape)\n        print('DataFrame examples : \\n')\n        display(df.head())\n        print(\"information about Dataset\")\n        display(df.info())\n        print('DataFrame Description : \\n')\n        display(df.describe().T)","646a82e1":"x=pd.read_csv('\/kaggle\/input\/neolen-house-price-prediction\/train.csv',index_col='Id')\nx_test=pd.read_csv(\"\/kaggle\/input\/neolen-house-price-prediction\/test.csv\",index_col='Id')","e1dbb216":"Explore(x)","8a6aee89":"Explore(x_test)","f0ee6604":"x.loc[:, x.dtypes == object]","68b88b32":"x['SalePrice'].hist(bins=200,rwidth = 0.5,figsize=(20,10))\nplt.title('House Prices')\nplt.show()","a915e7cb":"x.groupby(['MoSold','YrSold']).count().plot(kind='barh',figsize=(14,22))\nplt.xlabel('Selling Count')\nplt.title('Times of Selling Houses')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()","5bb39722":"x.groupby(['Neighborhood']).count().plot(kind='barh',figsize=(15,22))\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()","deb3c7df":"fig, ax = plt.subplots(figsize=(30,22))\ncorr_matrix = x.corr().abs()\nsns.heatmap(corr_matrix, annot=True, linewidths=.5, ax=ax,center=0)","d4ac91d5":"house_null_count=x.isnull().sum()*100\/len(x)\nmiss_house_plot=house_null_count.sort_values(ascending=False).head(15).plot(kind='barh',figsize=(50,22),fontsize=30)","cdd4f68d":"house_null_count.sort_values(ascending=False)[0:10]","510e72bb":"drop_columns_list=x.columns[(house_null_count\/100) > .50]\ndrop_columns_list=list(drop_columns_list)\ndrop_columns_list","0b9dd12f":"x.drop(columns=['Alley', 'PoolQC', 'Fence','MiscFeature'], inplace=True)\nx_test.drop(columns=['Alley', 'PoolQC', 'Fence','MiscFeature'], inplace=True)","14f8417c":"house_row_null_count=x.isnull().sum(axis=1)\nhouse_row_null_count.describe()","d66c4de9":"obj_cols=x.select_dtypes('object').columns\nobj_cols","547e9426":"for col in obj_cols:\n    print('{} column has {} unique values'.format(col,x[col].nunique()))","87a5d3e7":"print(x['Utilities'].unique())\nprint(x['CentralAir'].unique())\nprint(x['LandSlope'].unique())\nprint(x['ExterQual'].unique())\nprint(x['ExterCond'].unique())\nprint(x['BsmtQual'].unique())\nprint(x['BsmtCond'].unique())\nprint(x['BsmtExposure'].unique())\nprint(x['BsmtFinType1'].unique())\nprint(x['BsmtFinType2'].unique())\nprint(x['HeatingQC'].unique())\nprint(x['KitchenQual'].unique())\nprint(x['FireplaceQu'].unique())\nprint(x['GarageQual'].unique())\nprint(x['GarageCond'].unique())","74d5e07b":"x['Utilities'] = x.Utilities.map({'AllPub':1,'NoSeWa':0})\nx['CentralAir'] = x.CentralAir.map({'Y':1,'N':0})\nx['LandSlope'] = x.LandSlope.map({'Gtl':0,'Mod':1,'Sev':3})\nx['ExterQual'] = x.ExterQual.map({'Gd':2,'TA':1,'Ex':3,'Fa':0})\nx['ExterCond'] = x.ExterCond.map({'Gd':3,'TA':2,'Ex':4,'Fa':1,'Po':0})\nx['BsmtQual'] = x.BsmtQual.map({'Fa':1,'TA':2,'Gd':3,'Ex':4})\nx['BsmtCond'] = x.BsmtCond.map({'Po':1,'Fa':2,'TA':3,'Gd':4})\nx['BsmtExposure'] = x.BsmtExposure.map({'No':1,'Mn':2,'Av':3,'Gd':4})\nx['BsmtFinType1'] = x.BsmtFinType1.map({'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\nx['BsmtFinType2'] = x.BsmtFinType2.map({'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\nx['HeatingQC'] = x.HeatingQC.map({'Gd':3,'TA':2,'Ex':4,'Fa':1,'Po':0})\nx['KitchenQual'] = x.KitchenQual.map({'Gd':2,'TA':1,'Ex':3,'Fa':0})\nx['FireplaceQu'] = x.FireplaceQu.map({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nx['GarageQual'] = x.GarageQual.map({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nx['GarageCond'] = x.GarageCond.map({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\nx_test['Utilities'] = x_test.Utilities.map({'AllPub':1,'NoSeWa':0})\nx_test['CentralAir'] = x_test.CentralAir.map({'Y':1,'N':0})\nx_test['LandSlope'] = x_test.LandSlope.map({'Gtl':0,'Mod':1,'Sev':3})\nx_test['ExterQual'] = x_test.ExterQual.map({'Gd':2,'TA':1,'Ex':3,'Fa':0})\nx_test['ExterCond'] = x_test.ExterCond.map({'Gd':3,'TA':2,'Ex':4,'Fa':1,'Po':0})\nx_test['BsmtQual'] = x_test.BsmtQual.map({'Fa':1,'TA':2,'Gd':3,'Ex':4})\nx_test['BsmtCond'] = x_test.BsmtCond.map({'Po':1,'Fa':2,'TA':3,'Gd':4})\nx_test['BsmtExposure'] = x_test.BsmtExposure.map({'No':1,'Mn':2,'Av':3,'Gd':4})\nx_test['BsmtFinType1'] = x_test.BsmtFinType1.map({'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\nx_test['BsmtFinType2'] = x_test.BsmtFinType2.map({'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\nx_test['HeatingQC'] = x_test.HeatingQC.map({'Gd':3,'TA':2,'Ex':4,'Fa':1,'Po':0})\nx_test['KitchenQual'] = x_test.KitchenQual.map({'Gd':2,'TA':1,'Ex':3,'Fa':0})\nx_test['FireplaceQu'] = x_test.FireplaceQu.map({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nx_test['GarageQual'] = x_test.GarageQual.map({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nx_test['GarageCond'] = x_test.GarageCond.map({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})","afa48cc7":"x['BsmtQual'].fillna(0, inplace=True)\nx['BsmtCond'].fillna(0, inplace=True)\nx['BsmtExposure'].fillna(0, inplace=True)\nx['BsmtFinType1'].fillna(0, inplace=True)\nx['BsmtFinType2'].fillna(0, inplace=True)\nx['FireplaceQu'].fillna(0, inplace=True)\nx['GarageQual'].fillna(0, inplace=True)\nx['GarageCond'].fillna(0, inplace=True)\n\nx_test['BsmtQual'].fillna(0, inplace=True)\nx_test['BsmtCond'].fillna(0, inplace=True)\nx_test['BsmtExposure'].fillna(0, inplace=True)\nx_test['BsmtFinType1'].fillna(0, inplace=True)\nx_test['BsmtFinType2'].fillna(0, inplace=True)\nx_test['FireplaceQu'].fillna(0, inplace=True)\nx_test['GarageQual'].fillna(0, inplace=True)\nx_test['GarageCond'].fillna(0, inplace=True)","2f3fa6b2":"print(x.shape)\nprint(x_test.shape)","e7bef123":"obj_cols=x.select_dtypes('object').columns\nobj_cols","60cd1db4":"x=pd.get_dummies(x,columns=obj_cols)\nx_test=pd.get_dummies(x_test,columns=obj_cols)","73e056ca":"print(x.shape)\nprint(x_test.shape)","c2238e10":"corelation=abs(x.corr()['SalePrice']) >= 0.4\ncorelation[corelation==True]\n","ab8f9652":"x=x[corelation.index[corelation]]\ncorelation=corelation.drop(['SalePrice'])\nx_test=x_test[corelation.index[corelation]]","fe5a8193":"x.head()","7a915c39":"missing_val_count_by_column = (x.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","8861fc86":"x['MasVnrArea'].hist()","86278cb2":"x['GarageYrBlt'].hist()","b5f3c939":"x['MasVnrArea']=x['MasVnrArea'].fillna(x['MasVnrArea'].median())\nx['GarageYrBlt']=x['GarageYrBlt'].fillna(x['GarageYrBlt'].median())\n\nx_test['MasVnrArea']=x_test['MasVnrArea'].fillna(x_test['MasVnrArea'].median())\nx_test['GarageYrBlt']=x_test['GarageYrBlt'].fillna(x_test['GarageYrBlt'].median())","d7c6b0f7":"x.var()","911216e4":"# scaler=StandardScaler()\n# x_imputed_cols=x_imputed.columns\n# x_test_imputed_cols=x_test_imputed.columns\n# x_imputed=pd.DataFrame(scaler.fit_transform(x_imputed),columns=x_imputed_cols)\n# x_test_imputed=pd.DataFrame(scaler.fit_transform(x_test_imputed),columns=x_test_imputed_cols)","0755f657":"# scaler=MinMaxScaler()\n# x_imputed_cols=x_imputed.columns\n# x_test_imputed_cols=x_test_imputed.columns\n# x_imputed=pd.DataFrame(scaler.fit_transform(x_imputed.astype(float)),columns=x_imputed_cols)\n# x_test_imputed=pd.DataFrame(scaler.fit_transform(x_test_imputed.astype(float)),columns=x_test_imputed_cols)","4ddc8e4a":"Y=x['SalePrice']\nx=x.drop(['SalePrice'],axis=1)","9d957e4a":"x_mean=x.mean()\nx_std=x.std()\nprint(x_mean[x_mean>1])\n\nx_test_mean=x_test.mean()\nx_test_std=x_test.std()\nprint(x_test_mean[x_test_mean>1])","927551e3":"x=0.5*(np.tanh(0.01*((x-x_mean)\/x_std))+1)\nx_test=0.5*(np.tanh(0.01*((x_test-x_test_mean)\/x_test_std))+1)","417e55c5":"print(x.var())\nprint(x_test.var())","c86cb807":"pca = PCA().fit(x)\nplt.rcParams[\"figure.figsize\"] = (20,10)\n\nfig, ax = plt.subplots()\n\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","e3260c3f":"# def reduce_data(df,comp_number=130):\n#     pca = PCA(n_components=comp_number).fit(df)\n#     reduced_data = pca.transform(df)\n#     reduced_data = pd.DataFrame(reduced_data)\n#     print(pca.explained_variance_ratio_.sum())\n#     return reduced_data","50a8a44a":"# reduced_x=reduce_data(x_imputed)\n# reduced_x_test=reduce_data(x_test_imputed)","0d14440e":"# print(reduced_x.shape)\n# print(reduced_x_test.shape)\n# print(Y.shape)","313b75f2":"x_train, x_validate, y_train, y_validate=train_test_split(x, Y, test_size=0.20, random_state=42,shuffle=True)","0c41473a":"lr=LinearRegression()\ndt=DecisionTreeRegressor()\nrf=RandomForestRegressor()\nsvm=SVR()\nxgb=XGBRegressor()\nadab=AdaBoostRegressor()\nclass_list=[lr, dt,rf,svm,xgb,adab]\nfor model in class_list:\n        start = time.time()\n        grid = GridSearchCV(estimator=model, param_grid={}, scoring='neg_root_mean_squared_error', cv=3, n_jobs=-1,verbose=3)\n        grid.fit(x, Y)\n        end = time.time()\n        print(model, '\\n', -grid.best_score_,'\\n', round(end-start),grid.best_estimator_)","33ef067c":"# param_grid = {'n_estimators': [50,100,150,300,500],\n#               'min_samples_split': [2,3],\n#              'warm_start':[True,False]}\n# kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n\n# best = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, scoring='neg_root_mean_squared_error', cv=kfold, n_jobs=-1,verbose=3)\n# best.fit(x, Y)\n\n# print(-best.best_score_,best.best_estimator_,best.best_params_)","cd039644":"# model=RandomForestRegressor(min_samples_split=3, n_estimators=50, warm_start=True)\n# model.fit(x_train,y_train)\n# prediction=model.predict(x_validate)\n# print(mean_squared_error(y_validate, prediction,squared=False))","af6e84d6":"# model=XGBRegressor()\n# model.fit(x_train,y_train)\n# prediction=model.predict(x_validate)\n# print(mean_squared_error(y_validate,prediction,squared=False))\n","139030be":"final_model=XGBRegressor()\nfinal_model.fit(x,Y)\nfinal_predict=final_model.predict(x_test)\nsubmission = pd.DataFrame({'Id': x_test.index,'SalePrice': final_predict})\nsubmission","70c64d67":"submission.to_csv('\/kaggle\/working\/submission_XGB_TanNorm_TopFeatures_final.csv', index=False)","b2a82bcd":"# After observing the results i got using the three methods i found that surprisengly the tanh normalization got the best results, it might be an indicator of much outliers found in the dataset","99c85207":"* **From previous cell exterior1st and exterior2nd and neighborhood columns has many unique values so they might increase complexity of the model after encoding but after reading the describtion of data i thought they might be important features so i decided to encode them and depend on the highly correlated features with the resultant encodings of all data to get smaller chunck of mmost usefull features to train the model**","23728ac1":"from the previous plot i realised that most of the houses where sold in the NAmes neighbourhood","d95b8b78":"from the previous plot i realised that most of the houses where sold in the summer months 6 and 7","6e40dd08":"# Checking missing values in the remaining features","2472a14e":"# Encoding the non ordinal features","0f52455d":"**The two features are skewed one for the right and the other to the left so i decided to fill its missing values using median**","f9ce6d11":"# Some plots and Visualizations to understand our Data more and get intuition of its behaviour","02f1ee45":"**as the following features were ordinal but in the discribtion it stated that the nan values in these columns mean that this feature is missing so i decided to fill the missing with 0 to try to indicat to the model that it has the least weight as these features are ordinal features**","9a11ac76":"# Data Standardization","eb4b7bda":"**Before trying to use correlation to select the most highly correlated features with SalePrice i tried to use the PCA after encoding most of the features but the results were not good enough**","28838846":"# Observe Object Type Columns and understand it from it's describtion","1c2d8103":"# Trying Different Models using GridSearchCV and select the best two models to try them","58f7b51e":"from the correlation heatmap i observed that the highest correlated features with the sale price where OverallQal which is normal and expected, but also the feature GrLivArea which is houses with  above ground level has high correlation with the SalePrice","03e21d26":"# Check the distribution of these two columns to decide what startegy to fill nan values","bc3522f5":"# Handle Object types columns","68d353f6":"From previouse cell i observed that most houses are in range beween 100,000 and 250,000","73de47cd":"# Preprocessing","600a41f4":"# Using Correlation to get the most correalated features with respect to our label SalePrice","03778c24":"**From describtion of data i found that there are ordinal features like Utilities, CentralAir, LandSlope, ExterQual, ExterCond,\nBsmtQual,BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, KitchenQual, FireplaceQu, GarageQual, GarageCond\nso i decided to encode them manually.**","b69a73c8":"# PCA","42e95e56":"# Some Exploration to understand our Data","ed4c68fa":"# Best Model Was XGBoost Model","d95809f1":"**Trying Diffrent Compinations of the hyperparameters with RandomForest to get the best compination**","b7d58136":"**Try Tanh Normalization Technique.\nTanh estimators are considered to be very efficient and robust normalization technique. It is not sensitive to outliers and it also converges faster than Z-score normalization.\nso i decided to try to use it and see the results.**","82d8d352":"**the best two models were Random Forest and XGBooost**","2f55e6ab":"# Split X and Y into Train and Validation with Ratios 80:20","69eb7d45":"Plotting percentage of missing values in each column","c39ba5f6":"More than have of the values of 'Alley', 'PoolQC', 'Fence', 'MiscFeature' are missing and it is hard to impute them correctly and deal with them, so i decided to drop them even if i think some of them are important features but i afm afraid to confuse the midel with incorrect handling for this huge number of missing values."}}