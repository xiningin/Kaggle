{"cell_type":{"94b2afe8":"code","f27a04a1":"code","df054f66":"code","547d10d8":"code","5c08a2dc":"code","f9e48eea":"code","f8484457":"code","ed1e5773":"code","cf98b654":"code","4ffc280f":"code","d9e6a14d":"code","d358204d":"code","120d0bad":"code","cf06ef84":"code","92dd4fa0":"code","fdbe1947":"code","bbd8c77b":"code","6ecc510f":"code","4b489d8b":"code","c7790462":"code","aee9a515":"code","e12fd211":"code","343214ca":"code","f9c92535":"code","f7d484ef":"code","a13d4836":"code","b3e2395a":"code","21ce24ea":"code","535657c8":"code","a02d80c8":"code","09cc4dc7":"code","d9b4fa92":"code","08bca06a":"code","0d07d009":"code","8b142315":"code","4d9c9d36":"code","dc9c681a":"code","f86a0a67":"code","50451d65":"code","7831b1b9":"code","9ebbac86":"code","801108fc":"code","e6130778":"code","a785be25":"code","5a706c24":"code","269cb02a":"code","869c64c2":"code","399f532d":"code","fdc81921":"code","398937e9":"code","4f3eeabb":"code","9fc53017":"code","3f7ab2d5":"code","118bb1e2":"code","b74b68a0":"code","e3827eea":"code","98e084f6":"code","fa3f8f49":"code","c28966fe":"code","910f29fc":"markdown","838bb5c4":"markdown","bb5becf8":"markdown","a9ab9215":"markdown","a526b23c":"markdown","dad293c7":"markdown","7c83d921":"markdown","60784d67":"markdown","78b0a87c":"markdown","01736818":"markdown","3514bd34":"markdown","802d7997":"markdown","f9877fe6":"markdown","24047710":"markdown","95f106b7":"markdown","a29b3e86":"markdown","56dd2081":"markdown","b2e99232":"markdown","ad127b99":"markdown","432d088f":"markdown","de87e253":"markdown","54291c3b":"markdown","d7169d01":"markdown","b6dac36f":"markdown","ae77d880":"markdown","8115e846":"markdown","761fd871":"markdown","03719d2e":"markdown","5e5b53af":"markdown","9d2b572c":"markdown","e762d070":"markdown","04e442e6":"markdown","178927be":"markdown","8247ea55":"markdown","ba99f258":"markdown","8ebde068":"markdown","dcddb338":"markdown","5c063057":"markdown","710965c3":"markdown","07e972a8":"markdown","f69220db":"markdown","643453ad":"markdown","2c4aeec3":"markdown","e49d4185":"markdown","89da5f08":"markdown","247cea95":"markdown","e1c23669":"markdown","222156dc":"markdown","d6ed9e11":"markdown","3207d296":"markdown","d7dbdf3b":"markdown","7393cf43":"markdown","42861b66":"markdown","24da0723":"markdown"},"source":{"94b2afe8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f27a04a1":"#Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","df054f66":"#import required modules\nfrom pandas_profiling import ProfileReport\nimport missingno as msno\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport math\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n#Pre-Processing libraries\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import FeatureUnion, Pipeline\n\n#Sk-Learn Models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Sk-Learn Model Selection\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.feature_selection import RFECV, RFE\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom scipy import stats","547d10d8":"#Set Seaborn Theme\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", rc=custom_params)\nsns.set_style(\"darkgrid\")","5c08a2dc":"#Setting to display Pipeline\nfrom sklearn import set_config\nset_config(display=\"diagram\")","f9e48eea":"df = pd.read_csv('..\/input\/progetto-data-intensive-applications\/bank-additional-full.csv', sep = \";\")","f8484457":"#Visualize the dataframe\ndf.head()","ed1e5773":"# profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n# profile.to_notebook_iframe()","cf98b654":"# Visualize missing values as a matrix\nmsno.matrix(df)","4ffc280f":"#","d9e6a14d":"# # Moving the reponse variable \"turnover\" to the front of the table\n# front = df['turnover']\n# df.drop(labels=['turnover'], axis=1,inplace = True)\n# df.insert(0, 'turnover', front)\n# df.head()","d358204d":"# Check for numbe of duplicates in the dataset\nprint(df.duplicated().sum())","120d0bad":"#Remove duplicates\ndf.drop_duplicates(inplace=True)","cf06ef84":"# Check for numbe of duplicates in the dataset\nprint(df.duplicated().sum())","92dd4fa0":"#Customers who successfully subscribed to a term deposit\n\"{:.2f}\".format(df[\"y\"].value_counts()[\"yes\"]\/(df[\"y\"].value_counts()[\"yes\"]+df[\"y\"].value_counts()[\"no\"]))","fdbe1947":"#Check the frequency of the output variable\npd.value_counts(df['y']).plot.bar()\nplt.title('Fraud class histogram')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\ndf['y'].value_counts()","bbd8c77b":"#Split the train and test data\ntrain, test =  train_test_split(df,test_size=0.2, random_state=123, stratify=df['y'])\nprint(train.shape)\nprint(test.shape)","6ecc510f":"df.info()","4b489d8b":"df.head()","c7790462":"df.info()","aee9a515":"df.head()","e12fd211":"#Get the list of Numerical columns for scaling\nnumCols = ['age',\n           'duration',\n           'campaign',\n           'pdays',\n           'previous',\n           'emp.var.rate',\n           'cons.price.idx',\n           'cons.conf.idx',\n           'euribor3m',\n           'nr.employed']","343214ca":"#Get the list of categorical columns for dummy encoding\ndummyColList = ['job',\n                'marital',\n                'education',\n                'default',\n                'housing',\n                'loan',\n                'contact',\n                'month',\n                'day_of_week',\n                'poutcome']","f9c92535":"#Custom Transformer that transforms Categorical columns\nclass DummyEncoding( BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, dummyColList):\n        self.dummyColList = dummyColList\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    #Transformer method we wrote for this transformer \n    def transform(self, X , y = None ):\n        \n        #One-Hot Encoding of categorical columns\n        #Get dummy variables\n        for each_col in self.dummyColList:\n            X[each_col] = pd.factorize(X[each_col])[0]\n        \n        return X","f7d484ef":"#Custom Transformer that scales Numerical columns\nclass CustomScaler( BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, numCols):\n        self.numCols = numCols\n        self.scaler = MinMaxScaler()\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        self.scaler.fit(X[numCols])\n        return self\n    \n    #Transformer method we wrote for this transformer \n    def transform(self, X , y = None ):\n        \n        X[numCols] = self.scaler.transform(X[numCols])\n        \n        return X","a13d4836":"train[\"y\"] = pd.factorize(train[\"y\"])[0]\ntest[\"y\"] = pd.factorize(test[\"y\"])[0]","b3e2395a":"#Defining the steps in the categorical pipeline \ncategorical_pipeline = Pipeline( steps = [('dummyEncodingTransformer', DummyEncoding(dummyColList))] )","21ce24ea":"#Defining the steps in the numerical pipeline     \nnumerical_pipeline = Pipeline( steps = [ ( 'min_max_scaler', CustomScaler(numCols)) ] )","535657c8":"#Combining numerical and categorical piepline into one full big pipeline horizontally \n#using FeatureUnion\npreProcessingPipeline = Pipeline( steps = [ ( 'categorical_pipeline', categorical_pipeline ), \n                                                  \n                                                  ( 'numerical_pipeline', numerical_pipeline ) ] )\n\n","a02d80c8":"preProcessingPipeline","09cc4dc7":"#Fit_transform the pipeline on training data\ntrain_transform = preProcessingPipeline.fit_transform(train)","d9b4fa92":"#Transform the pipeline on test data\ntest_transform = preProcessingPipeline.transform(test)","08bca06a":"train_transform.head()","0d07d009":"# Putting response variable to y\ny_train = train_transform.pop('y')\nX_train = train_transform","8b142315":"# Putting response variable to y\ny_test = test_transform.pop('y')\nX_test = test_transform","4d9c9d36":"sm = SMOTE(random_state=2)\nX_train, y_train = sm.fit_resample(X_train, y_train.ravel())","dc9c681a":"#Hyper Parameters for different models \nparamsLogReg = {\n                'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n                'C' : [100, 10, 1.0, 0.1, 0.01],\n                'solver' : ['lbfgs','newton-cg','liblinear','sag','saga']\n                }\n\nparamsRidge = {\"alpha\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\nparamsSGD = {}\nparamsKNN = {\n            'n_neighbors' : range(1,21,2),\n            'weights' : ['uniform','distance'],\n            'metric' : ['minkowski','euclidean','manhattan']\n            }\n\nparamsDT = {\n            'max_depth': [2, 3, 5, 10, 20],\n            'min_samples_leaf': [5, 10, 20, 50, 100],\n            'criterion': [\"gini\", \"entropy\"]\n            }\n\nparamsRF = {'n_estimators': [25, 50, 100, 150, 200], \n            'max_depth': [3, 5, 7, 9], \n            'max_features': [\"auto\", \"sqrt\", \"log2\"], \n            'random_state': [42]\n           }\n\nparamsBC = {\n            \"n_estimators\": [10, 100, 1000]\n            }\n\nparamsGBC = {\n            \"n_estimators\": [10, 100, 1000],\n            \"learning_rate\": [0.001, 0.01, 0.1],\n            \"subsample\": [0.5, 0.7, 1.0],\n            \"max_depth\": [3, 7, 9]\n            }\n\n","f86a0a67":"#List of models to evaluate\nmodels = [{\"modelName\": \"LogisticRegression\", \n           \"model\": LogisticRegression(), \n           \"modelAvgCVScore\": 10, \n           \"modelParams\": paramsLogReg, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"Ridge\", \n           \"model\": RidgeClassifier(), \n           \"modelAvgCVScore\": 11, \n           \"modelParams\": paramsRidge, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []},\n          \n          {\"modelName\": \"SGD\", \n           \"model\": SGDClassifier(), \n           \"modelAvgCVScore\": 9, \n           \"modelParams\": paramsSGD, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"KNN\", \n           \"model\": KNeighborsClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsKNN, \n           \"cvResults\": [], \n           \"rfeStatus\": False, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"DecisionTree\", \n           \"model\": DecisionTreeClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsDT, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n        \n          {\"modelName\": \"RandomForest\", \n           \"model\": RandomForestClassifier(n_estimators = 100), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsRF, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"BaggingClassifier\", \n           \"model\": BaggingClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsBC, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          },\n          \n          {\"modelName\": \"BoostingClassifier\", \n           \"model\": GradientBoostingClassifier(), \n           \"modelAvgCVScore\": 0, \n           \"modelParams\": paramsGBC, \n           \"cvResults\": [], \n           \"rfeStatus\": True, \n           \"rfeModels\": [], \n           \"rfeInputFeaturesCount\": [], \n           \"RFECVResults\": []\n          }\n          \n         ]","50451d65":"#Minimun number of features for RFE\nmin_features = 2\n\n#Maximum number of features for RFE\nmax_features = X_train.shape[1]\n\n#Step size in RFECV\nrfecvStep = 3\n\n#Perform manual RFE\nrfeStatus = False\n\n#Perform Auto RFE\nrfeCVStatus = True","7831b1b9":"#Scoring parameter\nscoring = \"roc_auc\"\n\n#Number of splits in K-Fold Cross Validation\nn_splits = 5\n\n#Random state\nrandom_state = 23\n\n#Shuffle in K-Fold cross validation\nshuffle = True","9ebbac86":"class Models:\n    \n    #Init Function\n    def __init__(self, models, min_features, max_features, rfecvStep, scoring, n_splits, random_state, shuffle, rfeStatus, rfeCVStatus):\n        \n        self.models = models\n        self.min_features = min_features\n        self.max_features = max_features\n        self.scoring = scoring\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.shuffle = shuffle\n        self.rfecvStep = rfecvStep\n        self.rfeStatus = rfeStatus\n        self.rfeCVStatus = rfeCVStatus\n    \n    #K-Fold Cross validation\n    def crossValidation(self, model, X, y):\n        #Instantiate KFold\n        kfold = KFold(n_splits=self.n_splits, random_state=self.random_state, shuffle=self.shuffle)\n        return cross_val_score(model, X, y, cv=kfold, scoring=self.scoring)\n    \n    #Function to runModels\n    def runModels(self, X, y):\n        \n        print(\"Starting to execute and compare various models...\")\n        #Loop over each model and do K-Fold Cross validation to select the best model\n        for each_model in self.models:\n            print(\"Running {}\".format(each_model[\"modelName\"]))\n            \n            #List of CV results of each model\n            each_model[\"cvResults\"] = self.crossValidation(each_model[\"model\"], X, y)\n            \n            #Average CV score\n            each_model[\"modelAvgCVScore\"] = each_model[\"cvResults\"].mean()\n            \n            \n            if math.isnan(each_model[\"modelAvgCVScore\"]):\n                each_model[\"modelAvgCVScore\"] = 0\n\n        print(\"All models successfully executed\")\n    \n    #Function to compare Model results        \n    def compareModels(self, results, names):\n        \n        #Print results of various models\n        for each_result, modelName in zip(results, names):\n            print(\"Model Name: %s:\" % (modelName))\n            print(\"Model Average CV Score: %f\" % (each_result.mean()))\n            print(\"Model CV Std (%f)\" % (each_result.std())) \n            print(\"CV_Results: \", (each_result))\n            print(\"\\n\")\n        \n        # Box Plot of Model Results\n        fig = plt.figure()\n        fig.suptitle('Algorithm Comparison')\n        ax = fig.add_subplot(111)\n        plt.boxplot(results)\n        ax.set_xticklabels(names)\n        plt.xticks(rotation=45)\n        plt.show()\n        \n    #Get shortlisted Model\n    def getShortlistedModel(self):\n        \n        #Get shortlisted model based on highest average CV Score\n        self.shortlistedModel = max(self.models, key=lambda x:x['modelAvgCVScore'])\n        \n        print(\"Shortlisted model is : \", self.shortlistedModel)\n            \n            \n    #Function to run RFE models and return the CV results\n    def runRFEModels(self, X, y):\n        \n        results = []\n        \n        print(\"These are the models: \", self.shortlistedModel[\"rfeModels\"])\n        \n        if ((self.rfeStatus == True) and (self.shortlistedModel[\"rfeStatus\"] == True)):\n        \n            #Loop over the rfe models for the shortlisted model\n            for each_model in self.shortlistedModel[\"rfeModels\"]:\n            \n                #CV results of each model\n                cvResults = self.crossValidation(each_model, X, y)\n                print(\"Average score is : \", cvResults.mean())\n            \n                results.append(cvResults)\n            \n        return results\n            \n            \n    #Get the list of RFE Models\n    def getRfeModels(self):\n\n        #Perform RFE on the model\n        for i in range(self.min_features, self.max_features):\n            rfe = RFE(estimator=self.shortlistedModel[\"model\"], n_features_to_select=i)\n            self.shortlistedModel[\"rfeModels\"].append(Pipeline(steps=[('selection',rfe),('model',self.shortlistedModel[\"model\"])]))\n            self.shortlistedModel[\"rfeInputFeaturesCount\"].append(i)\n\n                    \n    #Set optimial number of features based on RFE Model\n    def getOptimalNumFeatures(self):\n#         self.shortlistedModel[]\n\n        #It is an array of arrays with each nested array containing CV results\n        results = modelObj.shortlistedModel[\"RFECVResults\"]\n        meanResults = [x.mean() for x in results]\n        maxpos = meanResults.index(max(meanResults)) \n\n        self.optimalFeatures =  list(range(self.min_features, self.max_features))[maxpos]\n\n    \n    #Run shortlisted RFE Model with optimal number of features to actually get feature names\n    def getBestFeatures(self, X, y):\n        \n        \n        rfe = RFE(estimator=self.shortlistedModel[\"model\"], n_features_to_select=self.optimalFeatures)\n        self.pipe = Pipeline(steps=[('selection',rfe), ('model', self.shortlistedModel[\"model\"])])\n        self.pipe.fit(X, y)\n\n        # summarize all features\n        print(\"Summarizing the results of RFE\")\n        for each_col, col_num in zip(X.columns, list(range(X.shape[1]))):\n            print('ColumnName: %s | Selected %s | Rank: %.3f' % (each_col, rfe.support_[col_num], rfe.ranking_[col_num]))\n\n\n        print(\"\\n\")\n        print(\"List of selected columns\")\n        print(list(zip(X.columns,rfe.support_,rfe.ranking_)))\n\n        #Visualize the selected columns\n        print(\"\\n\")\n        print(\"Visualize selected columns\")\n        print(X.columns[rfe.support_])\n\n        return X.columns[rfe.support_]\n        \n    \n    #Run RFECV Model to find the names of optimal features\n    def getBestFeaturesCV(self, X, y):\n\n        rfecv = RFECV(estimator=self.shortlistedModel[\"model\"], step=self.rfecvStep, cv=self.n_splits)\n\n        self.pipe = Pipeline(steps=[('selection',rfecv), ('model', self.shortlistedModel[\"model\"])])\n        self.pipe.fit(X, y)\n\n        # summarize all features\n        print(\"Summarizing the results of RFE\")\n        for each_col, col_num in zip(X.columns, list(range(X.shape[1]))):\n            print('ColumnName: %s | Selected %s | Rank: %.3f' % (each_col, rfecv.support_[col_num], rfecv.ranking_[col_num]))\n\n\n        print(\"\\n\")\n        print(\"List of selected columns\")\n        print(\"this is rfecv support: \", rfecv.support_)\n        print(list(zip(X.columns,rfecv.support_,rfecv.ranking_)))\n\n        #Visualize the selected columns\n        print(\"\\n\")\n        print(\"Visualize selected columns\")\n        print(X.columns[rfecv.support_])\n\n        return X.columns[rfecv.support_]\n    \n        \n    def getParams(self, modelName):\n        \n        for each_param in self.params:\n            \n            if each_param[\"modelName\"] == modelName:\n                return each_param[\"modelParams\"]\n            \n            else:\n                return 0\n               \n    def hyperParameterOptimization(self):\n        # Instantiate the grid search model\n        grid_search = RandomizedSearchCV(estimator=self.shortlistedModel[\"model\"], \n                           param_distributions=self.shortlistedModel[\"modelParams\"], \n                           cv=self.n_splits, n_jobs=-1, verbose=1, scoring = self.scoring)\n        \n        grid_search.fit(X_train, y_train)\n        \n        return grid_search.best_estimator_\n    \n    def runFinalModel(self):\n        pass\n    \n    def evaluateModel(self):\n        pass","801108fc":"#Instantiate Models class\nmodelObj = Models(models, min_features, max_features, rfecvStep, scoring, n_splits, random_state, shuffle, rfeStatus, rfeCVStatus)","e6130778":"#Run the models and print score\nmodelObj.runModels(X_train, y_train)","a785be25":"#Store CV results and names of various models\nresults = [x[\"cvResults\"] for x in modelObj.models]\nnames = [x[\"modelName\"] for x in modelObj.models]","5a706c24":"#Plot the results\nmodelObj.compareModels(results, names)","269cb02a":"#Set the shortlisted Model\nmodelObj.getShortlistedModel()","869c64c2":"#Get the List of RFE Models based on number of features and the shortlisted model\n\nif ((modelObj.rfeStatus == True) and (modelObj.shortlistedModel[\"rfeStatus\"] == True)):\n   \n    #Get RFE Models\n    modelObj.getRfeModels()\n    \n    #Run RFE Models and get rfeCVResults\n    modelObj.shortlistedModel[\"RFECVResults\"] = modelObj.runRFEModels(X_train, y_train)\n    \n    #Compare RFE Model results\n    modelObj.compareModels(modelObj.shortlistedModel[\"RFECVResults\"], list(range(modelObj.min_features, modelObj.max_features)))\n    \n    #Select the best performing RFE Model and number of features\n    modelObj.getOptimalNumFeatures()\n    \n    #Get the names of selected columns by RFE\n    selectColumns = modelObj.getBestFeatures(X_train, y_train)","399f532d":"#View the shortlisted Model\nmodelObj.shortlistedModel","fdc81921":"#Check if rfeCVStatus == True\nif ((modelObj.rfeCVStatus == True) and (modelObj.shortlistedModel[\"rfeStatus\"] == True)):\n    selectColumns = modelObj.getBestFeaturesCV(X_train, y_train)","398937e9":"#If we are doing Recursive Feature Elimination\nif (((modelObj.rfeStatus == True) or (modelObj.rfeCVStatus == True)) and (modelObj.shortlistedModel[\"rfeStatus\"] == True)):\n    X_train_rfe = X_train[selectColumns]\n    X_test_rfe = X_test[selectColumns]\n    \nelse:\n    X_train_rfe = X_train\n    X_test_rfe = X_test","4f3eeabb":"X_train_rfe","9fc53017":"params = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}","3f7ab2d5":"modelObj.shortlistedModel[\"model\"]","118bb1e2":"finalModel = modelObj.hyperParameterOptimization()","b74b68a0":"#Visualize the Final Model\nfinalModel","e3827eea":"#Fit the final Model\nfinalModel.fit(X_train_rfe, y_train)","98e084f6":"#Predictions on Final Model\ny_test_pred = finalModel.predict(X_test_rfe)","fa3f8f49":"print(accuracy_score(y_test, y_test_pred))\nconfusion_matrix(y_test, y_test_pred)","c28966fe":"cm = confusion_matrix(y_test, y_test_pred, labels=finalModel.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=finalModel.classes_)\ndisp.plot()\n\nplt.show()","910f29fc":"# 15. Feature Selection","838bb5c4":"## 4. Load the required libraries","bb5becf8":"## 8. Dataset Overview - Exploratory Data Analysis","a9ab9215":"--------","a526b23c":"--------","dad293c7":"*The `classification goal` is to `predict` if the `client will subscribe a term deposit` (variable y).*","7c83d921":"### 7.3 Check and remove duplicates","60784d67":"# 14. RFECV","78b0a87c":"## 7. Data manipulation","01736818":"## 2. Goal","3514bd34":"### 9.5 Class Imbalance treatment - SMOTE","802d7997":"<blockquote><b>Only 11% of the customers<\/b> contacted successfully subscribed to a term deposit.<\/blockquote>","f9877fe6":"# Banking Analytics - Predicting Deposit Subscriber","24047710":"### 7.2 Move target variable to the beginning of the dataset","95f106b7":"## 5. Load the dataset","a29b3e86":"-----","56dd2081":"-------","b2e99232":"# 18. Evaluate Final Model","ad127b99":"- XXX","432d088f":"# 11. Models Class","de87e253":"### 10.2 Models List","54291c3b":"### 10.3 RFE\/RFECV Inputs","d7169d01":"### Check for missing values","b6dac36f":"### 11.3 Run multiple models","ae77d880":"## 6. Overview of the dataset","8115e846":"### 9.1 Create train-test split","761fd871":"### 11.2 Instantiate Models class","03719d2e":"# 16. Model Tuning - Hyperparameter optimization","5e5b53af":"------","9d2b572c":"-------","e762d070":"### 10.1 Hyper Parameters for the model","04e442e6":"### 9.4 Extract X and y","178927be":"# 17. Final Model","8247ea55":"### 9.2 Custom Pre-Processing Transformers","ba99f258":"### 9.3 Pre-Processing pipeline","8ebde068":"### 11.4 Compare model performance","dcddb338":"### 10.4 Cross-Validation Inputs","5c063057":"# 1. About the dataset","710965c3":"---------","07e972a8":"## 3. Approach\n- Load the required libraries\n- Load the dataset\n- Overview of the dataset\n- Data Manipulation\n- Exploratory Data Analysis\n- Data Pre-Processing\n- Model Selection\n- Recursive Feature Elimination\n- Select Features\n- Model Tuning - Hyperparameter optimization\n- Final Model\n- Evaluate Final Model","f69220db":"## End of notebook","643453ad":"## Key Findings","2c4aeec3":"### 11.1 Models Class\nThe Models Class provides the following functionalities\n- Initialize various Classification models along with respective hyper-parameters\n- Run and compare model performance\n- Select the best performing model\n- Perform RFE on the best performing model\n- Select the best features\n- Run the final model","e49d4185":"## 9. Data Pre-Processing Pipeline","89da5f08":"**Project Name**: Banking Analytics - Predicting Deposit Subscriber\n\n**Author**: Ankur Dhamija\n\n**Connect on Linkedin**: https:\/\/www.linkedin.com\/in\/ankurdhamija\/","247cea95":"-----","e1c23669":"# 10. Model Inputs","222156dc":"### 7.1 Rename columns","d6ed9e11":"# 12. Shortlist top performing model","3207d296":"-------","d7dbdf3b":"# 13. Recursive Feature Elimination","7393cf43":"---------","42861b66":"The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. \n\nAttribute Information:\n\nInput variables:\n#### bank client data:\n1 - age (numeric)\n\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n\n#### related with the last contact of the current campaign:\n\n8 - contact: contact communication type (categorical: 'cellular','telephone')\n\n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n#### other attributes:\n\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 \nmeans client was not previously contacted)\n\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n#### social and economic context attributes\n\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n\n17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n\n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n\n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n\n#### Output variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n\n","24da0723":"-----"}}