{"cell_type":{"a0560987":"code","00460e2a":"code","4e9745d5":"code","536113a9":"code","13c9b53c":"code","eb6b201d":"code","e07298eb":"code","0fbd1db3":"code","c33062ee":"code","8e2482ac":"code","e10e3e67":"code","b64f159b":"code","62195b4b":"code","3ad0cdeb":"code","f8176e7c":"code","275ae20f":"code","ce263db2":"code","75547a4a":"code","8d7c2fd5":"code","71154c03":"code","efc007a2":"code","0ff0b0c8":"code","97ca2eac":"code","24c3b31a":"code","b6cc5479":"code","be2d708a":"code","efbdfd03":"code","8666d367":"code","81083081":"code","298300cc":"code","135c6a5a":"code","23363cc6":"code","6efeb14e":"code","6453399a":"code","29e40074":"code","d446b8c3":"code","5c0e720e":"code","c26ece1a":"code","93eacf43":"code","464de665":"code","d0d6ec3f":"code","064e3dbd":"code","fe849e67":"code","558d5bef":"markdown","540e33a1":"markdown","b176173e":"markdown","61944ccb":"markdown","49974869":"markdown","07ad03eb":"markdown","6603a19d":"markdown","403f7288":"markdown","4afe61dd":"markdown","6378c088":"markdown","27143b4c":"markdown","289e1fc6":"markdown","74db19d8":"markdown","e562b7c0":"markdown"},"source":{"a0560987":"!pip install transformers==3.0.2","00460e2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e9745d5":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","536113a9":"# from transformers import BertTokenizer, TFBertModel\nfrom transformers import TFAutoModel, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa","13c9b53c":"import transformers\ntransformers.__version__","eb6b201d":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    ","e07298eb":"print('Number of replicas:', strategy.num_replicas_in_sync)","0fbd1db3":"train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")","c33062ee":"train.head()","8e2482ac":"train.premise.values[1]","e10e3e67":"train.hypothesis.values[1]","b64f159b":"train.label.values[1]","62195b4b":"labels, frequencies = np.unique(train.label.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","3ad0cdeb":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","f8176e7c":"model_name = 'joeddav\/xlm-roberta-large-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","275ae20f":"tokenizer(['hey','i'],['hello am i ready', 'k'], padding='longest')","ce263db2":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('<\/s>')\n   return tokenizer.convert_tokens_to_ids(tokens)","75547a4a":"encode_sentence(\"I love machine learning\")","8d7c2fd5":"# def encode(hypotheses, premises, tokenizer):\n    \n#   num_examples = len(hypotheses)\n  \n#   sentence1 = tf.ragged.constant([\n#       encode_sentence(s)\n#       for s in np.array(hypotheses)])\n#   sentence2 = tf.ragged.constant([\n#       encode_sentence(s)\n#        for s in np.array(premises)])\n\n#   cls = [tokenizer.convert_tokens_to_ids(['<s>'])]*sentence1.shape[0]\n#   mid_s = [tokenizer.convert_tokens_to_ids(['<\/s>'])]*sentence1.shape[0]\n#   input_word_ids = tf.concat([cls, sentence1, mid_s, sentence2], axis=-1)\n\n#   input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n# #   type_cls = tf.zeros_like(cls)\n# #   type_s1 = tf.zeros_like(sentence1)\n# #   type_s2 = tf.ones_like(sentence2)\n# #   input_type_ids = tf.concat(\n# #       [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n# #   inputs = {\n# #       'input_word_ids': input_word_ids.to_tensor(default_value=1),\n# #       'input_mask': input_mask,\n# #       'input_type_ids': input_type_ids}\n\n#   inputs = {\n#       'input_word_ids': input_word_ids.to_tensor(default_value=1),\n#       'input_mask': input_mask}\n\n#   return inputs","71154c03":"def encode(hypotheses, premises, tokenizer):\n    \n    input_word_ids = []\n    input_mask = []\n    \n    \n    \n    for h,p in zip(hypotheses, premises):\n        \n        sentences = tokenizer([h],[p])\n        input_word_ids.extend(sentences['input_ids'])\n        input_mask.extend(sentences['attention_mask'])\n        \n    input_word_ids = tf.keras.preprocessing.sequence.pad_sequences(input_word_ids, padding='post', value = 1)\n    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, padding='post', value = 0)\n\n    inputs = {\n      'input_ids': tf.cast(input_word_ids, dtype = tf.float32),\n      'attention_mask': tf.cast(input_mask, dtype = tf.float32)}\n\n    return inputs","efc007a2":"# def encode(hypotheses, premises, tokenizer):\n    \n#     inputs = tokenizer(list(hypotheses), list(premises), padding='longest')\n    \n#     inputs['input_ids'] = np.array(inputs['input_ids'])\n#     inputs['attention_mask'] = np.array(inputs['attention_mask'])\n    \n\n#     return inputs","0ff0b0c8":"train_input = encode(train.premise.values, train.hypothesis.values, tokenizer)","97ca2eac":"# max_len = len(train_input['input_word_ids'][0])\nmax_len = len(train_input['input_ids'][0])","24c3b31a":"max_len","b6cc5479":"# train_input['input_mask'][1]\ntrain_input['attention_mask'][1]","be2d708a":"# train_input['input_word_ids'][1]\ntrain_input['input_ids'][1]","efbdfd03":"max_len = max_len\n\ndef build_model():\n    bert_encoder = TFAutoModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n#     input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n#     embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    embedding = bert_encoder([input_word_ids, input_mask])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    opt = tfa.optimizers.RectifiedAdam(\n    lr=1e-5,\n    total_steps=2000,\n    warmup_proportion=0.9,\n    min_lr=0.9e-5)\n\n#     ranger = tfa.optimizers.Lookahead(opt, sync_period=6, slow_step_size=0.5)\n    \n#     model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n#     model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n#     model.compile(ranger, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","8666d367":"from tensorflow.keras.callbacks import ModelCheckpoint\ncheckpoint_path=\"model.h5\"\ncheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\ncallbacks_list = [checkpoint]","81083081":"with strategy.scope():\n    model = build_model()\n    model.summary()","298300cc":"batch_size=16\nds = tf.data.Dataset.from_tensor_slices((train_input, tf.cast(train.label.values, dtype= tf.float32))).cache().shuffle(1024)","135c6a5a":"ds_size = len(train.label.values)","23363cc6":"ds_size","6efeb14e":"train_size = int(0.80 * ds_size)\ntrain_size","6453399a":"train_ds = ds.take(train_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\ntest_ds = ds.skip(train_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)","29e40074":"model.fit(train_ds, epochs = 4, verbose = 1, validation_data = test_ds, callbacks=callbacks_list)","d446b8c3":"model.load_weights('.\/model.h5')","5c0e720e":"test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntest_input = encode(test.premise.values, test.hypothesis.values, tokenizer)","c26ece1a":"test.head()","93eacf43":"test.shape","464de665":"predictions = [np.argmax(i) for i in model.predict(test_input)]","d0d6ec3f":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","064e3dbd":"submission.head()","fe849e67":"submission.to_csv(\"submission.csv\", index = False)","558d5bef":"## Generating & Submitting Predictions","540e33a1":"Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\nIn this tutorial we'll look at the _Contradictory, My Dear Watson_ competition dataset, build a preliminary model using Tensorflow 2, Keras, and HuggingFace, and prepare a submission file.","b176173e":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","61944ccb":"And now we've created our submission file, which can be submitted to the competition. Good luck!","49974869":"## Creating & Training Model","07ad03eb":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","6603a19d":"We can use the pandas head() function to take a quick look at the training set.","403f7288":"Now, we can incorporate the transformer into a Keras Functional Model. For more information about the Keras Functional API, see: https:\/\/www.tensorflow.org\/guide\/keras\/functional.\n\nThis model was inspired by the model in this notebook: https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert#BERT-and-Its-Implementation-on-this-Competition, which is a wonderful introduction to NLP!","4afe61dd":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data","6378c088":"Let's set up our TPU.","27143b4c":"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set.","289e1fc6":"Let's look at one of the pairs of sentences.","74db19d8":"## Preparing Data for Input","e562b7c0":"## Downloading Data"}}