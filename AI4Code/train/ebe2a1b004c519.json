{"cell_type":{"65dd85b2":"code","0564cfaf":"code","192afa15":"code","d898056d":"code","36dfe2ff":"code","449f9839":"markdown","d8eb6ebb":"markdown","35b9259e":"markdown","64053c16":"markdown","6fee2600":"markdown","f0912e79":"markdown","cba865b2":"markdown","a33e6779":"markdown","3864777c":"markdown","80a467cf":"markdown","76bec084":"markdown"},"source":{"65dd85b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import *\nimport itertools\nfrom numpy import sqrt\nfrom numpy import argmax\n#!pip install RegscorePy\n#!pip install aic\n#import RegscorePy\nfrom RegscorePy import * \nfrom sklearn.linear_model import LogisticRegression\n#Importing tqdm for the progress bar\nfrom tqdm import tnrange, tqdm_notebook\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sn # Heatmap \nimport matplotlib.pyplot as plt # Plots\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nTraining_dataset = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nTraining_dataset = Training_dataset[Training_dataset['Embarked'].notna()]\nTraining_dataset = Training_dataset[Training_dataset['Age'].notna()]\n# Cabin variable is ingnored for analysis due sparcity and anaonimity on series A-F\nTraining_dataset = Training_dataset.drop(['Cabin'], axis=1)","0564cfaf":"#Preprocessing the training dataset\n# List of column names PassengerId  Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked\n# Checking NANs and Treating blanks\nprint(\"Pclass null: \",Training_dataset[\"Pclass\"].isnull().sum())\nprint(\"Age null count: \",Training_dataset[\"Age\"].isnull().sum())\nprint(\"Sibsp null count: \",Training_dataset[\"SibSp\"].isnull().sum())\nprint(\"Parch null count: \",Training_dataset[\"Parch\"].isnull().sum())\nprint(\"Sex null count: \",Training_dataset[\"Sex\"].isnull().sum())\nprint(\"Fare null count: \",Training_dataset[\"Fare\"].isnull().sum())\n#print(\"Cabin null count: \",Training_dataset[\"Cabin\"].isnull().sum())\nprint(\"Embarked null count: \",Training_dataset[\"Embarked\"].isnull().sum())\nprint(\"Number of rows in data: \", Training_dataset.shape[0])\n\n# Creating dummy variables for Pclass\ndummy_container = pd.DataFrame(Training_dataset[\"Pclass\"])\ndummy_container[\"Pclass_1st\"] = 0\ndummy_container[\"Pclass_1st\"][dummy_container[\"Pclass\"] == 1] = 1\ndummy_container[\"Pclass_2nd\"] = 0\ndummy_container[\"Pclass_2nd\"][dummy_container[\"Pclass\"] == 2] = 1\nTraining_dataset[\"Pclass_1st\"] = dummy_container[\"Pclass_1st\"]\nTraining_dataset[\"Pclass_2nd\"] = dummy_container[\"Pclass_2nd\"]\nTraining_dataset = Training_dataset.drop(['Pclass'], axis=1)\n\n#Univariate analysis of Pclass\ndummy_container[\"Survived\"] = 0\ndummy_container[\"Survived\"][Training_dataset[\"Survived\"] == 1] = 1\ndummy_container.hist(column=\"Pclass\", by=\"Survived\")\ndel dummy_container\n\n#Uni and Bivariate analysis of variables\nTraining_dataset.hist(column=\"Age\")\nTraining_dataset.hist(column=\"Age\",by=\"Survived\")\nTraining_dataset.hist(column=\"Sex\",by=\"Survived\") \nTraining_dataset.hist(column=\"Fare\")\nTraining_dataset.hist(column=\"Fare\",by=\"Survived\")\nprint(sum(1 for x in Training_dataset[\"Fare\"] if x>=400)) #Outliers\n# Removing Outliers in Fare\nTraining_dataset = Training_dataset[Training_dataset[\"Fare\"]<= 400]\nTraining_dataset.hist(column=\"SibSp\")\nTraining_dataset.hist(column=\"SibSp\",by=\"Survived\")\nTraining_dataset.hist(column=\"Parch\")\nTraining_dataset.hist(column=\"Parch\",by=\"Survived\") #Ouliers but treatment not needed\n#Percentage proportion of survived in data\nprint((Training_dataset[\"Survived\"].sum())\/len(Training_dataset[\"Survived\"]))\n\n#Writing the required data for model preparation\nTrain_forlogit = pd.DataFrame(Training_dataset[[\"Survived\",\"Pclass_1st\",\"Pclass_2nd\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]])\n#Dummy variable creation and correlation analysis\nTrain_forlogit = pd.get_dummies(Train_forlogit)\nTrain_forlogit = Train_forlogit.drop(['Sex_male'], axis=1)\nTrain_forlogit = Train_forlogit.drop(['Embarked_S'], axis=1)\ncorrMatrix = Train_forlogit.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()\ndel corrMatrix  #Fare vs pclass1 has correlation 0.66 but this has been accepted \n\n########################  MODEL1 LOGISTIC REGRESSION    ####################\n# Spliting the data into train and test for feature selection\nX = Train_forlogit[[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex_female\",\"Embarked_C\",\"Embarked_Q\"]]\ny = Train_forlogit[\"Survived\"]\n\n","192afa15":"X_train, X_test,y_train,y_test = train_test_split(X,y,test_size =0.3, random_state= 0)\n## Logistic regression function\ndef fit_logistic_reg(X1,Y1,X2,Y2):\n    # Fit Logistic regression model on X1,Y1 and Test on data X2,Y2 then output AICs\n    Logreg= LogisticRegression()\n    Logreg.fit(X1,Y1)\n    #Predicting and Calculating Training AIC for the model\n    Y1_predicted =Logreg.predict(X1).tolist()\n    Train_AIC = aic.aic(Y1,Y1_predicted, p)\n    #Scoring and calculating Testing AIC for the model\n    Y2_predicted =Logreg.predict(X2).tolist()\n    Test_AIC = aic.aic(Y2,Y2_predicted, p)\n    Training_Accuracy = accuracy_score(Y1,Y1_predicted)\n    Testing_Accuracy  = accuracy_score(Y2,Y2_predicted)\n    del Y1_predicted\n    del Y2_predicted\n    return Train_AIC,Test_AIC,Training_Accuracy,Testing_Accuracy\n\n##################################   Best Subset Selection ##############################\n##Initializing variables\nk = 9\nTrainAIC_list, TestAIC_list, feature_list = [],[], []\nnumb_features = []\nTr_ac, TT_ac = [],[]\n\n#Looping over k = 1 to k = 9 features in X_train\nfor k in tnrange(1,len(X_train.columns) + 1, desc = 'Loop...'):\n\n    #Looping over all possible combinations: from 9 choose k\n    for combo in itertools.combinations(X_train.columns,k):\n        p = len(combo)\n        tmp_result = fit_logistic_reg(X_train[list(combo)],y_train,X_test[list(combo)],y_test) #Store temp result \n        TrainAIC_list.append(tmp_result[0])                                                         #Append lists\n        TestAIC_list.append(tmp_result[1])\n        feature_list.append(combo)\n        numb_features.append(len(combo))\n        Tr_ac.append(tmp_result[2])\n        TT_ac.append(tmp_result[3])\n\n#Store in DataFrame\nSummary_Subset_Selection = pd.DataFrame({'numb_features': numb_features,'TrainAIC': TrainAIC_list, 'TestAIC':TestAIC_list,'features':feature_list,'TrainingAccuracy':Tr_ac,'TestingAccuracy':TT_ac})\nSummary_Subset_Selection[\"Absolute_Train_AIC\"] = abs(Summary_Subset_Selection[\"TrainAIC\"])\nSummary_Subset_Selection[\"Absolute_Test_AIC\"] = abs(Summary_Subset_Selection[\"TestAIC\"])\nSummary_Subset_Selection = Summary_Subset_Selection.sort_values(['Absolute_Train_AIC','Absolute_Test_AIC'],ascending=[True,True])\n#Data summary has been outputed and was further worked in excel - Work on this file is mentioned as markdown\nSummary_Subset_Selection.to_csv(\"Subset_AIC_Data.csv\",index=False)","d898056d":"#########------------------------------- Cross Validation of Choosen models\n\n# Cross_Validation Logistic Regression Function\ndef CV_logistic_reg(X1,Y1,X2,Y2,I):\n    # fit a model\n    model = LogisticRegression()\n    model.fit(X1[list(Model_Feature_List[I])],Y1)\n    # predict probabilities\n    yhat_Train = model.predict_proba(X1[list(Model_Feature_List[I])])\n    yhat_Test = model.predict_proba(X2[list(Model_Feature_List[I])])\n    # keep probabilities for the survived outcome\n    yhat_Train = yhat_Train[:, 1]\n    yhat_Test = yhat_Test[:, 1]\n    # calculate roc curves\n    fpr, tpr, thresholds = roc_curve(Y2, yhat_Test)\n    # calculate the g-mean for each threshold\n    gmeans = sqrt(tpr * (1-fpr))\n    # locate the index of the largest g-mean\n    ix = argmax(gmeans)\n    ##Create labels from both yhat based on threshold\n    yhat_Train_labelled = [1 if i>=thresholds[ix] else 0 for i in yhat_Train]\n    yhat_Test_labelled = [1 if i>=thresholds[ix] else 0 for i in yhat_Test]\n    #Calculate Training and Testing accuracy\n    Training_Accuracy = accuracy_score(Y1,yhat_Train_labelled)\n    Testing_Accuracy  = accuracy_score(Y2,yhat_Test_labelled)\n    print(\"ROC PLOT FOR MODEL ID\",I,\"Cross_Validation Loop Number \",K+1)\n    # plot the roc curve for the model\n    plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n    plt.plot(fpr, tpr, marker='.', label='Logistic')\n    plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    # show the plot\n    plt.show()\n    # Return the required values\n    return Training_Accuracy,Testing_Accuracy,thresholds[ix],gmeans[ix]\n\n# Initialize the choosen models and required containers\nModel_Feature_List = [[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Fare\",\"Sex_female\",\"Embarked_Q\"],[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Sex_female\",\"SibSp\"],[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Sex_female\"]]\nModel_ID_list,CV_LoopNumber_list,Train_Accuracy_list,Test_Accuracy_list,thresholds,gmeans = [],[],[],[],[],[]\n## Cross Validation of All choosen models\nfor K in range(6):\n    ## Random Splitting of data into training and testing sets\n    X_train, X_test,y_train,y_test = train_test_split(X,y,test_size =0.3)\n    for I in range(3):\n        tmp_results = CV_logistic_reg(X_train,y_train,X_test,y_test,I)\n        CV_LoopNumber_list.append(K+1)\n        Model_ID_list.append(I+1)\n        Train_Accuracy_list.append(tmp_results[0])\n        Test_Accuracy_list.append(tmp_results[1])\n        thresholds.append(tmp_results[2])\n        gmeans.append(tmp_results[3])\n    \n### Update the Summary data frame with function outputs\nCross_Validation_Summary = pd.DataFrame({'Model_ID': Model_ID_list,'CV_LoopNumber': CV_LoopNumber_list, \n                                         'Train_Accuracy':Train_Accuracy_list,'Test_Accuracy':Test_Accuracy_list,\n                                        'Thresholds':thresholds, 'Gmeans':gmeans})\n## Writing this file as output\nCross_Validation_Summary.to_csv(\"Cross_validation_summary.csv\",index=False)\n\n## Understanding the final model summary\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y_train,X_train[list(Model_Feature_List[2])])\nresult=logit_model.fit()\nprint(result.summary2())\n\n\nlogit_model=sm.Logit(y_train,X_train[list(Model_Feature_List[1])])\nresult=logit_model.fit()\nprint(result.summary2())\n\n#Final Model Choosen - [\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Sex_female\",\"SibSp\"]","36dfe2ff":"# Initialize the choosen models and required containers\nModel_Feature_List = [[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Fare\",\"Sex_female\",\"Embarked_Q\"],[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Sex_female\",\"SibSp\"],[\"Pclass_1st\",\"Pclass_2nd\",\"Age\",\"Sex_female\"]]\n\n## Formating the Testing data\nTesting_dataset = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nTesting_dataset = Testing_dataset.drop(['Cabin','Name','Parch','Ticket','Embarked','Fare'], axis=1)\n# Creating dummy variables\ndummy_container = pd.DataFrame(Testing_dataset[\"Pclass\"])\ndummy_container[\"Pclass_1st\"] = 0\ndummy_container[\"Pclass_1st\"][dummy_container[\"Pclass\"] == 1] = 1\ndummy_container[\"Pclass_2nd\"] = 0\ndummy_container[\"Pclass_2nd\"][dummy_container[\"Pclass\"] == 2] = 1\nTesting_dataset[\"Pclass_1st\"] = dummy_container[\"Pclass_1st\"]\nTesting_dataset[\"Pclass_2nd\"] = dummy_container[\"Pclass_2nd\"]\nTesting_dataset = Testing_dataset.drop(['Pclass'], axis=1)\ndel dummy_container\nTesting_dataset = pd.get_dummies(Testing_dataset)\nTesting_dataset = Testing_dataset.drop(['Sex_male'], axis=1)\nTesting_dataset[\"Age\"].fillna(Testing_dataset[\"Age\"].mean(), inplace=True)\n\n#Prepping the Fitting data\nX_Input = X[list(Model_Feature_List[1])]\nTest_Input = Testing_dataset[list(Model_Feature_List[1])]\n#Fitting the final model in old function standards\nFinal_model = LogisticRegression()\nFinal_model.fit(X_Input,y)\n## Threshold = 0.431435\n# Score probabilities\nScoring_Test_data = Final_model.predict_proba(Test_Input)\n# keep probabilities for the survived outcome\nScoring_Test_data = Scoring_Test_data[:, 1]\n# Create labels from both yhat based on threshold\nScoring_Data = [1 if i>=0.431435 else 0 for i in Scoring_Test_data]\n#Adding scoring labels to the Testing Data\nTesting_dataset[\"Survived\"] = Scoring_Data\n\n##Making a submission file\nSubmission1 = pd.DataFrame({'PassengerId':Testing_dataset['PassengerId'],'Survived':Testing_dataset['Survived']})\nSubmission1.to_csv('Titanic_Primary_Submissions.csv',index=False)","449f9839":"**QUESTIONS TO THE READERS** \n1. Looking at the Graphs shown in Step 3. If a radial basis network (RBN) was implemented, do you think it would more or less be a boosted version of the above algorithm?  \n2. If you would have chosen a Simple NN with cross entropy as a cost function, can you comment on the weight decay and your model complexity? \n","d8eb6ebb":"**Step 1: Understand the Algorithm - Two Class Logistic Regression**\n \n* Consider a matrix X describing the independent variable data in which each column corresponds to an independent variable. Also, Y be the matrix describing the dependent variable values for the data points or rows in X.  Let xi and yi represent elements of X and Y respectively at ith index. \n* From the Bayes theorem Posterior Probability of dependent variable given independent variables P(Y|X) is equal to Class conditional probability P(X|Y) times prior P(Y) divided by the marginal P(X) probability. \n* In logistic regression we directly put our assumptions on the Posterior probability unlike other techniques (like Discriminate analysis).\n* Assumptions \u2013\n    1. There are two classes denoted by y = {0,1}. \n    2. Density Function: Density function represents the posterior probability in every case of yi, where yi is an element        at ith index in the matrix Y, is \n\n![image.png](attachment:d2eb6c21-990f-4f52-9c39-0ef9bdc1bcd3.png)\n![image.png](attachment:8f74b2f4-93e4-463b-b3d0-f48b31c223fe.png)\n\n* Thus, this technique uses a logistic function (1 \u2013 Sigmoid function) to wrap linear (\u03b20 + \u03b2T * X) into a space between 0 and 1.","35b9259e":"**FINAL MODEL COMPILATION AND SCORING**","64053c16":"* Short Listed models after this exercise were shown in the below picture\n![image.png](attachment:4235129f-92de-4b39-bbae-55bf8c942b2d.png)\n\n* Final model was chosen based on accuracies and AICs together.\n\n# PLEASE READ THE ARTICLE ALONG WITH THE CODE PROVIDED FOR BETTER UNDERSTANDING\n\n**ALSO TRY THE QUESTIONS AT THE END**","6fee2600":"# PRE-PROCESSING UNI AND BI VARIATE ANALYSIS OF DATA","f0912e79":"**Step 2: Deciding on a Cost Function - Akaike Information Criterion**\n\n* Cost Function (or error function same in this case) is a mathematical way in which you would define error for your model. There are many Cost functions that one can choose from, some of them are as follows \u2013 \n    1. Log Loss or Maximum likelihood function\n    2. Akaike Information Criterion\n    3. Bayesian Information Criterion\n    4. Entropy functions\n    5. Minimum Description length\n* Cost Functions are typically used in regressions to estimate model parameters like \u03b2\u2019s here. By minimizing the cost function, we could reach to the optimal values for \u03b2\u2019s. Akaike Information criterion was chosen instead of a simple log loss since there was a reason to believe in penalizing complex models \u2013 After primary analysis we could see that the sample was very restricted. The cost function can be described as follows\n                                            AIC = -2\/N * LL + 2 * k\/N\n                        Where N is the Number of data points in the training data set\n                                    LL is the simple log likelihood of the model\n                                     K is the number of parameters in the model\n\n* The additional term with k in the above formula would also act as a form of regularization. \n","cba865b2":"This notebook is part of a pilot program aimed at teaching sustainable modelling practices, managing ML and DL models at work. We would love to hear from you about your personal experiences managing ML, DL, Digital Transformation projects. Try this survey out and let us know what problems regarding an ML\/DL or Digital Transformation project bother you the most.\nhttps:\/\/forms.gle\/BArJ2JDke4SeuHZQ8\nLet's start the article.","a33e6779":"**WORK ON BEST SUBSET SELECTION SUMMARY OUTPUT FILE**\n* RELATIVE AIC(c) METRIC = exp(0.5 * (AIC-AIC(lowest))) WAS COMPUTED\n* AIC(c) WEIGHT WAS COMPUTED ACCORDINGLY\n* TOP 3 BEST FIT MODELS WERE SELECTED ON THE BASIS OF ACCURACY AND AIC ANALYSIS \n\n**CROSS VALIDATION 6 FOLD FOR THE 3 BEST FIT MODELS SELECTED**","3864777c":"# MODEL - LOGISTIC REGRESSION\n**FEATURE AND MODEL SELECTION PROCESS FOLLOWED:**\n* BEST SUBSET SELECTION PROCEDURE WAS ADOPTED AS THE SIZE OF (DATA SET)X(NUMBER OF FEATURES) IS SMALL\n* AKAIKE INFORMATION CRITERION WAS ADOPTED IN COMPARING MODELS\n* PIP INSTALL THE PACKAGE AND IMPORT IT TO RUN THE FOLLOWING CELL (IF ERROR)","80a467cf":"#                         LOGISTIC REGRESSION FOR SMALL DATASETS USING AIC, BEST SUBSET SELECTION","76bec084":"**Step 3: Model Selection \u2013 Best Subset Selection**\n\n* There are multiple methods for model selection like Forward- stepwise, Backward-stepwise, etc.  since the data set was small, we used Best Subset selection. In this method, we generally plot the test and train error over model complexity parameter. The least complex model to have optimal test and train error will be selected.\n* We plotted the absolute AICs and Test, train accuracies over model complexity\n\n![image.png](attachment:54b7bd4c-1b7b-4374-8c0e-c5cd4bb59153.png)\n![image.png](attachment:9366ad5b-0f62-41d1-9678-62ab0503e1f1.png)\n\n* Interesting observation \u2013 The pattern of test and train accuracies or AICs show that a model performing with almost similar accuracy and error (in both test and train data) is available at every model complexity level. \n* Simply the model complexity is not a very important differentiator between true and empirical error under our current assumptions stated in step 1.\n\n"}}