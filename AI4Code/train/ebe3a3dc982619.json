{"cell_type":{"af50066a":"code","0f852c0a":"code","eab7f746":"code","9e8af958":"code","7b0b78c9":"code","11696596":"code","8449258b":"code","dd7ee12b":"code","6addb1bf":"code","6e965070":"code","acf8ffc5":"code","b2daea3a":"code","68fbbe7b":"code","6652918e":"code","0fcc6d44":"code","3859be43":"code","e7c7688a":"code","4a04a9fb":"code","dec826c7":"code","74dc3daf":"code","5c961083":"code","7704db92":"code","5df237bf":"code","c461f4b0":"code","3397175c":"code","bf37ade2":"code","5d2e3531":"code","919e4ebb":"code","c2a6f3e8":"markdown","d07437b1":"markdown","6a4c5ab8":"markdown","4b6c2391":"markdown","1dad0969":"markdown","332ca057":"markdown","4a8d7751":"markdown","6d3120c9":"markdown","954b8b43":"markdown","8f1d2143":"markdown","e469ab75":"markdown","13099be6":"markdown","b0583e5e":"markdown","e4d5c4cb":"markdown","2f59ffaf":"markdown","695fe1f0":"markdown","77ed2bb0":"markdown","c59716a3":"markdown","21b7e4e4":"markdown","ba2af53d":"markdown","0bcb414e":"markdown","9d2238c4":"markdown","9e5467d9":"markdown","90b4c7ae":"markdown","162cb92b":"markdown","4b31f258":"markdown","fb0e7254":"markdown","4906048c":"markdown","172d0704":"markdown","0da36dbe":"markdown","94577e1d":"markdown","b2712213":"markdown","9fd7b629":"markdown","0a5ae75b":"markdown","3e0a8ceb":"markdown"},"source":{"af50066a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))","0f852c0a":"import numpy as np\nimport os\nfrom imageio import imread\nfrom skimage.transform import resize\nimport datetime\n\nimport pandas as pd\n\nimport PIL\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.regularizers import l2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom glob import glob\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","eab7f746":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom tensorflow import keras\nimport tensorflow as tf\ntf.random.set_seed(30)","9e8af958":"train_doc = np.random.permutation(open('\/kaggle\/input\/gesturerecognitiondataset\/Project_data\/train.csv').readlines())\nval_doc = np.random.permutation(open('\/kaggle\/input\/gesturerecognitiondataset\/Project_data\/val.csv').readlines())\nbatch_size = 32\nimg_size = 100","7b0b78c9":"def generator(source_path, folder_list, batch_size):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    img_idx = [1,2,4,5,7,8,10,11,13,14,16,17,19,20,22,23,25,26,28,29]\n    while True:\n        t = np.random.permutation(folder_list)\n        num_batches = len(t)\/\/batch_size\n        for batch in range(num_batches): # we iterate over the number of batches\n            batch_data = np.zeros((batch_size,20,img_size,img_size,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'\/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames\/images of a folder to read them in\n                    path = source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]\n                    \n                    image = imread(source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]).astype(np.float32)\n                    # resize the image uniformaly to 100 X 100\n                    new_size = (img_size,img_size)\n                    image = resize(image, new_size)\n                    # Normalize the image using simple Normalization technique for image data by dividing by 255.0                  \n                    batch_data[folder,idx,:,:,0] = (image[:,:,0])\/255\n                    batch_data[folder,idx,:,:,1] = (image[:,:,1])\/255\n                    batch_data[folder,idx,:,:,2] = (image[:,:,2])\/255\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n            \n           # write the code for the remaining data points which are left after full batches  \n        if len(t)%batch_size != 0:\n            new_batch_size =  len(t)%batch_size # we iterate over the number of batches\n            batch_data = np.zeros((new_batch_size,20,img_size,img_size,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_labels = np.zeros((new_batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(new_batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'\/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames\/images of a folder to read them in\n                    path = source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]\n                    \n                    image = imread(source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]).astype(np.float32)\n                    # resize the image as done above\n                    new_size = (img_size,img_size)\n                    image = resize(image, new_size)\n                    # Normalize the image as done above\n                    batch_data[folder,idx,:,:,0] = (image[:,:,0])\/255\n                    batch_data[folder,idx,:,:,1] = (image[:,:,1])\/255\n                    batch_data[folder,idx,:,:,2] = (image[:,:,2])\/255\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n\n","11696596":"def plot(history):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n    axes[0].plot(history.history['loss'])   \n    axes[0].plot(history.history['val_loss'])\n    axes[0].legend(['loss','val_loss'])\n\n    axes[1].plot(history.history['categorical_accuracy'])   \n    axes[1].plot(history.history['val_categorical_accuracy'])\n    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])","8449258b":"curr_dt_time = datetime.datetime.now()\ntrain_path = '\/kaggle\/input\/gesturerecognitiondataset\/Project_data\/train'\nval_path = '\/kaggle\/input\/gesturerecognitiondataset\/Project_data\/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 30\nprint ('# epochs =', num_epochs)","dd7ee12b":"# importing specific libraries for model building\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU,LSTM, TimeDistributed,Dropout, Flatten\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Conv3D, MaxPooling3D, Conv2D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.applications import densenet\n\n# number of classes are 5, as we know that number of gestures to be recognized are 5 in number\nnum_classes = 5","6addb1bf":"model = Sequential([\n    # 1st conv layer\n  layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same', input_shape=(20,img_size,img_size,3)),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n  layers.Dropout(0.25),\n    \n    #2nd conv layer\n  layers.Conv3D(128, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n  layers.Dropout(0.25),\n  \n    #3rd conv layer\n  layers.Conv3D(256, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n  layers.Dropout(0.25),\n    \n    # flatten\n  layers.Flatten(),\n    # 1st dense layer\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(0.25),\n    # output layer with softmax\n  layers.Dense(num_classes,activation='softmax')\n])","6e965070":"optimiser = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","acf8ffc5":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","b2daea3a":"model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '\/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',period = 1)\n\nLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=2,mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\ncallbacks_list = [LR]","68fbbe7b":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences\/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences\/\/batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences\/batch_size)\nelse:\n    validation_steps = (num_val_sequences\/\/batch_size) + 1","6652918e":"# running model fit with 10 epochs for experimental model\nnum_epochs = 10\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator,\n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","0fcc6d44":"#lets plot the model performance graph\nplot(history)","3859be43":"model = Sequential([\n    \n    #1st conv layer\n  layers.Conv3D(64, kernel_size=(3, 3, 3), padding='same', activation='elu',input_shape=(20,img_size,img_size,3)),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n  \n    #2nd conv layer\n  layers.Conv3D(128, kernel_size=(3, 3, 3), padding='same', activation='elu'),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n\n    #3rd conv layer\n  layers.Conv3D(256, kernel_size=(3, 3, 3), padding='same', activation='elu'),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n    \n    #4th conv layer\n  layers.Conv3D(256, kernel_size=(3, 3, 3), padding='same', activation='elu'),\n  layers.MaxPooling3D(pool_size=(2, 2, 2)),\n\n    \n    # flatten\n  layers.Flatten(),\n    # 1st dense layer\n  layers.Dense(256, activation='elu'),\n  layers.Dropout(0.25),\n    \n    # 2nd Dense layer\n  layers.Dense(256, activation='elu'),\n  layers.Dropout(0.25),\n  \n    # output layer with softmax\n  layers.Dense(num_classes,activation='softmax')\n])","e7c7688a":"# using adam optimizer with learning rate 0.001\noptimiser = optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","4a04a9fb":"# Epochs 30\nnum_epochs = 30\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","dec826c7":"# Lets plot the performance graph\nplot(history)","74dc3daf":"model = Sequential()\n\n# 1st CNN layer\nmodel.add(TimeDistributed(Conv2D(32, (3,3),activation='elu'),input_shape=(20,img_size,img_size,3)))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n# 2nd CNN layer\nmodel.add(TimeDistributed(Conv2D(64, (3, 3), activation='elu')))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n# 3rd CNN Layer\nmodel.add(TimeDistributed(Conv2D(128, (3,3), activation='elu')))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n# 4th CNN Layer\nmodel.add(TimeDistributed(Conv2D(256, (3,3),activation='elu')))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n# 5th CNN Layer\nmodel.add(TimeDistributed(Conv2D(256, (3,3),activation='elu')))\nmodel.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n# Flatten\nmodel.add(TimeDistributed(Flatten()))\n\n# 2 LSTM layers with tanh activation\nmodel.add(LSTM(128,activation='tanh', return_sequences=True))\nmodel.add(LSTM(128,activation='tanh', return_sequences=False))\nmodel.add(Dropout(0.25))\n\n# Dense layer\nmodel.add(Dense(128, activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\n# output layer with softmax\nmodel.add(Dense(num_classes,activation='softmax'))","5c961083":"#optimiser = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\noptimiser = optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","7704db92":"# Epochs 30\nnum_epochs=30\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","5df237bf":"# Lets plot the performance graph\nplot(history)","c461f4b0":"densenet = keras.applications.DenseNet169(include_top=False, input_shape=(img_size, img_size, 3), weights='imagenet')\ntrainable = 5\nfor layer in densenet.layers[:-trainable]:\n     layer.trainable = False\nfor layer in densenet.layers[-trainable:]:\n    layer.trainable = True","3397175c":"model = Sequential()\n\n# densenet layer\nmodel.add(TimeDistributed(densenet,input_shape=(20,img_size,img_size,3)))\n\n# Flatten\nmodel.add(TimeDistributed(Flatten()))\n\n# GRU Layer\nmodel.add(GRU(128))\nmodel.add(Dropout(0.25))\n\n# Dense layer\nmodel.add(Dense(128, activation='elu'))\nmodel.add(Dropout(.25))\n\n# final output layer with softmax\nmodel.add(Dense(num_classes,activation='softmax'))","bf37ade2":"#optimiser = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\noptimiser = optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","5d2e3531":"# Epocchs 30\nnum_epochs = 30\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","919e4ebb":"# Lets plot the performance graph\nplot(history)","c2a6f3e8":"#### Analysing the graphs above both loss abd categorical accuracy is quite decent.","d07437b1":"Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.","6a4c5ab8":"#### Checking the size of train and validation dataset","4b6c2391":"### Sample Experimental Conv3D Model\n\nTrying out the first model just to experiment and play around to tweak various hyperparameters like: <br>\nbatch_size<br>\nimage_size<br>\nframes_per_video<br>\nOptimizer to be used<br>\nLearning rate to be used<br>\n\n### Model 1","1dad0969":"#### Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.","332ca057":"###  Conv2D + LSTM model \n\nNow we are buiding the model using Conv2D and then usinf LSTM layers to do the model fit,\nThis is a CNN + LSTM model\n\n### Model 3","4a8d7751":"####  Analysing the above model fit hisory the best model checkpoint is : <br>\n\nEpoch 26\/30\n21\/21 [==============================] - 202s 10s\/step - loss: 0.2392 - categorical_accuracy: 0.9095 - val_loss: 0.8297 - val_categorical_accuracy: 0.7400\n","6d3120c9":"### Defining the callbacks to be used during model fit \n+ ModelCheckpoint callback enables to save the trained model as per our requirement , here we are saving the models at regular intervals so that we can use the .h5 file later\n+ ReduceLROnPlateau callback enables to tweak the learning rate in the middle of the model fit and helps the model in faster convergence and leading the model to the global minima faster","954b8b43":"# Gesture Recognition Project\nIn this project, we are going to build a 3D Conv model that will be able to predict the 5 gestures correctly.\n\n#### Importing required libraries","8f1d2143":"### Thus comparing the model history and graphs of the above 3 models the best performing model is the Transfer Learning using DenseNet169 model with : <br>\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05. Epoch 24\/30 21\/21 [==============================] - 206s 10s\/step - loss: 0.0399 - categorical_accuracy: 0.9955 - val_loss: 0.3262 - val_categorical_accuracy: 0.8800\n                                \n <br> \n","e469ab75":"## Model Building\nHere we make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.","13099be6":"#### this is somewhat decent but not as good as the previous Conv3D model","b0583e5e":"Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.","e4d5c4cb":"#### Looking at the above execution history, we see that the model is not learning properly and we need to tune a lot of parameters\n#### like epochs, optimizers and model architecture also etc","2f59ffaf":"#### The `steps_per_epoch` and `validation_steps` are used by `fit` method to decide the number of next() calls it need to make.","695fe1f0":"### Improved Conv3D model \n\nThe improved Conv3D model has 4 conv3d layers with Maxpooling3D at each step\nwe have 2 Dense layers with dropouts followed by the final output layer with softmax\n\n### Model 2","77ed2bb0":"## Generator\nThis is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.","c59716a3":"### Plot graph","21b7e4e4":"#### Analyzing the graph both val loss and val categorical accuracy show a bit of overfitting","ba2af53d":"#### Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n\n##### Trying the experimental model with SGD optimizer and learning rate 0.001","0bcb414e":"### PLot the graph","9d2238c4":"#### We set the random seed so that the results don't vary drastically.","9e5467d9":"#### Looking at the graph above both loss and categorical accuracy give very good results","90b4c7ae":"### Plot graph","162cb92b":"#### According to the model fit history above the best model checkpoint thus obtained for this model is :<br>\n\nEpoch 25\/30\n21\/21 [==============================] - 201s 10s\/step - loss: 1.1882 - categorical_accuracy: 0.4419 - val_loss: 1.0158 - val_categorical_accuracy: 0.6400","4b31f258":"In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.","fb0e7254":"#### Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.","4906048c":"#### Looking at the model fit history above the best model checkpoint thus obtained is : <br>\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 24\/30\n21\/21 [==============================] - 206s 10s\/step - loss: 0.0399 - categorical_accuracy: 0.9955 - val_loss: 0.3262 - val_categorical_accuracy: 0.8800\n\n","172d0704":"### Transfer Learning using DenseNet169\n\nHere we use the concept of Transfer learning and use a pretrained model and then use a GRU layer for final training.\nAmong the models available in the Keras API (Keras Applications) such as VGG, ResNet, DenseNet, InceptionV3 and MobileNet.\nDenseNet-169 was chosen because despite having a depth of 169 layers it is relatively low in parameters compared to other models, and the architecture handles the vanish gradient problem well\n\n### Model 4","0da36dbe":"#### A utility function to plot the accuracy - loss graph after fitting the various models to analyze and compare their performance ","94577e1d":"+ train loss : 0.39% <br>\n+ train categorical accuracy : 99% <br>\n+ val loss : 3.2% <br>\n+ val categorical accuracy: 88%","b2712213":"After lot of analysis and experimentation we found that the ideal batch size, frames_per_video and image size to be used are :<br>\n##### batch_size = 32<br>\n##### image_size = 100 x 100<br>\n##### frames_per_video = 20<br>","9fd7b629":"Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture.","0a5ae75b":"#### Looking at the above graph we see that both train loss and val loss decreases, but the val categorical accuracy is jumpy and is not reliable","3e0a8ceb":"### Plot graph"}}