{"cell_type":{"b7898266":"code","c08773f8":"code","fd20bef7":"code","5c51a7a1":"code","60628ca3":"code","cc34e97a":"code","71c2a488":"code","fcaeb299":"code","4bf45256":"code","2e435b4c":"code","de9899ac":"code","b6a94122":"code","d7e279ee":"markdown","cfbe3372":"markdown","86c5e226":"markdown","8fbd8586":"markdown","2359c0f8":"markdown","02ea41b8":"markdown","dfba9a15":"markdown","a9d336a4":"markdown","239f9726":"markdown","1cf0f476":"markdown"},"source":{"b7898266":"import os\npath = \"..\/input\/janestreetimputeddata\/\"\nfor path, directories, files in os.walk(path):\n    print(set([(f.split('_', 1)[1][:-4] if 'ive' in f else f.split('_', 1)[0]) for f in files]))","c08773f8":"import pandas as pd\n!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null\n# !pip install datatable\nfrom datatable import fread\nimport gc\nfrom fancyimpute import *\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport janestreet","fd20bef7":"train = fread(\"..\/input\/jane-street-market-prediction\/train.csv\").to_pandas().set_index('ts_id', drop=True)\ntrain.drop(columns=[f\"resp_{i}\" for i in range(1, 5)], inplace=True)\nprint(f'Done loading data. Train shape is {train.shape}')\ntrain.head()","5c51a7a1":"TARGET = 'resp'\nFEATURES = [f\"feature_{i}\" for i in range(1, 130)]\ntrain_pos, train_neg = train.loc[train.feature_0 > 0], train.loc[train.feature_0 < 0]\ndel train\ntrain_pos.drop(columns=[TARGET, 'feature_0'], inplace=True)\ntrain_neg.drop(columns=[TARGET, 'feature_0'], inplace=True)\ngc.collect()","60628ca3":"if False:\n    y_pos = (train_pos.loc[:, TARGET] > 0).astype(int)\n    y_neg = (train_neg.loc[:, TARGET] > 0).astype(int)\n    y_pos.to_csv('positive_target.csv')\n    y_neg.to_csv('negative_target.csv')\n    train_pos[FEATURES].to_csv('positive_original.csv')\n    train_neg[FEATURES].to_csv('negative_original.csv')\n    def impute_missing(train_, method_name='', method=None):\n        for date in tqdm(set(train_.date)):\n            pd.DataFrame(method.fit_transform(train_.loc[train_.date==date, FEATURES]), columns=FEATURES).to_csv(f\"{method_name}.csv\", mode='a', header=False)\n    gc.collect()\n    for k in (1, 3, 5):\n        for train_, sign in zip([train_pos, train_neg], ['positive', 'negative']):\n            impute_missing(train_, method_name=f\"{sign}_knn_{k}\", method=KNN(k=k, verbose=0))\n    train_pos[FEATURES].fillna('ffill', axis=0).fillna('bfill', axis=0).to_csv('positive_forward_backward.csv')\n    train_neg[FEATURES].fillna('ffill', axis=0).fillna('bfill', axis=0).to_csv('negative_forward_backward.csv')\n    for train_, sign in zip([train_pos, train_neg], ['positive', 'negative']):\n        impute_missing(train_, method_name=f\"{sign}_soft_impute\", method=SoftImpute(verbose=0))\n    for train_, sign in zip([train_pos, train_neg], ['positive', 'negative']):\n        impute_missing(train_, method_name=f\"{sign}_iterative_impute\", method=IterativeImputer())","cc34e97a":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\ny_pos = pd.read_csv(path+'positive_target.csv', index_col=0)\ny_neg = pd.read_csv(path+'negative_target.csv', index_col=0)","71c2a488":"nan_neg = pd.read_csv(path+\"nan_neg.csv\", header=None, sep=' ').values.astype(int)\nnan_neg.shape","fcaeb299":"nan_pos = pd.read_csv(path+\"nan_pos.csv\", header=None, sep=' ').values.astype(int)\nnan_pos.shape","4bf45256":"gc.collect()","2e435b4c":"# Split into X and y\nfrom copy import deepcopy as dc\nX_pos = dc(train_pos[FEATURES].values)\nX_neg = dc(train_neg[FEATURES].values)\ndel train_pos, train_neg\ngc.collect()","de9899ac":"file = 'soft_impute'\nX_pos[nan_pos[0], nan_pos[1]] = pd.read_csv(path+f\"positive_{file}.csv\", header=None, sep=' ').values.flatten()\nX_neg[nan_neg[0], nan_neg[1]] = pd.read_csv(path+f\"negative_{file}.csv\", header=None, sep=' ').values.flatten()","b6a94122":"# Train model\n# Parameters from: https:\/\/www.kaggle.com\/hamditarek\/market-prediction-xgboost-with-gpu-fit-in-1min\nmodels = {}\nfor X, y, sign in zip([X_pos, X_neg], [y_pos.values.flatten(), y_neg.values.flatten()], [+1, -1]):\n    print(f\"Fitting model for {sign} feature_0\")\n    model = xgb.XGBClassifier(\n        n_estimators=300,\n        max_depth=11,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.7,\n        random_state=2020,\n        tree_method='gpu_hist'\n    )\n    model.fit(X, y)\n    models[sign] = model\n    print(f'Finished training model for {sign} feature_0')\n\n# Clear memory\ndel X_pos, X_neg, y_pos, y_neg\ngc.collect()\n\n# Create submission\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:    \n    test_weight, test_sign = test_df.iloc[0].weight, test_df.iloc[0].feature_0\n    if test_weight > 0:\n        proba = models[test_sign].predict_proba(test_df[FEATURES].values)[0, 1]\n        sample_prediction_df.action = int(2 * proba)\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","d7e279ee":"# Efficient Ways To Impute Missing Values For Better Prediction","cfbe3372":"This notebook intends to compare efficient ways to impute missing values for better model learning.\n\nIt is essentially based on the [fancyimpute package](https:\/\/github.com\/iskandr\/fancyimpute) and this [XGBoost prediction notebook](https:\/\/www.kaggle.com\/nanomathias\/xgboost-risk-based-weighted-predictions).\n\nThe original XGBoost notebook only tells the model where NaNs are, and does not try to impute them and deal with data as a whole. That is what I will try to do here, hopefully beating the baseline with clever data hole-filling !","86c5e226":"The datatable package is useful for reading huge csv quickly. It reduces CPU walltime from 2 minutes to 10 seconds. \n\nFor more on dealing with big data, see [this wonderful notebook](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets).","8fbd8586":"The 2 main differences are :\n- I use separate models depending on the sign of *feature_0*\n- I use the imputed data from several techniques.","2359c0f8":"# Using the Imputed datasets for prediction","02ea41b8":"In this part I rely mainly on the XGBoost model from [this notebook](https:\/\/www.kaggle.com\/nanomathias\/xgboost-risk-based-weighted-predictions).","dfba9a15":"Right from the beginning, I will split my data between where feature_0 = +1 and where feature_0 = -1. \n\nFor the reasons of that choice, see the excellent illustrations in the 2 EDA notebooks cited precedently.","a9d336a4":"For this part, I provide the code I used to create the datasets. However it is very long to run so I recommend skipping this part and only reading the final files from [the dataset](https:\/\/www.kaggle.com\/louise2001\/janestreetimputeddata) I published.","239f9726":"![luigi-gDlO8tPiKj8-unsplash.jpg](attachment:luigi-gDlO8tPiKj8-unsplash.jpg)\n<span><center>Photo by <a href=\"https:\/\/unsplash.com\/@luigir?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Luigi<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/holes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/center><\/span>","1cf0f476":"# Imputing Data"}}