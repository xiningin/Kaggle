{"cell_type":{"5fb8acef":"code","75448932":"code","66f7f2e0":"code","ac42ee5a":"code","e8a344c9":"code","964338ba":"code","8cdc501a":"code","86364be0":"code","4e832c22":"code","9a8fb4c6":"code","19db4fd2":"code","f66b741a":"code","11d65906":"code","aa739be6":"code","d6962efa":"code","b3c74bb9":"code","a1bbc761":"code","39b02743":"code","0ed89d98":"code","73c61ac4":"code","a2ea32d7":"code","0e96969f":"code","4bbf00f8":"code","bf48177f":"code","b09e2a94":"code","16291cc1":"code","c63ce505":"code","c391779d":"code","9f6cb072":"code","0ba73433":"code","05bab94b":"code","6fffa927":"code","64f0d60f":"code","348871e3":"code","49e10add":"code","1c2d28c6":"code","d61be8f4":"code","fb517407":"code","7528b326":"code","524d2c30":"code","e952821e":"code","6a8a502b":"code","1c053405":"code","8ae763ff":"code","472a2fe2":"code","df1d736f":"code","27ee2a82":"code","d48974d7":"code","2fbcb186":"code","f5679308":"code","46d584f9":"code","7a7ca586":"code","a82a9067":"code","24751e50":"code","dcdadafb":"code","8944d43a":"code","e6b2a3c3":"code","a968f7a1":"code","9f602725":"code","93399b98":"code","3032e5fb":"code","b22b94b8":"code","c61ae7e9":"code","b68a8f7a":"code","334fb88a":"markdown","54ba573d":"markdown","326a0c02":"markdown","6adb5ee6":"markdown","4dedfcaa":"markdown","ca03b731":"markdown","30db1590":"markdown","065db5e9":"markdown","0e7bbd7e":"markdown","c24007cd":"markdown","42233149":"markdown","0a5ad484":"markdown","918bb1b0":"markdown","726524b2":"markdown","f1f3fcc0":"markdown","2de79b22":"markdown","21395d0d":"markdown","780b145f":"markdown","68a38192":"markdown","29e99509":"markdown","e768667c":"markdown","954d28f5":"markdown","02945bfb":"markdown","26d8d626":"markdown","ae527ed1":"markdown","05c5ad44":"markdown","ef74ffbd":"markdown","2e4bb6cc":"markdown","b54cb7e0":"markdown","83858693":"markdown","fd6ade48":"markdown","121ca020":"markdown"},"source":{"5fb8acef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","75448932":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling","66f7f2e0":"train = pd.read_csv('..\/input\/hackathon\/train.csv')\ntest = pd.read_csv('..\/input\/new-test\/test.csv')","ac42ee5a":"train.describe()","e8a344c9":"X_train = train.iloc[:, :-1]\ny_train = train.iloc[:, -1]","964338ba":"X_train = X_train.drop(columns = ['loan_id','financial_institution', 'loan_purpose'])","8cdc501a":"test = test.drop(columns = ['financial_institution', 'loan_purpose'])","86364be0":"X_train['source'].unique()","4e832c22":"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","9a8fb4c6":"def plotCorrelationMatrix(df, graphWidth):\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for give dataframe', fontsize=15)\n    plt.show()","19db4fd2":"def plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","f66b741a":"X_train['first_payment_date'].unique()","11d65906":"test['first_payment_date'].unique()","aa739be6":"test['first_payment_date'] = test['first_payment_date'].map({'Apr-12': '04\/2012', 'Mar-12':'03\/2012', 'May-12': '05\/2012', 'Feb-12':'02\/2012'})","d6962efa":"X_train['origination_date'].unique()","b3c74bb9":"test['origination_date'].unique()","a1bbc761":"test['origination_date'] = test['origination_date'].map({'01\/02\/12': '2012-02-01', '01\/01\/12': '2012-01-01', '01\/03\/12': '2012-03-01'})","39b02743":"from sklearn.preprocessing import LabelEncoder","0ed89d98":"le = LabelEncoder()\nX_train['source'] = le.fit_transform(X_train['source'])\ntest['source'] = le.transform(test['source'])","73c61ac4":"le1 = LabelEncoder()\nX_train['first_payment_date'] = le1.fit_transform(X_train['first_payment_date'])\ntest['first_payment_date'] = le1.transform(test['first_payment_date'])","a2ea32d7":"le2 = LabelEncoder()\nX_train['origination_date'] = le2.fit_transform(X_train['origination_date'])\ntest['origination_date'] = le2.transform(test['origination_date'])","0e96969f":"X_train['to_pay'] = X_train['unpaid_principal_bal'] + (X_train['unpaid_principal_bal']*X_train['loan_term']*X_train['interest_rate'])\/100\ntest['to_pay'] = test['unpaid_principal_bal'] + (test['unpaid_principal_bal']*test['loan_term']*X_train['interest_rate'])\/100","4bbf00f8":"X_train['loan%'] = X_train['loan_to_value']\/(1\/X_train['debt_to_income_ratio'])\/X_train['insurance_percent']\ntest['loan%'] = test['loan_to_value']\/(1\/test['debt_to_income_ratio'])\/test['insurance_percent']","bf48177f":"X_train['interest'] = (X_train['unpaid_principal_bal']*X_train['loan_term']*X_train['interest_rate'])\/36000\ntest['interest'] = (test['unpaid_principal_bal']*test['loan_term']*X_train['interest_rate'])\/36000","b09e2a94":"X_train['comp_interest'] = X_train['unpaid_principal_bal']*((1 + (X_train['interest_rate']\/12))**(X_train['loan_term']\/360))\ntest['comp_interest'] = test['unpaid_principal_bal']*((1 + test['interest_rate']\/12)**(test['loan_term']\/360))","16291cc1":"from xgboost import XGBClassifier","c63ce505":"remove = ['loan%']","c391779d":"from sklearn.preprocessing import StandardScaler","9f6cb072":"X_train['m13'] = y_train","0ba73433":"X_train['m13'].value_counts()","05bab94b":"target = 'm13'\nIDcol = ['loan_id']\nfrom sklearn import model_selection, metrics","6fffa927":"clf = XGBClassifier()","64f0d60f":"predictors = [x for x in X_train.columns if x not in [target]+IDcol + remove]","348871e3":"X_train[predictors] = X_train[predictors].astype(np.float64)","49e10add":"predictors","1c2d28c6":"clf.fit(X_train[predictors], X_train[target])","d61be8f4":"train_pred = clf.predict(X_train[predictors])","fb517407":"n = X_train.shape[0]\np = X_train.shape[1] - 1","7528b326":"from sklearn.metrics import r2_score\nr2 = r2_score(X_train['m13'], train_pred)\nadj_r2 = 1 - ((1-r2)*((n-1)\/(n-p-1)))\nprint(adj_r2)","524d2c30":"test[target] = clf.predict(test[predictors])","e952821e":"IDcol.append(target)","6a8a502b":"submission = pd.DataFrame({x: test[x] for x in IDcol})","1c053405":"submission['m13'].sum()","8ae763ff":"submission.to_csv('alg0.csv', index = False)","472a2fe2":"featimp = pd.Series(clf.feature_importances_,index= predictors).sort_values(ascending=False)\nfeatimp.plot(kind='bar', title='Feature Importances')\nplt.ylabel('Feature Importance Score')","df1d736f":"Count_paid_del = len(X_train[X_train['m13'] == 0])\nCount_unpaid_del = len(X_train[X_train['m13'] == 1])\npercentage_of_paid_del = Count_paid_del\/(Count_paid_del+Count_unpaid_del)\nprint('percentage of paid delequency is',percentage_of_paid_del*100)","27ee2a82":"def undersample(df, times):\n    unpaid_indices = np.array(df[df.m13 == 1].index)\n    paid_indices = np.array(df[df.m13 == 0].index) \n    paid_indices_undersample = np.array(np.random.choice(paid_indices, (times*Count_unpaid_del), replace = False))\n    undersample_data = np.concatenate([unpaid_indices, paid_indices_undersample])\n    undersample_data = df.iloc[undersample_data, :]\n    return(undersample_data)","d48974d7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report","2fbcb186":"predictors = [x for x in X_train.columns if x not in [target]+IDcol + remove]","f5679308":"from sklearn.model_selection import cross_validate \nfrom sklearn.model_selection import learning_curve,GridSearchCV","46d584f9":"import xgboost as xgb","7a7ca586":"def modelfit(alg, train, test, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(train[predictors].values, label=train[target].values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='error', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(train[predictors], train[target],eval_metric='error')\n        \n    #Predict training set:\n    train_predictions = alg.predict(train[predictors])\n    train_predprob = alg.predict_proba(train[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(train[target].values, train_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(train[target], train_predprob))\n    featimp = pd.Series(alg.feature_importances_,index= predictors).sort_values(ascending=False)\n    featimp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","a82a9067":"i = 25\ntrain_undersample = undersample(X_train, i)\ntest_undersample = undersample(test, i)\n","24751e50":"from sklearn.neighbors import KNeighborsClassifier","dcdadafb":"from sklearn.ensemble import RandomForestClassifier","8944d43a":"from sklearn.metrics import accuracy_score\nfrom vecstack import stacking","e6b2a3c3":"predictors","a968f7a1":"col = X_train.columns\nfor col in predictors:\n    print(max(X_train[col]))","9f602725":"sc = StandardScaler()\nX_train[predictors] = sc.fit_transform(X_train[predictors])\ntest[predictors] = sc.transform(test[predictors])","93399b98":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier = Sequential()\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 28))\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))","3032e5fb":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","b22b94b8":"classifier.fit(X_train[predictors], X_train[target], batch_size = 100, epochs = 100)","c61ae7e9":"test.shape","b68a8f7a":"test[target] = classifier.predict(test[predictors])\ntest[target] = (test[target] > 0.19)\nIDcol.append(target)\nsubm = pd.DataFrame({x: test[x] for x in IDcol})\nprint(subm['m13'].sum())\nsubm.to_csv('XGB35S.csv', index = False)","334fb88a":"featimp = pd.Series(model.feature_importances_,index= predictors).sort_values(ascending=False)\nprint(featimp)","54ba573d":"param_test7 = {\n 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n}\ngsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch7.fit(X_train[predictors],X_train[target])\ngsearch7.best_params_, gsearch7.best_score_","326a0c02":"for i in range(23,24):\n    print('the undersample data for {} proportion'.format(i))\n    print()\n    train_undersample = undersample(X_train, i)\n    test_undersample = undersample(test, i)\n    model = XGBClassifier()\n    sub = modelfit(model, train_undersample, test, predictors, target, IDcol)\n    print('m13 sum = ', sub[target].sum())\n    featimp = pd.Series(model.feature_importances_,index= predictors).sort_values(ascending=False)\n    print(max(featimp))","6adb5ee6":"gsearch1.best_params_, gsearch1.best_score_","4dedfcaa":"gsearch5.best_params_, gsearch5.best_score_","ca03b731":"model1 = [\n    XGBClassifier( learning_rate =0.0001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n    RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt', max_depth = 90, min_samples_leaf = 5, min_samples_split = 9),\n    XGBClassifier( learning_rate =0.001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\n]","30db1590":"remove = ['number_of_borrowers', 'm1', 'm2', 'co-borrower_credit_score',\n          'source', 'insurance_type', 'first_payment_date',\n          'origination_date', 'm13', 'loan%', 'loan_term',\n          'insurance_percent', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8','comp_interest', 'interest']","065db5e9":"plotCorrelationMatrix(X_train, 8)","0e7bbd7e":"plotCorrelationMatrix(X_train[predictors], 8)","c24007cd":"model = XGBClassifier( learning_rate =0.001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\nmodel.fit(S_train2, train_undersample[target])\ntest[target] = model.predict(S_test2)\nIDcol.append(target)\nsubm = pd.DataFrame({x: test[x] for x in IDcol})\nprint(subm['m13'].sum())\nsubm.to_csv('XGB35S.csv', index = False)","42233149":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=3,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb1, X_train, test, predictors, target)","0a5ad484":"S_train2, S_test2 = stacking(model2, S_train1, train_undersample[target], S_test1,\n                           regression = False,\n                           mode = 'oof_pred_bag',\n                           needs_proba = False,\n                           save_dir = None,\n                           metric = accuracy_score,\n                           n_folds = 4,\n                           stratified = True, \n                           shuffle = True,\n                           random_state = 0,\n                           verbose = 2)","918bb1b0":"models = [classifier(batch_size = 1000, epochs = 100),\n    XGBClassifier( learning_rate =0.001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n    KNeighborsClassifier(n_neighbors = 8, leaf_size = 25, algorithm = 'brute', weights = 'distance'),\n    RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt', max_depth = 90, min_samples_leaf = 5, min_samples_split = 9),\n    XGBClassifier( learning_rate =0.001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\n]","726524b2":"param_test5 = {\n 'subsample':[i\/100.0 for i in range(80,100,5)],\n 'colsample_bytree':[i\/100.0 for i in range(80,100,5)]\n}\ngsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.9,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(X_train[predictors],X_train[target])","f1f3fcc0":"clf = KNeighborsClassifier(n_neighbors = 5)\nparam_test1 = {\n    'weights': ['uniform', 'distance']\n}\ngsearch1 = GridSearchCV(estimator = KNeighborsClassifier(n_neighbors = 8, leaf_size = 25, algorithm = 'brute'), \nparam_grid = param_test1, scoring='roc_auc',n_jobs =-1,iid=False, cv=5)\ngsearch1.fit(X_train[predictors],X_train[target])","2de79b22":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb1, X_train, test, predictors, target)","21395d0d":"param_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\nmin_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\nobjective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \nparam_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(X_train[predictors],X_train[target])\n","780b145f":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train[predictors],X_train[target])\ngsearch3.best_params_, gsearch3.best_score_","68a38192":"param_test1 = {\n    'n_estimators': [300, 500, 1000]\n}\ngsearch1 = GridSearchCV(estimator = RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt', max_depth = 90, min_samples_leaf = 5, min_samples_split = 9), \nparam_grid = param_test1, scoring='roc_auc',n_jobs =-1,iid=False, cv=5)\ngsearch1.fit(X_train[predictors],X_train[target])","29e99509":"gsearch1.best_params_, gsearch1.best_score_","e768667c":"plotPerColumnDistribution(X_train, 25, 1)","954d28f5":"model2 = [\n    XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n    XGBClassifier( learning_rate =0.01, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n    XGBClassifier( learning_rate =0.001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n    XGBClassifier( learning_rate =0.0001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n    XGBClassifier( learning_rate =0.00001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\n]","02945bfb":"param_test2 = {\n 'max_depth':[3,4],\n 'min_child_weight':[1,2,3]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2.fit(X_train[predictors],X_train[target])","26d8d626":"#undersampling due to imbalanced dataset","ae527ed1":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n min_child_weight=1, gamma=0, subsample=0.9, colsample_bytree=0.9,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \nparam_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(X_train[predictors],X_train[target])\ngsearch4.best_params_, gsearch4.best_score_","05c5ad44":"i = 35\ntrain_undersample = undersample(X_train, i)\ntest_undersample = undersample(test, i)\nmodel = XGBClassifier(n_estimators = 100, early_stopping_rounds = 20)\nmodel.fit(train_undersample[predictors], train_undersample[target])\ntest[target] = model.predict(test[predictors])\nIDcol.append(target)\nsubm = pd.DataFrame({x: test[x] for x in IDcol})\nsubm.to_csv('XGB23.csv', index = False)","ef74ffbd":"i = 35\ntrain_undersample = undersample(X_train, i)\ntest_undersample = undersample(test, i)\nmodel = XGBClassifier( learning_rate =0.1, n_estimators=100, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\nmodel.fit(train_undersample[predictors], train_undersample[target])\ntest[target] = model.predict(test[predictors])\nIDcol.append(target)\nsubm = pd.DataFrame({x: test[x] for x in IDcol})\nprint(subm['m13'].sum())\nsubm.to_csv('XGB35.csv', index = False)","2e4bb6cc":"X_train.shape[0]","b54cb7e0":"S_train1, S_test1 = stacking(model1, train_undersample[predictors], train_undersample[target], test[predictors],\n                           regression = False,\n                           mode = 'oof_pred_bag',\n                           needs_proba = False,\n                           save_dir = None,\n                           metric = accuracy_score,\n                           n_folds = 4,\n                           stratified = True, \n                           shuffle = True,\n                           random_state = 0,\n                           verbose = 2)","83858693":"gsearch1.best_params_, gsearch1.best_score_","fd6ade48":"gsearch2.best_params_, gsearch2.best_score_","121ca020":"model = XGBClassifier( learning_rate =0.001, n_estimators=1000, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.9, colsample_bytree=0.85, reg_alpha = 0.01,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\nmodel.fit(train_undersample[predictors], train_undersample[target])\ntest[target] = model.predict(test[predictors])\nIDcol.append(target)\nsubm = pd.DataFrame({x: test[x] for x in IDcol})\nprint(subm['m13'].sum())\nsubm.to_csv('XGB35.csv', index = False)"}}