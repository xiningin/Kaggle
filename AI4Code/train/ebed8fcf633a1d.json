{"cell_type":{"84dfb531":"code","bb5c74e0":"code","da9e971f":"code","83c97373":"code","9496eeb7":"code","cc37d12a":"code","e87dd822":"code","87991edf":"code","90775115":"code","626c3390":"code","9994478a":"code","25fcc6a5":"code","fde9840e":"code","1871da0a":"code","fd740db4":"code","abd6cead":"code","1ce50053":"code","c38e2afe":"code","b5868139":"code","2ca214e1":"code","fbc77226":"markdown","9c11fdc8":"markdown","06cddada":"markdown","e4a39424":"markdown","896fe294":"markdown","e27939a1":"markdown","4eba47c2":"markdown"},"source":{"84dfb531":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sn\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bb5c74e0":"df_05_07 = pd.read_csv(\"..\/input\/accidents_2005_to_2007.csv\")\ndf_09_11 = pd.read_csv(\"..\/input\/accidents_2009_to_2011.csv\")\ndf_12_14 = pd.read_csv(\"..\/input\/accidents_2012_to_2014.csv\")\n\ndf = pd.concat([df_05_07, df_09_11, df_12_14], axis=0)","da9e971f":"print(\"Number of Duplicates: \",(df.duplicated().sum()))\ndf[\"Dup_Flag\"] = df.duplicated()\ndf = df.loc[df[\"Dup_Flag\"]==False]\ndf.drop((\"Dup_Flag\"), axis=1, inplace=True)","83c97373":"df.describe()","9496eeb7":"df.info()","cc37d12a":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame(data=percent_missing.values, index=percent_missing.index)\nmissing_value_df.columns = [\"Missing Value %\"]\n","e87dd822":"missing_value_df","87991edf":"df[\"Accident_Severity\"].value_counts().plot(\"barh\", alpha=0.75, figsize=(6,4), title=\"Distribution of Accident Severity\")","90775115":"Day_of_Week_D = df[\"Day_of_Week\"].value_counts().sort_index()\/len(df)*100\nDay_of_Week_D.index = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\nplt.bar(Day_of_Week_D.index, Day_of_Week_D.values, alpha=0.75, color=\"orange\")\nplt.title(\"Distribution of Accidents by Day of the Week\")\nfor i, val in enumerate(Day_of_Week_D.values):\n    plt.text(i-0.3, val-1,  str( \"{:.{}f}\".format( val, 1 )), color='blue', fontweight='bold')\n    ","626c3390":"Acc_By_Date = pd.DataFrame()\nAcc_By_Date[\"Accident_Severity\"] = df[\"Accident_Severity\"]\nAcc_By_Date[\"Date\"] = pd.to_datetime(df[\"Date\"])\nAcc_By_Date[\"Hour\"] = df.Time.str.slice(0,2)\nAcc_By_Date[\"Year\"] = df[\"Year\"]\nAcc_By_Date[\"Day_of_Week\"] = df[\"Day_of_Week\"]","9994478a":"Total_Acc = Acc_By_Date[\"Date\"].value_counts()\n\nplt.figure(1, figsize=(7,7))\nsn.distplot(Total_Acc.values)\nplt.title(\"Distribution of Total Accidents Per Day\")\nplt.text(100, 0.004,'Mean:'+ str( \"{:.{}f}\".format( Total_Acc.mean(), 1 )), fontsize=12)\nplt.text(100, 0.0038,'Median:'+ str( \"{:.{}f}\".format( Total_Acc.median(), 1 )), fontsize=12)\nplt.text(100, 0.0036,'STD:'+ str( \"{:.{}f}\".format( Total_Acc.std(), 1 )), fontsize=12)\nplt.show()\n","25fcc6a5":"fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(14,14))\ncount = 0\n\nfor i, col in enumerate(Acc_By_Date[\"Year\"].unique()):\n\n    Total_Acc_by_Year = Acc_By_Date[\"Date\"].loc[Acc_By_Date[\"Year\"]==col].value_counts()\n    \n    ax[i\/\/3,count].set_ylim([0.0,0.007])\n    sn.distplot(Total_Acc_by_Year.values,  ax=ax[i\/\/3,count])\n    ax[i\/\/3,count].title.set_text(\"Year:\"+str(col))\n    ax[i\/\/3,count].text(100, 0.006,'Mean:'+ str( \"{:.{}f}\".format( Total_Acc_by_Year.mean(), 1 )), fontsize=12)\n    ax[i\/\/3,count].text(100, 0.0055,'Median:'+ str( \"{:.{}f}\".format( Total_Acc_by_Year.median(), 1 )), fontsize=12)\n    ax[i\/\/3,count].text(100, 0.0050,'STD:'+ str( \"{:.{}f}\".format( Total_Acc_by_Year.std(), 1 )), fontsize=12)\n    \n    count = count + 1\n    if count == 3: count = 0\nplt.show()","fde9840e":"Acc_By_Hour = Acc_By_Date[\"Hour\"].value_counts().sort_index()\n\nAcc_By_Hour = (Acc_By_Hour\/Acc_By_Hour.sum())*100\n\nplt.figure(1, figsize=(8,5))\nplt.bar(Acc_By_Hour.index, Acc_By_Hour.values, alpha=0.75)\nplt.title(\"Distribution of Accidents by Hour\")\nplt.ylabel(\"%\")\nplt.xlabel(\"Hour of the Day (24hr)\")\n","1871da0a":"Day_Of_Week_Dict = {1:\"Sun\", 2:\"Mon\", 3:\"Tue\", 4:\"Wed\", 5:\"Thu\", 6:\"Fri\", 7:\"Sat\"}\n\nfig, ax = plt.subplots(nrows=4, ncols=2, figsize=(16,16))\ncount = 0\n\nfor i, col in enumerate(Acc_By_Date[\"Day_of_Week\"].sort_values().unique()):\n    Acc_By_Hour_By_Day = Acc_By_Date[\"Hour\"].loc[Acc_By_Date[\"Day_of_Week\"]==col].value_counts().sort_index()\n    Acc_By_Hour_By_Day = (Acc_By_Hour_By_Day\/Acc_By_Hour_By_Day.sum())*100\n    \n    ax[i\/\/2, count].bar(Acc_By_Hour_By_Day.index, Acc_By_Hour_By_Day.values, alpha=0.75)\n    ax[i\/\/2, count].title.set_text(\"Distribution of Accidents by Hour for:\" + str(Day_Of_Week_Dict.get(col)))\n    ax[i\/\/2, count].set_ylim([0,11])\n    ax[i\/\/2, count].set_ylabel(\"%\")\n\n    count = count + 1\n    if count == 2: count = 0\n        \nax[3,1].set_yticks([])\nax[3,1].set_xticks([])\nplt.show()\n","fd740db4":"#Number_of_Casualties\nplt.figure(1, figsize=(7,7))\nsn.distplot(df[\"Number_of_Vehicles\"].values)\nplt.title(\"Distribution of Number of Vehicles involved\")\nplt.show()\n","abd6cead":"#Number_of_Casualties\nplt.figure(1, figsize=(7,7))\nsn.distplot(df[\"Number_of_Casualties\"].values)\nplt.title(\"Distribution of Number of Casualties involved\")\nplt.show()\n","1ce50053":"plt.figure(1, figsize=(7,7))\nsn.scatterplot(df[\"Number_of_Casualties\"].values, df[\"Number_of_Vehicles\"].values)\nplt.title(\"Vehicles vs Casualties\")\nplt.xlabel(\"Number_of_Casualties\")\nplt.ylabel(\"Number_of_Vehicles\")\nplt.show","c38e2afe":"Object_Col = df.drop([\"Accident_Index\", \"Date\", \"Time\"], axis=1).select_dtypes(\"object\").columns\n\nObject_Col = list(Object_Col)\n\nObject_Col.append(\"Speed_limit\")\nObject_Col.append(\"Local_Authority_(District)\")\nObject_Col.append(\"1st_Road_Class\")\nObject_Col.append(\"2nd_Road_Class\")\nObject_Col.append(\"Police_Force\")\nObject_Col.append(\"Urban_or_Rural_Area\")\n","b5868139":"for i, col in enumerate(Object_Col):\n\n    Agg = df[col].value_counts().sort_values()\n    \n    Agg = (Agg\/Agg.sum())*100\n    \n    if len(Agg) < 20:\n    \n        plt.figure(i, figsize=(7,5))\n        plt.barh(Agg.index.astype(str), Agg.values, alpha=0.75,  color=\"orange\")\n        plt.title(str(col))\n        plt.rc('font', size=12)    \n        plt.show()\n        \n    else:\n        \n        Agg = Agg.head(20)\n        plt.figure(i, figsize=(7,5))\n        plt.barh(Agg.index.astype(str), Agg.values, alpha=0.75, color=\"orange\")\n        plt.title(str(col) + \" top 20\")\n        plt.rc('font', size=12)    \n        plt.show()","2ca214e1":"Acc_Sev_Dict = {1:\"Fatal\",\n                2:\"Serious\",\n                3:\"Slight\"}\n\nfor j, col in enumerate(Object_Col):\n    \n    plt.figure(j, figsize=(7,5))\n    \n    if len(df[col].unique()) >20:\n        pass\n    \n    else:\n        \n        for i in range(1,4):\n            DV = df[col].loc[df[\"Accident_Severity\"]==i].value_counts()\n            DV = (DV\/DV.sum())*100\n            plt.barh(DV.index.astype(str), DV.values, alpha=0.40, label=Acc_Sev_Dict[i])\n    \n        plt.legend()\n        plt.title(col)\n        plt.show()","fbc77226":"**Numerical Data**","9c11fdc8":"What is the distribution of a potential target variable","06cddada":"% of missing values by feature","e4a39424":"Lets now overlay Accident_Severity onto these features:","896fe294":"**Categorical Data**\nBar plots for fields with data type object and fields who datatype is interger but still categorical data:\n","e27939a1":"Distribution of Number of Accidents by day (24hr period)","4eba47c2":"**Time**\nBelow I will attempt to look at the number of accidents by the time features of this data set such as Day of the Week and Date"}}