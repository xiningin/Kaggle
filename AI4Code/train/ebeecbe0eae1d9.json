{"cell_type":{"0861c92a":"code","7511bcdf":"code","212de54c":"code","d860cc3e":"code","42e14d94":"code","b6365e1a":"code","523ca459":"code","4abf74b8":"code","7f9d5537":"code","0e337b80":"code","663e5684":"code","32a9fd79":"code","1a326a9e":"code","92000182":"code","3ee28011":"code","abfff13f":"code","4ad61daf":"code","ba305f8c":"code","28e9dcba":"code","1f5740b9":"code","9f5c0976":"code","2aba7a5d":"code","839422a5":"code","7ce6e42c":"code","4fb5c045":"code","6aafad3e":"code","0b72393b":"code","1ef13f23":"code","5e11300f":"code","d177bdb3":"code","a7127856":"code","99201e74":"code","a5b689aa":"code","8d6d631c":"markdown","50a228da":"markdown","21744317":"markdown","042ced8d":"markdown","74b03ba1":"markdown","c6725ba9":"markdown","c04ccf62":"markdown","da47aebb":"markdown","30f8547a":"markdown"},"source":{"0861c92a":"ls","7511bcdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13'\nmetadata_path = f'{root_path}\/all_sources_metadata_2020-03-13.csv'\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","212de54c":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","d860cc3e":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","42e14d94":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            return data\n#             data = data + \"<br>\" + words[i]\n#             total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","b6365e1a":"from ast import literal_eval\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 50:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:50]\n        summary = get_breaks(' '.join(info), 50)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 50)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n#     try:\n#         # if more than one author\n#         authors = literal_eval(meta_data['authors'].values[0])\n#         if len(authors) > 2:\n#             # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n#             dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n#         else:\n#             # authors will fit in plot\n#             dict_['authors'].append(\". \".join(authors))\n#     except Exception as e:\n#         # if only one author - or Null valie\n#         dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n#     try:\n#         title = get_breaks(meta_data['title'].values[0], 40)\n#         dict_['title'].append(title)\n#     # if title was not provided\n#     except Exception as e:\n#         dict_['title'].append(meta_data['title'].values[0])\n    \n#     # add the journal information\n#     dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'abstract_summary'])\ndf_covid.head()","523ca459":"df_covid['body_text'][6]","4abf74b8":"list_covid = df_covid['abstract'].tolist()","7f9d5537":"# list_covid[0]","0e337b80":"cd ..","663e5684":"mkdir fastText","32a9fd79":"!curl -Lo fastText\/crawl-300d-2M.vec.zip https:\/\/dl.fbaipublicfiles.com\/fasttext\/vectors-english\/crawl-300d-2M.vec.zip","1a326a9e":"mkdir encoder","92000182":"!unzip fastText\/crawl-300d-2M.vec.zip -d fastText\/","3ee28011":"!curl -Lo encoder\/infersent2.pkl https:\/\/dl.fbaipublicfiles.com\/infersent\/infersent2.pkl","abfff13f":"!git clone https:\/\/github.com\/facebookresearch\/InferSent","4ad61daf":"import numpy as np\nimport torch\nfrom InferSent.models import InferSent\nimport torch\nmodel_version = 2\nMODEL_PATH = \"encoder\/infersent%s.pkl\" % model_version\nparams_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\nmodel = InferSent(params_model)\nmodel.load_state_dict(torch.load(MODEL_PATH))\n# Keep it on CPU or put it on GPU\nuse_cuda = False\nmodel = model.cuda() if use_cuda else model\n# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\nW2V_PATH = 'GloVe\/glove.840B.300d.txt' if model_version == 1 else 'fastText\/crawl-300d-2M.vec'\nmodel.set_w2v_path(W2V_PATH)\n# Load embeddings of K most frequent words\nmodel.build_vocab_k_words(K=100000)\n# Load some sentences\n","ba305f8c":"subset_list_covid = list_covid\ntargets = []\nnew_subset_list_covid = []\nembeddings_summed = []\n\nfrom nltk.tokenize import sent_tokenize\nfor idx,each in enumerate(subset_list_covid):\n    if (len(each) != 0):\n        each = each[:int(len(each)\/2)] #reduce workload..\n    for sent in sent_tokenize(each):\n        new_subset_list_covid.append(sent) \n    if len(new_subset_list_covid) == 0 : new_subset_list_covid.append('empty') \n    embeddings = model.encode(new_subset_list_covid, bsize=128, tokenize=False, verbose=True)\n    embeddings = [sum(x)\/len(x) for x in zip(*embeddings)]\n    embeddings_summed.append(embeddings)\n    new_subset_list_covid = []\n    targets.append(idx)","28e9dcba":"from nltk.tokenize import sent_tokenize\nran = ' '. join(x) for \nsent_tokenize()","1f5740b9":"len(embeddings_summed)","9f5c0976":"def cosine(u, v):\n    return np.dot(u, v) \/ (np.linalg.norm(u) * np.linalg.norm(v))","2aba7a5d":"!pip install umap","839422a5":"import umap.umap_ as umap\nreducer = umap.UMAP(n_neighbors = 5)\numap_embedding = reducer.fit_transform(embeddings_summed)\numap_embedding.shape","7ce6e42c":"print (umap_embedding)","4fb5c045":"# !pip install seaborn","6aafad3e":"import matplotlib.pyplot as plt\nimport seaborn as sns","0b72393b":"plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1])\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection COVID-19 Dataset -using infersent - first 100 documents', fontsize=24);","1ef13f23":"!pip install hdbscan","5e11300f":"import hdbscan\nclusterer = hdbscan.HDBSCAN(min_cluster_size=2).fit(umap_embedding)\ncolor_palette = sns.color_palette('deep', 8)\ncluster_colors = [color_palette[x] if x >= 0\n                  else (0.5, 0.5, 0.5)\n                  for x in clusterer.labels_]\ncluster_member_colors = [sns.desaturate(x, p) for x, p in\n                         zip(cluster_colors, clusterer.probabilities_)]\nplt.scatter(*umap_embedding.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.8)","d177bdb3":"subset_list_covid = list_covid[:4]\n# subset_list_covid.append(irrelevant_article)\nnew_subset_list_covid = []\ntargets = []\nembeddings_summed2 = []\n\nfrom nltk.tokenize import sent_tokenize\nfor idx,each in enumerate(subset_list_covid):\n    for sent in sent_tokenize(each):\n        new_subset_list_covid.append(sent)\n    embeddings = model.encode(new_subset_list_covid, bsize=128, tokenize=False, verbose=True)\n    embeddings = [sum(x)\/len(x) for x in zip(*embeddings)]\n    embeddings_summed2.append(embeddings)\n    new_subset_list_covid = []\n    targets.append(idx)","a7127856":"list_covid[1]","99201e74":"list_covid[4]","a5b689aa":"cosine(umap_embedding[1],umap_embedding[4])","8d6d631c":"For clustering algorithm, i chose hdbscan.. it's one of the better ones..","50a228da":"To compose sentence representations into a paragraph representation, we simply sum the InferSent representations of all the sentences in the paragraph. This approach is inspired by the sum of word representations as composition function for forming sentence representations (Iyyer et al., 2015).","21744317":"# Function for cosine distance measure","042ced8d":"# UMAP - Reduce Dimension from 4096 to 2 for visualization","74b03ba1":"# Download relevant modules for InferSent","c6725ba9":"# Get all the paragraphs from abstract as a list to process (Body text too much to process..)","c04ccf62":"# Extract relevant tools needed for Infersent","da47aebb":"# COVID-19 - A clustering approach (InferSent semantics embedding + Umap dimension reductioning + HDBScan clustering)\n## 1. Semantics understanding\n\n* InferSent semantics embedding (4096 features) - this is chosen according to http:\/\/hunterheidenreich.com\/blog\/comparing-sentence-embeddings\/ as the top scorer model for sentence semantics relatedness (SICK-R)\n* Conversion of sentence embedding to paragraph is simply the average of the vectors (Iyyer et al., 2015). - https:\/\/towardsdatascience.com\/sentence-embedding-3053db22ea77\n\n## 2. Dimension Reduction\n\n* Employed the approach of UMAP for dimension reduction from 4096 to 2D, arguably better than PCA\n\n## 3. Clustering\n\n* Employed the approach of hdbScan as a clustering algo\n\n**Credits**:\nThe data preprocessing codes were taken from https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering, to convert the JSON files into dataframes*","30f8547a":"# Clustering using hdbscan"}}