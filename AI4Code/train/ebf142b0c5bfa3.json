{"cell_type":{"247bbe4c":"code","e1bfe45b":"code","e605c7fb":"code","abd74b83":"code","5fa1ee83":"code","acf42bd9":"code","6c85331b":"code","e37e3fa9":"code","da610cac":"code","536f4b74":"code","8efbd101":"code","6a14e2a1":"code","2ddfcb27":"code","87998a3c":"code","de33bdf3":"code","45fb5298":"code","e6d7f99b":"code","fae851c3":"code","8fbf3bb7":"code","2622a05c":"code","feb038a1":"code","9580a7fc":"code","21439522":"code","66cec4ba":"code","af75702f":"code","4e0699d8":"code","a7657444":"code","35716c9f":"code","708729c6":"code","b1616802":"code","c2aaf5de":"code","c60e3707":"code","70d22137":"code","adb71b48":"code","b384b054":"code","db33fa3f":"code","8f8998b5":"code","6e81777e":"code","12d5a477":"code","af74d10c":"code","42c28744":"code","1480b8d8":"code","737bc4fe":"code","ae4ecddd":"code","17feb53b":"code","ac0ca286":"code","e5aa9825":"code","9cd5811f":"code","fd0246c7":"code","4f3a637b":"code","8f039248":"code","02425eb1":"code","dc243ae1":"code","024c83d5":"code","2cd2bb36":"code","a4e745bf":"code","8b23245b":"code","08a1dc77":"code","8f623dec":"code","a57042c9":"code","12819941":"code","48d3fa7a":"code","8deecce0":"code","8d20dea2":"code","b4a16c4e":"code","e79fc835":"markdown","c724b47c":"markdown","8ba009f1":"markdown","36525916":"markdown","720ce26e":"markdown","8e10ec03":"markdown","4da6a6be":"markdown","14eb6a69":"markdown","e6ebc79b":"markdown","403f41ef":"markdown","1c27544a":"markdown","9acd2393":"markdown","b8d37cc4":"markdown","214f2640":"markdown","da0435d0":"markdown","b7a139a4":"markdown","e7d49105":"markdown","3d0dfd91":"markdown","299defe1":"markdown","096fe1eb":"markdown","3b364ee2":"markdown","a40e53e6":"markdown","eb08211f":"markdown","d5058f53":"markdown","56381391":"markdown","cd000ef2":"markdown","aa691ebb":"markdown","14d82cf1":"markdown","41206ee1":"markdown","72f5b27d":"markdown","79ecd57b":"markdown"},"source":{"247bbe4c":"from functools import partial\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom category_encoders import TargetEncoder\n\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\n\n\npd.options.display.max_rows=200\npd.set_option('mode.chained_assignment', None)\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\nsimplefilter(\"ignore\", category=RuntimeWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e1bfe45b":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","e605c7fb":"y_train = train.Survived.copy()\ntrain = train.drop('Survived', axis=1)\nX_test = test","abd74b83":"train.info()","5fa1ee83":"# if you go through the names, you will find out that every name has a designation associated, these designations can\n# provide us with an extra categorical feature which we can encode using binary encoding or the mean target encoding!!!\n# so dont just drop name column!!\n\ndef name_labeling(df):\n    for i in ['Mr.', 'Mrs.', 'Miss', 'Master', 'Army', 'Revered\/Important', 'rare', 'Doctor']:\n        if i == 'Army':\n            df.Name[df.Name.str.contains(pat='(Major. |Col. |Capt. )', regex=True)] = 'Army'\n\n        elif i == 'Revered\/Important':\n            df.Name[df.Name.str.contains(pat='(Rev. |Countess. |Jonkheer. |Sir. |Lady. )', regex=True)] = 'Revered\/Important'\n\n        elif i == 'rare':\n            df.Name[df.Name.str.contains(pat='(Mme. |Ms. |Mlle. |Don. |Dona. )', regex=True)] = 'rare'\n\n        elif i == 'Doctor':\n            df.Name[df.Name.str.contains(pat='Dr. ')] = 'doctor'\n\n        else:\n            df.Name[df.Name.str.contains(pat=f'{i}', regex=False)] = i\n    return df","acf42bd9":"# char = X_train.Ticket[X_train.Ticket.str.contains(pat='([a-zA-Z])')].str.split(n=1, expand=True).iloc[:, 0]\n# char.unique()\n\n# using the above code you can first of all find out that what are the tickets which have alphabets in ticket numbers and then\n# use those alphabet patterns to separate them out using regex.\n\ndef ticket_labeling(df):\n    for label, pattern in [('ca', 'C[.]?A[.]?'),('soton', 'SOTON'), ('ston', 'STON'), ('wc', 'W[.]?[\/]?C'), \n                           ('sc', 'S[.]?C[.]?'), ('a', 'A[.]?'), ('soc', 'S[.]?O[.]?[\/]?C'), ('pp', 'PP'), \n                           ('fc', '(F.C.|F.C.C.)'), ('rest_char', '[A-Z]'), ('small_serial_number', '^\\d{3,5}$'), \n                           ('large_serial_number', '^\\d{6,7}$')]:\n        \n        df.Ticket[df.Ticket.str.contains(pattern)] = label\n        \n    return df\n","6c85331b":"def cabin_labeling(df):\n    for i in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:\n        \n        df.Cabin[df.Cabin.str.contains(i, na=False)] = i\n        \n        # temporary nan value imputation for visualization.\n        \n        df.Cabin.fillna('missing', inplace=True)\n        \n    return df","e37e3fa9":"temp = cabin_labeling(train.copy())","da610cac":"temp = temp.groupby(['Pclass', 'Cabin'])[['Name']].count().rename(columns={'Name':'Passengers'})","536f4b74":"temp = temp.reset_index()\ntemp_no_missing_value = temp[temp.Cabin != 'missing']\ntemp_missing_value = temp[temp.Cabin == 'missing']","8efbd101":"plt.figure(figsize=(16, 8))\n\nplt.subplot(121)\nsns.set(font_scale=1.5)\nsns.barplot(data=temp_no_missing_value, x='Cabin', y='Passengers', hue='Pclass')\n\nplt.subplot(122)\nsns.set(font_scale=1.5)\nsns.barplot(data=temp_missing_value, x='Cabin', y='Passengers', hue='Pclass')\nplt.show()","6a14e2a1":"plt.figure(figsize=(12, 8))\nsns.set(font_scale=1.5)\nsns.violinplot(data=train, x='Sex', y='Age', hue='Pclass')\nplt.show()","2ddfcb27":"def combined_labeling(df):\n    \n    return cabin_labeling(ticket_labeling(name_labeling(df)))","87998a3c":"X_train = combined_labeling(train.copy())\nX_test = combined_labeling(test.copy())","de33bdf3":"def proportions(df):\n    \"\"\"\n    this function returns the proportions of passengers in various cabins on the basis of Pclass\n    \"\"\"\n    df = df.groupby(['Pclass', 'Cabin'])['Name'].count().reset_index().rename(columns={'Name':'Passengers'})\n    \n    # lets find proportion of passengers in different cabins for a given pclass.\n\n    total_passengers_in_cabins = df.Passengers[df.Cabin != 'missing'].sum()  # total passengers in cabins in a pclass.\n\n    cabin_proportions = df['Passengers'][df.Cabin != 'missing'] \/ total_passengers_in_cabins\n\n    return cabin_proportions\n\ndef no_cabins(df):\n    \"\"\"\n    This function returns the number of passengers in a given Pclass with out any Cabin feature value\n    \"\"\"\n    \n    df = df.groupby(['Pclass', 'Cabin'])['Name'].count().reset_index().rename(columns={'Name':'Passengers'})\n    \n    return df.Passengers[df.Cabin == 'missing']\n\n\ndef cabin_imputer(df, missing_vals, cabin_proportions):\n    \n    if all(df.Pclass == 1):\n\n        imputation_ndarray = np.random.choice(['A', 'B', 'C', 'D', 'E', 'T'], size=missing_vals[0], p=cabin_proportions[1])\n        \n        missing_values_index = df[df.Cabin == 'missing'].index\n        \n        imputation_series = pd.Series(imputation_ndarray, index=missing_values_index)\n        \n        return imputation_series\n    \n        \n    elif all(df.Pclass == 2):\n        \n        imputation_ndarray = np.random.choice(['D', 'E', 'F'], size=missing_vals[1], p=cabin_proportions[2])\n        \n        missing_values_index = df[df.Cabin == 'missing'].index\n        \n        imputation_series = pd.Series(imputation_ndarray, index=missing_values_index)\n        \n        return imputation_series\n    \n        \n    elif all(df.Pclass == 3):\n        \n        imputation_ndarray = np.random.choice(['E', 'F', 'G'], size=missing_vals[2], p=cabin_proportions[3])\n        \n        missing_values_index = df[df.Cabin == 'missing'].index\n        \n        imputation_series = pd.Series(imputation_ndarray, index=missing_values_index)\n        \n        return imputation_series\n\n\ndef imputer(df):\n    \n    # Using the information revealed by the above violin plot, lets impute the missing age values grouping by the pclass and sex\n    # feature as shown below.\n    df.Age.fillna(df.groupby(['Pclass', 'Sex']).Age.transform('mean'), inplace=True)\n    \n    \n    \n    cabin_proportions = X_train.groupby(['Pclass']).apply(proportions) # cabin proportions with respect to \n                                                                       # Pclass(multi indexed with Pclass as first index)\n    missing_vals = list(df.groupby(['Pclass']).apply(no_cabins))  # list containing missing cabin values in each Pclass\n    \n    \n    # we have to keep the proportions of cabins same as that in X_train for X_test as we are imputing cabins using the info\n    # from training set, but the missing vals have are different values for X_train and X_test so we have to keep missing vals\n    # specific for both the data sets.\n    missing_val_imputed_series = df.groupby(['Pclass']).apply(partial(cabin_imputer, \n                                                                      missing_vals=missing_vals, \n                                                                      cabin_proportions=cabin_proportions)) \n    # get the multi indexed series\n    \n    missing_val_imputed_series = missing_val_imputed_series.reset_index().set_index('PassengerId') \n    # reset index and set PassengerId as index\n    \n    missing_val_imputed_series = missing_val_imputed_series.drop('Pclass', axis=1).sort_index().iloc[:, 0] \n    # finally we drop the Pclass column and sort the index, the object type is DataFrame, so we have to use .iloc[:, 0]\n    # to take out the series.\n   \n    \n    for index, val in zip(missing_val_imputed_series.index, missing_val_imputed_series):\n        df.Cabin[index] = val\n        \n    \n    # lets fill out all the other missing values using the most frequent value.\n    \n    for feature in df.columns:\n        df[feature].fillna(df[feature].mode()[0], inplace=True)\n        \n    # finally there is a 'T' cabin with only one instance in training Cabin feature and no occurence in test set so we should\n    # convert it to 'E'.\n    \n    df.Cabin[df.Cabin == 'T'] = 'E'\n    \n    # finally lets drop the passengerid as index\n    df.reset_index(drop=True, inplace=True)\n        \n    return df","45fb5298":"X_train_imputed = imputer(X_train.copy())\nX_test_imputed = imputer(X_test.copy())","e6d7f99b":"class FeatureEngineering(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, drop_Cabin=False, drop_Name=False, Embarked_target=False, SibSp_Parch_simplify=True, \n                 drop_Ticket=False, scaler='MinMaxScaler', smoothing=10, test=False):\n        self.drop_Cabin = drop_Cabin\n        self.drop_Name = drop_Name\n        self.drop_Ticket = drop_Ticket\n        self.Embarked_target = Embarked_target\n        self.SibSp_Parch_simplify = SibSp_Parch_simplify\n        self.scaler = scaler\n        self.smoothing = smoothing\n        self.target_encoder = None\n        self.test = test\n\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        X['Pclass'] = X.Pclass.astype(str)\n        \n        dummies = pd.get_dummies(X.loc[:, ['Pclass', 'Sex']])\n        X = pd.concat([X, dummies], axis=1).drop(['Pclass', 'Sex'], axis=1)\n        \n        \n        # next we will target encode the Name, Ticket and Cabin. We want only the training set to fit to the Target encoding\n        # class object and not the test set.\n        \n        if not self.test and len(X) > 400: # assuming that cross validation test set size will never be greater tham 400\n            \n            # i do not know whether it will be better to target encode 'Embarked' or one hot encode it, so lets test it out!!!\n            self.cols = ['Ticket', 'Name', 'Cabin'] if not self.Embarked_target else ['Cabin', 'Name', 'Ticket', 'Embarked']\n            \n            self.target_encoder = TargetEncoder(cols=self.cols, smoothing=self.smoothing)\n            self.target_encoder.fit(X.loc[:, self.cols], y_train.iloc[X.index])\n            X.loc[:, self.cols] = self.target_encoder.transform(X.loc[:, self.cols])\n            \n        else:\n            X.loc[:, self.cols] = self.target_encoder.transform(X.loc[:, self.cols])\n            \n            \n            \n        if self.drop_Cabin:\n            \n            X.drop('Cabin', axis=1, inplace=True)\n            \n        if self.drop_Name:\n            \n            X.drop('Name', axis=1, inplace=True)\n            \n        if self.drop_Ticket:\n            \n            X.drop('Ticket', axis=1, inplace=True)\n    \n            \n        if self.SibSp_Parch_simplify:\n            X['SibSp'].replace({0:'No', 1:'Yes', 2:'Yes', 3:'Yes', 4:'Yes', \n                                5:'Yes', 6:'Yes', 7:'Yes', 8:'Yes', 9:'Yes'}, inplace=True)\n            X['Parch'].replace({0:'No', 1:'Yes', 2:'Yes', 3:'Yes', 4:'Yes', \n                                5:'Yes', 6:'Yes', 7:'Yes', 8:'Yes', 9:'Yes'}, inplace=True)\n            \n            dummies = pd.get_dummies(X.loc[:, ['SibSp', 'Parch']])\n            X = pd.concat([X, dummies], axis=1).drop(['SibSp', 'Parch'], axis=1)\n            \n        X = pd.get_dummies(X)\n                \n        \n        features_to_scale = [feature for feature in X.columns if X[feature].max() != 1 and X[feature].min() != 1]\n\n        # we do not want to scale one hot encoded features columns.\n        \n        if not self.test and len(X) > 400:\n            if self.scaler == 'StandardScaler':\n                self.scale_transformer = StandardScaler()\n            \n            elif self.scaler == 'MinMaxScaler':\n                self.scale_transformer = MinMaxScaler()\n                \n            elif self.scaler == 'RobustScaler':\n                self.scale_transformer = RobustScaler()\n                \n            scaled_features = self.scale_transformer.fit_transform(X.loc[:, features_to_scale])\n            X.loc[:, features_to_scale] = pd.DataFrame(data=scaled_features, columns=features_to_scale, index=X.index)\n            \n        else:\n            scaled_features = self.scale_transformer.fit_transform(X.loc[:, features_to_scale])\n            X.loc[:, features_to_scale] = pd.DataFrame(data=scaled_features, columns=features_to_scale, index=X.index)\n        \n        return X\n            ","fae851c3":"def results(cv_results_, n):\n    df = pd.DataFrame(cv_results_)[['params', 'mean_test_score']].nlargest(n, columns='mean_test_score')\n    for i in range(len(df)):\n        print(f'{df.iloc[i, 0]} : {df.iloc[i, 1]}')","8fbf3bb7":"param_grid = {'feature_engineering__drop_Cabin':[True, False],\n              'feature_engineering__drop_Ticket':[True, False],\n              'feature_engineering__drop_Name':[True, False],\n              'feature_engineering__Embarked_target':[True, False], \n              'feature_engineering__SibSp_Parch_simplify':[False],\n              'feature_engineering__scaler':['StandardScaler'],\n              'feature_engineering__smoothing':[5]}\n\n\nfeature_engineering_pipeline = Pipeline([('feature_engineering', FeatureEngineering()),\n                                         ('model', LogisticRegression())])\n\ngrid = GridSearchCV(feature_engineering_pipeline, param_grid, \n                    cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42), \n                    scoring='accuracy', verbose=2, n_jobs=-1)","2622a05c":"grid.fit(X_train_imputed.copy(), y_train)","feb038a1":"results(grid.cv_results_, n=5)","9580a7fc":"fe = FeatureEngineering(drop_Cabin=True, drop_Name=False, drop_Ticket=False, Embarked_target=False, SibSp_Parch_simplify=False, \n                        scaler='StandardScaler', smoothing=5)","21439522":"X_train_fe = fe.fit_transform(X_train_imputed.copy())","66cec4ba":"fe.test = True","af75702f":"X_test_fe = fe.transform(X_test_imputed.copy())","4e0699d8":"def parameter_plot(model, X, y, n_estimators=[100, 200, 300, 400, 600, 900, 1300, 1700, 2000, 2500], hyper_param=None, **kwargs):\n    param_name, param_vals = hyper_param\n    param_grid = {'n_estimators':n_estimators,\n                  f'{param_name}':param_vals}\n    \n    grid = GridSearchCV(model(**kwargs), param_grid, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42), \n                        scoring='accuracy', n_jobs=-1, verbose=2)\n    grid.fit(X, y)\n    results = pd.DataFrame(grid.cv_results_)['mean_test_score'].values\n    results = results.reshape(len(param_vals), len(n_estimators))\n    \n    plt.figure(figsize=(15, 9))\n    for i in range(1, len(param_vals) + 1):\n        plt.plot(n_estimators, results[i-1], label=f'{param_name} - {param_vals[i-1]}')\n      \n    plt.legend()\n    plt.show()","a7657444":"def learning_curve_plotter(Model, X, y, params_1, params_2, step=50):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n    \n    plt.figure(figsize=(16, 7))\n    for i, (name, params) in enumerate([params_1, params_2]):\n        train_score = []\n        val_score = []\n        plt.subplot(1, 2, i+1)\n        for j in range(100, len(X_train), step):\n            model = Model(**params).fit(X_train[:j], y_train[:j])\n            train_score.append(model.score(X_train[:j], y_train[:j]))\n            val_score.append(model.score(X_test, y_test))\n            \n        plt.plot(train_score, 'r-', label='Training accuracy')\n        plt.plot(val_score, 'b-', label='Validation accuracy')\n        plt.title(f'{name}')\n        plt.xlabel('Training set size')\n        plt.ylabel('Accuracy')\n        plt.legend()\n            \n    plt.show()","35716c9f":"param_grid_logreg = {'penalty':['elasticnet'],\n                     'C':[0.03],\n                     'l1_ratio':[0.0],\n                     'solver':['saga']} # these params are the best params","708729c6":"grid_logreg = GridSearchCV(LogisticRegression(), param_grid_logreg, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","b1616802":"grid_logreg.fit(X_train_fe, y_train)","c2aaf5de":"results(grid_logreg.cv_results_, n=40)","c60e3707":"param_grid_knn = {'n_neighbors':[20],\n                     'weights':['uniform'],\n                  'algorithm':['ball_tree']}","70d22137":"grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","adb71b48":"grid_knn.fit(X_train_fe, y_train)","b384b054":"results(grid_knn.cv_results_, n=60)","db33fa3f":"param_grid_svc = {'C':[0.5],\n                  'kernel':['rbf'],\n                  'gamma':[0.1]}","8f8998b5":"grid_svc = GridSearchCV(SVC(), param_grid_svc, cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","6e81777e":"grid_svc.fit(X_train_fe, y_train)","12d5a477":"results(grid_svc.cv_results_, n=40)","af74d10c":"parameter_plot(RandomForestClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 8, 9, None]))","42c28744":"param_grid_random = {'n_estimators':[200, 500],\n                     'max_depth':[5, 9],\n                     'max_samples':[0.5, 0.7],\n                     'max_features':[0.5, 0.7], \n                     'min_samples_split':[2, 5, 8]} ","1480b8d8":"grid_random = GridSearchCV(RandomForestClassifier(), param_grid_random, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=4)","737bc4fe":"grid_random.fit(X_train_fe, y_train)","ae4ecddd":"results(grid_random.cv_results_, n=40)","17feb53b":"params_1 = ('Regularized model', {'max_depth': 5, 'max_features': 0.5, 'max_samples': 0.7, \n                                  'min_samples_split': 5, 'n_estimators': 500})\n\nparams_2 = ('Best model', {'max_depth': 9, 'max_features': 0.7, 'max_samples': 0.7, \n                           'min_samples_split': 5, 'n_estimators': 500})\n\nlearning_curve_plotter(RandomForestClassifier, X_train_fe, y_train, params_1, params_2, step=30)","ac0ca286":"parameter_plot(GradientBoostingClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 6, 9]))","e5aa9825":"parameter_plot(GradientBoostingClassifier, X_train_fe, y_train, hyper_param=('learning_rate', [0.01, 0.03, 0.05, 0.07, 0.1]),\n              max_depth=3) # we have passed depth - 3 to **kwargs","9cd5811f":"param_grid_gradient = {'max_depth':[3],\n                       'n_estimators':[300, 400, 500],\n                       'learning_rate':[0.035, 0.055],\n                       'subsample':[0.4, 0.6],\n                       'max_features':[0.4, 0.6],\n                       'min_samples_split':[2, 5, 8, 12]\n                      }","fd0246c7":"grid_gradient = GridSearchCV(GradientBoostingClassifier(), param_grid_gradient, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","4f3a637b":"grid_gradient.fit(X_train_fe, y_train)","8f039248":"results(grid_gradient.cv_results_, n=25)","02425eb1":"params_1 = ('Regularized model', {'learning_rate': 0.035, 'max_depth': 3, 'max_features': 0.4, \n                                  'min_samples_split': 10, 'n_estimators': 400, 'subsample': 0.6})\nparams_2 = ('Best model', {'learning_rate': 0.055, 'max_depth': 3, 'max_features': 0.4, \n                           'min_samples_split': 2, 'n_estimators': 400, 'subsample': 0.4})\n\nlearning_curve_plotter(GradientBoostingClassifier, X_train_fe, y_train, params_1, params_2, step=100)","dc243ae1":"parameter_plot(XGBClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 6]))","024c83d5":"parameter_plot(XGBClassifier, X_train_fe, y_train, \n               hyper_param=('learning_rate', [0.01, 0.03, 0.05, 0.07, 0.1]), max_depth=6) ","2cd2bb36":"param_grid_xgb = {'n_estimators':[300, 450],\n                  'learning_rate':[0.02, 0.03],\n                  'max_depth':[6],\n                  'subsample':[0.5, 0.7],\n                  'colsample_bylevel':[0.5, 0.7],\n                  'reg_lambda':[1, 5, 15, ]\n                 }","a4e745bf":"grid_xgb = GridSearchCV(XGBClassifier(), param_grid_xgb, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","8b23245b":"grid_xgb.fit(X_train_fe, y_train)","08a1dc77":"results(grid_xgb.cv_results_, n=30)","8f623dec":"params_1 = ('Regularized model', {'colsample_bylevel': 0.5, 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 300, 'reg_lambda': 5, 'subsample': 0.7})\nparams_2 = ('Best model', {'colsample_bylevel': 0.5, 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 450, 'reg_lambda': 1, 'subsample': 0.5})\n\nlearning_curve_plotter(XGBClassifier, X_train_fe, y_train, params_1, params_2, step=50)","a57042c9":"logreg = LogisticRegression(**{'C': 0.03, 'l1_ratio': 0, 'penalty': 'elasticnet', 'solver': 'saga'})\nsvc = SVC(**{'C': 0.5, 'gamma': 0.1, 'kernel': 'rbf'})\nknn = KNeighborsClassifier(**{'algorithm': 'ball_tree', 'n_neighbors': 20, 'weights': 'uniform'})\nrfc = RandomForestClassifier(**{'max_depth': 5, 'max_features': 0.5, 'max_samples': 0.7, \n                                  'min_samples_split': 5, 'n_estimators': 500})\ngradient = GradientBoostingClassifier(**{'learning_rate': 0.055, 'max_depth': 3, 'max_features': 0.4, \n                           'min_samples_split': 2, 'n_estimators': 400, 'subsample': 0.4})\nxgb = XGBClassifier(**{'colsample_bylevel': 0.5, 'learning_rate': 0.03, 'max_depth': 6, \n                       'n_estimators': 300, 'reg_lambda': 5, 'subsample': 0.7})\n\nestimators = [('logreg', logreg), ('knn', knn), ('svc', svc), ('rfc', rfc), ('gradient', gradient), \n              ('xgb', xgb)]\n\n\nstack = StackingClassifier(estimators=estimators,\n                           cv=10, n_jobs=-1)","12819941":"stack.fit(X_train_fe, y_train)","48d3fa7a":"y_preds = stack.predict(X_test_fe)","8deecce0":"submission = pd.DataFrame({'PassengerId':test.index, \n              'Survived':y_preds})","8d20dea2":"submission.to_csv('submission.csv', index=False)","b4a16c4e":"pd.read_csv('submission.csv')","e79fc835":"### KNeighborsCLassifier","c724b47c":"From above plot learning rate of 0.04 - 0.06 looks good for n_estimtors between 300 - 500. We can also observe that as the number of trees increase, the model starts to overfit, which is worse for higher learning rates, in contrast learning rate of 0.01 actually takes lead with more estimators which is quite intuitive","8ba009f1":"#### Lets summarize the way we just grid searched good parameters\n\n* we first checked out our tree ensemble model's performance on the given dataset and then shortlisted range of max depths and n_estimators which are the mone of the most influential parameters.\n\n* then we grid searched the shorlisted parameter values and with some other parameters such as max_samples and max_features, these parameters are must to search as they diversify our trees and make them less similar to each other.\n\n* after we get the results, we then start searching the small parameter space near the optimal parameters obtained from gridsearch, we will usually see increase in accuracy for next few small gridsearchs this way.\n\n* Check for overfitting and pick that model that has good accuracy with least overfitting...","36525916":"*We also have a very interesting relation between the **pclass, sex and age** which can help us better impute the missing age values, this is shown in the visualization below.*","720ce26e":"Regularization is not too great, we might as well use the best model","8e10ec03":"#### Tips:\n\n* When grid searching hyperparameters you should look at top 10 or so best results to get a better idea, if you find a parameter mix which gives you a decent result(not the best) but it is more regulrized than the top result, choose the regularized option and accept the loss in accuracy(if its not a lot), This will ensure that the variance is low and you will actually have more chance of achieving similar result with first model than the best model as per grid search. \n\n* The above practice becomes a cumbersome for tree ensemble methods if you just have to go through the grid search results, simple reason being that there are too many parameters which affect the bias and variance, unlike LogisticRegression or SVC. In this case we will plot the results!!! check out the code below.","4da6a6be":"Looking at the above plot it is clear that max depth of 3 is out of question. for the depths from 5-9 we can see that as we increase the depth the accuracy increase, but overfitting also increases. We should choose a depth of 5 and 9 for testing for 300 to 500 trees. while using grid search we will also explore other parameters such as max_samples, max_features which will help diversify the trees even more. \n\nThe titanic dataset is quite small so we might not be able to leverage most out of the above practice, but you can imagine if the data is too large, it will be best to first get an idea of how the params affect the accuracy and then select a narrow Randomized search, as grid search will be take too much time.","14eb6a69":"## Feature Engineering\n\n\nWe have imputed our data with reasonable values, now there is the part of feature engineering, we wont go too deep, but there are some questions that i wish to ask my model. \n\n* As we know that we have assumed a lot while imputing Cabin feature, is it better to just drop it or we should keep it?\n* is it better to one hot encode Embarked or Target Encode it?\n* is it better to Simplify SibSp and Parch as relatives(1) or no relatives(0) instead of discrete number of relatives and one hot encode it, or is it better to let it stay as it is?\n* Should we drop Ticket?\n* Should we drop Name?\n* How much smoothing should we use for our raget encoded columns?\n* Which is better StandardScaler, MinMaxScaler or RobustScaler?\n\nTo make our model answer these questions, we can put all of it in a feature engineering class and combine this transformation step with our model using a pipeline and cross validate... But beware the dataset is too small and variance is quite high, we might not get a plausible result from crossvalidations, in which case we will just have to check out few of the first combinations on our whole Stacking ensemble and check out the final result.","e6ebc79b":"Now as we have choosen a max depth, lets see how learning rate affects accuracy with change in estimators","403f41ef":"**If you like work, an upvote would be appreciated!!!! :D**\n\nIf you have any suggestions please share in the comments!!\n\nThe best i could get with this notebooks was 0.797, which help me push upto 1090 on leaderboard, which is about Top 5-6%\n\nIf you want to squeeze more out of the dataset used, you can try implementing stacking by yourself and then create a dataframe of predictions made by all the models and train a neural network with dropouts on it, finally when testing turn training = True to for dropout layers and make around 50 - 100 predictions, all these predictions will be different as we turned on the training = True, when we take mean out of all these predictions, we can get an even better result, let me now in the comments if you would want me to demonstrate this technique!!\n\nCheers and best of luck for future Kaggling!!","1c27544a":"**model with params with more regularization**\n* {'colsample_bylevel': 0.5, 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 300, 'reg_lambda': 5, 'subsample': 0.7} : 0.8316 :(\n\n**Best model**\n* {'colsample_bylevel': 0.5, 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 450, 'reg_lambda': 1, 'subsample': 0.5} : 0.8378\n\n(Grid search values may vary if it runs again)","9acd2393":"From the above plots learning rate of 0.01 - 0.03 looks good for estimators between 200 - 500","b8d37cc4":"in **Cabin** feature we have a main cabin type for example 'A', 'B' etc. but the problem is that the feature has a lot of null\nvalues, we should extract the main cabin name and fill out all the null values as 'NaN' for now, as we will see that there\nis a relation between the Cabin and the Pclass, we can use this relation to impute the missing values in Cabin!!!","214f2640":"## Hyperparameter Tuning","da0435d0":"It's quite evident that only 1st class passengers were present in Cabin A, B, C and then some 2nd class in Cabin D, hence while imputing values for 1st class we will use this information and impute values using this information. Similarly we can impute the missing values in 3rd class with only Cabin E, F and G. \n\nThis is one of the good approach to the large missing values present in the Cabin feature, it is only an estimation with the data already present, this might help us use the information present in the 200 non-missing values, in Cabin feature, and improve the accuracy or it might just create unnecessary noise for the model. We can always ask the model if it likes this imputed Cabin feature or not using a Feature Engineering class which allows us to drop and keep the Cabin feature. we can then add feature engineering class and a classification model for example Logistic regression in a pipeline and then grid search the feature engineering class's parameters to find out whether the model found the Cabin feature useful or not.... We will be performing this search in the code below!!!","b7a139a4":"### RandomForestClassifier\n\n\nLets first check the max_depth vs n_estimator relationship for the given dataset for our RandomForestClassifier from the plot then we can narrow down our grid search.","e7d49105":"**model with params with more regularization**\n* {'learning_rate': 0.055, 'max_depth': 3, 'max_features': 0.4, 'min_samples_split': 12, 'n_estimators': 400, 'subsample': 0.6} : 0.826629 :(\n\n**Best model**\n* {'learning_rate': 0.055, 'max_depth': 3, 'max_features': 0.4, 'min_samples_split': 2, 'n_estimators': 400, 'subsample': 0.4} : 0.8355\n\n\n*(Grid search values may vary if it runs again)*","3d0dfd91":"### XGBClassifier","299defe1":"From the above visualization we can clearly see that the age of passengers are dependent upon the gender and the pclass, in 1st class the average age of the passenger is higher than both the 2nd and 3rd class, also in 1st class the average age of women is lower than the average age of men. **We can utilize this knowledge to better impute the missing Age values in dataset!!!**","096fe1eb":"### SVC","3b364ee2":"From the above feature engineering practice we were able to score accuracy of about 0.772 on the test submission using Logistic Regression, But we can do better than that. In this section we will be finding best hyper parameters for a number of models with different strengths and weaknesses so that we can finally blend the results and achieve higher accuracy. The idea is that every model has different strengths and weaknesses, there will be instances that,  for eg. SVC classifies correctly but LogisticRegression cannot, but LogisticRegression might classify some other instances correctly which SVC cannot, these difference will be learnt by the final **meta model** in stacking and we will be able to achieve higher accuracy.\n\nWe will train the following models as base models for stacking:\n* LogisticRegression\n* SVC\n* KNeighborsClassifier\n* RandomForestClassifier\n* GradientBoostingClassifier\n* XGBCLassifier\n\n","a40e53e6":"Well its quite clear which model we should be choosing!!!! the top model is horribly overfitting the data which can result in the top model not performing good on the final test set, whereas our regularized option will perform way better as per above results..... **Dont just straight away choose the best grid search params folks!!!!**","eb08211f":"lets choose one model which is more regularized than the top one....\n\n**model with params with more regularization**\n* {'max_depth': 5, 'max_features': 0.5, 'max_samples': 0.7, 'min_samples_split': 5, 'n_estimators': 500} : 0.8288 :(\n\n**Best model**\n* {'max_depth': 9, 'max_features': 0.7, 'max_samples': 0.7, 'min_samples_split': 5, 'n_estimators': 500} : 0.837\n\n\n*(Grid search values may vary if it runs again)*\n\n\ntop model has **more trees, more max_samples ratio and more depth**, these parameters if increased, reduces bias.","d5058f53":"### GradientBoostingClassifier","56381391":"### LogisticRegression","cd000ef2":"max depth 6 looks to be quite good","aa691ebb":"## Stacking","14d82cf1":"## Missing value imputation","41206ee1":"Funny part is that with just 0.83 accuracy in grid search the learning curve is way better for the regularized model against the best model which scored 0.837 in grid search!!! **even accuracy for regularized model is way better when we look at learning curve validation accucary**  :D\n \n\nfor all three tree ensembles, we could overfit them to reach accuracy of 0.85 in grid search, but grid searching alone gives such a superficial estimate of the tru accuracy!!!!! ","72f5b27d":"Well, to be honest the above results do not tell us much about which is the best data set, so i just tested out few of the top ones and found the mix below to be the best.","79ecd57b":"n_estimators from range 100 - 400 looks good, we will be testing depth 3 with 200-400 n_estimators. We can observe that as the n_estimators increase the model starts to overfit the data and accuracy decreases. "}}