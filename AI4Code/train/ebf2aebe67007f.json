{"cell_type":{"c6ba329b":"code","81f008d5":"code","fcebffa2":"code","520763d7":"code","eebed008":"markdown","46115960":"markdown","f4c6fa6f":"markdown","f3181d1b":"markdown"},"source":{"c6ba329b":"## Make Imports \nfrom plotly.offline import init_notebook_mode, iplot\nfrom wordcloud import WordCloud, STOPWORDS \nfrom nltk.stem import WordNetLemmatizer\nfrom IPython.core.display import HTML\nfrom nltk.corpus import stopwords \nfrom collections import Counter\nimport matplotlib.pyplot as plt \nimport plotly.graph_objs as go\nfrom plotly import tools\nimport IPython.display\nfrom PIL import Image \nimport seaborn as sns\nimport pandas as pd \nimport numpy as np \nimport warnings\nimport operator\nimport string \nimport re \n\n## Environment Preparation\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\nstopwords = stopwords.words(\"english\")\npunc = string.punctuation \nlem = WordNetLemmatizer()\n\n## Load dataset\nuser_followers = pd.read_csv(\"..\/input\/UserFollowers.csv\")\nkernels_tag_df = pd.read_csv(\"..\/input\/KernelTags.csv\")\nmessages = pd.read_csv(\"..\/input\/ForumMessages.csv\")\nforums_df = pd.read_csv(\"..\/input\/Forums.csv\")\nkernels_df = pd.read_csv(\"..\/input\/Kernels.csv\")\ntags_df = pd.read_csv(\"..\/input\/Tags.csv\")\nusers = pd.read_csv(\"..\/input\/Users.csv\")\n\n## function to clean the text\ndef clean_text(txt):    \n    txt = txt.lower()\n    txt = re.sub('<[^<]+?>', '', txt)\n    txt = \"\".join(x for x in txt if x not in punc)\n    words = txt.split()\n    words = [wrd for wrd in words if wrd not in stopwords]\n    words = [wrd for wrd in words if not wrd.startswith(\"http\")]\n    txt = \" \".join(words)\n    return txt\n\n## function to generate ngrams\ndef ngrams(txt, n):\n    txt = txt.split()\n    output = []\n    for i in range(len(txt)-n+1):\n        output.append(\" \".join(txt[i:i+n]))\n    return output\n\n## function to create bins \ndef wordmap(val):\n    if val < 10:\n        return \"0-10\"\n    elif val < 20:\n        return \"10-25\"\n    elif val < 50:\n        return \"25-50\"\n    elif val < 100:\n        return \"50-100\"\n    else:\n        return \"100+\"\n\n## main Analysis Function\ndef _analyze_profile(username):\n    account_id = users[users[\"UserName\"] == username][\"Id\"].iloc(0)[0]\n    name = users[users[\"UserName\"] == username][\"DisplayName\"].iloc(0)[0]\n\n    mydf = messages[messages[\"PostUserId\"] == account_id]\n    mydf['PostDate'] = pd.to_datetime(mydf[\"PostDate\"])\n    mydf[\"weekday\"] = mydf[\"PostDate\"].dt.weekday\n    mydf[\"monthday\"] = mydf[\"PostDate\"].dt.day\n    mydf[\"hour\"] = mydf[\"PostDate\"].dt.hour\n    mydf[\"clean_message\"] = mydf[\"Message\"].fillna(\"\").apply(lambda x : clean_text(x))\n    mydf[\"word_len\"] = mydf[\"clean_message\"].apply(lambda x : len(x.split()))\n    mydf[\"char_len\"] = mydf[\"clean_message\"].apply(lambda x : len(x.replace(\" \",\"\")))\n\n    text = \" \".join(mydf[\"clean_message\"].dropna())\n    unigrams = Counter(ngrams(text, 1))\n    bigrams = Counter(ngrams(text, 2))\n    unigrams_d = dict(unigrams)\n    bigrams_d = dict(bigrams)\n\n    mydf[\"word_len_bin\"] = mydf[\"word_len\"].apply(wordmap)\n    mydf[\"char_len_bin\"] = mydf[\"char_len\"].apply(wordmap)\n\n    mydf['Date'] = mydf['PostDate'].dt.date\n    mydf['month'] = mydf['PostDate'].dt.month\n    dateComments = mydf['Date'].value_counts().to_frame().reset_index().sort_values(\"index\")\n\n    mytags = []\n    forumIDs = list(mydf['ForumTopicId'].values)\n    for forum_id in forumIDs:\n        try:\n            kernel_id = kernels_df[kernels_df[\"ForumTopicId\"] == forum_id]['Id'].iloc(0)[0]\n            taglist = list(kernels_tag_df[kernels_tag_df[\"KernelId\"] == kernel_id][\"TagId\"].values)\n            mytags.extend(tags_df[tags_df[\"Id\"].isin(taglist)]['Name'].values)\n        except Exception as E:\n            pass\n    \n    metrics = {\"mytags\" : mytags, \"unigrams\" : unigrams, \"unigrams_d\" : unigrams_d,\n               \"bigrams\" : bigrams, \"bigrams_d\" : bigrams_d, \"dateComments\" : dateComments, \n               \"name\" : name, \"account_id\" : account_id}\n    return mydf, metrics\n\n## Main Visualization Function\ndef _prepare(mydf, metrics):\n    mytags = metrics[\"mytags\"]\n    unigrams = metrics[\"unigrams\"]\n    bigrams = metrics[\"bigrams\"]\n    bigrams_d = metrics[\"bigrams_d\"]\n    unigrams_d = metrics[\"unigrams_d\"]\n    dateComments = metrics[\"dateComments\"] \n    account_id = metrics[\"account_id\"]\n    name = metrics[\"name\"]\n\n    mpp = {0 : \"Mon\", 1: \"Tue\", 2: \"Wed\", 3:\"Thu\", 4:\"Fri\", 5:\"Sat\", 6:\"Sun\"}\n    mydf[\"word_density\"] = mydf[\"char_len\"] \/ (1+mydf[\"word_len\"])\n    mydf[\"word_density\"] = mydf[\"word_density\"].apply(lambda x : round(x,2))\n\n    daymp = {}\n    lst = mydf[\"weekday\"].value_counts().to_frame().reset_index()\n    for l,day in lst.iterrows():\n        daymp[day[\"index\"]] = day[\"weekday\"]\n    sorted_x = sorted(daymp.items(), key=operator.itemgetter(1), reverse = True)\n    \n    # insights_t = {\n    #     \"Total Discussions\" : len(mydf),\n    #     \"Average Discussions Per Day\" : int(np.mean(np.array(dateComments[\"Date\"].values))),\n    #     \"Average Discussions Per Month\" : int(np.mean(mydf[\"month\"].value_counts().values)),\n    #     \"Maximum Discussions on Single Day\" : int(np.max(np.array(dateComments[\"Date\"].values))),\n    #     \"Maximum Discussions Date\" : str(dateComments[dateComments[\"Date\"] == np.max(np.array(dateComments[\"Date\"].values))][\"index\"].iloc(0)[0]),\n    #     \"Most Discussions WeekDay\" : mpp[sorted_x[0][0]],\n    #     \"Least Discussions WeekDay\" : mpp[sorted_x[-1][0]],\n    #     \"Average Words Per Discussions\" : int(np.mean(mydf[\"word_len\"])),\n    #     \"Average Characters Per Discussions\" : int(np.mean(mydf[\"char_len\"])),\n    #     \"Average Word Density Per Discussions\" : round(np.mean(mydf[\"word_density\"]),2),\n    #     \"Top KeyWord Used\" : unigrams.most_common()[0][0],\n    #     \"Top Tag Followed\" : Counter(mytags).most_common(1)[0][0],\n    #     \"Total Discussions\" : len(mydf),\n    #    \"Average Discussions Per Day\" : int(np.mean(np.array(dateComments[\"Date\"].values))),\n    #    \"Average Discussions Per Month\" : int(np.mean(mydf[\"month\"].value_counts().values)),\n    #     \"Maximum Discussions on Single Day\" : int(np.max(np.array(dateComments[\"Date\"].values))),\n    #     \"Maximum Discussions Date\" : str(dateComments[dateComments[\"Date\"] == np.max(np.array(dateComments[\"Date\"].values))][\"index\"].iloc(0)[0]),\n    #    \"Most Discussions WeekDay\" : mpp[sorted_x[0][0]],\n    #    \"Least Discussions WeekDay\" : mpp[sorted_x[-1][0]],\n    #    \"Average Words Per Discussions\" : int(np.mean(mydf[\"word_len\"])),\n    #    \"Average Characters Per Discussions\" : int(np.mean(mydf[\"char_len\"])),\n    #    \"Average Word Density Per Discussions\" : round(np.mean(mydf[\"word_density\"]),2),\n    #    \"Top KeyWord Used\" : unigrams.most_common()[0][0],\n    #    \"Top Tag Followed\" : Counter(mytags).most_common(1)[0][0],\n    #}\n    \n    insights = {\n        \"Total Discussions\" : len(mydf),\n        \"Average Discussions Per Day\" : int(np.mean(np.array(dateComments[\"Date\"].values))),\n        \"Average Discussions Per Month\" : int(np.mean(mydf[\"month\"].value_counts().values)),\n        \"Maximum Discussions on Single Day\" : int(np.max(np.array(dateComments[\"Date\"].values))),\n        \"Maximum Discussions Date\" : str(dateComments[dateComments[\"Date\"] == np.max(np.array(dateComments[\"Date\"].values))][\"index\"].iloc(0)[0]),\n        \"Most Discussions WeekDay\" : mpp[sorted_x[0][0]],\n        \"Least Discussions WeekDay\" : mpp[sorted_x[-1][0]],\n        \"Average Words Per Discussions\" : int(np.mean(mydf[\"word_len\"])),\n        \"Average Characters Per Discussions\" : int(np.mean(mydf[\"char_len\"])),\n        \"Average Word Density Per Discussions\" : round(np.mean(mydf[\"word_density\"]),2),\n        \"Top KeyWord Used\" : unigrams.most_common()[0][0],\n        \"Top Tag Followed\" : Counter(mytags).most_common(1)[0][0],\n    }\n\n    tabs1 = list(insights.keys())\n    tabvals1 = list(insights.values())\n    tr9 = go.Table(header=dict(values=['Metric', 'Value'], line = dict(color='#7D7F80'), fill = dict(color='#a1c3d1'), align = ['left'] * 2),\n                     cells=dict(values=[tabs1, tabvals1], line = dict(color='#7D7F80'), fill = dict(color='#EDFAFF'), align = ['left'] * 2))\n\n    layout = dict(title=\"Report Summary : \" + name, height=500)\n    data = [tr9]\n    fig = dict(data=data, layout=layout)\n    iplot(fig)\n\n    lst = unigrams.most_common(15)\n    tr1 = go.Bar(y= [c[0] for c in lst][::-1], x= [c[1] for c in lst][::-1], orientation=\"h\") \n    lst = bigrams.most_common(15)\n    tr2 = go.Bar(y= [c[0] for c in lst][::-1], x= [c[1] for c in lst][::-1], orientation=\"h\") \n\n    dvals = [daymp[0], daymp[1], daymp[2], daymp[3], daymp[4], daymp[5], daymp[6]]\n    tr3 = go.Bar(x= [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"], y= dvals, marker=dict(color=\"#c7a6ea\")) \n    lst = mydf[\"monthday\"].value_counts()\n    tr4 = go.Bar(x= lst.index, y= lst.values, marker=dict(color=\"#c7a6ea\")) \n\n    lst = dict(mydf[\"word_len_bin\"].value_counts())\n   # xx = [\"0-10\", \"10-25\", \"25-50\", \"50-100\", \"100+\"]\n    xx = [\"0-10\", \"10-25\", \"25-50\", \"50-100\"]\n    yy = [lst[v] for v in xx]\n    tr5 = go.Bar(x= xx, y= yy, marker=dict(color=\"#eaa5b8\")) \n    lst = mydf[\"char_len_bin\"].value_counts()\n    yy = [lst[v] for v in xx]\n    tr6 = go.Bar(x= xx, y= yy, marker=dict(color=\"#eaa5b8\")) \n\n    lst = Counter(mytags).most_common(15)\n    tr7 = go.Scatter(x=[c+3 for c in range(len(lst))], y = [c[1] for c in lst], text=[c[0] for c in lst], \n                     textposition='top right', textfont=dict(size=10), mode='markers+text', marker=dict(color=\"#42f4a4\", size=25 ))\n\n    fig = tools.make_subplots(rows=5, cols=2, vertical_spacing = 0.05, print_grid=False, specs = [[{\"colspan\" : 2},None], [{},{}], [{},{}], [{\"colspan\" : 2},None], [{\"colspan\" : 2},None]], \n                             subplot_titles=[\"Discussions by Date\", \"Day of the Week - Discussion Activity\", \"Day of the Month - Discussion Activity\",\n                                             \"Number of Words per Discussions\", \"Number of Characters used per Discussions\", \n                                             \"Top Kernel Tags Followed\", \"Followers Gained Since 2017\" ])\n\n    tr8 = go.Scatter(x = dateComments[\"index\"], y = dateComments[\"Date\"], mode=\"lines+markers\", line=dict(color=\"orange\", width=3))\n    myfol = user_followers[user_followers[\"FollowingUserId\"] == account_id]\n    myfol[\"Date\"] = pd.to_datetime(myfol[\"CreationDate\"])\n    tmp = myfol[\"Date\"].value_counts().to_frame().reset_index().sort_values(\"index\")\n    tr10 = go.Scatter(x = tmp[\"index\"], y = tmp[\"Date\"], mode=\"lines+markers\", line=dict(color=\"pink\", width=3))\n\n    fig.append_trace(tr8, 1, 1)\n    fig.append_trace(tr3, 2, 1)\n    fig.append_trace(tr4, 2, 2)\n    fig.append_trace(tr5, 3, 1)\n    fig.append_trace(tr6, 3, 2)\n    fig.append_trace(tr7, 4, 1)\n    fig.append_trace(tr10, 5, 1)\n\n    fig['layout'].update(barmode='group', title = 'Kaggle Discussions Analysis Report: ' + name,\n        titlefont=dict(size=22,color='#000'),                     \n        margin=dict(t=100, b=100),\n        paper_bgcolor='rgb(254, 247, 234)',\n        plot_bgcolor='rgb(254, 247, 234)',\n        height=1300,\n        showlegend=False)\n    iplot(fig);\n    \n## main wordcloud function\ndef _wc(mydf):\n    wordcloud = WordCloud(max_font_size=40, max_words=12000, colormap='Dark2_r', random_state=42).generate(str(mydf['clean_message']))\n    fig = plt.figure(figsize=(12,8))\n    plt.imshow(wordcloud)\n    plt.title(\"Top Used Words\")\n    plt.axis('off')\n    plt.show()\n\n## Function to generate Ngram Bubble Cloud\ndef _ngramCloud(bigrams, username):\n    strr = \"id,value,value1\\nproject,\\n\"\n    num = 1\n    cnt = 1\n    sizes =[9000,7500,6000,5000,4000,2500,2200,1900,1800,1860]\n    for j, each in enumerate(bigrams.most_common(100)):\n        val = each[1]\n        strr += \"project.\" +str(num)+\".\"+ str(each[0]) + \",\" + str(val) + \",\" + str(val) + \"\\n\"\n        if cnt % 2 == 0:\n            num += 1\n        cnt += 1\n        if cnt == 100:\n            break\n    fout = open(\"flare\"+username+\".csv\", \"w\")\n    fout.write(strr)\n\n    html_p1 = \"\"\"<!DOCTYPE html><svg id='idd_\"\"\"+username+\"\"\"' width=\"760\" height=\"760\" font-family=\"sans-serif\" font-size=\"10\" text-anchor=\"middle\"><\/svg>\"\"\"\n    js_p1 = \"\"\"require.config({paths: {d3: \"https:\/\/d3js.org\/d3.v4.min\"}});\n    require([\"d3\"], function(d3) {var svg=d3.select(\"#idd_\"\"\"+username+\"\"\"\"),width=+svg.attr(\"width\"),height=+svg.attr(\"height\"),format=d3.format(\",d\"),color=d3.scaleOrdinal(d3.schemeCategory20c);var pack=d3.pack().size([width,height]).padding(1.5);d3.csv(\"flare\"\"\"+username+\"\"\".csv\",function(t){if(t.value=+t.value,t.value)return t},function(t,e){if(t)throw t;var n=d3.hierarchy({children:e}).sum(function(t){return t.value}).each(function(t){if(e=t.data.id){var e,n=e.lastIndexOf(\".\");t.id=e,t.package=e.slice(0,n),t.class=e.slice(n+1)}}),a=(d3.select(\"body\").append(\"div\").style(\"position\",\"absolute\").style(\"z-index\",\"10\").style(\"visibility\",\"hidden\").text(\"a\"),svg.selectAll(\".node\").data(pack(n).leaves()).enter().append(\"g\").attr(\"class\",\"node\").attr(\"transform\",function(t){return\"translate(\"+t.x+\",\"+t.y+\")\"}));a.append(\"circle\").attr(\"id\",function(t){return t.id}).attr(\"r\",function(t){return t.r}).style(\"fill\",function(t){return color(t.package)}),a.append(\"clipPath\").attr(\"id\",function(t){return\"clip-\"+t.id}).append(\"use\").attr(\"xlink:href\",function(t){return\"#\"+t.id}),a.append(\"svg:title\").text(function(t){return t.value}),a.append(\"text\").attr(\"clip-path\",function(t){return\"url(#clip-\"+t.id+\")\"}).selectAll(\"tspan\").data(function(t){return t.class.split(\/(?=[A-Z][^A-Z])\/g)}).enter().append(\"tspan\").attr(\"x\",0).attr(\"y\",function(t,e,n){return 13+10*(e-n.length\/2-.5)}).text(function(t){return t})});});\"\"\"\n    h = display(HTML(html_p1))\n    j = IPython.display.Javascript(js_p1)\n    IPython.display.display_javascript(j)\n\n## master function to generate the complete report \ndef _generate_report(username):\n    mydf, metrics = _analyze_profile(username)\n    _prepare(mydf, metrics)\n    display(HTML(\"Top Ngrams Used\"))\n    _ngramCloud(metrics[\"bigrams\"], username)\n    _wc(mydf)","81f008d5":"_generate_report(\"yashikanigam\")","fcebffa2":"_generate_report(\"mrisdal\")","520763d7":"_generate_report(\"shivamb\")","eebed008":"Here is report of Shivam Bansal's profile\n\n# Example Report 2 : Shivam Bansal\n<hr>\n","46115960":"Lets look at reports from the Kaggle Team User Megan Risdal\n\n# Example Report 1 : Megan Risdal \n<hr>","f4c6fa6f":"# Analyse your kaggle profile | Generate Report\n\n<br>\nThanks to [Shivam Bansal](https:\/\/www.kaggle.com\/shivamb)'s amazing kernel, I was able to analyze my discussions activity on Kaggle !\n\nTo create your own report, FORK this kernel OR Shivam's [original kernel](https:\/\/www.kaggle.com\/shivamb\/analyse-your-kaggle-profile-framework), and invoke the _generate_report function with your username as the argument. \n\n## _generate_report(\"your_kaggle_user_name\")\n\n<br>\n# My Report : yashikanigam   \n<hr>\n","f3181d1b":"To generate the analysis report for your kaggle profile, Follow these steps: \n\n1. Fork This Kernel (Also, upvote the kernel)\n2. In the last cell, call _generate_report(\"your_kaggle_user_name\")"}}