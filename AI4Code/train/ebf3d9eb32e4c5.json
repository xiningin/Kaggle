{"cell_type":{"58600345":"code","bdf46960":"code","fb8a6b38":"code","27373751":"code","b625df21":"code","bdef335b":"code","b114f095":"code","0f9985d5":"code","8f38c7d9":"code","877c64df":"code","6979e71a":"code","f090aec3":"code","f0c13890":"code","726d1235":"code","fe55f80a":"code","c13b621e":"code","7f98248c":"code","4b947683":"code","cb99dffb":"code","1517060e":"code","11dec1a5":"code","6d812d81":"code","076de87b":"code","c8066de9":"code","26bae1bf":"code","bb957192":"code","7ba606ca":"code","e434dce6":"code","0031e882":"code","2645d14d":"code","79fd2e5a":"code","7a9b4fb7":"code","b614c2a0":"code","9b363498":"code","03292ed0":"code","c8e46411":"code","bc36215f":"code","37005fe5":"code","22bd1b14":"code","73052e5f":"code","5631d2c7":"code","a57f5891":"code","e74579ad":"code","d5526a31":"code","a832103f":"code","cc793d6e":"code","03a6fcc8":"code","38a7003b":"code","d3b06352":"code","cd8dfc10":"code","dad9ddf3":"code","f6e51ec5":"code","9f89f224":"code","f71eebc5":"code","deb28827":"code","392b1e60":"code","8d2588bd":"code","46692bfb":"code","847ca200":"code","3f9a64ed":"code","d8ce35f1":"code","e2abf486":"code","f5b64d59":"code","d2730881":"code","45d1d86b":"code","2dcbfdbc":"code","794a87e1":"code","deacf62e":"code","fb665dc3":"code","a00131a8":"code","67ab5ea5":"code","20ccc43f":"code","5b52af1a":"code","f2749d54":"code","6a30ba01":"code","ab674c4a":"code","3e0fbc6b":"code","98378ec9":"code","3d445f18":"code","afc4f4b1":"code","dfcba7fa":"code","3271122b":"code","48f59dec":"code","a10208a6":"code","c0177a51":"code","23974c4c":"code","62b3fc4a":"code","07bf1abb":"code","bc79e229":"code","41169d27":"code","91e14676":"markdown","0f364547":"markdown","99a630ea":"markdown","65df2a3a":"markdown","b0adc0e7":"markdown","f5f3568c":"markdown","5408cf47":"markdown","f5812374":"markdown","0e2f759b":"markdown","c9f0b976":"markdown","2c475379":"markdown","dd54d883":"markdown","f5ea3718":"markdown","dcfd0c0a":"markdown","382ebdc4":"markdown","4eda641a":"markdown","5d61723b":"markdown","13b3f10c":"markdown","e21b911d":"markdown","f0b145f4":"markdown","49c51e88":"markdown","69f5bf19":"markdown","3dc8f7a4":"markdown","560f3fb2":"markdown","9287251b":"markdown","0eb4467d":"markdown","8bfa8e9e":"markdown","17efde70":"markdown","8f724952":"markdown","4368e818":"markdown","61788c57":"markdown","2c487df5":"markdown","26d6ceb9":"markdown","2397df7b":"markdown","60d22a2d":"markdown","e17845ff":"markdown","338b2861":"markdown","e7874ed5":"markdown","6a308d47":"markdown","738f6262":"markdown","5840889b":"markdown"},"source":{"58600345":"import math,time,random, datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder,label_binarize\n\n\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection,tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool,cv\n\nimport warnings\nwarnings.filterwarnings('ignore')","bdf46960":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","fb8a6b38":"gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","27373751":"# looking at the training data\ntrain.head()","b625df21":"# looking at the test data\ntest.head()","bdef335b":"# length of training and test data\nlen(train) , len(test)","b114f095":"# Submission FIle\ngender_submission.head()","0f9985d5":"train.describe()","8f38c7d9":"# Plotting graphic of missing values\nmissingno.matrix(train,figsize=(30,10))","877c64df":"train.columns","6979e71a":"# let's write a function to see how many missing values are there\ndef finding_missing_values(df,column):\n    missing_values = {}\n    df_length=len(df)\n    for col in column:\n        total_column_values = df[col].value_counts().sum()\n        missing_values[col] = df_length-total_column_values\n    return missing_values\n\nmissing_values = finding_missing_values(train,column=train.columns)\nmissing_values","f090aec3":"df_bin = pd.DataFrame() # for Discritisized continuous variables\ndf_con = pd.DataFrame() # for continuous variables","f0c13890":"train.dtypes","726d1235":"# How many Survived\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived',data = train)\n\ntrain.Survived.value_counts()","fe55f80a":"# let's add this to our subset dataframe\ndf_bin['Survived'] = train['Survived']\ndf_con['Survived'] = train['Survived']","c13b621e":"df_bin.head()","7f98248c":"df_con.head()","4b947683":"# lets see the distribution\nsns.distplot(train.Pclass)","cb99dffb":"# adding to out sub dataframes\ndf_bin['Pclass'] = train['Pclass']\ndf_con['Pclass'] = train['Pclass']","1517060e":"df_con.head()","11dec1a5":"train.Name.value_counts()","6d812d81":"# count of each gender\nfig = plt.figure(figsize=(20,5))\nsns.countplot(y='Sex',data=train)","076de87b":"# adding sex to subset dataframes\ndf_bin['Sex'] = train['Sex']\n\n# female-1, male -0\ndf_bin['Sex'] = np.where(df_bin['Sex']=='female' ,1,0)\n\ndf_con['Sex'] = train['Sex']","c8066de9":"df_bin.head()","26bae1bf":"missing_values['Age']","bb957192":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});","7ba606ca":"train.SibSp.value_counts()","e434dce6":"df_bin['SibSp'] = train['SibSp']\ndf_con['SibSp'] = train['SibSp']","0031e882":"plot_count_dist(train,\n                bin_df =df_bin,\n               label_column = 'Survived',\n               target_column = 'SibSp',\n               figsize=(20,10))","2645d14d":"train.Parch.value_counts()","79fd2e5a":"df_bin['Parch'] = train['Parch']\ndf_con['Parch'] = train['Parch']","7a9b4fb7":"sns.countplot(y='Ticket',data=train)","b614c2a0":"train.Ticket.value_counts()","9b363498":"sns.countplot(y='Fare',data=train)","03292ed0":"train.Fare.value_counts()","c8e46411":"train.Fare.dtype","bc36215f":"df_con['Fare'] = train['Fare']\ndf_bin['Fare'] = pd.cut(train['Fare'],bins=5)","37005fe5":"df_bin.Fare.value_counts()","22bd1b14":"plot_count_dist(train,\n                bin_df =df_bin,\n               label_column = 'Survived',\n               target_column = 'Fare',\n               figsize=(20,10),\n               use_bin_df=True)","73052e5f":"# How many missing values does Cabin have?\ntrain.Cabin.isnull().sum()","5631d2c7":"# How many missing values does Embarked have?\ntrain.Embarked.isnull().sum()","a57f5891":"# What kind of values are in Embarked?\ntrain.Embarked.value_counts()","e74579ad":"# What do the counts look like?\nsns.countplot(y='Embarked', data=train);","d5526a31":"# Add Embarked to sub dataframes\ndf_bin['Embarked'] = train['Embarked']\ndf_con['Embarked'] = train['Embarked']","a832103f":"# Remove Embarked rows which are missing values\nprint(len(df_con))\ndf_con = df_con.dropna(subset=['Embarked'])\ndf_bin = df_bin.dropna(subset=['Embarked'])\nprint(len(df_con))","cc793d6e":"df_bin.head()","03a6fcc8":"# One hot encodding binned variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin, columns=one_hot_cols)\n\ndf_bin_enc.head()","38a7003b":"df_con.head()","d3b06352":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","cd8dfc10":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","dad9ddf3":"df_con_enc.head()","f6e51ec5":"selected_df= df_con_enc","9f89f224":"selected_df.head()","f71eebc5":"# Split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) # data\ny_train = selected_df.Survived # labels","deb28827":"# Shape of the data \nX_train.shape,y_train.shape","392b1e60":"def fit_ml_algo(algo,X_train,y_train,cv):\n    \n    model = algo.fit(X_train,y_train)\n    acc= round(model.score(X_train,y_train)*100,2)\n    \n    # Cross validation\n    train_pred = model_selection.cross_val_predict(algo,\n                                                  X_train,\n                                                  y_train,\n                                                  cv=cv,\n                                                  n_jobs=-1)\n    \n    # Cross validation accuracy metrics\n    acc_cv= round(metrics.accuracy_score(y_train,train_pred)*100,2)\n    \n    return train_pred,acc,acc_cv","8d2588bd":"train_pred, acc_log1,acc_cv_log1 = fit_ml_algo(LogisticRegression(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log1,acc_cv_log1","46692bfb":"train_pred, acc_log2,acc_cv_log2 = fit_ml_algo(KNeighborsClassifier(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log2,acc_cv_log2","847ca200":"train_pred, acc_log3,acc_cv_log3 = fit_ml_algo(GaussianNB(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log3,acc_cv_log3","3f9a64ed":"train_pred, acc_log4,acc_cv_log4 = fit_ml_algo(LinearSVC(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log4,acc_cv_log4","d8ce35f1":"train_pred, acc_log5,acc_cv_log5 = fit_ml_algo(SGDClassifier(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log5,acc_cv_log5","e2abf486":"train_pred, acc_log6,acc_cv_log6 = fit_ml_algo(DecisionTreeClassifier(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log6,acc_cv_log6","f5b64d59":"train_pred, acc_log7,acc_cv_log7 = fit_ml_algo(GradientBoostingClassifier(),\n                                            X_train,\n                                            y_train,\n                                            10)\n\nacc_log7,acc_cv_log7","d2730881":"# View the data for the CatBoost model\nX_train.head()","45d1d86b":"y_train.head()","2dcbfdbc":"# Define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","794a87e1":"# Use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","deacf62e":"# CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","fb665dc3":"# How long will this take?\nstart_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","a00131a8":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","67ab5ea5":"\nmodels = pd.DataFrame({\n    'Model': [ 'Logistic Regression','KNN', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_log1,\n        acc_log2,\n        acc_log3,\n        acc_log5,\n        acc_log4,\n        acc_log6,\n        acc_log7,\n        acc_catboost,\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","20ccc43f":"\ncv_models = pd.DataFrame({\n    'Model': [ 'Logistic Regression','KNN', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_log1,\n        acc_cv_log2,      \n        acc_cv_log3,\n        acc_cv_log5, \n        acc_cv_log4,\n        acc_cv_log6,\n        acc_cv_log7,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","5b52af1a":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp","f2749d54":"# Plot the feature importance scores\nfeature_importance(catboost_model, X_train)","6a30ba01":"metrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=True)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","ab674c4a":"# We need our test dataframe to look like this one\nX_train.head()","3e0fbc6b":"# Our test dataframe has some columns our model hasn't been trained on\ntest.head()","98378ec9":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","3d445f18":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","afc4f4b1":"# Let's look at test, it should have one hot encoded columns now\ntest.head()","dfcba7fa":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","3271122b":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])","48f59dec":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","a10208a6":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","c0177a51":"# What does our submission have to look like?\ngender_submission.head()","23974c4c":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","62b3fc4a":"# How does our submission dataframe look?\nsubmission.head()","07bf1abb":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","bc79e229":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('..\/catboost_submission.csv', index=False)\nprint('Submission CSV is ready!')","41169d27":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"..\/catboost_submission.csv\")\nsubmissions_check.head()","91e14676":"# Stochastics Gradietn Descent","0f364547":"# Loading the data\n","99a630ea":"# Embarked\nPort where passenger boarded","65df2a3a":"\nThis means Catboost has picked up that all variables except Fare can be treated as categorical.","b0adc0e7":"* There are some missing values in the 'Age' Col","f5f3568c":"# Data Description\n\n1. survival\tSurvival\t0 = No, 1 = Yes\n2. pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n3. sex\tSex\t\n4. Age\tAge in years\t\n5. sibsp\t# of siblings \/ spouses aboard the Titanic\t\n6. parch\t# of parents \/ children aboard the Titanic\t\n7. ticket\tTicket number\t\n8. fare\tPassenger fare\t\n9. cabin\tCabin number\t\n10. embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","5408cf47":"# Precision and Recall","f5812374":"# Defining a function to fit machine learning algos","0e2f759b":"# Parch\n\nNo. of parents\/ children that passengers have","c9f0b976":"# Function to create count and distribution visualizations","2c475379":"# Feature Importance","dd54d883":"removing the empty rows\/\n","f5ea3718":"681 unique values","dcfd0c0a":"# model Results","382ebdc4":"# Linear Support Vector MAchines (SVC)","4eda641a":"# Decision Tree Classifier","5d61723b":"# K-Nearest Neighbors","13b3f10c":"# SubSp\nNo. of siblings\/spouses the passenger has aboard the Titanic","e21b911d":"# Submission","f0b145f4":"# Cabin","49c51e88":"# Perform CatBoost cross-validation","69f5bf19":"# Age","3dc8f7a4":"# Feature Encoding","560f3fb2":"# Ticket\n\nticket num of the passenger","9287251b":"# Gradient Boost Trees","0eb4467d":"# **Pclass** \n\n1 - 1st\n2 - 2nd\n3 - 3rd","8bfa8e9e":"Everyone has a unique name.. so we won't move forward using this Name variable","17efde70":"# Building ML Models","8f724952":"# Logistic Regression","4368e818":"let's look in another way","61788c57":"# Sex","2c487df5":"# **Name:**\n\n","26d6ceb9":"## Seperating the data","2397df7b":"# Exploring each Feature individually\n\n## **Target** -**Survived**\n\n0- Not Survived\n1- Survived","60d22a2d":"284 Unique values\n","e17845ff":"# Gaussian NB","338b2861":"# Fare\nTicker Cost","e7874ed5":"# TO perform DataAnalysis, let's create two new Dataframes\n\n* One for Discritised Continuous Variables and Another for Continuous Variables","6a308d47":"## Regular model scores","738f6262":"Fare is a float number, lets add it to our continuous sub dataframe","5840889b":"# CatBoost Algorithm\n\n* it is a SOTA open-source gradient boosting on Decision Trees"}}