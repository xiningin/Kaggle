{"cell_type":{"4cd5670f":"code","1ac5d88c":"code","30c62783":"code","a57b751d":"code","c91a5be0":"code","4c6aa18c":"code","cb0ad4fb":"code","d9d67111":"code","89cb6ec5":"code","0da1a595":"code","404ad679":"code","dbf0fa94":"code","220820ab":"code","e2cf8d90":"code","130285cd":"code","5e3a25b6":"code","92a117a6":"code","a0734795":"code","bc260d33":"code","8ce59b65":"code","a459a7af":"code","ab1cf67f":"code","8d102e41":"code","5cc428ac":"code","a4e3b43e":"code","ed810138":"code","dc49c1d2":"code","9759c778":"code","c651bd8b":"code","5ece4cb9":"code","f61f0350":"code","220a5518":"code","09943673":"code","d7fd171d":"code","0c46cab1":"code","c996c8d7":"code","17268c46":"code","77cfcd51":"code","fa5ba853":"code","8d6031da":"code","a3e26c9e":"code","0af21fa1":"code","23e7b234":"code","18150ba5":"code","28ced8aa":"code","5f396e89":"code","f98c99a8":"code","96117717":"code","da2b835c":"code","a58c5097":"code","35759ecc":"code","a3477408":"markdown","48b90890":"markdown","c8bee9a4":"markdown","ce168155":"markdown","89e70386":"markdown","b2546fc3":"markdown","3cf52250":"markdown","68962ef7":"markdown","f56e2cf3":"markdown","fbe201e3":"markdown","4568ea7c":"markdown","2034db64":"markdown","a6c8c005":"markdown","ba2573bd":"markdown","a6b9163c":"markdown","cba93795":"markdown","f20c7401":"markdown","ffb48aab":"markdown","d94875ed":"markdown","4001fbe5":"markdown","f4fc4c80":"markdown","950635ea":"markdown","0260f569":"markdown","58c873a8":"markdown","ff788b50":"markdown","fa81a5d8":"markdown","4ebd4aca":"markdown","8cd2904f":"markdown","16e563e8":"markdown","a805a9c5":"markdown","91210dcd":"markdown","6c024032":"markdown","bdb9ea80":"markdown","42ca5f67":"markdown","eaa27114":"markdown","01ae2bcc":"markdown","a87d423a":"markdown","8cf2e072":"markdown","3599471c":"markdown","ff1e01b4":"markdown","6736c37a":"markdown","6b2c0881":"markdown","89453be6":"markdown","ffd34322":"markdown","9afc40b4":"markdown","5d5d7915":"markdown","12a0e858":"markdown","3265a79f":"markdown","90f28a71":"markdown","8b410b92":"markdown","9e02ae02":"markdown","5b28454b":"markdown","329dee91":"markdown"},"source":{"4cd5670f":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport random \nimport re\n\nimport pandas as pd \nimport numpy as np \nfrom scipy.stats import kurtosis, skew \nfrom scipy import stats\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n# Importing librarys to use on interactive graphs\nimport plotly.offline as plty\nfrom plotly import tools\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot, plot \nimport plotly.graph_objs as go \n\n\n# to set a style to all graphs\nplt.style.use('fivethirtyeight')\ninit_notebook_mode(connected=True)\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")","1ac5d88c":"\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n","30c62783":"def DataDesc(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","a57b751d":"\nDataDesc(df_train)\n","c91a5be0":"\nDataDesc(df_test)\n","4c6aa18c":"\n\ndf_train['Title'] = df_train['Name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.',x).group(1).strip())\ndf_test['Title'] = df_test['Name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.',x).group(1).strip())\n\n","cb0ad4fb":"group = df_train.groupby('Title').size().rename('Count').reset_index()\nfig = px.pie(group, \n             values='Count', names='Title', \n             color_discrete_sequence=[\"#264653\",\"#2a9d8f\",\"#e9c46a\",\"#f4a261\",\"#e76f51\",'#457b9d'],\n            title='Name Titles',\n            width=600,\n            height=400)\n\nfig.update_layout(\n    margin=dict(l=100, r=0, t=30, b=50),\n    width = 800,\n    height = 500,\n    paper_bgcolor=\"#ffffff\",\n)\nfig.show()","d9d67111":"Title_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n                   }\n    \n# we map each title to the desired category\ndf_train['Title'] = df_train.Title.map(Title_Dictionary)\ndf_test['Title'] = df_test.Title.map(Title_Dictionary)","89cb6ec5":"group = df_train.groupby(['Title','Survived']).size().rename('Count').reset_index()\n\nfig  = px.histogram(data_frame=group, \n              x='Title', \n              y='Count', \n              color='Survived',\n              color_discrete_sequence=[\"#0d3b66\",\"#faf0ca\"],\n              template='plotly_white')\n\nfig.update_layout(width=900, height=450, \n                  title= {'text': \"Titles after grouping\",\n                          'y':0.95,'x':0.5,\n                          'xanchor': 'center',\n                          'yanchor': 'top'},\n                 barmode='group',\n                 showlegend=True,\n                 margin = dict(l=25, r=10, t=50, b=10))","0da1a595":"group = df_train.groupby(['Title']).agg({'Survived':'mean'}).reset_index()\n\nfig  = px.bar(data_frame=group, \n              x='Title', \n              y='Survived',\n              color='Survived',\n              template='plotly_white',\n              color_continuous_scale=[\"#b76935\",\"#a56336\",\"#935e38\",\"#815839\",\"#6f523b\",\"#5c4d3c\",\"#4a473e\",\"#38413f\",\"#263c41\",\"#143642\"])\n\nfig.update_layout(width=900, height=450, \n                  title= {'text': \"Chances of Survival based on Titles\",\n                          'y':0.95,'x':0.5,\n                          'xanchor': 'center',\n                          'yanchor': 'top'},\n                 margin = dict(l=25, r=10, t=50, b=10))","404ad679":"age_died = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 0)]\nage_surv = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 1)]\n\n#figure size\nplt.figure(figsize=(10,5))\n\n# Ploting the 2 variables that we create and compare the two\nsns.distplot(age_surv[\"Age\"], bins=24, color='#0d3b66')\nsns.distplot(age_died[\"Age\"], bins=24, color='#f5dd90')\nplt.title(\"Distribuition of Age\",fontsize=20)\nplt.xlabel(\"Age\",fontsize=15)\nplt.ylabel(\"\")\nplt.show()","dbf0fa94":"\n\ndf_train.groupby(['Pclass','Sex','Title'])['Age'].median()\n\n","220820ab":"\ndf_train[df_train['Age'].isnull()]['Age'] = df_train.groupby(['Pclass','Sex','Title'])['Age'].transform('median')\ndf_test[df_test['Age'].isnull()]['Age'] = df_test.groupby(['Pclass','Sex','Title'])['Age'].transform('median')\n","e2cf8d90":"fig = plt.figure(figsize=(15,5))\n\nax1 = fig.add_subplot(121)\n_ = sns.distplot(age_surv[\"Age\"], bins=24, color='#f5dd90', ax=ax1)\n_ = ax1.set_title('Survived', fontsize=20)\n_ = ax1.set_xlabel(\"Age\",fontsize=15)\n_ = ax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(122)\n_ = sns.distplot(age_died[\"Age\"], bins=24, color='#0d3b66', ax=ax2)\n_ = ax2.set_title('Not Survived', fontsize=20)\n_ = ax2.set_xlabel(\"Age\",fontsize=15)\n_ = ax2.set_ylabel(\"\")\n","130285cd":"#creating the intervals that we need to cut each range of ages\ninterval = (0, 5, 12, 18, 25, 35, 60, 120) \n\n#Seting the names that we want use to the categorys\ncategories = ['babies', 'Small Children', 'Big Children','Teen', 'Mid Aged', 'Full Aged', 'Senior']\n\n# Applying the pd.cut and using the parameters that we created \ndf_train[\"Age_cat\"] = pd.cut(df_train.Age, interval, labels=categories)   \n\ndf_test[\"Age_cat\"] = pd.cut(df_test.Age, interval, labels=categories) ","5e3a25b6":"group = df_train.groupby(['Age_cat','Survived']).size().rename('Count').reset_index()\n\nfig  = px.histogram(group, \n              x='Age_cat', \n              y='Count',\n              color='Survived',\n              color_discrete_sequence=[\"#457b9d\",\"#fca311\"],\n              template='plotly_white')\n\nfig.update_layout(width=900, height=400, \n                  barmode='group',\n                  title= {'text': \"Age Group Survival\",\n                          'y':0.95,'x':0.5,\n                          'xanchor': 'center',\n                          'yanchor': 'top'},\n                 showlegend=True,\n                 margin = dict(l=25, r=10, t=50, b=10))\n                 \n                 \nfig.show()","92a117a6":"fare_surv = df_train[(df_train['Fare'] > 0) & (df_train['Survived'] == 1)]\nfare_died = df_train[(df_train['Fare'] > 0) & (df_train['Survived'] == 0)]\n\n#figure size\nplt.figure(figsize=(15,5))\n\n# Ploting the 2 variables that we create and compare the two\nsns.distplot(fare_surv[\"Fare\"], bins=24, color='#f98948')\nsns.distplot(fare_died[\"Fare\"], bins=24, color='#9ba2ff')\nplt.title(\"Distribuition of Fare\",fontsize=20)\nplt.xlabel(\"Fare\",fontsize=15)\nplt.ylabel(\"\")\nplt.show()","a0734795":"plt.figure(figsize=(12,5))\n\nsns.boxenplot(x=\"Survived\", y = 'Fare', \n              data=df_train[df_train['Fare'] > 0], palette=['#436a36','#802c3e']) \nplt.title(\"Fare Quartiles\", fontsize=20) \nplt.xlabel(\"Survival\", fontsize=18) \nplt.ylabel(\"Fare\", fontsize=16) \nplt.xticks(fontsize=18)","bc260d33":"fig = px.box(y = 'Fare',\n              data_frame=df_train[(df_train['Fare'] > 0) & (df_train['Fare'] < 250)],\n            color_discrete_sequence=[\"#262a10\"],\n              template='plotly_white') \n\nfig.update_layout(width=900, height=400, \n                  title= {'text': \"Fare distribution\",\n                          'y':0.95,'x':0.5,\n                          'xanchor': 'center',\n                          'yanchor': 'top'},\n                 margin = dict(l=25, r=10, t=50, b=10))\n                 \n                 \nfig.show()","8ce59b65":"#Filling the NA's with -0.5\ndf_train['Fare'] = df_train['Fare'].fillna(-0.5)\n\n#intervals to categorize\n\n# -1 to 0 for Fare = -0.5\/No information(null)\n\nquant = (-1, 0, 8, 15, 31, 64, 600)\n\n#Quartiles\nlabel_quants = ['NA', 'Q1', 'Q2', 'Q3', 'Q4','UpperFence']\n\n\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\n\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","a459a7af":"group = df_train.groupby(['Fare_cat','Survived']).size().rename('Count').reset_index()\n\nfig  = px.histogram(group, \n              x='Fare_cat', \n              y='Count',\n              color='Survived',\n              color_discrete_sequence=[\"#3581b8\",\"#fcb07e\"],\n              template='plotly_white')\n\nfig.update_layout(width=900, height=400, \n                  barmode='group',\n                  title= {'text': \"Fare Quartiles Survival\",\n                          'y':0.95,'x':0.5,\n                          'xanchor': 'center',\n                          'yanchor': 'top'},\n                 showlegend=True,\n                 margin = dict(l=25, r=10, t=50, b=10))\n                 \n                 \nfig.show()","ab1cf67f":"cols = ['Name','Ticket','Fare','Cabin','Age']\n\ndef drop_col(df):\n    for column in cols:\n        df.drop(column, axis=1, inplace=True)\n\n# Training Data\ndrop_col(df_train)\n\n#Test Data\ndrop_col(df_test)\n","8d102e41":"df_train.head()","5cc428ac":"fig = plt.figure(figsize=(20,20))\n\nax1 = fig.add_subplot(321)\n_ = sns.countplot(df_train['Survived'], palette=['#9fb8ad','#475841'], ax=ax1)\n_ = ax1.set_title('Survived', fontsize=20)\n_ = ax1.set_xlabel(\"\")\n_ = ax1.set_ylabel(\"\")\n_ = ax1.set_xticklabels(['No','Yes'], fontsize=13)\n\n\nax2 = fig.add_subplot(322)\n_ = sns.countplot(data=df_train, x='Sex',hue='Survived',palette=['#9fb8ad','#475841'], ax=ax2)\n_ = ax2.set_title('Sex', fontsize=20)\n_ = ax2.set_xlabel(\"\")\n_ = ax2.set_ylabel(\"\")\n_ = ax2.set_xticklabels(['Male','Female'], fontsize=13)\n\nax3 = fig.add_subplot(323)\n_ = sns.countplot(data=df_train, x='Pclass',hue='Survived',palette=['#9fb8ad','#475841'], ax=ax3)\n_ = ax3.set_title('Passenger Class', fontsize=20)\n_ = ax3.set_xlabel(\"\")\n_ = ax3.set_ylabel(\"\")\n_ = ax3.set_xticklabels(['First','Second','Third'], fontsize=13)\n\nax4 = fig.add_subplot(324)\n_ = sns.countplot(data=df_train, x='SibSp',hue='Survived',palette=['#9fb8ad','#475841'], ax=ax4)\n_ = ax4.set_title('Siblings & Spouses', fontsize=20)\n_ = ax4.set_xlabel(\"\")\n_ = ax4.set_ylabel(\"\")\n#_ = ax3.set_xticklabels(['First','Second','Third'], fontsize=13)\n\nax5 = fig.add_subplot(325)\n_ = sns.countplot(data=df_train, x='Embarked',hue='Survived',palette=['#9fb8ad','#475841'], ax=ax5)\n_ = ax5.set_title('Embarked', fontsize=20)\n_ = ax5.set_xlabel(\"\")\n_ = ax5.set_ylabel(\"\")\n_ = ax5.set_xticklabels(['Southampton','Cherbourg','Queenstown'], fontsize=13)\n\n\nax6 = fig.add_subplot(326)\n_ = sns.countplot(data=df_train, x='Parch',hue='Survived',palette=['#9fb8ad','#475841'], ax=ax6)\n_ = ax6.set_title('Parents & Children', fontsize=20)\n_ = ax6.set_xlabel(\"\")\n_ = ax6.set_ylabel(\"\")\n#_ = ax5.set_xticklabels(['Southampton','Cherbourg','Queenstown'], fontsize=13)","a4e3b43e":"_ = sns.factorplot(x=\"Sex\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5,aspect= 1, palette = [\"#456990\",\"#ef8354\"])\n_ = plt.ylabel(\"Probability(Survive)\", fontsize=15)\n_ = plt.xlabel(\"Sex\", fontsize=15)\n_ = plt.show()\n","ed810138":"_ = sns.factorplot(x=\"Title\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5,aspect= 2, palette = [\"#114b5f\",\"#028090\",\"#e4fde1\",\"#456990\",\"#f45b69\"])\n_ = plt.ylabel(\"Probability(Survive)\", fontsize=15)\n_ = plt.xlabel(\"Title\", fontsize=15)\n\n_ = plt.show()","dc49c1d2":"_ = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5,aspect= 2.5, palette = [\"#5f0f40\",\"#9a031e\",\"#fb8b24\",\"#e36414\",\"#0f4c5c\"])\n_ = plt.ylabel(\"Probability(Survive)\", fontsize=15)\n_ = plt.xlabel(\"SibSp Number\", fontsize=15)\n\n_ = plt.show()","9759c778":"_ = sns.factorplot(x=\"Parch\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5,aspect= 2.5, palette = [\"#5f0f40\",\"#9a031e\",\"#fb8b24\",\"#e36414\",\"#0f4c5c\"])\n_ = plt.ylabel(\"Probability(Survive)\", fontsize=15)\n_ = plt.xlabel(\"Parch Number\", fontsize=15)\n\n_ = plt.show()","c651bd8b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n\nrf = RandomForestClassifier()\n\nparams = {\n                'max_depth' : [1,2,3,4,5,6],\n               'min_samples_leaf' : [0.01,0.02,0.04,0.06],\n                'max_features' : [0.1,0.2,0.4,0.8],\n                'n_estimators' : [100,150,200,250,300]\n                \n        }\n\nrf_cv = RandomizedSearchCV(estimator=rf,\n                          param_distributions=params,\n                           n_iter=100,\n                          cv=10,\n                          scoring='accuracy',\n                          n_jobs=-1,\n                           verbose=3\n                          )\nX = df_train.drop(['PassengerId','Survived'], axis=1)\ny = df_train['Survived']\n\n# Label Encoding\nfor col in ['Embarked','Title','Age_cat','Fare_cat','Sex']:\n    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n\n# Training\nrf_cv.fit(X, y)\n\n\n#Best Estimator\nrf_best = rf_cv.best_estimator_","5ece4cb9":"# Get feature importance\nselected_features = X.columns.to_list()\nfeature_importance = pd.DataFrame(selected_features, columns = [\"Feature Label\"])\nfeature_importance[\"Feature Importance\"] = rf_best.feature_importances_\n\n# Sort by feature importance\nfeature_importance = feature_importance.sort_values(by=\"Feature Importance\", ascending=False)\n\n# Set graph style\nsns.set(font_scale = 1)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\n\n# Set figure size and create barplot\nf, ax = plt.subplots(figsize=(12, 5))\nsns.barplot(x = \"Feature Importance\", y = \"Feature Label\",\n            palette = reversed(sns.color_palette('winter', 15)),  data = feature_importance)\n\n# Generate a bolded horizontal line at y = 0\nax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n\n# Turn frame off\nax.set_frame_on(False)\n\n# Tight layout\nplt.tight_layout()\n\n# Save Figure\nplt.savefig(\"feature_importance.png\", dpi = 1080)","f61f0350":"import eli5 \nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf_best, random_state=105).fit(X, y)\neli5.show_weights(perm, feature_names = X.columns.to_list())","220a5518":"from sklearn.tree import export_graphviz\n\nestimator = rf_best.estimators_[10]\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = selected_features,\n                class_names = ['Not Survived','Survived'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","09943673":"import shap \n\nexplainer = shap.TreeExplainer(rf_best)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values[1], X, plot_type=\"bar\")","d7fd171d":"shap.summary_plot(shap_values[1], X)","0c46cab1":"df_train = pd.get_dummies(df_train, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                          prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)\n\ndf_test = pd.get_dummies(df_test, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                         prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)","c996c8d7":"df_train.head(3)","17268c46":"from sklearn.model_selection import train_test_split\n\n# For Training & Validation\nX = df_train.drop(['PassengerId','Survived'], axis=1)\ny = df_train['Survived']\n\n\n# For submission.csv\nX_test = df_test.drop(['PassengerId'], axis=1)\n\n\n\n# Creating train & validation sets, stratified sampling on Survived(dataframe = y)\nX_train, X_val, y_train, y_val = train_test_split(X, y, \n                                                 test_size=0.25, \n                                                 random_state=123,\n                                                 stratify=y)","77cfcd51":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(X)\n\nX_train = scaler.fit_transform(X_train)\n\nX_val = scaler.fit_transform(X_val)\n\nX_test = scaler.fit_transform(X_test)","fa5ba853":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegression(),\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]","8d6031da":"from sklearn.model_selection import ShuffleSplit, KFold\nfrom sklearn.model_selection import cross_validate\n\nshuffle_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 123 )\nkfold = KFold(n_splits=10, random_state=123)\n\n\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nresults, names  = [], [] \n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    \n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    cv_results = cross_validate(alg, X, y, cv  = kfold)\n    \n    names.append(MLA_name)\n    results.append(cv_results['test_score'])\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","a3e26c9e":"# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names[:10], y=results[:10])\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","0af21fa1":"# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names[10:], y=results[10:])\nax.set_xticklabels(names[10:])\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","23e7b234":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nC = np.logspace(-4,4,9)\ntol = np.logspace(-6,2,9)\n\n\nparam_grid = {'C':C, 'tol':tol}\n\nsvc_model = SVC(probability=True)\nsvc_grid = GridSearchCV(svc_model, param_grid=param_grid, cv=kfold, verbose=1)\n\nsvc_grid.fit(X, y)\n\nsvc_best = svc_grid.best_estimator_","18150ba5":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n","28ced8aa":"from sklearn.metrics import classification_report, confusion_matrix\ny_pred = svc_best.predict(X_val)\n\ny_pred_prob = svc_best.predict_proba(X_val)[:,1]\n\nprint(confusion_matrix(y_val, y_pred))\n\nprint(classification_report(y_val, y_pred))\n\ncnf_matrix = confusion_matrix(y_val, y_pred)\nclass_names = ['Did not Survive','Survived']\nnp.set_printoptions(precision=2)\n\n\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","5f396e89":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Titanic classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","f98c99a8":"auc(fpr,tpr)","96117717":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom keras.layers import BatchNormalization, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import RMSprop\n\n\n# MODEL\nmodel = Sequential()\n\n\n# LAYERS\n# 1.\nmodel.add(Dense(18,input_shape=(X_train.shape[1],),activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\n\n# 2.\nmodel.add(Dense(60, activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\n\n# OutPut layer\nmodel.add(Dense(1, activation='sigmoid', kernel_initializer='uniform'))\n\n\n\n\n\n# COMPILE\nmodel.compile(optimizer='adam',\n             loss='binary_crossentropy',\n              metrics=['accuracy'],\n             )\n\n\n\n# CALLBACKS\nearlystop = EarlyStopping(monitor='val_loss',patience = 20)\nmodcheck = ModelCheckpoint('check_pt1.hdf5', save_best_only=True)","da2b835c":"# TRAIN\n\nhistory= model.fit(X_train, y_train,\n                     validation_data=(X_val, y_val),\n                     callbacks=[earlystop,modcheck],\n                     epochs=500,\n                     batch_size=30,\n                     verbose=2)","a58c5097":"# summarizing historical accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","35759ecc":"# summarizing historical accuracy\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a3477408":"                                                  Titanic ML from Disaster","48b90890":"**Distribution of Age based on Survival**","c8bee9a4":"**Filling the null Age values based on above different groups**","ce168155":"**Survival is definitely higher for females**","89e70386":"**Survived vs Did not Survive**","b2546fc3":"**Shap Analysis:**\nValues in Red indicate High Values \n    * Sex = 1(high), Sex = 0(Low)\n    * Pclass = 3 (high), Pclass = 1 (low)\n\nValues present on the X-axis are Shap values (Higher\/+ve value means positive impact on Survival)\n\n**Observations:**\n        1. Sex      :  Sex = 0\/Female\/Blue have higher chances of Survival\n        2. Pclass   :  Lower values of Pclass(i.e First class) have higher chances of survival","3cf52250":"### Column Description:\n\n\n            1. survival : Survival 0 = No, 1 = Yes\n            2. pclass   : Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n            3. sex      : Sex\n            4. Age      : Age in years\n            5. sibsp    : # of siblings \/ spouses aboard the Titanic\n            6. parch    : # of parents \/ children aboard the Titanic\n            7. ticket   : Ticket number\n            8. fare     : Passenger fare\n            9. cabin    : Cabin number\n            10. embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n\n* pclass: A proxy for socio-economic status (SES)\n        1st = Upper\n        2nd = Middle\n        3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n        Sibling = brother, sister, stepbrother, stepsister\n        Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n        Parent = mother, father\n        Child = daughter, son, stepdaughter, stepson\n        Some children travelled only with a nanny, therefore parch=0 for them.","68962ef7":"**It seems that not many with title Mr. had survived**","f56e2cf3":"### 2. Reading Files","fbe201e3":"Plotting Loss","4568ea7c":"**Most Common Age based on Class,Sex & Title**","2034db64":"### SVC seemed to have performed the best, hence let's optimize this model","a6c8c005":"**Three thrre most import Features that are responsible for identifying Survival are:**\n            1. Sex\n            2. Pclass\n            3. title","ba2573bd":"**One Hot Encoding**","a6b9163c":"**Probability\/Chances of Survival**","cba93795":"**Grouping Titles**","f20c7401":"ROC curve","ffb48aab":"**Using Permutation importance**","d94875ed":"### 8. Modelling","4001fbe5":"**Function to describe columns in detail.**","f4fc4c80":"**Shap Analysis**","950635ea":"### 1. Importing Libraries","0260f569":"### III. Working on Fare","58c873a8":"**Categorizing Age**","ff788b50":"**Test Data**","fa81a5d8":"**Here also we have the same top three features that are responsible for survival**","4ebd4aca":"### 6. Feature Importance using RForrest, Permutation, Shap Analysis","8cd2904f":"### 5. Exploring the Dataset","16e563e8":"### Observations From Above box plot:\n      1. Quartile 1  = 7.92\n      2. Median      = 14.4\n      3. Quartile 3  = 30.6\n      4. Upper Fence = 63.3","a805a9c5":"### Observations from train and test Dataset:\n\n        1. Survived  has two unique values (1&0) which is present in the train data set and absent in the test dataset\n        2. Pclass has 3 unique values \n        3. Age column has high entropy which means that the age values range very high and low values\n        4. Same is the case with Fare\n        5. Embarked column has 3 unique values\n\nTrain data:\n\n        * Age has 177 missing values\n        * Embarked has 2 missing values\n        * Cabin has 687 missing values\n\nTest data:\n\n        * Age has 86 missing values\n        * Fare has 1 missing value\n        * Cabin has 327 missing values ","91210dcd":"**Training Data**","6c024032":"**Information on different Quartitles of Fare**","bdb9ea80":"**Creating Different Fare Categories from Above observations**","42ca5f67":"# Don't Forget to give a thumbs up !!!! Thanks for Viewing.\n","eaa27114":"### 4. Cleaning and Feature Engineering","01ae2bcc":"**Random Forrest**","a87d423a":"### 3. Knowing the two datasets","8cf2e072":"**Splitting**","3599471c":"### 7. Preprocessing","ff1e01b4":"Plotting Accuracy","6736c37a":"### I. Working on Name","6b2c0881":"![image.png](attachment:image.png)","89453be6":"### IV. Dropping Columns that are not required\n\n            1. Name   : We have created Title from Name\n            2. Fare   : We have created different Fare Quartiles\n            3. Age    : We have created a new feature based of age groups\n            4. Ticket : Does not have any information which could be used by our Models\n            5. Cabin  : A lot of Null Values, hence removing it","ffd34322":"        Area Under the Curve\n    Score,\n    Average = 60-70\n    Good = 70-85\n    Very good = 85-95\n    Exceptional = 95+","9afc40b4":"### II. Working on Age","5d5d7915":"### Keras (ANN)","12a0e858":"**Extracting Title from Name**","3265a79f":"**Scaling**","90f28a71":"### This kernel will provide an analysis through the Titanic Disaster to understand the Survivors patterns\n\nWe will be deeply analyze what were the paramaters that led to deplorable deaths of the passengers using Random Forrest, Permutation Importance and Shap algorithms.\n\nI will handle with data (transform, missings, manipulation), explore the data (descritive and visual) and also create a Deep Learning model\n\nPlease give a thumbs up if you like the presentations in the Notebook","8b410b92":"**Plot Confusion Matrix**","9e02ae02":"**Viewing one of the trees from the Forrest**","5b28454b":"**Survival is higher for people having 1 or 2 siblings**","329dee91":"**Choosing the model that performs best on the Dataset**"}}