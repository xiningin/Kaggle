{"cell_type":{"4cee06fe":"code","96c9ce29":"code","9aa86cb2":"code","91ff2d33":"code","8b8f901b":"code","1be4da4a":"code","c19ffbce":"code","3b5f8998":"code","43dee6fd":"code","3c32fdde":"code","165e0de7":"code","ecadc169":"code","66b34b45":"code","1d32ca4b":"code","7687212c":"code","69a82a37":"code","1f76ca93":"code","88c99bc0":"code","83b521cd":"code","9e941396":"code","33814ea3":"code","40b8f4d5":"code","96841dac":"code","9238f9ce":"code","6ceb8df4":"code","ffe6e3dd":"code","66236199":"code","99d7d163":"code","7e5e2b53":"code","d9444d7d":"code","4a159f8a":"code","1cf2ab43":"code","4b85ef8c":"code","4aa405f1":"code","1c8ee8d0":"code","dd0b023a":"code","dce51047":"code","5198126a":"code","3a62bb20":"code","8057e4e4":"code","da553cc9":"code","a2e1db86":"code","de08832d":"code","fe2fef23":"markdown","4990ccfc":"markdown","0c00773d":"markdown","4b3c35b9":"markdown","327e043a":"markdown","66ffd9c9":"markdown","6778d30c":"markdown","93a6e68b":"markdown","88a27c7b":"markdown","a90fb071":"markdown","c18435ea":"markdown","e25c495c":"markdown","c58fe944":"markdown","922cd586":"markdown","d825be4b":"markdown","68fbe874":"markdown","3c28c6a9":"markdown","1e0b697b":"markdown","9d135d4b":"markdown","908dca85":"markdown","07862c60":"markdown","9255cb25":"markdown"},"source":{"4cee06fe":"!pip install IQA_pytorch","96c9ce29":"USE_TPU = False","9aa86cb2":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nfrom torch.optim import *\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom IQA_pytorch import DISTS, utils\n\n#TPU \ud559\uc2b5\uc6a9\nif USE_TPU:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.xla_multiprocessing as xmp\n\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import font_manager, rc\nfrom IPython import display\nimport random\nimport glob\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport warnings\nimport sys\nfrom tqdm import tqdm\nimport pickle\nimport gc\nimport random\n\n\nwarnings.filterwarnings(\"ignore\")","91ff2d33":"gc.collect()\ntorch.cuda.empty_cache()","8b8f901b":"%matplotlib inline\n\nplt.rcParams['axes.unicode_minus'] = False\nfontpath = \"..\/input\/koreanfont\/NanumBrush.ttf\"\nfontprop = font_manager.FontProperties(fname=fontpath)\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nplt.rcParams['figure.dpi'] = 150  \nplt.ioff()","1be4da4a":"#\ud559\uc2b5 \uae30\uae30 \uad00\ub828\nUSE_CUDA = torch.cuda.is_available()\n\nprint(\"\ud559\uc2b5 \uae30\uae30 : {0}\".format(\"GPU\" if USE_CUDA else \"CPU\"))\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\ncpu_device = torch.device(\"cpu\")\n\nDEBUG = False\n\nRANDOM_SEED = 2004\n\n# \ud559\uc2b5 \uad00\ub828\nstart_epoch = 0\nall_epochs = 1\nbatch_size = 14\nTESTNUM = 5\n\nlrG = 0.0002\nlrD = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\nL1lambda = 100\nGAMMA = 0 # if gamma is zero, the neural network is normal pix2pix\n\nTIME_STEP = 4\nTEST_TIME_STEP = 8\n\npatch = (1,256\/\/2**4,256\/\/2**4)\n\n#\uacbd\ub85c \uad00\ub828\nDATASET1_PATH = '..\/input\/the-cloudcast-dataset'\n\n#\uccb4\ud06c\ud3ec\uc778\ud2b8 \uad00\ub828\nUSE_CHECKPOINT = False\n\nOLD_PATH = '..\/input\/pix2pix-compared-to-oraclegan'\nOLD_GENERATOR_MODEL = os.path.join(OLD_PATH, 'Generator.pth')\nOLD_DISCRIMINATOR_MODEL = os.path.join(OLD_PATH, 'Discriminator.pth')\nOLD_G_LOSS = os.path.join(OLD_PATH, 'gloss.txt')\nOLD_D_LOSS = os.path.join(OLD_PATH, 'dloss.txt')","c19ffbce":"torch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\nprint('Random Seed : {0}'.format(RANDOM_SEED))","3b5f8998":"def log(text):\n    global DEBUG\n    if DEBUG:\n        print(text)","43dee6fd":"def torch_tensor_to_plt(img):\n    img = img.detach().numpy()[0]\n    img = np.transpose(img, (1, 2, 0))\n    return img ","3c32fdde":"def plt_image_animation(frames, update_func):\n    fig, ax = plt.subplots(figsize=(4,4))\n    plt.axis('off')\n    anim = animation.FuncAnimation(fig, update_func, frames=frames)\n    video = anim.to_html5_video()\n    html = display.HTML(video)\n    display.display(html)\n    plt.close()","165e0de7":"plt_image_animation(15, lambda t : plt.imshow(np.load(join(DATASET1_PATH, '2017M01', '{0}.npy'.format(t))), cmap='gray'))","ecadc169":"transformer = transforms.Compose([transforms.ToTensor(),\n                                  torchvision.transforms.Resize(128),\n                                  transforms.Normalize((0.5), (0.5)),\n                                 ])\n","66b34b45":"nowpath = \"\"\n\nclass TimeStepImageDataset(Dataset):\n    def __init__(self, date, time_step, transform=None):\n        self.date = date\n        self.time_step = time_step\n        self.transformer = transform\n        self.file = []\n        \n        file_list = glob.glob(join(self.date, '*'))\n        self.file = [file for file in file_list if (file.endswith(\".npy\") and not file.endswith('TIMESTAMPS.npy'))]\n        \n    def __len__(self):\n        return len(self.file)-self.time_step     \n    \n    def transform(self, image):\n        if self.transformer:\n            return self.transformer(image)\n        else :\n            return image\n\n    def __getitem__(self, idx):\n        global nowpath\n        log(join(self.date, str(idx)+'.npy'))\n        X = self.transform(np.load(join(self.date, str(idx)+'.npy')))\n        nowpath = join(self.date, str(idx)+'.npy')\n        Y_list = []\n        for i in range(1, self.time_step+1):\n            Y_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        Y = torch.cat(Y_list)       \n        return X, Y","1d32ca4b":"DATASET1_DIRS = glob.glob(join(DATASET1_PATH, '*'))\n\nrandom.shuffle(DATASET1_DIRS)\n\ntraindatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[:20]):\n    traindatasetlist.append(TimeStepImageDataset(name, TIME_STEP, transform=transformer))\ntrain_dataset = torch.utils.data.ConcatDataset(traindatasetlist)\n\ntestdatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[20:]):\n    testdatasetlist.append(TimeStepImageDataset(name, TEST_TIME_STEP, transform=transformer))\ntest_dataset = torch.utils.data.ConcatDataset(testdatasetlist)","7687212c":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataloader_bs1_shuffle = DataLoader(test_dataset, batch_size=1, shuffle=True)\ntest_dataloader_bs1_noshuffle = DataLoader(test_dataset, batch_size=1, shuffle=False)","69a82a37":"def ShowDatasetImage(x, y):\n    grid = torchvision.utils.make_grid(y)\n\n    fig = plt.figure(figsize=(2, 2))\n    plt.imshow(torch_tensor_to_plt(x.unsqueeze(0)), cmap='gray')\n    plt.axis('off')\n    plt.title('\uc785\ub825 (\ud604\uc7ac \uae30\uc0c1 \uc601\uc0c1)', fontproperties=fontprop)\n    plt.show()   \n    \n    fig = plt.figure(figsize=(8, 2.5))\n    plt.title('\uc2e4\uc81c \ub2f5\uc548', fontproperties=fontprop)\n    plt.axis('off')\n    for i in range(1, TIME_STEP+1):\n        ax = fig.add_subplot(1, TIME_STEP, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(y[i-1].unsqueeze(0)), cmap='gray')\n        ax.set_title('{0}\ubd84 \ud6c4\uc758 \uae30\uc0c1 \uc601\uc0c1'.format(15*i), fontproperties=fontprop)\n    plt.show()\n\n    del x, y","1f76ca93":"for ind, (x, y) in enumerate(train_dataset):\n    if ind != 0:\n        continue\n    ShowDatasetImage(x, y)\n    break","88c99bc0":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n        super().__init__()\n\n        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_channels)),\n\n        layers.append(nn.LeakyReLU(0.2))\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.down = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.down(x)\n        return x","83b521cd":"class UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0):\n        super().__init__()\n\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU()\n        ]\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.up = nn.Sequential(*layers)\n\n    def forward(self,x,skip):\n        x = self.up(x)\n        x = torch.cat((x,skip),1)\n        return x","9e941396":"class GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64,128)                 \n        self.down3 = UNetDown(128,256)               \n        self.down4 = UNetDown(256,512,dropout=0.5) \n        self.down5 = UNetDown(512,512,dropout=0.5)      \n        self.down6 = UNetDown(512,512,dropout=0.5)             \n        self.down7 = UNetDown(512,512,dropout=0.5)              \n        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5)\n\n        self.up1 = UNetUp(512,512,dropout=0.5)\n        self.up2 = UNetUp(1024,512,dropout=0.5)\n        self.up3 = UNetUp(1024\/\/2,512,dropout=0.5)\n        self.up4 = UNetUp(1024,512,dropout=0.5)\n        self.up5 = UNetUp(1024,256)\n        self.up6 = UNetUp(512,128)\n        self.up7 = UNetUp(256,64)\n        self.up8 = nn.Sequential(\n            nn.ConvTranspose2d(128,out_channels,4,stride=2,padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        #d7 = self.down7(d6)\n        #d8 = self.down8(d7)\n        u2 = d6\n        #u1 = self.up1(d8,d7)\n        #u2 = self.up2(u1,d6)\n        u3 = self.up3(u2,d5)\n        u4 = self.up4(u3,d4)\n        u5 = self.up5(u4,d3)\n        u6 = self.up6(u5,d2)\n        u7 = self.up7(u6,d1)\n        u8 = self.up8(u7)\n        \n        return u8","33814ea3":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.relu = nn.ReLU(False)\n\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out","40b8f4d5":"class Discriminator(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=1):\n        super(Discriminator, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear1 = nn.Linear(1*1*512*block.expansion, 1024)\n        self.linear2 = nn.Linear(1024, num_classes)\n\n        self.relu = nn.ReLU(False)\n        self.sigmoid = nn.Sigmoid()\n        self.avg_pool2d = nn.AvgPool2d(16, 16)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool2d(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear1(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n        return out","96841dac":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) == nn.Conv2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","9238f9ce":"Generator = GeneratorUNet().to(device)\nDiscriminator = Discriminator(BasicBlock, [3, 4, 6, 3]).to(device)\n\nsummary_g = Generator.apply(weights_init)\nsummary_d = Discriminator.apply(weights_init)","6ceb8df4":"optimizerG = Adam(Generator.parameters(), lr=lrG, betas=(beta1, beta2))\noptimizerD = Adam(Discriminator.parameters(), lr=lrD, betas=(beta1, beta2))","ffe6e3dd":"img_list = []\nG_loss = []\nD_loss = []\n\nFAKE_LABEL = 0.0\nREAL_LABEL = 1.0","66236199":"l1loss = nn.L1Loss()\nbceloss = nn.BCELoss()","99d7d163":"def generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=0.0):\n    def G_error(G_output, real, D_output):\n        return l1loss(G_output, real)*L1lambda + bceloss(D_output, real_label)\n    \n    next_input = sketch\n    error = None\n    \n    real_list = []\n    for i in range(TIME_STEP):\n        real_list.append(real[:,i,:,:,:])\n    \n    for ind, y in enumerate(real_list):\n        G_output = netG(next_input)\n        next_input = G_output.clone().detach()\n        D_output = netD(G_output).view(-1)     \n        \n        if ind==0:\n            error = G_error(G_output, y, D_output)\n        else :\n            error += (gamma ** ind) * G_error(G_output, y, D_output)\n            \n        del G_output, D_output\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    return error","7e5e2b53":"def discriminator_error(netG, netD, sketch, real, real_label, fake_label, avg=True):\n    output_g = netG(sketch)\n    outputs_fake = netD(output_g.detach()).view(-1)\n    errD = bceloss(outputs_fake, fake_label)\n    \n    del output_g, outputs_fake\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(real[:,i,:,:]).view(-1)\n        if avg:\n            errD += bceloss(outputs_real, real_label)\/TIME_STEP\n        else:\n            errD += bceloss(outputs_real, real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","d9444d7d":"def generate(use_checkpoint=True):\n    global Generator, Discriminator, optimizerG, optimizerD, G_Loss, D_Loss, start_epoch\n    \n    if os.path.isdir(OLD_PATH) and use_checkpoint:        \n        checkpoint = torch.load(OLD_GENERATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Generator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerG.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        checkpoint = torch.load(OLD_DISCRIMINATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Discriminator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerD.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        with open(OLD_G_LOSS, 'rb') as f:\n            G_loss = pickle.load(f)\n            \n        with open(OLD_D_LOSS, 'rb') as f:\n            D_loss = pickle.load(f)\n        \n        print('\uc774\uc804 \ud559\uc2b5 \uacb0\uacfc\ub97c \uc774\uc5b4\uc11c \ud559\uc2b5\ud569\ub2c8\ub2e4.')\n    else :\n        print('\ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ud569\ub2c8\ub2e4.')","4a159f8a":"nowepoch = 0\nstrange_error_limit = 10\nstrange_error_num = 0\n\ndef fit(device, num_epochs=1000):\n    global nowepoch\n    print(\"\ud559\uc2b5\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4.\")\n    iters = 0\n    for epoch in range(start_epoch+1, num_epochs+start_epoch+1):\n        nowepoch = epoch\n        print(f\"EPOCH{epoch}:\")\n        result = train_one_epoch(device, train_dataloader, Generator, Discriminator, optimizerG, optimizerD, epoch, num_epochs)\n        if not result:\n            return\n    \n\ndef train_one_epoch(device, dataloader, netG, netD, optimizerG, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n            \n            #Train Discriminator\n            netG.eval()\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error(netG, netD, sketch, real, real_label, fake_label)\n            \n\n            \n            log('D \uc190\uc2e4 \uacc4\uc0b0 \uc644\ub8cc')\n            errD.backward()\n            log('D \uc5ed\uc804\ud30c \uc644\ub8cc')\n            optimizerD.step()\n            if USE_TPU:\n                xm.optimizer_step(optimizerD, barrier=True)\n            log('\uc635\ud2f0\ub9c8\uc774\uc800 D \uc2a4\ud0ed \uc644\ub8cc')\n        \n            #Train Generator\n            netG.train()\n            netD.eval()\n            netG.zero_grad()\n            \n            \n            errG = generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=GAMMA)\n\n            \n            log('G \uc190\uc2e4 \uacc4\uc0b0 \uc644\ub8cc')\n            errG.backward()\n            log('G \uc5ed\uc804\ud30c \uc644\ub8cc')\n            optimizerG.step()\n            if USE_TPU:\n                xm.optimizer_step(optimizerG, barrier=True)\n            log('\uc635\ud2f0\ub9c8\uc774\uc800 G \uc2a4\ud0ed \uc644\ub8cc')\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d\/%d][%d\/%d]\\tLoss_G: %.4f\\tLoss_D: %.4f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errG.item(), errD.item()))\n                \n            \n\n            G_loss.append(errG.item())\n            D_loss.append(errD.item())\n            \n            del errG, errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return True","1cf2ab43":"generate(use_checkpoint=USE_CHECKPOINT)","4b85ef8c":"summary = Generator.train()\nsummary = Discriminator.train()\n\nif all_epochs>0:\n    fit(device, num_epochs=all_epochs)\n\nsummary = Generator.eval()\nsummary = Discriminator.eval()","4aa405f1":"plt.figure(figsize=(10,5))\nplt.title('Loss of Generator')\nplt.plot(G_loss,label=\"\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","1c8ee8d0":"plt.figure(figsize=(10,5))\nplt.title('Loss of Discriminator')\nplt.plot(D_loss,label=\"train\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","dd0b023a":"def model_predict(model, time, input):\n    if time%15==0 and time!=0:\n        model.eval()\n        num = time\/\/15\n        \n        next_input = input\n        for i in range(num):\n            next_input = model(next_input).clone().detach()\n        return next_input\n    else:\n        raise ValueError('Please set the time to a multiple of 15.')","dce51047":"from IQA_pytorch import SSIM, utils\n\ntoPILImage = transforms.ToPILImage()\nssim_model = SSIM(channels=1)\n\ndef one_time_step_ssim_score(dataloader, model, time_step, num=-1):\n    model.eval()\n    score = 0\n    total = 0\n    for ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n        x, y = x.squeeze(0).to(device), y.squeeze(0).to(device)\n        outputG = model_predict(model, time_step*15, x.unsqueeze(0))\n\n        sketch = utils.prepare_image(toPILImage(outputG.squeeze(0))).to(device)\n        real = utils.prepare_image(toPILImage(y[time_step-1])).to(device)\n\n        score += ssim_model(sketch, real, as_loss=False).item()\n        total += 1\n\n        del x, y, outputG, sketch, real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        if num != -1:\n            if ind+1 >= num:\n                break\n            \n    print(\"SSIM Score of the prediction {0} minutes later : {1}\".format(time_step*15, score\/total))\n    return score\/total\n\nfor ind in range(1, TEST_TIME_STEP+1):\n    one_time_step_ssim_score(test_dataloader_bs1_shuffle, Generator, ind, num=2000)","5198126a":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_noseries_ls = []\nreal_noseries_ls = []\n\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n        \n    outputg = Generator(x).to(cpu_device)\n    \n    outputg = outputg*127.5+127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg)*30)\n    cv2.imwrite('.\/Real_NOSERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_noseries_ls.append('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1))\n    real_noseries_ls.append('.\/Real_NOSERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_noseries.zip\", 'w') as my_zip:\n    for i in ai_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n\n\nwith zipfile.ZipFile(\"real_noseries.zip\", 'w') as my_zip:\n    for i in real_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \nfor file in (ai_noseries_ls + real_noseries_ls):\n    os.remove(file)\n    \nprint('NOSERIES \uc0dd\uc131 \uc644\ub8cc')","3a62bb20":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_series_ls = []\nreal_series_ls = []\n\nnext_input = None\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_noshuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    if ind == start_ind:\n        next_input = x.clone().detach().to(device)\n        cv2.imwrite('.\/Input_SERIES.png', torch_tensor_to_plt(next_input.to(cpu_device)*127.5+127.5)*30)\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    outputg_series = Generator(next_input).to(cpu_device)\n    next_input = outputg_series.clone().detach().to(device)\n\n    outputg_series = outputg_series * 127.5 + 127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_SERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg_series)*30)\n    cv2.imwrite('.\/Real_SERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_series_ls.append('.\/AI_SERIES_Answer{0}.png'.format(ind+1))\n    real_series_ls.append('.\/Real_SERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_series.zip\", 'w') as my_zip:\n    for i in ai_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n\nwith zipfile.ZipFile(\"real_series.zip\", 'w') as my_zip:\n    for i in real_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \nprint('SERIES \uc0dd\uc131 \uc644\ub8cc')","8057e4e4":"v1 = cv2.VideoWriter('pix2pix_series.mp4',cv2.VideoWriter_fourcc(*'mp4v'), 3, (128, 128))\nfor name in ai_series_ls:\n    v1.write(cv2.imread(name))\nv1.release()\n\nv2 = cv2.VideoWriter('real_series.mp4',cv2.VideoWriter_fourcc(*'mp4v'), 3, (128, 128))\nfor name in real_series_ls:\n    v2.write(cv2.imread(name))\nv2.release()\n\nprint('\uc601\uc0c1 \uc0dd\uc131 \uc644\ub8cc')\nprint('video path : \".\/pix2pix_series\" and \".\/real_series.mp4\"')","da553cc9":"for file in (ai_series_ls + real_series_ls):\n    os.remove(file)","a2e1db86":"torch.save({\n            'epoch': nowepoch,\n            'model_state_dict': Generator.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            }, 'Generator.pth')\n\ntorch.save({\n            'epoch': nowepoch,\n            'model_state_dict': Discriminator.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            }, 'Discriminator.pth')","de08832d":"with open('.\/gloss.txt', 'wb') as f:\n    pickle.dump(G_loss, f)\nwith open('.\/dloss.txt', 'wb') as f:\n    pickle.dump(D_loss, f)","fe2fef23":"## **\uc2e4\ud5d8**","4990ccfc":"## **\ube44\uc6a9 \ud568\uc218 \uc120\uc5b8**\n\ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ube44\uc6a9 \ud568\uc218\ub97c \uc120\uc5b8\ud569\ub2c8\ub2e4.","0c00773d":"### \uc2e0\uacbd\ub9dd \uc900\ube44","4b3c35b9":"### \uccb4\ud06c\ud3ec\uc778\ud2b8 \uc801\uc6a9","327e043a":"## **\uacb0\uacfc \uc800\uc7a5**","66ffd9c9":"### \uc774\uc678 \ud559\uc2b5 \ubcc0\uc218 \uc120\uc5b8","6778d30c":"### Discriminator \ube44\uc6a9 \ud568\uc218","93a6e68b":"### \uba54\ubaa8\ub9ac \ud655\uc778\uc6a9 \ud568\uc218 \uc120\uc5b8","88a27c7b":"## **\uc2e0\uacbd\ub9dd \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uad6c\ucd95**\n\ud559\uc2b5\ud560 \uc2e0\uacbd\ub9dd \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uad6c\ucd95\ud569\ub2c8\ub2e4.\n\uc0ac\uc6a9\ud558\ub294 \uc2e0\uacbd\ub9dd \uad6c\uc870\ub294 Pix2Pix\ub85c, Generator\uc640 Discriminator\ub77c\ub294 \ub450\uac00\uc9c0 \uc2e0\uacbd\ub9dd\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n\nGenerator\ub294 UNet, Discriminator\uc740 CNN\uc785\ub2c8\ub2e4.","a90fb071":"### Import","c18435ea":"### \ub370\uc774\ud130\uc14b \uc120\uc5b8 \ubc0f \uc804\ucc98\ub9ac","e25c495c":"### Generator \ube44\uc6a9 \ud568\uc218 (\ubbf8\ub798 \uac00\uce58 \ud310\uc815 \ud568\uc218)","c58fe944":"### Install","922cd586":"### \ud559\uc2b5 \ud568\uc218 \uc120\uc5b8","d825be4b":"## **\ubaa9\ud45c**\n1. \ud604\uc7ac \uae30\uc0c1 \uc601\uc0c1\uc744 \uc774\uc6a9\ud558\uc5ec \ubbf8\ub798\uc758 \uae30\uc0c1 \uc601\uc0c1\uc744 \uc608\uce21\ud569\ub2c8\ub2e4.","68fbe874":"### \uc2e0\uacbd\ub9dd \ubc0f \ud544\uc694 \ube14\ub7ed \uc120\uc5b8","3c28c6a9":"## **\ud559\uc2b5**","1e0b697b":"# \ubcf8 \ucf54\ub4dc\ub294 [OracleGAN](https:\/\/www.kaggle.com\/lapl04\/oraclegan-pix2pix-for-time-series-image\/)\uacfc \uc131\ub2a5\uc744 \ube44\uad50\ud558\uae30 \uc704\ud568\uc785\ub2c8\ub2e4.\n# This code is to compare performance with [OracleGAN](https:\/\/www.kaggle.com\/lapl04\/oraclegan-pix2pix-for-time-series-image\/).","9d135d4b":"### \ub370\uc774\ud130 \uc2dc\uac01\ud654","908dca85":"### \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc9c0\uc815","07862c60":"## **Goal**\n1. Predict future weather images using current weather images.","9255cb25":"### \uc635\ud2f0\ub9c8\uc774\uc800 \uc900\ube44\n\ud574\ub2f9 \ud0d0\uad6c\uc5d0\uc11c\ub294 Generator\uc640 Discriminator \ubaa8\ub450 \uc635\ud2f0\ub9c8\uc774\uc800\ub85c Adam\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4."}}