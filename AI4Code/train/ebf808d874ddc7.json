{"cell_type":{"bd0642db":"code","3be2d219":"code","50881e0f":"code","0b88cb87":"code","bf090dde":"code","5916c668":"code","82fc55f0":"markdown","82fbcc8f":"markdown","d009468e":"markdown"},"source":{"bd0642db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3be2d219":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","50881e0f":"X = train.drop('claim', axis =1).copy()\ny = train.claim","0b88cb87":"import lightgbm as lgbm\nfrom optuna.integration import LightGBMPruningCallback\nimport optuna  # pip install optuna\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold","bf090dde":"best_params = {'n_estimators': 10000,\n 'learning_rate': 0.20731879670353354,\n 'num_leaves': 380,\n 'max_depth': 11,\n 'min_data_in_leaf': 200,\n 'lambda_l1': 100,\n 'lambda_l2': 70,\n 'min_gain_to_split': 10.725878344141766,\n 'bagging_fraction': 0.9,\n 'bagging_freq': 1,\n 'feature_fraction': 0.6000000000000001}\n\nfrom sklearn.metrics import mean_squared_error\nmodel = lgbm.LGBMClassifier(objective=\"binary\", **best_params)\nmodel.fit(X, y)","5916c668":"tt_predict = model.predict(test)\n\nsubmission['claim'] = tt_predict\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","82fc55f0":"\ndef objective(trial, X, y):\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"binary_logloss\",\n            early_stopping_rounds=100,\n            callbacks=[\n                LightGBMPruningCallback(trial, \"binary_logloss\")\n            ],  # Add a pruning callback\n        )\n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n\n    return np.mean(cv_scores)\n    \nstudy = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial, X, y)\nstudy.optimize(func, n_trials=20)","82fbcc8f":"print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nbest_params = study.best_params","d009468e":"best_params"}}