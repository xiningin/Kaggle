{"cell_type":{"5efdf0bd":"code","55ccc61d":"code","af62ed09":"code","23ad0c8e":"code","7a9bcac7":"code","1323242d":"code","27230052":"code","47623c8f":"code","7391a5c2":"code","4c4d1fe8":"code","361112c2":"code","3fb65184":"code","694e0e97":"code","51074c91":"code","3ea4f2c7":"code","9ceac1eb":"code","435bfc05":"code","a59b3429":"code","821166e4":"code","3df694b9":"code","dc851a65":"code","fefc6a9c":"code","5af3b6a6":"code","038bb923":"code","99129db7":"code","b0037fe6":"code","b140b318":"code","2db17af4":"code","03bdc2f0":"code","8190ceb5":"code","88d2907d":"code","8c695e28":"code","31ed5264":"code","dd77df0c":"code","29239612":"code","858714d6":"code","1079089b":"code","f7f507da":"code","435ab7fc":"code","3089b5c3":"code","652c8f57":"code","65ff3dcd":"markdown","ddfe1f83":"markdown","6b13c2ab":"markdown","7e9f3be9":"markdown","51ac6a27":"markdown","a780ff58":"markdown","01f11f95":"markdown","aff598d9":"markdown","29c98ef7":"markdown","62cb5c63":"markdown","66d8ce3f":"markdown","d4dfd129":"markdown","4292dc4c":"markdown","dcc38c80":"markdown","ecf2f5c4":"markdown","5bbb63e5":"markdown","cbd93ad0":"markdown","0b3fbde6":"markdown","b97cf5a3":"markdown","69f1d534":"markdown","f0430307":"markdown","22316793":"markdown","26f86d21":"markdown","514acc71":"markdown","a537b88e":"markdown","a1af9ff0":"markdown","6f30e17c":"markdown","f1fed36c":"markdown","0f7b59ae":"markdown","5d7b2a39":"markdown","f6c9c364":"markdown","fa004b7f":"markdown","dde8f01b":"markdown","d1eb730a":"markdown","556eb9f8":"markdown","bdd88a33":"markdown","ca8854b9":"markdown"},"source":{"5efdf0bd":"#Installing the pyspellchecker library\n!pip install pyspellchecker\n\n#Loading the libraries used in this notebook\nimport numpy as np\nimport pandas as pd\n\nfrom string import punctuation\nimport re\nimport nltk\nfrom spellchecker import SpellChecker\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n#Loading the data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","55ccc61d":"train[train.target == 1].head(10)","af62ed09":"train[train.target == 0].head(10)","23ad0c8e":"print('The number of rows in the training set is {}.\\nMissing values in each column:\\n'.format(train.shape[0]))\nprint(train.isna().sum())","7a9bcac7":"print('There are {} unique values in the keyword column, which are given by\\n'.format(len(train['keyword'].unique().tolist())))\nprint(train['keyword'].unique().tolist())","1323242d":"print('Percentage of disaster tweets with the \\'oil%20spill\\' keyword: {}'\\\n      .format(100*train[train.keyword=='oil%20spill']['target'].sum()\/train[train.keyword=='oil%20spill'].shape[0]))\ntrain[train.keyword=='oil%20spill'].head()","27230052":"print('Percentage of disaster tweets with the \\'thunder\\' keyword: {}'\\\n      .format(100*train[train.keyword=='thunder']['target'].sum()\/train[train.keyword=='thunder'].shape[0]))\ntrain[train.keyword=='thunder'].head()","47623c8f":"print('There are {} unique values in the location column. The first 10 of them are given by\\n'.format(len(train['location'].unique().tolist())))\nprint(train['location'].unique().tolist()[1:11])","7391a5c2":"print('Percentage of disasters if location is null: {}'.format(\\\n    100*train[train['location'].isna()]['target'].sum()\/train[train['location'].isna()]['target'].shape[0]))\nprint('Percentage of disasters if location is not null: {}'.format(\\\n    100*train[~train['location'].isna()]['target'].sum()\/train[~train['location'].isna()]['target'].shape[0]))","4c4d1fe8":"_ = sns.countplot(train['target'])","361112c2":"X_train = train.copy()\nX_train['keyword'] = X_train['keyword'].fillna('none') #Filling null values\n\n#CountVectorizer discarding english common stopwords and using a vocabulary with at most 1000 words\nvect = CountVectorizer(max_features=1000,stop_words='english')\nvect.fit(X_train.text)\ntext_features = vect.transform(X_train.text)\nX_train[vect.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect.get_feature_names())\n\n#Target variable\ny_train = X_train['target']\n\n#One-hot-encoding and dropping columns\nX_train = pd.get_dummies(X_train,columns=['keyword']).drop(columns=['id','location','text','target'])","3fb65184":"#Ridge Classifier\nclf = RidgeClassifier()\n\nalpha = [0.03,0.1,0.3,1,3,10,30]\nparam_grid = dict(alpha=alpha)\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=3,scoring='f1_macro')\n\ngrid_search_result = grid_search.fit(X_train,y_train)\n\nbest_score, best_params = grid_search_result.best_score_,grid_search_result.best_params_\nprint(\"Ridge Classifier F1 score: %f using %s\" % (best_score, best_params))","694e0e97":"#Naive Bayes classifier\nclf = MultinomialNB()\n\nalpha = [1,0.8,0.6,0.4,0.2]\nparam_grid = dict(alpha=alpha)\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=3,scoring='f1_macro')\n\ngrid_search_result = grid_search.fit(X_train,y_train)\n\nbest_score, best_params = grid_search_result.best_score_,grid_search_result.best_params_\nprint(\"Naive Bayes F1 score: %f using %s\" % (best_score, best_params))","51074c91":"X_test = test.copy()\nX_test['keyword'] = X_test['keyword'].fillna('none')\n\ntext_features = vect.transform(X_test.text)\n\nX_test[vect.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect.get_feature_names())\n\nX_test = pd.get_dummies(X_test,columns=['keyword']).drop(columns=['id','location','text'])\nX_test = X_test[X_train.columns.tolist()]\n\ny_pred = grid_search_result.predict(X_test)\noutput = pd.DataFrame({'id': test.id, 'target': y_pred})\noutput.to_csv('simple_NB_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\npd.read_csv('simple_NB_submission.csv').head()","3ea4f2c7":"fig,ax = plt.subplots(1,2,figsize=(16,5))\ntrain[train.target == 1]['text'].str.len().hist(ax=ax[0],bins=20)\ntrain[train.target == 0]['text'].str.len().hist(color='blue',ax=ax[1],bins=20)\nax[0].set_title('Disaster')\nax[0].set_xlabel('Tweet size')\nax[0].set_ylabel('Counts')\nax[1].set_title('Not a disaster')\nax[1].set_xlabel('Tweet size')\nax[1].set_ylabel('Counts')\n\nplt.show()","9ceac1eb":"def most_common_words(tweets,n_words = 10,stop_words = None):\n    vect = CountVectorizer(max_features=n_words,stop_words = stop_words)\n    vect.fit(tweets)\n    X = vect.transform(tweets)\n    X_df = pd.DataFrame(X.toarray(),columns=vect.get_feature_names())\n    most_common_words = X_df.sum().sort_values()\n    return most_common_words","435bfc05":"fig,ax = plt.subplots(1,2,figsize=(16,5))\nmost_common_words(train.text).plot(kind='barh',ax=ax[0])\nmost_common_words(train.text,stop_words='english').plot(kind='barh',ax=ax[1])\nax[0].set_title('With stopwords')\nax[0].set_xlabel('Counts')\nax[0].set_ylabel('Word')\nax[1].set_title('Without stopwords (english)')\nax[1].set_xlabel('Counts')\nax[1].set_ylabel('Word')\n\nplt.show()","a59b3429":"fig,ax = plt.subplots(1,2,figsize=(16,5))\nmost_common_words(train[train.target==1].text,stop_words='english').plot(kind='barh',ax=ax[0])\nmost_common_words(train[train.target==0].text,stop_words='english').plot(kind='barh',ax=ax[1],color='blue')\nax[0].set_title('Disaster')\nax[0].set_xlabel('Counts')\nax[0].set_ylabel('Word')\nax[1].set_title('Not a disaster')\nax[1].set_xlabel('Counts')\nax[1].set_ylabel('Word')\n\nplt.show()","821166e4":"X_train = train.copy()\nX_train['keyword'] = X_train['keyword'].fillna('none')\n\nvect = CountVectorizer(max_features=1000,stop_words='english',token_pattern=r\"\\w\\w+|!|\\?|#|@\")\nvect.fit(X_train.text)\ntext_features = vect.transform(X_train.text)\n\nX_train[vect.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect.get_feature_names())\n\ny_train = X_train['target']\nX_train = pd.get_dummies(X_train,columns=['keyword']).drop(columns=['id','location','text','target'])\n\nclf = MultinomialNB()\n\nalpha = [1,0.8,0.6,0.4,0.2]\nparam_grid = dict(alpha=alpha)\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=3,scoring='f1_macro')\n\ngrid_search_result = grid_search.fit(X_train,y_train)\n\nbest_score, best_params = grid_search_result.best_score_,grid_search_result.best_params_\nprint(\"Naive Bayes F1 score: %f using %s\" % (best_score, best_params))","3df694b9":"emoji_pattern = re.compile(\"[\"\n                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                        u\"\\U00002702-\\U000027B0\"\n                        u\"\\U000024C2-\\U0001F251\"\n                        \"]+\", flags=re.UNICODE)\n\ntrain['emojis'] = train['text'].apply(lambda x: len(re.findall(emoji_pattern,x)))\nprint('total number of emojis on tweets: ',train['emojis'].sum())","dc851a65":"train = train.drop(columns=['emojis'])\n\nhashtag_count = {}\nfor row in train['text']:\n    hashtags = re.findall('#\\w+',row)\n    for hashtag in hashtags:\n        if hashtag.lower() not in hashtag_count:\n            hashtag_count[hashtag.lower()] = 1\n        else:\n            hashtag_count[hashtag.lower()] += 1\n\nhashtag_count_series = pd.Series(hashtag_count)\n_ = hashtag_count_series.sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\n_ = plt.xlabel('Counts')","fefc6a9c":"hashtag_count_disaster = {}\nhashtag_count_not_disaster = {}\n\nfor row in train[train.target==1]['text']:\n    hashtags = re.findall('#\\w+',row)\n    for hashtag in hashtags:\n        if hashtag.lower() not in hashtag_count_disaster:\n            hashtag_count_disaster[hashtag.lower()] = 1\n        else:\n            hashtag_count_disaster[hashtag.lower()] += 1\n            \n            \nfor row in train[train.target==0]['text']:\n    hashtags = re.findall('#\\w+',row)\n    for hashtag in hashtags:\n        if hashtag.lower() not in hashtag_count_not_disaster:\n            hashtag_count_not_disaster[hashtag.lower()] = 1\n        else:\n            hashtag_count_not_disaster[hashtag.lower()] += 1\n            \nhashtag_count_disaster_series = pd.Series(hashtag_count_disaster)\nhashtag_count_not_disaster_series = pd.Series(hashtag_count_not_disaster)\n\nfig,ax = plt.subplots(1,2,figsize=(16,5))\nhashtag_count_disaster_series.sort_values(ascending=False)[:10].sort_values().plot(kind='barh',ax=ax[0])\nhashtag_count_not_disaster_series.sort_values(ascending=False)[:10].sort_values().plot(kind='barh',ax=ax[1],color='blue')\nax[0].set_title('Disaster')\nax[0].set_xlabel('Counts')\nax[1].set_title('Not a disaster')\nax[1].set_xlabel('Counts')\nplt.show()","5af3b6a6":"mention_count = {}\nfor row in train['text']:\n    mentions = re.findall('@\\w+',row)\n    for mention in mentions:\n        if mention.lower() not in mention_count:\n            mention_count[mention.lower()] = 1\n        else:\n            mention_count[mention.lower()] += 1\n            \nmention_count_series = pd.Series(mention_count)\n_ = mention_count_series.sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\n_ = plt.xlabel('Counts')","038bb923":"mention_count_disaster = {}\nmention_count_not_disaster = {}\n\nfor row in train[train.target==1]['text']:\n    mentions = re.findall('@\\w+',row)\n    for mention in mentions:\n        if mention.lower() not in mention_count_disaster:\n            mention_count_disaster[mention.lower()] = 1\n        else:\n            mention_count_disaster[mention.lower()] += 1\n            \n            \nfor row in train[train.target==0]['text']:\n    mentions = re.findall('@\\w+',row)\n    for mention in mentions:\n        if mention.lower() not in mention_count_not_disaster:\n            mention_count_not_disaster[mention.lower()] = 1\n        else:\n            mention_count_not_disaster[mention.lower()] += 1\n            \nmention_count_disaster_series = pd.Series(mention_count_disaster)\nmention_count_not_disaster_series = pd.Series(mention_count_not_disaster)\n\nfig,ax = plt.subplots(1,2,figsize=(16,5))\nmention_count_disaster_series.sort_values(ascending=False)[:10].sort_values().plot(kind='barh',ax=ax[0])\nmention_count_not_disaster_series.sort_values(ascending=False)[:10].sort_values().plot(kind='barh',ax=ax[1],color='blue')\nax[0].set_title('Disaster')\nax[0].set_xlabel('Counts')\nax[1].set_title('Not a disaster')\nax[1].set_xlabel('Counts')\nplt.show()","99129db7":"#Counting URLs\ntrain['URLs'] = train['text'].apply(lambda x: len(re.findall(r'http\\S+|www\\.\\S+',x)))\n\n#Removing URLs\nurl = re.compile(r'http\\S+|www\\.\\S+')\ntrain['text'] = train['text'].apply(lambda x: url.sub(r'',x))\n\n#Extracting tweet sizes\ntrain['text_size'] = train['text'].str.len()\n\n#Counting punctuations, hashtags, and mentions\ntrain['!'] = train['text'].apply(lambda x: len(re.findall('!',x)))\ntrain['?'] = train['text'].apply(lambda x: len(re.findall('\\?',x)))\ntrain['#'] = train['text'].apply(lambda x: len(re.findall('#',x)))\ntrain['@'] = train['text'].apply(lambda x: len(re.findall('@',x)))\n\n#Counting unique hashtags that appear in at least 0.1% of the tweets\nfor hashtag in hashtag_count_series[hashtag_count_series >= 7].index:\n    train[hashtag] = train['text'].apply(lambda x: len(re.findall(hashtag,x.lower())))\n    \n#Counting unique mentions that appear in at least 0.1% of the tweets    \nfor mention in mention_count_series[mention_count_series >= 7].index:\n    train[mention] = train['text'].apply(lambda x: len(re.findall(mention,x.lower())))\n\n#Removing hashtags and mentions\nhashtags_and_mentions = re.compile(r'@\\w+|#\\w+')\ntrain['text'] = train['text'].apply(lambda x: hashtags_and_mentions.sub(r'',x))\n\n#Defining a function that remove stopwords\ndef remove_stopwords(text,stop_words):\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text\n\n#Counting words that do not belong to the stopwords from the nltk corpus (english)\nstop_words = nltk.corpus.stopwords.words(\"english\")\ntrain['text_number_words'] = train['text'].apply(lambda x: len(re.findall(r'\\w+',remove_stopwords(x.lower(),stop_words))))\n\n#Counting capital words\ntrain['capital_words'] = train['text'].apply(lambda x: len(re.findall(r'[A-Z][A-Z]+',x)))\n\n#Counting unknown words using pyspellchecker\nspell = SpellChecker()\ntrain['unknown_words'] = train['text'].apply(lambda x: len(spell.unknown(re.findall(r'\\w+',x.lower()))))","b0037fe6":"#Defining the stemmer\nstemmer = nltk.stem.LancasterStemmer()\n\n#Function that splits and stems a keyword from the keyword column\ndef keyword_stemming(keyword,stemmer):\n    stemmed_keyword = ''\n    for word in keyword.split('%20'):\n        stemmed_keyword = stemmed_keyword + ' ' + stemmer.stem(word)\n    return stemmed_keyword[1:]\n\ntrain['keyword'] = train['keyword'].fillna('none').astype(str)\ntrain['keyword'] = train['keyword'].apply(lambda x: keyword_stemming(x,stemmer)).astype('category')\n\nprint('there are {} unique values in the keyword column, which are given by\\n'.format(len(train['keyword'].unique().tolist())))\nprint(train['keyword'].unique().tolist())","b140b318":"fig,ax = plt.subplots(1,2,figsize=(16,5))\nsns.countplot(x='!',data=train,hue='target',hue_order=[1,0],ax=ax[0])\nsns.countplot(x='?',data=train,hue='target',hue_order=[1,0],ax=ax[1])\nax[0].set_xlabel('Number of !')\nax[0].set_ylabel('Counts')\nax[1].set_xlabel('Number of ?')\nax[1].set_ylabel('Counts')\n\nplt.show()","2db17af4":"fig,ax = plt.subplots(1,2,figsize=(16,5))\nsns.countplot(x='#',data=train,hue='target',hue_order=[1,0],ax=ax[0])\nsns.countplot(x='@',data=train,hue='target',hue_order=[1,0],ax=ax[1])\nax[0].set_xlabel('Number of #')\nax[0].set_ylabel('Counts')\nax[1].set_xlabel('Number of @')\nax[1].set_ylabel('Counts')\n\nplt.show()","03bdc2f0":"fig,ax = plt.subplots(1,2,figsize=(16,5))\nsns.countplot(x='capital_words',data=train,hue='target',hue_order=[1,0],ax=ax[0])\nsns.countplot(x='unknown_words',data=train,hue='target',hue_order=[1,0],ax=ax[1])\nax[0].set_xlabel('Number of capital words')\nax[0].set_ylabel('Counts')\nax[1].set_xlabel('Number of unknown words')\nax[1].set_ylabel('Counts')\n\nplt.show()","8190ceb5":"X_train = train.copy()\n\nvect_text = CountVectorizer(max_features=1000,stop_words='english')\nvect_text.fit(X_train.text)\ntext_features = vect_text.transform(X_train.text)\nX_train[vect_text.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect_text.get_feature_names())\n\nvect_keyword = CountVectorizer(stop_words='english')\nvect_keyword.fit(X_train.keyword)\nkeyword_features = vect_keyword.transform(X_train.keyword)\nX_train[vect_keyword.get_feature_names()] = pd.DataFrame(keyword_features.toarray(),columns=vect_keyword.get_feature_names())\n\ny_train = X_train['target']\nX_train = X_train.drop(columns=['id','keyword','location','text','target'])\n\nclf = MultinomialNB()\n\nalpha = [1,0.8,0.6,0.4,0.2]\nparam_grid = dict(alpha=alpha)\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=3,scoring='f1_macro')\n\ngrid_search_result = grid_search.fit(X_train,y_train)\n\nbest_score, best_params = grid_search_result.best_score_,grid_search_result.best_params_\nprint(\"Naive Bayes F1 score: %f using %s\" % (best_score, best_params))","88d2907d":"test['URLs'] = test['text'].apply(lambda x: len(re.findall(r'http\\S+|www\\.\\S+',x)))\ntest['text'] = test['text'].apply(lambda x: url.sub(r'',x))\ntest['text_size'] = test['text'].str.len()\ntest['!'] = test['text'].apply(lambda x: len(re.findall('!',x)))\ntest['?'] = test['text'].apply(lambda x: len(re.findall('\\?',x)))\ntest['#'] = test['text'].apply(lambda x: len(re.findall('#',x)))\ntest['@'] = test['text'].apply(lambda x: len(re.findall('@',x)))\n\nfor hashtag in hashtag_count_series[hashtag_count_series >= 7].index:\n    test[hashtag] = test['text'].apply(lambda x: len(re.findall(hashtag,x.lower())))\n    \nfor mention in mention_count_series[mention_count_series >= 7].index:\n    test[mention] = test['text'].apply(lambda x: len(re.findall(mention,x.lower())))\n\ntest['text'] = test['text'].apply(lambda x: hashtags_and_mentions.sub(r'',x))\ntest['text_number_words'] = test['text'].apply(lambda x: len(re.findall(r'\\w+',remove_stopwords(x.lower(),stop_words))))\ntest['capital_words'] = test['text'].apply(lambda x: len(re.findall(r'[A-Z][A-Z]+',x)))\ntest['unknown_words'] = test['text'].apply(lambda x: len(spell.unknown(re.findall(r'\\w+',x.lower()))))\ntest['keyword'] = test['keyword'].fillna('none').astype(str)\ntest['keyword'] = test['keyword'].apply(lambda x: keyword_stemming(x,stemmer)).astype('category')","8c695e28":"X_test = test.copy()\ntext_features = vect_text.transform(X_test.text)\nX_test[vect_text.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect_text.get_feature_names())\nkeyword_features = vect_keyword.transform(X_test.keyword)\nX_test[vect_keyword.get_feature_names()] = pd.DataFrame(keyword_features.toarray(),columns=vect_keyword.get_feature_names())\n\nX_test = X_test.drop(columns=['id','keyword','location','text'])\nX_test = X_test[X_train.columns.tolist()]\n\ny_pred = grid_search_result.predict(X_test)\noutput = pd.DataFrame({'id': test.id, 'target': y_pred})\noutput.to_csv('metadata_NB_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\npd.read_csv('metadata_NB_submission.csv').head()","31ed5264":"def train_NB_clf(df,max_df=0.5,min_df=2,stopwords=None,max_ngram=1,alpha=[1]):\n    X_train = df.copy()\n\n    vect_text = CountVectorizer(max_df = max_df, min_df = min_df,stop_words = stopwords,ngram_range = (1,max_ngram))\n    vect_text.fit(df.text)\n    text_features = vect_text.transform(X_train.text)\n\n    vect_keyword = CountVectorizer(stop_words='english')\n    vect_keyword.fit(X_train.keyword)\n    keyword_features = vect_keyword.transform(X_train.keyword)\n\n    y_train = X_train['target']\n    X_train = X_train.drop(columns=['id','keyword','location','text','target'])\n\n    X_train[vect_text.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect_text.get_feature_names())\n    X_train[vect_keyword.get_feature_names()] = pd.DataFrame(keyword_features.toarray(),columns=vect_keyword.get_feature_names())\n\n    clf = MultinomialNB()\n    param_grid = dict(alpha=alpha)\n    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=3,scoring='f1_macro')\n    grid_search_result = grid_search.fit(X_train,y_train)\n    \n    return vect_text,vect_keyword,grid_search_result","dd77df0c":"vect_text,vect_keyword,grid_search_result = train_NB_clf(train,stopwords='english')\nprint(\"Naive Bayes F1 score: %f\" % (grid_search_result.best_score_))","29239612":"vect_text,vect_keyword,grid_search_result = train_NB_clf(train,stopwords='english',max_ngram=2)\nprint(\"Naive Bayes F1 score using 1-grams and 2-grams: %f\" % (grid_search_result.best_score_))\nvect_text,vect_keyword,grid_search_result = train_NB_clf(train,stopwords='english',max_ngram=3)\nprint(\"Naive Bayes F1 score using 1-grams, 2-grams, and 3-grams: %f\" % (grid_search_result.best_score_))","858714d6":"vect_text,vect_keyword,grid_search_result = train_NB_clf(train,max_ngram=1)\nprint(\"Naive Bayes F1 score using only 1-grams: %f\" % (grid_search_result.best_score_))\nvect_text,vect_keyword,grid_search_result = train_NB_clf(train,max_ngram=2)\nprint(\"Naive Bayes F1 score using 1-grams and 2-grams: %f\" % (grid_search_result.best_score_))\nvect_text,vect_keyword,grid_search_result = train_NB_clf(train,max_ngram=3)\nprint(\"Naive Bayes F1 score using 1-grams, 2-grams, and 3-grams: %f\" % (grid_search_result.best_score_))","1079089b":"vect_text,vect_keyword,grid_search_result = train_NB_clf(train,max_ngram=2)\n\nX_test = test.copy()\ntext_features = vect_text.transform(X_test.text)\nkeyword_features = vect_keyword.transform(X_test.keyword)\n\nX_test = X_test.drop(columns=['id','keyword','location','text'])\n\nX_test[vect_text.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect_text.get_feature_names())\nX_test[vect_keyword.get_feature_names()] = pd.DataFrame(keyword_features.toarray(),columns=vect_keyword.get_feature_names())\n\ny_pred = grid_search_result.predict(X_test)\noutput = pd.DataFrame({'id': test.id, 'target': y_pred})\noutput.to_csv('ngrams_NB_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\npd.read_csv('ngrams_NB_submission.csv').head()","f7f507da":"def text_processor(text,stemmer):\n    text = text.lower()\n    tokenized_text = nltk.word_tokenize(text)\n    tokenized_text = [word for word in tokenized_text if word not in punctuation]\n    stemmed_text = ''\n    for word in tokenized_text:\n        #The stemmer transforms news to new, but we do not want that.\n        if word != 'news':\n            stemmed_text = stemmed_text + ' ' + stemmer.stem(word)\n        else:\n            stemmed_text = stemmed_text + ' ' + word\n    return stemmed_text[1:]\n\n#Testing the new function\ntext_processor('Hi, I am learning #NLP, and I\\' not sure if this transformation will be good to this problem...',stemmer)","435ab7fc":"#Applying the pre-processing to the training and test datasets\ntrain['text'] = train['text'].apply(lambda x: text_processor(x,stemmer))\ntest['text'] = test['text'].apply(lambda x: text_processor(x,stemmer))","3089b5c3":"vect_text,vect_keyword,grid_search_result = train_NB_clf(train,max_ngram=1)\nprint(\"Naive Bayes F1 score using only 1-grams: %f\" % (grid_search_result.best_score_))\nvect_text,vect_keyword,grid_search_result = train_NB_clf(train,max_ngram=2)\nprint(\"Naive Bayes F1 score using 1-grams and 2-grams: %f\" % (grid_search_result.best_score_))","652c8f57":"X_test = test.copy()\ntext_features = vect_text.transform(X_test.text)\nkeyword_features = vect_keyword.transform(X_test.keyword)\n\nX_test = X_test.drop(columns=['id','keyword','location','text'])\n\nX_test[vect_text.get_feature_names()] = pd.DataFrame(text_features.toarray(),columns=vect_text.get_feature_names())\nX_test[vect_keyword.get_feature_names()] = pd.DataFrame(keyword_features.toarray(),columns=vect_keyword.get_feature_names())\n\ny_pred = grid_search_result.predict(X_test)\noutput = pd.DataFrame({'id': test.id, 'target': y_pred})\noutput.to_csv('ngrams_stemming_NB_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\npd.read_csv('ngrams_stemming_NB_submission.csv').head()","65ff3dcd":"Our score on the leaderboard is now about 0.785, an improvement of ~ 0.4%. It is a small improvement, but at least we know that these changes do not hurt the model.","ddfe1f83":"### How to achieve better results?","6b13c2ab":"The dataset does not have many columns, containing only the location from where the tweet was published, the keyword and the corresponding text of the tweet, We can already see that we will need to do a lot of NLP in order to deal with the tweets. For example, hashtags may retain important information and mispelled words like \"goooooooaaaaaal\" may hurt the model (or be another important information for predicting disaster tweets).","7e9f3be9":"# 1 - Introduction","51ac6a27":"Now that we did a lot of work exploring the data and creating features, we can try to handle the text column with more care. This step is tricky, since we risk losing information in every transformation that we apply to the tweets. To be aware of the consequences of every change that we make on the text, we will always validate the model and then decide if we keep or discard the modifications. To make the discussion easier to follow and the code concise, below I define a function for training a Naive Bayes classifier on our dataset. For now, I let open the possibility of\n\n1. Restricting the words on the vocabulary based on the maximum and minimum frequency of appearance in the dataset;\n2. Discarting or not any chosen set of stopwords from the vocabulary;\n3. Accounting for any number of n-grams on the vocabulary (1-grams, 2-grams, 3-grams, ...)","a780ff58":"It makes no difference... Due to its poor statistics and no useful information, we shall not concern about this column anymore.\n\nFinally, let's see if the target data is unbalanced, i.e., if we have much more disaster tweets (or normal tweets) than normal tweets (or disaster tweets).","01f11f95":"Let's begin by a simple improvement. Instead of using a maximum number of text features of 1000, let's use the default parameters of the train_NB_clf function, which only discards words that appear less than two times on the dataset or on more than 50% of the tweets. ","aff598d9":"# Classifying tweets step by step","29c98ef7":"We have some missing values in the 'keyword' column, and a lot on the 'location' column. Let's pay some attention to these columns now, starting by 'keyword'.","62cb5c63":"That is a 0.1% improvement, which is not sufficient to draw any conclusions now. But we must keep in mind that adding punctuations to the algorithm may be helpful.\n\n**What about emojis (like \ud83d\ude02)?** We expect some tweets to have emojis on them. However, if we try to find the [pattern of an emoji](https:\/\/stackoverflow.com\/questions\/33404752\/removing-emojis-from-a-string-in-python\/49146722#49146722) on the dataset we do not find any, as can be seen below","66d8ce3f":"The stemmer's work is not perfect, as we can see by looking at the words above, but still, we reduced the number of unique keywords from 222 to 161.\n\nBefore proceeding, let's just look at some plots of the new features to see if everything makes sense.","d4dfd129":"It seems that some hashtags appears more in disaster tweets and others in non-disaster tweets. The same happens with mentions, as can be seen below.","4292dc4c":"# 3 - Exploring the data and creating more features","dcc38c80":"As usual, we begin by getting familiar with the dataset, trying to grasp what may be important to the analysis and what may be a problem. After that, we make a simple machine learning model just to see how well we can perform without any sophisticated data cleaning and feature engineering, establishing a ground to compare later.","ecf2f5c4":"We can see a clear difference between the words present in disaster and non-disaster tweets.\n\nWhat about punctuations? The CountVectorizer excludes all kinds of punctuations by default, but maybe counting the number them for each tweet helps identifying disaster tweets. Let's run the Naive Bayes model again, but now counting some punctuations (! and ?), hashtags (#) and mentions (@)\n\n**Note:** I tried to use many different punctuations, and the ones that gave the best model performance were \"!\" and \"?\".","5bbb63e5":"The score on the test set was about 0.782, which is much higher than on the validation set (why? I do not know yet). This will be our ground when trying out different feature extraction and engineering methods.","cbd93ad0":"Now it's time to further explore the data and try different ideas for new features. I must mention that this section is very influenced by [this](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove) and [this](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert) notebook made by the community, which were the firsts that I looked at. We begin by a very common information about text, which is its size. Let's look at the distribution of text size in disaster and non-disaster tweets.","0b3fbde6":"Now let's see how these changes behave in the same environment of the simple model (the idea here is to see if these changes bring a positive or negative impact to the score). Later, we shall start improving the model itself, by adding more features and using ngrams.","b97cf5a3":"Our score on the test set with this model is about 0.799.\n\nWhat if we also stem the tweets? This may be good to make our model treat equally words like bomb, bomber and bombing, but at the same time we risk losing information. The only way to know if this will be good or not is to test.","69f1d534":"With all this in mind, we will create new features that may help the model achieve better scores. These are:\n\n- **Number of URLs in each tweet:** URLs lead to noise in the bag of words and have no meaning *a priori*. The only information that we can extract from them is the number of URLs that appear in a given tweet. After obtaining this we remove the URLs from all tweets.\n\n- **Length of the text (without urls):** as simple as it is, maybe the size of the text that was actually written by the user provide some useful information.\n\n- **Number of each punctuation (\"!\", \"?\", ...).** \n\n- **Number of hashtags and mentions (# and @):** the CountVectorizer already count the words appearing in hashtags and mentions, however, it is treated as any other word in the text. I want to make the model treat separately words in these two formats.\n\n- **Number of each unique hashtag\/mention for each tweet:** as discussed before, some hashtags\/mentions appear more on disaster tweets than others. It may be useful count these unique hashtags\/mentions.\n\n- **Number of words without URLs, mentions, hashtags, and stop words.**\n\n- **Number of words not belonging to the english vocabulary:** this contains mispelling and abbreviations, but care should be taken since it may also contain name of places, companies, webpages etc. It may be worth trying to separate these categories later.\n\n- **Number of words in capital letters.**\n\n- **Split keyword on '%20' and stemming:** the keyword feature is very important, but contains some problems. There are many keywords that have two or three words separated by a '%20' string, while others share the same meaning (like bomb, bomber and bombing). Hence, it is worth splitting and then stemming this feature.\n\n**Note:** we could also treat the text feature with more care (remove more noise, stemming, ...), but, for now, let's restrict ourselves to the creation of these features and see how these impact the  Naive Bayes classifier.\n\nBelow, I create all these features and remove URLs, hashtags and mentions from the text. This is accomplished using the re and nltk packages. A very good tutorial on NLP and how to use nltk can be found [here](https:\/\/www.kaggle.com\/alvations\/basic-nlp-with-nltk)","f0430307":"Here my purpuse was just to learn the basics of NLP, which is a very rich field with many challenges even in a getting started competition of sentiment analysis. We used only a simple and naive Bayes model, and I find amazing that we were able to reach so good scores (when comparing with the leaderboard best scores). Further improvements, however, would require better NLP techniques and more sophisticated algorithms, such as [BERT](https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270). Nonetheless, I will not worry about it now since I already accomplished my objectives. Maybe I come back here in the future to learn more about NLP.","22316793":"This column is probably very important for predicting disaster tweets. I imagine that keywords like oil%20spill are strongly related to oil spill disasters, but keywords like thunder are not necessarily related to disasters (it may be related to a normal thunder in a rainy day or Thor, the thunder god!). Let's see if that is the case.","26f86d21":"Hi! My name is Yuri and I'm studying data science and machine learning. After taking courses on this subject and practicing standard machine learning problems, I decided to learn natural language processing (NLP). NLP is a topic that fascinates me due to its huge challenges, some of them still unsolved today, and the wealth of possible applications in real life problems. In this notebook, my idea is to present my workflow sequentially, writing down everything that was important to my decisions. The goal here is not to provide a detailed analysis using state-of-the-art technologies such as BERT, but rather show how my understanding of the problem and NLP tools evolved with data exploration and by reading other notebooks and discussions made by the Kaggle community. I think this notebook will be very useful to me in future projects, and I hope it may be useful to other beginners like me when learning NLP.\n\n**Finally, I'm still learning from this and other competitions, so, any comments or constructive criticism will be helpful :)**","514acc71":"It is not 50%-50%, but the data is definitely not unbalanced.\n\n**Time to make our first simple model.** Our approach is the following:\n\n1. Fill the nulls in the keyword column with 'none', so that the model treat null values separately.\n2. Use the CountVectorizer of scikit-learn to make a bag of words of the text feature. We discard common english stopwords of the vocabulary and retain only the 1000th most frequent words of the dataset.\n3. OneHotEncode the keyword feature, then discard id, location, and text.\n4. Use a RidgeClassifier and a Naive Bayes Classififer (which is widely used for text classification) tuned with GridSearchCV to predict the target variable.\n5. Validate the model using the macro-averaged f1 score. In principle, we could use the f1 score since our target variable is binary, but as the competition leaderboard uses the mean f1 score, I will rather use the same metric in this notebook. You can check a discussion about it [here](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/discussion\/123165).\n\nThis is more or less the approach used in the [getting started tutorial](https:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial). Further improvements will be made after that.","a537b88e":"I found this really weird since there are other notebooks where the authors use this pattern to remove emojis from the dataset, but they do not calculate the total number of emojis before this transformation. For this reason, I decided to take a look on some tweets to see if that is actually true. I did not found anything that looks like an emoji. Maybe, when extracting the tweets from the web the emojis were already removed. Hence, we will skip any analysis concerning emojis.\n\n**Let's move to hashtags and mentions.** What hashtags and mentions are used more frequently in tweets? ","a1af9ff0":"That's huge. Even the simple model using only a bag of words benefited from the inclusion of stopwords. From the above scores, we can see that the best approach is to use 1-grams and 2-grams, but not 3-grams. Let's make a prediction on the test set now, before proceeding to the next moves. ","6f30e17c":"There is not a clear signature on the distribution about the target variable. Most of the tweets have lengths between 100 and 140 characteres, and just a few have less than 40. We note, however, that non-disaster tweets are more prone to be in an intermediate range (40 - 100) than disaster tweets.\n\nAnother interesting visualization concerns the most common words that appear in the tweets. In order to do that, we define a function that returns a pandas series with the counts of the most frequent words in a list of tweets.","f1fed36c":"# 4 - N-grams and more NLP","0f7b59ae":"Ok, a large vocabulary is better for the Naive Bayes model than a smaller one. This score will be our ground now. Let's see if including 2-grams or 3-grams also has a good impact on the score.","5d7b2a39":"The score did not improve on the validation set. However, we had a slightly better score on the test set (~0.801). I think this is just a fluctuation and that stemming the words on the tweets did not change the performance of the Naive Bayes classifier. ","f6c9c364":"If we remember that 2533 entries in this column are null values, we can see that more than half of the non-null entries are unique. Also, many of the values in these columns are nonsense, like 'World Wide!!', and 'milky way'. This column will probably not help us very much. Let's see if at least the information about having or not a non-null value in this column has impact on the target variable.","fa004b7f":"The inclusion of 2-grams and 3-grams was really bad! Why? After thinking about it for a time and reading some interesting articles about sentiment analysis, I realized that there are problems related to discarting the usual english stopwords. As it is very well explained in [this article](https:\/\/towardsdatascience.com\/why-you-should-avoid-removing-stopwords-aa7a353d2a52) and discussed in [this stackoverflow topic](https:\/\/datascience.stackexchange.com\/questions\/15765\/nlp-why-is-not-a-stop-word), some words like 'no' and 'more' are in the list of english stopwords, but have a high importance on sentiment analysis. For example, the strings 'more disaster' and 'no disaster' have the same meaning for our model if we discart these stopwords. For 2-grams, this may be even worse because the model will never be able to capture the context of the tweet.\n\nTo verify if our above reasoning is correct, let's now validate the model taking into consideration all stopwords.","dde8f01b":"As expected, there are many english stopwords in the tweets, but just removing them is not enough to clean the tweets. From the left plot, we see that the most common words are http and https, which are the beginning of URLs. The CountVectorizer do not know that URLs are not words, and we must deal with it separately.\n\nWhat about the most common words on disaster and non-disaster tweets?","d1eb730a":"Our intuition was right. The keyword column is indeed important. We expect that only keywords with a disaster\/not disaster rate around 50% will not be very useful for predicting disaster tweets. Hence, we shall keep this column for further analysis.\n\nLet's investigate the 'location' column now.","556eb9f8":"# 2 - First look at the data and simple modeling","bdd88a33":"The changes seem to improve our predictions.","ca8854b9":"Both classifiers performed more or less the same. For predictions on the test set we shall use the Naive Bayes classifier."}}