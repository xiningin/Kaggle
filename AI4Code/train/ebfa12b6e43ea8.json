{"cell_type":{"7e50824d":"code","0494cc64":"code","a1f2026a":"code","18e7b77e":"code","b5eea488":"code","7454136c":"code","9176be2b":"code","47825e6d":"code","a8d4431c":"code","3a502f3d":"code","ef126a00":"code","aec79ba9":"code","d4946d65":"code","136006a0":"code","0fac9b8a":"code","cdf1a534":"code","d1dbefe3":"code","8f54c524":"code","a36605de":"code","6fc7d68c":"code","3e744233":"code","277a122f":"code","9a73acf3":"code","80edd538":"code","9ad72f1b":"code","063ff97b":"code","8c509e16":"code","05a2e683":"code","c603eef2":"code","abfbdcb7":"markdown","939b488c":"markdown","5a1f8ff3":"markdown","b5e37562":"markdown","40534e45":"markdown","b41da8f4":"markdown","831f222c":"markdown","32a533f2":"markdown","c791817b":"markdown","7208dc62":"markdown","a696ccc1":"markdown"},"source":{"7e50824d":"import numpy as np # linear algebra\nimport tensorflow as tf # \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom skimage.util.montage import montage2d\nimport os\nfrom cv2 import imread, createCLAHE # read and equalize images\nimport cv2\nfrom glob import glob\nfrom keras import layers, models\n%matplotlib inline\nimport matplotlib.pyplot as plt","0494cc64":"def add_grid_tf(in_layer,  # type: tf.Tensor\n                       x_cent=0.0,  # type: tf.Tensor\n                       y_cent=0.0,  # type: tf.Tensor\n                       x_wid=1.0,  # type: tf.Tensor\n                       y_wid=1.0  # type: tf.Tensor\n                       ):\n    # type: (...) -> tf.Tensor\n    \"\"\"\n    Adds spatial grids to images for making segmentation easier\n    :param in_layer: the base image to use for x,y dimensions\n    :param x_cent: the x mid coordinate\n    :param y_cent: the y mid coordinate\n    :param x_wid: the width in x (pixel spacing)\n    :param y_wid: the width in y (pixel spacing)\n    :return:\n    \"\"\"\n    with tf.variable_scope('add_grid'):\n        batch_size = tf.shape(in_layer)[0]\n        xg_wid = tf.shape(in_layer)[1]\n        yg_wid = tf.shape(in_layer)[2]\n        x_min = x_cent - x_wid\n        x_max = x_cent + x_wid\n        y_min = y_cent - y_wid\n        y_max = y_cent + y_wid\n        xx, yy = tf.meshgrid(tf.linspace(x_min, x_max, xg_wid),\n                             tf.linspace(y_min, y_max, yg_wid),\n                             indexing='ij')\n        \n        xx = tf.reshape(xx, (xg_wid, yg_wid, 1))\n        yy = tf.reshape(yy, (xg_wid, yg_wid, 1))\n        xy_vec = tf.expand_dims(tf.concat([xx, yy], -1), 0)\n        txy_vec = tf.tile(xy_vec, [batch_size, 1, 1, 1])\n        return txy_vec","a1f2026a":"def project_gaussians(gaus_coord,  # type: tf.Tensor\n                      proj_grid # type: tf.Tensor \n                       ):\n    # type: (...) -> tf.Tensor\n    \"\"\"\n    Project M gaussians on a grid of points\n    :param gaus_coord: the n, m, 4 (x, y, w, h)\n    :param proj_grid: the xx yy grid to project on (n, R, C, 2)\n    :return:\n    \"\"\"\n    with tf.variable_scope('gauss_proj'):\n        batch_size = tf.shape(gaus_coord)[0]\n        n_gaus = tf.shape(gaus_coord)[1]\n        xg_wid = tf.shape(proj_grid)[1]\n        yg_wid = tf.shape(proj_grid)[2]\n        with tf.variable_scope('create_m_grids'):\n            \"\"\"create a grid for each gaussian\"\"\"\n            grid_prep = lambda x: tf.tile(tf.expand_dims(x, 1), [1, n_gaus, 1, 1])\n            xx_grid = grid_prep(proj_grid[:, :, :, 0])\n            yy_grid = grid_prep(proj_grid[:, :, :, 1])\n\n        with tf.variable_scope('create_rc_coords'):\n            \"\"\"create coordinates for each position and \"\"\"\n            coord_prep = lambda x: tf.tile(tf.expand_dims(tf.expand_dims(x, 2), 3), \n                                      [1, 1, xg_wid, yg_wid])\n            c_x = coord_prep(gaus_coord[:, :, 0])\n            c_y = coord_prep(gaus_coord[:, :, 1])\n            c_w = 0.75*coord_prep(0.5+0.45*gaus_coord[:, :, 2])\n            c_h = coord_prep(0.5+0.45*gaus_coord[:, :, 3])\n        with tf.variable_scope('transform_coords'):\n            x_trans = (xx_grid-c_x)\/c_w\n            y_trans = (yy_grid-c_y)\/c_h\n            all_gauss = tf.exp(-(tf.pow(x_trans, 2)+tf.pow(y_trans, 2)))\n            sum_gauss = tf.reduce_sum(all_gauss, 1)\n            return tf.expand_dims(sum_gauss, -1)","18e7b77e":"b_shape = (1, 10, 11, 1)\nt_image = np.arange(10*11).reshape(b_shape, order='F')\nt_image = t_image \/\/ b_shape[1]\nt_image = np.abs(t_image-1.0*b_shape[1]\/\/2)\nprint(t_image.shape, t_image.dtype)\ng_input = np.array([\n    [0, -0.5, 1, 0.5],\n    [0, 0.5, 0.5, 1.0]\n])\ng_input = np.expand_dims(g_input, 0)\nprint(g_input.shape)\nin_lay = layers.Input(t_image.shape[1:], name='In_Image')\nin_coords = layers.Input(g_input.shape[1:], name='In_Coords')\ngrid_lay = layers.Lambda(add_grid_tf, name='AddGrid')(in_lay)\ngaus_lay = layers.Lambda(lambda x: project_gaussians(x[0], x[1]))([in_coords, grid_lay])\n\ns_model = models.Model(inputs=[in_lay,\n                              in_coords], \n                       outputs=[grid_lay, \n                                gaus_lay])\ni_img = np.concatenate([t_image]*2, 0)\ng_coord = np.concatenate([g_input]+[g_input[:, :, [1, 0, 2, 3]]], 0)\nprint(i_img.shape, g_coord.shape)\n[o_img, g_img] = s_model.predict([i_img, g_coord])\nprint(o_img.shape, g_img.shape)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 3))\nk = 0\nax1.imshow(i_img[k, :, :, 0])\nax2.imshow(o_img[k, :, :, 0], cmap='RdBu', vmin=-1, vmax=1)\nax3.imshow(o_img[k, :, :, 1], cmap='RdBu', vmin=-1, vmax=1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3))\nax1.imshow(g_img[0, :, :, 0], cmap='gray', vmin=0, vmax=1)\nax2.imshow(g_img[1, :, :, 0], cmap='gray', vmin=0, vmax=1)","b5eea488":"cxr_paths = glob(os.path.join('..', 'input', 'pulmonary-chest-xray-abnormalities',\n                              'Montgomery', 'MontgomerySet', '*', '*.png'))\ncxr_images = [(c_path, \n               [os.path.join('\/'.join(c_path.split('\/')[:-2]),'ManualMask','leftMask', os.path.basename(c_path)),\n               os.path.join('\/'.join(c_path.split('\/')[:-2]),'ManualMask','rightMask', os.path.basename(c_path))]\n              ) for c_path in cxr_paths]\nprint('CXR Images', len(cxr_paths), cxr_paths[0])\nprint(cxr_images[0])","7454136c":"from skimage.io import imread as imread_raw\nfrom skimage.transform import resize\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage') # skimage is really annoying\nOUT_DIM = (512, 512)\ndef imread(in_path, apply_clahe = False):\n    img_data = imread_raw(in_path)\n    n_img = (255*resize(img_data, OUT_DIM, mode = 'constant')).clip(0,255).astype(np.uint8)\n    if apply_clahe:\n        clahe_tool = createCLAHE(clipLimit=2.0, tileGridSize=(16,16))\n        n_img = clahe_tool.apply(n_img)\n    return np.expand_dims(n_img, -1)","9176be2b":"img_vol, seg_vol = [], []\nfor img_path, s_paths in tqdm(cxr_images):\n    img_vol += [imread(img_path)]    \n    seg_vol += [np.max(np.stack([imread(s_path, apply_clahe = False) for s_path in s_paths],0),0)]\nimg_vol = np.stack(img_vol,0)\nseg_vol = np.stack(seg_vol,0)\nprint('Images', img_vol.shape, 'Segmentations', seg_vol.shape)","47825e6d":"np.random.seed(2018)\nt_img, m_img = img_vol[0], seg_vol[0]\n\nfig, (ax_img, ax_mask) = plt.subplots(1,2, figsize = (12, 6))\nax_img.imshow(np.clip(255*t_img, 0, 255).astype(np.uint8) if t_img.shape[2]==3 else t_img[:,:,0],\n              interpolation = 'none', cmap = 'bone')\nax_mask.imshow(m_img[:,:,0], cmap = 'bone')","a8d4431c":"from keras.layers import Conv2D, Activation, Input, UpSampling2D, concatenate, BatchNormalization\nfrom keras.layers import LeakyReLU\nfrom keras.initializers import RandomNormal\ndef c2(x_in, nf, strides=1, dr = 1):\n    x_out = Conv2D(nf, kernel_size=3, padding='same',\n                   kernel_initializer='he_normal', \n                   activation='linear',\n                   strides=strides, \n                   dilation_rate=(dr, dr))(x_in)\n    x_out = LeakyReLU(0.1)(x_out)\n    return x_out\ndef unet_enc(vol_size, enc_nf, pre_filter = 8):\n    src = Input(shape=vol_size + (1,), name = 'EncoderInput')\n    # down-sample path.\n    x_in = BatchNormalization(name = 'NormalizeInput')(src)\n    x_in = c2(x_in, pre_filter, 1)\n    x0 = c2(x_in, enc_nf[0], 2)  \n    x1 = c2(x0, enc_nf[1], 2)  \n    x2 = c2(x1, enc_nf[2], 2)  \n    x3 = c2(x2, enc_nf[3], 2) \n    return Model(inputs = [src], \n                outputs = [x_in, x0, x1, x2, x3],\n                name = 'UnetEncoder')","3a502f3d":"def feat_to_coords(in_shape, base_filters = 32):\n    in_x = layers.Input(in_shape[1:], name='LungFeatureTensor')\n    x1 = c2(in_x, base_filters, strides=(2, 1))\n    x1 = c2(x1, base_filters*2, strides=(2, 2))\n    x1 = c2(x1, base_filters*2, strides=(2, 2))\n    x1 = layers.Flatten()(x1)\n    x1 = layers.Dropout(0.5)(x1)\n    x1 = layers.Dense(4, activation='tanh')(x1)\n    x1 = layers.Reshape((1, 4))(x1)\n    return models.Model(inputs=[in_x], outputs=[x1])","ef126a00":"from keras.models import Model\nfrom keras import layers\ndef unet(vol_size, enc_nf, dec_nf, full_size=True, edge_crop=48):\n    \"\"\"\n    unet network for voxelmorph \n    Args:\n        vol_size: volume size. e.g. (256, 256, 256)\n        enc_nf: encoder filters. right now it needs to be to 1x4.\n            e.g. [16,32,32,32]\n            TODO: make this flexible.\n        dec_nf: encoder filters. right now it's forced to be 1x7.\n            e.g. [32,32,32,32,8,8,3]\n            TODO: make this flexible.\n        full_size\n    \"\"\"\n    def _mini_model(in_size):\n        # inputs\n        raw_src = Input(shape=in_size, name = 'HalfInput')\n        full_xy_grid = layers.Lambda(add_grid_tf, name='AddGrid')(raw_src)\n        src = layers.GaussianNoise(0.2)(raw_src)\n        enc_model = unet_enc(vol_size, enc_nf)\n        # run the same encoder on the source and the target and concatenate the output at each level\n        x_in, x0, x1, x2, x3 = [s_enc for s_enc in enc_model(src)]\n\n        print(x3._keras_shape)\n        x = c2(x3, dec_nf[0])\n        # split the image in half\n        feat_model = feat_to_coords(x._keras_shape,  dec_nf[0])\n        y_coord = feat_model(x)\n        gaus_lay = layers.Lambda(lambda x: project_gaussians(x[0], x[1]), name='ProjectGaussians')\n        y_seg = gaus_lay([y_coord, full_xy_grid])\n        return models.Model(inputs=[raw_src], outputs=[y_seg])\n    \n    full_src = Input(shape=vol_size + (1,), name = 'ImageInput')\n    x_left = layers.Lambda(lambda x: x[:, :, :vol_size[1]\/\/2], name='LeftImage')(full_src)\n    x_right = layers.Lambda(lambda x: x[:, :, vol_size[1]\/\/2:], name='RightImage')(full_src)\n    half_model = _mini_model(x_left._keras_shape[1:])\n    left_gaus = half_model(x_left)\n    right_gaus = half_model(x_right)\n    full_seg = layers.concatenate([left_gaus, right_gaus], axis=2)\n    # prepare model\n    model = Model(inputs=[full_src], outputs=[full_seg])\n    return model","aec79ba9":"# use the predefined depths\nnf_enc=[16,32,32,32]\nnf_dec=[32,32,32,32,32,16,16,2]\nnet = unet(OUT_DIM, nf_enc, nf_dec)\n# ensure the model roughly works\na = net.predict([np.zeros((1,)+OUT_DIM+(1,))])\nprint(a.shape)\nnet.summary()","d4946d65":"from keras.optimizers import Adam\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\nreg_param = 1.0\nlr = 3e-4\ndice_bce_param = 0.3\nuse_dice = True\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) \/ (union + smooth), axis=0)\ndef dice_p_bce(in_gt, in_pred):\n    return dice_bce_param*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))\/K.sum(y_true)\n\nnet.compile(optimizer=Adam(lr=lr), \n              loss=[dice_p_bce], \n           metrics = [true_positive_rate, 'binary_accuracy'])","136006a0":"from sklearn.model_selection import train_test_split\ntrain_vol, test_vol, train_seg, test_seg = train_test_split((img_vol-127.0)\/127.0, \n                                                            (seg_vol>127).astype(np.float32), \n                                                            test_size = 0.2, \n                                                            random_state = 2018)\nprint('Train', train_vol.shape, 'Test', test_vol.shape, test_vol.mean(), test_vol.max())\nprint('Seg', train_seg.shape, train_seg.max(), np.unique(train_seg.ravel()))\nfig, (ax1, ax1hist, ax2, ax2hist) = plt.subplots(1, 4, figsize = (20, 4))\nax1.imshow(test_vol[0, :, :, 0])\nax1hist.hist(test_vol.ravel())\nax2.imshow(test_seg[0, :, :, 0]>0.5)\nax2hist.hist(train_seg.ravel());","0fac9b8a":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 7, \n                  width_shift_range = 0.05, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                   brightness_range = [0.75, 1.5],\n                  zoom_range = [0.8, 1.5],  \n               # anatomically it doesnt make sense, but many images are flipped\n                  horizontal_flip = True,  \n                  vertical_flip = False,\n                  fill_mode = 'nearest',\n               data_format = 'channels_last')\n\nimage_gen = ImageDataGenerator(**dg_args)\n\ndef gen_augmented_pairs(in_vol, in_seg, batch_size = 16):\n    while True:\n        seed = np.random.choice(range(9999))\n        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n        g_vol = image_gen.flow(in_vol, batch_size = batch_size, seed = seed)\n        g_seg = image_gen.flow(in_seg, batch_size = batch_size, seed = seed)\n        for i_vol, i_seg in zip(g_vol, g_seg):\n            yield i_vol, ((i_seg\/255.0)>0.5).astype(np.float32)","cdf1a534":"train_gen = gen_augmented_pairs(train_vol, train_seg, batch_size = 16)\ntest_gen = gen_augmented_pairs(test_vol, test_seg, batch_size = 16)\ntrain_X, train_Y = next(train_gen)\ntest_X, test_Y = next(test_gen)\nprint(train_X.shape, train_Y.shape)\nprint(test_X.shape, test_Y.shape)","d1dbefe3":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage2d(train_X[:, :, :, 0]), cmap = 'bone')\nax1.set_title('CXR Image')\nax2.imshow(montage2d(train_Y[:, :, :, 0]), cmap = 'bone')\nax2.set_title('Seg Image')","8f54c524":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage2d(test_X[:, :, :, 0]), cmap = 'bone')\nax1.set_title('CXR Image')\nax2.imshow(montage2d(test_Y[:, :, :, 0]), cmap = 'bone')\nax2.set_title('Seg Image')","a36605de":"from skimage.segmentation import mark_boundaries\nfrom skimage.color import label2rgb\ntry:\n    from skimage.util.montage import montage2d\nexcept:\n    from skimage.util import montage2d\ndef add_boundary(in_img, in_seg, cmap = 'bone', norm = True, add_labels = True):\n    if norm:\n        n_img = (1.0*in_img-in_img.min())\/(1.1*(in_img.max()-in_img.min()))\n    else:\n        n_img = in_img\n    rgb_img = plt.cm.get_cmap(cmap)(n_img)[:, :, :3]\n    if add_labels:\n        return label2rgb(image = rgb_img, label = in_seg.astype(int), bg_label = 0)\n    else:\n        return mark_boundaries(image = rgb_img, label_img = in_seg.astype(int), color = (0, 1, 0), mode = 'thick')\ndef show_full_st(in_img, in_seg, gt_seg):\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (30, 10))\n    out_mtg = add_boundary(montage2d(in_img[:, :, :, 0]), \n                           montage2d(gt_seg[:, :, :, 0]>0.5))\n    ax1.imshow(out_mtg)\n    ax1.set_title('Ground Truth')\n    out_mtg = add_boundary(montage2d(in_img[:, :, :, 0]), \n                           montage2d(in_seg[:, :, :, 0]>0.5))\n    ax2.imshow(out_mtg)\n    ax2.set_title('Prediction')\n    out_mtg = montage2d(in_seg[:, :, :, 0]-gt_seg[:, :, :, 0])\n    ax3.imshow(out_mtg, cmap='RdBu', vmin=-1, vmax=1)\n    ax3.set_title('Difference')\ndef show_examples(n=1, with_roi = True):\n    roi_func = lambda x: x[:, \n                               OUT_DIM[0]\/\/2-32:OUT_DIM[0]\/\/2+32,\n                               OUT_DIM[1]\/\/2-64:OUT_DIM[1]\/\/2,\n                               :\n                              ]\n    for (test_X, test_Y), _ in zip(test_gen, range(n)):\n        seg_Y = net.predict(test_X)\n        show_full_st(test_X, seg_Y, test_Y)\n        show_full_st(roi_func(test_X), roi_func(seg_Y), roi_func(test_Y))\n\nshow_examples(1)","6fc7d68c":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('cxr_reg')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n                                   patience=4, \n                                   verbose=1, mode='min', epsilon=0.0001, cooldown=2, min_lr=1e-6)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","3e744233":"from IPython.display import clear_output\nloss_history = net.fit_generator(train_gen, \n                  steps_per_epoch=len(train_vol)\/\/train_X.shape[0],\n                  epochs = 50,\n                  validation_data = (test_vol, test_seg),\n                  callbacks=callbacks_list\n                 )\nclear_output()","277a122f":"net.load_weights(weight_path)\nnet.save('full_model.h5')","9a73acf3":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\nax1.plot(loss_history.history['loss'], '-', label = 'Loss')\nax1.plot(loss_history.history['val_loss'], '-', label = 'Validation Loss')\nax1.legend()\n\nax2.plot(100*np.array(loss_history.history['binary_accuracy']), '-', \n         label = 'Accuracy')\nax2.plot(100*np.array(loss_history.history['val_binary_accuracy']), '-',\n         label = 'Validation Accuracy')\nax2.legend()","80edd538":"show_examples(2)","9ad72f1b":"import pydicom\nfrom glob import glob\nbase_rsna_dir = os.path.join('..', 'input', 'rsna-pneumonia-detection-challenge')\ntest_mean, test_std = test_X.mean(), test_X.std()\ndef read_dicom_as_float(in_path):\n    out_mat = pydicom.read_file(in_path).pixel_array\n    norm_mat = (out_mat-1.0*np.mean(out_mat))\/np.std(out_mat)\n    # make the RSNA distribution look like the training distribution\n    norm_mat = norm_mat*test_std+test_mean\n    return np.expand_dims(norm_mat, -1).astype(np.float32)\nall_rsna_df = pd.DataFrame({'path': glob(os.path.join(base_rsna_dir, \n                                                      'stage_*_images', '*.dcm'))})\nall_rsna_df.sample(3)","063ff97b":"from keras import layers\nin_shape = read_dicom_as_float(all_rsna_df.iloc[0,0]).shape\nin_img = layers.Input(in_shape, name='DICOMInput')\nscale_factor = (2,2)\nds_dicom = layers.AvgPool2D(scale_factor)(in_img)\nunet_out = net(ds_dicom)\nus_out = layers.UpSampling2D(scale_factor)(unet_out)\nunet_big = Model(inputs=[in_img], outputs=[us_out])\nunet_big.save('big_model.h5')\nunet_big.summary()","8c509e16":"fig, m_axs = plt.subplots(2, 3, figsize = (10, 8), dpi=300)\nfor c_ax, (_, c_row) in zip(m_axs.flatten(), \n                            all_rsna_df.sample(6).iterrows()):\n    c_img = read_dicom_as_float(c_row['path'])\n    c_seg = unet_big.predict(np.expand_dims(c_img, 0))[0]\n    gray_img = c_img[:, :, 0]\n    gray_img = gray_img-1.0*np.mean(gray_img)\n    gray_img \/= 3.0*np.std(gray_img)\n    gray_img = (gray_img+0.5)\n    c_ax.imshow(label2rgb(image=np.clip(gray_img, 0, 1),\n                          label=c_seg[:, :, 0]\/\/0.25, bg_label=0))\n    c_ax.axis('off')\nfig.savefig('rsna_pneumonia_rois.png')","05a2e683":"import zipfile as zf\nfrom io import BytesIO\nfrom PIL import Image\nbatch_size = 12\nwith zf.ZipFile('masks.zip', 'w') as f:\n    for i, c_rows in tqdm(all_rsna_df.groupby(lambda x: x\/\/batch_size)):\n        cur_x = np.stack(c_rows['path'].map(read_dicom_as_float), 0)\n        cur_pred = np.clip(unet_big.predict(cur_x), 0, 1)\n        for out_img, (_, c_row) in zip(cur_pred[:, :, :, 0], c_rows.iterrows()):\n            arc_name = os.path.relpath(c_row['path'], base_rsna_dir)\n            arc_name, _ = os.path.splitext(arc_name)\n            out_pil_obj = Image.fromarray((255*out_img).astype(np.uint8))\n            out_obj = BytesIO()\n            out_pil_obj.save(out_obj, format='png')\n            out_obj.seek(0)\n            f.writestr('{}.png'.format(arc_name), out_obj.read(), zf.ZIP_STORED)","c603eef2":"!ls -lh *.zip","abfbdcb7":"# Create Training Data Generator\nHere we make a tool to generate training data from the X-ray scans","939b488c":"## Show Untrained Results\nHere we show random untrained results","5a1f8ff3":"# Apply to RSNA Data\nHere we load the RSNA data and apply the model to all of the images","b5e37562":"## Adding Augmentation\nHere we use augmentation to get more data into the model","40534e45":"### Training Data","b41da8f4":"# Overview\nHere we use the montgomery dataset for Tuberculosis (not very healthy lungs) since it includes lung segmentations as a basis for learning how to segment lungs in the pneumonia dataset. We then generate masks for all of the images which can be used in future steps for detecting pneumonia better. Here instead of using a standard U-net we use a Gaussian mixture as a model for the two lungs. We develop a basic gaussian mixture layer and then train it on the dataset.\n\n1. Build a layer for drawing gaussian-mixtures from parameters\n1. Organize the Training Data for Segmentation\n1. Build Augmentation Pipeline and Generators\n1. Build an encoder to produce two sets of parameters\n1. Feed parameters into Gaussian Mixtures and evaluated by BCE and DICE\n1. Train the Model\n1. Adapt model for full images\n1. Apply to RSNA Data","831f222c":"# Make a Simple Model\nHere we make a simple U-Net to create the lung segmentations","32a533f2":"### Validation Data","c791817b":"## Make Predictions and Export Zip\nHere we make all of the predictions and create a zip file with all of the masks","7208dc62":"### Show results on the training data","a696ccc1":"## Make sure the layers work\nHere we have a tiny bit of debug code to make sure the layers output what they are supposed to"}}