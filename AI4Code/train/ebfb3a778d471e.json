{"cell_type":{"4250dbcf":"code","914862c5":"code","5580443d":"code","d6fd1790":"code","0ae632cd":"code","bd3237c0":"code","c0666e5c":"code","3c20072c":"code","9e9b14d8":"code","ab1c539a":"code","960b6465":"code","c92c6fe0":"code","573f7eed":"code","d3f75b25":"code","68855249":"code","7503774c":"code","1533577e":"code","66320d0b":"code","57992123":"code","5fce0695":"code","4a874e6b":"code","e73d236e":"code","c4edfc57":"code","990660dc":"code","6722c274":"code","9ed1091c":"code","7070102b":"code","96a24495":"code","ec33138a":"code","50567e29":"code","fa6bb2d7":"markdown","c79bc27c":"markdown","0e718a5f":"markdown","908c0dae":"markdown","d79096df":"markdown","430b3627":"markdown","eaf829ad":"markdown","4f9441a2":"markdown","b473b5b1":"markdown","df181381":"markdown","2bcf0ea9":"markdown","531dc6bb":"markdown","b3c235d6":"markdown","a633a686":"markdown","6da112a4":"markdown","ed663619":"markdown","575462c8":"markdown","a615fe50":"markdown"},"source":{"4250dbcf":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","914862c5":"import pandas as pd\ntrain_data= pd.read_csv('\/kaggle\/input\/atis-airlinetravelinformationsystem\/atis_intents_train.csv',\n                       names= [\"target\", \"text\"])\n\ntest_data= pd.read_csv('\/kaggle\/input\/atis-airlinetravelinformationsystem\/atis_intents_test.csv',\n                       names= [\"target\", \"text\"])\n\ntrain_data","5580443d":"train_data.groupby(\"target\").count()","d6fd1790":"train_data= train_data.append(train_data.loc[train_data.target.isin([\"atis_flight_time\", \"atis_quantity\"]), :])","0ae632cd":"from sklearn.preprocessing import OneHotEncoder as OHE\n\ny_encoder= OHE().fit(np.array(train_data.target).reshape(-1,1))","bd3237c0":"ytr_encoded= y_encoder.transform(np.array(train_data.target).reshape(-1,1)).toarray()\nyts_encoded= y_encoder.transform(np.array(test_data.target).reshape(-1,1)).toarray()","c0666e5c":"import nltk","3c20072c":"train_data[\"lower_text\"]= train_data.text.map(lambda x: x.lower())\ntest_data[\"lower_text\"]= test_data.text.map(lambda x: x.lower())","9e9b14d8":"from nltk import word_tokenize\n\ntrain_data[\"tokenized\"]= train_data.lower_text.map(word_tokenize)\ntest_data[\"tokenized\"]= test_data.lower_text.map(word_tokenize)","ab1c539a":"from nltk.corpus import stopwords\nfrom string import punctuation\n\ndef remove_stop(strings, stop_list):\n    classed= [s for s in strings if s not in stop_list]\n    return classed\n\nstop= stopwords.words(\"english\")\nstop_punc= list(set(punctuation))+ stop\n\ntrain_data[\"selected\"]= train_data.tokenized.map(lambda df: remove_stop(df, stop_punc))\ntest_data[\"selected\"]= test_data.tokenized.map(lambda df: remove_stop(df, stop_punc))","960b6465":"from nltk.stem import PorterStemmer\n\ndef normalize(text):\n    return \" \".join(text)\n\nstemmer= PorterStemmer()\n\ntrain_data[\"stemmed\"]= train_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])\ntrain_data[\"normalized\"]= train_data.stemmed.apply(normalize)\n\ntest_data[\"stemmed\"]= test_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])\ntest_data[\"normalized\"]= test_data.stemmed.apply(normalize)","c92c6fe0":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer= Tokenizer(num_words= 10000)\ntokenizer.fit_on_texts(train_data.normalized)\n\ntokenized_train= tokenizer.texts_to_sequences(train_data.normalized)\ntokenized_test= tokenizer.texts_to_sequences(test_data.normalized)","573f7eed":"tokenizer.word_index.keys().__len__()","d3f75b25":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain_padded= pad_sequences(tokenized_train, maxlen= 20, padding= \"pre\")\ntest_padded= pad_sequences(tokenized_test, maxlen= 20, padding= \"pre\")","68855249":"train_padded.shape","7503774c":"#this function transform final processed text (columns padded) into 3D matrix (samples, steps, unique_words)\n#matrix contents one hot encoded words. Encoding was done for each step and based on unique words\n\ndef transform_x(data, tokenizer):\n    output_shape= [data.shape[0],\n                  data.shape[1],\n                  tokenizer.word_index.keys().__len__()]\n    results= np.zeros(output_shape)\n    \n    for i in range(data.shape[0]):\n        for ii in range(data.shape[1]):\n            results[i, ii, data[i,ii]-1]= 1\n    return results\n\nxtr_transformed= transform_x(train_padded, tokenizer)\nxts_transformed= transform_x(test_padded, tokenizer)","1533577e":"from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy as CC\nfrom tensorflow.keras.activations import relu, softmax\nfrom tensorflow.keras.initializers import he_uniform, glorot_uniform\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.regularizers import l2\n\n\nclass LSTMModel(object):\n    \n    def build_model(self, input_dim, output_shape, steps, dropout_rate, kernel_regularizer, bias_regularizer):\n        input_layer= Input(shape= (steps, input_dim))\n        \n        #make lstm_layer\n        lstm= LSTM(units= steps)(input_layer)\n        dense_1= Dense(output_shape, kernel_initializer= he_uniform(),\n                       bias_initializer= \"zeros\", \n                       kernel_regularizer= l2(l= kernel_regularizer),\n                       bias_regularizer= l2(l= bias_regularizer))(lstm)\n        x= BatchNormalization()(dense_1)\n        x= relu(x)\n        x= Dropout(rate= dropout_rate)(x)\n        o= Dense(output_shape, kernel_initializer= glorot_uniform(),\n                 bias_initializer= \"zeros\", \n                 kernel_regularizer= l2(l= kernel_regularizer), \n                 bias_regularizer= l2(l= bias_regularizer))(dense_1)\n        o= BatchNormalization()(o)\n        output= softmax(o, axis= 1)\n        \n        loss= CC()\n        metrics= AUC()\n        optimizer= Adam()\n        self.model= Model(inputs= [input_layer], outputs= [output])\n        self.model.compile(optimizer= optimizer, loss= loss, metrics= [metrics])\n        \n        \n    def train(self, x, y, validation_split, epochs):\n        self.model.fit(x, y, validation_split= validation_split, epochs= epochs)\n        \n    def predict(self, x):\n        return self.model.predict(x)","66320d0b":"steps= xtr_transformed.shape[1]\ndim= xtr_transformed.shape[2]\noutput_shape= ytr_encoded.shape[1]\n\nmodel= LSTMModel()\nmodel.build_model(input_dim= dim,\n                  output_shape= output_shape,\n                  steps= steps, \n                  dropout_rate= 0.5, \n                  bias_regularizer= 0.3, \n                  kernel_regularizer= 0.3)","57992123":"model.train(xtr_transformed, ytr_encoded,\n           0.2, 0)","5fce0695":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","4a874e6b":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","e73d236e":"model.train(xtr_transformed, ytr_encoded,\n           0.2,1)","c4edfc57":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","990660dc":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","6722c274":"model.train(xtr_transformed, ytr_encoded,\n           0.1,0)","9ed1091c":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","7070102b":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","96a24495":"model.train(xtr_transformed, ytr_encoded,\n           0.5,0)","ec33138a":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","50567e29":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","fa6bb2d7":"**Build Model**","c79bc27c":"**Convert text to lowercase**","0e718a5f":"**Pad Text**","908c0dae":"**Word Tokenize**","d79096df":"Train","430b3627":"**Stemming**","eaf829ad":"Test","4f9441a2":"# **LSTM Modelling**","b473b5b1":"**Resample training data**","df181381":"## case 2","2bcf0ea9":"**Create X Matrix (samples, steps, wordlist)**","531dc6bb":"## Case 3\n","b3c235d6":"**Evaluation**","a633a686":"**Tokenize with tensorflow**","6da112a4":"## **Text Preprocessing With NLTK and Tensorflow**","ed663619":"**Remove Stop Words**","575462c8":"## **Target One Hot Encoding**","a615fe50":"# **Data Preprocessing**"}}