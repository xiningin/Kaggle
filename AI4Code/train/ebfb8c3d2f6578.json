{"cell_type":{"6852476e":"code","133ecd9b":"code","dbb71882":"code","b4309400":"code","a41ee13a":"code","056e788b":"code","8e6899a3":"code","7d49c772":"code","9652a1c2":"code","ed5c53a8":"code","04980b6f":"code","3ba0b58f":"markdown","79806c48":"markdown","8992cbec":"markdown","02c36c5b":"markdown","13ce5cd1":"markdown","0f97eddf":"markdown","d9ededf7":"markdown","f20a64f4":"markdown","1537aa5e":"markdown","9c4d32a3":"markdown","34d38d72":"markdown","d4038757":"markdown"},"source":{"6852476e":"# Import Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","133ecd9b":"# Load data\ntrain = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", index_col=0)\ntrain.head()","dbb71882":"# Let's see the shape of the training data\ntrain.shape\n# >> Training data has 300000 rows and 25 columns","b4309400":"# Let's check for missing values\ntrain.isnull().sum()\n# >> The trainining dataset don't have any missing values (In reality you will have some missing values)","a41ee13a":"# Let's check for unique values in each column\ntrain.nunique()","056e788b":"fig, ax = plt.subplots(5, 2, sharex=True, figsize=(15,15), sharey=True)\ncat_no=0\nfor i in range(5):\n    for j in range(2):\n        temp_dict=train.groupby(f'cat{cat_no}')[f'cat{cat_no}'].count()\n        sns.barplot(ax=ax[i, j], x=temp_dict.keys(), y=temp_dict.values)\n        ax[i, j].set_title(f'cat{cat_no}')\n        for p in ax[i, j].patches:\n            ax[i, j].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.8, p.get_height()),\n                             ha='center', va='bottom', color='black', rotation=55, fontsize=12)\n        cat_no+=1\nfor axe in ax.flat:\n    axe.set(xlabel='Different categorical values', ylabel='Total number of values')\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor axe in ax.flat:\n    axe.label_outer()    ","8e6899a3":"fig, ax = plt.subplots(14, sharex=True, figsize=(15,40), sharey=True)\ncont_no=0\nfor i in range(14):\n        x = train[[f'cont{cont_no}']]\n        sns.histplot(ax=ax[i], data=train, x=f'cont{cont_no}', kde=True)\n        ax[i].set_title(f'Histogram of cont{cont_no}')\n        #\"\"\"\n        for p in ax[i].patches:\n            ax[i].annotate(f'{x.describe()}'.format(p.get_height()), (-0.2, 10000),\n                             ha='left', va='bottom', color='red', rotation=0, fontsize=10)\n            break\n        #\"\"\"        \n        cont_no+=1\n","7d49c772":"plt.figure(figsize=(15,5))\nx=train['target']\ng=sns.histplot(data=train, x='target', kde=True)\nplt.title(f'Histogram of target')\n#\"\"\"\nfor p in g.patches:\n    g.annotate(f'{x.describe()}'.format(p.get_height()), (0, 200),\n                             ha='left', va='bottom', color='red', rotation=0, fontsize=14)\n    break\n    \nplt.show()","9652a1c2":"sns.boxplot(data=train, x='target')","ed5c53a8":"cat_no=0\nfor i in range(10):\n    size_y = train[f'cat{cat_no}'].nunique()\/\/2\n    plt.figure(figsize=(15,size_y))\n    sns.boxplot(data=train, x='target', y=f'cat{cat_no}')\n    cat_no+=1\n    \n    plt.show()    ","04980b6f":"# Compute the correlation matrix\ncorr = train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","3ba0b58f":"### Boxplot for Comparison\nBoxplots are useful for comparing sets of observations. Let's see how Categorical variables effect Target distribution.","79806c48":"From the above graphs you can easily see that there is a huge imbalance in categorical classes. This imbalance in classes might biase our prediction. We'll try to solve this later. For now let's look at how the continuous features behave.","8992cbec":"It seems that, target has almost no correlation with cont0, cont1, cont13 and positive and negative correlation with other variables.","02c36c5b":"### Let's plot the target variable","13ce5cd1":"### Let's Look for Correlation between Traget with Continuous Variables","0f97eddf":"As you can see, how each classes in categorical variables affecting the overall distribution of the **target**. This will also have affect on our predictions. Let's think about dealing with them later.","d9ededf7":"## N.B: You shouldn't try to find out the distribution or relations in Test Dataset. As it can bias your model.","f20a64f4":"### Let's plot the Categorical Variables\n","1537aa5e":"For the 30-days-of-ML competition, we need to predict a continuous **target** based on a number of feature coulmns.\nThe training set has 10 categorical variables from cat0 - cat9 and 14 continuous variable from cont0 - cont13. ","9c4d32a3":"We can be aware of the outliers from Boxplots.","34d38d72":"In a ideal world, the distribution of the **target** should follow Gaussian Distribution or Bell shaped curve. But in reality, the distribution of **target** is skewed left, centered at about 8.19 with most of the data between 7.7 and 8.7, a range of roughly 10.3, and some outliers present below 7.","d4038757":"### Let's plot the Continuous variables"}}