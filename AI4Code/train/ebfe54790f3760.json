{"cell_type":{"5b370679":"code","8dbbc772":"code","8ab78721":"code","78163847":"code","5516e815":"code","1b426c67":"code","dd7794d9":"code","3a3f2099":"code","71048d28":"code","4a15e133":"markdown","db889b35":"markdown","d0f813e5":"markdown","ddd4d60c":"markdown","bdb77eb0":"markdown","0f603d2c":"markdown","67145ee1":"markdown","6075cb27":"markdown","3e0c9dcc":"markdown"},"source":{"5b370679":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom multiprocessing import Pool\nfrom nltk.corpus import wordnet\nimport pylab as pl\nimport seaborn as sns\n\ntqdm.pandas()","8dbbc772":"df = pd.read_csv('\/kaggle\/input\/1-million-reddit-comments-from-40-subreddits\/kaggle_RC_2019-05.csv')","8ab78721":"lemmatizer = WordNetLemmatizer()\n\ndef worker_tagging(text):\n    return nltk.pos_tag(word_tokenize(text))\n\nwith Pool(9) as p:\n    # NOTE: if you run this code yourself, tqdm is strugging a bit to update with the multiprocessing\n    # Do not worry if the progress bar gets stuck, it updates by BIG increments\n    tagged = p.map(worker_tagging, tqdm(list(df.body)))\n\ndf['tagged'] = tagged","78163847":"def get_wordnet_pos(treebank_tag):\n    # nltk taggers uses the treebank tag format, but WordNetLemmatizer needs a different \\ simpler format\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n\ndef lemmatize(lemmatizer, token_tag):\n    lemmas = []\n    for i, j in token_tag:\n        j = get_wordnet_pos(j)\n        try:\n            lemmas.append(lemmatizer.lemmatize(i, j))\n        except KeyError:  # some tags are not known by WordNetLemmatizer\n            lemmas.append(i)\n    return lemmas\n\ndf['tokens'] = df.tagged.progress_apply(lambda r: lemmatize(lemmatizer, r))\n","5516e815":"from collections import Counter, defaultdict\n\nCATEGORY_HEADER = 'subreddit'\ntokens_counter = Counter()\ncategory_tokens_counter = defaultdict(Counter)\n_ = df.progress_apply(lambda r: (tokens_counter.update(r['tokens']), category_tokens_counter[r[CATEGORY_HEADER]].update(r['tokens'])), axis=1)\nnb_tokens = sum(tokens_counter.values())\nnb_token_per_category = {category: sum(c.values()) for category, c in category_tokens_counter.items()}","1b426c67":"TOPN_TOKENS = 3000\nprint('keeping %s tokens out of %s' % (TOPN_TOKENS, nb_tokens))\n\ndef representativeness(token, tokens_counter, category_tokens_counter, nb_tokens, nb_token_per_categ):\n    representativeness_scores = {\n        categ: category_tokens_counter.get(categ).get(token, 0) \/ tokens_counter.get(token) * nb_tokens \/ nb_token_per_categ[categ] for categ in category_tokens_counter.keys()\n    }\n    representativeness_scores['token'] = token\n    representativeness_scores['token_count'] = tokens_counter.get(token)\n    return representativeness_scores\n\nrepresentativeness_df = pd.DataFrame([representativeness(x[0], tokens_counter, category_tokens_counter, nb_tokens, nb_token_per_category) for x in tokens_counter.most_common(TOPN_TOKENS)])\nrepresentativeness_df.sort_values(by='token_count', inplace=True, ascending=False)","dd7794d9":"BAN_SET = {'\/','*','^'}\n\ndef ban_token(token, ban_set):\n    return bool(set(token).intersection(ban_set))\n\nrepresentativeness_df['ban'] = representativeness_df.token.apply(lambda r: ban_token(r, BAN_SET))\nrepresentativeness_df = representativeness_df[representativeness_df.ban == False]\nrepresentativeness_df = representativeness_df.set_index('token')","3a3f2099":"TOPN_PER_SUB = 12\nMAX_VISUAL_TOKEN_LEN = 12\n\nfig, axes = pl.subplots(7, 6, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')\naxes = axes.flatten()\n[pl.setp(ax.get_xticklabels(), rotation=90) for ax in axes]\npl.subplots_adjust(wspace=0.8)\npl.subplots_adjust(hspace=0.4)\nfor i, subreddit in enumerate(category_tokens_counter.keys()):\n    sorted_scores = representativeness_df[subreddit].sort_values()\n    topn = sorted_scores.tail(TOPN_PER_SUB)\n    xlabels = [i if len(i) < MAX_VISUAL_TOKEN_LEN else i[:MAX_VISUAL_TOKEN_LEN-2] + '..' for i in topn.index ]\n    sns.barplot(topn.values, xlabels, ax=axes[i])\n    axes[i].set_title(subreddit)\npl.title(\"Most over used words per subreddit (top %s words)\" % TOPN_TOKENS)\n","71048d28":"fig, axes = pl.subplots(7, 6, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')\naxes = axes.flatten()\n[pl.setp(ax.get_xticklabels(), rotation=90) for ax in axes]\nfig.tight_layout()\nfor i, subreddit in enumerate(category_tokens_counter.keys()):\n    sorted_scores = representativeness_df[subreddit].sort_values()\n    representativeness_df[subreddit].hist(ax=axes[i], bins=100)\n    axes[i].set_title(subreddit)\n    axes[i].set_yscale('log')\n    axes[i].set_xlim(0, 60)\npl.title(\"Representativeness distribution \/ subreddit\")","4a15e133":"# NLP: Basic text cleaning\n\n1. Tokenization\n2. Pos-Tagging\n3. Lemmas\n\nEven tho I usually use `spacy` these days, I choose to use `NLTK` here since, when doing the tokenization and pos-tag together in multiprocess, it runs quite a bit faster.","db889b35":"# Introductory Note\n\n### Objectives\n* Mostly having fun with the data at hand\n* Seeing if we can uncover different trends from the various subreddits.\n* Use as a basic NLP tutorial\n* Giving people visualization ideas\n\n### On performance\nThe data is small enough that I am charging everything in RAM and often using the code that I find the clearer even when it isn't the most RAM effective (eg not processing data in batch etc...). If you are interested for more efficient ways to process the data let me know.","d0f813e5":"# Tokens representativeness\nGiven a word\/token how representative is it of a given class\/category (here subreddit).  \nHere I use a custom which is:  \nIn average, how many times more is this token present in the given subreddit compared to the overall corpus.  \nThe formula is pretty simple:  \n\n>     R = (word_count(class) * nb_words(GLOBAL)) \/ (word_count(GLOBAL) * nb_words(class))\n\nThis constrains the value to\n`[0, nb_words(GLOBAL) \/ nb_words(subreddit)]`  \n`0` -> token never found in the corpus subset corresponding to my class  \n`nb_words(GLOBAL) \/ nb_words(subreddit)` -> token ONLY found in the corpus subset corresponding to my class\n\nNOTE: there is plenty of other ways to find such words (e.g using TFIDF), but I wanted to stay on a very simple and intuitive formula. ","ddd4d60c":"### Representativeness\n\nWe compute the representativeness for each couple `[token, subreddit]`  \n> Only the `TOPN_TOKENS` most frequents tokens are kept (you can play with the value) since tokens with a low count are too susceptible to out of the ordinary events and the representativeness metric doesn't make any sense (plenty of tokens would the reach the minimum and maximum values or R if they are present only a few times through the corpus)","bdb77eb0":"### Top N most representatives tokens \/ subreddit","0f603d2c":"### Representativeness distribution \/ subreddit","67145ee1":"### Tokens ban\nWe ban certain tokens specific to reddit formatting that were not processed correctly during Tokenization (certain URLS, markdown etc...) and that happen to be quite frequent. They were ot removed earlier because even tho they mostly add noise for the coming graph they can still be interesting to study (these tokens are also pretty category-specific)","6075cb27":"### Creating token counts\nWe build our corpus token count and the token counts per category","3e0c9dcc":"### Example code with spacy: lemmas\n\nThis is not the most efficient way to do it, I am not really making use of the fact that `nlp.pipe` returns a generator, but it is mostly for the curious and to show spacy multiprocessing capabilities.  \nAs a matter of fact I think the following approach barely bits in RAM (16 Gb), if it does at all.\n\n```\nimport spacy\nnlp = spacy.load('en')\ndocs = nlp.pipe(df.body, n_process=10)\ndf['tokens'] = [[i.lemma_ for i in r] for r in tqdm(docs)]\n```"}}