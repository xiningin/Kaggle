{"cell_type":{"d82512b0":"code","aded10d2":"code","283b9fa3":"code","942399f8":"code","fa6d8f17":"code","bee854b3":"code","e5c57378":"code","a9c60d50":"code","4fdbd512":"code","106ec1b4":"code","44d6e314":"code","212af10b":"code","47f8e800":"code","a957a11b":"code","d77a6e34":"code","cec31f8e":"code","567f0316":"code","1471a814":"code","d7c531a9":"code","305c6613":"code","1213483f":"code","0de598d9":"code","088aead6":"code","59555155":"code","c31deb59":"code","5c8814b3":"code","76bd55cf":"code","44a71800":"code","ba2217bc":"code","f9647a97":"code","601f0b97":"code","aa8a8c52":"code","ba9e3a4e":"code","bb054fe6":"code","e99d4431":"code","215156f5":"code","21f99784":"code","e9307612":"code","e01c6eaf":"code","a29b82aa":"code","53d94539":"code","2883f5ed":"code","77b475a2":"code","d085aefc":"code","1f615d2c":"code","e65875de":"code","ba8816fa":"code","6c26f026":"code","56b33fcf":"markdown","31a22d2d":"markdown","56401328":"markdown","ef158505":"markdown","ad5a6dad":"markdown","a4619a8a":"markdown","0bddc3f3":"markdown","08139382":"markdown","88a14100":"markdown","2e9ae3e6":"markdown","20f5b163":"markdown","da0c7a79":"markdown","b033c04d":"markdown","ffb3ab7f":"markdown","0b94ed95":"markdown","d9f14060":"markdown","09b778eb":"markdown","1e92ffe5":"markdown","2877d9cf":"markdown","b45f1d20":"markdown","a540c1b7":"markdown","fb891a89":"markdown","bcda0a6a":"markdown","6cad3baa":"markdown","656868dc":"markdown","ae09299a":"markdown","2e9ae9e2":"markdown","b1751f44":"markdown","43bf2d8e":"markdown","e548a224":"markdown","02a8bf90":"markdown","40c33f89":"markdown","c61b1cc2":"markdown","378b48d7":"markdown","2afce86d":"markdown","e62d4f64":"markdown"},"source":{"d82512b0":"import numpy as np\n\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable \n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch","aded10d2":"channels = 1 # suggested default : 1, number of image channels (gray scale)\nimg_size = 28 # suggested default : 28, size of each image dimension\nimg_shape = (channels, img_size, img_size) # (Channels, Image Size(H), Image Size(W))","283b9fa3":"latent_dim = 100 # suggested default. dimensionality of the latent space","942399f8":"cuda = True if torch.cuda.is_available() else False # GPU Setting","fa6d8f17":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        def block(input_features, output_features, normalize=True):\n            layers = [nn.Linear(input_features, output_features)]\n            if normalize: # Default\n                layers.append(nn.BatchNorm1d(output_features, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True)) # inplace=True : modify the input directly. It can slightly decrease the memory usage.\n            return layers # return list of layers\n        \n        self.model = nn.Sequential(\n            *block(latent_dim, 128, normalize=False), # Asterisk('*') in front of block means unpacking list of layers - leave only values(layers) in list\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))), # np.prod(1, 28, 28) == 1*28*28\n            nn.Tanh() # result : from -1 to 1\n        )\n\n    def forward(self, z): # z == latent vector(random input vector)\n        img = self.model(z) # (64, 100) --(model)--> (64, 784)\n        img = img.view(img.size(0), *img_shape) # img.size(0) == N(Batch Size), (N, C, H, W) == default --> (64, 1, 28, 28)\n        return img","bee854b3":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512), # (28*28, 512)\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid() # result : from 0 to 1\n        )\n    \n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1) #flatten -> from (64, 1, 28, 28) to (64, 1*28*28)\n        validity = self.model(img_flat) # Discriminate -> Real? or Fake? (64, 784) -> (64, 1)\n        return validity","e5c57378":"adversarial_loss = torch.nn.BCELoss()","a9c60d50":"generator = Generator()\ndiscriminator = Discriminator()","4fdbd512":"generator","106ec1b4":"discriminator","44d6e314":"if cuda:\n    generator.cuda()\n    discriminator.cuda()\n    adversarial_loss.cuda()","212af10b":"import pandas as pd\nfrom torch.utils.data import Dataset","47f8e800":"class DatasetMNIST(Dataset): # inherit abstract class - 'Dataset'\n    \n    def __init__(self, file_path, transform=None):\n        self.data = pd.read_csv(file_path)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        # load image ad ndarray type (H, W, C)\n        # be carefull for converting dtype to np.uint8 (Unsigned integer (0 to 255))\n        # in this example, We use ToTensor(), so we define the numpy array like (H, W, C)\n        \n        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((28,28,1))\n        label = self.data.iloc[index, 0]\n        if self.transform is not None:\n            image = self.transform(image)\n        \n        return image, label\n        ","a957a11b":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","d77a6e34":"train","cec31f8e":"for index in range(1, 6): # N : 5 (Number of Image)\n    temp_image = train.iloc[index, 1:].values.astype(np.uint8).reshape((28,28,1))\n    temp_label = train.iloc[index, 0]\n    print('Shape of Image : ',temp_image.shape)\n    print('label : ', temp_label)","567f0316":"dataset = DatasetMNIST(file_path='..\/input\/digit-recognizer\/train.csv', \n                       transform=transforms.Compose(\n                           [#transforms.Resize(img_size), # Resize is only for PIL Image. Not for numpy array\n                            transforms.ToTensor(), # ToTensor() : np.array (H, W, C) -> tensor (C, H, W)\n                            transforms.Normalize([0.5],[0.5])]\n                       ))","1471a814":"temp_img, _ =  dataset.__getitem__(0) # We don't need label, so _","d7c531a9":"temp_img.size() # before ToTensor() : (28,28,1), after : (1,28,28)","305c6613":"temp_img.max(), temp_img.min() # before Normalize([0.5],[0.5]) : 0 ~ 1, after : -1 ~ 1","1213483f":"batch_size = 64 # suggested default, size of the batches\ndataloader = DataLoader( # torch.utils.data.DataLoader\n    dataset,\n    batch_size=batch_size,\n    shuffle=True\n)","0de598d9":"temp_images, _ = iter(dataloader).next() # We don't use label, so _\nprint('images shape on batch size = {}'.format(temp_images.size()))","088aead6":"# suggested default - beta parameters (decay of first order momentum of gradients)\nb1 = 0.5\nb2 = 0.999\n\n# suggested default - learning rate\nlr = 0.0002 ","59555155":"optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1,b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1,b2))","c31deb59":"Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor","5c8814b3":"from tqdm.notebook import tqdm","76bd55cf":"# Visualize result\nimport matplotlib.pyplot as plt","44a71800":"n_epochs = 10 # suggested default = 200\nfor epoch in range(n_epochs):\n    for i, (imgs, _) in enumerate(tqdm(dataloader)): # This code(enumerate) is dealt with once more in the *TEST_CODE below.\n                                                     # Used 'tqdm' for showing progress \n        \n        # Adversarial ground truths (For more detail, refer *Read_More below)\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) # imgs.size(0) == batch_size(1 batch) == 64, *TEST_CODE\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # And Variable is for caclulate gradient. In fact, you can use it, but you don't have to. \n                                                                                # requires_grad=False is default in tensor type. *Read_More\n        \n        # Configure input\n        real_imgs = imgs.type(Tensor) # As mentioned, it is no longer necessary to wrap the tensor in a Variable.\n      # real_imgs = Variable(imgs.type(Tensor)) # requires_grad=False, Default! It's same.\n    \n# ------------\n# Train Generator\n# ------------\n        optimizer_G.zero_grad()\n        \n        # sample noise 'z' as generator input\n        z = Tensor(np.random.normal(0, 1, (imgs.shape[0],latent_dim))) # Random sampling Tensor(batch_size, latent_dim) of Gaussian distribution\n        # z.shape == torch.Size([64, 100])\n        \n        # Generate a batch of images\n        gen_imgs = generator(z)\n        # gen_imgs.shape == torch.Size([64, 1, 28, 28])\n        \n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid) # torch.nn.BCELoss() compare result(64x1) and valid(64x1, filled with 1)\n        \n        g_loss.backward()\n        optimizer_G.step()\n        \n# ------------\n# Train Discriminator\n# ------------\n        optimizer_D.zero_grad()\n        \n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid) # torch.nn.BCELoss() compare result(64x1) and valid(64x1, filled with 1)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) # We are learning the discriminator now. So have to use detach() \n                                                                             \n        d_loss = (real_loss + fake_loss) \/ 2\n        \n        d_loss.backward()# If didn't use detach() for gen_imgs, all weights of the generator will be calculated with backward(). \n        optimizer_D.step()\n         \n    \n\n# ------------\n# Real Time Visualization (While Training)\n# ------------\n        \n        sample_z_in_train = Tensor(np.random.normal(0, 1, (imgs.shape[0],latent_dim)))\n        # z.shape == torch.Size([64, 100])\n        sample_gen_imgs_in_train = generator(sample_z_in_train).detach().cpu()\n        # gen_imgs.shape == torch.Size([64, 1, 28, 28])\n        \n        if ((i+1) % 200) == 0: # show while batch - 200\/657, 400\/657, 600\/657\n            nrow=1\n            ncols=5\n            fig, axes = plt.subplots(nrows=nrow,ncols=ncols, figsize=(8,2))\n            plt.suptitle('EPOCH : {} | BATCH(ITERATION) : {}'.format(epoch+1, i+1))\n            for ncol in range(ncols):\n                axes[ncol].imshow(sample_gen_imgs_in_train.permute(0,2,3,1)[ncol], cmap='gray')\n                axes[ncol].axis('off')\n            plt.show()\n    print(\n        \"[Epoch: %d\/%d] [Batch: %d\/%d] [D loss: %f] [G loss: %f]\"\n        % (epoch+1, n_epochs, i+1, len(dataloader), d_loss.item(), g_loss.item())\n    )","ba2217bc":"# result of enumerate\ncount = 1\nfor i, (imgs,label) in enumerate(dataloader):\n    print('Shape of Batch Images : \\n', imgs.shape)\n    print('Labels (1~64) : \\n', label)\n    print('-'*100)\n    if count == 5:\n        break\n    else:\n        count += 1","f9647a97":"Tensor(10,1) # Just 10 for a quick look. \n             # We dealt with Tensor(imgs.size(0),1) above. Tensor(64,1) <-- len(H) == batch_size == 64, len(W) == 1","601f0b97":"Tensor(10,1).fill_(1.0) # _ means inplace, fill the Tensor with 1.0","aa8a8c52":"sample_img = iter(dataloader).next()[0]","ba9e3a4e":"sample_img.shape, sample_img.dtype","bb054fe6":"sample_img.requires_grad # default : False ","e99d4431":"Variable(sample_img).requires_grad # exactly same","215156f5":"sample_img.requires_grad_(True) # set requires_grad to True, _ means inplace","21f99784":"sample_img.requires_grad # Yes. requires_grad changed correctly. you don't need wrapping tensor with Variable. ","e9307612":"np.random.normal(0,1,(64,100))","e01c6eaf":"np.random.normal(0,1,(64,100)).shape","a29b82aa":"# latent vector\nsample_z = Tensor(np.random.normal(0, 1, (64,100)))\nsample_z.shape","53d94539":"# generated images\nsample_gen_imgs = generator(sample_z)\nsample_gen_imgs.shape","2883f5ed":"# discrimination result\nsample_discrim_result = discriminator(sample_gen_imgs)\nsample_discrim_result.shape","77b475a2":"adversarial_loss","d085aefc":"sample_valid.shape","1f615d2c":"sample_valid = Tensor(64,1).fill_(1.0)\nsample_g_loss = adversarial_loss(sample_discrim_result, sample_valid)\nsample_g_loss","e65875de":"!pip install torchviz\nimport torchviz","ba8816fa":"X = torch.ones((28,28), dtype=torch.float32, requires_grad=True)\nsquare_X = X**2\ncubic_X = X**3\n\nresult = (square_X+cubic_X).sum()\n\ntorchviz.make_dot(result)\n","6c26f026":"X = torch.ones((28,28), dtype=torch.float32, requires_grad=True)\nsquare_X = X**2\ncubic_X = X.detach()**3\n\nresult = (square_X+cubic_X).sum()\n\ntorchviz.make_dot(result)","56b33fcf":"### Step 10. Training","31a22d2d":"> TEST CODE : Read CSV file","56401328":"> TEST CODE : shape of z and gen_imgs and discriminator(gen_imgs)","ef158505":"This kernel is for those new to gan.\n\nAnd It was coded with pytorch, and all the code was converted into the familiar Jupyter notebook form for data analysts and machine learning engineers by referring to the gan official Python code.\n\nI hope that many Kaglers will be interested in Generative Adversarial Networks(GAN), and that it will be shared and helpful to more people. So let's get started!","ad5a6dad":"### Step 6. Initialize generator and Discriminator","a4619a8a":"### Step 3. Define Generator","0bddc3f3":"> TEST CODE : transform from table data to image data with basic preprocessing","08139382":"Since we use the Kaggle dataset, it is necessary to read the csv file and convert it into an image format.\n\n\nFor the code to load and convert Kaggle MNIST data, I referred to [Pytorch Dataset and DataLoader](https:\/\/www.kaggle.com\/pinocookie\/pytorch-dataset-and-dataloader).\n","88a14100":"### Step 9. Define optimizers","2e9ae3e6":"--------------","20f5b163":"However, it can be confusing here. Obviously, the derivative is computed through the loss, but not a single `requires_grad` is set to `true`.\n\nRemember, It's different from `model's parameter`. Internally, the parameters of each Module are stored in Tensors with `requires_grad=True` !","da0c7a79":"![image](https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpMdme%2FbtqA1ArBCOy%2FqqGg7IvV0hpqVkvBuEFpJK%2Fimg.png)","b033c04d":"### Step 1. Import Libraries","ffb3ab7f":"## Index\n```\nStep 1. Import Libraries\nStep 2. Initial Setting\nStep 3. Define Generator\nStep 4. Define Discriminator\nStep 5. Define Loss Function\nStep 6. Initialize Generator and Discriminator\nStep 7. GPU Setting\nStep 8. Configure Data Loader\nStep 9. Define Optimizers\nStep 10. Training\n```\n---","0b94ed95":"> TEST CODE : detach()\n- refer : [What is PyTorch '.detach()' method? - theroyakash, 2020-11](https:\/\/dev.to\/theroyakash\/what-is-pytorch-detach-method-15oo)","d9f14060":"> TEST CODE : enumerate >> [docs.python.org\/enumerate](https:\/\/docs.python.org\/3\/library\/functions.html#enumerate)","09b778eb":"### Step 8. Configure Data Loader","1e92ffe5":"> TEST CODE : Ramdom Sampling (H,W) from a normal(Gaussian) distribution ","2877d9cf":"![image](https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHwq72%2FbtqAY6E0wYb%2FBFRgtJWTY3Ij9BKks7vsM1%2Fimg.png)","b45f1d20":"### Step 7. GPU Setting","a540c1b7":"\nSo far, we have completed the basic model of gan and tested the model through mnist data.\n\nI plan to implement and test more various gan models from the very basics through this series, so let's study together!\n\nThank you and Enjoy your Kaggle! :)","fb891a89":"> Read More\n- [What is the difference between nn.ReLU() and nn.ReLU(inplace=True)?](https:\/\/discuss.pytorch.org\/t\/whats-the-difference-between-nn-relu-and-nn-relu-inplace-true\/948)\n- [Tanh](https:\/\/wiki.documentfoundation.org\/Documentation\/Calc_Functions\/TANH)\n- [Unpacking Operators in Python](https:\/\/towardsdatascience.com\/unpacking-operators-in-python-306ae44cd480)","bcda0a6a":"> Read More\n- [BCELoss(Binary Cross Entropy Loss)](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.BCELoss.html)","6cad3baa":"> TEST CODE : \n- Read CSV file\n- Tensor(imgs.size(0),1)\n- Tensor(imgs.size(0),1).fill_(1.0)","656868dc":"> TEST CODE : Construct image from csv file","ae09299a":"### Step 5. Define Loss Function","2e9ae9e2":"# Generative Adversarial Networks(GAN) - PyTorch Tutorial","b1751f44":"> TEST CODE : g_loss","43bf2d8e":"### Step 2. Initial setting","e548a224":"> Read More\n1. [torchvision.transforms](https:\/\/pytorch.org\/vision\/stable\/transforms.html)\n    - .ToTensor | Convert a PIL Image or numpy.ndarray to tensor. This transform does not support torchscript.\n    - .ToTensor | Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8","02a8bf90":"> TEST CODE : Variable and requires_grad","40c33f89":"> Read More\n- [Sigmoid](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function)\n- [pytorch in x = x.view (x.size (0), -1) understanding](https:\/\/www.programmersought.com\/article\/11412923760\/)","c61b1cc2":"> TEST CODE : Define dataloader that can load image by batch","378b48d7":"## MAIN Reference\n1. [PyTorch-GAN | Github\/eriklindernoren | Collection of PyTorch implementations of GAN](https:\/\/github.com\/sw-song\/PyTorch-GAN)\n","2afce86d":"> Read More\n- [What is Ground Truth? - Definition from Techpedia](https:\/\/www.techopedia.com\/definition\/32514\/ground-truth)\n- [tqdm. A Fast, Extensible Progress Bar for Python and CLI](https:\/\/github.com\/tqdm\/tqdm)\n- [Variables are no longer necessary to use autograd with tensors](https:\/\/pytorch.org\/docs\/stable\/autograd.html#variable-deprecated)\n- [np.random.normal(loc=0.0, scale=1.0, size=None)](https:\/\/numpy.org\/doc\/stable\/reference\/random\/generated\/numpy.random.normal.html)\n\n","e62d4f64":"### Step 4. Define Discriminator"}}