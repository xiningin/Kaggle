{"cell_type":{"e1867b99":"code","9a063df7":"code","1d9164d4":"code","2d9f3059":"code","a82b42f5":"code","97759bd4":"code","7725391c":"code","b1f8a469":"code","ae505c79":"code","62b755c1":"code","8c138e50":"code","890ccc04":"code","02400698":"code","2f4ca8c1":"code","9050df4a":"code","7fb4e977":"code","5a10c1f1":"code","a1881557":"code","7b164ea4":"code","25130b29":"code","6c5d18ca":"code","301f61ec":"code","75f73738":"code","dca28c51":"code","dfc53b89":"code","7b74b387":"code","1b617117":"code","7ab9ce10":"code","dec215c7":"code","9101ad3d":"code","1e4aa0b1":"code","06d8154e":"code","757c982a":"code","466fbd7f":"code","b89626ea":"code","0da6a79b":"code","2d72b0fa":"code","a95c90f7":"code","1157c318":"code","3367efc5":"code","b12cb9bb":"code","e211fed5":"code","559104a6":"code","4160d102":"code","3a13bafc":"code","ff45341e":"code","18f0b2d0":"code","9efae26f":"code","f2ad1b21":"code","6e1ae569":"code","c95003c6":"code","5a56ccd5":"code","1d90204a":"code","1c851a51":"code","4032513f":"code","daabeaad":"code","1eb15bff":"code","101ce834":"code","703e4b38":"code","261857d9":"code","404ebd82":"code","32c7b50c":"code","41fbc4e8":"code","700fb2fd":"code","c5034555":"code","f6b7c889":"code","1ddc0a41":"code","9a588f11":"code","062ad16c":"code","cee6818d":"code","a5b704ae":"code","ed28ee52":"code","9df4ce48":"markdown","0d76f974":"markdown","84d7e150":"markdown","26663ceb":"markdown","8e3a373d":"markdown","e567a697":"markdown","c00abdf1":"markdown","0bfae3e0":"markdown","113dc0d6":"markdown","549e2112":"markdown","48a64936":"markdown","107f1319":"markdown","5d0d4620":"markdown","5b5eae5d":"markdown","8035154b":"markdown","7f373474":"markdown","10913db3":"markdown","219cc1b3":"markdown","03094b12":"markdown","85cbdfe5":"markdown","aa8c1a70":"markdown","7f38a99a":"markdown","977637f6":"markdown"},"source":{"e1867b99":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, Lasso, ElasticNetCV, BayesianRidge, LassoLarsIC\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nimport math","9a063df7":"data_train = pd.read_csv(\"\/kaggle\/input\/train.csv\")\ndata_train","1d9164d4":"data_test = pd.read_csv(\"\/kaggle\/input\/test.csv\")\ndata_test","2d9f3059":"y = data_train.SalePrice\ntrain_without_response = data_train[data_train.columns.difference(['SalePrice'])]\nresult = pd.concat([train_without_response,data_test], ignore_index=True)\nresult","a82b42f5":"result.head()","97759bd4":"result.tail()","7725391c":"result.info()","b1f8a469":"result.shape #Numero di colonne e righe ","ae505c79":"result.columns","62b755c1":"result.describe()","8c138e50":"y.describe()","890ccc04":"sns.distplot(data_train['SalePrice']);\nprint(\"Skewness coeff. is: %f\" % data_train['SalePrice'].skew())\nprint(\"Kurtosis coeff. is: %f\" % data_train['SalePrice'].kurt())","02400698":"data_year_trend = pd.concat([data_train['SalePrice'], data_train['YearBuilt']], axis=1)\ndata_year_trend.plot.scatter(x='YearBuilt', y='SalePrice', ylim=(0,800000));","2f4ca8c1":"data_bsmt_trend = pd.concat([data_train['SalePrice'], data_train['TotalBsmtSF']], axis=1)\ndata_bsmt_trend.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0,800000));","9050df4a":"data_GrLivArea_trend = pd.concat([data_train['SalePrice'], data_train['GrLivArea']], axis=1)\ndata_GrLivArea_trend.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000));","7fb4e977":"data_PoolArea_trend = pd.concat([data_train['SalePrice'], data_train['PoolArea']], axis=1)\ndata_PoolArea_trend.plot.scatter(x='PoolArea', y='SalePrice', ylim=(0,800000));","5a10c1f1":"data = pd.concat([data_train['SalePrice'], data_train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","a1881557":"corr_matrix = result.corr()\nf, ax1 = plt.subplots(figsize=(12,9)) \nax1=sns.heatmap(corr_matrix,vmax = 0.9); ","7b164ea4":"corrmat = data_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(9,9))\ng = sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","25130b29":"var = data_train[data_train.columns[1:]].corr()['SalePrice'][:]\nvar","6c5d18ca":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(data_train[cols], height = 2.5)\nplt.show();","301f61ec":"total_null = result.isnull().sum().sort_values(ascending=False) #First sum and order all null values for each variable\npercentage = (result.isnull().sum()\/result.isnull().count()).sort_values(ascending=False) #Get the percentage\nmissing_data = pd.concat([total_null, percentage], axis=1, keys=['Total', 'Percentage'])\nmissing_data.head(20)","75f73738":"result = result.drop((missing_data[missing_data[\"Percentage\"] > 0.15]).index,1) #Drop All Var. with null values > 1\n#data_train = data_train.drop(data_train.loc[data_train['Electrical'].isnull()].index) #Delete the single null value in Electrical\nresult.isnull().sum()","dca28c51":"del result[\"KitchenAbvGr\"]\ndel result[\"YrSold\"]\ndel result[\"MoSold\"]\ndel result[\"MiscVal\"]\ndel result[\"ScreenPorch\"]\ndel result[\"X3SsnPorch\"]\ndel result[\"BsmtHalfBath\"]\ndel result[\"LowQualFinSF\"]\ndel result[\"OverallCond\"]\ndel result[\"EnclosedPorch\"]\ndel result[\"MSSubClass\"]\ndel result[\"X1stFlrSF\"]\ndel result[\"YearBuilt\"]\ndel result[\"YearRemodAdd\"] \ndel result[\"BsmtFinSF2\"] #0 variance\ndel result[\"BsmtFinSF1\"] #Because BsmtFinSF1 + BsmtUnfSF + BsmtFinSF2 = TotalBsmtSF\ndel result[\"BsmtUnfSF\"] #Because BsmtFinSF1 + BsmtUnfSF + BsmtFinSF2 = TotalBsmtSF\ndel result[\"PoolArea\"] #0 variance\ndel result[\"GarageYrBlt\"] #Dropped for the same reason of YearBuilt, it might mislead our predictions\ndel result[\"GarageCond\"] #0 Variance\ndel result[\"GarageArea\"] #High Correlation\ndel result[\"TotRmsAbvGrd\"] #High Correlation\nresult","dfc53b89":"result['ExterCond'].value_counts()","7b74b387":"del result[\"Street\"]\ndel result[\"LandContour\"]\ndel result[\"Utilities\"]\ndel result[\"LandSlope\"]\ndel result[\"Condition2\"]\ndel result[\"RoofMatl\"]\ndel result[\"BsmtFinType2\"] #0 variance\ndel result[\"Electrical\"] #0 Variance\ndel result[\"Condition1\"]#Too many levels versione 2\ndel result[\"BldgType\"]#versione 2\ndel result[\"HouseStyle\"]#versione 2\ndel result[\"Exterior1st\"]#versione 2\ndel result[\"Exterior2nd\"]#versione 2\ndel result[\"Foundation\"]#versione 2\ndel result[\"CentralAir\"]#0 variance\ndel result[\"Functional\"]#0 variance\ndel result[\"SaleType\"]#0 variance\ndel result[\"SaleCondition\"]#0 variance\ndel result[\"RoofStyle\"]#0 variance\nresult","1b617117":"result.shape","7ab9ce10":"#Here we encode ExterQual in a rank\nresult.loc[result['ExterQual'] == \"Ex\", 'ExterQual'] = 5\nresult.loc[result['ExterQual'] == \"Gd\", 'ExterQual'] = 4\nresult.loc[result['ExterQual'] == \"TA\", 'ExterQual'] = 3\nresult.loc[result['ExterQual'] == \"Fa\", 'ExterQual'] = 2\nresult.loc[result['ExterQual'] == \"Po\", 'ExterQual'] = 1\nresult['ExterQual']","dec215c7":"#Here we encode ExterCond in Rank\nresult.loc[result['ExterCond'] == \"Ex\", 'ExterCond'] = 5\nresult.loc[result['ExterCond'] == \"Gd\", 'ExterCond'] = 4\nresult.loc[result['ExterCond'] == \"TA\", 'ExterCond'] = 3\nresult.loc[result['ExterCond'] == \"Fa\", 'ExterCond'] = 2\nresult.loc[result['ExterCond'] == \"Po\", 'ExterCond'] = 1\nresult['ExterCond']","9101ad3d":"#Here we encode HeatingQC in Rank\nresult.loc[result['HeatingQC'] == \"Ex\", 'HeatingQC'] = 5\nresult.loc[result['HeatingQC'] == \"Gd\", 'HeatingQC'] = 4\nresult.loc[result['HeatingQC'] == \"TA\", 'HeatingQC'] = 3\nresult.loc[result['HeatingQC'] == \"Fa\", 'HeatingQC'] = 2\nresult.loc[result['HeatingQC'] == \"Po\", 'HeatingQC'] = 1\nresult['HeatingQC']","1e4aa0b1":"#Here we encode BsmtFinType1 in Rank\nresult.loc[result['BsmtFinType1'] == \"GLQ\", 'BsmtFinType1'] = 6\nresult.loc[result['BsmtFinType1'] == \"ALQ\", 'BsmtFinType1'] = 5\nresult.loc[result['BsmtFinType1'] == \"BLQ\", 'BsmtFinType1'] = 4\nresult.loc[result['BsmtFinType1'] == \"Rec\", 'BsmtFinType1'] = 3\nresult.loc[result['BsmtFinType1'] == \"LwQ\", 'BsmtFinType1'] = 2\nresult.loc[result['BsmtFinType1'] == \"Unf\", 'BsmtFinType1'] = 1\nresult['BsmtFinType1'].fillna(0, inplace= True)\nresult['BsmtFinType1']","06d8154e":"#Here we encode BsmtCond in Rank\nresult.loc[result['BsmtCond'] == \"Ex\", 'BsmtCond'] = 5\nresult.loc[result['BsmtCond'] == \"Gd\", 'BsmtCond'] = 4\nresult.loc[result['BsmtCond'] == \"TA\", 'BsmtCond'] = 3\nresult.loc[result['BsmtCond'] == \"Fa\", 'BsmtCond'] = 2\nresult.loc[result['BsmtCond'] == \"Po\", 'BsmtCond'] = 1\nresult['BsmtCond'].fillna(0, inplace= True)\nresult['BsmtCond']","757c982a":"#Here we encode BsmtQual in Rank\nresult.loc[result['BsmtQual'] == \"Ex\", 'BsmtQual'] = 5\nresult.loc[result['BsmtQual'] == \"Gd\", 'BsmtQual'] = 4\nresult.loc[result['BsmtQual'] == \"TA\", 'BsmtQual'] = 3\nresult.loc[result['BsmtQual'] == \"Fa\", 'BsmtQual'] = 2\nresult.loc[result['BsmtQual'] == \"Po\", 'BsmtQual'] = 1\nresult['BsmtQual'].fillna(0, inplace= True)\nresult['BsmtQual']","466fbd7f":"#Here we encode KitchenQual in Rank\nresult.loc[result['KitchenQual'] == \"Ex\", 'KitchenQual'] = 4\nresult.loc[result['KitchenQual'] == \"Gd\", 'KitchenQual'] = 3\nresult.loc[result['KitchenQual'] == \"TA\", 'KitchenQual'] = 2\nresult.loc[result['KitchenQual'] == \"Fa\", 'KitchenQual'] = 1\nresult['KitchenQual']","b89626ea":"#Here we encode BsmtExposure in Rank\nresult.loc[result['BsmtExposure'] == \"Gd\", 'BsmtExposure'] = 4\nresult.loc[result['BsmtExposure'] == \"Av\", 'BsmtExposure'] = 3\nresult.loc[result['BsmtExposure'] == \"Mn\", 'BsmtExposure'] = 2\nresult.loc[result['BsmtExposure'] == \"No\", 'BsmtExposure'] = 1\nresult['BsmtExposure'].fillna(0, inplace= True)\nresult['BsmtExposure']","0da6a79b":"#Here we encode GarageQual in Rank\nresult.loc[result['GarageQual'] == \"Ex\", 'GarageQual'] = 5\nresult.loc[result['GarageQual'] == \"Gd\", 'GarageQual'] = 4\nresult.loc[result['GarageQual'] == \"TA\", 'GarageQual'] = 3\nresult.loc[result['GarageQual'] == \"Fa\", 'GarageQual'] = 2\nresult.loc[result['GarageQual'] == \"Po\", 'GarageQual'] = 1\nresult['GarageQual'].fillna(0, inplace= True)\nresult['GarageQual']","2d72b0fa":"del result[\"GarageQual\"]#perch\u00e8 tutti i valori sono 3","a95c90f7":"#Here we encode GarageFinish in Rank\nresult.loc[result['GarageFinish'] == \"Fin\", 'GarageFinish'] = 4\nresult.loc[result['GarageFinish'] == \"RFn\", 'GarageFinish'] = 3\nresult.loc[result['GarageFinish'] == \"Unf\", 'GarageFinish'] = 2\nresult['GarageFinish'].fillna(0, inplace= True)\nresult['GarageFinish']","1157c318":"#HERE WE FILL THE LAST NAs IN THOSE VARIABLES WHICH WE CAN NOT RANK\nresult['MasVnrType'].fillna(\"None\", inplace= True)","3367efc5":"result['MasVnrArea'].fillna(0, inplace= True)","b12cb9bb":"result['GarageType'].fillna(\"No Garage\", inplace= True)","e211fed5":"#Correlation matrix with new encoded variables\ncorr_matrix = result.corr()\nf, ax1 = plt.subplots(figsize=(25,25)) #Crea il sistema di riferimento\nax1=sns.heatmap(corr_matrix,vmax = 0.9); #Con Seaborn fai una heatmap che ha val. max. 0.9","559104a6":"corrmat = data_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.3]\nplt.figure(figsize=(9,9))\ng = sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","4160d102":"pd.set_option('display.max_columns', 70)","3a13bafc":"#Near 0 variance\ndel result['ExterCond']","ff45341e":"del result['BsmtCond'] #Near 0 variance","18f0b2d0":"#Here we extract the numerical variables, this will come in handy later on\nn_features = result.select_dtypes(exclude = [\"object\"]).columns","9efae26f":"def mod_outlier(df):\n        df1 = df.copy()\n        df = df._get_numeric_data()\n\n\n        q1 = df.quantile(0.25)\n        q3 = df.quantile(0.75)\n\n        iqr = q3 - q1\n\n        lower_bound = q1 -(1.5 * iqr) \n        upper_bound = q3 +(1.5 * iqr)\n\n\n        for col in df.columns:\n            for i in range(0,len(df[col])):\n                if df[col][i] < lower_bound[col]:            \n                    df[col][i] = lower_bound[col]\n\n                if df[col][i] > upper_bound[col]:            \n                    df[col][i] = upper_bound[col]    \n\n\n        for col in df.columns:\n            df1[col] = df[col]\n\n        return(df1)\n\nresult = mod_outlier(result)","f2ad1b21":"for i in result[n_features]:\n    sns.boxplot(x=result[i])\n    plt.show()","6e1ae569":"result","c95003c6":"#Here we split train and test back and we attach \"SalePrice\" to the train\ndata_train_new, data_test_new = result[:1100], result[1101:]\ndata_train_new['SalePrice'] = y","5a56ccd5":"data_train_new","1d90204a":"data_test_new","1c851a51":"data_train_dummies = pd.get_dummies(data_train_new)\ndata_train_dummies","4032513f":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    rmse= np.sqrt(-cross_val_score(model, X.values, Y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","daabeaad":"X = data_train_dummies[data_train_dummies.columns.difference(['SalePrice'])]","1eb15bff":"Y = data_train_dummies['SalePrice']","101ce834":"X","703e4b38":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .30, random_state = 40)","261857d9":"lr = LinearRegression()\nlr.fit(X_train,Y_train)","404ebd82":"print(lr.intercept_)","32c7b50c":"print(lr.coef_)","41fbc4e8":"predicted = lr.predict(X_test)\nplt.figure(figsize=(15,8))\nplt.scatter(Y_test,predicted)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()","700fb2fd":"score = rmsle_cv(lr)\nprint(\"\\nLinear Regression score: {:.4f}\\n\".format(score.mean()))","c5034555":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(Y_test, predicted))\nprint('MSE:', metrics.mean_squared_error(Y_test, predicted))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, predicted)))","f6b7c889":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","1ddc0a41":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9a588f11":"##lasso = linear_model.Lasso()\n### y_pred = cross_val_predict(lasso, X, y, cv=5)","062ad16c":"GBoost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n#RMSE estimated through the partition of the train set\nGBoost.fit(X_train, Y_train)\nrmse = math.sqrt(mean_squared_error(Y_test, GBoost.predict(X_test)))\nprint(\"RMSE: %.4f\" % rmse)","cee6818d":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a5b704ae":"regressor = RandomForestRegressor(n_estimators=300, random_state=0)\nregressor.fit(X,Y)\n\n","ed28ee52":"# Score model\nscore = rmsle_cv(regressor)\nprint(\"Random Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9df4ce48":"# Our initial considerations \nLooking forward to our columns, we found some variables which can have an high correlation with our main variable SalePrice:\n- __Year Built__\n- __TotalBsmtSF__\n- __GrLivArea__\n- __PoolArea__\n\nThese are variables related to the conditions of the building, its age and some \"extra luxury\" features such as __PoolArea__. \nIn principle they are all characteristics which can rise the price of an abitation. \nAnother theory we suggested was to consider mainly the \"inner\" part of the house, such as __KitchenQual__ or __CentralAir__, but these could be too general features which mainly all the houses can have.\n\nNow, with these prior hypotesis, let's dive into the \"__SalePrice__\" analysis.","0d76f974":"# Create Dummy Variables","84d7e150":"## K-Fold Cross Validation","26663ceb":"# Random Forest","8e3a373d":"# Model","e567a697":"The aim of this project is to build a Machine Learning model in order to predict the appropriate price of a house given a set of features.\nWe decided to divide our analysis into 5 parts:\n - First look at the problem and general understanding of the variables;\n - Study the main variable (\"SalePrice\"); \n - Study how the main variable is related to the other feature;\n - Data Preprocessing: make some cleaning on our training data set in order to better visualize and estimate;\n - Build a model in order to predict SalePrice \n \n***","c00abdf1":"## Lasso","0bfae3e0":"By these analysis we discovered that our previsions were quite correct.\n\n__Year Built__ seems to have a slight relation with our main variable, and people, as we thought, tend to buy newer houses. \n\nInstead, for __TotalBsmtSF__ and __GrLivArea__ there seems be a stronger relation with __SalePrice__. ","113dc0d6":"# SalePrice Analysis","549e2112":"## G Boosting","48a64936":"These measures of symmetry are useful in order to understand the symmetry of the distribution of our main variable.\nOur distribution is highly skewed and present a longer tail on the right. \nThe high value of kurtosis can determine an higher probability of outliers values.","107f1319":"# The other variables","5d0d4620":"#### Importing Libraries and play a bit with our dataset  ","5b5eae5d":"<h1><center>Predict the House Price <\/center><\/h1>\n<h2><center>Fabrizio Rocco<\/center><\/h2>","8035154b":"Using this kind of plot we can deduce if there's some collinearity between 2 or more variables.\nIn particoular, there are some white blocks which have to be analyzed:\n1. __GarageYrBlt__ and __YearBuilt__\n2. __TotRmsAbvGrd__ and __GrLivArea__\n3. __TotalBsmtSF__ and __X1stFlrSF__\n4. __GarageArea__ and __GarageCars__\n \nKnowing the meaning of these pairs of variables seems trivial to notice a collinearity between pairs \"1\", \"3\" and \"4\".\nFor the \"2\" pair the difference is slightly more subtle because the house area and the total number of rooms, not always are related. \nFor example two houses with the same living area can be inhabited by different number of peoples and so the actual disposition\/number of the rooms can be different.\n\nLet's restrict our matrix a bit more.","7f373474":"# Split categorical and numerical variables","10913db3":"We have to do some considerations. \nLet's divide our null values into 2 groups:\n - __PoolQC__, __MiscFeature__, __Alley__, __Fence__, __FireplaceQu__ and __LotFrontage__.\nThese are all variables which presents many null values. In general, by common opinion, we can discourage variables which have more than 15% of missing values. \nThese are not vital information for someone who wants to buy an house, such as __FireplaceQu__ or, for example, many houses doesn't have an __Alley__ access. We can drop them.\n\nThe second group:\n - __GarageX__ properties\nIf we look carefully, all of these variables have the same number of null values! Maybe this can be a strange coincidence, or just that they all refer to the same variable Garage, in which \"Na\" means \"There is no Garage\". The same occurs for __BsmtX__ and MasVnr__, which means that we will have to deal with them afterwards.","219cc1b3":"# Outliers","03094b12":"Now our goal is to deal with null values and try to understand for each one what can we do: maybe we can replace them or maybe we can just skip them. ","85cbdfe5":"![hou_1982323b.jpg](attachment:hou_1982323b.jpg)","aa8c1a70":"# Number of null vallues ","7f38a99a":"# Heatmap Correlation Matrix","977637f6":"## Linear Regression"}}