{"cell_type":{"7e8cfbbd":"code","22003a22":"code","0f13bf4f":"code","9e68d5c7":"code","4d7525a3":"code","ceb0d102":"code","4c3c1d99":"code","f6f264f7":"code","b2b32a88":"code","5163677e":"code","3b4b2bfc":"code","ace5598b":"code","acbdbaae":"code","46b042d8":"code","9e2beab0":"code","22f47fea":"code","2535f77f":"code","c2cd06fd":"code","1917069f":"code","9f01efce":"code","69d5a73c":"code","5e3fefb4":"code","2247aa06":"code","294838ca":"code","19cbcf59":"code","258cb751":"code","5b8e8d1d":"markdown","98126b2d":"markdown","69451aa7":"markdown","5f836e94":"markdown","f0cb79bd":"markdown","3cdb2fde":"markdown","e0769755":"markdown","d4317e08":"markdown","b55c2fd1":"markdown","839dd90f":"markdown","c4b69c04":"markdown","af70551d":"markdown","197583b9":"markdown","cc47bc9c":"markdown","ddabdbfa":"markdown","0677ac10":"markdown","86a6bd2d":"markdown","2b4d1637":"markdown"},"source":{"7e8cfbbd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical","22003a22":"import h5py\n\n# Open the file as read only\nh5f = h5py.File('..\/input\/street-view-house-nos-h5-file\/SVHN_single_grey1.h5', 'r')\n\n# Load the training and the test set\nX_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\n\n\n# Close this file\nh5f.close()","0f13bf4f":"len(X_train), len(X_test),X_train.shape, X_test.shape","9e68d5c7":"# visualizing the first 10 images in the dataset and their labels\nplt.figure(figsize=(10, 1))\n\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i], cmap=\"gray\")\n    plt.axis('off')\n\nplt.show()\nprint('label for each of the above image: %s' % (y_train[0:10]))","4d7525a3":"# Shape of the images and the first image\n\nprint(\"Shape:\", X_train[0].shape)\nprint()\nprint(\"First image:\\n\", X_train[0])","ceb0d102":"# Reshaping the dataset to flatten them. Remember that we are trying to reshape the 2D image data into a 1D array\n\nX_train = X_train.reshape(X_train.shape[0], 1024)\nX_test = X_test.reshape(X_test.shape[0], 1024)","4c3c1d99":"# Normalize inputs from 0-255 to 0-1\n\nX_train = X_train\/255\nX_test = X_test\/255","f6f264f7":"# New shape \n\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Test set:', X_test.shape, y_test.shape)","b2b32a88":"# one hot encode output\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","5163677e":"print(X_train.shape)\nprint(y_train.shape)","3b4b2bfc":"#Fixing the seed for random number generators\nnp.random.seed(42)\nimport random\nrandom.seed(42)\ntf.random.set_seed(42)","ace5598b":"#Importing losses and optimizers modules\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import optimizers\n\n#Define the function\ndef nn_model_1():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(1024, )),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(10, activation = 'softmax')\n    ]) \n    \n    #declare adam optimizer with learning rate of 0.001 \n    adam = optimizers.Adam(learning_rate=0.001)\n    \n    #compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics = ['accuracy'])\n    return model","acbdbaae":"# Build the model\nmodel_1 = nn_model_1()","46b042d8":"#Print the summary\nmodel_1.summary()","9e2beab0":"# Fit the model\nhistory_model_1 = model_1.fit(X_train, y_train, epochs=20, validation_split=0.2, batch_size=128, verbose = 1)","22f47fea":"# plotting the accuracies\n\ndict_hist = history_model_1.history\nlist_ep = [i for i in range(1,21)]\n\nplt.figure(figsize = (8,8))\nplt.plot(list_ep,dict_hist['accuracy'],ls = '--', label = 'accuracy')\nplt.plot(list_ep,dict_hist['val_accuracy'],ls = '--', label = 'val_accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","2535f77f":"#Clearing keras backend\nfrom tensorflow.keras import backend\nbackend.clear_session()","c2cd06fd":"#Fixing the seed for random number generators\nnp.random.seed(42)\nimport random\nrandom.seed(42)\ntf.random.set_seed(42)","1917069f":"#Importing losses and optimizers modules\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import optimizers\n\n#Define the function\ndef nn_model_2():\n    model = tf.keras.Sequential([\n      tf.keras.layers.Dense(256, activation='relu', input_shape=(1024, )),\n      tf.keras.layers.Dense(128, activation='relu'),\n      tf.keras.layers.Dropout(rate = 0.2),\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(32, activation='relu'),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.Dense(10, activation = 'softmax')             \n    ]) \n    \n    #declare adam optimizer with learning rate of 0.0005 \n    adam = optimizers.Adam(learning_rate=0.0005)\n    \n    #compile the model\n    model.compile(optimizer=adam, loss= 'categorical_crossentropy', metrics= ['accuracy'])\n    \n    return model","9f01efce":"# Build the model\nmodel_2 = nn_model_2()","69d5a73c":"#Print the model summary\nmodel_2.summary()","5e3fefb4":"# Fit the model\nhistory_model_2 = model_2.fit(X_train,y_train, epochs=30, validation_split=0.2, batch_size=128, verbose = 1)","2247aa06":"# plotting the accuracies\n\ndict_hist = history_model_2.history\nlist_ep = [i for i in range(1,31)]\n\nplt.figure(figsize = (8,8))\nplt.plot(list_ep,dict_hist['accuracy'],ls = '--', label = 'accuracy')\nplt.plot(list_ep,dict_hist['val_accuracy'],ls = '--', label = 'val_accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","294838ca":"test_pred = model_2.predict(X_test)\n\ntest_pred = np.argmax(test_pred, axis=-1)","19cbcf59":"#Converting each entry to single label from one-hot encoded vector\ny_test = np.argmax(y_test, axis=-1)","258cb751":"#importing required functions\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n#Printing the classification report\nprint(classification_report(y_test, test_pred))\n\n#Plotting the heatmap using confusion matrix\ncm = confusion_matrix(y_test, test_pred)\nplt.figure(figsize=(8,5))\nsns.heatmap(cm, annot=True,  fmt='.0f')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","5b8e8d1d":"#### **Build and train a ANN model**","98126b2d":"### **Plotting the validation and training accuracies**","69451aa7":"# **Artificial Neural Networks: Street View Housing Number Digit Recognition**\n\n\n### **Context:** \n--------------\n\nOne of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.\n\nThe SVHN dataset contains labeled digits cropped from street level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents. \n\n### **Objective:**\n----------------\n\nBuild a feed foward neural network model that can identify the digits in the images. \n\n### **Note:**\n----------------\nANN is not the best choice to deal with image data. CNN's has proven much better performance. This notebook is just for practice.","5f836e94":"### **Plotting the validation and training accuracies**","f0cb79bd":"## **Visualizing images**\n- Use X_train to visualize the first 10 images\n- Use Y_train to print the first 10 labels","3cdb2fde":"#### **Observations:__________**\n - The classification report tells us that numbers 0 and 4 have the highest f1-score (0.81) meaning they have the best chances of being accurately recognized. Whereas, numbers 3 and 8 have the lowest f1-score of (0.77).\n - Number 3 has the lowest precision and 0 has the highest, this means that the model is classifing other numbers as 3 which not valid. Whereas 0 has the lowest chances of being invalidly positive.\n - Number 4 has the highest recall, whereas 5 and 6 have the lowest. It indicates that the model is struggling to identify all 5's and 6's as what they are. Opposed to 4, which the model idenifies in high rates of completion. \n - The confusion matrix shows that the model confused 5 with 3 and 6 with 8. This complements the above obervations about 3, 5 and 6. which assures that they are having some prediction issues in general.\n","e0769755":"#### **Build and train the new ANN model as per the above mentioned architecture**","d4317e08":"Let's build one more model with higher complexity and see if we can improve the performance of the model.","b55c2fd1":"## **Load the dataset**\n- Let us now load the dataset that is available as a .h5 file.\n- Split the data into train and the test dataset","839dd90f":"#### **Print the classification report and the confusion matrix**","c4b69c04":"- There are 42,000 images in the training data and 18,000 images in the testing data. ","af70551d":"## **Data preparation**","197583b9":"Let's check the number of images in the training and testing data.","cc47bc9c":"## **Model Building**\n\nNow that we have done the data preprocessing, let's build an ANN model.","ddabdbfa":"**Observations:_______**\n- Accuracy of both training dataset and validation dataset is symetric to some level. This rejects any overfitting concerns.\n- Accuracy of both is growing rapidly as the model progresses with epochs to 8, then accuracy still improves, but slowly.\n- After epoch 7 we can see small ups and downs in the validation dataset accuracy. This is not a concern as the overall trend is positive.\n- Overall the model is better than the previous with accuracy of 0.77 compared to 0.70.\n","0677ac10":"## **Predictions on the test data**","86a6bd2d":"## **Importing libraries**","2b4d1637":"**Observations:_______**\n- The Accuracy of both training and validation is almost symetric. This is a good indicator that the model is not overfitting the training dataset.\n- The Accuracy starts to increase steadily until epoch 6, then it increases in a lower rate until epoch 17 where we observe minimum increase afterward.\n- We can see that the accuracy of validation dataset is slightly higher than the training dataset until epoch 10, where they start replacing places. This could give a sign that the model starts to fit the training dataset better, but not to the point of overfitting yet.\n- Overall, the model is not overfit and is giving accuracy of 0.7 on training dataset and roughly the same on validation dataset."}}