{"cell_type":{"7626a08f":"code","1744be9f":"code","effe912e":"code","ee697319":"code","0cf43d1d":"code","eb1dc3c4":"code","150b8578":"code","23129da0":"code","712649c1":"code","bb66bd6b":"code","14ba94c7":"code","0e79b31e":"code","c30df26d":"code","42b8e24c":"code","749421e9":"code","bc79bc20":"code","b5b89b1c":"code","e15806ee":"code","ef8df75b":"code","1a7b58f0":"code","0775907f":"code","3ee2a7de":"code","0e9fe4a3":"code","394de515":"code","5b819766":"code","92a42140":"code","faefc198":"code","949cb8a8":"code","23aac6d1":"code","113a77a3":"code","a9899434":"code","a99d6bf4":"code","f9cb927c":"code","bf63b13d":"code","12d29d02":"code","06e96b9c":"code","7db221a5":"code","7c38e81c":"code","20d772fe":"code","0445c803":"code","a6feffce":"code","a826c71b":"code","080aea39":"code","eeb5e85e":"code","3a57ddfc":"code","b2e6b6ec":"code","d7f06322":"code","acdee213":"code","bce84e29":"markdown","f4a5bf4b":"markdown","c61abb9f":"markdown","2c256fb4":"markdown","8ef0f3b2":"markdown","ae7396d0":"markdown","1d689408":"markdown","ba5c9375":"markdown","2d11793b":"markdown","ee5faa17":"markdown","e54bdeaa":"markdown","46a53160":"markdown","96e23e5f":"markdown","bf1365a3":"markdown","e821211b":"markdown","a584dd8b":"markdown","c93b4207":"markdown","54562a40":"markdown","f5d74889":"markdown","d53632e8":"markdown","d75c90aa":"markdown","99235dfe":"markdown","6c2a4e30":"markdown","328aee3b":"markdown","86285edb":"markdown","13db44b2":"markdown","f3cf5215":"markdown","71de481f":"markdown","fd958b2d":"markdown","094a904e":"markdown","33135968":"markdown","0f6ab7ae":"markdown","ef2f1f9e":"markdown","a0ba2d1c":"markdown","0bfee06a":"markdown","a030e250":"markdown","cb5f622b":"markdown","0c677a00":"markdown","f7223a62":"markdown"},"source":{"7626a08f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set_theme(style = \"darkgrid\")","1744be9f":"def categorical_probability(col_name):\n    \"\"\"\n    Calculates probability distribution of Default and No Default\n\n    INPUTS:\n        columns \u2014 DataFrame of a particular categorical variable\n    \n    OUTPUT:\n        DataFrame with p_default, p_no_default for each sub-category, sorted in p_default descending order\n    \"\"\"\n    # Calculate probability\n    column_df = pd.crosstab(index=data[col_name], \n                               columns=data[\"risk_flag\"],\n                                margins=True)\n\n    column_unique = data[col_name].unique()\n    all_cols = np.append(column_unique, \"col_total\")\n    column_df.index = all_cols\n    column_df.columns = [\"no_default\",\"default\", \"row_total\"]\n\n\n    column_df['p_default'] = round(column_df['default'] \/ column_df['row_total'], 4)\n    column_df['p_no_default'] = round(column_df['no_default'] \/ column_df['row_total'], 4)\n    column_df_probability = pd.DataFrame( [column_df['p_default'], column_df['p_no_default']], columns=column_df.index ).T\n    column_df_probability = column_df_probability.drop( labels=['p_no_default'], axis=1)\n    column_df_probability.sort_values(by=['p_default'], ascending=False, inplace=True)\n    \n    # Display table\n    table = column_df_probability\n    cm = sns.color_palette(\"crest\", as_cmap=True)\n    styled_table = table.style.background_gradient(cmap=cm)\n    display(styled_table)","effe912e":"def categorical_valcount_hist(feature):\n    print(data[feature].value_counts())\n    fig, ax = plt.subplots( figsize = (6,6) )\n    sns.countplot(x=feature, ax=ax, data=data)\n    plt.show()","ee697319":"data = pd.read_csv(\"..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\ndata.drop(\"Id\", axis=1, inplace=True)\ndata.head()","0cf43d1d":"rows, columns = data.shape\nprint('Rows:', rows)\nprint('Columns:', columns)","eb1dc3c4":"data.info()","150b8578":"data.isnull().sum()","23129da0":"data.columns = ['income', 'age', 'experience', 'marital_status', 'house_ownership',\n       'car_ownership', 'profession', 'city', 'state', 'current_job_yrs',\n       'current_house_yrs', 'risk_flag']\n\nnumerical = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\"]\ncategorical = [\"marital_status\", \"house_ownership\", \"car_ownership\", \"profession\", \"city\", \"state\"] ","712649c1":"data.describe()","bb66bd6b":"data[numerical].skew()","14ba94c7":"data.hist( figsize = (20, 20) )\nplt.show()","0e79b31e":"for num_col in numerical:\n    print( data[num_col].value_counts().sort_index() )\n    print()","c30df26d":"data[\"risk_flag\"].value_counts()","42b8e24c":"fig, ax = plt.subplots( figsize = (12,8) )\n\ncorr_matrix = data.corr()\ncorr_heatmap = sns.heatmap( corr_matrix, cmap = \"flare\", annot=True, ax=ax, annot_kws={\"size\": 14})\n\nplt.show()","749421e9":"categorical_valcount_hist('marital_status')","bc79bc20":"categorical_valcount_hist('house_ownership')","b5b89b1c":"categorical_valcount_hist('car_ownership')","e15806ee":"print( \"Total categories in CITY:\", len( data[\"city\"].unique() ) )\nprint()\ndata[\"city\"].value_counts() ","ef8df75b":"print( \"Total categories in STATE:\", len( data[\"state\"].unique() ) )\nprint()\nprint( data[\"state\"].value_counts() )","1a7b58f0":"print( \"Total categories in Profession:\", len( data[\"profession\"].unique() ) )\nprint()\ndata[\"profession\"].value_counts()","0775907f":"categorical_probability('state')","3ee2a7de":"categorical_probability('city')","0e9fe4a3":"categorical_probability('profession')","394de515":"for numerical in [\"income\",\"age\",\"experience\",\"current_job_yrs\",\"current_house_yrs\"]:\n    fig, ax = plt.subplots( figsize = (8,6) )\n    sns.boxplot(x = \"risk_flag\", y = numerical, data = data)","5b819766":"fig, ax = plt.subplots( figsize = (8, 6) )\nsns.countplot(x='house_ownership', hue='risk_flag', ax=ax, data=data)","92a42140":"fig, ax = plt.subplots( figsize = (8,6) )\nsns.countplot(x='car_ownership', hue='risk_flag', ax=ax, data=data)","faefc198":"fig, ax = plt.subplots( figsize = (8,6) )\nsns.countplot( x='marital_status', hue='risk_flag', data=data )","949cb8a8":"num = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\"]\ncat = [\"marital_status\", \"house_ownership\", \"car_ownership\"]\n\nfig, ax = plt.subplots(3, 5, figsize=(20, 15))\n\nplt.figure(figsize=(50, 50))\nfor c in range( len(cat) ):\n    for n in range( len(num) ):\n        cat_feature = cat[c]\n        num_feature = num[n]\n        sns.boxplot( x=cat_feature, y=num_feature, hue='risk_flag', data = data, ax=ax[c,n])\n    \n        \nplt.show()","23aac6d1":"fig, ax = plt.subplots( figsize = (10,8) )\nsns.boxplot(x = \"risk_flag\", y = \"current_job_yrs\", hue='house_ownership', data = data)","113a77a3":"for column in categorical:\n    unique_categories = data[column].nunique()\n    print( column, \":\" + str(unique_categories) )","a9899434":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce","a99d6bf4":"label_encoder = LabelEncoder()\n\nfor col in ['marital_status','car_ownership']:\n    data[col] = label_encoder.fit_transform( data[col] )","f9cb927c":"x = data.drop(\"risk_flag\", axis=1)\ny = data[\"risk_flag\"]\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y, random_state = 2021)","bf63b13d":"y_train.index","12d29d02":"onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\ntest_data = x_train.copy()\n\nhouse_ownership_values = onehot_encoder.fit_transform( test_data[['house_ownership']] )\n\nhouse_ownership_labels = np.array([\"norent_noown\", \"owned\", \"rented\"]).ravel()\nhouse_ownership_df = pd.DataFrame(house_ownership_values, columns=house_ownership_labels)\n\ntest_data.reset_index(drop=True, inplace=True)\nhouse_ownership_df.reset_index(drop=True, inplace=True)\ntest_data = pd.concat([ test_data, house_ownership_df], axis=1)","06e96b9c":"high_card_features = ['profession', 'city', 'state']\n\ncount_encoder = ce.CountEncoder()\n\n# Transform the features, rename the columns with the _count suffix, and join to dataframe\ncount_encoded = count_encoder.fit_transform( data[high_card_features] )\ndata = data.join(count_encoded.add_suffix(\"_count\"))","7db221a5":"data.drop(labels=['profession', 'city', 'state'], axis=1, inplace=True)\ndata.head()","7c38e81c":"x = data.drop(\"risk_flag\", axis=1)\ny = data[\"risk_flag\"]","20d772fe":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y, random_state = 420)","0445c803":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import StratifiedKFold\n# from imblearn.over_sampling import SMOTE\n# from imblearn.pipeline import Pipeline\n\n# smote_sampler = SMOTE(random_state=420)\n\n# pipeline = Pipeline(steps = [['smote', smote_sampler],\n#                              ['classifier', rf_clf]])","a6feffce":"# Bootstrapping\n# rf_bootstrap = [True, False]\n\n# # Split criterion\n# rf_criterion = ['gini', 'entropy']\n\n# params_grid = {\n#                'classifier__criterion': rf_criterion,\n#                'classifier__bootstrap': rf_bootstrap\n#               }","a826c71b":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import recall_score\n\n# stratified_kfold = StratifiedKFold(shuffle=True, n_splits=10, random_state=420)\n\n# rf_gridsearch = GridSearchCV(estimator = pipeline,\n#                            param_grid = params_grid,\n#                            scoring = 'recall',\n#                            cv = stratified_kfold,\n#                            n_jobs = -1 )\n\n# rf_gridsearch.fit(x_train, y_train)\n\n# best_parameters = rf_gridsearch.best_params_\n# print(best_parameters)","080aea39":"# best_parameters = rf_gridsearch.best_params_\n# print(best_parameters)","eeb5e85e":"# {'classifier__bootstrap': True, 'classifier__criterion': 'gini'}","3a57ddfc":"# from sklearn.model_selection import cross_val_score\n# from sklearn.metrics import make_scorer\n# from sklearn.metrics import fbeta_score, recall_score\n\n# rf_clf = RandomForestClassifier(criterion='gini', bootstrap=True, random_state=420)\n\n# smote_sampler = SMOTE(random_state=420)\n\n# pipeline = Pipeline(steps = [['smote', smote_sampler],\n#                              ['classifier', rf_clf]])\n\n# stratified_kfold = StratifiedKFold(shuffle=True, n_splits=10, random_state=420)\n\n# cross_val = cross_val_score(estimator=pipeline,\n#                             X = x_train,\n#                             y = y_train,\n#                             scoring='recall',\n#                             cv=stratified_kfold,\n#                             n_jobs=-1)","b2e6b6ec":"# print( cross_val.sum() \/ 10 )","d7f06322":"from sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\nrf_clf = RandomForestClassifier(criterion='gini', bootstrap=True, random_state=420)\n\nsmote_sampler = SMOTE(random_state=420)\n\npipeline = Pipeline(steps = [['smote', smote_sampler],\n                             ['classifier', rf_clf]])\n\npipeline.fit(x_train, y_train)\n\ny_pred = pipeline.predict(x_test)","acdee213":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n\nprint(\"-------------------------TEST SCORES-----------------------\") \nprint(f\"Recall: { round(recall_score(y_test, y_pred)*100, 2) }\")\nprint(f\"Precision: { round(precision_score(y_test, y_pred)*100, 2) }\")\nprint(f\"F1-Score: { round(f1_score(y_test, y_pred)*100, 4) }\")\nprint(f\"Accuracy score: { round(accuracy_score(y_test, y_pred)*100, 2) }\")\nprint(f\"AUC Score: { round(roc_auc_score(y_test, y_pred)*100, 2) }\")","bce84e29":"### Prepare Pipeline: Smote and RandomForestClassifier","f4a5bf4b":"## 5.2 Categorical features against `risk_flag`","c61abb9f":"# 2. Basic understanding of the data set","2c256fb4":"### Closer look at the distribution of numerical columns","8ef0f3b2":"# 1. Setting up the notebook","ae7396d0":"## 5.1 Numerical features against `risk_flag`","1d689408":"### Checking for collinearity","ba5c9375":"# 6. 3D plots for features against `risk_flag`","2d11793b":"# 5. 2D plots for features against `risk_flag`","ee5faa17":"# 3. Exploring numerical columns\n---","e54bdeaa":"### Check for skewness in the data\nValues more than 1 or less than -1 are considered skewed.","46a53160":"### Summarising Part 6:\n- No significant patterns in the data between features and risk_flag","96e23e5f":"### Summarising Part 5:\n- No significant patterns in the data between features and risk_flag","bf1365a3":"### Functions for the notebook\nCustom functions that I have created to run cells in the notebook","e821211b":"### Calculating Probability of Default for features with High Cardinality","a584dd8b":"### Re-train with best parameters","c93b4207":"### Taking a look at the target column","54562a40":"### Import libraries","f5d74889":"### Summarising Part 2:\n- There are 252,000 instances in the dataset, and:\n  - 12 columns, where there are:\n    - 6 numerical columns\n    - 6 categorical columns\n  - No missing values within the dataset","d53632e8":"## Splitting the data into train and test splits","d75c90aa":"### Renaming columns\n- Standardise lower case\n- Store numerical and categorical features","99235dfe":"### Summarising Part 3:\n- Numerical columns have different scales\n    - Feature scaling should be done to ensure that all numerical features are along the same scale\n- Target column `risk_flag` has lopsided distributed \n    - May require upsampling technique like Synthetic Minority Oversampling Technique (SMOTE)\n- Strong correlation between `current_job_yrs` and `experience`\n    - May drop one column during feature selection process or use Principal Component Analysis (PCA)","6c2a4e30":"### Taking a look at number of categories per feature, and the distribution","328aee3b":"# 8. Cross Validation","86285edb":"### GridSearch Cross Validation","13db44b2":"# End of Exploratory Data Analysis","f3cf5215":"### Import data\nThe dataset was obtained from <a href=\"https:\/\/www.kaggle.com\/subhamjain\/loan-prediction-based-on-customer-behavior\">Kaggle<\/a>","71de481f":"The following encoding will be done to the categorical features:\n- marital_status, car_ownership \u2013 binarise\n- house_ownership \u2013 one-hot\n- profession, city, state \u2013 count encoding","fd958b2d":"### Checking cardinality of categorical features before deciding encoding method","094a904e":"# 7. Feature Engineering","33135968":"# 4. Exploring the categorical features","0f6ab7ae":"### Plot histogram to see distribution of the data","ef2f1f9e":"### Average recall score across 10 folds","a0ba2d1c":"# IS424 Data Mining & Biz Analytics\n# Project: Loan Prediction\n\nHey everyone!\n\nI am a university student at Singapore Management University. This notebook is for one of my class's project! \\\nYou may use the Table of Contents to jump through the different sections.\n\n1. Exploratory Data Analysis\n2. Feature Engineering\n3. RandomForest Classifier\n\n**Any comments on the notebook, and how I can do things better, is highly appreciated!**","0bfee06a":"#### Check for any missing values","a030e250":"### Understand the number of rows and features in the dataset","cb5f622b":"### Zooming into some plots","0c677a00":"### Summarising Part 4:\n- `marital_status`, `car_ownership` and `house_ownership` can be binarised or one-hot encoded\n- While `city` and `state` are similar features, it can be seen that `city` have some rather significant relationships with `risk_flag`\n    - might consider either one during feature selection\n- No significant relationship between `profession` and `risk_flag`","f7223a62":"### Check the data type that we are working with"}}