{"cell_type":{"09ee7605":"code","b1c0810d":"code","509460e1":"code","96689423":"code","912b48c8":"code","1b1410f4":"code","99efbb4c":"code","d8d6bfd7":"code","e7456851":"code","915757ab":"code","d08487d7":"code","8ae0864b":"code","378d6198":"code","96c7a5ab":"code","07e9fbd6":"code","c2e2cbe2":"code","d31bec49":"code","de1f0ca2":"code","bc47ce70":"code","7d5ba9be":"code","430d78dc":"code","abb33272":"code","3704fc6c":"code","3047e86b":"code","690aa2ca":"code","32f7fc20":"code","38075e22":"code","58d8cdf5":"code","4825a9f9":"code","eb8ecb7b":"code","9ae5b5a2":"code","321b3dae":"code","057fb696":"code","45374956":"code","c918511c":"code","2e64075e":"code","3b244cec":"code","afd61587":"code","e397be45":"code","362c49a9":"code","6291db0b":"code","041bfb12":"code","d0eef947":"markdown","d456a5af":"markdown","e97fdca4":"markdown","462e1d2a":"markdown","86b2e1fc":"markdown","a1f90a6d":"markdown","b6bbc020":"markdown","24cdf1dc":"markdown","bde06d9b":"markdown","e32491ce":"markdown"},"source":{"09ee7605":"#python specific modules\nimport os\nimport time\nimport copy\nimport pathlib\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom PIL import Image","b1c0810d":"#pytorch specific modules\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optmim\nfrom torch.optim import lr_scheduler\nfrom torchvision import models, datasets, transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","509460e1":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","96689423":"base_path = pathlib.Path(\"..\/input\/\")","912b48c8":"df_data = pd.read_csv(base_path\/'train.csv')","1b1410f4":"#Custom data generator class\nclass CactusDataset(Dataset):\n    \"\"\"\n    Dataset to generate batches of multiple images and labels from a CSV file.\n    Purpose: To work with CSV files where the format is (file_name, cclass_label)\n    and generate batches of data(images, labels) on-the-fly.\n    \"\"\"\n    def __init__(self, df_data, image_path, image_size, transform=None):\n        self.data = df_data\n        self.image_path = image_path\n        self.transform = transform\n        \n    def __len__(self):\n        \"\"\"\n        Returns the no of datapoints in the dataset\n        \"\"\"\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Returns a batch of data given an index\n        \"\"\"\n        image_name = self.data.iloc[index, 0]\n        image = Image.open(str(self.image_path) + '\/' +image_name)\n        image = image.convert('RGB')\n        image = image.resize(image_size, Image.ANTIALIAS) \n        if self.transform is not None:\n            image = self.transform(image)\n        label = self.data.iloc[index, 1]\n        label = torch.from_numpy(np.asarray(label))\n        \n        return image, label","99efbb4c":"train_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1),\n    transforms.RandomAffine(0.1),\n    transforms.RandomGrayscale(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])","d8d6bfd7":"valid_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])","e7456851":"image_path = base_path\/'train'\/'train'\nimage_size = (224, 224)\nbs = 64","915757ab":"cac_dataset = CactusDataset(df_data, image_path, image_size, transform=train_transform)","d08487d7":"sample_loader = torch.utils.data.DataLoader(cac_dataset, batch_size=8, shuffle=True)\nimages, labels = next(iter(sample_loader))","8ae0864b":"def display_image(inp, title=None):\n    inp = inp.numpy()\n    inp = np.transpose(inp, (1,2,0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = inp * std + mean\n    inp = np.clip(inp,0,1)\n    if title is not None:\n        plt.title(title)\n    plt.figure(figsize=(32,6))\n    plt.imshow(inp)\n    plt.pause(0.001)","378d6198":"out = torchvision.utils.make_grid(images, nrow=8, padding=0)\ndisplay_image(out, title=None)\nprint(labels)","96c7a5ab":"X = df_data['id']\ny = df_data['has_cactus']","07e9fbd6":"X_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                stratify=y, \n                                                test_size=0.20, shuffle=True)","c2e2cbe2":"df_train = pd.DataFrame({'id': X_train.values, 'has_cactus':y_train.values})\ndf_val = pd.DataFrame({'id':X_val.values, 'has_cactus':y_val.values})","d31bec49":"cac_dataset_train = CactusDataset(df_train, image_path, image_size, transform=train_transform)\ncac_dataset_valid = CactusDataset(df_val, image_path, image_size, transform=valid_transform)","de1f0ca2":"train_loader = torch.utils.data.DataLoader(cac_dataset_train, batch_size=32, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(cac_dataset_valid, batch_size=32, shuffle=True)","bc47ce70":"def get_base_model(model_name, num_classes, pretrained=True, unfreeze=True, to_gpu=True):\n    if model_name == \"resnet\":\n        model = models.resnet152(pretrained=pretrained)\n        input_features = model.fc.in_features\n        fc_custom = nn.Linear(input_features, num_classes)\n        model.fc = fc_custom\n    \n    elif model_name == \"densenet\":\n        model = models.densenet121(pretrained=pretrained)\n        input_features = model.classifier.in_features\n        fc_custom = nn.Linear(input_features, num_classes)\n        model.classifier = fc_custom\n    \n    if unfreeze:\n        for param in model.parameters():\n            param.requires_grad = True\n    \n    if to_gpu:\n        model = model.to(device)\n    return model","7d5ba9be":"model = get_base_model(\"resnet\", 2)","430d78dc":"model","abb33272":"criterion = nn.CrossEntropyLoss()\noptimizer = optmim.SGD([\n            {'params': model.layer1.parameters(), 'lr': 1e-6},\n            {'params': model.layer2.parameters(), 'lr': 1e-5},\n            {'params': model.layer3.parameters(), 'lr': 1e-4},\n            {'params': model.layer4.parameters(), 'lr': 1e-4},\n            {'params': model.fc.parameters(), 'lr': 1e-3}\n        ], lr=1e-3)\n#optimizer = optmim.SGD(model.parameters(), lr=1e-7, momentum=0.9)\n#scheduler = lr_scheduler.StepLR(optimizer, 20, gamma=0.1)","3704fc6c":"def get_auc_score(y_true, y_pred):\n    y_true = np.array([item for sublist in y_true for item in sublist])\n    y_pred = np.array([item for sublist in y_pred for item in sublist])\n    return roc_auc_score(y_true, y_pred)","3047e86b":"n_epochs = 50","690aa2ca":"def train_model(model, dataloaders, criterion, optimizer, epochs=50):\n    start_time = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_loss = np.Inf\n    metrics = defaultdict(list)\n    losses = defaultdict(list)\n    \n    for epoch_no in range(epochs):\n        print(\"*\" * 100)\n        print(f\"Starting epoch no {epoch_no+1}\")\n        for phase in ['train','valid']:\n            y_true = list()\n            y_pred = list()\n            \n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    idxs, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                probs = torch.nn.functional.softmax(outputs, dim=1)[:, 1]\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                y_true.append(list(labels.data.cpu().numpy()))\n                y_pred.append(list(probs.detach().cpu().numpy()))\n            \n            auc_score = get_auc_score(y_true, y_pred)\n            \n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = float(running_corrects) \/ len(dataloaders[phase].dataset)\n                \n            if phase == 'valid' and epoch_loss < best_loss:\n                print(f\"Validation loss decreased from {best_loss} to {epoch_loss}. Saving Model \")\n                #best_acc = epoch_acc\n                best_loss = epoch_loss\n                checkpoint = {'model': model,\n                              'state_dict': model.state_dict(),\n                              'optimizer' : optimizer.state_dict()\n                }\n                torch.save(checkpoint, 'model_resnet_50_v2.pth')\n                #torch.save(model.state_dict(), \"model_resnet_50_v1.0.pt\")\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n            print(f\"Ending epoch no {epoch_no+1} with below stats\")\n            print(f\"-------------Stats for {phase}-------------\")\n            print(f\"Loss: {epoch_loss}... Accuracy: {epoch_acc}\")\n            print(f\"AUC for {phase} is {auc_score}\")\n            \n            metrics[phase].append(epoch_acc)\n            losses[phase].append(epoch_loss)\n            \n    time_elapsed = time.time() - start_time\n    print(f\"Total time taken in training: {time_elapsed \/ 60} minutes\")\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, metrics, losses\n    ","32f7fc20":"def plot_metrics_with_loss(metrics, losses):\n    train_metrics = metrics['train']\n    valid_metrics = metrics['valid']\n    \n    train_loss = losses['train']\n    valid_loss = losses['valid']\n    \n    x = list(range(1, n_epochs+1))\n    \n    fig = plt.figure(figsize=(15,10))\n    \n    plt.subplot(2, 2, 1)\n    plt.title('Training Loss Graph over multiple epochs')\n    plt.plot(x, train_loss)\n\n    plt.subplot(2, 2, 2)\n    plt.title('Validation Loss Graph over multiple epochs')\n    plt.plot(x, valid_loss)\n\n    plt.subplot(2, 2, 3)\n    plt.title('Training Metrics(Accuracy) Graph over multiple epochs')\n    plt.plot(x, train_metrics)\n\n    plt.subplot(2, 2, 4)\n    plt.title('Validation Metrics(Accuracy) Graph over multiple epochs')\n    plt.plot(x, valid_metrics)\n\n    plt.show()","38075e22":"dataloaders = {\n        'train': train_loader,\n        'valid': valid_loader\n    }\nmodel, metrics, losses = train_model(model, dataloaders, criterion, optimizer)\nplot_metrics_with_loss(metrics, losses)","58d8cdf5":"df_submission = pd.read_csv(base_path\/'sample_submission.csv')","4825a9f9":"def load_model(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    model.cpu()\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n\n    model.eval()\n    return model","eb8ecb7b":"model = load_model('model_resnet_50_v2.pth')","9ae5b5a2":"test_image_path = base_path \/ 'test' \/ 'test'","321b3dae":"def add_full_path(file_name):\n    full_path = str(test_image_path) + '\/' + file_name\n    return full_path","057fb696":"df_submission['id'] = df_submission['id'].apply(add_full_path)","45374956":"def read_image(image_path):\n    image = Image.open(image_path)\n    image = image.convert('RGB')\n    image = image.resize(image_size, Image.ANTIALIAS)\n    return image","c918511c":"test_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])","2e64075e":"def get_predictions(image_path):\n    image = read_image(image_path)\n    image = test_transform(image)\n    image = torch.unsqueeze(image, 0)\n    pred = model(image)\n    p_cactus = torch.nn.functional.softmax(pred, dim=1)[0][1].item()\n    return p_cactus\n","3b244cec":"df_submission['has_cactus'] = df_submission['id'].apply(get_predictions)","afd61587":"df_submission.head()","e397be45":"def remove_full_path(image_path):\n    paths = image_path.split(\"\/\")\n    path = paths[4]\n    return path","362c49a9":"df_submission['id'] = df_submission['id'].apply(remove_full_path)","6291db0b":"df_submission.head()","041bfb12":"df_submission.to_csv('submission_resnet50_unfreeze_1.csv', index=None)","d0eef947":"### 3. Defining Model architecture and parameters","d456a5af":"#### Defining Data Augmentation(For training)","e97fdca4":"### 2. Data Prep","462e1d2a":"### 8. Submission","86b2e1fc":"### Defining Optimizers and Loss functions","a1f90a6d":"### 1. Importing Modules","b6bbc020":" ### 5. Defining Training Function","24cdf1dc":"### Splitting Data into Training and Validation","bde06d9b":"### 7. Splitting Data and Doing cross validation","e32491ce":"#### Visualising Data"}}