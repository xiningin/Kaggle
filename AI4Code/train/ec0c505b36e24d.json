{"cell_type":{"1511b249":"code","dd8f4198":"code","bc94b3fc":"code","5bed1c6e":"code","95a7ae7d":"code","2c565236":"code","cfac27b6":"code","d73a6a0a":"code","ea40b115":"markdown","c3fd8341":"markdown","e5a07c4b":"markdown","3ae64717":"markdown","5f7aa653":"markdown","3d6e09bd":"markdown","35834b74":"markdown","d95cc841":"markdown"},"source":{"1511b249":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd8f4198":"data = pd.read_csv('\/kaggle\/input\/xtu18-deep-lierning-hw0\/train.csv')\nprint(data.head(18))","bc94b3fc":"pm25=data[data['observation']=='PM2.5'] #\u4ec5\u4fdd\u7559PM2.5\u4e00\u9879\u89c2\u6d4b\u6570\u636e\ntrain = pm25.drop(['Date', 'stations', 'observation'], axis=1)#\u5220\u9664\u65e0\u5173\u7279\u5f81\nprint(train)","5bed1c6e":"train_x = [] \ntrain_y = []\nfor i in range(15):\n    x = train.iloc[:, i:i + 9] #pandas \u7684\u7d22\u5f15\u65b9\u6cd5\n    x.columns = np.array(range(9))\n    y = train.iloc[:, i + 9]\n    y.columns = np.array(range(1))\n    train_x.append(x)\n    train_y.append(y)\ntrain_x = pd.concat(train_x) # (3600, 9) Dataframe\u7c7b\u578b\uff0c\u5c06\u6570\u636e\u62fc\u63a5\u8d77\u6765\ntrain_y = pd.concat(train_y)\ntrain_y = np.array(train_y, float)#\u6570\u636e\u5904\u7406\uff0c\u53d6\u6bcf\u592924\u4e2a\u5c0f\u65f6\uff0c\u5341\u4e94\u7ec4\u6570\u636e\uff0c\u5171240\u5929\u4f5c\u4e3a\u8bad\u7ec3\u96c6\ntrain_x = np.array(train_x,float)#\u8f6c\u5316\u6570\u636e\u7c7b\u578b\ndef standard(X):#\u6807\u51c6\u5316\u51fd\u6570 \u91c7\u53d6z-socre\u6807\u51c6\u5316\n    X_mean = X.mean(axis=0)  \n    X_std = X.std(axis=0)  \n    X1 = (X-X_mean)\/X_std\n    return X1\ntrain_x = standard(train_x)","95a7ae7d":"def gd(x, y, eta=0.01, n_iters=1e6):#\u5404\u53c2\u6570\u5206\u522b\u4e3a\uff0c\u8bad\u7ec3\u96c6\uff0c\u9884\u6d4b\u76ee\u6807\uff0c\u5b66\u4e60\u7387\uff0c\u8fed\u4ee3\u6b21\u6570\n        # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\n    def J(theta, x, y):\n        return np.sum((y - x.dot(theta)) ** 2) \/ len(y)#  \u6b8b\u5dee\u5e73\u65b9\u548c\u516c\u5f0f \n        # \u5bf9\u635f\u5931\u51fd\u6570\u6c42\u5bfc\u51fd\u6570\n    def dJ(theta, x, y):\n        return x.T.dot(x.dot(theta) - y) * 2. \/ len(y) # \u6c42\u5bfc\u516c\u5f0f\u5bf9\u635f\u5931\u51fd\u6570\u6c42\u5bfc\n    def gradient_descent(x, y, initial_theta, eta, n_iters=1e6, epsilon=1e-15):#\u5404\u53c2\u6570\u5206\u522b\u4e3a\uff0c\u7279\u5f81\u77e9\u9635\uff0c\u9884\u6d4b\u76ee\u6807\u503c\uff0c\u521d\u59cb\u53c2\u6570\uff0c\u5b66\u4e60\u7387\uff0c\u8fed\u4ee3\u6b21\u6570\uff0c\u7cbe\u5ea6\n           \n        theta = initial_theta\n        cur_iter = 0 #\u771f\u5b9e\u8fed\u4ee3\u6b21\u6570\n\n        while cur_iter < n_iters:\n            gradient = dJ(theta, X_1, y)\n            last_theta = theta #\u53c2\u6570\n            theta = theta - eta * gradient\n            if (abs(J(theta, x, y) - J(last_theta, x, y)) < epsilon):#\u8fed\u4ee3\u7ec8\u6b62\u6761\u4ef6\n                break\n            cur_iter += 1\n        return theta\n    X_1 = np.hstack([np.ones((len(x), 1)), x])\n    initial_theta = np.zeros(X_1.shape[1]) # \u521d\u59cb\u5316theta\n    theta = gradient_descent(X_1,y, initial_theta, eta, n_iters)\n    return theta\n\ndef predict(X_predict,theta):#\u9884\u6d4b\u7ed3\u679c\n    X_1 = np.hstack([np.ones((len(X_predict), 1)), X_predict])\n    return X_1.dot(theta)","2c565236":"theta = gd(train_x,train_y, eta=0.01, n_iters=1e6)\ntest = pd.read_csv('\/kaggle\/input\/xtu18-deep-lierning-hw0\/testdata.csv')\ntest = test[test['AMB_TEMP']=='PM2.5']\ntest_x = test.iloc[:, 2:]\ntest_x = np.array(test_x, float)\ntest_x = standard(test_x)\nresult = predict(test_x,theta)\nsampleSubmission = pd.read_csv( '\/kaggle\/input\/xtu18-deep-lierning-hw0\/sampleSubmission.csv')\nsampleSubmission['value'] = result\nsampleSubmission.to_csv('submit.csv',index=False)\nprint(pd.read_csv('submit.csv'))","cfac27b6":"def fit_gd(x, y, eta=0.01, n_iters=1e6,epsilon=1e-8):#\u5404\u53c2\u6570\u5206\u522b\u4e3a\uff0c\u8bad\u7ec3\u96c6\uff0c\u6d4b\u8bd5\u96c6\uff0c\u5b66\u4e60\u7387\uff0c\u8fed\u4ee3\u6b21\u6570\n        # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\n    \n    def J(theta, x, y):\n        return np.sum((y - x.dot(theta)) ** 2) \/ len(y)#  \u6b8b\u5dee\u5e73\u65b9\u548c\u516c\u5f0f \n        # \u5bf9\u635f\u5931\u51fd\u6570\u6c42\u5bfc\u51fd\u6570\n    def dJ(theta, x, y):\n        return x.T.dot(x.dot(theta) - y) * 2. \/ len(y) # \u6c42\u5bfc\u516c\u5f0f\u5bf9\u635f\u5931\u51fd\u6570\u6c42\u5bfc\n    def gradient_descent(x, y, initial_theta, eta, n_iters=1e6):#\u5404\u53c2\u6570\u5206\u522b\u4e3a\uff0c\u7279\u5f81\u77e9\u9635\uff0c\u9884\u6d4b\u76ee\u6807\u503c\uff0c\u521d\u59cb\u53c2\u6570\uff0c\u5b66\u4e60\u7387\uff0c\u8fed\u4ee3\u6b21\u6570\uff0c\u7cbe\u5ea6\n          \n        theta = initial_theta\n        cur_iter = 0 #\u771f\u5b9e\u8fed\u4ee3\u6b21\u6570\n        loss = []\n        t = []\n\n        while cur_iter < n_iters:\n            gradient = dJ(theta, X_1, y)#\u68af\u5ea6\n            last_theta = theta #\u53c2\u6570\n            theta = theta - eta * gradient\n            if (abs(J(theta, x, y) - J(last_theta, x, y)) < epsilon):#\u8fed\u4ee3\u7ec8\u6b62\u6761\u4ef6\n                break\n            loss.append(J(theta, x, y))\n            cur_iter += 1\n            t.append(cur_iter)\n        return theta,t,loss\n    X_1 = np.hstack([np.ones((len(x), 1)), x])\n    initial_theta = np.zeros(X_1.shape[1]) # \u521d\u59cb\u5316theta\n    theta,t,l= gradient_descent(X_1, y, initial_theta, eta, n_iters)\n    return theta,t,l\nfrom pylab import *\nx = fit_gd(train_x, train_y, eta=0.01, n_iters=1e6)[1]\ny = fit_gd(train_x, train_y, eta=0.01, n_iters=1e6)[2]\n\n# plot\u4e2d\u53c2\u6570\u7684\u542b\u4e49\u5206\u522b\u662f\u6a2a\u8f74\u503c\uff0c\u7eb5\u8f74\u503c\uff0c\u7ebf\u7684\u5f62\u72b6\uff0c\u989c\u8272\uff0c\u900f\u660e\u5ea6,\u7ebf\u7684\u5bbd\u5ea6\u548c\u6807\u7b7e\nplt.plot(x, y, 'b', label='the change of lossfunction with iters ')\n\n# \u663e\u793a\u6807\u7b7e\uff0c\u5982\u679c\u4e0d\u52a0\u8fd9\u53e5\uff0c\u5373\u4f7f\u5728plot\u4e2d\u52a0\u4e86label='\u4e00\u4e9b\u6570\u5b57'\u7684\u53c2\u6570\uff0c\u6700\u7ec8\u8fd8\u662f\u4e0d\u4f1a\u663e\u793a\u6807\u7b7e\nplt.legend(loc=\"upper right\")\nplt.xlabel('iters')\nplt.ylabel('lossfunction')\nplt.show()\nprint(f'\u68af\u5ea6\u4e0b\u964d\u6cd5\u6700\u5c0f\u635f\u5931\u51fd\u6570\u503c\u4e3a{min(y)}\uff0c\u8fed\u4ee3\u6b21\u6570\u4e3a{max(x)}')\n\ndef fit_adam(x, y,n_iters = 1e6,epsilon=1e-8):#adam\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u76ee\u6807\n        # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\n    \n    def J(theta, x, y):\n        return np.sum((y - x.dot(theta)) ** 2) \/ len(y)#  \u6b8b\u5dee\u5e73\u65b9\u548c\u516c\u5f0f \n        # \u5bf9\u635f\u5931\u51fd\u6570\u6c42\u5bfc\u51fd\u6570\n    def dJ(theta, x, y):\n        return x.T.dot(x.dot(theta) - y) * 2. \/ len(y) # \u6c42\u5bfc\u516c\u5f0f\u5bf9\u635f\u5931\u51fd\u6570\u6c42\u5bfc\n    def adam(x,y,initial_theta,n_iter=1e6,eta = 0.01,p1=0.9,p2=0.999):\n        theta = initial_theta\n        cur_iter = 0\n        loss = []\n        t = []\n        s = np.zeros(x.shape[1])\n        r = np.zeros(x.shape[1]) #\u521d\u59cb\u5316\u4e00\u9636\u77e9\u53d8\u91cf\u4e0e\u4e8c\u9636\u77e9\u53d8\u91cf\n        while cur_iter < n_iters:\n            gradient = dJ(theta,X_1,y)\n            last_theta = theta\n            s = p1*s + (1-p1)*gradient\n            r = p2*r + (1-p2)*(gradient**2)\n            s_cap = s\/(1-(p1**(cur_iter+1)))\n            r_cap = r\/(1-(p2**(cur_iter+1)))\n            delta_theta = -(eta*(s_cap\/(np.sqrt(r_cap)+epsilon)))\n            theta = theta+delta_theta\n            if (abs(J(theta, x, y) - J(last_theta, x, y)) < epsilon):#\u8fed\u4ee3\u7ec8\u6b62\u6761\u4ef6\n                    break\n            loss.append(J(theta, x, y))\n            cur_iter += 1\n            t.append(cur_iter)\n        return theta,t,loss\n    X_1 = np.hstack([np.ones((len(x), 1)), x])\n    initial_theta = np.zeros(X_1.shape[1]) # \u521d\u59cb\u5316theta\n    theta,t,l= adam(X_1,y,initial_theta,n_iter=1e6,eta = 0.01,p1=0.9,p2=0.999)\n    return theta,t,l\n\ndef predict(X_predict,theta):#\u9884\u6d4b\u7ed3\u679c\n    X_1 = np.hstack([np.ones((len(X_predict), 1)), X_predict])\n    return X_1.dot(theta)\ntheta,t,l = fit_adam(train_x, train_y,n_iters = 1e6)\nplt.plot(t, l, 'r',  label='the change of lossfunction with iters ')\nplt.legend(loc=\"upper right\")\nplt.xlabel('iters')\nplt.ylabel('lossfunction')\nplt.show()\nprint(f'adam\u7b97\u6cd5\u6700\u5c0f\u635f\u5931\u51fd\u6570\u503c\u4e3a{min(l)}\uff0c\u8fed\u4ee3\u6b21\u6570\u4e3a{max(x)}')\n","d73a6a0a":"a,x,y = fit_gd(train_x, train_y, eta=0.01, n_iters=1e6,epsilon = 1e-2 )\n\n\n# plot\u4e2d\u53c2\u6570\u7684\u542b\u4e49\u5206\u522b\u662f\u6a2a\u8f74\u503c\uff0c\u7eb5\u8f74\u503c\uff0c\u7ebf\u7684\u5f62\u72b6\uff0c\u989c\u8272\uff0c\u900f\u660e\u5ea6,\u7ebf\u7684\u5bbd\u5ea6\u548c\u6807\u7b7e\nplt.plot(x, y, 'b', label='the change of lossfunction with iters ')\n\n# \u663e\u793a\u6807\u7b7e\uff0c\u5982\u679c\u4e0d\u52a0\u8fd9\u53e5\uff0c\u5373\u4f7f\u5728plot\u4e2d\u52a0\u4e86label='\u4e00\u4e9b\u6570\u5b57'\u7684\u53c2\u6570\uff0c\u6700\u7ec8\u8fd8\u662f\u4e0d\u4f1a\u663e\u793a\u6807\u7b7e\nplt.legend(loc=\"upper right\")\nplt.xlabel('iters')\nplt.ylabel('lossfunction')\nplt.show()\nprint(f'\u68af\u5ea6\u4e0b\u964d\u6cd5\u6700\u5c0f\u635f\u5931\u51fd\u6570\u503c\u4e3a{min(y)}\uff0c\u8fed\u4ee3\u6b21\u6570\u4e3a{max(x)}')\ntheta,t,l = fit_adam(train_x, train_y,n_iters = 1e6,epsilon=1e-2)\nplt.plot(t, l, 'r',  label='the change of lossfunction with iters ')\nplt.legend(loc=\"upper right\")\nplt.xlabel('iters')\nplt.ylabel('lossfunction')\nplt.show()\nprint(f'adam\u7b97\u6cd5\u6700\u5c0f\u635f\u5931\u51fd\u6570\u503c\u4e3a{min(l)}\uff0c\u8fed\u4ee3\u6b21\u6570\u4e3a{max(x)}')","ea40b115":"# \u68af\u5ea6\u4e0b\u964d\u6cd5\u4f18\u5316\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u7b2c\u5341\u4e2a\u5c0f\u65f6PM2.5\u7684\u503c\n* 1.\u7279\u5f81\u9009\u62e9\n     \n\u5c06\u6570\u636e\u6253\u5370\u51fa\u6765\uff0c\u89c2\u5bdf\u6570\u636e\u96c6\uff0c\u5e76\u6839\u636e\u9898\u76ee\u8981\u6c42\uff0c\u6839\u636e\u524d\u4e5d\u4e2a\u5c0f\u65f6\u8fde\u7eed\u7684\u5404\u7c7b\u6307\u6807\uff0c\u9884\u6d4b\u7b2c\u5341\u4e2a\u5c0f\u65f6\u7684PM2.5\u7684\u503c\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c\u8be5\u6570\u636e\u96c6\u5171\u6709\u5341\u516b\u7c7b\u89c2\u6d4b\u6307\u6807\uff0c\u7ecf\u5c1d\u8bd5\uff0c\u5728\u6b64\u6b21\u6570\u636e\u96c6\u4e2d\uff0c\u53ea\u9009\u53d6PM2.5\u4e00\u9879\u89c2\u6d4b\u6307\u6807\u6765\u8fdb\u884c\u7ebf\u6027\u56de\u5f52\u5efa\u6a21\u7ed3\u679c\u76f8\u8f83\u4e0e\u5c0618\u79cd\u89c2\u6d4b\u6307\u6807\u90fd\u9009\u53d6\u8fdb\u53bb\uff0c\u5b9e\u73b0\u96be\u5ea6\u4e0e\u6700\u7ec8\u7ed3\u679c\u90fd\u8981\u597d\u4e00\u4e9b,\u9020\u6210\u6b64\u73b0\u8c61\u7684\u90e8\u5206\u539f\u56e0\u5e94\u8be5\u662f\u89c2\u6d4b\u6307\u6807\u4e4b\u95f4\u5e76\u975e\u662f\u7b80\u5355\u7684\u7ebf\u6027\u76f8\u5173\u6027\uff0c\u60f3\u8981\u77e5\u9053\u66f4\u5177\u4f53\u7684\u539f\u56e0\u53ef\u5bf9\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\uff0c\u5728\u6b64\u4e0d\u505a\u6df1\u5165\u63a2\u7a76\u3002\n","c3fd8341":"* \u901a\u8fc7\u4e24\u8005\u635f\u5931\u51fd\u6570\u53d8\u5316\u56fe\u50cf\u4ee5\u53ca\u8fed\u4ee3\u6b21\u6570\u5bf9\u6bd4\uff0c\u53ef\u770b\u51fa\uff0c\u5728\u672c\u6570\u636e\u96c6\u4e2d\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u4ee5\u5f88\u5feb\u7684\u901f\u5ea6\u904d\u4e0b\u964d\u5230\u4e86\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u503c\u6bd4\u8f83\u5c0f\u7684\u4f4d\u7f6e\uff0c\u800cadam\u7b97\u6cd5\u5219\u663e\u5f97\u66f4\u4e3a\u7a33\u59a5\u4e00\u70b9,\u5728\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6bd4\u8f83\u4f4e\u7684\u65f6\u5019\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u660e\u663e\u6bd4adam\u7b97\u6cd5\u66f4\u8fc5\u901f\uff0c\u5f53\u7cbe\u5ea6\u8981\u6c42\u6bd4\u8f83\u9ad8\u4e24\u8005\u6548\u679c\u4fbf\u5dee\u4e0d\u591a\u4e86\uff0c\u4ee5\u4e0b\u662f\u5728\u9009\u53d6\u540c\u6837\u7684\u521d\u59cb\u5b66\u4e60\u7387\u4e14\u8fed\u4ee3\u4e2d\u6b62\u7cbe\u5ea6\u8981\u6c42\u6bd4\u8f83\u4f4e\u7684\u635f\u5931\u51fd\u6570\u968f\u8fed\u4ee3\u6b21\u6570\u53d8\u6362\u66f2\u7ebf\uff0c","e5a07c4b":"* \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c","3ae64717":"* \u7ebf\u6027\u56de\u5f52\u6a21\u578b\n\n\u5efa\u7acb\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5229\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u5b66\u4e60\u53c2\u6570 ","5f7aa653":"* \u5c1d\u8bd5\u5176\u4ed6\u4f18\u5316\u7b97\u6cd5\n\n\u7f51\u4e0a\u98ce\u8bc4\u8f83\u597d\u7684adam\u81ea\u9002\u5e94\u7b97\u6cd5\u4e0e\u57fa\u7840\u68af\u5ea6\u4e0b\u964d\u6cd5\u6548\u679c\u5bf9\u6bd4","3d6e09bd":"* \u6570\u636e\u9884\u5904\u74062\n\n\u9898\u76ee\u8981\u6c42\u9009\u53d6\u4e00\u5929\u5185\u524d9\u4e2a\u5c0f\u65f6\u7684\u6570\u636e\u4f5c\u4e3a\u7279\u5f81\uff0c\u4e00\u5929\u6700\u591a\u53ef\u5206\u51fa\u5341\u4e94(24-9)\u7ec4\u6570\u636e,\u6700\u540e\u4e00\u5171\u5c31\u670915*240 =3600\u7ec4\u6570\u636e\uff0c\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u6709\u4e00\u79cd\u505a\u6cd5\u662f\u5c06\u6240\u6709\u7684\u4e00\u5e7412\u4e2a\u670820\u5929\u5185\u6240\u6709\u7684\u6570\u636e\u4e32\u5728\u4e00\u8d77\uff0c\u8003\u8651\u5230\u8bad\u7ec3\u96c6\u4e2d\u4e3a\u6bcf\u4e2a\u6708\u524d\u4e8c\u5341\u5929\u7684\u6570\u636e\uff0c\u5982\u679c\u8fd9\u6837\u505a\u7684\u8bdd\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u65f6\u5019\uff0c\u4e0a\u4e00\u4e2a\u6708\u7684\u6700\u540e\u4e00\u5929\u4e0e\u4e0b\u4e00\u4e2a\u6708\u7684\u7b2c\u4e00\u5929\u7684\u6570\u636e\u5e76\u4e0d\u662f\u8fde\u7eed\u7684\uff0c\u6545\u8fbe\u4e0d\u5230\u8fde\u7eed\u5341\u4e2a\u5c0f\u65f6\u6570\u636e\u7684\u8981\u6c42\uff0c\u6545\u91c7\u53d6\u7b2c\u4e00\u79cd\u65b9\u6cd5\n\n","35834b74":"* \u901a\u8fc7\u4ee5\u4e0a\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa\uff0c\u5bf9\u4e8e\u6b64\u95ee\u9898\uff0c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u5df2\u7ecf\u8db3\u591f\u9002\u7528\u4e86\uff0c\u91c7\u7528adam\u7b97\u6cd5\u4f1a\u589e\u52a0\u66f4\u591a\u7684\u8ba1\u7b97\u91cf\uff0c\u53cd\u800c\u663e\u5f97\u6709\u4e9b\u753b\u86c7\u6dfb\u8db3\u4e86\uff0c\u5f53\u7136\uff0c\u672c\u6b21\u5206\u6790\u4ec5\u9488\u5bf9\u6b64\u672c\u6a21\u578b\uff0c\u5bf9\u4e8e\u5176\u4ed6\u95ee\u9898\u4ee5\u53ca\u672c\u95ee\u9898\u7684\u5176\u4ed6\u6a21\u578b\uff0c\u5927\u6982\u7387\u6709\u66f4\u5408\u9002\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee3\u7801\u7565\u663e\u6742\u4e71\uff0c\u8bf7\u591a\u89c1\u8c05","d95cc841":"* \u6570\u636e\u9884\u5904\u74061\n\n\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u540e\uff0c\u51b3\u5b9a\u4ec5\u9009\u53d6PM2.5\u4e00\u9879\u89c2\u6d4b\u6307\u6807\u4f5c\u4e3a\u5efa\u6a21\u7279\u5f81\uff0c\u518d\u6839\u636e\u9898\u76ee\u8981\u6c42\uff0c\u5bf9\u5269\u4e0b\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5904\u7406\u6bcf\u8fde\u7eed\u7684\u4e5d\u4e2a\u5c0f\u65f6\u4e3a\u4e00\u7ec4\u6570\u636e,\u7ecf\u8fc7\u5904\u7406\u540e\uff0c\u5269\u4f59240\u7ec4\u6570\u636e\uff0c\u6bcf\u7ec4\u4e3a\u4e00\u592924\u5c0f\u65f6"}}