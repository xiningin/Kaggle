{"cell_type":{"12dd799a":"code","ddec9ec7":"code","611d5736":"code","8d1e378a":"code","a0483509":"code","ac13a763":"code","e2c31ce3":"code","5acbf312":"code","83577f55":"code","cc637953":"code","e68f61c4":"code","2774b5d5":"code","cbcf57ad":"code","b9db60b7":"code","8dbed097":"code","6eb9ab8f":"code","154ecac3":"code","231b4c1e":"code","5829abaa":"code","ab1abeb7":"code","fea544f8":"code","34cf3b8f":"code","06ddba17":"code","ce58e35c":"code","1d39eb40":"code","0e362395":"code","99f3b0eb":"code","a34ae070":"code","1cda1587":"code","074b0dc8":"code","0050871b":"code","470e444c":"code","9756d496":"code","70cb87ab":"code","bf64ee1d":"code","a5a0f3f7":"code","d01298a0":"code","3185dcf0":"code","a6867c98":"code","28b917bc":"code","e715c8bb":"code","ce46e193":"code","1766ebc7":"code","e4af1ebd":"code","59ca34ce":"code","d21c2ceb":"code","d49e3368":"code","7a6f7fd2":"code","5c375ce0":"code","64c889aa":"code","f0ec94ac":"code","d0b26d4d":"markdown","a5ac28a4":"markdown","0b868f57":"markdown","bd5c568c":"markdown","2b8083cb":"markdown","3942820d":"markdown","d52aa58a":"markdown","d4b8d2d1":"markdown","4fe18074":"markdown","8418cc02":"markdown","166e20d9":"markdown","421306e2":"markdown","e63187d0":"markdown","3fdc4c6d":"markdown","ab5723e9":"markdown","b1ef0f3b":"markdown","6fcfef95":"markdown","8bae07a8":"markdown","0e35424c":"markdown","a662abf0":"markdown","d95dd3d9":"markdown","82ed4464":"markdown","1f68c1b8":"markdown","4e82c396":"markdown","fa077e13":"markdown","b9b7a4ca":"markdown","10239271":"markdown","4358a5fe":"markdown","5a38d577":"markdown","33518cd8":"markdown","a322f2eb":"markdown","063a7dbd":"markdown","46dd39a8":"markdown","85e6d558":"markdown"},"source":{"12dd799a":"import os\nimport gc\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\n\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew","ddec9ec7":"# List files available\nprint(os.listdir(\"..\/input\/home-credit-default-risk\/\"))","611d5736":"# Training data\napp_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","8d1e378a":"# Testing data features : row 48744 \/ columns : 121\napp_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","a0483509":"app_train['TARGET'].value_counts()","ac13a763":"app_train['TARGET'].astype(int).plot.hist();","e2c31ce3":"# Number of each type of column\napp_train.dtypes.value_counts()","5acbf312":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","83577f55":"def labelencoder_counter(df) :\n    # Create a label encoder object\n    le = LabelEncoder()\n    le_count = 0\n\n    # Iterate through the columns\n    for col in df:\n        if app_train[col].dtype == 'object':\n            # If 2 or fewer unique categories\n            if len(list(app_train[col].unique())) <= 2:\n                # Train on the training data\n                le.fit(app_train[col])\n                # Transform both training and testing data\n                app_train[col] = le.transform(app_train[col])\n                app_test[col] = le.transform(app_test[col])\n\n                # Keep track of how many columns were label encoded\n                le_count += 1\n\n    return print('%d columns were label encoded.' % le_count)","cc637953":"labelencoder_counter(app_train)","e68f61c4":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","2774b5d5":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","cbcf57ad":"def correlations(number) :\n    # Find correlations with the target and sort\n    correlations = app_train.corr()['TARGET'].sort_values()\n\n    # Display correlations\n    print('Most Positive Correlations:\\n', correlations.tail(20))\n    print('\\nMost Negative Correlations:\\n', correlations.head(20))","b9db60b7":"correlations(10)","8dbed097":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","6eb9ab8f":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","154ecac3":"(app_train['DAYS_BIRTH'] \/ -365).describe()","231b4c1e":"(app_train['DAYS_EMPLOYED'] \/ -365).describe()","5829abaa":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] \/ 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] \/ 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","ab1abeb7":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])","fea544f8":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","34cf3b8f":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","06ddba17":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","ce58e35c":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","1d39eb40":"from sklearn.impute import SimpleImputer\n\n# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nSimpleImputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = SimpleImputer.fit_transform(poly_features)\npoly_features_test =SimpleImputer.transform(poly_features_test)\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","0e362395":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","99f3b0eb":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","a34ae070":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","1cda1587":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","074b0dc8":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nSimpleImputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nSimpleImputer.fit(train)\n\n# Transform both training and testing data\ntrain = SimpleImputer.transform(train)\ntest = SimpleImputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","0050871b":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","470e444c":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","9756d496":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","70cb87ab":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","bf64ee1d":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","a5a0f3f7":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","d01298a0":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","3185dcf0":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","a6867c98":"from sklearn.impute import SimpleImputer\n\npoly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nSimpleImputer = SimpleImputer(strategy = 'median')\n\npoly_features = SimpleImputer.fit_transform(app_train_poly)\npoly_features_test = SimpleImputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","28b917bc":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","e715c8bb":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","ce46e193":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","1766ebc7":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","e4af1ebd":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n\n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n\n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n\n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","59ca34ce":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","d21c2ceb":"fi_sorted = plot_feature_importances(fi)","d49e3368":"submission.to_csv('baseline_lgb.csv', index = False)","7a6f7fd2":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","5c375ce0":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","64c889aa":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knolwedge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","f0ec94ac":"fi_sorted = plot_feature_importances(fi_domain)","d0b26d4d":"### Feature Importances\n- \uc5b4\ub5a4 \ubcc0\uc218\uac00 \uad00\ub828\uc131\uc774 \ub192\uc740\uc9c0 \ud655\uc778.","a5ac28a4":"- Random Forest \uc758 \uae30\ub2a5\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ud06c\uac8c \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\uc74c\uc744 \ud655\uc778\ud568.\n- Gradient Boosting Model \uc5d0\ub294 \uc801\uc6a9\uac00\ub2a5\ud568.","0b868f57":"### \ubc94\uc8fc\ud615 \ubcc0\uc218\ub4e4\uc744 \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ud560 \uac83\uc778\uac00?\n\n### Label encoding\n- \ubb38\uc790\uc5f4 \uac12\ub4e4\uc744 \uc22b\uc790\ud615\uc73c\ub85c \ubcc0\uacbd.\n- \ud568\uc218 : labelencoder_counter(df)\n- df \uc5d0 \uc77d\uc740 \ud30c\uc77c\uc744 \ubcc0\uc218\ub85c \uc785\ub825 : app_train = pd.read_csv('.\/input\/application_train.csv')","bd5c568c":"### \ucee4\ub110 \ubc00\ub3c4 \ucd94\uc815\n- \uc5f0\ub839\uc774 \ub300\uc0c1\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2dc\uac01\ud654(seaborn kdeplot \uc0ac\uc6a9).","2b8083cb":"# \uc804\uc0b0\ud559\ud504\ub85c\uc81d\ud2b8 : Machine Leaning \uc744 \ud1b5\ud55c \ub9ac\uc2a4\ud06c \uad00\ub9ac \ud504\ub85c\uc81d\ud2b8\n\n### \ubcf8 \ud504\ub85c\uc81d\ud2b8\uc758 \ubaa9\uc801\n - \uc740\ud589\uc5d0\uc11c \uc0ac\uc6a9\uac00\ub2a5\ud55c \uc5f0\uccb4 \uc608\uce21 \ud504\ub85c\uc81d\ud2b8\ub97c \uc9c4\ud589\ud558\ub294 \uac83\uc774 \uadf8 \ubaa9\uc801\uc774\ub098, \uc2e4\uc81c \uc740\ud589\uc758 \ub370\uc774\ud130\ub97c \uac00\uc838\uc624\ub294 \uac83\uc5d0 \uc5b4\ub824\uc6c0\uc774 \uc788\uc5b4 \ubcf8 \ud504\ub85c\uc81d\ud2b8\ub294 \uce90\uae00 \ub0b4 Home Credit Default Risk \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c \uc9c4\ud589\ud558\uae30\ub85c \ud568.\n - \uae30\ubcf8\uc801\uc778 \ubd84\uc11d \ubc29\ud5a5\uc740 \uce90\uae00 \ub0b4 \uac00\uc7a5 \ucd94\ucc9c \uc218\uac00 \ub9ce\uc740 https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction \ub97c \ucc38\uace0\ud558\uc5ec \uc9c4\ud589\ud568.\n - \uba38\uc2e0 \ub7ec\ub2dd\uc744 \uc704\ud55c \uae30\ubcf8\uc801\uc778 \uc9c0\uc2dd\uacfc \ubc29\ubc95\ub4e4\uc744 \uc6b0\uc120 \uc815\ub9ac\ud558\uace0 \ucd94\ud6c4 \uc774\ub97c \ud65c\uc6a9\ud558\uc5ec \ub354 \ub098\uc740 \uacb0\uacfc\uc640 \uae30\ub2a5\uc744 \ub9cc\ub4e4\uc5b4\ub0b4\uae30 \uc704\ud574 \uc9c0\uc18d\uc801\uc73c\ub85c \uc218\uc815 \ubc0f \ubcf4\uc644 \uc608\uc815\uc784.","3942820d":"### \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d\n - \ud568\uc218 : correlations(number)\n - number \uc5d0 \ud30c\uc545\ud558\uace0\uc790 \ud558\ub294 \uc22b\uc790\ub97c \uc785\ub825.","d52aa58a":"### \ub370\uc774\ud130 \ud0c0\uc785 \ubd84\uc11d","d4b8d2d1":"### \uacb0\uce21\uce58(Missing Value) \ud655\uc778\n - \ud568\uc218 : missing_values_table(df)\n - df \uc5d0 \uc77d\uc740 \ud30c\uc77c\uc744 \ubcc0\uc218\ub85c \uc785\ub825 : app_train = pd.read_csv('..\/input\/application_train.csv')\n - \ub204\ub77d \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub294 \ubc29\ubc95 : https:\/\/dining-developer.tistory.com\/19 \ucc38\uace0.","4fe18074":"### EXT_SOURCE \n- Target\uacfc \uac00\uc7a5 \uc74c\uc758 \uc0c1\uad00\uacc4\uc218\ub97c \uac00\uc9c0\ub294 \ubcc0\uc218\ub4e4.\n- \uac12\uc774 \ud074\uc218\ub85d \ub300\ucd9c \uc0c1\ud658 \ube44\uc728\uc774 \uc99d\uac00\ud568.","8418cc02":"### Baseline\n- Logistic Regression\n- \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc778\ucf54\ub529.\n- \uacb0\uce21\uac12\uc744 \ucc44\uc6b0\uace0 \ub370\uc774\ud130 \uc804\ucc98\ub9ac(strategy = 'median').","166e20d9":"### Import package","421306e2":"### \ub300\ud45c\uc801\uc778 \ud328\ud0a4\uc9c0\n\n### - numpy \nNumerical Python\uc744 \uc758\ubbf8\ud558\ub294 \ub118\ud30c\uc774\ub294 \ud30c\uc774\uc36c\uc5d0\uc11c \uc120\ud615\ub300\uc218 \uae30\ubc18\uc758 \ud504\ub85c\uadf8\ub7a8\uc744 \uc27d\uac8c \ub9cc\ub4e4 \uc218 \uc788\ub3c4\ub85d \uc9c0\uc6d0\ud558\ub294 \ub300\ud45c\uc801\uc778 \ud328\ud0a4\uc9c0\ub85c \ub8e8\ud504\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \ub300\ub7c9 \ub370\uc774\ud130\uc758 \ubc30\uc5f4 \uc5f0\uc0b0\uc744 \uac00\ub2a5\ud558\uac8c \ud558\ubbc0\ub85c \ube60\ub978 \ubc30\uc5f4 \uc5f0\uc0b0 \uc18d\ub3c4\ub97c \ubcf4\uc7a5\ud568.\n\n### - pandas\n\ud310\ub2e4\uc2a4\ub294 \ub370\uc774\ud130 \ucc98\ub9ac\ub97c \uc704\ud574 \uc874\uc7ac\ud558\ub294 \uac00\uc7a5 \uc778\uae30 \uc788\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \ud589\uacfc \uc5f4\ub85c \uc774\ub904\uc9c4 2\ucc28\uc6d0 \ub370\uc774\ud130\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uac00\uacf5\/\ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud568.\n\n### - sklearn\n\uc0ac\uc774\ud0b7\ub7f0\uc740 \ud30c\uc774\uc36c \uba38\uc2e0\ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac \uc911 \uac00\uc7a5 \ub9ce\uc774 \uc0ac\uc6a9\ub418\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc704\ud55c \ub2e4\uc591\ud55c \uc54c\uace0\ub9ac\uc998\uacfc \uac1c\ubc1c\uc744 \uc704\ud55c \ud3b8\ub9ac\ud55c \ud504\ub808\uc784\uc6cc\ud06c\uc640 API\ub97c \uc81c\uacf5\ud568.\n \n### - matplotlib\nmatplotlib.pyplot \ubaa8\ub4c8\uc740 MATLAB\uacfc \ube44\uc2b7\ud558\uac8c \uba85\ub839\uc5b4 \uc2a4\ud0c0\uc77c\ub85c \ub3d9\uc791\ud558\ub294 \ud568\uc218\uc758 \ubaa8\uc74c\uc73c\ub85c matplotlib.pyplot \ubaa8\ub4c8\uc758 \uac01\uac01\uc758 \ud568\uc218\ub97c \uc0ac\uc6a9\ud574\uc11c \uac04\ud3b8\ud558\uac8c \uadf8\ub798\ud504\ub97c \ub9cc\ub4e4\uace0 \ubcc0\ud654\ub97c \uc904 \uc218 \uc788\uc74c.\n\n### - seaborn\nSeaborn\uc740 matplotlib \uae30\ubc18\uc758 \uc2dc\uac01\ud654 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uc720\uc775\ud55c \ud1b5\uacc4 \uadf8\ub798\ud53d\uc744 \uadf8\ub9ac\uae30 \uc704\ud55c \uace0\uae09 \uc778\ud130\ud398\uc774\uc2a4\ub97c \uc81c\uacf5\ud568.\n\n### - lightgbm\nXGBoost\uc640 \ud568\uaed8 \uac00\uc7a5 \uac01\uad11\uc744 \ubc1b\uace0 \uc788\ub294 \ubd80\uc2a4\ud305 \uacc4\uc5f4 \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \ud559\uc2b5 \uc2dc\uac04\uacfc \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc740 \ud3b8\uc774\uba70 \ub9ac\ud504 \uc911\uc2ec \ud2b8\ub9ac\ubd84\ud560 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud568.\n\ub2e8, \uc801\uc740 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \uc0ac\uc6a9\ud560 \uacbd\uc6b0 \uacfc\uc801\ud569\uc774 \ubc1c\uc0dd\ud558\uae30 \uc26c\uc6c0(10,000\uac74 \uc774\ud558\uc758 \ub370\uc774\ud130 \uc138\ud2b8).","e63187d0":"- \ud559\uc2b5 \ub370\uc774\ud130(train)\n- \uac01 \uce7c\ub7fc\uc5d0 \uad00\ub828\ud55c \uc0ac\ud56d\uc740 https:\/\/chocoffee20.tistory.com\/6 \ucc38\uace0\ud568.","3fdc4c6d":"- DAYS_EMPLOYED : \uc774\uc0c1\uce58 \ubc1c\uacac.\n- \uc774\uc0c1\uce58 : \ucd5c\uc18c\uac12 NAME_INCOME_TYPE \uc5f4\uc5d0\uc11c Pensioner \ub85c \uc5f0\uae08\uc218\uae09\uc790\uc784\uc744 \ud655\uc778\ud568.\n- \uc774\ub7ec\ud55c \uac12\ub4e4\uc758 \ucc98\ub9ac\uc5d0 \ub300\ud574 \ud604\uc7ac \uace0\ubbfc\uc911\uc774\uba70 \ucd94\ud6c4 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uc5ec \uc218\uc815, \ubcf4\uc644 \uc608\uc815\uc784.","ab5723e9":"### \ucd94\uac00\uc0ac\ud56d\n- https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features?scriptVersionId=6025993 \ub97c \ucc38\uace0\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc740 \ubcc0\uc218\ub4e4\uc774 \uacb0\uacfc\uac12\ub4e4\uacfc \uc720\uc758\ubbf8\ud55c \uc0c1\uad00\uad00\uacc4\uac00 \uc788\uc74c\uc744 \ud655\uc778\ud568.\n    - CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n    - ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n    - CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n    - DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age","b1ef0f3b":"### Polynomial Features\n- interaction terms : \ubcc0\uc218\ub97c \uacb0\ud569\ud558\uc5ec \ub300\uc0c1\uacfc\uc758 \uad00\uacc4\ub97c \ud655\uc778.","6fcfef95":"### \ub098\uc774\ub97c 5\ub144 \ub2e8\uc704\ub85c \ubd84\ub9ac\ud558\uc5ec \ub370\uc774\ud130 \ubd84\uc11d\n- \ub098\uc774 \ubcc0\uc218\ub97c \uc808\ub300\uac12\uc73c\ub85c \ubcc0\ud658\ud558\uc5ec \ud655\uc778\uc774 \uc27d\ub3c4\ub85d \ubcc0\uacbd\ud568.","8bae07a8":"### \ub370\uc774\ud130 \ubd84\uc11d\n\uc6b0\uc120 \ubd84\uc11d\ud560 \ub370\uc774\ud130\ub294 \uce90\uae00 \ub0b4 Home Credit Default Risk \ub370\uc774\ud130\ub85c \uac01 \ud30c\uc77c\uc774 \uc758\ubbf8\ud558\ub294 \ubc14\ub294 \ub2e4\uc74c\uacfc \uac19\uc74c.\n - application : \ub300\ucd9c \uc2e0\uccad \uc2dc \uc791\uc131\ud55c \ub0b4\uc6a9.\n - Previous application : \uacfc\uac70 \ub300\ucd9c \uae30\ub85d.\n - bureau : \uac1c\uc778\uc2e0\uc6a9\ud3c9\uac00\uae30\uad00\uc5d0 \uae30\ub85d\ub41c \uc2e0\uccad\uc790\uc758 \uacfc\uac70 \ud0c0\uae08\uc735\uae30\uad00\uacfc \uc2e0\uc6a9\uac70\ub798 \ub0b4\uc5ed(\uad6d\ub0b4\uc758 NICE \/ KCB).\n - application_train.csv : \ud559\uc2b5 \uba54\uc778 \ud14c\uc774\ube14.\n - application_test.csv : \ud14c\uc2a4\ud2b8 \uba54\uc778 \ud14c\uc774\ube14.\n - bureau.csv : \uc2e0\uc6a9\ud3c9\uac00\uae30\uad00\uc5d0\uc11c \uc81c\uacf5\ud55c \uc2e0\uc6a9\ub3c4 \uc815\ubcf4.\n - bureau_balance.csv : \uc774\uc804 \uc2e0\uc6a9\uac70\ub798 \uc6d4 \uc794\uc561 \uc815\ubcf4.\n - POS_CASH_balance.csv : \uc2e0\uc6a9 \uac70\ub798 \uc815\ubcf4.\n - credit_card_balance.csv : \uc2e0\uc6a9\uce74\ub4dc \uc6d4 \uc794\uc561 \uc815\ubcf4.\n - previous_application.csv : \uc774\uc804 \uac00\uacc4\uc2e0\uc6a9\ub300\ucd9c \uc815\ubcf4.\n - installments_payments.csv : \ub300\ucd9c \uc0c1\ud658 \ub0b4\uc5ed \uc815\ubcf4.\n - HomeCredit_columns_description.csv : \ud30c\uc77c\uc758 \uc5f4\uc5d0 \ub300\ud55c \uc815\ubcf4.","0e35424c":"- \uc0c8\ub86d\uac8c \uc0dd\uc131\ud55c \ubcc0\uc218 \uc911 \uc77c\ubd80\ub294 \uc6d0\ub798 \ubcc0\uc218\ubcf4\ub2e4 \ubaa9\ud45c\uc640\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub354 \ud06c\ub2e4\ub294 \uc810\uc744 \uc54c \uc218 \uc788\uc74c.\n- \ubcc0\uc218\ub4e4 \uac04\uc758 \uad00\uacc4\ub97c \uace0\ub824\ud558\uc5ec \ub354 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \ubcc0\uc218\ub4e4\uc758 \uc870\ud569\uc744 \ucc3e\uc544\ubcfc \uc608\uc815\uc784.","a662abf0":"- \uc624\ube0c\uc81d\ud2b8 \ud0c0\uc785 \ubd84\uc11d","d95dd3d9":"- \uba38\uc2e0\ub7ec\ub2dd\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \uacb0\uce21\uac12\ub4e4\uc744 \ucc44\uc6cc \ub123\uc5b4\uc57c \ud568.\n- \uc774 \ud6c4 \uacfc\uc815\uc5d0\uc11c \uacb0\uce21\uce58\uc758 \ube44\uc728\uc774 \ub192\uc740 \uce7c\ub7fc\uc758 \uacbd\uc6b0 \uc774 \uc5f4\uc744 \uc0ac\uc6a9\ud560 \uac83\uc778\uc9c0 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc744 \uac83\uc778\uc9c0 \uc815\ud560 \uc608\uc815\uc784.","82ed4464":"- \uc804\uccb4\uc801\uc73c\ub85c Target\uacfc \ubcc0\uc218\ub4e4\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \ud3b8\uc740 \uc544\ub2c8\uc9c0\ub9cc \uc774\ub7ec\ud55c \ubcc0\uc218\ub4e4\uc774 \uacb0\uacfc\ub97c \uc608\uce21\ud558\ub294\ub370\uc5d0\ub294 \ub3c4\uc6c0\uc774 \ub420 \uc218 \uc788\uc74c.","1f68c1b8":"\uc774 \uacb0\uacfc\ub97c \ubcf4\uc558\uc744 \ub54c \uc81c\ub300\ub85c \uc0c1\ud658\ub418\uc9c0 \uc54a\uc740 \ub300\ucd9c\ubcf4\ub2e4 \uc0c1\ud658\ub41c \ub300\ucd9c\uc758 \ube44\uc728\uc774 \ud6e8\uc52c \ub192\uc740 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70(0\uac12 : \ub300\ucd9c \uc0c1\ud658, 1\uac12 : \ub300\ucd9c \uccb4\ub0a9)\n- imbalanced class problem\n\ubb38\uc81c\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc74c\uc744 \uc608\uc0c1\ud560 \uc218 \uc788\uc74c.\n- \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud558\uc5ec https:\/\/www.kaggle.com\/kaanboke\/catboost-lightgbm-xgboost-explained-by-shap \ub97c \ucc38\uace0\ud558\uc5ec \ucd94\uac00\uc801\uc73c\ub85c \uc791\uc5c5 \uc9c4\ud589 \uc608\uc815\uc784(Deals With Imbalanced Data).\n\nimbalanced class problem \uc774\ub780?\n- \ub2e4\uc218 \ud074\ub798\uc2a4\uc758 \uc218\uac00 \uc18c\uc218 \ud074\ub798\uc2a4\uc758 \uc218\ubcf4\ub2e4 \uc6d4\ub4f1\ud788 \ub9ce\uc740 \ud559\uc2b5\uc0c1\ud669\uc744 \uc758\ubbf8.\n- \ubd84\ub958 \uc131\ub2a5\uc774 \uc800\ud558\ub418\ub294 \ubb38\uc81c\uac00 \ubc1c\uc0dd.\n- https:\/\/blog.naver.com\/tjdtjdgus99\/222208515494 \ucc38\uace0","4e82c396":"### \uc0c1\uad00\uacc4\uc218\n- \uc77c\ubc18\uc801\uc778 \ud574\uc11d\n    - .00-.19 : very weak\n    - .20-.39 : weak\n    - .40-.59 : moderate\n    - .60-.79 : strong\n    - .80-1.0 :very strong\n- TARGET \uacfc DAYS_BIRTH \uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uac00\uc7a5 \ub192\uc740 \uac83\uc73c\ub85c \ud655\uc778\ub428.","fa077e13":" - \ud559\uc2b5(train) \/ \uc2dc\ud5d8(test) \ub370\uc774\ud130\uc5d0 \ubaa8\ub450 \ub3d9\uc77c\ud55c \uc5f4\uc774 \ud544\uc694\ud568.\n - \uc5f4\uc744 \uae30\uc900\uc73c\ub85c \uc815\ub82c : axis = 1","b9b7a4ca":"- \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130(test)\n- \ud559\uc2b5 \ub370\uc774\ud130\uc640\uc758 \ucc28\uc774 : TARGET \uc5f4\uc774 \uc5c6\uc74c\n- [TARGET] : 0 \uacfc 1(default)\ub85c \uad6c\uc131\ub41c \uc5f0\uccb4 \uc815\ubcf4\ub85c \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\uc11c\ub294 \uc774\ub97c \uc608\uce21\ud558\uc5ec\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \uc5f4\uc774 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc74c.","10239271":"### \uc774\uc0c1\uce58 \ud655\uc778\n- \uc0c1\uad00\uad00\uacc4\uac00 \uac00\uc7a5 Column\uc744 \uba3c\uc800 \ubd84\uc11d\ud568.\n- DAYS_BIRTH : \ud604\uc7ac \ub300\ucd9c\uacfc \ube44\uad50\ud558\uc5ec \uae30\ub85d\ub418\uae30 \ub54c\ubb38\uc5d0 \uc74c\uc218\uc774\ubbc0\ub85c \ub098\ub204\uae30\ub97c \uc74c\uc218\ub85c \ud558\uc5ec \ub354 \uc27d\uac8c \ub098\uc774\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub3c4\ub85d \ud568.\n- mean : \ud3c9\uade0\n- min : \ucd5c\uc19f\uac12\n- max : \ucd5c\ub313\uac12\n- std : \ud45c\uc900\ud3b8\ucc28","4358a5fe":"### One-hot encoding\n- \uace0\uc720\uac12\uc5d0 \ud574\ub2f9\ud558\ub294 \uce7c\ub7fc\uc5d0\ub294 1, \ub098\uba38\uc9c0\uc5d0\ub294 0\uc744 \ud45c\uc2dc.\n- \ud559\uc2b5(train) \/ \uc2dc\ud5d8(test) \ub370\uc774\ud130\uc5d0 \ubaa8\ub450 \ub3d9\uc77c\ud55c \uc5f4\uc774 \ud544\uc694\ud568.\n- \uc5f4\uc744 \uae30\uc900\uc73c\ub85c \uc815\ub82c : axis = 1","5a38d577":"### Light Gradient Boosting Machine","33518cd8":"# Home Credit Default Risk Competition (Introduction KOR)","a322f2eb":"- \ub098\uc774\uac00 \uc5b4\ub9b4\uc218\ub85d default \ube44\uc728\uc774 \ub354 \ub192\uc740 \uac83\uc744 \uc54c \uc218 \uc788\uc74c.","063a7dbd":"### Most Important Features\n- EXT_SOURCE\n- DAYS_BIRTH","46dd39a8":"### Random Forest","85e6d558":"### \uc9c4\ud589\uc608\uc815 \uc0ac\ud56d\n- \ub370\uc774\ud130 \uc804\ucc98\ub9ac : \uc81c\uacf5\ub41c \ub370\uc774\ud130\uc758 \ud56d\ubaa9 \ubfd0\ub9cc \uc544\ub2c8\ub77c \uc774\ub97c \uacb0\ud569\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ubcc0\uc218\ub97c \ub9cc\ub4e4\uace0 \uc774\ub97c \ud1a0\ub300\ub85c \ubd84\uc11d \uc9c4\ud589.\n- \uc0ac\uc6a9\uc790 \ud3b8\uc758\uc131 : \uc2dc\uac01\ud654 \ubc0f \ubd84\uc11d \ubaa8\ub378\uc744 \ud568\uc218\ud654 \ud558\uc5ec \uc0ac\uc6a9\uc790\uac00 \ub354 \uc27d\uac8c \ud504\ub85c\uadf8\ub7a8\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud504\ub85c\uadf8\ub798\ubc0d \uc9c4\ud589.\n- \uc815\ud655\ub3c4 : \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc5bb\uae30 \uc704\ud574 \uc5ec\ub7ec \uc790\ub8cc\ub4e4 \ucc38\uace0\ud558\uc5ec \uc218\uc815 \ubcf4\uc644.\n- https:\/\/www.kaggle.com\/c\/kaggle-survey-2021\/discussion\/279327#1553191 \uc5d0 \ub098\uc628 \ud301\uc744 \ucc38\uace0\ud558\uc5ec \uacc4\uc18d\uc801\uc73c\ub85c \uc0c8\ub85c\uc6b4 \ubaa8\ub378 \uc2dc\ub3c4."}}