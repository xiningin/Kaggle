{"cell_type":{"e2871e6a":"code","2abfbd90":"code","d2226d7c":"code","735d259e":"code","e15b6856":"code","1195b18f":"code","27f4853f":"code","400f7301":"code","14f47db6":"code","8abebf0c":"code","2fafc362":"code","3c518382":"markdown","44dce1e4":"markdown","6d47a861":"markdown","a86cebf6":"markdown","fa92f4aa":"markdown","9d8c16ce":"markdown","a2689eb1":"markdown","08ea36e3":"markdown","c70fc225":"markdown","1cfe5743":"markdown","c65ca209":"markdown","358acf1c":"markdown","028e4335":"markdown","dcfd1234":"markdown","c398bd36":"markdown","36132bf9":"markdown","ba8d3153":"markdown","17643579":"markdown","7f977917":"markdown","08924f10":"markdown","dd118de9":"markdown"},"source":{"e2871e6a":"# import some utilities\nimport torch.nn as nn\nimport torch\nfrom torch import optim\nimport torchvision\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n","2abfbd90":"# for this we will be using mnist data \n\ntransform = transforms.Compose([transforms.Resize(32),transforms.ToTensor()])\n\ntrain_data = torchvision.datasets.KMNIST('data', train=True, download=True, transform=transform)\nvalid_data = torchvision.datasets.KMNIST('data', train=False, download=True, transform=transform)\n\n# prepare data loader\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=32, num_workers=2)\nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size=32, num_workers=2)","d2226d7c":"model_v = torchvision.models.vgg13_bn(pretrained=True)\n\nfor param in model_v.parameters():\n    param.requires_grad = False\n    \nclassifier = nn.Sequential(\n        nn.Linear(25088,4096),\n        nn.ReLU(),\n        nn.Dropout(p=0.5),\n        nn.Linear(4096,4096),\n        nn.ReLU(),\n        nn.Dropout(p=0.5),\n        nn.Linear(4096,10))\n\nmodel_v.features[0] = nn.Conv2d(1, 64,3,1,1)\n\nmodel_v.classifier = classifier","735d259e":"# initial a dummy model\nmodel = nn.Linear(30, 1)\n\nlambda1 = lambda epoch: epoch \/10\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\nfor epoch in range(10):\n    # train()\n    optimizer.step()\n    # validate()\n    scheduler.step()\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")\nplt.plot(lr)","e15b6856":"# initial a dummy model\nmodel = nn.Linear(30, 1)\n\nlambda1 = lambda epoch: 0.50 \noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda1)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\nfor epoch in range(10):\n    # train()\n    optimizer.step()\n    # validate()\n    scheduler.step()\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")    \nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")\nplt.plot(lr)","1195b18f":"# initial a dummy model\nmodel = nn.Linear(30, 1)\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.8)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\nfor epoch in range(10):\n    # train()\n    optimizer.step()\n    # validate()\n    scheduler.step()\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")\nplt.plot(lr)","27f4853f":"# initial a dummy model\nmodel = nn.Linear(30, 1)\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,5,8], gamma=0.5)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\nfor epoch in range(10):\n    # train()\n    optimizer.step()\n    # validate()\n    scheduler.step()\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")    \nplt.plot(lr)","400f7301":"# initial a dummy model\nmodel = nn.Linear(30, 1)\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\nfor epoch in range(10):\n    # train()\n    optimizer.step()\n    # validate()\n    scheduler.step()\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")    \nplt.plot(lr)","14f47db6":"# initial a dummy model\nmodel = nn.Linear(30, 1)\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\nfor epoch in range(10):\n    # train()\n    optimizer.step()\n    # validate()\n    scheduler.step()\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")    \nplt.plot(lr)","8abebf0c":"model_v.cuda()\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=2,min_lr=0.001)\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\n\n# Training\nfor epoch in range(10):\n    \n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    model.train()\n    for i,(data, target) in enumerate(train_loader):\n        \n        data, target = data.cuda(), target.cuda()\n        #clear the gradiant of all optimizer variable\n        optimizer.zero_grad()\n        # forward pass\n        output = model_v(data)\n        # calculate batch loss\n        loss = criterion(output, target)\n        # backward pass\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    # validate the model\n    \n    model.eval()\n    for i, (data,target) in enumerate(valid_loader):\n        \n        data, target = data.cuda(), target.cuda()\n        output = model_v(data)\n        # calculate loss\n        loss = criterion(output, target)\n        # updata the validation loss\n        valid_loss += loss.item()*data.size(0)\n    # update lr if valid loss not decreases \n    scheduler.step(valid_loss)\n    print('epoch={}, learning rate={:.4f}'.format(epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n\nplt.title(\"learning vs. epoch\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"learning rate\")    \nplt.plot(lr)","2fafc362":"model_v.cuda()\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.CyclicLR(optimizer,base_lr=0.001,max_lr=0.1,mode='triangular2')\n\nprint('Initial learning rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\nlr = []\n\n# Training\nfor epoch in range(10):\n    \n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    model.train()\n    for i,(data, target) in enumerate(train_loader):\n        \n        data, target = data.cuda(), target.cuda()\n        #clear the gradiant of all optimizer variable\n        optimizer.zero_grad()\n        # forward pass\n        output = model_v(data)\n        # calculate batch loss\n        loss = criterion(output, target)\n        # backward pass\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        # update lr if valid loss not decreases \n        scheduler.step()\n        # update training loss\n        lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n        train_loss += loss.item()*data.size(0)\n        \n    # validate the model\n    \n    model.eval()\n    for i, (data,target) in enumerate(valid_loader):\n        \n        data, target = data.cuda(), target.cuda()\n        output = model_v(data)\n        # calculate loss\n        loss = criterion(output, target)\n        # updata the validation loss\n        valid_loss += loss.item()*data.size(0)\n  \n    \nplt.title(\"learning vs. train image batch\")\nplt.xlabel(\"train batch\")\nplt.ylabel(\"learning rate\")    \nplt.plot(lr)","3c518382":"## **CosineAnnealingLR:**\n\n\nSet the learning rate of each parameter group using a cosine annealing schedule. When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:\n\n   $$\n\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right)\n$$\n","44dce1e4":"### **Prepare data and model for examples**","6d47a861":"----","a86cebf6":"---","fa92f4aa":"## **MultiplicativeLR:**\n\n**MultiplicativeLR** is like LambdaLR except in MultiplicativeLR learning rate  updated by a constent factor per epoch, on first epoch learning rate calculated by multiplying given factor by inintial learning rate, and then learning rate after first epoch calculated by multiplying given factor by learning rate of previous epoch(epoch-1).  \n\n\nexample: This is an example of what is happening in code cell below:\n\n    epoch 1:  factor =  0.5, init learning rate 0.1,\n              updated learning rate => 0.5 x lr = 0.5 x 0.1 = 0.05\n              \n    epoch 2:  factor =  0.5, learning rate of previous epoch = 0.05,\n              updated learning rate => 0.5 x 0.05 = 0.025\n              \n    epoch 3:  factor =  0.5, learning rate of previous epoch = 0.025,\n              updated learning rate => 0.5 x 0.025 = 0.0125\n              \n    .....\n    .....","9d8c16ce":"---","a2689eb1":"---","08ea36e3":"---","c70fc225":"## **MultiStepLR:**\n\n**MultiStepLR** is like StepLR except in this we can set milestones states of when we want to change learning rate by gamma.\n\nexample: This is an example of what is happening in code cell below:\n\n    : milestones = [2,5,8], gamma= 0.5\n    \n    epoch 1-3: learning rate = 0.1 x 0.5 = 0.05\n    \n    epoch 4-6: learning rate = 0.05 x 0.5 = 0.025\n    \n    epoch 7-9: learning rate = 0.025 x 0.5 = 0.0125\n    \n    ---\n    ---","1cfe5743":"## **LambdaLR:**\n\n","c65ca209":"**LambdaLR** basically increases the learning rate per epoch by given function, In LambdaLR after every epoch learning rate updated by given function and then multiplied by learning rate.\n\nexample: This is an example of what is happening in code cell below:\n\n    epoch 1: epoch\/10 = 1\/10 = 0.1,  \n    multiply it by lr:= lr * 0.1 = 0.01\n    UPDATED LEARNING RATE IS :- 0.01\n    \n    epoch 2: epoch\/10 = 2\/10 = 0.2,  \n    multiply it by lr:= lr * 0.1 = 0.02\n    UPDATED LEARNING RATE IS :- 0.02\n    \n    epoch 3: epoch\/10 = 3\/10 = 0.3,  \n    multiply it by lr:= lr * 0.1 = 0.03\n    UPDATED LEARNING RATE IS :- 0.03\n    \n    .....\n    .....","358acf1c":"# What lr_scheduler do?\n\nlr_scheduler method uses for adjest the learning rate for optimizer. while training model we want learning rate change according our requirements, for example at the initial state of training our we need little bit high learning rate than later states.\nif our learning too little then model will take lot of time to train, and if learning rate high at later state of training then model will not achive global minima. so thats why adjust learning rate is essential for getting higher accuracy.","028e4335":"## **ReduceLROnPlateau:**\n\n    Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced.","dcfd1234":"---","c398bd36":"## **StepLR:**\n\n**StepLR** updates the learning rate after every step_size by gamma, that means if step_size is 7 then learning rate will be updated after every 7 epoch by multiplying the current learning rate to gamma.\n\nExample: This is an example of what is happening in code cell below:\n    \n    gamma = 0.8, initial learning rate = 0.1.\n    \n    epoch 1-2:  learning rate = 0.1 x 0.8 = 0.08\n    \n    epoch 3-4: learing rate = 0.08 x 0.8 = 0.064\n    \n    epoch 5-6: learning rate = 0.064 x 0.8 = 0.0512\n\n    ...\n    \n","36132bf9":"---","ba8d3153":"### **Note: All type of scheduling operation perform after validation in every epoch except CyclickLR, OneCycleLR and CosineAnnealingWarmRestarts. These 3 performs in every batch iteration, so remember to put these three after optimizer step in training itertion. we will be going through examples of that.**","17643579":"![upvote](https:\/\/tenor.com\/view\/fucking-die-redditor-stupid-cuck-funny-cat-fumble-leonardo-de-carpio-gif-15197525.gif)","7f977917":"## **CyclicLR:**\n\n   \n\n    Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.\n    \n \n #### **Note: In CyclicLR learning rate updates in every train_batch iteration.***","08924f10":"* **Prepare data and model for examples** \n\n* [LambdaLR](#LambdaLR:)\n\n* [MultiplicativeLR](#MultiplicativeLR:)\n\n* [StepLR](#StepLR:)\n\n* [MultiStepLR](#MultiStepLR:)\n\n* [ExponentialLR](#ExponentialLR:)\n\n* [CosineAnnealingLR](#CosineAnnealingLR:)\n\n* [ReduceLROnPlateau](#ReduceLROnPlateau:)\n\n* [CyclicLR](#CyclicLR:)","dd118de9":"## **ExponentialLR:**\n\nIn **ExponentialLR** learning rate increases exponentially after every epoch. that means gamma will be multiplied by current learning rate after every epoch.\n\nexample: This is an example of what is happening in code cell below:\n        \n       gamma = 0.5, Inintial learning rate = 0.1\n        \n       epoch 1: learning rate = initial_lr x gamma = 0.1 x 0.5 = 0.05\n       \n       epoch 2: learning rate = current_lr x gamma = 0.05 x 0.5 = 0.025\n       \n       ---\n       \n       ---"}}