{"cell_type":{"fd531d47":"code","51f85ea3":"code","275d07e1":"code","a558e901":"code","99da2669":"code","db007397":"code","6858e3b9":"code","ddccec15":"code","525d4c37":"code","5c8e583f":"code","52fb2daa":"code","f4b19e52":"code","8dd6bb67":"code","24fb6533":"code","ed0b40a0":"code","eee9aa2d":"code","111a3d26":"code","74c29d6d":"code","754a096f":"code","4fdee105":"code","b6332799":"code","14791c96":"code","64effd95":"code","966a7730":"code","ebbbcad5":"code","8b29453e":"code","1ecda0e4":"code","26494099":"markdown","8362d6b0":"markdown","54a2bbb7":"markdown","381a0668":"markdown","9206cbba":"markdown","d8722c5d":"markdown","1003315f":"markdown","aa0ab351":"markdown","31e617c9":"markdown","f7204d51":"markdown","0044af30":"markdown","9062ab72":"markdown","eb2c0509":"markdown","ee743f73":"markdown","1df98d97":"markdown","21b2952e":"markdown","a0887353":"markdown","2be032c7":"markdown","be29af7c":"markdown"},"source":{"fd531d47":"import numpy as np\nimport pandas as pd\nimport xgboost\nimport math\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom __future__ import division\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import cross_validation, tree, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.metrics import explained_variance_score\nimport sklearn.learning_curve as curves\nfrom time import time\nfrom sklearn.metrics import r2_score\nimport os\nprint(os.listdir(\"..\/input\"))\n","51f85ea3":"data = pd.read_csv('..\/input\/kc_house_data.csv')\n","275d07e1":"# Copying data to another dataframe df_train for our convinience so that original dataframe remain intact.\n\ndf_train=data.copy()\ndf_train.rename(columns ={'price': 'SalePrice'}, inplace =True)","a558e901":"# Now lets see the first five rows of the data\ndata.head()","99da2669":"print(len(data))\n# Check the number of features in the data set\nprint(len(data.columns))\n# Check the data types of each column\nprint(data.dtypes)","db007397":"# Check any number of columns with NaN or missing values \nprint(data.isnull().any().sum(), ' \/ ', len(data.columns))\n# Check any number of data points with NaN\nprint(data.isnull().any(axis=1).sum(), ' \/ ', len(data))","6858e3b9":"# As id and date columns are not important to predict price so we are discarding it for finding correlation\nfeatures = data.iloc[:,3:].columns.tolist()\ntarget = data.iloc[:,2].name","ddccec15":"# Finding Correlation of price woth other variables to see how many variables are strongly correlated with price\ncorrelations = {}\nfor f in features:\n    data_temp = data[[f,target]]\n    x1 = data_temp[f].values\n    x2 = data_temp[target].values\n    key = f + ' vs ' + target\n    correlations[key] = pearsonr(x1,x2)[0]","525d4c37":"# Printing all the correlated features value with respect to price which is target variable\ndata_correlations = pd.DataFrame(correlations, index=['Value']).T\ndata_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]","5c8e583f":"var = 'sqft_living15'\ndata = pd.concat([data['price'], data[var]], axis=1)\ndata.plot.scatter(x=var, y='price', ylim=(3,9000000));","52fb2daa":"var = 'bedrooms'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 6))\nfig = sns.violinplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);\n\n\n","f4b19e52":"var = 'grade'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 6))\nfig = sns.violinplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);","8dd6bb67":"var = 'bathrooms'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(20, 20))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);","24fb6533":"var = 'sqft_living'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(3,8000000));\n","ed0b40a0":"var = 'floors'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(20, 20))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);","eee9aa2d":"#Pairplots to visualize strong correlation\nsns.set()\ncols = ['SalePrice', 'sqft_living', 'grade', 'sqft_above', 'view', 'bathrooms','bedrooms','sqft_basement']\nsns.pairplot(df_train[cols], height = 3.5)\nplt.show();","111a3d26":"new_data = df_train[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','waterfront','yr_built','lat','bedrooms','long']]","74c29d6d":"X = new_data.values\ny = df_train.SalePrice.values","754a096f":"X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y ,test_size=0.2)","4fdee105":"from sklearn.ensemble import RandomForestRegressor\nrand_regr = RandomForestRegressor(n_estimators=400,random_state=0)\nstart = time()\nrand_regr.fit(X_train, y_train)\nend=time()\ntrain_time_rand=end-start\nrandom=rand_regr.score(X_test,y_test)\npredictions = rand_regr.predict(X_test)\nexp_rand = explained_variance_score(predictions,y_test)","b6332799":"from sklearn.ensemble import GradientBoostingRegressor\nstart = time()\nest=GradientBoostingRegressor(n_estimators=400, max_depth=5, loss='ls',min_samples_split=2,learning_rate=0.1).fit(X_train, y_train)\nend=time()\ntrain_time_g=end-start\ngradient=est.score(X_test,y_test)\n\npred = est.predict(X_test)\nexp_est = explained_variance_score(pred,y_test)","14791c96":"from sklearn.ensemble import AdaBoostRegressor\nstart = time()\nada=AdaBoostRegressor(n_estimators=50, learning_rate=0.2,loss='exponential').fit(X_train, y_train)\nend=time()\ntrain_time_ada=end-start\npred=ada.predict(X_test)\nadab=ada.score(X_test,y_test)\npredict = ada.predict(X_test)\nexp_ada = explained_variance_score(predict,y_test)","64effd95":"from sklearn.tree  import DecisionTreeRegressor\ndecision=DecisionTreeRegressor()\nstart = time()\ndecision.fit(X_train, y_train)\nend=time()\ntrain_time_dec=end-start\ndecc=decision.score(X_test,y_test)\ndecpredict = decision.predict(X_test)\nexp_dec = explained_variance_score(decpredict,y_test)","966a7730":"# Comparing Models on the basis of Model's Accuracy Score and Explained Variance Score of different models\nmodels_cross = pd.DataFrame({\n    'Model': ['Gradient Boosting','AdaBoost','Random Forest','Decision Tree'],\n    'Score': [gradient,adab,random,decc],\n     'Variance Score': [exp_est,exp_ada,exp_rand,exp_dec]})\n    \nmodels_cross.sort_values(by='Score', ascending=False)","ebbbcad5":"import matplotlib.pyplot as plt\nimport numpy as np\nmodel = ['Adaboost', 'GBOOST', 'Random forest', 'Decision Tree']\nTrain_Time = [\n    train_time_ada,\n    train_time_g,\n    train_time_rand,\n    train_time_dec\n    \n]\nindex = np.arange(len(model))\nplt.bar(index, Train_Time)\nplt.xlabel('Machine Learning Models', fontsize=15)\nplt.ylabel('Training Time', fontsize=15)\nplt.xticks(index, model, fontsize=10, )\nplt.title('Comparison of Training Time of all ML models')\nplt.show()","8b29453e":"def ModelComplexity(X, y):\n    \"\"\" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. \"\"\"\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the max_depth parameter from 1 to 10\n    max_depth = np.arange(1,11)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = curves.validation_curve(GradientBoostingRegressor(), X, y, \\\n        param_name = \"max_depth\", param_range = max_depth, cv = cv, scoring = 'r2')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    plt.figure(figsize=(7, 5))\n    plt.title('Gradient Boosting Regressor Complexity Performance')\n    plt.plot(max_depth, train_mean, 'o-', color = 'r', label = 'Training Score')\n    plt.plot(max_depth, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    plt.fill_between(max_depth, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    plt.fill_between(max_depth, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n    \n    # Visual aesthetics\n    plt.legend(loc = 'lower right')\n    plt.xlabel('Maximum Depth')\n    plt.ylabel('Score')\n    plt.ylim([-0.05,1.05])\n    plt.show()","1ecda0e4":"ModelComplexity(X_train, y_train)","26494099":"In this step we check whether data contain null or missing values. What is the size of the data. What is the datatype of each column. What are unique values of categorical variables etc.","8362d6b0":"**STEP 4 : EDA or DATA VISUALIZATION **","54a2bbb7":"**STEP 3 : FINDING CORRELATION**","381a0668":"This is also a very important step in your prediction process as it help you to get aware you about existing patterns in the data how it is relating to your target variables etc.","9206cbba":"From the above figure it is inferred that decision tree has taken negligible amount of time to train where as Randome forest has taken maximum time and it is yet obvious because as we increase the number of tree 400 in this case training time will increase so we should look out for optimal model which has greater accuracy and less training time in comparison to other\nSo, in this case GBoost is the best choice as its accuracy is highest and it is taking less time to train wrt accuracy.","d8722c5d":"**STEP 6: APPLYING MACHINE LEARNING MODEL**","1003315f":"As zipcode is negatively correlated with sales price , so we can discard it for sales price prediction.","aa0ab351":"**STEP 7 : PLOTTING OF COMPLEXITY CURVE**","31e617c9":"**STEP 1:   IMPORTING LIBRARIES**","f7204d51":"Now, we know about the overall structure of a dataset . So let's apply some of the steps that we should generally do while applying machine learning models.\n","0044af30":"**Welcome to my kernel**\n\nIn this dataset we have to predict the sales price of  houses in King County, Seattle. It includes homes sold between May 2014 and May 2015.\nBefore doing anything we should first know about the dataset what it contains what are its features and what is the structure of data.\n\nThe dataset cantains **20** house features plus the price, along with **21613** observations.\n\nThe description for the 20 features is given below: <br>\n\n**1. id** :- It is the unique numeric  number assigned to each house being sold. <br>\n**2. date** :- It is the date on which the house was sold out. <br>\n**3. price**:- It is the price of house which we have to predict so this is our target variable and aprat from it are our features. <br>\n**4. bedrooms** :- It determines number of bedrooms in a house. <br>\n**5. bathrooms** :- It determines number of bathrooms in a bedroom of a house. <br>\n**6.  sqft_living** :- It is the measurement variable which determines the measurement of house in square foot. <br>\n**7. sqft_lot** : It is also the measurement variable which determines  square foot of the lot. <br>\n**8. floors**: It determines total floors means levels of house. <br>\n**9. waterfront** : This feature determines whether a house has a view to waterfront 0 means no 1 means yes. <br>\n**10. view** : This feature determines whether a house has been viewed or not  0 means no 1 means yes. <br>\n**11. condition** : It determines the overall condition of a house on a scale of 1 to 5. <br>\n**12. grade** : It determines the overall grade given to the housing unit, based on King County grading system on a scale of 1 to 11. <br>\n**13. sqft_above** : It determines square footage of house apart from basement. <br>\n**14. sqft_basement** : It determines square footage of the basement of the house. <br>\n**15. yr_built** : It detrmines the date of building of the house. <br>\n**16. yr_renovated** : It detrmines year of renovation of house. <br>\n**17. zipcode**  : It determines the zipcode of the location of the house. <br>\n**18. lat** : It determines the latitude of the location of the house. <br>\n**19.  long** : It determines the longitude of the location of the house. <br>\n**20. sqft_living15** : Living room area in 2015(implies-- some renovations)  <br>\n**21. sqft_lot15** : lotSize area in 2015(implies-- some renovations) <br>","9062ab72":"**STEP 2:  DATA CLEANING AND PREPROCESSING**","eb2c0509":"**Interpretation of the Curve**\n\nAt a maximum depth of 1, model suffers from high bias. Two scores are quite close,but both the scores are too far from acceptable level so I think it's a high bias problem.In other words, the model is underfitting. The data points obviously follow some sort of curve, but our predictor isn\u2019t complex enough to capture that information. Our model is biased in that it assumes that the data will behave in a certain fashion even though that assumption may not be true. A key point is that there\u2019s nothing wrong with our training\u2014this is the best possible fit that a linear model can achieve. There is, however, something wrong with the model itself in that it\u2019s not complex enough to model our data.\n\nAt a maximun depth of 10, model suffers from high variance since training score is 1.0 but validation score is about 0.7.In other words, a model is overfitting. Again, the data points suggest a sort of graceful curve. However, our model uses a very complex curve to get as close to every data point as possible. Consequently, a model with high variance has very low bias because it makes little to no assumption about the data. In fact, it adapts too much to the data.\n\nAs we see from the curve, max depth of 5 best generalizes the unseen data. As max depth increases, bias becomes lower and variance becomes higher. We should keep the balance between the two. Just after depth 5 training score increase upwards and validation score starts to goes down, so I it begins to suffer from overfitting. So that's why 5 should be a good choice.","ee743f73":"**STEP 5 : SPLITTING DATA INTO TRAINING AND TESTING SET**","1df98d97":"In this step we check by finding correlation of all the features wrt target variable i.e., price to see whether they are positively correlated or negatively correlated to find if they help in prediction process in model building process or not. But this is also one of the most important step as it also involves domain knowledge of the field of the data means you cannot simply remove the feature from your prediction process just because it is negatively correlated because it may contribute in future prediction for this you should take help of some domain knowledge personnel.","21b2952e":"**STEP 7 : ANALYZING TRAINING TIME EACH MODEL HAS TAKEN**","a0887353":"**Conclusion**\n\nSo, we have seen that  **accuracy** of gradient boosting is around  **89.28%** and also achieved decent **variance score** of **0.87** which is very close to 1 .\nTherefore, it is inferred that  **Gradient Boosting** is the suitable model for this dataset.\n\nFurther we can also perform model optimization by using GridSearch to find the appropriate parameters to increase the accuracy by fine tuning hyperparameters.\n\nApart from that,  your valuable suggestions for further improvement and optimization are always welcome from my side do comment !!","2be032c7":"The training dataset and test dataset must be similar, usually have the same predictors or variables. They differ on the observations and specific values in the variables. If you fit the model on the training dataset, then you implicitly minimize error or find correct responses. The fitted model provides a good prediction on the training dataset. Then you test the model on the test dataset. If the model predicts good also on the test dataset, you have more confidence. You have more confidence since the test dataset is similar to the training dataset, but not the same nor seen by the model. It means the model transfers prediction or learning in real sense.\n\nSo,by splitting dataset into training and testing subset, we can efficiently measure our trained model since it never sees testing data before.Thus it's possible to prevent overfitting.\n\nI am just splitting dataset into 20% of test data and remaining 80% will used for training the model.","be29af7c":"The following code cell produces a graph for a Gradient Boosting model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves \u2014 one for training and one for validation. Similar to the learning curves, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the performance_metric function."}}