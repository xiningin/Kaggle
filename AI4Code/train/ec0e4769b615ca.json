{"cell_type":{"9e924357":"code","d686d884":"code","5fdfc5aa":"code","7cdf62b4":"code","0735e891":"code","8be574e0":"code","e0fea92f":"code","fcc9b57f":"code","319428d4":"code","214b64fb":"code","e43245af":"code","22181213":"code","d0d56e00":"code","c8402574":"code","1826cdae":"code","fc5b6247":"code","4e2a027f":"code","a88c052e":"code","9a722d85":"code","dd827a37":"code","0489e5da":"code","f47ecdda":"code","3491e415":"code","45a4d402":"code","a85bf1e6":"code","edd0fe34":"code","95889825":"code","891038a5":"code","c5a08101":"code","e8c56c34":"code","ffdd4544":"code","2aa3a2be":"code","5deb506e":"code","7a7ccb0c":"code","a8e8bb41":"code","ce6374bc":"code","4a7aa4de":"code","c6526575":"code","2e3308af":"code","367a28e8":"code","72f864a2":"code","a54fd374":"code","af1fc923":"code","5affdec4":"code","dd6d5d6a":"code","e0177606":"code","704c3370":"code","5c524b3a":"code","00ce0221":"code","ca3d9484":"code","1503eb3a":"code","bef5bde8":"code","a6a82a27":"code","e7d7bd9f":"code","20cea3f8":"code","ff8ce2f6":"code","5040cf4a":"code","19fc5a1d":"code","b247edd9":"code","6737e325":"code","e45890f8":"code","d093c0cc":"markdown","1897432d":"markdown","2889751c":"markdown","fce4ede5":"markdown","82816af6":"markdown","0c7e6b8d":"markdown","927eb748":"markdown","3d4d9ebc":"markdown","e6df9b0e":"markdown","1587bec9":"markdown","6cb994f6":"markdown","cfcce7ce":"markdown","0f0bcab8":"markdown","c9235b2f":"markdown","18fe2fd0":"markdown","3f4f1b85":"markdown","a9751549":"markdown","c8f901b9":"markdown","7f80873e":"markdown","316af7c2":"markdown","d42869ec":"markdown","01c4d08d":"markdown","1a572982":"markdown","0c2568f7":"markdown","5bbc2ff3":"markdown","60e97fc7":"markdown","0fe4e6da":"markdown","b8b5ea2b":"markdown","77cc04d5":"markdown","b7b78066":"markdown","1e3b969e":"markdown","8ef61995":"markdown","68976dd5":"markdown","82372e44":"markdown","2b84fb83":"markdown","90ecbc55":"markdown","1edcfbfb":"markdown","3d4bf284":"markdown","54907dcc":"markdown","b162d150":"markdown","4fb4b15f":"markdown","159b0eb8":"markdown","1d39ed54":"markdown","917f7772":"markdown","4cfa68b5":"markdown","70141e62":"markdown","5b76ad29":"markdown","4a676131":"markdown","7bc23fc5":"markdown","388a77bd":"markdown","95d9a105":"markdown","5ed1ce92":"markdown","03d45b1f":"markdown","bd11b8e1":"markdown","2b711989":"markdown","93b1a1e3":"markdown","dd209a3d":"markdown","92973265":"markdown","1f6a985b":"markdown","297929b5":"markdown","82b8d722":"markdown","67ae2e0a":"markdown","ab290e2a":"markdown","39b233e6":"markdown","b635fc16":"markdown","6664fe04":"markdown","d4e9a8fa":"markdown","5fb2b994":"markdown","07d6a09a":"markdown","2b09ec86":"markdown","da3b05e3":"markdown","5e9415f8":"markdown","41401f02":"markdown","6f261b79":"markdown","fe28ac4a":"markdown"},"source":{"9e924357":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d686d884":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","5fdfc5aa":"\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","7cdf62b4":"data_description = open(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\", \"r\")\nprint(data_description.read())","0735e891":"train.shape","8be574e0":"test.shape","e0fea92f":"for col in train.columns:\n    print(col, str(sum(train[col].isnull())))","fcc9b57f":"num_columns = [name for name in train.columns.to_list() if train[name].dtype in ['int64', 'float64']]\nnum_columns = [name for name in num_columns if name not in ['SalePrice','Id'#,'MSSubClass'\n                                                           ]]","319428d4":"ohe_columns = [#'MSZoning',\n             'LandContour',\n             'LotConfig',\n             'Neighborhood',\n             'BldgType',\n             #'HouseStyle',\n             'RoofStyle',\n             #'RoofMatl',\n             'Foundation',\n             #'Heating',\n             #'Electrical',\n             'GarageType',\n             'GarageFinish',\n             #'MiscFeature',\n             #'SaleType',\n             'SaleCondition',\n              'Street',\n              'CentralAir'\n                ]","214b64fb":"qnadict = {'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2, 'NA':0}\n\ncustom_dic = {'LotShape':{'Reg':0, 'IR1':1, 'IR2':2, 'IR3':3},\n             'LandSlope':{'Gtl':0, 'Mod':1, 'Sev':2},\n             'BsmtExposure':{'NA':0, 'No':0, 'Mn':1, 'Av':2, 'Gd':3},\n             'Functional':{'Typ':0, 'Min1':-1, 'Min2':-2, 'Mod':-3, 'Maj1':-4, 'Maj2':-5, 'Sev':-6, 'Sal':-7, 'NA':0},\n              'PavedDrive':{'Y':2, 'P':1, 'N':0}\n             }\n\nqnadict_cols = ['BsmtQual',\n               'BsmtCond',\n               'FireplaceQu',\n               'GarageQual',\n               'GarageCond',\n               'PoolQC',\n               'ExterQual',\n               'ExterCond',\n               'HeatingQC',\n               'KitchenQual']\n    \nfor col in qnadict_cols:\n    custom_dic[col] = qnadict\n    \ndict_columns = list(custom_dic.keys())","e43245af":"oth_columns = ['Utilities',\n               'Condition1',\n               'Condition2',\n#                'Exterior1st',\n#                'Exterior2nd',\n               'MasVnrType',\n               'MasVnrArea',\n               'BsmtFinType1',\n               'BsmtFinType2',\n               'BsmtFinSF1',\n               'BsmtFinSF2',\n               'Fence'\n              ] ","22181213":"priv_dict = {'GdPrv':2, 'MnPrv':1, 'GdWo':0, 'MnWw':0, 'NA':0}\nwood_dict = {'GdPrv':0, 'MnPrv':0, 'GdWo':2, 'MnWw':1, 'NA':0}","d0d56e00":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DictProcessor(BaseEstimator, TransformerMixin):\n    def __init__(self, dictionaries):\n        self.dictionaries = dictionaries\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        \n        X = X.fillna('NA')\n        \n        for column in self.dictionaries.keys():\n            X[column] = X[column].apply(lambda row : self.dictionaries[column][row])\n        \n        return X","c8402574":"from sklearn.preprocessing import LabelBinarizer\n\nclass OtherEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, sparse_output=False):\n        self.sparse_output = sparse_output\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        X[['MasVnrArea','BsmtFinSF1','BsmtFinSF2']] = X[['MasVnrArea','BsmtFinSF1','BsmtFinSF2']].fillna(0)\n        X = X.fillna('NA')\n        \n        lb = LabelBinarizer()\n        \n        #~~~~~~~~~~~~~~~~~~~~~~~~~\n        \n        X['Sewage'] = (X['Utilities']=='AllPub').astype(int) # In practice it is only a matter of sewage or not sewage\n        X = X.drop('Utilities',axis=1)\n            \n        #~~~~~~~~~~~~~~~~~~~~~~~~~\n            \n        cond1 = lb.fit_transform(X['Condition1'])\n        cond2 = lb.transform(X['Condition2'])\n        cond = cond1 + cond2\n        X[lb.classes_] = cond\n        X = X.drop(['Condition1', 'Condition2','Norm'],axis=1) # The normal condition does not add any information\n        \n        #~~~~~~~~~~~~~~~~~~~~~~~~~\n        \n#         ext1 = lb.fit_transform(X['Exterior1st'])\n#         ext2 = lb.transform(X['Exterior2nd'])\n#         ext = ext1 + ext2\n#         X[lb.classes_] = ext\n#         X = X.drop(['Exterior1st', 'Exterior2nd'],axis=1)\n        \n        #~~~~~~~~~~~~~~~~~~~~~~~~~\n        \n        mvt = lb.fit_transform(X['MasVnrType'].astype(str))\n        X[lb.classes_] = mvt\n        for col in lb.classes_:\n            X[col] = X[col] * X['MasVnrArea']\n        X = X.drop(['MasVnrArea','MasVnrType','NA'],axis=1)\n        \n        #~~~~~~~~~~~~~~~~~~~~~~~~~\n        \n        X['Basement'] = (X['BsmtFinType1'] == 'NA').astype(int) # Putting in an attribute which says whether there is a basement or not\n        \n        bft1 = lb.fit_transform(X['BsmtFinType1'].astype(str))\n        for col in range(bft1.shape[1]):\n            bft1[:,col] = bft1[:,col] * X['BsmtFinSF1']\n\n        bft2 = lb.fit_transform(X['BsmtFinType2'].astype(str))\n        for col in range(bft2.shape[1]):\n            bft2[:,col] = bft2[:,col] * X['BsmtFinSF2']\n\n        bft = bft1 + bft2\n        X[lb.classes_] = bft\n        \n        X = X.drop(['BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2', 'NA'],axis=1)\n        \n        #~~~~~~~~~~~~~~~~~~~~~~~~~\n        \n        X['PrivacyFence'] = X['Fence'].apply(lambda row : priv_dict[row])\n        X['WoodFence'] = X['Fence'].apply(lambda row : wood_dict[row])\n        X = X.drop(['Fence'],axis=1)        \n        \n        return X","1826cdae":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler())\n    ])\n\nohe_pipeline = Pipeline([\n    ('imp', SimpleImputer(strategy=\"constant\", fill_value=\"NA\")),\n    ('one_hot', OneHotEncoder()),\n    ('std_scaler', StandardScaler(with_mean=False))\n    ])\n\ndict_pipeline = Pipeline([\n    ('dict_processor', DictProcessor(custom_dic)),\n    ('std_scaler', StandardScaler())\n    ])\n\nother_pipeline = Pipeline([\n    ('other', OtherEncoder()),\n    ('std_scaler', StandardScaler())\n    ])\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_columns),\n    (\"ohe\", ohe_pipeline, ohe_columns),\n    (\"dict\", dict_pipeline, dict_columns),\n    (\"other\", other_pipeline, oth_columns)\n    ])","fc5b6247":"explore_train = train.copy()","4e2a027f":"train_num_norm = num_pipeline.fit_transform(explore_train[num_columns])\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2).fit(train_num_norm)\ntrain_pca = pca.transform(train_num_norm)\n\nexplore_train['pca1'] = train_pca[:, 0]\nexplore_train['pca2'] = train_pca[:, 1]","a88c052e":"pca.explained_variance_ratio_","9a722d85":"correlations = pd.DataFrame(data=num_columns,columns=['names'])\n\n\ncorrelations['pca1_correls'] = correlations['names'].apply(lambda row : round(explore_train['pca1'].corr(train[row]),3))\ncorrelations['pca2_correls'] = correlations['names'].apply(lambda row : round(explore_train['pca2'].corr(train[row]),3))","dd827a37":"correlations = correlations.sort_values('pca1_correls',axis=0,ascending=False)\ncorrelations","0489e5da":"correlations = correlations.sort_values('pca2_correls',axis=0,ascending=False)\ncorrelations","f47ecdda":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nexplore_train['LogSalePrice'] = np.log(explore_train['SalePrice'])\n\nplt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='HouseStyle',\n                data=explore_train\n               )","3491e415":"explore_train['LogSalePrice'] = np.log(explore_train['SalePrice'])\n\nplt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='LogSalePrice',\n                data=explore_train\n               )","45a4d402":"explore_train[explore_train['pca1'] > 11]","a85bf1e6":"train = train.drop([523,1298])","edd0fe34":"qual_num_columns = ['OverallCond',\n                    'OverallQual',\n                    'MSSubClass']\n\nsize_num_columns = ['LotArea',\n                    'LotFrontage',\n                    '1stFlrSF',\n                    '2ndFlrSF',\n                    'LowQualFinSF',\n                    'GrLivArea',]\n\nbath_num_columns = ['BsmtFullBath',\n                    'BsmtHalfBath',\n                    'FullBath',\n                    'HalfBath']\n\ngarage_num_columns = ['GarageYrBlt',\n                      'GarageCars',\n                      'GarageArea']\n\nbasement_num_columns = ['BsmtFinSF1',\n                        'BsmtFinSF2',\n                        'BsmtUnfSF',\n                        'TotalBsmtSF']\n\nhouse_num_columns = ['BedroomAbvGr',\n                     'KitchenAbvGr',\n                     'TotRmsAbvGrd',\n                     'WoodDeckSF']\n\nporch_num_columns = ['OpenPorchSF',\n                     'EnclosedPorch',\n                     '3SsnPorch',\n                     'ScreenPorch',]\n\nfeature_num_columns = ['MasVnrArea',\n                       'PoolArea',\n                       'MiscVal',\n                       'Fireplaces']\n\ntime_num_columns = ['YearBuilt',\n                    'YearRemodAdd',\n                    'MoSold',\n                    'YrSold']","95889825":"from functools import reduce\n\nfig, axs = plt.subplots(3,2,figsize=(20,10))\n\naxs = reduce(lambda x,y: x+y, axs.tolist()) # We convert the array of subplots to a list to make it easier to work with\n\nfor i in range(len(size_num_columns)):\n    axs[i].hist(train[size_num_columns[i]],bins=50)\n    axs[i].set_title(size_num_columns[i])","891038a5":"fig, axs = plt.subplots(2,2,figsize=(20,10))\n\naxs = reduce(lambda x,y: x+y, axs.tolist()) # We convert the array of subplots to a list to make it easier to work with\n\nfor i in range(len(bath_num_columns)):\n    axs[i].hist(train[bath_num_columns[i]],bins=3)\n    axs[i].set_title(bath_num_columns[i])","c5a08101":"plt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='FullBath',\n                data=explore_train\n               )","e8c56c34":"fig, axs = plt.subplots(1,3,figsize=(20,5))\n\nfor i in range(len(garage_num_columns)):\n    axs[i].hist(train[garage_num_columns[i]],bins=10)\n    axs[i].set_title(garage_num_columns[i])","ffdd4544":"plt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='GarageCars',\n                data=explore_train\n               )","2aa3a2be":"fig, axs = plt.subplots(2,2,figsize=(20,5))\n\naxs = reduce(lambda x,y: x+y, axs.tolist()) # We convert the array of subplots to a list to make it easier to work with\n\nfor i in range(len(basement_num_columns)):\n    axs[i].hist(train[basement_num_columns[i]],bins=20)\n    axs[i].set_title(basement_num_columns[i])","5deb506e":"plt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='TotalBsmtSF',\n                data=explore_train[explore_train['TotalBsmtSF'] < 3000]\n               )","7a7ccb0c":"fig, axs = plt.subplots(2,2,figsize=(20,5))\n\naxs = reduce(lambda x,y: x+y, axs.tolist()) # We convert the array of subplots to a list to make it easier to work with\n\nfor i in range(len(house_num_columns)):\n    axs[i].hist(train[house_num_columns[i]],bins=8)\n    axs[i].set_title(house_num_columns[i])","a8e8bb41":"plt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='TotRmsAbvGrd',\n                data=explore_train[explore_train['TotRmsAbvGrd'] < 14]\n               )","ce6374bc":"fig, axs = plt.subplots(2,2,figsize=(20,5))\n\naxs = reduce(lambda x,y: x+y, axs.tolist()) # We convert the array of subplots to a list to make it easier to work with\n\nbs = {0:30, 1:30, 2:12, 3:5}\n\nfor i in range(len(time_num_columns)):\n    axs[i].hist(train[time_num_columns[i]],bins=bs[i])\n    axs[i].set_title(time_num_columns[i])","4a7aa4de":"plt.subplots(figsize=(15, 10))\nsns.scatterplot('pca1','pca2',hue='YrSold',\n                data=explore_train[explore_train['TotRmsAbvGrd'] < 14]\n               )","c6526575":"correlations['price_correls'] = correlations['names'].apply(lambda row : round(train['SalePrice'].corr(train[row]),3))\ncorrelations = correlations.sort_values('price_correls',axis=0,ascending=False)\ncorrelations","2e3308af":"MSSubClass_vs_Style = pd.DataFrame(data=np.nan,index=train['MSSubClass'].unique(),columns=train['HouseStyle'].unique())\n\nfor cl in train['MSSubClass'].unique():\n    for st in train['HouseStyle'].unique():\n        MSSubClass_vs_Style[st][cl] = len(train[(train['MSSubClass']==cl) & (train['HouseStyle']==st)])\n\nfor ty in train['BldgType'].unique():\n    MSSubClass_vs_Style[ty] = np.nan\n        \nfor cl in train['MSSubClass'].unique():\n    for ty in train['BldgType'].unique():\n        MSSubClass_vs_Style[ty][cl] = len(train[(train['MSSubClass']==cl) & (train['BldgType']==ty)])\n        \nmssc_dict = {20:'1-STORY 1946 & NEWER ALL STYLES',\n             30:'1-STORY 1945 & OLDER',\n             40:'1-STORY W\/FINISHED ATTIC ALL AGES',\n             45:'1-1\/2 STORY - UNFINISHED ALL AGES',\n             50:'1-1\/2 STORY FINISHED ALL AGES',\n             60:'2-STORY 1946 & NEWER',\n             70:'2-STORY 1945 & OLDER',\n             75:'2-1\/2 STORY ALL AGES',\n             80:'SPLIT OR MULTI-LEVEL',\n             85:'SPLIT FOYER',\n             90:'DUPLEX - ALL STYLES AND AGES',\n             120:'1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n             150:'1-1\/2 STORY PUD - ALL AGES',\n             160:'2-STORY PUD - 1946 & NEWER',\n             180:'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n             190:'2 FAMILY CONVERSION - ALL STYLES AND AGES'}        \n        \nMSSubClass_vs_Style['yr'] = np.nan\nMSSubClass_vs_Style['txt'] = np.nan\n\nfor i in MSSubClass_vs_Style.index.to_list():\n    MSSubClass_vs_Style['yr'][i] = round(train[train['MSSubClass']==i]['YearBuilt'].mean(),0)\n    MSSubClass_vs_Style['txt'][i] = mssc_dict[i]\n    \nMSSubClass_vs_Style.sort_index()","367a28e8":"f, ax = plt.subplots(figsize=(15, 10))\nax.set_xscale('log')\nax.set_yscale('log')\nsns.scatterplot('LotArea','GrLivArea',hue='MSZoning',\n                data=train)","72f864a2":"for i in train['MSZoning'].unique():\n    print(i + \": \" + str(round(train[train['MSZoning']==i]['SalePrice'].mean(), 0)))","a54fd374":"train['Neighborhood'].value_counts()","af1fc923":"neighborhood_prices = pd.DataFrame(data=np.nan,index=train['Neighborhood'].unique(),columns=['Price'])\n\nfor neighborhood in train['Neighborhood'].unique():\n    neighborhood_prices['Price'][neighborhood] = train['SalePrice'][train['Neighborhood']==neighborhood].mean()\n    \nneighborhood_prices = neighborhood_prices.sort_values('Price',axis=0,ascending=False)\nneighborhood_prices","5affdec4":"plt.figure(figsize = (12,5))\nplt.hist(train['YearBuilt'],bins=20)\nplt.xlabel('YearBuilt')\nplt.ylabel('Frequency')\nplt.show()","dd6d5d6a":"plt.figure(figsize = (12,5))\nplt.hist(train['YearRemodAdd'],bins=20)\nplt.xlabel('YearRemodAdd')\nplt.ylabel('Frequency')\nplt.show()","e0177606":"ages = pd.DataFrame(data=train['Neighborhood'].unique(),columns=['neighbourhoods'])\n\nages['av_date'] = ages['neighbourhoods'].apply(lambda area : round(train[train['Neighborhood']==area]['YearBuilt'].mean()))\nages['price'] = ages['neighbourhoods'].apply(lambda area : round(train[train['Neighborhood']==area]['SalePrice'].mean()))\nages['av_remod_date'] = ages['neighbourhoods'].apply(lambda area : round(train[train['Neighborhood']==area]['YearRemodAdd'].mean()))\n\nages = ages.sort_values('av_date',axis=0,ascending=True)\nages","704c3370":"proc_train = full_pipeline.fit_transform(train.drop(['SalePrice'], axis=1))","5c524b3a":"proc_test = full_pipeline.transform(test)","00ce0221":"X = proc_train\ny = np.log(train['SalePrice'])","ca3d9484":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","1503eb3a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nreg = LinearRegression()\nscores = cross_val_score(reg, X, y,\nscoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\ndisplay_scores(rmse_scores)","bef5bde8":"from sklearn import linear_model\n\nridge_reg = linear_model.Ridge(alpha=1500)\nscores = cross_val_score(ridge_reg, X, y,\nscoring=\"neg_mean_squared_error\", cv=10)\nridge_rmse_scores = np.sqrt(-scores)\ndisplay_scores(ridge_rmse_scores)","a6a82a27":"lasso_reg = linear_model.Lasso(alpha=1500)\nscores = cross_val_score(lasso_reg, X, y,\nscoring=\"neg_mean_squared_error\", cv=10)\nlasso_rmse_scores = np.sqrt(-scores)\ndisplay_scores(lasso_rmse_scores)","e7d7bd9f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n{'n_estimators': [3, 10, 30, 50], 'max_features': [2, 4, 6, 8, 10, 12]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\n\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error')\ngrid_search.fit(X, y)","20cea3f8":"grid_search.best_estimator_","ff8ce2f6":"rf_reg = RandomForestRegressor(max_features=12, n_estimators=50)\nscores = cross_val_score(rf_reg, X, y,\nscoring=\"neg_mean_squared_error\", cv=10)\nrf_rmse_scores = np.sqrt(-scores)\ndisplay_scores(rf_rmse_scores)","5040cf4a":"import xgboost as xgb\n\nxgbr = xgb.XGBRegressor(verbosity=0) \nscores = cross_val_score(xgbr, X, y,\nscoring=\"neg_mean_squared_error\", cv=10)\nxgb_rmse_scores = np.sqrt(-scores)\ndisplay_scores(xgb_rmse_scores)","19fc5a1d":"from catboost import CatBoostRegressor\nfrom mlxtend.regressor import StackingRegressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          xgb.XGBRegressor(),\n                                         RandomForestRegressor(max_features=12, n_estimators=50),\n                                         linear_model.Ridge(alpha=1500)),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\nscores = cross_val_score(stack_gen, proc_train, np.log(train['SalePrice']),\n                    scoring=\"neg_mean_squared_error\", cv=10)\nstack_rmse_scores = np.sqrt(-scores);","b247edd9":"display_scores(stack_rmse_scores)","6737e325":"stack_gen.fit(X, y)\nstack_gen.predict(proc_test)","e45890f8":"sample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsample_submission['SalePrice'] = np.exp(stack_gen.predict(proc_test))\nsubmission = sample_submission\nsubmission.to_csv('my_submission.csv', index=False)","d093c0cc":"# Regression","1897432d":"Now let's go through some of the different columns and see what they look like.","2889751c":"It looks like, for the most part, all the MSZoning classes are encompassed by other features. But note that actually there is some inconsistency in the data: for example, the property in MSSubClass 20 which is listed as having two storeys.","fce4ede5":"For me, if you had to pick one plot which gave the best overview of the data set, this would be it.\n\nThis plot illustrates well the different sorts of properties that exist in the data set and how they are distributed in terms of quality and size.\n* The three largest groups are 1-storey, 1.5-storey, and 2-storey houses.\n* The 1-storey houses are clearly differentiated from the 1.5-storey and 2-storey differentiated along the pca2 axis (~size).\n* The 1.5-storey and 2-storey houses are broadly the same in terms of pca2, but the 2-storey houses tend to have a higher value of pca1.\n* Apart from these three big groups, one can also discern some smaller groups.\n* Split-foyer houses, which have the lowest values of both principal components out of any of the types of properties.\n* Properties which have an unfinished floor tend to have low values of pca1, with the 2.5-storey houses having higher values of pca2 than the unfinished, which makes sense if pca2 is roughly size.","82816af6":"# Preprocessing","0c7e6b8d":"We consider now the basements of the properties.","927eb748":"**Looking at specific columns**","3d4d9ebc":"We now define two classes to carry out the preprocessing for the dict_columns and oth_columns.","e6df9b0e":"It is clear that there is a large distinction between the ages of different neighbourhoods, spanning between the old town built on average in 1923, and NridgHt built in 2006. But, while the newer areas tend to be pricier, this is not a straightforward trajectory: prices go up as well as down as the areas get older.","1587bec9":"Now we look at garages.","6cb994f6":"Commercial property is much cheaper than the rest, while high and medium density are cheaper than low density, with floating village being the most expensive.","cfcce7ce":"We get a nice view of the different garage sizes in the principal component plot. As pca1 increases, the garages have a larger car capacity.","0f0bcab8":"The most correlated attribute with pca1 is OverallQual and with pca2 is 2ndFlrSF. Hence, one can roughly think of pca1 as quality and pca2 as size. These are the two principal axes which discriminate between different properties.","c9235b2f":"So ridge regression does improve the overfitting problem substantially, as one would expect.","18fe2fd0":"It will also be useful to read the text file describing the different columns.","3f4f1b85":"There is quite a difference in prices between different areas, as one would expect.","a9751549":"We now look at which features are most correlated with the two principal components.","c8f901b9":"Now we look at the attributes which concern bathrooms.","7f80873e":"We split the columns of the training set into four different lists:\n* num_columns, which contains columns with numerical data;\n* ohe_columns, which contains columns where we will apply one-hot encoding;\n* dict_columns, which contains data where we will apply a dictionary to convert the data to numerical data;\n* oth_columns, where we will apply some miscellaneous preprocessing.\n\nSome columns we comment out because they cause errors: for instance, in one-hot encoding the values may be different between the train and test sets.","316af7c2":"We now define pipelines to process the different sorts of columns, and assemble them together into one large pipeline which will be able to preprocess our entire data set.","d42869ec":"Now we look at the attributes concerning features of the house.","01c4d08d":"Here we have excluded the properties with the very largest basements to get a better idea of the general picture. As pca1 (quality) increases, the size of the basement does too. It is interesting that it is the quality of the property that is correlated with basement size, and not the size of the property (pca2). Indeed, if anything, the size of the basement seems to decrease with pca2.","1a572982":"Let's look at the MSSubClass column first, which is described as follows:\n\nMSSubClass: Identifies the type of dwelling involved in the sale.\t\n\n        20\t1-STORY 1946 & NEWER ALL STYLES\n        30\t1-STORY 1945 & OLDER\n        40\t1-STORY W\/FINISHED ATTIC ALL AGES\n        45\t1-1\/2 STORY - UNFINISHED ALL AGES\n        50\t1-1\/2 STORY FINISHED ALL AGES\n        60\t2-STORY 1946 & NEWER\n        70\t2-STORY 1945 & OLDER\n        75\t2-1\/2 STORY ALL AGES\n        80\tSPLIT OR MULTI-LEVEL\n        85\tSPLIT FOYER\n        90\tDUPLEX - ALL STYLES AND AGES\n       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150\t1-1\/2 STORY PUD - ALL AGES\n       160\t2-STORY PUD - 1946 & NEWER\n       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES","0c2568f7":"We now try an ensemble approach where we combine several different regressions.","5bbc2ff3":"Again, not quite as good as ridge regression.","60e97fc7":"The distribution of the number of bathrooms is very distinctive in the principal-component plot. Recall that pca1 is roughly quality. Most properties have one or two bathrooms, but the properties with two bathrooms tend to have significantly higher values of pca1. A few properties have 0 bathrooms and 3 bathrooms, with those with 0 bathrooms tend to have lower values of *both* principal components while those with 3 bathrooms have higher values of both.","0fe4e6da":"To get a broad picture of what the data set looks like, we do some dimensionality reduction using principal components analysis. We do this using the numerical columns, which we preprocess using our numerical pipeline.","b8b5ea2b":"We set up our X and y for the regressions. We take the logarithm of SalePrice, since the competition is scored using the root mean squared error of the logarithm of SalePrice.","77cc04d5":"The remodelling dates are distributed quite differently to the dates when the houses were originally built. There are no houses remodelled earlier than 1950, but a large spike in 1950. It is hard to think of a good explanation for this.","b7b78066":"Now we look at MSZoning, which is described as follows:\n\nMSZoning: Identifies the general zoning classification of the sale.\n\t\t\n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density","1e3b969e":"The residential medium density and floating village residential stands out quite clearly if plotted agains two different measures of area. These are, of course, much smaller than the residential low density properties.","8ef61995":"'Fence' will require some processing by dictionaries, so we define these below.","68976dd5":"We now try XGB regression.","82372e44":"Only a relatively small proportion of the dataset is explained by the first two principal components, which makes sense given the complexity of the dataset.","2b84fb83":"**Looking at groups of features**","90ecbc55":"So the test set and the train set are of a similar size.","1edcfbfb":"We plot MSZoning against two different measures of size of the property to see how the classes are distributed. We use log scales to get a better idea of the picture.","3d4bf284":"First we apply the pipeline to both the test and train sets to convert the data into a form suitable for regression. We must drop the 'SalePrice' column, so that the two dataframes we are processing are of the same size. This is what the pipeline requires.","54907dcc":"We now try random forest regression, for which we use grid search to find the best parameters.","b162d150":"It is interesting to note that most properties sell in the middle of the year. Year sold does not seem to add much information: all years are broadly the same apart from 2009, and that is probably an artefact of when the data were collected. A lot of remodelling was done in 1950, but otherwise houses tend to be built and remodelled more recently. Note the undulating pattern in year built though, suggesting the boom and bust cycle of the housing market.\n\nOne can check that year sold does not show up in the principal component plot, as one would expect. The same is true of month sold.","4fb4b15f":"It is interesting that total number of rooms above ground seems to be better correlated with the second principal component, rather than the first, unlike basement size.","159b0eb8":"We choose the best estimator found by grid search and test it using cross-validation.","1d39ed54":"Now we look at some specific columns which are interesting. First let us see which columns are best correlated with SalePrice.","917f7772":"Now we look at 'Neighborhood': Physical locations within Ames city limits\n\n       Blmngtn\tBloomington Heights\n       Blueste\tBluestem\n       BrDale\tBriardale\n       BrkSide\tBrookside\n       ClearCr\tClear Creek\n       CollgCr\tCollege Creek\n       Crawfor\tCrawford\n       Edwards\tEdwards\n       Gilbert\tGilbert\n       IDOTRR\tIowa DOT and Rail Road\n       MeadowV\tMeadow Village\n       Mitchel\tMitchell\n       Names\tNorth Ames\n       NoRidge\tNorthridge\n       NPkVill\tNorthpark Villa\n       NridgHt\tNorthridge Heights\n       NWAmes\tNorthwest Ames\n       OldTown\tOld Town\n       SWISU\tSouth & West of Iowa State University\n       Sawyer\tSawyer\n       SawyerW\tSawyer West\n       Somerst\tSomerset\n       StoneBr\tStone Brook\n       Timber\tTimberland\n       Veenker\tVeenker","4cfa68b5":"Lasso regression doesn't perform as well as ridge regression.","70141e62":"The top five columns correlated with SalePrice are OverallQual, GrLivArea, TotalBsmtSF, GarageCars, 1stFlrSF.","5b76ad29":"Finally, we obtain something better than ridge regression. We use this to make the predictions for our submission.","4a676131":"Mostly properties have full bathrooms outside the basement, unsurprisingly.","7bc23fc5":"The two data points on the far right of this principal component plot seem to buck this trend, so we will treat them as outliers and remove them from the training set.","388a77bd":"We use the sample submission as a template for our submission. Since we have predicted the log prices, we must exponentiate before we submit.","95d9a105":"Note that, according to the data_description file, missing values in the dataset are not actually missing values per se, but rather instances where the relevant feature does not exist. We bear this in mind when preprocessing, and tend to fill in missing values with things like 'NA'. Columns with lots of missing values are not columns where there is a lot of missing information, but rather columns where only very few properties possess the particular attribute.","5ed1ce92":"Now we look at YearBuilt: Original construction date.","03d45b1f":"If we look at how the log of SalePrice behaves in the principal component plot, we see that it largely follows the first principal component.","bd11b8e1":"This is still not quite as good as ridge regression.","2b711989":"Garages are mostly built recently and mostly house two cars. The garage area is positively skewed.","93b1a1e3":"We will use cross-validation, so we define the following function to display the cross-validation scores. This code is taken from *A Hands-on Approach to Machine Learning*.","dd209a3d":"Plotting the data against the first two principal components whilst colouring according to HouseStyle gives us a good broad picture of the data set.","92973265":"We first try straightforward linear regression on all the processed data.","1f6a985b":"We look at the attributes which concern size of the property.","297929b5":"Let's look at how big the train and test set are.","82b8d722":"These distributions mostly have the positive skew one would expect. Note that many properties do not have second floors. Indeed, LowQualFinSF seems to be an effectively useless property.","67ae2e0a":"We first create a copy of the training set so that any additions we make do not affect the original data set.","ab290e2a":"# Extracting data","39b233e6":"We begin by extracting the data we need.","b635fc16":"# Exploratory data analysis","6664fe04":"The distribution around the different areas is fairly even.","d4e9a8fa":"Much of the information contained in MSSubClass is also contained in other columns, for instance HouseStyle and BldgType. Let's see the extent to which this is true.","5fb2b994":"**Principal component analysis**","07d6a09a":"There are two notable features of this distribution. Firstly, it grows as time passes. This reflects both the growth of the city and the fact that houses do not last forever. The second feature is that it undulates, which could reflect the boom and bust cycle of the housing market.\n\nRecall that YearBuilt is well-correlated with SalePrice: 0.522897. Now we look at YearRemodAdd: Remodel date (same as construction date if no remodeling or additions). This is slightly less well correlated with SalePrice: 0.507101. I suppose that this is because remodelling the house is less of a big deal than rebuilding it.","2b09ec86":"Again positive skew, although BedroomAbvGr is fairly normally distributed.","da3b05e3":"These distributions have the positive skew one would expect. There are quite a large number of houses with unfinished basements.","5e9415f8":"Now we define the dictionaries which we will use to process the relevant columns. Many of these columns have the same grading scale, which we encode in the dictionary \"qnadict\".","41401f02":"The ordinary linear regression is massively overfitting, hence the very high errors. Instead we try some forms of regularised regression.","6f261b79":"We now group several features together so that we can get a picture of how the dataset behaves with respect to certain sorts of attributes.","fe28ac4a":"Now we look at temporal attributes of the properties."}}