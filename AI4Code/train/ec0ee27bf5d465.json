{"cell_type":{"5c4a2205":"code","82a6cd98":"code","786b127d":"code","0a3bf43f":"code","4da087a1":"code","6f933bec":"code","d0be95c4":"code","fb134fab":"code","9bb71fd6":"code","4e4cb8e6":"code","a2b1abd1":"code","77c5de39":"code","8e9f07ce":"code","fd107931":"code","5e565a52":"code","e29e82b4":"code","a7ab2978":"markdown","ada5cc9f":"markdown","6cb07e5a":"markdown","2c8b00c7":"markdown","1eb76ee7":"markdown","43e9d30d":"markdown","fc5bbcd5":"markdown","446c2b4e":"markdown","09c75fd4":"markdown","a7b78924":"markdown","b9807770":"markdown","bfdb78d5":"markdown","9067a0a0":"markdown","50186305":"markdown"},"source":{"5c4a2205":"import numpy as np\nimport matplotlib.pyplot as plt \n\nimport pandas as pd  \nimport seaborn as sns \n\n%matplotlib inline","82a6cd98":"from sklearn.datasets import load_boston\n\nboston_dataset = load_boston()\n\n# boston_dataset is a dictionary\n# let's check what it contains\nboston_dataset.keys()","786b127d":"boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston.head()","0a3bf43f":"boston['MEDV'] = boston_dataset.target","4da087a1":"# check for missing values in all the columns\nboston.isnull().sum()","6f933bec":"# set the size of the figure\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\n# plot a histogram showing the distribution of the target values\nsns.distplot(boston['MEDV'], bins=30)\nplt.show()","d0be95c4":"# compute the pair wise correlation for all columns  \ncorrelation_matrix = boston.corr().round(2)","fb134fab":"# use the heatmap function from seaborn to plot the correlation matrix\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)","9bb71fd6":"plt.figure(figsize=(20, 5))\n\nfeatures = ['LSTAT', 'RM']\ntarget = boston['MEDV']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = boston[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('MEDV')","4e4cb8e6":"X = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns = ['LSTAT','RM'])\nY = boston['MEDV']","a2b1abd1":"from sklearn.model_selection import train_test_split\n\n# splits the training and test data set in 80% : 20%\n# assign random_state to any value.This ensures consistency.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","77c5de39":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlin_model = LinearRegression()\nlin_model.fit(X_train, Y_train)","8e9f07ce":"# model evaluation for training set\n\ny_train_predict = lin_model.predict(X_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\nr2 = r2_score(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\n\n# model evaluation for testing set\n\ny_test_predict = lin_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","fd107931":"# plotting the y_test vs y_pred\n# ideally should have been a straight line\nplt.scatter(Y_test, y_test_predict)\nplt.show()","5e565a52":"from sklearn.preprocessing import PolynomialFeatures\n\ndef create_polynomial_regression_model(degree):\n  \"Creates a polynomial regression model for the given degree\"\n  poly_features = PolynomialFeatures(degree=degree)\n  \n  # transform the features to higher degree features.\n  X_train_poly = poly_features.fit_transform(X_train)\n  \n  # fit the transformed features to Linear Regression\n  poly_model = LinearRegression()\n  poly_model.fit(X_train_poly, Y_train)\n  \n  # predicting on training data-set\n  y_train_predicted = poly_model.predict(X_train_poly)\n  \n  # predicting on test data-set\n  y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n  \n  # evaluating the model on training dataset\n  rmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predicted))\n  r2_train = r2_score(Y_train, y_train_predicted)\n  \n  # evaluating the model on test dataset\n  rmse_test = np.sqrt(mean_squared_error(Y_test, y_test_predict))\n  r2_test = r2_score(Y_test, y_test_predict)\n  \n  print(\"The model performance for the training set\")\n  print(\"-------------------------------------------\")\n  print(\"RMSE of training set is {}\".format(rmse_train))\n  print(\"R2 score of training set is {}\".format(r2_train))\n  \n  print(\"\\n\")\n  \n  print(\"The model performance for the test set\")\n  print(\"-------------------------------------------\")\n  print(\"RMSE of test set is {}\".format(rmse_test))\n  print(\"R2 score of test set is {}\".format(r2_test))","e29e82b4":"create_polynomial_regression_model(2)","a7ab2978":"I**mport the required Libraries**","ada5cc9f":"**Data Visualization**","6cb07e5a":"**Data preprocessing**","2c8b00c7":"**Load the data into pandas dataframe**","1eb76ee7":"**Correlation matrix**","43e9d30d":"**Split the data into training and testing sets**","fc5bbcd5":"**Prepare the data for training**","446c2b4e":"## Polynomial Regression on Boston Housing Dataset\n\n**In this notebook we do a comparative study of Linear Regression and Polynomial Regression accuracy on the Boston Housing Dataset**\n\nThis data was originally a part of UCI Machine Learning Repository and has been removed now. This data also ships with the scikit-learn library. \nThere are 506 samples and 13 feature variables in this data-set. The objective is to predict the value of prices of the house using the given features.\n\nThe description of all the features is given below:\n\n  **CRIM**: Per capita crime rate by town\n\n  **ZN**: Proportion of residential land zoned for lots over 25,000 sq. ft\n\n  **INDUS**: Proportion of non-retail business acres per town\n\n  **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n  **NOX**: Nitric oxide concentration (parts per 10 million)\n\n  **RM**: Average number of rooms per dwelling\n\n  **AGE**: Proportion of owner-occupied units built prior to 1940\n\n  **DIS**: Weighted distances to five Boston employment centers\n\n  **RAD**: Index of accessibility to radial highways\n\n  **TAX**: Full-value property tax rate per $10,000\n\n  **B**: 1000(Bk - 0.63)\u00b2, where Bk is the proportion of [people of African American descent] by town\n\n  **LSTAT**: Percentage of lower status of the population\n\n  **MEDV**: Median value of owner-occupied homes in $1000s\n  \n  \n  \n\n","09c75fd4":"# **Polynomial Regression**\n\nWe can see that **LSTAT** doesn't vary exactly in a linear way. Let's apply the Polynomial Regression with **degree 2** and test. \n\nTo generate the higher order degrees, we use PolyniomialFeatures class from sklearn library. ","a7b78924":"**We can observe that the error has reduced after using polynomial regression as compared to linear regression**","b9807770":"# **Linear Regression**","bfdb78d5":"**Observations**\n\n\n\n\n*   From the above coorelation plot we can see that **MEDV** is strongly correlated to **LSTAT**, **RM**\n\n*  **RAD** and **TAX** are stronly correlated, so we don't include this in our features together to avoid multi-colinearity\n\n\n","9067a0a0":"**Load the Boston Housing DataSet from scikit-learn**","50186305":"**The target values is missing from the data. Create a new column of target values and add it to dataframe**"}}