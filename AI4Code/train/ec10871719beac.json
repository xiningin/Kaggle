{"cell_type":{"9ff8a2fb":"code","573338aa":"code","a521bd87":"code","a9cf0f1a":"code","5c6ea148":"code","863b897f":"code","f3679fcb":"code","f03ac70d":"code","839c76e3":"code","03dbec9d":"code","780ea833":"code","e404f633":"code","4ddd5d76":"code","9c5864ad":"code","141e9e19":"code","9a55c802":"code","c2891891":"code","897395d9":"code","9b2050bb":"code","cbe5c969":"code","039a23ab":"code","8817c388":"code","08b7c66e":"code","2aec58f4":"code","9b8506e6":"code","d76fae0a":"code","8c618b91":"code","e162349b":"code","7e26a482":"code","308795be":"code","b7def77f":"code","990752d2":"code","add4dc52":"code","84e106e4":"code","6b4e551e":"code","5aa19f00":"code","45f5ab98":"code","8e1bb397":"code","9a7ce0de":"code","1bd4dc2b":"code","5d396728":"code","f9bea22d":"code","590f971b":"code","a68940a2":"code","1b6f9a0d":"code","a012c07c":"code","4339f7cf":"code","ffb6f169":"code","48bcf844":"code","2cd5d7b2":"code","b46e97fc":"code","1f869ed0":"code","a0958add":"code","2e69ecf4":"code","60198ea2":"code","37e9b19f":"code","e4e05b5c":"code","e2cd9b77":"code","9371744f":"code","c6c9eed0":"code","81b9a7e4":"code","7ef2427c":"code","139063e1":"code","85129a83":"code","9bcb29c4":"code","c7ff2027":"code","d72dfaae":"code","abcd6768":"code","438c6fd7":"code","20351da9":"code","326a887f":"code","f9e917b8":"code","086b7c68":"code","cf46a1bc":"code","2252be4a":"code","dbae7a47":"code","82d92c1e":"code","cda161e6":"code","7b9e584b":"code","9eeb4cda":"code","cb087b3e":"code","9f5b74ee":"code","02ff1c2e":"code","8893f5b4":"code","7c294a53":"code","6ce84ba6":"code","34654860":"code","67e7a829":"code","db358625":"code","9830e5ce":"code","31db5c79":"code","ec5a413b":"code","af8d413b":"code","a345d7c0":"code","92b99975":"code","d3a76ace":"code","c805ada3":"code","37797be6":"code","2687c669":"code","a4399040":"code","e8d069b0":"code","c32f4a97":"code","b76cadef":"markdown","ca50842e":"markdown","144e783e":"markdown","76cddba6":"markdown","f1f46a4f":"markdown","83919312":"markdown","aa9f577e":"markdown","d4eb7ccf":"markdown","5e181a68":"markdown","703bdd8b":"markdown","d2ac4173":"markdown","99a331af":"markdown","23ccb301":"markdown","a86690e1":"markdown","662fedfd":"markdown","6015522b":"markdown","9bf86044":"markdown","1979c9cc":"markdown","2d003226":"markdown","f2d5770e":"markdown","98f9e425":"markdown","f9091784":"markdown","1e3d202a":"markdown","6a91a8e5":"markdown","ee11cdef":"markdown","61467dc7":"markdown","605ecc6b":"markdown","33732fde":"markdown","952fb95b":"markdown","73fd6e3a":"markdown","e71dfe49":"markdown","26068219":"markdown","655a0277":"markdown","b5e37f32":"markdown","0f1c4d25":"markdown","23a4c100":"markdown","92086dd2":"markdown","8ebbf29c":"markdown","9be2a4ca":"markdown","873eb6b7":"markdown","f43855af":"markdown","163b719f":"markdown","b7e2fffb":"markdown","53b186ca":"markdown","64b0678e":"markdown","a37bc331":"markdown","eceb18a8":"markdown","9c7188ff":"markdown","7aafb50c":"markdown","02cb3df5":"markdown","32a9e1c6":"markdown","cf9dd93d":"markdown","d4bd2d45":"markdown","587c40c7":"markdown","0ac6d608":"markdown","208812e5":"markdown","88831184":"markdown","cf487e2d":"markdown","f0a13015":"markdown","feb102b4":"markdown","c9f1f4d8":"markdown","44e82987":"markdown","67ce2d77":"markdown","00166bd0":"markdown","43fa957b":"markdown","f5349143":"markdown","9d8db21d":"markdown"},"source":{"9ff8a2fb":"import os\n\n# To save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n","573338aa":"import tarfile\nimport urllib\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()","a521bd87":"fetch_housing_data()","a9cf0f1a":"import pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","5c6ea148":"housing = load_housing_data()\nhousing.head()","863b897f":"housing.info()","f3679fcb":"housing[\"ocean_proximity\"].value_counts()","f03ac70d":"housing.describe()","839c76e3":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))\nplt.show()","03dbec9d":"import numpy as np\nnp.random.seed(42)","780ea833":"def split_train_test(data,test_ratio):\n    shuffled_indices=np.random.permutation(len(data))\n    test_set_size=int(len(data)*test_ratio)\n    test_indices=shuffled_indices[:test_set_size]\n    train_indices=shuffled_indices[test_set_size:]\n    return data.iloc[train_indices],data.iloc[test_indices]","e404f633":"train_set,test_set=split_train_test(housing,0.2)\nprint(len(train_set))\nprint(len(test_set))","4ddd5d76":"from zlib import crc32\n\ndef test_set_check(identifier,test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data,test_ratio,id_column):\n    ids=data[id_column]\n    in_test_set=ids.apply(lambda id_: test_set_check(id_,test_ratio))\n    return data.loc[~in_test_set],data.loc[in_test_set]","9c5864ad":"import hashlib\n\ndef test_set_check(identifier,test_ratio,hash=hashlib.md5):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio","141e9e19":"def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio","9a55c802":"housing_with_id = housing.reset_index()   # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","c2891891":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","897395d9":"test_set.head()","9b2050bb":"from sklearn.model_selection import train_test_split\ntrain_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)","cbe5c969":"test_set.head()","039a23ab":"housing[\"median_income\"].hist()","8817c388":"housing[\"income_cat\"]=pd.cut(housing[\"median_income\"],\n                             bins=[0.,1.5,3.0,4.5,6., np.inf],\n                             labels=[1,2,3,4,5])","08b7c66e":"housing[\"income_cat\"].value_counts()","2aec58f4":"housing[\"income_cat\"].hist()","9b8506e6":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index,test_index in split.split(housing,housing[\"income_cat\"]):\n    strat_train_set=housing.loc[train_index]\n    strat_test_set=housing.loc[test_index]","d76fae0a":"strat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set)","8c618b91":"housing[\"income_cat\"].value_counts() \/ len(housing)","e162349b":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)\n\ncompare_props=pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\" : income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"]=100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"]=100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","7e26a482":"compare_props","308795be":"for set_ in (strat_train_set,strat_test_set):\n    set_.drop(\"income_cat\",axis=1,inplace=True)","b7def77f":"housing= strat_train_set.copy()","990752d2":"housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\")","add4dc52":"housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.1)\nsave_fig(\"visualization_plot\")","84e106e4":"housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.4,\n            s=housing[\"population\"]\/100,label=\"population\",figsize=(10,7),\n            c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),colorbar=True,\n            sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")","6b4e551e":"corr_matrix=housing.corr()","5aa19f00":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","45f5ab98":"from pandas.plotting import scatter_matrix\nattributes=[\"median_house_value\",\"median_income\",\"total_rooms\",\n            \"housing_median_age\"]\nscatter_matrix(housing[attributes],figsize=(12,8))\nsave_fig(\"scatter_plot_matrix\")","8e1bb397":"housing.plot(kind=\"scatter\",x=\"median_income\",y=\"median_house_value\",\n            alpha=0.1)\nsave_fig(\"income_vs_house_value_scatterplot\")","9a7ce0de":"housing[\"rooms_per_household\"]=housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"]=housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","1bd4dc2b":"corr_matrix=housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","5d396728":"housing.describe()","f9bea22d":"# drop labels for training set\nhousing= strat_train_set.drop(\"median_house_value\",axis=1)\nhousing_labels=strat_train_set[\"median_house_value\"].copy()","590f971b":"sample_incomplete_rows=housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows","a68940a2":"# Option 1 - Get rid of the corresponding districts\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])","1b6f9a0d":"# Option 2 - Get rid of the whole attribute\nsample_incomplete_rows.drop(\"total_bedrooms\",axis=1)","a012c07c":"# Option 3 - Set the values to some value (zero,mean,median,etc)\nmedian=housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median,inplace=True)","4339f7cf":"sample_incomplete_rows","ffb6f169":"from sklearn.impute import SimpleImputer\nimputer=SimpleImputer(strategy=\"median\")","48bcf844":"housing_num=housing.drop(\"ocean_proximity\",axis=1)","2cd5d7b2":"imputer.fit(housing_num)","b46e97fc":"imputer.statistics_","1f869ed0":"housing_num.median().values","a0958add":"X=imputer.transform(housing_num)","2e69ecf4":"X","60198ea2":"housing_tr=pd.DataFrame(X,columns=housing_num.columns,\n                       index=housing_num.index)","37e9b19f":"housing_tr.loc[sample_incomplete_rows.index.values]","e4e05b5c":"imputer.strategy","e2cd9b77":"housing_cat=housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","9371744f":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder=OrdinalEncoder()\nhousing_cat_encoded=ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","c6c9eed0":"ordinal_encoder.categories_","81b9a7e4":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder=OneHotEncoder()\nhousing_cat_1hot=cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","7ef2427c":"housing_cat_1hot.toarray()","139063e1":"cat_encoder=OneHotEncoder(sparse=False)\nhousing_cat_1hot=cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","85129a83":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\ncol_names=\"total_rooms\",\"total_bedrooms\",\"population\",\"households\"\nrooms_ix,bedrooms_ix,population_ix,households_ix= [\n    housing.columns.get_loc(c) for c in col_names]\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True):\n        self.add_bedrooms_per_room=add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        rooms_per_household=X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household=X[:,population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room=X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                        bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder= CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs=attr_adder.transform(housing.values)","9bcb29c4":"housing_extra_attribs","c7ff2027":"housing_extra_attribs=pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\",\"population_per_household\"],\n    index=housing.index)","d72dfaae":"housing_extra_attribs.head()","abcd6768":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline=Pipeline([('imputer',SimpleImputer(strategy=\"median\")),\n                      ('attribs_adder',CombinedAttributesAdder()),\n                      ('std_scaler',StandardScaler()),])\nhousing_num_tr=num_pipeline.fit_transform(housing_num)","438c6fd7":"housing_num_tr","20351da9":"from sklearn.compose import ColumnTransformer\nnum_attribs=list(housing_num)\ncat_attribs=list(housing_cat)\n\nfull_pipeline=ColumnTransformer([\n        (\"num\",num_pipeline,num_attribs),\n        (\"cat\",OneHotEncoder(),cat_attribs),\n    ])\nhousing_prepared=full_pipeline.fit_transform(housing)","326a887f":"housing_prepared","f9e917b8":"housing_prepared.shape","086b7c68":"from sklearn.linear_model import LinearRegression\n\nlin_reg=LinearRegression()\nlin_reg.fit(housing_prepared,housing_labels)","cf46a1bc":"some_data=housing.iloc[:5]\nsome_labels=housing_labels.iloc[:5]\nsome_data_prepared=full_pipeline.transform(some_data)\n\nprint(\"Predictions: \",lin_reg.predict(some_data_prepared))","2252be4a":"print(\"Labels: \",list(some_labels))","dbae7a47":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions=lin_reg.predict(housing_prepared)\nlin_mse=mean_squared_error(housing_labels,housing_predictions)\nlin_rmse=np.sqrt(lin_mse)\nlin_rmse","82d92c1e":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg=DecisionTreeRegressor()\ntree_reg.fit(housing_prepared,housing_labels)","cda161e6":"housing_predictions=tree_reg.predict(housing_prepared)\ntree_mse=mean_squared_error(housing_labels,housing_predictions)\ntree_rmse=np.sqrt(tree_mse)\ntree_rmse","7b9e584b":"from sklearn.model_selection import cross_val_score\n\nscores=cross_val_score(tree_reg,housing_prepared,housing_labels,\n                       scoring=\"neg_mean_squared_error\",cv=10)\ntree_rmse_scores=np.sqrt(-scores)","9eeb4cda":"def display_scores(scores):\n    print(\"Scores: \",scores)\n    print(\"Mean: \",scores.mean())\n    print(\"Standard deviation: \",scores.std())\n    \ndisplay_scores(tree_rmse_scores)","cb087b3e":"lin_scores=cross_val_score(lin_reg,housing_prepared,housing_labels,\n                          scoring=\"neg_mean_squared_error\",cv=10)\nlin_rmse_scores=np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","9f5b74ee":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg=RandomForestRegressor(n_estimators=100,random_state=42)\nforest_reg.fit(housing_prepared,housing_labels)","02ff1c2e":"housing_predictions=forest_reg.predict(housing_prepared)\nforest_mse=mean_squared_error(housing_labels,housing_predictions)\nforest_rmse=np.sqrt(forest_mse)\nforest_rmse","8893f5b4":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","7c294a53":"from sklearn.svm import SVR\n\nsvm_reg=SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared,housing_labels)\nhousing_predictions=svm_reg.predict(housing_prepared)\nsvm_mse=mean_squared_error(housing_labels,housing_predictions)\nsvm_rmse=np.sqrt(svm_mse)\nsvm_rmse","6ce84ba6":"svm_scores=cross_val_score(svm_reg,housing_prepared,housing_labels,\n                          scoring=\"neg_mean_squared_error\",cv=10)\nsvm_rmse_scores=np.sqrt(-svm_scores)\ndisplay_scores(svm_rmse_scores)","34654860":"from sklearn.model_selection import GridSearchCV\n\nparam_grid=[\n    {'n_estimators':[3,10,30],'max_features':[2,4,6,8]},\n    {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]},\n  ]\n\nforest_reg=RandomForestRegressor(random_state=42)\ngrid_search=GridSearchCV(forest_reg,param_grid,cv=5,\n                        scoring='neg_mean_squared_error',\n                        return_train_score=True)\ngrid_search.fit(housing_prepared,housing_labels)","67e7a829":"grid_search.best_params_","db358625":"grid_search.best_estimator_","9830e5ce":"cvres=grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(np.sqrt(-mean_score),params)","31db5c79":"pd.DataFrame(grid_search.cv_results_)","ec5a413b":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs={\n        'n_estimators':randint(low=1,high=200),\n        'max_features':randint(low=1,high=8),\n    }\n\nforest_reg=RandomForestRegressor(random_state=42)\nrnd_search=RandomizedSearchCV(forest_reg,param_distributions=param_distribs,\n                             n_iter=10,cv=5,scoring='neg_mean_squared_error',\n                             random_state=42)\nrnd_search.fit(housing_prepared,housing_labels)","af8d413b":"rnd_search.best_params_","a345d7c0":"rnd_search.best_estimator_","92b99975":"cvres=rnd_search.cv_results_\nfor mean_score,params in zip(cvres['mean_test_score'],cvres['params']):\n    print(np.sqrt(-mean_score),params)","d3a76ace":"feature_importances=grid_search.best_estimator_.feature_importances_\nfeature_importances","c805ada3":"extra_attribs=[\"rooms_per_hhold\",\"pop_per_hhold\",\"bedrooms_per_room\"]\ncat_encoder=full_pipeline.named_transformers_['cat']\ncat_one_hot_attribs=list(cat_encoder.categories_[0])\nattributes=num_attribs+extra_attribs+cat_one_hot_attribs\nsorted(zip(feature_importances,attributes),reverse=True)","37797be6":"final_model=grid_search.best_estimator_\n\nX_test=strat_test_set.drop('median_house_value',axis=1)\ny_test=strat_test_set['median_house_value'].copy()\n\nX_test_prepared=full_pipeline.transform(X_test)\nfinal_predictions=final_model.predict(X_test_prepared)\n\nfinal_mse=mean_squared_error(y_test,final_predictions)\nfinal_rmse=np.sqrt(final_mse)","2687c669":"final_rmse","a4399040":"from scipy import stats\n\nconfidence=0.95\nsquared_errors=(final_predictions-y_test)**2\nnp.sqrt(stats.t.interval(confidence,len(squared_errors)-1,\n                        loc=squared_errors.mean(),\n                        scale=stats.sem(squared_errors)))","e8d069b0":"m=len(squared_errors)\nmean=squared_errors.mean()\ntscore=stats.t.ppf((1+confidence)\/2,df=m-1)\ntmargin=tscore*squared_errors.std(ddof=1)\/np.sqrt(m)\nnp.sqrt(mean-tmargin),np.sqrt(mean+tmargin)","c32f4a97":"zscore=stats.norm.ppf((1+confidence)\/2)\nzmargin=zscore*squared_errors.std(ddof=1)\/np.sqrt(m)\nnp.sqrt(mean-zmargin),np.sqrt(mean+zmargin)","b76cadef":"The **info()** method is useful to get a quick description of the data, in particular the total number of rows, each attribute's type, and the number of non-null values.","ca50842e":"**pd.cut()** - (Pandas) Divides bin value into discrete intervals. This function is also useful for going from continuous variable to a categorical variable.\n\n**np.inf** - (NumPy) IEEE 754 floating point representation of positive infinity","144e783e":"Function to load the data:","76cddba6":"We can get the best estimator like this:","f1f46a4f":"Let's measure this regression model's root mean squared error(**RMSE**) on the whole training set using Scikit learn's **mean_squared_error()** function","83919312":"The **value_counts()** (Pandas) method returns the count of various categories of a particular feature. The resulting object will be in descending order so that the first element is the most frequently occurring element.. I texcludes NA values by default ","aa9f577e":"The total_bedrooms attribute has some missing values.","d4eb7ccf":"The param_grid tells Scikit Learn to first evaluate all 3 x 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict, then try all 2 x 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True (which is the default value for this hyperparameter)\n\nThe Grid Search will explore 12 + 6 = 18 combinations of RandomForestRegressor hyperparameter values, and it will train each model 5 times (since we are using five fold cross validation). In other words, all in all, there will be 18 x 5 = 90 rounds of training.\n\nWe can check the best combination of parameters like this:","5e181a68":"Now we should remove the income_cat attribute so that the data is back to its original state:","703bdd8b":"We obtain the best solution by setting the max_features hyperparameter to 8 and the n_estimators hyperparameter to 30. The RMSE score for this combination is 49,682 which is slightly better than the score we got earlier using the default hyperparameter values (which was 50,182).","d2ac4173":"One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (eg: for ordered categories such as \"bad\",\"avearge\",\"good\" and \"excellent\"). Obviously this is not the case for the ocean_proximity column (for example, categories 0 and 4 are more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute\nequal to 1 when the category is \u201c<1H OCEAN\u201d (and 0 otherwise), another attribute equal to 1 when the category is \u201cINLAND\u201d (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). Scikit-Learn provides a OneHotEncoder encoder to convert integer categorical values into one-hot vectors.","99a331af":"housing_extra_attribs is a NumPy array. We have lost all the column names (unfortunately, that's a problem with Scikit Learn). To recover a DataFrame, we can run this:","23ccb301":"This plot reveals a few things. First, the correlation is indeed very strong. We can clearly see the upward trend, and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500,00. But this plot reveals other less obvious straight lines: a horizontal line around 450,000, another around 350,000, perhaps another at 280,000, and a few more below that. We may want to try removing the corresponding districts to prevent the algorithm from learning to reproduce these data quirks. ","a86690e1":"Remove the text attribute because median can only be calculated on numerical attributes.","662fedfd":"the **train_test_split()** (Scikit) has some additional features. First, there is a random_state parameter that allows us to set the random generator seed. Second, we can pass it multiple datasets with an identical number of rows, and it will spilt them on the same indices (this is very useful if we have separate Dataframe for labels.","6015522b":"One way to evaluate the Decision Tree model would be to use Scikit Learn's K-fold cross validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the decision tree model 10 times, picking a fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores","9bf86044":"**hist()** method plots a histogram for each numerical attribute for the entire dataset.\n\nThe hist() method relies on Matplotlib, which in turn relies on a user-specified graphical backend to draw on our screen. So before we can plot anything, we need to specify which backend Matplotlib should use. The simplest option is to use %matplotlib inline . This tells Jupyter to set up Matplotlib so it uses Jupyter's own backend. Note that calling show() is optional in a jupyter notebook, as Jupyter will automatically display plots when a cell is executed.","1979c9cc":"**np.random.permutation(x)**- (Numpy) Randomly permutes a sequence, or return a permuted range.","2d003226":"### Correlations","f2d5770e":"The most promising attribute to predict the median house value is the median income","98f9e425":"Compare against the actual values","f9091784":"Let's train a more powerful model, capable of finding complex non linear relationships, the **DecisionTreeRegressor**.","1e3d202a":"Scikit-learn's cross validation features expect a utility function (greater is better) rather than a cost function (lower is better), so scoring function is actually the opposite of the MSE (i.e, a negative value), which is why the preceding code computes -scores before calculating the square root. ","6a91a8e5":"# Data preprocessing","ee11cdef":"Notice that the total_bedrooms attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature.","61467dc7":"**StratifiedShuffleSplit** (scikit) cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. **n_splits** is the number of re shuffling and splitting iterations.","605ecc6b":"Notice that the output is SciPy sparse matrix, instead of a NumPy array. This is very useful when we have categorical attributes with thousands of categories. After one hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. We can use it mostly like a normal 2D array, but if we really want to convert it to a (dense) Numpy array, just call the **toarray()** method","33732fde":"We see that the Decision tree model is overfitting so badly that it performs worse than the linear regression model.\n\nLet's try our hands at **Random Forest Regressor**.","952fb95b":"If we set the **np.random.seed(fixed_number)** , everytime we call the numpy's other random function, the result will be same.","73fd6e3a":"The housing dataset does not have an identifier column. The simplest solution is to use the row index as ID :\n","e71dfe49":"Let's try the full preprocessing pipeline on a few training instances","26068219":"# Fine tune the model","655a0277":"Now, the decision tree doesn't look as good as it did earlier. In fact, it seems to perform worse than the linear regression model.\n\nCross-validation allows you to get not only an estimate of the performance of the model, but also a measure of how precise this estimate is (i.e, its standard deviation). The decision tree has a score of approximately 70,666 ,generally + or - 2928. We would have not have this information if we just used one validation set. But cross-validation comes at a cost of training the model several times.","b5e37f32":"Let's revert to a clean training set (by copying strat_train_set once again). Let's also separate the predictors and the labels, since we don't want to apply the same transformations to the predictors and the target values.","0f1c4d25":"Some estimators (such as an imputer) can also transform a dataset; these are called **transformers**. The transformation is performed by the **transform()** method with the dataset to transform as a parameter. It returns the transformed dataset.\n\nAll transformers also have a convenience method called **fit_transform()** that is equivalent to calling fit() and then transform()","23a4c100":"The result is a plain NumPy array containing the transformed features. If we want to put it back into a pandas DataFrame:","92086dd2":"**Stratified Sampling** - The population is divided into homogeneous subgroups called strata, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population.","8ebbf29c":"As we can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed.","9be2a4ca":"The correlation coefficient ranges from -1 to 1. When it is close to 1, it means strong positive correlation and when it is close to -1, it means strong negative correlation. Note that the correlation coefficient only measures linear correlations (if x goes up, then y generally goes up or down). It may completely miss out on non-linear relationships (if x is close to 0, then y generally goes up)  \n\nAnother way to check for correlation between attributes is to use the pandas **scatter_matrix()** function, which plots every numerical attribute against other numerical attribute","873eb6b7":"All the estimator's hyperparameters are accessible directly via public instance variables (eg: **imputer.strategy**), and all the estimator's learned parameters are acceeible via public instance variables with an underscore suffix (eg: **imputer.statistics_**)","f43855af":"Most district's median_house_values range from 120,000 to 265,000, so a typical prediction error of 68,628 is not very satisfying. This is an example of model underfitting the training data. When this happens it means that the features do not provide enough information to make good predictions, or that the model is not powerful enough. The main ways to fix underfitting are to select a more powerful model, to feed the algorithm with better features,or to reduce the constraints on the model. This model is not regularized, so it rules out this option","163b719f":"So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformation to each column.","b7e2fffb":"Alternatively, we can set sparse=False when creating the OneHotEncoder","53b186ca":"**reset_index()** - (Pandas) It reset the index of the dataframe, and use the default one instead.  \n\nIf we use row index as unique identifier, we need to make sure that the new data gets apppended to the end of the dataset and that no row ever gets deleted. If this is not possible, then we can try the most stable features to build a unique ID:","64b0678e":"We can compute a 95% confidence interval for the test RMSE:","a37bc331":"The above solution will break the next time you fetch an updated dataset.To have a stable train\/test split even after updating the dataset, a common solution is to use instance's identifier to decide whether or not it should go in the test set (assuming that instances have a unique and immutable identifier).\n\nWe can compute a hash of each instance's identifier and put that instance in the test set is the hash is lower than or equal to 20% of the maximum hash value. This ensures the test set will remain consistent across multiple runs, even if we refresh the dataset. The new test will contain 20% of the new instances, but will not contain any instance that was previously in the training set.\n\nHere's the implementation:","eceb18a8":"# Get the data\n\nFunction to fetch the data:","9c7188ff":"Let's try our hands at **Support Vector Machines**.","7aafb50c":"**kind** : str object type \n\n- \u2018line\u2019 : line plot (default)\n- \u2018bar\u2019 : vertical bar plot\n- \u2018barh\u2019 : horizontal bar plot\n- \u2018hist\u2019 : histogram\n- \u2018box\u2019 : boxplot\n- \u2018kde\u2019 : Kernel Density Estimation plot\n- \u2018density\u2019 : same as \u2018kde\u2019\n- \u2018area\u2019 : area plot\n- \u2018pie\u2019 : pie plot\n- \u2018scatter\u2019 : scatter plot\n- \u2018hexbin\u2019 : hexbin plot\n\n**figsize** : (Pandas) a tuple which is used to change the size of the plot.\n\n**get_cmap** : (Matplotlib) has number of in built colormaps.\n\n**sharex=False** fixes a display bug (the x-axis values and legend were not displayed).","02cb3df5":"Let's create a copy so that we can play without harming the training set:","32a9e1c6":"Now we can use this trained imputer to transform the training set by replacing missing values with the learned medians.","cf9dd93d":"# Select and train a model","d4bd2d45":"It is much more likely that the model has badly overfit the data.","587c40c7":"We see that it is hard to analyse any particular pattern. Setting the **alpha** option to 0.1 makes it much easier to visualize the places where there is high density of data points.","0ac6d608":"Now let's build a pipeline for preprocessing the numerical attributes:","208812e5":"Alternatively, we could use z-scores rather than t-scores:","88831184":"We could compute the interval manually like this:\n","cf487e2d":"Let's look at the score of each hyperparameter combination tested during grid search","f0a13015":"Now let's look at the housing prices. The radius of each circle represents the district's population and the color represents the price. We will use a predefined color map called **jet**, which ranges from blue (low prices) to red (high prices)","feb102b4":"Check that this result is same as manually computing the median of each attribute:","c9f1f4d8":"The results of Random Forests look very promising. However, note that the score on training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set.","44e82987":"The **describe()** method shows the summary of numerical attributes.","67ce2d77":"More about transformers:\nhttps:\/\/towardsdatascience.com\/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65","00166bd0":"# Data Visualization","43fa957b":"Any object that can estimate some parameters based on a dataset is called an **estimator** (eg: an imputer is an estimator). The estimation itself is performed by the **fit()** method and it takes only the dataset as the parameter. Any other parameter needed to guide the estimation process is considered a hyperparameter (such as imputer's strategy)","f5349143":"Let's create a custom transformer to add extra attributes.","9d8db21d":"Now let's preprocess the categorical input feature, ocean_proximity:"}}