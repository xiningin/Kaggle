{"cell_type":{"a0a50105":"code","f00cc33b":"code","5764923d":"code","493f75ea":"code","040f7e96":"code","237f5a7c":"code","9149ea75":"code","e9cd6601":"code","ca3fd948":"code","066d1776":"code","f8872fea":"code","5712b979":"code","64afdcea":"code","1cf11814":"code","c1ae2da4":"code","7f9ae0a5":"code","50cfd8d0":"code","395eea89":"code","b9c343d2":"code","0068109e":"code","5fecaa74":"code","024df00f":"markdown","0796ba4d":"markdown","9166738a":"markdown","971cfacd":"markdown","39df8bfe":"markdown"},"source":{"a0a50105":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f00cc33b":"from fastai import *\nfrom fastai.tabular import *","5764923d":"train = pd.read_csv('\/kaggle\/input\/electrical-consumption\/train_6BJx641.csv')\ntest =  pd.read_csv('\/kaggle\/input\/electrical-consumption\/test_pavJagI.csv')\nprint(train.shape)\nprint(test.shape)","493f75ea":"comb = pd.concat([train,test])\nprint(comb.shape)","040f7e96":"comb['datetime'] = pd.to_datetime(comb['datetime'])","237f5a7c":"from pykalman import KalmanFilter\ndef Kalman1D(observations,damping=1):\n    # To return the smoothed time series data\n    observation_covariance = damping\n    initial_value_guess = observations[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    initial_value_guess\n    kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(observations)\n    return pred_state\n\n# Kalman Filter\nobservation_covariance = .0015\ncomb['temperature'] = Kalman1D(comb.temperature.values,observation_covariance)\ncomb['var1'] = Kalman1D(comb.var1.values,observation_covariance)\ncomb['pressure'] = Kalman1D(comb.pressure.values,observation_covariance)\ncomb['windspeed'] = Kalman1D(comb.windspeed.values,observation_covariance)\n#test['signal'] = Kalman1D(test.signal.values,observation_covariance)","9149ea75":"# samp = pd.DataFrame()\n# import random\n# samp['A'] = [random.randint(1,100) for i in range(0,25)]\n# samp['C'] = ['a' for i in range(0,5)] + ['b' for i in range(0,5)] + ['c' for i in range(0,15)]\n# samp['emw'] = samp.groupby(['C'])['A'].apply(lambda x : x.ewm(span=24,min_periods=1).mean())\n# samp","e9cd6601":"def booleancon(x):\n    if x == True:\n        return 1\n    else:\n        return 0\n    \n\n\ndef daypart(x):\n    if x >= 0 and x<=4:\n        return 101\n    elif x>=5 and x<=8:\n        return 102\n    elif x>=9 and x<=12:\n        return 103\n    elif x>=13 and x<=16:\n        return 104\n    elif x>=17 and x<=19:\n        return 105\n    elif x>=20 and x<=23:\n        return 106\n    else:\n        return 0\n    \ndef seasons(x):\n    if x>=3 and x<5:\n        return 0\n    elif x>=6 and x<=8:\n        return 1\n    elif x>=9 and x<=11:\n        return 2\n    elif x>=12 and x<=2:\n        return 3\n    else:\n        return 0\n    \ndef var2(x):\n    if x=='A':\n        return 1\n    elif x=='B':\n        return 2\n    else:\n        return 3\n\ndef time_pr(train):\n    train = add_datepart(train,'datetime',drop=False,time=True)\n    #train.drop(['datetimeIs_month_end', 'datetimeIs_quarter_end','datetimeIs_year_start','datetimeIs_year_end'], axis=1, inplace=True)\n    train['datetimeIs_month_end'] = train['datetimeIs_month_end'].apply(booleancon)\n    train['datetimeIs_month_start']   = train['datetimeIs_month_start'].apply(booleancon)\n    train['datetimeIs_quarter_start'] = train['datetimeIs_quarter_start'].apply(booleancon)\n    train['datetimeIs_quarter_end'] = train['datetimeIs_quarter_end'].apply(booleancon)\n    train['datetimeIs_year_start'] = train['datetimeIs_quarter_start'].apply(booleancon)\n    train['datetimeIs_year_end'] = train['datetimeIs_quarter_end'].apply(booleancon)\n    train['var2'] = train['var2'].apply(var2)\n    train['daypart'] = train['datetimeHour'].apply(daypart)\n    train['season'] = train['datetimeMonth'].apply(seasons)\n    train['year_month'] = train['datetimeYear'].astype(str)+'_'+train['datetimeMonth'].astype(str)\n    train['MonthCat'] = 'M'+train['datetimeMonth'].astype(str)\n    train['HourCat'] = 'H'+train['datetimeHour'].astype(str)\n    for c in ['temperature','var1','pressure','datetimeElapsed']:\n        d = {}\n        d['mean'+c] = train.groupby(['year_month'])[c].mean()\n        d['median'+c] = train.groupby(['year_month'])[c].median()\n        d['max'+c] = train.groupby(['year_month'])[c].max()\n        d['min'+c] = train.groupby(['year_month'])[c].min()\n        d['std'+c] = train.groupby(['year_month'])[c].std()\n        d['mean_abs_chg'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            train[v] = train['year_month'].map(d[v].to_dict())\n        train['range'+c] = train['max'+c] - train['min'+c]\n        train['maxtomin'+c] = train['max'+c] \/ train['min'+c]\n        train['abs_avg'+c] = (train['abs_min'+c] + train['abs_max'+c]) \/ 2\n        \n    \n    for c in ['temperature','var1','pressure','datetimeElapsed']:\n        train['signal_shift_+1'+c] = train.groupby(['year_month'])[c].shift(1)\n        train['signal_shift_-1'+c] = train.groupby(['year_month'])[c].shift(-1)\n        train['signal_shift_+2'+c] = train.groupby(['year_month'])[c].shift(2)\n        train['signal_shift_-2'+c] = train.groupby(['year_month'])[c].shift(-2)\n        train['signal_shift_+3'+c] = train.groupby(['year_month'])[c].shift(3)\n        train['signal_shift_-3'+c] = train.groupby(['year_month'])[c].shift(-3)\n        train['signal_shift_+4'+c] = train.groupby(['year_month'])[c].shift(4)\n        train['signal_shift_-4'+c] = train.groupby(['year_month'])[c].shift(-4)\n        train['signal_shift_+5'+c] = train.groupby(['year_month'])[c].shift(5)\n        train['signal_shift_-5'+c] = train.groupby(['year_month'])[c].shift(-5)\n    return train","ca3fd948":"pd.set_option('display.max_columns', 1000)  # or 1000\npd.set_option('display.max_rows', 1000)  # or 1000\npd.set_option('display.max_colwidth', 199)  # or 199\ncomb = time_pr(comb)","066d1776":"comb.head()","f8872fea":"dummy_train = comb[comb['datetimeDay']<=16].fillna(method='bfill').fillna(method='ffill')\ndummy_test = comb[(comb['datetimeDay']>16) & (comb['datetimeDay']<=23)].fillna(method='bfill').fillna(method='ffill')\n\ncol = []\nfor i in comb.columns:\n    if i!= 'electricity_consumption' and i!='ID' and i!='datetime' and i!='year_month' and i!='MonthCat' and i!='HourCat':\n        col.append(i)\n\nactual_data = comb[comb['datetimeDay']<=23].fillna(method='bfill').fillna('ffill')","5712b979":"from sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef variance_inflation_factors(X,col):\n    vifs = {}\n    \n    for i in col:\n        cols = [z for z in col]\n        cols.remove(i)\n        sub_X = X[cols].values\n        sub_y = X[[i]].values\n        clf = LinearRegression()\n        sub_clf = clf.fit(sub_X, sub_y)\n        sub_y_pred = clf.predict(sub_X)\n        \n        sub_r2 = r2_score(sub_y, sub_y_pred)\n        \n        vif = 1 \/ (1 - sub_r2)\n        vifs[i] =vif\n        \n    return vifs\n\ncol_dict = variance_inflation_factors(actual_data,col)\n\nfor k,v in col_dict.items():\n    if v <=100:\n        col.append(k)\n    else:\n        print(k,' removed becouse vif values is ',v)","64afdcea":"x_train = dummy_train[col].values\ny_train = dummy_train['electricity_consumption'].values\nx_test = dummy_test[col].values\ny_test = dummy_test['electricity_consumption'].values","1cf11814":"import lightgbm as lgb\nd_train = lgb.Dataset(x_train, label=y_train)\nd_test = lgb.Dataset(x_test, label=y_test)\nparams = {}\nparams['application']='root_mean_squared_error'\nparams['num_boost_round'] = 3000\nparams['learning_rate'] = 0.017\nparams['boosting_type'] = 'gbdt'\nparams['metric'] = 'rmse'\nparams['sub_feature'] = 0.833\nparams['num_leaves'] = 15\nparams['min_split_gain'] = 0.05\nparams['min_child_weight'] = 27\nparams['max_depth'] = 8\nparams['num_threads'] = 15\nparams['max_bin'] = 400\nparams['lambda_l2'] = 0.10\nparams['lambda_l1'] = 0.30\nparams['feature_fraction']= 0.833\nparams['bagging_fraction']= 0.979\nparams['seed']=1729\nparams['extra_trees'] = True\nparams['top_k'] = 23\nparams['path_smooth'] = 0.10\nclf = lgb.train(params, d_train, 2000,d_test,verbose_eval=200, early_stopping_rounds=200)","c1ae2da4":"from xgboost import XGBRegressor\np=XGBRegressor(n_estimators=30000,random_state=1729,learning_rate=0.017,max_depth=4,n_jobs=4)\n# max_depth=5,0.018\np.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='rmse',early_stopping_rounds=500,verbose=200)\n","7f9ae0a5":"from catboost import CatBoostRegressor\ncb_model = CatBoostRegressor(n_estimators = 1000,\n    loss_function = 'RMSE',\n    eval_metric = 'RMSE',random_state=1729)\ncb_model.fit(x_train, y_train, use_best_model=True, eval_set=(x_test, y_test), early_stopping_rounds=50)","50cfd8d0":"X = actual_data[col].values\nY = actual_data['electricity_consumption'].values","395eea89":"\nd_train = lgb.Dataset(X, label=Y)\nparams = {}\nparams['application']='root_mean_squared_error'\nparams['num_boost_round'] = 3000\nparams['learning_rate'] = 0.017\nparams['boosting_type'] = 'gbdt'\nparams['metric'] = 'rmse'\nparams['sub_feature'] = 0.833\nparams['num_leaves'] = 15\nparams['min_split_gain'] = 0.05\nparams['min_child_weight'] = 27\nparams['max_depth'] = 8\nparams['num_threads'] = 15\nparams['max_bin'] = 400\nparams['lambda_l2'] = 0.10\nparams['lambda_l1'] = 0.30\nparams['feature_fraction']= 0.833\nparams['bagging_fraction']= 0.979\nparams['seed']=1729\nparams['extra_trees'] = True\nparams['top_k'] = 23\nparams['path_smooth'] = 0.10\n\nclf = lgb.train(params, d_train, 2000)","b9c343d2":"# p=XGBRegressor(n_estimators=1216,random_state=1729,learning_rate=0.017,max_depth=4,n_jobs=4)\n# # max_depth=5,0.018\n# p.fit(X,Y)\n","0068109e":"test = comb[comb['datetimeDay']>23]\nx_test = test[col].values\npred = clf.predict(x_test)\ntest['electricity_consumption'] =[round(i) for i in pred]\ntest[['ID','electricity_consumption']].to_csv('result.csv',header=True,index = None)","5fecaa74":"lgb.plot_importance(clf,importance_type='split', max_num_features=25)","024df00f":"LGBM","0796ba4d":"XGBOOST","9166738a":"CatBoost","971cfacd":"PRE-PROCESSING","39df8bfe":"Feature Selection"}}