{"cell_type":{"ad37d080":"code","a1fca483":"code","07a22bb7":"code","5c683874":"code","ed7a9323":"code","3bea6ed5":"code","68ab1679":"code","a5d43bb6":"code","9903fd67":"code","7dd9d88b":"code","4022a819":"code","f74202f7":"code","85c55239":"code","576fc812":"code","58d9218a":"code","07f6c835":"code","f04d9bde":"code","b44dc54f":"code","51a4aae8":"code","f99ef7eb":"code","520e541b":"code","71b8a75a":"code","a24b1682":"code","60a41fa7":"code","286f270e":"code","07beb9ad":"markdown","d8a6a091":"markdown","2e32180b":"markdown","2dcdc11d":"markdown"},"source":{"ad37d080":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1fca483":"train = pd.read_csv('\/kaggle\/input\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/test.csv')","07a22bb7":"train.head()","5c683874":"tweets = train['tweet'].values\nlabels = train['label'].values\ntest_input = test['tweet'].values\ntest_label = list()","ed7a9323":"from nltk.corpus import stopwords\nStopWords = stopwords.words('english')","3bea6ed5":"appos = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n\"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"he'd\" : \"he would\",\n\"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\",\n\"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n\"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n\"we've\" : \"we have\", \"what'll\" : \"what will\",\"what're\" : \"what are\",\"what's\" : \"what is\", \"what've\" : \"what have\",\n\"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\",\n\"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\",\n\"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"}","68ab1679":"from string import punctuation\ndef tweet_format(tweets):\n    all_tweets = list()\n    for text in tweets:\n        lower_case = text.lower()\n        words = lower_case.split()\n        formatted = [appos[word] if word in appos else word for word in words]\n        formatted_test = list()\n        for word in formatted:\n            if word not in StopWords:\n                formatted_test.append(word)\n        formatted = \" \".join(formatted_test)\n        punct_text = \"\".join([ch for ch in formatted if ch not in punctuation])\n        all_tweets.append(punct_text)\n    all_text = \" \".join(all_tweets)\n    all_words = all_text.split()\n    for i in range(len(all_tweets)):\n        if all_tweets[i].startswith(\"user\"):\n            all_tweets[i] = all_tweets[i].replace(\"user\", '')\n    return all_tweets, all_words","a5d43bb6":"from collections import Counter \n# Counts the occurence of each word\nall_tweets, all_words = tweet_format(tweets)\ncount_words = Counter(all_words)\ntotal_words = len(all_words)\nsorted_words = count_words.most_common(total_words)\nvocab_to_int = {w:i+1 for i,(w,c) in enumerate(sorted_words)}","9903fd67":"all_tweets[0]","7dd9d88b":"def encode_tweets(tweets):\n    '''\n    encodes review into an array\n    '''\n    All_tweets = list()\n    for text in tweets:\n        text = text.lower()\n        text = \"\".join([ch for ch in text if ch not in punctuation])\n        All_tweets.append(text)\n    encoded_tweets = list()\n    for tweet in All_tweets:\n        encoded_tweet = list()\n        for word in tweet.split():\n            if word not in vocab_to_int.keys():\n                encoded_tweet.append(0)\n            else:\n                encoded_tweet.append(vocab_to_int[word])\n        encoded_tweets.append(encoded_tweet)\n    return encoded_tweets","4022a819":"def pad_sequences(encoded_tweets, sequence_length = 30):\n    ''' \n    Return features of tweet_ints, where each review is padded with 0's or truncated to the input seq_length.\n    ensures all tweets have the same length\n    '''\n    features = np.zeros((len(encoded_tweets), sequence_length), dtype=int)\n    \n    for i, tweet in enumerate(encoded_tweets):\n        tweet_len = len(tweet)\n        if (tweet_len <= sequence_length):\n            zeros = list(np.zeros(sequence_length-tweet_len))\n            new = zeros + tweet\n        else:\n            new = tweet[:sequence_length]\n        features[i,:] = np.array(new)\n    return features","f74202f7":"def preprocess(tweets):\n    \"\"\"\n    This Function will tranform reviews in to model readable form\n    \"\"\"\n    formated_tweets, all_words = tweet_format(tweets)\n    encoded_tweets = encode_tweets(formated_tweets)\n    features = pad_sequences(encoded_tweets, 30)\n    return features","85c55239":"import matplotlib.pyplot as plt\n%matplotlib inline","576fc812":"encoded_tweets = encode_tweets(tweets)\ntweet_len = [len(encoded_tweet) for encoded_tweet in encoded_tweets]\npd.Series(tweet_len).hist()\nplt.show()\npd.Series(tweet_len).describe()","58d9218a":"#splitting data into 90%train and 10%validation\nfeatures = preprocess(tweets)\nx_train = features[:int(0.90 * len(features))]\ny_train = labels[:int(0.90 * len(features))]\nx_valid = features[int(0.90 * len(features)):]\ny_valid = labels[int(0.90 * len(features)):]\nprint(len(y_train), len(y_valid))","07f6c835":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#creating Tensor Dataset\n#torch.Tensor has default dtype float32 but from_numpy() inherits a default dtype of int32\ntrain_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_valid), torch.from_numpy(y_valid))\n\n#data loader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, drop_last = True)\nvalid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True, drop_last = True)","f04d9bde":"#obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","b44dc54f":"#creating the LSTM class\nimport torch.nn as nn \n\nclass CustomLSTM(nn.Module):\n    '''\n    this will be the LSTM model which will be used to perform the sentiment analysis\n    '''\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):\n        '''\n        initialize the model by setting up the layers\n        '''\n        super().__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        #embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = drop_prob, batch_first = True)\n        \n        #dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        #Linear and sigmoid layer\n        self.fc1 = nn.Linear(hidden_dim, 64)\n        self.fc2 = nn.Linear(64, 16)\n        self.fc3 = nn.Linear(16, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        '''\n        performing forward pass of our model on some input and hidden state\n        '''\n        batch_size = x.size()\n        \n        #Embedding and LSTM\n        embedd = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedd, hidden)\n        \n        #stacking up lstm output\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        #dropout and fully connected layers\n        out = self.dropout(lstm_out)\n        out = self.fc1(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        out = self.dropout(out)\n        out = self.fc3(out)\n        sig_out = self.sigmoid(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        \"\"\"\n        Initialize Hidden STATE\n        \"\"\"\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","51a4aae8":"vocab_size = len(vocab_to_int) + 1\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nnet = CustomLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)","f99ef7eb":"#loss and optimization functions\nlr = 0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr = lr)\n\n#checking if GPU is available\ntrain_on_gpu = torch.cuda.is_available()\n\nepochs = 5\ncounter = 0\nprint_every = 100\nclip = 5 #gradient clipping\n\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\nfor e in range(epochs):\n    #initialize the hidden state (h0, c0)\n    train_loss = []\n    h = net.init_hidden(batch_size)\n    \n    #loop through each batch\n    for inputs, labels in train_loader:\n        counter += 1\n        \n        if(train_on_gpu):\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n            \n        # Creating new variables for the hidden state, otherwise we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        #sets gradients of all parameters to zero initially\n        net.zero_grad()\n        \n        output, h = net(inputs, h)\n        \n        #calculating loss and performing backprop\n        loss = criterion(output.squeeze(), labels.float())\n        train_loss.append(loss)\n        loss.backward()\n        \n        #clip_grad_norm helps prevent grad explosion\n        nn.utils.clip_grad_norm(net.parameters(), clip)\n        optimizer.step()\n        \n        #loss stats\n        if counter % print_every == 0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n                \n                #creating new variables for the hidden state, otherwise  we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n                \n                inputs, labels = inputs.cuda(), labels.cuda()\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n                \n                val_losses.append(val_loss.item())\n            \n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","520e541b":"def test_model(test_input):\n    output_list = list()\n    batch_size = 64\n    net.eval()\n    with torch.no_grad():\n        test_tweet = preprocess(test_input)\n        for tweet in test_tweet:\n            #convert to tensor to pass into the model\n            feature_tensor = torch.from_numpy(tweet).view(1, -1)\n            if(train_on_gpu):\n                feature_tensor = feature_tensor.cuda()\n            batch_size = feature_tensor.size(0)\n            #initialize hidden state\n            h = net.init_hidden(batch_size)\n            #get the output from the model\n            output, h = net(feature_tensor, h)\n            pred = torch.round(output.squeeze())\n            output_list.append(pred)\n        test_labels = [int(i.data.cpu().numpy()) for i in output_list]\n        return test_labels","71b8a75a":"test_labels = test_model(test_input)","a24b1682":"output = pd.DataFrame()\noutput['id'] = test['id']\noutput['label'] = test_labels","60a41fa7":"output","286f270e":"output.to_csv('submission.csv', index=False)","07beb9ad":"# **Creating own embeddings based on the corpus**","d8a6a091":"# **Splitting the Data and Building the Model**","2e32180b":"# **Text PreProcessing**","2dcdc11d":"# **Analyzing tweet length**"}}