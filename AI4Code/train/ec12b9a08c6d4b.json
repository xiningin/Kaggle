{"cell_type":{"48cb00bf":"code","fc2867d4":"code","bfd367ed":"code","869f5a75":"code","c50a8f76":"code","d075e333":"code","1d438e54":"code","bc6fef26":"code","00f5bb10":"code","3b11955b":"code","0f1d1d71":"code","2813d3ac":"code","abb3c482":"code","5bfb50c9":"code","ac6476ca":"code","07f8ea74":"code","8a1328d5":"code","8f5c3358":"code","de545c41":"code","8ac82180":"code","e6bb5fe5":"code","f1da4bbf":"code","6fa4a6d3":"code","df99f6d1":"code","5f545de4":"code","cabfa6b0":"code","53b0e80b":"code","f821b31f":"code","634a357c":"code","67773aae":"code","302ad5cf":"code","045b7bab":"code","0a458a78":"code","fac8981e":"code","34b03dc1":"code","ceb7f26f":"code","25c51e5d":"code","44bb78ca":"code","02eadcc2":"code","44b04cd0":"code","346d127c":"code","9c45e572":"code","2f27171f":"code","4109e174":"code","dd3e9461":"code","8af48361":"code","f74d0961":"code","712b9ad1":"code","2ed69b06":"markdown","7c4fd565":"markdown","52a98fb5":"markdown","4731a010":"markdown","38a2551d":"markdown","88e555da":"markdown","7a2df86b":"markdown","5b3eb776":"markdown","2bbfe85a":"markdown","e3babf79":"markdown","e3fc31df":"markdown","df7b450f":"markdown","a36c8986":"markdown"},"source":{"48cb00bf":"%matplotlib inline \nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nimport seaborn as sns \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans","fc2867d4":"data = pd.read_csv('..\/input\/states_all.csv')","bfd367ed":"data.shape","869f5a75":"print (data.columns) \ndata.head()","c50a8f76":"data.info()","d075e333":"data.shape","1d438e54":"print (data['YEAR'].max())\nprint (data['YEAR'].min())","bc6fef26":"data = data.dropna(subset=['ENROLL'])","00f5bb10":"data.shape","3b11955b":"1492-1229","0f1d1d71":"del data['PRIMARY_KEY']","2813d3ac":"data.set_index('STATE')\ndata.head()","abb3c482":"#picking columns that are relevant to the scoring \ndata.isnull().any()","5bfb50c9":"scores = ['AVG_MATH_4_SCORE', 'AVG_MATH_8_SCORE','AVG_READING_4_SCORE','AVG_READING_8_SCORE']","ac6476ca":"scores_df = data[scores].dropna().copy()\nprint (scores_df.isna().sum())","07f8ea74":"scores_df.shape","8a1328d5":"scores_df.isna().sum()","8f5c3358":"scores_df.index","de545c41":"X = StandardScaler().fit_transform(scores_df)\nX","8ac82180":"kmeans = KMeans(n_clusters=4)\nmodel = kmeans.fit(X)\nprint(\"model\\n\", model)","e6bb5fe5":"centers = model.cluster_centers_\ncenters","f1da4bbf":"def pd_centers(featuresUsed, centers):\n    colNames = list(featuresUsed)\n    colNames.append('prediction')\n\n    # Zip with a column called 'prediction' (index)\n    Z = [np.append(A, index) for index, A in enumerate(centers)]\n\n    # Convert to pandas data frame for plotting\n    P = pd.DataFrame(Z, columns=colNames)\n    P['prediction'] = P['prediction'].astype(int)\n    return P","6fa4a6d3":"from pandas.plotting import parallel_coordinates","df99f6d1":"from itertools import cycle, islice","5f545de4":"def parallel_plot(data):\n\tmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(data)))\n\tplt.figure(figsize=(15,8)).gca().axes.set_ylim([-3,+3])\n\tparallel_coordinates(data, 'prediction', color = my_colors, marker='o')","cabfa6b0":"P = pd_centers(scores, centers)\nP","53b0e80b":"parallel_plot(P[P['AVG_MATH_8_SCORE'] < 1])","f821b31f":"parallel_plot(P[P['AVG_READING_8_SCORE'] < 1])","634a357c":"data.head()","67773aae":"data[data['YEAR'].isin([2015])].sort_values(by = 'AVG_MATH_4_SCORE')","302ad5cf":"print ('Minnesota enrollment for 2015 = 807044')\nprint ('Alabama enrollment for 2015 = 734974')","045b7bab":"revenue_data = data[['STATE','YEAR','TOTAL_REVENUE','FEDERAL_REVENUE','STATE_REVENUE','LOCAL_REVENUE']] \nrevenue_data.tail()","0a458a78":"year = 2016 \nstate1 = 'MINNESOTA'\n\n\nfilter1 = revenue_data['STATE'].str.contains(state1)  \nfilter2 = revenue_data['YEAR'].isin([year])\n\nrev = revenue_data[filter1 & filter2] \ntype(rev)","fac8981e":"year = 2016 \nstate1 = 'ALABAMA'\n\n\nfilter1 = revenue_data['STATE'].str.contains(state1)  \nfilter2 = revenue_data['YEAR'].isin([year])\n\nrev1 = revenue_data[filter1 & filter2] \ntype(rev1)","34b03dc1":"df = pd.concat([rev,rev1])\n##uSE THE PANDAS.PLOT.BAR() 11PM AT NIGHT GOING TO SLEEP \ndf","ceb7f26f":"del df['YEAR']","25c51e5d":"df\n","44bb78ca":"df_melt = pd.melt(df,id_vars=['STATE'] , var_name='revenue')\ndf_melt\n                  ","02eadcc2":"plt.figure(figsize=(10,6))\nsns.barplot(x = 'revenue', y= 'value', hue='STATE', data=df_melt)","44b04cd0":"expenditure_data = data[['STATE','YEAR','TOTAL_EXPENDITURE', 'INSTRUCTION_EXPENDITURE',\n       'SUPPORT_SERVICES_EXPENDITURE', 'OTHER_EXPENDITURE',\n       'CAPITAL_OUTLAY_EXPENDITURE']]\nexpenditure_data.head()","346d127c":"year = 2016 \nstate1 = 'MINNESOTA'\n\n\nfilter3 = expenditure_data['STATE'].str.contains(state1)  \nfilter4 = expenditure_data['YEAR'].isin([year])\n\nexp = expenditure_data[filter3 & filter4] \nexp","9c45e572":"year = 2016 \nstate1 = 'ALABAMA'\n\n\nfilter3 = expenditure_data['STATE'].str.contains(state1)  \nfilter4 = expenditure_data['YEAR'].isin([year])\n\nexp1 = expenditure_data[filter3 & filter4] \nexp1","2f27171f":"df1 = pd.concat([exp,exp1])\ndf1","4109e174":"df1 = df1.drop('YEAR', 1)","dd3e9461":"df1","8af48361":"df_melt = pd.melt(df1,id_vars=['STATE'] , var_name='expenditure')\ndf_melt\n  ","f74d0961":"df_melt","712b9ad1":"plt.figure(figsize=(15,10))\nsns.barplot(x = 'expenditure', y= 'value', hue='STATE', data=df_melt)","2ed69b06":"## What are the spending characteristics that correlate the most with high test scores? ##","7c4fd565":"Remove 263 lines of data where the enrollment number was NaN","52a98fb5":"Here we are going to try to begin the clustering process","4731a010":"Lets find two states to compare based on what we know on the clustering results. ","38a2551d":"## Now are we to compare a low scoring state to a high scoring school state\n* In one year, how does a high scoring state spend the revenue compared to low scoring state? ","88e555da":"We have reindexed the data so that the data is the State","7a2df86b":"Lets drop NA values from the enroll column ","5b3eb776":"### Now that we understand what our data looks like, lets clean our data to perform some analysis ","2bbfe85a":"Based on this we will compare Minnesota to Alabama. Two states in different clusters based on what we know about how \nthe reading scores for fourth graders can be a good gage on overall scoring. ","e3babf79":" \n* What are the lowest educational scoring states? Lets use cluster analysis to see if we can categorize the bottom of the group\n* How does spending compare to states that scoring very well? \n  \n","e3fc31df":"## Clustering results \nWe can make a conclusion that the states that have low scores have similarly low scores across all the scoring criteria, except 8th grade reading score which does not follow the same cluster. \n\nWe can make similar observation that the states that score well, score well in all the categories and merit further investigation into there spending techniques. ","df7b450f":"Lets see a state example. ","a36c8986":"## what does our data look like? Lets do some basic exploring "}}