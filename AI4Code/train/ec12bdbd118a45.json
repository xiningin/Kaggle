{"cell_type":{"26d6689d":"code","256d231a":"code","093a6cea":"code","a05cc273":"code","09dac5c0":"code","b5fd8e8f":"code","ebb4d721":"code","77cd8c57":"code","f0b11d64":"code","569bef46":"code","7d89ab84":"code","f7a7fb53":"code","0ecd18a7":"code","0b25faad":"code","90a6ea4f":"code","72d35236":"code","9783fdfe":"code","247d2ca8":"code","f59699ab":"code","0a141366":"code","9ed71324":"code","f029c013":"code","c9aed0a2":"code","aa8f91f7":"code","49106a00":"markdown","46aeeea2":"markdown","c16fa3f4":"markdown","eb429d9f":"markdown","cfa019d1":"markdown","9cec4e28":"markdown","9abe5581":"markdown","a93eca1d":"markdown","028174b0":"markdown","644075f0":"markdown","c44fcf69":"markdown","9441dbab":"markdown","096f8f94":"markdown","534582ed":"markdown","3d15e271":"markdown","fef49628":"markdown"},"source":{"26d6689d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","256d231a":"from sklearn.impute import SimpleImputer\nmedian_imputer = SimpleImputer(strategy = 'median')\nmostfreq_imputer = SimpleImputer(strategy = 'most_frequent')","093a6cea":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ncombine = [test_data, train_data]\ntrain_data.head()","a05cc273":"train_data.tail()","09dac5c0":"train_data.info()","b5fd8e8f":"train_data.describe()","ebb4d721":"train_data.isnull().sum()","77cd8c57":"train_data.describe(include=['O'])","f0b11d64":"cols = train_data.columns\ndis_cols = cols.drop(['Survived', 'PassengerId','Name','Age','Ticket','Fare','Cabin'])\nfor x in dis_cols:\n    ana = train_data[[x, 'Survived']].groupby(x).mean().sort_values(by='Survived',ascending=False)\n    print(ana)\n    print(\"_\"*10)","569bef46":"import matplotlib.pyplot as plt\nimport seaborn as sns\ng = sns.FacetGrid(train_data, col = \"Survived\", size = 10)\ng.map(plt.hist, 'Age', bins = 50)\nplt.xticks(np.arange(0,81, step = 5))\nplt.show()\nplt.figure(figsize = (14,6))\ng = sns.FacetGrid(train_data, col = \"Survived\", size = 10)\ng.map(plt.hist, 'Fare', bins = 20)\nplt.xticks(np.arange(0,520, step = 50))\nplt.show()","7d89ab84":"train_data.plot.scatter(x='Pclass', y='Fare', c='Survived', colormap='viridis')","f7a7fb53":"train_data = train_data.drop(['Cabin','Ticket'], axis = 1)\ntest_data = test_data.drop(['Cabin', 'Ticket'], axis = 1)\ncombine = [train_data, test_data]\ntrain_data.head()","0ecd18a7":"train_data['Age'] = median_imputer.fit_transform(train_data[['Age']])\ntest_data['Age'] = median_imputer.transform(test_data[['Age']])\ntrain_data['Fare'] = median_imputer.fit_transform(train_data[['Fare']])\ntest_data['Fare'] = median_imputer.transform(test_data[['Fare']])\ntrain_data['Embarked'] = mostfreq_imputer.fit_transform(train_data[['Embarked']])\ntest_data['Embarked'] = mostfreq_imputer.transform(test_data[['Embarked']])","0b25faad":"for df in combine:\n    df['family'] = df['SibSp']+df['Parch']\ntrain_data = train_data.drop(['SibSp','Parch'], axis = 1)\ntest_data = test_data.drop(['SibSp','Parch'],axis=1)\ncombine = [train_data, test_data]","90a6ea4f":"train_data[['family', 'Survived']].groupby('family').mean().sort_values(by='Survived',ascending=False)","72d35236":"for df in combine:\n    df['Alone'] = 0\n    df.loc[df['family']==0, 'Alone']=1\ntrain_data = train_data.drop(['family'],axis = 1)\ntest_data = test_data.drop(['family'],axis = 1)\ncombine = [train_data, test_data]\ntrain_data[['Alone', 'Survived']].groupby('Alone').mean().sort_values(by='Survived',ascending=False)","9783fdfe":"for df in combine:\n    df['Title'] = df.Name.str.extract('([A-Za-z]+)\\.',expand = False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","247d2ca8":"for df in combine:\n    df['Title'] = df['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Mlle','Mme','Ms','Rev','Sir'],'Others')\ntrain_data[['Title','Survived']].groupby(['Title']).mean()","f59699ab":"train_data = train_data.drop(['Name'],axis=1)\ntest_data = test_data.drop(['Name'],axis = 1)\ncombine = [train_data,test_data]\nfor df in combine:\n    df['Sex'] = df['Sex'].map({'female':0,'male':1}).astype(int)","0a141366":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\nfeatures = ['Embarked', 'Title']\nOH_train_cols = pd.DataFrame(encoder.fit_transform(train_data[features]))\nOH_test_cols = pd.DataFrame(encoder.transform(test_data[features]))\nOH_train_cols.index = train_data.index\nOH_test_cols.index = test_data.index\ntrain_data = train_data.drop(features, axis = 1)\ntest_data = test_data.drop(features, axis = 1)\ntrain_data = pd.concat([train_data, OH_train_cols], axis = 1)\ntest_data = pd.concat([test_data, OH_test_cols], axis = 1)","9ed71324":"from sklearn.model_selection import GridSearchCV\nX = train_data.drop(['PassengerId','Survived'], axis = 1)\ny = train_data['Survived']\nX_test = test_data.drop(['PassengerId'], axis =1).copy()","f029c013":"from sklearn.ensemble import RandomForestClassifier\ngrid_feature = {'max_features':[10,11,12,13]}\nmodel = RandomForestClassifier(n_estimators = 1000,\n                             random_state=1,\n                             n_jobs=-1)\ngrid = GridSearchCV(model, param_grid = grid_feature)\ngrid.fit(X,y)\nprint(grid.best_params_)\nprint(grid.best_score_)","c9aed0a2":"model = RandomForestClassifier(n_estimators = 1000,\n                               max_features = 12,\n                             random_state=1,\n                             n_jobs=-1)\nmodel.fit(X, y)\ny_test = model.predict(X_test)\nmodel.score(X, y)","aa8f91f7":"submission = pd.DataFrame({\"PassengerId\":test_data['PassengerId'],\"Survived\":y_test})\nsubmission.to_csv('titanic_submission.csv', index = False)","49106a00":"Let's deal with \"Name\" and \"Sex\".","46aeeea2":"* sibsp means the number of siblings or spouses aboard the ship.\n* parch means the number of parents or children aboard the ship.\n* embarked means the port of embarkation.\n\nIn here, we can see that there are some missing data.\nAlso, we can see that sex and embarked are categories.\n\nBefore we preprocess these data, we need more information about it.","c16fa3f4":"Let's check those missing data","eb429d9f":"Next, we start to clean our data. First we need to delete \"cabin\", since there are only 204 non-missing data out of 891. Also \"ticket\", since I cannot find any information in it.","cfa019d1":"From here, we can see that if we guess that all women were alive and all men were dead, there are 74% chances win.\nTherefore, we need to create a model that winning chance better than 74% \n\nAlso, we can see that survive rate seems like related to Pclass.","9cec4e28":"It seems that the number of family aboard is not matter, but whether there is a family member aboard or not is matter.","9abe5581":"Now, we need to deal with \"SibSp\" and \"Parch\". Actually, these two columns can be combined together as \"family\".","a93eca1d":"It seems that I was wrong about the relation between Fare and Pclass. However, we can see that the relation between Pclass and Survived are better than between Fare and Survived.","028174b0":"First, we need to read and understand our data.","644075f0":"Analysis the data First","c44fcf69":"We can research whether or not there is some kind of relation between survive rate and number of family members aboard. ","9441dbab":"Next, I want to fill in those missing data.\n* Those in \"Embarked\" columns, I will fill in with most frequency\n* Those in \"Fare\", I will fill in with median.","096f8f94":"we find out that when age is smaller than 12, the survived rate become bigger. On the other hand, when age is bigger than 40, the survived rate become smaller.","534582ed":"Finally, let's train our model. In here, we use random forest.","3d15e271":"This is my first time to challenge a competition without helping.\nAlso, since I want to become a data analyst or data scientist in future, I want to practice the skills of explaination and maybe to build a blog.","fef49628":"I guess Pclass may be related to Fare. I need to prove my guess."}}