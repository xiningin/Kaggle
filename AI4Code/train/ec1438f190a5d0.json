{"cell_type":{"9ddf8595":"code","666a7c20":"code","833d1105":"code","d528f87e":"code","65f4582d":"code","789f8a40":"code","9dc65c0a":"code","e5dfaa01":"code","03fb0700":"code","7d030a03":"code","2881704f":"code","2977d215":"code","d12b5bb0":"code","84ce04fe":"code","42146a90":"code","2f1824dd":"code","34f8d2df":"code","c92b19fc":"code","8f175b3f":"markdown","73af8d74":"markdown","581710b0":"markdown","4d4c1050":"markdown","d85f98d3":"markdown","4632a253":"markdown","b445f72f":"markdown","a55bce8e":"markdown","9fb43e68":"markdown"},"source":{"9ddf8595":"!pip install pretty_midi\n!pip install midi2audio","666a7c20":"from tqdm import tqdm\nimport os\nimport math\nimport random\nimport pandas as pd\nimport numpy as np\nimport IPython\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom IPython import *\nimport os\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split\nimport pretty_midi\nimport torch\nimport math as m\nimport torch.optim as optim\nimport collections\nfrom itertools import chain\nfrom torch import tensor\nfrom midi2audio import FluidSynth\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'","833d1105":"volume = 50\ndef untenosrize(t): return [pretty_midi.containers.Note(volume, int(note[0]), float(note[1]), float(note[2])) for note in t]\ndef tenosrize(r):\n    a = pretty_midi.PrettyMIDI(r)\n    tnotes = []\n    for b in a.instruments:\n        b = b.notes\n        notes =  tensor([[c.pitch, c.start, c.get_duration()] for c in b])\n        tnotes.append(notes)\n    return tnotes[0]\ndef dic(t, dics):\n    shape = t.shape\n    t = t.clone()\n    for a in range(3):\n        for b in tqdm(range(len(t[:, a]))):\n            t[b, a] = dics[a].index([t[b, a].item()])\n    t = t.type(torch.int)\n    return t.reshape(shape)\ndef dic(t, dics):\n    t = t.clone()\n    for a in range(3):\n        t[:, a] = tensor([dics[a].index([t[b, a].item()]) for b in range(len(t[:, a]))])\n    return t.type(torch.int)\ndef undic(t, dics):\n    shape = t.shape\n    l = []\n    for a in range(3):\n        l.append(tensor(list(map(dics[a].__getitem__, t[:, a]))))\n    l = torch.stack(l, dim=1)\n    return l.reshape(shape)\ndef featurize(t):\n    index = torch.argsort(t[:, 1], dim=0)\n    t = torch.stack([t[int(a)] for a in index])\n    out = t[1:, 1] - t[:-1, 1]\n    t[1:, 1]= out\n    return t\ndef unfeaturize(t):\n    t = t.clone()\n    for a in range(len(t)-1):\n        out = t[a+1, 1]+ t[a, 1]\n        t[a+1, 1]= out\n    out = t[:, 2]+t[:, 1]\n    t[:, 2] = out\n    return t","d528f87e":"path = '..\/input\/maestropianomidi\/maestro-v3.0.0\/'\nsubdir = [x[0] for x in os.walk(path)][1:]\ndata = []\nfor files in subdir:\n    for file in os.listdir(files):\n        data.append(os.path.join(files, file))\ntime_step = 32","65f4582d":"def get_dics(directory):\n    song = []\n    for t in tqdm(directory):\n        t = featurize(tenosrize(t))\n        t[:, 1] = torch.clamp(t[:, 1], max=3.9687)\n        t[:, 2] = torch.clamp(t[:, 2], min=1\/time_step , max=4)\n        song.append(t)\n    t, out = torch.cat(song), torch.cat(song)\n    t[:, [1,2]] = torch.round(t[:, [1,2]]*time_step)\/time_step\n    dics = [list(np.array(torch.unique(t[:, a]).type(torch.float))) for a in range(3)]\n    dics[0] = [np.float32(a) for a in range(128)]\n    return t, dics\n#t, dics = get_dics(data)\ndics = torch.load('..\/input\/maestrov3preprocessed\/dics.pkl')\ndef unlatent(t, dics=dics):\n    t = undic(t, dics)\n    t = untenosrize(unfeaturize(t))\n    return t\nsongs = torch.load(\"..\/input\/maestrov3preprocessed\/songs.pkl\")\n#songs = dic(t, dics)\n#torch.save(dics, \"dics.pkl\")\n#torch.save(songs, \"songs.pkl\")","789f8a40":"batch_size = 32\nsequence_len = 128\nsplit = torch.split(songs, sequence_len)[:-1]\nx , y = torch.stack(split)[1:], torch.stack(split)[:-1]\ndslen = len(x)\/\/10\nxtrain, ytrain = x[:dslen*9], y[:dslen*9]\nxtest, ytest = x[dslen*9:], y[dslen*9:]\nclass trainset(Dataset):\n    def __init__(self, data):\n        self.x, self.y = data\n    def __len__(self): return len(self.x)\n    def __getitem__(self, index): \n        y = self.y[index]\n        y = nn.functional.one_hot(y.type(torch.long), num_classes=len(dics[2]))\n        return self.x[index], y\ntrain, test = trainset([xtrain, ytrain]), trainset([xtest, ytest])\ntrain, test = DataLoader(train, batch_size=batch_size, shuffle = True), DataLoader(test, batch_size=batch_size, shuffle = True)","9dc65c0a":"t, tok = get_dics(data[:2])\nlat = dic(t, tok)\nqwe = unlatent(lat, tok)\nmid = pretty_midi.PrettyMIDI(data[0])\np1 = \"test_uncompressed.mid\"\np2 = \"test_compressed.mid\"\nmid.write(p1)\nmid.instruments[0].notes = qwe\nmid.write(p2)\nfs = 44000","e5dfaa01":"# oringal\nmid = pretty_midi.PrettyMIDI(p1)\nIPython.display.Audio(mid.synthesize(fs=fs), rate=fs)","03fb0700":"#compressed\nmid = pretty_midi.PrettyMIDI(p2)\nIPython.display.Audio(mid.synthesize(fs=fs), rate=fs)","7d030a03":"p = data[0]\nnumiter = range(999).__iter__()\ndef gener(gen, x, dis= None, sequences=100, escape_count=10):\n    output = []\n    gen.to(device)\n    output = []\n    for a in range(sequences):\n        if dis==None:\n            x = gen(x, generate=True)\n        else:\n            dis.to(device)\n            samples = [gen(x.type(torch.int), generate=True).type(torch.float32) for a in range(escape_count)]\n            score = [dis(samples[a]) for a in range(escape_count)]\n            x = samples[score.index(max(score))]\n        output.append(x)\n    return undic(torch.cat(output, dim=1).squeeze().type(torch.int), dics)\ndef make_song(gen, p=p, sequence_len=sequence_len, dis=None, sequences= 25, escape_count=10, evalu=False):\n    with torch.no_grad():\n        if evalu:\n            model.eval()\n        else: model.train()\n        x,y = next(iter(train))\n        x = x[0].unsqueeze(dim=0)\n        preds = gener(gen, x.to(device), dis=dis, sequences=sequences, escape_count=escape_count)\n        out = untenosrize(unfeaturize(preds.squeeze()))\n        mid = pretty_midi.PrettyMIDI(p)\n        mid.instruments[0].notes = out\n        itera = str(numiter.__next__())\n        mid.write(\"song \" + itera + '.mid')","2881704f":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = sequence_len):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) \/ d_model))\n        pe = torch.zeros(batch_size, max_len, 1, d_model)\n        pe[:, :, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, :, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\ndef compose(f, x): return f(x)\nclass transformer(nn.Module):\n    def __init__(self, d_model=1024, nhead=16, d_hid=16, nlayers=6, dropout= 0.25, nembeds=128):\n        super().__init__()\n\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        self.embeds = nn.Embedding(nembeds, d_model)\n        self.pos_embeds = PositionalEncoding(d_model, dropout)\n        layers = nn.TransformerEncoderLayer(d_model*3, nhead, d_hid, dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(layers, nlayers)\n        self.decoder = nn.Linear(d_model*3, nembeds*3)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.d_model = d_model\n        self.mask = torch.triu(torch.ones(sequence_len, sequence_len) * float('-inf'), diagonal=1).to(device)\n        \n    def forward(self, x, generate=False):\n        x = self.embeds(x)* math.sqrt(self.d_model)\n        x = self.pos_embeds(x)\n        sp = x.shape\n        x = torch.reshape(x, (sp[0], sp[1], (self.d_model*3)))\n        x = self.transformer_encoder(x, self.mask)\n        x= self.decoder(x)\n        shap = x.shape\n        x = torch.reshape(x, (shap[0], shap[1], 3, shap[2]\/\/3))\n        if generate:\n            x = torch.argmax(x, dim=3)\n        return x","2977d215":"def eval(model, dis=None):\n    model.eval()\n    model.to(device)\n    with torch.no_grad():\n        loss, false_preds, true_preds = [], [], [ ]\n        count = 0\n        for x,y in test:\n            x,y = x.to(device), y.to(device)\n            if count == 0:\n                shape = torch.numel(x)\n                count = 1\n            preds = model(x)\n            if dis != None:\n                gen = torch.argmax(preds, dim=3)\n                dis.to(device) \n                false_preds.append(dis(gen).mean().item())\n                true_preds.append(dis(x).mean().item())\n            loss.append(creloss(preds, y).item())\n        numelloss = round((sum(loss)\/len(loss))\/shape, 4)\n        avloss = round(sum(loss)\/len(loss), 4)\n        if dis != None:\n            false_preds = round(sum(false_preds)\/len(false_preds), 4)\n            true_preds = round(sum(true_preds)\/len(true_preds), 4)\n            print(\"Eval batch loss \" + str(avloss) + \" numel loss \" + str(numelloss) + \" False \" + str(false_preds) + \" True \" + str(true_preds))\n        else:\n            print(\"Eval batch loss \" + str(avloss) + \" numel loss \" + str(numelloss))","d12b5bb0":"from tqdm import tqdm\ndef smax(t): return t.exp()\/torch.sum(t.exp(), dim=2, keepdim=True)\ndef creloss(x, y):\n    shap = x.shape\n    x = smax(x)\n    return -torch.sum(y*torch.log(x))\ndef fit(model, dl, epochs=1, lr=0.0001):\n    count = 0\n    model.to(device)\n    lossfunc = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(dl), epochs=epochs)\n    model.train()\n    total_loss = []\n    count = 0\n    for a in range(epochs):\n        for x,y in tqdm(dl):\n            if count == 0:\n                shape = torch.numel(x)\n                count +=1 \n            x,y = x.to(device), y.to(device)\n            preds = model(x)\n            loss = creloss(preds, y)\n            with torch.no_grad():\n                total_loss.append(loss.item())\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n        avloss = sum(total_loss)\/len(total_loss)\n        print(\"training batch loss \" + str(avloss) + \" numel loss \" + str(avloss\/shape))\n        eval(model)","84ce04fe":"model = transformer()\nfit(model, train, lr=0.0008, epochs=3)\ntorch.save(model, \"music_transformer.pkl\")\nmake_song(model)\nmake_song(model, evalu=True)","42146a90":"mid = pretty_midi.PrettyMIDI(\"song 0.mid\")\nIPython.display.Audio(mid.synthesize(fs=fs), rate=fs)","2f1824dd":"class discriminator(nn.Module):\n    def __init__(self, input_dim=3, lin_dim=256, lstm_dim=256, lstm_layers=4, dropout=0.5):\n        super().__init__()\n        self.droupout = nn.Dropout(dropout) \n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, lin_dim \/\/ 2),\n            nn.LeakyReLU(0.2),\n            nn.Linear(lin_dim \/\/ 2, lin_dim),\n            nn.LeakyReLU(0.2))\n        self.conv = nn.Sequential(\n            nn.Conv1d(lstm_dim, 8, 8, 4),\n            nn.LeakyReLU(0,1),\n            nn.Conv1d(8, 1, 8, 8),\n            nn.LeakyReLU(0,1),\n            nn.Linear(3, 1))\n        self.act= nn.Sigmoid()\n        self.lin = nn.Sequential(nn.Linear(4, 1))\n        self.lstm = nn.LSTM(input_size=lin_dim, hidden_size=lstm_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout, bidirectional=False)\n\n    def forward(self, x):\n        x = x.type(torch.float)\n        x = self.mlp(x)\n        x, h = self.lstm(x)\n        x = self.conv(x.permute(0, 2, 1)).permute(0, 1, 2)\n        return self.act(x.squeeze())","34f8d2df":"def traindis(gen, dis, epochs=1, lr=0.001, noise_scale=10):\n    dis_opt =  optim.Adam(dis.parameters(), lr=lr)\n    gen.train().to(device)\n    dis.train().to(device)\n    lossfunc = nn.BCELoss().to(device)\n    for a in range(epochs):\n        for x,y  in tqdm(train):\n            x, y =x.to(device), y.to(device)\n            y = torch.argmax(y, dim=3).type(torch.float32)\n            dis_opt.zero_grad()\n            fake = gen(x, generate=True).type(torch.float32)\n            fake_preds = dis(fake)\n            real_preds= dis(y)\n            fake_loss = lossfunc(fake_preds, (torch.zeros_like(fake_preds)))\n            real_loss = lossfunc(real_preds,(torch.ones_like(real_preds)))\n            dis_loss = (fake_loss + real_loss)\/2\n            dis_loss.backward()\n            dis_opt.step()\n        eval(model, dis)","c92b19fc":"dis = discriminator()\ntraindis(model, dis)\nmake_song(model, dis=dis)\nmid = pretty_midi.PrettyMIDI(\"song 1.mid\")\nIPython.display.Audio(mid.synthesize(fs=fs), rate=fs)","8f175b3f":"# Generate music with model","73af8d74":"# Dataloading and Featurizing\nBasiclly a midi(Musical Instrument Digital Interface) note conistes of four things pitch, start time, end time, and volume. What I do first is take the Midi and convert it to a tensor containing the pitch, and volume, the duritation between the last note and the current note, and duration of the note. I changed the repersenations of timings to reduce the range of the timing vaules.\n\nThen I shifted the timings to be in 16 notes, and tokenized the data, then clamped the range of some features so I could put all the vaules in one tensor without padding. I use preprocessed data because it takes ~ an hour to process maestro.","581710b0":"# Conclusion\nThe music is alright, about a quater the time the music localy compareble to human made samples, but it lacks any  long term conistancy. This was a good project still with lots of ways to impove this model.","4d4c1050":"# Discriminator\nSince the transformer use dropout when generating new songs, their is some randomness in the generated sample. To take advantage of with this a Discriminator picks the best sample out of a few generated samples. The Discriminator is an lstm followed by convulations, that's heavly normalized (50% dropout).","d85f98d3":"# Traing loop\nHere their's a custom Cross entropy loss becuase pytorch's inbuilt CEL doesn't work on these are dimensions","4632a253":"# Evaulation function","b445f72f":"# Model\nI addatied this transformer from the pytorch docs (https:\/\/pytorch.org\/tutorials\/beginner\/transformer_tutorial.html)\nA lot of the stuff I read said sparse transformers worked better which I might try a some point.","a55bce8e":"# Imports","9fb43e68":"This is a test song after going through featurizing and compression, definatble noticable, but it will make the model work a lot better. Particular notes that are split into three parts aren't nicely presevered by 16th note compression. The only way i could get to display audio was through synthensizer library so that's why all the display audio sounds like it's form a syntehsizer, the higher quality unsynthesied audio is in the output of this notebook."}}