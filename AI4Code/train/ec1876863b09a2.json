{"cell_type":{"3a58f2eb":"code","5ba07e32":"code","748e3713":"code","6504693b":"code","46b90db6":"code","c2365240":"code","924dd480":"code","daeb7731":"code","65241df3":"code","6d3672ec":"code","2abd822e":"code","6374504e":"code","9d200f16":"code","971c23bc":"code","fbf0ef86":"code","cef1ef00":"code","e83e25c1":"code","0c8b0d20":"code","b3d09a71":"code","0728bc4e":"code","a57c1c7f":"code","aca02140":"code","9179ce06":"code","91b8e7cf":"code","485a097a":"code","0a5d4c0a":"markdown","407c11e9":"markdown","57c0969f":"markdown","b986ce2e":"markdown","3c46b8cb":"markdown","5a78a806":"markdown","d83535c2":"markdown"},"source":{"3a58f2eb":"# load and clean-up data\nfrom numpy import nan\nfrom numpy import isnan\nfrom pandas import read_csv\nfrom pandas import to_numeric\nfrom numpy import split\nfrom numpy import array\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import ConvLSTM2D","5ba07e32":"# fill missing values with a value at the same time one day ago\ndef fill_missing(values):\n    one_day = 60 * 24\n    for row in range(values.shape[0]):\n        for col in range(values.shape[1]):\n            if isnan(values[row, col]):\n                values[row, col] = values[row - one_day, col]\n\n# load all data\ndataset = read_csv('..\/input\/household_power_consumption.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n# mark all missing values\ndataset.replace('?', nan, inplace=True)\n# make dataset numeric\ndataset = dataset.astype('float32')\n# fill missing\nfill_missing(dataset.values)\n# add a column for for the remainder of sub metering\nvalues = dataset.values\ndataset['sub_metering_4'] = (values[:,0] * 1000 \/ 60) - (values[:,4] + values[:,5] + values[:,6])\n# save updated dataset\ndataset.to_csv('household_power_consumption.csv')","748e3713":"# resample minute data to total for each day\n# load the new file\ndataset = read_csv('household_power_consumption.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n# resample data to daily\ndaily_groups = dataset.resample('D')\ndaily_data = daily_groups.sum()\n# summarize\nprint(daily_data.shape)\nprint(daily_data.head())\n# save\ndaily_data.to_csv('household_power_consumption_days.csv')","6504693b":"# split into standard weeks\n\n# split a univariate dataset into train\/test sets\ndef split_dataset(data):\n    # split into standard weeks\n    train, test = data[1:-328], data[-328:-6]\n    # restructure into windows of weekly data\n    train = array(split(train, len(train)\/7))\n    test = array(split(test, len(test)\/7))\n    return train, test\n\n# load the new file\ndataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\ntrain, test = split_dataset(dataset.values)\n# validate train data\nprint(train.shape)\nprint(train[0, 0, 0], train[-1, -1, 0])\n# validate test\nprint(test.shape)\nprint(test[0, 0, 0], test[-1, -1, 0])","46b90db6":"# univariate multi-step lstm\n\n# split a univariate dataset into train\/test sets\ndef split_dataset(data):\n    # split into standard weeks\n    train, test = data[1:-328], data[-328:-6]\n    # restructure into windows of weekly data\n    train = array(split(train, len(train)\/7))\n    test = array(split(test, len(test)\/7))\n    return train, test\n\n# evaluate one or more weekly forecasts against expected values\ndef evaluate_forecasts(actual, predicted):\n    scores = list()\n    # calculate an RMSE score for each day\n    for i in range(actual.shape[1]):\n        # calculate mse\n        mse = mean_squared_error(actual[:, i], predicted[:, i])\n        # calculate rmse\n        rmse = sqrt(mse)\n        # store\n        scores.append(rmse)\n    # calculate overall RMSE\n    s = 0\n    for row in range(actual.shape[0]):\n        for col in range(actual.shape[1]):\n            s += (actual[row, col] - predicted[row, col])**2\n    score = sqrt(s \/ (actual.shape[0] * actual.shape[1]))\n    return score, scores\n\n# summarize scores\ndef summarize_scores(name, score, scores):\n    s_scores = ', '.join(['%.1f' % s for s in scores])\n    print('%s: [%.3f] %s' % (name, score, s_scores))\n\n# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            x_input = data[in_start:in_end, 0]\n            x_input = x_input.reshape((len(x_input), 1))\n            X.append(x_input)\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return array(X), array(y)\n\n# train the model\ndef build_model(train, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 70, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # define model\n    model = Sequential()\n    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model\n\n# make a forecast\ndef forecast(model, history, n_input):\n    # flatten data\n    data = array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, 0]\n    # reshape into [1, n_input, 1]\n    input_x = input_x.reshape((1, len(input_x), 1))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat\n\n# evaluate a single model\ndef evaluate_model(train, test, n_input):\n    # fit model\n    model = build_model(train, n_input)\n    # history is a list of weekly data\n    history = [x for x in train]\n    # walk-forward validation over each week\n    predictions = list()\n    for i in range(len(test)):\n        # predict the week\n        yhat_sequence = forecast(model, history, n_input)\n        # store the predictions\n        predictions.append(yhat_sequence)\n        # get real observation and add to history for predicting the next week\n        history.append(test[i, :])\n    # evaluate predictions days for each week\n    predictions = array(predictions)\n    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n    return score, scores","c2365240":"# load the new file\ndataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n\n# split into train and test\ntrain, test = split_dataset(dataset.values)","924dd480":"# evaluate model and get scores\nn_input = 7\nscore, scores = evaluate_model(train, test, n_input)","daeb7731":"# summarize scores\nsummarize_scores('lstm', score, scores)","65241df3":"# plot scores\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\npyplot.plot(days, scores, marker='o', label='lstm')\npyplot.show()","6d3672ec":"# train the model\ndef build_model(train, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 20, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(200, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model","2abd822e":"# evaluate model and get scores\nn_input = 14\nscore, scores = evaluate_model(train, test, n_input)","6374504e":"# summarize scores\nsummarize_scores('lstm', score, scores)","9d200f16":"# plot scores\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\npyplot.plot(days, scores, marker='o', label='lstm')\npyplot.show()","971c23bc":"# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            X.append(data[in_start:in_end, :])\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return array(X), array(y)\n \n# train the model\ndef build_model(train, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 50, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(200, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model\n \n# make a forecast\ndef forecast(model, history, n_input):\n    # flatten data\n    data = array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, :]\n    # reshape into [1, n_input, n]\n    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat","fbf0ef86":"# evaluate model and get scores\nn_input = 14\nscore, scores = evaluate_model(train, test, n_input)","cef1ef00":"# summarize scores\nsummarize_scores('lstm', score, scores)","e83e25c1":"# plot scores\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\npyplot.plot(days, scores, marker='o', label='lstm')\npyplot.show()","0c8b0d20":"# univariate multi-step encoder-decoder cnn-lstm\n\n# split a univariate dataset into train\/test sets\ndef split_dataset(data):\n    # split into standard weeks\n    train, test = data[1:-328], data[-328:-6]\n    # restructure into windows of weekly data\n    train = array(split(train, len(train)\/7))\n    test = array(split(test, len(test)\/7))\n    return train, test\n\n# evaluate one or more weekly forecasts against expected values\ndef evaluate_forecasts(actual, predicted):\n    scores = list()\n    # calculate an RMSE score for each day\n    for i in range(actual.shape[1]):\n        # calculate mse\n        mse = mean_squared_error(actual[:, i], predicted[:, i])\n        # calculate rmse\n        rmse = sqrt(mse)\n        # store\n        scores.append(rmse)\n    # calculate overall RMSE\n    s = 0\n    for row in range(actual.shape[0]):\n        for col in range(actual.shape[1]):\n            s += (actual[row, col] - predicted[row, col])**2\n    score = sqrt(s \/ (actual.shape[0] * actual.shape[1]))\n    return score, scores\n\n# summarize scores\ndef summarize_scores(name, score, scores):\n    s_scores = ', '.join(['%.1f' % s for s in scores])\n    print('%s: [%.3f] %s' % (name, score, s_scores))\n\n# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            x_input = data[in_start:in_end, 0]\n            x_input = x_input.reshape((len(x_input), 1))\n            X.append(x_input)\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return array(X), array(y)\n\n# train the model\ndef build_model(train, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 20, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(200, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model\n\n# make a forecast\ndef forecast(model, history, n_input):\n    # flatten data\n    data = array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, 0]\n    # reshape into [1, n_input, 1]\n    input_x = input_x.reshape((1, len(input_x), 1))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat\n\n# evaluate a single model\ndef evaluate_model(train, test, n_input):\n    # fit model\n    model = build_model(train, n_input)\n    # history is a list of weekly data\n    history = [x for x in train]\n    # walk-forward validation over each week\n    predictions = list()\n    for i in range(len(test)):\n        # predict the week\n        yhat_sequence = forecast(model, history, n_input)\n        # store the predictions\n        predictions.append(yhat_sequence)\n        # get real observation and add to history for predicting the next week\n        history.append(test[i, :])\n    # evaluate predictions days for each week\n    predictions = array(predictions)\n    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n    return score, scores","b3d09a71":"# load the new file\ndataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n# split into train and test\ntrain, test = split_dataset(dataset.values)","0728bc4e":"# evaluate model and get scores\nn_input = 14\nscore, scores = evaluate_model(train, test, n_input)\n# summarize scores\nsummarize_scores('lstm', score, scores)","a57c1c7f":"# plot scores\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\npyplot.plot(days, scores, marker='o', label='lstm')\npyplot.show()","aca02140":"# univariate multi-step encoder-decoder convlstm\n\n# split a univariate dataset into train\/test sets\ndef split_dataset(data):\n    # split into standard weeks\n    train, test = data[1:-328], data[-328:-6]\n    # restructure into windows of weekly data\n    train = array(split(train, len(train)\/7))\n    test = array(split(test, len(test)\/7))\n    return train, test\n\n# evaluate one or more weekly forecasts against expected values\ndef evaluate_forecasts(actual, predicted):\n    scores = list()\n    # calculate an RMSE score for each day\n    for i in range(actual.shape[1]):\n        # calculate mse\n        mse = mean_squared_error(actual[:, i], predicted[:, i])\n        # calculate rmse\n        rmse = sqrt(mse)\n        # store\n        scores.append(rmse)\n    # calculate overall RMSE\n    s = 0\n    for row in range(actual.shape[0]):\n        for col in range(actual.shape[1]):\n            s += (actual[row, col] - predicted[row, col])**2\n    score = sqrt(s \/ (actual.shape[0] * actual.shape[1]))\n    return score, scores\n\n# summarize scores\ndef summarize_scores(name, score, scores):\n    s_scores = ', '.join(['%.1f' % s for s in scores])\n    print('%s: [%.3f] %s' % (name, score, s_scores))\n\n# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            x_input = data[in_start:in_end, :]\n            x_input = x_input.reshape((len(x_input), -1))\n            X.append(x_input)\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return array(X), array(y)\n\n# train the model\ndef build_model(train, n_steps, n_length, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 20, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape into subsequences [samples, time steps, rows, cols, channels]\n    train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n    model.add(Flatten())\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(200, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model\n\n# make a forecast\ndef forecast(model, history, n_steps, n_length, n_input):\n    # flatten data\n    data = array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, :]\n    # reshape into [samples, time steps, rows, cols, channels]\n    input_x = input_x.reshape((1, n_steps, 1, n_length, input_x.shape[1]))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat\n\n# evaluate a single model\ndef evaluate_model(train, test, n_steps, n_length, n_input):\n    # fit model\n    model = build_model(train, n_steps, n_length, n_input)\n    # history is a list of weekly data\n    history = [x for x in train]\n    # walk-forward validation over each week\n    predictions = list()\n    for i in range(len(test)):\n        # predict the week\n        yhat_sequence = forecast(model, history, n_steps, n_length, n_input)\n        # store the predictions\n        predictions.append(yhat_sequence)\n        # get real observation and add to history for predicting the next week\n        history.append(test[i, :])\n    # evaluate predictions days for each week\n    predictions = array(predictions)\n    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n    return score, scores","9179ce06":"# load the new file\ndataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n# split into train and test\ntrain, test = split_dataset(dataset.values)","91b8e7cf":"# define the number of subsequences and the length of subsequences\nn_steps, n_length = 2, 7\n# define the total days to use as input\nn_input = n_length * n_steps\nscore, scores = evaluate_model(train, test, n_steps, n_length, n_input)\n# summarize scores\nsummarize_scores('lstm', score, scores)","485a097a":"# plot scores\ndays = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\npyplot.plot(days, scores, marker='o', label='lstm')\npyplot.show()","0a5d4c0a":"Adopted from https:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption\/","407c11e9":"## LSTM Model With Univariate Input and Vector Output","57c0969f":"A further extension of the CNN-LSTM approach is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM for each time step.\n\nThis combination is called a Convolutional LSTM, or ConvLSTM for short, and like the CNN-LSTM is also used for spatio-temporal data.\n\nUnlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN-LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units themselves.\n\nFor more information for how the equations for the ConvLSTM are calculated within the LSTM unit, see the paper:\n\nConvolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, 2015.\nThe Keras library provides the ConvLSTM2D class that supports the ConvLSTM model for 2D data. It can be configured for 1D multivariate time series forecasting.\n\nThe ConvLSTM2D class, by default, expects input data to have the shape:\n\n`[samples, timesteps, rows, cols, channels]`\n\nWhere each time step of data is defined as an image of (rows * columns) data points.\n\nWe are working with a one-dimensional sequence of total power consumption, which we can interpret as one row with 14 columns, if we assume that we are using two weeks of data as input.\n\nFor the ConvLSTM, this would be a single read: that is, the LSTM would read one time step of 14 days and perform a convolution across those time steps.\n\nThis is not ideal.\n\nInstead, we can split the 14 days into two subsequences with a length of seven days. The ConvLSTM can then read across the two time steps and perform the CNN process on the seven days of data within each.\n\nFor this chosen framing of the problem, the input for the ConvLSTM2D would therefore be:\n\n`[n, 2, 1, 7, 1]`\n\nOr:\n\n* Samples: n, for the number of examples in the training dataset.\n* Time: 2, for the two subsequences that we split a window of 14 days into.\n* Rows: 1, for the one-dimensional shape of each subsequence.\n* Columns: 7, for the seven days in each subsequence.\n* Channels: 1, for the single feature that we are working with as input (**NB!** changed to 8!)\n\nYou can explore other configurations, such as providing 21 days of input split into three subsequences of seven days, and\/or providing all eight features or channels as input.\n\nWe can now prepare the data for the ConvLSTM2D model.\n\nFirst, we must reshape the training dataset into the expected structure of \n`[samples, timesteps, rows, cols, channels]`\n\n```\n# reshape into subsequences [samples, time steps, rows, cols, channels]\ntrain_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n# reshape into subsequences [samples, time steps, rows, cols, channels]\ntrain_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n```\n\nWe can then define the encoder as a ConvLSTM hidden layer followed by a flatten layer ready for decoding.\n\n```\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\nmodel.add(Flatten())\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\nmodel.add(Flatten())\n```\n\nWe will also parameterize the number of subsequences (n_steps) and the length of each subsequence (n_length) and pass them as arguments.\n\nThe rest of the model and training is the same. The build_model() function with these changes is listed below.\n\n```\n# train the model\ndef build_model(train, n_steps, n_length, n_input):\n\t# prepare data\n\ttrain_x, train_y = to_supervised(train, n_input)\n\t# define parameters\n\tverbose, epochs, batch_size = 0, 20, 16\n\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n\t# reshape into subsequences [samples, time steps, rows, cols, channels]\n\ttrain_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n\t# reshape output into [samples, timesteps, features]\n\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n\t# define model\n\tmodel = Sequential()\n\tmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n\tmodel.add(Flatten())\n\tmodel.add(RepeatVector(n_outputs))\n\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n\tmodel.add(TimeDistributed(Dense(1)))\n\tmodel.compile(loss='mse', optimizer='adam')\n\t# fit network\n\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n\treturn model\n```\n```\n# train the model\ndef build_model(train, n_steps, n_length, n_input):\n\t# prepare data\n\ttrain_x, train_y = to_supervised(train, n_input)\n\t# define parameters\n\tverbose, epochs, batch_size = 0, 20, 16\n\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n\t# reshape into subsequences [samples, time steps, rows, cols, channels]\n\ttrain_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n\t# reshape output into [samples, timesteps, features]\n\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n\t# define model\n\tmodel = Sequential()\n\tmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n\tmodel.add(Flatten())\n\tmodel.add(RepeatVector(n_outputs))\n\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n\tmodel.add(TimeDistributed(Dense(1)))\n\tmodel.compile(loss='mse', optimizer='adam')\n\t# fit network\n\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n\treturn model\n```\n\nThis model expects five-dimensional data as input. Therefore, we must also update the preparation of a single sample in the forecast() function when making a prediction.\n\n```\n# reshape into [samples, time steps, rows, cols, channels]\ninput_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n# reshape into [samples, time steps, rows, cols, channels]\ninput_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n```\nThe forecast() function with this change and with the parameterized subsequences is provided below.\n```\n# make a forecast\ndef forecast(model, history, n_steps, n_length, n_input):\n\t# flatten data\n\tdata = array(history)\n\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n\t# retrieve last observations for input data\n\tinput_x = data[-n_input:, 0]\n\t# reshape into [samples, time steps, rows, cols, channels]\n\tinput_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n\t# forecast the next week\n\tyhat = model.predict(input_x, verbose=0)\n\t# we only want the vector forecast\n\tyhat = yhat[0]\n\treturn yhat\n    \n# make a forecast\ndef forecast(model, history, n_steps, n_length, n_input):\n\t# flatten data\n\tdata = array(history)\n\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n\t# retrieve last observations for input data\n\tinput_x = data[-n_input:, 0]\n\t# reshape into [samples, time steps, rows, cols, channels]\n\tinput_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n\t# forecast the next week\n\tyhat = model.predict(input_x, verbose=0)\n\t# we only want the vector forecast\n\tyhat = yhat[0]\n\treturn yhat\n```\nWe now have all of the elements for evaluating an encoder-decoder architecture for multi-step time series forecasting where a ConvLSTM is used as the encoder.","b986ce2e":"## ConvLSTM Encoder-Decoder Model With Multivariate Input","3c46b8cb":"## Encoder-Decoder LSTM Model With Univariate Input","5a78a806":"## Encoder-Decoder LSTM Model With Multivariate Input","d83535c2":"## CNN-LSTM Encoder-Decoder Model With Univariate Input"}}