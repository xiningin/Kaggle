{"cell_type":{"56637ddd":"code","3b832979":"code","2714ec4b":"code","82cc2503":"code","d312f8a4":"code","eb925565":"code","ebd42d02":"code","bc8d67dc":"code","a619c871":"code","1914e2f8":"code","d44cd662":"code","fb34e110":"code","fff3c6cd":"code","26c3434e":"code","a3eb3eec":"code","42149c7c":"code","c275fd0c":"code","fcaff25d":"code","7d887828":"code","88120553":"code","04a26729":"code","5ec9c746":"code","1e7f3923":"code","6110c8d9":"code","a5f7dc11":"code","1ca704b4":"code","f3cf9330":"code","c2b92be1":"code","2ec49fb7":"code","bc801f56":"code","380686ab":"code","cd2df3e7":"code","50ffe854":"code","a47d2c0a":"code","7554caa2":"code","998ad7cd":"code","ed162d3f":"code","bd9e0489":"code","3cdd3ef6":"code","fb1d6194":"code","5bcc6649":"code","8cd2c303":"code","f4dac75f":"code","5060c2a4":"code","dde48b24":"code","0dbf0f64":"code","364e83d2":"code","842d36cc":"code","8c93f8f4":"code","be6fbdc0":"code","4b801f7d":"code","bbd3aea4":"code","80001e0c":"code","f1960199":"code","74e95a74":"code","0495e477":"code","84c7d223":"code","3a6c99d1":"code","7b988944":"code","9f55a822":"code","1b8b53e8":"code","54ca8a46":"code","7998d8dc":"code","349c0566":"code","f256eb5f":"code","8d4af769":"code","99a35d70":"code","b65d565d":"code","89bf91f1":"code","8445a96c":"code","9a2e0517":"code","a9fc84ec":"code","5048d8ba":"code","f5c6ee4c":"code","8da6393f":"code","6d1adfda":"code","4aaab0f7":"code","5db3d215":"code","9c24a19e":"code","2eb2f95f":"code","e3fb596b":"markdown","608a1d25":"markdown","9b3fb897":"markdown","fc733b51":"markdown","b616f1b9":"markdown","c17c8a08":"markdown","54bd3300":"markdown","cf8ba0fc":"markdown","f4acb782":"markdown","eaf27abd":"markdown","81220e57":"markdown","813588e4":"markdown","e91c2d73":"markdown","f4ce0b6d":"markdown","1b9a6245":"markdown","0d9813c1":"markdown","ce2dd4e6":"markdown","4a8463e3":"markdown","2507e762":"markdown"},"source":{"56637ddd":"#import libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,PolynomialFeatures,RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\nfrom sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\nfrom sklearn.feature_selection import RFE, VarianceThreshold\nfrom sklearn.model_selection import GridSearchCV,KFold,RandomizedSearchCV, StratifiedKFold,cross_val_score\nfrom sklearn.metrics import r2_score\nsns.set_context(\"paper\", font_scale = 1, rc={\"grid.linewidth\": 3})\npd.set_option('display.max_rows', 100, 'display.max_columns', 400)\nfrom scipy.stats import skew,boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom xgboost import XGBRFRegressor,XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor","3b832979":"#Reading in the data\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_train.head()","2714ec4b":"df_test.head()","82cc2503":"#Information gathering\ndf_train.info()","d312f8a4":"print(df_train.shape)\nprint(df_test.shape)","eb925565":"#Getting insights of the features and outliers\ndf_train.describe([0.25,0.50,0.75,0.99])","ebd42d02":"#Getting insights on test dataset as well\ndf_test.describe([0.25,0.50,0.75,0.99])","bc8d67dc":"#Checking percentage of null values present in training dataset \nmissing_num = df_train.isna().sum().sort_values(ascending=False)\nmissing_perc = (df_train.isna().sum()\/len(df_train)*100).sort_values(ascending=False)\nmissing_perc","a619c871":"#Calculating percentage of null values\ndef null_values(dataframe):\n  missing_values = dataframe.isna().sum().sort_values(ascending=False)\n  missing_perc = (((dataframe.isna().sum())\/len(dataframe))*100).sort_values(ascending=False)\n  return missing_values, missing_perc\n\n#Passing in the training and test datasets to calculate the percentage of missing values in training and test data\nnull_sum_train, null_perc_train = null_values(df_train)\nnull_sum_test, null_perc_test = null_values(df_test)","1914e2f8":"null_sum_train","d44cd662":"null_perc_train","fb34e110":"miss_train_sum_perc = pd.concat([null_sum_train, null_perc_train], axis=1, keys=['Sum', 'Percentage'])\nmiss_test_sum_perc = pd.concat([null_sum_test, null_perc_test], axis=1, keys=['Sum', 'Percentage'])","fff3c6cd":"miss_train_plot = miss_train_sum_perc[miss_train_sum_perc['Percentage']>0]\nmiss_train_plot","26c3434e":"miss_test_plot = miss_test_sum_perc[miss_test_sum_perc['Percentage']>0]\nmiss_test_plot","a3eb3eec":"#Printing the numerical dataframe\ndf_numerical_train = df_train.select_dtypes(include=['int64','float64']).drop(['SalePrice','Id'],axis=1)\ndf_numerical_train.head()","42149c7c":"#Printing the categorical dataframe\ndf_categorical_train = df_train.select_dtypes(exclude=['int64','float64'])\ndf_categorical_train.head()","c275fd0c":"#Important \ndef showvalues(ax,m=None):\n    for p in ax.patches:\n        ax.annotate(\"%.1f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                    ha='center', va='center', fontsize=14, color='k', rotation=0, xytext=(0, 7),\n                    textcoords='offset points',fontweight='light',alpha=0.9)","fcaff25d":"plt.figure(figsize=(20, 20))\nplt.subplot(2, 1, 1)\nax1 = sns.barplot(x=miss_train_plot.index, y='Percentage', data=miss_train_plot)\nshowvalues(ax1)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\n\nplt.subplot(2, 1, 2)\nax2 = sns.barplot(x=miss_test_plot.index, y='Percentage', data=miss_test_plot)\nshowvalues(ax2)\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, horizontalalignment='right')\n\nplt.show()","7d887828":"#Dropping Id columns from train and test sets\ndf_train.drop(['Id'], axis=1)\ndf_test.drop(['Id'], axis=1)\nprint(df_train.shape)\nprint(df_test.shape)","88120553":"len(df_train.select_dtypes(include=['int64','float64']).columns)","04a26729":"#Visualising numerical predictor variables with Target Variables, and this as well can be used for univariate distributions as well\ndf_train_num = df_train.select_dtypes(include=['int64','float64'])\nfig,axs= plt.subplots(12,3,figsize=(20, 80))\n#adjust horizontal space between plots \nfig.subplots_adjust(hspace=0.6)\n\n#We need to flatten the axes for iterating over them. Here the axes in the dimension [12, 3] is transformed to a vector consisting of 12*3 = 36 values.\nfor i,ax in zip(df_train_num.columns,axs.flatten()):\n    sns.scatterplot(x=i, y='SalePrice', hue='SalePrice',data=df_train,ax=ax,palette='viridis_r')\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n\n#ax.set_yticks(np.arange(0,900001,100000))\n    ax.set_title('SalePrice'+' - '+str(i),fontweight='bold',size=20)\n\n#Usually vertically stacked-line columns don't have much outliers","5ec9c746":"#Visualizing categorical predictors with target variable\ndef facetgrid_boxplot(x, y, **kwargs):\n  sns.boxplot(x=x, y=y)\n  x = plt.xticks(rotation=90)\n\n#pd.melt is a useful function. You have written its functionality in your notebook.\nf = pd.melt(df_train, id_vars=['SalePrice'], value_vars=sorted(df_categorical_train.columns))\ng = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False)\n\n#Mapping onto the function where it will plot the boxplot\ng = g.map(facetgrid_boxplot, 'value', 'SalePrice')","1e7f3923":"# Distribution of Target variable (SalePrice)\nplt.figure(figsize=(8,6))\nsns.distplot(df_train['SalePrice'],hist_kws={\"edgecolor\": (1,0,0,1)})","6110c8d9":"# Skew and kurtosis for SalePrice \nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","a5f7dc11":"#Applying log transformation to remove skewness and make target variable normally distributed(we apply natural log here)\ndf_train['SalePrice'] = np.log1p(df_train['SalePrice'])","1ca704b4":"#Distribution of Target variable (SalePrice) - again to see if the skewness has been removed\nplt.figure(figsize=(8,6))\n\n#hist_kws parameter refers to edgecolour of bins in histogram\nsns.distplot(df_train['SalePrice'],hist_kws={\"edgecolor\": (1,0,0,1)})\n#Now saleprice is normally distributed","f3cf9330":"#Plotting the Pearsson's correlation heatmap between the numerical features\n#The code has been derived from Seaborn API docs.\nplt.figure(figsize=(30, 20))\ncorr = df_numerical_train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, cmap='RdPu')","c2b92be1":"#Let's concatenate the train and test datasets together and study them simultaneously\ny = df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'], axis = 1)\ndf_combined = pd.concat([df_train, df_test], axis=0)","2ec49fb7":"df_combined.head()","bc801f56":"columns = df_combined.columns.tolist()\nnumerical_cols = [col for col in df_combined.columns if df_combined[col].dtype != 'object']\ncategorical_cols = [col for col in columns if col not in numerical_cols]","380686ab":"numerical_cols","cd2df3e7":"categorical_cols","50ffe854":"#Visualizing several numerical features:(individually)\ndf_combined['GrLivArea'].plot.hist()","a47d2c0a":"df_combined['GrLivArea'].plot.box()","7554caa2":"#Since the feature has qyuite a large number of outliers in upper bridge, we will try capturing and replacing them","998ad7cd":"quantile75 = df_combined['GrLivArea'].quantile(0.75)\nquantile25 = df_combined['GrLivArea'].quantile(0.25)\nIQR = quantile75 - quantile25\nIQR\nprint(quantile75)","ed162d3f":"extreme_upper = df_combined['GrLivArea'].quantile(0.75) + 4.5 * IQR\nextreme_lower = df_combined['GrLivArea'].quantile(0.25) + 3 * IQR\nextreme_upper","bd9e0489":"#Let's replace the outliers in upper part of feature set with 75th quantile values\ndf_combined.loc[df_combined['GrLivArea']>=4500, 'GrLivArea'] = quantile25\n#Or you can also choose to drop these outlier values from dataframe","3cdd3ef6":"df_combined['GrLivArea'].plot.hist()","fb1d6194":"df_combined['age']=df_combined['YrSold']-df_combined['YearBuilt']\n\n# See why its been done like this\n# Some of the non-numeric predictors are stored as numbers; convert them into strings will convert those columns into dummy variables later.\ndf_combined['MSSubClass'] = df_combined[['MSSubClass']].astype(str) \ndf_combined['YrSold'] = df_combined['YrSold'].astype(str) #year\ndf_combined['MoSold'] = df_combined['MoSold'].astype(str) #month","5bcc6649":"df_combined[numerical_cols].isna().sum()","8cd2c303":"df_combined[categorical_cols].isna().sum()","f4dac75f":"#Functional: Home functionality (Assume typical unless deductions are warranted)\ndf_combined['Functional'] = df_combined['Functional'].fillna('Typ')\ndf_combined['Electrical'] = df_combined['Electrical'].fillna('SBrkr') #Filling with modef\n\n# data description states that NA refers to \"No Pool\"\ndf_combined['PoolQC'] = df_combined['PoolQC'].fillna(\"Missing\")\n\n# Replacing the missing values with 0, since no garage = no cars in garage inferred from data dictionary\ndf_combined['GarageYrBlt'] = df_combined['GarageYrBlt'].fillna(0)\n \ndf_combined['KitchenQual'] = df_combined['KitchenQual'].fillna(\"TA\")\ndf_combined['Exterior1st'] = df_combined['Exterior1st'].fillna(df_combined['Exterior1st'].mode()[0])\ndf_combined['Exterior2nd'] = df_combined['Exterior2nd'].fillna(df_combined['Exterior2nd'].mode()[0])\ndf_combined['SaleType'] = df_combined['SaleType'].fillna(df_combined['SaleType'].mode()[0])\n\n#None means no Garage\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n   df_combined[col] = df_combined[col].fillna(\"Missing\")\n\n#None means no Basement\nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n  df_combined[col] = df_combined[col].fillna(\"Missing\")\n\ndf_combined['MSZoning'] = df_combined.groupby('MSSubClass')['MSZoning'].transform(lambda x : x.fillna(x.mode()[0]))\n#The pandas update function is used to 'Modify in place using non-NA values from another DataFrame. Aligns on indices. There is no return value.'\n#All of the above is stated according to documentation.\nobject_cols = []\nfor col in categorical_cols:\n  if col != 'Source':\n    object_cols.append(col)\ndf_combined.update(df_combined[object_cols].fillna(\"Missing\"))\ndf_combined.update(df_combined[numerical_cols].fillna(0))","5060c2a4":"df_combined[categorical_cols].isna().sum()","dde48b24":"df_combined[numerical_cols].isna().sum()","0dbf0f64":"#Hence we have successfully imputed all missing values with required variables\n#One more thing to be learnt \ndf_combined.isna().sum()","364e83d2":"#We have already removed skewness from target variable, we need to check out the skewness among various features too\ndf_combined_num = df_combined.select_dtypes(['int64', 'float64'])\nskew_features = df_combined_num.apply(lambda x : x.skew()).sort_values(ascending=False)\n\nskew_high = skew_features[skew_features > 0.6] \n#This command returns a series\n#Krish Naik teaches skewness very well\n\nhigh_indices = skew_high.index\n#This returns a list of indices\n\nskew_high\n#Writing ","842d36cc":"#Refer this for boxplot(numerical value distribution)\nfig, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df_combined_num , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"FeatureNames\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","8c93f8f4":"#Normalize skewed features using a box-cox normal distribution, we can surely use other techniques but it works very well on this dataset\n#Check out for other techniques used to normalize skewed features(sum of them being)\n#People usually use box-cox and StandardScaler for removing skewed data\nfor i in high_indices:\n    #What's this 1.002 used for?\n    df_combined[i] = boxcox1p(df_combined[i], boxcox_normmax(df_combined[i] + 1.002))","be6fbdc0":"#Refer this for boxplot(numerical value distribution)\nfig, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df_combined_num[high_indices] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature Names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","4b801f7d":"from scipy import stats\nhigh_indices_list = list(high_indices)","bbd3aea4":"# nrows = 11\n# ncols = 2\n\n# fig, axes = plt.subplots(nrows, ncols, figsize=(20, 15))\n\n# #Initializing lazy counter\n# count = 0\n\n# for i in range(nrows):\n#   for j in range(ncols):\n#       ax = axes[i, j]\n\n#       if count < len(high_indices.tolist()):\n#         ax.plot(df_combined[high_indices_list[count]])\n#         ax.set(xlabel=\"Feature Names\")\n#         ax.title(\"Skewness distribution of a feature variable\")","80001e0c":"#NOt useful columns in our predictions, more than 99% rows have same value.\nprint(df_combined['Utilities'].value_counts())\n#NOt useful columns in our predictions, more than 99% rows have same value.\nprint(df_combined['Street'].value_counts())\n#NOt useful columns in our predictions, more than 99% rows have same value.\nprint(df_combined['PoolQC'].value_counts())","f1960199":"#As we can see above, those columns have very little other useful data as their values and are primarily composed of a single feature. It would be better \n#if we drop them as they are adding up as a redundant feature without giving much insights about the data.\ndf_combined = df_combined.drop(['Utilities', 'Street', 'PoolQC'], axis=1)","74e95a74":"#The main difference between apply and transform functions is that while apply passes the dataframe in the form of columns to the custom function, \n#whereas the transform method passes the dataframe as a series to the custom function.\n#Let's check out the number of 0's in dataset\nfor col in numerical_cols:\n  print(col, \"\\t\", len(list(df_combined.loc[df_combined[col] == 0, col].index)))\n#Hence we can observe there are many columns containing 0 as a value\n#We need some way to remove their unusefulness as they may be useful","0495e477":"df_combined['PoolArea'+'_impute'] = df_combined['PoolArea'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['MiscVal'+'_impute'] = df_combined['MiscVal'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['ScreenPorch'+'_impute'] = df_combined['ScreenPorch'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['3SsnPorch'+'_impute'] = df_combined['3SsnPorch'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['EnclosedPorch'+'_impute'] = df_combined['EnclosedPorch'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['WoodDeckSF'+'_impute'] = df_combined['WoodDeckSF'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['OpenPorch'+'_impute'] = df_combined['OpenPorchSF'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['HalfBath'+'_impute'] = df_combined['HalfBath'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['Fireplaces'+'_impute'] = df_combined['Fireplaces'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['Fireplaces'+'_impute'] = df_combined['Fireplaces'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['2ndFlrSF'+'_impute'] = df_combined['2ndFlrSF'].apply(lambda x : 1 if x>0 else 0)\ndf_combined['LowQualFinSF'+'_impute'] = df_combined['LowQualFinSF'].apply(lambda x : 1 if x>0 else 0)","84c7d223":"#Why the heck did we do this?\ndf_combined['TotalBsmtSF'] = df_combined['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\ndf_combined['2ndFlrSF'] = df_combined['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\ndf_combined['LotFrontage'] = df_combined['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\ndf_combined['MasVnrArea'] = df_combined['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\ndf_combined['BsmtFinSF1'] = df_combined['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)","3a6c99d1":"def log_transform(result, features):\n  m = result.shape[1]\n\n  for feature in features:\n    \n    #The Pandas assign function assigns a new column to the dataframe with a modified feature. Look up the docs for further information.\n    result = result.assign(newcol = pd.Series(np.log(1.01+result[feature])))\n    #columns.values returns a numpy array\n    result.columns.values[m] = feature + '_log' \n    m += 1\n\n  return result\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n\ndf_combined = log_transform(df_combined, log_features)","7b988944":"df_combined_num = df_combined.select_dtypes(include=['int64', 'float64'])\ndf_combined_cat = df_combined.select_dtypes(exclude=['int64', 'float64'])\n\ndf_combined_cat = pd.get_dummies(df_combined_cat, drop_first=True)\n\ndf_combined_final = pd.concat([df_combined_num, df_combined_cat], axis=1)","9f55a822":"df_combined_final.head()","1b8b53e8":"#Based on describe method and scatter plot, we remove the following mostly-outlier-based features:\n#Do them later","54ca8a46":"#There are a ton of features which can be removed based on outliers. I have noted them down. I will remove them if required. But for now, I want the \n#regualarization algortihms do the talking\nvar_threshold = VarianceThreshold(threshold=0.1)\nvar_threshold.fit(df_combined_final)\n\ndf_combined_final.columns[var_threshold.get_support()]\nlen(df_combined_final.columns)","7998d8dc":"less_variance_columns = [col for col in df_combined_final.columns if col not in df_combined_final.columns[var_threshold.get_support()]]\nless_variance_columns\nlen(less_variance_columns)","349c0566":"df_combined_final.drop(less_variance_columns, axis=1, inplace=True)","f256eb5f":"df_combined_final.drop(['Id'], axis=1, inplace=True)","8d4af769":"df_combined_final.head()","99a35d70":"X = df_combined_final[ : len(df_train)]\nX_test = df_combined_final[len(df_train) : ]","b65d565d":"print(X.shape)\nprint(X_test.shape)\nprint(y.shape)\n#We actually need to separate out the train, validation and test sets. What we actually have as X_test.....U now huh...","89bf91f1":"kfold = KFold(n_splits=10, random_state=42, shuffle=True) \n#K-Fold cross-validation\n#Also search K-Fold cross-validation using GridSearchCV and Randomized SearchCV","8445a96c":"#Error functiuon to compute error\ndef rmsle(y_val, predictions):\n  return np.sqrt(mean_squared_error(y_val, predictions))\n\n#Assigning scoring paramter to 'neg_mean_squared_error' beacause 'mean_squared_error' is not \n# available inside cross_val_score method\ndef cv_rmse(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfold))\n    return (rmse)","9a2e0517":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)","a9fc84ec":"print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n#So we can see everything is going as planned","5048d8ba":"ridge_regressor = Ridge() \nparams = {'alpha': [1,0.1,0.01,0.001,0.0001,0] , \"fit_intercept\": [True, False], \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)","f5c6ee4c":"grid_ridge = GridSearchCV(ridge_regressor, param_grid=params, cv=kfold, n_jobs=-1, scoring='neg_mean_squared_error')\ngrid_ridge.fit(X_train, y_train)\nalpha = grid_ridge.best_params_\nridge_score = grid_ridge.best_score_\nprint(alpha)\nprint(ridge_score)","8da6393f":"regressor_ridge_best = Ridge(alpha=0.01, fit_intercept='True', solver='cholesky')\nregressor_ridge_best.fit(X_train, y_train)\npredictions = regressor_ridge_best.predict(X_val)","6d1adfda":"eval = rmsle(y_val, predictions)\nr2_score(y_val, predictions) #defined as (1 - (SSres\/SStot.))","4aaab0f7":"regressor_ridge_best.fit(X, y)","5db3d215":"predictions_final = regressor_ridge_best.predict(X_test)","9c24a19e":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.iloc[:, 1] = predictions_final","2eb2f95f":"submission.to_csv(\".\/submission_prediction.csv\", index=False)","e3fb596b":"**Yes You Can**","608a1d25":"# Data Visualization","9b3fb897":"Robust Scaler : Used when you have outliers in the dataset and you don't want the model to learn extravagant(outlier values) and perform poorly. But in our case, we have already removed the outliers. Still ","fc733b51":"**Advanced Feature** **Selection**","b616f1b9":"Hence we observe that the outlier treatment has been taken care of through box-cox transformation","c17c8a08":"***SalePrice isn't normally distributed. It is positively or right skewed.***","54bd3300":"We will perform some advanced Feature Engineering techniques such as:\n1. Outlier Detection and removal(for extremely skewed distributions of features)\n2. Missing Value Imputation\n3. Scaling and Normalization(if required)\n\n\n\n","cf8ba0fc":"Creating more features by log transformation:","f4acb782":"**Cross validation**","eaf27abd":"Hence we can see there is a high degree of multicollinearity in the dataset. We can either drop them by performing certain feature selection techniques or let some regularization ML techniques(such as Lasso and Ridge regression), do the needful.","81220e57":"**Splitting data into training and validation sets.**","813588e4":"So we can see that a humongous number of features in the dataset are redundant and the algorithm won't be able to learn enough features from them. So its better if we drop them.","e91c2d73":"**19 attributes have missing values and 5 features( PoolQC,MiscFeature,Alley,Fence,FireplaceQu) have missing percentage greater than 45%**","f4ce0b6d":"**Defining Cross-Validation and all possible regression evaluation metrics in the notebook**","1b9a6245":"**Ridge(L2) regularization regressor**","0d9813c1":"Handling skewness in predictor variables:","ce2dd4e6":"Now that every possible transformation of data has been taken care of, let's one-hot encode our categorical variables. That's as easy as it sounds.","4a8463e3":"EDA only for learning purposes done separately. Else you should always combine both the train and test sets and then only perform EDA for easier analysis. That is done later anyways.","2507e762":"# **Feature Engineering + Feature Selection**"}}