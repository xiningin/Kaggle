{"cell_type":{"45d27429":"code","6cd18b4f":"code","a5cc6045":"code","69124a39":"code","21dc23c9":"code","220da790":"code","d24e1a6c":"code","83a9136e":"code","c95e9b60":"code","bd4acdd2":"code","cf48743c":"code","3df2e9c7":"code","db982a84":"code","77697e05":"code","a40a5834":"code","fbf3ad75":"code","1db0007c":"code","be1e255b":"code","1dd157e7":"code","a508ac3c":"code","044ac476":"code","ef5019f1":"code","040f364a":"code","60513d28":"code","7f3ea29e":"code","9b9265e1":"code","5bce0a44":"code","64135510":"code","4b8c0626":"code","c5474479":"code","92bba44c":"code","24f997f5":"code","be313522":"code","c8a736bc":"code","3d2631f1":"code","56903bf6":"code","5ee089be":"code","cc97786b":"code","feb57bca":"code","19e98f3e":"code","513f93de":"code","8c24a608":"code","700afc69":"code","9fc0abc3":"code","2049de1d":"code","68bce92d":"code","2de34276":"code","1c7164a5":"code","edb4c323":"code","63191651":"code","34cef51a":"code","5a34657e":"code","dfc3ae4c":"code","d5ab69b9":"code","8a07534d":"code","f6cd5d79":"code","dce5f94d":"code","c35dcef0":"code","23dbf677":"code","99855b2e":"code","79072899":"code","20eec77b":"code","021c61d9":"code","6b81d791":"code","0665dbec":"code","6f9ea787":"code","12f9686b":"code","580d8eb2":"code","b8c876c7":"code","230676df":"code","7c77364f":"code","1902a86b":"code","f28c800d":"code","5f25f620":"code","48f847d6":"code","4e36f042":"code","92543a9e":"code","e846cd25":"code","36064d6d":"code","63c0dc87":"code","6090ce1d":"code","39a9c6a3":"code","8e20737f":"code","644eacc4":"code","57572c0e":"code","a63b046e":"code","28ca1ecf":"code","9c2cf32d":"code","c0ac51b7":"code","6611263a":"code","b8a56477":"code","54b0ad83":"code","8274e566":"code","1c2dd9c7":"code","73f30a76":"code","36a3e331":"code","30824349":"markdown","5f63cfa7":"markdown","1f177819":"markdown","8d9aff3a":"markdown","d6108721":"markdown","eccc43d5":"markdown","3e611dc1":"markdown","d93d6bfe":"markdown","f4dcfdd4":"markdown","d8f937f2":"markdown","a2c943d1":"markdown","9d1b089d":"markdown","0e782039":"markdown","a9de15eb":"markdown","8dcbea39":"markdown","03d8b20c":"markdown","3d0700f7":"markdown","8b44472d":"markdown","bec4d7e6":"markdown","8a2b9f9c":"markdown","3a4a1517":"markdown","35786e05":"markdown","b51149ed":"markdown","05a02038":"markdown","300c6094":"markdown","e720e3ba":"markdown","d2298c6a":"markdown","1f4058e0":"markdown","eb9e1963":"markdown","0c4a8eb4":"markdown","b637fcfe":"markdown","dda385df":"markdown","33753b43":"markdown","2b5e1558":"markdown","e540ef60":"markdown","c3c82969":"markdown"},"source":{"45d27429":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import style\nimport os\npd.options.mode.chained_assignment = None\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode()\nsns.set_style(\"white\")\n\n\ndef saving_figure_path(fig_name, tight_layout=True):\n    path = os.path.join(\"Desktop\/My_Projects\",fig_name + \".png\")\n    print(\"Wait until we save the image\", fig_name)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=\"png\", dpi=300)\n\nMAIN_PATH = '..\/input\/'\ndf = pd.read_csv(MAIN_PATH +'bank.csv')\nterm_deposits = df.copy()\n# Have a grasp of how our data looks.\ndf.head()","6cd18b4f":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","a5cc6045":"df.dtypes","69124a39":"df.describe()","21dc23c9":"# No missing values.\ndf.info()","220da790":"# Let's see how the numeric data is distributed.\nimport matplotlib.pyplot as plt\n\ndf.hist(bins=10, figsize=(14,10), color='#E14906')\nplt.show()","d24e1a6c":"# Move the deposit column to the first column.\ndep = df['deposit']\n#Drop the deposit column\ndf.drop(labels=['deposit'], axis=1,inplace = True)\ndf.insert(0, 'deposit', dep)\ndf.head()\n","83a9136e":"# There are more nos than yes. \ndf[\"deposit\"].value_counts()\n#df.head()","c95e9b60":"# Convert the columns that contain a Yes or No. (Binary Columns)\ndef convert_to_int(df, new_column, target_column):\n    df[new_column] = df[target_column].apply(lambda x: 0 if x == 'no' else 1)\n    return df[new_column].value_counts()","bd4acdd2":"convert_to_int(df, \"deposit_int\", \"deposit\") #Create a deposit int\nconvert_to_int(df, \"housing_int\", \"housing\") # Create housingint column\nconvert_to_int(df, \"loan_int\", \"loan\") #Create a loan_int column\nconvert_to_int(df, \"default_int\", \"default\") #Create a default_int column","cf48743c":"# Drop the binary columns and leave the same column in the form of integers 0 = No and 1 = Yes\ndf.drop(['housing', 'loan', 'default'], axis=1, inplace=True)","3df2e9c7":"df.head()","db982a84":"# We have the amount of targeted potential clients in each of the different months of the year.\n# The cross_month var. simply states the (%) of how many p.clients accepted or refused a suscription to a term deposit.\nprint(df['month'].value_counts())\ncross_month = pd.crosstab(df['month'], df['deposit']).apply(lambda x: x\/x.sum() * 100)\ncross_month","77697e05":"# Let's create a date column that will be interesting.\n# We will assume the year is 2017\ndf['year'] = 2017\nlst = [df]\n\n# Create a column with the numeric values of the months.\nfor column in lst:\n    column.loc[column[\"month\"] == \"jan\", \"month_int\"] = 1\n    column.loc[column[\"month\"] == \"feb\", \"month_int\"] = 2\n    column.loc[column[\"month\"] == \"mar\", \"month_int\"] = 3\n    column.loc[column[\"month\"] == \"apr\", \"month_int\"] = 4\n    column.loc[column[\"month\"] == \"may\", \"month_int\"] = 5\n    column.loc[column[\"month\"] == \"jun\", \"month_int\"] = 6\n    column.loc[column[\"month\"] == \"jul\", \"month_int\"] = 7\n    column.loc[column[\"month\"] == \"aug\", \"month_int\"] = 8\n    column.loc[column[\"month\"] == \"sep\", \"month_int\"] = 9\n    column.loc[column[\"month\"] == \"oct\", \"month_int\"] = 10\n    column.loc[column[\"month\"] == \"nov\", \"month_int\"] = 11\n    column.loc[column[\"month\"] == \"dec\", \"month_int\"] = 12\n\n# Change datatype from int32 to int64\ndf[\"month_int\"] = df[\"month_int\"].astype(np.int64)\ndf.head()","a40a5834":"df.drop(['day','year', 'deposit'], axis=1, inplace=True)\n# Rename deposit_int column for deposit and then move it to the first.","fbf3ad75":"df = df.rename(columns={\"deposit_int\": \"deposit\"})","1db0007c":"first = df['deposit']\ndf.drop(labels=['deposit'], axis=1,inplace = True)\n# insert (loc, column, values) --> loc is the same as position in the column.\ndf.insert(0, 'deposit', first)\ndf[\"deposit\"].value_counts()","be1e255b":"# Convert duration to minutes of conversation.\ndecimal_points = 2\ndf['duration'] = df['duration'] \/ 60\ndf['duration'] = df['duration'].apply(lambda x: round(x, decimal_points))\ndf.head()","1dd157e7":"f,ax=plt.subplots(1,2,figsize=(18,8))\ncolors=[\"#F08080\", \"#00FA9A\"]\nlabels = 'Refused a T.D. Suscription', 'Accepted a T.D. Suscription'\ndf['deposit'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True, colors=colors, labels=labels,fontsize=14)\nax[0].set_title('Term Deposits', fontsize=20)\nax[0].set_ylabel('% of Total Potential Clients', fontsize=14)\nsns.countplot('deposit',data=df,ax=ax[1], palette=colors)\nax[1].set_title('Term Deposits', fontsize=20)\nax[1].set_xticklabels(['Refused', 'Accepted'], fontsize=14)\nplt.show()","a508ac3c":"import seaborn as sns\n\nf, axes = plt.subplots(ncols=3, figsize=(15, 6))\n\n# Graph Employee Satisfaction\nsns.distplot(df['month_int'], kde=False, color=\"#ff3300\", ax=axes[0]).set_title('Months of Marketing Activity Distribution')\naxes[0].set_ylabel('Potential Clients Count')\naxes[0].set_xlabel('Months')\n\n# Graph Employee Evaluation\nsns.distplot(df['age'], kde=False, color=\"#3366ff\", ax=axes[1]).set_title('Age of Potentical Clients Distribution')\naxes[1].set_ylabel('Potential Clients Count')\n\n# Campaigns\nsns.distplot(df['campaign'], kde=False, color=\"#546E7A\", ax=axes[2]).set_title('Calls Received in the Marketing Campaign')\naxes[2].set_ylabel('Potential Clients Count')\n\nplt.show()","044ac476":"\nmonths_example = pd.crosstab(index=df['deposit'],\n                            columns=df['month_int'],\n                            margins=True)\n\n# Gives the percent of suscribed term deposits\n(months_example\/months_example.loc['All']) * 100\n\n# Gives the amount of suscribed vs non-suscribed term deposits accounts per month.\nmonths_example\n# We need to take the values into a list format in order to fit it to the plotly graphs.\nnodeposit = pd.DataFrame(months_example.iloc[0])\ndeposit = pd.DataFrame(months_example.iloc[1])\n# Total is the total number of Potential clients contacted.\ntotal = pd.DataFrame(months_example.iloc[2])\ndep = deposit.values.tolist()\nnodep = nodeposit.values.tolist()\ntot = total.values.tolist()\n# We use [:12] in order to skip the last row which is the \"All\" row.\nnodep = nodep[:12]\ndep = dep[:12]\ntot = tot[:12]","ef5019f1":"from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode(connected=True)\n\nlabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nvalues = tot\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1', '#0099ff', '#ff944d', '#ebebe0', '#cc80ff', '#ff80bf',\n         '#ffff66', '#99ffbb' '#b3e6ff']\n\ntrace = Pie(labels=labels, values=values,\n               hoverinfo='label+percent', textinfo='value', \n               textfont=dict(size=16),\n               marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\n\ndata = [trace]\nlayout = Layout(\n    title= \"# of Potential Clients Targeted per Month\"\n)\nfigure = dict(data=data, layout=layout)\n# iplot(figure)","040f364a":"fig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(df.loc[(df['deposit'] == 0),'month_int'] , color='#F08080',shade=True,label='Refused a T.D Suscription')\nax=sns.kdeplot(df.loc[(df['deposit'] == 1),'month_int'] , color='#00FA9A',shade=True, label='Accepted a T.D Suscription')\nax.set(xlabel='Months of the Year', ylabel='Frequency')\nplt.title('Frequency of Distribution of Deposits by Month - Finding Useful Patterns')","60513d28":"cross_months = pd.crosstab(df['deposit'], df['month_int'])\nprint(cross_months)\nnodeposit_rate = pd.DataFrame(cross_months.iloc[0])\ndeposit_rate = pd.DataFrame(cross_months.iloc[1])\nnodeposit_rate = nodeposit_rate.values.tolist()\ndeposit_rate = deposit_rate.values.tolist()\nprint(deposit_rate[0])","7f3ea29e":"import plotly.plotly as py\nfrom plotly.graph_objs import *\n\ntrace1 = {\"x\": nodep, \n          \"y\": ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n          \"marker\": {\"color\": \"#B40404\", \"size\": 12}, \n          \"mode\": \"markers\", \n          \"name\": \"Refused a T.D. Suscription\", \n          \"type\": \"scatter\"\n}\n\ntrace2 = {\"x\": dep, \n          \"y\": ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n          \"marker\": {\"color\": \"#0080FF\", \"size\": 12}, \n          \"mode\": \"markers\", \n          \"name\": \"Accepted a T.D Suscription\", \n          \"type\": \"scatter\", \n}\n\ndata = [trace1, trace2]\nlayout = {\"title\": \"Marketing Campaign Performance in the Year\", \n          \"xaxis\": {\"title\": \"# of Accounts\", }, \n          \"yaxis\": {\"title\": \"Month of the Year\"}}\n\nfig = go.Figure(data=data, layout=layout)\n# iplot(fig)","9b9265e1":"# deposit_rate - nodeposit_rate\ndeposit_rate = np.array(deposit_rate)\nnodeposit_rate = np.array(nodeposit_rate)\ndifference = deposit_rate - nodeposit_rate\ndifference\n\ndifference_quantity = np.column_stack((tot, difference))\ndifference_quantity\n\neffectiveness_ratio = np.divide(difference, tot) * 100\neffectiveness_ratio = np.around(effectiveness_ratio, 2)\neffectiveness_ratio = pd.DataFrame(effectiveness_ratio)\neffectiveness_ratio = effectiveness_ratio.values.tolist()\n\n# The most effective months to operate are March, September, October, December \n# (months that people are more likely to open bank accounts.)","5bce0a44":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\n# Add data\nmonth = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n         'August', 'September', 'October', 'November', 'December']\n\nratio = effectiveness_ratio\n\nline = go.Scatter(\n    x = month,\n    y = ratio,\n    text = '(%)',\n    fill = 'tonexty',\n    mode = 'lines',\n    name='Effectiveness Ratio',\n    line = dict(\n        color = ('#ff3300'),\n        width= 4,\n        dash = 'dot',\n    \n    )\n\n)\n\ndata = [line]\n\nlayout = dict(title='Effectiveness per Month',\n             xaxis=dict(title='Month'),\n              yaxis=dict(title='Effectiveness ratio')\n             )\n\nfig = dict(data=data, layout=layout)\n# iplot(fig)","64135510":"# Create the Season column.\ndf['season'] = np.nan\nlst=[df]\n# The conditions for determining each of the seasons.\nfor column in lst:\n    column.loc[(column[\"month_int\"] >= 3) & (column[\"month_int\"] <= 5), 'season'] = 'spring'\n    column.loc[(column[\"month_int\"] >= 6) & (column[\"month_int\"] <= 8), 'season'] = 'summer'\n    column.loc[(column[\"month_int\"] >= 9) & (column[\"month_int\"] <= 11), 'season'] = 'fall'\n    column.loc[column[\"month_int\"] <= 2, 'season'] = 'winter'\n    column.loc[column[\"month_int\"] == 12, 'season'] = 'winter'\n    \ndf['season'].value_counts()\n","4b8c0626":"fig = plt.figure(figsize=(15,4),)\n\ncolors = ['#F08080', '#00FA9A']\n\nax = sns.countplot(y='season', data=df,\n           hue='deposit',\n           palette = colors\n          ) \n\nplt.title(\"Deposits by Season\", fontsize=16)\nplt.ylabel(\"Seasons\", fontsize=12)\nplt.xlabel(\"Level of Marketing Activity\", fontsize=12)\nlegend_name = plt.legend()\nlegend_name.get_texts()[0].set_text('Refused a T.D Suscription')\nlegend_name.get_texts()[1].set_text('Accepted a T.D Suscription')\n\nplt.show()","c5474479":"#Check to see if the Season column was created.\ndf.head()","92bba44c":"colors = ['#DD4040', '#58AD4B', '#0096AA', '#9B00AA']\n\n\ng = sns.lmplot(x='duration', y='campaign', data=df,\n           fit_reg=False, # No regression line\n           hue='deposit',\n        palette = colors,\n        scatter_kws={'alpha':0.6}\n          ) \n\ng.fig.set_size_inches(12,8)\n# title\nnew_title = 'Status of Term Deposits'\ng._legend.set_title(new_title)\n# replace labels\nnew_labels = ['Refused a T.D Suscription', 'Accepted a T.D Suscription']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n\nplt.axis([0,66,0,65])\nplt.axhline(y=5, linewidth=3, color=\"#424242\", linestyle='--')\nplt.annotate('After 5 Calls potential clients \\n tend to refuse \\n unless duration is high!', xy=(33, 5), xytext=(33,13),\n            arrowprops=dict(facecolor='#190707', shrink=0.05))\nplt.xlabel('Duration of the Calls (Minutes)', fontsize=14)\nplt.ylabel('Campaign Calls', fontsize=14)\n\nplt.show()","24f997f5":"# Based from the graphs above we know that the first campaigns were the most successfull.\n# Can we determine why they were successfull?\n# Let's create a Ordinal column which somehow tells after when the potential client was contacted again. Pdays.\ndf.head()\nsuccess_rate = pd.crosstab(df['deposit'], df['campaign']).apply(lambda x: x\/x.sum() * 100)\nprint(success_rate) #Notice rejection for offers increases after three calls that should be the threshold for the marketing team\n#Nevetheless, this makes 90% of our potential clients so only 10% is likely to reject, we still save time and effort.\nonecall_nod = success_rate.at[0,1]\nonecall_d = success_rate.at[1,1]\ntwocalls_nod = success_rate.at[0,2]\ntwocalls_d = success_rate.at[1,2]\nthreecalls_nod = success_rate.at[0,3]\nthreecalls_d = success_rate.at[1,3]\nfourcalls_nod = success_rate.at[0,4]\nfourcalls_d = success_rate.at[1,4]\nfivecalls_nod = success_rate.at[0,5]\nfivecalls_d = success_rate.at[1,5]\n\nper_deposit = df['deposit'].value_counts()\/11162 * 100\naccepted = per_deposit[0]\nrefused = per_deposit[1]\n# Round all variables\nonecall_nod =  round(onecall_nod, 2)\nonecall_d = round(onecall_d, 2)\ntwocalls_nod = round(twocalls_nod,2)\ntwocalls_d = round(twocalls_d, 2)\nthreecalls_nod = round(threecalls_nod, 2)\nthreecalls_d = round(threecalls_d, 2)\nfourcalls_nod = round(fourcalls_nod, 2)\nfourcalls_d = round(fourcalls_d, 2)\nfivecalls_nod = round(fivecalls_nod, 2)\nfivecalls_d = round(fivecalls_d, 2)\naccepted = round(accepted, 2)\nrefused = round(refused, 2)","be313522":"import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as FF\n\n# Add table data\ntable_data = [['Number of <br> Calls', 'Refused T.D', 'Accepted T.D'],\n              ['One Call', onecall_nod , onecall_d],\n              ['Two Calls', twocalls_nod , twocalls_d],\n              ['Three Calls', threecalls_nod, threecalls_d],\n              ['Four Calls', fourcalls_nod, fourcalls_d],\n              ['Five Calls', fivecalls_nod, fivecalls_d],\n              ['Overall Performance', refused, accepted]]\n# Initialize a figure with FF.create_table(table_data)\nfigure = FF.create_table(table_data, height_constant=60)\n\n# Add graph data\ncalls = ['One Call', 'Two Calls', 'Three Calls', 'Four Calls', 'Five Calls', 'Overall Performance']\nRef = [onecall_nod, twocalls_nod, threecalls_nod, fourcalls_nod, fivecalls_nod, refused]\nAcc = [onecall_d, twocalls_d, threecalls_d, fourcalls_d, fivecalls_d, accepted]\n# Make traces for graph\ntrace1 = go.Bar(x=calls, y=Ref, xaxis='x2', yaxis='y2',\n                marker=dict(color=['#E74C3C', '#E74C3C',\n                                  '#E74C3C', '#E74C3C',\n                                   '#E74C3C', '#FA5050']),\n                name='Refused T.D. Suscriptions',\n               text='%')\ntrace2 = go.Bar(x=calls, y=Acc, xaxis='x2', yaxis='y2',\n                marker=dict(color=['#229954','#229954',\n                                  '#229954', '#229954',\n                                   '#229954', '#05FA6C']),\n                name='Accepted T.D. Sucriptions',\n               text='%')\n\ntrace3 = go.Scatter(x=calls, y=Ref, xaxis='x2', yaxis='y2',\n                   marker=dict(color=['#FC0000']),\n                   name='Incremental Refuse Rate',\n                   text='%')\n\n# Add trace data to figure\nfigure['data'].extend(go.Data([trace1, trace2, trace3]))\n\n# Edit layout for subplots\nfigure.layout.yaxis.update({'domain': [0, .45]})\nfigure.layout.yaxis2.update({'domain': [.6, 1]})\n# The graph's yaxis2 MUST BE anchored to the graph's xaxis2 and vice versa\nfigure.layout.yaxis2.update({'anchor': 'x2'})\nfigure.layout.xaxis2.update({'anchor': 'y2'})\nfigure.layout.yaxis2.update({'title': 'Performance'})\n# Update the margins to add a title and see graph x-labels. \nfigure.layout.margin.update({'t':75, 'l':50})\nfigure.layout.update({'title': 'Number of calls to a particular client'})\n# Update the height because adding a graph vertically will interact with\n# the plot height calculated for the table\nfigure.layout.update({'height':800})\n\n# Plot!\n# iplot(figure)","c8a736bc":"fig = plt.figure(figsize=(15,4),)\nax=sns.kdeplot(df.loc[(df['deposit'] == 0),'age'] , color='#F08080',shade=True,label='Refused T.D. Suscriptions')\nax=sns.kdeplot(df.loc[(df['deposit'] == 1),'age'] , color='#00FA9A',shade=True, label='Accepted T.D. Suscriptions')\nax.set(xlabel='Age of Individuals', ylabel='Frequency')\nplt.title('Frequency of Distribution of Deposits by Age - Finding Useful Patterns')","3d2631f1":"# This is to create each of the categories.\nlst = [df]\nfor column in lst:\n    column.loc[column[\"age\"] < 30,  \"age_category\"] = 20\n    column.loc[(column[\"age\"] >= 30) & (column[\"age\"] <= 39), \"age_category\"] = 30\n    column.loc[(column[\"age\"] >= 40) & (column[\"age\"] <= 49), \"age_category\"] = 40\n    column.loc[(column[\"age\"] >= 50) & (column[\"age\"] <= 59), \"age_category\"] = 50\n    column.loc[column[\"age\"] >= 60, \"age_category\"] = 60\n \ndf['age_category'] = df['age_category'].astype(np.int64)\ndf.dtypes","56903bf6":"# How likely was each age category to suscribe to a term deposit.\ncross_age_category = pd.crosstab(df['deposit'], df['age_category']).apply(lambda x: x\/x.sum() * 100)\ncross_age_category","5ee089be":"# Number of potential clients in each age category.\ndf[\"age_category\"].value_counts()","cc97786b":"import seaborn as sns\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(12,8))\nsns.countplot(x=\"age_category\", data=df, palette=\"Set2\")\nax.set_title(\"Different Age Categories\", fontsize=20)\nax.set_xlabel(\"Age Categories\")\nplt.show()","feb57bca":"# There was a positive ratio of Suscribing Term Deposits  of people in their 20s (or younger) and 60s (or older)\nsns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(15, 4))\ncolors = [\"#F08080\", \"#00FA9A\"]\nlabels = ['No Deposit', 'Deposit']\nsns.countplot(y=\"age_category\", hue='deposit', data=df, palette=colors).set_title('Employee Salary Turnover Distribution')\nax.set_ylabel(\"Age Category\")\nlegend_name = plt.legend()\nlegend_name.get_texts()[0].set_text('Refused a T.D Suscription')\nlegend_name.get_texts()[1].set_text('Accepted a T.D Suscription')","19e98f3e":"sns.set(style=\"white\")\nfig, ax = plt.subplots(figsize=(14,8))\nsns.countplot(x=\"job\", data=df, palette=\"Set1\")\nax.set_title(\"Occupations of Potential Clients\", fontsize=20)\nax.set_xlabel(\"Types of Jobs\")\nplt.show()","513f93de":"cross_job_category = pd.crosstab(df['deposit'], df['job']).apply(lambda x: x\/x.sum() * 100)\nprint(cross_job_category)\n\nnodeposit_by_job = pd.DataFrame(cross_job_category.iloc[0])\nnodeposit_by_job = np.around(nodeposit_by_job, 1)\nnodeposit_by_job = nodeposit_by_job.values.tolist()\ndeposit_by_job = pd.DataFrame(cross_job_category.iloc[1])\ndeposit_by_job = np.around(deposit_by_job, 1)\ndeposit_by_job = deposit_by_job.values.tolist()\n\nnodeposit_by_job","8c24a608":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\n\n\n\ntrace1 = go.Bar(\n    x=['Admin', 'Blue Collar', 'Entrepreneur',\n         'Housemaid', 'Management', 'Retired',\n       'Self-Employed', 'Services', 'Student',\n       'Technician', 'Unemployed', 'Unknown'],\n    y=nodeposit_by_job,\n    text= 12 * ['(%) Refused T.D Suscription'],\n    name='Refused T.D Suscription',\n    marker=dict(\n        color='#FF3633',\n    )\n)\ntrace2 = go.Bar(\n    x=['Admin', 'Blue Collar', 'Entrepreneur',\n         'Housemaid', 'Management', 'Retired',\n       'Self-Employed', 'Services', 'Student',\n       'Technician', 'Unemployed', 'Unknown'],\n    y=deposit_by_job,\n    text=12 * ['(%) Accepted T.D Suscription'],\n    name='Accepted T.D Suscription',\n    marker=dict(\n        color='#00ff99',\n    )\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title= 'Status of Accounts by Occupation',\n    barmode='stack',\n    xaxis=dict(\n        title='Type of Job',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Percent (%)',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\n# iplot(fig, filename='stacked-bar')","700afc69":"import seaborn as sns\n\n# Consider removing this graph unless it tells us something.\n\nax = plt.figure(figsize=(14,8))\n# 0 = Did not suscribe term deposits, 1 = Did suscribe term deposits.\nax = sns.boxplot(x=\"job\", y=\"age\", hue=\"deposit\",\n                  data=df, palette={0:'#F08080', 1:'#00FA9A'})\nplt.title(\"Age vs Occupation\", fontsize=16)\nplt.xlabel(s=\"Type of Job\", fontsize=14)\nplt.ylabel(s=\"Age\", fontsize=14)\nlegend_name = plt.legend()\nlegend_name.get_texts()[0].set_text('Refused Opening a T.D')\nlegend_name.get_texts()[1].set_text('Accepted Opening a T.D')\n\nplt.show()","9fc0abc3":"corr = df.corr()\n\nsns.heatmap(corr,annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","2049de1d":"df['balance_categories'] = np.nan\n\nlst = [df]\n\nfor column in lst:\n    column.loc[column['balance'] <= 0, 'balance_categories'] = 'no balance'\n    column.loc[(column['balance'] > 0) & (column['balance'] <= 1000), 'balance_categories'] = 'low balance'\n    column.loc[(column['balance'] > 1000) & (column['balance'] <= 5000), 'balance_categories'] = 'average balance'\n    column.loc[column['balance'] > 5000, 'balance_categories'] = 'high balance'\n    \ndf.head()\ndf['balance_categories'].value_counts()\n# We are targeting by a lot people with low balance!","68bce92d":"fig, ax = plt.subplots(figsize=(12,8))\ng = sns.countplot(x=\"balance_categories\", data=df)\n\nplt.title(\"Target Request per Balance Categories\")\nplt.xlabel('Balance Categories')\nplt.ylabel(\"Number of Potential Clients Targeted\")\n\nplt.show()","2de34276":"fig, ax = plt.subplots(figsize=(12,8))\ng = sns.countplot(x=\"balance_categories\", data=df, hue='deposit', palette={0:'#F08080', 1:'#00FA9A'})\n\nlegend_name = plt.legend()\nlegend_name.get_texts()[0].set_text('Refused to Suscribe a T.D')\nlegend_name.get_texts()[1].set_text('Accepted to Suscribe a T.D')\nplt.xlabel('Balance Categories')\n\n\nplt.show()","1c7164a5":"colors = ['255, 87, 51', '93, 109, 126  ']\nlabels = [\"No\", \"Yes\"]\n\ng = sns.factorplot(x=\"balance_categories\",\n                   hue=\"deposit\", col=\"age_category\",\n                   data=df, kind=\"count\",\n                   size=4, aspect=.7, palette={0:'#DD4040', 1:'#58AD4B'});\n\ng.set_xticklabels(rotation=30)\ng.axes[0,0].set_xlabel('')\ng.axes[0,1].set_xlabel('')\ng.axes[0,2].set_xlabel('Balance Categories')\ng.axes[0,3].set_xlabel('')\ng.axes[0,4].set_xlabel('')\n\n# Set title\ng.axes[0,0].set_title('Youngest Category')\ng.axes[0,1].set_title('In their 30s')\ng.axes[0,2].set_title('In their 40s')\ng.axes[0,3].set_title('In their 50s')\ng.axes[0,4].set_title('Eldest Category')\ng.axes[0,0].set_ylabel('Amount of Balance')\n# replace labels\nnew_labels = ['Refused', 'Accepted']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n\nplt.show()","edb4c323":"# Apparently having a house loan was a huge reason for not suscribing a term deposit.\ncross_house = pd.crosstab(df['deposit'], df['housing_int']).apply(lambda x: x\/x.sum() * 100)\nprint(cross_house)\nnodeposits = cross_house.iloc[0]\nnodeposits = np.around(nodeposits, 2)\nnodeposits = nodeposits.values.tolist()\ndeposits = cross_house.iloc[1]\ndeposits = np.around(deposits, 2)\ndeposits = deposits.values.tolist()\ndeposits","63191651":"import plotly.plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Bar(\n    x=['No House Loan', 'Has a House Loan'],\n    y=nodeposits,\n    text=['(%) Refused to Open D.A', '(%) Refused to Open D.A'],\n    name='Refused Suscriptions',\n    marker=dict(\n        color='#FE4835',\n        line=dict(\n        color='#FF1E07',\n        width=3,\n        )\n    ),\n    opacity=0.8\n)\ntrace2 = go.Bar(\n    x=['No House Loan', 'Has a House Loan'],\n    y=deposits,\n    text=['(%) Accepted to Open D.A', '(%) Accepted to Open D.A' ],\n    name='Accepted Suscriptions',\n    marker=dict(\n        color='#07FF58',\n        line=dict(\n        color='#07C344',\n        width=3,\n        )\n    ),\n    opacity=0.8\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack', \n    title= \"Previous House Loans VS Term Deposit Suscriptions\",\n)\n\nfig = go.Figure(data=data, layout=layout)\n# iplot(fig, filename='stacked-bar')","34cef51a":"# People with no balance and low balance have a higher probability of having a house loan which in return will lead \n# to potential clients that refused suscribing term deposits.\ncross_house = pd.crosstab(df[\"housing_int\"], df[\"balance_categories\"]).apply(lambda x: x\/x.sum() * 100)\ncross_house","5a34657e":"fig, ax = plt.subplots(figsize=(10,8))\ncolors = [\"#0B6121\", \"#DF0101\"]\nax = sns.barplot(x='housing_int', y='balance_categories', data=df, hue=\"deposit\", palette=colors)\nax.set_title(\"How likely is for a balance category \\n to have a housing loan?\")\nax.set_xlabel('totalCount')\nax.set_ylabel('Balance Categories')\nlegend_name = plt.legend()\nlegend_name.get_texts()[0].set_text('No House Loan')\nlegend_name.get_texts()[1].set_text('Has a House Loan')\nplt.show()","dfc3ae4c":"dep = term_deposits['deposit']\nterm_deposits.drop(labels=['deposit'], axis=1,inplace=True)\nterm_deposits.insert(0, 'deposit', dep)\nterm_deposits.head()\n# housing has a -20% correlation with deposit let's see how it is distributed.\n# 52 %\nterm_deposits[\"housing\"].value_counts()\/len(term_deposits)","d5ab69b9":"term_deposits[\"loan\"].value_counts()\/len(term_deposits)","8a07534d":"from sklearn.model_selection import StratifiedShuffleSplit\n# Here we split the data into training and test sets and implement a stratified shuffle split.\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_set, test_set in stratified.split(term_deposits, term_deposits[\"loan\"]):\n    stratified_train = term_deposits.loc[train_set]\n    stratified_test = term_deposits.loc[test_set]\n    \nstratified_train[\"loan\"].value_counts()\/len(df)\nstratified_test[\"loan\"].value_counts()\/len(df)","f6cd5d79":"# Separate the labels and the features.\ntrain_data = stratified_train # Make a copy of the stratified training set.\ntest_data = stratified_test\ntrain_data.shape\ntest_data.shape\ntrain_data['deposit'].value_counts()","dce5f94d":"# Definition of the CategoricalEncoder class, copied from PR #9151.\n# Just run this cell, or copy it to your code, no need to try to\n# understand every line.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists\/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","c35dcef0":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","23dbf677":"train_data.info()","99855b2e":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector([\"age\", \"balance\", \"day\", \"campaign\", \"pdays\", \"previous\",\"duration\"])),\n    (\"std_scaler\", StandardScaler()),\n])","79072899":"numerical_pipeline.fit_transform(train_data)","20eec77b":"categorical_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector([\"job\", \"education\", \"marital\", \"default\", \"housing\", \"loan\", \"contact\", \"month\",\n                                     \"poutcome\"])),\n    (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense'))\n])","021c61d9":"categorical_pipeline.fit_transform(train_data)","6b81d791":"from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ])","0665dbec":"X_train = preprocess_pipeline.fit_transform(train_data)\nX_train","6f9ea787":"y_train = train_data['deposit']\ny_test = test_data['deposit']\ny_train.shape","12f9686b":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\ny_train = encode.fit_transform(y_train)\ny_test = encode.fit_transform(y_test)\ny_train_yes = (y_train == 1)\ny_train\ny_train_yes","580d8eb2":"some_instance = X_train[1250]","b8c876c7":"import time\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Decision Tree\": tree.DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=18),\n    \"Neural Net\": MLPClassifier(alpha=1),\n    \"Naive Bayes\": GaussianNB()\n}","230676df":"# Thanks to Ahspinar for the function. \nno_classifiers = len(dict_classifiers.keys())\n\ndef batch_classify(X_train, Y_train, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['classifier', 'train_score', 'training_time'])\n    count = 0\n    for key, classifier in dict_classifiers.items():\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        df_results.loc[count,'classifier'] = key\n        df_results.loc[count,'train_score'] = train_score\n        df_results.loc[count,'training_time'] = t_diff\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_diff))\n        count+=1\n    return df_results","7c77364f":"df_results = batch_classify(X_train, y_train)\nprint(df_results.sort_values(by='train_score', ascending=False))","1902a86b":"# Use Cross-validation.\nfrom sklearn.model_selection import cross_val_score\n\n# Logistic Regression\nlog_reg = LogisticRegression()\nlog_scores = cross_val_score(log_reg, X_train, y_train, cv=3)\nlog_reg_mean = log_scores.mean()\n\n# SVC\nsvc_clf = SVC()\nsvc_scores = cross_val_score(svc_clf, X_train, y_train, cv=3)\nsvc_mean = svc_scores.mean()\n\n# KNearestNeighbors\nknn_clf = KNeighborsClassifier()\nknn_scores = cross_val_score(knn_clf, X_train, y_train, cv=3)\nknn_mean = knn_scores.mean()\n\n# Decision Tree\ntree_clf = tree.DecisionTreeClassifier()\ntree_scores = cross_val_score(tree_clf, X_train, y_train, cv=3)\ntree_mean = tree_scores.mean()\n\n# Gradient Boosting Classifier\ngrad_clf = GradientBoostingClassifier()\ngrad_scores = cross_val_score(grad_clf, X_train, y_train, cv=3)\ngrad_mean = grad_scores.mean()\n\n# Random Forest Classifier\nrand_clf = RandomForestClassifier(n_estimators=18)\nrand_scores = cross_val_score(rand_clf, X_train, y_train, cv=3)\nrand_mean = rand_scores.mean()\n\n# NeuralNet Classifier\nneural_clf = MLPClassifier(alpha=1)\nneural_scores = cross_val_score(neural_clf, X_train, y_train, cv=3)\nneural_mean = neural_scores.mean()\n\n# Naives Bayes\nnav_clf = GaussianNB()\nnav_scores = cross_val_score(nav_clf, X_train, y_train, cv=3)\nnav_mean = neural_scores.mean()\n\n# Create a Dataframe with the results.\nd = {'Classifiers': ['Logistic Reg.', 'SVC', 'KNN', 'Dec Tree', 'Grad B CLF', 'Rand FC', 'Neural Classifier', 'Naives Bayes'], \n    'Crossval Mean Scores': [log_reg_mean, svc_mean, knn_mean, tree_mean, grad_mean, rand_mean, neural_mean, nav_mean]}\n\nresult_df = pd.DataFrame(data=d)","f28c800d":"# Gradient Boosting Classifier is the best performer.\nresult_df = result_df.sort_values(by=['Crossval Mean Scores'], ascending=False)\nresult_df","5f25f620":"# Cross validate our Gradient Boosting Classifier\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(grad_clf, X_train, y_train, cv=3)","48f847d6":"from sklearn.metrics import accuracy_score\ngrad_clf.fit(X_train, y_train)\nprint (\"Gradient Boost Classifier accuracy is %2.2f\" % accuracy_score(y_train, y_train_pred))\n# Our model has 73% accuracy.","4e36f042":"from sklearn.metrics import confusion_matrix\n# 4697: no's, 4232: yes\nconf_matrix = confusion_matrix(y_train, y_train_pred)\nf, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.subplots_adjust(left=0.15, right=0.99, bottom=0.15, top=0.99)\nax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5, minor=False)\nax.set_xticklabels(\"\")\nax.set_yticklabels(['Refused T. Deposits', 'Accepted T. Deposits'], fontsize=16, rotation=360)\nplt.show()","92543a9e":"# Let's find the scores  for precision and recall.\nfrom sklearn.metrics import precision_score, recall_score\n# The model is 77% sure that the potential client will suscribe to a term deposit. \n# The model is only retaining 60% of clients that agree to suscribe a term deposit.\nprint('Precision Score: ', precision_score(y_train, y_train_pred))\n# The classifier only detects 60% of potential clients that will suscribe to a term deposit.\nprint('Recall Score: ', recall_score(y_train, y_train_pred))","e846cd25":"from sklearn.metrics import f1_score\n\nf1_score(y_train, y_train_pred)","36064d6d":"y_scores = grad_clf.decision_function([some_instance])\ny_scores","63c0dc87":"# Increasing the threshold decreases the recall.\nthreshold = 0\ny_some_digit_pred = (y_scores > threshold)","6090ce1d":"y_scores = cross_val_predict(grad_clf, X_train, y_train, cv=3, method=\"decision_function\")\nneural_y_scores = cross_val_predict(neural_clf, X_train, y_train, cv=3, method=\"predict_proba\")\nnaives_y_scores = cross_val_predict(nav_clf, X_train, y_train, cv=3, method=\"predict_proba\")","39a9c6a3":"# hack to work around issue #9589 introduced in Scikit-Learn 0.19.0\nif y_scores.ndim == 2:\n    y_scores = y_scores[:, 1]\n\nif neural_y_scores.ndim == 2:\n    neural_y_scores = neural_y_scores[:, 1]\n    \nif naives_y_scores.ndim == 2:\n    naives_y_scores = naives_y_scores[:, 1]","8e20737f":"y_scores.shape","644eacc4":"# How can we decide which threshold to use? We want to return the scores instead of predictions with this code.\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, threshold = precision_recall_curve(y_train, y_scores)","57572c0e":"def precision_recall_curve(precisions, recalls, thresholds):\n    fig, ax = plt.subplots(figsize=(12,8))\n    plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precisions\")\n    plt.plot(thresholds, recalls[:-1], \"#424242\", label=\"Recalls\")\n    plt.title(\"Precision and Recall \\n Tradeoff\", fontsize=18)\n    plt.ylabel(\"Level of Precision and Recall\", fontsize=16)\n    plt.xlabel(\"Thresholds\", fontsize=16)\n    plt.legend(loc=\"best\", fontsize=14)\n    plt.xlim([-2, 4.7])\n    plt.ylim([0, 1])\n    plt.axvline(x=0.13, linewidth=3, color=\"#0B3861\")\n    plt.annotate('Best Precision and \\n Recall Balance \\n is at 0.13 \\n threshold ', xy=(0.13, 0.83), xytext=(55, -40),\n             textcoords=\"offset points\",\n            arrowprops=dict(facecolor='black', shrink=0.05),\n                fontsize=12, \n                color='k')\n    \nprecision_recall_curve(precisions, recalls, threshold)\nplt.show()","a63b046e":"from sklearn.metrics import roc_curve\n# Gradient Boosting Classifier\n# Neural Classifier\n# Naives Bayes Classifier\ngrd_fpr, grd_tpr, thresold = roc_curve(y_train, y_scores)\nneu_fpr, neu_tpr, neu_threshold = roc_curve(y_train, neural_y_scores)\nnav_fpr, nav_tpr, nav_threshold = roc_curve(y_train, naives_y_scores)","28ca1ecf":"def graph_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.figure(figsize=(10,6))\n    plt.title('ROC Curve \\n Gradient Boosting Classifier', fontsize=18)\n    plt.plot(false_positive_rate, true_positive_rate, label=label)\n    plt.plot([0, 1], [0, 1], '#0C8EE0')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('ROC Score of 91.73% \\n (Not the best score)', xy=(0.25, 0.9), xytext=(0.4, 0.85),\n            arrowprops=dict(facecolor='#F75118', shrink=0.05),\n            )\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#F75118', shrink=0.05),\n                )\n    \n    \ngraph_roc_curve(grd_fpr, grd_tpr, threshold)\nplt.show()","9c2cf32d":"from sklearn.metrics import roc_auc_score\n\nprint('Gradient Boost Classifier Score: ', roc_auc_score(y_train, y_scores))\nprint('Neural Classifier Score: ', roc_auc_score(y_train, neural_y_scores))\nprint('Naives Bayes Classifier: ', roc_auc_score(y_train, naives_y_scores))","c0ac51b7":"def graph_roc_curve_multiple(grd_fpr, grd_tpr, neu_fpr, neu_tpr, nav_fpr, nav_tpr):\n    plt.figure(figsize=(8,6))\n    plt.title('ROC Curve \\n Top 3 Classifiers', fontsize=18)\n    plt.plot(grd_fpr, grd_tpr, label='Gradient Boosting Classifier (Score = 91.72%)')\n    plt.plot(neu_fpr, neu_tpr, label='Neural Classifier (Score = 91.54%)')\n    plt.plot(nav_fpr, nav_tpr, label='Naives Bayes Classifier (Score = 80.33%)')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(grd_fpr, grd_tpr, neu_fpr, neu_tpr, nav_fpr, nav_tpr)\nplt.show()","6611263a":"# 97% that this potential client will not suscribe to a term deposit.\ngrad_clf.predict_proba([some_instance])","b8a56477":"# Let's see what does our classifier predict.\ngrad_clf.predict([some_instance]) ","54b0ad83":"# Our classifier predicted that the potential client will reject to suscribe a term deposit.\n# Let's confirm if this is true.\ny_train[1250]","8274e566":"term_deposits.head()","1c2dd9c7":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nplt.style.use('seaborn-white')\n\n# Convert the columns into categorical variables\nterm_deposits['job'] = term_deposits['job'].astype('category').cat.codes\nterm_deposits['marital'] = term_deposits['marital'].astype('category').cat.codes\nterm_deposits['education'] = term_deposits['education'].astype('category').cat.codes\nterm_deposits['contact'] = term_deposits['contact'].astype('category').cat.codes\nterm_deposits['poutcome'] = term_deposits['poutcome'].astype('category').cat.codes\nterm_deposits['month'] = term_deposits['month'].astype('category').cat.codes\nterm_deposits['default'] = term_deposits['default'].astype('category').cat.codes\nterm_deposits['loan'] = term_deposits['loan'].astype('category').cat.codes\nterm_deposits['housing'] = term_deposits['housing'].astype('category').cat.codes\n\n# Let's create new splittings like before but now we modified the data so we need to do it one more time.\n# Create train and test splits\ntarget_name = 'deposit'\nX = term_deposits.drop('deposit', axis=1)\n\n\nlabel=term_deposits[target_name]\n\nX_train, X_test, y_train, y_test = train_test_split(X,label,test_size=0.2, random_state=42, stratify=label)\n\n# Build a classification task using 3 informative features\ntree = tree.DecisionTreeClassifier(\n    class_weight='balanced',\n    min_weight_fraction_leaf = 0.01\n    \n)\n\n\n\ntree = tree.fit(X_train, y_train)\nimportances = tree.feature_importances_\nfeature_names = term_deposits.drop('deposit', axis=1).columns\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\ndef feature_importance_graph(indices, importances, feature_names):\n    plt.figure(figsize=(12,6))\n    plt.title(\"Determining Feature importances \\n with DecisionTreeClassifier\", fontsize=18)\n    plt.barh(range(len(indices)), importances[indices], color='#31B173',  align=\"center\")\n    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)\n    plt.ylim([-1, len(indices)])\n    plt.axhline(y=1.85, xmin=0.21, xmax=0.952, color='k', linewidth=3, linestyle='--')\n    plt.text(0.30, 2.8, '46% Difference between \\n duration and contacts', color='k', fontsize=15)\n    \nfeature_importance_graph(indices, importances, feature_names)\nplt.show()","73f30a76":"# Our three classifiers are grad_clf, nav_clf and neural_clf\nfrom sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(\n    estimators=[('gbc', grad_clf), ('nav', nav_clf), ('neural', neural_clf)],\n    voting='soft'\n)\n\nvoting_clf.fit(X_train, y_train)","36a3e331":"from sklearn.metrics import accuracy_score\n\nfor clf in (grad_clf, nav_clf, neural_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    predict = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, predict))","30824349":"# Outline: <br>\n***\nA. **Attribute Descriptions**<br>\nI. *[Bank client data](#bank_client_data)<br>\nII. *[Related with the last contact of the current campaign](#last_contact)<br>\nIII. [Other attributes](#other_attributes) <br>\n\nB. **Structuring the data:** <br>\nI. *[Overall Analysis of the Data](#overall_analysis)<br>\nII. *[Data Structuring and Conversions](#data_structuring) <br>\n\nC. **Exploratory Data Analysis (EDA)**<br>\nI. *[Accepted vs Rejected Term Deposits](#accepted_rejected) <br>\nII. *[Distribution Plots](#distribution_plots) <br>\n\nD. **Different Aspects of the Analysis: **<br>\nI. *[Months of Marketing Activty](#months_activity) <br>\nII. *[Seasonalities](#seasonality) <br>\nIII. *[Number of Calls to the potential client](#number_calls) <br>\nIV. *[Age of the Potential Clients](#age_clients) <br>\nV. [Types of Occupations that leads to more term deposits suscriptions](#occupations) <br>\n\nE. **Correlations that impacted the decision of Potential Clients.**\nI. *[Analysis of our Correlation Matrix](#analysis_correlation) <br>\nII. *[Balance Categories vs Housing Loans](#balance_housing)<br>\nIII. [Negative Relationship between H.Loans and Term Deposits](#negative_relationship) <br>\n\nF. ** Classification Model **<br>\nI. [Introduction](#classification_model)<br> \nII. [Stratified Sampling](#stratified)<br>\nIII. [Classification Models](#models)<br>\nIV. [Confusion Matrix](#confusion)<br>\nV. [Precision and Recall Curve](#precision_recall)<br>\nVI. [Feature Importances Decision Tree C.](#decision) <br>\n\nG. ** Next Campaign Strategy**<br>\nI. [Actions the Bank should Consider](#bank_actions)<br>\n\n# A. Attributes Description: <br>\n\nInput variables:<br>\n# Ai. bank client data:<br>\n<a id=\"bank_client_data\"><\/a>\n1 - **age:** (numeric)<br>\n2 - **job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n3 - **marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n4 - **education:** (categorical: primary, secondary, tertiary and unknown)<br>\n5 - **default:** has credit in default? (categorical: 'no','yes','unknown')<br>\n6 - **housing:** has housing loan? (categorical: 'no','yes','unknown')<br>\n7 - **loan:** has personal loan? (categorical: 'no','yes','unknown')<br>\n8 - **balance:** Balance of the individual.\n# Aii. Related with the last contact of the current campaign:\n<a id=\"last_contact\"><\/a>\n8 - **contact:** contact communication type (categorical: 'cellular','telephone') <br>\n9 - **month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n10 - **day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n11 - **duration:** last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>\n# Aiii. other attributes:<br>\n<a id=\"other_attributes\"><\/a>\n12 - **campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n13 - **pdays:** number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>\n14 - **previous:** number of contacts performed before this campaign and for this client (numeric)<br>\n15 - **poutcome:** outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n\nOutput variable (desired target):<br>\n21 - **y** - has the client subscribed a term deposit? (binary: 'yes','no')","5f63cfa7":"# B. Structuring the Data:\n<img src=\"https:\/\/media.giphy.com\/media\/yiHz2qxBP45tS\/giphy.gif\">\n##  Bi. Overall Analysis:\n<a id=\"overall_analysis\"><\/a>\n## Summary:\n***\n<ul>\n<li type=\"square\"> <b>Mean Age<\/b> is aproximately 41 years old. (Minimum: 18 years old and Maximum: 95 years old.)<\/li><br>\n<li type=\"square\"> The <b>mean balance<\/b> is 1,528. However, the Standard Deviation (std) is a high number so we can understand through this that the balance is heavily distributed across the dataset.<\/li><br>\n<li type=\"square\">As the data information said it will be better to drop the duration column since duration is highly correlated in whether a potential client will buy a term deposit. Also, <b>duration is obtained after the call is made to the potential client<\/b> so if the target client has never received calls this feature is not that useful. The reason why duration is highly correlated with opening a term deposit  is because the more the bank talks to a target client the higher the probability the target client will open a term deposit since a higher duration means a higher interest (commitment) from the potential client. <\/li><br>\n<\/ul>\n\n**Note: There are not that much insights we can gain from the descriptive dataset since most of our descriptive data is located not in the \"numeric\" columns but in the \"categorical columns\".**\n","1f177819":"# Confusion Matrix: \n<a id=\"confusion\"><\/a>\n<img src=\"https:\/\/computersciencesource.files.wordpress.com\/2010\/01\/conmat.png\">\n\n## Insights of a Confusion Matrix: \nThe main purpose of a confusion matrix is to see how our model is performing when it comes to classifying potential clients that are likely to suscribe to a term deposit. We will see in the confusion matrix four terms the True Positives, False Positives, True Negatives and False Negatives.<br><br>\n\n**Positive\/Negative:** Type of Class (label) [\"No\", \"Yes\"]\n**True\/False:** Correctly or Incorrectly classified by the model.<br><br>\n\n**True Negatives (Top-Left Square):** This is the number of **correctly** classifications of the \"No\" class or potenial clients that are **not willing** to suscribe a term deposit. <br><br>\n\n**False Negatives (Top-Right Square):** This is the number of **incorrectly** classifications of the \"No\" class or potential clients that are **not willing** to suscribe a term depositt. <br><br>\n\n**False Positives (Bottom-Left Square):** This is the number of **incorrectly** classifications of the \"Yes\" class or potential clients that are **willing** to suscribe a term deposit. <br><br>\n\n**True Positives (Bottom-Right Square):** This is the number of **correctly** classifications of the \"Yes\" class or potenial clients that are **willing** to suscribe a term deposit.","8d9aff3a":"## Bii. Data Structuring and Integer Conversions:\n<a id=\"data_structuring\"><\/a>\n***\n<img src=\"https:\/\/media.giphy.com\/media\/drYDHLKzpjgli\/giphy.gif\">\n## Instructions:\nIn this section we will convert into <b> integer <\/b> (For plotting reasons) columns that we think are vital to determine patterns in our next section of Exploratory Data Analysis (EVA). Here we are basically making certain modifications in our main dataset in order to be able to explore our data.","d6108721":"# E. Correlations that impacted the decision of Potential Clients:\n<a id=\"analysis_correlation\"><\/a>\n<img src=\"https:\/\/media.giphy.com\/media\/VVPKOXc6aY1Lq\/giphy.gif\">\n\n## Ei. Correlation Matrix:\n***\n\n## What does the correlation matrix tells us?\n<ul>\n<li type=\"square\">The more <b>green<\/b> the square is, the more <b>positive correlated<\/b> it is with the column. The more <b>red<\/b> the square is, the more <b>negative correlated<\/b> the columns are.<\/li>\n<li type=\"square\"> In this section we will try to see which columns are <b>positively correlated<\/b> and <b>negatively correlated<\/b> with the deposit column. What we are trying to do in this excercise is to see some further patterns.<\/li>\n<\/ul>","eccc43d5":"# C. Exploratory Data Analysis (EVA)\n***\n<img src=\"https:\/\/media.giphy.com\/media\/qTDdlvS5z2aT6\/giphy.gif\">","3e611dc1":"# D. Different Aspects of the Analysis:\n## Di. Months of Marketing Activity:\n<a id=\"months_activity\"><\/a>\n***\n\n<img src=\"https:\/\/media.giphy.com\/media\/rM0wxzvwsv5g4\/giphy.gif\" width=800 height=400> <br><br>\n\n## Why Analyze Marketing Activity?\nThe main reason why we are analyzing marketing activity is to see if there is any **pattern during the months were people tended to suscribe more term deposits.** There could be **external factors** that could influence the individual to suscribe to a term deposit at a specific month. For instance, by looking at the Months distribution plot above, we can see that the months of highest marketing activity in order to attract term deposits is May. However, there are questions we should ask ourselves in order to decipher patterns as to when is the right time for the bank to increase the marketing activity to potential clients. \n\n## Summary:\n***\n<ul>\n<li type=\"square\"> The month of <b>May<\/b> was the month of highest marketing activity (25.3%), while <b>December<\/b> was the month of lowest marketing activity (0.985%). <\/li>\n<li type=\"square\"> There is a wide gap during the month of May between rejected and accepted term deposit suscriptions. (Check the Distribution Plot)<\/li>\n<li type=\"square\"> May was the month with the highest level of activity however, it had the lowest ratio (negative) meaning there were <b>more rejected requests<\/b> for term deposits suscriptions than accepted requests. <\/li>\n<li type=\"square\"> <b>March, September, October and December had the highest ratios<\/b> nevertheless, the marketing activity (requested offers from the marketing department) was much lower.<\/li>\n<\/ul>","d93d6bfe":"# Negative relationship between Loans and Suscriptions of Term Deposits:\n<a id=\"negative_relationship\"><\/a>\n## Summary\n***\n<ul>\n<li type=\"square\">There is a <b>negative correlation<\/b> of 20% for potential clients who have house loans. <\/li>\n<li type=\"square\"> Potential clients that had a housing loan were more <b>eager to refuse offers<\/b> to suscribe a term deposit in the bank. <\/li>\n<\/ul>","f4dcfdd4":"## Dv. What type of occupation leads to more Term Deposit Suscriptions?\n<a id=\"occupations\"><\/a> \n<img src=\"https:\/\/media.giphy.com\/media\/3oKIPqXWNJswXf1InS\/giphy.gif\" height=\"1000\" width=\"600\">\n\n## Summary\n***\n<ul>\n<li type=\"square\"> People working in <b>Management<\/b>, <b>Blue-Collars<\/b> and <b>Technicians<\/b> received the most offers from the marketing department to suscribe term deposits. <\/li>\n<li type=\"square\"> <b>Students<\/b>, <b>entrepreneurs<\/b> and <b>housemaids<\/b> received the less amount of offers from the marketing department.<\/li>\n<li type=\"square\"> <b>74.7% of the students<\/b> suscribed term deposits (Which was expected since the youngest segment of the population is most likely to be a student) <\/li>\n<li type=\"square\"><b>66.3% of people who were retired<\/b> were willing to suscribe term deposits (This was also expected since the oldest segment of the population is most likely to be retirees). <\/li>\n<li type=\"square\"><b>56.6% of the unemployed<\/b> were willing to suscribe term deposits. (People tend to save more when they are not able to find jobs since there is most likely no source of income). <\/li>\n<li type=\"square\">In the boxplot, the potential clients who belong to the retired category and refused to suscribe a term deposit <b>were much younger<\/b> (median age: Around 57) than the potential clients who accepted to suscribe a term deposit (median age: Around 66). <\/li>\n<\/ul>","d8f937f2":"<iframe width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\" src=\"\/\/plot.ly\/~AlexanderBach\/580.embed\"><\/iframe>","a2c943d1":"# ROC Curve (Receiver Operating Characteristic):\nThe **ROC curve** tells us how well our classifier is classifying between term deposit suscriptions (True Positives) and non-term deposit suscriptions. The **X-axis** is represented by False positive rates (Specificity) and the **Y-axis** is represented by the True Positive Rate (Sensitivity.) As the line moves the threshold of the classification changes giving us different values. The closer is the line to our top left corner the better is our model separating both classes.\n","9d1b089d":"# Overfitting:\n## Example of Overfitting:\n<img src=\"https:\/\/tomrobertshaw.net\/img\/2015\/12\/overfitting.jpg\"><br><br>\n## Example of Cross Validation:\n<img src=\"http:\/\/vinhkhuc.github.io\/assets\/2015-03-01-cross-validation\/5-fold-cv.png\">\n<br><br> \n## Brief Description of Overfitting?\nThis is an **error** in the modeling algorithm that takes into consideration random noise in the fitting process rather than the **pattern** itself.  You can see that this occurs when the model gets an **awsome** score in the training set but when we use the test set (Unknown data for the model) we get an **awful** score. This is likely to happen because of overfitting of the data (taking into consideration random noise in our pattern). What we want our model to do is to take the overall pattern of the data in order to correctly classify whether a potential client will suscribe to a term deposit or not. In the examples above, it is most likely that the Decision Tree Classifier and Random Forest classifiers are overfitting since they both give us nearly perfect scores (100% and 99%) accuracy scores. <br><br>\n\n## How can we avoid Overfitting?\nThe best alternative to avoid overfitting is to use **cross validation.** Taking the training test and splitting it. For instance, if we split it by 3, 2\/3 of the data or 66% will be used for training and 1\/3 33% will be used or testing and we will do the testing process three times. This algorithm will iterate through all the training and test sets and the main purpose of this is to grab the overall pattern of the data.","0e782039":"<iframe width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\" src=\"\/\/plot.ly\/~AlexanderBach\/572.embed\"><\/iframe>","a9de15eb":"# Precision and Recall:\n<a id=\"precision_recall\"><\/a>\n**Recall:** Is the total number of \"Yes\" in the label column of the dataset. So how many \"Yes\" labels does our model detect. <br><br>\n**Precision:** Means how sure is the prediction of our model that the actual label is a \"Yes\".\n\n## Recall Precision Tradeoff:\nAs the precision gets higher the recall gets lower and vice versa. For instance, if we increase the precision from 30% to 60% the model is picking the predictions that the model believes is 60% sure. If there is an instance where the model believes that is 58% likely to be a potential client that will suscribe to a term deposit then the model will classify it as a **\"No.\"** However, that instance was actually a **\"Yes\"** (potential client did suscribe to a term deposit.) That is why the higher the precision the more likely the model is to miss instances that are actually a **\"Yes\"!**","8dcbea39":"## Cii. Distribution Plots (Months-Age-Type of Job):\n<a id=\"distribution_plots\"><\/a>\n***\n<img src=\"https:\/\/media.giphy.com\/media\/xTiTnsjKXiCp0uMnsY\/giphy.gif\">\n## Questions to Determine:\n<h3>Months <\/h3>\n<ul>\n<li type='square'> What was the month were the marketing department had the highest amount of offers to potential clients?<\/li>\n<li type='square'> What were the months with the lowest level of offers made from the marketing department?<\/li>\n<li type='square'> Which months had the highest positive ratio [(Accepted - Refused)\/ Total Number of Offers]<\/li>\n<\/ul>\n\n<h3> Age Categories <\/h3> \n<ul>\n<li type='square'> From the age categories, which were the age categories that receive the most and least offers? <\/li>\n<li type='square'> Should we change our strategy? (For instance, should we create a marketing campaign that focuses in the elder or younger segment of the population? <\/li>\n<\/ul>\n\n<h3> Job Status <\/h3>\n<ul>\n<li type='square'> Is there a specific job status that had a significantly higher amount of suscribed term deposits. <\/li>\n<li type='square'> Note: Job status is highly correlated with age categories. (For instance, students are most likely to be individuals from our youngest age category. <\/li>\n<\/ul>\n\n<h3> Marketing Campaigns <\/h3>\n<ul> \n<li type='square'> Which Marketing Campaigns were the most successful? <\/li>\n<li type='square'> What was the probability that potential clients were buying a term deposit  per each of the campaigns? <\/li>\n<li type='square'> What charasteristics made the campaigns that were effective successful? <\/li>\n<\/ul>","03d8b20c":"# References:\n1) Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aur\u00e9lien G\u00e9ron.<br>\n2) Special Thanks to Ahmet Taspinar. (Insights on a handul of functions) Link: http:\/\/ataspinar.com\/2017\/05\/26\/classification-with-scikit-learn\/ <br>\n3) Special Thanks to Randy Lao and his Predicting Employee KernelOver work. Link: https:\/\/www.kaggle.com\/randylaosat\/predicting-employee-kernelover","3d0700f7":"<iframe width=\"800\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"\/\/plot.ly\/~AlexanderBach\/578.embed\"><\/iframe>","8b44472d":"# Bank Marketing DataSet - Intelligent Targeting:\n***\n## Marketing Introduction:\n*The process by which companies create value for customers and build strong customer relationships in order to capture value from customers in return.*\n\n**Kotler and Armstrong (2010).**\n***\n\n<img src=\"https:\/\/media.giphy.com\/media\/l378c04F2fjeZ7vH2\/giphy.gif\">\n\n**Marketing campaigns** are characterized by  focusing on the customer needs and their overall satisfaction. Nevertheless, there are different variables that determine whether a marketing campaign will be successful or not. There are certain variables that we need to take into consideration when making a marketing campaign. <br>\n\n## The 4 Ps:\n1) Segment of the <b>Population:<\/b> To which segment of the population is the marketing campaign going to address and why? This aspect of the marketing campaign is extremely important since it will tell to which part of the population should most likely receive the message of the marketing campaign. <br><br>\n2) Distribution channel to reach the customer's <b>place<\/b>: Implementing the most effective strategy in order to get the most out of this marketing campaign. What segment of the population should we address? Which instrument should we use to get our message out? (Ex: Telephones, Radio, TV, Social Media Etc.)<br><br>\n3) <b> Price:<\/b> What is the best price to offer to potential clients? (In the case of the bank's marketing campaign this is not necessary since the main interest for the bank is for potential clients to open depost accounts in order to make the operative activities of the bank to keep on running.)<br><br>\n4) <b> Promotional<\/b> Strategy: This is the way the strategy  is going to be implemented and how are potential clients going to be address. This should be the last part of the marketing campaign analysis since there has to be an indepth analysis of previous campaigns (If possible) in order to learn from previous mistakes and to determine how to make the marketing campaign much more effective.","bec4d7e6":"## Div. Age of Potential Clients:\n<a id=\"age_clients\"><\/a>\n***\n<img src=\"https:\/\/media.giphy.com\/media\/qAuqPAddvy9uo\/giphy.gif\">\n\n## Age Categories:\n**Note: We will create categories for the range of different type of ages:** <br>\n<ul>\n<li type='square'> <b>20:<\/b> This category which will be an int for plotting purposes will include all the ages ranging from 18-29.<\/li>\n<li type='square'> <b>30:<\/b> This category will include all the ages ranging from 30-39. <\/li>\n<li type='square'> <b>40:<\/b> This category will include all the ages ranging from 40-49. <\/li>\n<li type='square'> <b>50:<\/b> This category will include all the ages ranging from 50-59. <\/li>\n<li type='square'> <b>60:<\/b> This category will include all the ages ranging from 60-95. (95 is our maximum age.)<\/li>\n<\/ul>\n\n## Summary:\n***\n<ul>\n<li type=\"square\"> Most of the potential clients the bank targeted have <b> 30-35 years old. <\/b> <\/li>\n<li type=\"square\"><b>20s and younger:<\/b> Around 60% of potentical clients in this category suscribed to term deposit suscriptions.<\/li>\n<li type=\"square\"><b>30s - 50s:<\/b> Around 40% of the potential clients in this category suscribed to term deposits accounts.<\/li>\n<li type=\"square\"> <b>60s and older:<\/b> Around 76% suscribed term deposits! <\/li>\n<li type=\"square\"> The <b>youngest<\/b> and <b>eldest<\/b> population segments were the most likely to open a term deposit account. <\/li>\n<\/ul>","8a2b9f9c":"# Which Features Influence the Result of a Term Deposit Suscription?\n## DecisionTreeClassifier:\n<a id=\"decision\"><\/a>\nThe top three most important features for our classifier are **Duration (how long it took the conversation between the sales representative and the potential client), contact (number of contacts to the potential client within the same marketing campaign), month (the month of the year).\n\n\n","3a4a1517":"# What Actions should the Bank Consider?\n<a id=\"bank_actions\"><\/a>\n<img src=\"https:\/\/media.giphy.com\/media\/l46Cy1rHbQ92uuLXa\/giphy.gif\">\n\n\n## Solutions for the Next Marketing Campaign (Conclusion):\n1) **Months of Marketing Activity:** We saw that the the month of highest level of marketing activity was the month of **May**. However, this was the month that potential clients tended to reject term deposits offers (Lowest effective rate: -34.49%). For the next marketing campaign, it will be wise for the bank to focus the marketing campaign during the months of **March, September, October and December.** (December should be under consideration because it was the month with the lowest marketing activity, there might be a reason why december is the lowest.)<br><br>\n2) **Seasonality:** Potential clients opted to suscribe term deposits during the seasons of **fall** and **winter**. The next marketing campaign should focus its activity throghout these seasons. <br><br>\n3) **Campaign Calls:** A policy should be implemented that states that no more than 3 calls should be applied to the same potential client in order to save time and effort in getting new potential clients. Remember, the more we call the same potential client, the likely he or she will decline to open a term deposit. <br><br>\n4) **Age Category:** The next marketing campaign of the bank should target potential clients in their 20s or younger and 60s or older. The youngest category had a 60% chance of suscribing to a term deposit while the eldest category had a 76% chance of suscribing to a term deposit. It will be great if for the next campaign the bank addressed these two categories and therefore, increase the likelihood of more term deposits suscriptions. <br><br>\n5) **Occupation:** Not surprisingly, potential clients that were students or retired were the most likely to suscribe to a term deposit. Retired individuals, tend to have more term deposits in order to gain some cash through interest payments. Remember, term deposits are short-term loans in which the individual (in this case the retired person) agrees not to withdraw the cash from the bank until a certain date agreed between the individual and the financial institution. After that time the individual gets its capital back and its interest made on the loan. Retired individuals tend to not spend bigly its cash so they are morelikely to put their cash to work by lending it to the financial institution. Students were the other group that used to suscribe term deposits.<br><br>\n6) **House Loans and Balances:** Potential clients in the low balance and no balance category were more likely to have a house loan than people in the average and high balance category. What does it mean to have a house loan? This means that the potential client has financial compromises to pay back its house loan and thus, there is no cash for he or she to suscribe to a term deposit account. However, we see that potential clients in the average and hih balances are less likely to have a house loan and therefore, more likely to open a term deposit. Lastly, the next marketing campaign should focus on individuals of average and high balances in order to increase the likelihood of suscribing to a term deposit. <br><br>\n\n7) **Develop a Questionaire during the Calls:** Since duration of the call is the feature that most positively correlates with whether a potential client will open a term deposit or not, by providing an interesting questionaire for potential clients during the calls the conversation length might increase. Of course, this does not assure us that the potential client will suscribe to a term deposit! Nevertheless, we don't loose anything by implementing a strategy that will increase the level of engagement of the potential client leading to an increase probability of suscribing to a term deposit, and therefore an increase in effectiveness for the next marketing campaign the bank will excecute. <br><br>\n\nBy combining all these strategies and simplifying the market audience the next campaign should address, it is likely that the next marketing campaign of the bank will be more effective than the current one.\n\n\n\n\n","35786e05":"# Regarding this Kernel:\nI know this is a well known dataset since it comes from <b> UCI Machine Learning Repository<\/b>. However, I believe there are some interesting insights you could see that you could integrate to your own data analysis. All in all, Kaggle is meant to learn from others and I hope this example suits you well. <br><br>\n<b>Please feel free to use this kernel to your projects it will be my pleasure!<\/b><br><br>\nAlso, I'm open to new ideas and things that I could improve to make this kernel even better! Open to constructie criticisms!\nLastly, I will like to give a special thanks to **Randy Lao** and his well-known **Predicting Employee Kernelover**. His kernel gave me different ideas as to how should I approach an analysis of a dataset.<br><br>\nAlso, I want to give credit to this stackoverflow post, which helped me change the name of legends from Facetgrids. <br>\nhttps:\/\/stackoverflow.com\/questions\/45201514\/edit-seaborn-plot-figure-legend <br>\nCheck it out if you are struggling with the same problem.\n\n# Regarding Plotly:\nAt the end of each plotly graph, you will notice that the iplot command has been commented out. The reason behind this is that currently for some reason the graphs are not displaying maybe it is a javascript issue. Nevertheless, I will be looking closely at this issue to see when it gets fixed. What I did in this case, is to add a tag from my plotly account in order to show the graph to you guys. Sorry for the inconvenience.","b51149ed":"# Dii. Seasonality:\n<a id=\"seasonality\"><\/a>\n***\n<img src=\"https:\/\/media.giphy.com\/media\/7wqLOqgtjjWO4\/giphy.gif\">\n## Marketing Activity in each of the Seasons:\nIn this section, we will just visualize how was the marketing activity during each of the seasons and how effective were the efforts of making people suscribing term deposits. Nevertheless, this section is **not that different from the previous one** since the only thing we are doing is creating four types of seasons(spring, summer, fall and winter). Then we will just visualize the level of activity and its effectiveness per seasonality (This should not be that different from the monthly analysis however, it should be simpler to grasp during which season should the bank focus its marketing activity.)\n\n## Summary:\n***\n<ul>\n<li type=\"square\"> Surprisingly individuals opted to suscribe more term deposits accounts during the season of <b> fall<\/b> and <b>winter<\/b>. <\/li>\n<li type=\"square\"> The Marketing team of the bank looks like they have focused their efforts to make people suscribe term deposits accounts during the seasons of <b> spring<\/b> and <b>summer.<\/b> <\/li>\n<\/ul>","05a02038":"# Classification Models:\n<a id=\"models\"><\/a>","300c6094":"## Diii. Number of Calls in this Campaign:<br>\n<a id=\"number_calls\"><\/a>\n## Campaign Calls:\nThis are the number of calls made to a potential clients within the same marketing campaign. Obviously, the **more the calls we make to a potential client in a shorter period of time, the more irritated the potential client will be** and thus, a higher level of probability for the potential client to refuse suscribing a term deposit.\n\n## Summary:\n***\n<ul>\n<li type=\"square\"> The more <b> the calls <\/b> to potential clients within the same marketing campaign, the more likely potential clients will refuse to open term deposits with the bank. <\/li>\n<li type=\"square\"> When a potential client was called more than <b>five times<\/b> the likelihood that he or she will accept in suscribing to a term deposit diminishes. <\/li>\n<li type=\"square\"> After the <b>third call<\/b> the probability that a potential client will <b> refuse to suscribe a term deposit <\/b> increases drastically (by approx. 5%). <\/li>\n<\/ul>","e720e3ba":"<iframe width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\" src=\"\/\/plot.ly\/~AlexanderBach\/576.embed\"><\/iframe>","d2298c6a":"<img src=\"https:\/\/www.mortgagechoice.com.au\/media\/3411404\/term-deposit.jpg\" width=1200 height=400>","1f4058e0":"<iframe width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\" src=\"\/\/plot.ly\/~AlexanderBach\/582.embed\"><\/iframe>","eb9e1963":"<iframe width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\" src=\"\/\/plot.ly\/~AlexanderBach\/574.embed\"><\/iframe>","0c4a8eb4":"## GradientBoosting Classifier Wins!\nGradient Boosting classifier is the best model to predict whether or not a **potential client** will suscribe to a term deposit or not.  84% accuracy!","b637fcfe":"Fortunately, there are no <b>missing values<\/b>. If there were missing values we will have to fill them with the median, mean or mode. I tend to use the median but in this scenario there is no need to fill any missing values.","dda385df":"## Eii. Balance Categories Vs Housing Loans:\n<a id=\"balance_housing\"><\/a>\n<img src=\"https:\/\/media.giphy.com\/media\/UGyQwm7NyM3ny\/giphy.gif\" height=400 width=400>\n\n## Description:\nWhat are we analyzing h in this section? Wouldn't it be interesting to see if the amount of balance of a potential client influences highly in whether a individual will have a house loan. We have seen in the correlation matrix, that having a house loan influences negatively in the decision of a potential client for suscribing to a term deposit. \n\n## Types of Balances: \nOne of the positive correlation that I find interesting to analyze in our correlation matrix is the balance of potential clients. Balance has a 0.08 (8%) positive correlation in whether a potential clients will suscribe a term deposit. In order to make things simplier we should **split the different balances into several categories.** The categories will be the following: <br><br>\n\n**a) No Balance**: This category will be the potential clients who do not have a balance or have a negative balance. This potential clients should not be that much of an interest to the bank, since at the end the banks are also looking for people who have balance in their accounts for investment reasons. <br><br>\n**b) Low Balance**: For this category, potential clients will have a balance greater than 0 but lower than 1,000.<br><br>\n**c) Average Balance**: For this category, potential clients will have a balance greater than 1,000 but lower than 5,000<br><br>\n**d) High Balance**: For this category potential clients will have a balance greater than 5,000.\n\n## Our Aim: \nSince balance has a positive correlation with whether a potential client will suscribe a term deposit, we should emphasize our marketing strategy on the people who are more likely to have a higher amount of balance. We will discover this in our analysis. In this case we will see from the **age categories** we analyzed previously, which of these categories tend to have a high balance and which of these categories is likely to suscribe a term deposit.\n\n## Summary:\n***\n<li type=\"square\"> The marketing campaign is targeting excessively potential clients from the <b>low balance segment.<\/b> (Remember the lower the balance, the more likely the potential client will refuse to suscribe a term deposit.) <\/li> \n<li type=\"square\"> We can see that in all the categories of age, our focus is on the <b>low balance category<\/b> or <b>no balance.<\/b><\/li> \n<li type=\"square\">The marketing campaign targets fewer people who have an <b>average balance<\/b> and <b>high balance.<\/b><\/li>\n<li type=\"square\"> People with no balance and low balance were more likely to have a <b>house loan<\/b><\/li>","33753b43":"# What is a Term Deposit? \nA **Term deposit** is a deposit that a bank or a financial institurion offers with a fixed rate (often better than just opening deposit account) in which your money will be returned back at a specific maturity time. For more information with regards to Term Deposits please click on this link from Investopedia:  https:\/\/www.investopedia.com\/terms\/t\/termdeposit.asp","2b5e1558":"# Classification Model:\n<a id=\"classification_model\">\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1280\/1*Jx3X4pD0KdiP8QDhi3d4Og.png\">","e540ef60":"## Ci. Accepted vs Rejected Term Deposits:\n<a id=\"accepted_rejected\"><\/a>\n***\n## Questions to Determine:\n<ul>\n<li type='square'> What <b>percentage(%)<\/b> of potential clients accepted to suscribe to term deposits vs refused to suscribe term deposits. <\/li>\n<li type='square'> Is there a huge difference between clients that <b>suscribed term deposits vs refused deposits?<\/b><\/li>\n<\/ul>\n\n## Summary:\n<ul>\n<li type='square'> <b>52.6% refused to suscribe to term deposits<\/b> while <b>47.4% accepted to suscribe term deposits.<\/b> Our labels are sort of equally distributed. <\/li>\n<\/ul>\n\n## What Next?\n<ul>\n<li type='square'> We should ask ourselves the following <b>question:<\/b> Is it possible to detect patterns that could tell us a better story about what factors influence potential clients to either <b>accept or refuse to buy term deposits?<\/b><\/li>\n<li type='square'> The next step of the analysis will focus on <b>three pillars: month, age, job status <\/b><\/li>\n<\/ul>","c3c82969":"## Stratified Sampling: <br>\n\n<a id=\"stratified\"><\/a>\n**Stratified Sampling:** Is an important concept that is often missed when developing a model either for regression or classification. Remember, that in order to avoid overfitting of our data we must implement a cross validation however, we must make sure that at least the features that have the greatest influence on our label (whether a potential client will open a term deposit or not) is **equally distributed**. What do I mean by this? <br><br>\n\n**Personal Loans:**<br>\nFor instance, having a personal loan is an important feature that determines whether a potential client will open a term deposit or not. To confirm it has a heavy weight on the final output you can check the correlation matrix above and you can see it has a -11% correlation with opening a deposit. What steps we should take before implementing stratified sampling in our train and test data?<br>\n1) We need to see how our data is distributed. <br>\n2) After noticiing that the column of loan contains 87% of \"no\" (Does not have personal loans) and 13% of \"yes\" (Have personal loans.) <br>\n3) We want to make sure that our training and test set contains the same ratio of 87% \"no\" and 13% \"yes\".\"\n**Stratified Sampling:** Is an important concept that is often missed when developing a model either for regression or classification. Remember, that in order to avoid overfitting of our data we must implement a cross validation however, we must make sure that at least the features that have the greatest influence on our label (whether a potential client will open a term deposit or not) is **equally distributed**. What do I mean by this? <br><br>\n\n**Personal Loans:**<br>\nFor instance, having a personal loan is an important feature that determines whether a potential client will open a term deposit or not. To confirm it has a heavy weight on the final output you can check the correlation matrix above and you can see it has a -11% correlation with opening a deposit. What steps we should take before implementing stratified sampling in our train and test data?<br>\n1) We need to see how our data is distributed. <br>\n2) After noticiing that the column of loan contains 87% of \"no\" (Does not have personal loans) and 13% of \"yes\" (Have personal loans.) <br>\n3) We want to make sure that our training and test set contains the same ratio of 87% \"no\" and 13% \"yes\"."}}