{"cell_type":{"807b82a7":"code","a4dc3756":"code","81ec7fe5":"code","e736ba38":"code","a817b67a":"code","cc231f53":"code","badb750f":"code","f1b15ab2":"code","a8443da1":"code","58c53f6c":"code","2baafa1b":"code","97108396":"code","0196b704":"code","5ecd2abc":"code","0be9a84c":"code","ca8f3db1":"code","4bb9ab32":"code","01e1ecba":"code","ec2aee29":"code","da9de38d":"code","79e9db6e":"code","297c2a5e":"code","f8d8dab8":"code","98987ea7":"code","b7400e36":"code","b8b1ec91":"code","85927c9e":"code","43a256e0":"code","6365a9cc":"code","54ca819c":"code","e15c77c8":"code","d17d2720":"code","d3aa6530":"code","e9379fcf":"code","6d9de747":"code","b1bb480e":"code","9a79cf4c":"code","1c7a1137":"code","19365407":"code","a4b1786c":"code","9279edc9":"code","69f5e82b":"code","d0bb3b6a":"code","642d33a0":"code","d913b457":"code","dcf578ba":"markdown","9a55c4b5":"markdown","965df2fa":"markdown","2a35960d":"markdown","1cfe00cc":"markdown","b23ee20c":"markdown","fd90850a":"markdown","02f7fea9":"markdown","9850a428":"markdown","76127fa8":"markdown"},"source":{"807b82a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4dc3756":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport time\nfrom subprocess import check_output\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder  \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.preprocessing import  StandardScaler\n\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score","81ec7fe5":"# check sample file\n\nsample_df = pd.read_csv(\"..\/input\/applied-ml-at-nida\/sample-solution.csv\")\n\nsample_df","e736ba38":"submission = sample_df[['Id','target']]\n\nsubmission.to_csv(\"submission_sp.csv\", index=False)\n\nsubmission.tail()","a817b67a":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/applied-ml-at-nida\/train-dataset.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/applied-ml-at-nida\/test-dataset.csv\")\n\ntrain_df","cc231f53":"# get data contain only heart disease patient\nmydf = train_df[train_df['target']==1]\nmydf","badb750f":"# plot histogram to check feature that behave like classifier\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15,15]\nplt.rcParams['hist.bins'] = [1,2]\nmydf.hist();","f1b15ab2":"train_df.loc[(train_df['oldpeak']< 0.5),'oldpeak'] = 0\ntrain_df.loc[(train_df['oldpeak']>= 0.5) & (train_df['oldpeak']< 1.5) ,'oldpeak'] = 1\n\ntrain_df.loc[(train_df['oldpeak']>= 1.5)  ,'oldpeak'] = 2\n\ntrain_df.loc[(train_df['cp']== 2) ,'cp'] = 1\n","a8443da1":"test_df.loc[(test_df['oldpeak']< 0.5),'oldpeak'] = 0\ntest_df.loc[(test_df['oldpeak']>= 0.5) & (test_df['oldpeak']< 1.5) ,'oldpeak'] = 1\n\ntest_df.loc[(test_df['oldpeak']>= 1.5)  ,'oldpeak'] = 2\n\ntest_df.loc[(test_df['cp']== 2) ,'cp'] = 1","58c53f6c":"# get other data\nmydf2 = train_df[train_df['target']==0]\nmydf2","2baafa1b":"mydf2.hist();","97108396":"cpdf = mydf[train_df['cp']==0]\ncpdf","0196b704":"cpdf.hist();","5ecd2abc":"cpdf2 = mydf[train_df['cp']==1]\ncpdf2","0be9a84c":"\ncpdf2.hist();","ca8f3db1":"cpdf3 = mydf[train_df['cp']==2]\ncpdf3","4bb9ab32":"cpdf3.hist();","01e1ecba":"cpdf4 = mydf[train_df['cp']==3]\ncpdf4","ec2aee29":"cpdf4.hist();","da9de38d":"# add bias column base on features that have high effect on output 1\ntrain_df['b1'] = 0\ntrain_df.loc[  (train_df['thal']==2)& (train_df['exang']==0)& (train_df['oldpeak']<0.5)&(train_df['slope']==2)& (train_df['cp']!= 0)& (train_df['ca']== 0),'b1']=1\n#train_df.loc[  (train_df['thal']==2)& (train_df['exang']==0)& (train_df['oldpeak']<1)&(train_df['slope']==2)& (train_df['cp']== 1),'b1']=1\ntrain_df.loc[  (train_df['thal']==2) & (train_df['slope']==2)& (train_df['ca']== 0) & (train_df['chol']<300),'b1']=1\ntrain_df","79e9db6e":"test_df['b1'] = 0\n\n#test_df.loc[  (test_df['thal']==2)& (test_df['exang']==0)& (test_df['oldpeak']<1)&(test_df['slope']==2)& (test_df['cp']== 1),'b1']=1\ntest_df.loc[  (test_df['thal']==2)& (test_df['exang']==0)& (test_df['oldpeak']<0.5)&(test_df['slope']==2)& (test_df['cp']!= 0)& (test_df['ca']== 0),'b1']=1\n\ntest_df.loc[  (test_df['thal']==2) &(test_df['slope']==2)& (test_df['ca']== 0)&(test_df['chol']<300),'b1']=1\ntest_df","297c2a5e":"# plot heatmap to check corelation\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(train_df.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)","f8d8dab8":"# not use\ndrop_list = []\n\nadd_list = ['age','trestbps','chol','fbs','restecg'] #,'emp.var.rate',\n \nfor col in xtrain.columns:\n    if col not in add_list:\n        drop_list += [col]\nprint(drop_list)\n\ntest_df = test_df.drop(drop_list, axis=1)\nxtrain = xtrain.drop(drop_list, axis=1)","98987ea7":"# Feature Testing\n# drop low corelation features\nytrain = train_df['target']\nxtrain = train_df.drop(['target'],axis=1)\n#drop_list = ['age','trestbps','chol','fbs','restecg']\ndrop_list = ['fbs','restecg','trestbps']\nxtest_df = test_df.drop(drop_list, axis=1)\nxtrain = xtrain.drop(drop_list, axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(xtrain,ytrain , test_size=0.33, random_state=42)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\ntest_df_sc = scaler.fit_transform(xtest_df)","b7400e36":"#RandomForest\n\nclf_rf = RandomForestClassifier(random_state=42)   \n   \nclr_rf = clf_rf.fit(x_train,y_train)\n\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nauc = roc_auc_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\nprint('AUC is: ',auc)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\n","b8b1ec91":"# KNN\nn = 0\nd_auc = 0\nfor i in range(1,100):\n\n    knn = KNeighborsClassifier(n_neighbors = i,weights='distance', metric = 'manhattan') #weight uniform give better auc but loss in acc\n    knn.fit(x_train, y_train)\n\n    ac = accuracy_score(y_test,knn.predict(x_test))\n    auc = roc_auc_score(y_test,knn.predict(x_test))\n    if auc > d_auc:\n        d_auc = auc\n        n = i\nprint('Accuracy is: ',ac)\nprint('AUC is: ',d_auc)\nprint(n)\ncm = confusion_matrix(y_test,knn.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","85927c9e":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\n\nac = accuracy_score(y_test,logreg.predict(x_test))\nauc = roc_auc_score(y_test,logreg.predict(x_test))\nprint('Accuracy is: ',ac)\nprint('AUC is: ',auc)","43a256e0":"# Support Vector Machine\nsvc = SVC()\nsvc.fit(x_train, y_train)\n\nac = accuracy_score(y_test,svc.predict(x_test))\nauc = roc_auc_score(y_test,svc.predict(x_test))\nprint('Accuracy is: ',ac)\nprint('AUC is: ',auc)","6365a9cc":"gaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\n\nac = accuracy_score(y_test,gaussian.predict(x_test))\nauc = roc_auc_score(y_test,gaussian.predict(x_test))\nprint('Accuracy is: ',ac)\nprint('AUC is: ',auc)","54ca819c":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\n\nac = accuracy_score(y_test,sgd.predict(x_test))\nauc = roc_auc_score(y_test,sgd.predict(x_test))\nprint('Accuracy is: ',ac)\nprint('AUC is: ',auc)","e15c77c8":"# Decision Tree\ndct = DecisionTreeClassifier()\ndct.fit(x_train, y_train)\n\nac = accuracy_score(y_test,dct.predict(x_test))\nauc = roc_auc_score(y_test,dct.predict(x_test))\nprint('Accuracy is: ',ac)\nprint('AUC is: ',auc)","d17d2720":"# Tensor Flow\nN, D= x_train.shape\n\nmodel = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(D,)),\n\n        tf.keras.layers.Dense(1, activation='hard_sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=['AUC'])\n\n#change metrics to 'AUC'\n\n#train model\nr = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=240)\n\n#evaluate model\nprint(\"Train score:\", model.evaluate(x_train, y_train))\nprint(\"Test score:\",model.evaluate(x_test,y_test))","d3aa6530":"\nr = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=240)\n\n#evaluate model\nprint(\"Train score:\", model.evaluate(x_train, y_train))\nprint(\"Test score:\",model.evaluate(x_test,y_test))","e9379fcf":"import matplotlib.pyplot as plt\nplt.plot(r.history['loss'],label='loss')\nplt.plot(r.history['val_loss'],label='val_loss')\nplt.legend()","6d9de747":"plt.plot(r.history['auc'],label='auc')\nplt.plot(r.history['val_auc'],label='val_auc')\nplt.legend()","b1bb480e":"# TF\n\nK = model.predict(test_df_sc)\n\nK = np.round(K,8).flatten()\n\n\n\ns_df = test_df\ns_df['target'] = K\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_TF_Bias.csv\", index=False)\n\nsubmission.tail()","9a79cf4c":"# RF\nK = clf_rf.predict_proba(test_df_sc)\n\nPP = []\nfor i in K:\n    PP += [i[1]]\nPP\n\n\ns_df = test_df\ns_df['target'] = PP\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_RF_Bias.csv\", index=False)\n\nsubmission.head()","1c7a1137":"# KNN  0.9625\nK = knn.predict_proba(test_df_sc)\n\nPP = []\nfor i in K:\n    PP += [i[1]]\nPP\n\n\ns_df = test_df\ns_df['target'] = PP\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_knn_Bias.csv\", index=False)\n\nsubmission.tail()","19365407":"K = logreg.predict_proba(test_df_sc)\nPP = []\nfor i in K:\n    PP += [i[1]]\nPP\ns_df = test_df\ns_df['target'] = PP\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_LR_Bias.csv\", index=False)\n\nsubmission.tail()","a4b1786c":"K = gaussian.predict_proba(test_df_sc)\nPP = []\nfor i in K:\n    PP += [i[1]]\nPP\n\ns_df = test_df\ns_df['target'] = PP\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_TFMLNB.csv\", index=False)\n\nsubmission.tail()","9279edc9":"K = sgd.predict(test_df_sc)\n\n\ns_df = test_df\ns_df['target'] = K\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_sgd.csv\", index=False)\n\nsubmission.tail()","69f5e82b":"K = model.predict(test_df_sc)\n\nK = np.round(K,10).flatten()\n\n\n\ns_df = test_df\ns_df['target'] = K\ns_df[\"id\"] = s_df.index + 1\nsubmission = s_df[['id','target']]\n\nsubmission.to_csv(\"submission_TFMLL.csv\", index=False)\n\nsubmission.tail()","d0bb3b6a":"test_df = test_df.drop(['target'],axis=1)\ntest_df = test_df.drop(['id'],axis=1)","642d33a0":"\ntest_df_sc = test_df_sc.drop(['target'],axis=1)\ntest_df_sc = test_df_sc.drop(['id'],axis=1)","d913b457":"test_df","dcf578ba":"g = sns.FacetGrid(train_df, col='target')\ng.map(plt.hist, 'b1', bins=20)","9a55c4b5":"# Build Model","965df2fa":"# we can see features that seperate heart disease patient from other is\n- chest pain\n- slope\n- ca\n- oldpeak\n- exang\n- thal","2a35960d":"# **EDA**","1cfe00cc":"train_df","b23ee20c":"g = sns.FacetGrid(train_df, col='target')\ng.map(plt.hist, 'age', bins=20)","fd90850a":"# Team: SA\n\nMember:\n\n1.Sarachakorn Apinyanun 6220412017\n\n2.Wisaram Khumgasem 6220412030\n\n3.Phoomiphat Charuensab 6220412027\n\n4.Nuthapon Suwanpotipra 6220412028","02f7fea9":"train_df[['age', 'target']].groupby(['age'], as_index=False).mean().sort_values(by='target', ascending=False)","9850a428":"# Compare heart disease patients base on chest pain","76127fa8":"x_train, x_test, y_train, y_test = train_test_split(xtrain,ytrain , test_size=0.33, random_state=42)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\ntest_df_sc = scaler.fit_transform(test_df)"}}