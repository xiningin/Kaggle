{"cell_type":{"20b9177f":"code","e8aae4f1":"code","93daaf47":"code","120a6fac":"code","8cbb2071":"code","7044028d":"code","9d2e1274":"code","ba379011":"code","253ef6e3":"code","c05dac9d":"code","8f3416f6":"code","e32d4782":"code","fd10609a":"code","da22b7b2":"code","3be6a44b":"code","b6dfc067":"code","4846e2fa":"code","a7d1d089":"code","a1fc4578":"code","53e69a53":"code","53caf81a":"code","809596b2":"code","5f8b5a07":"code","2f569994":"code","15b609c8":"code","48e520b3":"code","d967fc9d":"code","7a878c35":"code","e27113ee":"code","8ab440a3":"code","c50b0b4a":"code","5e10f7d0":"code","5fa4e4be":"code","04bc5816":"code","06d39cf7":"code","7669326a":"code","64510075":"code","a3c41caa":"code","c4853db9":"code","f662ed09":"code","0b8b12e1":"code","4519ae05":"code","41d3a001":"code","16f64a22":"code","17149076":"code","1b37b904":"code","141a99f0":"code","79962a45":"code","24c48c70":"code","7aa8e5dc":"code","5f9c60af":"code","480f2b4e":"code","518c9af7":"code","bc6e8c73":"code","67194581":"code","8bb74149":"code","b815b034":"code","da784ca7":"markdown","3c394370":"markdown","d47bbe67":"markdown","c3f3a825":"markdown","f437419a":"markdown","37e45f09":"markdown","bd15b0cc":"markdown","f493851a":"markdown","a798f7bd":"markdown","c6df2efa":"markdown","bd599ded":"markdown","5ebf85b1":"markdown","c28dde95":"markdown"},"source":{"20b9177f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('.\/'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8aae4f1":"#!python -m pip install --upgrade pip","93daaf47":"#!pip install librosa","120a6fac":"#!add-apt-repository -y ppa:mscore-ubuntu\/mscore-stable\n#!apt install -y musescore","8cbb2071":"!apt-get install -y lilypond\n#!add-apt-repository -y ppa:mscore-ubuntu\/mscore3-stable\n#!apt install -y musescore3","7044028d":"!pip install music21","9d2e1274":"import pandas as pd\nimport seaborn as sn\nimport librosa\nimport re\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom scipy.io.wavfile import read\nimport sys\nsys.path.append('\/path\/to\/ffmpeg')\nimport pydub \nimport os\nimport re\n\nfrom numpy import array\nimport numpy as np\nfrom numpy import zeros\nfrom numpy import argmax\n\nimport math\n\n\nfrom shutil import copyfile\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nfrom sklearn.cluster import KMeans\n\n\nimport random\nfrom random import randint\nfrom datetime import datetime\n\nnp.set_printoptions(threshold=50)\n\n# load sample\nimport sklearn\nimport librosa\nfrom scipy import signal\nimport pickle\n\n\nimport plotly.express as px\nimport numpy as np","ba379011":"import cv2\nfrom shutil import copyfile","253ef6e3":"import re\nimport csv\n\nfrom IPython.display import Image, display\n\nfrom pathlib import Path\nfrom music21.converter.subConverters import ConverterMusicXML\nfrom music21.converter.subConverters import ConverterLilypond","c05dac9d":"import xml.etree.ElementTree as ET\nfrom music21 import instrument, converter, note, chord, stream, duration, tempo, meter\nimport music21","8f3416f6":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import * ","e32d4782":"#from subprocess import check_output\n#print(check_output([\"ls\", \"-la\",\"..\/..\/usr\/bin\/musescore3\"]).decode(\"utf8\"))","fd10609a":"#music21.environment.set('musescoreDirectPNGPath', 'C:\\\\Program Files\\\\MuseScore 3\\\\bin\\\\MuseScore3.exe')\n#music21.environment.set('musicxmlPath', 'C:\\\\Program Files\\\\MuseScore 3\\\\bin\\\\MuseScore3.exe')","da22b7b2":"#!whereis mscore","3be6a44b":"\n#music21.environment.set('pdfPath', '\/usr\/bin\/musescore3')\n#music21.environment.set('graphicsPath', '\/usr\/bin\/musescore3')\n#music21.environment.set('musescoreDirectPNGPath', '\/usr\/bin\/musescore3')\n#music21.environment.set('musicxmlPath', '\/usr\/bin\/musescore3')","b6dfc067":"#music21.configure.run()","4846e2fa":"batch_size_matrix = 45\nnEpocas = 200\n\nimg_height = 300\nimg_width = 800\ncheckpoint_filepath=\"weights-improvement-00-0000-bigger.hdf5\"\n","a7d1d089":"def int_to_note(integer):\n    # convert pitch value to the note which is a letter form. \n    note_base_name = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n    octave_detector = (integer \/\/ 12) \n    base_name_detector = (integer % 12) \n    note = note_base_name[base_name_detector] + str((int(octave_detector))-1)\n    if ('-' in note):\n        note = note_base_name[base_name_detector] + str(0)\n        return note\n    return note","a1fc4578":"def note_to_int(note): # converts the note's letter to pitch value which is integer form.\n    # source: https:\/\/musescore.org\/en\/plugin-development\/note-pitch-values\n    # idea: https:\/\/github.com\/bspaans\/python-mingus\/blob\/master\/mingus\/core\/notes.py\n    #print(note)\n    note_base_name = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n    if isinstance(note, int) :\n        value = note\n    elif ('#-' in note):\n        first_letter = note[0]\n        base_value = note_base_name.index(first_letter)\n        octave = note[3]\n        value = base_value + 12*(int(octave)-(-1))\n        \n    elif ('#' in note): \n        first_letter = note[0]\n        base_value = note_base_name.index(first_letter)\n        octave = note[2]\n        value = base_value + 12*(int(octave)-(-1))\n        \n    elif ('-' in note): \n        first_letter = note[0]\n        base_value = note_base_name.index(first_letter)\n        octave = note[2]\n        value = base_value + 12*(int(octave)-(-1))\n        \n    else:\n        first_letter = note[0]\n        base_val = note_base_name.index(first_letter)\n        octave = note[1]\n        value = base_val + 12*(int(octave)-(-1))\n    return value","53e69a53":"def midiConverter(path, file):\n    newFile  = re.sub(r\"[^0-9a-zA-Z]+\", \"\", file[:-4]) +'.mid'\n    os.rename(os.path.join(path, file),os.path.join(path, newFile))\n    file = newFile\n    \n    csvReport = os.path.join(path, file[:-4]+'\/report.csv')\n    \n    midi = converter.parse(os.path.join(path, file))\n    s2 = instrument.partitionByInstrument(midi)\n    resultado = []\n    \n    fieldnames = ['file', 'path', 'pathImage', 'instrument', 'nota', 'durationNote', 'offset']\n    \n    print(csvReport)\n    Path(os.path.join(path, file[:-4])).mkdir(parents=True, exist_ok=True)\n    \n    with open( csvReport , mode='w', encoding='UTF8', newline='') as report_file:\n        writer = csv.DictWriter(report_file, fieldnames=fieldnames)\n        writer.writeheader()\n        \n    for part in s2.parts:\n        incAux = 0\n        tempoAux = 0.0\n        if part.getInstrument(returnDefault=False).instrumentName == None:\n            part.getInstrument(returnDefault=False).instrumentName = \"teste\"\n        instrumentPath = str(os.path.join(path, file).replace('.mid','')+'\/'+re.sub(r\"[^0-9a-zA-Z]+\", \"\", part.getInstrument(returnDefault=False).instrumentName))\n        print('convers\u00e3o: ',instrumentPath)\n        Path(instrumentPath).mkdir(parents=True, exist_ok=True)\n                                \n            \n        result = [[]]\n        musicStream = stream.Stream()\n        durationIntAux = None\n        try:\n            notes_to_parse = part.recurse() \n            \n            for element in notes_to_parse:\n                if isinstance(element, meter.TimeSignature):\n                    timeSignatureMeter = element\n                    timeSignature = float(element.numerator)\n                    tempoAux = float(element.numerator)\n                    break\n                    \n            for element in notes_to_parse:\n                \n                #print('element:',element)\n                objeto = None\n                if isinstance(element, note.Note):\n                    #nota = note_to_int(str(element.pitch))\n                    nota = str(element.pitch)\n                    try:\n                        durationNote = float(element.duration.quarterLength)\n                    except:\n                        vetAux = str(element.duration.quarterLengths).split(\"\/\")\n                        durationNote = float(vetAux[0])\/float(vetAux[len(vetAux)-1])\n                    \n                    try:\n                        offset = float(element.offset)\n                    except:\n                        offset = str(element.offset).split(\"\/\")\n                        offset = float(vetAux[0])\/float(vetAux[len(vetAux)-1])\n                        \n                    objeto = [nota, durationNote, offset]\n                    \n                \n                #elif isinstance(element, chord.Chord):\n                #    nota = '.'.join(str(n) for n in element.normalOrder)\n                #    try:\n                #        durationNote = float(element.duration.quarterLength)\n                #    except:\n                #        vetAux = str(element.duration.quarterLengths).split(\"\/\")\n                #        durationNote = float(vetAux[0])\/float(vetAux[len(vetAux)-1])\n                #    \n                #    \n                #    try:\n                #        offset = float(element.offset)\n                #    except:\n                #        offset = str(element.offset).split(\"\/\")\n                #        offset = float(vetAux[0])\/float(vetAux[len(vetAux)-1])\n                #        \n                #    objeto = [nota, durationNote, offset]\n                \n                #elif isinstance(element, note.Rest):\n                #    try:\n                #        durationNote = float(element.duration.quarterLength)\n                #    except:\n                #        vetAux = str(element.duration.quarterLengths).split(\"\/\")\n                #        durationNote = float(vetAux[0])\/float(vetAux[len(vetAux)-1])\n                #    \n                #    \n                #    try:\n                #        offset = float(element.offset)\n                #    except:\n                #        offset = str(element.offset).split(\"\/\")\n                #        offset = float(vetAux[0])\/float(vetAux[len(vetAux)-1])\n                #        \n                #    objeto = [None, durationNote, offset]\n                \n                elif isinstance(element, meter.TimeSignature):\n                    timeSignatureMeter = element\n                    timeSignature = float(element.numerator)\n                    #print(\"elif isinstance(element, meter.TimeSignature):\")\n            \n                #elif isinstance(element, duration.GraceDuration):\n                #    element.duration.quarterLength = 0.5\n                    \n                #elif isinstance(element, tempo.MetronomeMark):\n                #    print(element.secondsPerQuarter)\n\n                #else:\n                #    print(element)\n                \n                #print(tempoAux)\n                \n                if objeto!=None:\n                    \n                    while tempoAux <= objeto[2]:\n                        if len(musicStream) != 0:\n                            try:\n                                conv = converter.subConverters.ConverterLilypond()\n                                conv.write(musicStream, fmt='lilypond', fp=instrumentPath+ '\/'+ str(incAux+1), subformats=['png'])\n                            \n                            except Exception as ezinho: \n                                print(ezinho)\n\n                        incAux += 1\n                        tempoAux += timeSignature\n                        result.append([])\n                        musicStream = stream.Stream()\n                        musicStream.append(timeSignatureMeter)\n\n                    if tempoAux > objeto[2]:\n                        result[-1].append(objeto)\n                        musicStream.append(element)\n                        \n                        #if os.path.isfile(instrumentPath+ '\/'+ str(incAux) + '.png'):\n                        with open( csvReport , mode='a', encoding='UTF8', newline='') as report_file:\n                            writer = csv.DictWriter(report_file, fieldnames=fieldnames)\n                            writer.writerows([{'file': file,\n                                                        'path': os.path.join(path, file),\n                                                        'pathImage': instrumentPath+ '\/'+ str(incAux+1) + '.png',\n                                                        'instrument': part.getInstrument(returnDefault=False).instrumentName, \n                                                        'nota': objeto[0], \n                                                        'durationNote': objeto[1],\n                                                        'offset': objeto[2]}])\n                                \n                                \n            if len(os.listdir(instrumentPath)) == 0:\n                os.rmdir(instrumentPath)\n                    \n            #print(\"terminou\")\n            if result != [[]]:\n                print(\"OK \"+part.getInstrument(returnDefault=False).instrumentName+'. Convertido com sucesso\\n')\n            else:\n                print(\"N\u00e3o havia informa\u00e7\u00f5es no instrumento:\", +part.getInstrument(returnDefault=False).instrumentName)\n                \n        except Exception as e: \n            print(\"*************************************\")\n            print(\"fail,\"+part.getInstrument(returnDefault=False).instrumentName+','+path+'\\n')\n            print(e, element)\n            print(\"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\n        finally:\n            print()\n\n    return","53caf81a":"\nimport zipfile\n\nwith zipfile.ZipFile('..\/input\/midi-classic-music\/midiclassics.zip', 'r') as zip_ref:\n    zip_ref.extractall('\/kaggle\/temp\/')\n","809596b2":"\nfor dirname, _, filenames in os.walk('\/kaggle\/temp\/'):\n    pastaFutura = dirname.replace('\/kaggle\/temp\/','.\/')\n    Path(pastaFutura).mkdir(parents=True, exist_ok=True)\n    for filename in filenames:\n        if filename.endswith(\".mid\"):\n            copyfile(os.path.join(dirname, filename), os.path.join(pastaFutura, filename))\n          ","5f8b5a07":"pasta = '.\/Vivaldi'\nfileMid = 'Concerto Viola d\\'Amore and Lute RV540.mid'\n\n#pasta = '.\/Beethoven'\n#fileMid = 'Andante in F Major.mid'\nmidiConverter(pasta, fileMid)","2f569994":"\ndf = pd.read_csv(os.path.join(pasta, re.sub(r\"[^0-9a-zA-Z]+\", \"\", fileMid[:-4]) +'\/report.csv'))","15b609c8":"df = df.loc[[os.path.isfile(i) for i in df['pathImage']]]","48e520b3":"df.describe()","d967fc9d":"df.instrument.unique()","7a878c35":"allInstruments = [df.loc[(df.instrument == instrumento)].pathImage.unique() for instrumento in df.instrument.unique()]","e27113ee":"dataset = [df.loc[(df.pathImage == instrument)] for instrument in [item for sublist in allInstruments for item in sublist]]","8ab440a3":"for i in dataset[:4]:\n    display(Image(i.pathImage.unique()[0]))\n    print('Path:   ',i.pathImage.unique()[0]) \n    print('Nota:   ',i.nota.values)\n    print('Duracao:',i.durationNote.values)\n    print('Offset: ',i.offset.values)","c50b0b4a":"\nY = list()\nX = list()\nXdata = list()\n\nindAux = 0","5e10f7d0":"allWords = list(set([int_to_note(i) for i in range(300)]))","5fa4e4be":"NOTA_TO_INT = dict((c, i) for i, c in enumerate(allWords))\nINT_TO_NOTA = {v: k for k, v in NOTA_TO_INT.items()}","04bc5816":"char_to_int = {chr(v): v for k, v in NOTA_TO_INT.items()}\nint_to_char = {v: chr(v) for k, v in NOTA_TO_INT.items()}\nlen(char_to_int)","06d39cf7":"#variavel auxiliar 1\nchar_to_int[chr(9609)] = 9609\nint_to_char[9609] = chr(9609)\n\nINT_TO_NOTA[9609] = ' '\n\nchr(9609), 9609","7669326a":"#variavel between character\nchar_to_int[chr(9610)] = 9610\nint_to_char[9610] = chr(9610)\n\nINT_TO_NOTA[9610] = ' '\nchr(9610), 9610","64510075":"#characteres end\nchar_to_int[chr(9608)] = 9608\nint_to_char[9608] = chr(9608)\n\nINT_TO_NOTA[9608] = ' '\nchr(9608), 9608","a3c41caa":"for item in dataset:\n    X.append(item.pathImage.unique()[0])\n    Y.append(item.nota.values)\nY = [chr(9610).join([int_to_char[note_to_int(ynho)] for ynho in y])+chr(9608) for y in Y]","c4853db9":"\n# Mapping characters to integers\nchar_to_num = layers.experimental.preprocessing.StringLookup(\n    vocabulary=list(char_to_int.keys()), num_oov_indices=0, mask_token=None)\n\n# Mapping integers back to original characters\nnum_to_char = layers.experimental.preprocessing.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n","f662ed09":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=142)","0b8b12e1":"X_train[:5], y_train[:5]","4519ae05":"def load_Conv2D(file, label):\n    # 1. Read image\n    img = tf.io.read_file(file)\n    # 2. Decode and convert to grayscale\n    img = tf.io.decode_png(img, channels=1)\n    # 3. Convert to float32 in [0, 1] range\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # 4. Resize to the desired size\n    #img = tf.image.resize(img, [img_height, img_width])\n    # 5. Transpose the image because we want the time\n    # dimension to correspond to the width of the image.\n    \n\n    diffLenWidth =  img_width - img.numpy().shape[1]\n    diffLenHeight =  img_height - img.numpy().shape[0]\n    imgAux = img.numpy()\n    imgAux = np.pad(imgAux, [(0, 0), (0, diffLenWidth), (0, 0)], mode='constant')\n    imgAux = np.pad(imgAux, [(0, diffLenHeight), (0, 0), (0, 0)], mode='constant')\n\n    img = tf.convert_to_tensor(imgAux, np.float32)\n    \n    img = tf.transpose(img, perm=[1, 0, 2])\n    \n    # retirar todas os chracteres repetidos\n    #repeat_pattern = re.compile(r'(\\w)\\1*')\n    #label = repeat_pattern.sub(r'\\1', label) \n    \n    Ylen = max([len(y) for y in Y])\n    diffY = Ylen -len(label.numpy().decode('utf-8'))\n    if diffY > 0:\n        for x in range( diffY ):\n            if x % 2 == 0:\n                label += chr(9609)\n            else:\n                label += chr(9610)\n                \n\n    label = char_to_num(tf.strings.unicode_split(label.numpy().decode('utf-8'), input_encoding=\"UTF-8\"))\n    \n    return img, label\n","41d3a001":"# CTCLayer normal\nclass CTCLayer(layers.Layer):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred):\n        # Compute the training-time loss value and add it\n        # to the layer using `self.add_loss()`.\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n\n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions\n        return y_pred","16f64a22":"# baidu deep speech\ndef build_modelConv2D():\n    # Inputs to the model\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    labels = layers.Input(name=\"label\", shape=(max([len(y) for y in Y])), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(\n        8,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((1, 1), name=\"pool1\")(x)\n    '''\n    x = layers.Conv2D(\n        16,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(input_img)\n    x = layers.MaxPooling2D((3,3), name=\"pool2\")(x)\n    '''\n    x = layers.Conv2D(\n        32,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv3\",\n    )(input_img)\n    x = layers.MaxPooling2D((5,5), name=\"pool3\")(x)\n    \n    '''\n    x = layers.Conv2D(\n        64,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv4\",\n    )(input_img)\n    x = layers.MaxPooling2D((5,5), name=\"pool4\")(x)\n    '''\n    \n    x = layers.Conv2D(\n        128,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv5\",\n    )(input_img)\n    x = layers.MaxPooling2D((7,7), name=\"pool5\")(x)\n\n\n    #x = layers.BatchNormalization(axis=-1, name='BN_2')(x)\n    \n    new_shape = (114, 5376)\n    \n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    \n    x = layers.Dense(2048, activation=\"relu\", name=\"dense0\")(x)\n    #x = layers.Dense(2048, activation=\"relu\", name=\"dense0\")(x)\n    #x = layers.Dense(1024, activation=\"relu\", name=\"dense1\")(x)\n    #x = layers.Dropout(0.1)(x)\n\n    # RNNs\n    #x = layers.Bidirectional(layers.LSTM(1024, return_sequences=True, dropout=0.1))(x)\n    x = layers.Bidirectional(layers.LSTM(512, return_sequences=True, dropout=0.2))(x)\n    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.2))(x)\n    #x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1))(x)\n    #x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.1))(x)\n    #x = layers.BatchNormalization(axis=-1, name='BN_3')(x)\n    \n    #x = layers.TimeDistributed(layers.Dense((len(char_to_int) + 1)*2, activation='relu') , name='time_distributed_2')(x)\n    #x = layers.TimeDistributed(layers.Dense(len(char_to_int) + 1, activation='softmax', name='dense3'), name='time_distributed_3' )(x)\n    x = layers.Dense(len(char_to_int) + 1, activation='softmax', name='time_distributed_3' )(x)\n    \n    # Add CTC layer for calculating CTC loss at each step\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n\n    # Define the model\n    model = keras.models.Model(\n        inputs=[input_img, labels], \n        outputs=output, \n        name=\"ocr_model_v1\", \n    )\n\n    # Optimizer\n    opt = keras.optimizers.Adam()\n    \n    model.compile(\n        optimizer=opt, \n        #loss='mean_squared_error'\n        #metrics = ['accuracy']\n        )\n    \n    return model\n\nplot_model(build_modelConv2D(), to_file='modelConv2D_plot.png', show_shapes=True, show_layer_names=True)","17149076":"def build_modelAttentionConv2D():\n    # Inputs to the model\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    labels = layers.Input(name=\"label\", shape=(142), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(\n        8,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((1, 1), name=\"pool1\")(x)\n    '''\n    x = layers.Conv2D(\n        16,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(input_img)\n    x = layers.MaxPooling2D((5,5), name=\"pool2\")(x)\n    \n    '''\n    x = layers.Conv2D(\n        32,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv3\",\n    )(input_img)\n    x = layers.MaxPooling2D((3,3), name=\"pool3\")(x)\n    \n    '''\n    x = layers.Conv2D(\n        64,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv4\",\n    )(input_img)\n    x = layers.MaxPooling2D((7,7), name=\"pool4\")(x)\n    '''\n    \n    x = layers.Conv2D(\n        128,\n        (5, 5),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv5\",\n    )(input_img)\n    x = layers.MaxPooling2D((7,7), name=\"pool5\")(x)\n\n    #x = layers.BatchNormalization(axis=-1, name='BN_2')(x)\n    \n    new_shape = (114, 5376)\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    #x = layers.Dense(len(char_to_int) + 1, activation=\"relu\", name=\"dense0\")(x)\n\n    if True:\n        x = layers.LSTM(200, return_sequences=True, dropout=0.1)(x)\n\n        encoder_last_h1, encoder_last_h2, encoder_last_c = layers.LSTM(\n             100, activation='elu', dropout=0.2, recurrent_dropout=0.2, \n             return_sequences=False, return_state=True)(x)\n\n        encoder_last_h1 = layers.BatchNormalization(momentum=0.6)(encoder_last_h1)\n        encoder_last_c = layers.BatchNormalization(momentum=0.6)(encoder_last_c)\n\n        decoder = layers.RepeatVector(142)(encoder_last_h1)\n\n        x = layers.LSTM(100, activation='elu', dropout=0.2, recurrent_dropout=0.2, return_state=False, return_sequences=True)(\n            decoder, initial_state=[encoder_last_h1, encoder_last_c])\n    else:\n        x = layers.Attention()(x,x)\n\n    x = layers.Bidirectional(layers.LSTM(100, return_sequences=True, dropout=0.1))(x)\n    #x = layers.Bidirectional(layers.LSTM(100, return_sequences=True, dropout=0.1))(x)\n    #x = layers.Bidirectional(layers.LSTM(100, return_sequences=True, dropout=0.1))(x)\n    #x = layers.LSTM(100, return_sequences=True, dropout=0.1)(x)\n    x = layers.TimeDistributed(layers.Dense(len(char_to_int) + 1, activation='softmax', name='dense3'), name='time_distributed_3')(x)\n\n    \n    # Add CTC layer for calculating CTC loss at each step\n    \n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n    \n    \n    # Define the model\n    model = keras.models.Model(\n        inputs=[input_img, labels], \n        outputs=output, \n        name=\"ocr_model_v1\", \n    )\n    \n    opt = keras.optimizers.Adam(lr=0.01, clipnorm=1)\n    \n    model.compile(\n        optimizer=opt,\n        #metrics = [\n        #  tf.keras.metrics.CategoricalAccuracy(name='acc'),\n        #]\n\n    )\n    \n    return model\n\n\nplot_model(build_modelAttentionConv2D(), to_file='modelAttentionConv2D_plot.png', show_shapes=True, show_layer_names=True)","1b37b904":"class CategoricalTruePositives(tf.keras.metrics.Metric):\n\n    def __init__(self, num_classes, batch_size,\n                 name=\"categorical_true_positives\", **kwargs):\n        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n\n        self.batch_size = batch_size\n        self.num_classes = num_classes    \n\n        self.cat_true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):    \n        #print(y_true, y_pred)\n        #f = open(\"teste.txt\", \"a\")\n        #f.write(\"isso funciona por\")\n        #f.write(str(y_true))\n        #f.write(str(y_pred))\n        #f.close() \n\n        y_true = K.argmax(y_true, axis=-1)\n        y_pred = K.argmax(y_pred, axis=-1)\n        y_true = K.flatten(y_true)\n\n        true_poss = K.sum(K.cast((K.equal(y_true, y_pred)), dtype=tf.float32))\n\n        self.cat_true_positives.assign_add(true_poss)\n\n    def result(self):\n\n        return self.cat_true_positives","141a99f0":"#max_length = max([len(label) for label in allWords])\nmax_length = len(allWords)\nprint(max_length)\n# A utility function to decode the output of the network\ndef decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    #print(input_len)\n    # Use greedy search. For complex tasks, you can use beam search\n    unpadded = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n    \n    results = unpadded[ :, :max_length ]\n    # Iterate over the results and get back the text\n    output_text = []\n    for res in results:\n        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n        output_text.append(res)\n    return output_text\n    \n# A utility function to decode the output of the network\ndef decode_batch_predictions(y_pred):\n    input_shape = tf.keras.backend.shape(y_pred)\n    input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n        input_shape[1], 'float32')\n    unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n    unpadded_shape = tf.keras.backend.shape(unpadded)\n    padded = tf.pad(unpadded,\n                    paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n                    constant_values=-1)\n    \n    output_text = []\n    for res in padded:\n        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n        output_text.append(res)\n    return output_text\n    #return padded","79962a45":"\ndef jaro_similarity(s1, s2):\n    \"\"\"\n    Computes the Jaro similarity between 2 sequences from:\n\n        Matthew A. Jaro (1989). Advances in record linkage methodology\n        as applied to the 1985 census of Tampa Florida. Journal of the\n        American Statistical Association. 84 (406): 414\u201320.\n\n    The Jaro distance between is the min no. of single-character transpositions\n    required to change one word into another. The Jaro similarity formula from\n    https:\/\/en.wikipedia.org\/wiki\/Jaro%E2%80%93Winkler_distance :\n\n        jaro_sim = 0 if m = 0 else 1\/3 * (m\/|s_1| + m\/s_2 + (m-t)\/m)\n\n    where:\n        - |s_i| is the length of string s_i\n        - m is the no. of matching characters\n        - t is the half no. of possible transpositions.\n    \"\"\"\n    # First, store the length of the strings\n    # because they will be re-used several times.\n    len_s1, len_s2 = len(s1), len(s2)\n\n    # The upper bound of the distanc for being a matched character.\n    match_bound = math.floor( max(len(s1), len(s2)) \/ 2 ) - 1\n\n    # Initialize the counts for matches and transpositions.\n    matches = 0  # no.of matched characters in s1 and s2\n    transpositions = 0  # no. transpositions between s1 and s2\n\n    # Iterate through sequences, check for matches and compute transpositions.\n    for ch1 in s1:     # Iterate through each character.\n        if ch1 in s2:  # Check whether the\n            pos1 = s1.index(ch1)\n            pos2 = s2.index(ch1)\n            if(abs(pos1-pos2) <= match_bound):\n                matches += 1\n                if(pos1 != pos2):\n                    transpositions += 1\n\n    if matches == 0:\n        return 0\n    else:\n        return 1\/3 * ( matches\/len_s1 +\n                       matches\/len_s2 +\n                      (matches-transpositions\/\/2) \/ matches\n                     )\n\ndef datasetPred(prediction_model, dataset):\n    originalTextos = []\n    eachJaroWinkler = []\n    predicaoTextos = []\n    \n    for batch in dataset:\n\n        batch_images = batch[\"image\"][:3]\n        batch_labels = batch[\"label\"][:3]\n\n        preds = prediction_model.predict(batch_images)\n        pred_texts = decode_batch_predictions(preds)\n\n        for label in batch_labels:\n            label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n            originalTextos.append(label)\n\n        for pred_text in pred_texts:\n            predicaoTextos.append(pred_text)\n\n    #\n    for iAux in range(len(originalTextos)):\n        print(\"Original:  \" + ''.join([INT_TO_NOTA[char_to_int[ynho]] for ynho in originalTextos[iAux]]) + \"\\r\\n\")\n        print(\"Predicted: \" + ''.join([INT_TO_NOTA[char_to_int[ynho]] for ynho in predicaoTextos[iAux].replace(\"[UNK]\",\"\u2588\") ]) + \"\\r\\n\")\n        eachJaroWinkler.append(jaro_similarity(originalTextos[iAux], predicaoTextos[iAux].replace(\"[UNK]\",\"\u2588\") ))\n        print()\n        \n    print(\"jaro winkler: \"+ str(sum(eachJaroWinkler)\/len(eachJaroWinkler)) )\n    \ndef predictingModel(model, train_dataset, validation_dataset):\n    prediction_model = keras.models.Model(\n        model.get_layer(name=\"image\").input, model.get_layer(name=\"time_distributed_3\").output\n    )\n\n    print(\"Validation dataset\")\n    datasetPred(prediction_model, validation_dataset.take(3))\n    print(\"Train dataset\")\n    datasetPred(prediction_model, train_dataset.take(3))","24c48c70":"\ndef plotHistory(nome, history):\n    \n    time = datetime.now()\n    timestampStr = time.strftime(\"%d %b %Y %H %M %S %f)\")\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    #plt.savefig(\"loss\"+timestampStr+\".png\")\n    plt.savefig(nome+\"loss.png\")\n    \n    plt.show()\n    '''\n    \n    time = datetime.now()\n    timestampStr = time.strftime(\"%d %b %Y %H %M %S %f)\")\n    plt.plot(history['mean_squared_error'])\n    plt.plot(history['val_mean_squared_error'])\n    plt.title('mean_squared_error')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(\"loss\"+timestampStr+\".png\")\n    plt.show()\n    \n    \n    time = datetime.now()\n    timestampStr = time.strftime(\"%d %b %Y %H %M %S %f)\")\n    plt.plot(history['mean_absolute_error'])\n    plt.plot(history['val_mean_absolute_error'])\n    plt.title('mean_absolute_error')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(\"loss\"+timestampStr+\".png\")\n    plt.show()\n    \n    \n    time = datetime.now()\n    timestampStr = time.strftime(\"%d %b %Y %H %M %S %f)\")\n    plt.plot(history['mean_absolute_percentage_error'])\n    plt.plot(history['val_mean_absolute_percentage_error'])\n    plt.title('mean_absolute_percentage_error')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig(\"loss\"+timestampStr+\".png\")\n    plt.show()\n    '''\n    \n    ","7aa8e5dc":"def getBestWeightsImprovement(nameModel):\n    print(\"Best weights \"+nameModel)\n    highest = 0\n    loss = 0.0\n    epocaNumber = 0\n    rootPath = \"\"\n    arquivo = nameModel+checkpoint_filepath\n    rootPath = '..\/input\/speechrecognition-pt-br\/'\n    #regex = re.compile(nameModel+checkpoint_filepath.replace(\"{epoch:02d}\",\"(.*)\").replace(\"{loss:.4f}\",\"(.*)\"))\n    #for root, dirs, files in os.walk('..\/input\/speechrecognition-pt-br\/'):\n    #    for file in files:\n    #        resultado = regex.match(file)\n    #        \n    #        if resultado != None:\n    #            numero = int(resultado.group(1))\n    #            if epocaNumber <= numero:\n    #                epocaNumber = numero\n    #                arquivo = nameModel+checkpoint_filepath.replace(\"{epoch:02d}\",resultado.group(1)).replace(\"{loss:.4f}\",resultado.group(2))\n    #                rootPath = root\n    \n    print(\"Find \"+rootPath+arquivo)\n    \n    if os.path.exists(rootPath+arquivo):                       \n        print('Moving '+arquivo)\n        copyfile(rootPath+arquivo, \".\/\"+arquivo)\n          \n    return epocaNumber, arquivo\n\ndef deleteAllExceptBestWeights(nameModel):\n    highest = 0\n    loss = 0.0\n    regex = re.compile(nameModel+checkpoint_filepath.replace(\"{epoch:02d}\",\"(.*)\").replace(\"{loss:.4f}\",\"(.*)\"))\n    for root, dirs, files in os.walk('.\/'):\n        for file in files:\n            resultado = regex.search(file)\n            if resultado != None:\n                if highest < int(resultado.group(1)):\n                    highest = int(resultado.group(1))\n                    loss = float(resultado.group(2))\n                #file = nameModel+checkpoint_filepath.replace(\"{epoch:02d}\",resultado.group(1)).replace(\"{loss:.4f}\",resultado.group(2))\n                #model.load_weights(file)\n                #os.remove( file )\n    item = nameModel+checkpoint_filepath.replace(\"{epoch:02d}\",'{0:02d}'.format(highest)).replace(\"{loss:.4f}\",'{:.4f}'.format(loss))\n    for root, dirs, files in os.walk('.\/'):\n        for file in files:\n            resultado = regex.search(file)\n            if resultado != None:\n                if item not in file:\n                    os.remove( root+file )\n                    ","5f9c60af":"def treinamento(trainDataset, validationDataset, modelCheckpoint, earlyStopping, nome ,modelo, numeroEpocas, executarTreinamento,stepsPerEpoch=0):\n    pickleHistory = nome+'.pkl'\n    \n    \n    history = modelo.fit(\n            trainDataset,\n            validation_data=(\n                validationDataset,\n            ),\n            epochs=numeroEpocas,\n            callbacks=[earlyStopping, modelCheckpoint],\n            #steps_per_epoch=stepsPerEpoch,\n\n            #initial_epoch = epocaNumber,\n            verbose = 1\n        )\n    modelo.save_weights(nome+\"weights.h5\")\n    \n    plotHistory(nome, history.history)\n    \n    predictingModel(modelo, train_dataset, validation_dataset)\n\n    return modelo","480f2b4e":"\ndef encode_single_sample(img_path, label):\n    img, label = tf.py_function(load_Conv2D, [img_path, label], [tf.float32, tf.int64])\n    return {\"image\": img, \"label\": label}\n    \n    \ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_dataset = (\n    train_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(batch_size_matrix)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\nvalidation_dataset = (\n    validation_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(batch_size_matrix)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n","518c9af7":"#mc = ModelCheckpoint(\"modelConv2D\"+checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40, min_delta=0.01)\n#tc = TensorBoard(log_dir=\"modelConv2D\", update_freq=1)\n#checkpoint_filepath\nnameModel = 'modelConv2Dv3'\nmc = ModelCheckpoint(nameModel+checkpoint_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=200, min_delta=0.01)\n\n\nmodel2D = build_modelConv2D()\nmodel2D.summary()\nmodel2D= treinamento(train_dataset, validation_dataset, mc, es, nameModel, model2D, nEpocas, True)\n","bc6e8c73":"import PIL","67194581":"\nprediction_model = keras.models.Model(\n    model2D.get_layer(name=\"image\").input, model2D.get_layer(name=\"time_distributed_3\").output\n)\noriginalTextos = []\npredicaoTextos = []\nvetorArquivo = []\n\nfor batch in validation_dataset.take(2):\n          \n    batch_images = batch[\"image\"]\n    batch_labels = batch[\"label\"]\n    \n    preds = prediction_model.predict(batch_images)\n    pred_texts = decode_batch_predictions(preds)\n\n    for image in batch_images:\n        vetorArquivo.append(image)\n        \n    for label in batch_labels:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        originalTextos.append(label)\n        \n\n    for pred_text in pred_texts:\n        predicaoTextos.append(pred_text)\n        \nfor iAux in range(3):\n    \n    shape = vetorArquivo[iAux].numpy().shape\n    teste = vetorArquivo[iAux].numpy().reshape(shape[0], shape[1]).transpose()\n\n    fig = px.imshow(teste, binary_string=True)\n    fig.show()\n\n    print(\"Original:  \" + ' '.join([INT_TO_NOTA[char_to_int[ynho]] for ynho in originalTextos[iAux]]) + \"\\r\\n\")\n    print(\"Predicted: \" + ' '.join([INT_TO_NOTA[char_to_int[ynho]] for ynho in predicaoTextos[iAux].replace(\"[UNK]\",\"\u2588\") ]) + \"\\r\\n\")\n","8bb74149":"#mc = ModelCheckpoint(\"modelAttentionConv2D\"+checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40, min_delta=0.01)\n\nnameModel = 'modelAttentionConv2Dv3'\nmc = ModelCheckpoint(nameModel+checkpoint_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=0.01)\n\nmodelAttention2D = build_modelAttentionConv2D()\nmodelAttention2D.summary()\n\ntreinamento(train_dataset, validation_dataset, mc, es, nameModel, modelAttention2D, nEpocas, False)","b815b034":"\nprediction_model = keras.models.Model(\n    modelAttention2D.get_layer(name=\"image\").input, modelAttention2D.get_layer(name=\"time_distributed_3\").output\n)\noriginalTextos = []\npredicaoTextos = []\nvetorArquivo = []\n\nfor batch in validation_dataset.take(2):\n          \n    batch_images = batch[\"image\"]\n    batch_labels = batch[\"label\"]\n    \n    preds = prediction_model.predict(batch_images)\n    pred_texts = decode_batch_predictions(preds)\n\n    for image in batch_images:\n        vetorArquivo.append(image)\n        \n    for label in batch_labels:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        originalTextos.append(label)\n        \n\n    for pred_text in pred_texts:\n        predicaoTextos.append(pred_text)\n        \nfor iAux in range(3):\n    \n    shape = vetorArquivo[iAux].numpy().shape\n    teste = vetorArquivo[iAux].numpy().reshape(shape[0], shape[1]).transpose()\n\n    fig = px.imshow(teste, binary_string=True)\n    fig.show()\n\n    print(\"Original:  \" + ' '.join([INT_TO_NOTA[char_to_int[ynho]] for ynho in originalTextos[iAux]]) + \"\\r\\n\")\n    print(\"Predicted: \" + ' '.join([INT_TO_NOTA[char_to_int[ynho]] for ynho in predicaoTextos[iAux].replace(\"[UNK]\",\"\u2588\") ]) + \"\\r\\n\")\n","da784ca7":"Resolvendo problema por nao conseguir filtar os arquivos existentes","3c394370":"Exemplo usando dataset do kaggle","d47bbe67":"# Configuracao Machine Learning","c3f3a825":"# Tentativa com descritor de fonetica alfabetica","f437419a":"# Extracao dos dados\n","37e45f09":"# Treinamento modelAttentionConv2D","bd15b0cc":"# Model CTC Conv2D","f493851a":"# Load dados para Conv2D","a798f7bd":"# Setando encoder para retornar matrix de dados","c6df2efa":"# Predicting model","bd599ded":"# Treinamento modelConv2D","5ebf85b1":"# Luong Attention CTC Conv2D","c28dde95":"# Tentativa de adicionar uma metrica customizada"}}