{"cell_type":{"e0dc0071":"code","b8f836bd":"code","7d05bfc4":"code","8e281226":"code","ccefddab":"code","47aa2d78":"code","edc4c84e":"code","85f8a03b":"code","f46b7c98":"code","eb6648e9":"code","692ba2a8":"code","c1a92d40":"code","8cda9c11":"code","ebe1d1e4":"code","bb19ea0c":"code","25339ba4":"code","1694b064":"code","28167fbc":"code","00253ad1":"markdown","7e1e234b":"markdown","a0b31178":"markdown","a6546b70":"markdown","2b7da382":"markdown","8d18536b":"markdown","b3099c81":"markdown","5f9908c6":"markdown","281daf8f":"markdown","61548921":"markdown","b06f544e":"markdown","cf21a11b":"markdown","64337c3b":"markdown","540669ee":"markdown","e10653c3":"markdown"},"source":{"e0dc0071":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import RobertaModel, RobertaTokenizer","b8f836bd":"class Settings:\n    batch_size=16\n    max_len=350\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    seed = 318","7d05bfc4":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    \nset_seed(Settings.seed)","8e281226":"class TrainValidDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.text = df[\"excerpt\"].values\n        self.target = df[\"target\"].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        texts = self.text[idx]\n        tokenized = self.tokenizer.encode_plus(texts, truncation=True, add_special_tokens=True,\n                                               max_length=self.max_len, padding=\"max_length\")\n        ids = tokenized[\"input_ids\"]\n        mask = tokenized[\"attention_mask\"]\n        targets = self.target[idx]\n        return {\n            \"ids\": torch.LongTensor(ids),\n            \"mask\": torch.LongTensor(mask),\n            \"targets\": torch.tensor(targets, dtype=torch.float32)\n        }","ccefddab":"class CommonLitRoBERTa(nn.Module):\n    def __init__(self, pretrained_path):\n        super().__init__()\n        self.roberta = RobertaModel.from_pretrained(pretrained_path)\n        \n    def forward(self, ids, mask):\n        output = self.roberta(ids, attention_mask=mask)\n        return output","47aa2d78":"model = CommonLitRoBERTa(\"..\/input\/roberta-transformers-pytorch\/roberta-base\")\nmodel.to(Settings.device)","edc4c84e":"tokenizer = RobertaTokenizer.from_pretrained(\"..\/input\/roberta-transformers-pytorch\/roberta-base\")\ntokenizer","85f8a03b":"# prepare dataset\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u6e96\u5099\ndf_train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n\ntrain_dataset = TrainValidDataset(df_train, tokenizer, Settings.max_len)\ntrain_loader = DataLoader(train_dataset, batch_size=Settings.batch_size,\n                          shuffle=True, num_workers=8, pin_memory=True)","f46b7c98":"# make mini batch data\n# \u30df\u30cb\u30d0\u30c3\u30c1\u30c7\u30fc\u30bf\u3092\u4f5c\u308b\nbatch = next(iter(train_loader))","eb6648e9":"ids = batch[\"ids\"].to(Settings.device)\nmask = batch[\"mask\"].to(Settings.device)\ntargets = batch[\"targets\"].to(Settings.device)\n\nprint(ids.shape)\nprint(mask.shape)\nprint(targets.shape)","692ba2a8":"output = model(ids, mask)\noutput","c1a92d40":"# last_hidden_state\nlast_hidden_state = output[0]\nprint(\"shape:\", last_hidden_state.shape)","8cda9c11":"# pooler output\npooler_output = output[1]\nprint(\"shape:\", pooler_output.shape)","ebe1d1e4":"# .detach() = make copies and remove gradient information  \n# .detach() = \u52fe\u914d\u60c5\u5831\u3092\u9664\u5916\u3057\u3066\u30c6\u30f3\u30bd\u30eb\u3092\u30b3\u30d4\u30fc\ncls_embeddings = last_hidden_state[:, 0, :].detach()\n\nprint(\"shape:\", cls_embeddings.shape)\nprint(\"\")\nprint(cls_embeddings)","bb19ea0c":"pd.DataFrame(cls_embeddings.numpy()).head()","25339ba4":"last_hidden_state.shape","1694b064":"# apply avg.pooling to word embeddings\n# \u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u306baverage pooling \u3092\u9069\u7528\u3059\u308b\npooled_embeddings = last_hidden_state.detach().mean(dim=1)\n\nprint(\"shape:\", pooled_embeddings.shape)\nprint(\"\")\nprint(pooled_embeddings)","28167fbc":"pd.DataFrame(pooled_embeddings.numpy()).head()","00253ad1":"In this Notebook,  I introduce how to get text embedding from RoBERTa (\/BERT\/ALBERT\/etc.).  \nThere are 2 methods to get text embedding from RoBERTa.\n1. get CLS Token\n2. pool RoBERTa output (RoBERTa output = word embeddings)  \n  \nIf I make mistakes, please let me know in the comments.","7e1e234b":"note!: pooler output \"not\" equal pooled_embeddings we calculated  \nWhat is pooler output ?  \n-> It takes the representation from the [CLS] token from top layer of RoBERTa encoder, and feed that through another dense layer.  \nreference: https:\/\/github.com\/google-research\/bert\/blob\/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9\/modeling.py#L224-L232  \n  \n\u6ce8\uff1a pooler output\u306f\u4eca\u79c1\u305f\u3061\u304c\u8a08\u7b97\u3057\u305fpooled_embeddings\u3068\u306f\u5168\u304f\u306e\u5225\u7269\u3002  \n\u306a\u3089\u3001pooler output\u3068\u306f\u4f55\u304b\uff1f\n-> 1. Get CLS Token \u3067\u53d6\u5f97\u3057\u305fCLS\u30c8\u30fc\u30af\u30f3\u3092\u5225\u306edense layer\u306b\u901a\u3057\u305f\u3082\u306e\u3002  \n\u53c2\u7167: https:\/\/github.com\/google-research\/bert\/blob\/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9\/modeling.py#L224-L232","a0b31178":"## 2. Pool RoBERTa Output","a6546b70":"# Dataset","2b7da382":"# Get Text Embeddings","8d18536b":"16 = num of texts, 350 = num of tokens in a text, 768 = dimension of word embedding  \n16 = \u6587\u7ae0\u306e\u6570\u3001350 = 1\u6587\u4e2d\u306e\u5358\u8a9etoken\u306e\u6570\u3001768 = \u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e\u6b21\u5143\u6570","b3099c81":"# Model","5f9908c6":"16 = num of texts, 768 = dimension of text embedding  \n16 = \u6587\u7ae0\u306e\u6570\u3001768 = text embedding\u306e\u6b21\u5143\u6570","281daf8f":"\u3053\u306enotebook\u3067\u306f\u3001RoBERTa (\u307e\u305f\u306fALBERT, BERT\u306a\u3069) \u3092\u4f7f\u3063\u3066\u6587\u7ae0\u306e\u30d9\u30af\u30c8\u30eb\u5316 (text embedding) \u3092\u884c\u3046\u65b9\u6cd5\u3092\u7d39\u4ecb\u3059\u308b\u3002  \nRoBERTa\u3092\u4f7f\u3063\u305ftext embedding\u306b\u306f2\u7a2e\u985e\u306e\u65b9\u6cd5\u304c\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002  \n1. CLS\u30c8\u30fc\u30af\u30f3\u3092\u53d6\u5f97\u3057\u3066\u305d\u308c\u3092\u6587\u7ae0\u306e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u3068\u898b\u505a\u3059\u65b9\u6cd5\n2. RoBERTa\u306e\u51fa\u529b\u3092\u30d7\u30fc\u30ea\u30f3\u30b0\u3059\u308b\u65b9\u6cd5 (\u3053\u3053\u3067\u3044\u3046\u51fa\u529b\u3068\u306f\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u305f\u3061\u306e\u3053\u3068)  \n  \n\u3082\u3057\u9593\u9055\u3048\u3066\u3044\u308b\u500b\u6240\u304c\u3042\u3063\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u3067\u6559\u3048\u3066\u304f\u3060\u3055\u3044","61548921":"![get cls token](https:\/\/jalammar.github.io\/images\/distilBERT\/bert-output-tensor-selection.png)","b06f544e":"## 1. Get CLS Token","cf21a11b":"16 = num of texts, 350 = num of word tokens in a text  \n16 = \u6587\u7ae0\u306e\u6570\u3001350 = 1\u6587\u306e\u4e2d\u306b\u3042\u308b\u5358\u8a9etoken\u306e\u6570","64337c3b":"2 outputs (last_hidden_state, pooler_output) from RoBERTa  \n2\u3064\u306e\u51fa\u529b\u304cRoBERTa\u304b\u3089\u5410\u304d\u51fa\u3055\u308c\u308b","540669ee":"use last_hidden_state  \nlast_hidden_state\u3092\u4f7f\u3046","e10653c3":"meaning of pooler output will be explained later.  \npooler output\u306e\u610f\u5473\u306f\u5f8c\u3067\u8aac\u660e\u3059\u308b\u3002"}}