{"cell_type":{"f3e5d671":"code","c6f51eb2":"code","eb354e4f":"code","fe0387c9":"code","4ce5e640":"code","1d5ab1e7":"code","ae38f7e9":"code","28a3e8a7":"code","841f96a4":"code","456472e9":"code","cfd8dc18":"code","7d22083a":"code","931fbcb2":"code","ff2c1ac8":"code","451c8620":"code","f4881bba":"code","cfd1243b":"code","a485888d":"code","eebc2900":"code","fbdf0fef":"code","ca16ce96":"code","627bcdfd":"code","4d4e3844":"code","cf0614ed":"code","725fd70b":"code","e3147946":"code","0e58fb62":"code","4477be6f":"code","6e1927ff":"code","c691e16d":"code","9010a416":"code","a9cc5b57":"code","625419eb":"code","1ef5320c":"code","f7404145":"code","08389408":"code","571393a1":"code","ca316878":"code","cb59fe2e":"code","2a5109e2":"markdown","6f48de20":"markdown","072f9477":"markdown","479017cc":"markdown","0e7ceafb":"markdown","392a3484":"markdown","e51eea82":"markdown","8498fd4f":"markdown","68f69fb6":"markdown","2b560703":"markdown","9bd16887":"markdown","a96966ba":"markdown","d6da21b5":"markdown","b6a97c67":"markdown","04d350c7":"markdown","a3b486eb":"markdown","b02c6e29":"markdown","9c9b6d41":"markdown","ce20fbed":"markdown","ac8f2ddb":"markdown","b8f814fb":"markdown","56ae8c20":"markdown","b6d66ce2":"markdown","55861013":"markdown","3c742434":"markdown","3e2e0ec9":"markdown"},"source":{"f3e5d671":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import preprocessing\nsns.set_style(\"whitegrid\")\n\n#Supress Warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\nSEED=42","c6f51eb2":"dry_beans = pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv')\ndry_beans = dry_beans.set_index(\"ID\")\ndry_beans.head()","eb354e4f":"#General Info:\ndry_beans.info()","fe0387c9":"#Check for duplicate rows:\ndry_beans.duplicated().sum()","4ce5e640":"#Statistical Summary:\ndry_beans.describe().T","1d5ab1e7":"dry_beans[\"y\"].value_counts().plot(kind='bar',\n                                   title= \"Bean type counts in the training data\",\n                                   xlabel= 'Bean type',\n                                   ylabel= 'Count');","ae38f7e9":"fig , axes = plt.subplots(4,4 , figsize=(25,25))\n\nfeatures = dry_beans.columns.drop('y')\n\nfor i in range(4):\n    for j in range(4):\n        feature_col = features[j + i * 4 ]\n        \n        sns.histplot(data = dry_beans[feature_col] , ax = axes[i,j])\n        median = dry_beans[feature_col].median()\n        axes[i,j].set_title(feature_col)\n        axes[i,j].axvline(median , color = 'red' , lw = 2 , alpha= .5)\n        \n        \n## Most Features is Skeed \n## Having Outlieres -> We Will Check it","28a3e8a7":"# Draw The Violinplot to Check outlier and see the distrubtion of all features:\n\nfig , axes = plt.subplots(8,2 , figsize=(20,40))\n\nfeatures = dry_beans.columns.drop('y')\n\nfor i in range(8):\n    for j in range(2):\n        feature_col = features[j + i * 2 ]\n        \n        sns.violinplot(data=dry_beans,\n               x=\"y\",\n               y=feature_col,\n               inner=\"quartile\",\n               ax=axes[i,j])\n        \n        axes[i,j].set_xlabel(None)\n","841f96a4":"plt.figure(figsize = [14, 10])\n\ncorr = dry_beans.corr()\nmask  = np.ones_like(corr)\nmask[np.tril_indices_from(corr)]= False\n\nwith  sns.axes_style(\"white\"):\n    sns.heatmap(corr, mask= mask,vmin=-1, vmax=1 ,square=False,annot=True, cmap='BrBG' , fmt = '.2f')","456472e9":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n","cfd8dc18":"y= dry_beans['y']\ndry_beans.drop('y' , axis=1 , inplace=True)","7d22083a":"## Default Pipine \n\ndef log_transform(X):\n    return np.log1p(X + 1)\n\ndef drop_columns(X):    \n    return X.drop('Area' , axis=1)\n\n\npreprocessor = Pipeline(steps=[\n#     ('drop_cols' , FunctionTransformer(drop_columns)),\n#     ('log_trans' , FunctionTransformer(log_transform)),\n#     ('poly' , PolynomialFeatures(interaction_only=True)),\n#     ('std_trans' , StandardScaler()),\n    ('power_trans' , PowerTransformer()),\n    ('pca', PCA(n_components=12)),\n\n])","931fbcb2":"## Custome Pipline to SVC\n\npreprocessor_svc = Pipeline(steps=[\n#     ('drop_cols' , FunctionTransformer(drop_columns)),\n#     ('log_trans' , FunctionTransformer(log_transform)),\n       ('poly' , PolynomialFeatures(interaction_only=True)),\n#     ('std_trans' , StandardScaler()),\n    ('power_trans' , PowerTransformer()),\n \n    ('pca', PCA(n_components=12)),\n\n])","ff2c1ac8":"from imblearn.over_sampling import SMOTE \nfrom sklearn.model_selection import train_test_split , GridSearchCV\n\n# split data with stratify on y label\nX_train, X_test, y_train, y_test = train_test_split(dry_beans, y, test_size=0.2, random_state=SEED ,\n                                                   stratify=y)\nstrategy={\n    'BOMBAY':700,\n    'BARBUNYA':1000,\n    'CALI':1300,\n    'HOROZ':1600,\n    'SEKER':1800,\n    'SIRA':2000\n    \n} \nsmote = SMOTE(k_neighbors=5 , random_state=42 ,sampling_strategy=strategy )\n\nX_train_resample , y_train_resample = smote.fit_resample(X_train , y_train)\n\n# X_train.shape , y_train.shape","451c8620":"# Boosting Algo : \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# xgboost Algo\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRFClassifier\n\n# Trees Based : \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n# Multi Layer Perceptron \nfrom sklearn.neural_network import MLPClassifier\n\n# SVC \nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import cross_val_score , GridSearchCV \nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# F1_score metric\nfrom sklearn.metrics import f1_score\n\nSEED =42\nCV = 5\n\n","f4881bba":"\n# Declare All Models in dictionary \nall_models ={\n\n    \n    \"XGBClassifier\" : XGBClassifier(random_state =SEED),\n    \"XGBRFClassifier\" : XGBRFClassifier( random_state =SEED),\n    \"SVC\" : SVC(kernel='rbf' , C = 10 , gamma='auto' ,random_state=SEED ),\n    \"LogisticRegression\" : LogisticRegression(penalty='l2' , solver='sag',random_state=SEED),\n\n    #     Stochastic GradentBossting\n    \"GradientBoostingClassifier\" : GradientBoostingClassifier(n_estimators= 100 \n                                                              ,subsample = .9  , random_state=SEED),\n    \"MLPClassifier\" : MLPClassifier(solver = 'adam' , random_state= SEED),\n}\n\n# Save All Scores to each Model \nscores = []\n# StratifiedShuffleSplit obj to split the date to train and test with stratify \nstratified_split = StratifiedShuffleSplit(n_splits=CV , test_size=.1, random_state=SEED)\n\n#loop over all model , fit data , get f1_score \nfor key , mdl in  all_models.items():\n    \n    sub_scores = 0\n    pip =  Pipeline(steps = [('pre_preprocessor' , preprocessor) , ('model' , mdl)])\n    \n    # Cross validation using StratifiedShuffleSplit\n    for train_index, test_index in stratified_split.split(X_train_resample, y_train_resample):\n    \n        trainX, _ = X_train_resample.loc[train_index], X_train_resample.loc[test_index]\n        trainY, _ = y_train_resample[train_index], y_train_resample[test_index]\n\n\n        \n        pip.fit(trainX , trainY)\n\n        y_pred = pip.predict(X_test)\n        sub_scores += f1_score(y_test , y_pred ,average='weighted')\n        \n    print('model name :' , key , '\\nScore =' , sub_scores.mean() * 100 \/ CV)\n    print('i = ', len(scores))\n    scores.append(sub_scores.mean() * 100 \/ CV)  \n\n    \nmodels_df = pd.DataFrame({'model': list(all_models.keys()) , 'score' : scores})  \nmodels_df.sort_values(by='score', ascending=False)","cfd1243b":"from sklearn.utils import class_weight\nclass_w = class_weight.compute_class_weight('balanced',np.unique(y) , y.unique())\nclass_w\n\n# .compute_sample_weigh","a485888d":"from sklearn.model_selection import train_test_split , GridSearchCV\n\n# # split data with stratify on y label\n# X_train, X_test, y_train, y_test = train_test_split(dry_beans, y, test_size=0.2, random_state=SEED ,\n#                                                     stratify=y)\n\n    ","eebc2900":"# # create SVC Pipline  \n# svc_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , \n#                                 ('svc' , SVC(random_state = SEED))])\n\n# # Search By these Parms\n# parameter_space = {\n#     'svc__C': [1 , 10 , 50,100],\n#     'svc__kernel': ['rbf', 'linear'],\n#     'svc__gamma': ['auto', 'scale',1e-3,1e-4],\n#     'svc__degree': [2 , 3],\n#     'svc__class_weight' : ['balanced' , None , class_w]\n# }\n\n\n# # Gride Search Obj to Fit our data and return the best parms\n# gs_clf = GridSearchCV(svc_pipline, parameter_space,cv=CV ,\n#                       scoring='f1_weighted', verbose=1)\n\n# gs_clf.fit(X_train_resample, y_train_resample)","fbdf0fef":"# gs_clf.best_params_","ca16ce96":"# Declare SVC By Tunned Parms\nsvc_model =  SVC(C = 10 , degree= 2 , gamma='scale' , class_weight='balanced' , probability=True)\n\nsvc_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor_svc) , \n                                ('svc' , svc_model)])\n\n# svc_pipline.fit(X_train , y_train)\nsvc_pipline.fit(X_train_resample , y_train_resample)\n\n\ny_pred = svc_pipline.predict(X_test)\n\nsvc_f1score =f1_score(y_test , y_pred , average='weighted') \nprint('svc_f1Score = ' , svc_f1score)","627bcdfd":"# lr = LogisticRegression(random_state = SEED , solver = 'sag')\n# logestic_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , ('lr' ,lr )])\n\n\n# # Search By these Parms\n# parameter_space = {\n# #     'lr__penalty': ['l1','l2'],\n#     'lr__C': [1,10,100],\n#     'lr__class_weight': ['balanced', None , class_w],\n#     'lr__solver' : ['newton-cg' , 'sag']\n\n# }\n# # Gride Search Obj to Fit our data and return the best parms\n# gs_lr = GridSearchCV(logestic_pipline, parameter_space, cv=CV , scoring='f1_weighted', verbose=1)\n\n\n# gs_lr.fit(X_train_resample, y_train_resample)\n","4d4e3844":"# gs_lr.best_params_","cf0614ed":"# Declare LR By Tunned Parms\nlr_model =  lr = LogisticRegression(random_state = SEED , C = 100, class_weight=None ,\n                                    solver = 'newton-cg')\nlr_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , \n                                ('lr' , lr_model)])\n\n# lr_pipline.fit(X_train , y_train)\nlr_pipline.fit(X_train_resample , y_train_resample)\n\n\ny_pred = lr_pipline.predict(X_test)\n\nlr_f1score =f1_score(y_test , y_pred , average='weighted') \nprint('lr_f1Score = ' , lr_f1score)\n","725fd70b":"# gs_clf.best_params_y_train_resample","e3147946":"\n# # create our model object m\n# mlpc=MLPClassifier(max_iter=500, random_state = SEED , early_stopping=True)\n\n# # declare pipline with MLPC object\n# mlpc_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , ('model' , mlpc)])\n\n# # Search By these Parms\n# parameter_space = {\n#     'model__hidden_layer_sizes': [(13,), (100,) ,(12,3,7), (10,3)],\n#     'model__activation': ['tanh', 'segmoid','relu'],\n#     'model__solver': ['lbfgs', 'adam'],\n#     'model__alpha': [0.0001, .01 , .001],\n#     'model__learning_rate_init' : [.001 , .0001 , .01],\n#     'model__learning_rate': ['constant','adaptive'],\n# }\n\n\n# # Gride Search Obj to Fit our data and return the best parms\n# gs_mlp = GridSearchCV(mlpc_pipline, parameter_space,cv=CV , scoring='f1_weighted', verbose=1)\n\n\n# gs_mlp.fit(X_train_resample, y_train_resample)\n","0e58fb62":"# gs_mlp.best_params_","4477be6f":"# Declare MLPC By Tunned Parms\nmlpc_model=MLPClassifier(max_iter=500,alpha = .01 , hidden_layer_sizes = (100,),learning_rate='constant',\n                   learning_rate_init=.01,\n                         solver= 'adam',random_state = SEED , early_stopping=True)\n\n\nmlpc_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , \n                                ('mlpc' , mlpc_model)])\n\n# mlpc_pipline.fit(X_train , y_train)\nmlpc_pipline.fit(X_train_resample , y_train_resample)\n\n\ny_pred = mlpc_pipline.predict(X_test)\n\nmlpc_f1score =f1_score(y_test , y_pred , average='weighted') \nprint('mlpc_f1Score = ' , mlpc_f1score)","6e1927ff":"# sgbt = GradientBoostingClassifier(random_state=SEED)\n\n# # declare pipline with MLPC object\n# sgbt_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , ('sgbt' , sgbt)])\n\n\n# # Search By these Parms\n# parameter_space = {\n#     \"sgbt__subsample\":[.8,.9],\n#     \"sgbt__max_features\" :[.2 , .3],\n#     \"sgbt__n_estimators\": [300 , 350 ,400],\n#     \"sgbt__max_depth\": [1,2],\n#     \"sgbt__learning_rate\" : [.1, .2 , .5]\n    \n# }\n\n\n# # Gride Search Obj to Fit our data and return the best parms\n# gs_sgbt = GridSearchCV(sgbt_pipline, parameter_space ,cv=CV , scoring='f1_weighted', verbose=1)\n\n\n# gs_sgbt.fit(X_train, y_train)\n","c691e16d":"# gs_sgbt.best_params_","9010a416":"# Declare SGBT By Tunned Parms\nsgbt = GradientBoostingClassifier(max_depth=2 , max_features=.2 ,learning_rate= .1 , n_estimators=400 ,\n                                  subsample= .8 ,random_state=SEED)\n\nsgbt_pipline = Pipeline(steps = [('pre_preprocessor' , preprocessor) , \n                                ('sgbt' , sgbt)])\n\n# sgbt_pipline.fit(X_train , y_train)\nsgbt_pipline.fit(X_train_resample , y_train_resample)\n\n\ny_pred = sgbt_pipline.predict(X_test)\n\nsgbt_f1score =f1_score(y_test , y_pred , average='weighted') \nprint('mlpc_f1Score = ' , sgbt_f1score)","a9cc5b57":"from sklearn.ensemble import VotingClassifier\n\nvoter = VotingClassifier([('svc', svc_pipline),('lr',lr_pipline ) ,\n                          ('mlpc',mlpc_pipline ) , ('sgbt',sgbt_pipline )],voting='soft')\n\nvoter.fit(X_train_resample, y_train_resample)","625419eb":"# print(\"The accuracy of the classifier on the validation set is \", \n#       (voter.score(X_test, y_test)))\n\ny_pred = voter.predict(X_test)\nf1_score(y_test , y_pred , average='weighted') ","1ef5320c":"# from mlxtend.classifier import StackingClassifier\n\n# lr = LogisticRegression(solver = 'sag')  # defining meta-classifier\n# svc_meta = SVC()\n\n# # define All Calssifer\n# classifiers = [svc_pipline ,lr_pipline  , mlpc_pipline , sgbt_pipline]\n\n# clf_stack = StackingClassifier(classifiers =classifiers, meta_classifier = MLPClassifier(), \n#                                use_probas = True, use_features_in_secondary = True)\n\n# model_stack = clf_stack.fit(X_train, y_train)   # training of stacked model\n# pred_stack = model_stack.predict(X_test)\n\n# f1_score(y_test , pred_stack , average='weighted') ","f7404145":"test_dry_beans = pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv' , index_col='ID')\ntest_dry_beans.head()","08389408":"prediction = voter.predict(test_dry_beans)","571393a1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca316878":"sup_data = pd.DataFrame()\nsup_data['ID'] = test_dry_beans.index.values\nsup_data['y'] =  prediction\nsup_data.head()","cb59fe2e":"\nsup_data[['ID', 'y']].to_csv('submission.csv', index=False)\n","2a5109e2":"<h1><center>Dry Beans Classification<\/center><\/h1>","6f48de20":"### HyperParameters Tuning (Logestic Regression) \n","072f9477":"## **Training Data EDA:**","479017cc":"- We can see a linear trend between many features.\n- Notice that **Bombay** class is mostly separated from other classes in some features, which means that despite having low count in the dataset, a model may still be able to correctly classify it.\n- The classes are clearly clustered within some scatterplots, mainly between the area and perimeter features with all other features.\n- Some features **(aspect ration, eccentricity, compactness)** seems to hit a bound when plotted against roundness , which indicates that (in the given data) no outliers occur above that bound.  ","0e7ceafb":"### TRY Multiple Model to select the best","392a3484":"### Try Stacking Techneques:\n","e51eea82":"### HyperParameters Tuning (SVC) \n","8498fd4f":"### **## Features Distributions:**","68f69fb6":"#### - The dataset consists of features describing the shape of the bean and the goal is to predict it's type.\n#### - This dataset is collected using a computer vision system that extracted shape features from beans images.\n#### - In total, 13,611 dry bean samples were obtained, the training data contains 10,834 of them.\n\n#### **Data fields:**\n1. **ID**, an ID for this instance.\n2. **Area - (A)**, The area of a bean zone and the number of pixels within its boundaries.\n3. **Perimeter - (P)**, Bean circumference is defined as the length of its border.\n4. **MajorAxisLength - (L)**, The distance between the ends of the longest line that can be drawn from a bean.\n5. **MinorAxisLength - (l)**, The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n6. **AspectRatio - (K)**, Defines the relationship between L and l : \\\\(K = \\frac{L}{l}\\\\)\n7. **Eccentricity - (Ec)**, Eccentricity of the ellipse having the same moments as the region.\n8. **ConvexArea - (C)**, Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n9. **EquivDiameter - (Ed)**, The diameter of a circle having the same area as a bean seed area: \\\\(E_{d} = \\sqrt{\\frac{4A}{\\pi}}\\\\)\n10. **Extent - (Ex)**, The ratio of the pixels in the bounding box to the bean area \\\\(Ex = \\frac{A}{A_{B}}\\\\) Where \\\\(A_{B} = \\\\) Area of bounding rectangle.\n11. **Solidity - (S)**, Also known as convexity. The ratio of the pixels in the convex shell to those found in beans: \\\\(S = \\frac{A}{C}\\\\)\n12. **Roundness - (R)**, Calculated with the following formula: \\\\(R = \\frac{4\\pi A}{P^2}\\\\)\n13. **Compactness - (CO)**, Measures the roundness of an object: \\\\(CO = \\frac{E_{d}}{L}\\\\)\n14. **ShapeFactor1** - \\\\((SF_{1})\\\\) Calculated with the following formula: \\\\(SF_{1} = \\frac{L}{A}\\\\)\n15. **ShapeFactor2** - \\\\((SF_{2})\\\\) Calculated with the following formula: \\\\(SF_{2} = \\frac{l}{A}\\\\)\n16. **ShapeFactor3** - \\\\((SF_{3})\\\\) Calculated with the following formula: \\\\(SF_{3} = \\frac{A}{\\frac{L}{2}*\\frac{L}{2}*\\pi}\\\\)\n17. **ShapeFactor4** - \\\\((SF_{4})\\\\) Calculated with the following formula: \\\\(SF_{4} = \\frac{A}{\\frac{L}{2}*\\frac{l}{2}*\\pi}\\\\)\n18. **y**, the class of the bean. It can be any of ***BARBUNYA*, *SIRA*, *HOROZ*, *DERMASON*, *CALI*, *BOMBAY*, and *SEKER***.\n\n#### **The general features of the specified dry beans are as follows:**\n> - **Cali;** It is white in color, its seeds are slightly plump and slightly larger than dry beans and in shape of kidney.\n> - **Horoz;** Dry beans of this type are long, cylindrical, white in color and generally medium in size.\n> - **Dermason;** This type of dry beans, which are fuller flat, is white in color and one end is round and the other ends are round.\n> - **Seker;** Large seeds, white in color, physical shape is round.\n> - **Bombay;** It is white in color, its seeds are very big and its physical structure is oval and bulging.\n> - **Barbunya;** Beige-colored background with red stripes or variegated, speckled color, its seeds are large, physical shape is oval close to the round.\n> - **Sira;** Its seeds are small, white in color, physical structure is flat, one end is flat, and the other end is round.\n\n<center><img src=\"https:\/\/i.ibb.co\/9hb9jKh\/Capture.jpg\" width=600 height=600\/><\/center>","2b560703":"## Grid Search on MLP Classifier","9bd16887":"- #### The dataset is clean:  no missing values or duplicated entries were found.","a96966ba":"### GET Classes Weight to Handle Imbalanced Classes ","d6da21b5":"## OverSampling The Data Using Smote Approach","b6a97c67":"##### - There are a lot of linearly correlated features:\n- area & convex area : **1.00**\n- compactness & shape factor 3 : **1.00**\n- equivalent diameter & perimeter: **0.99**\n- equivalent diameter & convex area: **0.99**\n- major axis length & perimeter: **0.98**\n- area & perimeter: **0.97**\n- convex area & perimeter : **0.97**\n- major axis length & equivalent diameter : **0.96**\n- minor axis length & equivalent diameter : **0.95**\n- minor axis length & convex area : **0.95**\n- minor axis length & shape factor 1 : **-0.95**\n- eccentricity & compactness : **-0.97**\n- eccentricity & shape factor 3 : **-0.98**\n- aspect ration & shape factor 3 : **-0.98**\n- aspect ration & compactness : **-0.99**\n\n##### **- We may try to select features manually, or use PCA to get independent features.**\n##### **- If a Tree-based model is used, we may not need to drop any features as they don't get affected by correlated features, but the best practice is to remove them when possible.**","04d350c7":"## **Importing Libraries:**","a3b486eb":"- #### The features vary greatly in scales, scaling maybe required.","b02c6e29":"### Check SVC F1_score","9c9b6d41":" - There's an imbalance between the count of each class.\n - Dermason is the most frequent class. **(2837)**\n - Bombay is the least frequent class. **(418)**\n - There's a big difference between the 2 classes, which should to be taken into account when building a model.","ce20fbed":"## Dealing With Test Set\n","ac8f2ddb":"### Transformation With PCA in  PipLine :\n","b8f814fb":"## Correlation Matrix:","56ae8c20":"## Create Voting Classifier using All Models Declared Above","b6d66ce2":"### Check LogesticRegression F1_score","55861013":"## Model Selection Step :\n","3c742434":"## Grid Search on stochastic GradientBoostingRegressory_train_resample","3e2e0ec9":"## Create MLPC with tunned parameters"}}