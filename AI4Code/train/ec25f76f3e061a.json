{"cell_type":{"9e8b84e8":"code","ea039dff":"code","2c335744":"code","38f980bd":"code","f62b84c7":"code","46884331":"code","7f0ddf99":"code","d721bab1":"code","86177ee6":"code","c7193574":"code","5ca30fad":"code","8349e333":"code","bed18a13":"code","6d3bf936":"code","7334442d":"code","c3b28a90":"code","7212e260":"code","ffeec9c3":"code","0eb71839":"code","284145ba":"code","98c6231c":"code","bfb3e60e":"code","2efc56e4":"code","4ec5c5e3":"code","d360f550":"markdown","1495d429":"markdown","f29915f7":"markdown","62dc35af":"markdown","1f5462c5":"markdown","f6c68415":"markdown","79334a0c":"markdown"},"source":{"9e8b84e8":"!pip install praw","ea039dff":"!pip install langdetect","2c335744":"\nimport praw\nimport pandas as pd\nimport datetime as dt\nfrom praw.models import MoreComments\nfrom langdetect import detect\nimport pickle","38f980bd":"import requests\n\nurl = \"https:\/\/api.pushshift.io\/reddit\/search\/submission\"\n\ndef crawl_page(subreddit: str, last_page = None):\n    \"\"\"\n    Crawl a page of results from a given subreddit.\n\n    :param subreddit: The subreddit to crawl.\n    :param last_page: The last downloaded page.\n\n    :return: A page or results.\n    \"\"\"\n    params = {\"subreddit\": subreddit, \"size\": 500, \"sort\": \"desc\", \"sort_type\": \"created_utc\"}\n    if last_page is not None:\n        if len(last_page) > 0:\n            # resume from where we left at the last page\n            params[\"before\"] = last_page[-1][\"created_utc\"]\n        else:\n            # the last page was empty, we are past the last page\n            return []\n    results = requests.get(url, params)\n    if not results.ok:\n        # something wrong happened\n        raise Exception(\"Server returned status code {}\".format(results.status_code))\n    return results.json()[\"data\"]","f62b84c7":"import time\n\ndef crawl_subreddit(subreddit, max_submissions=2000):\n    \"\"\"\n    Crawl submissions from a subreddit.\n    :param subreddit: The subreddit to crawl.\n    :param max_submissions: The maximum number of submissions to download.\n    :return: A list of submissions.\n    \"\"\"\n    submissions = []\n    last_page = None\n    while last_page != [] and len(submissions) < max_submissions:\n        last_page = crawl_page(subreddit, last_page)\n        submissions += last_page\n        time.sleep(3)\n    return submissions[:max_submissions]","46884331":"def subredditArabic(subredditName,limit):\n    lastest_submissions = crawl_subreddit(subredditName,limit)\n    topics_data=praw_submissions_comments(lastest_submissions,subredditName,limit)\n    return topics_data, lastest_submissions","7f0ddf99":"def praw_submissions_comments(lastest_submissions,subredditName,limit_sub):\n    reddit = praw.Reddit(client_id='client', \n                     client_secret='secret', \n                     user_agent='user', \n                     username='username', \n                     password='dardesh')\n    subreddit = reddit.subreddit(subredditName)\n    top_subreddit = subreddit.top(limit=limit_sub)\n    topics_dict = { \"title\":[], \n                    \"id\":[], \n                    \"url\":[],\n                    \"comment\":[]\n                }\n    for sub in lastest_submissions:\n        submission = reddit.submission(id=sub[\"id\"])\n        try:\n            if(detect(submission.title)=='ar'):\n                for top_level_comment in submission.comments:\n                    if isinstance(top_level_comment, MoreComments, ):\n                        continue\n                    try:\n                        if(detect(top_level_comment.body)=='ar'):\n                            topics_dict[\"title\"].append(submission.title)\n                            topics_dict[\"id\"].append(submission.id)\n                            topics_dict[\"url\"].append(submission.url)\n                            topics_dict[\"comment\"].append(top_level_comment.body)\n                    except:\n                        pass\n        except:\n            pass\n    topics_data = pd.DataFrame(topics_dict)\n    topics_data.to_pickle(subredditName) \n    topics_data.to_csv(r'Egypt_subreddit.csv', index = False)\n    return topics_data","d721bab1":"import csv\nimport pandas as pd \n\ndef process_reddits(file_train,file_test,subredditFile):\n    \n    df = pd.read_csv(subredditFile)\n    #shuffling the file\n    print(df.shape[0])\n    df = df.sample(frac=1)\n    print(df.shape[0])\n    train_row=round(df.shape[0]*0.8)\n    counter=0\n    for index, row in df.iterrows():\n        \n        q = '[\u0627\u0646\u062a] : ' + row['title']\n        a = '[\u062f\u0631\u062f\u0634] : ' + clean_comment(row['comment'])\n        if(counter<train_row):\n            file_train.write(q)\n            file_train.write('\\n')\n            file_train.write(a)\n            file_train.write('\\n')\n        else:\n            file_test.write(q)\n            file_test.write('\\n')\n            file_test.write(a)\n            file_test.write('\\n')\n        counter+=1\n            ","86177ee6":"import re\n\n\ndef clean_comment(original_text):\n    original_text = re.sub(r'http\\S+','', original_text)\n    original_text = original_text.replace(\"[\", \"\") \n    original_text = original_text.replace(\"]\", \"\") \n    original_text = original_text.replace(\"{\", \"\") \n    original_text = original_text.replace(\"}\", \"\") \n    original_text = original_text.replace(\"(\", \"\") \n    original_text = original_text.replace(\")\", \"\") \n    original_text = original_text.replace(\"\ufffd\", \"\") \n    original_text = original_text.replace(\"\uf665\", \"\") \n    original_text = original_text.replace(\"**\", \"\")\n    original_text = original_text.replace(\"##\", \"\") \n    original_text = original_text.replace(\"&#x200B;\", \"\") \n    original_text = original_text.replace(\"\\u202c\", \"\") \n\n\n    return original_text","c7193574":"!pip install transformers==4.2.1\n!pip install pyarabic\n!git clone https:\/\/github.com\/aub-mind\/arabert","5ca30fad":"import importlib, pkg_resources, tokenizers\nimportlib.reload(pkg_resources)\nimportlib.reload(tokenizers)","8349e333":"#textwrap enables formating of long text\nimport textwrap\n\nfrom transformers import pipeline, GPT2TokenizerFast\nfrom arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\nfrom arabert.preprocess import ArabertPreprocessor\n\n#you can choose any aragpt2 model since they all have the same preprocessing\n\narabert_processor = ArabertPreprocessor(model_name=\"aragpt2-base\")","bed18a13":"!nvidia-smi","6d3bf936":"import torch\ndevice = 0 if torch.cuda.is_available() else -1\nprint(device)","7334442d":"model_name = \"aubmindlab\/aragpt2-base\"\n\naragpt2_pipeline = pipeline(\"text-generation\",model=model_name,device=device)","c3b28a90":"!cp -r '..\/input\/arabicreddit\/dardesh_train_ar_eg.txt' .\/\n!cp -r '..\/input\/arabicreddit\/dardesh_test_ar_eg.txt' .\/","7212e260":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"aubmindlab\/aragpt2-base\")\n\ntrain_path = '.\/dardesh_train_ar_eg.txt'\ntest_path = '.\/dardesh_test_ar_eg.txt'","ffeec9c3":"from transformers import TextDataset,DataCollatorForLanguageModeling\n\ndef load_dataset(train_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=128)\n\n    test_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=test_path,\n          block_size=128)\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,test_dataset,data_collator\n\ntrain_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)","0eb71839":"from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n\nmodel = AutoModelWithLMHead.from_pretrained(\"aubmindlab\/aragpt2-base\")\n\ntraining_args = TrainingArguments(\n    output_dir=\".\/trained_model\", #The output directory\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs=5, # number of training epochs\n    per_device_train_batch_size=32, # batch size for training\n    per_device_eval_batch_size=32,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps between two evaluations.\n    save_steps=800, # after # steps model is saved\n    warmup_steps=500,# number of warmup steps for learning rate scheduler\n    )\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset)","284145ba":"trainer.train()","98c6231c":"trainer.save_model('.\/trained_model')","bfb3e60e":"!zip -r file.zip .\/trained_model","2efc56e4":"from transformers import pipeline\n\nbot = pipeline('text-generation',model='.\/trained_model', tokenizer='aubmindlab\/aragpt2-base',config={'max_length':35})\n","4ec5c5e3":"\nwhile True:\n    ques = input(\"Question : \")\n\n    inp = '[\u0627\u0646\u062a] : '+ques+'\\n'+'[\u062f\u0631\u062f\u0634] : '\n\n    result = bot(inp)[0]['generated_text']\n\n    print(result)\n\n","d360f550":"<a href=\".\/file.zip\"> Download File <\/a>\n","1495d429":"I used the above method to create dardesh_train_ar files and dardesh_train_ar_eg files. Where the ar files consits of both subreddits and the eg consits of only Egypt subreddit. \n","f29915f7":"# Text Cleaning and Formatting\n\nFirst I shuffled the csv file. Then I did some cleaning and removed links and unknown characters.Then Formated the text to be in chatbot format while splitting the file into training and testing. \n","62dc35af":"# Data Collection and Processing\n\n\nI've collected data from 2 subreddits, Egypt and Arabs and formatted them into chat bot structure and split them into testing and training. \n\nI used pushshift io to crawl all sumbmissions in a sub reddit as reddit api only returns 1000 submissions per subreddit as max.\n\nI followed these tutorials: \n\nhttps:\/\/www.storybench.org\/how-to-scrape-reddit-with-python\/ \n\nhttps:\/\/www.textjuicer.com\/2019\/07\/crawling-all-submissions-from-a-subreddit\/\n\n\nI have mapped each subreddit title to all the comments, mimkcing question answer pairs. I did this for the Arabic and Egypt subreddits. \n\nI have also removed subreddits or comments that were entirely wrriteen in english. I was left with csv files for each subreddit. Each csv file had a coloumn for the title, comment, date and link. ","1f5462c5":"# Model Training\n\n\nI have used these tutorials as a refrence and starting point for training my model.\n\nhttps:\/\/www.philschmid.de\/fine-tune-a-non-english-gpt-2-model-with-huggingface\n\nhttps:\/\/colab.research.google.com\/drive\/1Bz-P-ucyLMaCBmgTjS_QR8RoGsZ5WHwo?usp=sharing\n\n\nI have used a pre-trained arabic gpt2 model, developed by Wessam Antoun and AUC Brain Lab.\n\nhttps:\/\/github.com\/aub-mind\/arabert\/tree\/master\/examples","f6c68415":"# Introduction\n\n\nIn this notebook I am trying to build an Arabic based chatbot with a reddit flare \"Dardesh\", using Hugging face open GPT2. I'll be using a pre-trained arabic model and fine tune it. ","79334a0c":"# Interacting with the model"}}