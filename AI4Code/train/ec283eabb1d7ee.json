{"cell_type":{"e6ac6002":"code","c26802fe":"code","76de4717":"code","70d20ef9":"code","03cf6cdd":"code","a180ed0c":"code","067bf410":"code","39206991":"code","5af0f77e":"code","46b4d0c9":"code","6de79225":"code","2bb3afe0":"code","258bed9e":"code","372c2fe4":"code","d609d163":"code","26d88534":"code","9a9a5ffb":"code","1aae3043":"code","c149cdcd":"code","176b06ef":"code","9f7f2358":"code","1df8515f":"code","3adf26e9":"code","d3834821":"code","473074bf":"code","a8aadc4b":"code","39b0ab41":"code","7f0f70da":"code","4ce3239f":"code","27bd3882":"code","c7e8e380":"code","7371363a":"code","9923fb3e":"code","b6fa0583":"markdown","649cfd82":"markdown","6ee0f9c5":"markdown","c2a36796":"markdown","a2c159c3":"markdown","7b7743f5":"markdown","d90d71fd":"markdown","ac80c618":"markdown","b157404f":"markdown","a511206f":"markdown","fcf6a516":"markdown","3720d413":"markdown","5b014319":"markdown","17f9e9e3":"markdown","d7b6f984":"markdown","a5b9c84c":"markdown","9e185403":"markdown","b5a70249":"markdown"},"source":{"e6ac6002":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nimport gplearn\nfrom gplearn.genetic import SymbolicTransformer, SymbolicRegressor\nfrom gplearn.functions import make_function\nfrom gplearn.fitness import make_fitness\nfrom sklearn.model_selection import StratifiedKFold,KFold\nimport random\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom lightgbm import LGBMClassifier\nfrom bayes_opt import BayesianOptimization\nimport warnings\nimport lightgbm as lgb\nfrom catboost import Pool, CatBoostClassifier\nfrom scipy.stats import rankdata","c26802fe":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","76de4717":"#distribution of classes in our dataset is uneven\nsns.countplot(train_df['target'])","70d20ef9":"train_df.describe()","03cf6cdd":"x = train_df.iloc[:, 2:]\ny = train_df.iloc[:, 1]","a180ed0c":"#using randomized PCA and trying to map the input features into 100 components\nrpca = PCA(n_components=100, svd_solver='randomized')\nrpca.fit(x)\nplt.plot(np.cumsum(rpca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","067bf410":"#defining a fitness function analogous to loss function\ndef _my_fit(y, y_pred, w):\n    return mean_squared_error(y,y_pred)\nmy_fit = make_fitness(_my_fit, greater_is_better=False)","39206991":"# Choose the mathematical functions we will combine together\nfunction_set = ['add', 'sub']\n#We can use the below functions as well to create features\n#'div', 'log', 'sqrt', 'log', 'abs', 'neg', 'inv',  'max', 'min', 'sin', 'cos', 'tan' \n\n# Create the genetic learning regressor\n#generations is an important parameter in this function\ngp = SymbolicRegressor(function_set=function_set, metric = my_fit,\n                       verbose=1, generations = 2, \n                       random_state=0, n_jobs=-1)","5af0f77e":"# Using NUMPY structures, remove one feature (column of data) at a time from the training set\n# Use that removed column as the target for the algorithm\n# Use the genetically engineered formula to create the new feature\n# Do this for both the training set and the test set\n\nX1a = np.array(x)\nsam = X1a.shape[0]\ncol = X1a.shape[1]\nX2a = np.zeros((sam, col))\n\nX_test1a = np.array(test_df.drop('ID_code',axis=1))\nsam_test = X_test1a.shape[0]\ncol_test = X_test1a.shape[1]\nX_test2a = np.zeros((sam_test, col_test))\n\nfor i in range(col) :\n    X = np.delete(X1a,i,1)\n    y = X1a[:,i]\n    gp.fit(X, y) \n    X2a[:,i] = gp.predict(X)\n    X = np.delete(X_test1a, i, 1)\n    X_test2a[:,i] = gp.predict(X)\n    \nX2 = pd.DataFrame(X2a)\nX_test2 = pd.DataFrame(X_test2a) ","46b4d0c9":"# Add the new features to the existing 200 features\nX_test1 = test_df.drop('ID_code',axis=1)\ny = train_df['target']\ntrain_df = pd.concat([x, X2], axis=1, sort=False) \ntest_df = pd.concat([X_test1, X_test2], axis=1, sort=False)  \ntrain_df = pd.concat([train_df,y],axis=1)\ntrain_df.head()\n","6de79225":"#augment train in each fold, don't touch valid and test.\n#upsample positive instances more.\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","2bb3afe0":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n!apt-get install -y -qq libboost-all-dev\n","258bed9e":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)\n","372c2fe4":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile\n!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","d609d163":"#check if the GPU is blocked or not\n!nvidia-smi","26d88534":"#Use Kfold or StratifiedKFold to split the training set for cross validation\n#n_splits is kept around 10 in this competition to prevent overfitting and get better results\nbayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df.iloc[:,2:], train_df.target.values))[0]","9a9a5ffb":"#Defining the lightgbm function with nfolds \nimport lightgbm as lgb\ntrain_df.columns = [str(x) for x in train_df.columns]\ntest_df.columns = [str(x) for x in test_df.columns]\ntarget = 'target'\npredictors = train_df.columns.values.tolist()\npredictors.remove('target')\ndef LGB_bayesian(\n    num_leaves,  # int\n    min_data_in_leaf,  # int\n    learning_rate,\n    min_sum_hessian_in_leaf,    # int  \n    feature_fraction,\n    lambda_l1,\n    lambda_l2,\n    min_gain_to_split,\n    max_depth):\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n\n    param = {\n        'num_leaves': num_leaves,\n        'max_bin': 63,\n        'min_data_in_leaf': min_data_in_leaf,\n        'learning_rate': learning_rate,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'min_gain_to_split': min_gain_to_split,\n        'max_depth': max_depth,\n        'save_binary': True, \n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,   \n\n    }    \n    \n    \n    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n                           label=train_df.iloc[bayesian_tr_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    \n    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n                           label=train_df.iloc[bayesian_val_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    num_round = 50\n    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=10, early_stopping_rounds = 10)\n    print('predict')\n    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n    \n    score = roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n    \n    return score","1aae3043":"#Defining the bounds of parameters for parameter tuning in bayesian optimization\nbounds_LGB = {\n    'num_leaves': (2, 4), \n    'min_data_in_leaf': (5, 8),  \n    'learning_rate': (0.005, 0.01),\n    'min_sum_hessian_in_leaf': (0.0001, 0.01),    \n    'feature_fraction': (0.5, 0.6),\n    'lambda_l1': (1.0, 3.0), \n    'lambda_l2': (0, 1.0), \n    'min_gain_to_split': (0, 1.0),\n    'max_depth':(2,4)\n}","c149cdcd":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)\ninit_points = 8\nn_iter = 8\nprint('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","176b06ef":"#Parameter grid for RandomizedSearchCV\nparam_grid = {\n    'is_unbalance': ['True'],\n    'boosting_type': ['gbdt'],\n    'max_depth' : [2,3],\n    'min_sum_hessian_in_leaf' : [0.0005,0.001],\n    'min_gain_to_split': list(range(0,5)),\n    'max_delta_step' : list(range(0,5)),\n    'lambda_l1': list(range(0,10)),\n    'lambda_l2': list(range(0,1)),\n    'learning_rate': [0.01],\n    'min_data_in_leaf': [50,100,200],\n    'max_bin': [500,750]\n}","9f7f2358":"lgb = LGBMClassifier(verbose=1,metric='auc',objective= 'binary')\nclf = RandomizedSearchCV(estimator = lgb, param_distributions = param_grid, n_iter = 5,\ncv = 3, verbose=2, random_state=42, n_jobs = -1)\nclf.fit(train_df.iloc[:,2:],train_df.target)\nprint(clf.best_params_)\nprint(clf.best_score_)","1df8515f":"#Best parameters from Bayesian Optimization\nprint(LGB_BO.max['target'])\nprint(LGB_BO.max['params'])","3adf26e9":"#Number of iterations has been kept very large in range of 20000 and max_depth,learning rate very small to make the model learn slowly but correctly\n\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1}\n","d3834821":"n_split = 11 #kept around 11 for better results\nkf = KFold(n_splits=n_split, random_state=432013, shuffle=True)\ny_valid_pred = 0 * train_df.target\ny_test_pred = 0","473074bf":"import lightgbm as lgb\nfor idx, (train_index, valid_index) in enumerate(kf.split(train_df)):\n    \n    y_train, y_valid = train_df['target'].iloc[train_index], train_df['target'].iloc[valid_index]\n    X_train, X_valid = train_df[predictors].iloc[train_index,:], train_df[predictors].iloc[valid_index,:]\n    X_tr, y_tr = augment(X_train.values, y_train.values)\n    X_tr = pd.DataFrame(X_tr)\n    \n    print( \"\\nFold \", idx)\n    trn_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    #num_iterations are taken very large , here 50 is just for sample purposes\n    fit_model = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    pred = fit_model.predict(X_valid)\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict(test_df)\ny_test_pred \/= n_split\n    ","a8aadc4b":"lightgbm_results = y_test_pred","39b0ab41":"#iterations should be taken large for better results\nmodel = CatBoostClassifier(loss_function=\"Logloss\",\n                           eval_metric=\"AUC\",\n                           task_type=\"GPU\",\n                           learning_rate=0.01,\n                           iterations=50,\n                           l2_leaf_reg=50,\n                           random_seed=432013,\n                           od_type=\"Iter\",\n                           depth=5,\n                           early_stopping_rounds=10,\n                           border_count=64\n                           #has_time= True \n                          )","7f0f70da":"for idx, (train_index, valid_index) in enumerate(kf.split(train_df)):\n    y_train, y_valid = train_df['target'].iloc[train_index], train_df['target'].iloc[valid_index]\n    X_train, X_valid = train_df[predictors].iloc[train_index,:], train_df[predictors].iloc[valid_index,:]\n    _train = Pool(X_train, label=y_train)\n    _valid = Pool(X_valid, label=y_valid)\n    print( \"\\nFold \", idx)\n    fit_model = model.fit(_train,\n                          eval_set=_valid,\n                          use_best_model=True,\n                          verbose=5000,\n                          plot=True\n                         )\n    pred = fit_model.predict_proba(X_valid)[:,1]\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict_proba(test_df)[:,1]\ny_test_pred \/= n_split","4ce3239f":"catboost_results = y_test_pred","27bd3882":"predict_list = []\npredict_list.append(catboost_results)\npredict_list.append(lightgbm_results)","c7e8e380":"print(\"Rank averaging on \", len(predict_list), \" outputs\")\npredictions = np.zeros_like(predict_list[0])\nfor predict in predict_list:\n    predictions = np.add(predictions, rankdata(predict)\/predictions.shape[0])  \npredictions \/= len(predict_list)\n\ntest_df = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.DataFrame({'ID_code' : test_df['ID_code'], 'target' : predictions})\nsubmission.to_csv('rank_average.csv', index=False)","7371363a":"test_df = pd.read_csv('..\/input\/test.csv')\npredictions = 0.5 * (catboost_results + lightgbm_results)\nsubmission = pd.DataFrame({'ID_code' : test_df['ID_code'], 'target' : predictions})\nsubmission.to_csv('blending.csv', index=False)\n","9923fb3e":"predictions = lightgbm_results\nsubmission = pd.DataFrame({'ID_code' : test_df['ID_code'], 'target' : predictions})\nsubmission.to_csv('lgb_best_params.csv', index=False)\n","b6fa0583":"# FEATURE ENGINEERING","649cfd82":"RANK AVERAGING - \n\nAssign ranks to data, dealing with ties appropriately","6ee0f9c5":"**ALGORITHMS**\n\nLIGHTGBM with DATA AUGMENTATION","c2a36796":"In order to leverage the GPU, we need to set the following parameters:\n*  'device': 'gpu',\n*  'gpu_platform_id': 0,\n* 'gpu_device_id': 0\n\nWe are now good to go on using LightGBM on GPU","a2c159c3":"It prevents overfitting fake interaction appearances. Without data augmentation, it may appear that certain combinations of variables predict target=0 or target=1 but this is just overfitting train. By shuffling values, you remove the possibility of fitting fake interaction appearances in train.","7b7743f5":"PRINCIPAL COMPONENT ANALYSIS","d90d71fd":"CATBOOST","ac80c618":"I have been following this competition since the beginning and it has been a huge learning experience for me as a beginner. I am documenting here all the techniques I learn via kernels and discussions in this competition.They include - \n* EXPLORATORY DATA ANALYSIS - \n>      Basic data exploration \n>      Visualizing target variable distribution\n>      Principal Component Analysis\n*  FEATURE ENGINEERING - \n>      Genetic features engineering\n>      Data Augmentation\n*  ALGORITHMS - \n>       LightGBM (with GPU accelaration)\n>       CatBoost (with GPU accelaration)\n*  HYPERPARAMETER TUNING -\n>       Bayesian Optimization\n>       RandomizedSearchCV\n*  RESULT AGGREGATION - \n>       Ranking\n>       Blending\n     ","b157404f":"**HYPERPARAMETER TUNING**\n\nBefore running the models, we need to tune the parameters. We will be using two techniques to do so :\n* RandomizedSearchCV\n* Bayesian Optimization","a511206f":"In Kaggle notebook setting, set the Internet option to Internet connected, and GPU to GPU on.\nWe first remove the existing CPU-only lightGBM library and clone the latest github repo.","fcf6a516":"DATA AUGMENTATION  --> Generating more samples from existing data to enhance the training set","3720d413":"Reading train and test files","5b014319":"**Exploratory Data Analysis**","17f9e9e3":"Clearly, 100 features can explain only 90 percent of variance in this data.\nIt looks like this data was PCA'd already","d7b6f984":"ALGORITHMS TO USE :\n* LightGBM\n* CatBoost \n\nBefore running any of the models, we will first learn to exploit them on GPU's","a5b9c84c":"**CATBOOST on GPU** - Add  task_type=\"GPU\" in model params","9e185403":"GENETIC FEATURE ENGINEERING","b5a70249":"BLENDING"}}