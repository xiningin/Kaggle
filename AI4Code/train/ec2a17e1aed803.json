{"cell_type":{"718e3bd9":"code","56196eaa":"code","c85fcc9c":"code","595eeea3":"code","2a5e507f":"code","e73c8822":"code","83f8ad59":"code","32e1e017":"code","a29b5da4":"code","96df0dbb":"code","bd69b50d":"code","585347da":"code","0aeedbc8":"code","6804820b":"code","2a05db39":"code","28921393":"code","edf2cc16":"code","dafdcac3":"code","5cb374ee":"code","45f3469d":"code","a2c956b7":"code","603ee22c":"code","08dd0dd1":"code","a6412974":"code","84d15fb3":"code","4ad65f01":"code","cc0939cc":"code","661395e8":"code","5ff80775":"code","a6e47a24":"code","b25af78c":"code","a6338e83":"code","c8158843":"code","dcc578b2":"code","39d3df24":"code","0631c8fe":"code","95abcea5":"markdown","83e68145":"markdown","4789b6dc":"markdown","b3fb7c8c":"markdown","e0522320":"markdown","342c73fa":"markdown","a56700a8":"markdown","e323713c":"markdown","94008c68":"markdown","91061c92":"markdown","425c9402":"markdown","5117042a":"markdown","365c78c7":"markdown","6dd7c522":"markdown","bc68ba19":"markdown","5d8944c6":"markdown","c4d19695":"markdown","6830ceaf":"markdown","a4686d6f":"markdown"},"source":{"718e3bd9":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","56196eaa":"products = pd.read_csv('..\/input\/amazon_baby.csv')\nproducts.head()","c85fcc9c":"import string\nproducts['review_clean']=products['review'].str.replace('[{}]'.format(string.punctuation), '')\nproducts.head()","595eeea3":"products = products.fillna({'review':''})  # fill in N\/A's in the review column","2a5e507f":"products = products[products['rating'] != 3]","e73c8822":"products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)","83f8ad59":"products.sample(5)","32e1e017":"from sklearn.model_selection import train_test_split","a29b5da4":"train_data,test_data = train_test_split(products,test_size = 0.20)","96df0dbb":"print('Size of train_data is :', train_data.shape)\nprint('Size of test_data is :', test_data.shape)","bd69b50d":"import gc\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\nvectorizer = HashingVectorizer(token_pattern=r'\\b\\w+\\b')\n\ntrain_matrix = vectorizer.transform(train_data['review_clean'].values.astype('U'))\ntest_matrix = vectorizer.transform(test_data['review_clean'].values.astype('U'))\ngc.collect()","585347da":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nsentiment_model = clf.fit(train_matrix,train_data['sentiment'])","0aeedbc8":"y_pred = sentiment_model.predict(test_matrix)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(sentiment_model.score(test_matrix, test_data.sentiment)))","6804820b":"model_coef = pd.DataFrame(sentiment_model.coef_)\nmodel_coef","2a05db39":"from scipy.stats import norm\nplt.figure(figsize = (15,7))\nsns.distplot(model_coef, fit = norm);\n","28921393":"pos_coef = np.sum(sentiment_model.coef_>-0)\nprint('Positive Coefficients are : ',pos_coef)","edf2cc16":"sample_test_data = test_data[10:13]\nsample_test_data","dafdcac3":"sample_test_data.iloc[0]['review']","5cb374ee":"sample_test_data.iloc[2]['review']","45f3469d":"sample_test_matrix = vectorizer.transform(sample_test_data['review_clean'])\nscores = sentiment_model.decision_function(sample_test_matrix)\nprint(scores)","a2c956b7":"for i in scores:\n    print(1\/(1+np.exp(-i)))","603ee22c":"test_scores = sentiment_model.decision_function(test_matrix)\nprint(test_scores)","08dd0dd1":"positive_reviews = np.argsort(-test_scores)[:20]\nprint(positive_reviews)\nprint(test_scores[positive_reviews[0]])\ntest_data.iloc[positive_reviews]","a6412974":"negative_reviews = np.argsort(test_scores)[:20]\nprint(negative_reviews)\nprint(test_scores[negative_reviews[0]])\ntest_data.iloc[negative_reviews]","84d15fb3":"predicted_y = sentiment_model.predict(test_matrix)\ncorrect_num = np.sum(predicted_y == test_data['sentiment'])\ntotal_num = len(test_data['sentiment'])\nprint(\"correct_num: {}, total_num: {}\".format(correct_num, total_num))\naccuracy = correct_num * 1.\/ total_num\nprint(accuracy)","4ad65f01":"significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n      'work', 'product', 'money', 'would', 'return']","cc0939cc":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_word_subset = CountVectorizer(vocabulary=significant_words) # limit to 20 words\ntrain_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'].astype('U'))\ntest_matrix_word_subset = vectorizer_word_subset.transform(test_data['review_clean'].astype('U'))","661395e8":"simple_model = LogisticRegression()\nsimple_model.fit(train_matrix_word_subset, train_data['sentiment'])","5ff80775":"simple_model_coef_table = pd.DataFrame({'word':significant_words,\n                                         'coefficient':simple_model.coef_.flatten()})\n#simple_model_coef_table\nsimple_model_coef_table.sort_values(['coefficient'], ascending=False)","a6e47a24":"len(simple_model_coef_table[simple_model_coef_table['coefficient']>0])","b25af78c":"model_coef_table = pd.DataFrame({'word':significant_words,\n                                         'coefficient':simple_model.coef_.flatten()})\n#simple_model_coef_table\nsimple_model_coef_table.sort_values(['coefficient'], ascending=False)","a6338e83":"vectorizer_word_subset.get_feature_names()","c8158843":"train_predicted_y = sentiment_model.predict(train_matrix)\ncorrect_num = np.sum(train_predicted_y == train_data['sentiment'])\ntotal_num = len(train_data['sentiment'])\nprint(\"correct_num: {}, total_num: {}\".format(correct_num, total_num))\ntrain_accuracy = correct_num * 1.\/ total_num\nprint(\"sentiment_model training accuracy: {}\".format(train_accuracy))\n\ntrain_predicted_y = simple_model.predict(train_matrix_word_subset)\ncorrect_num = np.sum(train_predicted_y == train_data['sentiment'])\ntotal_num = len(train_data['sentiment'])\nprint(\"correct_num: {}, total_num: {}\".format(correct_num, total_num))\ntrain_accuracy = correct_num * 1.\/ total_num\nprint(\"simple_model training accuracy: {}\".format(train_accuracy))","dcc578b2":"test_predicted_y = sentiment_model.predict(test_matrix)\ncorrect_num = np.sum(test_predicted_y == test_data['sentiment'])\ntotal_num = len(test_data['sentiment'])\nprint(\"correct_num: {}, total_num: {}\".format(correct_num, total_num))\ntest_accuracy = correct_num * 1.\/ total_num\nprint(\"sentiment_model training accuracy: {}\".format(test_accuracy))\n\ntest_predicted_y = simple_model.predict(test_matrix_word_subset)\ncorrect_num = np.sum(test_predicted_y == test_data['sentiment'])\ntotal_num = len(test_data['sentiment'])\nprint(\"correct_num: {}, total_num: {}\".format(correct_num, total_num))\ntest_accuracy = correct_num * 1.\/ total_num\nprint(\"simple_model training accuracy: {}\".format(test_accuracy))","39d3df24":"positive_label = len(test_data[test_data['sentiment']>0])\nnegative_label = len(test_data[test_data['sentiment']<0])\nprint(\"positive_label is {}, negative_label is {}\".format(positive_label, negative_label))","0631c8fe":"baseline_accuracy = positive_label*1.\/(positive_label+negative_label)\nprint(\"baseline_accuracy is {}\".format(baseline_accuracy))","95abcea5":"### Learn another classifier with fewer words","83e68145":"Consider the coefficients of simple_model. How many of the 20 coefficients (corresponding to the 20 significant_words) are positive for the simple_model?","4789b6dc":"**There should be over 100,000 coefficients in this sentiment_model. Recall from the lecture that positive weights w_j correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. Calculate the number of positive (>= 0, which is actually nonnegative) coefficients.**","b3fb7c8c":"#### If score is greater than zero, sentiment is positive and negative otherwise.","e0522320":"### Train a sentiment classifier with logistic regression.","342c73fa":"###### We'll ignore all reviews with rating 3 as it indicates average.","a56700a8":"### Training a Simple Model","e323713c":"### Split into training and test sets\nLet's perform a train\/test split with 80% of the data in the training set and 20% of the data in the test set. If you are using SFrame, make sure to use seed=1 so that you get the same result as everyone else does. (This way, you will get the right numbers for the quiz.)","94008c68":"### Assigning sentiment column\n\n**Now, we will assign reviews with a rating of 4 or higher to be positive reviews, while the ones with rating of 2 or lower are negative. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label. A good way is to create an anonymous function that converts a rating into a class label and then apply that function to every element in the rating column.**","91061c92":"#### Let's dig deeper into the first row of the sample_test_data. Here's the full review:","425c9402":"#### Probability that sentiment is 1","5117042a":"### Making predictions with logistic regression\nNow that a model is trained, we can make predictions on the test data. In this section, we will explore this in the context of 3 data points in the test data. Take the 11th, 12th, and 13th data points in the test data and save them to sample_test_data. The following cell extracts the three data points from the  test_data and print their content:","365c78c7":"#### That review seems pretty positive. Now, let's see what the next row of the sample_test_data looks like. As we could guess from the rating (-1), the review is quite negative.","6dd7c522":"It is quite common to use the majority class classifier as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.","bc68ba19":"#### Build the word count vector for each review\nWe will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as bag-of-word features. Since most words occur in only a few reviews, word count vectors are sparse. For this reason, scikit-learn and many other tools use sparse matrices to store a collection of word count vectors. Refer to appropriate manuals to produce sparse word count vectors. General steps for extracting word count vectors are as follows:\n\nLearn a vocabulary (set of all words) from the training data. Only the words that show up in the training data will be considered for feature extraction.\nCompute the occurrences of the words in each review and collect them into a row vector.\nBuild a sparse matrix where each row is the word count vector for the corresponding review. Call this matrix train_matrix.\nUsing the same mapping between words and columns, convert the test data into a sparse matrix test_matrix.\nThe following cell uses CountVectorizer in scikit-learn. Notice the token_pattern argument in the constructor.","5d8944c6":"#### Now, let's see what the next row of the sample_test_data looks like. As we could guess from the rating (-1), the review is quite negative.","c4d19695":"#### This dataset is Course Assignment in Machine Learning course by University of Washington.\n[ML-Classification](https:\/\/www.coursera.org\/learn\/ml-classification\/exam\/gLYyI\/predicting-sentiment-from-product-reviews)","6830ceaf":"#### Find the most positive (and negative) review\n We now turn to examining the full test dataset, test_data, and use sklearn.linear_model.LogisticRegression to form predictions on all of the test data points.\n\nUsing the sentiment_model, find the 20 reviews in the entire test_data with the highest probability of being classified as a positive review. We refer to these as the \"most positive reviews.\"\n\n**To calculate these top-20 reviews, use the following steps:**\n\nMake probability predictions on test_data using the sentiment_model.\nSort the data according to those predictions and pick the top 20.","a4686d6f":"Are the positive words in the simple_model also positive words in the sentiment_model?"}}