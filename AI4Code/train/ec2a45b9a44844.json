{"cell_type":{"d3e61821":"code","053ef39e":"code","bdf76fc3":"code","251670c9":"code","6fe01efc":"code","e9f666f7":"code","9e3d8118":"code","b3755f63":"code","0f4d64ef":"code","b88801f0":"code","754c0bff":"markdown","c9b7a8f7":"markdown","017d1fd5":"markdown","225f84cf":"markdown","e45f9f7a":"markdown","0dadf196":"markdown","b6e4d263":"markdown","b7fb36bd":"markdown"},"source":{"d3e61821":"# import system modules\nimport os\nimport sys\nimport datetime\nimport random\n\n# import external helpful libraries\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport h5py\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\n# import keras\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization \nfrom keras.layers import Input, UpSampling2D, concatenate  \nfrom keras.optimizers import Nadam, SGD\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, TensorBoard\n\n# possible libraries for metrics\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n\n#K-Fold Cross Validation\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Set the random seed to ensure reproducibility\nnp.random.seed(1234)\ntf.random.set_seed(1234)","053ef39e":"#Creating Data Generator\nclass xray_data_generator(keras.utils.Sequence):\n    \"\"\"\n    Data generator derived from Keras' Sequence to be used with fit_generator.\n    \"\"\"\n    def __init__(self, seq, dims=(331,331), batch_size=32, shuffle=True):\n        # Save params into self\n        self.dims = dims\n        self.batch_size = batch_size\n        self.seq = seq\n        self.shuffle = shuffle\n        \n        # create data augmentor\n        self.aug = iaa.SomeOf((0,3),[\n                #iaa.Fliplr(), # horizontal flips\n                iaa.Affine(scale={\"x\": (0.75, 1.25), \"y\": (0.75, 1.25)}),\n                iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}),    \n                iaa.Affine(rotate=(-10, 10)), # rotate images\n                iaa.Multiply((0.8, 1.2)),  #random brightness\n                iaa.Affine(shear=(-10, 10)),\n                #iaa.GammaContrast((0.8, 1.2)),\n                iaa.GaussianBlur(sigma=(0.0, 1.0))\n                                    ],\n                random_order=True\n                             )\n\n        # shuffle the dataset\n        if self.shuffle:\n          random.shuffle(self.seq)    \n\n    def get_data(self, index):\n        '''\n        Given an index, retrieve the image and apply processing,\n        including resizing and converting color encoding. This is\n        where data augmentation can be added if desired.\n        '''\n        img_path, class_idx = self.seq[index]\n        # Load the image\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, self.dims)\n\n        # if grayscale, convert to RGB\n        if img.shape[-1] == 1:\n            img = np.stack((img,img,img), axis=-1)\n\n        # by default, cv2 reads images in using BGR format\n        # we want to convert it to RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # normalize values to [0, 1]\n        img = img.astype(np.float32)\/255.\n\n        # augment image\n        img = self.aug.augment_image(img)\n\n        # Load the labels\n        label = keras.utils.to_categorical(class_idx, num_classes=2)\n        \n        return img, label\n      \n    def get_classes(self):\n        class_idxs = [class_idx for _, class_idx in self.seq]\n        return np.array(class_idxs)\n\n    def __len__(self):\n        '''\n        Returns the number of batches per epoch.\n        Used by Keras' fit_generator to determine number of training steps.\n        '''\n        return int(np.floor(len(self.seq) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        '''\n        Actual retrieval of batch data during training.\n        Data is retrieved by calling self.get_data on an index\n        which is then batched, and returned\n        '''\n        # create empty batches\n        batch_img = np.empty((self.batch_size,) + self.dims + (3,))\n        batch_label = np.empty((self.batch_size,) + (2,))\n\n        # load the images and labels into the batch\n        # using the get_data method defined above\n        for i in range(self.batch_size):\n            img, label = self.get_data(index*self.batch_size+i)    \n            batch_img[i] = img\n            batch_label[i] = label\n\n        return batch_img, batch_label\n\n    def on_epoch_end(self):\n        '''\n        Shuffles the data sequence after each epoch\n        '''\n        if self.shuffle:\n          random.shuffle(self.seq)","bdf76fc3":"# Edit this variable to point to your dataset, if needed\ndataset_path = \"..\/input\/gametei2020\/dataset\"\n\n#Form a full dataset \ndef combine_dataset(dataset_path, split1, split2):\n    split1_path = os.path.join(dataset_path, split1)\n    split2_path = os.path.join(dataset_path, split2)\n    data_out = []\n    \n    # iterate each class\n    classes = [\"NORMAL\", \"PNEUMONIA\"]\n    # notice that class_idx = 0 for NORMAL, 1 for PNEUMONIA\n    for class_idx, _class in enumerate(classes):\n        class_path1 = os.path.join(split1_path, _class) # path to each class dir\n        class_path2 = os.path.join(split2_path, _class)\n        # iterate through all files in dir\n        for filename in os.listdir(class_path1):\n            # ensure files are images, if so append to output\n            if filename.endswith(\".jpeg\"):\n                img_path = os.path.join(class_path1, filename)\n                data_out.append((img_path, class_idx))\n        for filename in os.listdir(class_path2):\n            # ensure files are images, if so append to output\n            if filename.endswith(\".jpeg\"):\n                img_path = os.path.join(class_path2, filename)\n                data_out.append((img_path, class_idx))\n                \n    return data_out\ndataset_seq = combine_dataset(dataset_path,split1 = \"train\",split2 = \"val\")\ndataset_pneumonia_cases = sum([class_idx for (img_path, class_idx) in dataset_seq])\ndataset_normal_cases = len(dataset_seq) - dataset_pneumonia_cases\nprint(\"Combined - Total: %d, Normal: %d, Pneumonia: %d\" % (len(dataset_seq), dataset_normal_cases, dataset_pneumonia_cases))","251670c9":"#Loading Splits\nsplit_df = pd.read_csv(\"..\/input\/splitting\/Split.csv\")\nn_folds = 4\nfold_seq = [[]for i in range(n_folds)]\n\nfor i in range(dataset_pneumonia_cases*2):\n    if split_df['fold'][i] == 1:\n        fold_seq[0].append((split_df['img'][i],split_df['class'][i]))\n    if split_df['fold'][i] == 2:\n        fold_seq[1].append((split_df['img'][i],split_df['class'][i]))\n    if split_df['fold'][i] == 3:\n        fold_seq[2].append((split_df['img'][i],split_df['class'][i]))\n    if split_df['fold'][i] == 4:\n        fold_seq[3].append((split_df['img'][i],split_df['class'][i]))   \n\nn_fold_pneumonia_cases = []\nn_fold_normal_cases = []\n\nfor j in range(n_folds):   \n    n_fold_pneumonia_cases.append (sum([class_idx for (img_path, class_idx) in fold_seq[j]])) #compute pneumonia cases by summing the total number of 1's\n    n_fold_normal_cases.append (len(fold_seq[j]) - n_fold_pneumonia_cases [j])                       # subtract from total to get normal cases\n    print(\"Oversampled Train split %d - Total: %d, Normal: %d, Pneumonia: %d\" % (j+1, len(fold_seq[j]), n_fold_normal_cases[j], n_fold_pneumonia_cases[j]))","6fe01efc":"#DenseNet201\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.applications import DenseNet201\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom keras.models import Model\nfrom keras import backend as K\n\n#Define the Base Model\ninput_shape=(331,331,3)\n\nbase_model = DenseNet201(\n    include_top=False,\n    weights='imagenet',\n    input_shape=input_shape,\n)\nbase_model.trainable = True\n\n#Adding dense layers into the pretrained DenseNet201 model\nmodel = models.Sequential()\nmodel.add(base_model)\nmodel.add(layers.GlobalAveragePooling2D())\n#model.add(BatchNormalization())\n#model.add(layers.Dense(1024, activation='elu',kernel_initializer='he_uniform', kernel_regularizer='l2'))\n#model.add(layers.Dropout(0.5))\n#model.add(layers.Dense(512, activation='elu',kernel_initializer='he_uniform', kernel_regularizer='l2'))\n#model.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(256, activation='elu',kernel_initializer='he_uniform', kernel_regularizer='l2'))\nmodel.add(layers.Dropout(0.5))\n#model.add(layers.Dense(128, activation='elu',kernel_initializer='he_uniform', kernel_regularizer='l2'))\n#model.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2, activation='sigmoid'))\n\n#model = Model(inputs=base_model.input, outputs=x)\nmodel.summary()\n#for i in range(14):\n# model.layers[i+1].trainable = False","e9f666f7":"# Setup training parameters\nlearning_rate = 1e-4\nepochs = 40\nearly_stop_patience = 5\n\n# Define optimizer\n# Here we are using the Adam optimizer which is usually a good starting point\noptimizer = Nadam(lr=learning_rate)\n# optimizer = SGD(lr=learning_rate)   # SGD Optimizer\n\n# Define callbacks for training\n# Early stop allows us to stop the training when there is no perceived\n# improvement anymore, based on the defined patience\n# COMMENTED OUT FOR BASELINE MODEL, PROVIDED FOR YOUR EASE OF USE\nearly_stop = EarlyStopping(monitor='val_loss',\n                            patience=early_stop_patience, \n                            verbose=1, \n                            mode='min')\n\n\n# Set up tensorboard for logging\n# setup logdir based on datetime\nlog_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")   # rename the folders for each trial as you want\ntensorboard_callback = TensorBoard(log_dir=log_dir)\n\n#Using focal loss as loss function \ndef focal_loss(alpha=0.25,gamma=2.0):\n    def focal_crossentropy(y_true, y_pred):\n        bce = K.binary_crossentropy(y_true, y_pred)\n        \n        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n        p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n        \n        alpha_factor = 1\n        modulating_factor = 1\n\n        alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n        modulating_factor = K.pow((1-p_t), gamma)\n\n        # compute the final loss and return\n        return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n    return focal_crossentropy\n\n# Compile the model based on our defined metrics and optimizer        \nmodel.compile(loss=focal_loss(), \n            metrics=['AUC'], \n            optimizer=optimizer) ","9e3d8118":"val_seq = []\ntrain_seq = []\nval_seq = fold_seq[3]  #We choose fold 4 as the validation data\nfor j in range(n_folds): #Other folds are used for training\n    if (j!=3): \n        train_seq += fold_seq[j] \n\ntrain_pneumonia_cases = sum([class_idx for (img_path, class_idx) in train_seq])   # compute pneumonia cases by summing the total number of 1's\ntrain_normal_cases = len(train_seq) - train_pneumonia_cases                       # subtract from total to get normal cases\nval_pneumonia_cases = sum([class_idx for (img_path, class_idx) in val_seq])       # compute pneumonia cases for validation dataset\nval_normal_cases = len(val_seq) - val_pneumonia_cases                             # compute normal cases for validation dataset\n\nprint(\"Train - Total: %d, Normal: %d, Pneumonia: %d\" % (len(train_seq), train_normal_cases, train_pneumonia_cases))\nprint(\"Validation - Total: %d, Normal: %d, Pneumonia: %d\" % (len(val_seq), val_normal_cases, val_pneumonia_cases))\ntrain_gen = xray_data_generator(train_seq) #input into data_generator\nval_gen = xray_data_generator(val_seq)     \n\n#Introduce class weights\nweight_for_0 = train_pneumonia_cases \/ len(train_seq)\nweight_for_1 = train_normal_cases \/ len(train_seq)\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n#print(class_weight)\n\n#Batch Sizes and Step Sizes\nbatch_size = 32\ntraining_step_size = len(train_seq)\/\/batch_size\nvalidation_step_size = len(val_seq)\/\/batch_size\n\n# ModelCheckpoint saves the best model weight so far.\ncheckpt = ModelCheckpoint(monitor='val_loss',\n                        filepath = 'best_DenseNet201_final.h5',\n                        mode = 'min',\n                        save_best_only=True)","b3755f63":"#Run the training\nhistory = model.fit_generator(train_gen, \n                          epochs=epochs, \n                          verbose=1, \n                          validation_data=val_gen,\n                          callbacks=[checkpt, \n                                    early_stop,   # COMMENTED OUT FOR BASELINE\n                                    tensorboard_callback    # may cause warning abt on_train_batch_end, also may need to be commented out on GCP\n                                    ],\n                          class_weight=class_weight,\n                          steps_per_epoch  = training_step_size,\n                          validation_steps = validation_step_size\n                          )\n","0f4d64ef":"# Model evaluation \nmodel = keras.models.load_model('best_DenseNet201_final.h5', custom_objects={'focal_crossentropy': focal_loss})\n\n# Create a validation generator that does not shuffle\n# This will allow our predicted value to match our true values in sequence\nnoshuf_val_gen = xray_data_generator(val_seq, batch_size=2, shuffle=False)\n\n# Predicted values\nraw = np.array([])\nraw = model.predict_generator(noshuf_val_gen)\npreds = np.argmax(raw, axis=1)\n# True values\ntrues = noshuf_val_gen.get_classes()\n\n# Compute metrics\nacc = accuracy_score(trues, preds)\nprec = precision_score(trues, preds)\nrec = recall_score(trues, preds)\nf1 = f1_score(trues, preds)\nauc = roc_auc_score(trues, preds)\n\n# Print metrics summary\nprint(\"Evaluation of model on val split 4:\")\nprint(\"Accuracy: %.3f\" % acc)\nprint(\"Precision: %.3f\" % prec)\nprint(\"Recall: %.3f\" %  rec)\nprint(\"F1: %.3f\" % f1)\nprint(\"AUC: %.3f\" % auc)\n\n\nfrom sklearn.metrics import confusion_matrix\nresults = confusion_matrix(trues, preds)\nprint(results)","b88801f0":"from mlxtend.plotting import plot_confusion_matrix\nplt.figure()\nplot_confusion_matrix(results,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","754c0bff":"# Chapter 5 - The DenseNet201 Model \n\n\nIn this chapter, we are going to build a model based on the DenseNet201 model on Keras.  which can then be used to train up our models in the later chapters.  \n\n## What is DenseNet201?  \n\nDenseNet further revived the previous ResNet by applying concatenation, so that the convolution layers are not only consecutively or skipped connected, but fully connected to each other. For example, the weights and outputs of layer 1 will not only propagate to the layer 2 or 3, but all other future layers. This allow layers to have a \"collective knowledge\" of the preceeding layers. With such intuition and architecture, DenseNet can more compact and thin, greatly enhancing the efficiency, while still maintain a comparable feature diversity and prediction performance.  \n\nYou may view more about DenseNet from this [blog](https:\/\/towardsdatascience.com\/review-densenet-image-classification-b6631a8ef803). You may also learn the details of DenseNet201 by reading the [original paper](https:\/\/arxiv.org\/pdf\/1608.06993.pdf).\n\n\n\n### Chapter Specifications:    \n**Input**:   \nGameTei2020 Folder (Containing labeled Train and Validation dataset, and unlabeled Test dataset)  \nSplit.csv (Store the splitting information of data)  \n\n**Output**:      \nbest_DenseNet201_final.h5 (Best model obtained during the training of DenseNet201)  \n  \n[Part 5.1 Data Generator](#Generator)  \n[Part 5.2 Loading the Dataset and Splits](#Load)  \n[Part 5.3 Building The Model](#Build)  \n[Part 5.4 Tuning the Model](#Tune)  \n[Part 5.5 Training The Model](#Train)  \n[Part 5.6 Model Evaluation](#Evaluate)","c9b7a8f7":"<div id='Tune'><\/div>  \n\n# Part 5.4 Tuning the Hyperparameters  \n\nWe kept most of the parameters in the starter notebook, but also made some changes. We also made 2 significant changes to the original model which include:   \n\n* Nadam: Nesterov-accelerated Adaptive Moment Estimation combines Nesterov-accelerated Gradient and Adam, which makes the gradient descent converges much quicker and probably more accurate too. You may read more about activation function [here](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/activation_functions.html). \n* Focal Loss: We customized a focal loss to replace the traditional binary cross entropy. Focal loss is a modifies binary loss by giving more focus on wrong classification than further improving already correct classifications. You can find more about focal loss [here](https:\/\/towardsdatascience.com\/neural-networks-intuitions-3-focal-loss-for-dense-object-detection-paper-explanation-61bc0205114e).  ","017d1fd5":"<div id=\"Load\"><\/div>\n\n# Part 5.2 Loading the Dataset and Splits\n\nNow we can load our combined dataset, and then by using the splitting information stored in split.csv, we divide the dataset into 4 folds. ","225f84cf":"<div id = 'Evaluate'><\/div>  \n\n# Part 5.6 Model Evaluation  \n\nIn the end of the chapter, we will evaluate our model by the our validation fold. We also construct a confusion matrix to better visualize the performance of our model.","e45f9f7a":"<div id = \"Build\"><\/div>  \n\n# Part 5.3 Building The Model\n\nIn this part we construct our DenseNet201 model. First we download the DenseNet201 model framework from [keras.applications](https:\/\/keras.io\/api\/applications\/), and then load the pre-trained weights from [imagenet](http:\/\/www.image-net.org\/). Afterwards, we add 1 dense hidden layer (we also attempted more layers but the AUC is comparable), 1 dropout layer and an output layer. For the dense layer, we also tried exponential linear unit (elu) as the activation, which performs better at negative value and gives smooths out slowly. More information can be found [here](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/activation_functions.html).  ","0dadf196":"<div id = \"Train\"><\/div>  \n\n# Part 5.5 Training the Model\n\nNow we can finally run the model. For our DenseNet201 model, we will train on split 1,2,3 and validate with split 4 of our dataset. ","b6e4d263":"<div id =\"Generator\"><\/div>  \n\n# Part 5.1 Define the Data Generator \n\nFirst we define the function *x_ray_data_generator* function, which is used together with *fit_generator* to transform our dataset into a datatype readable by the model. We can also specify our image augmentations here. In our example, we performed scaling, translation, rotation, brightness modification shearing and blurring to the images, and each image can perform 0 to 3 of these augmentations. We did not use *iaa.OneOf* because we believe more augmentations can be done simultaneously on a single image, and we avoided more than 3 augmentations because this can potentially distort our images. Besides, we also meticulously choose the augmentors, such as the parameters and deleting some less possible ones (i.e. horizontal flips are less probable in CXRs because doctors rarely flip them and rarely do we see situs inversus patients). For more information about agmentators, please read [here](\"https:\/\/imgaug.readthedocs.io\/en\/latest\/\").","b7fb36bd":"# CodeBlue's Solution to Kaggle CXR Pneumonia Classification   \n# Highest Public Score: 0.96434  \n\n\nTeam Composition:  \n\nWong Tsz Him (Leader)  \nMatilde Biaconi, Prachi Shah (Members)  \n  \n## Content:\n* Chapter 1 - Data Pre-processing  \n* Chapter 2 - The Xception Model  \n* Chapter 3 - The InceptionResNetV2 Model  \n* Chapter 4 - The InceptionV3 Model  \n* ->  Chapter 5 - The DenseNet Model  \n* Chapter 6 - Ensemble Learning and Final Output  \n\nKey Changes and Features:  \n1. K-Fold Splitting and Cross Validation (Chapter 1)\n2. Minority Data Oversampling (Chapter 1)\n3. New Image Augmentators (Chapter 2,3,4,5)\n4. Transfer Learning (Xception, InceptionResNetV2, InceptionV3, DenseNet201) (Chapter 2,3,4,5)\n5. New Functions and Hyperparameters (NAdam, Focal_Loss) (Chapte 2,3,4,5)    \n6. Ensemble Learning (Voting, Sum, Neural Network Ensembling) (Chapter 6)\n\n\n\n"}}