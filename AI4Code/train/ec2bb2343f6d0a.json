{"cell_type":{"5316aceb":"code","4aae10aa":"code","c0fac475":"code","5eb6b014":"code","6e2b9157":"code","ecbcdf6e":"code","eadba0ad":"code","ad093cb4":"code","b0abec6a":"code","f5b7c2fe":"code","222f2de1":"code","e4823115":"code","665c0bff":"code","8dda3793":"code","088a12fd":"code","e3d70096":"code","f6e4d8d7":"code","460d360f":"code","d1b00a9c":"code","f4662735":"code","a1ddb5c4":"code","711c828f":"code","064b81bf":"code","0a405728":"code","931b7a8f":"code","aa393715":"markdown","fec8086c":"markdown","f7bbb42f":"markdown","4909c0b8":"markdown","bce3365e":"markdown","c4f7f6f6":"markdown","3ef88c08":"markdown","aebdc8d4":"markdown","3bf812b5":"markdown","121a1df9":"markdown","a15f67c9":"markdown","866c279e":"markdown","9d3fc204":"markdown","65d83e11":"markdown","b771a65d":"markdown"},"source":{"5316aceb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4aae10aa":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\n\nfrom copy import deepcopy\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport optuna\nfrom optuna import Trial, visualization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score, mean_squared_error","c0fac475":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\n\ntrain_df.head()","5eb6b014":"feature_cols = train_df.drop(['id', 'target'], axis=1).columns\n\nx = train_df[feature_cols]\ny = train_df['target']\n\nprint(x.shape, y.shape)","6e2b9157":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_indexs = train_df.index\ntest_indexs = test_df.index\n\ndf =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\ndf = df.drop('id', axis=1)\n\nlen(train_indexs), len(test_indexs)","ecbcdf6e":"def fix_skew(features):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numerical_columns = features.select_dtypes(include=['int64','float64']).columns\n\n    # Check the skew of all numerical features\n    skewed_features = features[numerical_columns].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n    high_skew = skewed_features[abs(skewed_features) > 0.5]\n    skewed_features = high_skew.index\n\n    # Perform the Box-Cox transformation\n    for column in skewed_features:\n        features[column] = boxcox1p(features[column], boxcox_normmax(features[column] + 1))\n        \n    return features","eadba0ad":"# I want to thanks @masumrumi for sharing this amazing plot!\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );","ad093cb4":"df.info()","b0abec6a":"plotting_3_chart(df, 'target')","f5b7c2fe":"num_rows, num_cols = 4,4\n\nf, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n#f.suptitle('Distribution of Features', fontsize=16)\n\nfor index, column in enumerate(df[feature_cols].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    g = sns.distplot(train_df[column], color=\"m\", label=\"%.2f\"%(train_df[column].skew()), ax=axes[i,j])\n    g = g.legend(loc=\"best\")\n\n\nplt.tight_layout()\nplt.show()","222f2de1":"corr = df[feature_cols].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8})\n# yticks\nplt.yticks(rotation=0)\nplt.show()","e4823115":"param_grid = {\n    'n_estimators': [5, 10, 15, 20],\n    'max_depth': [2, 5, 7, 9]\n}\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n# Instantiate model with 100 decision trees\nclf = XGBRegressor(random_state = 42)\n\nclf.fit(x_train, y_train)","665c0bff":"# Use the forest's predict method on the test data\npredictions = clf.predict(x_test)\n\n# Calculate the absolute errors\nerrors = abs(predictions - y_test)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n","8dda3793":"def objective(trial,data=x,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    \n    # To select which parameters to optimize, please look at the XGBoost documentation:\n    # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n    param = {\n        'tree_method':'gpu_hist',  # Use GPU acceleration\n        'lambda': trial.suggest_loguniform(\n            'lambda', 1e-3, 10.0\n        ),\n        'alpha': trial.suggest_loguniform(\n            'alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6,0.7,0.8,1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [150, 200, 300, 3000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [4,5,7,9,11,13,15,17]\n        ),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int(\n            'min_child_weight', 1, 300\n        ),\n    }\n    model = XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","088a12fd":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=5)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","e3d70096":"study.trials_dataframe().head()","f6e4d8d7":"# plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","460d360f":"# plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","d1b00a9c":"# plot feature importance for algorithm parameters\nvisualization.plot_param_importances(study)","f4662735":"# finally plot best parameters\nstudy.best_params","a1ddb5c4":"#Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","711c828f":"best_params = study.best_params\nbest_params['tree_method'] = 'gpu_hist'\nbest_params['random_state'] = 42\n\nclf = XGBRegressor(**(best_params))\n\nclf.fit(x, y)","064b81bf":"preds = pd.Series(clf.predict(test_df.drop('id', axis=1)), name='target')\npreds = pd.concat([test_df['id'], preds], axis=1)","0a405728":"preds.head()","931b7a8f":"preds.to_csv(\"submission.csv\", index=False)","aa393715":"# 4. XGBoost Optuna Optimization","fec8086c":"## Target distribution","f7bbb42f":"# 5. Train final model\n\nNow we are going to train the final model with the best parameters","4909c0b8":"# 1. Data Visualization \ud83d\udcca","bce3365e":"# 6. Submission","c4f7f6f6":"## Correlation analysis","3ef88c08":"We can check that the variables are low correlated so we cna go ahead with the full set.","aebdc8d4":"### Visualization\n\nNow, that we have the optimization done, we can take a look of the output of the algorithm.","3bf812b5":"![tps-jan21.jpg](attachment:tps-jan21.jpg)","121a1df9":"# 3. Simple model: XGBoost Regressor","a15f67c9":"## Features distribution analysis","866c279e":"![optuna-logo.png](attachment:optuna-logo.png)","9d3fc204":"# Tabular Playground Series \ud83d\udcda - Jan 2021 \ud83d\udcc8","65d83e11":"# 2. Feature Engineering \u2699\ufe0f\n\nI am not going to do any feature engineering since the variables seems to be very clean and clear and there are no missing values.\n\nIn the future we can try to:\n\n* Try to transform target feature to better fit a normal distribution. \n* Sometime (not always) is useful to try a similar transformation for the model features as well; in their case, also scaling will improve the result with some of the models;","b771a65d":"As we can see, the target is close to binomial without much skewness. We could latter try to adjust it to a Gaussian."}}