{"cell_type":{"d0ec1f33":"code","660921ba":"code","70213b33":"code","47dc81c6":"code","ccd32b05":"code","f9f732c6":"code","bef9c0b6":"code","007c5d92":"code","0f0071c8":"code","56c0a1ef":"code","cb1e617a":"code","7ebeae40":"code","7b881dfd":"code","09307687":"code","e5d22155":"code","eeb4f6a5":"code","80466301":"code","2ca0ce8a":"code","7fb99f77":"code","7883de42":"code","af85e127":"code","079fb9f0":"code","29f92a83":"code","2dd3db2e":"code","abfcadf2":"code","75d812e1":"code","80194e87":"code","00bbda1f":"code","231a3b53":"code","786370d4":"code","ed50f957":"code","0519ccfd":"code","11972481":"code","07a9932b":"code","d01e01e6":"code","9517c2ef":"code","fd7048df":"code","25cfe36b":"code","25af7d9f":"code","6123187d":"code","ead96c45":"markdown","8f5313a0":"markdown","7479aa02":"markdown","6a693433":"markdown","41bdbf07":"markdown","551b1dda":"markdown","5d5ecf31":"markdown","ff403530":"markdown"},"source":{"d0ec1f33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom warnings import filterwarnings as filt\nfrom scipy.stats import skew, norm \n\nfilt('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,6)\npd.options.display.max_columns = None\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","660921ba":"df = pd.read_csv('\/kaggle\/input\/forest-cover-type-dataset\/covtype.csv')\ndf.shape","70213b33":"df.head()","47dc81c6":"cvtp = pd.DataFrame(df.Cover_Type.value_counts()).sort_values('Cover_Type', ascending = False)\ncvtp['ct_per'] = cvtp.Cover_Type \/ df.shape[0]\nsns.countplot(df.Cover_Type)","ccd32b05":"cvtp","f9f732c6":"df.isnull().values.sum()","bef9c0b6":"df.describe()","007c5d92":"soil = df[[f\"Soil_Type{i}\" for i in range(1,41)]] # types of soils\nwilderness = df[[f\"Wilderness_Area{i}\" for i in range(1,5)]]\ncont_feats = df[[c for c in df.columns if c not in soil.columns and c not in wilderness.columns]].iloc[:, :-1]","0f0071c8":"# these 4 diff wilderness area corresponds to 4 different areas \nfig, ax = plt.subplots(2, 2)\nfig.tight_layout()\nsns.countplot(wilderness.Wilderness_Area1, ax = ax[0,0])\nsns.countplot(wilderness.Wilderness_Area2, ax = ax[0,1])\nsns.countplot(wilderness.Wilderness_Area3, ax = ax[1,0])\nsns.countplot(wilderness.Wilderness_Area4, ax = ax[1,1])","56c0a1ef":"skewness = pd.DataFrame(skew(df), columns = ['skews'], index = df.columns).sort_values('skews', ascending = True)\nplt.figure(figsize = (12,10))\nplt.barh(skewness.index, skewness.skews)","cb1e617a":"print(df.Soil_Type7.value_counts())\nsns.distplot(df.Soil_Type15)","7ebeae40":"low_soil = []\nfor s in soil.columns:\n#     ones = df.shape[0] - soil[s].value_counts().iloc[0]\n    ones = soil[s].value_counts()[1]\n    if  ones <  1000:\n        print(f\"{s} ones count : {ones}    {round(ones \/ df.shape[0], 4)} %\")\n        low_soil.append(s)","7b881dfd":"fig, ax = plt.subplots(cont_feats.shape[1], 2, figsize = (18,16))\nfig.tight_layout()\nfor row, col in enumerate(cont_feats.columns):\n    sns.distplot(df[col], ax = ax[row, 0])\n    sns.boxplot(df[col], ax = ax[row, 1])","09307687":"sns.heatmap(cont_feats.corr(), fmt = '.1f', annot = True, cmap = 'icefire')","e5d22155":"sns.scatterplot(data = df, x = 'Hillshade_3pm', y = 'Aspect', hue = 'Cover_Type')","eeb4f6a5":"from eli5 import show_weights\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.feature_selection import mutual_info_classif\nfrom pdpbox.pdp import *\nfrom sklearn.ensemble import RandomForestClassifier\nimport shap","80466301":"def sample(x, y, frac = 0.005):\n#     x = x.sample(frac = frac)\n#     y = y.loc[x.index]\n    x_big, x, y_big, y = train_test_split(x, y, test_size = frac, stratify = y)\n    return x, y\n\ndef permImp(x, y, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y)\n    model = RandomForestClassifier().fit(x, y)\n    perm = PermutationImportance(model).fit(x, y)\n    return show_weights(perm, feature_names = x.columns.tolist())\n\ndef plot_mi(score):\n    score = score.sort_values('mi_score', ascending = True)\n    plt.barh(score.index, score.mi_score)\n    plt.title('mutual info clf')\n    return \n\ndef mi_score(x, y, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y)\n    score = pd.DataFrame(mutual_info_classif(x, y, discrete_features = False), columns = ['mi_score'], index = x.columns ).sort_values('mi_score', ascending = False)\n    plot_mi(score)\n    return score\n\ndef isolate(x, y, col, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y)\n    model = RandomForestClassifier().fit(x, y)\n    pdp_dist = pdp_isolate(model, model_features = x.columns, dataset = x, feature = col)\n    return pdp_plot(pdp_dist, feature_name = col, ncols = 3)\n\ndef interact(x, y, cols, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y)\n    model = RandomForestClassifier().fit(x, y)\n    pdp_dist = pdp_interact(model, model_features = x.columns, dataset = x, features = cols)\n    return pdp_interact_plot(pdp_dist, feature_names = cols)\n\ndef forceplot(x, y, classes = 1, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y)\n    classes -= 1\n    x_samp = x.sample(n = 1)\n    x = x.drop(x_samp.index)\n    y = y.drop(x_samp.index)\n    model = RandomForestClassifier().fit(x, y)\n    explainer = shap.TreeExplainer(model)\n    exp_values = explainer.expected_value[classes]\n    shap_values = explainer.shap_values(x_samp)[classes]\n    return shap.force_plot(exp_values, shap_values, feature_names = x.columns.tolist() )","2ca0ce8a":"from sklearn.model_selection import train_test_split","7fb99f77":"x = df.drop(['Cover_Type'], axis = 1)\ny = df.Cover_Type\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.05, random_state = 123, stratify = y)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","7883de42":"x_train.shape[0] * 0.05","af85e127":"permImp(x_train, y_train, 0.05)","079fb9f0":"plt.figure(figsize = (13,15))\nmscore = mi_score(x_train, y_train, 0.05)","29f92a83":"for ms in mscore[mscore.mi_score == 0].index:\n    if ms in low_soil:\n        print(ms)\n        \nno_info_feats = mscore[mscore.mi_score == 0].index\nuseless_feats = set(np.concatenate([no_info_feats, low_soil]))\n' , '.join(useless_feats)","2dd3db2e":"isolate(x_train, y_train, 'Elevation',0.05);","abfcadf2":"interact(x_train, y_train, ['Elevation', 'Horizontal_Distance_To_Roadways'], 0.05);","75d812e1":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler","80194e87":"# let's not forget we do have these useless feats\nuseless_feats","00bbda1f":"def best_model(x, y, frac = 0, fold = 10):\n    if frac > 0:\n        x, y = sample(x, y, frac)\n        \n    models = [SVC(), KNeighborsClassifier(), RandomForestClassifier(), LGBMClassifier()]\n    mnames = ['svm', 'knn', 'random forest', 'lgbm']\n    scalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler()]\n    snames = ['none', 'std', 'robust', 'min-max']\n    scores = [[] for _ in range(4)]\n    \n    print(f'Total iterations : {len(models) * len(scalers)}')\n    iterr = 0\n    for model in models:\n        for ind, scaler in enumerate(scalers):\n            iterr += 1\n            print(f'iteration :===> {iterr} \/ {len(models) * len(scalers)}')\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n            kf = KFold(n_splits = fold, shuffle = True)\n            score = cross_val_score(model, x, y, cv = kf, scoring = 'f1_micro').mean()\n            scores[ind].append(score)\n    \n    return pd.DataFrame(scores, columns = mnames, index = snames).T\n\ndef get_score(xt, yt, xtest, ytest, model, scaler = None, frac = 0):\n    if frac > 0:\n        xt, yt = sample(xt, yt, frac)\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    print(' Reports '.center(70, '='))\n    print()\n    print(f\"Training acc score : {model.score(xt, yt)}\")\n    print(f\"Testing acc score  : {model.score(xtest, ytest)}\")\n    print()\n    print(classification_report(ytest, pred))\n    print()\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.1f', annot = True)\n\ndef gridcv(x, y, model, params, scaler = None ,frac = 0, fold = 10):\n    if frac > 0:\n        x, y = sample(x, y, frac)\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    kf = KFold(n_splits = fold, shuffle = True)\n    clf = GridSearchCV(model, param_grid = params, scoring = 'f1_micro', return_train_score = True, cv = kf)\n    clf.fit(x, y)\n    res = pd.DataFrame(clf.cv_results_).sort_values('mean_test_score', ascending = False)\n    return clf.best_estimator_, clf.best_params_, res[['mean_train_score', 'mean_test_score', 'params']]\n\ndef plot_cv(report):\n    sns.lineplot(x = report.reset_index().index, y = report.mean_train_score )\n    sns.lineplot(x = report.reset_index().index, y = report.mean_test_score )\n    plt.legend(['training score', 'testing score'])\n    plt.title('f1_micro score on training and testing')","231a3b53":"x_train.shape[0] * 0.05","786370d4":"%%time\nget_score(x_train, y_train, x_test, y_test, SVC(), RobustScaler(), 0.05)","ed50f957":"%%time\nbest_model(x_train, y_train, 0.05, 3)","0519ccfd":"%%time\nget_score(x_train, y_train, x_test, y_test, RandomForestClassifier(), RobustScaler())","11972481":"params = {\n    'n_estimators' : [50,75,100],\n    'max_depth' : [None, 8, 15],\n    'class_weight' : [None, 'balanced']\n}\n\npip_params = { f\"model__{key}\" : values for key, values in params.items()}\npip_params","07a9932b":"%%time\nclf, best_param, results = gridcv(x_train, y_train, RandomForestClassifier(), pip_params, RobustScaler(), 0.05)","d01e01e6":"plot_cv(results)","9517c2ef":"results.iloc[0], best_param","fd7048df":"%%time\nget_score(x_train, y_train, x_test, y_test, clf)","25cfe36b":"shap.initjs()\nforceplot(x_train, y_train,1, 0.05)","25af7d9f":"plot_mi(mscore.loc[useless_feats])","6123187d":"%%time\nget_score(x_train.drop(useless_feats, axis = 1), y_train, x_test.drop(useless_feats, axis = 1), y_test, clf)","ead96c45":"wow that's a nice improvement","8f5313a0":"random forest did pretty good at classifying the cover types , pretty ironic forest model is good at classifying forest covers ","7479aa02":"from this PD plot:\n\n* for cover type 1, chances are super high if the elevation is between 3000 - 3250\n* for cover type 2 and 7, chances are super low if the elevation is above 3000\n* for cover type 3, 4 and 6 , chances are low if the elevation is greater than 2000\n* for cover type 5, chance are little bit high if the elevation is anywhere between 2000 - 3000 ","6a693433":"from this plot we can interept more insights , but lets only focus on cover type - 4 and 5 since they are the least occuring cover type :\n\n* cover type 4 are elevated max of ~2100\n* cover type 4 are little distant from the roadways \n* cover type 5 trees are usually near road ways \n* cover type 5 elevation is max of ~2800 ","41bdbf07":"lets see the important features","551b1dda":"from the permutation and mutual info plot, looks like elevation is most important feature","5d5ecf31":"from this force plot, looks like some soil types are not really helping the model, let's try dropping them useless feats ","ff403530":"it is doing pretty good, lets try to get the best model"}}