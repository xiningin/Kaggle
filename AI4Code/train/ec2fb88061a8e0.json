{"cell_type":{"07160599":"code","70e4ae24":"code","58927bb4":"code","0593c9ea":"code","a8953c6e":"code","f6a64b4d":"code","887fafcc":"code","42ff07ed":"code","07f11ac2":"code","bb174427":"code","d73ffdc9":"code","352bce48":"code","6244485f":"code","6c52512f":"code","63a7ed3a":"code","adfb800c":"code","95e6bf3b":"code","10049311":"code","c032e63a":"code","3a0e483b":"code","48332543":"code","a321451e":"code","f03ab93a":"code","503c127f":"code","faf47fa4":"code","1b79e59a":"markdown","ed30c490":"markdown","a71867bc":"markdown","c163cd0e":"markdown","72837fb0":"markdown","42244f53":"markdown","ca53f604":"markdown","585ec705":"markdown","1c604dae":"markdown","a30fb384":"markdown","0eaa9452":"markdown","a3753f36":"markdown","e65a050c":"markdown","caf71f8b":"markdown","d3b24595":"markdown","7050cdc7":"markdown","be9df812":"markdown","b079856e":"markdown","8cdfca08":"markdown","fc945a5a":"markdown","068f7868":"markdown"},"source":{"07160599":"# Libraries\nimport numpy as np \nimport pandas as pd\n\nimport seaborn as sns\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nimport cv2\nimport imageio\nfrom IPython.display import Video, display\n\nimport warnings\nwarnings.filterwarnings('ignore')","70e4ae24":"train_tracking = pd.read_csv('..\/input\/nfl-impact-detection\/train_player_tracking.csv')\ntest_tracking = pd.read_csv('..\/input\/nfl-impact-detection\/test_player_tracking.csv')\n\n\ntrain_labels = pd.read_csv('..\/input\/nfl-impact-detection\/train_labels.csv')\nimage_labels = pd.read_csv('..\/input\/nfl-impact-detection\/image_labels.csv')\nvideo_labels = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/train_labels.csv')\n\nsub_sample = pd.read_csv('..\/input\/nfl-impact-detection\/sample_submission.csv')","58927bb4":"image_labels.head()","0593c9ea":"image_labels.info()","a8953c6e":"def img_show(index):\n    im = cv2.imread(\"..\/input\/nfl-impact-detection\/images\/\" + image_labels[\"image\"][index])\n    plt.imshow(im)","f6a64b4d":"img_show(0)","887fafcc":"# Read in the video labels file\n\nvideo_labels.head()","42ff07ed":"def vid_show(index):\n    video_name = video_labels['video'][index]\n    video_path = f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\"\n    display(Video(data=video_path, embed=True))","07f11ac2":"vid_show(0)","bb174427":"image_labels.head(5)","d73ffdc9":"# Bounding box function for Images\ndef box_image(index):\n    name = image_labels['image'][index]\n    box_color = (0, 0, 0)    # Bounding box color -> Black\n    img = imageio.imread(f\"\/kaggle\/input\/nfl-impact-detection\/images\/{name}\")\n    image = image_labels.loc[image_labels['image'] == name]\n    for i, j in image.iterrows():\n        color = box_color \n\n        # Add a box around the helmet\n        # Note that cv2.rectangle requires us to specify the top left pixel and the bottom right pixel\n        cv2.rectangle(img, (j.left, j.top), (j.left + j.width, j.top + j.height), color,thickness=1)\n        \n    # Display the image with bounding boxes added\n    plt.imshow(img)\n    plt.show()","352bce48":"box_image(1)","6244485f":"train_labels.nunique().to_frame().rename(columns={0:\"Count\"})","6c52512f":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n\nsns.distplot(train_labels[\"gameKey\"].value_counts(), ax=ax[0, 0], rug=True, color=\"red\")\nax[0, 0].set_title(\"Game Counts\")\n\nsns.distplot(train_labels[\"playID\"].value_counts(), ax=ax[0, 1], rug=True, color=\"blue\")\nax[0, 1].set_title(\"Play Counts\")\n\nsns.distplot(train_labels[\"label\"].value_counts(), ax=ax[1, 0], rug=True, color=\"green\")\nax[1, 0].set_title(\"Labels Counts\")\n\nsns.distplot(train_labels[\"video\"].value_counts(), ax=ax[1, 1], rug=True, color=\"yellow\")\nax[1, 1].set_title(\"Videos Counts\")\n\nfig.show()","63a7ed3a":"train_labels['video'].nunique()","adfb800c":"play_frame_count = train_labels[['gameKey','playID','frame']].drop_duplicates()[['gameKey','playID']].value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.distplot(play_frame_count, bins=15)\nax.set_title('Distribution of frames per video file')\nplt.show()","95e6bf3b":"train_labels['area'] = train_labels['width'] * train_labels['height']\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.distplot(train_labels['area'].value_counts(),\n             bins=10)\nax.set_title('Distribution bounding box sizes')\nplt.show()","10049311":"train_labels['impactType'].value_counts().plot(kind='bar',title='Impact Type Count',figsize=(12, 4))\n\nplt.show()\n\ntrain_labels['impactType'].value_counts()","c032e63a":"sns.catplot(x=\"view\", hue=\"impactType\", col=\"confidence\",\n                data=train_labels, kind=\"count\")","3a0e483b":"sns.catplot(x=\"view\", hue=\"impactType\", col=\"visibility\",\n                data=train_labels, kind=\"count\")","48332543":"impact_occ = train_labels[['video','impact']].fillna(0)['impact'].mean() * 100\nprint(f'Of all bounding boxes, {impact_occ:0.4f}% of them involve an impact event')","a321451e":"train_labels['confidence'].dropna().astype('int').value_counts().plot(kind='bar',\n          title='Confidence Type Label Count',\n          figsize=(12, 4))\nplt.show()\n\ntrain_labels['confidence'].value_counts()","f03ab93a":"sns.catplot(x=\"impactType\", hue=\"confidence\", col=\"view\",\n                data=train_labels, kind=\"count\")","503c127f":"train_labels['visibility'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Visibility Label Count',\n          figsize=(12, 4))\nplt.show()\n\ntrain_labels['visibility'].value_counts()","faf47fa4":"sns.catplot(x=\"impactType\", hue=\"visibility\", col=\"view\",\n                data=train_labels, kind=\"count\")","1b79e59a":"<a id=\"1\"><\/a> \n\n# 1. Understanding the Competition\nThis competition is part of the NFL\u2019s annual 1st and Future Competition, which has been designed to spur innovation in athlete safety and performance.\n\nThe NFL is actively addressing the need for a computer vision system to detect on-field helmet impacts as part of the \u201cDigital Athlete\u201d platform, and the league is calling on Kagglers to help.\n\nIn this competition, it is expected to develop a computer vision model that automatically detects helmet impacts that occur on the field. The dataset is of more than one thousand definitive head impacts from thousands of game images, labelled video from the sidelines and end zones, and player tracking data.\n\nThe data also documents the position, speed, acceleration, and orientation for every player on the field during NFL games.\n\nThis competition is evaluated using a micro F1 score at an Intersection over Union (IoU) threshold of 0.35.\n\n","ed30c490":"### Impact Type Count\nTypes of Impacts recorded here are:\n* Helmet\n* Shoulder\n* Body\n* Ground\n* Hand\n* shoulder'","a71867bc":"### Bounding box size\nThis depends on various factors like,\n* The distance between player and camera.\n* The camera's angle and zoom relative to the field.\n* One player's helmet may be blocked from view by another player.\n\n\nHere, we are taking area (width x height) of the bounding box.","c163cd0e":"### Visibility\n* Not Visible from View = 0 \n* Minimum = 1 \n* Visible = 2\n* Clearly Visible = 3","72837fb0":"<a id=\"5\"><\/a> \n# 5. Visualizing Data\n<a id=\"6\"><\/a> \n## 5.1 Image Data","42244f53":"### Impact Occurance Percentage","ca53f604":"<a id=\"2\"><\/a> \n# 2.Understanding Data\n\n### The dataset consists of three types of data:\n\n* **Image Data:**\n    Image Data consist of about 10,000 images and associated helmet labels. This is to be used for building a helmet detection system.\n\n* **Video Data:**\n    Video Data consists of 120 videos (60 plays) from both a sideline and endzone point of view for each play. It has been associated with helmet and helmet impact labels, which has to be used for building a helmet impact detection system.\n\n* **Tracking Data:**\n    Tracking data consists of tracking for all players in the provided 60 plays.\n\n\n### Data files:\n\n* **train_labels.csv** - Helmet tracking and collision labels for the training set.\n* **sample_submission.csv** - A valid sample submission file.\n* **image_labels.csv** - contains the bounding boxes corresponding to the images.\n* **[train\/test]_player_tracking.csv** - Each player wears a sensor that allows us to precisely locate them on the field.\n\n\n\n### Folders:\n* **\/train\/** contains the mp4 video files for the training plays. \n  (Both an endzone and sideline view.)\n    \n* **\/test\/** contains the videos for the test set. \n    \n* **\/images\/** contains the additional annotated images of player helmets.","585ec705":"<a id=\"8\"><\/a>\n# 6. Bounding Boxes","1c604dae":"### Lenght of videos","a30fb384":"<a id=\"12\"><\/a> \n# Upcoming updates:\n\n<b> \n* Bounding Box displays in Videos\n* Finding in-depth insights about the dataset\n* Model design and implementation\n* Final submission.\n<\/b>\n\nAll these updates will be here soon, keep motivating me till then.\n\n\n<a id=\"13\"><\/a> \n# Note to the Readers\n<b>\nThis is my first attempt on Kaggle, I am still finding my way around over here, motivate me and push me to learn more.\n\nIf you wish to suggest me updates, feel free to do so.\n<\/b>\n\n## Did you upvote or comment yet? Please do... :D","0eaa9452":"<a id=\"9\"><\/a>\n## 6.1 Bounding Boxes in Images","a3753f36":"### Number of unique elements in each feature","e65a050c":"<a id=\"11\"><\/a>\n# 7. Exploratory Data Analysis","caf71f8b":"120 unique videos that comprise of two views of one game play each.\nTherefore, 60 gameplays with two views of each.","d3b24595":"The videos range from approximately 300 frames to 600 frames per video.","7050cdc7":"<a id=\"4\"><\/a> \n# 4. Importing Data","be9df812":"<a id=\"3\"><\/a> \n# 3.Libraries","b079856e":"<a id=\"7\"><\/a> \n## 5.2 Video Data","8cdfca08":"### Confidence\n* Possible = 1\n* Definitive = 2\n* Definitive and Obvious = 3","fc945a5a":"# Contents\n* 1. [Understanding the Competition](#1)\n* 2. [Understanding Data](#2)\n* 3. [Libraries](#3)\n* 4. [Importing Data](#4)\n* 5. [Visualizing Data](#5)\n    * 5.1 [Image Data](#6)\n    * 5.2 [Video Data](#7)\n* 6. [Bounding Boxes](#8)\n    * 6.1 [Bounding boxes in Images](#9)\n    \n* 7. [Exploratory Data Analysis](#11)\n\n* [Upcoming Updates](#12)\n* [Note to Readers](#13)","068f7868":"Let us know the number of(unique) videos in our dataset."}}