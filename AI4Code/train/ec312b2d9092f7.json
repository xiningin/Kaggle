{"cell_type":{"a6cfae0e":"code","e1773db1":"code","efc89bef":"code","cc5a2a67":"code","13bd0fdd":"code","ed25eeca":"code","a4bf0088":"code","a6cef62f":"code","f23249e5":"code","c5dfef73":"code","586a328e":"code","73ab6bc2":"code","e5966a82":"code","d76d6af1":"code","f7ccb9f5":"code","50f0438c":"code","424ca62f":"code","18d6cc98":"code","8aea5924":"code","36179a1c":"code","5e78119c":"code","29b1fd03":"code","69c790da":"code","52cb5654":"code","2bbf88e3":"code","c927f063":"code","c7ce1141":"markdown","f23a124c":"markdown","f07b0b6c":"markdown","450f12d8":"markdown","73a368ff":"markdown","4b222796":"markdown"},"source":{"a6cfae0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom matplotlib import pyplot\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1773db1":"data = pd.read_csv('..\/input\/insurance\/insurance.csv')\ndata.head()\ndata_copy = data.copy()","efc89bef":"data.head()","cc5a2a67":"data.describe()","13bd0fdd":"# check the null value\ndata.isnull().sum() ","ed25eeca":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata_copy['sex'] = le.fit_transform(data_copy['sex'])\ndata_copy['region'] = le.fit_transform(data_copy['region'])\ndata_copy['smoker'] = le.fit_transform(data_copy['smoker'])\ndata_copy.head()","a4bf0088":"# draw the correlation matrix, \n# If the value is close to 1:strong positive correlation.\n# When it is close to -1:strong negative correlation.\nimport seaborn as sns \ncorrelation_matrix = data_copy.corr().round(2)\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)\n\n# charges and smoker is very related","a6cef62f":"X = np.array(data_copy.drop(['charges'],axis=1))\ny = np.array(data_copy['charges'])\n# size = y.shape[0]","f23249e5":"def  featureNormalize(X):\n    \"\"\"\n    Normalizes the features in X. returns a normalized version of X where\n    the mean value of each feature is 0 and the standard deviation\n    is 1. This is often a good preprocessing step to do when working with\n    learning algorithms.\n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n).\n    \n    Returns\n    -------\n    X_norm : array_like\n        The normalized dataset of shape (m x n).\n    \n    Instructions\n    ------------\n    First, for each feature dimension, compute the mean of the feature\n    and subtract it from the dataset, storing the mean value in mu. \n    Next, compute the  standard deviation of each feature and divide\n    each feature by it's standard deviation, storing the standard deviation \n    in sigma. \n    \n    Note that X is a matrix where each column is a feature and each row is\n    an example. You needto perform the normalization separately for each feature. \n    \n    Hint\n    ----\n    You might find the 'np.mean' and 'np.std' functions useful.\n    \"\"\"\n    # You need to set these values correctly\n    X_norm = X.copy()\n    mu = np.zeros(X.shape[1])\n    sigma = np.zeros(X.shape[1])\n\n    # =========================== YOUR CODE HERE =====================\n    mu = np.mean(X, axis = 0)\n    sigma = np.std(X, axis = 0)\n    X_norm = (X - mu) \/ sigma\n    \n    # ================================================================\n    return X_norm, mu, sigma","c5dfef73":"#normalize features\nX, mu, sigma = featureNormalize(X)\nX = np.concatenate([np.ones((y.shape[0], 1)), X], axis=1)","586a328e":"#Only use one sets of training, cross validation and test\nfrom sklearn.model_selection import train_test_split\nX_train_cv, X_test, y_train_cv, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train_cv, y_train_cv, test_size=0.2,random_state=0)","73ab6bc2":"# K fold test: each *_*_kf list has N_SPLITS elements, each element is a training\/cv\/test set\nfrom sklearn.model_selection import KFold\nN_SPLITS = 5\nkf = KFold(n_splits=N_SPLITS, shuffle=True)\nX_train_kf = []\nX_cv_kf = []\nX_test_kf = []\ny_train_kf = []\ny_cv_kf = []\ny_test_kf = []\nfor train, test in kf.split(X):\n    X_train_cv, X_test = X[train], X[test]\n    y_train_cv, y_train = y[train], y[test]\n    X_train, X_cv, y_train, y_cv = train_test_split(X_train_cv, y_train_cv, test_size=0.2,random_state=0)\n    X_train_kf.append(X_train)\n    X_cv_kf.append(X_cv)\n    X_test_kf.append(X_test)\n    y_train_kf.append(y_train)\n    y_cv_kf.append(y_cv)\n    y_test_kf.append(y_test)","e5966a82":"#I don't know whether we need it\ndef computeCostMulti(X, y, theta):\n    \"\"\"\n    Compute cost for linear regression with multiple variables.\n    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        A vector of shape (m, ) for the values at a given data point.\n    \n    theta : array_like\n        The linear regression parameters. A vector of shape (n+1, )\n    \n    Returns\n    -------\n    J : float\n        The value of the cost function. \n    \n    Instructions\n    ------------\n    Compute the cost of a particular choice of theta. You should set J to the cost.\n    \"\"\"\n    # Initialize some useful values\n    m = y.shape[0] # number of training examples\n    \n    # You need to return the following variable correctly\n    J = 0\n    \n    # ======================= YOUR CODE HERE ===========================\n    \n    J = 1\/(2*m)*np.sum(np.square(np.dot(X, theta)-y))\n    \n    # ==================================================================\n    return J","d76d6af1":"#I don't know whether we need it\ndef gradientDescentMulti(X, y, theta, alpha, num_iters):\n    \"\"\"\n    Performs gradient descent to learn theta.\n    Updates theta by taking num_iters gradient steps with learning rate alpha.\n        \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        A vector of shape (m, ) for the values at a given data point.\n    \n    theta : array_like\n        The linear regression parameters. A vector of shape (n+1, )\n    \n    alpha : float\n        The learning rate for gradient descent. \n    \n    num_iters : int\n        The number of iterations to run gradient descent. \n    \n    Returns\n    -------\n    theta : array_like\n        The learned linear regression parameters. A vector of shape (n+1, ).\n    \n    J_history : list\n        A python list for the values of the cost function after each iteration.\n    \n    Instructions\n    ------------\n    Peform a single gradient step on the parameter vector theta.\n\n    While debugging, it can be useful to print out the values of \n    the cost function (computeCost) and gradient here.\n    \"\"\"\n    # Initialize some useful values\n    m = y.shape[0] # number of training examples\n    \n    # make a copy of theta, which will be updated by gradient descent\n    theta = theta.copy()\n    \n    J_history = []\n    \n    for i in range(num_iters):\n        # ======================= YOUR CODE HERE ==========================\n\n        theta = theta-(alpha\/m)*(np.dot(X, theta)-y).dot(X)\n        # =================================================================\n        \n        # save the cost J in every iteration\n        J_history.append(computeCostMulti(X, y, theta))\n    \n    return theta, J_history","f7ccb9f5":"def linearRegCostFunction(X, y, theta, lambda_=0.0):\n    \"\"\"\n    Compute cost and gradient for regularized linear regression \n    with multiple variables. Computes the cost of using theta as\n    the parameter for linear regression to fit the data points in X and y. \n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset. Matrix with shape (m x n + 1) where m is the \n        total number of examples, and n is the number of features \n        before adding the bias term.\n    \n    y : array_like\n        The functions values at each datapoint. A vector of\n        shape (m, ).\n    \n    theta : array_like\n        The parameters for linear regression. A vector of shape (n+1,).\n    \n    lambda_ : float, optional\n        The regularization parameter.\n    \n    Returns\n    -------\n    J : float\n        The computed cost function. \n    \n    grad : array_like\n        The value of the cost function gradient w.r.t theta. \n        A vector of shape (n+1, ).\n    \n    Instructions\n    ------------\n    Compute the cost and gradient of regularized linear regression for\n    a particular choice of theta.\n    You should set J to the cost and grad to the gradient.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(theta.shape)\n\n    # ====================== YOUR CODE HERE ======================\n    h = X.dot(theta)\n    J = 1\/(2*m)*np.sum(np.square(h-y))+lambda_\/(2*m)*np.sum(np.square(theta[1:]))\n    \n    grad = 1\/m*(h-y).dot(X)\n    \n    grad[1:] = grad[1:]+lambda_\/m*theta[1:]\n\n\n    # ============================================================\n    return J, grad","50f0438c":"# not really useful, just a try\ntheta = np.ones(X_train.shape[1])\ntheta, J = gradientDescentMulti(X_train, y_train, theta, 0.2, 10)\nprint(theta)\nJ, _ = linearRegCostFunction(X_train, y_train, theta, 0.2)\n\nprint('Cost at theta:\\t   %f ' % J)","424ca62f":"from scipy import optimize\n# This function is from utils toolbox provided by course\ndef trainLinearReg(linearRegCostFunction, X, y, lambda_=0.0, maxiter=200):\n    \"\"\"\n    Trains linear regression using scipy's optimize.minimize.\n\n    Parameters\n    ----------\n    X : array_like\n        The dataset with shape (m x n+1). The bias term is assumed to be concatenated.\n\n    y : array_like\n        Function values at each datapoint. A vector of shape (m,).\n\n    lambda_ : float, optional\n        The regularization parameter.\n\n    maxiter : int, optional\n        Maximum number of iteration for the optimization algorithm.\n\n    Returns\n    -------\n    theta : array_like\n        The parameters for linear regression. This is a vector of shape (n+1,).\n    \"\"\"\n    # Initialize Theta\n    initial_theta = np.zeros(X.shape[1])\n\n    # Create \"short hand\" for the cost function to be minimized\n    costFunction = lambda t: linearRegCostFunction(X, y, t, lambda_)\n\n    # Now, costFunction is a function that takes in only one argument\n    options = {'maxiter': maxiter}\n\n    # Minimize using scipy\n    res = optimize.minimize(costFunction, initial_theta, jac=True, method='TNC', options=options)\n    return res.x","18d6cc98":"theta = trainLinearReg(linearRegCostFunction, X_train, y_train, lambda_=0)","8aea5924":"def learningCurve(X, y, Xval, yval, lambda_=0):\n    \"\"\"\n    Generates the train and cross validation set errors needed to plot a learning curve\n    returns the train and cross validation set errors for a learning curve. \n    \n    In this function, you will compute the train and test errors for\n    dataset sizes from 1 up to m. In practice, when working with larger\n    datasets, you might want to do this in larger intervals.\n    \n    Parameters\n    ----------\n    X : array_like\n        The training dataset. Matrix with shape (m x n + 1) where m is the \n        total number of examples, and n is the number of features \n        before adding the bias term.\n    \n    y : array_like\n        The functions values at each training datapoint. A vector of\n        shape (m, ).\n    \n    Xval : array_like\n        The validation dataset. Matrix with shape (m_val x n + 1) where m is the \n        total number of examples, and n is the number of features \n        before adding the bias term.\n    \n    yval : array_like\n        The functions values at each validation datapoint. A vector of\n        shape (m_val, ).\n    \n    lambda_ : float, optional\n        The regularization parameter.\n    \n    Returns\n    -------\n    error_train : array_like\n        A vector of shape m. error_train[i] contains the training error for\n        i examples.\n    error_val : array_like\n        A vecotr of shape m. error_val[i] contains the validation error for\n        i training examples.\n    \n    Instructions\n    ------------\n    Fill in this function to return training errors in error_train and the\n    cross validation errors in error_val. i.e., error_train[i] and \n    error_val[i] should give you the errors obtained after training on i examples.\n    \n    Notes\n    -----\n    - You should evaluate the training error on the first i training\n      examples (i.e., X[:i, :] and y[:i]).\n    \n      For the cross-validation error, you should instead evaluate on\n      the _entire_ cross validation set (Xval and yval).\n    \n    - If you are using your cost function (linearRegCostFunction) to compute\n      the training and cross validation error, you should call the function with\n      the lambda argument set to 0. Do note that you will still need to use\n      lambda when running the training to obtain the theta parameters.\n    \n    Hint\n    ----\n    You can loop over the examples with the following:\n     \n           for i in range(1, m+1):\n               # Compute train\/cross validation errors using training examples \n               # X[:i, :] and y[:i], storing the result in \n               # error_train[i-1] and error_val[i-1]\n               ....  \n    \"\"\"\n    # Number of training examples\n    m = y.size\n\n    # You need to return these values correctly\n    error_train = np.zeros(m)\n    error_val   = np.zeros(m)\n\n    # ====================== YOUR CODE HERE ======================\n    for i in range(1, m + 1):\n        theta_t = trainLinearReg(linearRegCostFunction, X[:i], y[:i], lambda_ = lambda_)\n        error_train[i - 1], _ = linearRegCostFunction(X[:i], y[:i], theta_t, lambda_ = 0)\n        error_val[i - 1], _ = linearRegCostFunction(Xval, yval, theta_t, lambda_ = 0)\n\n        \n    # =============================================================\n    return error_train, error_val","36179a1c":"error_train, error_val = learningCurve(X_train, y_train, X_cv, y_cv, lambda_=0)\n\nmax_err_train = np.max(error_train)\nmax_err_val = np.max(error_val)\nmax_err = np.max(np.array([max_err_train, max_err_val]))\n\npyplot.plot(np.arange(1, y_train.size+1), error_train, np.arange(1, y_train.size+1), error_val, lw=2)\npyplot.title('Learning curve for linear regression')\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('Number of training examples')\npyplot.ylabel('Error')\npyplot.axis([0, 100, 0, max_err])\n#underfit","5e78119c":"def validationCurve(X, y, Xval, yval):\n    \"\"\"\n    Generate the train and validation errors needed to plot a validation\n    curve that we can use to select lambda_.\n    \n    Parameters\n    ----------\n    X : array_like\n        The training dataset. Matrix with shape (m x n) where m is the \n        total number of training examples, and n is the number of features \n        including any polynomial features.\n    \n    y : array_like\n        The functions values at each training datapoint. A vector of\n        shape (m, ).\n    \n    Xval : array_like\n        The validation dataset. Matrix with shape (m_val x n) where m is the \n        total number of validation examples, and n is the number of features \n        including any polynomial features.\n    \n    yval : array_like\n        The functions values at each validation datapoint. A vector of\n        shape (m_val, ).\n    \n    Returns\n    -------\n    lambda_vec : list\n        The values of the regularization parameters which were used in \n        cross validation.\n    \n    error_train : list\n        The training error computed at each value for the regularization\n        parameter.\n    \n    error_val : list\n        The validation error computed at each value for the regularization\n        parameter.\n    \n    Instructions\n    ------------\n    Fill in this function to return training errors in `error_train` and\n    the validation errors in `error_val`. The vector `lambda_vec` contains\n    the different lambda parameters to use for each calculation of the\n    errors, i.e, `error_train[i]`, and `error_val[i]` should give you the\n    errors obtained after training with `lambda_ = lambda_vec[i]`.\n\n    Note\n    ----\n    You can loop over lambda_vec with the following:\n    \n          for i in range(len(lambda_vec))\n              lambda = lambda_vec[i]\n              # Compute train \/ val errors when training linear \n              # regression with regularization parameter lambda_\n              # You should store the result in error_train[i]\n              # and error_val[i]\n              ....\n    \"\"\"\n    # Selected values of lambda (you should not change this)\n    lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n\n    # You need to return these variables correctly.\n    error_train = np.zeros(len(lambda_vec))\n    error_val = np.zeros(len(lambda_vec))\n\n    # ====================== YOUR CODE HERE ======================\n    for i in range(len(lambda_vec)):\n        lambda_t = lambda_vec[i]\n        theta_t = trainLinearReg(linearRegCostFunction, X, y, lambda_ = lambda_t)\n        error_train[i], _ = linearRegCostFunction(X, y, theta_t, lambda_ = 0)\n        error_val[i], _ = linearRegCostFunction(Xval, yval, theta_t, lambda_ = 0)\n\n\n    # ============================================================\n    return lambda_vec, error_train, error_val","29b1fd03":"lambda_vec, error_train, error_val = validationCurve(X_train, y_train, X_cv, y_cv)\n\npyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('lambda')\npyplot.ylabel('Error')","69c790da":"# polynomial regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npol = PolynomialFeatures (degree = 2)\nx_pol = pol.fit_transform(X)\nx_train_cv, x_test, y_train_cv, y_test = train_test_split(x_pol, y, test_size=0.2, random_state=0)\nx_train, x_cv, y_train, y_cv = train_test_split(x_train_cv, y_train_cv, test_size=0.2, random_state=0)\nPol_reg = LinearRegression()\nPol_reg.fit(x_train, y_train)\ny_train_pred = Pol_reg.predict(x_train)\ny_test_pred = Pol_reg.predict(x_test)\ny_cv_pred = Pol_reg.predict(x_cv)\nprint(Pol_reg.intercept_)\nprint(Pol_reg.coef_)\nprint(Pol_reg.score(x_test, y_test))\nprint(Pol_reg.score(x_cv, y_cv))","52cb5654":"lambda_vec, error_train, error_val = validationCurve(x_train, y_train, x_cv, y_cv)\n\npyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('lambda')\npyplot.ylabel('Error')","2bbf88e3":"error_train, error_val = learningCurve(x_train, y_train, x_cv, y_cv, lambda_=0)\n\nmax_err_train = np.max(error_train)\nmax_err_val = np.max(error_val)\nmax_err = np.max(np.array([max_err_train, max_err_val]))\n\npyplot.plot(np.arange(1, y_train.size+1), error_train, np.arange(1, y_train.size+1), error_val, lw=2)\npyplot.title('Learning curve for linear regression')\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('Number of training examples')\npyplot.ylabel('Error')\npyplot.axis([0, 100, 0, max_err])","c927f063":"##Predicting the charges\ny_test_pred = Pol_reg.predict(x_test)\n##Comparing the actual output values with the predicted values\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})\ndf","c7ce1141":"# Linear Regression","f23a124c":"# Data Retrieve and Pre-processing","f07b0b6c":"## Using K-fold","450f12d8":"## Learning Curve Plots","73a368ff":"## Using one sets of training, cross validation and test","4b222796":"### **`validationCurve` may be useful when doing polynomial regression**"}}