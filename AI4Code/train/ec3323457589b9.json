{"cell_type":{"d71a80fc":"code","2b4342ed":"code","61adf97c":"code","ee521cbc":"code","9472ae67":"code","35c4cbbc":"code","3c9106fc":"code","91a95b21":"code","905ab0e3":"code","b8078df3":"code","1e8c703e":"code","b53ea14a":"code","76929a1d":"code","e91fbfd3":"code","1f551e83":"markdown","ded8d6ad":"markdown","af61360b":"markdown","8e0c24df":"markdown","c2b1a01d":"markdown","cb1e9096":"markdown","55d52614":"markdown","6b5577af":"markdown","d69a4c60":"markdown","75471e7f":"markdown","612f5ca0":"markdown","41fc3097":"markdown","f472221e":"markdown","f66ac72b":"markdown","f1d8a755":"markdown","a3f8f61d":"markdown","ac124c3b":"markdown","3080393e":"markdown","bbe59827":"markdown","40d07c72":"markdown","d450ac12":"markdown","084a9f5f":"markdown","6d9fa5de":"markdown","807bc414":"markdown","27091c09":"markdown","0f2ef537":"markdown","1b7283e7":"markdown"},"source":{"d71a80fc":"import pandas as pd\nimport numpy as np","2b4342ed":"data=pd.read_csv(\"..\/input\/magic1\/magic04.data\",header=None)\ndata.head(1)","61adf97c":"data.columns=['fLength', 'fWidth','fSize','fConc','fConcl','fAsym','fM3Long','fM3Trans','fAlpha','fDist','class']","ee521cbc":"data.head(1)","9472ae67":"data.info()","35c4cbbc":"data.describe()","3c9106fc":"data['class'].value_counts()","91a95b21":"shuffle=data.iloc[np.random.permutation(len(data))]\ntele=shuffle.reset_index(drop=True)\ntele.head(2)","905ab0e3":"tele['class']=tele[\"class\"].map({'g':0,\"h\":1})\ntele.head(2)","b8078df3":"tele_class=tele['class'].values","1e8c703e":"pd.isnull(tele).any","b53ea14a":"\nfrom sklearn.model_selection import train_test_split\ntraining_indices, validation_indices = training_indices, testing_indices = train_test_split(tele.index,\n                                                                                            stratify = tele_class,\n                                                                                            train_size=0.75, test_size=0.25)","76929a1d":"training_indices.size, validation_indices.size","e91fbfd3":"from tpot import TPOTClassifier\nfrom tpot import TPOTRegressor\n\ntpot = TPOTClassifier(generations=5,verbosity=2)\n\ntpot.fit(tele.drop('class',axis=1).loc[training_indices].values,\n         tele.loc[training_indices,'class'].values)","1f551e83":"<a id=\"1\"><\/a>\n<h2 style='background:RED; border:10; color:GREEN'><center>5) LIMITATIONS<\/center><h2>","ded8d6ad":"Here you will use tpot with generations = 5 and the rest of the parameters at default values. The parameter verbosity = 2 states how much information TPOT communicates while it's running.","af61360b":"**You should also do missing value treatment before using tpot. To check the number of missing values column-wise, you can execute the following:**","8e0c24df":"As you can notice the best performing classifier within the time frame specified is now XGBoost with SelectPercentile() and RobustScaler() as the pre-processing steps","c2b1a01d":"![image.png](attachment:image.png)","cb1e9096":"**To give names to the columns of the DataFrame, you can use the attribute columns on the pandas DataFrame and assign it to a list containing the names of the columns.**","55d52614":"* [INTRODUCTION](#1)\n* [INSTALLATION](#2)\n* [IMPLEMENTATION](#3)\n* [LIMITATIONS](#4)\n","6b5577af":"<a id=\"1\"><\/a>\n<h2 style='background:RED; border:10; color:GREEN'><center>3) IMPLEMENTATION<\/center><h2>","d69a4c60":"# **It will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.**","75471e7f":"<a id=\"1\"><\/a>\n<h2 style='background:RED; border:10; color:GREEN'><center> ==> TPOT-DATA SCIENCE ASSISTANT <==<\/center><h2>","612f5ca0":"To install tpot on your system, you can run the command sudo pip install tpot on command line terminal ","41fc3097":"**TPOT can take a long time to finish its search**\n* [Running TPOT isn\u2019t as simple as fitting one model on the dataset. It is considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with numerous preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyper-parameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline. That\u2019s why it usually takes a long time to execute and isn\u2019t feasible for large datasets.](#1)\n\n**TPOT can recommend different solutions for the same dataset**\n* [If you're working with a reasonably complex dataset or run TPOT for a short amount of time, different TPOT runs may result in different pipeline recommendations. When two TPOT runs recommend different pipelines, this means that the TPOT runs didn't converge due to lack of time or that multiple pipelines perform more-or-less the same on your dataset.](#2)","f472221e":"**Now you store the class labels, which you need to predict, in a separate variable tele_class**","f66ac72b":"![image.png](attachment:image.png)","f1d8a755":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='font-size:60px;background:black; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>TPOT IN PYTHON<\/center><\/h2>","a3f8f61d":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='font-size:20px;background:green; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>TABLE OF CONTENTS<\/center><\/h2>","ac124c3b":"<a id=\"1\"><\/a>\n<h2 style='background:RED; border:10; color:GREEN'><center>2) INSTALLATION<\/center><h2>","3080393e":"![image.png](attachment:image.png)","bbe59827":"You will now split the DataFrame into a training set and a testing set just like you do while doing any type of machine learning modeling. You can do this via sklearn's cross_validation train_test_split. The parameters are tele.index as indexes of the DataFrame, train_size = 0.75 to keep 75% of the data in training set, test_size = 0.25 to keep the rest 25% data in testing set and stratify = tele_class the class label's values in the dataset. Note the validation set is just to give us an idea of the test set error. Here it is kept to be the same as a test set.","40d07c72":"**Before using tpot, it is essential you do the labeling of categorical variables in your DataFrame**","d450ac12":"![image.png](attachment:image.png)\n","084a9f5f":"**With the right data, computing power and machine learning model you can discover a solution to any problem, but knowing which model to use can be challenging for you as there are so many of them like Decision Trees, SVM, KNN, etc. That's where genetic programming can be of great use and provide help. Genetic algorithms are inspired by the Darwinian process of Natural Selection, and they are used to generate solutions to optimization and search problems in computer science**\n* The Python library tpot built on top of scikit-learn uses genetic programming to optimize your machine learning pipeline\n* [**For instance, in machine learning, after preparing your data you need to know what features to input to your model and how you should construct those features. Once you have those features, you input them into your model to train on, and then you tune your hyperparameters to get the optimal results. Instead of doing this all by yourselves through trial and error, TPOT automates these steps for you with genetic programming and outputs the optimal code for you when it's done.**](#1)","6d9fa5de":"<a id=\"1\"><\/a>\n<h2 style='background:RED; border:10; color:GREEN'><center> ==> GENETIC PROGRAMMING <==<\/center><h2>","807bc414":"<a id=\"1\"><\/a>\n<h2 style='background:RED; border:10; color:GREEN'><center>1) INTRODUCTION<\/center><h2>","27091c09":"**Now it's time to use the tpot library to suggest us the best pipeline for this binary classification problem. To do so, you have to import TPOTClassifier class from the tpot library. Had this been a regression problem you would have imported TPOTRegressor class.**\n\nTPOTClassifier has a wide variety of parameters","0f2ef537":"**In this tutorial, you will learn how to use a very unique library in python, tpot . The reason why this library is unique is that it automates the entire Machine Learning pipeline and provides you with the best performing machine learning model.**\n* In theory, you can find and apply a plethora of techniques for each of these components, but they all might perform differently for different datasets. The challenge is to find the best performing combination of techniques so that you can minimize the error in your predictions. This is the main reason that nowadays people are working to develop Auto-ML algorithms and platforms so that anyone, without any machine learning expertise, can build models without spending much time or effort. One such platform is available as a python library: TPOT. ","1b7283e7":"***It's generally a good idea to randomly shuffle the data before starting to avoid any type of ordering in the data. You can rearrange the data in the DataFrame using numpy's random and permutation() function. To reset the index numbers after the shuffle use reset_index() method with drop = True as a paramete***"}}