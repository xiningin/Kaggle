{"cell_type":{"b95e13d1":"code","66a4be27":"code","58e7d7c6":"code","4c3b6711":"code","e39e5e24":"code","b3ebe340":"code","f007a252":"code","d8d80300":"code","b5d46003":"code","98a14c8b":"code","f81842ca":"code","10ea5518":"code","4c160238":"code","d589d51c":"code","713a6832":"code","755a9487":"code","733d7627":"code","64798f98":"code","15fbd279":"code","b867c5c0":"code","f2551f19":"code","d36dc85b":"code","5b823899":"code","097d478c":"code","56759dc5":"code","5df44808":"code","7907b664":"code","548214c9":"code","05576acc":"code","296877ff":"code","7a90656c":"code","f5437c68":"code","37b8506f":"code","1d73996c":"code","12ce9594":"code","06387389":"code","7ca6bdc7":"code","8e3133fe":"code","ed4fd4ab":"code","3a8fe333":"code","e4ec8ec7":"code","92271676":"code","7e725f0a":"code","562dce79":"code","92619459":"code","5d88d646":"code","780ea1a9":"code","454b44a8":"code","63b972be":"code","4cde5dc7":"code","ae4f8189":"code","3b7c4c7a":"code","da44fac8":"code","e67142b9":"code","4fe3ddbb":"code","ee310c44":"code","4fc8567a":"code","d2b04c2b":"code","ba08ff2d":"code","671af3dc":"code","04045e64":"code","ceb58e9e":"code","d3392d5d":"code","b06bc0ad":"code","39d02783":"code","f4f5d186":"code","a649b601":"code","7322b7ec":"code","3dd1a666":"code","a258207a":"code","9c2d46da":"code","6e50f46c":"code","f3db44fb":"code","47cc4125":"code","094f3a56":"code","c7675091":"code","38d5462b":"code","dad32368":"code","7a118430":"code","0983401f":"code","69efeb9a":"code","e6b2ba51":"code","ad076a41":"code","0c8365f6":"code","a284751b":"code","f954e6e9":"code","fa57c907":"code","d3790675":"code","7c0790c9":"code","abdf9ea0":"code","d4f7cf5b":"code","836a89aa":"code","e0779852":"code","64baaaf4":"code","d8d6f02b":"code","ba59b53d":"code","6d1c50a0":"code","8717eb2e":"code","dc9c0933":"code","d0a63f3c":"code","03ad1703":"code","99dbb553":"code","4a364918":"code","7727260b":"code","12d251e3":"markdown","9afa09e2":"markdown","db4d6d4b":"markdown","b3536903":"markdown","f83cc833":"markdown","01e9c660":"markdown","9634f922":"markdown","eb0c4e0d":"markdown","fc330f32":"markdown","9cb52c45":"markdown","bb913eee":"markdown","1e7acdde":"markdown","a2487b43":"markdown","86236892":"markdown","f1d1d6fb":"markdown","18d8606e":"markdown"},"source":{"b95e13d1":"#import all we needed module\nimport numpy as np\nimport pandas as pd \nimport sqlite3\nimport matplotlib.pyplot as plt\nfrom  sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\n#from sklearn import cross_validation******** this is not working\nfrom collections import Counter","66a4be27":"import os\nprint(os.listdir(\"..\/input\/\")) ","58e7d7c6":"con = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite')\n\n# we neglect the review having a score = 3\n\nfiltered_data = pd.read_sql_query('''select *from reviews where Score !=3''',con)\n\ndef partition(x):\n    if x<3 :\n        return 'negative'\n    return 'positive'\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition)\nfiltered_data['Score'] = positiveNegative","4c3b6711":"filtered_data.shape","e39e5e24":"filtered_data.head()","b3ebe340":"filtered_data['Score'].value_counts()","f007a252":"display = pd.read_sql_query('''select * from reviews where Score != 3 and userId = \"AR5J8UI46CURR\" order by ProductID''',con)\ndisplay.head()","d8d80300":"# we sorting a data according to productid in ascending order\n\nsorted_data = filtered_data.sort_values('ProductId',axis = 0,ascending=True,inplace = False,kind='quicksort',na_position='last')","b5d46003":"final = sorted_data.drop_duplicates(subset={'UserId','ProfileName','Time','Text'})\nfinal.shape","98a14c8b":"# After removing duplication , we will see how much %of data still remaing\n\n(final['Id'].size*1.0)\/(filtered_data['Id'].size*1.0)*100","f81842ca":"display = pd.read_sql_query('''select * from reviews where Score != 3 and Id = 44737 or Id= 64422 order by ProductId ''',con)\ndisplay.head()","10ea5518":"#we remove duplication using HelpfulnessDenominator and HelpfulnessNumerator.\n\nfinal = final[final.HelpfulnessNumerator<= final.HelpfulnessDenominator]","4c160238":"final.shape","d589d51c":"final['Score'].value_counts()","713a6832":"import re \n\ni = 0\nfor sent in final['Text'].values:\n    if(len(re.findall('<.>*?',sent))):\n        print(i)\n        print(sent)\n        break;\n    i+=1","755a9487":"import string\nimport re\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') # initialising snowball stemmer\n\ndef cleanhtml(sentence): #function to clean word of any html tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr , ' ',sentence)\n    return cleantext\n\ndef cleanpunc(sentence): #function to clean word of any punctuation or special character\n    cleaned = re.sub(r'[?|!|\\,|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r'',cleaned)\n    return cleaned\nprint(stop)\nprint('***********************************************')\nprint(sno.stem('tasty'))","733d7627":"#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n# this code takes a while to run as it needs to run on 500k sentences.\n\ni = 0\nstrl = ' '\nfinal_string  = []\nall_positive_words=[] # store words from +ve reviews here\nall_negattive_words=[]#store words from -ve reviews here\ns= ''\n\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent) #remove html tag\n    \n    for w in sent.split():\n        for cleaned_word in cleanpunc(w).split():\n            if((cleaned_word.isalpha()) & (len(cleaned_word)>2)):\n                if(cleaned_word.lower() not in stop):\n                    s = (sno.stem(cleaned_word.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if(final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s) #list of all words use to store +ve list \n                    if (final['Score'].values)[i] == 'negative':\n                        all_negattive_words.append(s) #list of all words use to store -Ve list\n                else:\n                    continue\n            else:\n                continue\n                \n    #print filtered sentens \n    strl = b\" \".join(filtered_sentence) #final string of cleaned words\n    final_string.append(strl)\n    i+=1","64798f98":"final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review ","15fbd279":"final.head(3) #below the processed review can be seen in the CleanedText Column \n\n\n# store final table into an SQlLite table for future.\nconn = sqlite3.connect('finalassignment.sqlite')\nc=conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)","b867c5c0":"import sqlite3\ncon = sqlite3.connect('finalassignment.sqlite')","f2551f19":"cleaned_data = pd.read_sql_query('select * from Reviews', con)","d36dc85b":"cleaned_data.shape","5b823899":"cleaned_data.head()","097d478c":"cleaned_data['Score'].value_counts()","56759dc5":"# To randomly sample 5k points from both class\n\ndata_p = cleaned_data[cleaned_data['Score'] == 'positive'].sample(n = 5000)\ndata_n = cleaned_data[cleaned_data['Score'] == 'negative'].sample(n = 5000)\nfinal_10k = pd.concat([data_p, data_n])\nfinal_10k.shape","5df44808":"# Sorting data based on time\nfinal_10k['Time'] = pd.to_datetime(final_10k['Time'], unit = 's')\nfinal_10k = final_10k.sort_values(by = 'Time')","7907b664":"final_10k.head()","548214c9":"#function to compute the k value\n\n\ndef k_classifier_brute(X_train , y_train):\n    \n    #creatting odd list of k for knn\n    myList = list(range(0,40))\n    neighbors = list(filter(lambda x:x%2!=0 , myList))\n    \n    #empty list that will hold cv score\n    cv_scores = []\n    \n    #perform 10-fold cross validation\n    for k in neighbors:\n        knn = KNeighborsClassifier(n_neighbors = k , algorithm = \"brute\")\n        scores = cross_val_score(knn , X_train , y_train , cv = 10 , scoring = 'accuracy')\n        cv_scores.append(scores.mean())\n        \n    #changing to misclassification error\n    MSE = [1 - x for x in cv_scores]\n    \n    #determinning best k\n    optimal_k = neighbors[MSE.index(min(MSE))]\n    print('\\nThe optimal number of neighbors is %d' % optimal_k)\n    \n    #plot misclassification error vs k\n    plt.plot(neighbors , MSE)\n    \n    \n    for xy in zip(neighbors , np.round(MSE,3)):\n        plt.annotate('(%s , %s)' %xy , xy=xy , textcoords = 'data')\n    plt.title(\"Misclassification Error Vs K\")\n    plt.xlabel('Number of Neighbors K')\n    plt.ylabel('Misclassification Error')\n    plt.show()\n    \n    print('the misclassification error for each k value is : ' , np.round(MSE,3))\n    return optimal_k","05576acc":"#7k data which will use to train model after vectorization\n\nX = final_10k['CleanedText']\nprint('shape of X :' , X.shape)","296877ff":"y = final_10k['Score']\nprint('shape of y :' , y.shape)","7a90656c":"# spliting data into 70% as a train and 30% as test data\n\nfrom sklearn.model_selection import train_test_split\nX_train , x_test , y_train , y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nprint(X_train.shape,y_train.shape, x_test.shape, y_test.shape)","f5437c68":"#train vectorizor\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nbow = CountVectorizer()\nX_train = bow.fit_transform(X_train)\nX_train","37b8506f":"X_train.shape","1d73996c":"#Test vectorizor\n\nx_test = bow.transform(x_test)","12ce9594":"x_test.shape","06387389":"# to choss optimal k using brute force algorithm\n\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\n\n#from sklearn import cross_validation not woeking\n\noptimal_k_bow = k_classifier_brute(X_train, y_train)\noptimal_k_bow","7ca6bdc7":"# instantiate learning model k  = optimal k\nknn_optimal = KNeighborsClassifier(n_neighbors = optimal_k_bow)\n\n#fitting the model\nknn_optimal.fit (X_train , y_train)\n\n#predict the response\npred = knn_optimal.predict(x_test)","8e3133fe":"# accurary on train data \n\ntrain_acc_bow = knn_optimal.score(X_train , y_train)\nprint('Train Accuracy :' ,train_acc_bow)\nprint('Train Accuracy : %f%%' % (train_acc_bow)) # this is accuracy in %age","ed4fd4ab":"# error on train data \n\ntrain_err_bow = 1-train_acc_bow\nprint('Train Error: %f%%' % (train_err_bow) )","3a8fe333":"# evaluate accuracy on test data \nacc_bow = accuracy_score(y_test , pred)*100\nprint('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_bow,acc_bow))","e4ec8ec7":"# Confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test , pred)\ncm","92271676":"# plot confusion matrix to discribe the performance of classifier.\n\nimport seaborn as sns\n\nclass_label = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm, index = class_label , columns = class_label)\nsns.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.show()","7e725f0a":"# to show main classification report\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test , pred))","562dce79":"# model for knn with bag of word\nmodels = pd.DataFrame({'Model': ['KNN with Bow'], 'Hyper Parameter(K)': [optimal_k_bow], 'Train Error': [train_err_bow], 'Test Error': [100-acc_bow], 'Accuracy': [acc_bow ]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","92619459":"# data\n\nX = final_10k['CleanedText']\nX.shape","5d88d646":"# traget \/ class label\n\ny = final_10k['Score']\ny.shape\n","780ea1a9":"#split data\n\nX_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nprint(X_train.shape,y_train.shape, x_test.shape, y_test.shape)","454b44a8":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nX_train = tf_idf_vect.fit_transform(X_train)\nX_train","63b972be":"# convert test text data to its vectorizer\n\nx_test = tf_idf_vect.transform(x_test)\nx_test.shape","4cde5dc7":"# to chossing optimal k\n\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\n\noptimal_k_tfidf = k_classifier_brute(X_train, y_train)\noptimal_k_tfidf","ae4f8189":"# instantiate learning model k = optimal k\nknn_optimal = KNeighborsClassifier(n_neighbors = optimal_k_tfidf)\n\n#fitting the model\nknn_optimal.fit(X_train , y_train)\n\n#predict the response\npred = knn_optimal.predict(x_test)","3b7c4c7a":"#Accuracy of train data\n\ntrain_acc_tfidf = knn_optimal.score(X_train , y_train)\nprint('train accuracy :' , train_acc_tfidf)\nprint('train accuracy : %f%%' % train_acc_tfidf)","da44fac8":"# error in train data\ntrain_err_tfidf = 1 - train_acc_tfidf \nprint('train accuracy :' , train_err_tfidf)\nprint('train accuracy : %f%%' % train_err_tfidf)","e67142b9":"# eveluate accuracy\n\nacc_tfidf = accuracy_score(y_test , pred) * 100\nprint('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_tfidf,acc_tfidf))","4fe3ddbb":"# Confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test , pred)\ncm","ee310c44":"import seaborn as sns\nclass_label = ['negetive' , 'positive']\ndf_cm = pd.DataFrame(cm , index = class_label , columns = class_label)\nsns.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')\nplt.show()","4fc8567a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test , pred))","d2b04c2b":"# model for knn with tfidf\nmodels = pd.DataFrame({'Model': ['KNN with TfIdf'], 'Hyper Parameter(K)': [optimal_k_tfidf], 'Train Error': [train_err_tfidf], 'Test Error': [100-acc_tfidf], 'Accuracy': [acc_tfidf ]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","ba08ff2d":"# data \nX = final_10k['Text']\nX.shape","671af3dc":"# target \/ class label \ny = final_10k['Score']\ny.shape","04045e64":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nX_train, x_test, y_train, y_test =train_test_split(X, y, test_size = 0.3)\nprint(X_train.shape, x_test.shape, y_train.shape, y_test.shape)","ceb58e9e":"import re\n\ndef cleanhtml(sentence): # function to clean the word of any html tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr , ' ' , sentence)\n    return cleantext\n\ndef cleanpunc(sentence): # function to clean the word of any puntuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|#]' , r'', sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]' , r'' , sentence)\n    return cleaned","d3392d5d":"# train  your own w2v model using your own train text corpus\n\nimport gensim\nlist_of_sent = []\nfor sent in X_train:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):\n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue\n    list_of_sent.append(filtered_sentence)","b06bc0ad":"w2v_model = gensim.models.Word2Vec(list_of_sent , min_count = 5 , size = 50 , workers = 4)","39d02783":"w2v_model.wv.most_similar('like')","f4f5d186":"w2v = w2v_model[w2v_model.wv.vocab]","a649b601":"w2v.shape","7322b7ec":"# Train your own Word2Vec model using your own test text corpus\nimport gensim\nlist_of_sent_test = []\nfor sent in x_test:\n    filtered_sentence=[]\n    sent=cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    list_of_sent_test.append(filtered_sentence)","3dd1a666":"w2v_model=gensim.models.Word2Vec(list_of_sent_test, min_count=5, size=50, workers=4)","a258207a":"w2v_model.wv.most_similar('like')","9c2d46da":"w2v = w2v_model[w2v_model.wv.vocab]","6e50f46c":"w2v.shape","f3db44fb":"# average Word2Vec for train data set.....\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in list_of_sent: # for each review\/sentence and this is for train data set...\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))","47cc4125":"# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors_test = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in list_of_sent_test: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors_test.append(sent_vec)\nprint(len(sent_vectors_test))\nprint(len(sent_vectors_test[0]))","094f3a56":"X_train = sent_vectors\n","c7675091":"x_test = sent_vectors_test","38d5462b":"optimal_k_avgw2v = k_classifier_brute(X_train , y_train)\noptimal_k_avgw2v","dad32368":"# instantiate learning model k = optimal_k\nknn_optimal = KNeighborsClassifier(n_neighbors=optimal_k_avgw2v)\n\n# fitting the model\nknn_optimal.fit(X_train, y_train)\n    \n# predict the response\npred = knn_optimal.predict(x_test)","7a118430":"# Accuracy on train data\ntrain_acc_avgw2v = knn_optimal.score(X_train, y_train)\nprint(\"Train accuracy\", train_acc_avgw2v)","0983401f":"# Error on train data\ntrain_err_avgw2v = 1-train_acc_avgw2v\nprint(\"Train Error %f%%\" % (train_err_avgw2v))","69efeb9a":"# evaluate accuracy\nacc_avg_w2v = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_avgw2v, acc_avg_w2v))","e6b2ba51":"print(\"Test Error %f%%\" %(100-(acc_avg_w2v)))","ad076a41":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, pred)\ncm","0c8365f6":"import seaborn as sns\nclass_label = [\"negative\", \"positive\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","a284751b":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","f954e6e9":"# model for knn with tfidf\nmodels = pd.DataFrame({'Model': ['KNN with average Word2Vec'], 'Hyper Parameter(K)': [optimal_k_avgw2v], 'Train Error': [train_err_avgw2v], 'Test Error': [100-acc_avg_w2v], 'Accuracy': [acc_avg_w2v ]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","fa57c907":"# TF-IDF weighted Word2Vec train dataset\ntfidf_feat = tf_idf_vect.get_feature_names() # tfidf words\/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in list_of_sent: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    sent_vec \/= weight_sum\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1  ","d3790675":"len(tfidf_sent_vectors)\n","7c0790c9":"X_train = tfidf_sent_vectors","abdf9ea0":"# tf idf  weigthed word2vec test dataset\n\ntfidf_feat = tf_idf_vect.get_feature_names() #tfidf words \/ col name\n\n#final tf idf is the sparse matrix with row = sentence col=word and cell val = tfidf\n\ntfidf_sent_vectors_test = [] #the tfdif w2v row = sentence review is stored in the list\n\nrow = 0\n\nfor sent in  list_of_sent_test: #for each review or sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length \n    weight_sum = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf idf of a word in sentence \/ review\n            tfidf = final_tf_idf[row , tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    sent_vec \/= weight_sum\n    tfidf_sent_vectors_test.append(sent_vec)\n    row += 1","d4f7cf5b":"len(tfidf_sent_vectors_test)","836a89aa":"x_test = tfidf_sent_vectors_test","e0779852":"X_train = np.nan_to_num(X_train)\nx_test = np.nan_to_num(x_test)","64baaaf4":"optimal_k_tfidf_w2v = k_classifier_brute(X_train , y_train)\noptimal_k_tfidf_w2v","d8d6f02b":"# instantiate Learning model k = optimal k \nknn_optimal = KNeighborsClassifier(n_neighbors = optimal_k_tfidf_w2v)\n\n#fitting the model\nknn_optimal.fit(X_train , y_train)\n\n#predict the response \npred = knn_optimal.predict(x_test)","ba59b53d":"# accuracy on train data\n\ntrain_acc_tfidf_w2v = knn_optimal.score(X_train, y_train)\nprint(\"Train accuracy\", train_acc_tfidf_w2v)","6d1c50a0":"# Error on train data\ntrain_err_tfidf_w2v = 1-train_acc_tfidf_w2v\nprint(\"Train Error %f%%\" % (train_err_tfidf_w2v))","8717eb2e":"# evaluate accuracy\nacc_tfidf_w2v = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_tfidf_w2v, acc_tfidf_w2v))","dc9c0933":"print(\"Test Error %f%%\" %(100-(acc_tfidf_w2v)))","d0a63f3c":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, pred)\ncm","03ad1703":"import seaborn as sns\nclass_label = [\"negative\", \"positive\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","99dbb553":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","4a364918":"# model for knn with tfidf\nmodels = pd.DataFrame({'Model': ['KNN with Tfidf Word2Vec'], 'Hyper Parameter(K)': [optimal_k_tfidf_w2v], 'Train Error': [train_err_tfidf_w2v], 'Test Error': [100-acc_tfidf_w2v], 'Accuracy': [acc_tfidf_w2v ]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","7727260b":"# model\nmodels = pd.DataFrame({'Model': ['KNN with Bow', \"KNN with TFIDF\", \"KNN with Average Word2Vec\", \"KNN with Tfidf Word2Vec \"], 'Hyper Parameter(K)': [optimal_k_bow, optimal_k_tfidf, optimal_k_avgw2v, optimal_k_tfidf_w2v], 'Train Error': [train_err_bow, train_err_tfidf, train_err_avgw2v, train_err_tfidf_w2v], 'Test Error': [100-acc_bow, 100-acc_tfidf, 100-acc_avg_w2v, 100-acc_tfidf_w2v], 'Accuracy': [acc_bow, acc_tfidf, acc_avg_w2v, acc_tfidf_w2v]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","12d251e3":"# Bag Of Word (BOW)","9afa09e2":"# W2V","db4d6d4b":"# Exploratory Data Analysis(EDA)","b3536903":"# Avg w2v ","f83cc833":"Observations:\n\nFrom above figure(misclassification error vs optimal k, it is showing that classification error for each value of k, when k is increaseing the error is decreasing. For example, if k = 1, error is 36%, k = 2, error is 35% and so on. As I tested our model on unseen data(test data) the accuracy is 71.166 when k = 39","01e9c660":"Conclusion:-above the table userid, profilrname,time,summary  are given.here we observed that userid,name are same so we remove all duplicates using of a eda.","9634f922":"# Tf-idf avg2vec","eb0c4e0d":"First we will read data and calculate number of data ","fc330f32":"Observations: 1) Optimal k is 39 2) Accuarcy if model is 60%","9cb52c45":"Observations: \nOptimal k is 37 \nFrom Confusion matrix, model predicted 1600 as positive & acutal postive are 1517 and predicted 1400 as negative & actual negative are 1483 which is quite good.\nSo we can say that, this model works well","bb913eee":"Conclusion: As per above table, it is cleared that KNN with tf-IDF is quite good.","1e7acdde":"# TF - IDF","a2487b43":"in a eda we clean a data and remove depulication ","86236892":"Now, we do preprocessing like removal of stop words, html tags and make all letters in lowercase.","f1d1d6fb":"# Amazon-Fine-Food-Review Practical Notebook","18d8606e":"I am using amazon find food review \nall data in one sqlite database "}}