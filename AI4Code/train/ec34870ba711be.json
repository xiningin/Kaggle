{"cell_type":{"02674b25":"code","288402c1":"code","bdcbc69a":"code","dbd78b54":"code","b212698b":"code","4957eb84":"code","5022fdaa":"code","724b5409":"code","78e07685":"code","ecef727c":"code","1d5aab37":"code","64185aa7":"code","90cab9bd":"code","e9bea97f":"code","af691676":"code","39231e54":"code","c04a277e":"code","d7b0b794":"markdown","7cfe02cd":"markdown","52e6da17":"markdown"},"source":{"02674b25":"import os\nimport requests\n\n# General packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport PIL.Image\n\nfrom IPython.display import Image, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")","288402c1":"os.listdir('..\/input\/wikipedia-image-caption\/')","bdcbc69a":"df = pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels\/test_image_pixels_part-00000.csv', sep='\\t', names=['image_url', 'b64_bytes', 'metadata_url'])\ndf","dbd78b54":"df.describe()","b212698b":"df.info()","4957eb84":"sub = pd.read_csv('..\/input\/wikipedia-image-caption\/sample_submission.csv')\nsub","5022fdaa":"sub.shape","724b5409":"test_file = pd.read_csv('..\/input\/wikipedia-image-caption\/test.tsv', sep='\\t')\ntest_file","78e07685":"fig = px.pie(test_file, values=test_file['language'].value_counts().values, names=test_file['language'].value_counts().index,\n            title='Languages Distribution', color_discrete_sequence=px.colors.sequential.RdBu, hole=.3\n            )\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig.show()","ecef727c":"import urllib","1d5aab37":"def get_links(df, num):\n    return df.image_url[:num].values\n\nlinks = get_links(df, 10)\n\n\ndef load_images(links):\n    images = []\n    \n    for link in links:\n        URL = link\n        try:\n\n            with urllib.request.urlopen(URL) as url:\n                with open('.\/temp.jpg', 'wb') as f:\n                    f.write(url.read())\n\n            img = PIL.Image.open('.\/temp.jpg')\n            img = np.asarray(img)\n            images.append(img)\n        except:\n            continue\n    return images\n\ndef display_images(images, title=None): \n    f, ax = plt.subplots(2,5, figsize=(18,12))\n    if title:\n        f.suptitle(title, fontsize = 30)\n\n    for i, image_id in enumerate(images):\n        ax[i\/\/5, i%5].imshow(image_id) \n   \n        ax[i\/\/5, i%5].axis('off')\n\n    plt.show() ","64185aa7":"images = load_images(links)","90cab9bd":"display_images(images)","e9bea97f":"links = df.image_url[20:30].values\nimages = load_images(links)\ndisplay_images(images)","af691676":"links = df.image_url[30:40].values\nimages = load_images(links)\ndisplay_images(images)","39231e54":"links = df.image_url[40:50].values\nimages = load_images(links)\ndisplay_images(images)","c04a277e":"links = df.image_url[50:60].values\nimages = load_images(links)\ndisplay_images(images)","d7b0b794":"### Work in progress...","7cfe02cd":"#### Some images is not downloading and temp.jpg is empty","52e6da17":"# Wikipedia - Image\/Caption Matching\n## Retrieve captions based on images\n\n\n<br>\n\n### Description\n\nA picture is worth a thousand words, yet sometimes a few will do. We all rely on online images for knowledge sharing, learning, and understanding. Even the largest websites are missing visual content and metadata to pair with their images. Captions and \u201calt text\u201d increase accessibility and enable better search. The majority of images on Wikipedia articles, for example, don't have any written context connected to the image. Open models could help anyone improve accessibility and learning for all.\n\nCurrent solutions rely on simple methods based on translations or page interlinks, which have limited coverage. Even the most advanced computer vision image captioning isn't suitable for images with complex semantics.\n\n\n### Data\n\nThe objective of this competition is to predict the target caption_title_and_reference_description given information about an images. The targets for this competition are in multiple languages.\n\n#### Files\n* train-{0000x}-of-00005.tsv - the training data (tab delimited)\n* test.tsv - the test data; the objective is to predict the target caption_title_and_reference_description for each row id\n* sample_submission.csv - a sample submission file in the correct format; note that multiple predictions (up to 5) are allowed for each id in the test data.\n* image_data_test\/\n* * image_pixels\/test_image_pixels_part-{0000x}.csv.gz\n* * * image_url: url to the original image file, e.g. https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Hovden.jpg\n* * * b64_bytes: base64 encoded bytes of the image file at a 300px resolution\n* * * metadata_url: url to the commons page of the image, e.g. https:\/\/commons.wikimedia.org\/wiki\/File:Hovden.jpg\n* * resnet_embeddings\/test_resnet_embeddings_part-{0000x}.csv.gz\n* * * image_url: url to the original image file, e.g. https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Hovden.jpg\n* * * embedding: a comma separated list of 2048 float values\n* image_data_train - Due to the size of the training image data (~275 Gb), it is hosted separately and can be found here. Note that not all of the training observations have corresponding image data.\n\n<code> kaggle competitions download -c wikipedia-image-caption <\/code>\n\n### Submission\n\nSubmissions will be evaluated using NDCG@5 (Normalized Discounted Cumulative Gain).\n\nThe submission should be a list of id,caption_title_and_reference_description pairs ranked from top to bottom according to their relevance (i.e., the top id is the most relevant caption_title_and_reference_description), with up to 5 predictions per id. Each line should be a single id,caption_title_and_reference_description pair.\n\n\n### Prizes\n\nThe top three winning teams will receive Wikipedia-branded merchandise"}}