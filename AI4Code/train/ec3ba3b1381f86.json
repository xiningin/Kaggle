{"cell_type":{"884e19a6":"code","ce52fbac":"code","b40e0c5f":"code","555bdbf4":"code","32ed8798":"code","964fbac2":"code","7153da8e":"code","b6094450":"code","aa616e82":"code","731ff49b":"code","2ee6b8cf":"code","33404e37":"code","8e424d49":"code","a75b76be":"code","8e1bef5f":"code","39412003":"code","ae84adb1":"code","941db912":"code","51b2ec5b":"code","9d6fb8c8":"code","2e671185":"code","fceb1055":"code","b9c4eb2c":"code","66c34061":"code","b8dfd9b4":"code","764349c9":"code","9c325424":"code","3440575d":"code","54eb9181":"code","07c7e33c":"code","52673902":"code","670fe396":"code","624ca821":"code","ab90d97b":"code","04835dab":"code","1bc41322":"code","d55e4caf":"code","619ddbb0":"code","fd42a977":"code","7acd92cd":"code","ddded406":"code","09ce38af":"code","6ad9e68b":"code","b02d8743":"code","9b3304ae":"code","5666013f":"code","e9f8386a":"code","818ce083":"code","6dd2ec0f":"code","4b07f49d":"code","5987d998":"code","9f43a072":"code","b2e45efc":"code","2e70109d":"code","8914056a":"code","40e9b4e2":"code","626000fa":"code","5dd380aa":"code","dd05ce6f":"code","390df905":"code","dcf75518":"code","225236e8":"code","5bdeb07f":"code","bb1d8f17":"code","487756aa":"code","e4433b65":"code","94ae935a":"code","6d613466":"code","00494f5e":"code","ca14940f":"code","5aaabd66":"code","e91ef4e8":"code","cde5ce20":"code","6773c8e7":"code","b0d5ed83":"code","1a275929":"code","50f7b139":"code","1478ce0d":"code","9c258739":"code","7fc971f2":"code","d4e79fcd":"code","56643125":"code","768a1938":"code","ba4a4c70":"code","0e53fe26":"code","c871d56b":"code","271ca41b":"code","5ace39b0":"code","0dafb77f":"code","c0b0dc6a":"code","e88ccc0b":"code","24b643c2":"code","86c56695":"code","94832739":"code","59959a77":"code","f0f720bb":"code","e0ae74cc":"code","c0715833":"code","f47f7137":"code","d5e2f337":"code","9463e23a":"code","10d4622b":"code","a8a6d2ca":"code","538a144b":"code","5f2adc31":"code","82c1fabd":"code","a691bd73":"code","8485a803":"code","977218f9":"code","0b0cb939":"code","548176c0":"code","a54fb0ff":"code","bb13d110":"code","13a5d587":"code","256a5478":"code","dffacf19":"code","d552b707":"code","6e74cc0f":"code","ec6ea1e2":"code","abe11c5d":"code","e5fb82c9":"code","22c18152":"code","02c988d0":"code","df67d492":"code","f883f12c":"markdown","a5f6de73":"markdown","c7c35f8b":"markdown","29e8546b":"markdown","5e2fc40e":"markdown","e2825194":"markdown","cbebb25e":"markdown","95d71bec":"markdown","b4465c1d":"markdown","78d7a05d":"markdown","c875ee57":"markdown","2c34f131":"markdown","abb7243b":"markdown","2b135511":"markdown","4892ab93":"markdown","0542f387":"markdown","426a159f":"markdown","6436e788":"markdown","571dcb73":"markdown","4167a4da":"markdown","83465c07":"markdown","d284b064":"markdown","ab61fc19":"markdown","ca8d2441":"markdown","4d8ce8a7":"markdown","8270b088":"markdown","5ca2c58a":"markdown","a3dbfe8b":"markdown","febd75bc":"markdown","85b83c44":"markdown","63a32f5e":"markdown","908c56b7":"markdown","0e245036":"markdown","de037370":"markdown","239dc0aa":"markdown","9b44929b":"markdown","d11fe842":"markdown","dfe52a44":"markdown","3cb399e8":"markdown"},"source":{"884e19a6":"import sys\n!curl -s https:\/\/course.fast.ai\/setup\/colab | bash\n!git clone https:\/\/github.com\/yabhi0807\/libml1.git \/kaggle\/tmp\/fastai # This is my repo with all the fastai(updated) libraries \nsys.path.append('\/kaggle\/tmp\/fastai')\n!mkdir \/kaggle\/tmp\/data\/\n!ln -s \/kaggle\/tmp\/* \/kaggle\/working\/","ce52fbac":"!mkdir \/kaggle\/tmp\/data\/nietzsche\/\n!cp -r ..\/input\/neitzshe\/* \/kaggle\/tmp\/data\/nietzsche\/\n!ls \/kaggle\/tmp\/data\/nietzsche\/","b40e0c5f":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.io import *\nfrom fastai.conv_learner import *\n\nfrom fastai.column_data import *","555bdbf4":"PATH='\/kaggle\/tmp\/data\/nietzsche\/'","32ed8798":"# get_data(\"https:\/\/s3.amazonaws.com\/text-datasets\/nietzsche.txt\", f'{PATH}nietzsche.txt')\ntext = open(f'{PATH}nietzsche.txt').read()\nprint('corpus length:', len(text))","964fbac2":"text[:1000]","7153da8e":"chars = sorted(list(set(text)))\nvocab_size = len(chars)+1\nprint('total chars:', vocab_size)","b6094450":"chars.insert(0, \"\\0\")\n\n''.join(chars[1:-6])","aa616e82":"char_indices = {c: i for i, c in enumerate(chars)}\nindices_char = {i: c for i, c in enumerate(chars)}","731ff49b":"idx = [char_indices[c] for c in text]\n\nidx[:10]","2ee6b8cf":"''.join(indices_char[i] for i in idx[:70])","33404e37":"cs=3\nc1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\nc2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\nc3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\nc4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]","8e424d49":"x1 = np.stack(c1_dat)\nx2 = np.stack(c2_dat)\nx3 = np.stack(c3_dat)","a75b76be":"y = np.stack(c4_dat)","8e1bef5f":"x1[:4], x2[:4], x3[:4]","39412003":"y[:4]","ae84adb1":"x1.shape, y.shape","941db912":"import torch","51b2ec5b":"n_hidden = 256","9d6fb8c8":"n_fac = 42","2e671185":"class Char3Model(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n\n        # The 'green arrow' from our diagram - the layer operation from input to hidden\n        self.l_in = nn.Linear(n_fac, n_hidden)\n\n        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        \n        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, c1, c2, c3):\n        in1 = torch.relu(self.l_in(self.e(c1)))\n        in2 = torch.relu(self.l_in(self.e(c2)))\n        in3 = torch.relu(self.l_in(self.e(c3)))\n        \n        h = V(torch.zeros(in1.size()).cuda())\n        h = torch.tanh(self.l_hidden(h+in1)) # using tanh for passing on from hidden(state) layer to hidden(state) layer\n        h = torch.tanh(self.l_hidden(h+in2))\n        h = torch.tanh(self.l_hidden(h+in3))\n        \n        return F.log_softmax(self.l_out(h))","fceb1055":"md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)","b9c4eb2c":"m = Char3Model(vocab_size, n_fac).cuda()","66c34061":"it = iter(md.trn_dl)\n*xs,yt = next(it)\nt = m(*V(xs)); t","b8dfd9b4":"opt = optim.Adam(m.parameters(), 1e-2)","764349c9":"fit(m, md, 4, opt, F.nll_loss)","9c325424":"set_lrs(opt, 0.001)","3440575d":"fit(m, md, 3, opt, F.nll_loss)","54eb9181":"def get_next(inp):\n    idxs = T(np.array([char_indices[c] for c in inp]))\n    p = m(*VV(idxs))\n    i = np.argmax(to_np(p))\n    return chars[i]","07c7e33c":"get_next('y. ')","52673902":"get_next('ppl')","670fe396":"get_next(' th')","624ca821":"get_next('and')","ab90d97b":"cs=8","04835dab":"c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)]","1bc41322":"c_out_dat = [idx[j+cs] for j in range(len(idx)-cs)]","d55e4caf":"xs = np.stack(c_in_dat, axis=0)\nxs.shape","619ddbb0":"y = np.stack(c_out_dat)\ny.shape","fd42a977":"xs[:cs,:cs]","7acd92cd":"y[:cs]","ddded406":"val_idx = get_cv_idxs(len(idx)-cs-1); val_idx.shape","09ce38af":"md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)","6ad9e68b":"class CharLoopModel(nn.Module):\n    # This is an RNN!\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(n_fac, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        for c in cs:\n            inp = torch.relu(self.l_in(self.e(c)))\n            h = torch.tanh(self.l_hidden(h+inp))\n        \n        return F.log_softmax(self.l_out(h), dim=-1)","b02d8743":"m = CharLoopModel(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-2)","9b3304ae":"fit(m, md, 1, opt, F.nll_loss)","5666013f":"set_lrs(opt, 0.001)","e9f8386a":"fit(m, md, 2, opt, F.nll_loss)","818ce083":"class CharLoopConcatModel(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        for c in cs:\n            inp = torch.cat((h, self.e(c)), 1)\n            inp = torch.relu(self.l_in(inp))\n            h = torch.tanh(self.l_hidden(inp))\n        \n        return F.log_softmax(self.l_out(h), dim=-1)","6dd2ec0f":"m = CharLoopConcatModel(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-3)","4b07f49d":"it = iter(md.trn_dl)\n*xs,yt = next(it)\nt = m(*V(xs)); t.size()","5987d998":"fit(m, md, 2, opt, F.nll_loss)","9f43a072":"set_lrs(opt, 1e-4)","b2e45efc":"fit(m, md, 1, opt, F.nll_loss)","2e70109d":"def get_next(inp):\n    idxs = T(np.array([char_indices[c] for c in inp]))\n    cs = VV(idxs)\n    test_v = [T(i).view(-1) for i in cs]\n    p = m(*test_v)\n    i = np.argmax(to_np(p))\n    return chars[i]","8914056a":"get_next('for thos')","40e9b4e2":"get_next('part of ')","626000fa":"get_next('queens a')","5dd380aa":"class CharRnn(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h) \n        \n        return F.log_softmax(self.l_out(outp[-1]), dim=-1)","dd05ce6f":"m = CharRnn(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-3)","390df905":"it = iter(md.trn_dl)\n*xs,yt = next(it)\ntorch.stack(xs).size(), yt.size()","dcf75518":"t = m.e(V(torch.stack(xs)))\nt.size()","225236e8":"ht = V(torch.zeros(1, 512, n_hidden))\noutp, hn = m.rnn(t, ht)\noutp.size(), hn.size()","5bdeb07f":"t = m(*V(xs)); t.size()","bb1d8f17":"fit(m, md, 3, opt, F.nll_loss)","487756aa":"set_lrs(opt, 1e-4)","e4433b65":"fit(m, md, 2, opt, F.nll_loss)","94ae935a":"def get_next(inp):\n    idxs = T(np.array([char_indices[c] for c in inp]))\n    cs = VV(idxs)\n    test_v = [T(i).view(-1) for i in cs]\n    p = m(*test_v)\n    i = np.argmax(to_np(p))\n    return chars[i]","6d613466":"get_next('for thos')","00494f5e":"def get_next_n(inp, n):\n    res = inp\n    for i in range(n):\n        c = get_next(inp)\n        res += c\n        inp = inp[1:]+c\n    return res","ca14940f":"get_next_n('for thos', 40)","5aaabd66":"c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]","e91ef4e8":"c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]","cde5ce20":"xs = np.stack(c_in_dat)\nxs.shape","6773c8e7":"ys = np.stack(c_out_dat)\nys.shape","b0d5ed83":"xs[:cs,:cs]","1a275929":"ys[:cs,:cs]","50f7b139":"val_idx = get_cv_idxs(len(xs)-cs-1)","1478ce0d":"md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)","9c258739":"class CharSeqRnn(nn.Module):\n    def __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h)\n        return F.log_softmax(self.l_out(outp), dim=-1)","7fc971f2":"m = CharSeqRnn(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-2)","d4e79fcd":"it = iter(md.trn_dl)\n*xst,yt = next(it)\ntorch.stack(xst).size(), yt.size()","56643125":"torch.stack(xst).view(-1,n_hidden).size()","768a1938":"yt.transpose(0,1).contiguous().view(-1)","ba4a4c70":"def nll_loss_seq(inp, targ):\n    sl,bs,nh = inp.size()\n    targ = targ.transpose(0,1).contiguous().view(-1)\n    return F.nll_loss(inp.view(-1,nh), targ)","0e53fe26":"fit(m, md, 20, opt, nll_loss_seq)","c871d56b":"set_lrs(opt, 1e-5)","271ca41b":"fit(m, md, 5, opt, nll_loss_seq)","5ace39b0":"m = CharSeqRnn(vocab_size, n_fac).cuda()\nopt = optim.Adam(m.parameters(), 1e-2)","0dafb77f":"m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))","c0b0dc6a":"fit(m, md, 20, opt, nll_loss_seq)","e88ccc0b":"set_lrs(opt, 1e-3)","24b643c2":"fit(m, md, 10, opt, nll_loss_seq)","86c56695":"from torchtext import vocab, data\n\nfrom fastai.nlp import *\nfrom fastai.lm_rnn import *\n\nPATH='\/kaggle\/tmp\/data\/neitzshe\/'\n\nTRN_PATH = 'trn\/'\nVAL_PATH = 'val\/'\nTRN = f'{PATH}{TRN_PATH}'\nVAL = f'{PATH}{VAL_PATH}'\n\n# Note: The student needs to practice her shell skills and prepare her own dataset before proceeding:\n# - trn\/trn.txt (first 80% of nietzsche.txt)\n# - val\/val.txt (last 20% of nietzsche.txt)\n\n%ls {PATH}","94832739":"%ls {PATH}trn","59959a77":"TEXT = data.Field(lower=True, tokenize=list)\nbs=64; bptt=8; n_fac=42; n_hidden=256\n\nFILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\nmd = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n\nlen(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)","f0f720bb":"torch.log_softmax","e0ae74cc":"class CharSeqStatefulRnn(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs):\n        self.vocab_size = vocab_size\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h.size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))","c0715833":"m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\nopt = optim.Adam(m.parameters(), 1e-3)","f47f7137":"fit(m, md, 4*10, opt, F.nll_loss)","d5e2f337":"set_lrs(opt, 1e-4)","9463e23a":"fit(m, md, 6, opt, F.nll_loss)","10d4622b":"set_lrs(opt, 1e-9)","a8a6d2ca":"fit(m, md, 2, opt, F.nll_loss)","538a144b":"# From the pytorch source\n\ndef RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))","5f2adc31":"class CharSeqStatefulRnn2(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNNCell(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h.size(1) != bs: self.init_hidden(bs)\n        outp = []\n        o = self.h\n        for c in cs: \n            o = self.rnn(self.e(c), o)\n            outp.append(o)\n        outp = self.l_out(torch.stack(outp))\n        self.h = repackage_var(o)\n        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))","82c1fabd":"m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\nopt = optim.Adam(m.parameters(), 1e-3)","a691bd73":"fit(m, md, 4, opt, F.nll_loss)","8485a803":"class CharSeqStatefulGRU(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.GRU(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h.size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))","977218f9":"# From the pytorch source code - for reference\n\ndef GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    gi = F.linear(input, w_ih, b_ih)\n    gh = F.linear(hidden, w_hh, b_hh)\n    i_r, i_i, i_n = gi.chunk(3, 1)\n    h_r, h_i, h_n = gh.chunk(3, 1)\n\n    resetgate = F.sigmoid(i_r + h_r)\n    inputgate = F.sigmoid(i_i + h_i)\n    newgate = F.tanh(i_n + resetgate * h_n)\n    return newgate + inputgate * (hidden - newgate)","0b0cb939":"m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n\nopt = optim.Adam(m.parameters(), 1e-3)","548176c0":"fit(m, md, 6, opt, F.nll_loss)","a54fb0ff":"set_lrs(opt, 1e-4)","bb13d110":"fit(m, md, 3, opt, F.nll_loss)","13a5d587":"from fastai import sgdr\n\nn_hidden=512","256a5478":"class CharSeqStatefulLSTM(nn.Module):\n    def __init__(self, vocab_size, n_fac, bs, nl):\n        super().__init__()\n        self.vocab_size,self.nl = vocab_size,nl\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n        \n    def forward(self, cs):\n        bs = cs[0].size(0)\n        if self.h[0].size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n    \n    def init_hidden(self, bs):\n        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n                  V(torch.zeros(self.nl, bs, n_hidden)))","dffacf19":"m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\nlo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)","d552b707":"os.makedirs(f'{PATH}models', exist_ok=True)","6e74cc0f":"fit(m, md, 2, lo.opt, F.nll_loss)","ec6ea1e2":"on_end = lambda sched, cycle: save_model(m, f'{PATH}models\/cyc_{cycle}')\ncb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\nfit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)","abe11c5d":"on_end = lambda sched, cycle: save_model(m, f'{PATH}models\/cyc_{cycle}')\ncb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\nfit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)","e5fb82c9":"def get_next(inp):\n    idxs = TEXT.numericalize(inp)\n    p = m(VV(idxs.transpose(0,1)))\n    r = torch.multinomial(p[-1].exp(), 1)\n    return TEXT.vocab.itos[to_np(r)[0]]","22c18152":"get_next('for thos')","02c988d0":"def get_next_n(inp, n):\n    res = inp\n    for i in range(n):\n        c = get_next(inp)\n        res += c\n        inp = inp[1:]+c\n    return res","df67d492":"print(get_next_n('for thos', 400))","f883f12c":"*idx* will be the data we use from now on - it simply converts all the characters to their index (based on the mapping above)","a5f6de73":"...and this is the next character after each sequence.","c7c35f8b":"### Test","29e8546b":"The number of latent factors to create (i.e. the size of the embedding matrix)","5e2fc40e":"### Test model","e2825194":"### Create and train model","cbebb25e":"### GRU","95d71bec":"### Setup","b4465c1d":"Pick a size for our hidden state","78d7a05d":"## RNN with pytorch","c875ee57":"### Identity init!","2c34f131":"### RNN loop","abb7243b":"Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters","2b135511":"Then create a list of the next character in each of these series. This will be the labels for our model.","4892ab93":"## Stateful model","0542f387":"Our inputs","426a159f":"## Our first RNN!","6436e788":"This is the size of our unrolled RNN.","571dcb73":"Our output","4167a4da":"### Test model","83465c07":"We're going to download the collected works of Nietzsche to use as our data.","d284b064":"## Setup","ab61fc19":"Let's take non-overlapping sets of characters this time","ca8d2441":"### Create inputs","4d8ce8a7":"For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to our model.","8270b088":"## Multi-output model","5ca2c58a":"So each column below is one series of 8 characters from the text.","a3dbfe8b":"The first 4 inputs and outputs","febd75bc":"### Putting it all together: LSTM","85b83c44":"### Create and train model","63a32f5e":"### Test model","908c56b7":"Map from chars to indices and back again","0e245036":"### Setup","de037370":"Sometimes it's useful to have a zero value in the dataset, e.g. for padding","239dc0aa":"### Create inputs","9b44929b":"### RNN","d11fe842":"Then create the exact same thing, offset by 1, as our labels","dfe52a44":"### Create and train model","3cb399e8":"## Three char model"}}