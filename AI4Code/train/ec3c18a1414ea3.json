{"cell_type":{"aaa256e2":"code","6231edb4":"code","6d9aa94e":"code","df7d8b02":"code","c4488694":"code","67c8ca47":"code","c9390e7d":"code","84fde9f8":"code","f238e08c":"code","b6022439":"code","9f95a837":"code","06517824":"code","694e5d78":"code","3aa3ba7e":"code","f05b55e9":"code","a2196a58":"code","ed963ba3":"code","afa28875":"code","35a4e56e":"code","67dc37de":"code","070f38a7":"code","730f1a22":"code","47f79a63":"code","ac4b01a6":"code","71529f03":"code","a1e2355e":"markdown","8844cc66":"markdown","2b2a6314":"markdown","b0dba9ff":"markdown","b86e0523":"markdown","2c253e49":"markdown","d803173f":"markdown","2df0b5f1":"markdown","1aaea64d":"markdown","82ee1167":"markdown","ba81d526":"markdown","d06d8482":"markdown","32e44334":"markdown","2698ea59":"markdown","b7886b06":"markdown"},"source":{"aaa256e2":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import PowerTransformer\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","6231edb4":"train_df =pd.read_csv('..\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv')\nclasses=(train_df.groupby('y').count()).index.tolist()","6d9aa94e":"''' \nFunction made by sara. This function is used to oversample the imbalanced data\n\n'''\n\nsizes={}\nfor c in classes:\n    sizes[c]=len(train_df[train_df.y==c])\nmax_size=max(sizes.values())\nt=sum(sizes.values())\nfor (k,v) in sizes.items():\n    diff=max_size-v\n    while diff >0:\n        to_add=((train_df[train_df.y==k]).head(diff).drop(columns='y'))+np.random.randint(1,100)\n        to_add['y']=k\n        train_df=train_df.append(to_add)\n        t+=len(to_add)\n        sizes[k]=sizes[k]+len(to_add)\n        diff=max_size-sizes[k]","df7d8b02":"train_df.groupby('y')['y'].count()","c4488694":"train_df.columns","67c8ca47":"cols_to_drop = ['ID','y']\n","c9390e7d":"X=train_df.drop(columns=cols_to_drop)\ny=train_df['y']\nscaler = StandardScaler()\ndata = scaler.fit_transform(X.values)\nscaled_X = pd.DataFrame(data,columns=X.columns)","84fde9f8":"# ['ID', 'Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n#        'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n#        'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n#        'ShapeFactor3', 'ShapeFactor4', 'y']\n\npca1 = PCA(n_components=1)\npca_data1 = pca1.fit_transform(scaled_X[['Perimeter','Area','ConvexArea','EquivDiameter']])","f238e08c":"pca2 = PCA(n_components=1)\npca_data2 = pca2.fit_transform(scaled_X[['ShapeFactor3','Eccentricity','AspectRation','Compactness']])","b6022439":"pca3 = PCA(n_components=1)\npca_data3 = pca3.fit_transform(scaled_X[['MinorAxisLength','ShapeFactor1']])\npca_data3.shape","9f95a837":"pcas_frame = pd.DataFrame({'PCA1':pca_data1[:,0],'PCA2':pca_data2[:,0],'PCA3':pca_data3[:,0]})\npcas_frame.head()","06517824":"cols =['ConvexArea','EquivDiameter','Eccentricity','AspectRation','MinorAxisLength','Compactness','Area']\nfinal_X = pd.concat([scaled_X.drop(columns= cols),pcas_frame],axis='columns')\nfinal_X.info()","694e5d78":"sns.heatmap(final_X.corr()>0.7, cmap=\"YlGnBu\", annot=True)","3aa3ba7e":"pca_all = PCA()\npca_all.fit(final_X)\ndata = pca_all.transform(final_X)\ndf14 = pd.DataFrame(data)\n# #,columns=final_X.columns","f05b55e9":"data.shape","a2196a58":"# ros = SMOTE(random_state=42)\n\n# X_res, y_res = ros.fit_resample(df14, y)\n# y_res.value_counts()","ed963ba3":"X_train, X_val, y_train, y_val = train_test_split(df14,y , test_size=0.2, random_state=2,stratify=y)","afa28875":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.linear_model import SGDClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.ensemble import GradientBoostingClassifier\n# from xgboost.sklearn import XGBClassifier\n# from sklearn.tree import DecisionTreeClassifier\n\n# methods=[RandomForestClassifier,LGBMClassifier,XGBClassifier,DecisionTreeClassifier]\n\n\n# for fun in methods: \n#     print(fun)\n#     model=fun()\n#     model.fit(X_train, y_train)\n#     pred_train=model.predict(X_train)\n#     print(f1_score(y_train,pred_train,average='macro'))\n\n#     pred_val=model.predict(X_val)\n#     print(f1_score(y_val,pred_val,average='macro'))\n#     print()","35a4e56e":"lgb = LGBMClassifier(learning_rate=0.19,max_depth=6,n_estimators=120,reg_lambda=0.25,num_leaves = 46,class_weight ='balanced').fit(X_train,y_train)","67dc37de":"sns.heatmap(X_train.corr()>0.70, cmap=\"YlGnBu\", annot=True)","070f38a7":"cross_val = cross_val_score(lgb,df14,y,  cv=5)\nprint(cross_val)\nprint(cross_val.mean())","730f1a22":"pred_train=lgb.predict(X_train)\nprint(f1_score(y_train,pred_train,average='macro'))\n#X_train,y_train  ,coef0=0.1\npred_val=lgb.predict(X_val)\nprint(f1_score(y_val,pred_val,average='macro'))\n# \n\n\nmat = (confusion_matrix(y_val,pred_val)).T\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=classes, yticklabels=classes)\n\nplt.xlabel('true label')\nplt.ylabel('predicted label')\nplt.show()","47f79a63":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef plotImp(model, X , num = 20, fig_size = (40, 20)):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    #plt.figure(figsize=fig_size)\n    #sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LGBM Features (avg over folds)')\n    plt.tight_layout()\n    #plt.savefig('lgbm_importances-01.png')\n    plt.show()\n    \nplotImp(lgb, X_train , num = 20, fig_size = (40, 20))  ","ac4b01a6":"test_df =pd.read_csv('..\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv')\n\n\n#need orginzing, anyway this is working on test data\n\ntest_X = scaler.transform(test_df.drop(columns=['ID']).values)\nscaled_test = pd.DataFrame(test_X,columns=X.columns)\npca_test1 = pca1.transform(scaled_test[['Perimeter','Area','ConvexArea','EquivDiameter']])\npca_test2 = pca2.transform(scaled_test[['ShapeFactor3','Eccentricity','AspectRation','Compactness']])\npca_test3 = pca3.transform(scaled_test[['MinorAxisLength','ShapeFactor1']])\npcas_frame_test = pd.DataFrame({'PCA1':pca_test1[:,0],'PCA2':pca_test2[:,0],'PCA3':pca_test3[:,0]})\n\ntest_X = pd.concat([scaled_test.drop(columns= cols),pcas_frame_test],axis='columns')\ndata_test = pca_all.transform(test_X)\ndf14_test = pd.DataFrame(data_test)","71529f03":"pred_test=lgb.predict(df14_test)\ntest_df['y']=pred_test\ntest_df[['ID','y']].to_csv('submission.csv', index=False)\ntest_df['y']","a1e2355e":"After deciding what features to get in PCA - refer to this notebook: [SIRA Vs DERMASON](https:\/\/www.kaggle.com\/decpeony\/sira-vs-derma?scriptVersionId=75958614) - let's normalize the data first.","8844cc66":"We decorrelate many of the features, but still other features are very high in correlation.","2b2a6314":"# Dry Bean Classification","b0dba9ff":"Adding the PCAs to a dataframe to concat them.","b86e0523":"PCA2 for features: 'ShapeFactor3','Eccentricity','AspectRation','Compactness'.","2c253e49":"PCA1 for features: Area, ConvexArea , EquivDiameter, Perimeter.","d803173f":"To understand more about data, check this notebook: [SIRA Vs DERMASON](https:\/\/www.kaggle.com\/decpeony\/sira-vs-derma?scriptVersionId=75958614) and this: [backStage](https:\/\/www.kaggle.com\/saramohey\/backstage?scriptVersionId=74898657)","2df0b5f1":"This cell was made to try different base classifiers. RandomForests and LGBM gave the best results.","1aaea64d":"Concating the two dataframes","82ee1167":"From paper: Multiclass classification of dry beans using computer vision and machine\nlearning techniques\nMurat Koklu\u204e, Ilker Ali Ozkan","ba81d526":"**Note: PCA isn't for dimensionality reduction, we use it here to capture the variance and increase classification accuracy and aslo to decorrelate the data.**","d06d8482":"This function is made to oversample the data. it made us get more than 95.00 on the public leaderboard.","32e44334":"OverSampling the data using SMOTE.","2698ea59":"It seems that our dataset is now balanced.","b7886b06":"\"There is a wide range of genetic diversity of dry bean which is the most produced one among the edible legume\ncrops in the world. Seed quality is definitely influential in crop production. Therefore, seed classification is\nessential for both marketing and production.\""}}