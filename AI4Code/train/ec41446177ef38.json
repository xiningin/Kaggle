{"cell_type":{"8741471a":"code","c13bc915":"code","d801ed27":"code","86c1eee5":"code","aa3a8d8c":"code","93d128f5":"code","a033d58c":"code","c8464fdd":"code","3ee4561d":"code","d0161503":"markdown","fe92b10c":"markdown","65fc4c73":"markdown","5bfef6a2":"markdown","adbc439a":"markdown","13ba3bf1":"markdown","86c6a6fe":"markdown","0817f232":"markdown","b24c5ac6":"markdown","00003ed2":"markdown","649bdd9c":"markdown","6d478b6f":"markdown"},"source":{"8741471a":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn import metrics\n\nimport time\n\nrand_seed = 713\nfolder_path=\"..\/input\/fingers\/train\/\"\n#img_file = \"{}{}.jpg\".format(folder_path, str(0))\nfiles = os.listdir(folder_path)\nprint(\"Files :\",len(files))\nimg_file = folder_path + files.pop()\nprint(\"what\",img_file)\nio1 = io.imread(img_file, as_gray=True)\nio2 = transform.rescale(io1, scale=0.5, anti_aliasing=True)\n\nprint(\"Before: shape\",io1.shape, \" size\", io1.size, \" min\", io1.min(), \" max\", io1.max())\nprint(\"After:  shape\",io2.shape, \" size\", io2.size, \" min\", io2.min(), \" max\", io2.max())\n\nplt.figure(figsize=(12,5))\n\nplt.subplot(121)\nplt.imshow(io1.reshape(128,128), cmap=\"gray\")\n\nplt.subplot(122)\nplt.imshow(io2.reshape(64, 64), cmap=\"gray\")\n\nplt.show()","c13bc915":"io2 = io2.flatten()\nimages_df = pd.DataFrame([io2])\ntag = int(img_file[-6])\nimages_df[\"tag\"] = tag\n\ntime_start = time.time()\n\nfor index in range(1,10000):\n    img_file = folder_path + files.pop()\n    if (img_file[-5] == \"L\"):\n        tag = int(img_file[-6])\n# if the image file isn't there, the notebook should throw an error so we deal with it\n\n        pic = io.imread(img_file, as_gray=True)\n        pic = transform.rescale(pic, scale=0.5, anti_aliasing=True).flatten()\n        new_image = pd.DataFrame([pic])\n        new_image[\"tag\"] = tag\n        images_df = images_df.append(new_image)\n\nprint('Loading images done! Time elapsed: {} seconds'.format(time.time()-time_start))\n\ny = images_df['tag']\nX = images_df.loc[:, ~images_df.columns.isin(['tag'])]\n\nprint(\"Dataframe is size\",X.shape)","d801ed27":"colours = {0:'blue', 1:'red', 2:'green', 3:'cyan', 4:'magenta', 5:'yellow', 6:'black', -1:'grey'}\n\n# We just want the first two principal components\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(10,5))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"Set1\")\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\nplt.show()","86c1eee5":"tsne = TSNE(n_components=2, verbose=1, n_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nplt.figure(figsize=(10,5))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=\"Set1\")\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\nplt.show()","aa3a8d8c":"X_umap = umap.UMAP(n_neighbors=15,\n                   min_dist=0.3,\n                   metric='correlation').fit_transform(X)\n\nplt.figure(figsize=(10,5))\nplt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap=\"Set1\")\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\nplt.show()","93d128f5":"kmethod2 = KMeans(n_clusters=2, random_state=rand_seed)\nkclusters2 = kmethod2.fit_predict(X)\n\nprint(\"The silhouette score of the 2 KMeans solution: {}\"\n      .format(metrics.silhouette_score(X, kclusters2, metric='euclidean')))\n\nplt.figure(figsize=(14,4))\nplt.subplot(1, 3, 1)\ncolorarr = [colours[x] for x in kclusters2]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(kclusters2), c=colorarr)\nplt.title('KMeans2')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\nkmethod4 = KMeans(n_clusters=4, random_state=rand_seed)\nkclusters4 = kmethod4.fit_predict(X)\n\nprint(\"The silhouette score of the 4 KMeans solution: {}\"\n      .format(metrics.silhouette_score(X, kclusters4, metric='euclidean')))\n\nplt.subplot(1, 3, 2)\ncolorarr = [colours[x] for x in kclusters4]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(kclusters4), c=colorarr)\nplt.title('KMeans4')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\n\nkmethod6 = KMeans(n_clusters=6, random_state=rand_seed)\nkclusters6 = kmethod6.fit_predict(X)\n\nprint(\"The silhouette score of the 6 KMeans solution: {}\"\n      .format(metrics.silhouette_score(X, kclusters6, metric='euclidean')))\n\nplt.subplot(1, 3, 3)\ncolorarr = [colours[x] for x in kclusters6]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(kclusters6), c=colorarr)\nplt.title('KMeans6')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\nplt.show()","a033d58c":"# Defining the agglomerative clustering\ndbs_cluster = DBSCAN(eps=5.0, min_samples=15)\n\n# Fit model\ndbsclusters = dbs_cluster.fit_predict(X)\n\nprint(\"The silhouette score of the DBSCAN solution: {}\"\n      .format(metrics.silhouette_score(X, dbsclusters, metric='euclidean')))\n\nplt.figure(figsize=(5,4))\ncolorarr = [colours[x] for x in dbsclusters]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(dbsclusters),\n            c=colorarr)\nplt.title('DBScan')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\nplt.show()","c8464fdd":"gmm_cluster2 = GaussianMixture(n_components=2, random_state=rand_seed)\ngclusters2 = gmm_cluster2.fit_predict(X)\nprint(\"The silhouette score of the 2 Gaussian Mixture solution: {}\"\n      .format(metrics.silhouette_score(X, gclusters2, metric='euclidean')))\n\nplt.figure(figsize=(12,4))\nplt.subplot(1, 3, 1)\ncolorarr = [colours[x] for x in gclusters2]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(gclusters2), c=colorarr)\nplt.title('GausMix2')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\ngmm_cluster4 = GaussianMixture(n_components=4, random_state=rand_seed)\ngclusters4 = gmm_cluster4.fit_predict(X)\nprint(\"The silhouette score of the 4 Gaussian Mixture solution: {}\"\n      .format(metrics.silhouette_score(X, gclusters4, metric='euclidean')))\n\nplt.subplot(1, 3, 2)\ncolorarr = [colours[x] for x in gclusters4]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(gclusters4), c=colorarr)\nplt.title('GausMix4')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\ngmm_cluster6 = GaussianMixture(n_components=6, random_state=rand_seed)\ngclusters6 = gmm_cluster6.fit_predict(X)\nprint(\"The silhouette score of the 6 Gaussian Mixture solution: {}\"\n      .format(metrics.silhouette_score(X, gclusters6, metric='euclidean')))\n\nplt.subplot(1, 3, 3)\ncolorarr = [colours[x] for x in gclusters6]\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(gclusters6), c=colorarr)\nplt.title('GausMix6')\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\nplt.show()","3ee4561d":"from sklearn.cluster import AffinityPropagation\n\ntime_start = time.time()\n\n# Defining the affinity propagation clustering\nafprop = AffinityPropagation(max_iter=2000, convergence_iter=50, verbose=True, random_state=rand_seed)\n\n# Fit model\napclusters = afprop.fit_predict(X)\nfound_clusters = len(np.unique(apclusters))\n\nprint('Affinity propagation done! Time elapsed 500 iter: {} seconds'.format(time.time()-time_start))\n\nprint(\"The silhouette score of affinity propagation: {}\"\n      .format(metrics.silhouette_score(X, apclusters, metric='euclidean')))\nprint(\"Clusters generated =\", found_clusters)\n\nif (found_clusters < 8):\n    plt.figure(figsize=(5,4))\n    colorarr = [colours[x] for x in apclusters]\n    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], label=str(apclusters),\n            c=colorarr)\n    plt.title('DBScan')\n    plt.xticks([])\n    plt.yticks([])\n    plt.axis('off')\n    plt.show()","d0161503":"### Dimensionality Reduction: UMAP ###\nGood visualization of the six clusters. But I might suspect\na 7th or 8th cluster by breaking up the red and yellow points,\nand we don't have great separation between the purple and blue\nclusters","fe92b10c":"### Clusterizing: Gaussian Mixture ###\nTried Gaussian Mixture with 2, 4, and 6 clusters.\nNone of the results are particularly effective.","65fc4c73":"### Conclusion: dimensionality reduction techniques and visualization ###\nI will use the t-sne vectors for visualizing the data for the rest of this notebook.\nI think it edges the UMAP result, and the PCA result was clearly inferior.","5bfef6a2":"# Challenge #\n~~~\nSpend some time on Kaggle and pick a beautiful dataset.\nThen, using that dataset, do the following:\n- Apply dimensionality reduction techniques to visualize the observations.\n- Apply clustering techniques to group similar observations.\n\nFor each of the tasks above, try several methods and select the best-performing one.\nDiscuss how dimensionality reduction and clustering algorithms enable you to\ngain insights regarding the data.\n~~~\nA preamble...\n\nThe class lessons for \"Unsupervised learning: clustering\" applied techniques to small\nimages, either of distinct consumer goods, or the numbers 0 to 9. I was looking for\nother images to use, and at first tried a set with animal images (lion, tiger, cheetah,\nfox, wolf) called \"Unsupervised Animal Classification\":\n\nhttps:\/\/www.kaggle.com\/shivam2811\/unsupervised-animal-dataset\n\nThat was a big mistake.\n\nThe analysis was mud, with nothing usable. In hindsight, I can appreciate that the\nsimple technique of working with a dataframe of image pixels needed more conditions.\nThe animal images in that set could be facing left, right, or center. Combined with\nthe great variety of backgrounds at the edge of the images made it difficult for\nthe algorithms.\n\nLearning from that failure, I chose an image set called \"Fingers\", which has a hand\nholding 0 to 5 fingers upraised.\n\nhttps:\/\/www.kaggle.com\/koryakinp\/fingers\n\nThe images all have the same dark background, and the hand positions were made as similar\nas possible.","adbc439a":"### Clusterizing: DBSCAN ###\nWhile it might be possible to fiddle with the parameters to get a better result, the best I could\ndo with DBSCAN was 1 cluster and all the other clusters being assigned to a common group.","13ba3bf1":"### Dimensionality Reduction: t-sne ###\n\nThis will be effective, since we can see the six distinct clusters (even if they weren't colored).\nHowever, if we did not know the number of clusters ahead of time, I might think that the right-most red\npoints are forming a tiny 7th cluster.","86c6a6fe":"### Conclusion: clustering techniques ###\nNone of the clusterizing algorithms used here were as effective as the t-sne or UMAP vectors for\ndistinguishing the image groups.\n(It might be possible that tweaking the algorithm hyperparameters could improve performance, but that\nrequires significant time spent on trial-and-error.)\n\nIn my opinion, KMeans is the highest performer out of the ones tested.","0817f232":"### Clusterizing: Affinity Propagation ###\n264 clusters is more than a few off. I'd need to learn what hyperparameters are\nneeded to control this algorithm, before I would chose to use it.","b24c5ac6":"There are 18000 files. The last two character indicate the number of fingers held up (0 to 5)\nand if it is left or right handed.\n\nI read in 10K files, and only used left handed images (5011 files).","00003ed2":"### Clusterizing: K-Means ###\nTried KMeans with 2, 4, and 6 clusters.\nIt started encouraging, with the far left cluster (in t-sne phase space) being found.\nThe 4 cluster is not too bad, as 3 actual clusters are grouped together, but there are\na small number of mis-assignments being made.\nThe 6 cluster solution had problems, mostly with the green and cyan clusters on the right.","649bdd9c":"### Dimensionality Reduction: PCA ###\n\nPCA does not perform well for visualization. The data points appear in a large mass, without\ndistinct separation.","6d478b6f":"In the lessons, we used images that were 28x28 with 256 greyscale.\n\nI decided to reduce the size of the images in this dataset from 128x128 to 64x64.\n(I believe the images are already greyscale, but it is safer to make certain the files\nare read in that way.)\n\nAfter the size reduction, the pixel values are between 0 and 1, so they are already standardized."}}