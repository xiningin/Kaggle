{"cell_type":{"6f9ba2a1":"code","c840a592":"code","e96f5fbf":"code","069f46e3":"code","2c94ba30":"code","2168552a":"code","aad21f6c":"code","76e496ad":"code","68942ee9":"code","be794cec":"code","408a248f":"code","f967bd49":"code","b86320a2":"code","6adc6692":"code","fabaa759":"markdown","1f2c0d76":"markdown","6fc11024":"markdown"},"source":{"6f9ba2a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.datasets import load_boston\n\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n\nfrom itertools import combinations\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,15)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","c840a592":"boston_dataset = load_boston()\nboston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston[\"MEDV\"] = boston_dataset.target\nboston.head()","e96f5fbf":"corr = boston.corr().abs()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","069f46e3":"X = boston[['LSTAT']].values\ny = boston['MEDV'].values\n\nlr = LinearRegression()\n\nquad = PolynomialFeatures(2)\ncub = PolynomialFeatures(3)\n\nX_quad = quad.fit_transform(X)\nX_cub = cub.fit_transform(X)","2c94ba30":"X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\nlr.fit(X, y)\ny_lin_fit = lr.predict(X_fit)\nlinear_r2 = r2_score(y, lr.predict(X))","2168552a":"lr.fit(X_quad, y)\ny_quad_fit = lr.predict(quad.transform(X_fit))\nquad_r2 = r2_score(y, lr.predict(X_quad))","aad21f6c":"lr.fit(X_cub, y)\ny_cub_fit = lr.predict(cub.transform(X_fit))\ncub_r2 = r2_score(y, lr.predict(X_cub))","76e496ad":"dtr = DecisionTreeRegressor(max_leaf_nodes=15)\ndtr.fit(X, y)\ny_tree_fit = dtr.predict(X_fit)\ntree_r2 = r2_score(y, dtr.predict(X))","68942ee9":"svr = SVR(C=3)","be794cec":"svr.fit(X, y)\ny_svr_fit = svr.predict(X_fit)\nsvr_r2 = r2_score(y, svr.predict(X))","408a248f":"plt.scatter(X, y, label=\"training points\", color=\"darkgray\", lw=5)\nplt.plot(X_fit, y_lin_fit, label=\"linear d=1 $R^2=%.2f$\" % linear_r2,\n         color='blue', lw=2, linestyle=\":\")\nplt.plot(X_fit, y_quad_fit, label=\"quadr d=2 $R^2=%.2f$\" % quad_r2,\n         color='red', lw=2, linestyle=\"-\")\nplt.plot(X_fit, y_cub_fit, label=\"cubic d=3 $R^2=%.2f$\" % cub_r2,\n         color='green', lw=2, linestyle=\"--\")\nplt.plot(X_fit, y_tree_fit, label=\"RegTree d=1 $R^2=%.2f$\" % tree_r2,\n         color='cyan', lw=2, linestyle=\"--\")\nplt.plot(X_fit, y_svr_fit, label=\"SVR d=1 $R^2=%.2f$\" % svr_r2,\n         color='purple', lw=2, linestyle=\"-\")\nplt.xlabel(\"% lower status of the population [LSTAT]\")\nplt.ylabel('Price [MEDV]')\nplt.legend(loc=\"upper right\")\nplt.show()","f967bd49":"X = boston.drop(\"MEDV\", axis=1)\ny = boston['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclfs = {\n    \"lr d=1\": Pipeline([('scaler', StandardScaler()), ('lr', lr)]),\n    \"lr d=2\": Pipeline([('scaler', StandardScaler()), (\"pf\", quad), ('lr', lr)]),\n    \"lr d=3\": Pipeline([('scaler', StandardScaler()), (\"pf\", cub), ('lr', lr)]),\n    \"dtr\": dtr,\n    \"svr\": Pipeline([('scaler', StandardScaler()), ('svr', svr)])\n}","b86320a2":"scores = {}\nfor clf_i in clfs:\n    clf = clfs[clf_i]\n    min_score_mse = 999999999999\n    min_score_r2 = None\n    best_features = None\n    for num in range(1, len(X)+1):\n        for featureset in combinations(X.columns, num):\n            clf.fit(X_train[list(featureset)], y_train)\n            predicted = clf.predict(X_test[list(featureset)])\n            mse = mean_squared_error(y_test, predicted)\n            r2 = r2_score(y_test, predicted)\n            #print(clf_i, featureset, \"r2: %s\\tmse:%s\" % (r2, mse))\n            if min_score_mse > mse:\n                min_score_mse = mse\n                min_score_r2 = r2\n                best_features = featureset\n    scores[clf_i] = {\"featureset\": best_features,\n                     \"mse\": min_score_mse,\n                     \"r2\": min_score_r2}","6adc6692":"for clf in scores:\n    print(clf, scores[clf])","fabaa759":"    5. \u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f 1 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u043d\u0430\u0438\u0445\u0443\u0434\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.\n    4. \u041c\u0435\u0442\u043e\u0434 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0437\u0430\u043c\u0435\u0442\u043d\u043e \u043b\u0443\u0447\u0448\u0435, \u043d\u043e \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e \u0442\u0430\u043a\u043e\u0439 \u0441\u0435\u0431\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\n    3. \u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f 2 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u043f\u043e\u0447\u0442\u0438 \u0432\u0441\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    2. \u0414\u0435\u0440\u0435\u0432\u043e \u0425\u043e\u0440\u0448\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0441\u0435\u0433\u043e \u043d\u0430 \u0442\u0440\u0451\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 ('NOX', 'RM', 'RAD')\n    1. \u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f 3 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u0425\u043e\u0442\u044f \u0438 \u043d\u0435\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043b\u0443\u0447\u0448\u0435 \u0447\u0435\u043c \u0443 \u0434\u0435\u0440\u0435\u0432\u0430","1f2c0d76":"\u041f\u0435\u0440\u0435\u0431\u043e\u0440\u043e\u043c \u0441\u0440\u0435\u0434\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u043e\u0432 \u0438 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0430\u0439\u0434\u0451\u043c \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u044f \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u043e\u0448\u0438\u0431\u043a\u0438","6fc11024":"\u0426\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0438\u043c\u0435\u0435\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0443\u044e \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u0441 LSTAT 0,74 \u0438 \u0441 RM 0.7. \u0422\u0430\u043a \u0436\u0435 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0437\u0430\u0432\u0438\u0441\u044f\u0442 \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0441\u0438\u043b\u044c\u043d\u043e \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 RAD \u0438 TAX, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0443\u0445\u0443\u0434\u0438\u0448\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439."}}