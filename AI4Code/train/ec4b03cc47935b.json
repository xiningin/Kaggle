{"cell_type":{"d8788971":"code","45741ea8":"code","29a11604":"code","d2e280b8":"code","2f530fd6":"code","ad8b7653":"code","1b8fdb54":"code","c2e8ae8b":"code","63f074ed":"code","f7502b6e":"code","5cd9b530":"code","4d5de858":"code","29a472bd":"code","ced3a741":"code","920161af":"code","b53e05a2":"code","3ae2fdff":"code","daa2d7bb":"code","381e6a14":"code","e48012c8":"code","c5760662":"code","ad65974f":"code","5af4ebea":"code","1191ee3b":"code","40925fbe":"code","a35334b8":"code","b3b55a24":"code","b6affdb7":"code","ad7768dd":"code","9f0d16f5":"code","61bebd12":"code","a073a68d":"code","9ca7fb0a":"code","a71ea414":"code","f0e57883":"code","a30d24d8":"code","7fe90b38":"code","1cdae720":"code","3d0d6f55":"code","8c26a9f0":"code","0d56f3a4":"code","bd485d0f":"code","7a51ee55":"code","9d81cf0f":"code","a4ee1a36":"code","8765112b":"code","7f59731e":"code","59b59dc1":"code","9f5a0352":"code","ba406203":"code","ad8beff3":"code","54929fa2":"code","6f70ece6":"code","3b65c740":"code","aa575c07":"code","75565390":"code","1ff8996a":"code","35ad6806":"code","163f4f0a":"code","cda89ffb":"code","fb17d09d":"code","fde1d894":"code","c7fa655d":"code","1aa7ffc0":"code","23901456":"code","4dd3ed2c":"code","093083bb":"code","36fc5a68":"code","25bebd2d":"code","ddedbc4e":"code","d3cb1ca1":"code","5ce4defc":"code","dd8cfaf9":"code","94259ae2":"code","932a9826":"code","37fcea41":"code","c4441ee0":"markdown","3cd633ca":"markdown","c0f1ed36":"markdown","9e83bcaa":"markdown","c50bec7c":"markdown","33002e62":"markdown","292defc9":"markdown","61aa4733":"markdown"},"source":{"d8788971":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","45741ea8":"# For iPlot graphs in Google Colab\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True) # For Notebooks\ncf.go_offline() # For offline use","29a11604":"dataset = pd.read_csv('..\/input\/regression-datasets\/Data.csv')\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,3].values","d2e280b8":"dataset.head()","2f530fd6":"dataset.describe()","ad8b7653":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan,strategy='mean')\nX[:,1:3] = imputer.fit_transform(X[:,1:3])","1b8fdb54":"X","c2e8ae8b":"sns.pairplot(dataset,hue='Purchased')","63f074ed":"dataset.iplot(kind='bar',x='Age',y='Salary')","f7502b6e":"dataset['Country'].iplot(kind='hist')","5cd9b530":"dataset.scatter_matrix()","4d5de858":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer","29a472bd":"columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX=np.array(columnTransformer.fit_transform(X),dtype=np.str)\ny=LabelEncoder().fit_transform(y)","ced3a741":"X,y","920161af":"from sklearn.model_selection import train_test_split","b53e05a2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","3ae2fdff":"X_test","daa2d7bb":"from sklearn.preprocessing import StandardScaler, Normalizer\n# Here i am testing StandardScaler first. Remember to check Normaliser also as we are not transforming dummy variables which are between 0-1.","381e6a14":"sc_x = StandardScaler()\n#nr_x = Normalizer()\n#X_train = nr_x.fit_transform(X_train)\n#X_test = nr_x.transform(X_test)\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)","e48012c8":"X_train","c5760662":"dataset = pd.read_csv('..\/input\/regression-datasets\/Salary_Data.csv')","ad65974f":"dataset.head()","5af4ebea":"dataset.describe()","1191ee3b":"X = dataset.iloc[:,:1].values\ny = dataset.iloc[:,1].values","40925fbe":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/3,random_state=0)","a35334b8":"X_train,y_train","b3b55a24":"from sklearn.linear_model import LinearRegression","b6affdb7":"regressor = LinearRegression()","ad7768dd":"regressor.fit(X_train,y_train)","9f0d16f5":"y_pred = regressor.predict(X_test)","61bebd12":"# Plotting training set results\nplt.scatter(X_train, y_train,color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (Training Set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')","a073a68d":"# Plotting test set results\nplt.scatter(X_test, y_test,color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (Test Set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')","9ca7fb0a":"dataset = pd.read_csv(\"..\/input\/regression-datasets\/50_Startups.csv\")","a71ea414":"dataset.head()","f0e57883":"dataset.describe()","a30d24d8":"X = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1].values","7fe90b38":"X[:5,:]","1cdae720":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nct = ColumnTransformer([('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nX = ct.fit_transform(X)","3d0d6f55":"X = X[:,1:]","8c26a9f0":"X[:5,]","0d56f3a4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","bd485d0f":"#checking the redial plot.. is it normally distributed or not\n#because OLS assumes that data is homoscadastic and residuals of errors are normally distributed\nfrom sklearn.linear_model import LinearRegression\nfrom yellowbrick.regressor import residuals_plot\nviz = residuals_plot(LinearRegression(), X_train, y_train, X_test, y_test)","7a51ee55":"# Though we don't need to scale our data for MLR as the algorithm will automatically take care of it.\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test)\nsc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train.reshape(-1,1))","9d81cf0f":"y_test","a4ee1a36":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","8765112b":"y_pred = lr.predict(X_test)","7f59731e":"# Building the optimal model using backward elimination model\nimport statsmodels.regression.linear_model as lm\nX = np.append(arr=np.ones((50,1)).astype(int), values=X, axis=1)","59b59dc1":"X_opt = X[:, [0,1,2,3,4,5]]\nX_opt = np.array(X_opt, dtype=float)\nregressor_OLS = lm.OLS(endog = y, exog = X_opt).fit() \nregressor_OLS.summary()","9f5a0352":"type(X_opt)","ba406203":"X_opt = X[:, [0,1,3,4,5]]\nX_opt = np.array(X_opt, dtype=float)\nregressor_OLS = lm.OLS(endog = y, exog = X_opt).fit() \nregressor_OLS.summary()","ad8beff3":"X=X.astype('float64')","54929fa2":"X_opt = X[:, [0,3,4,5]]\nregressor_OLS = lm.OLS(endog = y, exog = X_opt).fit() \nregressor_OLS.summary()","6f70ece6":"X_opt = X[:, [0,3,5]]\nregressor_OLS = lm.OLS(endog = y, exog = X_opt).fit() \nregressor_OLS.summary()","3b65c740":"X_opt = X[:, [0,3]]\nregressor_OLS = lm.OLS(endog = y, exog = X_opt).fit() \nregressor_OLS.summary()","aa575c07":"import statsmodels.api as sm\n\n#define figure size\nfig = plt.figure(figsize=(12,8))\n\n#produce regression plots\nfig = sm.graphics.plot_regress_exog(regressor_OLS,1,fig=fig)","75565390":"dataset = pd.read_csv('..\/input\/regression-datasets\/Position_Salaries.csv')","1ff8996a":"dataset.head()","35ad6806":"dataset.describe()","163f4f0a":"X = dataset.iloc[:,1:2].values\ny = dataset.iloc[:,2].values","cda89ffb":"plt.plot(X,y)\nplt.xlabel('Level')\nplt.ylabel('Salary')\nplt.title(\"Polynomial relation in Level vs Salary\")","fb17d09d":"from sklearn.linear_model import LinearRegression\nlin_reg1 = LinearRegression()\nlin_reg1.fit(X,y)","fde1d894":"plt.scatter(X, y,color='red')\nplt.plot(X, lin_reg1.predict(X), color='blue')\nplt.title('Salary vs Level (Simple Linear Model)')\nplt.xlabel('Level')\nplt.ylabel('Salary')","c7fa655d":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=4)\nX_poly = poly_reg.fit_transform(X)\nlin_reg2 = LinearRegression()\nlin_reg2.fit(X_poly, y)","1aa7ffc0":"X_grid = np.arange(min(X),max(X),step=0.1)\nX_grid = X_grid.reshape((len(X_grid)),1)\nplt.scatter(X, y,color='red')\nplt.plot(X_grid, lin_reg2.predict(poly_reg.fit_transform(X_grid)), color='blue')\nplt.title('Salary vs Level (Polynomial Linear Model)')\nplt.xlabel('Level')\nplt.ylabel('Salary')","23901456":"dataset = pd.read_csv('..\/input\/regression-datasets\/Position_Salaries.csv')\nX = dataset.iloc[:,1:2].values\ny = dataset.iloc[:,2:].values","4dd3ed2c":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nX = sc_x.fit_transform(X)\ny = sc_y.fit_transform(y)","093083bb":"from sklearn.svm import SVR\nreg = SVR(kernel='rbf')\nreg.fit(X,y)","36fc5a68":"X_grid = np.arange(min(X),max(X),step=0.1)\nX_grid = X_grid.reshape((len(X_grid)),1)\nplt.scatter(X, y,color='red')\nplt.plot(X_grid, reg.predict(X_grid), color='blue')\nplt.title('Salary vs Level (SVR Model)')\nplt.xlabel('Level')\nplt.ylabel('Salary')","25bebd2d":"y_pred = sc_y.inverse_transform(reg.predict(sc_x.transform([[6.5]])))","ddedbc4e":"y_pred","d3cb1ca1":"dataset = pd.read_csv('..\/input\/regression-datasets\/Position_Salaries.csv')\nX = dataset.iloc[:,1:2].values\ny = dataset.iloc[:,2:].values","5ce4defc":"from sklearn.tree import DecisionTreeRegressor\nreg = DecisionTreeRegressor()\nreg.fit(X,y)","dd8cfaf9":"X_grid = np.arange(min(X),max(X),step=0.1)\nX_grid = X_grid.reshape((len(X_grid)),1)\nplt.scatter(X, y,color='red')\nplt.plot(X_grid, reg.predict(X_grid), color='blue')\nplt.title('Salary vs Level (Decision Tree Regression)')\nplt.xlabel('Level')\nplt.ylabel('Salary')","94259ae2":"dataset = pd.read_csv('..\/input\/regression-datasets\/Position_Salaries.csv')\nX = dataset.iloc[:,1:2].values\ny = dataset.iloc[:,2:].values","932a9826":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor(n_estimators=10)\nreg.fit(X,y)","37fcea41":"X_grid = np.arange(min(X),max(X),step=0.01)\nX_grid = X_grid.reshape((len(X_grid)),1)\nplt.scatter(X, y,color='red')\nplt.plot(X_grid, reg.predict(X_grid), color='blue')\nplt.title('Salary vs Level (Random Forest Regression)')\nplt.xlabel('Level')\nplt.ylabel('Salary')","c4441ee0":"# **4. Polynomial Linear Regression**","3cd633ca":"# **2. Simple Linear Regression**","c0f1ed36":"# **5. Support Vector Regression**","9e83bcaa":"From the above graph of residuals we can say that this data is fairly random distributed hence does not suffer from the problem of heteroscedaticity, it has also somewhat uniform distribution of the residuals against the target in two dimensions. This seems to indicate that our linear model is good but can be improved. We can also see from the histogram that our error is not perfectly normally distributed around zero, which also generally indicates it is not a best fitted model, improvements can be done.","c50bec7c":"# **1. Data Preprocessing**","33002e62":"# **7. Random Forest Regression**","292defc9":"# **3. Multiple Linear Regression**","61aa4733":"# **6. Descion Tree Regression**"}}