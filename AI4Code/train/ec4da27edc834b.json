{"cell_type":{"1b0b7ce1":"code","8fd6efdc":"code","048188e8":"code","bca3de0a":"code","0cdb9bff":"code","75896557":"code","4ab666c4":"code","aa9aa705":"code","cbfd2765":"code","f8674e84":"code","3b4f3bbd":"code","132c94a9":"code","e2d70fb0":"code","554eb94b":"code","6e8d44d7":"code","87bcaefc":"code","e1e6a1eb":"code","b06f0727":"code","96e3a3fa":"code","4a545d44":"code","4e4fc4d4":"code","3e68a3ac":"code","5010e8bf":"code","fc0e89e2":"code","661b82d0":"code","bc1a4832":"code","54fc44e7":"code","7d5121f5":"code","3f82cd49":"code","a7f47fa7":"code","605a5f8b":"markdown","e4828a73":"markdown","997a4c59":"markdown","add81327":"markdown"},"source":{"1b0b7ce1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fd6efdc":"#Importa as bibliotecas\nimport pandas as pd\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom imblearn.over_sampling import RandomOverSampler\nfrom xgboost import XGBClassifier","048188e8":"# Carregando os dados\ndf = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')\n\ndf.shape, test.shape","bca3de0a":"#Verifica nulos\ndf_nan = df.isna().sum()[df.isna().sum() >0]\ndf_nan","0cdb9bff":"#Fun\u00e7\u00e3o para imputar valores NA \ndef imput_na(df):\n    df['v2a1'] = df['v2a1'].fillna(df['v2a1'].median())\n    df['v18q1'] = df['v18q1'].fillna(-1)\n    df['rez_esc'] = df['rez_esc'].fillna(-1)\n    df['meaneduc'] = df['meaneduc'].fillna(df['meaneduc'].median())\n    df['SQBmeaned'] = df['SQBmeaned'].fillna(df['SQBmeaned'].median())\n    return df","75896557":"#Substitui yes e no por 1 e 0\n\ndef yes_no(df, column, tipo): \n    map_yes_no = {'yes': 1, 'no': 0}\n    df[column] = df[column].replace(map_yes_no).astype(tipo)\n    return(df)\n","4ab666c4":"#Imputa\ndf = imput_na(df)\n\n#Substitui yes e no\ndf = yes_no(df, 'edjefa', int)\ndf = yes_no(df, 'edjefe', int)\ndf = yes_no(df, 'dependency', float)","aa9aa705":"#Visualiza o tipo object\ndf.select_dtypes('object').head()","cbfd2765":"# Verificando quantidades e tipos\ndf.info()","f8674e84":"feats = list(df.select_dtypes(['float','int']).columns)\n","3b4f3bbd":"## Identificando vari\u00e1veis\n\ninvalida = []\nbinarios = []\ncategorica = []\nquantitativa = []\nfor feat in feats:\n    if df[feat].nunique() == 1:   \n        invalida.append(feat)\n    elif df[feat].nunique() == 2:  \n        binarios.append(feat)\n    elif df[feat].nunique() < 16: \n        categorica.append(feat)\n    else:\n        quantitativa.append(feat)         ","132c94a9":"## Identificando vari\u00e1veis\ninvalida = []\nbinarios = []\ncategorica = []\nquantitativa = []\nfor feat in feats:\n    if df[feat].nunique() == 1:   \n        invalida.append(feat)\n    elif df[feat].nunique() == 2:  \n        binarios.append(feat)\n    elif df[feat].nunique() < 16:  \n        categorica.append(feat)\n    else:\n        quantitativa.append(feat)         ","e2d70fb0":"## Histograma de vari\u00e1veis discretas\n\n\nax = df[quantitativa].hist(bins=10, grid=True, figsize=(10,20), rwidth=0.9)\nplt.suptitle(\"Histograma das vari\u00e1veis discretas\", fontsize = 10)\nplt.show();","554eb94b":"# Vari\u00e1vel Target\n#Agrupa por valor\ntarget = (\n    pd.DataFrame(df.groupby('Target').size())\n    .reset_index()\n    .rename(columns={0:'quantidade'})\n)\ntarget['Target'] = target.Target.astype(str)\n#Plota\nplt.figure(figsize=(8,5))\nplt.grid(linestyle=':', linewidth=0.7)\nplt.bar(target.Target,target.quantidade)\nplt.show();","6e8d44d7":"#Verifica os valores\ntarget['percent'] = round((target.quantidade\/target.quantidade.sum())*100)\ntarget","87bcaefc":"#Separa features em target e explicativas\nX = df.drop(['Id','Target', 'idhogar','elimbasu5'], axis=1) \ny = df['Target']\n#Separa em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","e1e6a1eb":"# Fazendo o over-sampling\nros = RandomOverSampler(random_state=42)\nX_ros,y_ros= ros.fit_resample(X_train,y_train)\n# Verificando o resultado\ny_ros.value_counts()","b06f0727":"\nxgbc = XGBClassifier(n_estimators=300, learning_rate=0.1, random_state=42)\n\nxgbc.fit(X_ros, y_ros);\n# Features\n#---------------------------------------------------------------\n#Prepara vizualiza\u00e7\u00e3o coeficientes xgbc\nxgbc_importance = (\n    pd.DataFrame(xgbc.feature_importances_, X_ros.columns)\n    .rename(columns={0:'valor'})\n    .sort_values('valor')\n    .reset_index()  \n)","96e3a3fa":"# Trabalhando com AdaBoost\n\n\nabc = AdaBoostClassifier(n_estimators=200, learning_rate=1.0, random_state=42)\nabc.fit(X_ros, y_ros);\n#-----------------------------------------------\nabc_importance = (\n    pd.DataFrame(abc.feature_importances_, X_ros.columns)\n    .rename(columns={0:'valor'})\n    .sort_values('valor')\n    .reset_index()  \n)\n","4a545d44":"# Treinando um modelo de RF Classifier\nrfc = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n\nrfc.fit(X_ros, y_ros);\n\n#Prepara vizualiza\u00e7\u00e3o coeficientes rfc\nrfc_importance = (\n    pd.DataFrame(rfc.feature_importances_, X_ros.columns)\n    .rename(columns={0:'valor'})\n    .sort_values('valor')\n    .reset_index()  \n)\n\n","4e4fc4d4":"#Visualiza acur\u00e1cia\npredict_xgbc = xgbc.predict(X_test)\naccuracy_xgbc = accuracy_score(y_test, predict_xgbc) * 100\n#----------------------------------------------------------\npredict_abc = abc.predict(X_test)\naccuracy_abc = accuracy_score(y_test, predict_abc) * 100\n#--------------------------------------------------------\npredict_rfc = rfc.predict(X_test)\naccuracy_rfc = accuracy_score(y_test, predict_rfc) * 100\n#--------------------------------------------------------\nprint('Acur\u00e1cia XGBClassifier:', accuracy_xgbc)\nprint('Acur\u00e1cia RFClassifier :', accuracy_rfc)\nprint('Acur\u00e1cia AdaBoostClassifier :', accuracy_abc)","3e68a3ac":"\n#Matriz de confus\u00e3o xgbc\nplot_confusion_matrix(xgbc,X_test,y_test);","5010e8bf":"#Matriz de confus\u00e3o com dados de teste\nplot_confusion_matrix(abc,X_test,y_test);","fc0e89e2":"#Matriz de confus\u00e3o com dados de teste\nplot_confusion_matrix(rfc,X_test,y_test);","661b82d0":"#Imputa\ntest = imput_na(test)\n##\n#Substitui yes e no\ntest = yes_no(test, 'edjefa', int)\ntest = yes_no(test, 'edjefe', int)\ntest = yes_no(test, 'dependency', float)","bc1a4832":"#Separa as features explicativas\nfeats_exp = test.drop(['Id','idhogar','elimbasu5'], axis=1) ","54fc44e7":"# Prever o Target de teste usando o modelo com maior acur\u00e1cia (rfc)\n#test['Target'] = rfc.predict(feats_exp).astype(int)","7d5121f5":"# Prever o Target de teste usando o modelo xgbc\n#test['Target'] = abc.predict(feats_exp).astype(int)\n","3f82cd49":"# Prever o Target de teste usando o modelo com maior acur\u00e1cia (rfc)\ntest['Target'] = xgbc.predict(feats_exp).astype(int)","a7f47fa7":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","605a5f8b":">O desbalanceamento de classe do dataset ocupa 63% da categoria 4 ","e4828a73":"# Treinando um modelo de RF Classifier\n\n","997a4c59":"> # Treinando um modelo de rXGBoost","add81327":"#  Treinando um modelo de AdaBoost\n"}}