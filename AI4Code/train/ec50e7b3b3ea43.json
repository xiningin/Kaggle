{"cell_type":{"1d15864a":"code","53d684bf":"code","283dece8":"code","9209ff51":"code","5d6d5b91":"code","c049792a":"code","f2af2bab":"code","99dd510d":"code","2a8ba8fa":"code","f571e714":"code","998475aa":"code","ed2c26b4":"code","b6ed90b4":"code","4ef463fa":"code","f589d37d":"code","e73530a3":"code","1bbd1da6":"code","1083fe37":"code","ee233ec0":"code","c3aee9c6":"code","0806ce17":"code","4b2b9489":"code","e27cbde5":"code","1ea65897":"markdown","cc9e4219":"markdown","7832c4cd":"markdown","daafa65c":"markdown","706f055a":"markdown","f77f037f":"markdown","3e21d5ab":"markdown","52119772":"markdown","72abecde":"markdown","8e477a63":"markdown"},"source":{"1d15864a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53d684bf":"df1 = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\ndf2 = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv')","283dece8":"df1.head()","9209ff51":"df1.info()","5d6d5b91":"df1=df1.rename(columns={\"class\": \"type\"})","c049792a":"df1.columns","f2af2bab":"df1.type.unique()","99dd510d":"# visualisation\n\nNormal = df1[df1[\"type\"] == 'Normal' ]\nAbnormal = df1[df1[\"type\"] == 'Abnormal' ]\n\nfor x in range(6):\n    for y in range(x+1,6):\n        plt.scatter(Normal.iloc[:,x], Normal.iloc[:,y], color='b', label='Normal', alpha=0.3)\n        plt.scatter(Abnormal.iloc[:,x], Abnormal.iloc[:,y], color='r', label='Abnormal', alpha=0.3)\n        plt.legend()\n        plt.xlabel(df1.columns[x])\n        plt.ylabel(df1.columns[y])\n        plt.show()","2a8ba8fa":"y=df1.type.values\nx=df1.drop(\"type\", axis=1)\n\n# normalization\nx=(x-x.min())\/(x.max()-x.min())\n","f571e714":"x.describe()","998475aa":"#train test split\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)","ed2c26b4":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model=KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train, y_train)\nprint('Score :', knn_model.score(x_test,y_test))","b6ed90b4":"score_list=[]\nfor each in range (1,50):\n    knn_model=KNeighborsClassifier(n_neighbors=each)\n    knn_model.fit(x_train,y_train)\n    score_list.append(knn_model.score(x_test,y_test))\n\nplt.plot(range(1,50),score_list)\nplt.xlabel('k values')\nplt.ylabel('Accuracy')\nplt.show()","4ef463fa":"print(score_list[20:30])","f589d37d":"df2=df2.rename(columns={\"class\": \"type\"})","e73530a3":"df2.type.unique()","1bbd1da6":"y=df2.type.values\nx=df2.drop(\"type\", axis=1)","1083fe37":"# Normalization\n\nx=(x-x.min())\/(x.max()-x.min())","ee233ec0":"x.describe()","c3aee9c6":"# train-test split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)","0806ce17":"# Applying KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train,y_train)\nprint(knn.score(x_test,y_test))","4b2b9489":"score_list=[]\nfor each in range(1,50):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    score_list.append(knn.score(x_test,y_test))\n\nplt.plot(range(1,50), score_list)\nplt.xlabel('k values')\nplt.ylabel('Accuracy')\nplt.show()","e27cbde5":"knn=KNeighborsClassifier(n_neighbors=12)\nknn.fit(x_train,y_train)\nknn.score(x_test,y_test)","1ea65897":"75% accuracy seems like a bit low. Let's optimize the number of neighbors to increase accuracy.","cc9e4219":"Let's optimize the number of neighbors.","7832c4cd":"Let's generate our train and test datasets.","daafa65c":"Now let's use KNN algorithm on the data and see how well it performs.","706f055a":"**KNN With Sklearn**\n\nOur data consists information of 310 patients, of whom 100 are normal, 60 have Disk Hernia and 150 have Spondylolisthesis. There are 6 variables to represent each patient. Dataset consists 2 files, first one has 3 categories I have written above, and the second file has 2 categories which are Normal and Abnormal(sum of Disk Hernia and Spondylolisthesis).\n\n\nWe will try to fit a KNN algorithm to the data in order to predict to which category a given patient belongs. We will work on each file separately.","f77f037f":"Some plots suggest that KNN can be used on this dataset. Let's do it.\n\nWe should normalize the data first, in order to prevent large numbers to outnumber smaller ones.","3e21d5ab":"We have the highest scores when number of neighbors are 22 and 26, which is 81.72%.","52119772":"We have the highest score when k=12, which is 76.34%.","72abecde":"# KNN with the 2-Categories Dataset","8e477a63":"# KNN with the 3-Categories Dataset"}}