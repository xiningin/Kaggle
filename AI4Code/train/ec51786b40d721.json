{"cell_type":{"34a57a92":"code","b4062e5f":"code","603c5b31":"code","153f3e43":"code","5953f244":"code","8d246b1e":"code","0e6b3650":"code","c93c61f0":"code","95656677":"code","065bba9b":"code","7481355c":"code","bd27129a":"code","1dc98860":"code","132e2672":"code","32ecc274":"code","cfb5cb72":"code","664fa1dc":"code","415bd5d6":"code","cbd32692":"code","5b6863bb":"code","ddcc29d6":"code","00ae42be":"code","7e12e226":"code","dec8a6cc":"code","764dcb93":"code","43c203d2":"code","80c27d45":"code","be64e788":"code","20d2b121":"code","54f27068":"code","16d4eef7":"code","0c4f8c4c":"code","95150a96":"code","5bf195d5":"markdown","49368ad8":"markdown","3a45a254":"markdown","49b55432":"markdown","b6925962":"markdown","ebdb9280":"markdown"},"source":{"34a57a92":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n","b4062e5f":"data = pd.read_csv(\"..\/input\/adult.csv\")","603c5b31":"data.head()","153f3e43":"data.shape","5953f244":"data.describe()","8d246b1e":"data.info()","0e6b3650":"columns = data.columns\ncolumns","c93c61f0":"# lets first filter the bad data elements to plot the data and get some insights\nlabels = data.income\nnewData = data.drop(labels = ['capital.gain', 'capital.loss', 'income'], axis = 1, inplace = False)","95656677":"newData","065bba9b":"x= sns.PairGrid(newData, hue='relationship')\n# x=x.map(plt.scatter)\nx= x.map_diag(plt.hist)\nx= x.map_offdiag(plt.scatter)\n# plt.figure(figsize=(20,50))\nx= x.add_legend()","7481355c":"sns.barplot('sex', 'education.num', data = newData)","bd27129a":"thisArray = np.unique(newData['hours.per.week'].values)\nsns.barplot('sex', 'hours.per.week', data = newData[:20], hue = 'hours.per.week')\n\n# np.unique(newData['hours.per.week'].values)","1dc98860":"sns.countplot(labels)\n","132e2672":"columns = data.columns\ncolumns","32ecc274":"newData.describe()","cfb5cb72":"attrib, counts = np.unique(newData['workclass'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\nnewData['workclass'][newData['workclass'] == '?'] = most_freq_attrib \n\nattrib, counts = np.unique(newData['occupation'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\nnewData['occupation'][newData['occupation'] == '?'] = most_freq_attrib \n\nattrib, counts = np.unique(newData['native.country'], return_counts = True)\nmost_freq_attrib = attrib[np.argmax(counts, axis = 0)]\nnewData['native.country'][newData['native.country'] == '?'] = most_freq_attrib \n","664fa1dc":"newData.head(20)\n# Just a check on whether above code works fine.","415bd5d6":"# A lot of features needed to be encoded. Labels such as workplace, relationship, sex, marital.status,\n# native.country has no order. Order can be naturally defined, but that would be personally biases.\n# In fact, that won't scale for new countries or fresh data.\n","cbd32692":"le1 = LabelEncoder()\n\nX = newData.apply(LabelEncoder().fit_transform) \nY = le1.fit_transform(np.array(labels))","5b6863bb":"Y.shape, X.shape","ddcc29d6":"X.head(3)","00ae42be":"\nX_encoded = pd.get_dummies(X,columns =[\"workclass\",\"education\",\"marital.status\", \"occupation\", \"relationship\",\n                                         \"race\", \"sex\",\"native.country\"], drop_first=True )","7e12e226":"Y.shape, X_encoded.shape","dec8a6cc":"from sklearn.model_selection import train_test_split as tts\nxtrain,xdev,ytrain,ydev = tts(X,Y, random_state = 2, shuffle = True, train_size = 0.7)\nxtest,x_dev, ytest,y_dev = tts(xdev,ydev, random_state =2 , shuffle = True, train_size = 0.5)\n","764dcb93":"xtrain.shape, xtest.shape, x_dev.shape, ytrain.shape, ytest.shape, y_dev.shape, ","43c203d2":"# We have the data and the dev set as well as the test set, let us try different algorithms","80c27d45":"from sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier as ETC\nfrom sklearn.ensemble import AdaBoostClassifier as ABC\nfrom sklearn.ensemble import BaggingClassifier as BC\n","be64e788":"clf = GBC(n_estimators = 100, max_depth=5) # no overfit, ~85% accuracy\nclf.fit(xtrain,ytrain)\nscore_dev = clf.score(x_dev, y_dev)\nscore_test = clf.score(xtest, ytest)\nscore_train = clf.score(xtrain,ytrain)\nprint(score_train,score_dev,score_test)","20d2b121":"# clf = SVC()\n# clf.fit(xtrain,ytrain)\n# score_dev = clf.score(x_dev, y_dev)\n# score_test = clf.score(xtest, ytest)\n# score_train = clf.score(xtrain,ytrain)\n# print(score_train,score_dev,score_test)\n\n# 0.9984204984204984 0.7568065506653019 0.7592137592137592\n# Takes a lot of time to run","54f27068":"clf = ETC(n_estimators = 50, criterion=\"entropy\", max_depth = 12)\nclf.fit(xtrain,ytrain)\nscore_dev = clf.score(x_dev, y_dev)\nscore_test = clf.score(xtest, ytest)\nscore_train = clf.score(xtrain,ytrain)\nprint(score_train,score_dev,score_test)","16d4eef7":"clf = ABC(n_estimators = 200, learning_rate = 1)\nclf.fit(xtrain,ytrain)\nscore_dev = clf.score(x_dev, y_dev)\nscore_test = clf.score(xtest, ytest)\nscore_train = clf.score(xtrain,ytrain)\nprint(score_train,score_dev,score_test)","0c4f8c4c":"clf = DTC(max_depth = 7)\nclf.fit(xtrain,ytrain)\nscore_dev = clf.score(x_dev, y_dev)\nscore_test = clf.score(xtest, ytest)\nscore_train = clf.score(xtrain,ytrain)\nprint(score_train,score_dev,score_test)","95150a96":"clf = BC(n_estimators = 50, n_jobs = 2)\nclf.fit(xtrain,ytrain)\nscore_dev = clf.score(x_dev, y_dev)\nscore_test = clf.score(xtest, ytest)\nscore_train = clf.score(xtrain,ytrain)\nprint(score_train,score_dev,score_test)","5bf195d5":"Step 1\n- Considering the columns","49368ad8":"## Data preprocessing Steps\n","3a45a254":"## Fixing the common nan values\n\n- Nan values were as ? in data. Hence we fix this with most frequent element in the entire dataset. It generalizes well, as we will see with the accuracy of our classifiers","49b55432":"- Various classifiers were explored. This is clear that classifiers have less to do with the accuracy and more to do with the data presented. Proper presentation of data  is absolute must\n### End","b6925962":"## The distribution for income","ebdb9280":"### Exploratory analysis of data"}}