{"cell_type":{"d9a43f06":"code","0af68996":"code","b116f00a":"code","741c2be7":"code","23657098":"code","579dd5f0":"code","331d4531":"code","fc8c497c":"code","62c0b660":"code","342fc7e7":"code","cb465092":"code","d21d5df4":"code","8767af2b":"code","85a76ba9":"code","18738da7":"code","b998c90f":"code","0ab3c218":"code","dc3301fe":"code","4c6c38d2":"code","42dea47c":"code","476cfac6":"code","8214b624":"code","f34d3e85":"code","79803052":"code","bd866174":"code","348dcb7b":"code","f9836a55":"code","9fe2d3f1":"code","a97813b9":"code","2667f557":"code","5aaa935f":"code","165b6434":"code","eaef5c4f":"code","e3b5c71b":"code","30ce66de":"code","1e614652":"code","ead0d197":"code","6e60daf8":"code","71646c44":"code","eae5e697":"code","4d9dec9e":"code","966ba020":"code","ce1ff202":"code","0ca07577":"code","3b7b1ecd":"code","714f9f9e":"code","768f4e3c":"code","b3d038e8":"code","fa110318":"code","221e24cd":"code","2fb1a556":"code","9aea2a7b":"code","76486085":"code","fac1b641":"markdown","99794555":"markdown","64009b6f":"markdown","1b0aec0b":"markdown"},"source":{"d9a43f06":"import pandas as pd\nimport numpy as np\n#import nltk\n#from nltk.corpus import stopwords\n#from nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation\n\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence \n","0af68996":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","b116f00a":"# Add the string 'empty' to empty strings\ntrain = train.fillna('empty')\ntest = test.fillna('empty')","741c2be7":"# Preview some of the pairs of questions\n\nfor i in range(10):\n    print(train.question1[i])\n    print(train.question2[i])\n","23657098":"!python -m spacy download en","579dd5f0":"\nre_br = re.compile(r'<\\s*br\\s*\/?>', re.IGNORECASE)\ndef sub_br(x): return re_br.sub(\"\\n\", x)\n\n#nlp = spacy.load(\"en\")\nnlp = spacy.load('en_core_web_sm')\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\ndef clean_text(text):\n    ''' Pre process and convert texts to a list of words '''\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    #text = text.split()\n\n    return text\n\nmy_tok = spacy.load('en')\ndef spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(clean_text(x))]\n\ndef remove_stop_words(tokens): return [tok for tok in tokens if tok not in spacy_stopwords]","331d4531":"sent = \"Motorola (company): Can I hack my Charter Motorolla DCX3400?\"","fc8c497c":"clean_sent = clean_text(sent)","62c0b660":"tokens = spacy_tok(clean_sent)\ntokens","342fc7e7":"remove_stop_words(tokens)","cb465092":"combined = pd.concat([train,test])","d21d5df4":"print(train.shape)\nprint(test.shape)\nprint(combined.shape)","8767af2b":"combined.head()","85a76ba9":"counts = Counter()\nquestions = ['question1', 'question2']\nfor question in questions:\n    for sent in train[question]:\n        try:\n            counts.update(remove_stop_words(spacy_tok(sent)))\n        except:\n            pass","18738da7":"'''counts = Counter()\nquestions = ['question1', 'question2']\nfor question in questions:\n    for sent in train[question][:10]:\n        counts.update(spacy_tok(sent))'''","b998c90f":"counts","0ab3c218":"len(counts.keys())","dc3301fe":"# Vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","4c6c38d2":"len(words)","42dea47c":"# WHat is the 99% quantile of  length of the sentence?\n\ncombined['len_q1'] = combined['question1'].apply(lambda x: len(x.split()))\ncombined['len_q2'] = combined['question2'].apply(lambda x: len(x.split()))","476cfac6":"combined['len_q1'].quantile(q = 0.99)","8214b624":"combined['len_q2'].quantile(q = 0.99)","f34d3e85":"# note that spacy_tok takes a while run it just once\ndef encode_sentence(sent, vocab2index, N=30, padding_start=True):\n    \n    x = remove_stop_words(spacy_tok(sent))\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc","79803052":"# Encoding questions in the train dataset\ntrain['enc_question1'] = train['question1'].apply(lambda x: encode_sentence(x,vocab2index, N=40, padding_start=True) )\ntrain['enc_question2'] = train['question2'].apply(lambda x: encode_sentence(x,vocab2index, N=40, padding_start=True) )\n","bd866174":"# Encoding questions in the test datset\ntest['enc_question1'] = test['question1'].apply(lambda x: encode_sentence(x,vocab2index, N=40, padding_start=True) )\ntest['enc_question2'] = test['question2'].apply(lambda x: encode_sentence(x,vocab2index, N=40, padding_start=True) )\n","348dcb7b":"from sklearn.model_selection import train_test_split","f9836a55":"VALID_IDX = 40000\nN = len(train)\nX_train = train[['enc_question1','enc_question2']][:(N-VALID_IDX)]\ny_train= train['is_duplicate'][:(N-VALID_IDX)].values\nX_valid = train[['enc_question1','enc_question2']][(N-VALID_IDX):]\ny_valid= train['is_duplicate'][(N-VALID_IDX):].values","9fe2d3f1":"X_train.reset_index(inplace=True, drop=True)\nX_valid.reset_index(inplace=True, drop=True)","a97813b9":"len(y_train)","2667f557":"len(X_train)","5aaa935f":"len(X_valid)","165b6434":"len(y_valid)","eaef5c4f":"X_valid.reset_index(inplace=True, drop=True)\nX_valid.head()","e3b5c71b":"class QuoraDataset(Dataset):\n    def __init__(self, df,y, N=40, padding_start=True):\n        self.df = df\n        self.y = y\n        self.x1 = df['enc_question1']\n        self.x2 = df['enc_question2']\n        # pos 1, neg 0\n    \n       # self.x1 = [encode_sentence(sent, vocab2index, N, padding_start) for sent in self.q1]\n       # self.x2 = [encode_sentence(sent, vocab2index, N, padding_start) for sent in self.q2]\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        x1 = self.x1[idx]\n        x2 = self.x2[idx]\n        return x1, x2, self.y[idx]","30ce66de":"train_ds_v0 = QuoraDataset(X_train,y_train, padding_start=False)\nvalid_ds_v0 = QuoraDataset(X_valid,y_valid, padding_start=False)","1e614652":"len(y_valid)","ead0d197":"train_ds_v0[364289]","6e60daf8":"valid_ds_v0[15468]","71646c44":"y_valid[0]","eae5e697":"batch_size = 32\ntrain_dl_v0 = DataLoader(train_ds_v0, batch_size=batch_size, shuffle=True)\nvalid_dl_v0 = DataLoader(valid_ds_v0, batch_size=batch_size)","4d9dec9e":"x1, x2,y = next(iter(train_dl_v0))","966ba020":"x1","ce1ff202":"class LSTMV0Model(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super(LSTMV0Model,self).__init__()\n        self.hidden_dim = hidden_dim\n        # Layers\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x1, x2, batch_size):\n        x1 = self.embeddings(x1)\n        x1 = self.dropout(x1)\n        \n        x2 = self.embeddings(x2)\n        x2 = self.dropout(x2)\n        self.batch_size = batch_size\n        \n        out_pack1, (ht1, ct1) = self.lstm(x1)\n        out_pack2, (ht2, ct2) = self.lstm(x2)\n        \n        # Distance\n        if self.batch_size == 1:\n            prediction = torch.exp(-torch.norm((ht1[-1].squeeze() - ht2[-1].squeeze()), 1))\n        else:\n            prediction = torch.exp(-torch.norm((ht1[-1].squeeze() - ht2[-1].squeeze()), 1, 1))\n        #print(prediction.unsqueeze(-1).size())\n        return prediction.unsqueeze(-1)","0ca07577":"def train_epocs_v0(model, batch_size, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x1, x2, y in train_dl:\n            # s is not used in this model\n            x1 = x1.long().cuda()\n            x2 = x2.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(x1, x2, batch_size)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics_v0(model, val_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss\/total, val_loss, val_acc))","3b7b1ecd":"def val_metrics_v0(model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x1, x2, y in valid_dl:\n        # s is not used here\n        x1 = x1.long().cuda()\n        x2 = x2.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(x1,x2, batch_size)\n        loss = F.binary_cross_entropy(y_hat, y)\n        y_pred = y_hat > 0.5\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss\/total, correct\/total","714f9f9e":"batch_size = 5000\ntrain_dl = DataLoader(train_ds_v0, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds_v0, batch_size=batch_size)","768f4e3c":"vocab_size = len(words)\nprint(vocab_size)\nmodel_v0 = LSTMV0Model(vocab_size, 50, 50).cuda()","b3d038e8":"train_epocs_v0(model_v0,batch_size = batch_size, epochs=50, lr=0.001)","fa110318":"test_dl = ","221e24cd":"### Make prediction","2fb1a556":"subm = pd.read_csv(\"..\/input\/sample_submission.csv\")","9aea2a7b":"#test = pd.read_csv('..\/input\/test.csv')","76486085":"sample_sub = pd.read_csv('..\/input\/sample_submission.csv')\ntest_ds = QuoraDataset(test, sample_sub.is_duplicate)\nbatch_size = 5000\ntest_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\ny1 = []\nfor x1, x2, y in test_dl:\n    x1 = x1.long().cuda()\n    x2 = x2.long().cuda()\n    y = y.float().cuda().unsqueeze(1)\n    y_hat = model_v0(x1,x2, batch_size)\n    y1.append([0 if x<=0.5 else 1 for x in y_hat])\ny_pred = [yi for sublist in y1 for yi in sublist]\nsample_sub.is_duplicate = y_pred\nsample_sub.to_csv(\"submit.csv\", index=False)","fac1b641":"### Data pre-processing\n","99794555":"### MODEL V0\nsimple model: turn question into sequence of GloVe embeddings, pass them through LSTM embedding layer to get question representation as vector (I used 100-dimensional vectors), merge two vectors into one and pull it through 2 fully connected layers and softmax. No manual feature engineering and model architecture thinking.","64009b6f":"### Building a vocabualry\n\nTo do so, we will first concatenate train and test and get the vocabulary from both sets","1b0aec0b":"###  Splitting into train and validation"}}