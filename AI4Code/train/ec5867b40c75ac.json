{"cell_type":{"df05baed":"code","6f2fe025":"code","80d07932":"code","ac318202":"code","a818be78":"code","f9637925":"code","49e6ef9e":"code","d99b8e5d":"code","2bbd941f":"code","29c25342":"code","887fec04":"code","9511e6e5":"code","e1aaf491":"markdown","58562846":"markdown","721416e4":"markdown","1f4b5ea9":"markdown","fd90eec3":"markdown","e725f0f0":"markdown","ce311b70":"markdown","a4a3923a":"markdown","9035a1f7":"markdown","e3e2b449":"markdown","b871c0de":"markdown"},"source":{"df05baed":"import numpy as np\n\nnorm_with_exponents = lambda data: np.exp(data) \/ np.sum(np.exp(data))\nnorm_by_sum = lambda data: data \/ np.sum(data)\n\ndata = np.array([10, 20, 35])\nprint(\"data\\t\\t\", data)\nprint(\"norm w\/ exp\\t\", norm_with_exponents(data))\nprint(\"norm by sum\\t\",norm_by_sum(data))\n\nprint(\"\\nReadable notation\")\nfor exp_notation in norm_with_exponents(data):\n    print(f\"{exp_notation}\\t= {exp_notation:.13f}\")\n\ndata = np.array([1, 2, 3.5])\nprint(\"\\n\\n\\ndata\\t\\t\", data)\nprint(\"norm w\/ exp\\t\", norm_with_exponents(data))\nprint(\"norm by sum\\t\",norm_by_sum(data))","6f2fe025":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(123)","80d07932":"# gen uniform rvs data of 100 samples\ndata = np.random.uniform(100,200,100)","ac318202":"sns.distplot(data)\n\nplt.title(\"data\")\nplt.xlabel(\"data\")\nplt.ylabel(\"counts\")\nplt.show()","a818be78":"data_norm = (data - data.min()) \/ (data.max() - data.min())","f9637925":"data_std = (data - data.mean()) \/ (data.std())","49e6ef9e":"data_div_by_sum = data \/ data.sum()","d99b8e5d":"data_norm_w_exponents = np.exp(data) \/ np.sum(np.exp(data))","2bbd941f":"fig, axarr = plt.subplots(1, 4)\nfig.set_size_inches(15,4)\n\nsns.distplot(data_norm, kde=False, ax=axarr[0])\naxarr[0].title.set_text(\"data_norm\")\naxarr[0].set_xlabel(\"data_norm\")\naxarr[0].set_ylabel(\"counts\")\n\nsns.distplot(data_std, ax=axarr[1])\naxarr[1].title.set_text(\"data_std\")\naxarr[1].set_xlabel(\"data_std\")\naxarr[1].set_ylabel(\"counts\")\n\nsns.distplot(data_div_by_sum, ax=axarr[2])\naxarr[2].title.set_text(\"data_div_by_sum\")\naxarr[2].set_xlabel(\"data_div_by_sum\")\naxarr[2].set_ylabel(\"counts\")\n\nsns.distplot(data_norm_w_exponents, ax=axarr[3])\naxarr[3].title.set_text(\"data_norm_w_exponents\")\naxarr[3].set_xlabel(\"Note: because of exp scaling\\nn_high_conf << n_low_conf\")\naxarr[3].set_ylabel(\"counts\")\n\nplt.show()\n\n\nplt.figure(figsize=(15,7))\nplt.hist(data_norm, fc=(0, 1, 0, 0.5))\nplt.hist(data_std, fc=(0, 0, 1, 0.5))\nplt.hist(data_div_by_sum, fc=(1, 0, 0, 1))\nplt.hist(data_norm_w_exponents, fc=(0, 0, 0, 0.5))\n\nplt.ylabel(\"counts\")\nplt.xlabel(\"feature\")\nplt.legend([\"data_norm\", \"data_std\", \"data_div_by_sum\", \"Normalisation by exponents\"])\nplt.grid()\n\nplt.show()","29c25342":"errors1 = np.random.normal(0,5, 100) \nerrors2 = np.random.normal(0,5, 100) \nfeat1 = np.arange(100, 200, 1) + errors1\nfeat2 = np.arange(500, 400, -1) + errors2\n\nplt.scatter(feat1, feat2)\nplt.xlabel(\"feat1\")\nplt.ylabel(\"feat2\")\nplt.title(\"2 feats are used as axes\")\nplt.show()","887fec04":"# normalisation\nfeat1_norm = (feat1 - feat1.min()) \/ (feat1.max() - feat1.min())\nfeat2_norm = (feat2 - feat2.min()) \/ (feat2.max() - feat2.min())\n\n# standardistion\nfeat1_std = (feat1 - feat1.mean()) \/ (feat1.std())\nfeat2_std = (feat2 - feat2.mean()) \/ (feat2.std())\n\n# div by sum\nfeat1_div_by_sum = (feat1) \/ (feat1.sum())\nfeat2_div_by_sum = (feat2) \/ (feat2.sum())","9511e6e5":"fig, axarr = plt.subplots(1, 3)\nfig.set_size_inches(18,4)\n\nsns.scatterplot(feat1_norm, feat2_norm, ax=axarr[0])\naxarr[0].title.set_text(\"data_norm\")\naxarr[0].set_xlabel(\"feat1_norm\")\naxarr[0].set_ylabel(\"feat2_norm\")\n\nsns.scatterplot(feat1_std, feat2_std, ax=axarr[1])\naxarr[1].title.set_text(\"data_std\")\naxarr[1].set_xlabel(\"feat1_std\")\naxarr[1].set_ylabel(\"feat2_std\")\n\nsns.scatterplot(feat1_div_by_sum, feat2_div_by_sum, ax=axarr[2])\naxarr[2].title.set_text(\"data_div_by_sum\")\naxarr[2].set_xlabel(\"feat1_div_by_sum\")\naxarr[2].set_ylabel(\"feat1_div_by_sum\")\n\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.scatter(feat1_norm, feat2_norm, fc=(0, 1, 0, 0.5))\nplt.scatter(feat1_std, feat2_std, fc=(0, 0, 1, 0.5))\nplt.scatter(feat1_div_by_sum, feat2_div_by_sum, fc=(1, 0, 0, 0.5))\n\nplt.ylabel('feat_2')\nplt.xlabel('feat_1')\nplt.legend([\"feat1_norm, feat2_norm\", \"feat1_std, feat2_std\", \"feat1_div_by_sum, feat2_div_by_sum\"])\nplt.title(\"Note Scales\")\nplt.grid()\nplt.show()","e1aaf491":"[Noramlisation Based On Scales](http:\/\/www.analytictech.com\/ba762\/handouts\/normalization.htm)","58562846":"# Types Covered\n\n- **01. Normalisation a.k.a Min-Max-Sacaling** \n- **02. Standardisation**\n- **03. Normalisation BY Sum**\n- **04. Normalisation WITH exponents a.k.a Softmax**","721416e4":"## Visualize","1f4b5ea9":"**D. Normalisation WITH Exponents**","fd90eec3":"**C. Division by Sum (Normalisation BY Sum)**","e725f0f0":"**A. Normalisation**","ce311b70":"**B. Standardisation**","a4a3923a":"> - Note **scales**\n> - `data_div_by_sum` has perfect PDF(not histogram) in range [0,1]","9035a1f7":"## 01. Normalisation a.k.a Min-Max-Scale\n\n> Squash all values between $[0, 1]$ keeping relative differences same.\n\n$$Transform(X) = \\frac{x_i - min}{max-min}$$\n\n## 02. Standardisation\n\n> Convert to std. normal distribution $N(\\mu=0, \\sigma=1)$. Centered by 0 and scales squashed by measure of variance\n\n$$Transform(X) = \\frac{x_i - \\mu}{\\sigma} \\sim N(0, 1)$$\n\n## 03. Division **BY** Sum\n\n> Squashed between $[0, 1]$ w\/ additional property such that, sum is  1\n\n$$Transform(X) = \\frac{x_i}{sum(X)}$$\nwhere $x_i \\in R^{+}$ \nIt generates transformed PDF in first quadrant in range [0,1]\n\n**Note:** For division by sum, all positive input data is preferred\n\n## 04. Normalisation **WITH** Exponents a.k.a Softmax\n\n> Squashed between each value [0,1] so that **One(or few) of the values is always extremely large.** *Extremity of the value depends upont it's value as welll as other values* \n\n$$Transform(X) = \\frac{e^{x_i}}{sum({e^{x_i}})}$$\n\n> $$ \\text{Skewness} \\propto ||X||_p $$\n\nConverts to log-normal distribution. **Higher the skewness, better** (@ output layer)\n\n**Note:** Dont think as inverse of log transform\n![image.png](attachment:image.png)\n[Zoom in](https:\/\/viewer.diagrams.net\/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=norm_w_exps_aka_softmax.drawio#Uhttps%3A%2F%2Fraw.githubusercontent.com%2Frakesh4real%2FApplied-Machine-Learning%2Fmaster%2Fnorm_w_exps_aka_softmax.drawio)","e3e2b449":"> - Normalisation -- squashes to positive quadrant\n> - Division by Sum -- squashes to positive quadrant (smaller scale)\n> - Standardization -- squashes to origin centered data\n","b871c0de":"## Better visualisation in 2-D \n\n> Two features are processed by same technique (normalized \/ standardized \/ divided by sum)"}}