{"cell_type":{"70adb257":"code","a9e4220c":"code","0c673fa3":"code","b14933b4":"code","31fd4c05":"code","1eb72c33":"code","2b11d302":"code","4d879d37":"code","ea9fe001":"code","b78ee616":"code","21cd60aa":"code","a8837bd2":"code","e7b81067":"code","46a2b840":"code","1dc8e4de":"code","76cf203a":"code","8bfd878a":"code","70b4284d":"code","992897bb":"code","483293e2":"code","df09a5e6":"code","ea22af28":"code","4a901d2a":"code","2c8bfb18":"code","65ecf3c9":"code","f51010b9":"code","c48879cf":"code","7d572876":"code","bac152cf":"code","cb099ca3":"code","83081bc6":"code","67e9e125":"code","e2b71584":"code","c0f49ebc":"code","c7d1d19a":"code","71990d7f":"markdown","a20de8e6":"markdown","443eecde":"markdown","c92a0427":"markdown","9c835bd2":"markdown","876c56bf":"markdown","89c19114":"markdown","afb55538":"markdown","1fc67315":"markdown","6ab4b824":"markdown","88894667":"markdown","5e55d8ff":"markdown","c35760f5":"markdown","9640f58f":"markdown","611fdeb9":"markdown","94ae727d":"markdown","e88db5af":"markdown","a918f94f":"markdown","15401950":"markdown","fd7d3c28":"markdown","0929f660":"markdown","74a365ce":"markdown","c7df9e17":"markdown","b5c4175c":"markdown","509972d3":"markdown","7eacf36c":"markdown","49f08a0c":"markdown","4c7e3a1a":"markdown","52273503":"markdown","a538a222":"markdown","ba33f237":"markdown","93718763":"markdown","981b2165":"markdown","0977ed87":"markdown","b453ee8c":"markdown","330e5a67":"markdown","9c815c34":"markdown","892648ad":"markdown","cb1424f6":"markdown","0c85d729":"markdown","7195288c":"markdown","38e2b662":"markdown","69d54da2":"markdown","ad16351a":"markdown","996f7133":"markdown","f1ce1023":"markdown","db3f95fa":"markdown","24de9bad":"markdown","cb078283":"markdown","3bccb350":"markdown","de83215f":"markdown","5cb6db90":"markdown","957b2770":"markdown","22da513e":"markdown","3487d90d":"markdown","9bf8bba2":"markdown","302331b2":"markdown","2df4958f":"markdown"},"source":{"70adb257":"# basics\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\n\n# string\nimport string\nimport unidecode\nimport re\nfrom textwrap import wrap # wrapping long text into lines\n\n# plot\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport geopandas as gpd\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n%matplotlib inline\n\n# text mining\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom stop_words import get_stop_words\n\n# Because we have some long strings to deal with:\npd.options.display.max_colwidth = 300","a9e4220c":"os.listdir('..\/input\/granddebat')","0c673fa3":"themes = {\n    'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv':'La fiscalit\u00e9 et les d\u00e9penses publiques',\n    'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv':\"Organisation de l'\u00e9tat et des services publics\",\n    'DEMOCRATIE_ET_CITOYENNETE.csv':'D\u00e9mocratie et citoyennet\u00e9',\n    'LA_TRANSITION_ECOLOGIQUE.csv':'La transition \u00e9cologique'\n}\n\nfilenames = list(themes.keys())\nthemes = list(themes.values())","b14933b4":"filepaths = [os.path.join(\"..\", \"input\", \"granddebat\", filename) for filename in filenames]\ncol_date = ['createdAt', 'publishedAt', 'updatedAt']\ndf_list = [pd.read_csv(filepath, low_memory=False,\n                       dtype={'authorZipCode':'str'},\n                       parse_dates=col_date) for filepath in filepaths]","31fd4c05":"col_common = set.intersection(*[set(df.columns) for df in df_list])\ncol_common","1eb72c33":"pd.concat([df[df.columns.intersection(col_common)] for df in df_list]).isnull().mean() * 100","2b11d302":"df_infos = pd.DataFrame({\n     'theme': themes,\n     'nb_contributions': [df.shape[0] for df in df_list],\n     'nb_questions': [sum(~df.columns.isin(col_common)) for df in df_list]\n    })\ndf_infos","4d879d37":"# Daily contributions\nday_contrib = pd.concat([df.createdAt for df in df_list]).dt.date.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nday_contrib.plot()\nax.set_title('Daily contributions')\nax.set_xlabel('Date')\nfig.autofmt_xdate()\nax.set_ylim(bottom=0)\nplt.show(fig)","ea9fe001":"# Hourly contributions\nhour_contrib = pd.concat([df.createdAt for df in df_list]).dt.hour.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nhour_contrib.plot()\nax.set_title('Hourly contributions')\nax.set_xlabel('Hour')\nax.set_ylim(bottom=0)\nplt.show(fig)","b78ee616":"# Maximal number of contributions per author per theme:\npd.DataFrame({'theme':themes,\n              'max_contrib_per_author':[df.groupby('authorId').size().max() for df in df_list]})","21cd60aa":"def mode_na(x): \n    m = pd.Series.mode(x)\n    return m.values[0] if not m.empty else np.nan\n\nauthors = pd.concat([df[df.columns.intersection(col_common)] for df in df_list])\n# With pandas>=0.24, we would use: pandas.Series.mode\nauthors = authors.groupby('authorId').agg({'id':'count', # number of contributions\n                                           'authorType':mode_na,\n                                           'authorZipCode':mode_na})","a8837bd2":"authors.shape[0]","e7b81067":"n_contrib = authors.id.value_counts().reset_index(name='counts')\nn_contrib.loc[n_contrib['index'] > 4, 'index'] = '>4'\nn_contrib = n_contrib.groupby('index').agg(sum)\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.barplot(x='index',\n            y='counts',\n            data=n_contrib.reset_index(),\n            palette=sns.color_palette('Blues'))\nax.set_xlabel('Number of contributions')\nax.set_title('Authors per number of contributions')\nplt.show()","46a2b840":"fig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='authorType',\n                   data=authors,\n                   palette=sns.color_palette('Blues'))\nax.set_yscale('log')\nax.set_title('Author types')\nplt.show()","1dc8e4de":"authors.authorZipCode.isnull().sum()","76cf203a":"authors.authorZipCode.str.len().value_counts()","8bfd878a":"authors.loc[authors.authorZipCode.str.len() != 5, 'authorZipCode'] = np.nan","70b4284d":"authors.authorZipCode[49789]","992897bb":"# French variant of csv:\ncommunes_fr = pd.read_table('..\/input\/communes-francaises\/code-postal-code-insee-2015.csv',\n                            encoding = 'utf-8', delimiter=\";\", dtype ={'Code_postal':'str'})\n\n# Population per department (population is at INSEE_COM scale, wich is the commune)\npopulation_dep = communes_fr[['INSEE_COM', 'CODE_DEPT', 'POPULATION']].drop_duplicates().\\\ngroupby('CODE_DEPT').sum()\n\n# Link zipcode\/department\nzc_dep = communes_fr[['Code_postal', 'CODE_DEPT']].drop_duplicates()\n\n# Authors per zipcode\nauthors_zc = authors.assign(code_postal=authors.authorZipCode.str.slice(0, 5)).\\\ngroupby('code_postal').size().reset_index(name='counts')\n\n# Authors per department\nauthors_dep = zc_dep.set_index('Code_postal').join(authors_zc.set_index('code_postal')).\\\ngroupby('CODE_DEPT').sum()\n# Adding population\nrate_dep = authors_dep.join(population_dep)\n# Computing rate\nrate_dep['author_rate'] = rate_dep.counts\/rate_dep.POPULATION * 1000 # per 1000","483293e2":"print(rate_dep.POPULATION.sum())\nprint(rate_dep.counts.sum())","df09a5e6":"map_dep = gpd.read_file('..\/input\/contours-des-departements-francais\/departements-20180101.shp',\n                        encoding = 'utf-8')\nmap_dep = map_dep[['code_insee', 'geometry']]\n\n# Mainland only\nmap_dep = map_dep[~map_dep.code_insee.isin(['971', '972','973','974','976'])]\n\n# In this dataset, department 69 (Lyon) is split in two: 69D and 69M. We merge them.\nmap_dep.loc[map_dep.code_insee.str.contains('69'), 'code_insee'] = '69'\nmap_dep = map_dep.dissolve(by='code_insee')\n\n# Add variable\nmap_dep = map_dep.join(rate_dep['author_rate'])\n\n# Set CRS from latitude\/longitude to Lambert93 for a better grid projection\nmap_dep.crs = {'init': 'epsg:4326'}\nmap_dep = map_dep.to_crs({'init': 'epsg:2154'})","ea22af28":"fig, ax = plt.subplots(1, figsize=(18, 12))\nmap_dep.plot(column='author_rate', cmap='Blues', ax=ax, linewidth=0.1, edgecolor='black')\nax.axis('off')\nax.set_title('Contributors of the Grand D\u00e9bat per inhabitant (\u2030)',\n             fontdict={'fontsize':'25', 'fontweight':'3'})\n\n# Add colorbar\nsm = plt.cm.ScalarMappable(cmap='Blues',\n                           norm=plt.Normalize(vmin=0, vmax=map_dep.author_rate.max()))\nsm._A = []\n# Place the cbar next to the plot\ncbar = fig.colorbar(sm,make_axes_locatable(ax).append_axes(\"right\", size=\"5%\"))\n\nplt.show()","4a901d2a":"questions = pd.concat([pd.DataFrame({'old_name':df_list[i].columns,\n                                     'df_id':i,\n                                     'theme':themes[i]}) for i in range(len(df_list))])\nquestions = questions[-questions[\"old_name\"].isin(col_common)].reset_index(drop=True)\nquestions = questions.assign(new_name=(pd.Series(\n    ['Q{}'.format(i) for i in range(1, questions.shape[0] + 1)])))\nquestions = questions.assign(question=pd.Series(\n    [name.split(' - ')[1] for name in questions.old_name]))\n\n# Questions rename\ndict_rename = {old:new for old, new in zip(questions.old_name,questions.new_name)}\nfor df in df_list:\n    df.rename(columns=dict_rename,inplace=True)\n    \nquestions.head(3)","2c8bfb18":"questions['nbrow'] = questions.apply(lambda g: df_list[g.df_id].shape[0], axis=1)\nquestions['nbnnull'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                       .notnull().sum(), axis=1)\nquestions['nbunique'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                        .nunique(), axis=1)\n\nquestions['nnull_rate'] = questions.nbnnull\/questions.nbrow * 100\nquestions['unique_rate'] = questions.nbunique\/questions.nbnnull * 100","65ecf3c9":"questions['closed'] = questions['nbunique'] <= 3\nsum(questions.closed)","f51010b9":"questions.groupby(['theme']).agg({'question':'count', 'closed':'sum',\n                                  'nbrow':'mean', 'nnull_rate':'mean'})","c48879cf":"questions.sort_values('nnull_rate').head(3)","7d572876":"df_list[1].Q40.value_counts().head(20)","bac152cf":"df_list[3].Q91.value_counts().head(10)","cb099ca3":"# Add frequencies to a countplot\n# Source: https:\/\/stackoverflow.com\/questions\/33179122\/seaborn-countplot-with-frequencies\ndef add_frequencies(ax, ncount):\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.1f} %'.format(100.*y\/ncount), (x.mean(), y), \n                ha='center', va='bottom', size='small', color='black', weight='bold')\n        \n# Countplot of questions_df\ndef countplot_qdf(questions_df, suptitle):\n    n = questions_df.shape[0]\n    \n    # If there is nothing to plot, we stop here\n    if n==0:\n        return\n    \n    # Numbers of rows and cols in the subplots\n    ncols = 3\n    nrows = (n+3)\/\/ncols\n    fig,ax = plt.subplots(nrows, ncols, figsize=(25,6*nrows))\n    fig.tight_layout(pad=9, w_pad=10, h_pad=7)\n    fig.suptitle(suptitle, size=30, fontweight='bold')\n    \n    # Hide exceeding subplots\n    for i in range(n, ncols*nrows):\n        ax.flatten()[i].axis('off')\n        \n    # Countplot for each question\n    for index, row in questions_df.iterrows():\n        plt.sca(ax.flatten()[index])\n        # We add the sort_values argument to always have the same order: Oui, Non...\n        xlabels = df_list[row.df_id].loc[:,row.new_name]\n        xlabels = xlabels.value_counts().index.sort_values(ascending=False)\n        axi = sns.countplot(x=row.new_name,\n                           data=df_list[row.df_id],\n                           order = xlabels)\n        # Wrap long questions into lines\n        axi.set_title(\"\\n\".join(wrap(row.new_name + '. ' + row.question, 60)))\n        axi.set_xlabel('')\n        # We also set a wrap here (for one very long answer...)\n        axi.set_xticklabels([\"\\n\".join(wrap(s, 17)) for s in xlabels])\n        axi.set_ylabel('Nombre de r\u00e9ponses')\n        add_frequencies(axi, row.nbnnull)\n        \n# Plotting questions, grouped by theme\nfor i in range(len(themes)):\n    countplot_qdf(questions[(questions.closed) & (questions.df_id == i)].reset_index(), themes[i])","83081bc6":"# Count words in a string, a word being here any sequence of characters between white spaces\ndef count_words(s):\n    if s is np.nan:\n        return(0)\n    return(len(s.split()))\n\n# For each dataframe:\n# filter on questions and title\n# count words for each contribution of each question\n# sum it all\nn_words = [df.filter(regex=r'title|^Q', axis=1).apply(np.vectorize(count_words)).sum().sum()\\\n           for df in df_list]\nsum(n_words)","67e9e125":"# Get French stop words (from both nltk and stop_words libraries)\nstop_words = list(set(get_stop_words('fr')).union(stopwords.words('french')))\n# Put them in lowercase ASCII\nstop_words = [unidecode.unidecode(w.lower()) for w in stop_words]\n# Add punctuation and some missing words\nstop_words = set(stop_words +\n                 list(string.punctuation) +\n                 [\"\u2019\", \"...\", \"'\", \"\", \">>\", \"<<\"] +\n                 [\"oui\", \"non\", \"plus\", \"toute\", \"toutes\", \"faut\"])","e2b71584":"# Get tokens from list of strings (can probably be optimised)\ndef get_tokens(s):\n    # MosesTokenizer has been moved out of NLTK due to licensing issues\n    # So we define a simple tokenizer based on regex, designed for French language\n    pattern = r\"[cdjlmnstCDJLMNST]['\u00b4`]|\\w+|\\$[\\d\\.]+|\\S+\"\n    tokenizer = RegexpTokenizer(pattern)\n    tokens = tokenizer.tokenize(\" \".join(s.dropna()))\n    # remove punctuation (for words like \"j'\")\n    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n    # lowercase ASCII\n    tokens = [unidecode.unidecode(w.lower()) for w in tokens]\n    # remove stop words from tokens\n    tokens = [w for w in tokens if w not in stop_words]\n    return(tokens)","c0f49ebc":"def plot_wordcloud(s, title, mw = 500):\n    wordcloud = WordCloud(width=1200, height=600, max_words=mw,\n                          background_color=\"white\").generate(\" \".join(s))\n    plt.figure(figsize=(20, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title, fontsize=50, pad=50)\n    plt.show()","c7d1d19a":"col_q = questions.new_name[~questions.closed].append(pd.Series('title'))\nfor i in range(len(themes)):\n    col_q_i = df_list[i].columns.intersection(col_q)\n    tokens = pd.concat([df_list[i][col].dropna() for col in col_q_i])\n    tokens = get_tokens(tokens)\n    plot_wordcloud(tokens, title = themes[i])","71990d7f":"# Conclusion\n<a id=\"conclusion\"><\/a>\n***","a20de8e6":"There is room for improvment: we count **singular** and **plural** separately, for instance \"*services public*\" and \"*service public*\" on the second graph. We need to add **stemming** here.\n\nWe conclude on these clouds, any interpretations on these results are up to you!","443eecde":"## Dataset size\n\nEach line of the dataframes corresponds to one **contribution**: the answers of an author to the questions of the corresponding theme. Let's see how many contributions we have for each dataset, and how many questions:","c92a0427":"We can see that all of those questions start with **\"Si...\"** *(\"If...\")*. They are conditional: an answer is not necessarily expected.  \n\nIf we pay attention we can notice that the `unique_rate` is also very low, this is because a lot of contributors answered **\"non concern\u00e9\"** *(\"not applicable\")*, for instance with question **Q40**:","9c835bd2":"But keep in mind it doesn't mean that the remaining zipcodes are necessarily correct! The remaining incorrect zipcodes will be lost as well. Here is an example:","876c56bf":"Some other questions have low `unique_rate` because they are **guided question**: choices were given but the respondant could decide to answer something else. This is the case for instance for questions **Q91**, **Q79** and **Q4**:","89c19114":"On the themes of **State organisation, democracy and citizenship**: when asked their opinion, contributors always take side for **change**. In particular, most popular demands include:\n* revising the **functioning of the administration** (**Q26** - **91%**) \n* taking into account the **blank vote** (**Q52** - **82%**) \n* **transforming the Assemblies** (**Q59** - **86%**)","afb55538":"On the theme of **ecological transition**, we can note that **69%** of respondents consider their daily life being impacted by climate change (**Q81**). However **95%** of them think they can personally contribute to the environmental protection (**Q83**). Solutions could arise from **heating method** (**61%** - **Q87**) or **mobility** (**42%** - **Q89**).","1fc67315":"As can be seen, around 50% of the authors submitted a single contribution. \n\nOnly **50,000** authors have at least 4 contributions and hence may have answered the 4 themes.","6ab4b824":"## When were the contributions submitted?","88894667":"# Shaping questions\n<a id=\"shaping\"><\/a>\n***","5e55d8ff":"We can now aggregate at the **theme** scale:","c35760f5":"Most of the information of the dataset lies in the **open questions**, but they are the most **difficult** to analysis!\n\nWe can start with with a basic statistic, the **number of words** contained in the whole dataset.","9640f58f":"In this section we will plot a map of contributors in France. For this purpose we will use the `authorZipCode` variable. First thing to check is the quality of this column: does it contains missing values?","611fdeb9":"# Closed questions analysis\n<a id=\"closed\"><\/a>\n***","94ae727d":"Now we use the second dataset to combine those data with the department boundaries.","e88db5af":"## Contributors map\n<a id=\"map\"><\/a>\n\n### Retrieving zipcodes","a918f94f":"After this very brief dataset analysis, it is time to focus on the variables of interest: **the questions**. Each dataframe contains several questions, but we will try to treat them all at once.  \n\nThe column names for the questions are a bit messy, we will rename them for more clarity. We build a dataframe containing information about each question: old and new name, title, and the theme and dataframe they are linked to.","15401950":"### Link it with open data","fd7d3c28":"## Environment\n\nLoading environment libraries and importing modules.","0929f660":"To ensure the consistency of our data, we can check if we have, as expected, a French population of 65 millions, and 250 thousand contributors:","74a365ce":"# Importing the data\n<a id=\"importing\"><\/a>\n***","c7df9e17":"We can use the `plot` function to plot a `GeoDataFrame` object:","b5c4175c":"To have an idea of how much each part of France contribute to the *Grand D\u00e9bat*, we must compute the **author rate**: number of contributors per inhabitant.  \n\nWe can compute the number of contributors per zipcode. The local open data in France (population, geo boundaries) is usually at the **commune** scale, but the relation between the commune and the zipcode is **many to many**! We have to aggregate to the **department** scale to have a **one to many** link with both the commune and the zipcode.\n\nWe will use two datasets:  \n\n1. **communes-francaises\/code-postal-code-insee-2015.csv**: a dataset containing the communes population, and link between commune, zipcode and department [(source)](https:\/\/data.opendatasoft.com\/explore\/dataset\/code-postal-code-insee-2015%40public\/table\/).\n2. **contours-des-departements-francais\/departements-20180101.shp**: the boundaries of French departments [(source)](https:\/\/www.data.gouv.fr\/fr\/datasets\/contours-des-departements-francais-issus-d-openstreetmap).\n\nLet's use the first dataset to compute the author rate per department:","509972d3":"# Who are the contributors?\n<a id=\"who\"><\/a>\n***","7eacf36c":"The contributions contain **167 million words**! That is equivalent to **325 times** ***Les Mis\u00e9rables***, or **154 times the whole** ***Harry Potter*** **series**.  \n\nIt is impossible to make an exhaustive analysis of the dataset via human reading... **Artificial Intelligence seems necessary** to interpret this dataset. Here you come kagglers!","49f08a0c":"Since we focus on contributors, we aggregate the table by `authorId` in order to have one line per author. If an author has several `authorType` or `authorZipCode` (that should be rare), we keep the most frequent one: the *mode*. \n\nWe also add a count statistics: how many contributions that author made over the whole dataset.","4c7e3a1a":"The number of contribution per hour reaches a peak in the late afternoon, between 18h and 19h (we don't know whether it is **UTC** or **CET** since the raw data are characters without TZ indication). ","52273503":"## Available variables\nThe 4 dataframes share some **common variables**, other columns are **questions** that are specific to the theme. The common variables are the following:","a538a222":"The next important step is to run a **tokenization**, i.e. splitting text into words. This might be tricky because of punctuation, wich is slightly different according to the language. There are some important features we have to take into considreation: **punctuation**, **case**, **encoding** and **stop words**.","ba33f237":"We notice that the great majority of respondents, 84% actually, are **citizens** *(mind the log scale!)*, i.e. they are neither politicals, officials nor part of an organisation.","93718763":"We will use the tokens to draw a **word cloud**. This is a visual representation of **n-gram** counts. The more frequent a term is, the bigger it will appear on the plot.","981b2165":"We will have a look at the `createdAt` variable to spot **when** the contributions were submitted, and at what time of the day.","0977ed87":"In this section we will have a closer look at the **authors** of the *Grand D\u00e9bat*. For each contribution we have an `authorID` that is shared among datasets. \n\nEveryone could submit any number of contribution for each theme. An author wrote **252 contributions** about the *organisation of the State*!","b453ee8c":"There is one file for each of the **4 main themes** of the debate. The file *EVENTS.csv* is different, it contains information about public events held regarding the debate, it will not be used here.  \n\nWe match each **file** with its corresponding **theme**.","330e5a67":"## Table of contents\n\n* [Importing the data](#importing)\n\n* [Discovering the dataset](#discovering)\n\n* [Who are the contributors?](#who)\n   \n* [Shaping the questions](#shaping)\n\n* [Closed questions analysis](#closed)\n\n* [Open questions analysis](#open)\n\n* [Conclusion](#conclusion)","9c815c34":"Good news, only 3 missing values! But are the other correct however?  \n\nIn France, the zipcode must contains **exactly 5 digits**. Let's check:","892648ad":"We can note that the two themes **ecological transition** and **taxation** gave more interest to respondants: they have both more contributions and less null values.\n\nIn particular, we see that there are lot of null values, we want to understand that. Let's see which questions have the most null values:","cb1424f6":"Let's see the files available in the dataset:","0c85d729":"Those 19 questions are **closed-ended question**: the answer is forced into a few choices, mainly **Yes** or **No**.","7195288c":"We can now import each file, all in one **list of dataframes** for easier use.  \n\nWe pay special attention to data types: *ZipCode* must be read as *strings* and date columns as *timestamps*.","38e2b662":"### Basic text mining\n\nTo finish, let's try some simple text mining. We will investigate the information hidden in the contributions. \n\nWe have **75 open questions**. If we do a question-wise analysis, this notebook will get very sprawling. We aggregate all questions at the **theme** scale.\n\nTo begin, one must define **stop words**: those are the most common words that don't give any insight, and must be filtered out when doing **natural language processing**.","69d54da2":"Let's plot a wordcloud for each of the 4 **themes**. We will see what are the **most raised topics** among each of them. \n\n*It may take a few minutes and some GBs of RAM.*","ad16351a":"# Discovering the dataset\n<a id=\"discovering\"><\/a>\n***","996f7133":"For each of the 19 **close questions**, we plot the count of each answer in order to identify most popular opinions.\n\nWe use the **seaborn** library for plotting.","f1ce1023":"The departments with the highest contributor rate are: **Paris** and **Haute-Garonne** (*7.1* and *6.3* contributors per 1000 inhabitant, respectively).  \nOn the contrary the departments with lowest rate are: **Seine-Saint-Denis** and **Haute-Corse** (<*1.8* contributors per 1000 inhabitant).\n","db3f95fa":"We can see a first peak at the very beginning of the *Grand D\u00e9bat* (the website was opened for contributions on tuesday 2019-01-22), and then another peak on sunday 2019-03-10. On those particular days, the submissions **exceeded 25,000 contributions by day**.\n\nLet's look also at the time the contributions were made:","24de9bad":"The dataset is rather clean. The variables `updatedAt` and `trashedStatus` are poorly filled because most contributions are neither updated nor trashed.","cb078283":"The dataset is huge: **over 500 thousand contributions**. There are a lot of analysis to be done!\nThe survey about *taxation* was the most answered, but this may be due to it being the shortest: 8 questions only.","3bccb350":"# Open questions analysis\n<a id=\"open\"><\/a>\n***","de83215f":"We can notice that some questions have **very few distinct answers**:","5cb6db90":"This is the end of this notebook for now, there are a lot more to say about this dataset, but the notebook is getting long. **Thank you for reading it**! \n\nIt was aimed to make you discover the dataset, and maybe inspire you for some more investigation on it.  \n\nWe have seen here only the surface of the data, I may do a second part with **deeper textual analysis**.  \n\nFeel free to **upvote** if you enjoyed this notebook, to **fork** it for further analysis, or to **comment** for any suggestion. All ideas are welcome!","957b2770":"The first statistics we can get out of this new dataframe is the **number of distinct contributors**:","22da513e":"# Grand D\u00e9bat National - Exploratory Data Analysis\n*** \n**J\u00e9r\u00e9my Lesuffleur - 06\/04\/2019**\n\n## Context\nFollowing the [Yellow vests movement](https:\/\/en.wikipedia.org\/wiki\/Yellow_vests_movement), the President of the French Republic launched the **Grand D\u00e9bat National**, a natio-wide public debate. Every French citizens were invited to express their views and propositions on four main themes: *ecological transition*, *taxation*, *organisation of the State*, *democracy and citizenship*.\n\nThis national debate produced a huge amount of data, mainly textual data, provided [in Open Data here](https:\/\/www.data.gouv.fr\/fr\/datasets\/donnees-ouvertes-du-grand-debat-national). In this kernel, we will load and **discover the dataset**, see what it contains, do an **exploratory analysis** and run some **natural language processing** algorithms. This notebook is not pretending to undertake a deep analysis of the dataset but rather **an overview of its content**.","3487d90d":"There are more than **250,000 distinct contributors**. As a city population, it would be in the [top 10 of France most populated cities](https:\/\/fr.wikipedia.org\/wiki\/Liste_des_communes_de_France_les_plus_peupl%C3%A9es)!","9bf8bba2":"As we can notice, the zipcode was not forced to be 5 digit. Some are more, some are less.\n\nSince we cannot use those incorrect zipcodes, we set them to **nan**. There are around 3000, it is not much regarding the number of contributors.","302331b2":"We see that we have some information about the **author**: a unique *Id*, a *Type* (we will dig into it later) and a *ZipCode*. Not that much: we don't have any information about the age or the gender of the author for instance.\n\nWe can wonder if those variables contain some **missing values**:","2df4958f":"For each question, we compute the following statistics:\n* **nbrow**: number of rows (i.e. number of contributions for the corresponding theme)\n* **nbnnull**: number of answers that are not *null* (answer is *null* if the contributor skipped that question)\n* **nbunique**: number of distinct answers\n* **nnull_rate**: nbnnull\/nbrow * 100\n* **unique_rate**: nbunique\/nbnnull * 100\n"}}