{"cell_type":{"78072e62":"code","1610d5a3":"code","ea83af61":"code","d53fa800":"code","da411089":"code","ea8f3d4b":"code","b69bffc1":"code","8402cbab":"code","235e40ad":"code","57cfc2da":"code","8b05309b":"code","69e3af31":"code","f655b1ae":"code","35b7b098":"code","e5c98811":"code","7d6247f0":"code","9934d893":"code","2ed5ada7":"code","b2466bdc":"code","b98a9a59":"code","c0fd3fef":"code","19b319bd":"code","bd283458":"code","4e28c669":"code","7c196058":"code","94c9cbac":"code","78c79da8":"code","112e7016":"code","b6e4e1b6":"code","e748d859":"code","f17cbbc2":"code","16d26e60":"code","1baf089e":"code","7da1c45f":"code","354e1a83":"code","1882a899":"code","9b0fddbc":"code","aced49c3":"code","12f208f5":"code","80fd6a44":"code","b998228d":"code","43f0a9bc":"code","a6e97c0b":"code","42b96fb8":"code","674da505":"code","54eba690":"code","344bf2b1":"code","ea3a982b":"code","71aadd21":"code","18194f48":"code","fbcd212d":"code","36b4b48f":"code","5bd0d704":"code","e5e6c20e":"code","d58ca1e1":"markdown","03fd61a5":"markdown","0eac9d14":"markdown","c827d198":"markdown","795b4e4c":"markdown","b6f16bc6":"markdown","be32a43f":"markdown","66d9a518":"markdown","feda544c":"markdown","8e32ed40":"markdown","9cc1ce50":"markdown","17d174b5":"markdown","236a06fb":"markdown","67be9264":"markdown","6cc82c62":"markdown","2da8b48a":"markdown","703bb3f1":"markdown","050aed59":"markdown","950ad126":"markdown","b5105dd3":"markdown","5246be26":"markdown","8542c39d":"markdown","93ffba4f":"markdown","eb7d93ea":"markdown","964497bb":"markdown","7f938fc7":"markdown","c26b18e1":"markdown","c1686e9f":"markdown","8e727a6e":"markdown","ecd43c7d":"markdown","98516346":"markdown","bbaa2cfc":"markdown","21c56be1":"markdown","12cb1e5a":"markdown","024961d6":"markdown","b637604b":"markdown","900bdeb7":"markdown","9db72734":"markdown","d336c018":"markdown","d1a8c0f6":"markdown","7bbb1e36":"markdown"},"source":{"78072e62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1610d5a3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# import iplot","ea83af61":"sample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n","d53fa800":"train_features.head()","da411089":"train_features.shape, train_targets_scored.shape, test_features.shape","ea8f3d4b":"train_features.info()","b69bffc1":"train_targets_scored.info()","8402cbab":"train_features.sig_id.nunique(), train_targets_scored.sig_id.nunique()","235e40ad":"train_features.cp_type.value_counts(normalize=True).plot(kind='pie', figsize=(12, 5), fontsize=12,\n                                                         title='CP Type', autopct='%1.1f%%')\nplt.show()","57cfc2da":"train_features.cp_time.value_counts(normalize=True).plot(kind='bar', figsize=(12, 5), fontsize=14,\n                                                         title='CP Time', xlabel='Time')\nplt.show()","8b05309b":"train_features.cp_dose.value_counts(normalize=True).plot(kind='bar', figsize=(12, 5), fontsize=14, \n                                                         title='CP Dose', xlabel='Dose')\nplt.show()","69e3af31":"gcols = [col for col in train_features.columns if 'g-' in col]\nccols = [col for col in train_features.columns if 'c-' in col]","f655b1ae":"g = sns.pairplot(train_features[gcols[:10]])\nplt.show()","35b7b098":"g = sns.pairplot(train_features[ccols[:10]])\nplt.show()","e5c98811":"train_target_count = train_targets_scored.sum()[1:].sort_values()","7d6247f0":"train_target_count[:50].plot(kind='barh', title='Least Target Occurances', fontsize='14', figsize=(5, 20))\nplt.show()","9934d893":"train_target_count[-50:].plot(kind='barh', title='Most Target Occurences', fontsize='12', figsize=(12, 20))\nplt.show()","2ed5ada7":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 14))\nfor row in range(2):\n    for col in range(2):\n        random_int = np.random.randint(1, 700)\n        train_features.loc[:2000, 'g-'+str(random_int)].plot(ax=ax[row][col], title='G - '+str(random_int))\nplt.show()","b2466bdc":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 14))\nfor row in range(2):\n    for col in range(2):\n        random_int = np.random.randint(1, 100)\n        train_features.loc[:2000, 'c-'+str(random_int)].plot(ax=ax[row][col], title='C - '+str(random_int), label='Train')\n        # test_features.loc[:2000, 'c-'+str(random_int)].plot(ax=ax[row][col], title='C - '+str(random_int), label='Test')\nplt.show()","b98a9a59":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 14))\nfor row in range(2):\n    for col in range(2):\n        random_int = np.random.randint(1, 700)\n        f = sns.boxplot(train_features.loc[:2000, 'g-'+str(random_int)], ax=ax[row][col])\n        f.set_title('G - '+str(random_int))\nplt.show()","c0fd3fef":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 14))\nfor row in range(2):\n    for col in range(2):\n        random_int = np.random.randint(1, 100)\n        f = sns.boxplot(train_features.loc[:2000, 'c-'+str(random_int)], ax=ax[row][col])\n        f.set_title('C - '+str(random_int))\nplt.show()","19b319bd":"plt.figure(figsize=(100, 100))\nsns.heatmap(train_features[ccols].corr())\nplt.show()","bd283458":"# gcols","4e28c669":"# Check correlation\ncols = gcols+ccols\ncorrelation = train_features[cols].corr()","7c196058":"len(cols), correlation.shape","94c9cbac":"gc_corr = {}\n\nfor i, c1 in enumerate(cols):\n    for j, c2 in enumerate(cols):\n        if i < j:\n            corr = correlation.iloc[i, j]\n            if corr >= 0.8:\n                gc_corr[c1] = c2","78c79da8":"useful_col = cols - gc_corr.keys()\nuseful_col = list(useful_col)\nuseful_col = useful_col + ['cp_type', 'cp_dose', 'cp_time']","112e7016":"# useful_col","b6e4e1b6":"len(useful_col), len(cols)","e748d859":"ctl_vehicle_id = train_features[train_features.cp_type == 'ctl_vehicle']['sig_id']\n\nctl_vehicle_id = list(ctl_vehicle_id)\n\nsum_target_cp_type_ctl_vehicle = train_targets_scored[train_targets_scored.sig_id.isin( ctl_vehicle_id)].sum()[1:].sum()\n\nprint('Training Data - For cp_type - ctl_vehicle total sum of targets : {}'.format(sum_target_cp_type_ctl_vehicle))","f17cbbc2":"# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# labelencoder = LabelEncoder()\n\n# train_features['cp_type_encoded'] = labelencoder.fit_transform(train_features.cp_type)\n# test_features['cp_type_encoded'] = labelencoder.transform(test_features.cp_type)","16d26e60":"train_features.columns","1baf089e":"# train_features['cp_time'] = train_features.cp_time.map({24:0, 48:1, 72:2})\n# test_features['cp_time'] = test_features.cp_time.map({24:0, 48:1, 72:2})","7da1c45f":"train_features['cp_type'] = train_features.cp_type.map({'trt_cp':0, 'ctl_vehicle':1})\ntest_features['cp_type'] = test_features.cp_type.map({'trt_cp':0, 'ctl_vehicle':1})","354e1a83":"train_features['cp_dose'] = train_features.cp_dose.map({'D1':0, 'D2':1})\ntest_features['cp_dose'] = test_features.cp_dose.map({'D1':0, 'D2':1})","1882a899":"train_features.cp_type.value_counts()","9b0fddbc":"# # train_cp_time = pd.get_dummies(train_features.cp_time, drop_first=True)\n\n# train_features['cp_dose_encoded'] = labelencoder.fit_transform(train_features.cp_dose)\n\n# # train_features = pd.concat([train_features, train_cp_time], axis = 1)\n\n# # test_cp_time = pd.get_dummies(test_features.cp_time, drop_first=True)\n# test_features['cp_dose_encoded'] = labelencoder.fit_transform(test_features.cp_dose)\n\n# # test_features = pd.concat([test_features, test_cp_time], axis = 1)","aced49c3":"train_features.shape, test_features.shape","12f208f5":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","80fd6a44":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input, BatchNormalization\nfrom keras.optimizers import RMSprop, Adam, SGD, Adamax\nfrom keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.regularizers import l1, l2, l1_l2\nfrom kerastuner.tuners import RandomSearch\n\nfrom tensorflow_addons.layers import WeightNormalization\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold","b998228d":"# class ClassificationReport(Callback):\n    \n#     def __init__(self, train_data=(), validation_data=()):\n        \n#         super(Callback ,self).__init__()\n        \n#         self.X_train, self.Y_train = train_data\n#         self.X_val, self.Y_val = validation_data\n        \n#         self.train_log_loss = []\n#         self.val_log_loss = []\n        \n#     def on_epoch_end(self, epoch, log={}):\n        \n#         train_prediction = np.round(self.model.predict(self.X_train, verbose=0))\n#         val_prediction = np.round(self.model.predict(self.X_val, verbose=0))\n        \n#         # training log loss\n#         train_loss = []\n#         for i, col in enumerate(self.Y_train.columns):\n#             # print(self.Y_train.loc[:, col].values.shape, train_prediction[:, i].shape)\n#             loss = log_loss(self.Y_train.loc[:, col].values, train_prediction[:, i].astype(float), labels=[0, 1])\n#             train_loss.append(loss)\n#         self.train_log_loss.append(np.mean(train_loss))\n        \n#         # validation log loss\n#         val_loss = []\n#         for i, col in enumerate(self.Y_val.columns):\n#             loss = log_loss(self.Y_val.loc[:, col].values, val_prediction[:, i].astype(float), labels=[0, 1])\n#             val_loss.append(loss)\n#         self.val_log_loss.append(np.mean(val_loss))\n        \n#         print(\"\\n Epoch - {}, Training Log Loss - {:.6}, Validation Log Loss - {:.6} \\n\".format(epoch+1,\n#                                                                                                 np.mean(train_loss),\n#                                                                                                 np.mean(val_loss)))","43f0a9bc":"# class MoA:\n    \n#     def __init__(self, X, Y, hp, folds=2, learning_rate=0.0001, dropout=0.1, seed=141, batch_size=128, epochs=10):\n        \n#         self.X = X\n#         self.Y = Y\n#         self.folds = folds\n#         self.learning_rate = learning_rate\n#         self.dropout = dropout\n#         self.seed = seed\n#         self.batch_size = batch_size\n#         self.epochs = epochs\n#         self.models = []\n#         self.scores = {}\n#         self.hp = hp\n    \n#     def build_model(self):\n        \n#         inp = Input(shape=(self.X.shape[1], ))\n#         batch_norm = BatchNormalization()(inp)\n        \n#         x1 = WeightNormalization(Dense(units=self.hp.Int('units',\n#                                                     min_value = 1024,\n#                                                     max_value = 4096,\n#                                                     step = 128),\n#                                        activation='selu'))(batch_norm)\n#         drop = Dropout(self.hp.Choice('learning_rate', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n        \n#         x1 = WeightNormalization(Dense(units=self.hp.Int('units',\n#                                                     min_value = 512,\n#                                                     max_value = 2048,\n#                                                     step = 128),\n#                                        activation='selu'))(batch_norm)\n#         drop = Dropout(self.hp.Choice('learning_rate', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n#         x1 = WeightNormalization(Dense(units=self.hp.Int('units',\n#                                                     min_value = 256,\n#                                                     max_value = 1024,\n#                                                     step = 128),\n#                                        activation='selu'))(batch_norm)\n#         drop = Dropout(self.hp.Choice('learning_rate', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n#         x1 = WeightNormalization(Dense(units=self.hp.Int('units',\n#                                                     min_value = 256,\n#                                                     max_value = 512,\n#                                                     step = 128),\n#                                        activation='selu'))(batch_norm)\n#         drop = Dropout(self.hp.Choice('learning_rate', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n#         dense = Dense(self.Y.shape[1], activation='sigmoid')(batch_norm)\n        \n#         model = Model(inputs=inp, outputs=dense)\n        \n#         model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n        \n#         return model\n    \n    \n#     def train_model(self):\n        \n#         mkf = MultilabelStratifiedKFold(n_splits=self.folds, random_state=seed, shuffle=True)\n        \n#         for fold, (train_idx, val_idx) in enumerate(mkf.split(self.X, self.Y)):\n            \n#             print('\\n Fold - {}'.format(fold))\n            \n#             X_train = self.X.loc[train_idx, :]\n#             Y_train = self.Y.loc[train_idx, :]\n            \n#             X_val = self.X.loc[val_idx, :]\n#             Y_val = self.Y.loc[val_idx, :]\n            \n#             # print(Y_val[: 2])\n            \n#             metrics = ClassificationReport(train_data=(X_train, Y_train), validation_data=(X_val, Y_val))\n            \n#             model = self.build_model()\n            \n#             reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=3, verbose=1,\n#                                                epsilon=self.learning_rate, mode='min')\n#             early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode= 'min')\n            \n#             history = model.fit(X_train, Y_train, batch_size=self.batch_size, epochs=self.epochs,\n#                       callbacks=[reduce_lr_loss, early_stop], validation_data=(X_val, Y_val), verbose=1)\n            \n#             self.models.append(model)\n            \n#             self.scores[fold] = {\n#                 'training_log_loss' : metrics.train_log_loss,\n#                 'validation_log_loss' : metrics.val_log_loss\n#             }\n            \n    \n#     def plot_learning_curve(self):\n        \n#         fig, ax = plt.subplots(nrows=self.folds, ncols=2, figsize=(20, self.folds * 6), dpi=100)\n        \n#         for i in range(self.folds):\n            \n#             sns.lineplot(x=np.arange(1, self.epochs+1), y=self.models[i].history.history['loss'], ax=ax[i][0],\n#                         label='Train Loss')\n#             sns.lineplot(x=np.arange(1, self.epochs+1), y=self.models[i].history.history['val_loss'], ax=ax[i][0],\n#                         label='Validation Loss')\n            \n#             sns.lineplot(x=np.arange(1, self.epochs+1), y=self.scores[i]['training_log_loss'], ax=ax[i][1],\n#                         label='Train Log-Loss')\n#             sns.lineplot(x=np.arange(1, self.epochs+1), y=self.scores[i]['validation_log_loss'], ax=ax[i][1],\n#                         label='Validation Log-Loss')\n            \n#             for j in range(self.folds):\n#                 ax[i][j].legend()\n#                 ax[i][j].set_xlabel('Epoch', size=12)\n#                 ax[i][j].tick_params(axis='x', labelsize=12)\n#                 ax[i][j].tick_params(axis='y', labelsize=12)\n            \n#     def predict(self, X_predict):\n        \n#         Y_predict = np.zeros((X_predict.shape[0], self.Y.shape[1]))\n        \n#         for i in range(self.folds):\n            \n#             temp_predict = self.models[i].predict(X_predict)\n            \n#             Y_predict = (Y_predict + temp_predict)\/self.folds\n        \n#         return Y_predict","a6e97c0b":"# def build_model(hp):\n        \n#         inp = Input(shape=(X.shape[1], ))\n#         batch_norm = BatchNormalization()(inp)\n        \n#         x1 = WeightNormalization(Dense(units=hp.Int('units_1',\n#                                                     min_value = 1024,\n#                                                     max_value = 4096,\n#                                                     step = 128),\n#                                        activation='elu'))(batch_norm)\n#         drop = Dropout(hp.Choice('dropout', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n        \n#         x1 = WeightNormalization(Dense(units=hp.Int('units_2',\n#                                                     min_value = 512,\n#                                                     max_value = 2048,\n#                                                     step = 128),\n#                                        activation='elu'))(batch_norm)\n#         drop = Dropout(hp.Choice('dropout', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n#         x1 = WeightNormalization(Dense(units=hp.Int('units_3',\n#                                                     min_value = 256,\n#                                                     max_value = 1024,\n#                                                     step = 128),\n#                                        activation='elu'))(batch_norm)\n#         drop = Dropout(hp.Choice('dropout', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)        \n\n#         x1 = WeightNormalization(Dense(units=hp.Int('units_4',\n#                                                     min_value = 256,\n#                                                     max_value = 512,\n#                                                     step = 128),\n#                                        activation='elu'))(batch_norm)\n#         drop = Dropout(hp.Choice('dropout', values=[0.2, 0.3, 0.4, 0.5, 0.6]))(x1)\n#         batch_norm = BatchNormalization()(drop)\n        \n#         dense = Dense(Y.shape[1], activation='sigmoid')(batch_norm)\n        \n#         model = Model(inputs=inp, outputs=dense)\n        \n#         model.compile(optimizer=Adam(), loss='binary_crossentropy')\n        \n#         return model","42b96fb8":"def build_model():\n        \n        inp = Input(shape=(X.shape[1], ))\n        batch_norm = BatchNormalization()(inp)\n        \n        x1 = WeightNormalization(Dense(units=1024,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)\n        \n        \n        x1 = WeightNormalization(Dense(640,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)\n        \n        x1 = WeightNormalization(Dense(768,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)        \n\n        x1 = WeightNormalization(Dense(units=256,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)\n        \n        dense = Dense(Y.shape[1], activation='sigmoid')(batch_norm)\n        \n        model = Model(inputs=inp, outputs=dense)\n        \n        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n        \n        return model","674da505":"def get_log_loss(Y, val_Y):    \n    full_loss = []\n    for i, col in enumerate(Y.columns):\n        loss = log_loss(Y.loc[:, col].values, val_Y[:, i].astype(float), labels=[0, 1])\n        full_loss.append(loss)\n\n    loss = np.mean(full_loss)\n    \n    # print(\"\\n Log Loss - {:.6} \\n\".format(loss))\n    \n    return loss","54eba690":"\n\n\ndef train_model(X, Y, X_Test, folds = 3, seed=3, batch_size = 128, epochs = 50, learning_rate=1e-4):\n    \n    submission = sample_submission.drop('sig_id', axis=1).copy()\n    submission.loc[:, :] = 0\n    \n    mkf = MultilabelStratifiedKFold(n_splits=folds, random_state=seed, shuffle=True)\n\n    final_log_loss = []\n    \n    T_logloss = []\n    V_logloss = []\n    \n    for n in range(seed):\n        \n        for fold, (train_idx, val_idx) in enumerate(mkf.split(X, Y)):\n\n            print('\\n Run - {}, Fold - {}'.format(n, fold))\n\n            X_train = X.loc[train_idx, :]\n            Y_train = Y.loc[train_idx, :]\n\n            X_val = X.loc[val_idx, :]\n            Y_val = Y.loc[val_idx, :]\n\n            model = build_model()\n\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=3, verbose=1,\n                                               epsilon=learning_rate, mode='min')\n            \n            early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode= 'min')\n            \n#             checkpoint = ModelCheckpoint(monitor = 'val_loss', verbose = 0, \n#                               save_best_only = True, save_weights_only = True, mode = 'min')\n\n            history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n                      callbacks=[reduce_lr_loss, early_stop], validation_data=(X_val, Y_val), verbose=2)\n\n            Train_Pred = model.predict(X_train)\n            Val_pred = model.predict(X_val)\n        \n            Train_Pred = Train_Pred\/(seed*(fold+1))\n            Val_pred = Val_pred\/(seed*(fold+1))\n            \n            train_logloss = get_log_loss(Y_train, Train_Pred)\n            val_logloss = get_log_loss(Y_val, Val_pred)\n        \n            print('Training Log Loss : {:.6}'.format(train_logloss))\n            print('Validation Log Loss : {:.6}'.format(val_logloss))\n\n            T_logloss.append(train_logloss)\n            V_logloss.append(val_logloss)\n            \n            submission += model.predict(X_Test)\n            \n            submission = submission\/((fold+1)*seed)\n    \n        final_T_logloss = np.mean(T_logloss)\n        final_V_logloss = np.mean(V_logloss)\n\n        print('Final Training Log Loss : {:.6}'.format(final_T_logloss))\n        print('Final Validation Log Loss : {:.6}'.format(final_V_logloss))\n    return submission","344bf2b1":"X = train_features.drop('sig_id', axis = 1)\nY = train_targets_scored.drop('sig_id', axis = 1)\n\nX_test = test_features.drop('sig_id', axis = 1)","ea3a982b":"submission = train_model(X, Y, X_test, folds = 3, seed=3, batch_size = 128, epochs = 50, learning_rate=1e-4)","71aadd21":"sample_submission.iloc[:, 1:] = submission","18194f48":"sample_submission","fbcd212d":"sample_submission.to_csv('submission.csv', index=False)","36b4b48f":"# tuner.get_best_hyperparameters","5bd0d704":"# Y_predicted = np.where(Y_predicted < 0.5, 0, 1)","e5e6c20e":"# submission = pd.concat([submission, submission1], axis=0)","d58ca1e1":"common = []\nfor id_ in tqdm_notebook(public_id):\n    if id_ in test_id:\n        common.append(id_)","03fd61a5":"X = X.drop(list(gc_corr.keys()), axis=1)","0eac9d14":"X_predict = test_features.drop(['sig_id'] + list(gc_corr.keys()), axis=1)","c827d198":"from tqdm import tqdm_notebook","795b4e4c":"submission1 = pd.DataFrame()\nsubmission1['sig_id'] = test_cp_type_id\nsubmission1[target_columns] = 0","b6f16bc6":"As well as we can see major outliers in negative dirrection. Let's confirm this by drawing box plots.","be32a43f":"test_cp_type_id = test_features[test_features.cp_type == 'ctl_vehicle']['sig_id'].tolist()","66d9a518":"Very Imbalance distribution of cp_type.","feda544c":"tuner.search(x, y,\n             epochs=20,\n             validation_data=(val_x, val_y), verbose=2)","8e32ed40":"Let's check the target distribution. Seems data labels are very imbalanced. few targets have only 6 occurences. So will need to carefully handle this imbalance problem.","9cc1ce50":"submission[submission.sig_id.isin(test_cp_type_id)][target_columns]","17d174b5":"submission.sum()[1:].sort_values().sum()","236a06fb":"submission.head()","67be9264":"X = train_features.drop(['sig_id'], axis=1)\nY = train_targets_scored.drop('sig_id', axis=1)","6cc82c62":"Train and Test data doesn't have duplicates. Great!","2da8b48a":"submission = pd.DataFrame()\nsubmission['sig_id'] = test_features['sig_id']\nsubmission[target_columns] = Y_predicted","703bb3f1":"Y_predicted","050aed59":"We can see outliers in genomes.","950ad126":"### Cells - Training Data","b5105dd3":"Distribution of CP Dose is also looks almost perfect and equal.","5246be26":"public_id = sample_submission.sig_id.values\ntest_id = submission.sig_id.values\n\npublic_commoon_id = list(set(test_id)-set(public_id))","8542c39d":"get_log_loss(val_y, val_predicted)","93ffba4f":"models = tuner.get_best_models(num_models=2)","eb7d93ea":"Let's see the correlation of cells. No strong correlation is there so everything looks good. We will not be checking correlation of genome because it has more than 700 columns.","964497bb":"### Genome - Training Data","7f938fc7":"submit = pd.DataFrame(index = list(test_id )+ public_commoon_id, columns=target_columns)\nsubmit.index.name = 'sig_id'\nsubmit[:] = 0\nsubmit.loc[submission.sig_id,:] = submission[target_columns].values\nsubmit.to_csv('submission.csv',index=True)","c26b18e1":"target_columns = train_targets_scored.drop('sig_id', axis=1).columns.tolist()\n# target_columns","c1686e9f":"# MoA - Mechanism of Action\n### What is the Mechanism of Action (MoA) of a drug? And why is it important?\n\nIn the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\n### How do we determine the MoAs of a new drug?\n\nOne approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.","8e727a6e":"Looking at the below graph of few genomes, data looks approximately normally distributed.","ecd43c7d":"Same intitution looks for cells also. One thing we can notice that data is somewhat left skewed.","98516346":"def get_log_loss(Y, val_Y):    \n    full_loss = []\n    for i, col in enumerate(Y.columns):\n        loss = log_loss(Y.loc[:, col].values, val_Y[:, i].astype(float), labels=[0, 1])\n        full_loss.append(loss)\n\n    print(\"\\n Log Loss - {:.6} \\n\".format(np.mean(full_loss)))","bbaa2cfc":"val_predicted1 = models[0].predict(val_x)\nval_predicted2 = models[1].predict(val_x)\n# val_predicted3 = models[2].predict(val_x)\n\nval_predicted = (val_predicted1 + val_predicted2)\/2","21c56be1":"models[0].fit(X, Y)\nmodels[1].fit(X, Y)\n# models[2].fit(X, Y)","12cb1e5a":"Y_predicted_1 = models[0].predict(X_predict)\nY_predicted_2 = models[1].predict(X_predict)\n# Y_predicted_3 = models[2].predict(X_predict)\n\nY_predicted = (Y_predicted_1 + Y_predicted_2)\/2","024961d6":"Distribution of CP Time looks almost perfect.","b637604b":"val_predicted1 = models[0].predict(val_x)\nval_predicted2 = models[1].predict(val_x)\n# val_predicted3 = models[2].predict(val_x)\n\nval_predicted = (val_predicted1 + val_predicted2)\/2\nget_log_loss(val_y, val_predicted)","900bdeb7":"sum_predicted_cp_type_ctl_vehicle = submission[submission.sig_id.isin(test_cp_type_id)].sum()[1:].sum()\n\nprint('Test Data - For cp_type - ctl_vehicle total sum of targets(Predicted) : {}'.format(sum_predicted_cp_type_ctl_vehicle))","9db72734":"# submission = submission[~submission.sig_id.isin(test_cp_type_id)]\nsubmission","d336c018":"from sklearn.model_selection import train_test_split\n\nx, val_x, y, val_y = train_test_split(X, Y, test_size=0.2, shuffle=True)","d1a8c0f6":"seed = 141\n# clf = MoA(X, Y, folds=3, learning_rate=1e-4, dropout=0.5, seed=141, batch_size=64, epochs=50, )\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=5,\n    executions_per_trial=3,\n    )","7bbb1e36":"X_predict.shape"}}