{"cell_type":{"cbd55828":"code","60ea4ca3":"code","42dc1aa3":"code","3b64656d":"code","9e9942e7":"code","d4bfe18c":"code","600f422d":"code","1a12af2f":"code","9ad0b720":"code","9cf926d4":"code","c3d182d0":"code","2dfadd4f":"code","51deffde":"code","f67a2254":"code","8ae2212f":"code","334db23a":"code","7849909f":"code","60d1e53e":"code","4518da08":"code","3dd84f67":"code","39f0107e":"code","2ccfb288":"code","631b5687":"code","666ef835":"code","740acbdb":"code","80368585":"code","cbe0f8ee":"code","90f419dd":"code","0b79a773":"code","067af762":"code","3c297f9e":"code","f7ceda4c":"code","1f9c0f70":"code","b8b79711":"code","fc20f0ce":"code","7ef83f68":"code","01a29826":"code","b24bfea9":"code","cf146216":"code","7df06f2f":"code","fff2469a":"markdown","0c47edd9":"markdown","04b039cd":"markdown","537dd3b1":"markdown","c21d4a59":"markdown","b30b4765":"markdown","01f9c246":"markdown","891ea047":"markdown"},"source":{"cbd55828":"!pip install vncorenlp\n!mkdir -p vncorenlp\/models\/wordsegmenter\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/VnCoreNLP-1.1.1.jar\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/models\/wordsegmenter\/vi-vocab\n!wget https:\/\/raw.githubusercontent.com\/vncorenlp\/VnCoreNLP\/master\/models\/wordsegmenter\/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp\/ \n!mv vi-vocab vncorenlp\/models\/wordsegmenter\/\n!mv wordsegmenter.rdr vncorenlp\/models\/wordsegmenter\/","60ea4ca3":"import os\nimport sys\nimport time\nimport datetime\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\n\nimport warnings\nfrom gensim.models import FastText\nwarnings.filterwarnings(\"ignore\")","42dc1aa3":"pd.options.display.max_colwidth=1000\ntrain_df = pd.read_csv(\"..\/input\/vietai-dataset\/assignment4-data\/Assignment4\/train.csv\")\ntrain_df.head()","3b64656d":"print('Number of train samples in total:', len(train_df))\nprint('Number of positives:', np.sum(train_df['class']==1))\nprint('Number of negatives:', np.sum(train_df['class']==0))","9e9942e7":"sample_positive = train_df[train_df['class'] == 1].sample(5)\nsample_positive","d4bfe18c":"sample_negative = train_df[train_df['class'] == 0].sample(5)\nsample_negative","600f422d":"test_df = pd.read_csv(\"..\/input\/vietai-dataset\/assignment4-data\/Assignment4\/test.csv\")\nprint('Number of test samples in total:', len(test_df))\ntest_df.head()","1a12af2f":"words_list = np.load('..\/input\/vietai-dataset\/assignment4-data\/Assignment4\/words_list.npy')\nprint('Prunned vocabulary loaded!')\nwords_list = words_list.tolist()\nword_vectors = np.load('..\/input\/vietai-dataset\/assignment4-data\/Assignment4\/word_vectors.npy')\nword_vectors = np.float32(word_vectors)\nprint ('Word embedding matrix loaded!')\nprint('Size of the vocabulary: ', len(words_list))\nprint('Size of the word embedding matrix: ', word_vectors.shape)","9ad0b720":"word2idx = {w:i for i,w in enumerate(words_list)}\nprint(list(word2idx.items())[:10])","9cf926d4":"word2idx['UNK']","c3d182d0":"# Lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u c\u00e2u, d\u1ea5u ngo\u1eb7c, ch\u1ea5m than ch\u1ea5m h\u1ecfi, v\u00e2n v\u00e2n..., ch\u1ec9 ch\u1eeba l\u1ea1i c\u00e1c k\u00ed t\u1ef1 ch\u1eef v\u00e0 s\u1ed1\nimport re\n# re = regular expressions\nstrip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n\ndef clean_sentences(string):\n    string = string.lower().replace(\"<br \/>\", \" \")\n    return re.sub(strip_special_chars, \"\", string.lower())","2dfadd4f":"def get_sentence_indices(sentence, max_seq_length, _words_list):\n    \"\"\"\n    Get index of each word in the sentence. Only letters, can be uppercase.\n    \n    Parameters\n    ----------\n    sentence: string\n        Sentence that needs to process\n    max_seq_length: int\n        Max number of words in the sentence\n    _words_list: list\n        a copy of words_list\n    \"\"\"\n    indices = np.zeros((max_seq_length), dtype='int32')\n    \n    # Lowercase and split the sentence into words\n    words = [word.lower() for word in sentence.split()]\n    \n    # Get \"unk\" index\n    unk_idx = word2idx['UNK']\n    ### TODO 1 ###\n    # Write code that fills the i-th index in \"indices\" with the i-th index in the \"words\"\n    # NOTE: len(indices) can be shorter than len(words)\n    ### START CODE HERE ###\n    for idx, word in enumerate(words):\n        if idx >= max_seq_length:\n            break\n        try:\n            indices[idx] = word2idx[word]\n        except:\n            indices[idx] = unk_idx\n    ### END CODE HERE ###\n    return indices","51deffde":"# V\u00ed d\u1ee5:\nsentence = \"Qu\u00e1n n\u00e0y b\u00e9_t\u00ed, nh\u01b0ng si\u00eau cute h\u1ea1t_me.\"\n\n# Ti\u1ec1n x\u1eed l\u00fd c\u00e2u\nsentence = clean_sentences(sentence)\nprint(sentence)\nsentence_indices = get_sentence_indices(sentence, max_seq_length=10, _words_list=words_list)\nprint(sentence_indices)","f67a2254":"print('Vector representation of sentence: {}'.format(sentence))\nprint(tf.nn.embedding_lookup(word_vectors,sentence_indices))","8ae2212f":"num_words = [len(clean_sentences(x).split()) for x in list(train_df['text'])]\nprint('The total number of samples is', len(train_df))\nprint('The total number of words in the files is', sum(num_words))\nprint('The average number of words in the files is', sum(num_words)\/len(num_words))","334db23a":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.hist(num_words, 100)\nplt.xlabel('Number of words in a sentence')\nplt.ylabel('Frequency')\nplt.axis([0, 600, 0, 5000])\nplt.show()","7849909f":"MAX_SEQ_LENGTH = 200","60d1e53e":"def text2ids(df, max_length, _word_list):\n    \"\"\"\n    Transform text in the dataframe into matrix index\n    NOTE: this is the train_ids.npy in the dataset\n    \n    Parameters\n    ----------\n    df: DataFrame\n        dataframe that stores the text\n    max_length: int\n        max length of a text\n    _word_list: numpy.array\n        array that stores the words in word vectors\n    \n    Returns\n    -------\n    ids: numpy.array\n        len(df) * max_length, contains indices of text\n    \"\"\"\n    ids = np.zeros((len(df), max_length), dtype='int32')\n    for idx, text in enumerate(tqdm(df['text'])):\n        ids[idx,:] = get_sentence_indices(clean_sentences(text), max_length, _word_list)\n    return ids","4518da08":"print(\"Converting train_df to train_ids...\")\ntrain_ids = text2ids(train_df, MAX_SEQ_LENGTH, words_list)\nnp.save('train_ids.npy', train_ids)","3dd84f67":"print('Word indices of the first review: ')\nprint(train_ids[0])","39f0107e":"# train_x, test_validation_x, train_y, test_validation_y  = train_test_split(train_ids, \n#                                                                            train_df['class'], test_size=0.2, random_state=2019)\n\ntrain_x, validation_x, train_y, validation_y  = train_test_split(train_ids, train_df['class'], test_size=0.2)\n\n# validation_x, test_x, validation_y, test_y = train_test_split(test_validation_x, \n#                                                               test_validation_y, test_size=0.5, random_state=2018)","2ccfb288":"BATCH_SIZE = 256","631b5687":"train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_x, validation_y))\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE)\n","666ef835":"for idx, (x,y) in enumerate(train_dataset):\n    if idx == 0:\n        print('X =',x)\n        print('y =',y)\nprint(\"Total: \", idx)","740acbdb":"def get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision + recall + K.epsilon())\n    return f1_val","80368585":"num_classes = 2 # Binary output\nlstm_output_dim = 64\nepochs=100","cbe0f8ee":"# F1 score and Acc callback, when it reaches a threshold\nclass F1ScoreCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get(\"get_f1\") is not None:\n            max_allowed = 0.98\n            if(logs.get(\"get_f1\") > max_allowed):\n                print(\"\\nReached {}% f1_score so cancelling training!\".format(max_allowed))\n                self.model.stop_training = True\nclass AccScoreCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get(\"acc\") is not None:\n            max_allowed = 0.98\n            if(logs.get(\"acc\") > max_allowed):\n                print(\"\\nReached {}% acc so cancelling training!\".format(max_allowed))\n                self.model.stop_training = True\n                \nf1_score_callback = F1ScoreCallback()\nacc_callback = AccScoreCallback()\nmodel_cp = tf.keras.callbacks.ModelCheckpoint(\"weight_model.h5\", \n                                              monitor=\"val_get_f1\", save_best_only=True, save_weights_only=True)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_get_f1\", \n                                              patience = epochs\/10, restore_best_weights=True)","90f419dd":"word_vectors.shape","0b79a773":"# Test with simple FNN\ndef create_test_model_1():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(word_vectors.shape[0], word_vectors.shape[1], name=\"embedding\", \n                                  embeddings_initializer=tf.keras.initializers.Constant(word_vectors), trainable=False),\n        tf.keras.layers.GRU(lstm_output_dim, dropout=0.42),\n        tf.keras.layers.Dense(word_vectors.shape[1]*3, activation=\"tanh\"),\n        tf.keras.layers.Dense(word_vectors.shape[1]*2, activation=\"tanh\"),\n        tf.keras.layers.Dense(word_vectors.shape[1], activation=\"tanh\"),\n        tf.keras.layers.Dense(word_vectors.shape[1]*0.5, activation=\"tanh\"),\n        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n    ])\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n              metrics=[get_f1, \"accuracy\"])\n    return model\n","067af762":"def create_test_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(word_vectors.shape[0], word_vectors.shape[1], name=\"embedding\", \n                                  embeddings_initializer=tf.keras.initializers.Constant(word_vectors), trainable=False),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_output_dim, return_sequences=True), name=\"bidi_lstm_1\"),\n        tf.keras.layers.Dropout(0.42, name=\"dropout_1\"),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_output_dim\/\/2), name=\"bidi_lstm_2\"),\n        tf.keras.layers.Dropout(0.42, name=\"dropout_2\"),\n        tf.keras.layers.Dense(lstm_output_dim, activation='tanh', kernel_regularizer=tf.keras.regularizers.l1_l2(), name=\"dense_1\"),\n        tf.keras.layers.Dropout(0.42, name=\"dropout_3\"),\n        tf.keras.layers.Dense(128, activation='tanh',kernel_regularizer=tf.keras.regularizers.l1_l2(), name=\"dense_2\"),\n        tf.keras.layers.Dropout(0.42, name=\"dropout_4\"),\n        tf.keras.layers.Dense(64, activation='tanh', name=\"dense_3\"),\n        tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_output\")\n    ])\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n              metrics=[get_f1])\n    return model","3c297f9e":"test_1=False\n\nif test_1:\n    model = create_test_model_1()\nelse:\n    model = create_test_model()\n    \nmodel.summary()\n","f7ceda4c":"history = model.fit(train_dataset, epochs=100, validation_data=validation_dataset, callbacks=[f1_score_callback,\n                                                                                              acc_callback,\n                                                                                              model_cp])","1f9c0f70":"plt.plot(history.history['get_f1'])\nplt.plot(history.history['val_get_f1'])\nplt.title('model f1 score')\nplt.ylabel('f1_score')\n\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')","b8b79711":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')","fc20f0ce":"model.save_weights(\"weight_model.h5\")","7ef83f68":"from vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\".\/vncorenlp\/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\ntokenized = []\nresult = []\nthreshold = 0.5\n\nfor idx, row in test_df.iterrows():\n    row = clean_sentences(row.text)\n    segmented_list = rdrsegmenter.tokenize(row)\n    segmented_sentence = \" \".join(item for innerlist in segmented_list for item in innerlist)\n    indices = get_sentence_indices(segmented_sentence, MAX_SEQ_LENGTH, words_list)\n    input_data = indices.reshape(1, *indices.shape)\n    tokenized.append(input_data)","01a29826":"for ele in tokenized:\n    prediction = model.predict(ele)\n    result.append(1 if prediction >= threshold else 0)\nprint(result[:20])\nprint(tokenized[:20])","b24bfea9":"test_df","cf146216":"submission_df = pd.read_csv(\"..\/input\/vietai-dataset\/assignment4-data\/Assignment4\/sample_submission.csv\")\nsubmission_df[\"class\"] = result\nsubmission_df.head(20)","7df06f2f":"submission_df.to_csv(\"rnn_submission.csv\", index=False)","fff2469a":"Positive review","0c47edd9":"Negative review","04b039cd":"## Load data","537dd3b1":"## Import necessary libraries","c21d4a59":"Convert word_list to word2idx dictionary for fast access","b30b4765":"Word vector taken from: https:\/\/fasttext.cc\/docs\/en\/crawl-vectors.html","01f9c246":"Load pd for submission (test data)","891ea047":"Clean sentence from special characters"}}