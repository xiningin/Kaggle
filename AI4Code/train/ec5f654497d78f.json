{"cell_type":{"bc53d4aa":"code","99aad715":"code","9d415a3d":"code","e5aac3ee":"code","de97a951":"code","8e549e66":"code","714939be":"code","7b8c8e48":"code","6cac1932":"code","c8f067d5":"code","435510ca":"code","910f4229":"code","0b3540a2":"code","3b6b99a8":"code","e0d844c4":"code","fc231242":"code","be5290a6":"code","7c4a780b":"code","ad3bea45":"code","7dc132bd":"code","c48e5494":"code","2847bcdd":"code","1ef97ba5":"code","81adef6f":"code","29be1316":"code","66395772":"code","c31a364d":"code","e2751977":"code","61161433":"code","94b118ef":"code","f7c53d33":"code","e211cc01":"markdown","c4d8b127":"markdown","c68e4fc4":"markdown","e8734d54":"markdown","fb5aa90e":"markdown","6fc4ca09":"markdown","a1922a9b":"markdown","44b18fb6":"markdown","2c59ae5e":"markdown","2c7a3e84":"markdown","f3c2efa9":"markdown","21cb9f6d":"markdown","2aeccdd0":"markdown","5fd67525":"markdown","6ad546d9":"markdown","549c20ee":"markdown","5230140b":"markdown"},"source":{"bc53d4aa":"import os\nimport numpy as np\nfrom glob import glob\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom time import time\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\nprint('Tensorflow Version:', tf.__version__)","99aad715":"images_dir = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Images\/'\nannotation_dir = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/'\ntrain_images = [\n    images_dir + file.strip()\n    for file in open(\n        annotation_dir + 'Flickr_8k.trainImages.txt', 'r'\n    ).read().split('\\n')\n][:-1]\nval_images = [\n    images_dir + file.strip()\n    for file in open(\n        annotation_dir + 'Flickr_8k.devImages.txt', 'r'\n    ).read().split('\\n')\n][:-1]\ntest_images = [\n    images_dir + file.strip()\n    for file in open(\n        annotation_dir + 'Flickr_8k.testImages.txt', 'r'\n    ).read().split('\\n')\n][:-1]\nlen(train_images), len(val_images), len(test_images)","9d415a3d":"annotation_dict = {}\nfor line in open(annotation_dir + 'Flickr8k.token.txt', 'r').read().split('\\n'):\n    try:\n        image_file, caption = line.split('\\t')\n        image_file = images_dir + image_file.split('#')[0]\n        caption = '<start> ' + caption + ' <end>'\n        annotation_dict[image_file] = caption\n    except:\n        pass\nlen(annotation_dict.keys())","e5aac3ee":"train_captions = [annotation_dict[image] for image in train_images]\nval_captions = [annotation_dict[image] for image in val_images]\ntest_captions = [annotation_dict[image] for image in test_images]","de97a951":"plt.imshow(Image.open(train_images[5999]))\nplt.xlabel(train_captions[5999])\nplt.show()","8e549e66":"BACKBONE_DICT = {\n    'inception_v3': {\n        'model': tf.keras.applications.InceptionV3,\n        'prep_func': tf.keras.applications.inception_v3.preprocess_input\n    },\n    'xception': {\n        'model': tf.keras.applications.Xception,\n        'prep_func': tf.keras.applications.xception.preprocess_input\n    },\n    'inception_resnet_v2': {\n        'model': tf.keras.applications.InceptionResNetV2,\n        'prep_func': tf.keras.applications.inception_resnet_v2.preprocess_input\n    },\n    'mobilenet': {\n        'model': tf.keras.applications.MobileNet,\n        'prep_func': tf.keras.applications.mobilenet.preprocess_input\n    },\n    'mobilenet_v2': {\n        'model': tf.keras.applications.MobileNetV2,\n        'prep_func': tf.keras.applications.mobilenet_v2.preprocess_input\n    }\n}","714939be":"class FeatureExtractor:\n    \n    def __init__(self, backbone='inception_v3'):\n        self.backbone = BACKBONE_DICT[backbone]['model'](include_top=False, weights='imagenet')\n        self.prep_func = BACKBONE_DICT[backbone]['prep_func']\n        self.feature_extraction_model = self.create_feature_extractor()\n    \n    def load_image(self, image_path):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, (299, 299))\n        image = self.prep_func(image)\n        return image, image_path\n    \n    def create_feature_extractor(self):\n        _input = self.backbone.input\n        _output = self.backbone.layers[-1].output\n        return tf.keras.Model(_input, _output)\n    \n    def cache_extracted_features(self, image_files, cache_dir, batch_size=16):\n        dataset = tf.data.Dataset.from_tensor_slices(image_files)\n        dataset = dataset.map(\n            self.load_image,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE\n        )\n        dataset = dataset.batch(16)\n        for image, path in tqdm(dataset):\n            batch_features = self.feature_extraction_model(image)\n            batch_features = tf.reshape(\n                batch_features, (\n                    batch_features.shape[0], -1,\n                    batch_features.shape[3]\n                )\n            )\n            for _feature, _path in zip(batch_features, path):\n                _path = _path.numpy().decode(\"utf-8\")\n                _path = os.path.join(cache_dir, _path.split('\/')[-1])\n                np.save(_path, _feature)","7b8c8e48":"feature_exractor = FeatureExtractor(backbone='inception_v3')","6cac1932":"try:\n    os.mkdir('.\/train_features')\nexcept:\n    print('Directory exists')\nfeature_exractor.cache_extracted_features(\n    train_images, batch_size=8,\n    cache_dir='.\/train_features'\n)","c8f067d5":"try:\n    os.mkdir('.\/val_features')\nexcept:\n    print('Directory exists')\nfeature_exractor.cache_extracted_features(\n    val_images, batch_size=8,\n    cache_dir='.\/val_features'\n)","435510ca":"try:\n    os.mkdir('.\/test_features')\nexcept:\n    print('Directory exists')\nfeature_exractor.cache_extracted_features(\n    test_images, batch_size=8,\n    cache_dir='.\/test_features'\n)","910f4229":"tokenizer = tf.keras.preprocessing.text.Tokenizer(\n    num_words=5000, oov_token=\"<unk>\",\n    filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ '\n)\ntokenizer.fit_on_texts(train_captions + val_captions + test_captions)","0b3540a2":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","3b6b99a8":"train_sequences = tokenizer.texts_to_sequences(train_captions)\nval_sequences = tokenizer.texts_to_sequences(val_captions)\ntest_sequences = tokenizer.texts_to_sequences(test_captions)","e0d844c4":"_max_len = 0\nfor seq in train_sequences + val_sequences + test_sequences:\n    _max_len = max(_max_len, len(seq))\n_max_len","fc231242":"padded_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    train_sequences, padding='post', maxlen=_max_len\n)\npadded_val_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    val_sequences, padding='post', maxlen=_max_len\n)\npadded_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    test_sequences, padding='post', maxlen=_max_len\n)\npadded_train_sequences.shape, padded_val_sequences.shape, padded_test_sequences.shape","be5290a6":"class DataLoader:\n    \n    def __init__(self, dataset='train'):\n        self.dataset = dataset\n    \n    def load_data(self, image_file, caption):\n        image_file = image_file.decode('utf-8').split('\/')[-1]\n        image_file = os.path.join(self.dataset + '_features', image_file)\n        feature_tensor = np.load(image_file)\n        return feature_tensor, caption\n    \n    def get_dataset(self, captions, batch_size=64, buffer_size=1000):\n        dataset = tf.data.Dataset.from_tensor_slices(\n            (glob(self.dataset + '_features\/*'), captions)\n        )\n        dataset = dataset.map(\n            lambda item1, item2: tf.numpy_function(\n                self.load_data, [item1, item2], [tf.float32, tf.int32]),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE\n        )\n        dataset = dataset.shuffle(buffer_size).batch(batch_size)\n        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n        return dataset","7c4a780b":"train_dataloader = DataLoader(dataset='train')\ntrain_dataset = train_dataloader.get_dataset(padded_train_sequences)\nx, y = next(iter(train_dataset))\nx.shape, y.shape","ad3bea45":"val_dataloader = DataLoader(dataset='val')\nval_dataset = val_dataloader.get_dataset(padded_val_sequences)\nx, y = next(iter(val_dataset))\nx.shape, y.shape","7dc132bd":"test_dataloader = DataLoader(dataset='test')\ntest_dataset = val_dataloader.get_dataset(padded_test_sequences)\nx, y = next(iter(test_dataset))\nx.shape, y.shape","c48e5494":"class Attention(tf.keras.Model):\n    \n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.w_1 = tf.keras.layers.Dense(units)\n        self.w_2 = tf.keras.layers.Dense(units)\n        self.v = tf.keras.layers.Dense(1)\n    \n    def call(self, features, hidden):\n        hidden = tf.expand_dims(hidden, 1)\n        score = tf.nn.tanh(self.w_1(features) + self.w_2(hidden))\n        attention_weights = tf.nn.softmax(self.v(score), axis=1)\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","2847bcdd":"class Encoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dimension):\n        super(Encoder, self).__init__()\n        self.dense = tf.keras.layers.Dense(embedding_dimension)\n    \n    def call(self, inputs):\n        return tf.nn.relu(self.dense(inputs))","1ef97ba5":"class Decoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dimension, units, vocabulary_size):\n        super(Decoder, self).__init__()\n        self.units = units\n        self.embedding = tf.keras.layers.Embedding(\n            vocabulary_size, embedding_dimension\n        )\n        self.gru = tf.keras.layers.GRU(\n            units, return_sequences=True,\n            return_state=True, recurrent_initializer='glorot_uniform'\n        )\n        self.dense_1 = tf.keras.layers.Dense(self.units)\n        self.dense_2 = tf.keras.layers.Dense(vocabulary_size)\n        self.attention = Attention(self.units)\n    \n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))\n    \n    def call(self, inputs, features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden)\n        x = self.embedding(inputs)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        x = self.dense_1(output)\n        x = tf.reshape(x, (-1, x.shape[2]))\n        x = self.dense_2(x)\n        return x, state, attention_weights","81adef6f":"encoder = Encoder(256)\ndecoder = Decoder(256, 512, 5001)","29be1316":"cross_entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none'\n)\n\ndef loss_function(y, y_pred):\n    mask = tf.math.logical_not(tf.math.equal(y, 0))\n    loss_ = cross_entropy_loss(y, y_pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_mean(loss_)","66395772":"optimizer = tf.keras.optimizers.Adam()","c31a364d":"checkpoint_path = \".\/checkpoints\/\"\nckpt = tf.train.Checkpoint(\n    encoder=encoder,\n    decoder=decoder,\n    optimizer = optimizer\n)\nckpt_manager = tf.train.CheckpointManager(\n    ckpt, checkpoint_path, max_to_keep=5\n)","e2751977":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n    ckpt.restore(ckpt_manager.latest_checkpoint)","61161433":"@tf.function\ndef train_step(x, y):\n    loss = 0\n    hidden_state = decoder.reset_state(batch_size=y.shape[0])\n    decoder_input = tf.expand_dims(\n        [tokenizer.word_index['<start>']] * y.shape[0], 1\n    )\n    with tf.GradientTape() as tape:\n        features = encoder(x)\n        for i in range(1, y.shape[1]):\n            predictions, hidden, _ = decoder(\n                decoder_input, features, hidden_state\n            )\n            loss += loss_function(y[:, i], predictions)\n            decoder_input = tf.expand_dims(y[:, i], 1)\n    total_loss = (loss \/ int(y.shape[1]))\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, trainable_variables)\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n    return loss, total_loss","94b118ef":"loss_history = []\nnum_steps = len(train_images) \/\/ 64\nfor epoch in range(start_epoch, 20):\n    start = time()\n    total_loss = 0\n    print('Epoch {}:'.format(str(epoch)))\n    for (batch, (x, y)) in enumerate(train_dataset):\n        batch_loss, total_batch_loss = train_step(x, y)\n        total_loss += total_batch_loss\n    loss_history.append(total_loss \/ num_steps)\n    if epoch % 5 == 0:\n        ckpt_manager.save()\n    print('Loss {:.6f}\\n'.format(total_loss \/ num_steps))\n    print ('Time taken: {} sec\\n'.format(time() - start))","f7c53d33":"plt.plot([loss.numpy() for loss in loss_history])","e211cc01":"### Feature Extractor Class","c4d8b127":"### Sanity Checks for the DataLoader","c68e4fc4":"## Reading the Dataset","e8734d54":"## Import Libraries","fb5aa90e":"### Attention","6fc4ca09":"## Dataloader","a1922a9b":"### Encoder","44b18fb6":"## Text Preprocessing","2c59ae5e":"## Feature Extraction","2c7a3e84":"### Caching Extracted Features","f3c2efa9":"Creating the Sequences","21cb9f6d":"### Defining the Tokenizer","2aeccdd0":"Dictionary for easy access of Backbone models and respective preprocessing functions","5fd67525":"### Decoder","6ad546d9":"Padding the sequences","549c20ee":"Adding Padding to the Tokenizer Word Index","5230140b":"## Models"}}