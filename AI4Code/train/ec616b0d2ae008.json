{"cell_type":{"01aa53dd":"code","c25fb60d":"code","117604d8":"code","b0a22bd8":"code","86bfb68d":"code","8b1ee027":"code","b57cb6c7":"code","ec1d0d42":"code","42a10202":"code","3f90b495":"code","cb9e53e6":"code","6585eb03":"code","dc424a49":"code","f326f89c":"code","dad9c835":"code","f2393e99":"code","bd9a4769":"code","f99d2600":"code","9c9c1957":"code","f653eed8":"code","7aa81585":"code","391d2578":"markdown","45a46dfc":"markdown","81d159c6":"markdown","8949b1c8":"markdown","c31df630":"markdown","0593e826":"markdown","da31d1cc":"markdown","3894cc61":"markdown","0b242d10":"markdown"},"source":{"01aa53dd":"!pip uninstall keras -y\n!pip install keras==2.2.5\n!pip install segmentation-models --quiet\n!pip install tta-wrapper --quiet","c25fb60d":"import os\nimport json\n\nimport albumentations as albu\nimport cv2\nimport keras\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom skimage.exposure import adjust_gamma\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport segmentation_models as sm","117604d8":"import keras.backend as K\nfrom keras.legacy import interfaces\nfrom keras.optimizers import Optimizer\n\nclass AdamAccumulate(Optimizer):\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., amsgrad=False, accum_iters=1, **kwargs):\n        if accum_iters < 1:\n            raise ValueError('accum_iters must be >= 1')\n        super(AdamAccumulate, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.amsgrad = amsgrad\n        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        completed_updates = K.cast(K.tf.floordiv(self.iterations, self.accum_iters), K.floatx())\n\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * completed_updates))\n\n        t = completed_updates + 1\n\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) \/ (1. - K.pow(self.beta_1, t)))\n\n        # self.iterations incremented after processing a batch\n        # batch:              1 2 3 4 5 6 7 8 9\n        # self.iterations:    0 1 2 3 4 5 6 7 8\n        # update_switch = 1:        x       x    (if accum_iters=4)  \n        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n        update_switch = K.cast(update_switch, K.floatx())\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n\n            sum_grad = tg + g\n            avg_grad = sum_grad \/ self.accum_iters_float\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = p - lr_t * m_t \/ (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n            else:\n                p_t = p - lr_t * m_t \/ (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon,\n                  'amsgrad': self.amsgrad}\n        base_config = super(AdamAccumulate, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","b0a22bd8":"def post_process(probability, threshold, min_size):\n    \"\"\"\n    Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored\n    \"\"\"\n    \n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    \n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((350, 525), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","86bfb68d":"train_df = pd.read_csv('..\/input\/understanding_cloud_organization\/train.csv')\ntrain_df['ImageId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\ntrain_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n\nprint(train_df.shape)\ntrain_df.head()","8b1ee027":"mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\nmask_count_df.sort_values('hasMask', ascending=False, inplace=True)\nprint(mask_count_df.shape)\nmask_count_df.head()","b57cb6c7":"sub_df = pd.read_csv('..\/input\/understanding_cloud_organization\/sample_submission.csv')\nsub_df['ImageId'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\ntest_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])","ec1d0d42":"def np_resize(img, input_shape):\n    \"\"\"\n    Reshape a numpy array, which is input_shape=(height, width), \n    as opposed to input_shape=(width, height) for cv2\n    \"\"\"\n    height, width = input_shape\n    return cv2.resize(img, (width, height))\n    \ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle2mask(rle, input_shape):\n    width, height = input_shape[:2]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return mask.reshape(height, width).T\n\ndef build_masks(rles, input_shape, reshape=None):\n    depth = len(rles)\n    if reshape is None:\n        masks = np.zeros((*input_shape, depth))\n    else:\n        masks = np.zeros((*reshape, depth))\n    \n    for i, rle in enumerate(rles):\n        if type(rle) is str:\n            if reshape is None:\n                masks[:, :, i] = rle2mask(rle, input_shape)\n            else:\n                mask = rle2mask(rle, input_shape)\n                reshaped_mask = np_resize(mask, reshape)\n                masks[:, :, i] = reshaped_mask\n    \n    return masks\n\ndef build_rles(masks, reshape=None):\n    width, height, depth = masks.shape\n    \n    rles = []\n    \n    for i in range(depth):\n        mask = masks[:, :, i]\n        \n        if reshape:\n            mask = mask.astype(np.float32)\n            mask = np_resize(mask, reshape).astype(np.int64)\n        \n        rle = mask2rle(mask)\n        rles.append(rle)\n        \n    return rles","42a10202":"def dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","3f90b495":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n                 base_path='..\/input\/understanding_cloud_organization\/train_images',\n                 batch_size=32, dim=(1400, 2100), n_channels=3, reshape=None, gamma=None,\n                 augment=False, n_classes=4, random_state=2019, shuffle=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.df = df\n        self.mode = mode\n        self.base_path = base_path\n        self.target_df = target_df\n        self.list_IDs = list_IDs\n        self.reshape = reshape\n        self.gamma = gamma\n        self.n_channels = n_channels\n        self.augment = augment\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n        self.on_epoch_end()\n        np.random.seed(self.random_state)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n        \n        X = self.__generate_X(list_IDs_batch)\n        \n        if self.mode == 'fit':\n            y = self.__generate_y(list_IDs_batch)\n            \n            if self.augment:\n                X, y = self.__augment_batch(X, y)\n            \n            return X, y\n        \n        elif self.mode == 'predict':\n            return X\n\n        else:\n            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.random_state)\n            np.random.shuffle(self.indexes)\n    \n    def __generate_X(self, list_IDs_batch):\n        'Generates data containing batch_size samples'\n        # Initialization\n        if self.reshape is None:\n            X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        else:\n            X = np.empty((self.batch_size, *self.reshape, self.n_channels))\n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['ImageId'].iloc[ID]\n            img_path = f\"{self.base_path}\/{im_name}\"\n            img = self.__load_rgb(img_path)\n            \n            if self.reshape is not None:\n                img = np_resize(img, self.reshape)\n            \n            # Adjust gamma\n            if self.gamma is not None:\n                img = adjust_gamma(img, gamma=self.gamma)\n            \n            # Store samples\n            X[i,] = img\n\n        return X\n    \n    def __generate_y(self, list_IDs_batch):\n        if self.reshape is None:\n            y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n        else:\n            y = np.empty((self.batch_size, *self.reshape, self.n_classes), dtype=int)\n        \n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['ImageId'].iloc[ID]\n            image_df = self.target_df[self.target_df['ImageId'] == im_name]\n            \n            rles = image_df['EncodedPixels'].values\n            \n            if self.reshape is not None:\n                masks = build_masks(rles, input_shape=self.dim, reshape=self.reshape)\n            else:\n                masks = build_masks(rles, input_shape=self.dim)\n            \n            y[i, ] = masks\n\n        return y\n    \n    def __load_grayscale(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = img.astype(np.float32) \/ 255.\n        img = np.expand_dims(img, axis=-1)\n\n        return img\n    \n    def __load_rgb(self, img_path):\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32) \/ 255.\n\n        return img\n    \n    def __random_transform(self, img, masks):\n        composition = albu.Compose([\n            albu.HorizontalFlip(),\n            albu.VerticalFlip(),\n            albu.ShiftScaleRotate(rotate_limit=30, shift_limit=0.1)\n        ])\n        \n        composed = composition(image=img, mask=masks)\n        aug_img = composed['image']\n        aug_masks = composed['mask']\n        \n        return aug_img, aug_masks\n    \n    def __augment_batch(self, img_batch, masks_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i, ], masks_batch[i, ] = self.__random_transform(\n                img_batch[i, ], masks_batch[i, ])\n        \n        return img_batch, masks_batch","cb9e53e6":"BATCH_SIZE = 32\n\ntrain_idx, val_idx = train_test_split(\n    mask_count_df.index, random_state=2019, test_size=0.2\n)\n\ntrain_generator = DataGenerator(\n    train_idx, \n    df=mask_count_df,\n    target_df=train_df,\n    batch_size=BATCH_SIZE,\n    reshape=(320, 480),\n    gamma=0.8,\n    augment=True,\n    n_channels=3,\n    n_classes=4\n)\n\nval_generator = DataGenerator(\n    val_idx, \n    df=mask_count_df,\n    target_df=train_df,\n    batch_size=BATCH_SIZE, \n    reshape=(320, 480),\n    gamma=0.8,\n    augment=False,\n    n_channels=3,\n    n_classes=4\n)","6585eb03":"opt = AdamAccumulate(lr=0.002, accum_iters=8)\n\nmodel = sm.Unet(\n    'resnet18', \n    classes=4,\n    input_shape=(320, 480, 3),\n    activation='sigmoid'\n)\nmodel.compile(optimizer=opt, loss=bce_dice_loss, metrics=[dice_coef])\nmodel.summary()","dc424a49":"model.load_weights('..\/input\/resunet18lb598\/model.h5')","f326f89c":"# checkpoint = ModelCheckpoint('model.h5', save_best_only=True)\n# es = EarlyStopping(monitor='val_dice_coef', min_delta=0.0001, patience=5, verbose=1, mode='max', restore_best_weights=True)\n# rlr = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.2, patience=2, verbose=1, mode='max', min_delta=0.0001)\n\n# history = model.fit_generator(\n#     train_generator,\n#     validation_data=val_generator,\n#     callbacks=[checkpoint, rlr, es],\n#     epochs=30,\n#     verbose=1,\n# )","dad9c835":"# history_df = pd.DataFrame(history.history)\n# history_df.to_csv('history.csv', index=False)\n\n# history_df[['loss', 'val_loss']].plot()\n# history_df[['dice_coef', 'val_dice_coef']].plot()\n# history_df[['lr']].plot()","f2393e99":"def post_process(probability, threshold, min_size):\n    \"\"\"\n    Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored\n    \"\"\"\n    \n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    \n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((350, 525), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","bd9a4769":"from tta_wrapper import tta_segmentation\n\nmodel = tta_segmentation(model, h_flip=True, h_shift=(-10, 10), merge='mean')","f99d2600":"best_threshold = 0.45\nbest_size = 15000\n\nthreshold = best_threshold\nmin_size = best_size\n\ntest_df = []\nencoded_pixels = []\nTEST_BATCH_SIZE = 500\n\nfor i in range(0, test_imgs.shape[0], TEST_BATCH_SIZE):\n    batch_idx = list(\n        range(i, min(test_imgs.shape[0], i + TEST_BATCH_SIZE))\n    )\n\n    test_generator = DataGenerator(\n        batch_idx,\n        df=test_imgs,\n        shuffle=False,\n        mode='predict',\n        dim=(350, 525),\n        reshape=(320, 480),\n        n_channels=3,\n        gamma=0.8,\n        base_path='..\/input\/understanding_cloud_organization\/test_images',\n        target_df=sub_df,\n        batch_size=1,\n        n_classes=4\n    )\n\n    batch_pred_masks = model.predict_generator(\n        test_generator, \n        workers=1,\n        verbose=1\n    ) \n    # Predict out put shape is (320X480X4)\n    # 4  = 4 classes, Fish, Flower, Gravel Surger.\n    \n    for j, idx in enumerate(batch_idx):\n        filename = test_imgs['ImageId'].iloc[idx]\n        image_df = sub_df[sub_df['ImageId'] == filename].copy()\n        \n        # Batch prediction result set\n        pred_masks = batch_pred_masks[j, ]\n        \n        for k in range(pred_masks.shape[-1]):\n            pred_mask = pred_masks[...,k].astype('float32') \n            \n            if pred_mask.shape != (350, 525):\n                pred_mask = cv2.resize(pred_mask, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n                \n            pred_mask, num_predict = post_process(pred_mask, threshold, min_size)\n            \n            if num_predict == 0:\n                encoded_pixels.append('')\n            else:\n                r = mask2rle(pred_mask)\n                encoded_pixels.append(r)\n                \nsub_df['EncodedPixels'] = encoded_pixels","9c9c1957":"def rle_decode(mask_rle, shape=(1400, 2100)):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape, order='F')  # Needed to align to RLE direction","f653eed8":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"white\")\nplt.figure(figsize=[60, 20])\nfor index, row in sub_df[:8].iterrows():\n    img = cv2.imread(\"...\/input\/understanding_cloud_organization\/test_images\/%s\" % row['ImageId'])[...,[2, 1, 0]]\n    img = cv2.resize(img, (525, 350))\n    mask_rle = row['EncodedPixels']\n    try: # label might not be there!\n        mask = rle_decode(mask_rle)\n    except:\n        mask = np.zeros((1400, 2100))\n    plt.subplot(2, 4, index+1)\n    plt.imshow(img)\n    plt.imshow(rle2mask(mask_rle, img.shape), alpha=0.5, cmap='gray')\n    plt.title(\"Image %s\" % (row['Image_Label']), fontsize=18)\n    plt.axis('off')    \n    \nplt.show()","7aa81585":"sub_df.to_csv('submission.csv', columns=['Image_Label', 'EncodedPixels'], index=False)\nsub_df.head(10)","391d2578":"# Utility Functions\n\nSource: https:\/\/www.kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\n\nUnhide below for the definition of `np_resize`, `build_masks`, `build_rles`.","45a46dfc":"# Visualisation\n\nRefers to [Understanding Clouds - EDA and Keras U-Net][1]\n\n[1]: https:\/\/www.kaggle.com\/dimitreoliveira\/understanding-clouds-eda-and-keras-u-net","81d159c6":"# Data Generator","8949b1c8":"# Preprocessing","c31df630":"## Loss function\n\nSource for `bce_dice_loss`: https:\/\/lars76.github.io\/neural-networks\/object-detection\/losses-for-segmentation\/","0593e826":"# About this kernel\n\nMostly copied from xhlulu's kernel [Satellite Clouds: U-Net with ResNet boilerplate][1]. I have added some new ideas such as gradient accumulation, gamma correction from discussions and other kernels. I am new to the field of image processing. So please correct me if you have found any mistake. Thank you very much.\n\n## Changelog\n* **V13**: Set threshold\/remove small masks\/TTA of ResUnet18's prediction.\n* **V12**: Save history to .csv.\n* **V11**: Reset the learning parameters back to V7. Try Resnet34 encoder. Add some visualisation on test predictions.\n* **V9**: Increased the initial learning rate to 0.005. Increased min_delta to 0.0005. \n* **V7**: Added gamma correction with gamma=0.8. Decreased min_delta to 0.0001.\n* **V3**: Added EarlyStopping and ReduceLROnPlateau with min_delta=0.001. Increased the initial learning rate to 0.002.\n* **V2**: Added gradient accumulation\n\n## References\n* `Satellite Clouds: U-Net with ResNet boilerplate`: https:\/\/www.kaggle.com\/xhlulu\/satellite-clouds-u-net-with-resnet-boilerplate\n* `Segmentation in PyTorch using convenient tools`: https:\/\/www.kaggle.com\/artgor\/segmentation-in-pytorch-using-convenient-tools\n* `A trick to use bigger batches for training: gradient accumulation`: https:\/\/www.kaggle.com\/c\/understanding_cloud_organization\/discussion\/105614#latest-616444\n* `[LB 0.628] simple segmentation approach`: https:\/\/www.kaggle.com\/c\/understanding_cloud_organization\/discussion\/104771#latest-614917\n* `Understanding Clouds Keras Unet`: https:\/\/www.kaggle.com\/saneryee\/understanding-clouds-keras-unet\n* `Understanding Clouds - EDA and Keras U-Net`: https:\/\/www.kaggle.com\/dimitreoliveira\/understanding-clouds-eda-and-keras-u-net\n\n[1]: https:\/\/www.kaggle.com\/xhlulu\/satellite-clouds-u-net-with-resnet-boilerplate","da31d1cc":"# Submission","3894cc61":"# Predict on Test Set","0b242d10":"# Training"}}