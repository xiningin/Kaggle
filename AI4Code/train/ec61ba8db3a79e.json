{"cell_type":{"169b9fcb":"code","7fcf5d14":"code","c5d751fc":"code","441d385d":"code","9aae2eac":"code","13b764f2":"code","1847dec2":"code","69f035dc":"code","0f166243":"code","be4bafc6":"code","5f18634f":"code","10ff97d1":"code","568f0f8c":"code","0b9e0cee":"code","f050c4d7":"code","71ca76f6":"code","39657f6a":"markdown","13a62cf6":"markdown","e234a5b7":"markdown","ab98f792":"markdown","0e0e7733":"markdown","fa11fe74":"markdown","7ca2563e":"markdown","1ba30818":"markdown"},"source":{"169b9fcb":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","7fcf5d14":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)] # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\nsvm_clf = Pipeline([\n (\"scaler\", StandardScaler()),\n (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n ])\nsvm_clf.fit(X, y)","c5d751fc":"svm_clf.predict([[6.5, 1.5]])","441d385d":"# Plotting\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\ndef plot_dataset(X, y, axes):\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n    plt.axis(axes)\n    plt.grid(True, which='both')\n    plt.xlabel(r\"$x_1$\", fontsize=20)\n    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.show()","9aae2eac":"from sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_svm_clf = Pipeline([\n (\"poly_features\", PolynomialFeatures(degree=3)),\n (\"scaler\", StandardScaler()),\n (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n ])\npolynomial_svm_clf.fit(X, y)","13b764f2":"def plot_predictions(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n\nplot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.show()","1847dec2":"from sklearn.svm import SVC\n\npoly_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n    ])\npoly_kernel_svm_clf.fit(X, y)\n\npoly100_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n    ])\npoly100_kernel_svm_clf.fit(X, y)\n","69f035dc":"fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n\nplt.sca(axes[0])\nplot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n\nplt.sca(axes[1])\nplot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\nplt.ylabel(\"\")\n\n# save_fig(\"moons_kernelized_polynomial_svc_plot\")\nplt.show()","0f166243":"rbf_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n    ])\nrbf_kernel_svm_clf.fit(X, y)","be4bafc6":"gamma1, gamma2 = 0.1, 5\nC1, C2 = 0.001, 1000\nhyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n\nsvm_clfs = []\nfor gamma, C in hyperparams:\n    rbf_kernel_svm_clf = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n        ])\n    rbf_kernel_svm_clf.fit(X, y)\n    svm_clfs.append(rbf_kernel_svm_clf)\n","5f18634f":"# Plotting\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n\nfor i, svm_clf in enumerate(svm_clfs):\n    plt.sca(axes[i \/\/ 2, i % 2])\n    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n    gamma, C = hyperparams[i]\n    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n    if i in (0, 1):\n        plt.xlabel(\"\")\n    if i in (1, 3):\n        plt.ylabel(\"\")\n\nplt.show()","10ff97d1":"np.random.seed(42)\nm = 100\nX = 2 * np.random.rand(m, 1) - 1\ny = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)\/10).ravel()","568f0f8c":"from sklearn.svm import SVR\n\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg.fit(X, y)","0b9e0cee":"def plot_svm_regression(svm_reg, X, y, axes):\n    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n    y_pred = svm_reg.predict(x1s)\n    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n    plt.plot(X, y, \"bo\")\n    plt.xlabel(r\"$x_1$\", fontsize=18)\n    plt.legend(loc=\"upper left\", fontsize=18)\n    plt.axis(axes)","f050c4d7":"svm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg1.fit(X, y)\nsvm_poly_reg2.fit(X, y)","71ca76f6":"fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\nplt.sca(axes[0])\nplot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\nplt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)\nplt.ylabel(r\"$y$\", fontsize=18, rotation=0)\nplt.sca(axes[1])\nplot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\nplt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)\n# save_fig(\"svm_with_polynomial_kernel_plot\")\nplt.show()","39657f6a":"### Kernel Selection\n- As a rule of thumb, you should always try the linear kernel first (remember that **LinearSVC** is much faster than SVC(kernel=\"linear\")), especially if the training set is very large or if it has plenty of features. \n- If the training set is not too large, you should try the **Gaussian RBF** kernel as well.","13a62cf6":"### SVM Regressor\n- It also supports linear and nonlinear regression.\n- The Objective is **reversed**.\n- Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations.\n- The width of the street is controlled by a **hyperparameter \u03f5**.\n- The model is said to be **\u03f5-insensitive**.\n\n#### Non - Linear SVM","e234a5b7":"### Nonlinear SVM Classification","ab98f792":"### Gaussian RBF Kernel\n- It includes adding new features and eliminating the older one. \n- This would make the function linear.\n\n\n- Increasing **gamma** makes the bell-shape curve narrower. The decision boundary ends up being more irregular around the instances.\n- So \u03b3 acts like a **regularization** hyperparameter: if your model is overfitting, you should reduce it, and if it is under fitting, you should increase it.","0e0e7733":"### Polynomial Kernel\n- Using **kernel tricks**, it is possible to get the same result as if you added many polynomial features, even with very high degree polynomials, without actually having to add them.\n- This trick is implemented by the **SVC class**.\n\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. \n- The hyperparameter **coef0** controls how much the model is influenced by highdegree polynomials versus low-degree polynomials.","fa11fe74":"### Support Vector Machine\n- Hyperplanes\n- Marginal Distance\n- Support Vectors\n- Linear Seperable\n- Non Linear Separable","7ca2563e":"#### Hyperplanes\n- It is a line that separates two classes in any classification scenario.\n- It is usually supported with **margines** that lie parallel to the hyperplane.\n- The points that lie the either side of the margine are classified.\n\n#### Marginal Distance\n- It is the distance between the two margine.\n- The **larger** the marginal distance, the better generalized the model would be.\n\n#### Support Vectors\n- These are points that pass through the margines and are the closest points to the margine.\n- They can be more than more points.\n\n#### LInear Separable\n- When points can be classified by a single line (hyperplane).\n\n#### Non Linear Separable\n- Some condition where points can't be separated linearly.\n- We use SVM Kernels\n- The kernels convert low dimension (2D) to High dimension (3D). ","1ba30818":"### Hyperparameters\n\n####  C Hyperparameter\n- A **smaller C** value leads to a wider street but more margin violations.\n- A **high** C value the classifier makes fewer margin violations but ends up with a smaller margin.\n- However, it seems likely that the first classifier will generalize better"}}