{"cell_type":{"9b996b99":"code","26220bfa":"code","34b60bc3":"code","198e65c6":"code","f5a0ed8a":"code","be0fcb91":"code","7beead37":"code","b2d013d1":"code","1756d536":"code","dd195bc5":"code","3efa1dfc":"markdown","3f754273":"markdown","0499fda7":"markdown"},"source":{"9b996b99":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\nimport ast\nimport os\nimport cv2\n\ndef EDA_plot_image_and_annotations(vid_id, vid_frame):\n    \n    entry = train_csv[(train_csv['video_id'] == vid_id) & (train_csv['video_frame'] == vid_frame)]\n    PATH = f\"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{vid_id}\/{vid_frame}.jpg\"\n    img = np.array(Image.open(PATH))\n    fig, ax = plt.subplots(1, figsize=(10, 8))\n    ax.axis('off')\n    ax.imshow(img)\n    \n    boxes = ast.literal_eval(entry.annotations.values[0])\n    for box in boxes:\n        rect = patches.Rectangle((box['x'], box['y']), box['width'], box['height'], linewidth=2, edgecolor='r', facecolor=\"none\")\n        ax.add_patch(rect)\n    plt.show()","26220bfa":"train_csv = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ntest_csv = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/test.csv')\ntrain_csv.info()","34b60bc3":"train_csv.head()","198e65c6":"test_csv.head()","f5a0ed8a":"print(f\"Video IDs in the train set: {train_csv.video_id.unique()}\")\nprint(f\"Video IDs in the test set: {test_csv.video_id.unique()}\")","be0fcb91":"num_seq = [len(train_csv[train_csv['video_id'] == i]) for i in range(3)]\nlabels = [\"0\", \"1\", \"2\"]\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\nax.set_facecolor('aliceblue')\nplt.grid(color=\"gray\", linestyle=\"-\", zorder=0)\nplt.ylabel(\"Number of Frames\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Video ID\", fontsize=16, fontweight=\"bold\")\nplt.title(\"Length of train videos\", fontsize=20, fontweight=\"bold\")\nplt.bar(labels, num_seq, color=\"orange\", zorder=3)\nplt.show()","7beead37":"ex_pic = plt.imread('..\/input\/tensorflow-great-barrier-reef\/train_images\/video_1\/10015.jpg')\nex_pic.shape","b2d013d1":"train_csv[\"number_fishs\"] = train_csv[\"annotations\"].apply(lambda x: len(ast.literal_eval(x)))\ntrain_csv.head()","1756d536":"max_num = max(train_csv.number_fishs)\nmax_sample = train_csv[train_csv[\"number_fishs\"] == max_num].sample()\nmax_vid_id = max_sample.video_id.values[0]\nmax_vid_frame = max_sample.video_frame.values[0]\n\nprint('\\033[1m' + f\"Maximum number of starfish in one frame: {max_num} (Video {max_vid_id}, Frame {max_vid_frame})\" + '\\033[0m')\nEDA_plot_image_and_annotations(max_vid_id, max_vid_frame)","dd195bc5":"cats = [str(i) for i in range(19)]\ndict_counts = dict()\n\nfor i in range(3):\n    set_ = train_csv[train_csv.video_id == i]\n    vid_id_counts = set_[\"number_fishs\"].value_counts().sort_index()\n    for j in range(len(cats)):\n        if j not in np.array(vid_id_counts.index):\n            vid_id_counts = vid_id_counts.append(pd.Series([0], index=[j]))\n    dict_counts.update({f\"Video {i}\": [i\/len(set_) for i in list(vid_id_counts.values)]})\n\ndef survey(results, category_names):\n    \n    labels = list(results.keys())\n    data = np.array(list(results.values()))\n    data_cum = data.cumsum(axis=1)\n    category_colors = plt.colormaps['RdYlGn'](\n        np.linspace(0.15, 0.85, data.shape[1]))\n\n    fig, ax = plt.subplots(figsize=(16.1, 6))\n    ax.invert_yaxis()\n    ax.xaxis.set_visible(False)\n    ax.set_xlim(0, np.sum(data, axis=1).max())\n\n    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n        widths = data[:, i]\n        starts = data_cum[:, i] - widths\n        rects = ax.barh(labels, widths, left=starts, height=0.5,\n                        label=colname, color=color)\n        \n    ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1),\n              loc='lower left', fontsize='small')\n\n    return fig, ax\n\nsurvey(dict_counts, cats)\nplt.show()","3efa1dfc":"# 1. EDA","3f754273":"# 0. Setup","0499fda7":"This figure shows the relative number of annotations in the frames of the three videos. As we can see, all three videos contain most of the time frames without annotations. On the one hand, the shortest video 0 labels not more than five starfishes in one frame, while the other videos include up to 18 fishes in one frame. On the other hand, the videos 1 and 2 have, relatively speaking, most of the time no starfish in front of the camera."}}