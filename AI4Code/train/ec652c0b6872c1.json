{"cell_type":{"57afa91a":"code","f814f196":"code","dab09768":"code","3a54372d":"code","08962b0a":"code","c26018fa":"code","5fbd6edf":"code","185bd6b5":"code","4a7db3db":"code","7e475513":"code","e2014680":"code","9d30b0a4":"code","2c78eea6":"code","9cc8e39f":"code","b50f7ad0":"code","7b51b113":"code","bef4ada9":"code","1ff3f940":"code","ba1aa6d5":"code","10e2c9e9":"markdown","0021e83b":"markdown","c4b3727c":"markdown","a41c0501":"markdown","7609100d":"markdown","3d9ee2ce":"markdown","1d27ba1e":"markdown","4d10b217":"markdown","090995c8":"markdown","97193ee7":"markdown","87983ccc":"markdown","2a2e8498":"markdown","8701fdc1":"markdown","074f6ef4":"markdown","36ee9ed1":"markdown","19c983e9":"markdown","259b1fb4":"markdown","9331b2b3":"markdown","3239f4c3":"markdown","1c676579":"markdown","fa3db1f1":"markdown","802e7408":"markdown","718fc5fc":"markdown","eab7b647":"markdown"},"source":{"57afa91a":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model, metrics, model_selection\nimport random","f814f196":"\"\"\"\nTask: load the data into a variable named data\n\"\"\"\nfile_name = '..\/input\/SAheart.data'\ndata = pd.read_csv(file_name, sep=',', index_col=0)","dab09768":"data.head()","3a54372d":"data['famhist_true'] = data['famhist'] == 'Present'\ndata['famhist_false'] = data['famhist'] == 'Absent'\ndata = data.drop(['famhist'], axis=1)\ndata.head()","08962b0a":"def split(data):\n    # control randomization for reproducibility\n    np.random.seed(42)\n    random.seed(42)\n    train, test = model_selection.train_test_split(data)\n    x_train = train.loc[:, train.columns != 'chd']\n    y_train = train['chd']\n    x_test = test.loc[:, test.columns != 'chd']\n    y_test = test['chd']\n    return x_train, y_train, x_test, y_test","c26018fa":"def plot_feature(data, feature_name):\n    plt.figure(figsize=(10, 3))\n    plt.scatter(data[feature_name], data['chd'])\n    plt.xlabel(feature_name)\n    plt.ylabel('chd')\n    plt.show()","5fbd6edf":"\"\"\"\nFeature list:\nsbp tobacco ldl adiposity famhist_true famhist_false typea obesity alcohol age\n\"\"\"\nplot_feature(data, 'tobacco')","185bd6b5":"def evaluate(model, x_train, y_train, x_test, y_test):\n    train_preds = model.predict(x_train)\n    test_preds = model.predict(x_test)\n    train_acc = metrics.accuracy_score(y_train, train_preds)\n    test_acc = metrics.accuracy_score(y_test, test_preds)\n    print('Train accuracy: %s' % train_acc)\n    print('Test accuracy: %s' % test_acc)","4a7db3db":"def split_train_evaluate(model, data):\n    x_train, y_train, x_test, y_test = split(data)\n    model.fit(x_train, y_train)\n    evaluate(model, x_train, y_train, x_test, y_test)\n\n# randomly pick some maybe reasonable hyperparameters\nmodel_bl = linear_model.SGDClassifier(\n    loss='log', alpha=0.1, max_iter=1000, tol=-np.inf, class_weight='balanced')\nsplit_train_evaluate(model_bl, data)","7e475513":"print('Sick: %s' % len(data[data['chd'] == True]))\nprint('Healthy: %s' % len(data[data['chd'] == False]))","e2014680":"302 \/ (160 + 302)","9d30b0a4":"# this may take a few seconds to run\nx_train, y_train, x_test, y_test = split(data)\ngrid_search = model_selection.GridSearchCV(\n    estimator=linear_model.SGDClassifier(loss='log', tol=-np.inf, class_weight='balanced'),\n    param_grid={'alpha': [0.1, 0.3],  # This is a tiny an very incomplete search space\n                'max_iter': [10000, 15000]},  # These may be way too large?\n    cv=10,\n    return_train_score=True)\ngrid_search.fit(x_train, y_train)","2c78eea6":"r = pd.DataFrame(grid_search.cv_results_)\n# we only want a subset of the columns for a precise summary\nr[['params', 'mean_train_score', 'mean_test_score']].head()","9cc8e39f":"# pull out best params, retrain, evaluate\nbest_alpha = grid_search.best_params_['alpha']\nbest_max_iter = grid_search.best_params_['max_iter']\nprint('Best alpha: %s' % best_alpha)\nprint('Best max_iter: %s' % best_max_iter)\nbest_model = linear_model.SGDClassifier(\n    loss='log', tol=-np.inf, class_weight='balanced', alpha=best_alpha, max_iter=best_max_iter)\nbest_model.fit(x_train, y_train)\nevaluate(best_model, x_train, y_train, x_test, y_test)","b50f7ad0":"\"\"\"\nTask: Beat the baseline 67.24% test accuracy\n\"\"\"","7b51b113":"def get_feature_set(data, wanted_features):\n    return data.loc[:, [col in wanted_features for col in data.columns]]","bef4ada9":"param_grid = {\n    'alpha': [0.0001, 1.],\n    'max_iter': [10, 10000]\n}","1ff3f940":"wanted_features = ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist_true', 'typea',\n                   'famhist_false', 'obesity', 'alcohol', 'age', 'chd']  # must have chd - DON'T REMOVE","ba1aa6d5":"# this may take a few seconds to load\nmy_data = get_feature_set(data, wanted_features)  # feature selection\nx_train, y_train, x_test, y_test = split(my_data) # splits\ngrid_search = model_selection.GridSearchCV(       # perform grid search\n    estimator=linear_model.SGDClassifier(loss='log', tol=-np.inf, class_weight='balanced'),\n    param_grid=param_grid,\n    cv=10,\n    return_train_score=True)\ngrid_search.fit(x_train, y_train)\nprint(grid_search.best_params_)\nbest_model = linear_model.SGDClassifier(loss='log', tol=-np.inf, class_weight='balanced',\n                                        **grid_search.best_params_)\nbest_model.fit(x_train, y_train)\nevaluate(best_model, x_train, y_train, x_test, y_test)","10e2c9e9":"## Loss Function\n\nMSE is not an appropriate loss function here. We are dealing with probabilities so the natural choice is negative log loss\n\n$$\\text{NLL} = -y \\log(h_\\theta) - (1 - y)\\log(1 - h_\\theta))$$\n\n$y$ is going to be either $1$ or $0$ so only one of these two terms will apply.","0021e83b":"Edit this cell to define your own grid search space.","c4b3727c":"### Manage Categorial Variable\n\nWe once again have a categorical variable, `famhist`, which we need to make into a one-hot encoding. A one-hot encoding assigns a class label to a position in list of binary variables (`0` or `1`), and puts a `1` in the position that corresponds to the desired class.\n\nSo for `famhist` we will make a one hot encoding of two binary variables\n\n$$\n\\begin{align}\n\\text{famhist: Present} \\rightarrow \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\\\\n\\text{famhist: Absent} \\rightarrow \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\\\\n\\end{align}\n$$","a41c0501":"### Class Weighting\n\nWe are doing binary classification, but we don't actually have an even number of classes in our training data","7609100d":"## Hypothesis\n\nOur hypothesis for logistic regression can be linear or non-linear. What is different with linear regression is we want to output class probabilities. We need a way to express our hypothesis as a probability value.\n\nTo do this we use the sigmoid function\n\n$$\\sigma(h) = \\frac{e^h}{e^h + 1} = \\frac{1}{1 + e^{-h}}$$\n\n![sigmoid](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/88\/Logistic-curve.svg)\n\nWe can see that this function is going to squash its input into the range $(0, 1)$ giving us a valid probability value.\n\nWe will pass our linear or non-linear function of the data through the sigmoid function to get our final hypothesis\n\n$$h_\\theta(\\mathbf{x}) = P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{a}^\\top\\mathbf{x})$$\n\nWe can arbitrarily decide what $y=1$ means - in the example let's say passes the exam. Then the probability of failing the exam is given by\n\n$$P(y=0|\\mathbf{x}) = 1 - P(y=1|\\mathbf{x}) = 1 - \\sigma(\\mathbf{a}^\\top\\mathbf{x})$$","3d9ee2ce":"This is just a helper function for below (don't edit it).","1d27ba1e":"Replace `data.columns` with a list of your own chose features to perform feature selection - e.g. `['feature1', 'feature2']`.","4d10b217":"So actually the above classifier is doing **worse** than just guessing the majority class.\n\nBut the class imbalance is the reason we specify the argument `class_weights='balanced'` so that our classifier will in fact take this into account during learning. Although our result above is actually terrible, this is due to poor hyperparameter selection as we shall see next.","090995c8":"## Optimization\n\nWe cannot use a closed form here so we will have to use **gradient descent**.\n\nWhen we learned about linear regression we noted that the loss surface was convex - from any point you know how to reach the global minimum.\n\nOur loss here is highly non-convex. Instead we need to use an iterative algorithm to take clever steps from an initial starting point to try and find a good local minimum. The picture looks like this\n\n![gradient_descent](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*f9a162GhpMbiTVTAua_lLQ.png)\n\nThe key idea is that if you take the gradient of the parameters with respect to the loss at a particular point, it will tell you how to change those parameters to reduce the loss\n\n![gradient_descent_2](https:\/\/cdn-images-1.medium.com\/max\/800\/0*rBQI7uBhBKE8KT-X.png)\n\nClearly in this diagram if we start on the right, we need to **decrease** $w$ in order to lower our loss. That's really the main idea.\n\nWith this algorithm we make a number of \"steps\" to update an improve the parameters. Each step the update is\n\n$$\\theta' = \\theta - \\beta \\frac{\\partial \\text{NLL}(h_\\theta, y)}{\\partial \\theta}$$\n\nHere the hyperparameter $\\beta$ is very important - it controls how big our steps are. In the above diagram if the learning rate made us step half the width of the \"U\" we would just bounce from side to side and never settle in the optimum.\n\nWe also need to pay attention to how many steps we take. In the diagram if we just took two we wouldn't give the algorithm time to settle. In practice we will take many more steps than two. Be aware that you will have to tune this hyperparameter.","97193ee7":"## Preprocess the Data\n\n### Load the Data\n\nAs usual, load the data with `pd.read_csv`. We have an index column in position zero and the separator is a comma. Load the data into a variable named `data`.","87983ccc":"## Data Visualization\n\nLet's now take a look at the relationships in the data. We can still use scatterplots for classification, but they look a bit different. Again let's make a convenient function for later use. As you will see we stretch the plot size to try and make these plots a bit clearer.","2a2e8498":"We'll take a look at one feature together then you should explore the rest of the dataset on your own and form your own opinions about which features are going to be useful.","8701fdc1":"The grid search results come in a dictionary that can be loaded directly into a pandas `DataFrame` for viewing so you can see what how parameter choices performed.","074f6ef4":"### Your Turn\n\nYou can use the grid search code above to find your best model and beat our baseline test accuracy of $67.24$ percent.\n\nYou will want to examine the parameters in the `SGDClassifier` documentation (http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) and make your own decisions about what a reasonable search space should look like.\n\nAlso be aware that if your best value for a parameter is on the edge of your search space, you want to expand the space further in that direction to see if you can keep climbing. For example if my space for $\\alpha$ was `[0.1, 0.5, 1.]`, and the best result came with `1.`, then I should definitely try `2.` and `5.` and so on.\n\nAlso don't forget feature selection. Return to the section where we were visualizing. Try and apply some $L2$ penalty through the parameters to perform ridge regression if you like.","36ee9ed1":"So actually the \"random\" (learned nothing) baseline should be the majority class prediction accuracy.","19c983e9":"### Grid Search\n\nWhat may be difficult about this task is that for the first time during our tutorials we have a number of hyperparameters. This is the common situation in machine learning practice. One approach to dealing with this issue is to define a reasonable set of values for each hyperparameter and then search over all combinations of them using cross validation on the training set. This technique is called **grid search**.\n\nWe can perform gid search with `sklearn.model_selection.GridSearchCV`, reference here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html.\n\nLet's try it out with a very small search space to show you how it works.","259b1fb4":"So we have already improved, and can now beat the majority class baseline (by a tiny bit).","9331b2b3":"## Classification\n\nIn regression we were trying to predict a continuous value - e.g. exam score.\n\nIn classification we predict a class label for a given data point - e.g. pass\/fail the exam. It might be a two-class problem like this (binary classification) or a many-class problem.\n\nNot only do we predict a class label, we also predict a probability for each class.\n\nOur example of pass fail can be pictured like this\n\n![logistic_regression](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6d\/Exam_pass_logistic_curve.jpeg)\n\nTo perform classification we need to make a **decision**. This requires defining a **decision boundary**. This is going to be an affine set - in 1D (i.e. with one feature) this will be a point; in 2D (with two features) this will be a line; with three features a plane; etc...\n\nIn our case above we could say if `num_hours_studying` $\\gt 2.7$ then we will predict `pass`.\n\nIn a bivariate case our decision boundary might look like this\n\n![bivariate_logistic_regression](https:\/\/i0.wp.com\/ucanalytics.com\/blogs\/wp-content\/uploads\/2017\/09\/Scatter-Plot-with-Boundary-Logistic-Regression.jpg?resize=768%2C578)","3239f4c3":"### Gradient Descent Model\n\nHere I will implement a baseline gradient descent model. I will not perform feature selection or tune regularization. You will then need to beat this baseline.\n\nThe baseline use the `sklearn.linear_model.SGDClassifier` class - reference here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier. By passing the argument `loss='log'` we get a logistic regression model.\n\nI append `bl` to the variables here to mark them as the baseline. Let's also add one more convenient helper function that will split the data, train the model, and return train and test accuracies.","1c676579":"## Fit Baseline Model\n\n### Evaluation\n\nSince we are predicting the label of our classes (heart disease or none), we have a much more intuitive measure of performance: prediction accuracy. To calculate this we will use `sklearn.metrics.accuracy_score` - reference here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html.","fa3db1f1":"Because we will perform feature selection later, this time let's take the time to make a convenient function to separate a dataset into `train` and `test` and also split $x$ and $y$ in boths sets for us. We do this for you.","802e7408":"# Logistic Regression Tutorial","718fc5fc":"## Dataset\n\nA retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa. There are roughly two controls per case of coronary heart disease (CHD). Many of the CHD positive men have undergone blood pressure reduction treatment and other programs to reduce their risk factors after their CHD event. In some cases the measurements were made after these treatments. These data are taken from a larger dataset, described in  Rousseauw et al, 1983, South African Medical Journal.  Downloaded from https:\/\/web.stanford.edu\/~hastie\/ElemStatLearn\/.\n\nFeatures:\n- sbp: systolic blood pressure\n- tobacco:\tcumulative tobacco (kg)\n- ldl: low densiity lipoprotein cholesterol\n- adiposity\n- famhist: family history of heart disease (Present, Absent)\n- typea: type-A behavior\n- obesity\n- alcohol: current alcohol consumption\n- age: age at onset\n- chd: response, coronary heart disease","eab7b647":"After defining your feature set and grid space, run the cell below to get your best result. Be patient it may take a while, especially if your grid space is big!"}}