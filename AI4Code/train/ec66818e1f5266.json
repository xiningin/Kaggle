{"cell_type":{"eb9e634e":"code","d0c3aff7":"code","7dc2aab4":"code","fa1f7ee8":"code","7956b9a5":"code","4fd34141":"code","d24fd512":"code","85389cfc":"code","abda348e":"code","32eac282":"code","370a8e6f":"code","87db0f82":"code","697229bf":"code","be423638":"code","1d0f01dd":"code","dee27c40":"code","494fd65d":"code","69a4ab31":"code","b7ebbbf8":"code","88d60746":"code","18df448c":"code","444ded78":"code","7f31f3d1":"code","7f5381cf":"code","45ef3bea":"code","2983aecf":"code","84d4f125":"code","f271a342":"code","9d7f7567":"code","09c87196":"code","12da3a40":"code","f8765126":"code","d35a32e1":"code","8db8f577":"code","6f0c5ce1":"code","8381da69":"code","4597d1c1":"code","df2ecb40":"code","40f02125":"code","12ad37a5":"code","0f76753e":"code","c2f468e7":"code","1c8278ed":"code","c077f9e6":"code","7f8f9d8f":"code","fcc58f6c":"code","77de8477":"code","dd5e0678":"code","f69a4abb":"code","3f24e1b9":"code","81bba2b3":"code","a558effd":"code","ed87cdef":"code","9ddc6e9c":"code","25ea323a":"code","e5462d26":"code","e0ced2f2":"code","2f1bddfb":"code","458b0836":"code","d9c559cf":"code","1400792e":"code","2a95f65e":"code","a7163acc":"code","f5c4613f":"code","bd1dba64":"code","58aec9c7":"code","aa8ae91a":"code","515802a7":"code","2083a2b4":"markdown","555a4cf8":"markdown","5811e0f4":"markdown","45cc1168":"markdown","95522b0d":"markdown","98555599":"markdown","31670356":"markdown","d6358ba9":"markdown","faa234ff":"markdown","b5391f65":"markdown","5b7c42b6":"markdown","c2d69c38":"markdown","d297864b":"markdown","fc44e8f0":"markdown","cce38259":"markdown","b42d45df":"markdown","3320c066":"markdown","94f33fca":"markdown","63e02216":"markdown","a459d561":"markdown"},"source":{"eb9e634e":"# Python 3 environment\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d0c3aff7":"# Dowload train data\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_data.head()","7dc2aab4":"# Dowload test data\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head()","fa1f7ee8":"# Copy data\ntrain_df = train_data.copy()\ntest_df = test_data.copy()","7956b9a5":"# # Copy raw_train_data DataFrame and show description\n# train_df.describe()","4fd34141":"# # Count missing values\n# train_df.isna().sum()","d24fd512":"# test_df.isna().sum()","85389cfc":"# # Plot a heat map to see the correlation between the parameters and the target variable (Survived)\n# heatmap = sns.heatmap(train_df[['Survived', 'SibSp', 'Parch', 'Age', 'Fare']].corr(), annot = True)\n# sns.set(rc={'figure.figsize':(7,5)})\n# # Fare value of correlation (0.26) with the Survived shows the more the fare is, the more are the chances of survival","abda348e":"# # Correlation between Fare and Surviving\n# import matplotlib.pyplot as plt \n# plt.figure(figsize=(25, 7))\n# plt.hist([train_df[train_df['Survived']==1]['Fare'], train_df[train_df['Survived']==0]['Fare']], \n#          stacked=True, color=['purple','blue'],\n#          bins=30, label=['Survived', 'Died']\n#         )\n# plt.xlabel('Fare')\n# plt.ylabel('Number of passenger')\n# plt.legend()","32eac282":"# # Correlation between number of siblings\/spouse and Surviving\n# train_df['SibSp'].unique()\n# bargraph_sibsp = sns.catplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind=\"bar\", height=5)\n# # Passengers having 1 or 2 siblings have good chances of survival","370a8e6f":"# # Correlation between Parch and Surviving\n# train_df['Parch'].unique()\n# bargraph_parch = sns.catplot(x = \"Parch\", y = \"Survived\", data = train_df, kind=\"bar\", height=5)\n# # Passengers having 3 members of close family have better chnces of survival","87db0f82":"# # Correlation between Age and Surviving\n# ageplot = sns.FacetGrid(train_df, col=\"Survived\", height=5)\n# ageplot = ageplot.map(sns.distplot, \"Age\")\n# ageplot = ageplot.set_ylabels(\"Survival Probability\")\n# # Higher age means less chances of survival","697229bf":"# # Correlation between Gender and Surviving\n# plt.figure(figsize=(7, 5))\n# plt.hist([train_df[train_df['Survived']==1]['Sex'], train_df[train_df['Survived']==0]['Sex']], \n#          stacked=True, color=['purple','blue'],\n#          bins=3, label=['Survived', 'Died']\n#         )\n# plt.xlabel('Sex')\n# plt.ylabel('Number of Survived')\n# plt.legend();\n# # Women had more chances of survival","be423638":"# # Correlation between economic class (Pclass) and Surviving\n# pclassplot = sns.catplot(x = \"Pclass\", y=\"Survived\", data = train_df, kind=\"bar\", height=4)\n# # Higher Pclass higher chances of survival","1d0f01dd":"# Fill missing value with random between meen-std and mean_std\nfor df in [train_df, test_df]:\n    mean = df[\"Age\"].mean()\n    std = df[\"Age\"].std()\n    is_null = df[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = df[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    df[\"Age\"] = age_slice\n    df[\"Age\"] = df[\"Age\"].astype(int)","dee27c40":"# Fill missing value with most common\nembarked_mode = train_df['Embarked'].mode()\nfor df in [train_df, test_df]:\n    df['Embarked'] = df['Embarked'].fillna(embarked_mode)","494fd65d":"# Fill missing value with mean \nfor df in [train_df, test_df]:\n    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())","69a4ab31":"# Combine SibSp, Parch (siblings\/spouse\/parents\/children) into new feature\nfor df in [train_df, test_df]:\n    df['Relatives'] = df['SibSp']+df['Parch']","b7ebbbf8":"# Calculate fare per person\nfor df in [train_df, test_df]:\n    df['PersonFare'] = df['Fare']\/(1+df['Relatives'])","88d60746":"# Turn Cabin number into Deck \ndecks = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', np.nan]\n\ndef substring_in(inp, sub):\n    for s in sub:\n        if str(inp) == str(s) or str(inp).find(s) != -1:\n            return s\n    return\n\nfor df in [train_df, test_df]:                                        \n    df['Deck'] = df['Cabin'].map(lambda x: substring_in(x, decks))","18df448c":"# Drop not used columns\nfor df in [train_df, test_df]:\n    df = df.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin'], axis=1, inplace=True)\ntrain_df.head()","444ded78":"from sklearn.preprocessing import StandardScaler\n\nfeatures_to_scale = ['Age', 'PersonFare']\ntrain_scaled, test_scaled = train_df.copy(), test_df.copy()\nfor df in [train_scaled, test_scaled]:\n    scaler = StandardScaler()\n    scaler.fit(df[features_to_scale])\n    df[features_to_scale] = scaler.transform(df[features_to_scale])","7f31f3d1":"encode_col_list = list(train_scaled.select_dtypes(include=['object']).columns)\nencode_col_list.extend(train_scaled[['Pclass', 'Relatives']])\nfor i in encode_col_list:\n    train_scaled = pd.concat([train_scaled,pd.get_dummies(train_scaled[i], prefix=i)],axis=1)\n    train_scaled.drop(i, axis = 1, inplace=True)\ntrain_scaled.head()","7f5381cf":"# train_scaled.columns.values","45ef3bea":"encode_col_list = list(test_scaled.select_dtypes(include=['object']).columns)\nencode_col_list.extend(test_scaled[['Pclass', 'Relatives']])\nfor i in encode_col_list:\n    test_scaled = pd.concat([test_scaled, pd.get_dummies(test_scaled[i], prefix=i)],axis=1)    \n    test_scaled.drop(i, axis = 1, inplace=True)\ntest_scaled.head()","2983aecf":"# test_scaled.columns.values","84d4f125":"test_scaled = test_scaled.T.reindex(train_scaled.columns.values).T.fillna(0)\ntest_scaled.columns.values","f271a342":"died = train_scaled.query('Survived == 0')\nsurvived = train_scaled.query('Survived == 1')\ndied['Survived'].value_counts(), survived['Survived'].value_counts()","9d7f7567":"balanced_df = pd.concat([survived, died.sample(len(survived), random_state=42)])\nbalanced_df['Survived'].value_counts()","09c87196":"from sklearn.model_selection import train_test_split\n\n# Randomize data\nbalanced_df = balanced_df.sample(frac=1, random_state=42)\nfeatures = balanced_df.columns[1:]\ntarget = 'Survived'\n\n# Dividing train data into train and validation data\nx_train, x_val, y_train, y_val = train_test_split(balanced_df[features], balanced_df[target].to_numpy().astype('int64'), train_size=0.8)\nx_train.shape, y_train.shape, x_val.shape, y_val.shape","12da3a40":"# Import classification report\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer, auc, roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","f8765126":"# from sklearn.linear_model import LogisticRegression\n\n# # Default LogisticRegression parameters\n# logistic_model = LogisticRegression(random_state=42)\n# logistic_model.fit(x_train, y_train)\n\n# # Returning classification report on logistic regression model prediction\n# print(classification_report(y_val, logistic_model.predict(x_val), target_names = ['Died', 'Survived']))","d35a32e1":"# # Plot features by importance\n# def feature_importance(model):\n#     feat_imp = abs(model.coef_[0])\n#     feat_imp = pd.Series(feat_imp, features).sort_values(ascending=False)\n#     feat_imp.plot(kind='bar', title='Feature Importances')\n#     plt.ylabel('Feature Importance Score')","8db8f577":"# # Find optimal hyperparameters for LogisticRegression\n# lm_params_grid = {'C': np.logspace(-4, 4, 20),\n#                   'class_weight': [None],\n#                   'dual': [False],\n#                   'fit_intercept': [True],\n#                   'intercept_scaling': [1],\n#                   'l1_ratio': [None],\n#                   'max_iter': [100, 1000, 2500, 5000], \n#                   'multi_class': ['auto'],\n#                   'n_jobs': [None],\n#                   'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n#                   'random_state': [42],\n#                   'solver': ['newton-cg','lbfgs','liblinear','sag','saga'],\n#                   'tol': [0.0001],\n#                   'verbose': [0],\n#                   'warm_start': [False]\n#                  }\n\n# lm_grid = GridSearchCV(estimator=LogisticRegression(),\n#                        param_grid=lm_params_grid,\n#                        cv=3,     \n#                        verbose=0,\n#                        n_jobs=-1\n#                       )\n\n# lm_grid.fit(x_train, y_train)","6f0c5ce1":"# lm_grid.best_params_","8381da69":"# lm_param_tuned = LogisticRegression().set_params(**lm_grid.best_params_)\n# lm_param_tuned.fit(x_train, y_train)\n\n# # Returning classification report on logistic regression model prediction\n# print(classification_report(y_val, lm_param_tuned.predict(x_val), target_names = ['Died', 'Survived']))","4597d1c1":"# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Activation\n# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n# from tensorflow.keras.optimizers import Adam\n# from kerastuner.tuners import RandomSearch\n# from kerastuner.engine.hyperparameters import HyperParameters\n# import time","df2ecb40":"# shallow_nn = Sequential()                                                \n# shallow_nn.add(InputLayer((x_train.shape[1],)))\n# shallow_nn.add(Dense(1, 'relu'))\n# shallow_nn.add(BatchNormalization())\n# shallow_nn.add(Dense(1, 'sigmoid'))\n\n# checkpoint = ModelCheckpoint('shallow_nn', save_best_only=True)\n# shallow_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# # shallow_nn.summary()","40f02125":"# shallow_nn.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20)","12ad37a5":"# def nn_prediction(model, x):\n#     return (model.predict(x).flatten() > 0.5).astype(int)\n# nn_prediction(shallow_nn, x_val)\n\n# # Returning classification report on shallow neural net model prediction\n# print(classification_report(y_val, nn_prediction(shallow_nn, x_val), target_names = ['Died', 'Survived']))","0f76753e":"# log_dir = f'{int(time.time())}'\n\n# def build_model(hp):\n#     model = Sequential()    \n#     model.add(InputLayer((x_train.shape[1],)))\n    \n#     # Tune the number of units in the first Dense layer\n#     hp_units = hp.Int('units', min_value=32, max_value=256, step=32)\n#     model.add(Dense(units=hp_units, activation='relu')) \n    \n#     # Tune the learning rate for the optimizer (0.01, 0.001, 0.0001)\n#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    \n#     for i in range(hp.Int('n_layers', 1, 3)):\n#         model.add(Dense(units=hp_units, activation='relu'))\n        \n#     model.add(Dense(1, 'sigmoid'))\n   \n#     model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n#                   loss='binary_crossentropy',\n#                   metrics=['accuracy'])    \n#     return model\n    \n# tuner = RandomSearch(build_model,\n#                      objective='val_accuracy',\n#                      max_trials=1,\n#                      executions_per_trial=3,\n#                      directory=log_dir\n#                     )\n\n# stop_early = EarlyStopping(monitor='val_loss', patience=5)\n\n# tuner.search(x=x_train, y=y_train, epochs=1, batch_size=64, validation_data=(x_val,y_val))","c2f468e7":"# # Get the optimal hyperparameters\n# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n# print(f\"\"\"\n# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n# layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n# is {best_hps.get('learning_rate')}.\n# \"\"\")","1c8278ed":"# # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n# model = tuner.hypermodel.build(best_hps)\n# history = model.fit(x=x_train, y=y_train, epochs=50, validation_data=(x_val,y_val))","c077f9e6":"# val_acc_per_epoch = history.history['val_accuracy']\n# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n# print(f'Best epoch: {best_epoch}')","7f8f9d8f":"# hypermodel = tuner.hypermodel.build(best_hps)\n# # Retrain the model\n# hypermodel.fit(x_train, y_train, epochs=best_epoch, validation_data=(x_val,y_val))","fcc58f6c":"# nn_prediction(hypermodel, x_val)\n\n# # Returning classification report on shallow neural net model prediction\n# print(classification_report(y_val, nn_prediction(hypermodel, x_val), target_names = ['Died', 'Survived']))","77de8477":"# from sklearn.svm import LinearSVC\n\n# # Default LinearSVC parameters \n# # svc = LinearSVC(class_weight='balanced', random_state=42)\n# svc = LinearSVC(random_state=42)\n# svc.fit(x_train, y_train)\n\n# # Returning classification report on linear svc model prediction\n# print(classification_report(y_val, svc.predict(x_val), target_names = ['Died', 'Survived']))","dd5e0678":"# svc.get_params(deep=True)","f69a4abb":"# # Find optimal hyperparameters for LinearSVC\n# svc_params_grid = {'C': np.logspace(-4, 4, 20),\n#                    'class_weight': ['balanced'],\n#                    'dual': [False, True],\n#                    'fit_intercept': [True],\n#                    'intercept_scaling': [1],\n#                    'loss': ['hinge', 'squared_hinge'],\n#                    'max_iter': [100, 1000, 2500, 5000],\n#                    'multi_class': ['ovr'],\n#                    'penalty': ['l2'],\n#                    'random_state': [42],\n#                    'tol': [0.0001],\n#                    'verbose': [0]\n#                   }\n\n# svc_grid = GridSearchCV(estimator=LinearSVC(),\n#                         param_grid=svc_params_grid,\n#                         cv=3,     \n#                         verbose=0,\n#                         n_jobs=-1\n#                        )\n\n# svc_grid.fit(x_train, y_train)","3f24e1b9":"# svc_grid.best_params_","81bba2b3":"# svc_param_tuned = LinearSVC().set_params(**svc_grid.best_params_)\n# svc_param_tuned.fit(x_train, y_train)\n\n# # Returning classification report on linear svc model prediction\n# print(classification_report(y_val, svc_param_tuned.predict(x_val), target_names = ['Died', 'Survived']))","a558effd":"# import all required packages\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# dictionary of models {model name:[algorithm, parameters grid]}\nmodels = {'gbc': [GradientBoostingClassifier(), {'ccp_alpha': [0.0],\n                                                 'criterion': ['friedman_mse'],\n                                                 'init': [None],\n                                                 'learning_rate': [0.1, 1.0],\n                                                 'loss': ['deviance'],\n                                                 'max_depth': [1, 3],\n                                                 'max_features': [None],\n                                                 'max_leaf_nodes': [None],\n                                                 'min_impurity_decrease': [0.0],\n                                                 'min_impurity_split': [None],\n                                                 'min_samples_leaf': [1],\n                                                 'min_samples_split': [2],\n                                                 'min_weight_fraction_leaf': [0.0],\n                                                 'n_estimators': [100],\n                                                 'n_iter_no_change': [None],\n                                                 'presort': ['deprecated'],\n                                                 'random_state': [42],\n                                                 'subsample': [1.0],\n                                                 'tol': [0.0001],\n                                                 'validation_fraction': [0.1],\n                                                 'verbose': [0],\n                                                 'warm_start': [False]}]\n         }\n\nclass Model:    \n    def __init__(self, x_train, y_train, x_val, y_val, name, model, params_grid):\n        self.x_train, self.y_train = x_train, y_train\n        self.x_val, self.y_val = x_val, y_val\n        self.name = name\n        self.model = model\n        self.params_grid = params_grid        \n        \n    def model_perform(self, model):\n        accuracy = accuracy_score(self.y_val, model.predict(self.x_val))\n        report = classification_report(self.y_val, model.predict(self.x_val), target_names = ['Died', 'Survived'])\n        params = model.get_params(deep=True)\n        return accuracy, report, params                \n              \n    # returns given model report: accuracy, classification report, parameters\n    def model_report(self, model):\n        accuracy, report, params = self.model_perform(model)\n        return f'{self.model} \\n\\n Accuracy: {accuracy} \\n\\n Classification report: \\n {report} \\n\\n Parameters: \\n {params}'\n        \n    # comparing model with default parameters and hyperparameters tuned model\n    def comparison_report(self, full=False):\n        default = self.default_params_model()\n        hypermode = self.hyperparameters_tuner()\n        d_accuracy, d_report, d_params = self.model_perform(default)\n        h_accuracy, h_report, h_params = self.model_perform(hypermode)\n        report = f'{self.model} default parameters model accuracy {d_accuracy} vs. {h_accuracy} hyperparameters tuned model \\n\\n'\n        if full:\n            report += 'Default parameters '+self.model_report(default)+'\\n\\n vs. \\n\\n Hyperparameters tuned '+self.model_report(hypermode)\n        return report\n        \n    # fits default parameters model\n    def default_params_model(self):\n        default = self.model.fit(self.x_train, self.y_train)        \n        return default\n            \n    # best hyperparameters cross validation grid search\n    def grid_search(self):      \n        model_grid = GridSearchCV(estimator=self.model, param_grid=self.params_grid, cv=3, verbose=0,  n_jobs=-1)\n        model_grid.fit(self.x_train, self.y_train)         \n        best_params = model_grid.best_params_\n        return best_params\n    \n    # sets best performed hyperparameters into a model, fits the model\n    def hyperparameters_tuner(self):\n        model_param_tuned = self.model.set_params(**self.grid_search())        \n        model_param_tuned.fit(self.x_train, self.y_train)\n        return model_param_tuned\n    \n    # returns hypermodel, prints accuracy\n    def hypermodel(self):\n        hypermodel = self.hyperparameters_tuner()\n        print(f'{self.model} hypermodel accuracy is {self.model_perform(hypermodel)[0]} \\n')\n        return hypermodel\n\n\nfor i in models:\n    name = i\n    algorithm = models[i][0]\n    params_grid = models[i][1]\n    model = Model(x_train, y_train, x_val, y_val, name, algorithm, params_grid)\n    hypermodel = model.hypermodel()\n    report = model.model_report(hypermodel)   \n    print(report)\n","ed87cdef":"# from sklearn.ensemble import GradientBoostingClassifier\n\n# # Default GradientBoostingClassifier parameters \n# gbc = GradientBoostingClassifier(random_state=42)\n# gbc.fit(x_train, y_train)\n\n# # Returning classification report on gradient booster model prediction\n# print(classification_report(y_val, gbc.predict(x_val), target_names = ['Died', 'Survived']))","9ddc6e9c":"# gbc.get_params(deep=True)","25ea323a":"# # Find optimal hyperparameters for GradientBoostingClassifier\n \n# gbc_params_grid = {'ccp_alpha': [0.0],\n#                    'criterion': ['friedman_mse'],\n#                    'init': [None],\n#                    'learning_rate': [0.1, 1.0],\n#                    'loss': ['deviance'],\n#                    'max_depth': [1, 3],\n#                    'max_features': [None],\n#                    'max_leaf_nodes': [None],\n#                    'min_impurity_decrease': [0.0],\n#                    'min_impurity_split': [None],\n#                    'min_samples_leaf': [1],\n#                    'min_samples_split': [2],\n#                    'min_weight_fraction_leaf': [0.0],\n#                    'n_estimators': [100],\n#                    'n_iter_no_change': [None],\n#                    'presort': ['deprecated'],\n#                    'random_state': [42],\n#                    'subsample': [1.0],\n#                    'tol': [0.0001],\n#                    'validation_fraction': [0.1],\n#                    'verbose': [0],\n#                    'warm_start': [False]\n#                    }\n\n# gbc_grid = GridSearchCV(estimator=GradientBoostingClassifier(),\n#                         param_grid=gbc_params_grid,\n#                         cv=3,     \n#                         verbose=0,\n#                         n_jobs=-1\n#                        )\n\n# gbc_grid.fit(x_train, y_train)","e5462d26":"# gbc_grid.best_params_","e0ced2f2":"# gbc_param_tuned = GradientBoostingClassifier().set_params(**gbc_grid.best_params_)\n# gbc_param_tuned.fit(x_train, y_train)\n\n# # Returning classification report on gradient booster model prediction\n# print(classification_report(y_val, gbc_param_tuned.predict(x_val), target_names = ['Died', 'Survived']))","2f1bddfb":"# from sklearn.ensemble import RandomForestClassifier\n\n# # Default RandomForestClassifier parameters\n# rf = RandomForestClassifier(random_state=42)\n# rf.fit(x_train, y_train)\n\n# # Returning classification report on random forest model prediction\n# print(classification_report(y_val, rf.predict(x_val), target_names = ['Died', 'Survived']))","458b0836":"# rf.get_params(deep=True)","d9c559cf":"# # Find optimal hyperparameters for RandomForestClassifier\n\n# rf_params_grid = {'bootstrap': [True],\n#                   'ccp_alpha': [0.0],\n#                   'class_weight': [None],\n#                   'criterion': ['gini'],\n#                   'max_depth': [5],\n#                   'max_features': ['auto'],\n#                   'max_leaf_nodes': [None],\n#                   'max_samples': [None],\n#                   'min_impurity_decrease': [0.0],\n#                   'min_impurity_split': [None],\n#                   'min_samples_leaf': [1],\n#                   'min_samples_split': [2],\n#                   'min_weight_fraction_leaf': [0.0],\n#                   'n_estimators': [100],\n#                   'n_jobs': [-1],\n#                   'oob_score': [False],\n#                   'random_state': [42],\n#                   'verbose': [0],\n#                   'warm_start': [False]\n#                  }\n\n# rf_grid = GridSearchCV(estimator=GradientBoostingClassifier(),\n#                        param_grid=rf_params_grid,\n#                        cv=3,     \n#                        verbose=0,\n#                        n_jobs=-1\n#                       )\n\n# rf_grid.fit(x_train, y_train)","1400792e":"# rf_grid.best_params_","2a95f65e":"# rf_param_tuned = RandomForestClassifier().set_params(**rf_grid.best_params_)\n# rf_param_tuned.fit(x_train, y_train)\n\n# # Returning classification report on random forest model prediction\n# print(classification_report(y_val, rf_param_tuned.predict(x_val), target_names = ['Died', 'Survived']))","a7163acc":"# from xgboost import XGBClassifier\n\n# # Default XGBClassifier parameters    \n# xgb = XGBClassifier(eval_metric=['mlogloss'], use_label_encoder=False, random_state=42)\n# xgb.fit(x_train, y_train)\n\n# # Returning classification report on random forest model prediction\n# print(classification_report(y_val, xgb.predict(x_val), target_names = ['Died', 'Survived']))","f5c4613f":"# xgb.get_params(deep=True)","bd1dba64":"# # Find optimal hyperparameters for XGBClassifier\n\n# xgb_params_grid = {'objective': ['binary:logistic'],\n#                    'use_label_encoder': [False],\n#                    'base_score': [0.5],\n#                    'booster': ['gbtree'],\n#                    'colsample_bylevel': [1],\n#                    'colsample_bynode': [1],\n#                    'colsample_bytree': [1],\n#                    'gamma': [0],\n#                    'gpu_id': [-1],\n#                    'importance_type': ['gain'],\n#                    'interaction_constraints': [''],\n#                    'learning_rate': [0.300000012],\n#                    'max_delta_step': [0],\n#                    'max_depth': [6],\n#                    'min_child_weight': [1],\n#                    'missing': [nan],\n#                    'monotone_constraints': ['()'],\n#                    'n_estimators': [100],\n#                    'n_jobs': [-1],\n#                    'num_parallel_tree': [1],\n#                    'random_state': [42],\n#                    'reg_alpha': [0],\n#                    'reg_lambda': [1],\n#                    'scale_pos_weight': [1],\n#                    'subsample': [1],\n#                    'tree_method': ['exact'],\n#                    'validate_parameters': [1],\n#                    'verbosity': [None],\n#                    'eval_metric': ['mlogloss']\n#                    }\n\n# xgb_grid = = GridSearchCV(estimator=XGBClassifier(),\n#                           param_grid=xgb_params_grid,\n#                           cv=3,     \n#                           verbose=0,\n#                           n_jobs=-1\n#                          )\n\n# xgb_grid.fit(x_train, y_train)","58aec9c7":"# xgb_grid.best_params_","aa8ae91a":"# xgb_param_tuned = XGBClassifier().set_params(**xgb_grid.best_params_)\n# xgb_param_tuned.fit(x_train, y_train)\n\n# # Returning classification report on random forest model prediction\n# print(classification_report(y_val, xgb_param_tuned.predict(x_val), target_names = ['Died', 'Survived']))","515802a7":"# # hypermodel = svc_param_tuned\n\n# x_test = test_scaled[test_scaled.columns[1:]]\n# # predictions = nn_prediction(hypermodel, x_test)\n# predictions = hypermodel.predict(x_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","2083a2b4":"# Feature engineering","555a4cf8":"## LinearSVC","5811e0f4":"# Building prediction models","45cc1168":"## RandomForestClassifier","95522b0d":"### Missing fare values","98555599":"### Run model on test data and save in output as submission.csv","31670356":"# Exploring data","d6358ba9":"# Balancing train data","faa234ff":"### LogisticRegression hyperparameters tuning","b5391f65":"# Transform categorical variables into dummies","5b7c42b6":"# Data processing","c2d69c38":"# Scale numerical data","d297864b":"# Dividing data into input and output\/ train and validation","fc44e8f0":"### Sequential hyperparameters tuning","cce38259":"## LogisticRegression","b42d45df":"## GradientBoostingClassifier","3320c066":"### Missing embarked values","94f33fca":"## Sequential (shallow neural net)","63e02216":"### Missing age values","a459d561":"# eXtreeme Gradient Boosting"}}