{"cell_type":{"ef3f4936":"code","3f73c4d2":"code","000a164e":"code","5e31b1f3":"code","ea0d1998":"code","48db14b1":"code","293f2cd9":"code","287218fa":"code","8a489b2d":"code","c947999c":"code","09e0c0fc":"code","5fcac55e":"markdown","dc22a11e":"markdown","933a0cf7":"markdown","95dc5710":"markdown","57525b5c":"markdown","82e13096":"markdown","0f004dea":"markdown","5b91b2ae":"markdown","5dea5f64":"markdown","1d3d263f":"markdown"},"source":{"ef3f4936":"# install cocoapi package\n!pip install git+https:\/\/github.com\/gautamchitnis\/cocoapi.git@cocodataset-master#subdirectory=PythonAPI\n# clone sourcecode from github\n!git clone https:\/\/github.com\/pytorch\/vision.git\n!cp vision\/references\/detection\/* .\/","3f73c4d2":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torchvision\nfrom torchvision import transforms as tf\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n# 5. Main Function & Train\nimport transforms as T\nfrom engine import train_one_epoch, evaluate\nimport utils","000a164e":"class PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n        # convert the PIL Image into a numpy array\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # convert everything into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","5e31b1f3":"# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","ea0d1998":"# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel = FasterRCNN(backbone,\n                   num_classes=2,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)","48db14b1":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model\n\nmodel = get_model_instance_segmentation(2)","293f2cd9":"def get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\ndataset = PennFudanDataset('..\/input\/pennfudanped\/PennFudanPed', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\n# For Training\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)   # Returns losses and detections\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions","287218fa":"def main():\n    \n    # Select CPU | GPU\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # our dataset has two classes only - background and person\n    num_classes = 2\n    \n    # use our dataset and defined transformations\n    dataset = PennFudanDataset('..\/input\/pennfudanped\/PennFudanPed', get_transform(train=True))\n    dataset_test = PennFudanDataset('..\/input\/pennfudanped\/PennFudanPed', get_transform(train=False))\n\n    # split the dataset in train and test set\n    # Generate numbers for shuffle\n    indices = torch.randperm(len(dataset)).tolist()\n    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n    # define training and validation data loaders\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=2, shuffle=True, num_workers=4,\n        collate_fn=utils.collate_fn)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n        collate_fn=utils.collate_fn)\n\n    # get the model using our helper function\n    model = get_model_instance_segmentation(num_classes)\n\n    # move model to the right device\n    model.to(device)\n\n    # construct an optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n    # and a learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=3,\n                                                   gamma=0.1)\n\n    # let's train it for 10 epochs\n    num_epochs = 2\n\n    for epoch in range(num_epochs):\n        # train for one epoch, printing every 10 iterations\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n        evaluate(model, data_loader_test, device=device)\n\n    print(\"That's it!\")\n\n# run main function\nmain()","8a489b2d":"# convert sample image to tensor\ntmp = plt.imread('..\/input\/pennfudanped\/PennFudanPed\/PNGImages\/FudanPed00005.png')\n# pred = model(tmp_t)\n# plt.imshow(tmp)\nprint('origin img shape : ',tmp.shape)\n\ntmp_t = tf.ToTensor()(tmp)\ntmp_t = tmp_t.unsqueeze(0)\nprint(tmp_t.size())","c947999c":"# prediction\nmodel.eval()\npred = model(tmp_t)\npred","09e0c0fc":"w = int(tmp.shape[0])\nh = int(tmp.shape[1])\ncoord = pred[0]['boxes'][0].detach().numpy().astype(int)\nprint(coord)\n# (x1, y1) (x2, y2)\nbox = cv2.rectangle(tmp,(coord[0],coord[1]),(coord[2],coord[3]),(255,0,0),1)\n\nfig, ax = plt.subplots(1,2,figsize=(10,5))\nax[0].imshow(tmp)\nax[1].imshow(box)","5fcac55e":"> ---\n> ### 3-1. Finetuning from a Pretrained Model\n> - Detection Model : Faster-RCNN ResNet50 + FPN","dc22a11e":"---\n# 5. Validation","933a0cf7":"> ---\n> ### 3-3. Instance Segmentation Model","95dc5710":"---\n# 3. Define Model","57525b5c":"---\n# 4. Main Function (Train)","82e13096":"> ---\n> ### 1-1. Install\/Import  packages & download source code","0f004dea":"---\n# 2. Load Dataset","5b91b2ae":"---\n# 1. Env Setting","5dea5f64":"> ---\n> ### 3-2. Modifying the model to add a different backbone","1d3d263f":"# Contents\n - [1. Env Setting](#1.-Env-Setting)\n   - 1-1. Install\/Import  packages & download source code\n   - 1-2. Load Dataset \n - [2. Load Dataset](#2.-Load-Dataset)\n - [3. Define Model](#3.-Define-Model)\n - [4. Main Function & Train](#4.-Main-Function-&-&Train)\n - [5. Validation](#5.-Validation)\n \n ref : https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html"}}