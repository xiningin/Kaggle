{"cell_type":{"b234d2b7":"code","f8f60324":"code","95bdcb86":"code","b94e7682":"code","c215af3a":"code","5901aeff":"code","e83e4161":"code","aeef2290":"code","23f10d28":"code","c095e444":"code","5de71e51":"code","cc952bcb":"code","f743feaf":"code","def2a450":"code","540f2b3a":"code","f26add57":"code","2accff8b":"code","7c55ef18":"code","1578ec3d":"code","6e2d5e14":"code","39f78c29":"code","61221fa3":"code","e88e19af":"markdown","2a56a181":"markdown","d84c6004":"markdown","7b38abfa":"markdown","5824c7d2":"markdown","cc0413c4":"markdown","e123f4d5":"markdown","030eb1ca":"markdown","92d18eb1":"markdown","afb8048b":"markdown","7cb5a707":"markdown"},"source":{"b234d2b7":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n","f8f60324":"\n# Importing the dataset\ndataset = pd.read_csv('..\/input\/Social_Network_Ads.csv')","95bdcb86":"#Checking the dataset\ndataset.head()","b94e7682":"#Check the metadata\ndataset.info()","c215af3a":"#Chceking the null values\ndataset.isnull().sum()","5901aeff":"#Check its dimensions\ndataset.shape","e83e4161":"#Plot UserID vs Purchased............\nx1 = dataset.iloc[:, 0].values\ny1 = dataset.iloc[:, 4].values\nplt.scatter(x1,y1,color='Orange',s=50)\nplt.xlabel('UserID')\nplt.ylabel('Purchased')\nplt.title('UserID vs Purchased')\nplt.show()","aeef2290":"#Plot Gender vs Purchased............\nx1 = dataset.iloc[:, 1].values\ny1 = dataset.iloc[:, 4].values\nplt.scatter(x1,y1,color='pink',s=50)\nplt.xlabel('Gender')\nplt.ylabel('Purchased')\nplt.title('Gender vs Purchased')\nplt.show()","23f10d28":"#Plot Age vs Purchased............\nx1 = dataset.iloc[:, 2].values\ny1 = dataset.iloc[:, 4].values\nplt.scatter(x1,y1,color='purple',s=50)\nplt.xlabel('Age')\nplt.ylabel('Purchased')\nplt.title('Age vs Purchased')\nplt.show()","c095e444":"#Plot Estimatedsalary vs Purchased............\nx1 = dataset.iloc[:, 3].values\ny1 = dataset.iloc[:, 4].values\nplt.scatter(x1,y1,color='red',s=50)\nplt.xlabel('Estimatedsalary')\nplt.ylabel('Purchased')\nplt.title('Estimatedsalary vs Purchased')\nplt.show()","5de71e51":"#Headmap:-To see the correlation between them!\nimport seaborn as sns\nplt.figure(figsize=(7,4)) #7 is the size of the width and 4 is parts.... \nsns.heatmap(dataset.corr(),annot=True,cmap='cubehelix_r') ","cc952bcb":"#Seperating dependent and indepndent values\nX = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values\n","f743feaf":"print(X)","def2a450":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n","540f2b3a":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","f26add57":"# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","2accff8b":"\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n","7c55ef18":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","1578ec3d":"#Accuray=(TN+TP)\/Total+\nAccuracy=(74+31)\/120\nAccuracy","6e2d5e14":"#Error_rate=(FN+FP)\/Total\nError_rate=(5+10)\/120\nError_rate","39f78c29":"\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('brown', 'yellow')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('yellow', 'brown'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n","61221fa3":"# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('blue', 'black')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('black', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","e88e19af":"**Dataset name:-** Social_network_ads.csv\n\n**About the dataset:-** Contains the following columns-\n            \n1.User ID\n\n2.Gender\n\n3.Age\n\n4.EstimatedSalary\n\n5.Purchased\n            \n            \n            \n            ","2a56a181":"# ||Logistic regression:-||#","d84c6004":"# ||GOAL||\n\n             To predict the users on the social network who on interacting with the advertisement either purchased the product or not?","7b38abfa":"**Note:-**  In Logistic regression we use Sigmoid activation function.","5824c7d2":"**Checking the correlation between each independent variable with Dependent variable!**","cc0413c4":"![](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/sigmoid.png)","e123f4d5":"**|| Introduction:- ||**\n\nLogistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n\n**|| Comparison to linear regression:- ||**\n\nGiven data on time spent studying and exam scores. Linear Regression and logistic regression can predict different things:\n\n1.**Linear Regression** could help us predict the student\u2019s test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).\n\n2.**Logistic Regression** could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model\u2019s classifications.\n            \n           ","030eb1ca":"Hello everyone!!\n\nToday i am working on logistic regression which is combination of half **linear regression** and **classification**.","92d18eb1":"**|| Sigmoid activation-||**\n\n\nIn order to map predicted values to probabilities, we use the sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n\n\n                                          S(z)=1\/(1+e\u2212z)\n                                          \n                                          \n                        where \n                                                \n                            s(z) = output between 0 and 1 (probability estimate)\n                            z = input to the function (your algorithm\u2019s prediction e.g. mx + b)\n                            e = base of natural log","afb8048b":"**Graph of sigmoid activation function:-**","7cb5a707":"# ||EXERCISE||"}}