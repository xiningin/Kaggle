{"cell_type":{"c8c9a94a":"code","db4d3f5d":"code","dded1c3c":"code","98a93ced":"code","fb81ba8b":"code","b2e3a3c1":"code","e2a4bab3":"code","8cc6a4de":"code","1db298a2":"code","91eb911c":"code","d82d2485":"code","e3b11f72":"code","43afaec2":"code","e62b1884":"code","c40e1fe2":"code","272ae3a7":"code","ec30e2e7":"code","f7a18da9":"code","f61f4c81":"code","9c3d3e39":"markdown","202401ac":"markdown","d584d2a4":"markdown","dc6bb101":"markdown","ef7b7af9":"markdown"},"source":{"c8c9a94a":"from __future__ import print_function, division\nfrom builtins import range\n# Note: you may need to update your version of future\n# sudo pip install -U future\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\n\nimport keras.backend as K\nif len(K.tensorflow_backend._get_available_gpus()) > 0:\n  from keras.layers import CuDNNLSTM as LSTM\n  from keras.layers import CuDNNGRU as GRU\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","db4d3f5d":"# configuration setting\nMAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 50\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 2\n\npath = '..\/input\/'\n\nEMBEDDING_FILE=f'{path}glove6b50d\/glove.6B.50d.txt'\ncomp = 'jigsaw-toxic-comment-classification-challenge\/'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'","dded1c3c":"# load in pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(EMBEDDING_FILE) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:], dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n\n","98a93ced":"\nprint('Loading in comments...')\n\ntrain = pd.read_csv(TRAIN_DATA_FILE)\n\nsentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\npossible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[possible_labels].values\n","fb81ba8b":"# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)","b2e3a3c1":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","e2a4bab3":"# pad sequences so that we get a N x T matrix\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)","8cc6a4de":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n  if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i] = embedding_vector\n\n","1db298a2":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)\n\n","91eb911c":"print('Building model...')\n\n# create an LSTM network with a single LSTM\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\n# x = LSTM(15, return_sequences=True)(x)\nx = Bidirectional(LSTM(15, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\noutput = Dense(len(possible_labels), activation=\"sigmoid\")(x)\n\nmodel = Model(input_, output)\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer=Adam(lr=0.01),\n  metrics=['accuracy']\n)\nmodel.summary()","d82d2485":"print('Training model...')\nr = model.fit(\n  data,\n  targets,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)\n","e3b11f72":"# plot some data\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","43afaec2":"# Plotting accuracies\nplt.plot(r.history['acc'], label='acc')\nplt.plot(r.history['val_acc'], label='val_acc')\nplt.legend()\nplt.show()","e62b1884":"p = model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))\n\n","c40e1fe2":"test = pd.read_csv(TEST_DATA_FILE)","272ae3a7":"list_sentences_test = test[\"comment_text\"].fillna(\"DUMMY_VALUE\").values","ec30e2e7":"list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","f7a18da9":"X_te = pad_sequences(list_tokenized_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X_te.shape)","f61f4c81":"y_test = model.predict([X_te], batch_size=BATCH_SIZE, verbose=1)\nsubmission = pd.read_csv(f'{path}{comp}sample_submission.csv')\nsubmission[possible_labels] = y_test\nsubmission.to_csv('submission.csv', index=False)","9c3d3e39":"# Importing Libraries","202401ac":"# Setting some configuration","d584d2a4":"# Model building : Bidirectional LSTM","dc6bb101":"# Preparing text samples and their labels","ef7b7af9":"# Preparation of Embedding Matrix"}}