{"cell_type":{"133bdebc":"code","7bb89709":"code","c9297d25":"code","bdd84e88":"code","f8dd72b2":"code","b37fc041":"code","087d6425":"code","27ca9d2d":"code","432d7b2e":"code","3ccf1ed2":"code","1657c762":"code","6f816efd":"code","7b26695a":"code","58490bf9":"code","2f190712":"code","14cf92e9":"code","f1316b7e":"code","e707c466":"code","e914f043":"code","35065f4e":"code","81c72223":"code","4d46cbe1":"code","559b7121":"code","222a40ac":"code","bb8ff16b":"code","356f68ea":"code","bb301ee4":"code","e725a342":"code","d145f241":"code","70e39da4":"code","7cb0b034":"code","96d2c30e":"code","308b7cf9":"code","32d17e03":"code","d5327cce":"code","0ca66f69":"code","8ba59f51":"code","43685e88":"code","8d9aee34":"code","f3c81f9c":"code","f69fcee1":"code","4790fb44":"code","7dd217b0":"code","29585700":"code","1322b1ee":"code","6e8e1309":"code","f0016bff":"code","d58862b9":"code","6bf23411":"code","73a35bb8":"code","d30f9ee1":"code","9efb24ca":"code","fb9ac37f":"code","bdfb0e85":"code","62a431ee":"code","76ea133d":"code","8ad7ae5b":"code","41a44e46":"code","5bf973ff":"code","218dd8d5":"code","f7e4398c":"code","acf6003f":"code","63b79684":"code","1b8c2726":"code","1bb60b0a":"code","21d0ad38":"code","c85d50ad":"code","b004a505":"markdown","be7508e8":"markdown","8ce740b8":"markdown","4c4af1a7":"markdown","6ed329f0":"markdown","9114b70f":"markdown","4a13f2b2":"markdown","75c49028":"markdown","588dc349":"markdown","faa8bbce":"markdown","0904191d":"markdown","6bb3478e":"markdown","85e82eca":"markdown","f4c7891b":"markdown","07cc83fc":"markdown","0dd8768c":"markdown","0bffd5e7":"markdown","b7488a3f":"markdown","8f5c883c":"markdown","26eee493":"markdown","7e5d10df":"markdown","6197bbd8":"markdown","5762d4ac":"markdown","1552508c":"markdown","d6ac4bc8":"markdown","85a39671":"markdown","394bbd44":"markdown","8a81dd65":"markdown","327db92a":"markdown","e406d9a7":"markdown","7b941aa9":"markdown"},"source":{"133bdebc":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as pyplt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","7bb89709":"# using the SQLite Table to read data.\ncon = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite') \n\nfiltered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con) \n\n\n# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\ndef partition(x):\n    if x < 3:\n        return 0\n    return 1\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition) \nfiltered_data['Score'] = positiveNegative\nprint(\"Number of data points in our data\", filtered_data.shape)\nfiltered_data.head(3)","c9297d25":"display= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\nORDER BY ProductID\n\"\"\", con)\ndisplay.head()","bdd84e88":"sample_data=filtered_data.head(5000)","f8dd72b2":"#Sorting data according to ProductId in ascending order\nsorted_data=sample_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')","b37fc041":"#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","087d6425":"#Checking to see how much % of data still remains\n(final['Id'].size*1.0)\/(sample_data['Id'].size*1.0)*100","27ca9d2d":"display= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND Id=44737 OR Id=64422\nORDER BY ProductID\n\"\"\", con)\n\ndisplay.head()","432d7b2e":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]","3ccf1ed2":"#Before starting the next phase of preprocessing lets see the number of entries left\nprint(final.shape)\n\n#How many positive and negative reviews are present in our dataset?\nfinal['Score'].value_counts()","1657c762":"# find sentences containing HTML tags\nimport re\ni=0;\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;","6f816efd":"\nstop = set(stopwords.words('english')) \nsno = nltk.stem.SnowballStemmer('english') \n\ndef cleanhtml(sentence): \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): \n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned\nprint(stop)\nprint(sno.stem('tasty'))","7b26695a":"#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n# this\nif not os.path.isfile('final.sqlite'):\n    final_string=[]\n    all_positive_words=[] # store words from +ve reviews here\n    all_negative_words=[] # store words from -ve reviews here.\n    for i, sent in enumerate(tqdm(final['Text'].values)):\n        filtered_sentence=[]\n        #print(sent);\n        sent=cleanhtml(sent) # remove HTMl tags\n        for w in sent.split():\n            for cleaned_words in cleanpunc(w).split():\n                if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                    if(cleaned_words.lower() not in stop):\n                        s=(sno.stem(cleaned_words.lower())).encode('utf8') #snoball stemmer\n                        filtered_sentence.append(s)\n                        if (final['Score'].values)[i] == 1: \n                            all_positive_words.append(s)\n                        if(final['Score'].values)[i] == 0:\n                            all_negative_words.append(s) \n        str1 = b\" \".join(filtered_sentence) \n        final_string.append(str1)\n\n    final['CleanedText'] = final['CleanedText']=final_string \n    final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")\n    conn = sqlite3.connect('final.sqlite')\n    c=conn.cursor()\n    conn.text_factory = str\n    final.to_sql('Reviews', conn,  schema=None, if_exists='replace', \\\n                 index=True, index_label=None, chunksize=None, dtype=None)\n    conn.close()\n    \n    \n    with open('positive_words.pkl', 'wb') as f: \n        pickle.dump(all_positive_words, f)      \n    with open('negitive_words.pkl', 'wb') as f:\n        pickle.dump(all_negative_words, f)     ","58490bf9":"if os.path.isfile('final.sqlite'):\n    conn = sqlite3.connect('final.sqlite')\n    final = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", conn)\n    conn.close()\nelse:\n    print(\"Please the above cell\")","2f190712":"final_reviews = final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\nfinal_reviews.shape","14cf92e9":"#split data into train, cross validate and test \n%matplotlib inline\nimport warnings\nfrom sklearn.model_selection import train_test_split\nX = final_reviews['CleanedText']\nY = final_reviews['Score']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33, random_state=0)\nX_tr, X_cv, Y_tr, Y_cv = train_test_split(X_train, Y_train, test_size=.33, random_state=0)","f1316b7e":"print('X_train, Y_train', X_train.shape, Y_train.shape)\nprint('X_test, Y_test', X_test.shape, Y_test.shape)\nprint('X_tr, Y_tr', X_tr.shape, Y_tr.shape)\nprint('X_cv, Y_cv', X_cv.shape, Y_cv.shape)","e707c466":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\n#from sklearn import cross_validation\nfrom sklearn.metrics import f1_score","e914f043":"#find knn to simple cross validation with Brute Force and KD-Tree\nfrom sklearn.metrics import roc_auc_score\ndef KNN(X_train,X_cv,Y_train,Y_cv):\n    k = []\n    max_k = 0\n    pred_cv = []\n    pred_train = []\n    max_roc_auc=-1\n    for i in range(1,24,2):\n        knn =KNeighborsClassifier(n_neighbors=i,algorithm='brute')\n        knn.fit(X_train,Y_train)\n        probs = knn.predict_proba(X_cv)\n        prob = knn.predict_proba(X_train)\n        # keep probabilities for the positive outcome only\n        probs = probs[:, 1]\n        prob  = prob[:, 1]\n        auc_score_test = roc_auc_score(Y_cv,probs)   #find AUC score\n        auc_score_train = roc_auc_score(Y_train,prob)\n        print(i,\" ------> \",auc_score_test)\n        pred_cv.append(auc_score_test)\n        pred_train.append(auc_score_train)\n        k.append(i)\n        if(max_roc_auc<auc_score_test):\n            max_roc_auc=auc_score_test\n            max_k=i\n    #print('\\nThe optimal number of neighbors is %d.' % max_k)\n    plt.plot(k, pred_cv,'r-', label = 'CV Data')\n    plt.plot(k,pred_train,'g-', label ='Train Data')\n    plt.legend(loc='upper right')\n    plt.title(\"K v\/s Auc Score\")\n    plt.ylabel('Auc Score')\n    plt.xlabel('K')\n    plt.show()\n    # calculate roc curve\n    fpr, tpr, thresholds = roc_curve(Y_cv,probs)\n    # plot no skill\n    pyplt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    pyplt.plot(fpr, tpr, marker='.')\n    #plt.plot(k,pred_cv)\n    pyplt.title(\"Line Plot of ROC Curve\")\n    pyplt.ylabel('True Positive Rate')\n    pyplt.xlabel('False Positive Rate')\n    pyplt.show()","35065f4e":"import scikitplot.metrics as skplt\ndef testing(X_train,Y_train,X_test,Y_test,algo,optimal_k):\n    if algo == 'brute':\n        knn =KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute') \n    else:\n        knn =KNeighborsClassifier(n_neighbors=optimal_k,algorithm='kd_tree')\n    knn.fit(X_train,Y_train)\n    probs = knn.predict_proba(X_test) \n    # keep probabilities for the positive outcome only\n    probs = probs[:, 1]\n    print(\"AUC Score\",roc_auc_score(Y_test,probs))\n    # calculate roc curve\n    fpr, tpr, thresholds = roc_curve(Y_test,probs)\n    # plot no skill\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    plt.plot(fpr, tpr, marker='.')\n    plt.show\n    prediction=knn.predict(X_test)\n    skplt.plot_confusion_matrix(Y_test,prediction)\n    print(\"macro f1 score for data :\",metrics.f1_score(Y_test, prediction, average = 'macro'))\n    print(\"micro f1 scoore for data:\",metrics.f1_score(Y_test, prediction, average = 'micro'))\n    print(\"hamming loss for data:\",metrics.hamming_loss(Y_test,prediction))\n    print(\"Precision recall report for data:\\n\",metrics.classification_report(Y_test, prediction))        ","81c72223":"#BoW\ncount_vect = CountVectorizer() #in scikit-learn\nbow_train = count_vect.fit_transform(X_tr)\nprint(\"The type of count vectorizer \",type(bow_train))\nprint(\"The shape of out text BOW vectorizer \",bow_train.get_shape())\n#print(\"the number of unique words \", final_counts.get_shape()[1])\nbow_cv = count_vect.transform(X_cv)\nbow_test = count_vect.transform(X_test)\nprint(\"CV Data Size: \",bow_cv.shape)\nprint(\"Test Data Size: \",bow_test.shape)","4d46cbe1":"#Normalize Data\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import Normalizer\nbow_train=preprocessing.normalize(bow_train)\nbow_cv=preprocessing.normalize(bow_cv)\nbow_test=preprocessing.normalize(bow_test)\nprint(\"The shape of out text BOW vectorizer \",bow_train.get_shape())\nprint(\"CV Data Size: \",bow_cv.shape)\nprint(\"Test Data Size: \",bow_test.shape)","559b7121":"# find optimal k using brute force\nKNN(bow_train,bow_cv,Y_tr,Y_cv)","222a40ac":"testing(bow_train,Y_tr,bow_test,Y_test,'brute',optimal_k=9)","bb8ff16b":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\ntfidf_train = tf_idf_vect.fit_transform(X_tr)\nprint(\"The type of count vectorizer \",type(tfidf_train))\nprint(\"The shape of out text TFIDF vectorizer \",tfidf_train.get_shape())\ntfidf_cv = tf_idf_vect.transform(X_cv)\ntfidf_test = tf_idf_vect.transform(X_test)\nprint(\"CV Data Size: \",tfidf_cv.shape)\nprint(\"Test Data Size: \",tfidf_test.shape)","356f68ea":"#Normalize Data\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import Normalizer\ntfidf_train=preprocessing.normalize(tfidf_train)\ntfidf_cv=preprocessing.normalize(tfidf_cv)\ntfidf_test=preprocessing.normalize(tfidf_test)\nprint(\"The shape of out text BOW vectorizer \",tfidf_train.get_shape())\nprint(\"CV Data Size: \",tfidf_cv.shape)\nprint(\"Test Data Size: \",tfidf_test.shape)","bb301ee4":"# find optimal_k\nKNN(tfidf_train,tfidf_cv,Y_tr,Y_cv)","e725a342":"##algorithm = brute force, optimal_k=11\ntesting(tfidf_train,Y_tr,tfidf_test,Y_test,'brute',optimal_k=11)","d145f241":"# Train our own Word2Vec model using our own text corpus\nimport gensim\ni=0\nlist_of_sent=[]\nfor sent in final_reviews['CleanedText'].values:\n    filtered_sentence=[]\n    sent=cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    list_of_sent.append(filtered_sentence)","70e39da4":"# min_count = 5 considers only words that occured atleast 5 times\nw2v_model=Word2Vec(list_of_sent,min_count=5, vector_size=50, workers=4)","7cb0b034":"# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in list_of_sent: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)    ","96d2c30e":"# split data into Train, CV and Test\nX_train1, X_test1, Y_train1, Y_test1 = train_test_split(sent_vectors,Y, test_size=.33, random_state=0)\nX_tr1, X_cv1, Y_tr1, Y_cv1 = train_test_split(X_train1, Y_train1, test_size=.33, random_state=0)\ntrain_avgw2v=preprocessing.normalize(X_tr1)\ncv_avgw2v=preprocessing.normalize(X_cv1)\ntest_avgw2v=preprocessing.normalize(X_test1)\nprint(train_avgw2v.shape)\nprint(cv_avgw2v.shape)\nprint(test_avgw2v.shape)\n","308b7cf9":"# find optimal k using brute force\nKNN(train_avgw2v,cv_avgw2v,Y_tr1,Y_cv1)","32d17e03":"#algorithm = brute force, optimal_k=7\ntesting(train_avgw2v,Y_tr1,test_avgw2v,Y_test1,'brute',optimal_k=7)","d5327cce":"#tf-idf weighted w2v\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidfw2v_vect = TfidfVectorizer()\ntfidfw2v= tfidfw2v_vect.fit_transform(final_reviews['CleanedText'].values) \nprint(type(tfidfw2v))\nprint(tfidfw2v.shape)","0ca66f69":"# TF-IDF weighted Word2Vec\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\nt=tfidfw2v_vect.get_feature_names()\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in list_of_sent:\n    sent_vec = np.zeros(50)\n    cnt_words = 0;\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            tfidf = tfidfw2v[row,t.index(word)]\n            sent_vec += (vec*tfidf)\n            cnt_words += tfidf\n        except:\n            pass\n    sent_vec \/= cnt_words\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1","8ba59f51":"#split dataset\nX_train2, X_test2, Y_train2, Y_test2 = train_test_split(tfidf_sent_vectors, final['Score'], test_size=0.33,random_state=0)\nX_tr2, X_cv2, Y_tr2, Y_cv2 = train_test_split(X_train2, Y_train2, test_size=.33, random_state=0)\ntrain_tfidfw2v=preprocessing.normalize(X_tr2)\ncv_tfidfw2v=preprocessing.normalize(X_cv2)\ntest_tfidfw2v=preprocessing.normalize(X_test2)\nprint(train_tfidfw2v.shape)\nprint(cv_tfidfw2v.shape)\nprint(test_tfidfw2v.shape)","43685e88":"# find optimal k using brute force\nKNN(train_tfidfw2v,cv_tfidfw2v,Y_tr2,Y_cv2)","8d9aee34":"#algorithm = brute, optimal_k= 1\ntesting(train_tfidfw2v,Y_tr2,test_tfidfw2v,Y_test2,'brute',optimal_k=1)","f3c81f9c":"#find knn to simple cross validation with KD-Tree\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import roc_auc_score\ndef KNN_kdtree(X_train,X_cv,Y_train,Y_cv,n_components):\n    k = []\n    max_k = 0\n    pred_cv = []\n    pred_train = []\n    max_roc_auc=-1\n    svd = TruncatedSVD(n_components=n_components)\n    X_train = svd.fit_transform(X_train)\n    X_cv = svd.fit_transform(X_cv)\n    for i in range(1,24,2):\n        knn =KNeighborsClassifier(n_neighbors=i,algorithm='kd_tree')\n        knn.fit(X_train,Y_train)\n        probs = knn.predict_proba(X_cv)\n        prob = knn.predict_proba(X_train)\n        # keep probabilities for the positive outcome only\n        probs = probs[:, 1]\n        prob = prob[:,1]\n        auc_score_test = roc_auc_score(Y_cv,probs) #find AUC score\n        auc_score_train = roc_auc_score(Y_train,prob)\n        print(i,\" ------> \",auc_score_test)\n        pred_cv.append(auc_score_test)\n        pred_train.append(auc_score_train)\n        k.append(i)\n        if(max_roc_auc<auc_score_test):\n            max_roc_auc=auc_score_test\n            max_k=i\n    #print('\\nThe optimal number of neighbors is %d.' % max_k)\n    plt.plot(k, pred_cv,'r-', label = 'CV Data')\n    plt.plot(k,pred_train,'g-', label ='Train Data')\n    plt.legend(loc='upper right')\n    plt.title(\"K v\/s Auc Score\")\n    plt.ylabel('Auc Score')\n    plt.xlabel('K')\n    plt.show()\n    # calculate roc curve\n    fpr, tpr, thresholds = roc_curve(Y_cv,probs)\n    # plot no skill\n    pyplt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    pyplt.plot(fpr, tpr, marker='.')\n    #plt.plot(k,pred_cv)\n    pyplt.title(\"Line Plot of ROC Curve\")\n    pyplt.ylabel('True Positive Rate')\n    pyplt.xlabel('False Positive Rate')\n    pyplt.show()","f69fcee1":"# for KD TREE take 30k data point\nfinal_review = final_reviews.head(30000) #Sampled amazon fine foood reviews final revievs data to 30k datapoints for time effiecieny\nfinal_review.shape","4790fb44":"#split data into train, cross validate and test \n%matplotlib inline\nimport warnings\nfrom sklearn.model_selection import train_test_split\nX = final_review['CleanedText']\nY = final_review['Score']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33, random_state=0)\nX_tr, X_cv, Y_tr, Y_cv = train_test_split(X_train, Y_train, test_size=.33, random_state=0)","7dd217b0":"print('X_train, Y_train', X_train.shape, Y_train.shape)\nprint('X_test, Y_test', X_test.shape, Y_test.shape)\nprint('X_tr, Y_tr', X_tr.shape, Y_tr.shape)\nprint('X_cv, Y_cv', X_cv.shape, Y_cv.shape)","29585700":"#BoW\ncount_vect = CountVectorizer() #in scikit-learn\nbow_train = count_vect.fit_transform(X_tr)\nprint(\"The type of count vectorizer \",type(bow_train))\nprint(\"The shape of out text BOW vectorizer \",bow_train.get_shape())\n#print(\"the number of unique words \", final_counts.get_shape()[1])\nbow_cv = count_vect.transform(X_cv)\nbow_test = count_vect.transform(X_test)\nprint(\"CV Data Size: \",bow_cv.shape)\nprint(\"Test Data Size: \",bow_test.shape)","1322b1ee":"#Normalize Data\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import Normalizer\nbow_train=preprocessing.normalize(bow_train)\nbow_cv=preprocessing.normalize(bow_cv)\nbow_test=preprocessing.normalize(bow_test)\nprint(\"The shape of out text BOW vectorizer \",bow_train.get_shape())\nprint(\"CV Data Size: \",bow_cv.shape)\nprint(\"Test Data Size: \",bow_test.shape)","6e8e1309":"# explain variance of training data\nfrom sklearn.decomposition import TruncatedSVD\ntsvd = TruncatedSVD(n_components=1000).fit(bow_train)\nplt.plot(np.cumsum(tsvd.explained_variance_ratio_))\nplt.xlim(0,1000,10)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","f0016bff":"# find optimal k using kd_tree\nKNN_kdtree(bow_train,bow_cv,Y_tr,Y_cv,n_components=1000)","d58862b9":"#algorithm = kd_tree, optimal_k=9\ntesting(bow_train,Y_tr,bow_test,Y_test,'kd_tree',optimal_k=15)","6bf23411":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\ntfidf_train = tf_idf_vect.fit_transform(X_tr)\nprint(\"The type of count vectorizer \",type(tfidf_train))\nprint(\"The shape of out text TFIDF vectorizer \",tfidf_train.get_shape())\ntfidf_cv = tf_idf_vect.transform(X_cv)\ntfidf_test = tf_idf_vect.transform(X_test)\nprint(\"CV Data Size: \",tfidf_cv.shape)\nprint(\"Test Data Size: \",tfidf_test.shape)","73a35bb8":"#Normalize Data\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import Normalizer\ntfidf_train=preprocessing.normalize(tfidf_train)\ntfidf_cv=preprocessing.normalize(tfidf_cv)\ntfidf_test=preprocessing.normalize(tfidf_test)\nprint(\"The shape of out text BOW vectorizer \",tfidf_train.get_shape())\nprint(\"CV Data Size: \",tfidf_cv.shape)\nprint(\"Test Data Size: \",tfidf_test.shape)","d30f9ee1":"from sklearn.decomposition import TruncatedSVD\ntsvd = TruncatedSVD(n_components=300).fit(tfidf_train)\nplt.plot(np.cumsum(tsvd.explained_variance_ratio_))\nplt.xlim(0,300,10)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","9efb24ca":"# find optimal_k\nKNN_kdtree(tfidf_train,tfidf_cv,Y_tr,Y_cv,n_components=300)","fb9ac37f":"#algorithm = brute force, optimal_k=11\ntesting(bow_train,Y_tr,bow_test,Y_test,'kd_tree',optimal_k=7)","bdfb0e85":"# Train our own Word2Vec model using our own text corpus\nimport gensim\ni=0\nlist_of_sent=[]\nfor sent in final_review['CleanedText'].values:\n    filtered_sentence=[]\n    sent=cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    list_of_sent.append(filtered_sentence)","62a431ee":"# min_count = 5 considers only words that occured atleast 5 times\nw2v_model=Word2Vec(list_of_sent,min_count=5,vector_size=50, workers=4)","76ea133d":"# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in list_of_sent: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)    ","8ad7ae5b":"# split data into Train, CV and Test\nX_train1, X_test1, Y_train1, Y_test1 = train_test_split(sent_vectors,Y, test_size=.33, random_state=0)\nX_tr1, X_cv1, Y_tr1, Y_cv1 = train_test_split(X_train1, Y_train1, test_size=.33, random_state=0)\ntrain_avgw2v=preprocessing.normalize(X_tr1)\ncv_avgw2v=preprocessing.normalize(X_cv1)\ntest_avgw2v=preprocessing.normalize(X_test1)\nprint(train_avgw2v.shape)\nprint(cv_avgw2v.shape)\nprint(test_avgw2v.shape)\n#print(len(X_train1))","41a44e46":"from sklearn.decomposition import TruncatedSVD\ntsvd = TruncatedSVD(n_components=49).fit(train_avgw2v)\nplt.plot(np.cumsum(tsvd.explained_variance_ratio_))\nplt.xlim(0,50,10)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","5bf973ff":"# find optimal k using kd_tree\nKNN_kdtree(train_avgw2v,cv_avgw2v,Y_tr1,Y_cv1,n_components=49)","218dd8d5":"#algorithm = brute force, optimal_k=7\ntesting(train_avgw2v,Y_tr1,test_avgw2v,Y_test1,'brute',optimal_k=11)","f7e4398c":"#tf-idf weighted w2v\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidfw2v_vect = TfidfVectorizer()\ntfidfw2v= tfidfw2v_vect.fit_transform(final_review['CleanedText'].values) \nprint(type(tfidfw2v))\nprint(tfidfw2v.shape)","acf6003f":"# TF-IDF weighted Word2Vec\nt=tfidfw2v_vect.get_feature_names()\ntfidf_sent_vectors = []; \nrow=0;\nfor sent in list_of_sent:\n    sent_vec = np.zeros(50)\n    cnt_words = 0;\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            tfidf = tfidfw2v[row,t.index(word)]\n            sent_vec += (vec*tfidf)\n            cnt_words += tfidf\n        except:\n            pass\n    sent_vec \/= cnt_words\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1","63b79684":"#split dataset\nX_train2, X_test2, Y_train2, Y_test2 = train_test_split(tfidf_sent_vectors, final_review['Score'], test_size=0.33,random_state=0)\nX_tr2, X_cv2, Y_tr2, Y_cv2 = train_test_split(X_train2, Y_train2, test_size=.33, random_state=0)\ntrain_tfidfw2v=preprocessing.normalize(X_tr2)\ncv_tfidfw2v=preprocessing.normalize(X_cv2)\ntest_tfidfw2v=preprocessing.normalize(X_test2)\nprint(train_tfidfw2v.shape)\nprint(cv_tfidfw2v.shape)\nprint(test_tfidfw2v.shape)","1b8c2726":"from sklearn.decomposition import TruncatedSVD\ntsvd = TruncatedSVD(n_components=49).fit(train_tfidfw2v)\nplt.plot(np.cumsum(tsvd.explained_variance_ratio_))\nplt.xlim(0,50,10)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative wexplained variance')","1bb60b0a":"# find optimal k using brute force\nKNN_kdtree(train_tfidfw2v,cv_tfidfw2v,Y_tr2,Y_cv2,n_components=49)","21d0ad38":"#algorithm = brute, optimal_k= 1\ntesting(train_tfidfw2v,Y_tr2,test_tfidfw2v,Y_test2,'kd_tree',optimal_k=11)","c85d50ad":"from prettytable import PrettyTable\nx = PrettyTable()\nx.field_names = [\"Vectorizer\", \"Model\", \"Best hyper parameter(K)\",\"Test Auc Score\"]\n\nx.add_row([\"BoW\",\"Brute\",9,77.83])\nx.add_row([\"BoW\",\"KD-Tree\",15,79.60])\nx.add_row([\"Tf-Idf\",\"Brute\",11,78.33])\nx.add_row([\"Tf-Idf\",\"KD-Tree\",7,76.07])\nx.add_row([\"Avg Word2Vec\",\"Brute\",7,83.91])\nx.add_row([\"Avg Word2Vec\",\"KD-Tree\",11,84.00])\nx.add_row([\"tf idf-Word2vec\",\"Brute\",1,51.7])\nx.add_row([\"tf idf-Word2vec\",\"KD-Tree\",11,80.95])\nfrom IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown(string))\nprintmd('****Final Conclusion:****')\nprint(x)","b004a505":"##  Bag of Words (BoW) ","be7508e8":"### KNN with simple CV Over the unseen or Test data in TF-IDF","8ce740b8":"## TF-IDF Weighted Word2Vec","4c4af1a7":"### KNN with simple CV Over the unseen or Test data in AvgW2V","6ed329f0":"## Avg W2V ","9114b70f":"### Sorting dataset based on 'Time' feature","4a13f2b2":"# Techniques for vectorization :-- ","75c49028":"### KNN with Test data in BoW","588dc349":"## Text Preprocessing: Stemming, stop-word removal and Lemmatization.\n\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews","faa8bbce":"As can be seen above the same user has multiple reviews of the with the same values for HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary and Text  and on doing analysis it was found that <br>\n<br> \nProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies, 8.82-Ounce Packages (Pack of 8)<br>\n<br> \nProductId=B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so on<br>\n\nIt was inferred after analysis that reviews with same parameters other than ProductId belonged to the same product just having different flavour or quantity. Hence in order to reduce redundancy it was decided to eliminate the rows having same parameters.<br>\n\nThe method used for the same was that we first sort the data according to ProductId and then just keep the first similar product review and delelte the others. for eg. in the above just the review for ProductId=B000HDL1RQ remains. This method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product.","0904191d":"## TF-IDF Weighted Word2Vec","6bb3478e":"##  Loading the data\n\nThe dataset is available in two forms\n1. .csv file\n2. SQLite Database\n\nIn order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently.\n<br> \n\nHere as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\".","85e82eca":"## Training Model with Brute Force and KD-Tree","f4c7891b":"### Spliting data ","07cc83fc":"# Exploratory Data Analysis\n\n## Data Cleaning: Deduplication\n\nIt is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.  Following is an example:","0dd8768c":"### KNN with simple CV Over the unseen or Test data in TF-IDF W2V","0bffd5e7":"##  TF-IDF","b7488a3f":"## TF-IDF ","8f5c883c":"# K-NN using KD-Tree with Simple CV","26eee493":"### KNN with simple CV Over the unseen or Test data in TF-IDF","7e5d10df":"# K-NN using Brute Force ","6197bbd8":"# Amazon Fine Food Reviews Analysis\n\n\n\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n\nNumber of reviews: 568,454<br>\nNumber of users: 256,059<br>\nNumber of products: 74,258<br>\nTimespan: Oct 1999 - Oct 2012<br>\nNumber of Attributes\/Columns in data: 10 \n\nAttribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review\n\n\n<b>Objective:-<\/b>\nGiven a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n\n<br>\n[Q] How to determine if a review is positive or negative?<br>\n<br> \n[Ans] We could use the Score\/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity\/negativity) of a review.\n\n\n","5762d4ac":"# Techniques for vectorization :--","1552508c":"### KNN with simple CV Over the unseen or Test data in AvgW2V ","d6ac4bc8":"## Avg W2V","85a39671":"### KNN with simple CV Over the unseen or Test data in BoW","394bbd44":"## Testing Model with Brute Force and KD-Tree","8a81dd65":"<b>Observation:-<\/b> It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions","327db92a":"##  Bag of Words (BoW) ","e406d9a7":"## Spliting dataset  ","7b941aa9":"### KNN with simple CV Over the unseen or Test data in AvgW2V"}}