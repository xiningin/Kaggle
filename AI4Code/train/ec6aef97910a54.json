{"cell_type":{"faf11830":"code","58bdd9bb":"code","67264aec":"code","694d81d0":"code","5ad8a90b":"code","43eac5c7":"code","404ecdeb":"code","314f4589":"code","76cd9178":"code","5f80f42b":"code","e664261a":"code","2fd7f117":"code","22f14b71":"code","c3c93fc1":"code","076f9af2":"code","f2a9c2b0":"code","b1e8ee04":"code","4936ef35":"code","8c1ab2dd":"code","a45f46fe":"code","ab6a20ea":"code","b931bc89":"code","d9a6276c":"code","25095b01":"code","9d6290c6":"code","122d7fae":"code","69bffadd":"code","c6711941":"code","0ebc6c33":"code","0b3dfffc":"code","1822be97":"code","23a5d543":"code","28afd45d":"code","de95b035":"code","1012b572":"code","9ffbceba":"code","4cd31805":"code","08a28b41":"code","a117acf2":"markdown","f013f343":"markdown","225ae01c":"markdown","8686cd76":"markdown","792fdc84":"markdown","0e175e34":"markdown","ed7217b4":"markdown","280d1e7c":"markdown","11ff615d":"markdown","c5ab996f":"markdown","acbe5cda":"markdown","d35c94d0":"markdown","60864666":"markdown","896720d7":"markdown","da2b3187":"markdown","5b094a79":"markdown"},"source":{"faf11830":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58bdd9bb":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax","67264aec":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","694d81d0":"test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","5ad8a90b":"# Save the \"Id\" column\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Remove column \"Utilitiles\"\n\ntrain.drop(\"Utilities\", axis = 1, inplace = True)\ntest.drop(\"Utilities\", axis = 1, inplace = True)","43eac5c7":"# Scatter plot between SalePrice and GrLivArea\n\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis = 1)\ndata.plot.scatter(x = var, y = 'SalePrice')","404ecdeb":"# The two values on the right-lower corner seem strange and they don't follow the trend, so we delete them.\n\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]","314f4589":"# Delete the two strange observations, we make a COPY of the original data and name the new data as \"train1\"\n# \"train1\" is the newly-generated data set without outliers\n\ntrain1 = train.copy()\ntrain1 = train1.drop(train1[train1['Id'] == 524].index)\ntrain1 = train1.drop(train1[train1['Id'] == 1299].index)\n\nprint(\"Dimension of the original data set: {}\".format(train.shape))\nprint(\"Dimension of train1: {}\".format(train1.shape))","76cd9178":"# Check the plot again\n\nvar = 'GrLivArea'\ndata = pd.concat([train1['SalePrice'], train1[var]], axis = 1)\ndata.plot.scatter(x = var, y = 'SalePrice')","5f80f42b":"total_number_of_missing = train1.isnull().sum().sort_values(ascending = False)\npercent = (train1.isnull().sum() \/ train1.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total_number_of_missing, percent], axis = 1, keys = ['Total', 'Percent'])\nmissing_data.head(20)","e664261a":"train1[\"PoolQC\"] = train1[\"PoolQC\"].fillna(\"None\")\ntrain1[\"MiscFeature\"] = train1[\"MiscFeature\"].fillna(\"None\")\ntrain1[\"Alley\"] = train1[\"Alley\"].fillna(\"None\")\ntrain1[\"Fence\"] = train1[\"Fence\"].fillna(\"None\")\ntrain1[\"FireplaceQu\"] = train1[\"FireplaceQu\"].fillna(\"None\")\n\n# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n\ntrain1[\"LotFrontage\"] = train1.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    train1[col] = train1[col].fillna(\"None\")\ntrain1['GarageYrBlt'] = train1['GarageYrBlt'].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train1[col] = train1[col].fillna(\"None\")\ntrain1[\"MasVnrType\"] = train1[\"MasVnrType\"].fillna(\"None\")\ntrain1[\"MasVnrArea\"] = train1[\"MasVnrArea\"].fillna(0)\ntrain1[\"MSZoning\"] = train1[\"MSZoning\"].fillna(train1[\"MSZoning\"].mode()[0])\ntrain1[\"Electrical\"] = train1[\"Electrical\"].fillna(train1[\"Electrical\"].mode()[0])","2fd7f117":"train1.isnull().sum().max()","22f14b71":"missing_in_test = test.isnull().sum().sort_values(ascending = False)\npercent_test = (test.isnull().sum() \/ test.isnull().count()).sort_values(ascending = False)\nmissing_data_test = pd.concat([missing_in_test, percent_test], axis = 1, keys = ['Total', 'Percent'])\nmissing_data_test.head(35)","c3c93fc1":"(missing_data_test.Total != 0).sum()","076f9af2":"test[\"PoolQC\"] = test[\"PoolQC\"].fillna(\"None\")\ntest[\"MiscFeature\"] = test[\"MiscFeature\"].fillna(\"None\")\ntest[\"Alley\"] = test[\"Alley\"].fillna(\"None\")\ntest[\"Fence\"] = test[\"Fence\"].fillna(\"None\")\ntest[\"FireplaceQu\"] = test[\"FireplaceQu\"].fillna(\"None\")\ntest[\"LotFrontage\"] = test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    test[col] = test[col].fillna(\"None\")\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    test[col] = test[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    test[col] = test[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    test[col] = test[col].fillna(\"None\")\ntest[\"MasVnrType\"] = test[\"MasVnrType\"].fillna(\"None\")\ntest[\"MasVnrArea\"] = test[\"MasVnrArea\"].fillna(0)\ntest[\"MSZoning\"] = test[\"MSZoning\"].fillna(test[\"MSZoning\"].mode()[0])\ntest[\"Functional\"] = test[\"Functional\"].fillna(\"Typ\")\ntest['KitchenQual'] = test['KitchenQual'].fillna(test['KitchenQual'].mode()[0])\ntest['Exterior1st'] = test['Exterior1st'].fillna(test['Exterior1st'].mode()[0])\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0])\ntest['SaleType'] = test['SaleType'].fillna(test['SaleType'].mode()[0])\n","f2a9c2b0":"test.isnull().sum().max()","b1e8ee04":"train1['SalePrice'].describe()","4936ef35":"# We do the histogram and QQ-plot for the reponse\n\nsns.distplot(train1['SalePrice'], fit = norm);\nfig = plt.figure()\nres = stats.probplot(train1['SalePrice'], plot = plt)","8c1ab2dd":"# The reponse is not normal, we do a log transformation to 'SalePrice' and name the data as 'train2'\n\ntrain2 = train1.copy()\ntrain2['SalePrice'] = np.log(train2['SalePrice'])\n\nsns.distplot(train2['SalePrice'], fit = norm);\nfig = plt.figure()\nres = stats.probplot(train2['SalePrice'], plot = plt)","a45f46fe":"ntrain = train2.shape[0]\nntest = test.shape[0]\ny_train = train2.SalePrice.values\nall_data = pd.concat((train2, test)).reset_index(drop=True)\nall_data.drop(['SalePrice', 'Id'], axis=1, inplace=True)\n\nprint(\"all_data size is: {}\".format(all_data.shape))","ab6a20ea":"all_data.isnull().sum().max()","b931bc89":"all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n#all_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(str)\n#all_data['YearBuilt'] = all_data['YearBuilt'].astype(str)\n#all_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str)","d9a6276c":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","25095b01":"numeric_features = all_data.dtypes[all_data.dtypes != 'object'].index","9d6290c6":"\nskewed_features = all_data[numeric_features].apply(lambda x: skew(x)).sort_values(ascending = False)\nprint(skewed_features)","122d7fae":"high_skew = skewed_features[abs(skewed_features) > 0.75]\nprint(high_skew)","69bffadd":"skew_index = high_skew.index","c6711941":"\nfor i in skew_index:\n    all_data[i] = boxcox1p(all_data[i], boxcox_normmax(all_data[i] + 1))","0ebc6c33":"# Getting dummy categorical features\n\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","0b3dfffc":"train_cleaned = all_data[:ntrain]\ntest_cleaned = all_data[ntrain:]","1822be97":"print(train_cleaned.shape)\nprint(test_cleaned.shape)","23a5d543":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\nimport lightgbm as lgb","28afd45d":"#Validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=1).get_n_splits(train_cleaned.values)\n    rmse= np.sqrt(-cross_val_score(model, train_cleaned.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","de95b035":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0006, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0006, l1_ratio=.9, random_state=1))\nscore_1 = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score_1.mean(), score_1.std()))\nscore_2 = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score_2.mean(), score_2.std()))","1012b572":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603,\n                             gamma=0.0468, \n                             learning_rate=0.05, \n                             max_depth=3, \n                             min_child_weight=1.7817, \n                             n_estimators=2200,\n                             reg_alpha=0.4640, \n                             reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =0, \n                             nthread = -1)\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","9ffbceba":"model_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=5,\n                              learning_rate=0.05, \n                              n_estimators=720,\n                              max_bin = 55, \n                              bagging_fraction = 0.8,\n                              bagging_freq = 5, \n                              feature_fraction = 0.2319,\n                              feature_fraction_seed=9, \n                              bagging_seed=9,\n                              min_data_in_leaf =6, \n                              min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","4cd31805":"lasso.fit(train_cleaned, y_train)\nlasso_pred = np.exp(lasso.predict(test_cleaned))","08a28b41":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = lasso_pred\nsub.to_csv('submission.csv', index = False)","a117acf2":"- Target Variable","f013f343":"- Lasso and Elastic Net","225ae01c":"##### If you found this notebook helpful, some upvotes would be very much appreciated.","8686cd76":"- LightGBM","792fdc84":"- Transform some numerical variables that are really categorical and get dummies","0e175e34":"## Step 1: Load Packages and Data","ed7217b4":"- Boxcox trandformation for train and test set separately","280d1e7c":"## Step 2: Data Cleaning\n- Deleting Outliers\n- Imputing Missing Values\n- Variable Transformation","11ff615d":"### Step 2.3: Variable Transformation","c5ab996f":"#### Warnings: Now the name of the two sets are \"train_cleaned\" & \"test_cleaned\", response is \"y_train\". It's ready to build model.","acbe5cda":"## Step 3: Model Building","d35c94d0":"- XGBoost","60864666":"## Step 4: Get Prediction and Output","896720d7":"- Combine train2 and test, delete 'Id' and 'SalePrice', save SalePrice as y_train, then do the transformation","da2b3187":"### Step 2.1: Deleting Outliers","5b094a79":"### Step 2.2: Imputing Missing Values"}}