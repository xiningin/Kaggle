{"cell_type":{"412e341c":"code","eda4b72f":"code","f6268580":"code","95bc293d":"code","901144e8":"code","20255c00":"code","45a9f750":"code","414c362e":"code","c3819b87":"code","6d496f68":"code","8c230186":"code","0806d6fc":"code","a3effd0b":"code","c6e2c12b":"code","7ebe8783":"code","2f854f7c":"markdown","e8a2e807":"markdown","e155266f":"markdown","8c3e0656":"markdown","ca75d063":"markdown","118f224a":"markdown","bca5c8a7":"markdown","2285b6d6":"markdown","e20066c8":"markdown","1f0b3cc5":"markdown","ffa3d595":"markdown"},"source":{"412e341c":"!pip install torchsummary\n!pip install tqdm\n!pip install torchvision --upgrade","eda4b72f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torchvision import models\n\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\nimport pickle\nimport statistics\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport tqdm.notebook as tqdm\nimport nltk\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Set train and test dataframe\n\ndf_train_label = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ndf_test_label = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')","f6268580":"class SETIBreakThroughDataset(Dataset):\n\n    def __init__(self, is_train=True, label_df=df_train_label, astype=torch.float32):\n        self.is_train = is_train\n        self.labels = label_df\n        self.astype = astype\n\n    def get_data(self, id: str):\n        if self.is_train:\n            return np.load(f\"..\/input\/seti-breakthrough-listen\/train\/{id[0]}\/{id}.npy\")\n        else:\n            return np.load(f\"..\/input\/seti-breakthrough-listen\/test\/{id[0]}\/{id}.npy\")\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item_id = self.labels.iloc[idx][\"id\"]\n        data = {}\n        X = self.get_data(item_id)\n        X = torch.tensor(X, dtype=self.astype)\n        data['X'] = X\n        if self.is_train:\n            y = self.labels.target.iloc[idx]\n            # y = torch.tensor(y, dtype=torch.long)\n            y = torch.tensor(y, dtype=torch.float)\n            data['y'] = y\n        return data","95bc293d":"from sklearn.model_selection import train_test_split\n\nbatch_size = 32\n\ntrainset, validset = train_test_split(df_train_label, test_size=0.1, stratify=df_train_label.target.to_numpy())\n\ntrain_dataset = SETIBreakThroughDataset(label_df=trainset)\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\nvalid_dataset = SETIBreakThroughDataset(label_df=validset)\nvalid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)","901144e8":"import gc\ndef clear_mem(model_name='model'):\n    \"\"\"\n    Clears GPU cache in notebook\n    \"\"\"\n    if model_name in locals():\n        print('deleting model...')\n        del model\n    for x in list(globals().keys()):\n        variable = eval(x)\n        if torch.is_tensor(variable) and variable.is_cuda:\n            print(x)\n            del variable\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# save model file\ndef save_model(model, save_path, **metrics):\n    \"\"\"\n    Save the model to the path directory provided\n    \"\"\"\n    if \"state_dict\" in metrics:\n        raise Warning(\"We will use states from the model instead.\")\n        del metrics[\"state_dict\"]\n    model_to_save = model.module if hasattr(model, 'module') else model\n    checkpoint = {'state_dict': model_to_save.state_dict()}\n    checkpoint.update(metrics)\n    torch.save(checkpoint, save_path)\n    return save_path, metrics\n\n# load model file\ndef load_model(save_path, model_class=None, model=None):\n    \"\"\"\n    Load the model from the path directory provided\n    \"\"\"\n    if model is None:\n        if model_class is None:\n            raise ValueError(\"No model to construct!\")\n        model = model_class()\n    checkpoint = torch.load(save_path)\n    model_state_dict = checkpoint['state_dict']\n    model.load_state_dict(model_state_dict)\n    metrics = {k:checkpoint[k] for k in checkpoint if k!='state_dict'}\n\n    return model, metrics","20255c00":"from sklearn.metrics import roc_auc_score\ndef fold_train(model, train_data_loader, valid_data_loader,\n               optimizer, max_epoch, patience=3, best_modelpath = \"best_model.pth\", device='cuda'):\n    \"\"\"\n    Define training loop\n    \"\"\"\n    best_avg_valid_loss = float('inf')\n    patience_count = patience\n    filepath = None\n    best_model_info = None\n    loss_fct = nn.BCEWithLogitsLoss()\n    for epoch in tqdm.trange(max_epoch, desc=\"training\", unit=\"epoch\"):\n        total_loss = 0.0\n        final_avg_loss = 0.0\n        with tqdm.tqdm(train_data_loader,desc=\"epoch {} train\".format(epoch + 1),\n                  unit=\"batch\",total=len(train_data_loader)) as train_batch_iterator:\n            model.train()\n            for i, batch_data in enumerate(train_batch_iterator, start=1):\n                optimizer.zero_grad()\n                out = model(x=batch_data['X'].to(device)).squeeze(1)\n                loss = loss_fct(out, batch_data['y'].to(device))\n                total_loss += loss.item()\n                loss.backward()\n                optimizer.step()\n                train_batch_iterator.set_postfix(mean_loss=total_loss \/ i, current_loss=loss.item())\n                final_avg_loss = total_loss \/ i\n        total_valid_loss = 0.0\n        final_avg_valid_loss = 0.0\n        true_labels = []\n        pred_labels = []\n        with torch.no_grad():\n            model.eval()\n            with tqdm.tqdm(valid_data_loader,desc=\"epoch {} valid\".format(epoch + 1),\n                      unit=\"batch\",total=len(valid_data_loader),leave=False) as valid_batch_iterator:\n                \n                for i, batch_data in enumerate(valid_batch_iterator, start=1):\n                    out = model(x=batch_data['X'].to(device)).squeeze(1)\n                    loss = loss_fct(out, batch_data['y'].to(device))\n                    total_valid_loss += loss.item()\n                    valid_batch_iterator.set_postfix(mean_loss=total_valid_loss \/ i, current_loss=loss.item())\n                    final_avg_valid_loss = total_valid_loss \/ i\n                    pred_labels.append(out.sigmoid().to('cpu').numpy())\n                    true_labels.append(batch_data['y'].to('cpu').numpy())\n        true_labels = np.concatenate(true_labels, axis=0)\n        pred_labels = np.concatenate(pred_labels, axis=0)\n        score = roc_auc_score(y_score=pred_labels, y_true=true_labels)\n\n        print(f\"Validation results for epoch #{epoch + 1}: average_loss={final_avg_valid_loss}, roc_auc_score={score}\")\n\n        if final_avg_valid_loss > best_avg_valid_loss:\n            patience_count -= 1\n        elif final_avg_valid_loss < best_avg_valid_loss:\n            filepath = best_modelpath\n            score_info = {'roc_auc_score':score,\n                          'train_loss':final_avg_loss,\n                          'valid_loss':final_avg_valid_loss,\n                          'epoch':epoch + 1}\n            print(\"Saving this model...\")\n            filepath, best_model_info = save_model(model, filepath,\n                                                   avg_valid_loss=final_avg_valid_loss,\n                                                   roc_auc_score=score)\n            \n        best_avg_valid_loss = min(final_avg_valid_loss, best_avg_valid_loss)\n        if patience_count == 0:\n            print(\"Early Stopping: the average validation loss did not improve.\")\n            break\n    return filepath, best_model_info, score_info","45a9f750":"print(dir(models))","414c362e":"import torchsummary\nclass SeqCNN_block(nn.Module):\n    def __init__(self, output_dim=576):\n        super(SeqCNN_block, self).__init__()\n        \n        # load the MobileNet.\n        self.rgb_conv = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n        self.cnn_model = models.mobilenet_v3_small(pretrained=True)\n        \n        # pool the final layer\n        self.final_pooler = nn.AdaptiveAvgPool2d((None, 1))\n        \n        # if different output dim thant 2048, then use linear layer to convert.\n        self.fc = nn.Identity()\n        if output_dim != 576:\n            self.fc = nn.Linear(576, output_dim)\n\n    def _cnn_forward(self, x):\n        x = self.rgb_conv(x)\n        x = self.cnn_model.features(x)\n        return x\n    \n    def forward(self, x):\n        x = self._cnn_forward(x)\n        x = self.final_pooler(x)\n        x = x.squeeze(-1)\n        x = x.transpose(1, 2)\n        \n        return self.fc(x)","c3819b87":"class CNNBiGRU(nn.Module):\n    def __init__(self, cnn_dim=576, rnn_dim=512, num_layers=2, dropout=0.1, mid_dimension=512):\n        super(CNNBiGRU, self).__init__()\n        self.hidden_dim = rnn_dim\n        self.cnn_dim = cnn_dim\n        self.cnn_model = SeqCNN_block(output_dim=self.cnn_dim)\n        self.rnn_model = nn.GRU(input_size=self.cnn_dim,\n                                hidden_size=rnn_dim,\n                                num_layers=num_layers,\n                                dropout=dropout,\n                                batch_first=True,\n                                bidirectional=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(in_features=self.hidden_dim * 4,\n                             out_features=mid_dimension)\n        self.fc2 = nn.Linear(in_features=mid_dimension, out_features=1)\n\n    def forward(self, x, label=None):\n        batch_size = x.size()[0]\n        image_row = x.size()[2]\n        image_col = x.size()[3]\n        \n        # we combine 6 images into batches\n        on_1 = x[:, 0].unsqueeze(1)\n        off_1 = x[:, 1].unsqueeze(1)\n        on_2 = x[:, 2].unsqueeze(1)\n        off_2 = x[:, 3].unsqueeze(1)\n        on_3 = x[:, 4].unsqueeze(1)\n        off_3 = x[:, 5].unsqueeze(1)\n        \n        conv_1 = self.cnn_model(on_1)\n\n        conv_1f = self.cnn_model(off_1)\n\n        conv_2 = self.cnn_model(on_2)\n\n        conv_2f = self.cnn_model(off_2)\n\n        conv_3 = self.cnn_model(on_3)\n\n        conv_3f = self.cnn_model(off_3)\n\n        # combine logits to a sequence\n        allseq = torch.cat([conv_1, conv_1f, conv_2, conv_2f, conv_3, conv_3f], dim=1)\n        dim1_shape = allseq.size()[1]\n        \n        seq, state = self.rnn_model(allseq)\n        seqs = seq.view(batch_size, 6, -1, 2 * self.hidden_dim)\n        \n        on_seqs = seqs[:, (0, 2, 4), :, :].reshape(batch_size, -1, 2 * self.hidden_dim)\n        off_seqs = seqs[:, (1, 3, 5), :, :].reshape(batch_size, -1, 2 * self.hidden_dim)\n        \n        \n        on_avg = torch.mean(on_seqs, dim=1)\n        off_avg = torch.mean(off_seqs, dim=1)\n        \n        x = torch.cat((on_avg, off_avg), dim=1)\n\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n\n        return x\n\nmodel = CNNBiGRU()\ntorchsummary.summary(model, (6, 273, 256), device='cpu')","6d496f68":"import gc\ngc.collect()\nmodel = model.cuda()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\nfilepath, best_model_info, score_info = fold_train(model, train_dataloader, valid_dataloader, optimizer, max_epoch=2)\nfilepath, best_model_info, score_info","8c230186":"model_class = type(model)\nclear_mem()","0806d6fc":"model, metrics = load_model(filepath, model_class=model_class)","a3effd0b":"model = model.cuda()\ndf_submit = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\ntest_dataset = SETIBreakThroughDataset(is_train=False, label_df=df_submit)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\npred_labels = []\nwith torch.no_grad():\n    model.eval()\n    with tqdm.tqdm(test_dataloader, unit=\"batch\",total=len(test_dataloader),leave=False) as test_batch_iterator:\n        for i, batch_data in enumerate(test_batch_iterator, start=1):\n            out = model(x=batch_data['X'].to('cuda')).squeeze(1)\n            pred_labels.append(out.sigmoid().to('cpu').numpy())\npred_labels = np.concatenate(pred_labels, axis=0)","c6e2c12b":"df_submit['target'] = pred_labels\ndf_submit","7ebe8783":"df_submit.to_csv(\"submission.csv\", index = False)","2f854f7c":"# Define some utility classes","e8a2e807":"# [SETI Breakthrough Listen - E.T. Signal Search](https:\/\/www.kaggle.com\/c\/seti-breakthrough-listen)\n>Find extraterrestrial signals in data from deep space \n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23652\/logos\/header.png?t=2021-02-24-19-15-30)","e155266f":"## 2. set train-test split","8c3e0656":"# Construct the model!\n - The model basically turns the final unpooled output into a sequence data, which the BiGRU can read from.","ca75d063":"## 1. The submodule to convert ResNet34 features to a sequence.","118f224a":"# Now train the model!","bca5c8a7":"## 1. define class","2285b6d6":"## 2. The full avg-pooling classifier model","e20066c8":"# implementation goals\n - explore combining CNN with RNN\n - RNN: Bidirectional GRU with 2 layers\n - CNN: MobileNetv3 model","1f0b3cc5":"# Now do inference on the best model","ffa3d595":"# Prepare Dataset!\n- We make a basic dataset class to get the tensors."}}