{"cell_type":{"f53d43b9":"code","bf827b92":"code","a592e85c":"code","f0cbc16f":"code","ee512625":"code","4d918e2a":"code","76097283":"code","65efbca9":"code","d97daf6e":"code","08f901e5":"code","9f957b93":"code","acddffa4":"code","ac64898c":"code","df05a23c":"code","34383209":"code","1c060506":"code","af89d18e":"code","1d09ca0e":"code","c04db6b4":"code","563d8318":"code","81834bd3":"code","bcc8ddc3":"code","af23ce18":"code","2bff8f25":"code","8bc960f8":"code","7fb218cb":"code","7dc7a1f0":"code","893b7a26":"code","d2dc2779":"code","4238be2f":"markdown","31ea7a1b":"markdown","b704ad61":"markdown","0802a3e7":"markdown","b916d9d9":"markdown","dfc104e5":"markdown","af7352cc":"markdown","dbac0262":"markdown","97d6a0cd":"markdown","253ead0b":"markdown","51b46518":"markdown","9a1a7173":"markdown","e49e3fb5":"markdown","fd339737":"markdown","ae7ae65d":"markdown","c6fbd1a8":"markdown","b0b99b78":"markdown","cd0dfcdc":"markdown","9bb06f20":"markdown","896ef238":"markdown","6ea79b85":"markdown","11c17dd2":"markdown","3da817fc":"markdown","d3499367":"markdown","01447400":"markdown","7815f48c":"markdown","fd8cd527":"markdown","d91731c6":"markdown","6b35a857":"markdown","27086c61":"markdown","c4d2e378":"markdown","acc5cdc3":"markdown","483e1e8d":"markdown","b1775bc8":"markdown","e888b89a":"markdown","d563af22":"markdown","8db08191":"markdown","e5677fd2":"markdown","7a564ce5":"markdown","e8636106":"markdown","6ee87136":"markdown","6f92098e":"markdown","3e0e752a":"markdown","8f48048a":"markdown","d134a7ae":"markdown","2f4348a4":"markdown","e5b57fb1":"markdown","e05f17d2":"markdown","db13e36a":"markdown","e1c9ecc5":"markdown","9bb19997":"markdown"},"source":{"f53d43b9":"!pip install -U git+https:\/\/github.com\/dgunning\/cord19.git","bf827b92":"from cord import ResearchPapers\n\nresearch_papers = ResearchPapers.load()","a592e85c":"research_papers.search('antiviral treatment')","f0cbc16f":"research_papers.searchbar('antiviral treatment')","ee512625":"since_covid = research_papers.since_sarscov2()\nsince_covid","4d918e2a":"research_papers.match('H[0-9]N[0-9]')","76097283":"research_papers.contains(\"Fauci\", column='authors').since_sarscov2()","65efbca9":"paper = research_papers[197]\npaper","d97daf6e":"paper = research_papers['asf5c7xu'] # or research_papers['5c31897d01f3edc7f58a0f03fceec2373fcfdc3d']\npaper","08f901e5":"research_papers.query(\"sha=='5c31897d01f3edc7f58a0f03fceec2373fcfdc3d'\")","9f957b93":"from ipywidgets import interact\nfrom IPython.display import display\n\npaper = research_papers['asf5c7xu']\n\ndef view_paper(ViewPaperAs):\n    if ViewPaperAs == 'Overview':\n        display(paper)\n    elif ViewPaperAs == 'Abstract':\n        display(paper.abstract)\n    elif ViewPaperAs == 'Summary of Abstract':\n        display(paper.summary)\n    elif ViewPaperAs == 'HTML':\n        display(paper.html)\n    elif ViewPaperAs == 'Text':\n        display(paper.text)\n    elif ViewPaperAs == 'Summary of Text':\n        display(paper.text_summary)\n    \ninteract(view_paper,\n         ViewPaperAs=['Overview', # Show an overview of the paper's important fields and statistics\n                      'Abstract', # Show the paper's abstract\n                      'Summary of Abstract', # Show a summary of the paper's abstract\n                      'HTML', # Show the paper's contents as (slightly) formatted HTML\n                      'Text', # Show the paper's contents\n                      'Summary of Text' # Show a summary of the paper's content\n                     ]\n        );","acddffa4":"research_papers.similar_to('asf5c7xu')","ac64898c":"import pandas as pd\nimport numpy as np\nimport ipywidgets as widgets\nfrom cord.core import cord_support_dir\n\n# Load the document vectors\nvectors = pd.read_parquet(cord_support_dir()\/ 'DocumentVectors.pq').reset_index()\ndisplay(widgets.HTML('<h4>Document Vectors<\/h4>'))\ndisplay(vectors.head())\n\n# Use the metadata of the research_papers\nmetadata = research_papers.metadata.copy()\n\nvector_metadata_merge = vectors.merge(metadata, on='cord_uid', how='right')\nvectors['covid_related'] = vector_metadata_merge.covid_related\nvectors['published'] = vector_metadata_merge.published\ndisplay(widgets.HTML('<h4>Document Vectors with covid_related and published<\/h4>'))\ndisplay(vectors.head())","df05a23c":"import altair as alt\n\n\nvector_df = pd.DataFrame({'x':vectors.x,\n                          'y': vectors.y,\n                          '1d': vectors['1d'],\n                          'cluster': vectors.cluster,\n                          'covid_related': vectors.covid_related,\n                          'published': vectors.published})  # Ensure 5000 limit\n\nalt.Chart(vector_df.sample(5000)).mark_point().encode(\n       x=alt.X('x', axis=None),\n       y=alt.Y('y', axis=None),\n       color= 'cluster:N'\n    ).properties(\n        title='CORD Research Papers in 2D space',\n        width=600,\n        height=400\n    ).configure_axis(\n        grid=False\n    ).configure_view(\n        strokeWidth=0\n    )","34383209":"import matplotlib.style as style\nstyle.use('fivethirtyeight')\ncovid_count = vector_df[['cluster', 'covid_related']].groupby(['cluster']).sum() \ncluster_count = vector_df[['cluster', 'covid_related']].groupby(['cluster']).count() \ncovid_cluster_stats = (covid_count \/ cluster_count) * 100\ncovid_cluster_stats = covid_cluster_stats.sort_values(['covid_related'])\nfig = covid_cluster_stats.plot.barh(grid=False, figsize=(6, 3), legend=False, title='% Covid Related');","1c060506":"vector_since = vector_df.query('published > \"2015-01-01\" & (cluster==2 | cluster==6)').copy()\nvector_since.loc[vector_since.published> '2020-06-30', 'published'] = pd.to_datetime('2020-03-30')\nif len(vector_since) > 5000:\n    vector_since = vector_since.sample(5000)\nalt.Chart(vector_since).mark_point().encode(\n       x=alt.X('published:T'),\n       y=alt.Y('1d'),\n       color= 'cluster:N'\n    ).properties(\n        title='CORD Research Papers since 2015',\n        width=600,\n        height=400\n    ).configure_axis(\n        grid=False\n    ).configure_view(\n        strokeWidth=0\n    )","af89d18e":"query =\"\"\"\nEfforts to identify the underlying drivers of fear, anxiety and stigma that\nfuel misinformation and rumor, particularly through social media.\n\"\"\"\nresearch_papers.search_2d(query)","1d09ca0e":"import pandas as pd\nimport numpy as np\nfrom rank_bm25 import BM25Okapi\npd.options.display.max_colwidth=160\n\nmeta_df = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv') # Or pd.read_csv()\nmeta_df = meta_df[['title', 'abstract', 'publish_time']].head(1000)\nmeta_df","c04db6b4":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import preprocess_documents, preprocess_string\n\nmeta_df_tokens = meta_df.abstract.fillna('').apply(preprocess_string)","563d8318":"from rank_bm25 import BM25Okapi\nimport numpy as np\n\nbm25_index = BM25Okapi(meta_df_tokens.tolist())\n\ndef search(search_string, num_results=10):\n    search_tokens = preprocess_string(search_string)\n    scores = bm25_index.get_scores(search_tokens)\n    top_indexes = np.argsort(scores)[::-1][:num_results]\n    return top_indexes\n\nindexes = search('novel coronavirus treatment')\nindexes","81834bd3":"meta_df.loc[indexes, ['abstract', 'publish_time']]","bcc8ddc3":"meta_df.loc[search('novel coronavirus treatment')]","af23ce18":"import multiprocessing\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Collection, Any\n\ndef is_notebook():\n    try:\n        from IPython import get_ipython\n        return get_ipython().__class__.__name__ == \"ZMQInteractiveShell\"\n    except (NameError, ImportError):\n        return False\n\nif is_notebook():\n    from tqdm.notebook import tqdm\nelse:\n    from tqdm import tqdm\n    \ndef ifnone(a: Any, b: Any) -> Any:\n    return b if a is None else a\n\ndef parallel(func, arr: Collection, max_workers: int = None):\n    \"Call `func` on every element of `arr` in parallel using `max_workers`.\"\n    max_workers = ifnone(max_workers, multiprocessing.cpu_count())\n    progress_bar = tqdm(arr)\n    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n        futures_to_index = {ex.submit(func, o): i for i, o in enumerate(arr)}\n        results = []\n        for f in as_completed(futures_to_index):\n            results.append((futures_to_index[f], f.result()))\n            progress_bar.update()\n        for n in range(progress_bar.n, progress_bar.total):\n            time.sleep(0.1)\n            progress_bar.update()\n        results.sort(key=lambda x: x[0])\n    return [result for i, result in results]","2bff8f25":"from functools import partial\n\ndef get_text(paper_json, text_key) -> str:\n    \"\"\"\n    :param paper_json: The json\n    :param text_key: the text_key - \"body_text\" or \"abstract\"\n    :return: a text string with the sections\n    \"\"\"\n    body_dict = collections.defaultdict(list)\n    for rec in paper_json[text_key]:\n        body_dict[rec['section']].append(rec['text'])\n\n    body = ''\n    for section, text_sections in body_dict.items():\n        body += section + '\\n\\n'\n        for text in text_sections:\n            body += text + '\\n\\n'\n    return body\n\n\nget_body = partial(get_text, text_key='body_text')\nget_abstract = partial(get_text, text_key='abstract')\n\ndef author_name(author_json):\n    first = author_json.get('first')\n    middle = \"\".join(author_json.get('middle'))\n    last = author_json.get('last')\n    if middle:\n        return ' '.join([first, middle, last])\n    return ' '.join([first, last])\n\n\ndef get_affiliation(author_json):\n    affiliation = author_json['affiliation']\n    institution = affiliation.get('institution', '')\n    location = affiliation.get('location')\n    if location:\n        location = ' '.join(location.values())\n    return f'{institution}, {location}'\n\n\ndef get_authors(paper_json, include_affiliation=False):\n    if include_affiliation:\n        return [f'{author_name(a)}, {get_affiliation(a)}'\n                for a in paper_json['metadata']['authors']]\n    else:\n        return [author_name(a) for a in paper_json['metadata']['authors']]","8bc960f8":"research_papers.display('dao10kx9', 'rjc3b4br',  'r0lduvs1', '7i422cht', 'pa9h6d0a', 'dbzrd23n', '5gbkrs73', '94tdt2rv', \n                        'xsgxd5sy', 'jf36as70', 'uz91cd6h')","7fb218cb":"from typing import Dict, List\nimport requests\n\nSPECTER_URL = \"https:\/\/model-apis.semanticscholar.org\/specter\/v1\/invoke\"\nMAX_BATCH_SIZE = 16\n\ndef chunks(lst, chunk_size=MAX_BATCH_SIZE):\n    \"\"\"Splits a longer list to respect batch size\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i: i + chunk_size]\n\n\ndef get_embeddings_for_papers(papers: List[Dict[str, str]]):\n    embeddings_by_paper_id: Dict[str, List[float]] = {}\n    for chunk in chunks(papers):\n        # Allow Python requests to convert the data above to JSON\n        response = requests.post(SPECTER_URL, json=chunk)\n\n        if response.status_code != 200:\n            print(\"Something went wrong on the spector API side .. try again\")\n            return None\n\n        for paper in response.json()[\"preds\"]:\n            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n\n    return embeddings_by_paper_id\n\ndef get_embeddings(title: str, abstract: str = None):\n    abstract = abstract or title\n    paper = {\"paper_id\": \"paper\", \"title\": title, \"abstract\": abstract}\n    embeddings = get_embeddings_for_papers([paper])\n    return embeddings['paper'] if embeddings else None\n\ndef plot_embeddings(vector):\n    df = pd.DataFrame(vector)\n    ax = df.plot.bar(figsize=(10,1))\n    ax.get_legend().remove()\n    ax.axes.set_xticklabels([])\n    ax.axes.set_yticklabels([])","7dc7a1f0":"embeddings = get_embeddings('Animal to human transmission')\n\nembeddings[:10]","893b7a26":"plot_embeddings(embeddings)","d2dc2779":"plot_embeddings(get_embeddings('Animal to human viral transmission'))\nplot_embeddings(get_embeddings('Bat to human viral transmission'))","4238be2f":"![covidpapercluster.png](attachment:covidpapercluster.png)","31ea7a1b":"Or use the search function directly in **meta_df.loc[]**","b704ad61":"# 2. Loading Research Papers","0802a3e7":"### Plot the embeddings","b916d9d9":"# 6. Exploratory Analysis and Charts","dfc104e5":"# References\n- [Wikipedia article on Okapi BM25](https:\/\/en.wikipedia.org\/wiki\/Okapi_BM25)\n- [The rank_bm25 python library](https:\/\/pypi.org\/project\/rank-bm25\/)\n- [Discussion on CORD Dataset Changes](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137474)","af7352cc":"# 7. Technical Notes\nWhat follows here will be a discussion of the technical design of the CORD library with code snippets","dbac0262":"### Selecting a paper by cord_uid\nYou can also select a paper using its **cord_uid**.","97d6a0cd":"# 5. Selecting Individual Papers\nIndividual papers can be selected from the ResearchPapers instance using the **[]** selectors, using the index number of the paper. Note that you will likely never know the index value of the paper that you need, but this ability will come in handy when using the search tool.","253ead0b":"### Creating a simple BM25 index\nTo show how to create a BM25 index, we will first load the metadata, and just use a subset of the columns and the rows","51b46518":"# A small contribution to the world from Canada","9a1a7173":"# 3. Searching Research Papers","e49e3fb5":"### Research papers since SARS-COV-2","fd339737":"### Identify which clusters are covid related","ae7ae65d":"### Widgets in Jupyter notebook\nThe code above also show how to use **ipywidgets interact** and **IPython display** to provide a userinterface inside a Jupyter notebook. ","c6fbd1a8":"### Selecting a paper by index\nYou can select any paper in the **ResearchPapers** instance using the index. This will locate and create a new **Paper** instance, which you can output to the notebook.","b0b99b78":"### Chart showing clusters of research papers\nThe chart below shows the research papers in 2D space. The chart below is done in **Altair** - the best Python charting library.","cd0dfcdc":"## Implementing the Specter Vector API\n\nThe AllenAI team has provided a public API for their Specter Vectors, which allows you to get the 768 dimension representation of a paper's title, abstract or text. Here is one implementation.\n\nFollow the official guide here\nhttps:\/\/github.com\/allenai\/paper-embedding-public-apis","9bb06f20":"## Vote for the kernel\nIf you have made it this far - please vote for the kernel. Thank you!","896ef238":"### With the searchbar\nYou can call searchbar() without any arguments to show the widget without any initial search results, or with an initial search string, which we will do below.","6ea79b85":"### Viewing a Research Paper\nOnce you have a research paper, there are many ways to view it.\n\n- **Overview** A nicely formatted view of the paper's important fields\n- **Abstract** The paper's abstract\n- **Summary** A summary of the paper's abstract using the **TextRank** algorithm\n- **Text** The text the paper\n- **HTML** The contents of the paper as somewhat nicely formatted HTML\n- **Text Summary** The text of the paper, summarized using the **TextRank** algorithm","11c17dd2":"## Table of Contents\n\n<a href=\"#1.--Installing-the-dgunning\/cord19-Library\">1. Installing the cord library<\/a>\n\n<a href=\"#2.--Loading-Research-Papers\">2. Loading Research Papers<\/a>\n\n<a href=\"#3.-Searching-Research-Papers\">3. Searching Research Papers<\/a>\n\n&nbsp;&nbsp;<a href=\"#With-the-search-function\">  With the search function<\/a>\n\n&nbsp;&nbsp;<a href=\"#With-the-searchbar\">  With the searchbar<\/a>\n\n<a href=\"#4.-Selecting-Research-Papers\">4. Selecting Research Papers<\/a>\n\n<a href=\"#5.-Selecting-Individual-Papers\">5. Selecting Individual Papers<\/a>\n\n<a href=\"#6.-Exploratory-Analysis-and-Charts\">6. Exploratory Analysis and Charts<\/a>\n\n<a href=\"#7.-Technical-Notes\">7. Technical Notes<\/a>\n\n&nbsp;&nbsp;<a href=\"#What-is-BM25\">  What is BM25<\/a>\n\n&nbsp;&nbsp;<a href=\"#Preprocessing-Text\">  Preprocessing Text<\/a>\n\n&nbsp;&nbsp;<a href=\"#Loading-JSON\">  Loading JSON<\/a>\n\n&nbsp;&nbsp;<a href=\"#Parallel-Processing-Code\">  Parallel Processing Code<\/a>\n\n\n\n","3da817fc":"## Example of displaying Research Papers on Information Sharing\nThe CORD research tool can be use to find papers specific for the tasks in this dataset. Here is an example","d3499367":"### Research Papers that match the H{num}N{num} pattern\nThe code below returns papers that match the regex \"H[0-9]N[0-9]\"","01447400":"![interactivesearch.png](attachment:interactivesearch.png)","7815f48c":"A more detailed notebook on **Information Sharing** is here https:\/\/www.kaggle.com\/dgunning\/cord-research-on-information-sharing","fd8cd527":"### Finding Similar Papers","d91731c6":"You can use the indexes to locate the rows in **meta_df** which best match the search terms","6b35a857":"This would be similar to querying research_papers for a single sha, but in this case a **ResearchPapers** instance is returned.","27086c61":"# Task Notebooks\n\nThis notebook is used as the base for the following Task notebooks\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/transmission-incubation-and-environment-stability\" target=\"_blank\">1. What is known about transmission, incubation, and environmental stability?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/what-do-we-know-about-covid-19-risk-factors\" target=\"_blank\">2. What do we know about COVID-19 risk factors?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/what-we-know-about-genetics-origin-and-evolution\" target=\"_blank\">3. What do we know about virus genetics, origin, and evolution?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/what-we-know-about-vaccines-and-therapeutics\" target=\"_blank\">4. What do we know about vaccines and therapeutics?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/what-we-know-about-medical-care\" target=\"_blank\">5. What has been published about medical care?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/current-research-nonpharmaceutical-interventions\" target=\"_blank\">6. What do we know about non-pharmaceutical interventions?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/current-research-on-diagnostics-and-surveillance\" target=\"_blank\">7. What do we know about diagnostics and surveillance?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/what-we-know-about-social-and-ethical-concerns\" target=\"_blank\">8. What has been published about ethical and social science considerations?<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/dgunning\/cord-research-on-information-sharing\" target=\"_blank\">9. What has been published about information sharing and inter-sectoral collaboration?<\/a>","c4d2e378":"### Preprocessing Text\nTo create the index, we need a list of the tokens in each string. The easiest way to do so is by using **gensim** or **NLTK** to tokenize each abstract into a list of tokens. Here we use **gensim**","acc5cdc3":"## Parallel Processing Code\nCreating the BM25 index from the contents of over 44000 research papers requires running the preprocessing and tokenization in parallel. The following code includes a function called **parallel** which applies a function over a Collection. The parallel function is a modification of a function by the same name from the **fastai v2 repository**, with the main changes being using **tqdm** progress bars, and ensuring the output list is sorted in the same order as the input list. \n\nOnce the **parallel** function is defined it can be used as follows `papers = parallel(load_json, list(json_catalog_path.glob('*.json')))`\n\nClick **Code** to view","483e1e8d":"The ResearchPapers instance provides two main ways of searching - using the function **search()** or using the function **searchbar()** which will show an interactive search bar. \n\n\n\n","b1775bc8":"![covidblue.png](attachment:covidblue.png)","e888b89a":"### Research Papers since COVID\n\nWe plot the 1d compression of the 768-dimension document vector over time, with the clusters identified by color. Then we plot clusters 2 and 6. We see a big increase in the number of papers since COVID appeared","d563af22":"![search2d.png](attachment:search2d.png)","8db08191":"## Connect on LinkedIn\nhttps:\/\/www.linkedin.com\/in\/dwight-gunning-860124\/","e5677fd2":"### Load Document Vectors\n\nEach research paper's content was converted to a document vector using **gensim's Doc2Vec**. We load the document vectors and filter out any empty vectors. (We have some empty vectors where the research paper was not associated with any JSON content)","7a564ce5":"### New Feature - 2D Visual Search","e8636106":"## What is BM25\n\n**BM25** stands for **Best Match the 25th iteration** and is a text search algorithm first developed in 1994. It is one of the best search algorithms available, and **Lucene**, and its derivatives **Solr** and **ElasticSearch** switched to a **BM25** variant around 2015.","6ee87136":"### With the search function\n\nThe ResearchPapers class has a function **search** in which you can specify search terms. The function will use the **BM25** index to retrieve the documents that most closely satisfy the search","6f92098e":"### Get the Embeddings\n\n","3e0e752a":"![code_for_canada.jpg](attachment:code_for_canada.jpg)","8f48048a":"## Loading JSON\n\nHere are some of the functions for loading JSON files\n\nClick **Code** to view","d134a7ae":"# Technical Design Notes\nFor current details on the design of the **cord** library, check the project on [github\/dgunning\/cord19](https:\/\/github.com\/dgunning\/cord19)\n\nThe **ResearchPapers** class is a container for the metadata, and the BM25 search index. It contains functions to find papers using **index** `research_papers[0]`,  **cord_uid** `research_papers[\"4nmc356g\"]`, **OR** to create subsets of ResearchPapers like `papers.since_sarscov2`, **OR** to run `search()` or display the `searchbar()`\n\nBecause the ResearchPapers class is simply a container for the metadata dataframe, and all useful information about each paper is on the dataframe as a column, including the **index_tokens**, tags such as **covid_related** etc, subsetting ResearchPapers is simply a matter of subsetting the **metadata** dataframe, then creating a new ResearchPapers instance. To create a ResearchPapers instance after a date means \n```{python}\n    def after(self, date, include_null_dates=False):\n        cond = self.metadata.published >= date\n        if include_null_dates:\n            cond = cond | self.metadata.published.isnull()\n        return self._make_copy(self.metadata[cond])\n```\nThus, we implement functions such as **head**, **tail**, **sample**, **query**, which just delegate to the metadata dataframe function and then create a new ResearchPapers instance.\n\n## What happens in ResearchPapers.load?\n1. **load_metadata** - Load the **metadata.csv** file\n```python\n@staticmethod\n    def load_metadata(data_path=None):\n        if not data_path:\n            data_path = find_data_dir()\n\n        print('Loading metadata from', data_path)\n        metadata_path = PurePath(data_path) \/ 'metadata.csv'\n        dtypes = {'Microsoft Academic Paper ID': 'str', 'pubmed_id': str}\n        renames = {'source_x': 'source', 'has_full_text': 'has_text'}\n        metadata = pd.read_csv(metadata_path, dtype=dtypes, low_memory=False,\n                               parse_dates=['publish_time']).rename(columns=renames)\n```\n2. **clean_metadata** - Clean the metadata\n```python\ndef clean_metadata(metadata):\n    print('Cleaning metadata')\n    return metadata.pipe(start) \\\n        .pipe(clean_title) \\\n        .pipe(clean_abstract) \\\n        .pipe(rename_publish_time) \\\n        .pipe(add_date_diff) \\\n        .pipe(drop_missing) \\\n        .pipe(fill_nulls) \\\n        .pipe(apply_tags)\n```\n\n3. **Create the BM25 Search Index**\n\n**ResearchPapers** can be indexed with the metadata *abstracts* OR with the *text* content of the paper. Indexing from the abstracts is straightforward - we just apply a **preprocess** function to clean and tokenize the abstract. Indexing from the texts - if no json-cache exists - happens by loading the JSON files and, tokenizing the texts and setting the **index_tokens** on the metadata. However, there is now a **json_cache** dataset comprised of the preprocessed text tokens, along with the JSOn file's sha - which we use to merge into the metadata.\n\nAfter the metadata is loaded and cleaned we create the **BM25** index inside of **ResearchPapers.__init__()**\n\n```python\n      \nif 'index_tokens' not in metadata:\n    print('\\nIndexing research papers')\n    if any([index == t for t in ['text', 'texts', 'content', 'contents']]):\n        _set_index_from_text(self.metadata, data_dir)\n    else:\n        print('Creating the BM25 index from the abstracts of the papers')\n        print('Use index=\"text\" if you want to index the texts of the paper instead')\n        tick = time.time()\n        self.metadata['index_tokens'] = metadata.abstract.apply(preprocess)\n        tock = time.time()\n        print('Finished Indexing in', round(tock - tick, 0), 'seconds')\n\n```\n\n## Creating Document Vectors\nCreating document vectors is simple. First we load the cached JSON tokens. (This is a cached version of the JSON files optimized for memory and disk space)\n\n```python\njson_tokens = []\nfor catalog in JSON_CATALOGS:\n    json_cache = load_json_cache(catalog)\n    json_tokens.append(json_cache)\n    \njson_tokens = pd.concat(json_tokens, ignore_index=True)\n```\n\nthen we train a **gensim Doc2Vec** model to create vectors with length **VECTOR_SIZE** (currently 20)\n\n```python\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(json_tokens.index_tokens)]\nmodel = Doc2Vec(documents, vector_size=VECTOR_SIZE, window=2, min_count=1, workers=8)\n```\nNext we can create a document vector for each json record\n\n```python\njson_tokens['document_vector'] = json_tokens.index_tokens.apply(model.infer_vector)\n```\n\n### Document Similarity Index\nThe document similarity index is based on [Annoy](https:\/\/github.com\/spotify\/annoy). Annoy is very simple and super fast, and will return the mst similar items to a given query.\n\n```python\nfrom annoy import AnnoyIndex\n\nannoy_index = AnnoyIndex(DOCUMENT_VECTOR_LENGTH, 'angular')  \nfor i in range(len(metadata)):\n    v = json_tokens.loc[i].document_vector\n    annoy_index.add_item(i, v)\n\nannoy_index.build(10) # 10 trees\n```","2f4348a4":"### Plot multiple embeddings\n\nIf we plot multiple embeddings together we can see the similarity of the vectors for similar queries","e5b57fb1":"### Match string columns\nTo select research papers that match on a string column, use `research_papers.match(<searchstring>)`. If no column is specified, then the abstract is used.\n\nThe following shows what has been published by **Anthony Fauci** since the SARS-COV-2 outbreak","e05f17d2":"# 4. Selecting Research Papers\n\nThere are many ways to select subsets of research papers including\n\n- **Papers since SARS**  `research_papers.since_sars()`\n- **Papers since SARS-COV-2** `research_papers.since_sarscov2()`\n- **Papers before SARS** `research_papers.before_sars()`\n- **Papers before SARS-COV-2** `research_papers.before_sarscov2()`\n- **Papers before a date** `research_papers.before('1989-09-12')`\n- **Papers after a date** `research_papers.after('1989-09-12')`\n- **Papers that contains a string** \n- **Papers that match a string (using regex)** \n\nHere we are interested in research papers since the sars-cov-2 outbreak","db13e36a":"# 1.  Installing the dgunning\/cord19 Library\n\nThe code for this kernel is maintained at https:\/\/github.com\/dgunning\/cord19. To install it in the Kaggle kernel, the Internet must be set to ON.","e1c9ecc5":"Now we create a **BM25Okapi** index from the tokens. We also implement a search function that returns the top 10 results from the search.\nNote that in search wer are asking the index to return the dataframe indexes of the tokens most similar to the search string.","9bb19997":"# Building a CORD19 Research Engine\n\nIn this notebook we build a **CORD Research Engine** on top of a **BM25** search index, along with a full document **Similarity Index** by storing the 786 dimension **Specter Vectors** in an **Annoy** index. Both these indexes complement each other - **BM25** is unmatched for search while the Specter Vectors provide unprecedented accuracy in similarity matching.\n\nThese indexes are complemented an intuitive user interface - both with an **interactive Search Bar** as well as a natural language like Python API `papers.similar_to`, all running within a notebook. The combination of these features pushes the boundary of what can be achieved in notebooks in terms of speed of information retrieval, assembly and publication. This is shown in the <a href=\"#Task-Notebooks\">9 task specific notebooks<\/a> created with the help of the CORD Research Engine.\n\nOf course the entire point to to provide insights into how to fight the SARS-COV2 pandemic, and we prioritize the surfacing of actionable research in this main notebook and in the task specific notebooks. The code for the engine is also available on github as will be explained below."}}